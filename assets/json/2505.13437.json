{
    "paper_title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance",
    "authors": [
        "Dian Shao",
        "Mingfei Shi",
        "Shengda Xu",
        "Haodong Chen",
        "Yongle Huang",
        "Binglu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advances in video generation, synthesizing physically plausible human actions remains a persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as \"switch leap with 0.5 turn\" poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, a Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce a physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhys's ability to generate more natural and plausible fine-grained human actions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 7 3 4 3 1 . 5 0 5 2 : r FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance Dian Shao1* Mingfei Shi1 Shengda Xu2 Haodong Chen3 Yongle Huang3 Binglu Wang4 1Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China 2School of Software, Northwestern Polytechnical University, Xian, China 3School of Automation, Northwestern Polytechnical University, Xian, China 4School of Astronautics, Northwestern Polytechnical University, Xian, China"
        },
        {
            "title": "Abstract",
            "content": "Despite significant advances in video generation, synthesizing physically plausible human actions remains persistent challenge, particularly in modeling fine-grained semantics and complex temporal dynamics. For instance, generating gymnastics routines such as switch leap with 0.5 turn poses substantial difficulties for current methods, often yielding unsatisfactory results. To bridge this gap, we propose FinePhys, Fine-grained human action generation framework that incorporates Physics to obtain effective skeletal guidance. Specifically, FinePhys first estimates 2D poses in an online manner and then performs 2D-to-3D dimension lifting via in-context learning. To mitigate the instability and limited interpretability of purely data-driven 3D poses, we further introduce physics-based motion re-estimation module governed by Euler-Lagrange equations, calculating joint accelerations via bidirectional temporal updating. The physically predicted 3D poses are then fused with data-driven ones, offering multi-scale 2D heatmap guidance for the diffusion process. Evaluated on three fine-grained action subsets from FineGym (FXJUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms competitive baselines. Comprehensive qualitative results further demonstrate FinePhyss ability to generate more natural and plausible fine-grained human actions. Project Page: FinePhys Webpage. 1. Introduction The rapid evolution of generative models, particularly diffusion models [27, 57], has significantly advanced progress in video generation. However, new challenges have emerged, as modeling temporal variationssuch as camera motions [72], background changes [37], and character movements [46]remains inherently difficult. These challenges *Corresponding Author Figure 1. Video generation results for fine-grained human action split leap with 1 turn. Our FinePhys demonstrates superior performance in generating physically plausible fine-grained human actions, while SOTA methods exhibit significant issues, including severe temporal inconsistencies [23], noticeable limb distortions [46], and character anomalies [7]. are especially pronounced in generating human actions, often leading to unnatural and inconsistent results [6, 18, 23]. Spatially, the human body exhibits strong structural coherence, which often causes models to generate abnormal anatomical features [19]. Temporally, motions must obey kinematic laws, yet recent studies [36] show that even stateof-the-art generative models fail to preserve fundamental physical principles such as Newtons laws of motion. In this work, we focus on an even more challenging task: 1 generating fine-grained human actions involving large body deformations and significant temporal changes. For example, when attempting to generate gymnastics actions, e.g., Split leap with 1 turn, existing state-of-the-art methods fail to provide satisfactory results (see Fig. 1). The biomechanical structure of human bodies in these cases is poorly preserved, let alone the plausibility of motion dynamics. To address these challenges, we introduce FinePhys, physics-aware framework for fine-grained human action generation, as shown in Fig. 2. Specifically, besides textual input, FinePhys first extracts online 2D poses from input videos, serving as compact prior for the biophysical structure. Then, using the newly proposed in-context learning technique, the 2D poses are lifted to 3D poses to enhance spatial perception. However, such purely data-driven 3D poses could ignore physical laws of motion, thus we propose PhysNet module that enforces Newtonian mechanics through Euler-Lagrange equations for rigid-body dynamics. This module bidirectionally re-estimates joint positions by modeling second-order kinematics (accelerations), yielding physics-refined 3D pose sequences. Finally, both data-driven and physically predicted 3D poses are fused, projected to 2D, and further encoded to provide multi-scale heatmaps, guiding the 3D-UNet denoising process. The key question is how to coherently incorporate physical laws into the learning process. Traditionally, there are three strategies [5, 81]: observational bias (via data), inductive bias (via networks), and learning bias (via losses). FinePhys integrates physics through all these asSpecifically, ❶ For observational bias, We inpects. clude pose as an additional modality to encode biophysical layouts and utilize in-context learning for 2D-to-3D lifting, where mean 3D poses from existing datasets are used as pseudo-3D references. ❷ To encode stronger inductive biases into FinePhys [34], we instantiate Lagrangian rigid body dynamics through fully differentiable neural network modules, whose output are parameters in the EulerLagrangian equation. ❸ For learning bias, we implement loss functions that adhere to the underlying physical processes. The main contributions are summarized as follows: We develop FinePhys, novel framework for fine-grained human action video generation, which employs skeletal data as structural priors and explicitly encodes Lagrange Mechanics via dedicated network modules; FinePhys incorporates physics into the generation process through multiple strategies, including the observational bias (2D-to-3D dimension lifting), inductive bias (PhysNet for parameter estimation in the Euler-Lagrangian equation), and learning bias (corresponding losses); Extensive experiments on fine-grained action subsets demonstrate that FinePhys significantly outperforms various baselines in producing more natural and physically plausible results. 2. Related Work Video Generation with Diverse Guidance. Video generation has been significantly advanced by the development of deep generative models [17, 55, 60], particularly diffusion models [27, 65]. The denoising process in diffusion models can be performed either directly in the pixel space or within lower-dimensional latent space [2, 25, 45, 57, 79], and FinePhys adopts the latter for efficiency. Early approaches in video generation extend the successful text-toimage (T2I) [51, 53, 54, 56, 57, 59] to text-to-video (T2V) generation [10, 12, 13, 46]. Although vivid frames could be produced, relying solely on textual guidance offers limited control over both spatial layouts and temporal dynamics. Recent methods have incorporated diverse forms of guidance and additional modalities to enhance control and realism in video generation, which roughly fall into two i.e., appearance [23, 26, 39, 77] and structure aspects, [13, 46, 88]. Examples from the former include generating videos conditioned on an image [35] (e.g., the first frame [23, 24], the last frame [52]) or enabling appearance customization into pre-trained T2I [20, 44]. The latter tries to utilize more structural guidance (e.g., depth [40, 41], skeleton [13, 33, 46, 87], edges [13], optical flows [41, 50], and trajectory [78]), combined with ad-hoc feature encoders [48, 82] to guide the generation process. To distinguish, our FinePhys online estimates 2D pose and transforms it into 3D skeletons for enhanced spatial guidance, and incorporates an awareness of motion laws through the proposed PhysNet module. Physics-informed Action/Motion Modeling. To achieve more realistic and reasonable motion modeling, several methods have emphasized the utilization of physics. Some works take advantage of Physics engines [21, 22, 29, 64, 80]. PhysGen [43] employs rigid-body physics simulations to convert single image and input forces into realistic videos, demonstrating the possibility of reasoning physical parameters from visual data. PhysDiff [81] also performs motion imitation within physics simulator by embedding physics-based projection module that iteratively guides the diffusion process. However, PhysDiff addresses only global artifacts such as penetration and floating, neglecting fine-grained human joint details. LARP [3] propose novel neural network as an alternative to traditional physics simulators to facilitate human action understanding from videos. Additionally, direct application of physical equations has also proved to be effective [74, 83, 86]. PIMNet [86] calculates human dynamic equations for future motions, but the joint state can be directly obtained from MoCap data, while our FinePhys estimates each state solely based on video input. Recently, PhysPT [84] has been proposed to estimate human dynamics from monocular videos based on SMPL representation. However, it incorporates physics into the training of neural networks through Lagrangian losses, 2 whereas our FinePhys explicitly estimates physical parameters of Euler-Lagrange equations (EL-Eq.). Furthermore, PhysMoP [85], designed for motion prediction, also relies on EL-Eq. but focuses on predicting future SMPL pose parameters based on previous ones, which is straightforward process. In contrast, FinePhys tackles more challenging task that involves modality transformation, dimension lifting, and visual content generation. Compared with previous works, FinePhys: (1) focuses on the extremely challenging task of generating fine-grained human action videos; (2) uses monocular videos as input, online estimates 2D poses, and transforms them into 3D through in-context learning; (3) explicitly instantiates EL-Eq. through the PhysNet module and calculates temporal variations of each joint bidirectionally without relying on simulators. 3. Methodology 3.1. Preliminaries Latent Diffusion Models (LDMs) [57] are widely used for generating visual content including images and videos. general pipeline is to use pre-trained autoencoder E() to compress high-dimension information into lowdimension latent representation z, i.e., E(x) = z, and then modeled with trainable DDPM [27]: (1) ℓ = EE(x),y,ϵN (0,1),t[ϵ ϵθ(zt, t, τθ(y)2 2], where τθ() denotes the condition encoder (e.g., CLIP text encoder [53]). The generated results are obtained via denoising in the latent space with condition guidance encoded. Physics of Rigid-body Dynamics could be modeled with diverse physical approaches (e.g., Newtonian, Lagrangian, or Hamiltonian), and they result in the equivalent sets of equations [49]. Among these, the EulerLagrange Equations (EL-Eq.) are widely used to predict the dynamics of rigid bodies. Assume as generalized coordinate system and q(t) as position function of time, is the Lagrangian. Given P(a, b, xa, xb) satisfying : [a, b] with q(a) = xa, q(b) = xb, the EL-Eq. can be defined as: qi (t, q(t), q(t)) qi (t, q(t), q(t)) = 0, dt (2) where and represent the velocities and accelerations of the joint respectively. For the kinematics of the full-body human model, the EL-Eq. can be converted into1 (q)q = J(q, q) C(q, q), (3) where is the generalized inertia matrix including body mass and other inertia terms; is vector of generalized forces acting on the human body, and denotes all other terms to enforce joint constraints. 1Details can be found in the Supplementary, as described in [42]. 3 3.2. Overview Task Definition and Problem Setting. In this work, we focus on novel and challenging task of generating finegrained human action videos. Specifically, the inputs during training are two folds: (1) in = {fi}T i=1: set of sampled frames from the entire video; and (2) textual descriptions that elaborate on the fine-grained category label (e.g., switch leap with one turn), enhanced by text extender (GPT-4 [1] here) to make them more comprehensible to the model [73]; During inference, the Gaussian noise is fed into the trained framework F. The output is fine-grained action videos out = { fi}T i=1, conditioned on the textual and 2D skeletal guidance, denoted as out = F(N oise, D, S2D). Overall Pipeline. The whole pipeline is illustrated in Fig. 2. Given video frames, FinePhys first performs online 2D pose estimation, producing the 2D skeleton sequence S2D RT J2. This sequence serves as an additional modality, providing compact and bio-structured representation of human actions. Subsequently, these 2D poses undergo an in-context learning process for dimensional lifting, resulting in data-driven 3D skeleton sequence S3D dd RT J3. However, most online estimators struggle to accurately estimate 2D poses, especially for complex movements like gymnastics [31, 62], leading to noisy 2D inputs. Moreover, the data-driven 2D-to-3D transformation lacks physical interpretability, making the estimation unreliable. To address these issues, we design physics-based module called PhysNet, which re-estimates the 3D motion dynamics by calculating bidirectional second-order temporal variations (i.e., accelerations) using well-established Euler-Lagrange equations, whose parameters are explicitly predicted. The refined 3D skeleton sequences, termed physically predicted 3D skeletons and denoted by S3D pp RT J3, are fused with S3D dd and then projected back to 2D to produce multi-scale latent maps. These skeletal heatmaps are integrated into various stages of the 3D-UNet architecture to guide the video generation process. For efficient tuning, we incorporate LoRA modules [23, 88] into the 3D-UNet structure. Specifically, for weight matrix Rdk, LoRA employs low-rank factorization technique to update the original parameters W0 Rdk with two trainable low-rank matrices Rdr and Rkr, where is smaller rank, i.e., = W0 + ABT . These lightweight modules effectively utilize the skeletal heatmaps to guide the denoising process. 3.3. FinePhys: Incorporating Physics The core of FinePhys lies in effectively leveraging the physical principles governing human motions. In the following paragraphs, we elaborate on the strategies and design to incorporate useful physics information into FinePhys: Observational Bias: 2D-to-3D Lifting. First, we clarify that the term bias here is not negative. Instead, it sigFigure 2. Overview of Finephys. FinePhys addresses the challenging task of generating fine-grained human action videos by explicitly incorporating physical equations exploiting pose modality. The pipeline begins with online extracting 2D poses, then transforms them into 3D using an in-context learning module, achieving the data-driven 3D skeleton sequence S3D dd . To incorporate the physical laws of motion, we introduce Phys-Net module to re-estimate the 3D positions of each human joint by accounting for second-order temporal variations (i.e., accelerations) in both forward and reverse directions, yielding physically predicted 3D poses S3D pp are fused, projected back into 2D space, encoded into multi-scale latent maps, and integrated into 3D-UNets to guide the denoising process. pp . Subsequently, S3D dd and S3D nifies the physical priors and experiences implicitly encoded in large-scale datasets [4]. To transform the 2D layout of human joints into 3D geometry, we employ an in-context learning (ICL) process that leverages these biases. Since only 2D skeletons are provided by the FineGym dataset [62], we first obtain pseudo 3D prior 3D RT J3, which is the calculated mean 3D skeleton sequence from widely used skeleton datasets, including Human3.6M [32] and AMASS [47]: (cid:88) (4) 3D = 1 i= 3D , where is the total number of samples from the above datasets. The ICL module requires few demonstration examples, typically input-output pairs, to form the prompts. In our approach, the prompts = {P 2D, 3D} consist of ground-truth 2D-3D skeleton pairs randomly selected from Human3.6M. The query = {S2D, 3D} is composed of detected 2D poses from FineGym video paired with the previously obtained 3D prior. The ICL process would generate data-driven 3D skeleton sequences, denoted by: dd = Trans2D3D (P, Q), S3D (5) where Trans2D3D is two-stream transformer composed of several spatial and temporal blocks as in [89]. This process is illustrated in the lower left of Fig. 2. Note that the whole 2D-to-3D lifting procedure benefits directly from the observed data, and the trainable module is expected to cap4 ture the underlying physical structures and rules, such as the relationship of limbs, the anatomical limits of joints, the spatial layout of the 3D human body, etc. Inductive Bias: Physics-Informed Module. Recent study [34] demonstrates that high-quality generative results are achieved through strong inductive biases incorporated within meticulously designed neural networks. Accordingly, we integrate PhysNet module into our framework to effectively exploit Lagrangian mechanics, as illustrated in Fig. 3. Our primary objective is to estimate the physical terms in the Euler-Lagrange equations to compute temporal variations. To accomplish this, given the data-driven 3D skeleton data S3D dd , we embed both global and local temporal dynamics using distinct encoders (i.e., global head E(g) and local head E(l) = E(g) q(g) = E(l) q(l)() q(l)() = E(l) where q(l)() represent the forward and reverse temporal direction, respectively. All subsequent computations are performed bidirectionally, with forward updates starting from the first three frames and reverse updates from the last three frames. For simplicity, the temporal direction ( and ) is omitted in the notation. Then we obtain the temporal state vector qt at each time step by fusing qglobal and qlocal . Using qt as input, we estimate the corresponding θ ): θ ( {S3D θ ( {S3D θ ( {S3D and q(l)() dd (t)}T dd (t 2), S3D dd (t), S3D t=1 ) RT (J3), dd (t 1), S3D dd (t 2)} ), (8) dd (t 1), S3D dd (t)} ), (7) (6) θ parameters within the Euler-Lagrange equation: (qt) qt = J(qt, qt) C(qt, qt). (9) Specifically, for the generalized forces ˆJt, and the joint constraints ˆCt in forward updating: (10) ˆJt = EPhys ˆCt = EPhys (qt) R51, (qt) R51. (11) Estimating the inverse inertia matrix 1 R5151 poses significant challenges due to its high dimensionality. As discussed in [34], imposing strong constraints or priors on the hypothesis space facilitates this estimation. Thus, we estimate 1 in two steps: (1) assume symmetry, and (2) incorporate Gaussian noise. The symmetry assumption is intuitively based on the prior knowledge that inertia tensors and mass matrices are typically symmetric in structural systems. Consequently, we define: ( ˆM 1 = EPhys = S( ( ˆM 1 ˆM 1 ) where ( ˆM 1 ( ˆM 1 However, motion can disrupt the bodys symmetry. To (qt) R5126, ) R5151, denotes the upper triangular matrix of ), and is the symmetric operation. (12) (13) ) ) account for this, we introduce noise parameter ˆNt: ˆN [i] = G( EPhys ˆNt = { ˆN [i] (qt) ), R51, } R5151, = {1, ..., 51}, (15) where is Gaussian sampling process with variance σ2 = 1, adding small random noise to each column vector ˆN [i] in ˆNt. With the estimated parameters, we compute: (14) qt = (( ˆM 1 ) + ˆNt) ( ˆJt ˆCt), (16) and then obtain the future states with the second-order central difference formula qt qt+12qt+qt1 : (dt)2 ˆqt+1 = qt (dt)2 + 2qt qt1 RT 51. Since updates occur bidirectionally, each middle timestep (excluding the first and last three) yields two estimates: ˆq t+1 from forward updating and ˆq t+1 from reverse updating. We average these estimates: ˆqt+1 = (ˆq t+1)/2. This results of state sequence ˆq = {ˆqt}T t=1 is input to the pose decoder Epose for the physically predicted 3D skeletons: t+1 + ˆq (17) pp = Epose(ˆq) RT 173. S3D (18) Learning Bias: Optimization Objectives. Learning bias refers to the incorporation of prior physical knowledge through penalty constraints. Specifically, the training of FinePhys involves the following stages, each employing loss functions adhering to the underlying physics: ① The pre-training stage aims to enhance the accuracy of 3D pose estimation from 2D inputs. To achieve this, we utilize large-scale datasets that provide ground truth 3D poses Figure 3. The PhysNet Module. Given the input S3D dd , PhysNet leverages both global and local temporal dynamics in bidirectional manner to estimate the terms of the Euler-Lagrange equations. By integrating with an ODE solver, the module can predict future and past states, thereby enhancing the original S3D dd across both temporal directions and producing physically predicted 3D sequences, denoted as S3D pp . S3D, such as Human3.6M and AMASS. The 2D poses are processed by both the in-context learning module and the PhysNet module, and are subsequently fused to obtain the estimated 3D poses: ˆS3D = F(S3D pp ). Additionally, within the PhysNet module, we introduce an auxiliary loss, Lnoise = (cid:80)T t=1 ˆNtF , to constrain the noise vector ˆNt that is added to perturb the symmetry of ˆM 1 (t denotes time steps). The loss during this stage is calculated as: dd , S3D (cid:88) (cid:88) L3D = ˆS3D t,j S3D t,j 2 2 + Lnoise, (19) j=1 where represents human joints, with = 17. t= ② The fine-tuning stage utilizes fine-grained human action videos from FineGym [62], which present greater challenges due to rapid temporal dynamics and significant body deformations. Additionally, FineGym does not provide ground truth 3D poses. Therefore, we project the estimated 3D poses ˆS3D t,j into 2D using the projection module and compute the re-projection loss in the 2D space: (cid:88) (cid:88) L2D = P( ˆS3D t,j ) S2D t,j 2 2 + Lnoise, (20) t=1 and Lnoise is as defined above. j=1 ③ Finally, the generation stage involves generating video frames, wherein the entire framework is trained end-to-end using generation losses. These losses include the spatial loss Lspat, the temporal loss Ltemp, and the appearance5 SALTO, totaling 350 videos. Specifically, We first train our skeletal heatmap encoder for 54k steps on the real-human part of HumanArt [33], following Follow-Your-Pose [46]. The 2D-to-3D module together with PhysNet module is pretrained on Human36M and AMASS (with 3D pose annotations) for 10 epochs. Then the PhysNet module and 2D projection module are fine-tuned based on 2D skeletons detected from FineGym online. We also tune the LoRA module for 8k steps on the fine-grained datasets FX-JUMP, FXTURN, and FX-SALTO. 4.2. Main Results The evaluations were conducted on three fine-grained human action subsets drawn from FineGym: FX-JUMP, FXTURN, and FX-SALTO. These subsets include challenging gymnastics actions executed by professional gymnasts. Prior to presenting detailed method comparisons, we introduce the evaluation metrics used for quantitative assessment and discuss specific anomalies observed when using them to evaluate fine-grained human action video generation. Evaluation Metrics. ❶ Automatic Metrics: We use PickScore [38] to measure the alignment between video frames and text prompts, CLIP Domain and CLIP Smooth Similarity [53] to evaluate semantic similarity and embedding stability, and Frechet Video Distance (FVD) [69] for video quality assessment. ❷ User Study: We conducted user study, leveraging human sensitivity and accuracy in assessing motion plausibility, bio-structure preservation, and visual acceptability. Specifically, participants were presented with set of videos simultaneously, including one video generated by our method alongside those from baseline methods. For each video, they rated the consistency of the following aspects on scale from 1 to 5: ① Text Alignment, ② Domain Consistency, and ③ Smooth Stability. The mean opinion score (MOS) is reported as the final result. Discussion and Improved Metrics. Existing metrics may be unreliable for evaluating video generation results [39]. In our experiments, we found that these evaluation anomalies are even more pronounced for fine-grained human action video generation. Below, we first elaborate limitations of the original CLIP-SIM and then introduce an improved version for more accurate evaluation. (1) The original CLIP-SIM metric measures semantic consistency (SC), domain consistency (DC), and temporal consistency (TC) are achieved by calculating similarities between textto-video, image-to-video, and video-to-video, respectively. However, for TC, fine-grained semantics are not well captured by CLIP [68], making it less effective. Additionally, DC relies on reference images generated by Stable Diffusion, which may yield high scores for entirely irrelevant visual content, as shown in Fig. 4. Moreover, the original TC only considers inter-frame similarity and comFigure 4. Original CLIP-SIM metrics fail to evaluate the generated results (e.g., T2I-Zero produces entirely irrelevant outputs yet achieves the highest smooth score according to the original CLIPSIM. In contrast, our enhanced CLIP-SIM* provides more reliable evaluation that better aligns with human judgment. debiased temporal loss Lad-temp, resulting in: Lspat = Ez0,c,ϵ,t,iU (1,T )[ϵ ϵθ(zt,i, t, τθ(c))2 2], Ltemp = Ez0,c,ϵ,t[ϵ ϵθ(zt, t, τθ(c))2 2], Lad-temp = Ez0,c,ϵ,t[ϕ(ϵ) ϕ(ϵθ(zt, t, τθ(c)))2 2], Lvideo = Lspat + Ltemp + Lad-temp, (24) where zt,i denotes the ith frame of zt, and ϕ is the debiasing operator as described in [88]. (21) (22) (23) 4. Experiments 4.1. Experimental Setup Training and Datasets. ❶ Pre-training Datasets: We first train the skeletal heatmap encoder on the HumanArt [33] dataset, using skeletons and images as inputs. This dataset contains large number of human skeleton-image pairs. To train our 2D-to-3D module, we collect diverse and realistic 3D human motion data for the pretraining phase of skeleton modeling, following the design of [71], including Human3.6M [32] and AMASS [70]. ❷ Fine-grained Action Datasets: we construct three subsets from FineGym [62]: FX-JUMP, FX-TURN, and FX-SALTO, derived from the Floor Exercise event in FineGym. These subsets possess different motion characteristics, and are used for tuning the FinePhys framework as well as for validation. Further details are provided in the Supplementary. Implementation Details. For the generation backbone, we use the official codebase of Stable Diffusion v1.5 [57] and the motion module checkpoints from AnimateDiff [23]. We extract video clips at resolution of 384 384 pixels, consisting of 16 frames each for training. Using FineGym, we selected 35 classes from FX-JUMP, FX-TURN, and FX6 Table 1. Comparison with state-of-the-art methods on two subsets of FineGym (FX-JUMP and FX-TURN). In this table, within Input denotes textual prompt, while P, I, D, and represent the pose, initial frame, depth map, and canny, respectively. In all cases, our FinePhys outperforms various baselines based on diverse conditions by large margin in terms of more reliable metrics, CLIPSIM*, and User Study (the insufficiency of the CLIP-SIM has been shown). Method Input User Study CLIP-SIM* Text. Domain. Smooth. Domain. Smooth. PickScore FVD Control-A-Video [13] arXiv23 Control-A-Video [13] arXiv23 VideoCrafter1 [7] arXiv23 Text2Video-Zero [37] ICCV23 Text2Video-Zero [37] ICCV23 Latte [45] arXiv24 Follow-Your-Pose [46] AAAI24 AnimateDiff [23] ICLR24 AnimateDiff [23] ICLR24 VideoCrafter2 [11] CVPR24 Follow-Your-Pose [46] AAAI24 AnimateDiff [23] ICLR24 AnimateDiff [23] ICLR24 FinePhys (Ours) T+D T+C T+I T+P T+P T+I T+P T+I T+P 3.43 3.10 2.53 1.93 1.83 1.97 2.20 2.20 2.73 2.23 2.67 3.17 3.20 4.13 3.13 2.63 2.57 1.80 1.90 2.03 2.13 2.57 2.60 2.50 2.53 3.07 3.20 3.86 w/o finetuning on FineGym 3.37 2.60 2.60 2.00 1.67 2.13 2.37 2.33 2.93 2.60 0.697 0.508 0.685 0.501 0.481 0.681 0.612 0.686 0.684 0. 0.706 0.520 0.682 0.509 0.484 0.675 0.627 0.686 0.699 0.651 w/ finetuning on FineGym 2.57 2.97 3.17 4.03 0.709 0.728 0.769 0.826 0.727 0.752 0.793 0.833 18.995 18.339 18.750 17.827 17.659 19.421 18.680 19.468 19.362 20. 19.360 19.070 19.705 19.941 632.68 637.79 510.09 897.61 904.50 590.41 640.12 704.74 535.79 697.73 506.26 522.14 529.38 484.49 Text. 26.456 18.755 24.821 19.368 16.725 27.197 27.198 28.629 26.604 26.296 28.929 26.791 27.033 27. CLIP-SIM Domain. Smooth. 0.640 0.591 0.591 0.613 0.620 0.693 0.647 0.669 0.629 0.714 0.587 0.546 0.583 0.520 0.900 0.899 0.869 0.921 0.978 0.906 0.888 0.938 0.881 0.964 0.905 0.880 0.873 0. pletely ignores changing motion dynamics. Therefore, the original CLIP-SIM metrics are inadequate for evaluating fine-grained human action video generation, as depicted in Fig. 4. (2) To provide more reliable evaluation, we introduce an improved version of the CLIP metrics: CLIP DS( , {Ij}) = 1 1 (cid:88) (cid:88) t=1 j=1 CLIP( ˆV (t), Ij), (25) CLIP TC( , Ref) = (cid:88) (cid:88) k=1 l=1 CLIP( (k), Ref (k)), (26) where are generated videos and {Ij}N j=1 are frames sampled from FineGym actions. For CLIP TC, Ref consists of randomly chosen reference videos from FineGym, and the calculation employs multi-step sampling strategy. As illustrated in Fig. 4, the new metrics provide more accurate evaluation of the generated results. Further details on these metrics are provided in the Supplementary Material. Quantitative Comparison with Baselines. Using the aforementioned metrics, we evaluate FinePhys against competitive baselines, including Control-A-Video [13], VideoCrafter1/2 [7, 11], Text2Video-Zero [37], Latte [45], Follow-Your-Pose [46], and AnimateDiff [23], on generating fine-grained human actions. Results are presented in Tab. 1. Despite the insufficiency of the original CLIP-SIM FinePhys significantly outperforms these baselines on the improved CLIP-SIM* metrics and the user study (which are widely recognized as more credible metrics for evaluating video generation results), demonstrating its superior ability to understand fine-grained human actions and generate more physically plausible motions. Qualitative Analysis. We visualize the generated results of FinePhys alongside baseline methods, including those with additional conditions [13, 23, 46] and purely textdriven approaches [7], as shown in Fig. 1 and Fig. 5. We Figure 5. Qualitative Results. Compared to other baselines, FinePhys demonstrates superior performance in understanding complex, fine-grained semantics, maintaining biomechanical consistency, and adhering to physical principles. observe that these strong baselines struggle to generate satisfactory results: VideoCrafter2 [11] (only textual conditions) frequently exhibits dramatic flaws like no actions and character anomalies; AnimateDiff [23] shows neglectable inconsistencies such as character changes and incorrect semantics for below; Control-A-Video [13] (conditioned on depth maps) fails to interpret the action dynamics correctly; Follow-Your-Pose [46] also relies on 2D skeletons, displays limb distortions and low visual quality. In contrast, FinePhys effectively interprets complex motion dynamics 7 Table 2. Quantitative evaluation of different pose results. Left Part shows results on Human3.6M in 3D spaces. Right Part shows results on FineGym in 2D spaces. Metrics include mean per joint position error (MPJPE), Normalized MPJPE (N-MPJPE), and mean per-joint velocity error (MPJVE). Note that S3D dd and S3D pp will be projected back into 2D for evaluation on FineGym. Pose Results S2D detect Sdd Spp Sdd+SMLP Sdd+Spp 3D Eval. on Human3.6M 2D Eval. on FineGym MPJPE N-MPJPE MPVPE MPJPE N-MPJPE MPVPE - 0.048 0.068 0.065 0.046 - 0.046 0.066 0.060 0.044 - 0.020 0.035 0.025 0.018 0.918 0.229 0.237 0.243 0. 0.254 0.215 0.213 0.237 0.147 0.379 0.108 0.097 0.140 0.094 (e.g., jumps with 0.5 turns), understands fine-grained semantics (e.g., turning in stand position with leg below horizontal), and better preserves the bio-structure of human body, being able to generate physically plausible actions. 4.3. Ablations and Analysis dd and S3D Transformation of skeleton data. We evaluate the pose results obtained from different modules and procedures by conducting the following two sets of experiments: ① Evaluation in 3D space is done on Human3.6M (3D pose annotations provided). The results are presented in Tab. 2. Since the actions in Human3.6M primarily involve daily activities with moderate pose variations, the in-context learning module achieves good results (S3D dd ). Additionally, averaging S3D pp reduces estimation errors, indicating that physically predicted poses can mitigate deviations in data-driven estimates and thereby validate our design. ② Evaluation in 2D space is performed on FineGym subsets (without 3D pose annotations). By projecting S3D dd and S3D pp into 2D, we obtain S2D dd and S2D pp , respectively, and compare them with the online estimated results S2D detect. As expected, S2D detect performs poorly due to extreme body deformations and rapid temporal changes. Notably, S2D pp exhibits higher accuracy, further validating the necessity of the PhysNet module for fine-grained action understanding. Combining dd and S2D S2D pp yields the best results, underscoring the significance of each module within our FinePhys. Robustness on noisy input during inference. Online 2D pose estimation for fine-grained action videos often produces highly noisy results, as illustrated in Fig. 6. Without additional processing and sophisticated designs, conditioning the generation process directly on such noisy pose inputs leads to poor outcomes, as the results from FollowYour-Pose [46] shown in Fig. 7. However, by leveraging the dimension lifting and PhysNet modules, FinePhys effectively restores distorted and missing poses, as shown in Fig. 6. These mechanisms collectively enhance the robustness of FinePhys when handling noisy pose inputs. Importance of the PhysNet module. ① First, without PhysNet, we would not obtain S3D pp . This abpp and S2D Figure 6. FinePhys effectively restores distorted and missing poses using the in-context learning and PhysNet modules, thereby providing enhanced skeletal guidance for the generation process. Figure 7. Generation results from FinePhys and Follow-YourPose [46] conditioned on noisy 2D pose input during inference. sence leads to inferior pose guidance, as shown in Tab. 2, and would logically lead to poorer generation results. ② To justify our design, we replaced PhysNet with simple MLP to embed the data-driven poses further. This substitution resulted in significantly worse performance, as demonstrated in the bottom rows of Tab. 2. Limitation and future work. While FinePhys significantly outperforms previous approaches in generating finegrained human action videos, it represents only an initial step. Generating fine-grained actions such as various salto, which involve simultaneous body rotations and rapid turns in the air, still remains highly intractable (thus not calculated in Tab. 1). Additionally, generating detailed frames may divert focus from the deeper integration of physical principles, thus we plan to utilize simpler scenarios and further explore the modeling of physics in future work. 5. Conclusion In this work, we address the challenging problem of generating fine-grained human action videos that involve significant body deformations and dynamic temporal changes. To address this issue, we propose FinePhys, physicsinformed framework that fully explores skeletal data as structural guidance. The core innovation lies in its comprehensive incorporation of physics through observational, inductive, and learning biases, i.e., by employing in-context learning for dimension lifting, by embedding the EulerLagrange equations within neural network modules, and by using appropriate loss functions. All these ensure the biophysical consistency and motion plausibility of the generated outputs. Both quantitative and qualitative results demonstrate FinePhyss superior performance. 8 6. Acknowledgments This work was founded by the National Natural Science Foundation of China (NSFC) under Grant 62306239, and was also supported by National Key Lab of Unmanned Aerial Vehicle Technology under Grant WR202413."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 13 [2] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 2 [3] Mykhaylo Andriluka, Baruch Tabanpour, Daniel Freeman, and Cristian Sminchisescu. Learned neural physics simulation for articulated 3d human pose reconstruction. In European Conference on Computer Vision, pages 320336. Springer, 2024. 2 [4] Chayan Banerjee, Kien Nguyen, Clinton Fookes, and George Karniadakis. Physics-informed computer vision: review and perspectives. arXiv preprint arXiv:2305.18035, 2023. 4 [5] Chayan Banerjee, Kien Nguyen, Clinton Fookes, and Karniadakis George. Physics-informed computer vision: review and perspectives. ACM Computing Surveys, 57(1):1 38, 2024. 2 [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1 [7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 1, [8] Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, and Dian Shao. Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 23012310, 2024. 19 [9] Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, and Dian Shao. Gaussianvton: 3d human virtual tryon via multi-stage gaussian splatting editing with image prompting. arXiv preprint arXiv:2405.07472, 2024. 20 [10] Haodong Chen, Lan Wang, Harry Yang, and Ser-Nam Lim. Omnicreator: Self-supervised unified generation with universal editing. arXiv preprint arXiv:2412.02114, 2024. 2 [11] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 7 [12] Harold Haodong Chen, Haojian Huang, Xianfeng Wu, Yexin Liu, Yajing Bai, Wen-Jie Shu, Harry Yang, and Ser-Nam Lim. Temporal regularization makes your video generator stronger. arXiv preprint arXiv:2503.15417, 2025. 2 [13] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. 2, 7 [14] Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song, and Ivor Tsang. Disentangling structured components: Towards adaptive, interpretable and scalable IEEE Transactions on Knowledge time series forecasting. and Data Engineering, 2024. 16 [15] Jinliang Deng, Feiyang Ye, Du Yin, Xuan Song, Ivor Tsang, and Hui Xiong. Parsimony or capability? decomposition delivers both in long-term time series forecasting. Advances in Neural Information Processing Systems, 37:6668766712, 2025. [16] Haodong Duan, Yue Zhao, Kai Chen, Dahua Lin, and Bo Dai. Revisiting skeleton-based action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 29692978, 2022. 13 [17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [18] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73467356, 2023. 1 [19] Guian Fang, Wenbiao Yan, Yuanfan Guo, Jianhua Han, Zutao Jiang, Hang Xu, Shengcai Liao, and Xiaodan Liang. Humanrefiner: Benchmarking abnormal human generation and refining with coarse-to-fine pose-reversible guidance. Proceedings of the European conference on computer vision (ECCV), 2024. 1 [20] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2 [21] Erik Gartner, Mykhaylo Andriluka, Erwin Coumans, and Cristian Sminchisescu. Differentiable dynamics for articuIn Proceedings of lated 3d human motion reconstruction. the IEEE/CVF conference on computer vision and pattern recognition, pages 1319013200, 2022. [22] Erik Gartner, Mykhaylo Andriluka, Hongyi Xu, and Cristian Sminchisescu. Trajectory optimization for physics-based reIn construction of 3d human pose from monocular video. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1310613115, 2022. 2 [23] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 1, 2, 3, 6, 7, 14 9 [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision, pages 330348. Springer, 2025. 2 [25] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. Proceedings of the European conference on computer vision (ECCV), 2024. 2 [26] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. 2 [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2, 3 [28] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. [29] Buzhen Huang, Liang Pan, Yuan Yang, Jingyi Ju, and Yangang Wang. Neural mocon: Neural motion control for physIn Proceedings of ically plausible human motion capture. the IEEE/CVF conference on computer vision and pattern recognition, pages 64176426, 2022. 2 [30] Haojian Huang, Haodong Chen, Shengqiong Wu, Meng Luo, Jinlan Fu, Xinya Du, Hanwang Zhang, and Hao Fei. Vistadpo: Video hierarchical spatial-temporal direct preference optimization for large video models. arXiv preprint arXiv:2504.13122, 2025. 19 [31] Yongle Huang, Haodong Chen, Zhenbang Xu, Zihan Jia, Haozhou Sun, and Dian Shao. Sefar: Semi-supervised fine-grained action recognition with temporal perturbation and learning stabilization. arXiv preprint arXiv:2501.01245, 2025. 3 [32] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):13251339, 2013. 4, 6, 13 [33] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: native skeleton-guided diffusion model for human image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1598815998, 2023. 2, 6, 13 [34] Zahra Kadkhodaie, Florentin Guth, Eero Simoncelli, and Stephane Mallat. Generalization in diffusion models arises In The from geometry-adaptive harmonic representations. Twelfth International Conference on Learning Representations, 2024. 2, 4, [35] Hitesh Kandala, Jianfeng Gao, and Jianwei Yang. Pix2gif: Motion-guided diffusion for gif generation. Proceedings of the European conference on computer vision (ECCV), 2024. 2 [36] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective, 2024. 1 [37] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. 1, 7 [38] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 6, 16 [39] Mingi Kwon, Seoung Wug Oh, Yang Zhou, Difan Liu, Joon-Young Lee, Haoran Cai, Baqiao Liu, Feng Liu, and Youngjung Uh. Harivo: Harnessing text-to-image models for video generation. Proceedings of the European Conference on computer Vision (ECCV), 2024. 2, 6, [40] Ariel Lapid, Idan Achituve, Lior Bracha, and Ethan Fetaya. Gd-vdm: Generated depth for better diffusion-based video generation. arXiv preprint arXiv:2306.11173, 2023. 2 [41] Jingyun Liang, Yuchen Fan, Kai Zhang, Radu Timofte, Luc Van Gool, and Rakesh Ranjan. Movideo: Motion-aware video generation with diffusion model. Proceedings of the European conference on computer vision (ECCV), 2024. 2 [42] Karen Liu and Sumit Jain. quick tutorial on multibody dynamics. Online tutorial, June, page 7, 2012. 3 [43] Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded imageto-video generation. In European Conference on Computer Vision ECCV, 2024. 2 [44] Haoming Lu, Hazarapet Tunanyan, Kai Wang, Shant Navasardyan, Zhangyang Wang, and Humphrey Shi. Specialist diffusion: Plug-and-play sample-efficient fine-tuning of text-to-image diffusion models to learn any unseen style. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14267 14276, 2023. 2 [45] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 2, [46] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 1, 2, 6, 7, 8 [47] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive In Proceedings of of motion capture as surface shapes. the IEEE/CVF international conference on computer vision, pages 54425451, 2019. 4, 13 [48] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. 2 10 [49] Richard Murray, Zexiang Li, and Shankar Sastry. mathematical introduction to robotic manipulation. CRC press, 2017. 3 [50] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generIn Proceedings of ation with latent flow diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1844418455, 2023. [51] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [52] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, and Sangpil Kim. Mevg: Multi-event video generation with text-to-video models. In European Conference on Computer Vision, pages 401418. Springer, 2024. 2 [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 3, 6, 14 [54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 2 [55] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [56] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3, 6, 13, 14 [58] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [59] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [60] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast In International conlarge-scale text-to-image synthesis. ference on machine learning, pages 3010530118. PMLR, 2023. 2 [61] Dian Shao, Yu Xiong, Yue Zhao, Qingqiu Huang, Yu Qiao, and Dahua Lin. Find and focus: Retrieve and localize video events with natural language queries. In Proceedings of the European Conference on Computer Vision (ECCV), pages 200216, 2018. 19 [62] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: hierarchical video dataset for fine-grained action underIn Proceedings of the IEEE/CVF conference on standing. computer vision and pattern recognition, pages 26162625, 2020. 3, 4, 5, 6, 13 [63] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Intra-and inter-action understanding via temporal action parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 730739, 2020. 19 [64] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Patrick Perez, and Christian Theobalt. Neural monocular 3d human motion capture with physical awareness. ACM Transactions on Graphics (ToG), 40(4):115, 2021. 2 [65] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [66] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 14 [67] Jianwei Tang, Jieming Wang, and Jian-Fang Hu. Predicting human poses via recurrent attention network. Visual Intelligence, 1(1):18, 2023. 20 [68] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 6 [69] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6, 16 [70] Timo Von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and moving camera. In Proceedings of the European conference on computer vision (ECCV), pages 601617, 2018. [71] Xinshun Wang, Zhongbin Fang, Xia Li, Xiangtai Li, Chen Chen, and Mengyuan Liu. Skeleton-in-context: Unified skeleton sequence modeling with in-context learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24362446, 2024. 6 [72] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 1 [73] Xiu-Shen Wei, Yu-Yan Xu, Chen-Lin Zhang, Gui-Song Xia, and Yu-Xin Peng. Cat: coarse-to-fine attention tree for semantic change detection. Visual Intelligence, 1(1):3, 2023. 3 11 prediction. 89498955, 2022. 2 IEEE Robotics and Automation Letters, 7(4): [87] Haoyu Zhao, Tianyi Lu, Jiaxi Gu, Xing Zhang, Qingping Zheng, Zuxuan Wu, Hang Xu, and Yu-Gang Jiang. Magdiff: Multi-alignment diffusion for high-fidelity video generation and editing. Proceedings of the European conference on computer vision (ECCV), 2024. 2 [88] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jia-Wei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-to-video diffusion models. In European Conference on Computer Vision, pages 273290. Springer, 2025. 2, 3, 6 [89] Wentao Zhu, Xiaoxuan Ma, Zhaoyang Liu, Libin Liu, Wayne Wu, and Yizhou Wang. Motionbert: unified perspective on learning human motion representations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1508515099, 2023. 4 [74] Kevin Xie, Tingwu Wang, Umar Iqbal, Yunrong Guo, Sanja Fidler, and Florian Shkurti. Physics-based human motion esIn Proceedings of the timation and synthesis from videos. IEEE/CVF International Conference on Computer Vision, pages 1153211541, 2021. 2 [75] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael Icon: Implicit clothed humans obtained from norBlack. In 2022 IEEE/CVF Conference on Computer Vimals. sion and Pattern Recognition (CVPR), pages 1328613296. IEEE, 2022. 20 [76] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael Black. Econ: Explicit clothed humans optimized via normal integration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 512523, 2023. 20 [77] Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, and Xiaokang Yang. Dialoguenerf: Towards realistic avatar faceto-face conversation video generation. Visual Intelligence, 2 (1):24, 2024. [78] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2 [79] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1845618466, 2023. 2 [80] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason Saragih. Simpoe: Simulated character control for 3d human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 71597169, 2021. 2 [81] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1601016021, 2023. 2 [82] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2 [83] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physics-based interaction with 3d objects via video generation. Proceedings of the European conference on computer vision (ECCV), 2024. 2 [84] Yufei Zhang, Jeffrey Kephart, Zijun Cui, and Qiang Ji. Physpt: Physics-aware pretrained transformer for estimating human dynamics from monocular videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23052317, 2024. [85] Yufei Zhang, Jeffrey Kephart, and Qiang Ji. Incorporating physics principles for precise human motion prediction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 61646174, 2024. 3 [86] Zhibo Zhang, Yanjun Zhu, Rahul Rai, and David Doermann. Pimnet: Physics-infused neural network for human motion 12 FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance"
        },
        {
            "title": "Contents",
            "content": "A. Training & Dataset Details . . . . . . . . . . . A.1. Overview . A.2. HumanArt Pre-training . . . . . A.3. Human3.6M and AMASS Pre-training . . . . . . . A.4. FineGym Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . B. Elaboration on Evaluation Metrics B.1. CLIP-SIM Metrics and Limitations B.2. The Improved CLIP-SIM* Metrics . B.3. Details of User Study . . . B.4. Other Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Additional Illustration & Analysis C.1. Elaboration on Euler-Lagrange Equations . . . . . . C.2. Visualization of the Pose Modality . C.3. More Generated Results and Comparison . . . . . . C.4. Limitation and Future Work. . . . . . . 13 13 13 13 13 14 14 15 16 16 17 17 17 18 18 A. Training & Dataset Details A.1. Overview We deploy FinePhys using PyTorch, and the training process consists of four steps: ❶ Pre-training the skeletal heatmap encoder on the HumanArt [33] dataset; ❷ Pretraining the 2D-to-3D module and the PhysNet module on Human3.6M [32] and AMASS [47] datasets; ❸ Fine-tune the 2D projection module and PhysNet module using the online detected 2D skeletons detected from FineGym [62]; ❹ Jointly fine-tuning the U-Net [58], PhysNet, and 2D projection modules on FineGym. The first three steps of training are conducted on Linux (Ubuntu) machine with 4 Nvidia 4090 GPUs within 48 hours, while step 4 utilizes two NVIDIA L20 GPUs and completes within 12 hours. Across all experiments, we apply linear noise scheduler with 1,000 timesteps, linearly increasing the beta values from 0.00085 to 0.012 to progressively reduce noise during training. The U-Net backbone incorporates motion module featuring temporal self-attention layers and positional encoding operating at resolutions [1, 2, 4, 8], enabling multi-scale temporal dynamics capture. The motion module is configured with eight attention heads, single transformer block, and dual temporal self-attention layers to effectively model temporal dependencies. To stabilize training, the module parameters are zero-initialized. We incorporate Low-Rank Adaptation (LoRA) [28] module with rank of 64 and dropout rate of 0.1, facilitating efficient 13 adaptation of the models spatial and temporal layers while minimizing the number of trainable parameters. Training utilizes the Adam optimizer with an initial learning rate of 5 104 and weight decay of 1 102. Additionally, gradient checkpointing is enabled to optimize GPU memory usage during training. A.2. HumanArt Pre-training Initially, we train the skeletal heatmap encoder on the HumanArt dataset, large-scale image collection containing 50K images with accurate pose and text annotations across various scenarios. We leverage the real-human subset, comprising 8,750 images with corresponding 2D skeleton annotations. The original COCO-format skeletons are converted to the Human3.6M format, both with 17 keypoints, and subsequently processed into limb heatmaps following the PoseConv3D approach [16]. We employ Stable Diffusion v1.5 [57] as the spatial generator and keep it frozen during training. A.3. Human3.6M and AMASS Pre-training To pre-train the 2D-to-3D module and PhysNet, we utilize diverse and realistic 3D human motion data from the Human3.6M and AMASS datasets. Both provide 3D pose annotations essential for skeleton modeling. We use 2D-3D skeleton pairs from Human3.6M as prompt pairs and pretrain both modules for 10 epochs. A.4. FineGym Fine-tuning For fine-tuning FinePhys, we use the FineGym [62] dataset, selecting three subsets with distinct motion dynamics: FXJUMP, FX-TURN, and FX-SALTO. FX-JUMP includes 11 classes (IDs 616), FX-TURN comprises 7 classes (IDs 17 23), and FX-SALTO contains 17 classes (IDs 2440), as detailed in Tab. 3. Example videos and poses are illustrated in Fig. 8. We generate captions for each video by prompting GPT4 [1] to transform existing textual descriptions into standardized prompts. The instruction provided to GPT-4 was: For each gymnastics move described in the labels below, write detailed description as if explaining to someone who is unfamiliar with gymnastics. For example, the label 2 turns on one leg with free leg optional below horizontal is converted to person executes two complete turns while balancing on one leg, allowing the lifted leg to remain below hip level or in any chosen position beneath the horizontal line throughout the turning sequence. This augmentation Figure 8. Example videos from FX-JUMP, FX-TURN and FX-SALTO. Each sample video has 16 frames, and the corresponding 2D skeleton sequence is also represented. enhances the models comprehension of textual prompts, facilitating subsequent video generation tasks. With the dataset augmented by extended descriptions, we first fine-tune the PhysNet and 2D projection modules for 10,000 training steps using online-detected 2D skeletons from FineGym. Subsequently, we jointly fine-tune the U-Net, PhysNet, and 2D projection modules for an additional 8,000 training steps. B. Elaboration on Evaluation Metrics In this section, we elaborate on the details of evaluation metrics used in our project. First, we discuss the limitation of the original CLIP-SIM metric [53] and the corresponding improved CLIP-SIM*. Then we introduce the details of the user study as well as other metrics. B.1. CLIP-SIM Metrics and Limitations We analyze the CLIP-SIM metric based on three aspects: semantic consistency, domain consistency, and temporal consistency [23]. Below, we detail each aspect and discuss their limitations. ❶ Semantic Consistency measures the alignment between textual prompts and the generated video frames. Specifically, for given text prompt and generated video with frames, the semantic consistency score is computed as the average CLIP similarity between and each frame of : CLIPtext(P, ) = 1 T (cid:88) t=1 CLIP(P, (t)). (27) Limitations of CLIPtext: The original semantic consistency metric struggles with fine-grained action labels due to semantic ambiguity and entanglement in the CLIP embedding space. As illustrated in Fig. 9, while the metric performs adequately for coarse-grained action categories (e.g., those from UCF101 [66]), it fails with FineGym labels where the embedded vectors of specific categories overlap significantly, rendering the metric ineffective for distinguishing between similar fine-grained actions. ❷ Domain Consistency assesses the similarity between generated video frames and reference images generated by an open-sourced image generation model, such as Stable Diffusion [57]. For reference image and generated video with frames, the domain consistency score is calculated as: CLIPdomain(I, ) = 1 (cid:88) t=1 CLIP(I, (t)). (28) Limitations of CLIPdomain: The domain consistency metric is unreliable for fine-grained actions because reference images generated by Stable Diffusion may not accurately reflect the nuances of specific actions or their dynamics, as shown in Fig. 10. Additionally, comparing the generated results in Fig.18, higher domain scores do not necessarily correspond to better representations of fine-grained videos. For instance, T2V-Zero generates nonsensical content that still achieves higher domain score than AnimateDiff, and VideoCrafters highest-scoring results often contain visible artifacts and limb inaccuracies. ❸ Temporal Consistency evaluates the smoothness of transitions between frames in generated video by computing the average CLIP similarity between randomly selected pairs of frames. Given generated video and set of 14 Table 3. Categories of FX-JUMP, FX-TURN, and FX-SALTO from Gym99. FX-JUMP from Gym Class ID Category 6 7 8 9 10 11 12 13 14 15 16 0 1 2 3 5 6 7 8 9 10 Switch leap with 0.5 turn Switch leap with 1 turn Split leap with 1 turn Split leap with 1.5 turn or more Switch leap (leap forward with leg change to cross split) Split jump with 1 turn Split jump (leg separation 180 degree parallel to the floor) Johnson with additional 0.5 turn Straddle pike or side split jump with 1 turn Switch leap to ring position Stag jump FX-TURN from Gym99 Class ID Category 17 18 19 20 21 23 0 1 2 3 5 6 2 turn with free leg held upward in 180 split position throughout turn 2 turn in tuck stand on one leg, free leg straight throughout turn 3 turn on one leg, free leg optional below horizontal 2 turn on one leg, free leg optional below horizontal 1 turn on one leg, free leg optional below horizontal 2 turn or more with heel of free leg forward at horizontal throughout turn 1 turn with heel of free leg forward at horizontal throughout turn FX-SALTO from Gym99 Class ID Category 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 Arabian double salto tucked Salto forward tucked Aerial walkover forward Salto forward stretched with 2 twist Salto forward stretched with 1 twist Salto forward stretched with 1.5 twist Salto forward stretched, feet land together Double salto backward stretched Salto backward stretched with 3 twist Salto backward stretched with 2 twist Salto backward stretched with 2.5 twist Salto backward stretched with 1.5 twist 0 1 2 3 4 5 6 7 8 9 10 11 12 Double salto backward tucked with 2 twist 13 Double salto backward tucked with 1 twist 14 Double salto backward tucked 15 Double salto backward piked with 1 twist 16 Double salto backward piked frame pairs P, the temporal consistency score is: 1 CLIPsmooth( ) = CLIP( (i), (j)). (cid:88) (i,j)P (29) 15 Figure 9. Limitations of semantic consistency in original CLIPSIM. We utilize CLIP models to obtain the embedded textual features and Probably Approximately Correct (PAC) for dimensionality reduction. The distribution of embedded category labels from FX-JUMP, FX-TURN and FX-SALTO as well as UCF101 is shown. Label features from FineGym are entangled, while those from UCF101 are clearly seperated. Limitations of CLIPsmooth: The original temporal consistency metric is unsuitable for fine-grained human actions, which inherently involve rapid and significant temporal changes. As demonstrated in Fig.17, models like T2IZero that generate predominantly static scenes paradoxically achieve the highest temporal consistency scores. This indicates that the metric fails to capture the dynamic nature of fine-grained actions, instead rewarding unnaturally smooth or static video sequences. B.2. The Improved CLIP-SIM* Metrics To overcome the aforementioned limitations, we propose an enhanced version of CLIP-SIM, termed CLIP-SIM*, specifically designed for evaluating fine-grained human action videos. CLIP-SIM* refines the calculations of domain consistency and temporal consistency by adopting datadriven approach, while leaving the original semantic consistency as minor metric. ❶ Improved Domain Consistency. Instead of relying on reference images generated by Stable Diffusion, CLIPSIM* leverages ground-truth videos to select more relevant reference images. Specifically, we randomly choose ground-truth videos and extract three representative frames (start, middle, end) from each to form the reference set {Ij}N j=1, as shown in the right part of Fig. 10. The domain consistency score is then computed as the average CLIP similarity between each generated frame and This modification allows CLIP smooth to effectively measure whether the generated video replicates the temporal dynamics of specific fine-grained actions, addressing the shortcomings of the original temporal consistency metric, as shown in Fig.19. B.3. Details of User Study As discussed in the main paper, we evaluate the generation results through user study, which provides more reliable assessment. In practice, each participant is presented with series of text-video, image-video, and video-video pairs and asked to rate semantic consistency, temporal consistency, and domain consistency on scale from 1 to 5. The layout of the user study interface is illustrated in Fig. 16. Specifically, we developed questionnaire that tested all baseline models alongside our results. Each video result was accompanied by the same textual descriptions, reference images, and reference videos. Participants were instructed to objectively evaluate the similarity of the video results to this reference information. To ensure impartiality, we omitted any details about the models used and distributed the questionnaire to 20 professionals unfamiliar with our work, thereby obtaining objective data. B.4. Other Metrics PickScore. PickScore [38] trains scoring function s() based on the CLIP framework using the large-scale user preferences dataset Pick-a-Pic to score the quality of generated images. Its performance in assessing generated images surpasses that of other evaluation metrics, even outperforming expert human annotators. Given text prompt and an image as input, PickScore calculates the score of the generated image as follows: s(P, I) = Etxt(P ) Eimg(I) τ (32) where Etxt and Eimg represent the text encoder and image encoder, respectively, and τ denotes the learned scalar temperature parameter of CLIP. While PickScore was originally developed for image evaluation, we have extended it to the domain of video evaluation. Specifically, given text prompt and generated video , we compute the average PickScore across all frames of the video: PickScore(P, ) = 1 (cid:88) t= s(P, (t)) (33) where (t) denotes the t-th frame of the generated video, and is the total number of frames. Frechet Video Distance (FVD). FVD [69] is widely In used metric for evaluating video generation models. the domain of temporal analysis [14, 15], it is highly correlated with the visual quality of generated samples and Figure 10. Domain image of original CLIP-SIM and the improved CLIP-SIM* from FX-JUMP, FX-TURN and FXSALTO. Reference images generated by Stable Diffusion may not accurately reflect the nuances of specific actions or their dynamics (Original CLIP-SIM), while CLIP-SIM* randomly selects one video from the given class and extracts three representative frames (start, middle, end) to form more reasonable reference set. all reference images: CLIP text( , {Ij}) = 1 1 (cid:88) (cid:88) t=1 j=1 CLIP( (t), Ij). (30) This approach ensures that the reference images are contextually and semantically aligned with the fine-grained actions being evaluated, thereby providing more accurate measure of domain consistency. ❷ Improved Temporal Consistency. To better assess the temporal dynamics of fine-grained actions, we propose an improved temporal consistency metric within CLIP-SIM*, which preserves the temporal changing patterns inherent Instead of enforcing smoothto specific action classes. ness across all frames, CLIP-SIM* compares the generated video with multiple reference videos from the same action category. For each action label, we select reference videos Ref and uniformly sample Ki frames from each reference video, where Ki {1, 2, 4, 8, 16}. The temporal consistency score is then calculated as: CLIP smooth( , Ref) = (cid:88) Ki(cid:88) l=1 k=1 CLIP( (k), Ref (k)). (31) 16 Figure 11. Visualization of different pose sequences on the class switch leap with 0.5 turn from the FX-Jump subset, demonstrating the complete transformation process within our framework. assesses temporal consistency. FVD utilizes pre-trained video recognition model to extract features from both real and generated videos, forming two sets of features, and then computes the mean and covariance matrices of these two sets. The FVD is represented as the Frechet distance between these two distributions: 1 2 ) FVD = µ µ2 + Tr(Σ + Σ 2(Σ Σ) (34) where µ and Σ are the mean and covariance matrix of the real video feature set, while µ and Σ are the mean and covariance matrix of the generated video feature set. However, as observed in [39], unsatisfactory video generation results could achieve higher FVD score, challenging its reliability. C. Additional Illustration & Analysis C.1. Elaboration on Euler-Lagrange Equations In the main paper, we use the following equation to represent the process in Lagrangian Mechanics: = = qT (q) (q). Then we calculate the Lagrangian is defined as: 1 2 qi and qi : L qi + qi = (cid:0)Mij(q) qj(cid:1) = qj Mij qk qk + Mij(q)qj qT (q) qi dt 1 2 (37) (38) (39) and substitute these results into the Euler-Lagrange equation: qi + qk qk + Mij(q)qj qT (q) qi (cid:18) qj Mij = 0, 1 (cid:19) (40) where qi represents the partial derivative of the potential energy with respect to the coordinates, i.e., the generalized force, i.e., J(q, q). Thus we could obtain the following formulation: (q)q = J(q, q) C(q, q). (41) (q)q = J(q, q) C(q, q), (35) C.2. Visualization of the Pose Modality which is common form used in robotics and dynamics, known as the equation of motion in terms of mass matrix (q), generalized forces J(q, q), and Coriolis and centrifugal forces C(q, q). Here we elaborate on its relation with the original Euler-Lagrange Equations, i.e.: qi (t, q(t), q(t)) dt qi (t, q(t), q(t)) = 0. (36) Assume the kinetic energy of the system is given by = 1 2 qT (q) q, and the potential energy is typically function of the generalized coordinates denoted by = (q), then Recall that our FinePhys framework fully leverages skeletal data through sequence of specialized modules: (1) The online pose estimator generates detected 2D poses, denoted as S2D detect; (2) Then the in-context-learning module processes and transforms them into S3D dd ; (3) After the PhysNet module we obtain S3D pp , (4) and finally we re-projected the average of S3D pp into 2D space to obtain S2D dd and S3D re-proj. Fig.11, Fig. 12, Fig. 13 present additional visualizations of these pose sequences, illustrating the entire transformation process within our framework. Due to the large vari17 Figure 12. Visualization of different pose sequences on the class 2 turn with free leg held upward in 180 split position throughout turn from the FX-Turn subset, demonstrating the complete transformation process within our framework. ation and high complexity of fine-grained actions, the detected 2D poses (S2D detect) exhibit significant misidentifications across joints throughout the video. The in-context learning module improves these poses, enabling S3D dd to partially reconstruct missing or distorted skeletons in each frame. However, in cases of severe distortion, the datadriven approach becomes unstable, resulting in S3D dd being noisy and physically implausible. The PhysNet module mitigates this issue by producing S3D pp , which is more stable and constrained, effectively correcting deviations in S3D dd . Consequently, the averaged and re-projected 2D poses (S2D re-proj) show substantial improvements compared to the original detections, validating the efficacy of our approach. generating continuous identical frames, which lack realistic motion dynamics. In contrast, CLIP-SIM* scores align more closely with human intuition, providing more reliable assessment of video quality. ❷ FinePhys consistently outperforms other baseline methods across different action categories. Baseline methods that lack guidance from physical information often produce unrealistic limb movements. For instance, Latte displays multiple limb artifacts in Class 14, and VideoCraft shows unrealistic levitation in Class 20 . In contrast, FinePhys incorporates physics modeling through the PhysNet module, resulting in more natural and coherent actions that adhere to real-world physical constraints. C.3. More Generated Results and Comparison C.4. Limitation and Future Work. In this section, we present additional qualitative results to demonstrate the effectiveness of our proposed FinePhys framework in generating fine-grained human action videos. We compare the generated results of FinePhys with those of baseline methods across three action subsets: FXJUMP, FX-TURN, and FX-SALTO, as illustrated in Fig. 17, Fig. 18, and Fig. 19, respectively. The key observations are as follows: ❶ Our CLIP-SIM* metric more accurately reflects the quality of video generation compared to the original CLIP-SIM metric. For example, methods such as Follow-Your-Pose and Latte achieve high scores on the original Domain Score, yet the generated actions exhibit significant inconsistencies with physical laws. Similarly, T2V-zero attains the highest score on the Smooth Score by Intractable Cases. Although FinePhys outperforms its competitors in generating results, significant challenges remain unresolved. High-speed motions and substantial body deformations pose considerable difficulties, particularly when they are intertwined, as seen in salto routines. Generating fine-grained actions such as double salto backward stretched is currently intractable, as shown in Fig. 14, let alone accurately distinguishing between actions like salto backward stretched with 2.5 twist, salto backward tucked with 1 twist, and double salto backward tucked with 1 twist. We encourage future research efforts to address these complex scenarios. Reliance on Initial Pose Detection. FinePhys fully utilizes the pose modality; however, the initial step of the 18 Figure 13. Visualization of different pose sequences on the class salto backward stretched with 2 twist from the FX-Salto subset, demonstrating the complete transformation process within our framework. Figure 14. Limitations in intractable cases. For class 31: double salto backward stretched, FinePhys fails to generate double salto, resulting in only single flip being observed. pipeline involves online 2D pose estimation. Due to the complexity of fine-grained human actions, we observed that the online pose estimator can occasionally fail completely, resulting in no detected 2D poses, as shown in Fig. 15. In such cases, the initial poses rely entirely on the pose prior used in the in-context learning module. Even if we can restore the human structure spatially, no motion is present. In future work, we will consider selecting appropriate scenarios to evaluate our current FinePhys implementations and explore additional modalities (e.g., optical flow) to address this issue. Focus on Fine-grained Human Actions. Although video generation techniques have been extensively explored and improved, applying these methods to the specific and challenging domain of fine-grained human actions can reveal the limitations of current approaches and inspire future adFigure 15. Negative Impact of Initial Pose Detection.. Current online pose estimators may fail completely due to the complexity of fine-grained human actions, which affects subsequent processing stages in the FinePhys framework. Even when the physical structure of the human body is spatially restored, the intricate motion dynamics cannot be accurately reconstructed, resulting in unrealistic or static video outputs. vancements [8, 30, 61, 63]. In this work, we select three fine-grained human action subsets, each encoding distinct 19 motion dynamics: ❶ Turning Focuses on precise rotational movements; ❷Jumping emphasizes rapid vertical motion combined with moderate rotations; ❸ While Salto involves complex aerial maneuvers with multiple twists and flips, and is the most challenging. By conducting comprehensive quantitative comparisons alongside qualitative analyses, we aim to draw greater attention to the challenges inherent in generating fine-grained human actions. This focused evaluation not only highlights the strengths and weaknesses of existing methods but also provides valuable insights for future research and development in this domain. Further Exploration on Physics. In future work, we aim to enhance the integration of physics modeling in video generation from diverse perspectives, such as collision dynamics, fluid interactions, etc. Currently, generating finegrained human actions restricts the models ability to focus solely on motion dynamics, as it must also account for the spatial structure of the human body [9, 67, 75, 76]. To address this complexity, we plan to simplify scenes by utilizing basic geometric shapes for environmental interactions, thereby reducing model complexity while maintaining robust incorporation of physical principles. Additionally, we will investigate the incorporation of physical laws into video generation, which may involve developing new algorithms or refining existing techniques to more accurately simulate real-world physical behaviors. 20 Figure 16. Display of the interface of User Study. 21 Figure 17. Qualitative Results on FX-JUMP. FX-JUMP focuses on the motion continuity of the gymnastics body. Compared with other baselines, our method demonstrates superior performance in understanding physical consistency. 22 Figure 18. Qualitative Results on FX-TURN. FX-TURN focuses on the minor difference of the gymnastics body. Compared with other baselines, our method demonstrates superior performance in understanding complex and fine-grained semantics, keeping the consistency of bio-physical characteristics, and adhering to the physical principles. 23 Figure 19. Qualitative Results on FX-SALTO. FX-SALTO demands gymnasticss body rotates 360 around horizontal axis with the feet passing over the head, which is the most difficult in all of three sub-datasets in FineGym. Compared with other baselines, results in our methods maintain better temporal consistency, more adhering to the bio-physical rules."
        }
    ],
    "affiliations": [
        "School of Astronautics, Northwestern Polytechnical University, Xian, China",
        "School of Automation, Northwestern Polytechnical University, Xian, China",
        "School of Software, Northwestern Polytechnical University, Xian, China",
        "Unmanned System Research Institute, Northwestern Polytechnical University, Xian, China"
    ]
}