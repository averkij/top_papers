{
    "paper_title": "Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency",
    "authors": [
        "Kaiwen Zheng",
        "Yuji Wang",
        "Qianli Ma",
        "Huayu Chen",
        "Jintao Zhang",
        "Yogesh Balaji",
        "Jianfei Chen",
        "Ming-Yu Liu",
        "Jun Zhu",
        "Qinsheng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobian-vector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop a parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the \"mode-covering\" nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as a long-skip regularizer. This integration complements sCM with the \"mode-seeking\" reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only $1\\sim4$ steps, accelerating diffusion sampling by $15\\times\\sim50\\times$. These results position rCM as a practical and theoretically grounded framework for advancing large-scale diffusion distillation."
        },
        {
            "title": "Start",
            "content": "Preprint LARGE SCALE DIFFUSION DISTILLATION VIA SCOREREGULARIZED CONTINUOUS-TIME CONSISTENCY Kaiwen Zheng1,2 Yuji Wang1 Qianli Ma2 Huayu Chen1,2 Jintao Zhang1 Yogesh Balaji2 Jianfei Chen1 Ming-Yu Liu Jun Zhu1, Qinsheng Zhang2 1Tsinghua University 2NVIDIA Corresponding Author https://research.nvidia.com/labs/dir/rcm"
        },
        {
            "title": "ABSTRACT",
            "content": "This work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. Although continuous-time consistency model (sCM) is theoretically principled and empirically powerful for accelerating academic-scale diffusion, its applicability to large-scale text-to-image and video tasks remains unclear due to infrastructure challenges in Jacobianvector product (JVP) computation and the limitations of standard evaluation benchmarks. We first develop parallelism-compatible FlashAttention-2 JVP kernel, enabling sCM training on models with over 10 billion parameters and high-dimensional video tasks. Our investigation reveals fundamental quality limitations of sCM in fine-detail generation, which we attribute to error accumulation and the mode-covering nature of its forward-divergence objective. To remedy this, we propose the score-regularized continuous-time consistency model (rCM), which incorporates score distillation as long-skip regularizer. This integration complements sCM with the mode-seeking reverse divergence, effectively improving visual quality while maintaining high generation diversity. Validated on large-scale models (Cosmos-Predict2, Wan2.1) up to 14B parameters and 5-second videos, rCM matches or surpasses the state-of-the-art distillation method DMD2 on quality metrics while offering notable advantages in diversity, all without GAN tuning or extensive hyperparameter searches. The distilled models generate high-fidelity samples in only 1 4 steps, accelerating diffusion sampling by 15 50. These results position rCM as practical and theoretically grounded framework for advancing large-scale diffusion distillation. 5 2 0 2 9 ] . [ 1 1 3 4 8 0 . 0 1 5 2 : r Figure 1: 5 random video samples from 4-step sCM, DMD2, and rCM on Wan2.1 1.3B. rCM resolves the quality issues of sCM while showing clear superiority to DMD2 in generation diversity. 1 Preprint Figure 2: High-level comparison of diffusion distillation methods. Despite the theoretical existence of forward divergence, GANs in practice still suffer from limited diversity and model collapse."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have been the cornerstone of generative AI, driving remarkable progress in visual domains such as image and video synthesis (Dhariwal & Nichol, 2021; Karras et al., 2022; Ho et al., 2022; Rombach et al., 2022; Esser et al., 2024; Brooks et al., 2024; Bao et al., 2024; Wan et al., 2025; Gao et al., 2025). They excel in generation quality, diversity, training stability and scalability compared to generative counterparts like generative adversarial networks (GANs) (Goodfellow et al., 2014), albeit suffering from slow inference. Training-free acceleration via specialized samplers (Song et al., 2021a; Zhang & Chen, 2022; Lu et al., 2022b) still requires over 10 steps to produce satisfactory samples due to the inherent discretization errors of numerical solvers, whereas training-based distillation enables few-step or even single-step generation. Representative diffusion distillation methods include knowledge distillation (Luhman & Luhman, 2021), progressive distillation (Salimans & Ho, 2022; Meng et al., 2023), consistency distillation (Song et al., 2023; Song & Dhariwal, 2023), score distillation (Wang et al., 2023; Luo et al., 2023b; Yin et al., 2024b;a; Salimans et al., 2024; Zhou et al., 2024) and adversarial distillation (Sauer et al., 2024b;a; Lin et al., 2024; 2025a). Among these, consistency models (CMs) (Song et al., 2023) are particularly appealing, as they circumvent the complexities associated with synthetic data generation or GAN training, maintain generation diversity, and achieve competitive performance on image benchmarks. More recently, continuous-time CM (sCM) (Lu & Song, 2024) has emerged as theoretically principled and elegant extension that, compared to its discrete-time predecessors, eliminates inherent discretization errors, decouples training from specific samplers, and dispenses with heuristic annealing schedules. When combined with consistency trajectory models (Kim et al., 2023; Heek et al., 2024), sCM further gives rise to the popular MeanFlow (Geng et al., 2025). However, the applicability of sCM to real-world, large-scale diffusion models remains unclear. Although sCM demonstrates scalability by distilling models up to 1.5B parameters on ImageNet 512512, practical application scenarios pose substantially different challenges. Modern largemodel training typically relies on infrastructures such as BF16 precision, FlashAttention and context parallelism (CP), which complicate and incur numerical errors in sCMs Jacobianvector product (JVP) computation. Moreover, prior evaluations are limited to weakly conditioned ImageNet benchmarks measured by FID, while text-to-image (T2I) and text-to-video (T2V) tasks are strongly conditioned and emphasize fine-grained attributes such as text rendering, which FID does not capture. Currently, scoreand adversarial-distillation methods, such as DMD2 (Yin et al., 2024a), remain the state of the art for large-scale diffusion distillation. Our work represents the first effort to scale up continuous-time consistency distillation to general application-level image and video diffusion models. To this end, we design dedicated infrastructure by developing FlashAttention-2 JVP kernel and enabling compatibility with parallelisms such as FSDP and CP. This allows us to explore sCMs scaling behavior by applying it to 10B+ models and high-dimensional video data. Through this investigation, we reveal the quality issues of sCM in fine-detail generation and identify the error accumulation characteristic of CMs. At conceptual level, we argue that the nature of diffusion distillation methods is governed by their forward (e.g., CMs), where the training examples are real or underlying divergence (Figure 2): teacher-generated data, and reverse (e.g., score distillation), where the student is supervised only on self-generated samples. Forward divergence is known to encourage mode-covering by penalizing underestimation of any training sample likelihoods, which often results in spread-out densities and Preprint low-quality samples. In contrast, reverse divergence is inherently mode-seeking and beneficial to the visual quality of diffusion models (Zheng et al., 2025a), despite suffering in diversity. To remedy the quality issues of sCM, we propose integrating score distillation as long-skip regularizer, which naturally complements sCM as they act on forward and reverse generation paths, respectively. We term the resulting framework, together with our other techniques like stable time-derivative computation, the score-regularized continuous-time consistency model (rCM). rCM requires no engineering complexities such as multi-stage training, GAN tuning or extensive architecture/hyperparameter search. We validate its scalability on unprecedentedly large-scale models including Cosmos-Predict2 (NVIDIA, 2025) and Wan2.1 (Wan et al., 2025), covering T2I and T2V tasks up to 5 seconds and 14B parameters. Empirically, rCM matches or even surpasses DMD2 on quality metrics, while offering significant advantages in generation diversity. These results establish rCM as promising and practical direction for large-scale diffusion distillation."
        },
        {
            "title": "2 BACKGROUND",
            "content": "2.1 DIFFUSION MODELS t=0 along with associated marginals {qt}T Diffusion models (DMs) (Ho et al., 2020; Song et al., 2020) learn continuous data distributions by gradually perturbing clean data x0 pdata with Gaussian noise, which generates trajectory {xt}T t=0, and then learning to reverse this process. The forward process follows closed-form transition kernel qt0(xtx0) = (αtx0, σ2 I) with predefined noise schedule αt, σt, enabling reparameterization as xt = αtx0 + σtϵ, ϵ (0, I). The sampling process of diffusion models can follow the probability flow ordinary differential equation (PF-ODE) dxt = (cid:2)f (t)xt 1 dt σ2 , and xt log qt(xt) is the score function (Song et al., 2020). key property of diffusion models is the theoretical equivalence of different parameterizations: the network may predict the score (xt log qt(xt)), the noise (ϵ), the clean data (x0), or the velocity (v), with optimal predictors being analytically interconvertible (Zheng et al., 2023b). With velocity parameterization vθ, diffusion models are trained by minimizing the mean square error (MSE) Ex0pdata,ϵ,t[w(t)vθ(xt, t) v2 2], where the regression target is = αtx0 + σtϵ (denote ft := dft/dt), and the PF-ODE is simplified to dxt dt = vθ(xt, t), commonly known as flow matching (Lipman et al., 2022). notable special case, rectified flow (Liu et al., 2022), employs the schedule αt = 1 t, σt = t. 2 g2(t)xt log qt(xt)(cid:3) dt, where (t) = log αt dt 2 log αt , g2(t) = dσ dt 2.2 CONSISTENCY MODELS Consistency models (CMs) (Song et al., 2023) aim to learn consistency function fθ : (xt, t) (cid:55) x0 which maps the point xt at arbitrary time on the teacher PF-ODE trajectory to the initial point x0. The consistency function must satisfy the boundary condition fθ(x, 0) x. To ensure unrestricted form and expressiveness of the neural network Fθ(xt, t), fθ is parameterized as fθ(x, t) = cskip(t)x + cout(t)Fθ(cin(t)x, cnoise(t)) with cskip(0) = 1, cout(0) = 0. This parameterization aligns with practices in diffusion models (Karras et al., 2022). fθ is the direct counterpart of the clean data predictor (denoiser) in diffusion models, and typically initialized from the teacher denoiser fteacher. CMs objective is to ensure consistent outputs at adjacent timesteps and on the teacher trajectory. Discrete-time CMs minimize the objective with > 0: Ex0pdata,ϵ,t [w(t)d (fθ(xt, t), fθ ( ˆxtt, t))] , where w() is positive weighting function, d(, ) is distance metric, θ is the stop-gradient version of θ, and ˆxtt is obtained by solving the teacher PF-ODE from (xt, t) to with numerical solvers. Discrete-time CMs suffer from discretization errors and require manually designed annealing schedules for (Song & Dhariwal, 2023; Geng et al., 2024). (1) (cid:104) Continuous-time CMs, represented by sCM (Lu & Song, 2024), offer clean upgrade by taking the limit 0. When d(x, y) = y2 the CM loss simplifies to 2, Ex0pdata,ϵ,t dt + tfθ(xt, t) dt is the tangent of fθ at (xt, t) along the teacher ODE trajectory dxt dt = vteacher(xt, t). sCM employs the TrigFlow noise schedule αt = cos(t), σt = sin(t) and preconditioning cskip(t) = w(t)fθ(xt, t) dfθ (xt,t) (cid:105) , where dfθ (xt,t) = xtfθ (xt, t) dxt dt 3 Preprint cos(t), cout(t) = sin(t) 1, such that Fθ is exactly the velocity predictor vθ. The loss fur- (cid:20)(cid:13) (cid:13)Fθ(xt, t) Fθ (xt, t) w(t) dfθ (xt,t) ther reduces to2 Ex0pdata,ϵ,t (cid:13) , where dfθ (xt,t) = (cid:21) dt (cid:13) 2 (cid:13) (cid:13) 2 ), and the full derivative dFθ (xt,t) dt dt + Fθ (xt,t) dt ) sin(t)(xt + dFθ (xt,t) cos(t)(Fθ (xt, t) dxt xtFθ (xt, t) dxt can be computed using forward-mode automatic differentiation, Jacobian-vector product (JVP). This objective is simple MSE which enforces the instantaneous self-consistency at (xt, t). Recent works MeanFlow (Geng et al., 2025) and AYF (Sabour et al., 2025) are essentially direct combination of sCM and consistency trajectory models (CTM) (Kim et al., 2023) under the rectified flow schedule (see Appendix E.1). = dt dt 2.3 SCORE DISTILLATION Score distillation methods aim to match the student distribution pθ with the teacher distribution pteacher, where samples pθ are generated via = Gθ(z), p(z) from noise prior p(z). Directly matching clean, high-dimensional data distributions is notoriously difficult (Song & Ermon, 2019). standard remedy is to introduce diffused version by perturbing through forward diffusion process, yielding xt with marginal pt, and to minimize certain reverse divergences: min θ Et[Df (pt θ pt teacher)], Df (pt θ pt teacher) = Ept θ(xt) (cid:16) (cid:104) rpt teacher,pt θ (cid:17)(cid:105) (xt) (2) (xt) = pt teacher(xt) pt θ(xt) teacher,pt θ where rpt is the likelihood ratio. For instance, variational score distillation (VSD) (Wang et al., 2023; Luo et al., 2023b) considers the reverse KL divergence (f (r) = log r), also known as distribution matching distillation (DMD) (Yin et al., 2024b); the more recent score identity distillation (SiD) (Zhou et al., 2024) considers the Fisher divergence (r) = xt log r2 2. The gradient θEt[Df (pt and the score functions xt log pt teacher(xt), which are available from diffusion models. As the student score xt log pt θ(xt) is intractable for the few-step generator Gθ, an auxiliary fake score network is introduced. It learns diffusion model over x0 pθ by minimizing Ex0pθ,ϵ,t[w(t)ffake(xt, t) x02 2] and serves as proxy for the student score. Like the critic/discriminator in GANs, the fake score is optimized jointly with the student θ via adversarial interplay. Both the student and the fake score are commonly initialized from the teacher diffusion model. teacher)] typically involves the generator gradient dGθ θ pt θ(xt), xt log pt dθ"
        },
        {
            "title": "3 SCALING UP SCM",
            "content": "We begin by scaling up sCM to T2I and T2V tasks and investigating its performance under different prompt types (see Table 5 for image and video text prompts used in this paper). 3.1 ALGORITHM DETAILS The original sCM relies on multiple implementation tricks for training stability, often requiring finetuning or even retraining the teacher model, which is impractical in most distillation scenarios. We first simplify the sCM implementation without compromising stability. Adapting to Any Noise Schedule. sCM employs the TrigFlow forward process xt = cos(t)x0 + σtϵ, while the teacher model is typically trained under other noise schedules such as rectified flow. Due to the equivalence between different noise schedules and parameterizations in diffusion models (Kingma et al., 2021; Zheng et al., 2023b), TrigFlow-consistent wrapped teacher can be constructed without retraining. Specifically, let the teacher time be traw with noise schedule αtraw , σtraw. reverse mapping ϕ (often analytic) from TrigFlow time to traw can be derived by matching the signal-to-noise ratio, i.e., by solving σtraw traw, traw) as the original αtraw = tan(t). Denote raw teacher(xraw 1There is data std parameter σd in original sCM formulation, inherited from EDM (Karras et al., 2022). For simplicity, we absorb it into x0 itself, i.e., define x0 := xraw 0 σd 2For simplicity, we absorb cnoise into Fθ itself. for original data xraw 0 . 4 Preprint teacher denoiser (can be transformed from other parameterizations). The wrapped teacher is fteacher(xt, t) := raw teacher (cid:16)(cid:113) ϕ(t) + σ2 α2 (cid:17) ϕ(t)xt, ϕ(t) , Fteacher(xt, t) := cos(t)xt fteacher(xt, t) sin(t) (3) All wrapping conversions are cheap and are performed under FP64 to ensure precision. We also wrap the student in the same way so that the raw student aligns with the raw teacher. Simplification. As our concerned models do not involve the unstable Fourier time embedding or AdaGN layers mentioned in sCM, but instead adopt positional time embedding, AdaLN, and QK normalization, we keep the network structure. Following sCMs tangent normalization, the loss is LsCM(θ) = Ex0pdata,ϵ,tpG (cid:34)(cid:13) (cid:13) Fθ(xt, t) Fθ (xt, t) (cid:13) (cid:13) g2 2 + (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (4) where pG is time distribution, = 0.1, and = w(t) dfθ (xt,t) . Although BF16 avoids the overflow issues as in sCMs FP16, we still follow the JVP rearrangement by setting w(t) = cos(t) and absorbing it into the JVP computation. sCMs adaptive weighting, as also noted in AYF (Sabour 2+c 1 remains nearly constant. (cid:13) (cid:13)Fθ Fθ (cid:13) et al., 2025), is unnecessary since = g2 g2 g2 2+c dt (cid:13) 2 (cid:13) (cid:13) 2 3.2 INFRASTRUCTURE While JVP can be computed with PyTorchs built-in forward-mode operator torch.func.jvp, it is not natively compatible with large-scale training setups, necessitating custom implementations. We detail our infrastructure design in Appendix and summarize below. Flash Attention. FlashAttention-2 (Dao, 2023) is widely used in large-scale training to reduce memory cost and improve throughput. To enable efficient JVP computation at scale, we develop Triton (Tillet et al., 2019) kernel that integrates JVP into the FlashAttention-2 forward pass with similar block-wise tiling, supporting both selfand cross-attention. FSDP. Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023) reduces the memory footprint by partitioning models across GPUs, but current torch.func.jvp implementation does not support FSDP modules. We therefore refactor networks to perform JVP within each layer: layers expose standard forward functions while additionally accepting tangent inputs and producing tangent outputs. As long as FSDP granularity matches the layer boundaries, models remain fully compatible. CP. Context (or sequence) parallelism partitions the input tensor of shape [B, H, L, C] (batch size B, number of heads H, sequence length L, head dimension C) across GPUs along the sequence dimension L, enabling training with long inputs. In the Ulysses (Jacobs et al., 2023) strategy, each GPU first holds slice of size [B, H, L/P, C] for QKV. An all-to-all operation then redistributes QKV to [B, H/P, L, C] for local attention, followed by another all-to-all to restore the sequence partition. This scheme naturally extends to JVP by distributing the tangents of QKV in the same way and replacing local attention with our FlashAttention-2 JVP kernel. 3.3 PITFALLS OF SCALED-UP SCM 3.3.1 EMPIRICAL OBSERVATION: QUALITY ISSUES We observe that sCM alleviates the blurriness of discrete-time CMs (Luo et al., 2023a) and are capable of generating sharp images. However, in scenarios requiring high accuracy or temporal consistency, distortions are pronounced. As shown in Figure 3, distillation with pure sCM leads to critical quality issues in both T2I and T2V tasks. (1) For T2I, the outputs are close to the teacher under typical prompts, but quality degradation becomes evident in challenging cases requiring fine details, such as small text rendering. Moreover, the issues cannot be solved simply by scaling up model size. (2) For T2V, the high sensitivity of human perception to temporal consistency makes artifacts notable across prompts. The results exhibit blurry textures and unstable object geometry across frames (e.g., object interpenetration), producing significant and distracting visual distortions. 5 Preprint Figure 3: 4-step generation results with pure sCM distillation. 3.3.2 THEORETICAL ANALYSIS: ERROR ACCUMULATION The distortions can be interpreted from the perspective of error accumulation. Intuitively, CMs aim to solve the teacher ODE in one step, essentially learning the integral (cid:82) 0 Fteacher(xτ , τ )dτ , where the errors accumulate as increases. Specifically, in sCM, the learning target is dfθ(xt, t) dt = cos(t)(Fθ (xt, t) Fteacher(xt, t)) sin(t)(xt + dFθ (xt, t) dt (cid:124) (cid:125) (cid:123)(cid:122) self-feedback (JVP) ) (5) dFθ , weighted by sin(t), introduces first-order self-feedback signal via JVP, which is numerically dt fragile compared to the zeroth-order signal Fθ , particularly under the limited precision of BF16 (Appendix E.2). Near = 0, the student closely resembles the teacher. As training progresses, errors propagate from small to large and are amplified by self-feedback. When cos(t) sin(t) 0 at large t, the teacher supervision from Fteacher vanishes and the learning dynamics are dominated by JVP."
        },
        {
            "title": "4 SCORE-REGULARIZED CONTINUOUS-TIME CONSISTENCY MODELS",
            "content": "4.1 QUALITY REPAIR WITH SCORE REGULARIZATION Figure 4: Illustration of rCM. Left: the forward consistency objective of sCM propagates error from small to large times; Right: reverse-divergence minimization serves as long-skip regularizer. As shown in Figure 4, we mitigate quality limitations of sCM by introducing score-based regularization on long-skip consistency, which complements sCM with reverse divergence. While SiD (Zhou et al., 2024) achieves impressive results on academic benchmarks, we observe no clear advantage in 6 Preprint T2I and T2V tasks and instead adopt the more memory-efficient DMD (Yin et al., 2024b): LDMD(θ) = Ex0pθ,ϵ,tpD (cid:34)(cid:13) (cid:13) x0 sg (cid:13) (cid:13) (cid:20) x0 ffake(xt, t) fteacher(xt, t) mean(abs(x0 fteacher(xt, t))) (cid:35) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) (6) where ffake is the denoiser of the fake score network, pD is time distribution and sg is the stopgradient operator. The final rCM objective is LrCM(θ) = LsCM(θ) + λLDMD(θ), where λ is balancing weight. Empirically, we find λ = 0.01 generalizes across all models and tasks. Rollout Strategy. Student generation x0 pθ is required for DMD loss and fake score training. As CM, the student supports arbitrary-step sampling by alternating reverse denoising and forward θ 0. We randomly choose noising from pure noise: t1 = π 2 the number of simulation steps from [1, Nmax] and only backpropagate the DMD loss through the final step tN 0. Unlike DMD2 (Yin et al., 2024a), which uses fixed t1, . . . tN , CM should explore the entire time range. We thus adopt stochastic scheme by iteratively drawing ˆtn pD and setting tn = min(ˆtn, tn1) to ensure monotonically decreasing timestep sequence. θ 0 +ϵ2 . . . θ 0 +ϵ1 t2 +ϵN 1 tN 4.2 STABLE TIME DERIVATIVE CALCULATION We propose plug-in techniques to stabilize the JVP dFθ dt = (xtFθ ) Fteacher + tFθ during rCM training, preventing sudden collapse after long training. As first noted in DPM-Solver-v3 (Zheng et al., 2023a) and verified in sCM, instability arises from the partial time derivative tFθ (xt, t), due to the oscillatory nature of trigonometric time embeddings. We find two strategies effective. Semi-Continuous Time. We compute (xtFθ ) Fteacher exactly via JVP, while approximating the time derivative with finite difference: tFθ (xt, t) cos(t)Fθ (xt,t)Fθ (xt,tt) , with = 104. This method is stable for 2B-scale T2I models and requires no architectural changes. sin(t) High-Precision Time. Finite-difference approximation, however, is sensitive to and becomes unstable for 10B+ models and video tasks. In such cases, we enforce FP32 precision for all time embedding layers using the torch.amp.autocast context, as in Wan. Although this introduces an initial mismatch with pretrained Cosmos networks, it ensures stable rCM training."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUPS Models, Tasks and Datasets. To demonstrate scalability and performance of rCM, we distill Cosmos-Predict2 (NVIDIA, 2025) T2I models (0.6B, 2B, 14B) and Wan2.1 (Wan et al., 2025) T2V models (1.3B, 14B). We leverage curated data from NVIDIA (2025), supplemented with synthetic data generated by Wan2.1 T2V 14B for Wan distillation. In principle, the training could also rely solely on teacher-generated synthetic data, as in Yin et al. (2024b; 2025); Huang et al. (2025). Implementation. Our implementation builds on the Cosmos-Predict2 codebase, with infrastructure support from FSDP2, Ulysses CP, and selective activation checkpointing (SAC). Training alternates between optimizing the student with the rCM loss and updating the fake score via the flow-matching loss L(θfake) = Ex0pθ,ϵ,tpD [Ffake(xt, t) v2 2]. The teacher denoiser employs classifier-free guidance (CFG) (Ho & Salimans, 2022), which is simultaneously distilled into the student. Both the student and the fake score networks are initialized from the teacher parameters. We perform full-parameter tuning without LoRA, highlighting the stability and performance of rCM. Evaluation Metrics. We use GenEval (Ghosh et al., 2023) to evaluate T2I models on complex compositional prompts, such as object counting, spatial relations, and attribute binding. For video generation, we adopt VBench (Huang et al., 2024) to systematically assess motion quality and semantic alignment. We report the number of function evaluations (NFE) as quantification of inference efficiency. For video models, we also report throughput in frames per second (FPS), tested with batch size 1 on single H100, covering both diffusion sampling and VAE decoding stages. The training algorithm and additional experiment details are provided in Appendix and D. Preprint 5.2 RESULTS Table 1: GenEval Results. Model #Params Resolution NFE Overall Single Object Two Object Counting Colors Position Color Attribution SD-XL (Podell et al., 2023) SD3.5-M (Esser et al., 2024) SD3.5-L (Esser et al., 2024) FLUX.1-dev (Labs, 2024) SANA-1.5 (Xie et al., 2025) Cosmos-Predict2 (NVIDIA, 2025) SDXL-LCM (Luo et al., 2023a) SDXL-Turbo (Podell et al., 2023) SDXL-Lightning (Lin et al., 2024) Hyper-SDXL (Ren et al., 2024) SDXL-DMD2 (Yin et al., 2024a) SD3.5-L-Turbo (Esser et al., 2024) FLUX.1-schnell (Labs, 2024) SANA-Sprint (Chen et al., 2025) Cosmos-Predict2 + DMD2 Cosmos-Predict2 + rCM Cosmos-Predict2 + rCM Cosmos-Predict2 + rCM Pretrained Models 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1360 768 1360 768 1360 768 502 402 282 50 202 352 352 352 0.55 0.63 0.71 0.66 0.81 0.81 0.83 0.84 Distilled Models 1024 1024 512 512 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1360 768 1360 768 1360 768 1360 768 1360 768 1360 768 1360 768 1360 768 1360 768 1360 768 1360 768 4 4 4 4 4 4 4 4 4 4 4 4 4 2 2 2 1 1 1 0.50 0.56 0.53 0.58 0.58 0.68 0.69 0.77 0.75 0.77 0. 0.79 0.81 0.83 0.78 0.82 0.81 0.78 0.81 0.82 2.6B 2.5B 8B 12B 4.8B 0.6B 2B 14B 2.6B 2.6B 2.6B 2.6B 2.6B 8B 12B 0.6B 1.6B 0.6B 2B 0.6B 2B 14B 0.6B 2B 14B 0.6B 2B 14B 0.98 0.98 0.98 0.98 0.99 1.00 1.00 1.00 0.99 1.00 0.98 1.00 1.00 0.99 0.99 1.00 1.00 1.00 0. 1.00 1.00 1.00 0.99 1.00 1.00 1.00 0.99 1.00 0.74 0.78 0.89 0.81 0.93 0.97 0.99 0.98 0.55 0.72 0.61 0.77 0.76 0.89 0.88 0.90 0.92 0.98 0. 0.99 0.98 0.98 0.98 0.99 0.99 0.98 0.97 0.98 0.39 0.50 0.73 0.74 0.86 0.74 0.73 0.79 0.38 0.49 0.44 0.48 0.52 0.68 0.64 0.71 0.59 0.76 0. 0.74 0.73 0.80 0.74 0.76 0.80 0.72 0.77 0.84 0.85 0.81 0.83 0.79 0.84 0.86 0.89 0.90 0.85 0.82 0.84 0.89 0.88 0.78 0.78 0.89 0.91 0.85 0. 0.88 0.84 0.86 0.86 0.85 0.87 0.86 0.85 0.89 0.15 0.24 0.34 0.22 0.59 0.59 0.65 0.64 0.07 0.11 0.11 0.11 0.11 0.23 0.30 0.61 0.54 0.46 0. 0.48 0.58 0.59 0.48 0.59 0.47 0.49 0.57 0.49 0.23 0.52 0.47 0.45 0.65 0.70 0.73 0.72 0.14 0.21 0.21 0.23 0.24 0.54 0.52 0.54 0.55 0.66 0. 0.66 0.72 0.73 0.66 0.74 0.73 0.66 0.71 0.72 Figure 5: Few-step T2I samples compared to open-sourced models. rCM can render fine-grained text details such as Casio G-Shock, 11:44 AM, and Thursday, March 22nd from the prompt. Figure 6: Comparison between different numbers of sampling steps. Preprint Table 2: VBench Results for Wan (480p). Retested with Diffusers and our augmented prompts. Model #Params Resolution NFE Throughput (FPS) Total Score Quality Score Semantic Score Wan2.1 T2V (Wan et al., 2025) Wan2.1 T2V + DMD Wan2.1 T2V + rCM Wan2.1 T2V + rCM Wan2.1 T2V + rCM Pretrained Models 832 480 81 832 480 81 50 2 50 Distilled Models 832 480 81 832 480 81 832 480 81 832 480 81 832 480 81 832 480 81 832 480 81 4 4 2 2 1 1 1.3B 14B 1.3B 1.3B 14B 1.3B 14B 1.3B 14B 0.72 0.18 14.6 14.6 4.5 23.0 8. 32.3 14.4 83.02 83.58 83.95 84.26 84.56 84.43 84.92 84.09 85. 82.65 83.02 85.58 85.38 85.43 84.90 85.57 83.60 83.57 79.26 80. 80.50 80.63 82.88 80.86 82.95 78.82 80.81 Table 3: VBench Results for Cosmos (720p). Model #Params Resolution NFE Throughput (FPS) Cosmos-Predict2 TI2V (NVIDIA, 2025) Cosmos-Predict2 TI2V + rCM 2B 2B 1280 704 93 1280 704 93 35 2 4 0.32 4.6 T2V Score 83.03 84.40 I2V Score 88.6 88.2 We evaluate the proposed rCM both qualitatively and quantitatively, comparing it with pretrained models as well as existing distillation baselines. We use 4-step generation by default, which strikes balance between high sample quality and substantial acceleration over the teacher model. Performance. For T2I, we report GenEval scores in Table 1 and provide qualitative comparisons with open-source models in Figure 5. On Cosmos-Predict2, rCM closely approaches the teachers performance and benefits from scaling, with the 14B model achieving state-of-the-art overall score of 0.83 in just 4 steps. Under challenging prompts such as small text rendering, rCM also matches the SOTA few-step model FLUX.1-schnell in visual quality. For T2V, rCM even surpasses the 480p Wan teacher on VBench  (Table 2)  , reaching total score of 85 when distilling Wan2.1 14B. We also apply rCM to Cosmos-Predict2 with higher resolution of 720p and the additional image-to-video (I2V) task  (Table 3)  , where similar phenomena are observed. This does not imply that the distilled model is strictly superior to the teacher, particularly in terms of diversity and physical consistency, but highlights rCMs ability to preserve quality under few-step generation. Comparison with DMD2. We implement the DMD2 (Yin et al., 2024a) baseline by additionally parameterizing discriminator as branch of the fake score network and incorporating the nonsaturating GAN loss to supplement DMD training. This branch takes intermediate features from the fake score network and queries them with single learnable token to produce discrimination logit, akin to APT (Lin et al., 2025a). As reported in Tables 1 and 2, rCM matches or even surpasses DMD2 in generation quality, measured by GenEval and VBench. Moreover, we observe rCMs clear diversity advantage, particularly in video generation. As highlighted in Figure 1, rCM retains the diversity of sCM, while simultaneously resolving sCMs visual quality issues. In contrast, DMD2 tends to produce collapsed generations, where objects converge to similar positions and orientations, leading to reduced diversity. These findings suggest that jointly leveraging forwardand reverse-divergence-based methods forms promising distillation paradigm, yielding models that simultaneously achieve high quality, strong diversity, and substantial speedups. Generation with Fewer Steps. We additionally report rCMs 1-step and 2-step results in Tables 1 and 2, and further compare few-step generation quality in Figure 6. For T2I, rCM produces reasonable samples across 14 steps, with GenEval scores degrading only slightly under 1or 2-step settings. For simple prompts, 1-step generations are nearly indistinguishable from 4-step, whereas for more challenging prompts they show clear deficiencies in detailed text rendering. For T2V, the task is more demanding: 1-step outputs appear blurry across prompts and exhibit marked drop in VBench scores. In contrast, 2-step generations already reach scores close to the teacher, though with minor shortcomings in quality and background fidelity. At 4 steps, rCM further refines fine details and even succeeds at rendering sharp text in complex backgrounds, such as street signs. Preprint Overall, these results highlight rCMs robustness under extremely few steps, enabling competitive T2I generation with only 1 step and T2V generation with only 2 steps."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present rCM, score-regularized continuous-time consistency model that scales diffusion distillation to large image and video models. By integrating forward-divergence-based consistency distillation with reverse-divergence-based score distillation, rCM remedies the quality limitations of sCM while showing superior diversity advantages compared to DMD2. Our distilled models achieve competitive text-to-image results in single step and text-to-video results in only 2 steps, delivering up to 50 acceleration over teacher models. Looking forward, we believe that combining forwardand reverse-divergence principles provides unifying paradigm that may inspire new research in generative modeling. ACKNOWLEDGMENTS We thank Guande He, Cheng Lu, and Weili Nie for valuable discussions."
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-tovideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. URL https://openai. com/research/video-generation-models-as-world-simulators, 3, 2024. Junsong Chen, Shuchen Xue, Yuyang Zhao, Jincheng Yu, Sayak Paul, Junyu Chen, Han Cai, Song Han, and Enze Xie. Sana-sprint: One-step diffusion with continuous-time consistency distillation. arXiv preprint arXiv:2503.09641, 2025. Ricky TQ Chen, Jens Behrmann, David Duvenaud, and Jorn-Henrik Jacobsen. Residual flows for invertible generative modeling. Advances in neural information processing systems, 32, 2019. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pp. 87808794, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. 10 Preprint Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, In Advances in Neural Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. Information Processing Systems, volume 27, pp. 26722680, 2014. Guande He, Kaiwen Zheng, Jianfei Chen, Fan Bao, and Jun Zhu. Consistency diffusion bridge models. Advances in Neural Information Processing Systems, 37:2351623548, 2024. Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the training-inference gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Sangyun Lee, Yilun Xu, Tomas Geffner, Giulia Fanti, Karsten Kreis, Arash Vahdat, and Weili Nie. Truncated consistency models. arXiv preprint arXiv:2410.14895, 2024. Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. Advances in neural information processing systems, 37:7569275726, 2024a. Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang Wang. T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. arXiv preprint arXiv:2410.05677, 2024b. Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. 11 Preprint Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025a. Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, and Lu Jiang. Autoregressive adversarial post-training for real-time interactive video generation. arXiv preprint arXiv:2506.09350, 2025b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. arXiv preprint arXiv:2410.11081, 2024. Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood training for score-based diffusion odes by high order denoising score matching. In International conference on machine learning, pp. 1442914460. PMLR, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:57755787, 2022b. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023a. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023b. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and In Proceedings of the IEEE/CVF Tim Salimans. On distillation of guided diffusion models. conference on computer vision and pattern recognition, pp. 1429714306, 2023. NVIDIA. Cosmos world foundation model platform for physical ai, 2025. URL https: //arxiv.org/abs/2501.03575. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. Advances in Neural Information Processing Systems, 37:117340117362, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your flow: Scaling continuous-time flow map distillation. arXiv preprint arXiv:2506.14603, 2025. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. Advances in Neural Information Processing Systems, 37: 3604636070, 2024. 12 Preprint Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024a. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 87103. Springer, 2024b. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021a. Yang Song and Prafulla Dhariwal. preprint arXiv:2310.14189, 2023. Improved techniques for training consistency models. arXiv Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415 1428, 2021b. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pp. 3221132252. PMLR, 2023. Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pp. 1019, 2019. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36:84068441, 2023. Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer, 2025. URL https://arxiv.org/ abs/2501.18427. Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024a. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024b. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. 13 Preprint Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36: 5550255542, 2023a. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Improved techniques for maximum likelihood In International Conference on Machine Learning, pp. 42363 estimation for diffusion odes. 42389. PMLR, 2023b. Kaiwen Zheng, Yongxin Chen, Huayu Chen, Guande He, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Direct discriminative optimization: Your likelihood-based visual generative model is secretly gan discriminator. arXiv preprint arXiv:2503.01103, 2025a. Kaiwen Zheng, Guande He, Jianfei Chen, Fan Bao, and Jun Zhu. Elucidating the preconditioning in consistency distillation. arXiv preprint arXiv:2502.02922, 2025b. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. 14 Preprint"
        },
        {
            "title": "A RELATED WORK",
            "content": "Consistency Models Consistency models (CMs) (Song et al., 2023) accelerate diffusion sampling by taking shortcuts along the teacher ODE trajectory and directly predicting the starting point. Consistency trajectory models (CTMs) (Kim et al., 2023) and multi-step CMs (Heek et al., 2024) generalize the approach to predict trajectory jumps to arbitrary intermediate points. CDBMs (He et al., 2024) adapt CMs to diffusion bridges models. However, CMs suffer from training instabilities and quality issues such as blur. Subsequent efforts address these limitations by introducing dedicated annealing schedules (Song & Dhariwal, 2023; Geng et al., 2024), preconditioning strategies (Zheng et al., 2025b), or segmented consistency schemes (Wang et al., 2024; Ren et al., 2024; Lee et al., 2024). Yet these approaches often come with added complexity, such as multi-stage training or extensive hyperparameter tuning. The recent sCM (Lu & Song, 2024) represents the most advanced CM solution, being theoretically principled, practically simple, and empirically effective on academic image benchmarks. MeanFlow (Geng et al., 2025) and AYF (Sabour et al., 2025), which directly combine sCM with CTM, have also drawn significant attention. Nonetheless, the applicability of sCM to large-scale, application-level image and video diffusion models remains unclear. SANA-Sprint (Chen et al., 2025) applies sCM to modest 1.6B text-to-image model, while deliberately sidestepping the key challenge of JVP computation by relying on base model with linear attention rather than the widely adopted FlashAttention, limiting the application scenarios. Video Diffusion Distillation Existing practices distill video diffusion models with CMs, score distillation or GANs. T2V-Turbo (Li et al., 2024a;b) employs CMs but relies on additional reward models to enhance quality. By contrast, we conduct pure distillation while still delivering remarkable video quality. APT (Lin et al., 2025a) applies an adversarial GAN loss for one-step video generation. Another line of work distills bidirectional teacher into an autoregressive student to enable realtime streaming video generation. Within this direction, CausVid (Yin et al., 2025) leverages DMD loss with diffusion forcing, while Self-Forcing (Huang et al., 2025) and APT2 (Lin et al., 2025b) introduce student forcing to address the exposure bias inherent in diffusion forcing. JVPs in Generative Modeling Jacobianvector products (JVPs) are fundamental computational primitive in generative modeling, as they enable efficient handling of high-dimensional Jacobian information without explicitly materializing the full matrix. They are widely employed in normalizing flows (Chen et al., 2019) and diffusion models (Song et al., 2021b; Lu et al., 2022a; Zheng et al., 2023b;a), for example to estimate matrix traces via Hutchinsons trick or to derive exact coefficients for the optimal sampler. To the best of our knowledge, this work is the first to integrate JVP signals into large-scale generative model training, with modern FlashAttention architectures, diverse parallelism strategies, 10B+ parameter networks, and high-dimensional video data."
        },
        {
            "title": "B ALGORITHM",
            "content": "We provide the detailed algorithm of rCM in Algorithm 1, where we adopt slightly different tangent warmup strategy compared to sCM. We find the tangent warmup not essential for rCM."
        },
        {
            "title": "C INFRASTRUCTURE",
            "content": "C.1 FLASHATTENTION-2 JVP FlashAttention-2 (Dao, 2023) is an optimized attention algorithm that reduces memory usage and improves throughput by tiling the sequence into blocks and streaming intermediate results without materializing the full attention matrix. Given query, key, and value sequences RN1d, K, RN2d, where N1 and N2 denote sequence lengths and is the head dimension, the attention output RN1d is computed as = QK RN1N2, = softmax(S) RN1N2 , = PV RN1d, where softmax is applied row-wise. In multi-head attention (MHA), this computation is carried out in parallel across heads as well as across the batch dimension (number of input sequences). 15 Preprint Algorithm 1 Score-Regularized Continuous-Time Consistency Model (rCM) Require: dataset D, teacher diffusion model θteacher with TrigFlow-wrapped consistency function fteacher and v-predictor Fteacher, student model θ with wrapped fθ, Fθ, fake score model θfake with wrapped ffake, Ffake, time distributions pG, pD, student update frequency , maximal number of simulation steps Nmax, number of tangent warmup iterations H, number of total iterations I. if or mod = 0 then Initialize: θ θteacher, θfake θteacher 1: for = 1 to do 2: 3: 4: 5: 6: dF θ dt JVP(Fθ , (xt, t), (cos(t) sin(t)Fteacher(xt, t), cos(t) sin(t))) x0 D, ϵ (0, I), pG, xt cos(t)x0 + sin(t)ϵ cos(t) sin(t) min(1, i/H) cos(t)(cid:112)1 r2 sin2(t)(cid:0)Fθ (xt, t) Fteacher(xt, t)(cid:1) r(cid:0)cos(t) sin(t)xt + cos(t) sin(t) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Fθ(xt, t) Fθ (xt, t) (cid:13) L(θ) g2 2 2+c // Generator Step dF θ dt (cid:1) if > then (1, Nmax) Starting from t1 = π 2 , iteratively sample timesteps t1, . . . , tN by ˆtn pD, tn = min(ˆtn, tn1) Perform backward simulation t1 ϵD (0, I), tD pD, xθ (cid:13) (cid:13) xθ (cid:13) (cid:13) L(θ) L(θ) + λ 0 sg end if Update the student θ with loss L(θ) θ 0 +ϵ2 . . . +ϵ1 t2 tD cos(tD)xθ (cid:20) ffake(xθ xθ tD mean(abs(xθ θ 0 0 + sin(tD)ϵD ,tD )fteacher(xθ 0 fteacher(xθ 0 tD ,tD ) ,tD ))) tD +ϵN 1 tN θ 0 to obtain xθ 0 (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 // Critic Step 2 , iteratively sample timesteps t1, . . . , tN by ˆtn pD, tn = min(ˆtn, tn1) else U(1, Nmax) Starting from t1 = π 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: Perform backward simulation t1 ϵ (0, I), pD, xt cos(t)xθ Update the fake score θfake with flow-matching loss L(θfake) = Ffake(xt, t) v2 2 +ϵ1 t2 0 + sin(t)ϵ, cos(t)ϵ sin(t)xθ +ϵ2 . . . +ϵN 1 tN θ 0 θ 0 θ 0 to obtain xθ 0 0 19: 20: 21: end if 22: 23: end for 16 Preprint For the Jacobianvector product (JVP), we seek the tangent tO RN1d given input tangents tQ RN1d and tK, tV RN2d, defined as tO = dO dV tV. By the chain rule, this can be expressed in matrix form as dK tK + dO dQ tQ + dO tS = tQK + QtK tP = tS ((P tS)1N21 N2 tO = tPV + PtV ) where denotes the element-wise product. Aggregating terms, we obtain tO = PtV (cid:124) (cid:123)(cid:122) (cid:125) + HV (cid:124)(cid:123)(cid:122)(cid:125) )O, where = tS diag(rowsum(H) (cid:125) (cid:124) (cid:123)(cid:122) As noted in Lu & Song (2024), both and tO can be computed within single streaming loop, analogous to the FlashAttention-2 forward pass. We make this procedure explicit in Algorithm 2. Algorithm 2 FlashAttention-2 Forward Pass with JVP Computation Require: Matrices Q, K, V, their tangents tQ, tK, tV, block sizes Bc, Br. 1: Split Q, tQ into Tr blocks Q1, . . . , QTr and tQ1, . . . , tQTr of size Br d. 2: Split K, tK, V, tV into Tc tV1, . . . , tVTc of size Bc d. blocks K1, . . . , KTc, tK1, . . . , tKTc, V1, . . . , VTc, 3: Split output into Tr blocks O1, . . . , OTr , and into Tr blocks L1, . . . , LTr . 4: Split output tangent tO into Tr blocks tO1, . . . , tOTr . 5: for = 1 to Tr do 6: 7: Load Qi, tQi from HBM to SRAM. Initialize mi ()Br , ℓi 0Br , Oi 0Brd, ri 0Br , Ai 0Brd, Bi 0Brd. + QitK . , tSij = tQiK Load Kj, tKj, Vj, tVj from HBM to SRAM. Compute Sij = QiK Compute mnew = max(mi, rowmax(Sij)). Compute Pij = exp(Sij mnew). Compute ℓnew = emimnew ℓi + rowsum( Pij). Compute Onew = diag(emimnew)Oi + PijVj. Compute Anew = diag(emimnew )Ai + PijtVj. Compute Hi,j = Pij tSij. Compute rnew = emimnew ri + rowsum( Hij). Compute Bnew = diag(emimnew )Bi + HijVj. Update mi mnew, ℓi ℓnew, Oi Onew, Ai Anew, ri rnew, Bi Bnew. for = 1 to Tc do 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: Write Oi, Li, tOi to HBM. 26: end for 27: return Oi, Li, tOi end for Compute Oi = diag(ℓnew)1Onew. Compute Li = mnew + log(ℓnew). Compute Ci = diag(rnew)Oi Compute tOi = diag(ℓnew)1(Ai + Bi Ci). C.2 NETWORK RESTRUCTURING To make JVP computation compatible with Fully Sharded Data Parallel (FSDP), we restructure the forward functions of network layers. Specifically, we define base class JVP (Listing 1) that extends torch.nn.Module and supports both standard forward execution and JVP-mode execution. When withT=True, the forward pass receives and returns both the primals and their tangents, with each primal and the correpsonding tangent wrapped in the TensorWithT tuple type. 17 Preprint For each layer, the original forward logic is moved into forward, while JVP computation is delegated to forward jvp using torch.func.jvp. Other components (e.g., parameter initialization) remain unchanged. Figure 7 shows an example restructuring of the RMSNorm layer. The attention block is an exception since the native FlashAttention-2 does not support JVP computation with torch.func.jvp. When implementing JVP-mode forward of the attention block, we replace the self-attention and cross-attention components with our implemented FlashAttention-2 JVP kernel, while the remaining modules still rely on torch.func.jvp. Listing 1 Base class JVP that supports both standard forward execution ( forward) and JVP-mode forward execution ( forward jvp). TensorWithT = Tuple[torch.Tensor, torch.Tensor] class JVP(torch.nn.Module): def __init__(self): super().__init__() def forward(self, *args, **kwargs): withT = kwargs.pop(\"withT\", False) if withT: return self._forward_jvp(*args, **kwargs) else: return self._forward(*args, **kwargs) def _forward_jvp(self, *args, **kwargs): raise NotImplementedError def _forward(self, *args, **kwargs): raise NotImplementedError class RMSNorm(torch.nn.Module): def __init__(self, dim: int, eps: float = 1e-5): super().__init__() self.eps = eps self.weight = nn.Parameter(torch.ones(dim)) class RMSNorm(JVP): def __init__(self, dim: int, eps: float = 1e-5): super().__init__() self.eps = eps self.weight = nn.Parameter(torch.ones(dim)) def reset_parameters(self): torch.nn.init.ones_(self.weight) def reset_parameters(self): torch.nn.init.ones_(self.weight) def _norm(self, x): def _norm(self, x): return * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) return * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) def forward(self, x: torch.Tensor) -> torch.Tensor: def _forward_jvp(self, x: TensorWithT) -> TensorWithT: output = self._norm(x.float()).type_as(x) return output * self.weight x_withT = x, t_x = x_withT out, t_out = torch.func.jvp(self._forward, (x,), (t_x,)) return (out, t_out.detach()) def _forward(self, x: torch.Tensor) -> torch.Tensor: output = self._norm(x.float()).type_as(x) return output * self.weight Figure 7: Restructuring example for the RMSNorm layer: (left) original implementation, (right) JVP-enabled implementation."
        },
        {
            "title": "D EXPERIMENT DETAILS",
            "content": "Training Details. The rCM training configurations for different models and tasks are summarized in Table 4. We maintain smoothed version of the student parameters using the power EMA (Karras et al., 2024), and use the EMA model for evaluation. We use the AdamW optimizer with β1 = 0, β2 = 0.999 and weight decay of 0.01 for both student and fake score optimizers, while disabling gradient clipping, which we find crucial for maintaining training stability of rCM. Evaluation Details. For GenEval, we repeat the 553 test prompts four times to reduce variance. For VBench, we follow standard practice and use GPT-4oaugmented prompts. We observe that σmax governs the trade-off between quality and diversity. We adopt timesteps [arctan(σmax), 1.3, 1.0, 0.6] for 4-step sampling and take the first entries when sampling with fewer than 4 steps. We set σmax = 80 for high-diversity visualizations, and in some cases increase 18 Preprint it when computing metrics that emphasize high quality. For the 8-step result in Figure 5, we use [arctan(σmax), 1.3, 1.0, 1.0, 0.6, 0.6, 0.3, 0.3]. Table 4: Training and evaluation configurations. Models EMA Length Batch Size Context Parallel Size Learning Rate (student) Learning Rate (fake score) CFG Scale Student Update Frequency Maximal Simulation Steps Tangent Warmup Iterations Total Iterations σmax pG pD Cosmos Predict2 T2I Wan2.1 T2V 0.6B 2B 14B 1.3B 14B 0.05 1024 1 1e-6 2e-7 4.5 5 4 0 80k 80 (0.8, 1.6) = arctan(z) 0.05 512 1 1e-6 2e-7 4.5 5 4 0 30k 80 (0.8, 1.6) = arctan(z) 0.05 256 1 1e-6 2e-7 4.5 5 4 0 25k 800 (0.8, 1.6) = arctan(z) 0.05 256 1 2e-6 4e-7 5.0 5 4 1000 10k 1600 (0.8, 1.6) z) = arctan( 0.05 64 10 1e-6 1e-7 5.0 10 4 200 10k 1600 (0.8, 1.6) z) = arctan( (0.0, 1.6) = arctan(z) (0.0, 1.6) = arctan(z) (0.0, 1.6) = arctan(z) U(0, 1) tRF = 5u 1+4u (cid:16) tRF 1tRF = arctan (cid:17) U(0, 1) tRF = 5u 1+4u (cid:16) tRF 1tRF = arctan (cid:17)"
        },
        {
            "title": "E MORE DISCUSSIONS",
            "content": "E.1 CONTINUOUS-TIME CONSISTENCY TRAJECTORY MODELS sCM can be easily combined with consistency trajectory models (CTM) (Kim et al., 2023; Heek et al., 2024), which adds an additional time condition < to CMs and consider more fine-grained transitions xt xs on the PF-ODE, forming an interpolation between diffusion models and consistency models. Specifically, we can define consistency trajectory function fθ : (xt, t, s) (cid:55) xs from to with preconditioning coefficients derived from the DDIM (Song et al., 2021a) step: fθ(xt, t, s) = cos(t s)xt sin(t s)Fθ (xt, t, s) (7) Continuous-time CTMs (denoted as sCTM) can be trained via similar instantaneous objective of sCM by simply changing the coefficients, as is independent of and remains uninvolved in the JVP computation w.r.t. t: (cid:34)(cid:13) (cid:13) Fθ (xt, t, s) Fθ (xt, t, s) w(t, s) (cid:13) (cid:13) dfθ(xt, t, s) dt Ext,t,s (8) (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 where dfθ (xt, t, s) dt (cid:18) = cos(t s) Fθ (xt, t, s) (cid:19) dxt dt (cid:18) sin(t s) xt + dFθ (xt, t, s) dt (cid:19) (9) The objective naturally recovers flow matching under = t: when w(t, t) = 1 (e.g., w(t, s) = cos(t s)), it is exactly the same as flow matching; other arbitrary w(t, s) > 0 gives an equivalent objective whose gradient is proportional to that of flow matching. Recent methods such as MeanFlow (Geng et al., 2025) and AYF (Sabour et al., 2025) are the same as sCTM under the rectified flow schedule, which simply changes the preconditioning to fθ(xt, t, s) = xt (t s)Fθ(xt, t, s) and adjusts the JVP coefficients accordingly. For distillation, we also implemented sCTM without extensive hyperparameter tuning, but observed that it underperforms sCM in both quality and diversity on basic T2I tasks (Figure 8). This suggests that sCTM (or MeanFlow) encounters greater optimization challenges than sCM for diffusion distillation, as learning arbitrary mappings along the ODE trajectory is inherently more demanding than learning the mapping solely to the initial point. E.2 ANALYSIS OF JVP ERRORS To avoid overflow issues in FP16, BF16 precision is required for neural network computation in large model training. However, we find that computing the JVP term dFθ under BF16 incurs dt substantially larger numerical errors compared to the zeroth-order signal Fθ . 19 Preprint (a) sCM (b) sCTM (MeanFlow) Figure 8: Comparison between sCM and sCTM for distillation. We implement sCTM by adding an additional time condition to the network, which goes through separate embedding layer and is added to the embedding of before normalization. We adopt the sCTM training objective in Eq. (8), along with sCM tricks such as tangent normalization. To quantify these errors, we compute Fθ using both BF16 and FP32 precision, and measure the relative L2 error with , where BF16 θ and FP32 θ denote outputs under BF16 and FP32, BF θ 2 2 θ FP32 θ 2 FP32 2 respectively. We repeat the procedure for the rearranged JVP term cos(t) sin(t) dFθ . Note that only dt the network precision is altered, while all wrapping conversions remain in FP64, consistent with the main algorithm. Figure 9 reports the relative L2 errors between BF16 and FP32 computations across 100 uniformly sampled timesteps from = 0 to π 2 , using Cosmos-Predict2 T2I models of 0.6B and 2B parameters. The results indicate that JVP computation is considerably more sensitive to limited BF16 precision than the network output. (a) Cosmos-Predict2-0.6B (b) Cosmos-Predict2-2B Figure 9: Relative L2 errors of the network output and JVP under BF16 precision. Empirically, JVP computation leads to substantially larger numerical errors compared to the network output."
        },
        {
            "title": "F PROMPTS",
            "content": "20 Preprint Table 5: Used prompts in this paper. Prompt Image Red squirrel drumming on tiny twig and acorn drums in autumn woods Casio G-Shock digital watch with metallic silver bezel and black face. The watch displays the time as 11:44 AM on Thursday, March 22nd, with additional features like Bluetooth connectivity, water resistance up to 20 bar, and multi-band 6 radio wave reception. The watch strap appears to be made of stainless steel, and the overall design emphasizes durability and functionality. an alarm clock Video stylish woman walks down Tokyo street filled with warm glowing neon and animated city signage. She wears black leather jacket, long red dress, and black boots, and carries black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating mirror effect of the colorful lights. Many pedestrians walk about. Animated scene features close-up of short fluffy monster kneeling beside melting red candle. The art style is 3D and realistic, with focus on lighting and texture. The mood of the painting is one of wonder and curiosity, as the monster gazes at the flame with wide eyes and open mouth. Its pose and expression convey sense of innocence and playfulness, as if it is exploring the world around it for the first time. The use of warm colors and dramatic lighting further enhances the cozy atmosphere of the image. In an urban outdoor setting, man dressed in black hoodie and black track pants with white stripes walks toward wooden bench situated near modern building with large glass windows. He carries black backpack slung over one shoulder and holds stack of papers in his hand. As he approaches the bench, he bends down, places the papers on it, and then sits down. Shortly after, woman wearing red jacket with yellow accents and black pants joins him. She stands beside the bench, facing him, and appears to engage in conversation. The man continues to review the papers while the woman listens attentively. In the background, other individuals can be seen walking by, some carrying bags, adding to the bustling yet casual atmosphere of the scene. The overall mood suggests moment of focused discussion or preparation amidst busy environment. References Figure 3, Figure 3,5,6 Figure 8 Figure 1,6 Figure 1 Figure"
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Tsinghua University"
    ]
}