{
    "paper_title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions",
    "authors": [
        "Zhenzhi Wang",
        "Jiaqi Yang",
        "Jianwen Jiang",
        "Chao Liang",
        "Gaojie Lin",
        "Zerong Zheng",
        "Ceyuan Yang",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate a single subject and inject conditions in a global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce a novel framework that enforces strong, region-specific binding of conditions from modalities to each identity's spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging a mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in a iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 4 8 9 9 0 . 6 0 5 2 : r InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions Zhenzhi Wang1, Jiaqi Yang2, Jianwen Jiang2, Chao Liang2, Gaojie Lin2, Zerong Zheng2, Ceyuan Yang2, Dahua Lin1 1CUHK MMLab, 2ByteDance wz122@ie.cuhk.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "End-to-end human animation with rich multi-modal conditions, e.g., text, image and audio has achieved remarkable advancements in recent years. However, most existing methods could only animate single subject and inject conditions in global manner, ignoring scenarios that multiple concepts could appears in the same video with rich human-human interactions and human-object interactions. Such global assumption prevents precise and per-identity control of multiple concepts including humans and objects, therefore hinders applications. In this work, we discard the single-entity assumption and introduce novel framework that enforces strong, region-specific binding of conditions from modalities to each identitys spatiotemporal footprint. Given reference images of multiple concepts, our method could automatically infer layout information by leveraging mask predictor to match appearance cues between the denoised video and each reference appearance. Furthermore, we inject local audio condition into its corresponding region to ensure layout-aligned modality matching in iterative manner. This design enables the high-quality generation of controllable multi-concept human-centric videos. Empirical results and ablation studies validate the effectiveness of our explicit layout control for multi-modal conditions compared to implicit counterparts and other existing methods. See our Project Page for more details."
        },
        {
            "title": "Introduction",
            "content": "By leveraging the priors of pretrained Diffusion Transformer-based (DiT) video diffusion models [6, 7, 8, 24, 100, 25, 79, 29, 9, 82, 66, 44, 76, 47], end-to-end human animation models, especially audio-driven approaches [26, 72, 90, 78, 14, 91, 69, 35, 45, 46] has achieved high-quality humancentric video generation and strong controllability from multi-modal conditions, such as text, image and audio. However, most of existing methods commonly hold an assumption of single-identity paradigm: all available conditions should be fused globally and implicitly assumed to describe one unique subject in the given image. Although this global injection strategy simplifies conditioning by sharing the same condition signal across all regions, it fundamentally limits scalability in scenarios involving multiple individuals or complex human-object interactions, where each entity requires distinct appearance and voice attributes. Recent multi-concept video customization methods, such as Video-Alchemist [13], ConceptMaster [34], Phantom [50], and SkyReels-A2 [22], enable injecting multiple reference images into single video, facilitating multi-person or human-object interaction scenarios. However, these methods face significant challenges when directly applied to human animation tasks. In such tasks, animation Equal contribution. Project lead. Preprint. Under review. Figure 1: Video frames generated from audio and multi-concept reference images (human heads/full bodies, objects, scenes) display rich, audio-matched expressions. Our method enables compositional generation including outfit changes, humanobject interactions, anime styles, dialogues even without start frame. Red and green wave icons denote speaking and listening, respectively. signals are highly specific to individual identities and demand precise conditioning and alignment with specific spatiotemporal regions. For example, audio signals are exclusively associated with the current speaker and are unrelated to the background or other individuals concepts. Existing customization methods, akin to end-to-end human animation approaches, still adopt video-level condition injection. While this approach may suffice for general video generation, it often causes confusion in human-centric video generation, making it difficult to produce satisfactory results, as shown in Fig. 3. For end-to-end human-centric video generation, conditioning inputs should ideally include not only global modalities (e.g., reference images and text descriptions) but also local modalities (e.g., audio). As mentioned, existing human animation and multi-concept customization methods fail to address this critical distinction. This limitation motivates us to propose new framework that enables the precise injection of local human-related modalities, capability that is both essential and urgently needed for robust multi-concept, human-centric video generation, as illustrated in Fig. 1. In this paper, we propose InterActHuman, video diffusion framework to achieve spatial alignment of multi-modal conditions for multi-concept human video generation. Unlike previous methods [13, 34], which rely on feature fusion and attention mechanisms to implicitly learn the relationships between condition signals and concepts, InterActHuman introduces an attention module that explicitly predicts the spatial locations where reference concepts appear in the video. This explicit alignment allows the model to accurately associate local audio conditions with the correct spatial regions via iterative mask prediction and masked audio attention during the diffusion inference process. Our framework offers two key advantages. First, it enforces stronger layout constraint by explicitly and precisely binding conditions to their corresponding spatial regions. Second, it provides unified interface for synchronously injecting all modalities such as visual and acoustic inputs, through the layout. These features make InterActHuman well-suited for multi-modal multi-concept human animation and serve as baseline framework for this domain. Although using an explicit mask for condition injection might seem straightforward at first glance, it introduces an intrinsic chicken-and-egg dilemma: during inference, the final video is not yet available, leaving the spatial positions of each identity uncertain, thus making accurate mask prediction impossible; yet without those masks, the spatial injection of local audio conditions cannot be achieved, resulting in an incomplete or misaligned video generation process. To address this dilemma, we leverage the iterative denoising process inherent in diffusion models [67]. Specifically, we introduce mask predictor branch in the diffusion pipeline and adopt an interleaved mask prediction strategy, wherein the mask predictions generated at step serve as guidance for condition injection in step + 1. This iterative refinement breaks the cyclic dependency by progressively discovering the spatial layout, ensuring that even in the absence of ground-truth video during inference, precise spatial alignment is achieved. In essence, our approach transforms the chicken-and-egg problem into sequential and converging process that robustly aligns local audio conditions. 2 Beyond model design, we developed scalable pipeline to automatically assemble high-quality, human-centric animation data to address the lack of suitable multi-concept datasets. This pipeline operates by: 1) accurately tracking individual identities to extract their mask information and images, and 2) aligning audio segments to each identity through lip synchronization. We curated dataset of over two million video-entity pairs, capturing both human-human and human-object interactions across wide range of object categories. In summary, our contributions are as follows: (1) We propose novel human animation framework capable of synthesizing multi-person and humanobject interactions, conditioned on multiple reference images, text descriptions, and audio inputs. The framework also supports long video generation as well as single full-image conditioning. (2) We highlight the importance of local condition injection for multi-concept, multi-modal video generation and introduce simple yet effective design that enables the model to handle both global and local conditions by automatically localizing the conditioned layout. Experimental results demonstrate that our proposed design significantly outperforms existing baselines."
        },
        {
            "title": "2 Related Works",
            "content": "Video Diffusion Models have enabled unprecedented quality text-to-video (T2V) or image-to-video (I2V) generation in recent years, thanks to diffusion-based generative models [28, 67, 39, 68, 52]. Early T2V approaches either adapt pretrained text-to-image networks in training-free manner [66, 87, 59] or fine-tune UNet-based latent diffusion architectures [24, 100, 7, 79]. To push the frontier, recent works compress spatiotemporal features with 3D causal VAEs [95], and migrate to Diffusion Transformer (DiT) backbones [75, 56, 10, 31]. Through progressive low-to-high resolution pretraining and fine-tuning [58, 42, 77], these models yield longer, more coherent, and high-quality videos. Our method is also built upon the pretrained DiT video generation models. Human Animation Models synthesize videos of people driven by text, reference images, human body poses or audios. Early GAN-based methods [64, 99, 65, 36, 81] are trained on small datasets [54, 64, 89, 101] for self-supervised pose transfer. Diffusion-based approaches [63, 98, 32, 84, 85] now surpass GAN-based methods by conditioning on 2D skeletons, 3D depth, or mesh sequences. Audiodriven portrait methods [94, 97, 73, 35, 18] have been extended toward full-body motion via two-stage pipelines for improved hand quality [17, 53, 71, 30] and one-stage unified framework designs [45, 46]. In summary, none of them explored multi-concept human animation, and our method is the first to enable multi-concept human animation with local audio conditions. Multi-Concept Video Customization Models have received limited attention. Early single-concept identity-preserving methods include Videobooth [38], which learns coarse-to-fine embeddings from WebVid [4] via Grounded-SAM [41, 51, 61]; ID-Animator [27], which integrates IP-Adapter [93] into AnimateDiff [24]; and ConsisID [96], which decouples frequency signals to preserve facial identity. More recently, Video-Alchemist [13], ConceptMaster [34], and Phantom [50] support multiple reference images and text descriptions via crossor self-attention injection for generalpurpose multi-concept customization. BlobGen-Vid [23] further enables local layout control of text-specified concepts using useror LLM-provided spatiotemporal masks. However, all existing methods rely solely on image and text conditions and lack support for multimodal inputs such as audio, which we argue are essential for truly versatile, human-centric video generation."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present InterActHuman, our multi-concept video generation framework designed to address the challenges of local condition matching for identities in multi-modal conditions. As shown in Fig. 2, the framework begins by customizing reference images of multiple concepts into video and predicting the mask regions corresponding to each reference appearance in the output video as layout cues with cross-attention as mask predictor. It operates on the features of the noisy video latent and the reference image latent and supervised by ground-truth masks. Finally, local audio conditions are injected into the designated regions guided by an iterative mask prediction procedure. 3 Figure 2: Illustration of our framework, which adaptively predicts masks as the spatial guidance of audio condition injection. In training, we train the mask predictor (cross-attn w/ MLP) with mask loss; in inference, we collect mask predictions to cache and leverage masks predicted from the last denoising step (t 1) to guide the audio cross-attn in the current denoising step (t). 3.1 Problem Setting and Preliminaries Problem Setting. Given caption that describes target video, alongside set of concept reference images {Xi = 1, . . . , } and identity-level audio {Yi = 1, . . . , } where denotes the number of distinct concepts, the objective of Multi-Concept Human Animation is to generate highquality videos that faithfully integrate all image-specified visual concepts with correct lip movements synchronized with each audio signal in accordance with the descriptive caption . Each concept should consistently preserve its visual identity as depicted in the provided images, while accurately expressing the semantic roles and behaviors described in the caption. Preliminaries. Transformer-based text-to-video latent diffusion models have recently demonstrated remarkable capabilities in generating high-quality video content. Our proposed InterActHuman framework is built upon the MMDiT-based video generation model [57, 21, 62] and utilize 3D Variational Autoencoder (VAE) [40], which compresses input videos into compact latent space. For training, we adopt the flow matching objective [49], which formulates the generative process as probability flow ordinary differential equation (ODE). This ODE transports clean latent representations z0 to their noisy counterparts zt along linear path, defined as zt = (1 t)z0 + tϵ at timestep t, where ϵ is sampled from standard Gaussian distribution. In our setting, which incorporates multi-modal conditions (image and audio), the output of the diffusion transformer is parameterized as vΘ(zt, t, cimg, caudio). This output is supervised to predict the velocity (z1 z0), resulting in the following training objective: = Et,z0,ϵ vΘ(zt, t, cimg, caudio) (z1 z0) 2 2. This formulation ensures the model effectively learns to generate videos conditioned on multi-modal inputs while maintaining high fidelity and temporal consistency. 3.2 Multi-Concept Reference Image Injection To handle multiple appearance images, we inject appearance cues via self-attention in the original DiT layers. Each reference image Xi is encoded into latent tensors xi using the same VAE as for noisy video frames. The reference latents are then flattened into token sequences and processed by the DiT along with the noisy latents v. The reference latents will reuse the parameters of DiT to extract features and interact with noisy latents at the self-attention layer in every DiT block, allowing appearance cues to propagate to the denoising pathway. Notably, no extra networks or additional parameters are required, thus preserving the models efficiency. 3.3 Layout Prediction and Local Condition Injection Although reference image injection has been addressed, leveraging spatiotemporal layouts for local conditions like precise audio alignment remains challenging. There is conflict between mask prediction and audio injection: if the forward pass is incomplete, masks are unavailable; if it is complete, its too late for local condition injection. We resolve this by using the multi-step inference process of diffusion models [67], where masks predicted at step guide multi-modal condition injection at step + 1. As our network learns spatiotemporal layouts for each identity, we obtain precise masks without requiring user annotations. (cid:17) = softmax Mask Predictor. Within each DiT block, we predict spatio-temporal mask that quantifies how strongly each reference image should influence each video frame. We attach lightweight maskpredictor head to each of the transformer layers. The head comprises: (1) shared linear projection that maps the hidden video features hv and the hidden reference features hr to query, key, and value tensors, (2) LayerNorm, (3) 3D RoPE positional encoding, (4) cross-attention module, and (5) two-layer MLP. After normalization and RoPE, we let the video tokens attend to single reference by (cid:16) QvKr computing a(l) , where is the head dimension and indexes the current layer. The attended feature a(l) is subsequently transformed by the MLP and passed through sigmoid to yield layer-specific mask m(l) [0, 1]T for reference Xi. Then we average the mask prediction from last few DiT blocks to form the final mask. Notably, the mask predictor is always trained to predict complete region of human, regardless of whether the reference image contains only the upper body, face, or full body of person. This design simplifies the mask prediction task and improves consistency for the conditioning module, ensuring robust and reliable conditioning across diverse scenarios. Moreover, since the mask predictors reuse in-block features and share parameters across references, they introduce minimal computational overhead while enabling explicit spatiotemporal layout control for each identity. Vr Mask Prediction by Caching During Inference.During inference, mask prediction requires the integration of results across all layers, which poses challenges in the early stages of denoising. At these initial steps, the latent representations of reference concepts are often too ambiguous, preventing the conditioning module from generating precise mask layouts. To address this issue, we adopt an iterative mask prediction strategy. At each denoising step, the mask predicted in the previous step is cached as layout guide for the current step, enabling the model to progressively improve the accuracy of spatiotemporal localization. This iterative process ensures that the masks dynamically adapt to the evolving video content, maintaining consistency for each concept throughout the sequence and improving the overall conditioning quality. Local Audio Conditioning. To prepare our model for multi-concept human-centric video generation, we first pretrain it on single-identity audio-conditioned animation by adding cross-attention-based audio conditioning without masks and mixed-conditions training strategy, following recent work [46]. It utilizes new cross-attention layer to inject wav2vec [2] audio features into each DiT block after the MMDiT layer. In multi-concept setting, we find that utilizing pretrained audio cross-attention has already lead to satisfactory per-identity audio-driven performance. So we implement local audio conditioning mainly in the iterative process in inference. Instead of processing the entire set of noisy video tokens as in previous works [46, 45], we inject wav2vec features into the tokens corresponding to specific identity, using the mask predicted in the previous step as guidance. To ensure smooth transitions in noisy video features and final output videos, we apply linear combination of meaningful audio features and muted audio features for each noisy video token, based on the predicted mask confidence along the mask edges. Moreover, such local audio conditioning enables us to generate multi-person videos with dialogues. As long as the dialogue audio are separated into individual audios as the input, our model could generate realistic multi-person videos where people take turns to speak, facilitating many applications. Training Loss and Strategies. The overall training loss combines flow matching diffusion loss (see Sec. 3.1) with focal loss[48] for mask classification, which mitigates the training instability and gradient explosion encountered with binary cross-entropy loss, likely due to foreground-background imbalance or low-quality masks. frame alignment flag is employed to exclude frames with invalid or low-quality masks from loss computation, ensuring robust temporal supervision. To address the copy-paste behavior of diffusion models, which often replicate subjects from reference images with minimal variation in camera angle or posture, we apply random masking to reference images, selectively revealing the head, full body, or clothing of the original subject appearance to encourage appearance diversity. We use an appearance conditioning ratio of face to full body of 0.7 : 0.3, while balancing the contributions of the diffusion loss and focal loss equally with ratio of 1 : 1. 5 Figure 3: Qualitative comparison with previous methods on multi-concept audio injection. Inference Strategies. Given reference images and text prompt, our method use Qwen2.5-VL [3] as the re-phraser to get detailed descriptions from each reference images, and then integrate them with the original text prompt to form the final prompt. We utilize shared classifier-free guidance (CFG) across audio and text. To achieve local control, audio conditions are injected solely within the predicted mask regions for each reference. Notably, early denoising steps yield unreliable masks, and using them for condition injection can suppress other references. Thus, mask-based condition injection is omitted for the first 10 denoising steps and applied subsequently using the mask predicted from the previous step. We only inject audio signals with masks into positive result while applying CFG, and use CFG strength as 6.5 and 50 denoising steps. 3.4 Multi-Concept Human-Centric Video Data Curation We curate large-scale, human-centric video dataset from large internal video dataset, filtering out videos that are too short (<4 seconds) or contain too many salient humans (>3) as detected by human pose detector [92]. Unlike previous multi-concept video generation methods [13, 34] that rely on conventional pipelines like Grounding-DINO [51] and SAM [41] for obtaining single reference image per identity, our multi-stage pipeline leverages advanced vision-language models. First, each raw video is densely captioned using Qwen2-VL [80] (distilled from Gemini-2.0-Pro), generating fine-grained descriptions of the environment, subjects appearance, actions, expressions, interacting salient objects, and inter-subject interactions. Next, these descriptions are parsed by zero-shot Gemini-2.0-Flash API to extract structured appearance phrases. For spatial supervision, we use Grounding-SAM2 [61] with the query person to produce accurate, temporally consistent masks, which are used both for extracting foreground reference images (with white background) and as ground-truth for our mask predictor. Our corpus comprises over 2.6M triplets of videos, per-frame masks, and captions, forming the foundation of InterActHuman."
        },
        {
            "title": "4 Experiments",
            "content": "Baselines. Since ConceptMaster [34] and Video-Alchemist [13] are not open-sourced, we do not have access to their models nor test sets. Therefore, we primarily compare our method with recent multi-concept video customization models through their publicly available APIs or public models, evaluating performance from the perspectives of visual appearance and adherence to text prompts, including Vidu2.0 [5], Pika2.1 [1], Kling 1.6 [43] and Phantom [50], and audio-driven human video generation methods, including DiffTED [30], DiffGest [102] + Mimiction [98], CyberHost [45], OmniHuman [46] and Kling 1.6 [43] with lip synchronization. Evaluation Metrics. As audio and appearance are key characteristics of humans, we follow current state-of-the-art method [46, 34] to evaluate audio-driven, multi-concept human-centric video generation. To comprehensively evaluate methods for this task, we consider five dimensions: 1) Concept fidelity: We leverage CLIP-I [60] and DINO-I [55] on subjects in the generated videos to assess whether they are aligned with the provided reference images. To get cropped subject 6 Table 1: Quantitative comparisons with audio-conditioned full-body animation baselines. Methods Single-Person Test Set Multi-Person Test Set Sync-C HKV HKC Sync-D IQA AES FVD DiffTED [30] DiffGest. [102]+Mimic. [98] CyberHost [45] Kling1.6 [43] + Lip-sync. OmniHuman [46] w/o mask OmniHuman [46] w/ fixed mask Ours 0.926 0.496 6.627 4.449 7.443 - 7.272 - 23.409 24.733 46.490 47.561 - 59.635 0.769 0.833 0.884 0.826 0.898 - 0.885 - - 8.974 8.401 9.482 7.068 6.670 - - 4.011 4.716 4.768 4.690 - - 2.856 3.444 3.466 3.369 - - 54.797 33.555 33.895 40. 4.757 3.467 22.881 Table 2: User preference evaluation. means publicly available version with Wan2.1-1.3B. Metric Kling [43] OmniHuman [46] Ours Pika [1] Audio-Driven Multi-Concept Customization Phantom [50] Kling [43] Vidu [5] Ours Avg. Score Top-1 (%) 1.70 14.5% 1.82 25.6% 2.48 2.22 59.9% 4.9% 2.46 9.9% 2.90 13.6% 4.01 3.40 22.2% 49.4% images in output videos, we randomly sample 5 frames in each video and use Florence-2 [88] to detect the subject bounding boxes. We then crop the detected regions and compute the CLIP-I and DINO-I scores. We also employ Face-Arc [19], Face-Cur [33] and Face-Glink [19] to evaluate the fidelity of facial features if the concept is human. 2) Prompt following: We utilize video-level CLIP [83] to measure the similarity between the input text prompts and the resulting video content. 3) Visual quality: We utilize q-align [86], vision-language model, for no-reference image quality assessment (IQA) and aesthetics score estimation (ASE). 4) Audio synchronization and human pose diversity: For lip synchronization, we leverage the widely used Sync-C and Sync-D [15] to compute audio-visual confidence. We also incorporate hand keypoint confidence (HKC) and hand keypoint variance (HKV) [45] to quantify hand-pose accuracy and motion richness, respectively. For simplicity, we assume each test video contains exactly two participants: one speaking (with meaningful audio as input) and one listening (with muted audio as input). 5) Distribution distance: We employ FVD [74] to measure the distance between generated and ground-truth videos. Test Sets. We use three test sets in our experiments: 1) single-person audio conditioned human animation test set following OmniHuman [46] (see Tab. 1); 2) our collected two-person audio conditioned human animation test set where only one person is talking (see Tab. 1 and Tab. 4); and 3) multi-concept video customization test set following Phantom [50], where we select 100 human-related pairs of reference images and text prompts in our experiments (see Tab. 3). 4.1 Comparisons with State-of-the-Arts Multi-Concept Audio-Driven Human Video Generation. In Tab. 1, our approach achieves state-ofthe-art or comparable performance in lip synchronization accuracy and motion diversity, particularly excelling in complex multi-person interactions where baseline methods[45, 43] struggle with accurate audio signal assignments. For single-person scenarios, our method performs on par with specialized models like OmniHuman [46]. In multi-person settings, existing methods including OmniHuman and its extensions as well as leading commercial video generation models with post-processed lip-sync, fail to deliver satisfactory results. Poor lip-sync accuracy highlights their inability to generate accurate lip movements for the correct person. Although OmniHuman with oracle masks (manually setting audio-conditioned regions) improves lip-sync accuracy, it still significantly degrades overall video quality, as reflected in the FVD metric. This clearly demonstrates the limitations of existing methods that rely on single-identity assumptions, hindering their applicability to broader scenarios. In contrast, our method achieves strong performance in both lip-sync accuracy and overall video synthesis quality, effectively addressing the challenges of audio-conditioned multi-person video generation while remaining compatible with existing settings. Fig. 3 presents qualitative results for multi-person audio-driven video generation. While Kling1.6 w/ lip-sync shows many audio assignment errors and OmniHuman w/ fixed mask shows inflexible audio control with many missing cases, our method consistently assigns audio signals to the correct identity and demonstrates better motion dynamics and more precise audio-driven animation. It is worth noting that all previous methods rely on reference 7 Table 3: Quantitative comparison of subject consistency, prompt following and visual quality.means publicly available version with Wan2.1-1.3B. Methods Decoupled Concept Fidelity Prompt Video Quality CLIP-I DINO-I Face-Arc Face-Cur Face-Glink ViCLIP-T AES IQA Vidu2.0 [5] Pika2.1 [1] Kling1.6 [43] Phantom [50] Ours 0.696 0.688 0.659 0. 0.744 0.458 0.459 0.420 0.476 0.533 0.568 0.579 0.552 0.589 0.598 0.562 0.566 0.547 0. 0.600 0.597 0.607 0.582 0.615 0.644 18.61 19.39 18.38 17.73 18.87 3.350 3.534 3.487 3. 4.689 4.791 4.787 4.812 3.565 4.903 Figure 4: Qualitative comparison with previous methods on subject consistency and text following. frame containing complete information to generate talking-person videos, while our method only needs reference appearance of humans head or full-body images and audios. User Study. We conducted user study to evaluate our method on two tasks: (1) lip synchronization in multi-person talking videos and (2) subject consistency in multi-concept customizations. The lip sync test used 19 videos from three methods, while subject consistency was assessed on 9 videos from five methods. We use the same model (labeled ours) for two evaluation tasks. Ten experienced users ranked each method with scores (higher values indicate superior performance). Tab. 2 reports the average scores and top-1 selection percentages. Our method achieved the highest scores and top-1 rates in both tasks, validating the effectiveness of our method. Multi-Concept Video Customization. Although our major contribution is not on multi-concept video customization, we show that our model is also capable of preserving multi-concept visual appearances. In Tab. 3, our method outperforms existing approaches [5, 1, 43] in preserving identity details and facial features, addressing the common degradation issue in multi-subject generation. Notably, we achieve this without sacrificing audio injection performance, while these methods are incapable to generate videos from audio conditions. It suggests our joint optimization framework on video generation and mask prediction successfully balances video generation. Our method ranks second in prompt following, likely due to our audio-driven human-centric training data focusing on talking and singing, which limits prompt diversity compared to text-to-video tasks. Despite this, qualitative results  (Fig. 4)  show that our approach maintains natural subject consistency and visual quality in both real-world and anime domains, outperforming previous methods that suffer from unnatural compositions and degraded visuals. 4.2 Ablation Study We validate our local audio injection designs via ablation on three variants: 1) Global audio [46] applies audio across the entire feature map; 2) ID Embedding injects audio features with learnable ID embedding without mask prediction; 3) Fixed Mask uses predefined static spatial masks. As shown in Tab. 4, our framework with predicted dynamic masks achieves the best Sync-D and FVD 8 Table 4: Ablation study on audio-driven multi-person animation methods. Variants Sync-D IQA AES FVD Global audio condition ID Embedding Fixed Mask Predicted Mask (Ours) 9.482 8.627 7. 6.670 4.768 4.658 4.690 3.466 3.338 3.369 33.895 35.665 40.239 4.757 3. 22.881 Figure 5: Qualitative ablation on audio injection strategies. scores. The fixed mask yields decent Sync-D but suffers motion artifacts (worst FVD), and global audio, while scoring best in IQA, offers poor audio-visual alignment. Qualitative results in Fig. 5 further reveal that global audio drives all identities uniformly, ID embedding often mismatches audio with identities, and fixed masks lose alignment when characters move. These findings underscore that current methods overlook the need for precise local conditions, highlighting the strength of our dynamic, adaptive mask prediction strategy for multi-person talking video generation."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced InterActHuman, novel end-to-end video diffusion framework that supports spatially aligned, multi-modal conditioning for multi-concept human animation. By integrating an efficient mask-prediction module into pretrained DiT backbone, our method automatically infers per-identity spatio-temporal layouts from reference images and uses these masks to guide local audio conditioning. This explicit layout binding enables each reference concept to retain its unique appearance and voice, even in complex scenes with multiple interacting entities. To facilitate training procedure of our model, we presented high-throughput pipeline for harvesting and annotating over 2.6 million identity-aware video snippets, complete with per-frame masks for humans and objects. Extensive experiments on both singleand multi-person benchmarks demonstrate that InterActHuman achieves state-of-the-art performance in lip synchronization, motion diversity and subject apperance fidelity, while maintaining competitive video quality. We hope it could serve as solid baseline for the multi-concept human animation and audio-driven multi-person video generation community, where further improvements could be built upon it. Limitations. Since the goal of this work is for human-centric video generation, the available data domain is inherently narrower than one used for general text-to-video pretraining, limiting the ability to follow the diverse text prompts. While our framework is designed to accommodate any number of concept images, the training dataset predominantly consists of videos featuring two to three individuals. This distribution may constrain the generalization of any number of inputs, suggesting further improvements could be achieved by enlarging the diversity and scale of training data. Broader Impacts. Our model could be used to generate misinformation by using celebrity images and voices. We will strictly restrict access and add watermarks to prevent misuse."
        },
        {
            "title": "References",
            "content": "[1] Pika art. https://pika.art/. Accessed: 2025-05-12. [2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. [5] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. [6] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563 22575, 2023. [9] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. Advances in Neural Information Processing Systems, 35:3176931781, 2022. [10] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213229. Springer, 2020. [12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [13] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. arXiv preprint arXiv:2501.06187, 2025. [14] Zhiyuan Chen, Jiajiong Cao, Zhiquan Chen, Yuming Li, and Chenguang Ma. Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions. arXiv preprint arXiv:2407.08136, 2024. [15] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, pages 251263. Springer, 2017. [16] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, pages 251263. Springer, 2017. [17] Enric Corona, Andrei Zanfir, Eduard Gabriel Bazavan, Nikos Kolotouros, Thiemo Alldieck, and Cristian Sminchisescu. Vlogger: Multimodal diffusion for embodied avatar synthesis. arXiv preprint arXiv:2403.08764, 2024. [18] Jiahao Cui, Hui Li, Yun Zhan, Hanlin Shang, Kaihui Cheng, Yuqi Ma, Shan Mu, Hang Zhou, Jingdong Wang, and Siyu Zhu. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks. arXiv preprint arXiv:2412.00733, 2024. 10 [19] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, pages 46904699, 2019. [20] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. [21] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [22] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. Skyreels-a2: Compose anything in video diffusion transformers. arXiv preprint arXiv:2504.02436, 2025. [23] Weixi Feng, Chao Liu, Sifei Liu, William Yang Wang, Arash Vahdat, and Weili Nie. Blobgen-vid: Compositional text-to-video generation with blob video representations. arXiv preprint arXiv:2501.07647, 2025. [24] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In International Conference on Learning Representations (ICLR), 2023. [25] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. [26] Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu, et al. Gaia: Zero-shot talking avatar generation. arXiv preprint arXiv:2311.15230, 2023. [27] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 68406851. Curran Associates, Inc., 2020. [29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [30] Steven Hogue, Chenxu Zhang, Hamza Daruger, Yapeng Tian, and Xiaohu Guo. Diffted: One-shot audio-driven ted talk video generation with diffusion-based co-speech gestures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19221931, 2024. [31] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. [32] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [33] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59015910, 2020. [34] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. [35] Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, and Yanbo Zheng. Loopy: Taming audio-driven portrait avatar with long-term motion dependency. arXiv preprint arXiv:2409.02634, 2024. [36] Jianwen Jiang, Gaojie Lin, Zhengkun Rong, Chao Liang, Yongming Zhu, Jiaqi Yang, and Tianyun Zhong. Mobileportrait: Real-time one-shot neural head avatars on mobile devices. arXiv preprint arXiv:2407.05712, 2024. [37] Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. Rtmpose: Real-time multi-person pose estimation based on mmpose. arXiv preprint arXiv:2303.07399, 2023. 11 [38] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu. Videobooth: Diffusion-based video generation with image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66896700, 2024. [39] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. [40] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. [42] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [43] Kuaishou. Kling video model. https://kling.kuaishou.com/en, 2024. Accessed: 29 April 2025. [44] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [45] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, and Yanbo Zheng. Cyberhost: Taming audio-driven avatar diffusion model with region codebook attention. arXiv preprint arXiv:2409.01876, 2024. [46] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. [47] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. [48] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988, 2017. [49] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [50] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, and Xinglong Wu. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079, 2025. [51] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [52] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [53] Rang Meng, Xingyu Zhang, Yuming Li, and Chenguang Ma. Echomimicv2: Towards striking, simplified, and semi-body human animation. arXiv preprint arXiv:2411.10061, 2024. [54] Nagrani, Chung, and Zisserman. Voxceleb: large-scale speaker identification dataset. Interspeech 2017, 2017. [55] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [56] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [57] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [58] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 12 [59] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv:2303.09535, 2023. [60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [61] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [62] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. [63] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. Human4dit: Free-view human video generation with 4d diffusion transformer. arXiv preprint arXiv:2405.17405, 2024. [64] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. [65] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1365313662, 2021. [66] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [67] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [68] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [69] Michal Stypulkowski, Konstantinos Vougioukas, Sen He, Maciej Zieba, Stavros Petridis, and Maja Pantic. Diffused heads: Diffusion models beat gans on talking-face generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 50915100, 2024. [70] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [71] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng Bo. Emo2: End-effector guided audio-driven avatar video generation. arXiv preprint arXiv:2501.10687, 2025. [72] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive-generating expressive portrait videos with audio2video diffusion model under weak conditions. arXiv preprint arXiv:2402.17485, 2024. [73] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2025. [74] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. [75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [76] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length In International Conference on Learning video generation from open domain textual descriptions. Representations, 2022. 13 [77] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [78] Cong Wang, Kuan Tian, Jun Zhang, Yonghang Guan, Feng Luo, Fei Shen, Zhiwei Jiang, Qing Gu, Xiao Han, and Wei Yang. V-express: Conditional dropout for progressive training of portrait video generation. arXiv preprint arXiv:2406.02511, 2024. [79] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [80] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [81] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1003910049, 2021. [82] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. Imaginator: Conditional spatiotemporal gan for video generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 11601169, 2020. [83] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [84] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, and Dahua Lin. Humanvid: Demystifying training data for camera-controllable human image animation. In Advances in Neural Information Processing Systems, 2024. [85] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Yuwei Guo, Dahua Lin, Tianfan Xue, and Bo Dai. Multiidentity human image animation with structural video diffusion. arXiv preprint arXiv:2504.04126, 2025. [86] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. [87] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [88] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. [89] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchmark for video face super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 657666, 2022. [90] Mingwang Xu, Hui Li, Qingkun Su, Hanlin Shang, Liwei Zhang, Ce Liu, Jingdong Wang, Luc Van Gool, Yao Yao, and Siyu Zhu. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation. arXiv preprint arXiv:2406.08801, 2024. [91] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. arXiv preprint arXiv:2404.10667, 2024. [92] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with twostages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. [93] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 14 [94] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He, and Zhou Zhao. Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis. In The Eleventh International Conference on Learning Representations, 2022. [95] Lijun Yu, Jos Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [96] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Identity-preserving text-to-video generation by frequency decomposition. arXiv preprint Li Yuan. arXiv:2411.17440, 2024. [97] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86528661, 2023. [98] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. [99] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36573666, 2022. [100] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. [101] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebv-hq: large-scale video facial attributes dataset. In European conference on computer vision, pages 650667. Springer, 2022. [102] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. Taming diffusion models for audio-driven co-speech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1054410553, 2023."
        },
        {
            "title": "A Algorithm of Our Model Implementation",
            "content": "Please refer to Algorithm 1 for our model implementation."
        },
        {
            "title": "B Experiment Details of Our Model",
            "content": "B.1 Implementation Details Our model was trained for 10,000 steps on 32 A800 GPUs with learning rate of 3e-5. We adopted the PyTorch framework combined with Fully Sharded Data Parallel (FSDP) to finetune the DiT model across multiple GPUs. Specifically, the model was partitioned such that different GPUs handled distinct portions of the models parameters. We configured the effective batch size so that every nodecomprising 8 GPUsprocessed 2 videos simultaneously. With 32 GPUs in total (i.e., 4 nodes), this resulted in an overall effective batch size of 8 videos. B.2 Ablation Details In Tab. 4 and Fig. 5 of the paper, we validate our local audio injection designs via ablation on three variants: 1) Global audio [46] applies audio across the entire feature map; 2) ID Embedding injects audio features with learnable ID embedding without mask prediction; 3) Fixed Mask uses predefined static spatial masks. For global audio, it is identical to the pretrained audio-driven model [46]. For fixed mask, it is manually input rectangle mask upon the audio cross-attention of pretrained audio-driven model [46]. It requires the generated human being static and it needs the user to ensure there is only one person inside mask. For ID embedding, here we provide some implementation details. Inspired by detection transformers [11], we introduce set of learnable ID embeddings Equery RN C. These embeddings serve as identity tokens for the individuals. Specifically, we add the same learnable ID embedding equery RC to paired reference image and audio segment. In this way, we expect the model to implicitly match an individual (defined by reference image) and audio segments in the output video to get synchronized lip movements. For the ID embedding dataset preparation, we incorporate multi-person videos into training. We use Active Speaker Detection (ASD) to extract talking persons identities as bounding box with identity-number. Then we use the bounding box and SAM2 masks to match the identity-number of each timestep of audio based on their IoU. Finally we use paired audio with timestep and reference image to generate video, and use flow-matching loss only to supervised it. No mask information is provided to the model and no mask supervision adopted. In the main paper, our experimental results in ablation study indicate that implicit matching of individuals and audio segments is worse than explicit matching with layout-aligned mask prediction It is worth noting that although our implementation shows this evidence, and audio injection. implicit matching is not necessarily worse than explicit matching. We show that straight-forward implementation of implicit matching cannot generate satisfactory results, yet there could be better implementations to improve this result in the future. Details of Audio-driven Base Models Architecture and Training This appendix provides detailed description of the network structure and training specifics for the pretrained audio-driven single-person video generation model, OmniHuman [46]. As noted in our main paper, the text-to-video base model undergoes post-pretraining for audio conditioning, following the OmniHuman [46] methodology. Subsequently, our multi-concept framework is built and trained on this foundation. C.1 Audio-driven Base Model Our foundational model is composed of Variational Autoencoder (VAE) and Latent Diffusion Transformer (DiT). The VAE includes an encoder, which compresses raw pixel data into compact latent representation, and decoder, which reconstructs the original pixel inputs from these latent features. The VAE achieves compression ratios of (4, 8, 8) for the (t, h, w) dimensions, respectively. Both the encoder and decoder utilize temporally causal convolutional architecture, facilitating image and video compression across both spatial and temporal domains within the joint latent space. The denoising latents possess 16 channels. The DiT Blocks are based on the dual-stream Diffusion Transformer (DiT)[20]. This transformer processes video and text tokens through multiple self-attention layers and feedforward networks (FFNs) to learn representations that are both shared and modality-specific. SwiGLU is employed as the activation function to enhance nonlinear modeling capabilities. Additionally, AdaSingle[12] is used for efficient timestep modulation. Moreover, two-thirds of the FFN weights in the deeper layers share parameters, creating hybrid-stream design that preserves model capacity while substantially improving parameter efficiency. C.2 Mixed Training Strategy We utilize multi-condition training strategy based on the framework established in OmniHuman [46]. Our method employs two-stage progressive training scheme to develop the base models capabilities. In the initial stage, the model is trained solely on text-to-video (T2V) data to build fundamental video generation abilities. The second stage introduces audio-synchronized datasets to expand the models functionality to include audio-driven generation and reference image injection. For this second stage, we follow two core principles from OmniHuman: 1) Tasks with stronger conditioning can leverage tasks with weaker conditioning and their associated data to broaden the effective training dataset; 2) Tasks with stronger conditioning should be assigned proportionally lower training ratios. Guided by these principles, we first train the reference image injection capability before introducing the audio-driven generation objective. This staged methodology facilitates efficient knowledge transfer while ensuring stable training dynamics throughout the multi-condition learning process. 16 Additional Details for Audio-driven Base Model Dataset D.1 Dataset Curation We initially filter the T2V data using rules 1 through 6. Following this, we apply rule 7 to further refine the audio-driven data. 1. Video Clip. We begin by using PySceneDetect ( https://github.com/Breakthrough/PySceneDetect) to identify and trim shot transitions and fades within video clips. After this process, all clips are standardized to duration of 5 to 30 seconds. 2. Human. Utilizing the annotated video captions, we implement rule-based system to detect keywords such as \"people\", \"human\", \"men\", \"women\", \"girl\", and \"boy\". If any of these keywords are found, the video is categorized as human-related. This technique ensures the dataset is effectively filtered to comprise only videos pertinent to human activities and interactions. 3. Subtitles. We use PaddleOCR ( https://github.com/PaddlePaddle/PaddleOCR) to detect subtitles in the videos and remove clips where subtitles change. This step guarantees that the dataset emphasizes continuous and consistent visual content, reducing distractions from textual variations and improving the data quality for subsequent tasks. 4. Visual Quality. We utilize Q-align [86] to evaluate the visual quality of the videos, filtering out clips that do not meet predefined threshold. This procedure ensures the dataset maintains high standard of visual clarity, which is essential for accurate analysis and robust model performance. By eliminating low-quality segments, we enhance the overall dependability and utility of the dataset. 5. Aesthetics. We employ Q-align [86] to assess the aesthetic appeal of the videos and discard clips falling below set threshold. Through aesthetic quality assessment, we can filter out videos containing post-production elements, thereby improving the quality of the training dataset. This measure ensures the dataset is composed of natural and unaltered video content, which better represents real-world scenarios and boosts the robustness and generalization of the trained models. 6. Motion. We use Raft [70] to calculate the optical flow of the videos and filter out clips exhibiting overly intense motion. This step ensures that the dataset includes only video clips with moderate and significant motion, which are more appropriate for analysis and model training. By removing clips with extreme motion, we enhance the stability and quality of the dataset, leading to more dependable results. 7. Syncnet. For audio-driven content, we employ SyncNet [16] to determine if the lip movements are synchronized with the audio. Videos displaying considerable asynchronization are excluded. This step ensures that the dataset consists only of high-quality, synchronized audio-visual data. By removing out-of-sync segments, we improve the overall quality and reliability of the dataset. Ultimately, we gather 2,000 hours of audio-driven data. D.2 Dataset Analysis To gain better understanding of the datasets distribution, we perform an analysis across three dimensions. Detailed definitions for each dimension are provided below. Human Size. Human size indicates the portion of the human body visible within the video frame. It is categorized into the following levels: Portrait (head and shoulders), Chest (from head to chest), Waist (from head to waist), Knees (from head to knees), and Full Body (the entire body is visible). To determine this, we first detect body keypoints using RTMpose [37], and then classify the human size based on the confidence scores of these keypoints. This method ensures robust and accurate categorization of the visible human body extent in each video. Motion Amplitude. After extracting body keypoints from the video, the amplitude of human motion is computed by measuring the displacement of the chest keypoint over time, relative to the width of the shoulders. Based on these calculations, we classify motion amplitude into four categories: Slight(< 0.1), Moderate(0.1 0.2), Significant(0.2 0.3), and Extreme(> 0.3). 17 Scene. Using video captioning, we employ Doubao to categorize videos into the following scenarios: Household, Work, Show, Outdoor Adventure, transportation, Arts and Crafts, Sports, and Others. Language. For data containing audio, we use language detection tool ( https://github.com/Mimino666/langdetect) to identify and count the types of languages present. These are categorized as English, Chinese, Spanish, French, German, Urdu, Hindi, and Others."
        },
        {
            "title": "E User Study Details",
            "content": "Here we show our questionnaires used in the user study. E.1 Multi-person Audio-Driven Video Generation Questionnaire Please take 10 minutes to complete the following ranking questions. For each question, consider the priority of the three videos based on the following dimensions and rank them accordingly: (1) Lip-sync Accuracy: Based on the audio you hear, do you think the lip movements are accurate? Ideally, only one persons lip movements should correspond to the audio, but it doesnt matter specifically who is speaking. It is considered poor if multiple people are speaking simultaneously or if no one is speaking when there is audio. When multiple methods all correctly show only one person speaking, rank them based on the quality of lip-sync with the audio. Perfect synchronization is good; missing syllables or incorrect lip shapes for certain syllables is poor. (2) Sense of Dialogue Among Multiple People: Is there feeling of conversation between the individuals? It is considered good if it portrays natural scenario where one person speaks and another listens. The judgment here is based on whether their expressions during the interaction appear natural and whether the overall feeling of the conversation is natural. (3) Video Quality: If all the above criteria are tied, then prioritize the video with higher quality. The priority of these three criteria decreases in the order listed. Aspect ratio and resolution should not be taken into account; you should not favor particular aspect ratio or higher resolution, but rather focus on the inherent video quality itself. E.2 Multi-concept Video Generation Questionnaire Please take 8 minutes to complete the following ranking questions. For each question, consider the priority of the five videos from the perspective of consistency between the reference images and the appearance in the video, and rank them accordingly: (1) Consistency with Reference Images: Based on the reference images, do you think all the listed reference images appear in the video? And does the appearance of those in the video match the images? Consider the priority in the following order from high to low: Ranked highest: Appearance is consistent, and all reference images appear. Next: All reference images appear, but the appearance of some reference images is inconsistent. Ranked lowest: Some reference images do not appear. (2) Video Quality: If the above criteria are tied, then prioritize the video with higher quality. (3) Motion Strength: If all the above criteria are tied, then prioritize the video with larger motion strength. The priority of these criteria decreases in the order listed. Aspect ratio and resolution should not be taken into account; you should not favor particular aspect ratio or higher resolution, but rather focus on the inherent video quality itself. 18 Algorithm 1 InterActHuman: Audio-Driven Multi-Concept Video Generation Inference Require: Text prompt , reference images {Xi}N Total diffusion steps S, mask injection threshold step Smask VAE Encoder/Decoder, diffusion transformer i=1, identity-level audio {Yi}N i=1 Ti Rephrase(Xi, ) xi VAE_Encoder(Xi) ai wav2vec(Yi) Ensure: Generated video 1: Preprocessing: 2: for 1 to do 3: 4: 5: 6: end for 7: ctext {T, T1, . . . , TN } 8: zS (0, I) 9: Initialize mask cache: {mprev 10: for downto 1 do 11: 12: 13: 14: 15: 16: 17: for 1 to do } 0 for each DiT block layer that we inject conditions do Reference Injection: Inject {xi} via concatnation and self-attention. Compute hidden video feature hv and reference feature hr . Qv Projq(hv). [Kr ). Apply LayerNorm and 3D RoPE to the features. Compute cross-attention: ] Projk,v(hr , Vr Generate detailed prompt via Qwen2.5-VL. Encode reference image. Extract audio features. Aggregate text conditions. Initialize the noisy video latent. 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: p(l) softmax (cid:16) Qv(Kr ) (cid:17) Vr . Predict layer mask: m(l) sigmoid (cid:16) (cid:17) MLP(p(l) ) . end for end for Aggregate Masks: mi 1 (cid:88) l=1 m(l) {1, . . . , }. Cache current masks: {mprev if < Smask then for 1 to do } {mi}. Audio Injection: inject local audio by updating the noisy latent hv. Qv Projq(hv). [Kmute Compute cross-attention: ] Projk,v(amute , Vmute ). [Ki, Vi] Projk,v(ai). pi softmax (cid:16) Qv(Ki) (cid:17) Vi, pmute softmax (cid:16) Qv(Kmute ) (cid:17) Vmute , Bind audio conditions: hv hv + mi pi + (1 mi) pmute , where denotes element-wise multiplication. end if Update latent via diffusion step via flow-matching formulas or custom samplers. end for 29: 30: 31: 32: end for 33: Decoding: Decode the final latent via VAE: return VAE_Decoder(z0)."
        }
    ],
    "affiliations": [
        "ByteDance",
        "CUHK MMLab"
    ]
}