{
    "paper_title": "Utility-Learning Tension in Self-Modifying Agents",
    "authors": [
        "Charles L. Wang",
        "Keir Dorchen",
        "Peter Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As systems trend toward superintelligence, a natural modeling premise is that agents can self-improve along every facet of their own design. We formalize this with a five-axis decomposition and a decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces a sharp utility--learning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding a single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 9 9 3 4 0 . 0 1 5 2 : r Utility-Learning Tension in Self-Modifying Agents Charles L. Wang Keir Dorchen Peter Jin"
        },
        {
            "title": "Abstract",
            "content": "As systems trend toward superintelligence, natural modeling premise is that agents can selfimprove along every facet of their own design. We formalize this with five-axis decomposition and decision layer, separating incentives from learning behavior and analyzing axes in isolation. Our central result identifies and introduces sharp utilitylearning tension, the structural conflict in self-modifying systems whereby utility-driven changes that improve immediate or expected performance can also erode the statistical preconditions for reliable learning and generalization. Our findings show that distribution-free guarantees are preserved iff the policy-reachable model family is uniformly capacity-bounded; when capacity can grow without limit, utility-rational self-changes can render learnable tasks unlearnable. Under standard assumptions common in practice, these axes reduce to the same capacity criterion, yielding single boundary for safe self-modification. Numerical experiments across several axes validate the theory by comparing destructive utility policies against our proposed two-gate policies that preserve learnability."
        },
        {
            "title": "1 Introduction",
            "content": "Classical learning theoryfrom realizable and agnostic PAC to information-theoretic and computational analysesrests on tacit premise: the learning mechanism is architecturally invariant. Parameters may adapt, but the agents update rules, representational scaffolding, topology, computational substrate, and meta-reasoning are treated as fixed. As capabilities trend toward strong open-ended autonomy, however, it is increasingly realistic to assume that advanced agents will self-improve broadly, rewriting not just weights but the very mechanisms by which they learn. Evidence for this shift already exists. Reinforcement learning and meta-learning instantiate constrained self-change (Sutton and Barto, 2018; Finn et al., 2017; Rajeswaran et al., 2019; Hospedales et al., 2022), while open-ended pipelines iterate code edits and tools (Zhang et al., 2025). Decisiontheoretic proposals investigate provably utility-improving modifications (Schmidhuber, 2005), safety analyses document pathologies (Orseau and Ring, 2011), and metagoal frameworks aim to stabilize goal evolution (Goertzel, 2024). What remains underdeveloped is learning-theoretic account of post-modification behavior: when do seemingly rational self-changes preserve the conditions under which learning is possible, and when do they destroy them? We prove policy-level learnability boundary: distribution-free PAC guarantees are preserved iff the policy-reachable family has uniformly bounded capacity. simple Two-Gate guardrail (validation margin τ + capacity cap K(m)) keeps trajectories on the safe side and yields VC-rate oracle inequality. 1 Our Contributions. Policy boundary (iff ). Under standard i.i.d. assumptions, distribution-free PAC learnability is preserved under self-modification iff the policy-reachable family has uniformly bounded capacity (VC/pseudodim). Axis reductions. Architectural and metacognitive edits reduce to induced hypothesis families; substrate changes matter only via the induced family. Hence the boundary depends solely on the supremum capacity of the reachable family. Two-Gate guardrail. computable accept/reject rule (validation improvement by margin τ + capacity cap K(m)) ensures monotone true-risk steps and an oracle inequality at VC rates for the final predictor."
        },
        {
            "title": "2.1 Decision-theoretic self-modification and safety",
            "content": "Godel Machines give proof-based framework for globally optimal self-modification under utility function (Schmidhuber, 2005). Safety analyses document pathologies for self-modifying agents, including reward hacking and self-termination (Orseau and Ring, 2011). Open-ended empirical systems iterate code and toolchain edits with benchmark gains but without proof obligations (Zhang et al., 2025). Proposals for metagoals aim to stabilize or moderate goal evolution during self-change (Goertzel, 2024). While these frameworks establish decision-theoretic foundations, they do not provide learning-theoretic guarantees about post-modification generalization. We study the learning-theoretic state of the agent after such edits. 2.2 Modern mechanisms for self-improvement Contemporary machine learning exhibits constrained forms of self-modification across multiple dimensions. Neural architecture search explores architectural topologies through differentiable, evolutionary, and reinforcement approaches (Liu et al., 2019; Elsken et al., 2019; Zoph and Le, 2017; Real et al., 2019). Automated machine learning systems perform pipeline and hyperparameter search and can trigger optimizer and model-family switches (Hutter et al., 2019; Feurer and Hutter, 2019; Li et al., 2017). Population-based training simultaneously evolves hyperparameters and weights across population of models (Jaderberg et al., 2017). Meta-learning adapts optimizers, initializations, and inductive biases across tasks (Finn et al., 2017; Rajeswaran et al., 2019; Hospedales et al., 2022). Reinforcement learning and multi-armed bandits provide policies for selecting modifications and exploration strategies (Sutton and Barto, 2018; Auer et al., 2002; Lai and Robbins, 1985; Slivkins, 2019). Representation growth through mixture of experts and adapters, and the use of external memory and retrieval, expand the effective function family and computation available at inference (Fedus et al., 2022; Houlsby et al., 2019; Hu et al., 2021; Graves et al., 2014, 2016; Lewis et al., 2020; Schick et al., 2023). Continual learning addresses sequential task acquisition while mitigating catastrophic forgetting (Kirkpatrick et al., 2017; Parisi et al., 2019; Van de Ven and Tolias, 2022). These mechanisms instantiate partial self-modificationadapting specific components while keeping the learning framework itself fixed. In contrast, true self-modifying agents can rewrite any axis of their design. In our framework, these mechanisms traverse representational, architectural, algorithmic, and metacognitive axes, and we establish when such traversals preserve or destroy learnability. 2.3 Learning theory for adaptive systems PAC learning provides distribution-free guarantees under fixed hypothesis class and algorithm (Shalev-Shwartz and Ben-David, 2014; Mohri et al., 2018; Blumer et al., 1989; Vapnik, 1998; Hanneke et al., 2024). Online learning theory establishes regret bounds for adaptive algorithms (ShalevShwartz, 2012; Hazan, 2016; Cesa-Bianchi and Lugosi, 2006), but assumes the learning mechanism 2 remains fixed. Transformation-invariant learners extend instance equivalence while keeping the learning mechanism fixed (Shao et al., 2022). Predictive PAC relaxes data assumptions with fixed learner (Pestov, 2010), and iterative improvement within constrained design spaces admits PAC-style analysis (Attias et al., 2025). Information-theoretic approaches bound generalization for adaptive and meta-learners via mutual information (Jose and Simeone, 2021; Chen et al., 2021; Wen et al., 2025). Stability connects optimization choices to generalization bounds (Bousquet and Elisseeff, 2002; Hardt et al., 2016). All of these results assume architectural invariance: the hypothesis class, update rule, or computational model is fixed ex ante. We remove this assumption and characterize when self-modification preserves PAC learnability."
        },
        {
            "title": "2.4 Computability and the substrate",
            "content": "ChurchTuring-equivalent substrates preserve solvability up to simulation overhead, whereas strictly weaker substrates with finite memory can forfeit learnability of classes that are otherwise PAClearnable; stronger-than-Turing models change the problem class under discussion (Akbari and Harrison-Trainor, 2024). This motivates treating substrate edits separately from architectural or representational changes and clarifies when invariance should be expected."
        },
        {
            "title": "3 Setup and Five-Axis Decomposition",
            "content": "At time the learner state is ℓt = (At, Ht, Zt, Ft, Mt) (cid:124)(cid:123)(cid:122)(cid:125) AHZFM , where (algorithmic), (representational), (architectural), (substrate), and (metacognitive) are the five axes. possibly stochastic modification map Φ : updates the system via ℓt+1 = Φ(ℓt, Dt). For {A, H, Z, F, } with state space , Xt+1 = ΦX (Xt, Dt, θX,t), θX,t ΘX . reasonable utility is computable from finite state and data (for example empirical or validation risk with optional resource or complexity terms) and normalized to [0, 1]. state (and its components) is policy-reachable under if it can be obtained by iterating Φ from ℓ0 using the following decision rule. Decision rule. candidate modification at time is executed iff there is formal proof in the agents current calculus that it yields an immediate utility increase: u(cid:0)Φ(ℓt, Dt), Envt (cid:1) > u(cid:0)ℓt, Envt (cid:1). We make no assumptions about proof-search efficiency; our results concern the post-modification learner. Policy-reachable families (general form). For any axis {A, H, Z, F, }, let Xreach(u) be the set of X-states appearing in policy-reachable trajectories under u. In particular, Xreach(u) = (cid:8) : along policy-reachable trajectory under and Xt = (cid:9). Capacity notion. Losses are bounded in [0, 1]. Our general statements hold for any uniform capacity notion that yields distribution-free uniform convergence (e.g., VC-subgraph or pseudodimension). For concreteness we instantiate to the 01 loss, where capacity reduces to the VC dimension VC(). (All bounds remain valid if VC is replaced by Pdim or VC-subgraph with the usual constants.) 3 Constants and notation. We use universal constant > 0 that may change from line to line and the convention O() to hide polylogarithmic factors in m, nv, 1/δ. Probabilities are over the draws of and unless specified. Axis isolation and substrate scope. Throughout, we analyze one axis at time while holding others fixed. Under ChurchTuring-equivalent substrates , learnability refers to classical (Turingbased) PAC; non-CT cases are treated separately in the substrate section 8. Data-path edits and i.i.d. integrity. Our PAC statements assume and are i.i.d. from and independent of each other. Permitted data-path operations are those that preserve i.i.d. draws (e.g., additional i.i.d. samples, balanced but label-independent subsampling, or predeclared splits). If selection depends on labels or on , standard importance-weighting or covariate-shift corrections must be used; otherwise guarantees may fail. Axes. 1. Algorithmic. Update rules, schedules, stopping, and internal randomness; the hypothesis family is fixed. 2. Representational. Changes to the hypothesis class or encoding, such as feature maps, basis expansions, or unions and refinements. 3. Architectural. Topology and information flow, including wiring, routing, depth or width, and memory addressing. 4. Substrate. Computational model and memory semantics, such as the machine model and memory capacity or discipline. 5. Metacognitive. scheduler that selects and approves modifications on an enabled axis. Why this decomposition matters. The five-axis decomposition provides conceptual toolkit for reasoning about arbitrary self-modification. Any concrete self-improving system can be analyzed by identifying which axes it modifies and tracing the induced changes to the hypothesis family. This has three immediate benefits: (i) it unifies seemingly disparate mechanisms under common analytical framework; (ii) it isolates which modifications actually affect learnability (those that change the reachable hypothesis family) from those that affect only computational efficiency; and (iii) it enables modular safety certificationone can verify capacity bounds axis-by-axis rather than re-analyzing entire systems from scratch. Concretely, these five axes already cover modern practice. Algorithmic edits appear in online hyperparameter and optimizer rewrites (Jaderberg et al., 2017). Representational expansions include mixture of experts and parameter-efficient adapters (Fedus et al., 2022; Houlsby et al., 2019; Hu et al., 2021). Architectural rewiring is explored by neural architecture search (Liu et al., 2019; Elsken et al., 2019). External memory, retrieval, and tool use expand effective computation while keeping computability assumptions unchanged (Graves et al., 2014, 2016; Lewis et al., 2020; Schick et al., 2023). Reflective and self-feedback agents instantiate metacognitive scheduling (Shinn et al., 2023; Madaan et al., 2023; Yao et al., 2023; Wang et al., 2023). As self-modifying capabilities become standard in deployed systems, this framework offers principled way to predict, diagnose, and control when self-improvement preserves or destroys generalization guarantees. Standing Assumptions and Scope. 1. (A1) Data (x, y) i.i.d. from fixed D; training and validation nv are independent. 2. (A2) Loss ℓ [0, 1]. 3. (A3) Capacity is any uniform-convergence notion (VC/pseudodim/VC-subgraph); we instantiate VC where convenient. 4 4. (A4) When computable proxy is used, it upper-bounds capacity: B() cap(). 5. (A5) Substrate semantics. If the substrate is ChurchTuring equivalent, solvability/learnability are measured in the classical (Turing) sense; non-CT substrates (e.g., finite memory or oracular/analog) may alter this and are treated separately in 8. 6. (A6) Axis isolation. We analyze one axis at time while holding the others fixed; multi-axis edits are discussed later. 7. (A7) Compute scope. We study sample complexity (learnability), not runtime, unless limits are intrinsic to ."
        },
        {
            "title": "4 Representational Self-Modification (MH)",
            "content": "Setting (fixed vs. modifiable). We analyze representational edits while holding the algorithmic procedure A, architecture Z, substrate , and metacognitive rule fixed. At time the learner has representation Ht and representational edit is Ht+1 = ΦH (Ht, Dt, θt). Data follow (x, y) i.i.d.; the training set Dm and validation set Dnv are independent. Loss ℓ [0, 1]; risk R(h) = E[ℓ(h(x), y)]; empirical risks (cid:98)RS, (cid:98)RV . Capacity is measured by VC(). utility is reasonable if it is computable, normalized to [0, 1], and (U1) non-decreasing in empirical fit on the active finite evidence, and (U2) strictly increasing in computable capacity bonus g(VC) with > 0. The decision rule executes an edit only when an immediate utility increase is formally provable. Proofs are deferred to Appendix 12. Reference family. We work with fixed capped reference family GK(m) (defined in 3), satisfying VC(GK(m)) K(m) and fixed ex ante before seeing . Policy-reachable family. For fixed u, Hreach(u) = { : along policy-reachable trajectory under with Ht = }. Unbounded representational power (URP). H, θ, with (H, ΦH ) has URP if for every there exist VC(cid:0)ΦH (H, D, θ)(cid:1) m. Local-URP : for every there exists an edit with VC(ΦH (H, , )) VC(H) + 1 that can fit the current finite sample. Policy-level learnability boundary Under (A1A7), distribution-free learnability is preserved under representational self-modification iff sup Hreach(u) VC(H ) < . Sketch. () uniform capacity cap gives uniform convergence on fixed capped reference family for all steps; ERM/AERM yields the standard VC rate for the terminal predictor. () If capacities along reachable subsequence diverge, VC lower bounds preclude any distribution-free sample complexity. (Full proof in Appendix 12.) 5 Figure 1: Representational axis MH . TwoGate accepts few early edits and then plateaus at lower test loss, while destructive policies continue modifying the hypothesis class and exhibit worsening generalization as complexity increases. Two-Gate finite-sample safety Given (S = m) and independent (V = nv), candidate edit producing Hnew is accepted only if (Validation) (cid:98)RV (hnew) (cid:98)RV (hold) (2εV + τ ), (Capacity) hnew GK(m) with VC(GK(m)) K(m), where εV is chosen so that, with probability 1 δV over , (cid:12)R(h) (cid:98)RV (h)(cid:12) (cid:12) sup hGK(m) (εV (cid:112)(K(m) + log(1/δV ))/nv). (cid:12) εV Then with probability at least 1 δV δ over draws of and S: (i) each accepted edit decreases true risk by at least τ ; and R(hT ) inf hGK(m) R(h) + (cid:16)(cid:112)(K(m) + log(1/δ))/m (cid:17) . Validation reuse (fixed ex ante). The same validation set may be reused adaptively across many edits provided the capped reference family GK(m) is fixed before seeing and the gate thresholds K(m), εV , τ do not depend on . If any of these are tuned using , fresh split or reusable holdout is required (see Appendix 12). Probability bookkeeping. All oracle inequalities are stated on the intersection of two events: (E1) the uniform validation event on GK(m) (probability 1 δV ) and (E2) the training-side uniform convergence event (probability 1 δ). By union bound, the final probability is 1 δV δ and does not depend on the number of accepted edits, since the bound is uniform over the fixed capped family. 6 Remark. Under URP, utilities that reward empirical fit and even slight increase in capacity can drive VC unbounded and destroy distribution-free learnability; see Appendix 12."
        },
        {
            "title": "5 Architectural Self-Modification (MZ)",
            "content": "Setting and reduction (fixed vs. modifiable). We analyze architectural edits while holding the learning algorithm A, substrate , and metacognitive rule fixed. An architecture induces hypothesis class H(Z) . At time t, an architectural edit produces Zt+1 = ΦZ(Zt, Dt, ϑt), hS(Zt+1) argmin hH(Zt+1) (cid:98)RS(h) (ERM in the induced class). Fix reasonable utility u. Let the policy-reachable architectures and induced classes be Zreach(u) = (cid:8) : on some proof-triggered trajectory from Z0 under with Zt = (cid:9), HZ reach(u) = { H(Z) : Zreach(u) }. Utility realism. The boundary and Two-Gate guarantees depend only on the capacity of the reachable family, not on explicit capacity rewards: even if has no bonus term, any policy that permits capacity-increasing edits can cross the boundary unless cap (e.g., via K(m)) is enforced. The stronger bonus variant is used only for the destruction theorems. Every run of MZ therefore corresponds to run of MH over the induced family HZ reach(u). Architectural representational reduction For any reasonable u, every proof-triggered induces representational trajectory H(Z0) H(Z1) over the trajectory Z0 Z1 fixed reference family G, with ERM/AERM inside each accepted H(Zt). Consequently HZ reach(u) = {H(Z) : Zreach(u)} G. Proof sketch. The decision semantics and utility are unchanged by renaming states from to the induced H(Z). Architectural boundary via induced policy-reachable family For any reasonable u, distribution-free PAC learnability under architectural self-modification is preserved iff sup ZZreach(u) VC(cid:0)H(Z)(cid:1) < . Equivalently, preservation holds iff sup reach(u) HZ VC(H ) K. Proof. Immediate by reduction to 4: apply the sharp boundary theorem for representation (Thm. 4) to the induced set HZ reach(u). Reference family and proxy-cap subfamily. Fix single parameterized super-family that contains every induced class: H(Z) for all Z. For define the proxy-cap subfamily Gproxy := { : with B(Z) and H(Z) }, where B(Z) is computable architectural capacity proxy satisfying VC(H(Z)) B(Z) (e.g., B(Z) = (Z) log (Z) for ReLU with parameters). Since each accepted H(Z) is subset of the fixed reference subfamily Gproxy ) K. No additional closure properties are required. , capacity is bounded by VC(Gproxy 7 K(m) with K(m)) K(m), and the validation gate enforce (cid:98)RV (hS(Znew)) (cid:98)RV (hS(Zold)) (2εV + τ ), Two-gate safety for architecture Let the capacity gate enforce H(Znew) Gproxy VC(Gproxy where εV is chosen by VC bound on Gproxy K(m). Then (i)(ii) hold as stated (by Thm. 4). K(m), K(m), and thresholds ex ante (before seeing ). If any choice is tuned on , we use fresh split or reusable-holdout mechanism; all theorems apply to the fixed family (Appendix 13.3). Validation reuse. We fix Gproxy Local architectural edits (Local-URPZ). We say MZ has Local-URPZ if for every there exists computable edit (ϑ, D) such that VC(cid:0)H(ΦZ(Z, D, ϑ))(cid:1) VC(H(Z)) + 1 and the new class H(ΦZ(Z, D, ϑ)) can interpolate the current finite evidence (e.g., fit S). Robust destruction under Local-URPZ Assume Local-URPZ. For any reasonable utility (non-decreasing in empirical fit on the active evidence and strictly increasing in computable capacity bonus g(VC)), there exist distribution and sample size such that the proof-trigger repeatedly accepts local architectural edits that increase capacity, the induced reachable set HZ reach(u) has unbounded VC, and distribution-free PAC learnability fails. Sketch. Each local step strictly increases (fit non-worse + increases), so the proof-trigger fires. Iteration yields unbounded VC; apply the necessity direction of Thm. 5. Proxy-cap sufficiency (computable architectural bounds). Often one has computable upper bound B(Z) on VC(H(Z)) (e.g., for ReLU networks B(Z) = (Z) log (Z) with (Z) parameters). for all and the capacity Proxy-cap two-gate guarantee If B(Z) VC(H(Z)) gate enforces B(Znew) K(m), then Corollary 5 holds with GK(m) = { : realizable by some with B(Z) K(m) } and with the same rate (the VC bound is applied to GK(m)). Proof. Since H(Znew) GK(m) and VC(GK(m)) K(m) by construction, the two-gate proof (Appendix 12) applies verbatim."
        },
        {
            "title": "6 Metacognitive Self-Modification (MM )",
            "content": "Setting (fixed vs. modifiable). We analyze metacognitive scheduling/filters while holding representation H, architecture Z, algorithm A, and substrate fixed; selects which edits to evaluate and applies acceptance/rejection using finite evidence. Metacognitive scheduler/filter. metacognitive rule can (i) choose which candidate edit to evaluate, (ii) when/how often to evaluate (scheduling), and (iii) randomize; it then accepts/rejects using only finite evidence (e.g., S, , capacity proxy, edit cost). Let HM,H reach(u) := (cid:110) : Pr (cid:2)t on proof-triggered trajectory from H0 under filtered by with Ht = (cid:3) > 0 (cid:111) . This is the M-filtered policy-reachable family. (Randomized is allowed; all guarantees below hold almost surely over internal randomness.) Boundary under metacognitive modifications. For any reasonable and metacognitive filter/scheduler , distribution-free PAC learnability is preserved iff sup HM,H reach(u) VC(H ) < . Proof sketch. Apply the representational sharp boundary to the -filtered family HM,H follows by the same destruction argument when the supremum is unbounded (Appendix 12). reach(u); necessity 8 Two-Gate as metacognition (safety + rate). If accepts only when (i) (cid:98)RV (ht+1) (cid:98)RV (ht) (2εV + τ ) and (ii) VC(Ht+1) K(m), then with probability 1 δV δ each accepted edit reduces true risk by τ , and R(hT ) inf hGK(m) by Thm. 4. R(h) + (cid:16)(cid:113) K(m)+log(1/δ) (cid:17) , Restorative metacognition (turning destructive utilities safe). Suppose the unfiltered reachable family satisfies supH Hreach(u) VC(H ) = (destructive). There exists metacognitive rule (e.g., Two-Gate with any computable nondecreasing schedule K(m)) such that supH HM,H reach(u) VC(H ) < , hence learnability is preserved. Proof sketch. simply rejects any proposal that violates the validation margin or the cap; the filtered family is contained in GK(m) (Appendix 12) and the boundary theorem applies. Edit efficiency under metacognitive margins. Under the Two-Gate rule with margin τ > 0, along any accepted trajectory #{accepted edits} R(h0) τ where := inf , R(h). hGK(m) Proof. Each accepted edit decreases true risk by at least τ ; telescope from R(h0) to the floor R. Scheduling and randomness. may be deterministic or randomized and may schedule when candidate edits are proposed. Because our deviation bound is uniform over the capped family, the safety and rate guarantees hold for every realized trajectory filtered by ; scheduling affects efficiency, not the boundary itself."
        },
        {
            "title": "7 Algorithmic Self-Modification (MA)",
            "content": "Setting (fixed vs. modifiable). We analyze algorithmic self-modification while holding the hypothesis class H, architecture Z, substrate , and metacognitive rule fixed. The learner may change its learning algorithm or schedule (optimizer, step sizes, noise level, stopping rule, etc.), producing an updated training procedure At+1 = ΦA(At, Dt, ϑt). Given training set of size m, the output predictor is ˆh = Alg(A, S, H) H. Takeaways. Algorithmic edits cannot cure infinite capacity; on finite capacity, ERM/AERM preserves PAC; and simple stability meta-policy (cap the step-mass (cid:80) ηt) controls the generalization gap during algorithmic self-modification. No algorithmic cure for infinite VC If VC(H) = , then no distribution-free PAC guarantee is possible for any algorithmic procedure; in particular, algorithmic self-modification cannot restore distribution-free learnability. Finite-VC is sufficient with ERM/AERM. If VC(H) < and the (possibly self-modified) training procedure is ERM/AERM over H, then for any δ (0, 1), with probability 1 δ over Dm, R(ˆh) inf hH R(h) + (cid:16)(cid:113) K+log(1/δ) (cid:17) . Stability meta-policy (step-mass cap). Assume bounded, Lipschitz, β-smooth losses (formalized in Appendix 14), and that training examples are sampled uniformly from during updates. Let self-modified training run on use (projected) SGD-like updates with step sizes {ηt}T t=1. Define the step-mass MT := (cid:80)T t=1 ηt. 9 Figure 2: MA (Algorithmic axis). Generalization gap (testtrain loss) vs. cumulative stepmass MT = (cid:80) ηt with fixed hypothesis class. TwoGate halts updates at preset budget B(m) and keeps the gap small; Destructive continues updating and exhibits larger, persistent gap. If self-modification preserves When algorithm changes invalidate ERM assumptions. ERM/AERM over the fixed H, Prop. 7 applies directly. If not, we control generalization via algorithmic stability: under standard smooth/Lipschitz conditions, the expected generalization gap scales as O(cid:0)((cid:80) ηt)/m(cid:1); see Thm. 7. Algorithmic stability via step-mass Under the conditions above, there exists constant > 0 (problem dependent, independent of m) such that E(cid:2)R(ˆh) (cid:98)RS(ˆh)(cid:3) T (cid:88) t=1 ηt = MT . In particular, metacognitive rule that caps MT B(m) guarantees E[gap] = O(B(m)/m); choosing B(m) = O(1) yields O(1/m) gap. Discussion. Prop. 7 says capacity, not optimizer choice, governs distribution-free learnability. Prop. 7 ensures algorithmic edits that continue to output ERM/AERM do not harm PAC guarantees when VC(H) < . Thm. 7 offers simple, actionable meta-policy: cap cumulative step-mass to keep the generalization gap small during algorithmic self-modification. Full proofs are in Appendix 14."
        },
        {
            "title": "8 Substrate Self-Modification (MF )",
            "content": "Setting (fixed vs. modifiable). We analyze substrate edits while holding the specification of H, Z, and fixed; switching substrates changes how these are executed (time/space) but not which hypotheses are definable nor which utilities are expressible. 10 Setting. The learner runs on computational substrate (hardware/VM). substrate edit is Ft+1 = ΦF (Ft, Dt, φt). Unless otherwise stated, the representation H, architecture Z, and algorithmic procedure are held fixed as specifications; switching substrates may change how they are executed (time/space), but not which hypotheses are definable nor which utilities are expressible. PAC learnability is classical (i.i.d., 01 loss unless noted). Takeaways. (i) Switching among ChurchTuring equivalent substrates preserves classical PAC learnability (CT-invariance). (ii) Downgrading to strictly weaker substrate (e.g., finite-state) can destroy PAC learnability even for problems learnable pre-switch. (iii) Beyond-CT substrates do not alter classical PAC guarantees unless they enlarge the induced hypothesis family; if they do, our policy-reachable boundary from 4 applies to the enlarged family. CT-invariance of PAC learnability If and are ChurchTuring equivalent, then problem that is distribution-freely PAC-learnable when run on remains distribution-freely PAC-learnable when run on , with the same sample complexity up to constant factors (computation may differ). Finite-state downgrade can destroy learnability There exists binary classification problem that is PAC-learnable on Turing-equivalent substrate but becomes not distribution-freely PAClearnable after switching to fixed finite-state substrate (bounded persistent memory), even when has finite VC dimension as specification. If stronger-than-Turing substrate does not enlarge the induced Beyond-CT substrates hypothesis family (the measurable set of predictors under consideration), classical PAC learnability is unchanged. If it does enlarge the effective family to , the learnability boundary is governed by supH reach(u) VC(H ) exactly as in 4. Discussion. Thm. 8 elevates substrate choice out of the classical PAC calculus: CT-equivalent machines only affect compute, not sample complexity. Prop. 8 formalizes the simpleton downgrading intuition: collapsing persistent memory imposes an information bottleneck that breaks distributionfree guarantees. Prop. 8 shows that any real change to learnability arises only through the induced hypothesis family; our policy-reachable boundary applies verbatim once that family changes. Full proofs appear in Appendix 15."
        },
        {
            "title": "9 Outlook",
            "content": "From theory to practice: Why capacity bounds matter. Practitioners may object that modern deep learning routinely violates PAC sample complexity through implicit regularizationwhy would capacity bounds matter for self-modifying systems? The answer is sequential compounding risk. single overparameterized model may generalize well, but self-modifying agent that repeatedly expands capacity across hundreds of edits accumulates risk that no implicit bias can rescue. Our experiments demonstrate this concretely: the destructive policy reaches test loss 0.409 after unbounded capacity growth, while the capacity-capped policy achieves 0.350a 17% relative improvement emerging not from single edit, but from accumulated drift as the reachable family explodes. The Two-Gate policy translates directly into practice: (i) track capacity proxy B() (parameter count for neural architectures, corpus size for retrieval systems, function-library size for tool-using agents); (ii) proportional to available training data; (iii) reject edits unless validation set schedule K(m) improves by margin τ . These checks are computationally cheap and can run in real-time during self-improvement loops. The alternative to capacity bounds is not trust implicit regularizationit is accepting that your system has entered regime where no learning guarantee is possible. Multi-axis modification: The realistic frontier. Real autonomous agents will simultaneously rewrite architectures, swap optimizers, expand tool libraries, and adjust metacognitive policiesnot modify single axis in isolation. Our framework extends naturally: all axes reduce to the same 11 boundary condition on sup VC(Hreach). When an agent modifies multiple axes, the induced hypothesis family is determined by the composition of editse.g., architecture combined with representation induces (Z ), and learnability requires the joint trajectory remain capacity-bounded. This has three practical implications. First, capacity bounds must be enforced globally, not per-axisa small architectural change combined with representational expansion can yield multiplicative VC growth. Second, axis interactions create emergent capacity explosions that independent per-axis budgets cannot prevent. Third, metacognitive policies become essential: in multi-axis settings, compounding accelerates capacity growth exponentially, making global capacity monitoring the only known mechanism for ex ante safety. The key open challenge is developing compositional capacity proxies that tractably upper-bound VC(H (Z ) tools) for complex compositionsthe gap between computable bounds B() and true VC dimension determines how conservative the Two-Gate policy must be. Towards sustainable self-improvement. The AutoML and NAS communities have achieved remarkable success by treating architecture search as unconstrained optimization. Our results suggest paradigm shift: future self-modifying systems should ask not what maximizes validation accuracy? but what maximizes accuracy subject to capacity remaining PAC-learnable for available data? This constraint does not eliminate innovationit channels self-improvement toward sustainable compounding of gains rather than compounding of risk. For open-ended agents operating over months or years, the capacity schedule K(m) can grow with accumulating data m, enabling unbounded absolute improvement while maintaining learnability. The failure mode is not self-improvement itself, but uncontrolled self-improvement that outruns the data. Without capacity bounds, seemingly rational modifications can lock in poor performance irreversiblyonce an agent crosses into the unbounded regime, no sample complexity guarantees recovery. For high-stakes deployments (medical AI, autonomous vehicles, financial systems), the choice is between principled capacity-aware selfmodification and accepting that you have no basis for trust."
        },
        {
            "title": "10 Conclusion",
            "content": "We have established sharp learnability boundary for self-modifying agents: distribution-free PAC guarantees are preserved if and only if the policy-reachable hypothesis family has uniformly bounded capacity. This result unifies representational, architectural, algorithmic, metacognitive, and substrate modifications under single criterionthe supremum VC dimension of states reachable under the agents utility function. The Two-Gate policy provides computable guardrail that enforces this boundary through validation margins and capacity caps, yielding oracle inequalities at standard VC rates. Our framework reveals that seemingly rational self-modifications can irreversibly destroy learnability when capacity grows without bound, even as they improve immediate performance. As AI systems gain the capability to rewrite their own learning mechanisms, the choice is between principled capacity-aware self-improvement that preserves generalization guarantees, or unconstrained optimization that enters regime where no learning theory can provide safety assurances. The path to safe open-ended autonomy requires recognizing that self-modification is not bug to eliminate, but capability to controland capacity bounds are the control mechanism that learning theory provides. References Akbari, S. and Harrison-Trainor, M. (2024). Computable learning of natural hypothesis classes. Attias, I., Blum, A., Saless, D., Naggita, K., Sharma, D., and Walter, M. (2025). PAC learning with improvements. Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235256. Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1989). Learnability and the vapnikchervonenkis dimension. Journal of the ACM, 36(4):929965. 12 Bousquet, O. and Elisseeff, A. (2002). Stability and generalization. Journal of Machine Learning Research, 2:499526. Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning, and Games. Cambridge University Press. Chen, Q., Shui, C., and Marchand, M. (2021). Generalization bounds for meta-learning: An information-theoretic analysis. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 2587825890. Elsken, T., Metzen, J. H., and Hutter, F. (2019). Neural architecture search: survey. Journal of Machine Learning Research, 20(55):121. Fedus, W., Zoph, B., and Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Feurer, M. and Hutter, F. (2019). Automl: survey of the state of the art. In Automated Machine Learning, pages 333. Springer. Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning, pages 11261135. Goertzel, B. (2024). Metagoals: Endowing self-modifying AGI systems with goal stability or moderated goal evolution: Toward formally sound and practical approach. Graves, A., Wayne, G., and Danihelka, I. (2014). Neural turing machines. Graves, A., Wayne, G., Reynolds, M., et al. (2016). Hybrid computing using neural network with dynamic external memory. Nature, 538(7626):471476. Hanneke, S., Larsen, K. G., and Zhivotovskiy, N. (2024). Revisiting agnostic PAC learning. Hardt, M., Recht, B., and Singer, Y. (2016). Train faster, generalize better: Stability of stochastic gradient descent. In International Conference on Machine Learning (ICML), pages 12251234. Hazan, E. (2016). Introduction to Online Convex Optimization, volume 2. Now Publishers. Hospedales, T. M., Antoniou, A., Micaelli, P., and Storkey, A. (2022). Meta-learning in neural networks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):5149 5169. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019). Parameter-efficient transfer learning for NLP. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2021). LoRA: Low-rank adaptation of large language models. Hutter, F., Kotthoff, L., and Vanschoren, J. (2019). Automated Machine Learning: Methods, Systems, Challenges. Springer. Jaderberg, M., Dalibard, V., Osindero, S., et al. (2017). Population based training of neural networks. Jose, S. T. and Simeone, O. (2021). Information-theoretic generalization bounds for meta-learning and applications. Entropy, 23(1):126. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et al. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):35213526. Lai, T. L. and Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):422. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., et al. (2020). Retrieval-augmented generation for knowledge-intensive NLP. In Advances in Neural Information Processing Systems (NeurIPS). 13 Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. (2017). Hyperband: novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):67656816. Liu, H., Simonyan, K., and Yang, Y. (2019). DARTS: Differentiable architecture search. In International Conference on Learning Representations (ICLR). Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., et al. (2023). Self-refine: Iterative refinement with self-feedback. Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2018). Foundations of Machine Learning. MIT Press, 2nd edition. Orseau, L. and Ring, M. (2011). Self-modification and mortality in artificial agents. In Artificial General Intelligence (AGI 2011), pages 110. Springer. Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter, S. (2019). Continual lifelong learning with neural networks: review. Neural Networks, 113:5471. Pestov, V. (2010). Predictive PAC learnability: paradigm for learning from exchangeable input data. Rajeswaran, A., Finn, C., Kakade, S., and Levine, S. (2019). Meta-learning with implicit gradients. In NeurIPS. Real, E., Aggarwal, A., Huang, Y., and Le, Q. V. (2019). Regularized evolution for image classifier architecture search. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 47804789. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. Schmidhuber, J. (2005). Godel machines: Fully self-referential optimal universal self-improvers. arXiv:cs/0309048. Shalev-Shwartz, S. (2012). Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107194. Shalev-Shwartz, S. and Ben-David, S. (2014). Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press. Shao, H., Montasser, O., and Blum, A. (2022). theory of PAC learnability under transformation invariances. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 3190431917. Shinn, N., Cassano, F., Labash, A., Gopinath, D., et al. (2023). Reflexion: Language agents with verbal reinforcement learning. Slivkins, A. (2019). Introduction to multi-armed bandits. Foundations and Trends in Machine Learning. Sutton, R. S. and Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press, 2 edition. Van de Ven, G. M. and Tolias, A. S. (2022). Three types of incremental learning. Nature Machine Intelligence, 4(12):11851197. Vapnik, V. N. (1998). Statistical Learning Theory. Wiley. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. (2023). Voyager: An open-ended embodied agent with large language models. Wen, W., Gong, T., Dong, Y., Zhang, W., and Liu, Y.-J. (2025). Towards sharper informationtheoretic generalization bounds for meta-learning. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. (2023). Tree of thoughts: Deliberate problem solving with large language models. 14 Zhang, J., Hu, S., Lu, C., Lange, R., and Clune, J. (2025). Darwin godel machine: Open-ended evolution of self-improving agents. Zoph, B. and Le, Q. V. (2017). Neural architecture search with reinforcement learning. In International Conference on Learning Representations."
        },
        {
            "title": "Hypothesis class induced by architecture Z",
            "content": "H(Z) Hreach(u) Policy-reachable hypothesis family under utility B() K(m) GK(m) τ (cid:98)RS, (cid:98)RV Computable capacity proxy (upper-bounds VC/pseudodim) Nondecreasing capacity cap schedule at sample size Reference subfamily with cap K(m) Validation margin in Two-Gate Empirical risks on train and validation inf hGK(m) R(h) Table 1: Notation."
        },
        {
            "title": "12 Full Proofs for Representational Self-Modification",
            "content": "Data, loss, risks. Samples (x, y) i.i.d. Training Dm and validation Dnv are independent. Loss ℓ [0, 1]; true risk R(h) = E(x,y)D[ℓ(h(x), y)]. Empirical risks are (cid:98)RS and (cid:98)RV on and . Representational edits and policies. At time the representation is hypothesis class Ht . representational edit is Ht+1 = ΦH (Ht, Dt, θt), where Dt is finite evidence (e.g., S, summary stats) and θt are edit parameters. Within any accepted class H, the learner outputs an ERM (or AERM) on S: hS(H) arg min hH (cid:98)RS(h). The decision rule executes an edit only when an immediate utility increase is formally provable from the finite evidence. Reasonable utilities. utility is reasonable if it is (i) computable from finite state/evidence and (ii) satisfies: (U1) Non-decreasing in empirical fit on the active finite evidence (e.g., 1 (cid:98)RS). (U2) Adds strictly increasing capacity bonus g(VC(H)) with g(k) > 0 for all k. We normalize [0, 1] WLOG. Policy-reachable family. Fix u. Let Hreach(u) be the set of classes for which there exists time on some proof-triggered trajectory from H0 under with Ht = . URP and Local-URP. The pair (H, ΦH ) has URP if for every there exist (H, θ, D) with VC(ΦH (H, D, θ)) m. It has Local-URP if for every there is an edit (θ, D) such that VC(ΦH (H, D, θ)) VC(H) + 1 and the new class can interpolate the current finite evidence (e.g., fit S). 16 Single capped reference family (indexing control). For each let GK be reference family with VC(GK) and assume the capacity gate (defined below) guarantees all accepted satisfy GK. This avoids pathologies from taking unions over arbitrarily many distinct classes with the same VC cap. VC uniform convergence. There exists universal constant > 0 such that for any class with VC(G) and any δ (0, 1), with probability 1 δ over sample of size n, (cid:12)R(h) (cid:98)R(h)(cid:12) (cid:12) (cid:12) sup hG (cid:114) + log(1/δ) . (1) We hide polylogarithmic factors in O()."
        },
        {
            "title": "12.1 Sharp policy-level boundary",
            "content": "Theorem (Sharp boundary; restated from Thm. 4). For any reasonable u, distribution-free PAC learnability is preserved under representational self-modification iff there exists < such that sup Hreach(u) VC(H ) K. Proof (sufficiency). Fix and assume supH Hreach(u) VC(H ) K. Along any policy-reachable run, all classes satisfy Ht GK with VC(GK) K. By (1) applied to GK and ERM in Ht, R(cid:0)hS(Ht)(cid:1) inf hHt R(h) + (cid:16)(cid:113) K+log(1/δ) (cid:17) uniformly for all t, with probability 1 δ over S. In particular the terminal predictor hT obeys the same bound, so = O(cid:0)(K + log(1/δ))/ϵ2(cid:1) suffices for (ϵ, δ)-accuracy. If supH Hreach(u) VC(H ) = , then for each there exists reachable (k) Proof (necessity). with VC(H (k)) k. Classical VC lower bounds imply any distribution-free learner needs = Ω(k/ϵ) samples for (ϵ, δ)-accuracy (even realizable). Since is unbounded along reachable trajectories, no uniform PAC guarantee exists. 12.2 Finite-sample safety of the two-gate policy Two gates. Given train (S = m) and independent validation (V = nv), accept an edit only if: (Validation) (cid:98)RV (hnew) (cid:98)RV (hold) (2εV + τ ), (Capacity) Hnew GK(m) with VC(GK(m)) K(m), where K() is nondecreasing, τ 0 is margin, and εV is chosen so that with probability 1 δV over , (cid:12) (cid:12)R(h) (cid:98)RV (h)(cid:12) (cid:12) εV (e.g., εV (cid:112)(K(m) + log(1/δV ))/nv by (1)). sup hGK(m) Theorem (Two-gate finite-sample safety; restated from Thm. 4). With probability 1 δV δ over (V, S): (i) each accepted edit decreases true risk by at least τ (monotone steps); and (ii) the terminal predictor hT satisfies the oracle inequality R(h) + (cid:16)(cid:113) K(m)+log(1/δ) (cid:17) . R(hT ) inf hGK(m) Proof. (i) On the event suphGK(m) R(h) (cid:98)RV (h) εV , R(hnew) (cid:98)RV (hnew) + εV (cid:98)RV (hold) (2εV + τ ) + εV R(hold) τ. (ii) By the capacity gate, hT GK(m). Apply (1) to GK(m) on and ERM in HT to obtain R(hT ) inf hGK(m) R(h) + (cid:16)(cid:113) K(m)+log(1/δ) (cid:17) with probability 1 δ. Union bound with the validation event yields the stated probability."
        },
        {
            "title": "12.3 Destruction under (local) URP",
            "content": "Theorem (Existential destruction under URP). Assume URP. There exists reasonable utility and problem that is PAC-learnable in the baseline class such that the proof-triggered policy executes representational edits that render the problem distribution-free unlearnable after modification. Proof. Let be any finite-VC concept class (PAC-learnable without modification). Define computable utility = α (1 (cid:98)RS(h)) + β g(VC(H)) with α, β > 0 and strictly increasing g. By URP, for the realized there exists an edit to with VC(H ) that interpolates (shatters S). Then strictly increases (perfect fit plus larger capacity bonus), which is provable from finite evidence; the proof-trigger executes the edit. With VC(H ) and only samples, standard VC lower bounds show distribution-free PAC learnability fails. Theorem (Utility-class robust destruction under Local-URP). Assume Local-URP. Then for any reasonable utility (satisfying (U1)(U2)), there exist distribution and sample size such that the proof-trigger repeatedly accepts capacity-increasing local edits, the policy-reachable VC is unbounded, and distribution-free PAC learnability fails. Proof. Local-URP ensures at each step computable edit with VC increase by at least 1 that preserves or improves empirical fit on the active evidence. By (U1)(U2), the utility strictly increases at each such step, hence the proof-trigger fires. Iteration yields unbounded policy-reachable VC, so by the necessity part of the boundary theorem learnability cannot be guaranteed distribution-free."
        },
        {
            "title": "13 Full Proofs for Architectural Self-Modification",
            "content": "Assumptions & tools (recap). Data (x, y) i.i.d.; training Dm and validation Dnv are independent. Loss ℓ [0, 1]; true risk R(h) = E[ℓ(h(x), y)]; empirical risks (cid:98)RS, (cid:98)RV on S, . Within any accepted class H, the learner outputs ERM hS(H) arg minhH (cid:98)RS(h). utility is reasonable if it is computable from finite evidence and satisfies: (U1) non-decreasing in empirical fit on the active evidence (e.g., 1 (cid:98)RS), and (U2) strictly increasing in computable capacity bonus g(VC(H)) with g(k) > 0. We use the standard VC uniform-convergence bound: for class with VC(G) and any δ (0, 1), (cid:12) (cid:12)R(h) (cid:98)R(h)(cid:12) (cid:12) (cid:113) K+log(1/δ) sup hG (with universal constant > 0). (2) Logarithmic factors are absorbed in O() as needed. Architectures induce classes and reference family. An architecture induces hypothesis class H(Z) . We assume single reference family such that H(Z) for all Z. For each N, let GK be subfamily with VC(GK) K. (Concrete choices include the realizable functions of supernet, and derived from an architectural proxy such as parameter count; see Prop. 13.3.) 18 Policy reachability in architecture. Fix reasonable utility and the proof-triggered decision rule. Define Zreach(u) = : on some proof-triggered trajectory from Z0 under with Zt = (cid:111) (cid:110) , and the induced family HZ reach(u) = (cid:8) H(Z) : Zreach(u) (cid:9) G."
        },
        {
            "title": "13.1 Reduction lemma: MZ → MH",
            "content": "Lemma (Architectural-to-representational reduction). Fix reasonable utility u. Any proof-triggered trajectory Z0 Z1 in MZ induces trajectory H(Z0) H(Z1) in MH over the family HZ reach(u), with predictors hS(Zt) arg minhH(Zt) (cid:98)RS(h) (ERM). Proof. By definition, at time the available hypotheses are exactly H(Zt). The learner outputs ERM within H(Zt). The proof-triggered rule accepts an edit iff there exists formal proof (from finite evidence) that increases. Because and the decision semantics are identical whether we name the state by Zt or by H(Zt), each accepted architectural edit corresponds to an accepted representational edit on the induced class, and vice versa. Thus the reachable set under maps to HZ reach(u), and the architectural run induces representational run along the mapped classes. Corollary (Architectural boundary by reduction). For any reasonable u, distribution-free PAC learnability under architectural self-modification is preserved iff supZZreach(u) VC(H(Z)) < . Proof. Apply the sharp boundary theorem for representation (Appendix 12.1) to the induced set HZ reach(u) using Lemma 13.1. See also the main-text statement Thm. 5. 13.2 Local architectural edits and robust destruction Local-URPZ. We say MZ has Local-URPZ if for every there exists computable edit (ϑ, D) such that: (i) VC(cid:0)H(ΦZ(Z, D, ϑ))(cid:1) VC(H(Z)) + 1; and (ii) the new class H(ΦZ(Z, D, ϑ)) can interpolate the active finite evidence (e.g., fit S). Theorem (Robust destruction under Local-URPZ). Assume Local-URPZ. For any reasonable utility satisfying (U1)(U2), there exist distribution and sample size such that the prooftrigger repeatedly accepts local architectural edits that increase capacity, the induced reachable set HZ reach(u) has unbounded VC, and distribution-free PAC learnability fails. Proof. Fix u. From any Zt, Local-URPZ guarantees computable edit (ϑt, Dt) producing Zt+1 with VC(H(Zt+1)) VC(H(Zt)) + 1 and perfect fit to the active finite evidence (e.g., S). By (U1) the empirical fit term in is non-worse; by (U2) the capacity bonus g(VC) strictly increases; the net increase in is computable fact from finite evidence. Therefore the proof-trigger fires and the edit is accepted. Iterating yields reachable sequence with unbounded VC(H(Zt)). Hence reach(u) VC(H ) = , and by the necessity direction of the sharp boundary (Appendix 12.1; supH HZ cf. Thm. 4) distribution-free PAC learnability cannot be guaranteed. 13.3 Proxy-capacity two-gate guarantee Computable architectural upper bounds. Suppose there exists computable function : with VC(cid:0)H(Z)(cid:1) B(Z) for all Z. (3) (Examples: for ReLU networks, B(Z) = (Z) log (Z) with (Z) the parameter count.) Proxy-capped reference subfamily. For define (cid:110) := : with B(Z) and H(Z) (cid:111) . Gproxy By (3), VC(cid:0)Gproxy (cid:1) K. 19 Proposition (Proxy-cap two-gate oracle inequality). Assume the two-gate policy with capacity gate B(Znew) K(m) and validation gate (cid:98)RV (cid:0)hS(Znew)(cid:1) (cid:98)RV (cid:0)hS(Zold)(cid:1) (2εV + τ ), where εV is chosen so that suphGproxy Then with probability 1 δV δ over (V, S): K(m) R(h) (cid:98)RV (h) εV with probability 1 δV (e.g., by (2)). 1. (Monotone steps) Each accepted architectural edit satisfies R(cid:0)hS(Znew)(cid:1) R(cid:0)hS(Zold)(cid:1) τ . 2. (Oracle inequality) The terminal predictor hS(ZT ) obeys R(cid:0)hS(ZT )(cid:1) inf hGproxy K(m) R(h) + (cid:16)(cid:113) K(m)+log(1/δ) (cid:17) . Proof. By the capacity gate and the definition of Gproxy K(m), every accepted class H(Znew) is subset of Gproxy K(m) with VC bounded by K(m). The monotone-step claim follows exactly as in the representational two-gate proof: on the high-probability validation event, R(hnew) (cid:98)RV (hnew) + εV (cid:98)RV (hold) (2εV + τ ) + εV R(hold) τ. For the oracle inequality, apply (2) on to the capped family Gproxy K(m) and use ERM in the final accepted class. Union bound the training and validation events to obtain probability 1 δV δ. Remark (using fixed reference subfamily). If the capacity gate is stated directly as H(Znew) GK(m) with VC(GK(m)) K(m), then Proposition 13.3 holds verbatim with Gproxy K(m) replaced by GK(m). 13.4 Pointers back to representational results Lemma 13.1 allows Theorem 5 in the main text to be proved by direct appeal to the representational sharp boundary (Appendix 12.1; cf. Thm. 4). Corollary 5 (main text) is an instantiation of the two-gate safety theorem (Appendix 12.2) with the architectural capacity gate H(Z) GK(m)."
        },
        {
            "title": "14 Full Proofs for Algorithmic Self-Modification",
            "content": "Assumptions. Data (x, y) i.i.d.; Dm. Loss ℓ(; z) is bounded in [0, 1], L-Lipschitz in the parameter θ (with respect to norm ), and β-smooth. Gradients are bounded θℓ(θ; z) G, or we use projection onto bounded domain of diameter so iterates remain bounded. The hypothesis class = {x (cid:55) fθ(x) : θ Θ} is fixed throughout the algorithmic edits. When we invoke ERM/AERM, ˆh arg minhH (cid:98)RS(h) (or an approximate minimizer). 14.1 No algorithmic cure for infinite VC If VC(H) = , classical VC lower bounds imply that for any learning algorithm (possibly randomized), there exist distributions for which, at any sample size m, the algorithm fails to achieve universal (ϵ, δ) guarantee (even in the realizable case). Algorithmic self-modification selects among training procedures but does not change H, hence does not change the lower bound. Therefore no distributionfree PAC guarantee is possible. 14.2 ERM/AERM on finite VC Assume VC(H) < . Standard uniform-convergence bounds give, with probability 1 δ, (cid:12)R(h) (cid:98)RS(h)(cid:12) (cid:12) (cid:12) (cid:113) K+log(1/δ) sup hH 20 for universal constant > 0 (polylogs hidden in O). If ˆh is ERM/AERM in H, then R(ˆh) (cid:98)RS(ˆh) + (cid:16)(cid:113) K+log(1/δ) (cid:17) inf hH (cid:98)RS(h) + (cid:16)(cid:113) K+log(1/δ) (cid:17) R(h) + (cid:16)(cid:113) K+log(1/δ) (cid:17) . inf hH Thus ERM/AERM preserves the PAC rate on fixed finite-VC class."
        },
        {
            "title": "14.3 Proof of Thm. 7: stability via step-mass",
            "content": "We prove uniform stability bound for (projected) SGD-like updates and then translate it to an expected generalization bound. Algorithm and neighboring samples. Let = (z1, . . . , zm) and S(i) be with the ith example replaced by an independent copy i. Run the same (possibly self-modified) algorithmic schedule on and S(i) with shared randomness. Denote parameter sequences {θt}T t=0 with updates t=0 and {θ t}T θt+1 = Π(θt ηtgt) , gt ℓ(θt; zIt), where Π is projection (if used) and It is the sampled index at step (uniform on [m] or any scheme that samples from S); define θ t+1 analogously with S(i) and the same It sequence. One-step sensitivity. By nonexpansiveness of projection and β-smoothness with step sizes ηt 1/β, θt+1 θ t+1 θt θ + ηtgt t. If It = then zIt is identical in both runs and gt t (by L-Lipschitzness of gradients under smoothness). If It = i, the gradients can differ by at most 2G. Taking conditional expectation over It (uniform sampling) yields Lθt θ E(cid:2)θt+1 θ t+1 (cid:12) (cid:12) θt, θ (cid:16) (cid:3) 1 + ηt (cid:17) θt θ + 2G ηt. Iterating from identical initialization gives EθT θ 2G (cid:88) ηt (cid:89) (cid:16) (cid:17) 1 + ηs t= s=t+1 2G e(L/m) (cid:80) ηs (cid:88) t= ηt 2GeL (cid:88) t=1 ηt, where we used (cid:80) constant). ηs MT and MT /m 1 for the exponent (or simply absorb e(L/m)MT into the From parameter sensitivity to loss stability. By L-Lipschitzness of ℓ(; z) in θ, (cid:12) (cid:12)ℓ(θT ; z) ℓ(θ ; z)(cid:12) (cid:12) θT θ . Taking expectation over all randomness (sample replacement, SGD sampling, and possibly algorithmic self-mod randomness) yields uniform stability parameter ϵstab := sup E(cid:2) ℓ(θT ; z) ℓ(θ ; z) (cid:3) (cid:88) t=1 ηt with := 2LGeL (or any problem-dependent constant absorbing smoothness/diameter factors). 21 Generalization gap. By standard stability-to-generalization transfer (uniform stability implies E[R(ˆh) (cid:98)RS(ˆh)] ϵstab), E(cid:2)R(ˆh) (cid:98)RS(ˆh)(cid:3)"
        },
        {
            "title": "C\nm",
            "content": "T (cid:88) t=1 ηt. This proves the claim. The bound extends to self-modified schedules because only the realized step sizes {ηt} enter the derivation; any randomness/decision logic is handled by the outer expectation, and the nonexpansive/smoothness arguments hold stepwise. Meta-policy corollary. (C/m) B(m). Choosing B(m) = O(1) gives O(1/m) expected gap; more generally, B(m) = O( gives O(1/ ηt B(m), then E[gap] m) If metacognitive rule enforces MT = (cid:80) m), etc."
        },
        {
            "title": "15 Full Proofs for Substrate Self-Modification",
            "content": "Assumptions. i.i.d. data; loss in [0, 1]; ERM/AERM as specified. substrate is computational model hosting the same specification (H, Z, A) unless explicitly stated. CT-equivalent means mutually simulable (e.g., universal TM, RAM, λ-calc) with finite simulation overhead independent of m. 15.1 CT-invariance (Thm. 8) Proof. Let be learning problem that is distribution-freely PAC-learnable on by algorithm Alg producing ˆh(S) with sample complexity m(ϵ, δ). Since is CT-equivalent to , there exists simulator SimF that, given the code of Alg, simulates it on with bounded overhead independent of the sample size m. Running SimF (Alg) on yields the same output hypothesis ˆh(S) for any dataset S. Hence the distributional correctness (and therefore sample complexity) is unchanged. Computation time may increase by constant/ polynomial factor but PAC definitions are insensitive to runtime. Therefore remains distribution-freely PAC-learnable on with the same m(ϵ, δ) up to constants. 15.2 Finite-state downgrade impossibility (Prop. 8) Modeling the downgrade. finite-state substrate has fixed number of persistent states (independent of m) available to the learner across training and prediction. Internally, any learning procedure is transducer whose internal memory is one of states; weights/parameters cannot grow with m. Problem and intuition. Consider thresholds on (or {0, 1} interpreted as integers): for N, define hk(x) = {x k}. The class Hthr = {hk : N} has VC(Hthr) = 1 (finite) and is PAC-learnable on CT-equivalent substrates (e.g., ERM picks ˆk between the largest positive and smallest negative). We show finite-state substrate cannot distribution-freely PAC-learn Hthr. Lemma (Pumping-style indistinguishability). For any finite-state learner with states and any > , there exist two training samples S, of size such that: (i) ends in the same internal state on and (with the same output hypothesis), yet (ii) there exists threshold target hk and test point on which the two training histories require different predictions to achieve small risk. Proof. Process the sorted stream of distinct integers {1, 2, . . . }; after distinct milestones the finite machine must revisit state (pigeonhole principle). Construct and that are identical except for the placement of one positive/negative label beyond the th milestone so that the same terminal state is reached but the correct threshold differs (choose between the conflicting milestones). Then 22 test point between those milestones is labelled differently by the two Bayes-consistent thresholds. Since the learner outputs the same hypothesis after and S, it must err on one of the two underlying distributions by constant amount (bounded away from 0). Proof of Prop. 8. Fix any finite-state learner and δ < 1/4. For each > , by Lemma 15.2 we can pick distribution supported on small interval around the conflicting milestones where the Bayes optimal threshold risk is 0 and the learners hypothesis (being the same for and S) incurs error at least > 0 with probability at least 1/2 over the draw of S. Hence no (ϵ, δ) distribution-free PAC guarantee is possible (take ϵ < c/2). The impossibility holds despite VC(Hthr) = 1, showing the downgrade (finite persistent memory) destroyed learnability. Remark (other families). An identical argument works for parities over {0, 1}d with unbounded or for intervals on N, and more generally any family whose Bayes-consistent parameter cannot be encoded in O(1) bits independent of m."
        },
        {
            "title": "15.3 Beyond-CT substrates (Prop. 8)",
            "content": "Proof. If is stronger than Turing but the measurable hypothesis family under consideration remains H, then PAC learnabilitydefined only in terms of approximation and sample complexitydepends on and the data distribution, not on computational strength. Thus the sample complexity guarantees are unchanged (computation of ERM/AERM might become easier, but that is outside classical PAC). If, however, the substrate edit enlarges the effective family to (e.g., enabling hypercomputable predictors or oracular label access), then learnability is governed by the induced policy-reachable family reach(u) VC(H ) < , and destruction otherwise. reach(u), and all results from 4 apply: preservation iff supH H"
        },
        {
            "title": "16 G¨odel Machine Foundations",
            "content": "Schmidhubers Godel Machine (Schmidhuber, 2005) provides theoretical framework for self-modifying agents that make provably optimal self-improvements. We summarize the key components. 16.1 Architecture and Decision Rule Godel Machine consists of an initial problem-solving policy e(1), proof searcher that tests proof techniques (programs generating formal proofs), an axiomatic system encoding hardware semantics, initial software p(1), environmental assumptions, and utility function u, plus candidate modification program switchprog. The machine executes switchprog (which may rewrite any part of the system, including the proof searcher itself) only when it has proven target theorem: u(cid:2)s(t1) (switchbit(t1) = 1), Env(t1)(cid:3) > u(cid:2)s(t1) (switchbit(t1) = 0), Env(t1)(cid:3), (4) where is the utility function, s(t) is system state at time t, and s(t1) (switchbit(t1) = b) denotes the hypothetical state with switchbit := b. The left side represents executing the modification; the right side represents continuing to search for alternatives. 16.2 Global Optimality Theorem (Schmidhuber 2005). Given any formalizable utility function and assuming consistency of the axiomatic system A, any self-modification obtained through proof of target theorem is globally optimal: the utility of executing the present switchprog is higher than the utility of waiting for the proof searcher to produce an alternative switchprog later. 16.3 Bias-Optimal Proof Search The initial proof searcher uses Bias-Optimal Proof Search (BIOPS), allocating time to proof techniques proportional to their prior probability (w). 23 Theorem (Schmidhuber 2005). to produce proof of difficulty measure k, then BIOPS requires at most O(f (k)) steps. If some unknown proof technique requires at most (k) steps Relation to our framework. While Godel Machines establish decision-theoretic optimality through formal proof, our work specializes this framework to provide distribution-free PAC learning guarantees by imposing tractable capacity bounds on the reachable hypothesis family."
        },
        {
            "title": "17 Experimental Details",
            "content": "Dataset. We generate binary labels from smooth ground-truth score on Rd (Gaussian inputs with mild interactions), passed through logistic link, plus bounded feature noise and small independent label-flip rate to induce nonzero Bayes error. single draw is split once into train/validation/test and reused across sequential edits. Representational axis MH . We simulate representational self-modification by increasing polynomial degree = 0, 1, 2, . . . in logistic model. Each candidate is trained on the fixed train set and accepted or rejected by one of four edit policies (Two-Gate, dest val nocap, dest val, dest train). We report the test 01 loss of the last accepted hypothesis at each step; curves end when an edit is rejected. Algorithmic axis MA. We hold the hypothesis class fixed and continue stochastic logistic training while tracking the cumulative step-mass MT = (cid:80) ηt. We compare capped TwoGate policy (halt when MT reaches preset budget) to destructive policy. We plot the generalization gap (testtrain loss) versus MT . 17.1 Experiment Hyperparameters Table 2: Experimental parameters for representational self-modification. Table 3: Experimental parameters for algorithmic self-modification. Parameter Train/Val/Test Max Degree Noise (σ) Label Flip L2 Reg. (C) Capacity (K) VC Const. (c0) Val. Margin (τmult) Confidence (δV ) Seeds Value 150/60/1k 30 1.2 35% 1.0 31 0.10 0.20 0.05 5 Parameter Value Task Setup Train Samples Val/Test Seeds Max Iter. (T ) Learn Rate (η0) Batch Size L2 Reg. Noise (σ) Label Flip Degree 5 logistic 500 1k/2k 20 50,000 0.01 32 105 0.6 20% Compared Policies for MH 1. Two-Gate: The safe policy enforcing both capacity gate and validation margin. 2. Destructive: Naively accepts any capacity increase if training loss does not increase. Compared Policies for MA 1. TwoGate: The stability meta-policy. Training is halted when the cumulative step-mass (MT = (cid:80) ηt) exceeds budget of = 2.5. 2. Destructive: The baseline policy. Training proceeds for the full 50,000 iterations with no constraint on step-mass."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "DeepMind",
        "ETH Zurich",
        "Google Brain",
        "IDSIA (Istituto Dalle Molle di Studi sull’Intelligenza Artificiale)",
        "Max Planck Institute for Intelligent Systems",
        "New York University",
        "SingularityNET",
        "Stanford University",
        "Technische Universität München",
        "University of Amsterdam",
        "University of Bath",
        "University of California, Berkeley",
        "University of Cambridge",
        "University of Edinburgh",
        "University of Freiburg",
        "University of Montreal (MILA)",
        "University of Oxford",
        "University of Toronto",
        "University of Washington"
    ]
}