{
    "paper_title": "WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation",
    "authors": [
        "Mingde Xu",
        "Zhen Yang",
        "Wenyi Hong",
        "Lihang Pan",
        "Xinyue Fan",
        "Yan Wang",
        "Xiaotao Gu",
        "Bin Xu",
        "Jie Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "User interface (UI) development requires translating design mockups into functional code, a process that remains repetitive and labor-intensive. While recent Vision-Language Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code model that generates executable interactive code; 3) a validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than general-purpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at \\href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\\texttt{https://webvia.github.io}}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 5 2 6 0 . 1 1 5 2 : r WebVIA: Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation Mingde Xu1,3*, Zhen Yang2,3*, Wenyi Hong2,3, Lihang Pan3, Xinyue Fan3, Yan Wang3, Xiaotao Gu3, Bin Xu2, Jie Tang2 1Faculty of Mathematics, University of Waterloo 2The Knowledge Engineering Group (KEG), Tsinghua University 3Zhipu AI m339xu@uwaterloo.ca, yang-zhen@mail.tsinghua.edu.cn, jietang@mail.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "User interface (UI) development requires translating design mockups into functional code, process that remains repetitive and laborintensive. While recent VisionLanguage Models (VLMs) automate UI-to-Code generation, they generate only static HTML/CSS/- JavaScript layouts lacking interactivity. To address this, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework comprises three components: 1) an exploration agent to capture multi-state UI screenshots; 2) UI2Code model that generates executable interactive code; 3) validation module that verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves more stable and accurate UI exploration than generalpurpose agents (e.g., Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit substantial improvements in generating executable and interactive HTML/CSS/JavaScript code, outperforming their base counterparts across both interactive and static UI2Code benchmarks. Our code and models are available at https://webvia.github.io."
        },
        {
            "title": "Introduction",
            "content": "User interface (UI) development is core step in modern software engineering, yet translating design mockups into functional code remains repetitive and labor-intensive process. Automated UI-toCode (UI2Code) has therefore emerged as promising direction, aiming to automatically transform UI screenshots into structured front-end code. Recent advances in Vision-Language Models (VLMs) (Liu et al., 2023; Wang et al., 2024a,b; Bai et al., 2025; Hong et al., 2025; Zhu et al., 2025) have created new opportunities for UI2Code that jointly interprets visual layouts and textual semantics, thus * Core contributors. Corresponding author. 1 moving beyond shallow pattern recognition (Beltramelli, 2018; Asıroglu et al., 2019; Chen et al., 2022) toward more robust and visually grounded code generation (Wu et al., 2025; Jiang et al., 2025). Despite these advances, current VLM-powered UI2Code approaches remain limited in functionality. As demonstrated in Figure 1, their outputs are typically restricted to static HTML/CSS/JavaScript code, which reproduce the visual appearance of interfaces but lack support for GUI interactions such as clicking, selecting, or entering text. The generated interfaces cannot correctly respond to user actions and therefore cannot be integrated into real-world UI development workflows. These limitations highlights the necessity for new framework capable of producing executable and truly interactive user interfaces. To address this gap, we propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation. The framework is composed of three components: (1) an exploration agent that interacts with the HTML environment to capture multiple UI screenshots across different interface states; (2) UI2Code model that leverages these screenshots to generate executable HTML/CSS code for an interactive GUI; (3) validation module that assesses the feasibility and interactivity of the generated GUI. We train two core models to ensure the high performance of WebVIA. The first is an exploration agent, WebVIA-Agent, that traverses the HTML environment to collect diverse interface states. We construct large-scale GUI interaction dataset, and experimental results show that WebVIA-Agent outperforms general-purpose models such as Gemini2.5-Pro (Comanici et al., 2025) in both stability and accuracy. The second is UI2Code model that generates executable and interactive interfaces. Based on Qwen-2.5-VL-7B and GLM-4.1V-9B, we train WebVIA-UI2Code-Qwen and WebVIAFigure 1: Motivating example illustrating the gap between static and interactive code generation. UI2Code-GLM using paired multi-state UI screenshots and executable interactive HTML. Compared with their respective base models, the two variants achieve improvements of 5.2 and 4.7 points on the Design2Code benchmark, and on UIFlow2Code they reach performance levels (75.9 and 84.9, respectively) close to that of the large-parameter stateof-the-art Claude-Sonnet-4 . Our contributions can be summarized as follows: Framework: We propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation, which bridges the gap between static UI rendering and executable, interactive front-end development. Model: Under the guidance of WebVIA, we train two dedicated models: an exploration agent for HTML environment interaction and state collection, and UI2Code model capable of generating executable code that supports real user interactions. Experiments: In our experiments, the WebVIA-Agent achieves higher stability and accuracy than general-purpose models such as Gemini-2.5-Pro, while the WebVIA-UI2Code model surpasses static layout reconstruction to produce robust and verifiable interactive web code."
        },
        {
            "title": "2.1 UI-to-Code Generation",
            "content": "The UI-to-Code (UI2Code) task aims to translate user interfaces into executable code, evolving from early deep learning approaches to recent visionlanguage model (VLM)-based meth2 ods. Before VLMs, works like Pix2Code (Beltramelli, 2018) used CNNRNN architectures for static UI generation. With VLMs, methods such as DeclarUI (Zhou et al., 2024) integrate vision and language models for better component grounding, while others enhance performance through largescale datasets (Laurençon et al., 2024; Yun et al., 2024; Gui et al., 2024b,a), multi-model collaboration (Jiang et al., 2025; Liang et al., 2024), or finegrained interface decomposition (Wu et al., 2025; Chen et al., 2025; Wan et al., 2024). However, these approaches remain limited to static HTML/CSS generation, lacking interactivity or functional validation. In contrast, WebVIA introduces an agentic, interaction-aware framework that enables verifiable and executable UI2Code generation. 2."
        },
        {
            "title": "Interactive Web Agents",
            "content": "Recent research has moved beyond static UI translation to explore interactive web environments, where agents perform multi-step actions on real webpages. Early systems such as World of Bits (Shi et al., 2017), WebGPT (Nakano et al., 2021), and BrowserGym (Chezelles et al., 2024) integrate language models with browser environments for reasoning and decision-making over dynamic content. Later benchmarks like WebArena (Zhou et al., 2023) and Mind2Web (Deng et al., 2023) enable grounded web interaction by allowing agents to perceive, plan, and execute GUI actions from both DOM structures and visual inputs. While these frameworks highlight the value of multimodal reasoning and feedback, existing agents remain optimized for single, predefined tasks, lacking the ability to systematically explore or verify interactive components. In contrast, our approach extends this paradigm toward agent-based UI synthesis and verification, where the agent actively traverses and tests webpage components to ensure functional and behavioral correctness."
        },
        {
            "title": "2.3 UI2Code Benchmark",
            "content": "Several benchmarks have advanced VLM-based UI2Code research. Design2Code (Si et al., 2024) introduces 484 real-world webpages for visualto-code evaluation, while Web2Code (Yun et al., 2024) and Flame-React (Ge et al., 2025) refine data pipelines via LLM-assisted layoutcode synthesis, though still relying on synthetic HTML. FullFront (Sun et al., 2025) broadens evaluation to the full front-end workflowdesign, perception, and code generation. More recently, Interaction2Code (Xiao et al., 2024) extends benchmarking to interactive UI2Code, assessing VLMs ability to reproduce functional behaviors such as event handling and state transitions. This manual dependency hinders large-scale random webpage generation and limits coverage of diverse interaction types. Moreover, the benchmarks evaluation paradigmrequiring explicit tagging of interactive elements and pre-specified commandsdiverges from real user behaviors, emphasizing instruction following rather than true interaction reasoning. To overcome these issues, we propose UIFlow2Code, scalable, flow-based benchmark that evaluates models ability to understand and reproduce multi-step interactions directly from visual and structural webpage states, eliminating the need for handcrafted task annotations and enabling behavior-grounded assessment."
        },
        {
            "title": "3 WebVIA Framework",
            "content": "We propose WebVIA, the first agentic framework for interactive UI-to-Code generation and validation, designed to move beyond static rendering toward executable and verifiable front-end development. As illustrated in Figure 2, WebVIA integrates three core components: an exploration agent that systematically interacts with HTML environments to uncover hidden states and produce validated UI screenshots, UI2Code model that leverages these screenshots to generate executable code with both layout fidelity and interactivity, and validation module that executes the generated code, verifies the support for intended GUI behaviors and functionalities. These components form pipeline that bridges the gap between static UI rendering and robust, interactive front-end development. Problem Formulation. We formalize interactive UI-to-Code generation as sequential decisionmaking problem over structured environment. The exploration agent interacts with webpage environment E. At each step t, the agent observes multimodal state st = (It, Dt), where It is the rendered screenshot and Dt is the DOM snapshot. The agent then selects and executes an action at , and the environment transitions to new state st+1. The action space consists of standard web interactions, including clicks, text inputs, selections, and navigations. The agent progressively uncovers hidden states and constructs an interaction graph = (S, ), where is the set of discovered states and the set of verified transitions. Based on this interaction graph G, the UI2Code model generates executable front-end code ˆC that faithfully reproduces the interactive behaviors of the original environment. Finally, the validation module executes the generated code and assesses the interactivity of the rendered GUI by verifying whether it can still support the transitions defined in the interaction graph G. Environment. The environment serves as the foundation of performing WebVIA framework. Inspired by prior work such as WebArena (Zhou et al., 2023), we implement dedicated web environment, denoted as WebEnv, which renders given HTML document within an isolated browser instance. The implementation builds on GymAPI for standardized interaction and Playwright for browser automation. WebEnv supports three core capabilities: (1) rendering and capturing full-page screenshots, (2) extracting DOM trees with annotated interactive elements and the corresponding XPaths via JavaScript instrumentation, and (3) executing user interactions within the browser. These functionalities enable systematic grounding of both the visual and structural aspects of the webpage, providing reliable basis for agent-driven exploration and evaluation. Real-world webpages are noisy and unstable due to advertisements, asynchronous loading, and external dependencies, making them unsuitable for controlled training and reproducible evaluation. To address this, we construct large-scale synthetic environment in which webpages are automatically generated from templates and textual specifications. This design provides diversity and systematic coverage of common interaction patterns, ensures full Figure 2: Overview of the WebVIA framework, which comprises three components: (a) an exploration agent to capture multi-state UI screenshots; (b) UI2Code model to generate interactive code; (c) validation module to verify the interactivity. controllability, and eliminates the unpredictability of real-world websites. The detailed synthesis pipeline is described in Appendix A.1."
        },
        {
            "title": "UI Discovery",
            "content": "The exploration agent is the core driver of WebVIAs interactive UI discovery process. It uncovers interactive elements within an HTML-based environment and constructs reliable interaction graph of the user interface. The agent has two key capabilities: action generation and interaction verification, and explores the GUI according to perceptionactionverification strategy: it grounds its understanding in rendered screenshots and DOM trees, proposes candidate interaction sequences, executes them in the environment, and verifies whether meaningful changes occur. This iterative process ensures comprehensive coverage of interactions and robustness against ineffective operations or redundant screenshots. Action Generation. The exploration agent begins by proposing candidate interaction sequences that systematically cover potential user operations on webpage. The agent, conditioned on its historical trajectory, observes the GUI state (i.e., the rendered screenshot and the DOM tree) and outputs action sequences. Each sequence can be primitive operation (e.g., single click) or composite workflow (e.g., text entry followed by button press). These sequences are executed within the environment to generate new candidate states for further exploration. Interaction Verification To ensure efficiency and correctness, the agent verifies each executed action sequence by comparing the resulting state against the initial state according to the following criteria: (1) whether the action sequence successfully executed, and (2) whether new interactive elements appeared on the page. The comparison result falls into three categories: sequences that fail to execute (non-interactive), sequences that succeed but reveal no new elements (usable but not explored further), and sequences that succeed and expose new elements (usable and retained for subsequent exploration). This interaction verification mechanism prunes redundant sequences and incorporates new interactive states into the exploration process. Exploration Strategy. We propose hybrid exploration strategy that integrates breadthfirst and depth-first search within perceptionactionverification loop. At each iteration, the agent generates and executes candidate actions in parallel (breadth-first) to maximize coverage, while promising states are further expanded through depth-first exploration to uncover longhorizon workflows. As illustrated in Figure 2 (a), this process incrementally expands an interaction graph, where each validated state becomes the root of subsequent exploration. By balancing breadth and depth, WebVIA achieves both comprehensive coverage of interactive elements and efficient discovery of dynamic states, thereby constructing reliable interaction graph for downstream code generation."
        },
        {
            "title": "3.2 Part 2: UI2Code Model for Interactive",
            "content": "Front-End Code Generation The ultimate objective of WebVIA is to move beyond static UI reconstruction and generate executable front-end code that faithfully captures the interactive functionality of the original interface. As illustrated in Figure 2(b), we introduce dedicated UI2Code model that is conditioned on the interaction graph derived from multiple screenshots produced by the exploration agent. Unlike prior approaches that rely on single static screenshot, our model benefits from diverse interface states captured during exploration, enabling it to synthesize functionally coherent GUI components that support essential interactive behaviors. Multimodal Inputs. Unlike prior approaches that rely on single static screenshot, our UI2Code model is conditioned on multiple screenshots collected during exploration, along with their verified interaction relationships. This structured input captures both the visual layouts of diverse interface states and the causal transitions induced by user actions. Leveraging this enriched representation, the model learns to reason about dynamic workflows rather than treating the UI as static rendering. Code Generation. Conditioned on these multimodal inputs, the UI2Code model generates executable front-end HTML/CSS/JavaScript code. In contrast to prior approaches that only reconstruct static layouts, our model explicitly preserves interactive functionality, ensuring that the generated code is both visually faithful and behaviorally reliable."
        },
        {
            "title": "Code",
            "content": "Existing evaluation methods for UI2Code models primarily focus on the visual fidelity of the generated interfaces, with little attention paid to verifying their interactivity. To address this limitation, the final component of WebVIA is the validation module, which ensures that the generated frontend code is functionally interactive. As shown in Figure 2(c), we adopt task-oriented validation procedure that directly evaluates the usability of the generated code. set of tasks (e.g., filling out form, submitting query, or navigating to target page) is pre-defined based on the interaction graph of the original GUI and executed on the synthesized interface. The synthesized interface passes validation if all tasks can be completed as intended. This evaluation goes beyond mere statetransition matching by providing direct measure of whether the generated code supports coherent end-to-end user workflows."
        },
        {
            "title": "4 Training Methodology",
            "content": "In this section, we describe how the two core models of WebVIAWebVIA-Agent and WebVIAUI2Codeare trained."
        },
        {
            "title": "4.1 WebVIA-Agent Training",
            "content": "The exploration agent is trained on multimodal webpage states, consisting of rendered screenshots and filtered DOM trees. Its training objectives are threefold: (1) to ensure stability across diverse webpage layouts, (2) to comprehensively detect and interact with standard interactive elements, and (3) to jointly support the two core functions of action generation and interaction verification. To support these objectives, we construct two complementary datasets. The Action Generation Dataset contains pairs of webpage states and annotated interaction sequences. Each entry includes screenshot, the corresponding DOM tree, and one or more ground-truth action sequences with their historical trajectories.The Interaction Verification Dataset stores the screenshots before and after executing operation sequences, together with annotations indicating whether meaningful changes (such as new elements or layout updates) have occurred. Details of the dataset construction process are provided in Appendix A.2. The curated datasets are used to supervise the agent through supervised fine-tuning (SFT) on GLM-4.1V-9B-base. The model learns to (i) predict valid action sequences from the action generation dataset, and (ii) classify meaningful transitions from the interaction verification dataset. Both tasks are optimized using cross-entropy loss. This joint training paradigm equips the agent with the ability to autonomously generate feasible interactions while reliably filtering out non-productive actions, providing robust foundation for the WebVIA exploration pipeline."
        },
        {
            "title": "4.2 WebVIA-UI2Code Model Training",
            "content": "The UI2Code model is trained to translate multiple UI screenshots into executable front-end code that preserves both layout fidelity and interactive functionality. Unlike conventional UI-to-code systems that rely solely on static screenshots, our training paradigm exploits the structured interaction graph collected by WebVIA, ensuring that the model learns to generate interactive HTML/CSS/- JavaScript code. We construct the WebView dataset with 11k synthesized webpages, each paired with its groundtruth HTML/CSS/JavaScript code. For every webpage, the exploration agent systematically discovers states and transitions, producing an interaction graph that contains rendered screenshots and validated action sequences. Instead of directly using the template-level HTML as supervision, we feed the exploration traces (multiple screenshots and their interaction graph) into Claude and generates the corresponding executable HTML/CSS/- JavaScript code. The resulting (interaction graph, generated code) pairs form the core training data for our UI2Code model, ensuring that the supervision is aligned with the observed multimodal states and their verified interactions. For fine-tuning, we structured promptresponse format, which is organized as: <think> </think><answer> </answer>. This formatting explicitly separates the reasoning context from the expected code output, enabling the model to learn stable mappings from multimodal observations to structured, executable code. The detailed data construction and data cases are provided in Appendix A.3. We perform supervised fine-tuning (SFT) on GLM-4.1V-9B-base and Qwen2.5-VL-7B-Instruct on these formatted pairs. adopt a"
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct comprehensive experiments to validate the effectiveness of the proposed WebVIA framework. Our evaluation is organized around its two trainable components: the exploration agent and the UI2Code model. For the exploration agent, we examine both its intrinsic ability to generate and verify UI actions, as well as its performance in the full pipeline of interaction screenshot collection. For the UI2Code model, we evaluate its capability to generate interactive HTML/CSS/JavaScript code from multiple screenshots, 6 focusing on structural fidelity and functional correctness."
        },
        {
            "title": "5.1 Evaluation Setup",
            "content": "Benchmark. Since no public benchmark exists for evaluating our proposed WebVIA framework, we construct two dedicated benchmarks. UIExplore-Bench evaluates the exploration agents ability to navigate complex webpages and collect interaction screenshots, while UIFlow2Code-Bench assesses the ability of UI2Code model to reconstruct webpages with both structural fidelity and functional correctness. All samples are carefully annotated to ensure accuracy and consistency. These benchmarks provide the first standardized protocol for this task and are designed to facilitate future research. Additional construction details of both UIExploreBench and UIFlow2Code-Bench are provided in Appendix A.4 and Appendix A.5, respectively. Baselines. The WebVIA framework is designed to be model-agnostic, allowing both the exploration agent and the UI2Code model to be instantiated with any vision-language models. To systematically evaluate the advantages of our trained exploration agent and UI2Code model, we compare against suite of state-of-the-art VLMs, including Claude-Sonnet-4, Claude-Sonnet-3.7, GPT-5, o4mini, GPT-4o and Gemini-2.5-pro. These models can be seamlessly integrated into the WEBVIA framework via API calls, without requiring any task-specific adaptation. The versions and API endpoints of baselines are provided in Appendix A.6."
        },
        {
            "title": "5.2 Single-Step Agent Evaluation",
            "content": "To assess the exploration agents performance independent of the full pipeline, we conduct singlestep experiments on two fundamental tasks: action generation and interaction verification. The full prompt templates used for Action Generation and Interaction Verification are provided in Appendix A.7. Action generation. In this task, the agent predicts list of valid interactive elements from given UI screenshot and DOM tree. We report Precision, which measures the proportion of correctly predicted actions among the agents selected actions, Recall, which measures the proportion of correctly predicted actions with respect to the ground-truth actions, and F1, which captures the harmonic mean of Precision and Recall. As shown in Table 1, the WebVIA-agent outperforms all baselines except Gemini-2.5-Pro. Its advantage stems from SFT training, which enables the agent to capture subtle structural patterns and focus on truly actionable elements. Different from Gemini-2.5-pro that tends toward an overly aggressive strategy of selecting nearly all visible elements, WebVIA-agent demonstrates more balanced and reliable behavior. Furthermore, WebVIA-agents predictions maintain high protocol fidelity, rarely producing mismatched DOM identifiers or invalid actions. By contrast, GPT-4o performs poorly, largely due to its inability to adhere to the prescribed interaction format, which undermines its applicability within the pipeline. The prompts Table 1: Comparison of action generation performance (Precision, Recall, and F1) on UIExplore-Bench with 87 action samples. Model Precision (%) Recall (%) F1 (%) Gemini-2.5-pro GPT-5 o4-mini GPT-4o Claude-Sonnet-3.7 Claude-Sonnet-4 WebVIA-Agent 74.01 81.66 79.01 4.77 75.29 81.16 82.37 95.94 88.41 91.80 5.43 95.18 89. 92.61 81.70 81.85 83.16 4.85 81.72 83.38 85.30 Interaction verification. Given set of images representing executed actions, the agent produces two Boolean outputs: \"pass\", indicating whether the sequence executes correctly, and \"terminate\", indicating whether the interaction introduces any new elements. We compute accuracy separately for both dimensions and use their average as the overall score. As shown in Table 2, the WebVIA-agent achieves the best performance across all three metrics, demonstrating its superior verification ability. The superior terminate accuracy of WebVIA-agent underscores the effectiveness of SFT training in enhancing visual understanding and distinguishing genuinely new interactive elements. For example, when an interaction contains repeat elements of the same type (e.g., multiple delete item buttons), the agent successfully identifies the redundancy and terminates the exploration branch, thereby preventing unnecessary actions."
        },
        {
            "title": "5.3 Pipeline-Level Agent Evaluation",
            "content": "While single-step experiments isolate the agents capabilities on action generation and interaction verification, they fail to provide comprehensive assessment of its effectiveness in realistic, end-toTable 2: Comparison of verification performance across baseline VLMs and the WebVIA-agent on UIExploreBench with 53 verification samples. Model Pass Acc Terminate Acc Overall Acc Gemini-2.5-pro GPT-5 o4-mini GPT-4o Claude-Sonnet-3.7 Claude-Sonnet-4 WebVIA-Agent 94.34 96.23 94.34 33.96 94.34 94.34 98.11 81.13 84.91 83.02 62.26 77.36 77.36 86.79 87.74 90.57 88.68 48.11 85.85 85.85 91. end scenarios. We further evaluate the exploration agent within the WebVIA framework, where it autonomously explores webpages, generates interaction traces, and collects representative screenshots. Evaluation metrics. To assess the effectiveness of the exploration agent throughout the entire collection of interaction screenshots, we adopt three complementary metrics: Completeness measures the coverage of action generation, i.e., the proportion of distinct UI elements successfully explored. Correctness quantifies the correctness of verification results, reflecting the agents ability to determine whether an executed action achieves its intended effect. Deduplication Rate quantifies the prevalence of redundant or repeated actions within the generated traces, serving as an indicator of exploration efficiency. We compute an overall score as weighted combination of the three metrics: Overall = 0.40 Comp + 0.35 Correct + 0.25 Dedup (1) , where the weights are empirically determined to balance coverage, correctness, and efficiency. Evaluation results. As reported in Table 3, WebVIA-Agent achieves the best overall score of 89.8%, surpassing all baseline models. It achieves the best Completeness (93.1%) and Correctness (97.7%), confirming its ability to both discover diverse actionable elements and reliably validate their outcomes. These improvements arise from supervised fine-tuning, which strengthens structural understanding and encourages balanced exploration strategy. For instance, when encountering redundant screenshots with overlapping content, WebVIA-Agent prioritizes unexplored regions and reducing duplication. By contrast, models such as o4-mini and Gemini-2.5-pro often re-trigger identical actions (e.g., repeatedly clicking the same button), yielding inefficiencies despite high nominal coverage. Table 3: Pipeline-level performance comparison across baseline VLMs and the proposed WebVIA-Agent on UIExplore-Bench with 56 webpages. Model Completeness (%) Correctness(%) Deduplication Rate (%) Overall Score (%) Gemini-2.5-pro GPT-5 o4-mini GPT-4o Claude-Sonnet-3.7 Claude-Sonnet-4 WebVIA-Agent 92.61 76.66 91.73 16.46 75.86 86. 93.12 95.39 90.19 94.07 62.63 94.06 95.07 97.71 5.60 93.82 52.73 97.45 72.36 80.36 72.73 71.83 85.69 82.80 52.87 81.35 87. 89.63 5."
        },
        {
            "title": "Interactive Code Generation Evaluation",
            "content": "To assess the capability of UI2Code model in generating fully interactive HTML/CSS/JavaScript code, we extend the evaluation beyond conventional static UI2Code task to interactive code generation. Evaluation metrics. For each generated HTML page, set of tasks is defined based on the corresponding input images. Each task undergoes the validation process and is labeled as either pass or fail. The final evaluation metric is calculated as the ratio of the number of pass tasks to the total number of tasks. Evaluation results. As reported in Table 4, supervised fine-tuning on the WebView dataset substantially improves the ability of both Qwen-2.5VL-7B and GLM-4.1V-9B to generate executable interactive HTML/CSS/JavaScript code, whereas their base counterparts fail to produce valid outputs. This contrast highlights that interactive training data are indispensable for enabling interaction capabilities. Interestingly, although our supervised fine-tuning is conducted exclusively on interactive UI2Code data, we also observe consistent improvements on static UI2Code benchmarks. This suggests that interactive training data provide richer structural and functional supervision than conventional single-state screenshots, thereby enhancing the models ability to capture layout fidelity and semantic alignment even in static scenarios. Details of the evaluation prompt for interactive code generation are provided in Appendix A.8."
        },
        {
            "title": "5.5 Average Trace Length of WebVIA-Agent",
            "content": "In our ablation study, we analyze the relationship between average trace length and overall performance as reported in Table 3. Here, trace length refers to the number of interaction steps executed by the pipeline-level WebVIA-Agent during full webpage exploration. lower trace length may Table 4: Performance comparison on static (Design2Code) and interactive UI2Code (UIFlow2Code). Model Design2Code UIFlow2Code Gemini-2.5-pro GPT-5 o4-mini GPT-4o Claude-Sonnet-3.7 Claude-Sonnet-4 Qwen2.5-VL-7B-Instruct WebVIA-UI2Code-Qwen GLM-4.1-V-9B-Base WebVIA-UI2Code-GLM 89.5 89.7 63.8 35.3 77.7 81.2 29.1 34.3 58.3 63.0 90.2 69.9 69.4 59.3 81.5 82.1 / 75. / 84.9 Figure 3: Correlation between the mean interaction trace length and the overall exploration score across our WebVIA-Agent and various VLMs. indicate more intelligent action planning, but it may also reflect premature termination that fails to capture deeper interactive elements. As shown in Figure 3, WebVIA-Agent achieves balanced mean trace length together with the highest overall performance. This combination indicates WebVIAAgent is not only efficient but also consistently effective, indicating that it strikes strong balance between quality and speed."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present WebVIA, an agentic framework for interactive UI-to-code generation and validation. Unlike prior methods limited to static HTML/CSS reconstruction, WebVIA introduces an ex8 plorationgenerationvalidation pipeline that enables interaction-aware, behavior-preserving code synthesis. Built upon large-scale GUI interaction and WebView data, two specialized agentsan exploration agent and UI2Code generatorjointly produce executable and verifiable web interfaces."
        },
        {
            "title": "Limitations",
            "content": "Although WebVIA establishes new paradigm for interactive UI-to-Code generation, there remain two limitations in scalability and generalization that need to be addressed before achieving broader applicability. (1) In the exploration stage of WebVIA pipeline, the action types are restricted to Click, Enter, and Select. Executing broader action types such as Drag and Draw requires precise pixel coordinates, which defers from our current approach of ID based DOM to XPath execution. (2) Training the agent primarily on synthetic Webpages may limit its ability to handle certain specialized interaction tasks in real-world settings. For example, WebVIA-Agent struggles with domains such as calculators or function-plotting interfaces, where interaction patterns deviate substantially from the structures observed in the training environment. These constraints delineate the current scope of WebVIA and point to concrete directions for extending its applicability in future research."
        },
        {
            "title": "References",
            "content": "Batuhan Asıroglu, Büsta Rümeysa Mete, Eyyüp Yıldız, Yagız Nalçakan, Alper Sezen, Mustafa Dagtekin, and Tolga Ensari. 2019. Automatic html code generation from mock-up images using machine learning techniques. In 2019 Scientific meeting on electricalelectronics & biomedical engineering and computer science (EBBT), pages 14. Ieee. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Tony Beltramelli. 2018. pix2code: Generating code from graphical user interface screenshot. In Proceedings of the ACM SIGCHI symposium on engineering interactive computing systems, pages 16. Wen-Yin Chen, Pavol Podstreleny, Wen-Huang Cheng, Yung-Yao Chen, and Kai-Lung Hua. 2022. Code generation from graphical user interface via attentionbased encoderdecoder model. Multimedia Systems, 28(1):121130. Yunnong Chen, Shixian Ding, YingYing Zhang, and 1 others. 2025. Designcoder: Hierarchy-aware and self-correcting ui code generation with large language models. arXiv preprint arXiv:2506.13663. De Chezelles, Thibault Le Sellier, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank Xu, Siva Reddy, Quentin Cappart, and 1 others. 2024. The browserarXiv gym ecosystem for web agent research. preprint arXiv:2412.05467. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114. Tong Ge, Yashu Liu, Jieping Ye, Tianyi Li, and Chao Wang. 2025. Advancing vision-language models in front-end development via data synthesis. arXiv preprint arXiv:2503.01619. Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Shaoling Dong, Xing Zhou, and Wenbin Jiang. 2024a. Vision2ui: real-world dataset with layout for code generation from ui designs. arXiv preprint arXiv:2404.06369. Yi Gui, Zhen Li, Yao Wan, and 1 others. 2024b. Webcode2m: real-world dataset for code genarXiv preprint eration from webpage designs. arXiv:2404.06369. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, and 1 others. 2025. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pages arXiv2507. Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael Lyu, and Xiangyu Yue. 2025. Screencoder: Advancing visual-to-code generation for front-end automation via modular multimodal agents. arXiv preprint arXiv:2507.22827. Hugo Laurençon, Léo Tronchon, and Victor Sanh. 2024. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029. Shanchao Liang, Nan Jiang, Shangshu Qian, and 1 others. 2024. Waffle: Finetuning multi-modal model for automated front-end development. arXiv preprint arXiv:2410.18362. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Ting Zhou, Yanjie Zhao, Xinyi Hou, Xiaoyu Sun, Kai Chen, and Haoyu Wang. 2024. Bridging design and development with automated declarative ui code generation. arXiv preprint arXiv:2409.11667. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, and 1 others. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and 1 others. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017. World of bits: An open-domain platform for web-based agents. In International Conference on Learning Representations (ICLR). Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. 2024. Design2code: Benchmarking multimodal code generation for auarXiv preprint tomated front-end engineering. arXiv:2403.03163. Haoyu Sun, Huichen Will Wang, Jiawei Gu, Linjie Li, and Yu Cheng. 2025. Fullfront: Benchmarking mllms across the full front-end engineering workflow. arXiv preprint arXiv:2505.17399. Yuxuan Wan, Chaozheng Wang, Yi Dong, and 1 others. 2024. Automatically generating ui code from screenshot: divide-and-conquer-based approach. arXiv preprint arXiv:2406.16386. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, and 1 others. 2024a. Qwen2vl: Enhancing vision-language models perception arXiv preprint of the world at any resolution. arXiv:2409.12191. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, and 1 others. 2024b. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499. Fan Wu, Cuiyun Gao, Shuqing Li, Xin-Cheng Wen, and Qing Liao. 2025. Mllm-based ui2code automation guided by ui layout information. Proceedings of the ACM on Software Engineering, 2(ISSTA):1123 1145. Jingyu Xiao, Yuxuan Wan, Yintong Huo, and 1 others. 2024. Interaction2code: Benchmarking mllm-based interactive webpage code generation from interactive prototyping. arXiv preprint arXiv:2411.03292. Sukmin Yun, Haokun Lin, Rusiru Thushara, and 1 others. 2024. Web2code: large-scale webpage-tocode dataset and evaluation framework for multimodal llms. arXiv preprint arXiv:2406.20098. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, and 1 others. 2023. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854."
        },
        {
            "title": "Webpage Design Instruction Template",
            "content": "A.1 Environment HTML Synthesis To enable scalable data generation for agent training, we construct simulated HTML synthesis pipeline (See Figure 4) that automatically produces diverse and interactive webpage environments. The pipeline begins with theme list (e.g., online shopping, news websites, online maps), from which general instruction template is provided to generate task-specific prompts. Using o4-mini, we expand each general instruction into detailed naturallanguage prompts that specify the structure, layout, and interaction requirements for webpages. These detailed prompts are then fed into Claude-Sonnet-4, which generates executable HTML/CSS/JavaScript documents formulated within the React framework (hereafter referred to as HTML documents/HTML codes), enabling the construction of interactive elements such as buttons, input forms, and navigation menus. This two-stage generation process ensures semantic diversity (via high-level theme variation) and functional richness (via prompt-guided interaction synthesis). The resulting webpages form large-scale synthetic environment for the WebVIAAgent, supporting consistent and reproducible training across varied interface types and behaviors. Figure 4: Overview of the webpage synthesis process in the WebVIA framework. Webpage Design Instruction Template. To guide the generation of diverse interactive webpages, we design general instruction template that can be automatically adapted to different themes in the synthesis pipeline. Each theme (e.g., Online Shopping, News Website, Online Maps, Portfolio Page) is inserted into the template to form specific prompt. The following template illustrates the general structure used to generate detailed webpage instructions, as shown in Figure 5. Code Generation Prompt. To ensure functional completeness and visual consistency, we design dedicated code generation prompt that explicitly instructs the model to generate self-contained and interaction-ready HTML code. Each generated Please write detailed prompt that will be used to instruct text-to-image model to generate an interactive webpage HTML code with the theme <INSERT THEME FROM THEME LIST>. Specific Requirements: 1. The webpage content should revolve around the specified theme and include wide variety of theme-related modules. 2. The webpage must contain multiple interactive elements, limited to buttons, input fields, and dropdown selectors. Each interactive component should cause corresponding and reasonable changes on the webpage. 3. The webpage content should be rich, detailed, and contextually diverse. 4. The output should only contain the final prompt for the AI to generate the webpagewithout explanations, metadata, or additional commentary. Figure 5: Template used to construct webpage design prompts for generating interactive webpage HTML code. page is automatically executed and rendered in browser environment using Playwright to verify both visual correctness and interactive functionality. Only webpages that successfully render and execute without errors are retained, ensuring that the synthesized dataset is composed of valid, executable, and behaviorally rich webpages. The following prompt defines the instruction used for generating executable HTML code, as illustrated in Figure 6. A.2 Training Dataset for Exploration Agent To train the WebVIA-Agent for robust and generalizable UI exploration, we construct large-scale GUI interaction dataset derived from the synthetic HTML environments described in Section A.1. Each webpage instance provides structured environment where the agent can perceive the DOM tree, rendered screenshot, and interaction history, enabling the model to learn both visual and semantic representations of interactive elements. Automated Data Construction. To efficiently construct the training data for the WebVIA-Agent, we design an automated data generation pipeline that produces two complementary datasets: (1) Action Generation and (2) Interaction Verification. Both are generated using the o4-mini model within"
        },
        {
            "title": "Code Generation Prompt",
            "content": "You are web development expert highly sensitive to details and interaction experience, proficient in React and Tailwind CSS. Please generate highly interactive single-page application with reasonable layout and rich content for the specified theme according to the following requirements. Basic Requirements: 1. Generate complete interactive single-page website rendered using React (v18) and Tailwind CSS (v3+). 2. Return only the full source code wrapped within <html>...</html> tags. Do not include markdown wrappers, explanations, or code comments. 3. Must include the following dependencies: < script src =\" https :// cdn . jsdelivr . net / npm / react@18 .0.0/ umd / react . development . js \" > </ script > < script src =\" https :// cdn . jsdelivr . net / npm / react - dom@18 .0.0/ umd / react - dom . development . js \" > </ script > < script src =\" https :// cdn . jsdelivr . net / npm / @babel / standalone / babel . js \" > </ script > < script src =\" https :// cdn . tailwindcss . com \" > </ script > < link rel =\" stylesheet \" href =\" https :// cdnjs . cloudflare . com / ajax / libs / font - awesome /5.15.3/ css / all . min . css \" > </ link > Interactivity and Functional Areas: 1. All interactive components (input, button, select) must trigger meaningful updates to the rendered page. 2. For editable content, use modals, dropdowns, or input forms with complete validation. 3. Use real pictures from https://picsum.photos/. Each image must have fixed URL and remain constant across reloads. Page Structure and Layout: 1. Include logical partitions (navigation, sidebar, main content, etc.) referencing modern app layouts. 2. Ensure all sections are populated; empty placeholders are not allowed. 3. The visual style must match the assigned theme (e.g., business, minimalism, tech, lifestyle). Notes: Do not output explanations or text outside the code. Ensure all theme-related UI logic is complete and intuitive. Webpage Description: <INSERT DETAILED PROMPT FROM STAGE 1> Figure 6: Code Generation Prompt used for large-scale HTML synthesis. the WebEnv environment, which supports both synthetic and real webpages. For the Action Generation dataset, o4-mini serves as general-agent and is executed once across the entire WebVIA environment, with its exploration trajectories recorded and subsequently reconstructed. The reconstruction format preserves both the historical context and the input (a paired visualstructural state, consisting of the rendered UI screenshot and its associated DOM hierarchy), while the output is represented as sequences of operations (e.g., boxed{click[1]}, boxed{enter[2][Hello World!], click[5]}). For the Interaction Verification dataset, the pipeline executes these action sequences. Each sequence at:t+k may consist of multiple actions, where each action produces an intermediate state st+1, st+2, . . . , st+k. The resulting set of screenshots across these successive states is jointly considered as the post-action evidence. Formally, each interaction tuple is represented as (st, at:t+k, {st+1, st+2, . . . , st+k}, rt), where rt {0, 1} indicates the correctness of the overall outcome. This dual-branch data generation process yields approximately 180K verified interac12 tion samples across 20K webpages, encompassing wide range of UI components, event bindings, and layout hierarchies. Human-in-the-Loop Verification. To ensure data reliability, we incorporate semi-automated verification stage, where annotators rely on rulebased checks to assess both the correctness of generated action sequences and the corresponding success labels. For action sequences, automated filtering is applied to remove cases with overly short selections or high redundancy, while inconsistent examples are corrected when necessary. Verification is handled through mixed procedure: annotators first sample and manually inspect subset of instances, and their findings are used to identify recurring types of outcomes that diverge from human judgment. These patterns are then formalized into rules, which guide the selective removal or adjustment of the affected cases. Discussion. The resulting dataset unifies both action generation and interaction verification supervision, encouraging WebVIA-Agent to reason not only about what actions to take but also about their functional outcomes. Despite being primarily trained on synthetic webpages, the agent exhibits strong generalization to real-world sites, successfully handling unseen layouts, interaction patterns, and DOM structures. A.3 Training Dataset for Interactive UI2Code"
        },
        {
            "title": "Model",
            "content": "To enable the WebVIA-UI2Code model to generate executable and interactive HTML code, we construct the WebView dataset, which aligns multi-state UI screenshots with their corresponding groundtruth interactive webpages. Each data instance captures both the static visual layout and the dynamic behavioral transitions of webpage, providing comprehensive supervision for learning interactionaware code generation. Data Generation Pipeline. As illustrated in Figure 7, the construction of the WebView dataset follows three-stage pipeline: (1) Webpage Construction. Using the HTML synthesis pipeline described in Appendix A.1, we generate large number of interactive webpages with diverse themes (e.g., shopping, news, portfolio, dashboard). Each page contains multiple interactive elements such as buttons, input fields, dropdowns, and forms, all bound with interactive behaviors. (2) State ExploFigure 7: Pipeline for constructing the WebView dataset. ration. The WebVIA-Agent interacts with each synthesized page to traverse all reachable states. During this process, it captures multi-state screenshots {I1, I2, ..., In} together with corresponding DOM snapshots and event logs, reflecting visual and structural transitions triggered by user interactions. (3) Interactive Code Generation. After obtaining multi-state UI screenshots and corresponding interaction traces from the exploration stage, we adopt multimodal instructionresponse formulation to transform these visual observations into executable code. Specifically, we prompt the Claude-Sonnet-4 model to generate code under structured reasoning format that explicitly separates the thought process and the final output using the tags <think> and <answer>. Within the <think> block, the model is encouraged to analyze the provided screenshots and interaction logs, infer component hierarchies, and reason about event dependencies. The final HTML implementation is then produced in the <answer> block, ensuring clear delineation between reasoning and generation. This design allows the model to perform interpretable, step-by-step reasoning about webpage structure and interactivity before emitting executable code, leading to more functionally correct, visually coherent, and behavior-consistent outputs. Prompt for Interactive Code Generation. To ensure consistent reasoning and interpretable generation during interactive code synthesis, we design multimodal instruction prompt tailored for CLAUDE-SONNET-4, as illustrated in Figure 8. Quality Assessment. To ensure that the synthesized interactive code is functionally executable and visually coherent, each generated HTML file is automatically rendered in browser environment powered by Playwright. The automatic check focuses on whether the page can be successfully loaded and rendered without runtime errors. For interactive components and state transitions, we rely on sample-based human verification: annotators review subset of interactions (e.g., clicking, text input, and selection), and their inspection con-"
        },
        {
            "title": "Interactive Code Generation Prompt",
            "content": "You are highly skilled at building interactive webpages with React and Tailwind, and can precisely reconstruct complete HTML interactive webpage based on multiple webpage screenshots provided by the user. Initial Interface Requirements: 1. Build the page strictly according to the first webpage screenshot provided by the user. It must be exactly the same as the first screenshot you receive. 2. Do not miss any details. Background colors, fonts, font sizes, spacing, borders, icons, and text must strictly match the screenshot. 3. Every line of text in the screenshot must be presented verbatim. 4. For images, please use real pictures from the https://picsum.photos/ library, with URLs like https://picsum. photos/id/.../.../.... Each image must explicitly list its URL. Do not use reusable image components. Each webpage components image URL must be fixed and must not be randomly regenerated each time. Task Requirements: 1. The user will send you multiple images. Each image represents screenshot of the webpage after single interactive operation, and all images together represent the screenshots resulting from all operations performed on this page. 2. The user will send you detailed operation-sequence list. Each item in the list represents one operation sequence and will tell you which image (by index in the images you received) is the starting image (the page before the operation), and which images correspond to the sequence of screenshots after each step in the operation. Locate these images yourself. An operation sequence may include multiple operations, i.e., it may span multiple images. Some operation sequences have many intermediate steps, but among the images only the first and the last are providedidentify the specific operation content yourself. 3. After locating these images, read the operation description for that item. There are three types of operations: input, click, and select. Correctly identify which interactive component in the screenshots corresponds to each operation, and implement them correctly in the generated HTML webpage. 4. All interactive operations given to you must be perfectly replicated in the generated HTML, meaning they must be fully functional, and once completed, the page must match the corresponding screenshots. Library Requirements: < script src =\" https :// cdn . jsdelivr . net / npm / react@18 .0.0/ umd / react . development . js \" > </ script > < script src =\" https :// cdn . jsdelivr . net / npm / react - dom@18 .0.0/ umd / react - dom . development . js \" > </ script > < script src =\" https :// cdn . jsdelivr . net / npm / @babel / standalone / babel . js \" > </ script > < script src =\" https :// cdn . tailwindcss . com \" > </ script > < link rel =\" stylesheet \" href =\" https :// cdnjs . cloudflare . com / ajax / libs / font - awesome /5.15.3/ css / all . min . css \" > </ link > You may use Google Fonts. Code Output Format: 1. Output only the code within the complete <html></html> tags. 2. Do not add markdown quotes or html before or after the code. Figure 8: The instruction prompt used for interactive code generation within the WEBVIA framework. firms that the vast majority of components behave as expected. This combined strategy allows us to filter out pages that fail to render while providing evidence that the generated interactions remain largely reliable, ensuring high-quality supervision for training the WebVIA-UI2Code model. A.4 UIExplore-Bench Construction To systematically evaluate the performance of the WebVIA-Agent, we construct UIExplore-Bench, benchmark specifically designed for assessing both fine-grained interaction reasoning and end-toend exploration performance. Unlike previous web agent datasets that primarily target task completion, UIExplore-Bench focuses on measuring the agents capability to recognize UI components, execute valid interactions, and verify their functional correctness. Benchmark Composition. comprises three complementary subsets: UIExplore-Bench Action Generation Set. This subset contains 87 annotated interaction samples, each represented as paired (image, DOM tree) inputs. Each sample specifies target action in natural language (e.g., click the Search button or enter text in the input field), enabling quantitative evaluation of the agents action prediction accuracy given specific webpage state. Interaction Verification Set. This subset includes 53 verification cases, each consisting of preand post-interaction screenshots along with DOM snapshots. The task requires the agent to determine whether the executed action produces functionally valid change (e.g., modal opening, content update, navigation), thereby assessing its ability to reason about dynamic webpage transitions. Pipeline-Level Evaluation Set. We build larger-scale evaluation suite containing 56 complete webpages. For each webpage, the agent must autonomously explore all interactive components, generate valid action traces, and collect representative screenshots across multiple UI states. This subset evaluates the agents full exploration pipelinefrom perception and action generation to interaction validation. Construction Pipeline. UIExplore-Bench comprises three subsets: an Action Generation Set, an Interaction Verification Set, and Pipeline Evaluation Set. Each subset targets distinct dimension of GUI reasoning and follows dedicated data construction pipeline to ensure reliability and coverage. UI < screenshot, DOM > pair, the leading models such as Claude-Sonnet-4, GPT-5, and Gemini-2.5-Pro +re are prompted to generate candidate interaction sequences. From each webpage, we retain the UI screenshot associated with the richest interaction set and aggregate all model-generated actions. Subsequently, human annotators manually verify the combined sequences to remove nonfunctional, ambiguous, or nonexistent interactions, resulting in high-quality ground-truth action set. Interaction Verification Set. This subset targets the evaluation of an agents ability to verify whether an interaction has been successfully executed. We reselect 100 webpages spanning diverse application domains and employ the WebVIA-Agent solely to collect paired preand post-interaction states, including screenshots and action logs. Subsequently, human annotators manually inspect and label these pairs to determine whether the executed interaction leads to valid and functionally consistent state transition. After filtering out ambiguous or redundant cases, 53 high-quality samples are retained as groundtruth verification data, each providing reliable reference for assessing interaction correctness. Pipeline Evaluation Set. This subset is designed to evaluate the agents end-to-end exploration capability within complete webpage environments. We re-select 100 webpages across diverse domains and conduct autonomous exploration using the leading models such as Claude-Sonnet-4, GPT-5, and Gemini-2.5-Pro within the WebVIA framework. For each webpage, we merge all screenshots from different models and perform manual annotation to filter out invalid, overly long, or non-existent interaction elements. After this refinement process, 56 webpages are retained as high-quality ground-truth cases. Action Generation Set. This subset focuses on evaluating the agents ability to generate valid interaction actions from single webpage state. We first select 100 webpages across diverse themes and employ the WebVIA-Agent to collect all possible interaction screenshots within each page. For every A.5 UIFlow2Code-Bench Construction To systematically evaluate interactive UI-to-Code generation, we construct UIFlow2Code-Bench, benchmark designed to assess models ability to generate executable, behavior-preserving HTML code from multi-state user interface (UI) observations. Unlike existing UI2Code benchmarks such 15 as Design2Code (Si et al., 2024) and FullFront (Sun et al., 2025), which focus solely on static layout reconstruction, UIFlow2Code-Bench explicitly incorporates state transitions and interaction traces, enabling fine-grained evaluation of interaction-aware code synthesis. Each baseline model (e.g., Claude-Sonnet, GPT-5, Gemini-2.5) was queried using the same textual instructions, with minimal format adaptation to comply with their API requirements. Temperature was fixed to 0.0 for deterministic outputs unless otherwise noted. Benchmark Composition. UIFlow2CodeBench contains 50 synthesized webpages covering diverse domains including e-commerce, news, dashboards, and portfolio sites. Each sample is composed of (1) sequence of multi-state UI screenshots captured during interaction, and (2) the corresponding executable ground-truth HTML implementation. On average, each webpage includes 46 interaction states and 812 functional components such as buttons, modals, forms, and dropdown menus. These paired multi-view samples enable fine-grained evaluation of interaction-aware code synthesis. Construction Pipeline. We select 100 webpages across diverse domains and employ the WebVIA framework to conduct systematic exploration using the WebVIA-Agent. For each webpage, the agent traverses all available interactive components and records corresponding multi-state UI screenshots. Human annotators then manually select 6 representative screenshots per webpage, each associated with 2 to 5 interaction actions, covering diverse visual layouts and interaction task types such as clicking, text input, and selection. These selected UI states collectively define the interaction trajectories that the model is expected to reproduce. In the evaluation phase, the generated HTML code is considered correct if it can faithfully execute the annotated actions and reproduce the corresponding state transitions, rather than merely replicating static visual appearance. This design ensures that UIFlow2Code-Bench emphasizes interaction consistency and functional correctness over superficial layout matching. A.6 Baselines Versions and API Endpoints Table 5 summarizes the specific model versions and API endpoints used for each vision-language model evaluated within the WEBVIA framework. A.7 Prompts for Baseline Models To ensure fair comparison across all baselines, we designed unified prompt templates for the two main subtasks in the WEBVIA framework: (1) Action Generation and (2) UI2Code Translation. Action Generation Prompt This task evaluates models ability to identify and describe actionable interactive elements given static webpage representation. Specifically, the model is provided with webpage screenshot and its corresponding DOM tree and is required to generate set of valid user actions (e.g., clicks, text inputs, or selections) that can be performed on the interface. The complete instruction template used for this task is illustrated in Figure 9. Interaction Verification Prompt. To evaluate whether predicted interaction leads to functionally correct state transition, the WebVIA-Agent employs specialized verification prompt. Given sequence of webpage screenshots before and after user actions, the model is required to determine whether the visual and structural changes align with the expected interaction outcome. This prompt guides the agent to reason about the consistency between DOM transitions and visual differences, distinguishing successful interactions (e.g., modals opening, content updates) from failed or redundant ones. The complete verification prompt is shown in Figure 10. A."
        },
        {
            "title": "Interactive Code Generation Evaluation\nPrompts",
            "content": "To rigorously evaluate the functionality and interactivity of the generated code, we design three-stage prompting protocol that aligns with the validation module described in Section 3.3. Each stage corresponds to distinct phase of task-oriented execution and enables consistent benchmarking of action reasoning, process tracking, and outcome verification. (1) Initial Action Selection Prompt. At the beginning of each evaluation episode, the model receives predefined task description (e.g., search for an item, fill out and submit form, or navigate to the contact page) along with the initial webpage screenshot and DOM tree. As shown in Figure 11, the following prompt is used to request the models first interaction decision. (2) Process-State Action Prompt. After execut16 Table 5: Versions and official API endpoints for each evaluated vision-language model. Model Claude-Sonnet-4 Claude-Opus-4 Claude-Sonnet-3.7 GPT-5 o4-mini GPT-4o Gemini-2.5-pro Gemini-2.5-flash Version claude-sonnet-4-20250514-thinking claude-opus-4-20250514-thinking claude-3-7-sonnet-20250219-thinking gpt-5-2025-08-07 o4-mini-2025-04-16 gpt-4o-2024-11-20 gemini-2.5-pro-preview-06-05 gemini-2.5-flash-preview-05-20 API Endpoint / URL https://www.anthropic.com/api https://www.anthropic.com/api https://www.anthropic.com/api https://platform.openai.com/docs/models https://platform.openai.com/docs/models https://platform.openai.com/docs/models https://ai.google.dev/gemini-api/docs/models https://ai.google.dev/gemini-api/docs/models"
        },
        {
            "title": "Action Generation Prompt",
            "content": "You are an interactive web assistant. now want to check whether all interactive buttons on this webpage work properly. For example, if there is search box on the page, please search with reasonable query and click confirm, expecting the page to change. Note that if multiple interactive components are almost identical, please select only one of them. For example, if the page has multiple similar items each with an \"Edit\" button, please choose only once. The current page state is part of the detection process. will send you which components have already been clicked. If you find that an image was clicked before, please focus on what is different in the image send you this time compared with the previous one. For example, if new window has popped up, please make sure to only select interactive components in the new part. If you find that the image send you this time is almost identical to one of the historical ones (for example, all buttons are the same, with only minor text differences), then directly reply with: All operations on this page are completed. Note: If two interactive buttons are not sequentially related (for example, two separate click buttons on the same page), please include only one in each boxed response, separating them. If they are sequentially related (for example, entering multiple values and then clicking confirm), please put them together in the same boxed response. Wrap your answers in LaTeX using boxed{}. Action Format: click[id] = click enter[id][text] = input text select[id][text] = select option Separate each action with comma. DOM elements clicked previously: {history_info_prompt} Important: Please return only the answer! Do not include anything extra! Page Information: {domtree} Figure 9: Prompt used for action generation in WebVIA. ing the initial interaction, the webpage transitions into new state. For each task branch (typically five per task), the model observes the updated screenshot and DOM tree corresponding to the current state and must decide the subsequent action. If the task is incomplete, the following process prompt is used (See Figure 12). (3) Task Completion Verification Prompt. Once task branch reaches termination, we verify whether the task goal has been successfully accomplished. The model receives the full textual task description and the sequence of screenshots collected during its execution. As shown in Figure 13, the following prompt is used for task-level verification. A.9 Demo Cases for Exploration To demonstrate the versatility and robustness of the WebVIA exploration process, we present qualitative examples of the agents behavior in both real-world and synthetic webpage environments. These cases highlight how WebVIA effectively handles complex UI layouts, multi-step operations, and dynamic visual feedback during autonomous exploration. Synthetic Webpage Exploration. Figures 1417 demonstrate the exploration trajectory of WebVIA-Agent on the synthetic webpages. Each figure corresponds to distinct interaction scenario generated within our procedural environment. The agent autonomously identifies visible"
        },
        {
            "title": "Interaction Verification Prompt",
            "content": "You will receive multiple webpage images as part of the verification process for interaction history. The multiple webpage images are arranged in chronological order: the last image represents the completion of the interaction, and the first image is the starting image. screenshot is taken after each operation until the final image completes the operation. For example, if there are only two images, then only one operation was performed: the pre-operation screenshot is the first image, and the post-operation screenshot is the second image. If there are four images, then three operations were performed: the pre-operation screenshot is the first image, after the first operation is the second image, after the second operation is the third image, and so on. Tasks: 1. Interaction Consistency Check: Determine whether the webpage shows changes consistent with the described interactive components after this interaction sequence. For example, clicking Edit should open an editing window; entering values and clicking Save should persist changes; clicking Cancel or Close should not. Carefully compare the final and initial images to infer whether the expected modification occurred. Reply Yes if changes are functionally correct, or No if the images remain largely unchanged. Extract your final answer and place it inside LaTeX boxed{}, followed by your reasoning. 2. Continuation Check: Compare the last image with the starting image to determine whether any new significant part has appeared on the webpage. If new interactive content appears and further checking is needed, wrap Continue with terminate{Continue}. If no new meaningful change is observed (e.g., no new section or trivial modifications), wrap Complete with terminate{Complete}. Both boxed answers and termination tags must be output, with detailed reasoning provided afterward. Interactive Action / Component Name: <interact_element_names> Figure 10: Prompt used for interaction verification during the WebVIA-Agent. interactive components such as buttons, forms, and dropdowns, and performs multi-step actions to manipulate the webpage state. Across these examples, WebVIA-Agent demonstrates its ability to perceive layout hierarchies, maintain consistency between visual and structural states, and accurately capture interaction outcomes. These exploration traces form the foundation for downstream UI2Code synthesis and interaction verification. Real-World Webpage Exploration. Figures 1822 present qualitative demonstrations of WebVIA-Agent exploring real-world webpages collected from open-access sites. Notably, although the agent is trained exclusively within our synthetic environment, it generalizes effectively to complex real webpages without any additional fine-tuning. It can accurately identify functional UI componentssuch as navigation bars, search boxes, and modal dialogsand execute multistep interactions involving both visual reasoning and structural understanding. During exploration, the agent maintains alignment between rendered screenshots and DOM hierarchies, correctly detecting dynamic transitions. These results demonstrate WebVIA-Agents strong zero-shot generalization ability from procedurally generated environments to real web interfaces, validating the robustness and transferability of its visualstructural reasoning process. A.10 Demo Cases for Interactive Code"
        },
        {
            "title": "Generation",
            "content": "To further illustrate the capabilities of WebVIAUI2Code, we showcase qualitative results of interactive UI-to-Code generation. Figures 2326 demonstrate WebVIA-UI2Code-GLM performing interactive code generation in procedurally synthesized environments. Each synthetic webpage is automatically composed of diverse UI components such as navigation bars, cards, modals, and dropdowns, each associated with predefined interaction logic. The model observes sequential webpage states during userinterface interactions and generates executable React + Tailwind code that faithfully reproduces both the visual layout and dynamic behaviors observed in the interaction flow. Furthermore, we conduct comparative study across multiple models, where each model receives the same sequence of multiple UI screenshots as input and is tasked with generating interactive HTML code. As shown in Figures 3029, WebVIA-UI2CodeGLM consistently produces structurally complete and functionally executable webpages, while baseline models often fail to maintain state consistency or omit interaction logic."
        },
        {
            "title": "Action Selection Prompt",
            "content": "You are an interactive web assistant. now want to check whether certain interactive buttons on this webpage are working properly. will give you several tasks. You need to read the current page and select an action sequence for each task. If you think the current page content is insufficient to complete task, please only select the interactive components on the page that can accomplish part of the task. Wrap each of your interactive components in LaTeX boxed{}. Action format: click[id] = click, enter[id][text] = input, select[id][text] = select option. Separate each action with comma. Please note, id refers to the identifier of this component in the DOM tree. At the same time, before each boxed{}, write task{<task name>}, and after each boxed{}, write state{Complete} or state{Continue}. Task list: {str(tasks)} Page information: {domtree} Figure 11: Prompt used in the Validation Module of the WebVIA framework to guide task-specific action selection. Process-State Action Prompt You are an interactive web assistant. now want you to complete task. You are currently in the detection process. Please read which buttons have already been clicked on the historical pages. Only select the buttons on the page that can actually be clicked. You should focus only on completing one task. Read the current page and select an ongoing action sequence for the current task. If you think the current page content is insufficient to complete the task, please only select the interactive components on the page that can accomplish part of the task. Wrap your interactive components with LaTeX boxed{}. Action format: click[id] = click, enter[id][text] = input, select[id][text] = select option. Separate each action with comma. At the same time, before each boxed{}, write task{<task name>}, and after each boxed{}, write state{Complete} or state{Continue}. Task content: {task_text} Page information: {domtree} Figure 12: Action execution prompt used in the Validation Module of the WebVIA framework for determining actionable components and task progress on interactive webpages."
        },
        {
            "title": "Task Completion Verification Prompt",
            "content": "Provide all the screenshots along this path (in chronological order). Task: {task_text} Please determine whether the expected webpage changes for this task have been completed. Important: Each task name corresponds to single interactive button or operation. For example, New only represents opening the new page, not saving. Thus, you only need to verify whether the new page can be opened. Only if the task explicitly specifies New - Input ... - Save do you need to confirm the saving step. Similarly, Delete only refers to opening the delete dialog, while Delete - Confirm Delete represents two operationsonly in the latter case should you check whether the deletion was actually completed. If the operation has been successfully completed, respond with boxed{Yes}; if not completed, respond with boxed{No}. Afterward, briefly explain your reasoning. Figure 13: Prompt used by Validation Module in the WebVIA framework to determine whether task has been successfully completed based on sequential webpage screenshots. 19 Figure 14: Exploration results of the WebVIA-Agent on synthesized web environments. 20 Figure 15: Exploration results of the WebVIA-Agent on synthesized web environments. 21 Figure 16: Exploration results of the WebVIA-Agent on synthesized web environments. 22 Figure 17: Exploration results of the WebVIA-Agent on synthesized web environments. 23 Figure 18: Exploration results of the WebVIA-Agent on real-world web environments. 24 Figure 19: Exploration results of the WebVIA-Agent on real-world web environments. 25 Figure 20: Exploration results of the WebVIA-Agent on real-world web environments. 26 Figure 21: Exploration results of the WebVIA-Agent on real-world web environments. 27 Figure 22: Exploration results of the WebVIA-Agent on real-world web environments. 28 Figure 23: Rendered UI2Code demo for WebVIA-UI2Code-GLM 29 Figure 24: Rendered UI2Code demo for WebVIA-UI2Code-GLM 30 Figure 25: Rendered UI2Code demo for WebVIA-UI2Code-GLM 31 Figure 26: Rendered UI2Code demo for WebVIA-UI2Code-GLM 32 Figure 27: Comparison of WebVIA-UI2Code and baseline renders on the same interaction trace. 33 Figure 28: Comparison of WebVIA-UI2Code and baseline renders on the same interaction trace. 34 Figure 29: Comparison of WebVIA-UI2Code and baseline renders on the same interaction trace. 35 Figure 30: Comparison of WebVIA-UI2Code and baseline renders on the same interaction trace."
        }
    ],
    "affiliations": [
        "Faculty of Mathematics, University of Waterloo",
        "The Knowledge Engineering Group (KEG), Tsinghua University",
        "Zhipu AI"
    ]
}