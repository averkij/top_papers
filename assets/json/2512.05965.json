{
    "paper_title": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor",
    "authors": [
        "Hongyu Li",
        "Manyuan Zhang",
        "Dian Zheng",
        "Ziyu Guo",
        "Yimeng Jia",
        "Kaituo Feng",
        "Hao Yu",
        "Yexin Liu",
        "Yan Feng",
        "Peng Pei",
        "Xunliang Cai",
        "Linjiang Huang",
        "Hongsheng Li",
        "Si Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-based image editing has emerged as a prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and a lack of deliberation. In this work, we propose a deliberative editing framework to 'think' while they edit, which simulates the human cognitive loop by iteratively executing a Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train a single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinker's thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by a large margin. We will release our data construction framework, datasets, and models to benefit the community."
        },
        {
            "title": "Start",
            "content": "EditThinker: Unlocking Iterative Reasoning for Any Image Editor Hongyu Li1,2 Manyuan Zhang2 Dian Zheng2,3 Ziyu Guo2,4 Yimeng Jia2 Kaituo Feng2,3 Hao Yu5 Yexin Liu2 Yan Feng2 Xunliang Cai2 Linjiang Huang1 Hongsheng Li3 Peng Pei2 Si Liu1 1Beihang University 2Meituan 3CUHK MMLab 4CUHK IMIXR 5Tsinghua University Porject Page: https://appletea233.github.io/think-while-edit 5 2 0 2 5 ] . [ 1 5 6 9 5 0 . 2 1 5 2 : r Figure 1. Overview of EditThinker. Subfigure (a) illustrates our multi-turn Think-while-Edit pipeline that iteratively Critiques, Refines, and Repeats the editing instruction, while subfigure (b) reports results on four image editing benchmarks, showing large gains for three existing editing methods and we use the dev version of FLUX.1 Kontext (denoted as FLUX.1 Kontext in the figure)."
        },
        {
            "title": "Abstract",
            "content": "Instruction-based image editing has emerged as prominent research area, which, benefiting from image generation foundation models, have achieved high aesthetic quality, making instruction-following capability the primary challenge. Existing approaches improve instruction adherence via supervised or reinforcement learning, yet single-turn success rates remain limited due to inherent stochasticity and lack of deliberation. In this work, we propose deliberative editing framework to think while they edit, which simulates the human cognitive loop by iteratively executing Think-while-Edit cycle: Critiquing results and Refining instructions , followed by Repeating the generation until satisfactory. Specifically, we train single MLLM, EditThinker, to act as the reasoning engine of this framework, which jointly produce the critique score, reasoning process, and refined instructions. We employ reinforcement learning to align the EditThinkers thinking with its editing, thereby generating more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly improves the instruction-following capability of any image editing model by large margin. We will release our data construction framework, datasets, and models to benefit the community. 1. Introduction Instruction-based image editing aims to edit user-given image following the given instructions, which has wide range of applications in content creation and world simulation. Current state-of-the-art editing methods [2, 28, 30] are typically built by fine-tuning strong image generation foundation models, contributing to excellent aesthetic quality of edited images. Consequently, the primary challenge has instead shifted toward achieving precise instruction-following capability. Instruction-based image editing is emerging as core capability for interactive visual systems, enabling practical applications such as digital content creation, virtual avatar design, and controllable world simulation. Compared to text-to-image generation, this task is inherently more challenging as it requires the model to simultaneously preserve identity, perform localized semantic modifications, and respect long-range visual consistency, all under freeform natural language instructions. Recently, inspired by the remarkable success of reinforcement learning (RL) in eliciting reasoning capabilities [6, 7, 14, 32, 43], the RL paradigm has also been extended to image editing [15, 20]. However, as shown in Figure 1, even after RL, the instruction-following performance in singleProject Leader. Corresponding Author. turn (i.e., Turn1 in the image) remains limited. In practice, single-turn editing model is tasked with jointly performing instruction understanding, visual planning, and content generation within single step. Due to this coupled and one-pass nature, the model is deprived of the opportunity to self-correct intermediate errors, leading to issues such as missing attributes. In essence: Current models mainly act as reactive executor, rather than reflective thinker. In this work, we explore novel perspective: enabling the editing system to think while it edits. Instead of improving the editor model itself, we equip it with Thinker implemented as Multimodal Large Language Model (MLLM) that executes Critique-Refine-Repeat loop. Specifically, the Thinker evaluates the editing result (Critique), refines the instruction based on identified deficiencies (Refine), and resubmits it to the editor for regeneration (Repeat). This pipeline can effectively address instructionfollowing limitations across different models. To validate this concept, we employed GPT-4.1 [8] as an expert Thinker to conduct multi-round instruction iterations on several state-of-the-art editing models (Qwen-ImageEdit [28], Flux-Kontext [2], Omnigen2 [30]). Remarkably, without fine-tuning the editing models, we achieved significant performance improvements across all models. Furthermore, we propose our EditThinker, MLLM with reasoning capability that implements this Think-whileEdit paradigm for any image editor. To achieve this, our framework incorporates two key contributions. First, we train single MLLM as the EditThinker to jointly output the critique score, the refined instruction, and its underlying reasoning process. After supervised fine-tuning (SFT) to adapt to the output format, we employ reinforcement learning (RL) to bridge the Think-while-Edit gap, aligning the EditThinkers planning with the practical capabilities and failure modes of the image edit models. Second, we construct THINKEDIT-140k via comprehensive multi-round instruction refinement framework. This automated pipeline generates tuples of high-fidelity source images, diverse editing requests, and detailed reasoning traces. Extensive experiments on four widely used benchmarks demonstrate the effectiveness of our EditThinker across diverse editing scenarios and edit models, yielding consistent performance improvements in all evaluated settings. We further conduct comprehensive ablation studies to analyze the impact of key components, including the thinking paradigm, the number of reasoning turns, the training strategy, and the choice of expert thinker. In summary, our main contributions are as follows: 1. We identify the limitation within single-turn instructiona novel Think-while-Edit following and propose paradigm, reframing the editing task as an iterative reasoning process. 2. We propose EditThinker, reasoning-driven MLLM trained with SFT and RL to iteratively critique, refine, and re-plan editing instructions. 3. We introduce THINKEDIT-140k, large-scale multiround dataset with unified supervision signals for instruction refinement and reasoning-based training. 4. Extensive experiments on four widely used benchmarks demonstrate the effectiveness of our method across diverse editing scenarios and edit models. 2. Related Work 2.1. Image Editing The emergence of diffusion models marked paradigm shift in Text-to-Image (T2I) synthesis [16, 19, 25, 26, 37]. Image editing, however, imposes stricter constraints to balance attribute modification with background preservation. Early solutions, ranging from inversion-based techniques [10, 21, 22] to explicit spatial controls [39, 41], improved precision but often suffered from computational overhead or limited semantic flexibility. While initial instruction-tuning attempts [3] introduced natural language control, they faced generalization bottlenecks. Recently, the field has advanced towards robust instruction-tuned models [2, 17, 45] and general-purpose Multimodal LLMs or Unifed Model [15, 24, 29, 30], evolving alongside foundational architectures like flow matching [12]. Although the foundational capabilities of editing models continue to improve, their instruction-following ability remains limited due to the inherent stochasticity and lack of deliberation in single-turn editing. In this work, we pioneer multi-round instruction iterative refinement paradigm that achieves performance improvements across any editing model, demonstrating the importance of the multi-round editing paradigm. 2.2. Reward Models for Image Editing Feedback and Reward Modeling in Image Editing. The correlation between Multimodal Large Language Models (MLLMs) and human perception [4, 42] has established the MLLM-as-a-Judge paradigm, facilitating their use as reward models (RMs) for generative tasks [23, 36]. However, translating holistic evaluations into effective training signals for image editing is non-trivial. Early attempts using discrete scores [9] or dense logit-based values [31] often failed to capture the fine-grained nuances required for precise visual modifications. To address these limitations, recent research has pivoted towards domain-specialized reward modeling. Notably, EditReward [33] constructed largescale human preference dataset to train reward model capable of rigorous data filtering and alignment. Building on this, EditScore [20] developed series of specialized reward models that surpass general-purpose VLM judges, successfully unlocking effective online reinforcement learning (RL) for editing policies. Despite these advancements in RL application, fundamental feedback lag remains. Existing specialized RMs primarily provide outcome-oriented feedbackthey evaluate the edited image after generation. This post-hoc signal acts as an external judge rather than an internal guide. In complex editing scenarios requiring multi-step reasoning, such scalar rewards fail to correct the intermediate logic of the generation process [44]. Consequently, an emerging paradigm seeks to utilize MLLMs not merely as judges, In this work, we shift but as internal planners [19, 38]. from maximizing static post-hoc reward to harnessing the MLLMs structured reasoning process to actively guide the editing model during execution. 3. Think-while-Edit To address the inherent limitations of current editing models in single-turn instruction following, we propose Thinkwhile-Edit framework, mimicking the human cognitive process of critique, reflect, and edit during creation. 3.1. Overall Framework Previous methods mainly operates in single turn: given source image Isrc and the origin instruction Ts, the editing model directly produces the final edited image. This process lacks the ability to iteratively refine the output or recover from failed edit. To address this limitation, we introduce MLLM-based Thinker that transforms single-pass editing into an iterative, multi-turn process. Our framework explicitly decouples the editing workflow into two distinct roles: Thinker for judging and reasoning, an Editor for execution, where the Thinker is trained via SFT and RL and the Editor is any existing image editing models (e.g., Qwen-Image-Edit, Flux-Kontext). Specifically, at each iteration t, the Thinker evaluates the previous output t1 edit and generates the instruction following score St, refined instruction Tt and the reasoning process Rt at the same time as: (St, Rt, Tt) = Thinker(Isrc, t1 edit, Tt1.Ts). (1)"
        },
        {
            "title": "Then the Editor executes the new instruction Tt on the",
            "content": "source image Isrc, generating the updated result edit as: edit = Editor(Isrc, Tt). (2) This iterative process, termed the Critique-RefineRepeat cycle, continues until the editing goal is achieved. 3.2. Design of the EditThinker We formulate EditThinker as dual-role model that simultaneously evaluates and plans. Unlike decoupled approaches that use separate models for evaluation (a MLLMbased scorer) and planning (a LLM-based rewriter), EditThinker performs both tasks in single forward pass. Figure 2. The Pipeline of Think-while-Edit. EditThinker is multi-round instruction iterative refinement framework. In the first round, the original image Isrc and instruction Ts are fed into an editor to produce an initial edited image edit. This edited image, along with the original image and instruction, is then fed into EditThinker, which generates the edit score St, refined prompt Tt, and corresponding reasoning process Rt. If the score falls below threshold, the framework proceeds to the next iteration with the refined prompt until satisfactory result is achieved. Our key insight is that effective planning requires deep evaluation: the model must first critique the previous output (generating score St and reasoning Rt) before producing refined instruction Tt. By generating Rt before Tt, EditThinker creates an explicit chain of thought that grounds instruction refinement in the visual critique of Isrc and t1 edit. To implement this dual-role design, we define structhat explicitly encodes the tured input-output evaluation-then-planning process. Input Tuple. EditThinker receives multimodal tuple (Isrc, t1 edit, Ts, Tt1) at each iteration t, providing complete context of the editing state: Isrc and Ts represents the original reference, t1 edit is the current result to be critiqued, and Tt1 is the previous instruction that produced it. Structured Output Format. The output is structured text string that serializes EditThinkers reasoning process: format <think> Reasoning process... </think> <score> [Ssem, Squal] </score> <answer> Refined prompt Tt </answer> Here, Squal is the perceptual quality of t1 edit, and Ssem is the semantic alignment with the original instruction Ts relative to Isrc. Both scores range from 0 to 10. 3.3. Training of EditThinker Training EditThinker to perform this dual-role task requires specialized dataset and multi-stage training strategy. We adopt two-stage approach: first, supervised fine-tuning (SFT) to learn the output format and basic reasoning, followed by reinforcement learning (RL) to optimize instruction refinement based on actual editing feedback. The data construction process is detailed in Section 4. 3.3.1. Supervised Fine-Tuning (Cold Start) Using the expert (GPT-4.1) demonstration dataset (detailed in Sec. 4), the base MLLM learns to adopt our structured I/O format (e.g., <think>, <score>, <answer>), mimic the experts reasoning style, and understand the principles of critiquing and refining instructions. 3.3.2. Reinforcement Learning Tuning (RLT) The SFT model learns how the expert would ideally reason, but this reasoning is not grounded in the practical limitations of real editors. The model has never observed actual editing failures or learned which types of instructions are prone to misinterpretation by specific editors. Consequently, an instruction Tt that appears optimal to the SFT model may still fail when executed by actual editors like Qwen-Image-Edit. This creates gap between ideal reasoning and practical execution. To bridge this gap, we introduce an RL stage that optimizes EditThinker based on actual editing feedback. We employ standard GRPO (Group Relative Policy Optimization) with carefully designed reward function. As defined in Sec. 3.2, EditThinker performs as dual roles agent (i.e., Critic and refiner), we design multi-component reward that provides learning signals for both aspects as follows: Critic Reward. This component trains the EditThinker to be more accurate critic. The model outputs predicted Figure 3. Data construction pipeline of our THINKEDIT. We construct our dataset through four sequential steps: (1) Trajectory Generation: We use several image edit models and expert evaluator GPT-4.1 to iteratively edit image, evaluate it and generates refined instructions until issuing stop token. (2) Trajectory Filter: An edit scorer assigns scores St to each step, retaining only trajectories where max(St>1) S1 and truncating them at the highest-scoring step k. (3) Step-wise Filter: We unroll trajectories into individual training samples pairing inputs (Isrc, t1 edit, Ts, Tt1) with outputs (Rt, Tt), then balance the dataset across task types and score distributions. (4) Data Partition: The filtered data is split for SFT and RL training. scores St (including Ssem and Squal) that should align with the actual quality of the edited result. We employ GPT-4.1 as the critic expert (E) to evaluate the resulting image edit. The critic reward, Rcritic, penalizes the prediction error as: Rcritic = St E(Isrc, edit, Ts). (3) This reward encourages EditThinker to calibrate its selfassessment: overestimating quality (predicting 9 when the actual score is 5) or underestimating both incur penalties. Through this feedback, the model learns to align its internal critique with the actual editing outcomes. Edit Reward. This is the primary reward that trains the EditThinker to be better refiner. It incentivizes the model to generate an instruction Tt that leads to measurable improvement in image quality and instruction following . We use differential reward, comparing the before state (I t1 edit) using the same expert E: edit) and the after state (I Redit = E(Isrc, edit, Ts) E(Isrc, t1 edit, Ts). (4) This reward is positive only if the generated instruction Tt successfully prompted the Editor to produce better image than the previous step. This directly grounds the planning ability of EditThinker in the practical execution results. The final reward Rtotal is as follows: Roverall = αRf ormat + βRcritic + γRedit, (5) where Rf ormat is the basic reasoning format reward, and α + β + γ = 1 . 4. THINKEDIT Dataset To train the EditThinker, we require high-quality dataset that captures the multi-turn Think while Edit cycle. As shown in Figure 3, we designed an automated data construction pipeline to simulate this process, consisting of four sequential steps: Trajectory Generation, Trajectory Filter, Step-wise Filter, and Data Partition. This pipeline allowed us to construct our THINKEDIT-140k dataset. We detail each step below. 4.1. Trajectory Generation The first stage focuses on simulating the multi-turn Think while Edit cycle. The pipeline begins with an Edit Data Pool containing diverse (Isrc, Ts) pairs. At each step t, the edit thinker expert (GPT-4.1) evaluates the current state (based on Isrc, Ts, and t1 edit) and generates new instruction (Tt), reasoning process (Rt) and stop token. Notably, the expert does not output score (St). Instead, it directly determines when to halt the process by issuing stop token. This design choice stems from our finding that single expert struggles to maintain high performance in both task refinement and output scoring simultaneously. If stop token is not issued, the image editor uses the new Tt to produce edit. This loop continues until the expert triggers the stop condition (or max-iteration limit is hit), thus completing full trajectory. 4.2. Trajectory Filter Since the edit thinker expert only generates refined instructions and stop token without quality scores, we employ an additional edit scorer to evaluate each step (t) edit and assign score St. After scoring all steps (S1, . . . , Sn), we apply two-stage filtering process: Filter Failed Trajectories. We retain only trajectories where at least one subsequent step (t > 1) achieves score higher than or equal to the initial step (i.e., max(St>1) S1). Trajectories failing this condition are discarded. Truncate Kept Trajectories. For retained trajectories, we identify the step with the highest score (Sk = max(St1)) and truncate the trajectory to include only steps from 1 to k. All subsequent steps (t > k) are discarded. 4.3. Step-wise Filter Finally, we process the curated trajectories from the Trajectory Filter to create the final training data through two steps: Sample Extraction. First, we unroll the truncated trajectories. Each individual step within trajectory is converted into distinct training sample. This sample pairs an input tuple (Isrc, t1 edit, Ts, Tt1) with its corresponding ground-truth expert output (Rt, Tt). The score St for that step, while retaining the score St as metadata for subsequent filtering. Distribution Balancing. We apply final filtering step to balance the dataset along two dimensions: Task Distribution: We balance samples across different task types (e.g., object removal, color modification, adding items) to ensure uniform coverage. Score Distribution: We normalize samples across score levels to ensure balanced representation of editing quality. ent task types and score distributions. When unrolled, these trajectories yielded 27k step-wise samples, which constitute our RL dataset. SFT Dataset. The SFT dataset is intended to teach the model the correct, stable refinement behavior. We therefore selected samples characterized by low score variance or consistent high quality. These low-fluctuation steps typically represent more straightforward, correct, and reliable refinement examples. This process resulted in separate dataset of 140k step-wise samples for SFT. 5. Experiments 5.1. Experimental Setup Implementation Detail. EditThinker is built upon the Qwen3-VL-8B-Instruct [1]. We perform SFT on our newly constructed THINKEDIT-SFT-140k dataset for one epoch. Key hyperparameters for training include learning rate of 2 105, batch size of 32. And we preform RL on THINKEDIT-RL-10k dataset for one epoch. Key hyperparameters for training include learning rate of 2 106, global batch size of 128, and rollout number(N) of 8 for generation, KL divergence penalty with coefficient of 1 103. MAX PIXELS is set to 1024 1024.The entire training process is conducted on 8 H800 GPUs and takes approximately 48 hours. For inference, we employ our think while edit paradigm with OmniGen2[30], Flux Kontext [dev][2] and Qwen-Image-Edit[28]. Benchmarks and Baselines. To comprehensively validate the effectiveness of our think while edit paradigm, we conduct composite evaluation on four distinct benchmarks: ImgEdit-Bench [40], GEdit-Bench [18] , RISEBench [47], and KRIS-Bench [34]. This suite of benchmarks was chosen for multi-faceted assessment, with RISEBench and KRIS-Bench specifically focusing on evaluating the reasoning capabilities of the edit models. 4.4. SFT and RL Data Split 5.2. Main Results After the Trajectory Filter, we obtained large pool of curated, high-quality trajectories. From this collection, we create two distinct datasets for our Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) phases. The split is based on the principle that SFT requires stable, highquality examples, while RL benefits most from dynamic examples of improvement. RL Dataset. We first identify trajectories that are most valuable for reinforcement learning. The key criterion is high intra-trajectory score variance (i.e., high-fluctuation scores, Var(St) > θ). These trajectories represent challenging cases where the model initially struggled but then managed to improve, providing rich reward signal for learning. We filtered for pool of 10k such high-variance trajectories, while also ensuring this set was balanced across differWe evaluate our EditThinker framework across comprehensive suite of four benchmarks to assess its performance on both general and reasoning-based editing tasks. For general image editing, we use ImgEdit-Bench and GEditBench-EN (results in Table 1). For complex reasoningbased editing, we utilize RISE-Bench and Kris-Bench (results in Table 2). Performance on General Editing. As shown in Table 1, our Think-while-Edit framework consistently and significantly enhances the performance of all base models. On ImgEdit-Bench, EditThinker boosts the Overall score of FLUX.1-Kontext [Dev] from 3.44 to 3.98, OmniGen2 from 3.4 to 3.5, and Qwen-Image-Edit from 4.36 to 4.37. This achieves highly competitive performance, surpassing several state-of-the-art models. This strong performance genTable 1. Comparison of fine-tuning results of different models on our dataset on ImgEdit-Bench. indicates results from our own tests without fine-tuning. Note that the performance of +EditThinker-Expert-GPT4.1 represents the oracle upper bound. Model IP2P [3] AnyEdit [11] UltraEdit [46] OmniGen [35] Step1X-Edit [17] ICEdit [45] BAGEL [5] OmniGen2 [30] Ovis-U1 [27] FluxKontext dev [13] UniWorld-V2 [15] GPT-4o OmniGen2 + EditThinker-8B + EditThinker-Expert-GPT4.1 Flux-Kontext-dev + EditThinker-8B + EditThinker-Expert-GPT4.1 Qwen-Image-Edit + EditThinker-8B + EditThinker-Expert-GPT4.1 Add Adjust Extract Replace Remove Background Style Hybrid Action Overall SC PQ ImgEdit-Bench GEdit-Bench-EN Open-source Models 2.45 3.18 3.44 3.47 3.88 3.58 3.56 3.57 4.13 3.76 4.29 1.83 2.95 2.81 3.04 3.14 3.39 3.31 3.06 3.62 3.45 4.44 1.44 1.88 2.13 1.71 1.76 1.73 1.70 1.77 2.98 2.15 4.32 2.01 2.47 2.96 2.94 3.40 3.15 3.30 3.74 4.45 3.98 4. 1.50 2.23 1.45 2.43 2.41 2.93 2.62 3.20 4.06 2.94 4.72 1.44 2.23 2.86 3.21 3.16 3.08 3.24 3.57 4.22 3.78 4.41 3.55 2.85 3.76 4.19 4.63 3.84 4.49 4.81 4.69 4.38 4.91 1.20 1.56 1.91 2.24 2.64 2.04 2.38 2.52 3.45 2.96 3.83 1.46 2.65 2.98 3.38 2.52 3.68 4.17 4.68 4.61 4.26 4.83 1.88 2.45 2.70 2.96 3.06 3.05 3.20 3.44 4.00 3.52 4. 3.58 3.18 - 5.96 7.66 - 7.36 7.16 - 6.52 8.39 5.49 5.82 - 5.89 7.35 - 6.83 6.77 - 7.38 8.02 3.68 3.21 - 5.06 6.97 - 6.52 6.41 6.42 6.00 7.83 4.61 4.33 2. 4.35 3.66 4.57 4.93 3.96 4. 4.20 - - 7.49 Proprietary Models Think-while-Edit 3.91 3.68 4.21 3.83 3.82 4.08 4.59 4.23 4.47 3.23 2.9 3.28 3.55 3.80 4.01 4.32 4.43 4. 2.03 3.14 3.04 2.18 3.52 3.45 3.79 4.24 4.18 2.84 2.83 3.80 3.91 4.09 4.44 4.57 4.20 4. 3.11 3.16 3.39 2.74 3.88 3.75 3.86 4.21 4.59 3.94 3.88 4.16 3.79 4.09 4.19 4.54 4.44 4. 4.59 4.62 4.61 4.42 4.52 4.59 4.83 4.76 4.81 2.76 2.35 2.97 2.82 3.21 3.73 3.85 3.91 3. 4.69 4.48 3.39 4.18 4.44 4.57 4.7 4.68 4.77 3.41 3.52 3.81 3.44 3.98 4.13 4.36 4.40 4. 6.47 6.59 7.34 6.62 7.59 7.83 8.01 8.30 8.57 7.04 7.16 7.24 7.61 7.63 7.66 7.87 7.86 7. 6.03 6.28 6.78 6.18 7.02 7.19 7.49 7.73 7.90 Table 2. Comparison of model performance on RISE-Bench. indicates results from our own tests with official model checkpoint. Model Temporal Causal Spatial Logical Overall Seedream-4.0 GPT-Image-1 Gemini-2.5-Flash-Image Step1X-Edit Ovis-U1 FLUX.1-Kontext-Dev BAGEL Qwen-Image-Edit BAGEL (w/ CoT) Proprietary Models 12.2 32.2 47.8 12.9 34.1 25.9 Open-source Models 2.2 3.3 5.5 5.6 10.0 17. 0.0 1.2 2.3 2.4 4.7 5.9 Think-while-Edit OmniGen2 + EditThinker-8B + EditThinker-Expert-GPT4.1 FLUX.1-Kontext-Dev + EditThinker-8B + EditThinker-Expert-GPT4.1 Qwen-Image-Edit + EditThinker-8B + EditThinker-Expert-GPT4.1 2.4 4.7 17.6 2.3 11.8 16.5 4.7 10.8 25.9 1.1 7.8 8.9 5.5 17.8 24. 10.0 23.3 32.2 11.0 37.0 37.0 2.0 4.0 13.0 14.0 17.0 21.0 7.0 5.0 8.0 13.0 20.0 33.0 17.0 27.0 40. 7.1 10.6 18.8 3.5 2.4 1.2 1.2 2.4 1.2 1.2 3.5 2.4 1.2 7.1 5.9 2.4 8.2 9.4 10.8 28.9 32. 1.9 2.8 5.8 6.1 8.9 11.9 3.1 3.4 9.2 5.8 14.4 20.6 8.9 17.8 27.5 which tests complex spatial, causal, and temporal reasoning, our EditThinker framework provides stable performance lift for all models. FLUX.1-Kontext [Dev] improves from 5.8 to 14.4, OmniGen2 from 3.1 to 3.4, and QwenImage-Edit from 8.9 to 17.8. Effect of the Expert Models Capability. We also observe that the performance of our framework scales with the capability of the EditThinker (Expert Model) itself. The tables show results for the same base model (e.g., FLUX.1-Kontext [Dev]) paired with different experts, such as EditThinker-8B and the stronger EditThinker (GPT-4.1). On ImgEdit-Bench, EditThinker-8B improves the FLUX score to 3.98, while the stronger EditThinker (GPT-4.1) boosts it even further to 4.13. This pattern holds across other models and benchmarks, demonstrating that using more capable expert model as the thinker directly translates to greater performance enhancement in the final editing results. 5.3. Ablation Study eralizes to the GEdit-Bench-EN dataset, where our method again provides stable gains, improving FLUX.1-Kontext [Dev] from 6.18 to 7.05, OmniGen2 from 6.19 to 6.28, and Qwen-Image-Edit from 7.49 to 7.73. Performance on Reasoning Editing. Crucially, our methods advantages are not limited to general edits; it provides equally consistent improvements on tasks requiring deep reasoning, as detailed in Table 2. On the RISE-Bench, We conduct series of ablation studies to validate the effectiveness of the key components within our EditThinker framework. We use the FLUX.1-Kontext [Dev] model as our baseline and evaluate on GEdit-Bench-EN and ImgEditBench, unless specified otherwise. Think Pattern Analysis We categorize model editing thinking paradigms into two main approaches: Think before Edit and Think while Edit. Think before Edit rewrites an opTable 3. Ablation on GEdit-Bench-EN with Thinking Paradigm Table 5. Ablation on GEdit-Bench-EN and ImgEdit-Bench with Training Stage with Think While Edit"
        },
        {
            "title": "Model",
            "content": "GEdit-Bench-EN"
        },
        {
            "title": "Model",
            "content": "GEdit-Bench-EN ImgEdit-Bench"
        },
        {
            "title": "G SC G PQ G O",
            "content": "Overall FLUX.1-Kontext [Dev] + Think before Edit + Think while Edit + Think before and while Edit 6.62 7.34 7.83 7.75 7. 7.64 7.66 7.60 6.18 6.82 7.19 7.06 FLUX.1-Kontext [Dev] + Qwen-VL3-8B + EditThinker-8B-SFT + EditThinker-8B-RL 6. 6.70 7.55 7.59 7.61 7.60 7.54 7.63 6.18 6.23 6.93 7.02 3. 3.42 3.57 3.95 Table 4. Ablation of turn number on GEdit-Bench-EN Table 6. Ablation on GEdit-Bench-EN with Expert Model in Think-While-Edit pipeline."
        },
        {
            "title": "Model",
            "content": "FLUX.1-Kontext [Dev] Trun 2 Trun 4 Trun 6 Trun 8 GEdit-Bench-EN"
        },
        {
            "title": "G SC G PQ G O",
            "content": "6.62 7.57 7.79 7.85 8.00 7.61 7.55 7.60 7.59 7.61 6.18 6.95 7.13 7.16 7."
        },
        {
            "title": "Model",
            "content": "FLUX.1-Kontext [Dev] + GPT 4.1 + Gemini 2.5 Pro + Doubao 1.5 + GPT-4o GEdit-Bench-EN"
        },
        {
            "title": "G SC G PQ G O",
            "content": "6.62 7.83 7.76 7.36 7.65 7.61 7.66 7.65 7.59 7.67 6.18 7.19 7.11 6.80 7. timized prompt using only the source image, while Think while Edit denotes our proposed iterative reasoning-andediting framework. As shown in Table 3 of the main paper, Think before Edit provides noticeable improvement but is consistently outperformed by Think while Edit. Furthermore, initializing Think while Edit with Think before Edit step leads to performance drop from 7.19 to 7.06. We hypothesize that the initial Think before Edit introduces bias in the first-round reasoning, which results in incomplete information transfer and negatively impacts downstream performance. Effectiveness of Thinking Rounds We first analyze the impact of the iterative refinement loops depth. As detailed in Table 4, the baseline model (equivalent to single pass, or Trun 1) achieves score of 6.18. Introducing our Think While Edit framework with maximum of two turns (Trun 2) immediately provides substantial performance boost to 6.95 O. We observe clear and consistent performance scaling as we increase the maximum number of allowed turns. The score climbs to 7.13 at 4 turns, 7.16 at 6 turns, and reaches peak of 7.30 at 8 turns. This strong positive correlation demonstrates that our framework effectively utilizes deeper, multi-step reasoning, allowing the model to iteratively correct errors and progressively enhance the editing outcome. Analysis on Training Stage We then ablate the contributions of our EditThinker-8B models two-stage training process. Table 5 presents this breakdown. The SFT stage alone (+ EditThinker-8B-SFT) is responsible for significant performance gain, lifting the score from 6.18 to 6.93 and the ImgEdit-Bench Overall score from 3.44 to 3.57. Subsequently, applying the Reinforcement Learning (RL) stage (+ EditThinker-8B-RL) provides an additional and crucial optimization. While it offers modest gain on GEdit-Bench (7.02 O), its impact is most pronounced on the ImgEditBench benchmark, where it elevates the Overall score from 3.57 (SFT) to 3.95 (RL). This demonstrates that SFT is vital for imparting the foundational refinement capabilities, while RL is highly effective in optimizing the experts judgment and fine-tuning its decision-making policy. Ablation of Different EditThinker Expert Finally, we investigate the scalability of our framework by plugging in different expert models, replacing our trained EditThinker8B. The results in Table 6 are striking. The baseline FLUX model scores 6.00 in this setup. When we simply substitute the expert with powerful, off-the-shelf proprietary model like GPT 4.1, the score leaps to 7.19. This result confirms two key insights: 1) Our Think While Edit framework is general and highly scalable paradigm, not limited to our specific trained expert. 2) The frameworks performance is directly and positively correlated with the underlying reasoning and critical capabilities of the expert model employed. 6. Conclusion We propose deliberative editing framework EditThinker that enables image editing models to think while they edit, addressing the limited instruction-following capability caused by inherent stochasticity and lack of deliberation in existing single-turn approaches. Our framework simulates the human cognitive process by iteratively executing Think-while-Edit cycle: Critiquing results, Refining instructions, and Repeating generation until satisfactory outcomes are achieved. Specifically, EditThinker is single MLLM trained to jointly produce critique scores, reasoning processes, and refined instructions. We employ reinforcement learning to align EditThinkers reasoning with actual editing outcomes, enabling more targeted instruction improvements. Extensive experiments on four benchmarks demonstrate that our approach significantly enhances the instruction-following capability of any image editing model by large margin. We release our data construction framework, datasets, and models to benefit the research community."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, arXiv Chunjiang Ge, et al. Qwen3-vl technical report. preprint arXiv:2511.21631, 2025. 6 [2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, 2025. 2, 3, 6 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 3, 7 [4] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal In Fortyllm-as-a-judge with vision-language benchmark. first International Conference on Machine Learning, 2024. 3 [5] Anne de Jong, Sacha AFT van Hijum, Jetta JE Bijlsma, Jan Kok, and Oscar Kuipers. Bagel: web-based bacteriocin genome mining tool. Nucleic acids research, 2006. 7 [6] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [7] Kaituo Feng, Manyuan Zhang, Hongyu Li, Kaixuan Fan, Shuang Chen, Yilei Jiang, Dian Zheng, Peiwen Sun, Yiyuan Zhang, Haoze Sun, et al. Onethinker: All-inone reasoning model for image and video. arXiv preprint arXiv:2512.03043, 2025. 2 [8] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and machines, 2020. 2 [9] Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, and Xinglong Wu. Onereward: Unified mask-guided image generation via multi-task human preference learning. arXiv preprint arXiv:2508.21066, 2025. 3 [10] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv:2208.01626, 2022. 3 arXiv preprint [11] Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, Xiangnan He, and Tat-seng Chua. Anyedit: Edit any knowledge encoded in language models. arXiv preprint arXiv:2502.05628, 2025. 7 [12] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 3 [13] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 7 [14] Hongyu Li, Songhao Han, Yue Liao, Junfeng Luo, Jialin Gao, Shuicheng Yan, and Si Liu. Reinforcement learning tuning for videollms: Reward design and data efficiency. arXiv preprint arXiv:2506.01908, 2025. 2 [15] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. 2, 3, 7 [16] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [17] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 3, 7 [18] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 6 [19] Zheyuan Liu, Munan Ning, Qihui Zhang, Shuo Yang, Zhongrui Wang, Yiwei Yang, Xianzhe Xu, Yibing Song, Weihua Chen, Fan Wang, et al. Cot-lized diffusion: Lets reinforce t2i generation step-by-step. arXiv preprint arXiv:2507.04451, 2025. 3 [20] Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Liu, et al. Editscore: Unlocking online rl for image editing via high-fidelity reward modeling. arXiv preprint arXiv:2509.23909, 2025. 2, 3 [21] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 3 [22] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 60386047, 2023. [23] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. 3 [24] OpenAI. Image generation API. https://openai. com/index/image-generation-api/, 2025. 3 [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3 [26] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [27] Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, et al. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025. 7 [28] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 2, [29] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3 [30] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 2, 3, 6, 7 [31] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. In ICLR, 2024. 3 [32] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. 2 [33] Keming Wu, Sicong Jiang, Max Ku, Ping Nie, Minghao Liu, and Wenhu Chen. Editreward: human-aligned reward model for instruction-guided image editing. arXiv preprint arXiv:2509.26346, 2025. 3 [34] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. [35] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In CVPR, 2025. 7 [36] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Wang, Weiyun Ye, Shihao Geng, Yiren Zhao, Jiaming Li, Cunjian Li, Hang Sun, et al. preferences for text-to-image generation. Neural Information Processing Systems, 2023. 3 Imagereward: Learning and evaluating human In Advances in [37] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. 3 [38] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. [39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 [40] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Imgedit: uniZhiyuan Yan, Bohan Hou, and Li Yuan. fied image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 6 [41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 3 [42] Qihui Zhang, Munan Ning, Zheyuan Liu, Yue Huang, Shuo Yang, Yanbo Wang, Jiayi Ye, Xiao Chen, Yibing Song, and Li Yuan. Upme: An unsupervised peer review framework for multimodal large language model evaluation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 91659174, 2025. 3 [43] Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng. Critique-grpo: Advancing llm reasoning with natural language and numerical feedback. arXiv preprint arXiv:2506.03106, 2025. 2 [44] Yinan Zhang, Eric Tzeng, Yilun Du, and Dmitry Kislyuk. Large-scale reinforcement learning for diffusion models. In European Conference on Computer Vision, pages 117. Springer, 2024. 3 [45] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 3, [46] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 7 [47] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. 6 EditThinker: Unlocking Iterative Reasoning for Any Image Editor"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Kris-Bench Result C.2. Details of EditThinker To further evaluate reasoning-centric editing capability, we additionally report results on Kris-Bench. As shown in Table 7, our method demonstrates strong performance on reasoning-driven edits. We observe consistent performance gains. The Overall Score for FLUX.1 Kontext [Dev] is lifted from 61.81 to 69.53, OmniGen2 from 50.52 to 53.09, and Qwen from 64.43 to 71.91. This further demonstrates the performance improvements achieved by our method on the Reasoning Editing task. B. More Ablation Analysis Multi-round Reasoning for EditThinker. The main paper reports GPT-4.1s multi-round reasoning performance as an approximate theoretical upper bound for the Think while Edit paradigm. Here, we further evaluate the multiround behavior of EditThinker-8B, as presented in Table 8. We observed continuous performance improvement from the baseline to Turn 8, rising from 6.18 to 7.03. The largest performance boost was observed at Turn 2, where the score jumped from 6.18 to 6.90. This is often because the initial prompt performs the worst, so the first refinement brings the most direct improvement. In contrast, the stages after Turn 2 typically involve further reflection on the previously rewritten prompts. EditThinker adopts unified prompt format for both training and inference. This design ensures that the behavior learned during supervision aligns seamlessly with the capabilities required at inference time, enabling the model to (1) evaluate the current result, (2) reason about potential issues, and (3) refine the instruction for the next round. At each iteration t, EditThinker receives multimodal tuple (Isrc, t1 edit , Ts, Tt1) that provides the complete context of the editing state. Here, Isrc and Ts represent the original source image and user instruction; t1 edit denotes the intermediate result from the previous turn; and Tt1 is the specific instruction that produced it. The maximum number of iterations for EditThinker is set to = 5. Based on this input, EditThinker outputs three components: scalar instruction-following score St, naturallanguage reasoning trace Rt, and refined editing instruction Tt. Diverging from prior systems that rely on binary <stop> flag, we implement continuous scoring scheme comprising Semantic Score and Quality Score. This offers two key advantages: (1) it provides smoother, more informative supervision signal for learning nuanced failure patterns; and (2) it enables precise control over inference quality, allowing users to trigger refinement only when the predicted score falls below specific threshold. The full prompt template used for the expert is provided in Figure 7. C. Additional Implementation Details D. Details of ThinkEdit-140K Dataset C.1. Details of EditThinker Expert To supervise EditThinker with high-quality reasoning traces and refined editing instructions, we employ GPT-4 as the expert model. At the t-th editing iteration, we provide the tuple (Isrc, t1 edit , Ts, Tt1) as input to the expert. The model then generates reasoning trace Rt, refined editing instruction Tt, and <stop> flag indicating whether the current edit successfully satisfies the users intent. The maximum number of iterations for this think-while-edit process is set to = 5. The expert prompt is meticulously designed to explicIt reitly encourage multi-step CritiqueRevise cycle. quires the model to: (1) evaluate whether the edited image fulfills the original instruction Ts, (2) identify failure causes through detailed reasoning, and (3) synthesize an improved instruction that corrects these errors without introducing new inconsistencies. The full prompt template used for the expert is provided in Figure 6. We obtain Ts and Isrc from three data sources: OpenGPT4o-Image, ShareGPT-4o-Image, and Pico-Banana-400K. From these sources, we sample 40K, 40K, and 60K editing instances respectively, ensuring that the editing categories are as evenly distributed as possible, resulting in total of 140K raw samples. We divide these samples into three splits and use GPT-4.1 as the EditThinker-Expert, while selecting OmniGen2, FLUX.1 Kontext [Dev], and QwenImage-Edit as the editors. After trajectory filtering, we retain 70K valid trajectories. Among them, 10K trajectories are selected for RL, while the remaining 60K undergo stepwise refinement and filtering, ultimately producing 140K high-quality samples for SFT. Additionally, subset of 27K filtered trajectories is used for RL training. E. Visualization We provide visualizations of the outputs generated by our framework across different settings. As shown in Figure 4, Table 7. Comparison of model performance on Kris-Bench. indicates results from our own tests with official model checkpoint. 0.0 indicates that the model was not evaluated on multi-image editing. Since our method currently does not support multi-image inputs, we excluded the Temporal subset of Factual Knowledge to ensure fair comparison. Model Doubao Step 3o vision Gemini 2.0 GPT-4o InstructPix2Pix OmniGen MagicBrush AnyEdit Emu2 Step1X-Edit HiDream-E1 ByteMorph FLUX.1 Kontext [Dev] OmniGen2 UniWorld-V1 Step1X-Edit v1.1 BAGEL BAGEL-Think Uni-CoT OmniGen2 + EditThinker-8B + EditThinker-Expert-GPT4.1 FLUX.1 Kontext [Dev] + EditThinker-8B + EditThinker-Expert-GPT4.1 Qwen-Image-Edit + EditThinker-8B + EditThinker-Expert-GPT4. Factual Knowledge Conceptual Knowledge Procedural Knowledge Overall Score Attribute Spatial Temporal Average Social Sci. Natural Sci. Average Logical Instruction Average 70.92 69.67 66.33 83.17 30.33 37.92 53.92 47.67 51.50 55.50 52.75 61.17 64.83 59.92 58.17 64.17 64.27 67.42 72.76 60.21 62.18 65.55 71.12 77.82 81. 72.73 78.48 83.70 59.17 61.08 63.33 79.08 21.33 28.25 39.58 45.17 48.83 51.75 49.42 62.00 60.92 52.25 54.50 61.75 62.42 68.33 72.87 54.67 53.92 56.83 67.25 65.50 74.67 73.33 73.83 76. 40.58 63.25 63.92 68.25 0.00 21.83 0.00 0.00 22.17 0.00 0.00 0.00 0.00 54.75 63.00 0.00 42.45 58.67 67.10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Proprietary Models 65.50 66.88 68.19 85. 63.30 66.70 65.26 79.80 Open-source Models 22.56 30.63 42.94 38.56 34.69 44.69 52.56 45.50 48.94 47.56 47.50 52.06 55.40 63.55 70.81 23.33 33.11 41.84 39.26 45.40 45.52 43.31 51.27 53.28 57.36 47.71 53.05 60.26 66.18 71.85 Think-while-Edit 53.60 61.50 63.35 56.60 73.44 77.60 63.50 76.20 81. 58.73 59.98 63.22 70.09 73.73 79.33 72.89 77.24 81.67 61.19 60.88 56.94 80.06 26.56 27.19 38.06 42.94 38.44 49.06 49.25 47.38 50.81 43.12 43.94 55.06 56.01 61.40 66.00 46.76 49.90 55. 58.91 69.04 74.98 60.40 70.69 80.53 62.23 62.32 59.65 81.37 25.59 28.02 39.24 41.88 37.54 48.01 50.05 46.92 50.36 44.20 44.80 54.34 55.86 61.92 67.16 48.42 52.71 57.22 58.35 70.09 75. 61.15 72.02 80.91 47.75 49.06 54.13 71.56 19.81 11.94 30.00 36.56 24.81 40.88 45.19 32.00 46.06 32.50 42.00 52.56 52.54 48.12 53.43 37.67 37.04 44.38 56.75 62.29 71.38 57.47 65.23 71. 60.58 54.92 71.67 85.08 14.75 35.83 23.08 26.92 45.00 22.75 30.08 31.33 39.00 63.08 53.83 36.75 50.56 50.22 73.93 56.67 58.78 60.44 63.72 65.33 65.50 67.97 66.89 76.07 54.17 51.99 62.90 78. 17.28 23.89 26.54 31.74 34.91 31.82 37.64 31.67 42.53 47.79 47.92 44.66 51.69 49.02 63.68 45.84 46.43 51.26 59.75 63.60 68.86 61.70 65.94 73.40 60.70 61.43 62.41 80.09 22.82 28.85 37.15 38.55 39.70 43.29 44.72 44.85 49.54 49.71 50.27 51.59 56.21 60.18 68. 50.52 53.09 57.34 61.81 69.53 74.93 64.43 71.91 79.34 Table 8. Ablation of turn number on GEdit-Bench-EN for EditThinker-8B"
        },
        {
            "title": "Model",
            "content": "FLUX.1 Kontext [Dev] Turn 2 Turn 4 Turn 6 Turn 8 GEdit-Bench-EN"
        },
        {
            "title": "G SC G PQ G O",
            "content": "6.62 7.44 7.51 7.57 7.61 7.61 7.60 7.60 7.62 7.58 6.18 6.90 6.94 7.01 7. EditThinker produces high-quality edits when combined including FLUX.1 Kontext [Dev], with various editors, OmniGen2, and Qwen-Image-Edit. In addition, Figure 5 visualizes EditThinkers iterative reasoning dynamics with FLUX.1 Kontext [Dev], highlighting how the Thinker critiques intermediate results and progressively refines the instruction over multiple rounds. Figure 4. Qualitative visualizations of EditThinker paired with different editors. Subfigures (a) and (b) show results with FLUX.1 Kontext [Dev], (c) and (d) use OmniGen2, and (e) and (f) use Qwen-Image-Edit. Figure 5. Visualization of EditThinkers reasoning traces and intermediate editing results when paired with FLUX.1 Kontext [Dev]. The figure illustrates how the Thinker evaluates the current output, identifies issues, and iteratively refines the instruction over multiple rounds. You are an expert image editing evaluator and prompt engineer. Your task is to: 1. Evaluate whether an edited image successfully fulfills the original user instruction. 2. If not satisfied, generate an improved rewritten prompt that addresses the shortcomings. Input Information. You will receive: Original Image: the input image before editing. Original User Instruction: the users initial editing request. Rewritten Prompt: the refined instruction that was used for editing. Edited Image: the resulting image after applying the rewritten prompt. Evaluation Criteria. A. Intent Alignment Does the edited image achieve the core goal of the user instruction? Are all requested changes present and correctly implemented? B. Quality Assessment Subject/Object Changes: correctness of additions/removals/replacements. Appearance Modifications: accuracy of color/style/material edits. Scene Changes: correctness of background/environment edits. Detail Preservation: important details remain intact. Visual Coherence: edited image looks natural and well-integrated. C. Common Failure Patterns Missing requested elements. Incorrect positioning or scale. Wrong colors or materials. Unnatural blending or artifacts. Lost subject details. Style inconsistency. Text errors (if applicable). Over-editing or under-editing. Evaluation Decision. SATISFIED: the edited image fulfills the original instruction with acceptable quality. Minor imperfections are acceptable if the core intent is achieved. NOT SATISFIED: the edit fails in key aspects. Major elements missing, incorrect, or severe quality issues refinement required. Prompt Refinement Strategy (If Not Satisfied). 1. Identify what went wrong. Compare original instruction rewritten prompt edited result. Identify mismatches between intent and execution. Determine whether the issue is clarity, specificity, or contradiction. 2. Refinement approaches. If vague: add specific descriptors, spatial relations, or context. If contradictory: resolve conflicts and simplify. If important details were lost: explicitly require preservation. If scale/position wrong: add precise location and size cues. If style incorrect: specify textures, lighting, materials. If over/under-edited: specify degree of modification. 3. Leverage all information. Reference visible content in the original and edited images. Retain what worked; correct what failed. Output Format. { is satisfied: reason: requirements. new rewritten prompt: } true/false, Detailed explanation of evaluation. If satisfied, explain why it meets If not satisfied, describe specific shortcomings., Only include if is satisfied is false. If satisfied, set to null. Examples. Example 1: Satisfied { is satisfied: reason: new rewritten prompt: true, null } The edited image successfully adds cat..., Example 2: Not Satisfied Lack of Specificity { is satisfied: reason: new rewritten prompt: false, The rewritten prompt was too vague..., Change the car color to blue... } Input Data: Original User Instruction: Ts Rewritten Prompt Used: Tt1 Images Order: [Original Image, Edited Image] Images: Isrc, t1 edit Figure 6. EditThinker Expert Prompt. The full expert instruction used for EditThinker Expert. At each iteration, the Expert observes (Isrc, t1 edit, Ts, Tt1) and produces stop flag, reasoning, and refined instruction. Images: Isrc, t1 edit Edit Evaluation and Prompt Refinement System. You are an expert image editing evaluator and prompt engineer. Your task is to: 1. Score the edited image from two perspectives and output the result in JSON format. 2. If you think the edited image is not good enough, generate an optimized rewritten prompt that addresses the original shortcomings; if you think it is good enough, output the original rewritten prompt. Input Information. You are shown two images in sequence: Original Image: the input image before editing. Edited Image: the latest edited image generated using the previous instruction. The textual instructions involved in this process are: Original User Instruction : Ts Previous Rewritten Instruction : Tt1 Evaluation Criteria (Score 010). A. Semantic Score (Instruction Following and Preservation). Evaluates how accurately the edit was performed. The edit fails if it either (A) fails to follow the text instruction, or (B) over-edits the image by changing content that was not supposed to be changed. (10) Follows the instruction perfectly and preserves all unchanged content. (0) Fails to follow the instruction or needlessly changes the original scene. B. Quality Score (Naturalness & Artifacts). Evaluates the technical quality of the newly edited image. (10) Looks natural, has no artifacts, and integrates seamlessly. (0) Looks unnatural (wrong shadow, lighting, or sense of distance) or contains severe artifacts (distortion, blurred faces, unusual body parts, disharmony). Prompt Refinement Strategy (if Not Good Enough). When generating new rewritten prompt, follow these steps: 1. What went wrong? Compare original instruction rewritten prompt edited result. Identify gaps between intent and execution. Determine if the issue is clarity, specificity, or contradiction. 2. Refinement approaches. If the rewritten prompt was too vague: add more specific descriptors (exact colors, positions, sizes), include spatial relationships and context, and specify interaction with existing elements. If the rewritten prompt was contradictory: resolve conflicts between requirements, prioritize core intent over secondary details, and simplify complex multi-part instructions. If important details were lost: explicitly state preservation requirements, add maintain [aspect] or preserve [feature] clauses, and reference specific elements from the original image. If positioning/scale was wrong: use more precise spatial descriptors, add relative size/scale indicators, and specify foreground/midground/background placement. If style/appearance was incorrect: use more specific visual vocabulary, add reference to the original images style elements, and include material/texture/lighting specifications. If the edit was over/under-processed: add modifiers such as subtle, gentle, dramatic, or significant, specify the degree of change more clearly, and balance enhancement with naturalness. 3. Leverage all information. Reference what is visible in the original image. Learn from what the previous rewritten prompt missed. Use the edited image as feedback on what went wrong. Maintain what worked, and only modify what needs to be improved. Output Format. The output consists of three parts: 1. statement: analysis process and reasoning. 2. JSON object: scores in different dimensions. 3. prompt: either the optimized rewritten prompt or the original rewritten prompt. An example output is shown below: If it is not good enough, explain the specific If the edited image is good <think> Detailed explanation of evaluation and new rewritten prompt. enough, explain why it meets the requirements. shortcomings. </think> <score> { semantic: [0--10], quality: [0--10] } </score> <answer> Improved rewritten prompt that addresses the identified issues and enhances clarity, specificity, and preservation requirements (if NOT GOOD ENOUGH) Original rewritten prompt (if GOOD ENOUGH) </answer> Figure 7. EditThinker prompt. The unified prompt template used for EditThinkers edit evaluation and instruction refinement. At each iteration, the Thinker observes (Isrc, edit, Ts, Tt1) and produces semantic and quality scores, reasoning, and refined instruction."
        }
    ],
    "affiliations": [
        "Beihang University",
        "CUHK IMIXR",
        "CUHK MMLab",
        "Meituan",
        "Tsinghua University"
    ]
}