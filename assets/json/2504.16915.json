{
    "paper_title": "DreamO: A Unified Framework for Image Customization",
    "authors": [
        "Chong Mou",
        "Yanze Wu",
        "Wenxu Wu",
        "Zinan Guo",
        "Pengze Zhang",
        "Yufeng Cheng",
        "Yiming Luo",
        "Fei Ding",
        "Shiwen Zhang",
        "Xinghui Li",
        "Mengtian Li",
        "Songtao Zhao",
        "Jian Zhang",
        "Qian He",
        "Xinglong Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 1 9 6 1 . 4 0 5 2 : r DreamO: Unified Framework for Image Customization Chong Mou1,2, Yanze Wu1, Wenxu Wu1, Zinan Guo1, Pengze Zhang1, Yufeng Cheng1, Yiming Luo1, Fei Ding1, Shiwen Zhang1, Xinghui Li1, Mengtian Li1, Songtao Zhao1, Jian Zhang2, Qian He1, Xinglong Wu1 1Intelligent Creation Team, ByteDance 2School of Electronic and Computer Engineering, Peking University https://mc-e.github.io/project/DreamO Figure 1: The image customization capability of our proposed DreamO."
        },
        {
            "title": "Abstract",
            "content": "Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing Corresponding author Preprint. Under review. unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct large-scale training dataset that includes various customization tasks, and we introduce feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, full-scale training stage to comprehensively enhance the customization capabilities, and final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions."
        },
        {
            "title": "Introduction",
            "content": "Due to the high-quality image generation and stable performance of diffusion models [14], substantial research efforts focus on controllable generation by leveraging their generative priors [34, 59]. Among these, image customization aims to ensure that generated outputs remain consistent with reference image in specific attributes, such as identity [50, 53, 10], object appearance [18, 42, 12], virtual try-on [6, 29, 48], and style [55, 38, 52]. Despite the abundance of task-specific approaches, developing unified framework for image customization remains challenge. Addressing this challenge has long been pursuit in the controllable generation community. For instance, early research Composer [16] jointly trains diffusion model with multi-condition input, e.g., depth, color, sketch. Subsequently, adding extra control module/adapter [59, 34] based on pre-trained diffusion models greatly saves training costs. Following this idea, several UniControl [39, 60] methods propose to train additional control blocks to support general spatial control on the generation result. However, their control ability is restricted to some simple spatial conditions, and the interactions between conditions are rigid and have control redundancy. Recently, the DiT [36] framework greatly scales up the performance of diffusion models. Based on DiT, OminiControl [47] proposes to integrate image conditions by unified sequence with diffusion latent. It can perform various consistency preserving tasks, e.g., identity, color, and layout. Despite its advantages, OminiControl is trained separately on different tasks, struggling to process multiple conditions and model their interactions. For instance, it has difficulty with content insertion for multiple objects and virtual try-on with specified identities. Recently, OmniGen [54] trains general generation control based on pre-trained large language model [2] (LLMs). UniReal [3] achieves this through video generation pretraining followed by full-model post-training. Although these approaches benefit from the strong understanding ability to better interpret generation intentions, they require an enormous training cost involving hundreds of GPUs. In this paper, we design unified image customization approach based on pre-trained diffusion transformer (DiT). With slight training cost, our method can support various consistency conditions (e.g., identity, subject, try-on, and style) and enables interactions among multiple control types, as shown in 1. All capabilities are deployed on single model. Specifically, we follow the unified sequence conditioning format introduced in OminiControl [47], and introduce routing constraint on the internal representations of DiT during training. This ensures content fidelity and promotes the disentanglement of different control conditions. placeholder strategy is also designed to enable control over the placement of conditions in the generated results. In addition, we construct large-scale training data covering multiple tasks and design progressive training strategy. This enables the model to progressively acquire robust and generalized image customization capabilities. In summery, this paper has the following contributions: 2 We propose DreamO, unified framework for image customization. It achieves various complex and multi-condition customization tasks by training small set of additional parameters on pre-trained DiT model. Based on representation correspondences within the diffusion model, we design feature routing constraint to enhance consistency fidelity and enable effective decoupling in multicondition scenarios. We introduce progressive training strategy that facilitates convergence in multi-task and complex task settings. Additionally, we design placeholder strategy to establish correspondence between textual descriptions and condition images. Extensive experiments demonstrate that our method not only achieves high-quality results across broad range of image customization scenarios, but also exhibits strong flexibility in adapting to multi-condition scenarios."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Diffusion Models As powerful generation paradigm, the diffusion model [14, 7] rapidly dominates the image generation community. Its high quality of generation and stable performance have been successfully applied to various tasks , e.g., text-to-image generation [44, 40, 35, 41] and image editing [13, 32, 56, 33]. To improve generation quality, some strategies are proposed, such as latent diffusion [41] and flow matching sampling [27]. Most early works utilize the UNet architecture as the diffusion backbone. Recently, the diffusion transformer [36] (DiT) architecture has emerged as superior choice, offering improved performance through straightforward scalability. For instance, Stable Diffusion 3 [8] and Flux [1] utilize transformer architecture with unified and sequential text/image input. 2.2 Controllable Image Generation Advancements in diffusion models drive the rapid development of controllable image generation. In this community, text is the most fundamental conditioning input, e.g., Stable Diffusion [41], Imagen [45], and DALL-E2 [40]. To achieve accurate spatial control, some methods, e.g., ControlNet [59] and T2I-Adapter [34], propose adding control modules on pre-trained diffusion models. UniControl [39, 60] propose to unify different spatial conditions with joint condition input. In addition to spatial control, some unspatial conditions are also studied. The IP-Adapter [57] utilizes cross-attention to inject image prompt in the diffusion model to control some unspatial properties, e.g., identity and style. In addition to these representative works, other related attempts [9, 12, 15, 23, 25, 30, 31] also help to broaden the scope of controllable image generation. Recent advancements in DiT-based diffusion models further promote the development of controllable image generation. For instance, in-context LoRA [17] and OminiControl [47] introduce novel approach by concatenating all input tokens (i.e., text, image, and conditions) and training LoRA with task-specific datasets for various applications. Subsequently, OmniGen [54] and UniReal [3] optimize the entire diffusion model in multiple stages on larger-scale training data, achieving improved understanding of input conditions. 2.3 Cross-attention Routing in Diffusion Models Existing studies (e.g., Prompt-to-Prompt [13]) demonstrate that text-visual cross-attention maps inherently establish spatial-semantic correspondence between linguistic tokens and visual generation. Notably, the routing pattern of cross-attention reveals critical property: when computing attention scores between specific condition feature and visual features, the response peaks consistently align with the spatial region of the corresponding subject in the synthesized image. Building upon this observation, UniPortrait [11] constrains the influence region of condition features for identity-specific generation in multi-face scenarios, and AnyStory [12] further extends this approach to subject-driven generation. In this paper, we investigate routing constraints within the DiT architecture. 3 Figure 2: Overview of our proposed DreamO, which can uniformly handle commonly used consistency-aware generation control."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries The Diffusion Transformer (DiT) model [36] employs transformer as the denoising network to refine diffusion latent. Specifically, in the input, the 2D image latent zt Rcwh is patchified into sequence of 1D tokens zt Rc( ), where (w, h) is spatial size, is the number of channels, and is the patch size. The image and text tokens are concatenated and processed by the DiT model in unified manner. Apart from model architectures, more efficient sampling strategies (e.g., Flow Matching [27]) are also proposed. Unlike DDPM [14], Flow Matching conducts the forward process by linearly interpolating between noise and data in straight line. At the time step t, latent zt is defined as: zt = tz0 + (1 t)ϵ, where z0 is the clean image, and ϵ (0, 1) is the Gaussian noise. The model is trained to directly regress the target velocity given the noised latent zt, timestep t, and condition y. The objective is to minimize the mean squared error loss: Ldif = E[(z0 ϵ) Vθ(zt, t, y)2], (1) where Vθ refers to the diffusion model. The DiT framework and Flow Matching are widely used in some recent diffusion models, such as Stable Diffusion3 [8] and Flux [1]. 3.2 Overview An overview of our method is presented in Fig 2. Specifically, we utilize the Flux-1.0-dev [1] as the base model to build unified framework for image customization, e.g., style, identity, subject appearance, and try-on. Given condition images = {C1, ..., Cn}, we first reuse the VAE [22] of Flux to encode the condition image to the same latent space as noisy latent. Subsequently, the 2D latent representations are transformed into 1D token sequences via patchification, and all tokens are concatenated along the sequence dimension before being fed into Flux. To enable the model to incorporate conditional image inputs, we introduce dedicated mapping layer at the input of Flux to encode the corresponding conditional image tokens. The position embedding (PE) of these conditional tokens is aligned with that of the noise latent using Rotary Position Embedding (RoPE) [46]. Inspired by non-overlapping position embedding in OminiControl [47], we extend these embeddings along the diagonal in similar fashion. To distinguish conditional latent from noisy image latent in the latent space, learnable condition embedding (CE) is added to the conditional latent at the input stage. Additionally, to support multi-condition scenarios, learnable index embedding (IE) [3] is introduced and added to each conditional latent based on its associated condition index. Following the design of OminiControl [47], we integrate Low-Rank Adaptation (LoRA) [43] modules into Flux as trainable parameters, enabling the model to better adapt to conditional tasks. 4 Figure 3: Visualization of cross-attention maps in subject-driven image generation. The first row shows results from model trained without routing constraints, while the second row presents results from model trained with routing constraints. Figure 4: Visualization of the placeholder effect. 3.3 Routing Constraint Inspired by UniPortrait [11] and AnyStory [12], in this paper, we design routing constraint in the DiT architecture for general image customization tasks. As illustrated in Fig. 2, within the condition-guided framework, cross-attention exists between condition images and the generation result: = Qcond,iKT , (2) where Qcond,i Rlcond,ic refers to the condition tokens of the i-th condition image. Rlc is the tokens of noisy image latent. The cross-attention map Rlcond,il is dense similarity between the i-th condition image and the generation result. To obtain the global response of the condition image across different locations of the generated output, we average the dense similarity matrix along the lcond,i dimension, resulting in response map Rl, representing the global similarity response of the condition image on the generated result. To visualize the image-to-image cross-attention map in the DiT framework, we conduct toy experiment. Specifically, we use the Subject200k [47] dataset to train the controllable framework in Fig. 2 on the subject-driven generation task. The first row of Fig 3 shows its generation result and inner cross-attention map. Here, we choose three layers (i.e., 2, 8, 14) in the double block of Flux to visualize the cross-attention map. It can be observed that under the DiT framework, the image-to-image attention exhibits spatial pattern, concerning around the target object, albeit with coarse granularity. To constrain the image-to-image attention focus on the specific subject, MSE loss is employed to optimize the attention within DiT across condition images and the generation result: Lroute = 1 nc nl nl1 (cid:88) nc1 (cid:88) j= i=0 5 Mj Mtarget,i2 2, (3) where and represents the condition index and layer index. nc and nl is the number of conditions and number of layers, respectively. Mtarget refers to the subject mask in the target image. In addition to image-to-image routing constraint, we also design placeholder-to-image routing constraint to establish correspondences between textual descriptions and condition inputs. Specifically, for the i-th condition, we append placeholder [ref #i] after the corresponding instance name, e.g., women rom [ref #1] and woman rom [ref #2] is walking in the park.. During the training for multi-condition tasks, we calculate the similarity between the conditional image tokens and the placeholder tokens. The routing constraint ensures that the similarity between the condition image tokens for [ref #i] and itself is 1, while its similarity to other placeholders is 0: Lholder = 1 nc nc1 (cid:88) i=0 Sof tmax(Qcond,i RT ) Bi 2, = [Tr1, Tr2, ..., Trnc ], (4) where Tr1 refers to the feature of [ref #i]. Bi is binary matrix, where the value is 1 when the placeholder matches the condition image, and 0 otherwise. The final loss function of our method is defined as: = λdif Ldif + λroute Lroute + λholder Lholder, where λdif , λroute and λholder are loss weights. Note that in order to allow the model to accommodate regular text input, we introduce normal text without placeholders with probability of 50% during training and discard Lholder accordingly. (5) As shown in the second row of Fig. 3, after training with routing constraint, the attention of the condition image clearly focuses on the target subject, and the generated result shows improved consistency with the reference image in terms of details. In addition to the improved consistency, this strategy also helps the decoupling in multi-reference cases. More details are presented in the ablation study, i.e., Fig. 11. Fig. 4 shows the effect of placeholder, which can control the placement of conditions in the generated result. 3.4 Training Data Construction In this work, we aim to achieve generalized image customization. To this end, we collect training dataset covering wide range of tasks. 3.4.1 Identity Paired Data Since high-quality identity paired data [26] is difficult to collect from the Internet, we adopt the open-source ID customization method PuLID [10] for dataset construction, as we find it exhibits high facial similarity and strong prompt alignment. Specifically, we provide PuLID-SDXL with reference face image and text prompt describing the desired style. This allows PuLID-SDXL to control the style of the generated portrait, resulting in training pairs of the form (reference face, prompt, stylized face). For photo-realistic scenes, we generate two images of the same identity using PuLID-FLUX, which then serve as mutual references. 3.4.2 Subject-driven Data For single-subject driven image customization, we utilize the open-source Subject200K [47] To refctify the absence of character-related conditions, we coldataset as training data. lect set of paired character-related data through retrieval and manual selection. For multi-subject driven image customization, we construct some two-column images through concatenation on the Subject200K dataset. the text prompt becomes: Generate two column image. he lef is {prompt1}. he right is {prompt2}, where prompt1 and prompt2 correspond to the descriptions of the two concatenated images, respectively. To enhance the capability to handle complex scenarios (particularly those involving multiple reference factors of the same type), we select subjects from the same category in certain proportion when constructing multi-object images through image concatenation. In addition, we also employ the open-source X2I-subject [54] dataset for multi-subject driven training. In this case, To enhance the subject-driven generation of human references, we build another dataset with pipeline like MoiveGen [37]. Starting from dataset of long videos, content-aware scene detection is used to 6 Figure 5: Visualization of cross-attention maps in some unseen scenarios. get multiple short video clips. To get the inner-clip instance matching, we use Mask2Former [5] to get the mask of all the people in the key frames and do object tracking in the video clip. To get the cross-clip instance matching, we adopt SigLip [58] embedding of all human objects and do clustering. After that, we use GPT-4o [19] with image and colorful instances-mask input to get the instance grounded caption. 3.4.3 Try-on Data In this task, we construct paired try-on dataset using two sources. portion of the data is collected directly from the web as paired images of models and clothing. For the other part, we first crawl high-quality model images as ground truth, then apply image segmentation to extract the clothing items and construct the corresponding paired data. All collected images undergo manual filtering to remove low-quality samples. Clothing regions are extracted using segmentation algorithm [20, 21]. 3.4.4 Style-driven Data In this paper, we aim to address two types of style transfer tasks: (1) style reference image control with text description of content, and (2) style reference image with content reference image. For the first type of style transfer task, the training data needs to include reference images of the same style along with target images. As shown in Fig. 13, we employ an internal style transfer model (i.e., an SDXL-based model utilizing decoupling strategy similar to InstantStyle [49] to generate images of the same style but different content under the guidance of two distinct prompts. For the second type of task, the training requires style reference images, content reference images, and target images. Here, the target images share the same style as the style reference, while maintaining the same content structure as the content reference image. Based on the Type 1 training data, we construct content reference images by generating natural images corresponding to style images through Canny-guided Flux [1]. More details are presented in A.1. 3.4.5 Routing Mask Extraction To obtain the labels for the routing constraint (i.e., Eq. 3), we extract target subject masks from various types of training data. Specifically, for the X2I-Subject [54] dataset, we use the InternVL [4] model to extract subject descriptions, then employ LISA [24] with the descriptions and target images to generate subject masks. For the Subject200K [47] dataset, InternVL is used to obtain subject names, followed by LISA to predict the corresponding masks. For face data, LISA is directly used to extract face masks from target images. More details are presented in A.1. Although the routing constraint is applied on limited set of training data (i.e., Subject200K [47], face data, and X2I-Subject [54]), our method demonstrates strong generalization to unseen scenarios. As illustrated in Fig. 5, the first row shows that in the try-on task, the condition image accurately localizes the target region. In the second row, under more complex conditions, the model effectively distinguishes the regions associated with different condition images. Specifically, the identity 7 Figure 6: The progressive training pipeline of our method. Left column shows the three training stages of our method. Right column shows the generation capability after the training of each stage. reference image focuses on the face and body, while the image with sunglasses attends directly to the sunglasses region, avoiding the face. 3.5 Progressive Training Process During the experiment, we find that training directly on all the data makes convergence difficult. This is primarily due to the limited capacity of the optimization parameters, which makes it challenging for the model to capture task-specific capabilities under complex data distributions. Furthermore, due to the impact of the image quality of the training data, there is divergence from the generative prior of Flux [1] in the quality of the generation after training. To address these issues, we design progressive training strategy that allows the model to smoothly converge across different tasks while rectifying the impact of training data on the high-quality generation prior of Flux [1]. The training pipeline is presented in Fig. 6. Specifically, we first optimize the model on subject-driven training data to initiate the model with consistency-preserving capability. Note that the Subject200K [47] dataset used in the training is generated by the base model (i.e., Flux), and thus shares similar distribution with the model generation space, which facilitates fast convergence. Since the X2I-subject [54] dataset is synthetically generated by MS-Diffusion [51], lot of training samples contain undesired artifacts and distortions. Therefore, during this warm-up stage, the two-column Subject200K images described in Sec. 3.4.2 are also utilized as part of the training data to facilitate rapid convergence of the multi-subject generation control. The right part of Fig. 6 illustrates that after the first training stage, the model acquires an initial subject-driven generation capability and presents strong text-following performance. In the second training stage, we incorporate all the training data and perform full-data tuning. This allows the model to further converge on all subtasks defined in this work. After the second stage of full-data training, we observe that the generation quality is heavily influenced by the training data, particularly by low-quality training samples. To realign the generation quality with the generative prior of Flux, we design an image quality refinement training stage. Specifically, we utilize Flux to generate around 40K training samples. During training, we use the original images as references to guide the model in reconstructing itself. To prevent copy-paste effects, we drop 95% tokens of reference images. After shot-time optimization, the generation quality improved significantly, achieving alignment with the generation prior of Flux."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Implementation Details In this paper, we adopt Flux-1.0-dev as our base model. The rank of additional LoRA [43] is set as 128, resulting in total parameter increase of 707M. For the training process, we employ the Adam [28] optimizer with learning rate of 4e-5 and train on 8 NVIDIA A100 80G GPUs. The batch 8 Figure 7: The capability of our proposed DreamO in identity-driven image customization. Figure 8: The capability of our proposed DreamO in subject-driven image customization. size is set as 8. The first training stage consists of 20K iterations, followed by 90K iterations in the second stage, and finally 3K iterations in the last training stage. 4.2 Model Capabilities Our proposed DreamO is unified framework capable of handling wide range of image customization tasks. Fig. 7 illustrates identity-driven image generation, where our method allows for both individual-specific customization and compositional identity control. That is, the model can generate images that preserve the identity of single person or blend features from multiple individuals, while allowing precise control over other attributes and scene details through text input. Fig. 8 shows the results of object-driven image customization. One can see that, the proposed DreamO supports both single-subject conditioning and multi-subject composition, demonstrating the ability to incorporate diverse visual elements into unified output while maintaining semantic consistency. Fig. 9 demonstrates our try-on capability. This includes visual try-on for specific identities and imaginative applications to arbitrary subjects, enabling creative reinterpretation of garments and their features. Fig. 10 demonstrates style-driven customization, with style exclusively guided by reference images, and other attributes controlled via text or image inputs, enabling fine-grained multimodal manipulation. The above results further highlight our methods ability to integrate heterogeneous control signalssuch as identity, object, and try-on, enabling more expressive and complex image customization. 9 Figure 9: The capability of our proposed DreamO in try-on image customization. Figure 10: The capability of our proposed DreamO in style-driven image customization. 4.3 Ablation Study Routing constraint. In this paper, we introduce routing constraint into DiT training to enhance generation fidelity and facilitate the decoupling of multi-condition control. To evaluate its effectiveness, we ablate the routing constraint during training, with results shown in Fig. 11. In single-condition generation, its removal leads to degraded reference fidelity, e.g., the clothing color becomes inconsistent with the reference. In multi-condition settings, it causes condition coupling, e.g., features of the two toys are crossed. These results confirm that the routing constraint improves the fidelity and disentanglement of different conditions. Progressive training. To enable the model to better converge on all sub-tasks under complex data distributions and to rectify the impact of training data distribution on generation quality, we design progressive training strategy. The effectiveness of this strategy is demonstrated in Fig. 12. One can see that directly training the model on all datasets leads to suboptimal convergence, particularly in complex tasks such as multi-subject consistency. Warming up on smaller and easier-to-learn dataset (e.g., Subject200K [47]) before joint training improves convergence, but the generation quality is easily influenced by the training data distribution, deviating from the generation priors of Flux. By introducing an image quality tuning stage, the model can produce higher-quality generation results. 10 Figure 11: The ablation study of routing constraint in our proposed DreamO."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present DreamO, unified framework for generalized image customization across various condition types (e.g., identity, style, subject, and try-on) within single pre-trained DiT framework. We first construct large-scale training dataset encompassing wide range of image customization tasks. By integrating all condition types into the DiT input sequence and introducing feature routing constraint, DreamO ensures high-fidelity consistency while effectively disentangling heterogeneous control signals. To address the challenges of learning strong task-specific capabilities under complex data distributions while preserving the generative priors of the base model (i.e., Flux), we design progressive training strategy. This approach enables the model to gradually acquire diverse control capabilities while maintaining the image quality of the base model. Extensive experiments demonstrate that DreamO can perform various image customization tasks with high quality. Furthermore, its lightweight LoRA-based design allows for efficient deployment with low computational cost. Figure 12: The ablation study of progressive training in our proposed DreamO."
        },
        {
            "title": "References",
            "content": "[1] https://github.com/black-forest-labs/flux?tab=readme-ov-file. [2] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [3] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. [4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [5] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Maskedattention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [6] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for authentic virtual try-on in the wild. In European Conference on Computer Vision, pages 206235. Springer, 2024. [7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [10] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in Neural Information Processing Systems, 37:36777 36804, 2024. [11] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving single-and multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. [12] Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, and Liefeng Bo. Anystory: Towards unified single and multiple subject personalization in text-to-image generation. arXiv preprint arXiv:2501.09503, 2025. [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-toprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [15] Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. Dreamtuner: Single image is enough for subject-driven generation. arXiv preprint arXiv:2312.13691, 2023. [16] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778, 2023. [17] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [18] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: narrowing real text word for real-time open-domain text-to-image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74767485, 2024. [19] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [20] Zhenchao Jin. Sssegmenation: An open source supervised semantic segmentation toolbox based on pytorch. arXiv preprint arXiv:2305.17091, 2023. [21] Zhenchao Jin, Xiaowei Hu, Lingting Zhu, Luchuan Song, Li Yuan, and Lequan Yu. Idrnet: Interventiondriven relation network for semantic segmentation. Advances in Neural Information Processing Systems, 36, 2024. [22] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. [24] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [25] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36:30146 30166, 2023. 12 [26] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86408650, 2024. [27] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [28] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [29] Junsheng Luan, Guangyuan Li, Lei Zhao, and Wei Xing. Mc-vton: Minimal control virtual try-on diffusion transformer. arXiv preprint arXiv:2501.03630, 2025. [30] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [31] Yuhang Ma, Wenting Xu, Jiji Tang, Qinfeng Jin, Rongsheng Zhang, Zeng Zhao, Changjie Fan, and Zhipeng Hu. Character-adapter: Prompt-guided region control for high-fidelity character customization. arXiv preprint arXiv:2406.16537, 2024. [32] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [33] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. [34] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. [35] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pages 1678416804. PMLR, 2022. [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [37] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2025. [38] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86938702, 2024. [39] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. [40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 13 [42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [43] Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning, 2023. [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-toimage diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. [45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [47] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [48] Zhenchen Wan, Dongting Hu, Weilun Cheng, Tianxi Chen, Zhaoqing Wang, Feng Liu, Tongliang Liu, Mingming Gong, et al. Mf-viton: High-fidelity mask-free virtual try-on with minimal input. arXiv preprint arXiv:2503.08650, 2025. [49] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. [50] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [51] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. [52] Zongze Wu, Yotam Nitzan, Eli Shechtman, and Dani Lischinski. Stylealign: Analysis and applications of aligned stylegan models. arXiv preprint arXiv:2110.11323, 2021. [53] Guangxuan Xiao, Tianwei Yin, William Freeman, Frédo Durand, and Song Han. Fastcomposer: Tuningfree multi-subject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. [54] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [55] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. [56] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023. [57] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [58] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [60] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and KwanYee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:1112711150, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Data Construction Style-driven data. The pipeline for constructing style transfer training data in this paper is shown in Fig. 13. First, we employ an internal style transfer model to generate paired images with the same style but different content, using different prompts under the same style reference. These paired images form the style transfer training data driven by the style reference image. Then, we use Canny-guided Flux [1] to generate natural image with the same content by referencing the edge of one of the style images, thereby forming style-reference and content-reference-driven style transfer training dataset with two style images. Figure 13: The pipeline for data construction of style transfer. (Style 1, Style 2) constitutes stylization training pair guided by style reference images. (Style 1, Style 2, Content Image) constitutes stylization training triplet guided by both style reference images and content reference image. Routing mask extraction. The pipeline for constructing style transfer training data in this paper is shown in Fig. 14. First, for the multi-subject dataset X2I-Subject [54], we utilize the VLM model InternVL [4] to extract descriptions of each target subject. Then, we use LISA [24] with the subject descriptions and target images as input to extract the masks of the target subjects in the target images. For the Subject200K dataset [47], we employ InternVL to obtain the name of the subject, followed by LISA to predict the mask of the subject with the name in the target image. For face data, we directly use LISA to extract face masks from target images. Figure 14: The pipeline for subject mask extraction from target images. The processing data includes X2I-Subject [54], Subject200K [47], and face data."
        }
    ],
    "affiliations": [
        "Intelligent Creation Team, ByteDance",
        "School of Electronic and Computer Engineering, Peking University"
    ]
}