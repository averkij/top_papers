{
    "paper_title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective",
    "authors": [
        "Yifan Zhong",
        "Fengshuo Bai",
        "Shaofei Cai",
        "Xuchuan Huang",
        "Zhang Chen",
        "Xiaowei Zhang",
        "Yuanfei Wang",
        "Shaoyang Guo",
        "Tianrui Guan",
        "Ka Nam Lui",
        "Zhiquan Qi",
        "Yitao Liang",
        "Yuanpei Chen",
        "Yaodong Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under a single framework: vision and language inputs are processed by a series of VLA modules, producing a chain of \\textit{action tokens} that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains a lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer a synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence."
        },
        {
            "title": "Start",
            "content": "2025-7-3 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Yifan Zhong1,2*, Fengshuo Bai2*, Shaofei Cai1,2, Xuchuan Huang1,2, Zhang Chen1,2, Xiaowei Zhang1,2, Yuanfei Wang2,3, Shaoyang Guo1,2, Tianrui Guan1,2, Ka Nam Lui1,2, Zhiquan Qi1,2, Yitao Liang1,2, Yuanpei Chen1,2 and Yaodong Yang1,2 1Institute for AI, Peking University, 2PKU-PsiBot Joint Lab, 3School of Computer Science, Peking University The remarkable advancements of vision and language foundation models in multimodal understanding, reasoning, and generation has sparked growing efforts to extend such intelligence to the physical world, fueling the flourishing of vision-language-action (VLA) models. Despite seemingly diverse approaches, we observe that current VLA models can be unified under single framework: vision and language inputs are processed by series of VLA modules, producing chain of action tokens that progressively encode more grounded and actionable information, ultimately generating executable actions. We further determine that the primary design choice distinguishing VLA models lies in how action tokens are formulated, which can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. However, there remains lack of comprehensive understanding regarding action tokens, significantly impeding effective VLA development and obscuring future directions. Therefore, this survey aims to categorize and interpret existing VLA research through the lens of action tokenization, distill the strengths and limitations of each token type, and identify areas for improvement. Through this systematic review and analysis, we offer synthesized outlook on the broader evolution of VLA models, highlight underexplored yet promising directions, and contribute guidance for future research, hoping to bring the field closer to general-purpose intelligence. Figure 1 We present unified framework of VLA from an action tokenization perspective. Action token refers broadly to any descriptive guidance iteratively generated by VLAs that ultimately leads to action execution, extending beyond the notion of raw action. 5 2 0 J 2 ] . [ 1 5 2 9 1 0 . 7 0 5 2 : r Equal contribution. Corresponding author(s): Yuanpei Chen{yuanpei.chen312@gmail.com},Yaodong Yang{yaodong.yang@pku.edu.cn}. 2025 PKU-PsiBot Joint Lab. All rights reserved Survey on Vision-Language-Action Models: An Action Tokenization Perspective"
        },
        {
            "title": "Executive Summary",
            "content": "VLA Unified Framework and Action Token Taxonomy. Current VLA models can be unified under single framework: vision and language inputs are processed by series of VLA modules to produce chain of action tokens that progressively encode more grounded and actionable information, ultimately generating executable actions. Core to this framework, action tokens can be categorized into language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. Action tokens in VLAs are generalized counterparts to language tokens in LLMs. Action Token Trends. The future of VLA models lies not in single dominant action token, but in their strategic synthesis. Language motion, limited in expressiveness, is unlikely to become mainstream, while language plans remain essential for task decomposition. Code is powerful alternative whose potential will be unlocked by building comprehensive function libraries that integrate perception and action primitives to solve complex, long-horizon tasks. key synergy is forming between affordances that provide semantic what-to-do guidance and trajectories that define precise how-to-do paths. This pairing is powerfully supported by world models, which can predict visual goal states to ground the generation of both token types. Latent representations are promising but face training challenges. Raw actions represent the ideal for end-to-end learning but remain limited by data availability. Finally, reasoning serves as meta-token to enhance all others, evolving from purely language-based to action-token-based reasoning with multimodal feedback and adaptive test-time computation. Emerging Action Token Types. Action token types are shaped by foundation model capabilities. Stronger models and new modalities (e.g., audio, tactile) will give rise to new token types and subtypes. VLA Architecture Trends. Effective VLA models are likely to adopt hierarchical architecture, with the top layer using language description and code to perform long-horizon planning and logic control. In the near term, the lower layers are expected to closely integrate video prediction of goal state, flow modeling of trajectory, and 3D interaction prediction of affordance to form intermediate motion representations, which are finally mapped to raw actions. In the long term, the lower layers evolve toward fully end-toend approach, directly predicting raw actions from subtask-level inputs. Reasoning is always integrated throughout VLA models as needed. From Imitation to Reinforcement Learning. By incorporating reinforcement learning, VLA models can overcome the limitations of imitation learning and achieve more human-like trial-and-error and self-guided exploration. However, real-world deployment requires more efficient RL algorithms to address high reset costs and low interaction efficiency. Additionally, VLMs can automate the generation of dense reward functions, accelerating model training and deployment. From VLA Models to VLA Agents. conscious effort should be made to evolve from VLA models to VLA agents, which are proactive systems that enhance perception-action capability with broader cognitive architecture of memory, exploration, planning, and reflection. This shift also entails transitioning from the current linear processing architecture to more complex, bidirectional, and graph-structured topologies. The Triad of Progress: Model, Data, and Hardware. Embodied AI aims to handle the unstructured, open-ended nature of the physical worldan ambition that demands synergy among models, data, and hardware. Despite this, progress is largely limited by constrained robotic platforms and scarce highquality embodied data, forcing most research into simplified lab settings far from real-world complexity. As result, the field remains in its infancy. Achieving robust, general-purpose intelligence requires the co-evolution of model, data, and hardware, advancing in tandem rather than in isolation. Safety and Alignment. Current VLA research primarily focuses on model capability. Future work must place greater emphasis on ensuring safety and human alignment. 2 Survey on Vision-Language-Action Models: An Action Tokenization Perspective"
        },
        {
            "title": "2 The Evolution of Language and Vision Foundation Models",
            "content": "2.1 Language Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Vision Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Vision-Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Embodied VLA Models as the Next Frontier . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Language Description as Action Tokens",
            "content": "4.1 Progress and Key Papers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Advantages of Language Descriptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Discussion and Future Directions"
        },
        {
            "title": "5 Code as Action Tokens",
            "content": "5.1 Evolution of Code-Based Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Brittleness and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Affordance as Action Tokens 6.1 Keypoints: Precise Interaction Anchors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Bounding Boxes: Coarse Grounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Segmentation Masks: Pixel-Level Regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Affordance Maps: Dense Spatial Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.5 Discussion and Future Directions 7 Trajectory as Action Tokens 7.1 Overview of Trajectories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Progress and Key Papers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Trajectory-Related Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Discussion and Future Directions 8 Goal State as Action Tokens 8.1 Single-Frame Image as Goal State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Multi-Frame Video as Goal State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.3 Advantages of Goal State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.4 Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Latent Representation as Action Tokens 9.1 Vision-Based Latent Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.2 Action-Based Latent Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.3 Goal-Based Latent Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.4 Advantages of Latent Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9.5 Limitations and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Raw Action as Action Tokens 10.1 Vision-Language Feature Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.2 Transformer-Based Generalists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.3 Autoregressive Robot VLA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.4 Video Pretraining and Robot Data Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.5 Diffusion-Based Action Chunking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.6 Heterogeneous Datasets and Unified Action Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.7 Recent Advancements 10.8 Conclusions and Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 6 9 10 11 12 14 14 16 17 17 17 18 18 19 20 21 22 22 22 23 23 24 25 25 26 27 27 28 28 29 30 30 31 31 31 32 32 32 35 35 36 36"
        },
        {
            "title": "12 Scalable Data Sources",
            "content": "12.1 Bottom Layer: Web Data and Human Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.2 Middle Layer: Synthetic and Simulation Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 12.3 Top Layer: Real-World Robot Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "13 General Discussions and Future Directions",
            "content": "13.1 Trends of Action Tokens and VLA Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.2 From VLA Models to VLA Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.3 From Imitation Learning to Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 13.4 From Restrictive Hardware to Full Dexterity and Modalities . . . . . . . . . . . . . . . . . . . 13.5 From Capability-Centric to Safety-Aware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13.6 From Data Scarcity to Data Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "14 Conclusion",
            "content": "37 37 38 39 39 39 40 40 42 42 43 43 44 44 45 45 45 Survey on Vision-Language-Action Models: An Action Tokenization Perspective 1. Introduction In recent years, Artificial Intelligence (AI) has made remarkable strides toward general-purpose intelligence. Central to this progress is the emergence of foundation models [1, 2]large neural networks trained on internet-scale data, which acquire broad and transferable capabilities by capturing the diverse knowledge and patterns embedded in their training corpora. As prominent example, Large Language Models (LLMs), such as GPT-4 [3] and DeepSeek-R1 [4], excel at natural language understanding, reasoning, and generation, forming the backbone of many text-based applications. In parallel, Vision Foundation Models (VFMs), such as CLIP [5], DINO [6, 7], and SAM [8, 9], have shown strong generalization across wide range of vision tasks. Building upon these, Vision-Language Models (VLMs), exemplified by GPT-4o [10], Gemini 2.5 Pro [11], and Qwen2.5-VL [12], integrate visual and textual modalities to enable multimodal processing and generation. Collectively, these models encode vast world knowledge, exhibit strong performance on complex tasks, and generalize to novel scenariosmaking them highly versatile and broadly applicable across domains. However, despite their impressive capabilities, these models remain confined to the digital world, limiting the impact on real-world tasks. To overcome this boundary, researchers have begun exploring ways to harness the perceptual and cognitive capabilities of foundation models to enhance task execution, thereby extending their intelligence into the physical world. This line of work has led to the emergence of Vision-Language-Action (VLA) models, which we formally define as models that generate actions conditioned on visual and linguistic inputs, and are built upon at least one large-scale vision or language foundation model. For example, SayCan [13], PaLM-E [14], and Code as Policies [15] utilize the language and code generation abilities of LLMs and VLMs to produce high-level action plans expressed in natural language or executable code, which are then interpreted and executed by low-level controllers. Other works focus on extracting actionable knowledge from foundation models, such as generating affordances for task-relevant objects [16] or predicting scene-level trajectories to guide downstream control [17]. separate line of research purposefully constructs latent representations of embodied action sequences via dedicated pretraining, and adapts VLMs to predict these representations, which are subsequently decoded and executed by policy controller [18]. In addition, parallel efforts have sought to extend the scaling laws [19, 20] observed in vision and language domains to the embodied setting, collecting large-scale embodied datasets and training generalist agents end-to-end on top of vision-language foundation models [21, 22, 23]. These diverse approaches have led to rapid proliferation of VLA models in robotic manipulation [24, 25], navigation [26, 27], and autonomous driving [28, 29, 30], demonstrating promising capabilities in multitask learning [31], long-horizon task completion [22], and strong generalization [32]. By leveraging foundation model intelligence, they offer new directions for addressing long-standing challenges in embodied AI, such as data scarcity and poor cross-embodiment transferability, and pave the way for agents capable of solving open-ended tasks expressed via open-vocabulary instructions in open-world physical environments. The rapid progress, promising empirical results, and growing diversity of VLA models create an urgent need for timely and systematic review to inform and guide future research. This need is further underscored by the underlying commonalities across seemingly disparate architectures. We observe that existing VLA models can generally be abstracted into unified framework: vision and language inputs are iteratively processed through sequence of VLA modules, producing chain of action tokens that gradually encode increasingly informative and actionable guidance, ultimately producing executable actions. Formally, we define VLA modules as maximal differentiable subnetworks in VLA models that support end-to-end gradient flow, or non-differentiable functional units such as motion planning. If multiple neural components are connected and jointly optimized, they are regarded as parts of the same module. Following the naming convention of language and image tokens in VLMs, we refer to the outputs of VLA modules as action tokens. Additionally, we also consider semantically meaningful intermediate representations within VLA modulessuch as latent representations constructed via dedicated pretraining [18] and goal images [33]as action tokens. Figure 1 illustrates the instantiations of VLA modules and action tokens in several representative VLAs, highlighting how they can be uniformly viewed, explained, and understood with our proposed framework. From this perspective, VLA models are primarily distinguished by how action tokens are formulated and organized. These tokens can be categorized into eight types: language description [24, 31], code [15, 34], affordance [16, 35], trajectory [36, 37], goal state [38, 33], latent representation [39, 18], raw action [21, 22], and reasoning [40, 41]. Figure 2 visualizes their common forms using single embodied task. Crucially, the design of action tokens shapes nearly every aspect of VLA models, including the choice of foundation model, data requirements, training and inference efficiency, interpretability, scalability, and applicability across tasks and environments. As such, action tokenization is central to the design of VLA models and necessitates thorough understanding. 5 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Despite its importance, the research community currently lacks systematic and in-depth understanding of action tokenization. This survey aims to fill that gap by providing structured overview of VLA research from the perspective of action tokenization. We begin by reviewing the evolution of vision and language foundation models, examining their design choices, scaling strategies, and capabilities. We then discuss the transition to embodied AI, in particular VLA models, and establish VLA as the next frontier (Section 2). With this background, we start introducing the VLA research landscape by presenting an overview of action tokens, including their taxonomy, definitions, comparisons, and organizational patterns within VLA models (Section 3). Subsequent sections delve into each major category of action tokens, analyzing their motivations, representative approaches, properties, advantages, limitations, and future work (Sections 4 to 11). We also summarize scalable data sources to inform and support future research (Section 12). Finally, based on the surveyed landscape and emerging trends, we outline future research directions for advancing the field of VLA (Section 13). Through this perspective, we hope to offer valuable insights and actionable guidance for the development of next-generation embodied AI systems. 2. The Evolution of Language and Vision Foundation Models This section first reviews the major advances in the evolution of language foundation models (LFMs, Section 2.1), vision foundation models (VFMs, Section 2.2), and vision-language models (VLMs, Section 2.3), elucidating their progress in terms of capabilities, technical innovations, and methodological approaches. Subsequently, we discuss the fields progression towards embodied AI, analyzing the significantly greater complexity of this domain and establishing embodied VLA as the next frontier. 2.1. Language Foundation Models The emergence of language foundation models can be largely traced back to the introduction of the Transformer architecture [42], which leverages multi-head self-attention and cross-attention mechanisms for scalable sequence modeling, and adopts an encoder-decoder structure for effective sequence-to-sequence generation. Building on this architecture, BERT [43] pretrains bidirectional Transformer encoder in self-supervised manner using masked language modeling and next sentence prediction objectives on large-scale unlabeled corpora, enabling the model to learn rich, context-aware representations that significantly improve downstream task performance. The Universal Sentence Encoders [44, 45] similarly employ Transformer encoders to learn transferable sentence-level encodings. T5 [46] retains the encoder-decoder structure, reformulates all natural language processing tasks into unified text-to-text format, and pretrains on the large-scale C4 dataset. Its pretrained encoder is widely used to produce high-quality language encoding for open-vocabulary inputs [23]. In contrast, GPT models [47, 48, 49] formulate all NLP tasks as next-token prediction, motivating the use of decoder-only Transformer architectures, also referred to as causal or autoregressive Transformers. By scaling model size to 175 billion parameters and pretraining on internet-scale corpora, GPT-3 [49] demonstrates impressive capabilities in language understanding and generation. More notably, it exhibits emergent behaviors such as in-context learning, where the model can perform tasks based solely on few examples provided at inference time. This demonstrates that the scalability of model architecture, training objectives, and data sources enables learning to be effectively applied at scale, resulting in general-purpose models that outperform task-specific systems. This paradigm shift aligns with the core insight of the Bitter Lesson [50] and marks the beginning of the large language model (LLM) era. To guide the efficient scaling of LLMs, scaling laws have been proposed to characterize predictable relationships between model size, data volume, compute requirements, and pretraining loss [20, 19]. These insights inform practical decisions about model design and resource allocation during large-scale training. InstructGPT [51] further advances the alignment of LLMs with human intent by applying supervised fine-tuning (SFT) on instruction-following datasets, followed by reinforcement learning from human feedback (RLHF). Since then, alignment techniques have been extensively studied to ensure that large AI models behave in accordance with safety considerations, human preferences, and values [52]. These technological advances have led to the development of highly capable commercial LLMs, such as GPT4 [3] and Claude, which demonstrate strong performance in open-ended dialogue, code generation, and chain-of-thought reasoning [53]. Orchestrated into an evolutionary coding agent called AlphaEvolve [54], Gemini 2.0 Flash and Gemini 2.0 Pro have jointly enabled remarkable breakthroughs in open scientific problems, including matrix multiplication. However, due to their closed-source nature and restricted API-based access, these models are difficult to inspect, fine-tune, or integrate into broader research and development workflows. 6 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Figure 2 Visualization of action tokens in single embodied task. Given the same vision and language inputs, different VLA models encode them into diverse action tokens, each conveying varying forms of actionable guidance and requiring distinct strategies for token generation and post-processing. 7 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Figure 3 Evolution timeline of foundation models, VLA models, and data sources. The U-shape reflects how the growing proliferation of VLA is supported by progress in foundation models and data. 8 Survey on Vision-Language-Action Models: An Action Tokenization Perspective To address these limitations, number of open-source LLMs, such as Llama [55, 56, 57], Gemma [58, 59, 60], and Mistral [61], have been released, with model sizes ranging from 2B to 70B parameters to accommodate varying requirements. Built on top of these models, parameter-efficient fine-tuning (PEFT) techniques [62, 63, 64, 65, 66, 67], such as LoRA [68], enable task-specific adaptation using significantly fewer trainable parameters and lower computational cost, making fine-tuning feasible in resource-constrained settings. To further scale model capacity without proportional increases in computation, Mixture-of-Experts (MoE) architectures have been introduced into LLMs, as exemplified by Switch Transformer [69] and Mixtral [70]. MoE models activate only subset of expert sub-networks for each input, allowing for significantly larger effective model capacity while maintaining efficient inference. Meanwhile, to address the quadratic time complexity of the Transformer architecture, alternative designs such as Mamba [71] have been proposed. Mamba replaces self-attention with selective state space updates, enabling linear-time sequence modeling while preserving strong performance across long contexts. Another line of work improves reasoning capabilities by scaling test-time computation. For example, OpenAI o1 [72] and DeepSeek-R1 [4] dynamically allocate computational resources during inference to enhance performance on complex reasoning tasks. In particular, DeepSeek-R1 acquires this capability through largescale reinforcement learning based on GRPO [73]. Finally, significant progress has been made in optimizing the infrastructure for training and deploying LLMs. range of parallelism strategies, including data parallelism [74], model parallelism [75], pipeline parallelism [76], and tensor parallelism [77], are actively used to scale training across distributed compute environments. Additionally, inference acceleration techniques such as model quantization, weight pruning, and speculative decoding have been developed to reduce latency and computational overhead during deployment. These advancements have made LLMs highly capable in knowledge, dialogue, code, and reasoning, while also enabling efficient training, deployment, and fine-tuning through mature infrastructure. They not only improve the usability of LLMs, but also support the development of vision and multimodal systems, forming key building blocks for embodied VLA models. 2.2. Vision Foundation Models Following the success of Transformer in the language domain, the computer vision community has begun to replace convolutional neural networks [78, 79, 80] with Vision Transformer (ViT) [81] as the default backbone of vision models to attain better performance when trained with large-scale datasets. This architectural shift naturally treats images as sequences of visual tokens, representational format that allows visual inputs to be handled similarly or jointly with textual inputs, facilitating cross-modal alignment and fusion in subsequent multimodal models. Additionally, the scalability of LLM training has also inspired researchers to explore scalable learning objectives in visual learning, in order to train general models on internet-scale visual data without human-annotated labels. As an early and successful attempt, CLIP [5] utilizes natural language supervision for image representation learning by training on 400 million image-text pairs with contrastive loss. This enables CLIP to learn robust and generalizable image representations and show impressive zero-shot transfer capabilities. SigLIP [82] improves upon CLIP by replacing the original softmax operations with sigmoid loss, boosting training efficiency and enhancing performance. Both CLIP and SigLIP have been widely employed as image encoders [83, 84], especially in scenarios requiring multimodal understanding, due to their joint training with textual supervision. However, relying on textual supervision also constitutes their limitations. Since textual descriptions are often high-level and abstract, the encoded image features of CLIP and SigLIP could lack complex pixel-level information, which is undesirable for tasks requiring detailed visual understanding. To tackle this, DINO [6, 7] directly learns from curated image datasets in self-supervised manner, obtaining rich, general-purpose visual features helpful for fine-grained downstream tasks such as semantic segmentation and depth estimation. Importantly, its encoded features could match similar regions across different objects, such as the wings of plane and bird, showing in-depth semantic understanding and world knowledge. Darcet et al. [85] proposes simple yet effective improvement to these ViT-based models by adding learnable register tokens along with the original [CLS] token and patch tokens to remove artifacts otherwise present in feature maps and increase performance on dense prediction tasks. Based on these pioneering image encoding efforts, subsequent research has developed foundation models tailored for specialized downstream vision tasks. Depth Anything [86] effectively utilizes self-generated pseudo-labels from large-scale unlabeled data for robust monocular depth estimation (MDE), while Depth 9 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Anything V2 [87] leverages ground-truth labels from synthetic data to enhance fine-grained detail preservation. The Segment Anything Model (SAM) [8] serves as foundation model for promptable image segmentation, and its successor SAM 2 [9] extends this capability to the video domain. These models can generate valid segmentation masks based on prompts in the form of points, bounding boxes, masks, andin the case of SAM text. Cutie [88] is an earlier model for video object segmentation (VOS) and has demonstrated robustness under diverse visual conditions [32]. SAMURAI [89] improves the visual object tracking (VOT) performance of SAM 2 by incorporating motion modeling and motion-aware memory selection, enabling more effective handling of fast motion, occlusion, and crowded scenes. CoTracker [90] complements this line of work by introducing transformer architecture for dense point tracking in long video sequences. In the field of open-vocabulary detection and grounding, series of models have progressively advanced region-level vision-language understanding. GLIP [91] unifies detection and phrase grounding within single pretraining framework by extending CLIP-style alignment to the region level. Grounding DINO [92] builds upon this with DETR-style architecture and contrastive region-text alignment, achieving strong performance on open-vocabulary grounding tasks. Grounding DINO 1.5 [93] scales model size and training data, improving generalization and setting new state-of-the-art results. Grounded SAM [94] further combines Grounding DINO with SAM to enable zero-shot language-driven segmentation. Grounded SAM 2 extends it towards grounding and track anything in videos. For high-fidelity image and video generation, diffusion models [95, 96, 97] have become the dominant approach. Early models like GLIDE [98], DALLE 2 [99], and Imagen [100] demonstrate the power of text-guided image synthesis, while Stable Diffusion [101, 102] enables efficient, open-domain generation with wide adoption. ControlNet [103] introduces spatial conditioning to support fine-grained control over structure and layout. For video, models such as VideoCrafter [104, 105] and PVDM [106] extend diffusion to the temporal domain for text-to-video synthesis. Sora [107] advances this further by employing flow-matching [108, 109] and learns physical priors to generate long-duration, high-resolution videos with strong temporal coherence. More recently, Veo 3 showcases impressive full-modality generation, including synchronized audio and motion, pushing the boundaries of realistic video synthesis. These advanced image and video generative models are also referred to as world models, since they encode vast physical common sense and world knowledge. In parallel, other world models such as Genie [110] and Genie 2 [111] simulate future visual dynamics conditioned on action sequences, enabling accurate and coherent rollout of environment evolution across time. Other efforts focus on developing foundation models for manipulation-relevant perception tasks. FoundationPose [112] is unified vision foundation model for robust and generalizable 6D pose estimation and tracking of novel objects, regardless of the availability of CAD models. HaMeR [113] leverages large-scale data and high-capacity transformer architectures to enable accurate and reliable hand mesh recovery from monocular input, facilitating hand pose extraction from human videos and supporting dexterous manipulation tasks. These advances in vision foundation models have provided general-purpose solutions for visual representation learning, vision-language alignment, common vision tasks, and generative modeling. Their capabilities in generalized visual understanding and generation have substantially accelerated the progress of multimodal learning and empowered broad spectrum of real-world applications. 2.3. Vision-Language Models The advancement of vision and language foundation models has naturally driven research toward multimodal understanding, reasoning, and generation, resulting in the rise of vision-language models. As an early effort, BLIP [114] introduces multimodal mixture of encoder-decoder (MED) architecture based on ViT and BERT for unified vision-language understanding and generation, along with data bootstrapping strategy that synthesizes captions and filters noisy web data into high-quality image-text pairs. To better harness readily-available unimodal foundation models, BLIP-2 [115] proposes Q-Former connector and two-stage training strategy to effectively align frozen pretrained image encoders with frozen LLMs, achieving strong vision-language performance with modest trainable parameters. Different architectural paradigms were also explored. Flamingo [116], for instance, employs Perceiver Resampler and gated cross-attention layers for cross-modal alignment. It also processes inputs in manner inherently compatible with interleaved visual and textual sequences, thereby enabling strong few-shot learning capabilities. LLaVA [83] represents milestone in the development of VLM architecture, which simply links CLIP vision encoder to the Vicuna LLM [117] via linear projection and is trained on visual instruction-tuning 10 Survey on Vision-Language-Action Models: An Action Tokenization Perspective data synthesized by GPT-4. LLaVA-1.5 [118] improves upon LLaVA by adopting stronger vision encoder, replacing the linear projection with an MLP, and training on larger dataset. The Qwen-VL family represents another prominent line of work. The initial Qwen-VL [119] combines the Qwen7B LLM [120] with ViT through position-aware cross-attention adaptor. Its specially designed input-output interface for images and bounding boxes, together with three-stage training strategy, enables interleaved image-text understanding and visual grounding capabilities. Its successor, Qwen2-VL [121], enhances spatialtemporal encoding with 2D RoPE and M-RoPE to support images and videos of varying resolutions and aspect ratios. It demonstrates strong multilingual capabilities and competitive performance on vision-language tasks such as captioning, VQA, and video understanding. Most recently, Qwen2.5-VL [12] extends dynamic resolution to the temporal domain and aligns M-RoPE time IDs with absolute time, enabling more refined temporal understanding. It incorporates window attention in the vision encoder to improve inference efficiency. Together with extensive high-quality data curation, Qwen2.5-VL delivers enhanced visual recognition, precise object grounding, robust document parsing, and long-video comprehension. Karamcheti et al. [84] explore key VLM design decisions around image preprocessing, architecture, and optimization, concluding that single-stage training, fused DINOv2 and SigLIP vision backbones, base LLMs, and co-training with language-only data are effective strategies. Building on these insights, they develop Prismatic VLMs, which consistently outperform LLaVA-1.5 across benchmarks and have been used later in OpenVLA [21]. PaliGemma [122], 3B VLM built on SigLIP [82] So400m [123] and Gemma 2B [58], is developed with focus on transferability and subsequently adopted as the backbone for the ùúã0 series of VLA models [22, 124, 24, 125, 126]. At the forefront of current capabilities are two proprietary models: GPT-4o [10] and Gemini 2.5 Pro [11]. Both exhibit leading performance on general vision-language benchmarks and have seen widespread adoption in real-world applications. GPT-4o is distinguished by its native support for image generation, while Gemini 2.5 Pro is recognized for its powerful reasoning capabilities, underscoring the rapid progress and practical utility of modern VLMs. 2.4. Embodied VLA Models as the Next Frontier The rapid advancements in foundation models are increasingly fueling imagination and propelling the pursuit of Artificial General Intelligence (AGI). As current foundation models primarily operate within the digital domain, representing digital AI, researchers are naturally shifting their focus to embodied AI, which aims to develop general-purpose agents capable of following human instructions in the physical world. However, we emphasize that embodied AI presents significantly bolder ambition than digital AI for several reasons. Fundamentally, the problems that embodied AI must solve introduce novel forms of open-endedness and challenges absent in digital AI. Whereas difficult digital cases may involve out-of-distribution (OOD) or adversarial inputs, the physical world is inherently unstructured, and even routine settings can be highly challenging. Free-flowing human conversations, inadvertent interventions, fallen chairs, cluttered rooms, and occlusions are common examples, not to mention even more difficult situations. comparable and perhaps more familiar problem, which we also consider part of embodied AI in this paper, is autonomous driving. While autonomous driving is already incredibly difficult, general-purpose embodied intelligence in the physical world must handle orders of magnitude more situations, leading to orders of magnitude greater challenges and difficulties. This imposes substantial demands on both model and data to support robust embodied AI. AGI Embodied AI Digital AI VLA Hardware Robotics Figure 4 Venn diagram showing the interrelationships among key AI fields. VLA models intersect with digital AI, hardware, and robotics, representing core subfield of Embodied AI and key area in the progression towards AGI. Furthermore, crucial realization is that embodied AI also involves requirements for robot hardware, which digital AI does not entail. To achieve general-purpose embodied intelligence, the hardware platform must possess the dexterity and robustness necessary for general tasks, level that is currently far from being met. Representative gaps include dexterous hands and robotic arms that are far from achieving human-level dexterity, heavy reliance on grippers, diversity and isolation of embodiments, and the lack of sensitive, full-coverage tactile sensors. Since 11 Survey on Vision-Language-Action Models: An Action Tokenization Perspective hardware perfection cannot be achieved in short time frame, reasonable expectation is that models, data, and hardware will develop synergistically, ultimately achieving general intelligence. The scope of this survey primarily focuses on the model and data aspects, but we also inform readers of the hardware challenges, which are often important considerations for model development. Given embodied AIs requirement for general visual and language capabilities, natural strategy is to build on foundation models and endow them with action capabilities. This direction has given rise to embodied VLA models, now central topic of investigation. Situated at the intersection of digital AI, robotics, and hardware, VLA constitutes core subfield of embodied AI and key area in the pursuit of AGI (Figure 4). The hundreds of VLA papers proposed to date illustrate rapidly expanding field (Figure 3), showing early yet limited signs of intelligence and generalization. This survey systematically reviews and analyzes these papers from the perspective of action tokenization to outline the research landscape. Notwithstanding recent progress, most evaluations remain confined to simplified laboratory settingspredominantly gripper-based manipulationand thus far from the requirements for general-purpose embodied agents in everyday environments. Consequently, the field is in its infancy, and substantial advances are still needed. The continued development of embodied VLA models is therefore poised to remain the next frontier of research for the foreseeable future. 3. Overview of Action Tokens Research in VLA models focuses on processing vision and language input to generate action output, leveraging foundation models. We observe that in designing VLA architectures and formulating training strategies, the concepts of VLA modules and action tokens naturally emerge. To map raw perception to action, VLA models must effectively comprehend the scene and instruction, ground the instruction within the scene, plan the current subtask, anticipate subsequent movement, and generate executable actions. The complexity and generality of embodied tasks further necessitate the switching, repetition, and recursion of these capabilities. To facilitate task-relevant information flow and refinement, VLAs delegate these capabilities to distinct modules, manage their respective generations, and logically link these modules and their generations to derive final actions. Consequently, the design of generation formats and the training strategies for these modules are central to VLAs. This survey reviews existing research from this perspective. We term the maximal differentiable subnetworks and non-differentiable functional units within VLA VLA modules, and their generations action tokens. Furthermore, semantically meaningful intermediate generations within VLA modules are also considered action tokens. The designation action token not only signifies that these generations encapsulate action-related information but also aligns with the naming convention of language token in LLMs. Indeed, action tokens in VLAs are generalized counterparts to language tokens in LLMs. To further clarify these concepts, Figure 1 highlights several representative examples. For given language instruction in the current environment, Hi Robot [24] employs fine-tuned PaliGemma model to predict the next subtask in natural language. This is followed by VLA modeltrained in manner similar to ùúã0 [22]that generates low-level robot commands. In this case, both the fine-tuned PaliGemma and the customized ùúã0 constitute VLA modules, while the intermediate language plan and the resulting raw actions serve as action tokens. Another example is VoxPoser [16], which also begins by using LLMs to decompose language instruction into subtasks. It then employs LLMs and VLMs to generate an affordance map for solving each subtask based on the current scene, and finally invokes motion planning module to convert the affordance map into raw actions. Here, the LLMs, VLMs, and motion planning algorithm all function as VLA modules, while the language plan, affordance map, and raw actions represent the corresponding action tokens. Other VLA models can similarly be analyzed by identifying their constituent VLA modules and action tokens according to this framework. Based on broad survey of existing literature, we observe that most VLA models conform to unified abstract framework, as illustrated in Figure 1: vision and language inputs are iteratively processed by sequence of VLA modules to produce chain of action tokens that progressively encode more grounded and actionable guidance, ultimately resulting in executable actions. This abstraction offers unified lens through which to interpret and compare diverse VLA architectures. As VLA leverages foundation models for the development of VLA modules and action tokens, the inherent diversity in these underlying models results in variety of action token formats. Existing VLA research has primarily investigated eight principal types of action tokens: language description, code, affordance, trajectory, goal state, latent representation, raw action, and reasoning. In Figure 2, we visualize common formats of these action tokens, employing the illustrative task: prepare tea. This visualization demonstrates that, for given Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 1 Overview of key advantages, limitations, and empirical results of each type of action token. Action Tokens Advantages Limitations Language Description (Section 4) Language Plan Well-supported by LLMs and VLMs; abundant co-training data; necessary for long-horizon planning Language Motion Multi-task data sharing Imperfect expressiveness (ambiguous; hard to describe dexterous manipulation); high latency Notable Empirical Achievements Make bed (ùúã0.5 [125])); make sandwich (Hi Robot [24]) Pull napkin from dispenser (RT-H [31]) Code (Section 5) Well-supported by LLMs; clear logic for planning and control; rich third-party libraries Overly rely on predefined APIs; brittle runtime execution Rearrange restore (Instruct2Act [127]) Keypoint Precise interaction targets Pour tea (ReKep [128]) Affordance (Section 6) Bounding Box Segmentation Mask Well-supported by VLMs; efficient for instance-level localization Capture fine-grained contours, geometry for functional region grounding Affordance Map Dense, interaction-centric, full-scene Trajectory (Section 7) Goal State (Section 8) Latent Representation (Section 9) Raw Action (Section 10) Reasoning (Section 11) Trainable from off-domain human videos; cross-task generalization Well supported by foundation models; high data scalability via hindsight relabeling and action-free video utilization; task specificity High data scalability by utilizing action-free human videos and cross-embodiment data; strong expressive potential (compact structure, implicit semantics, multi-modal integration) Minimal human knowledge; minimal action token annotation; similar training strategy and scaling potential to VLMs; efficient fine-tuning Enhance generation of target action tokens; complex problem solving Future work should better capture 3D spatial structures; lacks temporal modeling of evolving affordance prediction; sensitive to visual noise, including occlusion and motion artifacts Limited 3D expressiveness; limited support from VLMs; insufficient semantic grounding Challenging to generate high-quality, consistent goal states; high latency Uninterpretable; future work should improve the granularity, comprehensiveness, and task-centric alignment of the latent space Data scarcity; difficulty in data collection; high latency; poor cross-embodiment generalization High latency; future work should develop flexible reasoning paradigms Dexterous grasping in cluttered scenes (DexGraspVLA [32]) Decision-making in open world (ROCKET-1 [129]) Deformable object manipulation (ManiFoundation [130]) Clean the table with duster (RT-Trajectory [36]) Transfer liquid using pipette (VPP [131]) Fold shorts (GO-1 [18]); mine diamond in Minecraft (OmniJARVIS [132]) Laundry folding (ùúã0 [22]); light match and light candle (Real-Time Chunking [133]) Autonomous driving (DriveVLM [30]) language instruction and observation, each type of action token encodes task-relevant guidance in distinct manner. Formal definitions of these action tokens are provided below. (1) Language description (Section 4): natural language expression that describes the intended action sequence, ranging from high-level and abstract language plan to low-level and concrete language motion. (2) Code (Section 5): An executable code snippet or pseudocode that either constitutes complete robot program or specifies low-level atomic operations. (3) Affordance (Section 6): spatially grounded representation that captures task-specific and interactionrelevant properties of objects, typically represented as keypoint, bounding box, segmentation mask, or affordance map. (4) Trajectory (Section 7): temporally ordered sequence of spatial states that captures the dynamic evolution of an object, end-effector, or scene. (5) Goal state (Section 8): predicted future observationsuch as an image, point cloud, or video clipthat visually represents the expected outcome of the intended action sequence, serving as an intermediate target for planning and execution. (6) Latent representation (Section 9): purposefully pretrained latent vector sequence that encodes actionrelevant information over temporal interval, typically extracted from large-scale datasets. Survey on Vision-Language-Action Models: An Action Tokenization Perspective (7) Raw action (Section 10): One or more low-level control commands that can be directly executed by robot. (8) Reasoning (Section 11): Natural language expressions that explicitly describe the decision-making process leading to specific action token. In the following sections, we systematically present VLA models categorized by each type of action token. For each category, we discuss the motivation for its adoption, review relevant literature, and analyze its advantages and limitations, while highlighting directions for future research. Each section also includes table summarizing the surveyed works, examining similarities and differences across multiple dimensions pertinent to the respective action token. In particular, the previous module and next module columns refer to the design strategies of the VLA modules preceding and succeeding the action token, respectively, often reflecting key innovations and thoughtful design choices in how the token is generated and transformed to enable effective VLA models. Additionally, Table 1 provides summary of the most salient advantages, limitations, and notable empirical results for each type of action token, facilitating comparison, understanding, and insight across categories. 4. Language Description as Action Tokens The advancements of LLMs and VLMs naturally motivate the use of language description as action tokens in VLA models, enabling direct leverage of their strengths in language understanding, generation, reasoning, and planning. Moreover, representing actions through natural language aligns closely with the way humans conceptualize and communicate plans, especially for complex and long-horizon tasks. Rather than executing primitive actions directly, humans tend to decompose high-level instruction into intermediate, semantically meaningful sub-steps, and further into precise motion commands when necessary. This hierarchical structure of tasks allows people to flexibly adapt their plans to different contexts and levels of control. Inspired by this, these language-based tokens in VLA models are also designed with varying levels of abstraction, broadly categorized into two types. At the upper end, language plans [136, 14, 13, 151, 24, 134, 16] typically describe an entire subtask or high-level goal in single phrase. Examples such as pick up the cup and place the cup on the table convey what the robot should accomplish, serving as semantic anchors that can be assigned to skills or policies. In contrast, at finer level, language motions [31, 150] specify low-level physical actions closer to motor control, using expressions such as move the arm forward and close gripper, which detail the execution of specific movements. This spectrum of abstraction provides conceptual framework that enables VLA models to organize, interpret, and execute embodied tasks at different levels of granularity, with the potential to support more human-like hierarchical planning. Motivated by these advantages, growing body of work has explored the incorporation of language description as action tokens in VLA, leading to diverse strategies for task decomposition, action sequencing, and execution management. We list them in Table 2. 4.1. Progress and Key Papers Early works, such as Language Planner [134], Socratic Models [135], and SayCan [13], demonstrate that LLMs can directly decompose high-level natural language instructions into semantically meaningful subgoals without task-specific training. This opens up the possibility of planning without domain-specific engineering. However, naive LLM-based planners face fundamental limitation: the absence of perceptual grounding. Operating without direct access to visual, spatial, or sensory inputs, they struggle to align abstract plans with the actual state of the environment and to adapt effectively to unanticipated physical contexts. To address this, these works introduce explicit grounding mechanisms. Socratic Models [135] pairs LLMs with VLMs that detect relevant objects and provide visual context, bridging the gap between abstract plans and physical reality. SayCan [13] re-weights LLM-generated plans with an affordance function that estimates the feasibility of each action plan given the environment. Inner Monologue [136] extends this further by introducing feedback loops: the system continuously prompts the LLM with signals like success detection, scene descriptions, or human feedback, enabling reflective, multi-turn reasoning and dynamic adjustment of plans. DoReMi [142] proposes dual-role framework, where the LLM generated both high-level plans and explicit execution constraints. These constraints are monitored by VLM-based detectors at runtime, ensuring the system can react to dynamic contingencies. Nevertheless, external grounding modules struggle to flexibly provide task-dependent information, cannot reason jointly with LLMs, and are often inadequate for handling fine-grained tasks in complex environments [143]. 14 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 2 Overview of VLA research using language description as action tokens. Format Paper Previous Module Model Training Strategy Generation Strategy Action Token Restrictiveness Next Module Model Training Strategy Task Embodiment Language Planner [134] Socratic Models [135] Codex-12B, GPT-3-175B, SentenceRoBERTa355M Frozen ViLD, LLM Frozen SayCan [13] PaLM-540B Frozen Inner Monologue [136] PaLM-540B, InstructGPT Frozen LLM generates plans, RoBERTa finds the best match within skill set VLM detects objects, LLM generates individual steps The atomic skill with the highest combined rating from LLM and affordance function is selected LLM generates and updates plans with textual feedback, utilizing few-shot prompting Predefined N/A N/A VirtualHome N/A Predefined CLIPortinspired policy Trained Tabletop rearrangement (simulation) UR5 with gripper (simulation) Predefined BC-Z Trained on 80K demonstrations Mobile manipulation (office kitchen) Everyday Robots Predefined CLIPort, BC-Z CLIPort trained on 20K Pick-Place demonstrations; BC-Z trained on 80K demonstrations Tabletop rearrangement (simulation, real-world); mobile manipulation (office kitchen) UR5e with gripper; Everyday Robots PaLM-E [14] PaLM-E-562B Trained on VQA, web text, manipulation datasets VLM generates plans Free-form Interactive Language policy, RT-1 Trained as in the original papers EmbodiedGPT [139] EmbodiedGPT Trained on EgoCOT via prefix tuning VLM generates sequence of sub-goals Free-form MLP policy network DoReMi [142] Vicuna-13B, BLIP-2 LLM frozen, BLIP-2 fine-tuned with LoRA LLM generates plans and constraints through few-shot in-context learning Predefined CLIPort, Transporter Nets, DeepMimic for locomotion Trained on 10 / 25 / 50 demonstrations per task Trained on via imitation learning or RL ViLa [143] GPT-4V N/A VLM generates plans via CoT reasoning in zero-shot mode Predefined Scripted, RL, BC policies Trained TAMP [137]; LanguageTable [138]; mobile manipulation xArm6 with cylindrical gripper; Everyday Robots Meta-World [140]; Franka Kitchen [141] Franka Panda (simulation) Tabletop manipulation; humanoid manipulation Tabletop manipulation (simulation, real-world) UR5e with gripper; Humanoid Franka Panda 3D-VLA[38] BLIP-2 FlanT5XL RoboMamba [146] CLIP, Mamba Fine-tuned on curated 3D embodied instruction tuning dataset containing 2M scene-languageaction pairs Alignment pretraining, instruction co-training ReplanVLM [147] GPT-4V N/A BUMBLE [148] GPT-4o N/A ReflectVLM [149] LLaVA-1.513B Trained on demonstrations Hi Robot [24] PaliGemma3B ùúã0.5 [125] PaliGemma3B Trained on teleoperated demonstrations segmented into short skills and synthetic prompts Trained on robot data, high-level subtask prediction data, and multi-modal web data VLM generates plans with interactive tokens Free-form Stable Diffusion v1.4, Point-E Fine-tuned Long-horizon tasks in RLBench [144] and CALVIN [145] Franka Panda VLM generates plans Free-form Simple policy head Trained on 10K end-effector Pose Predictions Vision-language tasks, pose prediction Franka Panda VLM generates task plans, another two VLMs detect internal and external errors VLM predicts subtasks and selects parameterized skills VLM proposes plans, diffusion model imagines future images, and VLM reflects on the plans Free-form NR Predefined NR NR NR Tabletop manipulation JAKA Zu 7 arm with gripper Long-horizon building-wide mobile manipulation NR Predefined Rule-based script controller N/A Manipulation tasks (1K interlocking object) Franka Panda VLM generates plans Free-form ùúã0 VLM generates plans Free-form ùúã0. RT-H [31] PaLI-X 55B Trained on Kitchen and Diverse datasets labeled with language motions VLM predicts fine-grained language motion phrases NaVILA [150] VLM generates mid-level actions with spatial information N/A indicates not applicable; NR indicates not reported. Trained on 2K YouTube egocentric touring videos ViLa Free-form PaLI-X 55B Free-form Visual locomotion policy Trained on teleoperated demonstrations Table bussing, make sandwich, grocery shopping UR5e with gripper; ARX with gripper; ARX with gripper and mobile base Trained on robot data, high-level subtask prediction data, and multi-modal web data Trained on Kitchen and Diverse datasets labeled with language motions Trained via PPO Household (real-world) Two mobile manipulator platforms Tabletop manipulation NR VLN-CE-Isaac [150]; navigation (25 tasks, real-world) Unitree Go2; Unitree H1; Booster 15 l a a i e g A Survey on Vision-Language-Action Models: An Action Tokenization Perspective To address these challenges, subsequent works have shifted towards more natural grounding approaches by directly incorporating visual inputs into the planning process through VLMs. PaLM-E [14] is large-scale embodied multimodal language model that unifies vision, language, and robot state information by encoding them into single multimodal input. This design enables deep integration of perception, allowing the model to directly generate plans conditioned on sensory inputs. To reduce training costs and enhance accessibility, EmbodiedGPT [139] adopts lightweight VLM architecture composed of pretrained and frozen components, and trains it on self-constructed EgoCOT dataset using parameter-efficient strategy. ViLa [143] utilizes GPT-4V as planner, showing that advances in foundation models can directly translate to improvements in VLA models without additional task-specific training. 3D-VLA [38] and RoboMamba [146] extend this paradigm by incorporating 3D scene understanding, spatial layouts, and visual affordance prediction into the planning loop. Subsequent works have gone beyond grounding to tackle long-horizon, complex tasks by introducing memory and reflection mechanisms. BUMBLE [148] and ReflectVLM [149] incorporate these mechanisms, allowing systems to handle interdependent subtasks and plan across diverse and complex environments. These agent-like capabilities mark shift from isolated planning to more integrated, adaptive behavior. Previous papers constrain generated language plans within the scope of predefined skill sets [134, 135, 13, 136] or scripted controllers [143, 149], limiting their flexibility in addressing complex instructions and open-ended scenarios. To overcome these limitations and enable handling of free-form prompts, research has increasingly focused on integrating more robust and generalizable low-level policies. Notable among these are Hi Robot [24] and ùúã0.5 [125], which exemplify this transition. Hi Robot [24] proposes hierarchical framework in which high-level VLM interprets complex prompts and dynamic user feedback, producing free-form language commands executed by low-level generalist control policy [22]. The generality of both the high-level and low-level components enables the system to handle multi-stage tasks and situated corrections across diverse platforms. Its successor ùúã0.5 [125] unifies the planner and controller into single VLA model, which first predicts high-level semantic subtasks and generates continuous low-level actions conditioned on these subtasks. By training on web-scale heterogeneous data, it can perform long-horizon, open-world tasks such as cleaning unseen kitchens with remarkable generalization. While the aforementioned works largely focus on language plans at the subtask level, another line of research investigates language motions as fine-grained linguistic descriptions of low-level movements. representative work in this direction is RT-H [31], which introduces an intermediate layer of language motion between visionlanguage inputs and action outputs to facilitate multi-task data sharing across diverse high-level tasks. Building on this design, RT-H adopts hierarchical architecture in which VLM first predicts the current language motion (e.g. move arm forward) conditioned on the instruction (e.g. pick coke can), and subsequently generates the low-level action based on both the instruction and the predicted language motion. This approach improves performance and enables more effective intervention. Beyond manipulation, NaVILA [150] applies this idea to navigation tasks by first generating mid-level spatial commands in natural language, such as move forward 75 cm, which are then executed by visual locomotion policy. These works collectively demonstrate that fine-grained language motions, by explicitly describing spatial and temporal micro-actions, can provide precise, interpretable guidance for low-level controllers. key advantage here is to enable better data sharing across different tasks at the language motion level, resulting in better language motion composition, generalization, and data efficiency. Also, the fine-grained language motions are more convenient for humans to correct in the context of current scenes. 4.2. Advantages of Language Descriptions primary advantage of using language descriptions as action tokens lies in their seamless integration with large foundation models. Both LLMs and VLMs possess strong out-of-the-box capabilities in understanding, reasoning, and planning, which enables zero-shot planning and significantly reduces the need for task-specific training. They can also directly benefit from ongoing advancements in in-context learning, memory, decoding strategies, and search techniques. Even when fine-tuning is required, the alignment between language descriptions and the models native output space makes the process more efficient and less disruptive than with other forms of action tokens, which often suffer from greater modality mismatches. Second, language description benefits from the abundance of co-training data. Empirical results in PaLME [14] and ùúã0.5 [125] show that co-training on such data can transfer rich world knowledge into VLA models, thereby improving generalization. Survey on Vision-Language-Action Models: An Action Tokenization Perspective Third, language description is particularly well-suited for long-horizon planning. In fact, language descriptions are almost necessary if VLA models are to perform complex, temporally extended tasks. Lastly, the interpretability of language descriptions facilitate human oversight and intervention, thereby enhancing safety, transparency, and controllability. Systems such as Hi Robot [24] and YAY Robot [152] exemplify how language-based plans enable seamless integration of human-in-the-loop corrections and dynamic feedback. Moreover, the correction data collected through online human interaction can be leveraged to iteratively improve model performance over time [31]. 4.3. Discussion and Future Directions One limitation of using language descriptions as action tokens arises from their imperfect expressiveness. While natural language is flexible and interpretable, it is inherently ambiguous and often insufficient for specifying fine-grained control behaviors particularly in contact-rich or deformable manipulation tasks [153, 154], where precise spatial and temporal details are critical. These issues may lead to miscommunication between system components and inadequate task grounding, both of which can hinder overall performance. Another limitation concerns latency. Generating high-quality language descriptions often depends on largescale models, which can incur inference delays and constrain applicability in dynamic or real-time scenarios. Potential remedies include employing inference acceleration techniques and developing asynchronous planning and execution frameworks. Looking beyond these limitations, promising research direction is to leverage language descriptions primarily for high-level planning - decomposing complex tasks into simpler subproblems that can then be more effectively addressed by VLA models utilizing alternative action token formats such as affordance (Section 6), trajectory (Section 7), or goal state (Section 8). These representations offer greater precision and efficiency for low-level execution, thereby enabling more reliable and scalable embodied intelligence. 5. Code as Action Tokens key challenge in VLA models lies in planning and controlling complex, long-horizon manipulation tasks that require structured reasoning and adaptability to dynamic environments. Traditional action representations, such as discrete signals or direct language commands, often lack the expressiveness required for this complexity. In response, code-based action tokens emerge as powerful alternative. These representations consist of executable code snippets or pseudocode that incorporate control structures like conditionals and loops. This format allows for direct execution through robot control APIs, enabling models to generate modular behaviors with explicit logic. It effectively supports both hierarchical planning and reactive control. Code offers distinct advantages over other action formats. It provides clear logical structures and can leverage rich third-party libraries. Furthermore, it creates transparent and verifiable bridge between high-level instructions and low-level robot primitives. Recent advances in LLMs have made it feasible to synthesize task-relevant codes from natural language and visual inputs. This paradigm has spurred growing body of research exploring code as structured and interpretable action representation for robotics [15, 34, 155, 156, 157, 153, 158, 127]. Table 3 summarizes representative VLA models that utilize code-based action tokens. 5.1. Evolution of Code-Based Action Two foundational works pioneer the use of code-based action representations in VLA research: Code as Policies [15] and ProgPrompt [34]. Code as Policies utilizes LLMs like GPT-3 or Codex [159] to map language instructions to Python code snippets. This generated code processes perceptual inputs, parameterizes low-level robot APIs, and executes tasks on the robot platform. key capability is its natural integration with third-party libraries like NumPy to perform complex spatial reasoning. At the same time, the system also generalizes effectively to new objects by bootstrapping from perception modules. This modularity allows its policy code to adapt to new behaviors through new instructions and APIs. Building on this, ProgPrompt extends the code generation process with finite state machine (FSM) framework. Specifically, ProgPrompt employs programmatic structures in prompts to guide LLMs, which integrates import declarations to specify robot capabilities, natural language comments to scaffold high-level reasoning, and assertions to validate execution states. The FSM framework orchestrates overall task execution, which defines explicit subtask transitions and uses reactive trigger mechanism, enabling the system to adapt to dynamic environmental changes. 17 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Recent research extends code-based action tokens by integrating commonsense reasoning and improving the grounding of the generated code in the physical world. For instance, ChatGPT for Robotics [155] explores diverse prompting strategies, such as free-form dialogue, code prompting, XML tags, and closed-loop reasoning, to better parse human intent. To generate more effective and grounded code, it emphasizes the importance of descriptive API names and clear task specifications within the prompt. Crucially, the generated code undergoes human-in-the-loop validation process, where feedback on its quality and safety is used for iterative improvement before final deployment on the robot. To address the perceptual limitations in Code as Policies [15], Instruct2Act [127] augments coding LLMs with specialized multi-modal foundation models for precise object segmentation and open-vocabulary classification. By offloading perception and semantic understanding, Instruct2Act effectively grounds high-level language instructions into precise, executable policy codes. Further advancing multimodal integration, RoboCodeX [156] focuses on fusing information from diverse sources, such as various scene datasets, diverse object datasets, and procedural task descriptions. It introduces novel tree-of-thought framework that synthesizes behaviors by combining visual, linguistic, and physical cues. The models reasoning capabilities are enhanced through fine-tuning on purpose-built multimodal dataset, leading to more accurate and generalizable robotic actions. Code-based action tokens are also effective for high-level planning and task generalization. For instance, to tackle long-horizon tasks, Text2Motion [153] leverages GPT-3 to generate valid goal states that define task success, providing clear termination criterion for planning. To reach this goal, the framework employs hybrid planner, which combines shooting-search planning for efficiency and greedy-search planning for reliable fallback. Addressing the practical deployment of such generated plans, RoboScript [157] introduces unified code generation pipeline that standardizes inputs and integrates diverse perception and motion planning tools. This design significantly enhances code flexibility and adaptability across various robots. Pushing the boundaries of generalization further, Chain-of-Modality [158] (not VLA model) introduces novel prompting strategy that guides VLMs to reason about multimodal human demonstrations (e.g., muscle or audio signals) to generate robot-executable code. 5.2. Brittleness and Challenges Despite their advantages, code-based action tokens face several significant practical limitations. Their expressiveness is inherently constrained by the capabilities of predefined perception and control API library [15]. When robots encounter highly dynamic, ambiguous, or previously unobserved environments, the pre-established APIs might be inadequate to accurately capture or express the novel behaviors required. Therefore, the systems adaptability and exploratory capacity in complex, open-world settings [34] are limited. For instance, if an API doesnt offer abstractions for environmental features like slippery surfaces or fragile objects, even perfectly written code will struggle to generate the nuanced actions needed for such scenarios. This reliance on rigid symbolic representations also leads to execution brittleness. Robotic policies arent just susceptible to internal generative errors from the LLMs (e.g., producing logically inconsistent or inefficient code); more critically, they fail when real-world environmental states violate an APIs presumed preconditions. This is core manifestation of the symbol grounding problemwhere abstract symbols in code cannot reliably map to complex real-world perceptions. For example, piece of code controlling robotic arm for grasping might assume that the objects surface is always dry and flat. If the actual object is wet or irregularly shaped, the code, though syntactically correct, could lead to failed grasp, object damage, or even hardware damage. This inherent brittleness directly translates into substantial safety risks, as seemingly innocuous code commands can trigger severe incidents in unforeseen circumstances. 5.3. Future Directions promising direction for future work is the development of comprehensive API function libraries to fully unlock the potential of code-based action tokens. Such framework should integrate rich set of modular functions, including multi-modal perception APIs (e.g., object detection and tracking), reasoning modules (e.g., spatial relationship analysis), and robust action primitives. By providing structured and reliable interface, this framework would enable VLMs to act as high-level orchestrators, generating executable code that composes these primitives to solve complex, long-horizon tasks in the real world. second future direction is integrating formal verification throughout the code lifecycle to enhance robustness. This includes verifying API libraries for consistency and safety and developing methods to dynamically verify LLM-generated code. Logical reasoning and constraint satisfaction can guide safe code generation, while static Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 3 Overview of VLA research using code as action tokens. Paper Previous Module Next Module Task Embodiment Model Training Details Model Training Details Code as Policies [15] Codex (code-davinci-002) API Calling ViLD, MDETR, impedance controller, trajectory-based controller ViLD, MDETR frozen Draw shape, pick-place; mobile manipulation ProgPrompt [34] GPT-3 API Calling ViLD, Contact-GraspNet, SceneCollisionNet, motion planning (MPPI) Contact-GraspNet, SceneCollisionNet, ViLD frozen VirtualHome (simulation); pick-place, sort objects (real-world) UR5e with Robotiq 2F85 gripper (RealSense D435); Everyday Robots Franka Panda ChatGPT for Robotics [155] ChatGPT API Calling Robot function library, YOLOv YOLOv8 frozen Navigation, object manipulation, AirSim [160] industrial inspection, AirSim obstacle avoidance (simulation); drone flight (real-world) NR Text2Motion [153] GPT-3 (text-davinci-003) API Calling Skills library, geometric feasibility planner N/A Pick-place (simulation) Franka Panda (Kinect V2) Instruct2Act [127] GPT-3 (text-davinci-003) API Calling SAM, CLIP Frozen RoboScript [157] GPT-3.5-turbo / GPT-4 / Gemini Pro API Calling GLIP, AnyGrasp, GAMMA, GIGA, motion planning (RRT) GLIP, AnyGrasp, GAMMA, GIGA frozen Visual manipulation; scene understanding; rotate, rearrange; rearrange restore, pick-restore Pick-place, insert into drawer RoboCodeX [156] RoboCodeX Pretrained and fine-tuned on self-collected dataset AnyGrasp, GAMMA, motion planning, ROS AnyGrasp, GAMMA frozen Pick-place, insert into drawer N/A indicates not applicable; NR indicates not reported. NR Franka Panda, UR5 with Robotiq 2F-85 gripper (RGB-D camera) UR5 with gripper, Franka Panda (3 RGB-D camera) analysis and model checking catch errors or prove safety before deployment. Finally, runtime monitoring ensures API preconditions are met, triggering safe shutdowns or recovery if anomalies occur. Another frontier is leveraging codes interpretability to enable effective human-robot collaboration. Unlike black-box models, codes transparency lets humans understand and intervene in robots logic. This supports two key paradigms: interactive debugging, where failures can be traced and fixed in real time, and collaborative refinement, where humans iteratively guide program improvement. Such human-in-the-loop systems are crucial for developing robotic agents that are not only capable but also trustworthy and controllable. 6. Affordance as Action Tokens Within the VLA paradigm, affordance serves as structured and spatially grounded action tokens that bridges visual perception and physical interaction. Recent research [161, 162, 130, 163] demonstrates that affordance representations utilize the spatial reasoning capabilities of vision-language foundation models to identify actionable regions and evaluate physical feasibility based on multimodal inputs. By abstracting away embodimentspecific control mechanisms, affordances enhance cross-platform generalization, allowing the same high-level instructions to be executed across various robotic systems. Moreover, they explicitly encode task-relevant interaction information, such as grasp points or manipulable surfaces, making them particularly effective for object-centric manipulation in real-world settings. Affordance can be expressed in various forms, each offering distinct insights into how robot may interact with objects in its environment. Recent research primarily explores keypoints [164, 128, 165], bounding boxes [166, 162, 32], segmentation masks [167, 163], and affordance maps [168, 16, 130]. We summarize these efforts in Table 4. For contact-rich task like kitchen cleanup, the choice of representation is crucial. Keypoints provide precise targets, ideal for pinpointing bowls rim for grasping or pressing small dishwasher button. Bounding boxes offer simpler, coarse localization sufficient for general object selection. For operations requiring fine-grained interaction, such as wiping the irregular interior of bowl, segmentation masks are superior as they capture the objects exact contour. Affordance maps provide dense, scene-level understanding of interaction possibilities. They highlight all graspable or wipeable regions simultaneously, enabling more complex spatial reasoning across multiple objects. Ultimately, the selection of an affordance representation 19 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 4 Overview of VLA research using affordance as action tokens. Format Paper Previous Module Next Module Task Embodiment Model Training Details Model Training Details KITE [164] Two-stream architecture Skill dataset (GTX 1070, 3h) PointNet++ [169] (20K points) 50 episodes/skill (GTX 1070, 1h) Tabletop instruction-following; semantic grasping; coffee making Franka Panda (3 RealSense D435) RoboPoint [170] CoPa [171] Vicuna-v1.5-13B with ViT-L/14 336px image encoder GPT-4V, OWL-ViT, SAM, GraspNet RAM [165] CLIP, Stable Diffusion ReKep [128] DINOv2, SAM 660K (image, relation) pairs from 10K scenes (16A100-80G, 40h) Motion planning N/A Pick-place Franka Panda Frozen GPT-4V Frozen Frozen Motion planning (cuRobo) Motion planning (constraint optimization) Motion planning (constraint optimization) Motion planning (MPPI) N/A N/A N/A N/A N/A Hammer nail, insert flower, pour water, etc. Franka Panda (2 RealSense D435) Articulated manipulation Franka Panda (RealSense D415); Unitree B1 with Z1 (RealSense D415) Tape box; dual-arm fold Franka Panda (single & dual arm) Pour tea, open jar, operate drawer Franka Panda with UMI gripper (2 RealSense D415) Move rope/cube/granular NR OmniManip [172] GPT-4o N/A KUDA [173] SAM Frozen GPT-4V for Robotics [166] Detic Frozen Task planner Hardwareindependent executable file Relocate juice, open drawer Nextage, Fetch with Shadow Dexterous Hand Lite A3VLM [162] SPHINX-1K, Llama-2 40 images per object in PartNet-Mobility (8A100-80G, 24h) Action primitives N/A Articulated manipulation Kuka with Robotiq gripper (RealSense D415) DexGraspVLA [32] Qwen2.5-VL72B-Instruct Frozen SAM, Cutie, DINOv2, Diffusion Policy 2K+ human demos (8A800-80G, 24h) Dexterous grasping in clutter MOO [161] OWL-ViT Frozen RT-1 SoFar [163] SAM, FlorenceFrozen PointSO Trained on RT-1 data and diverse pick data across set of 90 diverse objects Pick, move near, knock, place into, place upright OrienText300K (8H800) Rearrangement; navigation RealMan RM75-6F with PsiBot G0-R (RealSense D405C, RealSense D435) Mobile manipulator with gripper Franka, UR5e, Flexiv (RealSense D415); Unitree GO2 (RealSense D455, LiDAR) RoboDexVLM [167] SAM Frozen AnyGrasp N/A Open-vocabulary pick-place UR5 with Inspire hand (RealSense D435i) SAM 2 Frozen TransformerXL 160M video frames (8A800-80G, 72h) Minecraft Virtual agent ROCKET-1 [174] CLIPort [175] Two-stream architecture (CLIP ResNet50, Transporter ResNet) Trained on self-collected data Motion primitive N/A VoxPoser [16] GPT-4, OWL-ViT, XMEM Frozen Motion planning N/A Ravens [176] (simulation); Pick, place, pack, move, fold, sweep (real-world) Move-avoid, set table, sweep trash UR5e with suction gripper (simulation); Franka Panda (real-world) Franka Panda (2 Azure Kinect) Franka Panda (RealSense 415) o K g u k n a m S M a ff ManipLLM [168] N/A N/A LLaMA with L-Adapter 10K successful samples (A100-40G, 10h) Articulated manipulation ManiFoundation [130] CVAE 3000 objects N/A indicates not applicable; NR indicates not reported. Motion planning (RRT-Connect) N/A Fold cloth, rearrange rope, breakfast preparation Kinova MOVO, Flexiv with Leap Hand involves fundamental trade-off between interaction precision, computational complexity, and the demands of the task. In the following parts, we analyze each representations characteristics in detail. 6.1. Keypoints: Precise Interaction Anchors Keypoints provide compact and precise representation of interaction targets, such as object handles or contact edges. They are typically defined as = [x, d], where x, ‚Ñù3, with denoting the spatial contact position and indicating the interaction direction. Benefiting from the precise spatial grounding capabilities of Survey on Vision-Language-Action Models: An Action Tokenization Perspective VLMs [7, 8, 177, 178, 25], several early VLA models have adopted keypoints to directly link vision-language perception with control-level execution. KITE [164] grounds language instructions in visual scenes by predicting task-relevant keypoints, which correspond to semantic object parts. These keypoints are then used in conditioned skills to carry out low-level actions. RoboPoint [170] builds upon this idea by constructing synthetic dataset to instruction-tune VLMs for spatial reasoning, allowing models to identify points satisfying relational constraints, which are subsequently executed through motion planning. CoPa [171] further enhances spatial grounding by incorporating common sense priors from VLMs into coarse-to-fine grounding pipeline, which first identifies plausible interaction regions and then refines them into actionable spatial constraints for subsequent motion planning. To ensure control robustness, KUDA [173] introduces two-level closed-loop control mechanism to facilitate robust model-based planning. Specifically, it uses VLM to generate task specifications that contain keypoints and their corresponding target positions. These specifications are then formulated as cost functions that guide the optimization of two-level controller. Moreover, this system employs retrieval-based prompt library, which strengthens few-shot grounding and system robustness. Beyond direct grounding, keypoints have also been adopted within structured frameworks that incorporate task semantics, relational constraints, and cross-domain knowledge. RAM [165] addresses the cost of in-domain data collection by constructing an affordance memory from diverse out-of-domain datasets. It uses VFMs for language-conditioned retrieval of relevant demonstrations, transferring 2D keypoints into 3D through probabilistic lifting, thus enabling zero-shot manipulation in novel environments. ReKep [128] formalizes manipulation as constraint optimization problem over tracked keypoints, where task goals are encoded as Python functions that impose geometric and relational costs among robots and objects. hierarchical solver plans SE(3) subgoals and optimizes actions via receding-horizon control, supporting bimanual and human-inthe-loop interaction with high spatial-temporal complexity. OmniManip [172] introduces an object-centric canonicalization process that maps objects to functional space. Within this structured space, keypoints act as reasoning primitives over which VLMs predict spatial constraints and interaction goals. To mitigate hallucinations and execution drift, it incorporates self-correcting loop that renders outcomes and resamples interaction points, while dual-level controller handles high-level planning and fine-grained pose tracking. An emerging direction extends static keypoints into temporal sequences, effectively transforming them into trajectory-based action tokens. This evolution enables systems to represent not only where to act but also how actions unfold over time. Magma [179] and VidBot [180] both predict sequences of keypoint positions conditioned on task instructions and visual observations, capturing fine-grained temporal dynamics for object-centric manipulation. By modeling temporally grounded keypoints, these systems support longer-horizon reasoning and enable temporally consistent action planning. This temporal extension enhances expressiveness and planning capability, offering natural bridge between spatial affordance and trajectory-level representations. 6.2. Bounding Boxes: Coarse Grounding Bounding boxes provide coarse yet efficient representation for instance-level localization in the visual scene. 2D bounding box is typically defined as = {(ùë•tl, ùë¶tl), (ùë•br, ùë¶br)}, marking the top-left and bottomright image-plane corners. In 3D, bounding boxes are commonly represented by eight spatial corner points {(ùë•ùëñ, ùë¶ùëñ, ùëßùëñ)ùëñ {1, . . . , 8}}, encoding the objects physical extent within the scene. While these representations lack fine-grained geometric detail, they offer robustness and computational simplicity. The advent of powerful open-vocabulary detectors (e.g., Grounding DINO [181], Detic [182], and OWL-ViT [183]) and VLMs (e.g., Qwen2.5-VL [12]) creates strong connection between visual understanding and physical manipulation by effectively localizing objects based on free-form language queries into bounding boxes. Several VLA models leverage bounding boxes to ground language instructions into object-centric visual inputs. DexGraspVLA [32] grounds domain-varying referential expressions by localizing domain-invariant bounding boxes of the target objects, which are then converted into segmentation masks. These masks are tracked across time using Cutie [88], enabling temporally consistent visual grounding throughout the grasping process. This pipeline illustrates broader trend in recent work: using bounding boxes as modular interfaces that connect referential language to spatially localized object representations. Bounding boxes serve as efficient perceptual abstraction that simplifies the mapping from language to actionable visual input, enabling task specification in open-vocabulary settings without requiring dense supervision. Beyond object localization by language instructions, bounding boxes can also support interaction inference and downstream action generation. Wake et al. [166] employs GPT-4V to process human demonstration videos, integrating hand and object bounding boxes to detect grasp and release events via spatial proximity. 21 Survey on Vision-Language-Action Models: An Action Tokenization Perspective These spatiotemporal cues serve as the basis for extracting affordance-relevant information, including grasp strategies and waypoint trajectories, which are then translated into robot-executable code. Extending this direction, A3VLM [162] models object articulation using structured triad comprising 3D bounding box, movement axis, and semantic label. To enable the prediction of this triad, it introduces dataset of object-level articulation annotations and fine-tunes the Llama-2-7B model with projection layer. Crucially, this robot-agnostic representation can translate directly into low-level robot action by simple action primitives, enabling generalization across diverse platforms and significant manipulation performance. 6.3. Segmentation Masks: Pixel-Level Regions Segmentation masks provide high-resolution spatial representations that capture fine-grained object contours and part-level geometry, enabling precise grounding of functional regions such as wipeable surfaces or graspable areas. Formally defined as binary matrices {0, 1}ùêªùëä , masks offer pixel-level detail that surpasses coarser abstractions like bounding boxes. With the advent of foundation models such as SAM [8] and Florence-2 [184], the quality and generalization of language-conditioned segmentation have significantly improved. Recent VLA models leverage these capabilities to extract affordance-aligned object regions from textual instructions. MOO [161] utilizes OWL-ViT to extract object representations, which are fused with textual instructions to inform policy learning in open-world manipulation tasks. SoFar [163] segments object masks using SAM, then uses them to construct object-centric point clouds and orientation-aware scene graphs. These representations guide PointSO in predicting functional directions (e.g., handle facing up) and support structured spatial reasoning. RoboDexVLM [167] adopts coarse-to-fine refinement pipeline to obtain high-quality masks, which are used to predict end-effector grasp poses via AnyGrasp [185]. Together, these methods demonstrate that segmentation masks provide structured, task-aligned representations that bridge perception and control in contact-rich manipulation tasks. more recent direction explores the use of segmentation masks as temporally anchored interaction interfaces. ROCKET-1 [174] introduces hierarchical system that leverages segmentation sequences extracted and tracked across time via SAM 2 [9] as persistent visual prompts. These temporally grounded masks support high-level reasoning and coherent action selection in dynamic environments, enabling robust object manipulation without fixed task templates. 6.4. Affordance Maps: Dense Spatial Fields Affordance maps represent scenes as spatial fields that assign each region graded suitability score for specific actions, reflecting prior interaction awareness. Typically it is formulated as ‚Ñùùêªùëä , where ùêª and ùëä denote spatial resolution. These maps encode object geometry, surface topology, and task-specific priors, enabling dense and instruction-conditioned interaction reasoning. CLIPort [175] adopts two-stream network to fuse semantic and spatial features for affordance prediction, guiding precise pick-place actions. IGANet [186] learns to generate pixel-wise affordance distributions conditioned on language inputs, allowing the same object to afford different actions under varying instructions. VoxPoser [16] expands this concept by prompting LLMs to synthesize affordance and constraint specifications in code form, which are then grounded to perceptual space via VLMs to form 3D value maps. These maps enable zero-shot trajectory synthesis over diverse tasks and objects without retraining. Beyond spatial grounding, affordance maps also support reasoning about physical contact and manipulation dynamics. ManipLLM [168] incorporates affordance maps into multimodal chain-of-thought framework, using them to encode region-level priors that guide manipulation-aware pose generation. The maps indicate where actions are most likely to induce meaningful object motion, improving precision and stability in complex scenes. ManiFoundation [130] further extends this line of work by treating manipulation as contact synthesis, leveraging force and motion heatmaps to represent contact-centric affordances. These maps encode where contact should occur, the force to apply, and the expected motion trajectory, enabling robust contact prediction for both rigid and deformable objects. As task complexity increases, such structured affordance priors offer scalable solution for grounding low-level control in physically realistic interaction fields. 6.5. Discussion and Future Directions Despite their advantages, affordance-based action tokens face several limitations that hinder effectiveness in real-world manipulation. First, most VLA models rely on 2D image representations, which inadequately capture the 3D geometry and spatial relationships required for precise control. Although models like A3VLM [162] and SoFar [163] incorporate partial 3D information, they still fall short in tasks involving complex object shapes and occlusions and scenarios common in dynamic (e.g., inserting components into moving assemblies) or delicate 22 Survey on Vision-Language-Action Models: An Action Tokenization Perspective (e.g., fine-grained part assembly) manipulations. Second, affordance tokens typically encode static object properties such as graspable handle or closeable door without modeling how these affordances evolve over time. These limitations impair their effectiveness in contact-rich tasks that demand continuous reasoning about changing affordance states. Finally, affordance representations are vulnerable to visual perturbations such as occlusion and motion blur. Specifically, keypoints degrade significantly under occlusion, and segmentation masks lose accuracy in visually challenging scenes, compromising manipulation performance. To address these challenges, we identify three promising research directions. Learning True 3D Affordances. critical next step is to move beyond 2D or projected 3D and learn affordances directly within native 3D representations. By grounding policies in structures like Neural Radiance Fields [187], 3D Gaussian Splatting [188], or explicit meshes, models can develop holistic understanding of object geometry, free space, and occlusion. This approach would unlock robust reasoning for complex tasks currently beyond reach, such as inserting part into hidden cavity or manipulating non-rigid objects in clutter. Modeling Temporal Affordance Dynamics. Future models should learn to predict how actions alter an objects affordances over time. For example, model should infer that executing lift lid action transitions the affordance state from openable to pourable. This temporal reasoning is fundamental for enabling long-horizon planning and succeeding in contact-rich, sequential tasks. Enhancing Policy Robustness and Uncertainty-Awareness. Real-world deployment demands policies that are resilient to visual ambiguity and aware of their own limitations. This requires dual focus. Models should be trained for greater robustness against visual perturbations using techniques like advanced data augmentation. And policies should quantify their own uncertainty by outputting probabilistic affordances. 7. Trajectory as Action Tokens One of the central challenges in scaling VLA models lies in the limited availability of robot data, particularly those annotated with action labels. To address this constraint, recent studies [37, 189, 190, 191] have proposed leveraging off-domain video data, which typically lacks explicit action annotations. These works use trajectories as proxy for action representations since they can be readily extracted from videos and encapsulate rich, actionable information across the entire manipulation process. We summarize representative trajectory-based methods in Table 5. In comparison with latent representations (Section 9) proposed by other works [110, 39, 18, 192, 193, 194, 195, 196], trajectory is relatively explicit action representation that is both explainable and understandable by humans, facilitating training and debugging. Another major challenge in VLA research is task generalization. For instance, policies conditioned on language-based action tokens often struggle to generalize zero-shot across semantically different tasks with similar low-level motion patternssuch as generalizing from wiping table to sliding block on desk. In contrast, trajectory-conditioned policies exhibit stronger generalization capabilities across such tasks, as demonstrated by RT-Trajectory [36]. 7.1. Overview of Trajectories Trajectory-based action tokens can be categorized into three distinct forms: Point Trajectory, Visual Trajectory, and Optical Flow. Each represents motion with different level of abstraction and information density. Point Trajectory is the most direct approach, encoding an action as sequence of discrete points, denoted as ‚Ñùùëá ùêæ2. This method models the path of ùêæ critical points over time span ùëá, which offers targeted and numerically precise guidance. In autonomous driving, models predict future vehicle waypoints in Birds Eye View (BEV) space [28, 30, 29, 197]. For robotic manipulation tasks, they generate 2D coordinate paths for end-effectors or objects within the image plane [37, 198]. Visual Trajectory directly renders path into the pixel space. Instead of just list of coordinates, the output is new image or video where the intended motion is visually depicted. This can be achieved by overlaying point sequences onto observation frames [190, 36] denoted as ‚Ñùùêªùëä 3 or by generating video flow [191] that materializes as visible curves over time, such as ‚Ñùùëá ùêªùëä 3. This form is highly interpretable as it shows the action in its visual context. Optical Flow offers the densest representation, formulated as motion field ‚Ñùùêªùëä 2. This field describes the motion of every pixel between frames, capturing the holistic dynamics of the entire scene rather than single path. By treating the collective movement of the scene as the action signal, this method can model complex, multi-object interactions implicitly [189, 199]. 23 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 5 Overview of VLA research using trajectory as action tokens. The Format column categorizes action tokens into three types: Point Trajectory, representing the paths of few keypoints; Visual Trajectory, representing path drawn directly onto the image; and Optical Flow, representing the motion of all pixels. ùëá is the temporal span, ùêæ the number of points, and ( ùêª, ùëä) the image resolution. Paper Previous Module Format Next Module Task Embodiment Model Training Details Model Training Details AVDC [189] Video diffusion model, GMFlow Diffusion model trained on Bridge [200] and 20 human demonstrations; GMFlow frozen Optical Flow ‚Ñùùêªùëä Rigid body transformation regression N/A Meta-World, iTHOR [201]; tabletop manipulation Franka Panda (RealSense D435) RT-Trajectory [36] Code as Policies / PALM-E Frozen Visual Trajectory ‚Ñùùêªùëä 3 RT-1 Trained on RT-1 dataset ATM [37] Track Transformer Trained on 50 action-free video demonstrations Point Trajectory ‚Ñùùëá ùêæ2 Transformer, MLP Trained on 10 action-labeled demonstrations LLARVA [203] LLaVA 1. Trained on 8.5M image-visual trace pairs Point Trajectory ‚Ñùùëá ùêæ2 (ùêæ = 1) N/A N/A Pick-place, open/close drawer, fold towel, swivel chair LIBERO [202] (simulation); pick-place, squeeze objects (real-world) RLBench (simulation); pick cubes, stack/destack cubes, (real-world) Everyday Robots UR5 with gripper Franka Panda Im2Flow2Act [191] Grounding DINO, TAPIR, CLIP, AnimateDiff Grounding DINO frozen; The decoder from Stable Diffusion fine-tuned; AnimateDiff fine-tuned via LoRA on human demonstration videos Visual Trajectory ‚Ñùùëá ùêªùëä 3 State encoder, temporal alignment module, diffusion action head Trained on 4800 simulated robot exploration data for 500 epochs Pick-place, pouring, open drawer, fold cloth UR5e with WSG-50 gripper (RealSense D415) FLIP [198] CVAE with transformer Trained on 40 videos Point Trajectory ‚Ñùùëá ùêæ2 Diffusion Policy Trained on 10 demonstrations with action labels and 50 demonstrations without action labels LIBERO-LONG, FMB [204] (simulation); fold cloth, unfold cloth (real-world) xArm6 (2 RealSense D435i) HAMSTER [190] VILA-1.5-13B N/A indicates not applicable. Fine-tuned on 770K object location tasks, 320K simulated 2D end-effector paths, 110K real robot 2D end-effector paths, 660K VQA Visual Trajectory ‚Ñùùêªùëä 3 RVT-2 / 3D-DA Trained on 320 teleoperation episodes Pick-place, knock down objects, press button Franka Panda 7.2. Progress and Key Papers Data scarcity has long been bottleneck in robotics. Trajectory-based action tokens offer solution by enabling learning from abundant off-domain videos. AVDC [189] predicts future frames using diffusion model trained on human or robot demonstration videos and generates optical flow using pretrained models, guiding downstream control with depth information. However, this is computationally expensive and prone to hallucinations. ATM [37] mitigates these issues by predicting trajectories of arbitrary points and requires only small amount of in-domain action-labeled data for low-level policy training. In contrast, Im2Flow2Act [191] requires no real-world robot data. It learns to generate video trajectories from human demonstration videos and trains trajectory-conditioned policy using simulation data. To bridge the embodiment gap, Im2Flow2Act focuses on object flow instead of arbitrary point flow. FLIP [198] incorporates world model built from videos, including dynamics, action, and value modules. It performs model-based planning and predicts action conditioned on both flow and video plan. Compared to ATM, FLIP samples denser flow points and achieves better performance, demonstrating the effectiveness of dense flow in low-level control. Trajectory-based action tokens demonstrate strong generalization across tasks, as well as visual and semantic variations. Even when tasks are semantically distinct, shared motion patterns in trajectory space enable cross-task generalization. For example, RT-Trajectory [36] encodes tasks via coarse 2D or 2.5D end-effector motion trajectories, on which an end-to-end policy (i.e., RT-1) is conditioned. RT-Trajectory outperforms RT-1 [205], RT-2 [206], and RT-1-Goal (RT-1 conditioned on goal images) on unseen tasks. In comparison with RT-Trajectory, HAMSTER [190] adopts hierarchical architecture, using VLM to synthesize 2D trajectories and low-level policy conditioned on 3D observations. This structure facilitates fine-tuning on large-scale 24 Survey on Vision-Language-Action Models: An Action Tokenization Perspective off-domain datasets, such as RoboPoint [170], thus improving its visual and semantic generalization. Another direction focuses on pretraining large models on trajectory-centric data. LLARVA [203] constructs unified robotic LLM via instruction tuning, incorporating structured information such as control mode, task, and proprioception. It outputs 2D trajectories and robot actions in text, showing greater flexibility across control modes. Despite leveraging 8.5M vision-action pairs from Open X-Embodiment (OXE) [207], its scale remains smaller than conventional LLM/VLM datasets. To leverage broader datasets, ARM4R [208] introduces threestage training paradigm: pretraining on EPIC-KITCHENS-100 [209], fine-tuning on 1-2K robot demonstrations, and predicting proprioceptive states. Its 4D trajectory representation enables superior performance over LLARVA and ATM. Magma [179] is foundation model for both UI navigation and robotic manipulation, which is trained on heterogeneous datasets with Set-of-Mark and Trace-of-Mark, endowing it with spatial-temporal reasoning capabilities that surpass VLA models trained solely on robot data like OpenVLA [21]. 7.3. Trajectory-Related Data Various types of data can be utilized to train trajectory-based VLA, such as internet-scale vision-language datasets, human videos, and existing robot data. Web-scale vision-language pairs can instill broad common sense into the policy. Some approaches [190] utilize VLMs to directly output keypoint sequences, which require vision-language datasets such as object location tasks [170] in the co-training phase to keep the VLMs generalization ability. Human and robot demonstrations further provide specific actionable knowledge. Trajectory labels can be directly extracted from existing videos without human annotations. One option is to use point tracking tools such as CoTracker [90], TAPIR [210], or optical flow methods like RAFT [211]. Another line of work, such as RT-Trajectory [36], extracts 2.5D trajectories from robot demonstrations using end-effector states. In either way, all the existing demonstration datasets, no matter human, simulated or real-robot, can be utilized with ease. In autonomous driving, trajectories and captions can also be automatically generated using pipelines like that in CoVLA [28], which combines Kalman Filter [212]-based trajectory prediction with rule-based and VLM-driven captioning. 7.4. Discussion and Future Directions Despite their advantages, trajectory-based action tokens face several key challenges. We identify three main areas: 3D spatial understanding, computational efficiency, and task suitability. Most work utilizes 2D trajectories, but 2D trajectories lack explicit 3D information. This can introduce ambiguity and restrict their applicability to non-planar tasks. Depth data serve as critical supplement: AVDC [189], RT-Trajectory [36], and HAMSTER [190] all incorporate depth information to mitigate this issue and provide richer 3D understanding. more fundamental challenge, however, is that point trajectories typically encode only position. They omit crucial orientation information, making them ill-suited for complex dexterous manipulation tasks. Future work could explore integrating full 3D spatial information into trajectory representations. Another significant challenge is computational efficiency. Many methods employ generative models to predict trajectories or videos, which are computationally expensive to train and to inference [189, 37, 191]. Other methods leverage VLMs to predict trajectories, but VLMs often output waypoints at low frequency, insufficient for smooth control [36, 203, 190]. One solution is to use traditional planning methods to refine these sparse outputs into high-frequency control signals [30]. To avoid re-planning at every timestep, other approaches predict full trajectory once and use temporal alignment module for real-time execution [191]. Developing lightweight yet expressive trajectory generation models remains critical research direction. Finally, the suitability of trajectory depends on the task and environment. Trajectories excel at tasks defined by precise motion paths, such as surface wiping or navigation. However, they are less effective in partially observed settings where complete path cannot be planned upfront. Furthermore, they lack the semantic richness for tasks involving complex interaction logic and do not inherently capture concepts like applying force or understanding object affordances. promising future direction involves creating hybrid action tokens that combine trajectory tokens with semantic concepts (e.g., grasp, increase force), enabling robots to handle wider and more complex set of tasks. 8. Goal State as Action Tokens When humans approach manipulation tasks, our brains dont just translate raw perception directly into action. Instead, we often engage in mental simulation, envisioning the desired outcomes before executing any steps. For instance, if asked to clean up the table, one first conceptualizes neat and organized table, then works 25 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 6 Overview of VLA research using goal state as action tokens. Type Paper Previous Module Format Next Module Model Training Details Model Training Details Task Generalization Embodiment SuSIE [214] Instruct Pix2Pix Fine-tuned on BridgeData V2, Something-Something [215] Image Diffusion Policy Trained on BridgeData V2 (Goal-Conditioned Behaviour Cloning) CALVIN; tabletop manipulation (real-world) Environment, Object (zero-shot) WidowX250 r - n m - u 3D-VLA [38] Conditional Diffusion Model Fine-tuned via LoRA on OXE, HoNY, RH20T, EPIC-KITCHENS-100, HOI4D RGB-D image, Point cloud BLIP2-FlanT5XL Fine-tuned on 2M 3D-language-action data pairs RLBench, CALVIN; tabletop manipulation (real-world) CoTDiffusion [216] Semantic Alignment Module, Diffusion Model Trained on 10K trajectories with annotated ground truth keyframe for each task (coarse-grained pretraining, fine-grained train) Image ViT Encoder, MLP Trained on 10K trajectories VIMA-Bench [217] Environment (zero-shot) Placement, Object, Task (zero-shot) NR N/A CoT-VLA [218] VILA-U Fine-tuned on robot action data, videos without action labels (VILA-Us vision tower frozen) Image Full-Attention Module (based on VILA-U) Fine-tuned on task demonstrations collected on downstream robot tasks LIBERO; Bridge-V2 [219], tabletop manipulation (real-world) Environment, Task, Instruction (zero-shot) WidowX, Franka Panda UniPi [220] Conditional Diffusion Model, Temporal Super Resolution CDM pretrained and fine-tuned, TSR fine-tuned on 14M video-text pairs, 60M image-text pairs, LAION-400M, and Bridge Video CNN inverse dynamic model Trained from scratch AVDC [189] Conditional Diffusion Model Trained on Bridge Video GMFlow [221], rigid body transformation regression Frozen VLP [154] PaLM-E, UniPi PaLM-E fine-tuned and UniPi trained from scratch on 10000 long-horizon trajectories Video LAVA with ResNet encoder Trained on RT-1, Bridge, RT-2, Ego4D, EPIC-KITCHENS-100, LAION-400M Gen2Act [222] Gemini, VideoPoet Frozen Human video ViT, Bootstap, Tap-vid ViT trained on RT-1 and 400 diverse robot trajectories; Bootstap, Tap-vid frozen VPP [131] Stable Video Diffusion Fine-tuned on Something-Something V2, self-collected datasets, internet robot datasets Video Diffusion Policy Trained on self-collected datasets FLIP [198] CVAE with transformer, DiT, LIV CVAE and DiT trained on long-horizon videos with flow annotations; LIV fine-tuned on long-horizon imperfect videos Video Diffusion Policy Trained on few demonstrations with action labels Bridge; tabletop manipulation (real-world, simulation) Meta-World, iTHOR; tabletop manipulation (real-world) Tabletop manipulation, group color, make line (real-world, simulation) Tabletop manipulation (real-world, simulation) CALVIN, Meta-World; tabletop manipulation, tool using (real-world) LIBERO, FMB (simulation); fold cloth, unfold cloth (real-world) Instruction, Object, Environment (zero-shot) Environment (zero-shot) NR Franka Panda (Realsense D435) Lighting, Object, Task (zero-shot) Mobile manipulator; Bi-manual ALOHA Environment, Object, Task (zero-shot) Mobile manipulator with gripper Environment, Object (zero-shot) Franka Panda, Xarm with an Xhand Environment, Instruction (zero-shot) xArm6 (2 RealSense D435i) GEVRM [223] DiT Fine-tuned on Bridge, CALVIN Video Diffusion Policy Trained on Image-action label pairs CALVIN; Bridge Environment (zero-shot) UR5 with gripper N/A indicates not applicable; NR indicates not reported. backward to determine the necessary actions. Drawing inspiration from this powerful human cognitive strategy, growing body of research in VLA models proposes utilizing predicted goal statea visual representation of the tasks intended outcomeas an intermediate action token. These works, including recent advancements like 3D-VLA [38], FLIP [198], and VPP [131], aim to bridge the gap between high-level instructions and low-level actions by grounding the what to do in visually rich and interpretable form. Typically, models employing goal states as action tokens adopt hierarchical architecture. high-level model, often generative model like DiT [213] or CVAE, is responsible for synthesizing the goal state based on the current observation and language instruction conditions. This generated goal state then conditions lower-level model, such as diffusion policy or MLP, which translates it into the final sequence of actions. This setup effectively establishes the goal state as crucial mental simulation step, situated between comprehending the instruction and synthesizing the actions. Goal states can be broadly categorized into two primary types based on their temporal dimension: single-frame images and multi-frame videos. To give concise overview, Table 6 lists the principal methods discussed in this section. 8.1. Single-Frame Image as Goal State Single-frame goal states typically take the form of 2D RGB images, 2.5D RGB-D images, or 3D point clouds to depict the entire desired scene, as demonstrated in recent works [218, 216, 38, 214], offering many key advantages. For instance, LangLf [224] demonstrates how methods leveraging goal images can achieve easy data scalability via hindsight relabeling. This technique ingests unsegmented streams of robot play data, 26 Survey on Vision-Language-Action Models: An Action Tokenization Perspective automatically samples short windows, and treats each windows final frame as goal image. This process autonomously generates large-scale robot action dataset with goal image annotations, entirely bypassing the need for manual labeling. Building upon the utility of goal images for data scaling and low-level control, subsequent works integrate high-level goal image generation to create complete hierarchical VLA models. For example, SuSIE [214] first leverages simple image-generative model for visuo-semantic reasoning before deferring to low-level policy to determine precise motor actuations. Specifically, high-level diffusion model generates goal images from language instructions, and lower-level DDPM decodes those images into the required action sequence. CoTDiffusion [216] further extends SuSIEs hierarchical diffusion architecture by integrating semantic alignment module, which enables the diffusion model to assess its own task completion progress. Another significant advantage of using goal images is their ability to leverage action-free videos for training the high-level goal image generator. CoT-VLA [218], for example, exploits action-free human videos to train its goal image generator. Unlike the diffusion-based architectures mentioned above, both stages in CoT-VLA are autoregressive VLMs [225]: the high-level model uses causal attention to synthesize goal images, while the lower-level model uses non-causal attention to generate corresponding action sequences. Beyond standard RGB images, some works like 3D-VLA [38] have extended single-frame goal states to encompass RGB-D images and point clouds. By enriching the visual encoding with depth and 3D geometric configuration, these approaches provide more precisely grounded and perceptually rich depiction of task goals. 8.2. Multi-Frame Video as Goal State Multi-frame goal states (typically short videos) offer richer temporal context compared with single-frame goal states. By capturing how scenes evolve, this additional temporal dimension provides crucial how-to-do cues, significantly reducing execution ambiguity and offering finer-grained motion information. Research in this area leverages multi-frame goal states through various innovations: Generating from Large-Scale DataOne approach focuses on generating future video content from vast datasets to inform action. UniPi [220], for instance, pioneered using internet-scale data for text-conditioned video generation, with an inverse dynamics model (MLP) then computing actions from these predicted video sequences. Extracting Implicit Action Cues from VideosOther works concentrate on extracting explicit or implicit action-relevant information directly from the generated goal videos. AVDC [189], for example, enables the model to leverage dense correspondences within the video without relying on any action labels. It achieves this by using diffusion model to synthesize future video frames and then extracts dense pixel-wise optical flow from these frames, which can then guide the lower-level policy. This method effectively translates visual motion into actionable guidance. Enhancing Generalization and RobustnessMulti-frame goal states are also explored for improving model generalization and robustness. Acknowledging that embodiment-specific strategies limit broader generalization, Gen2Act [222] and FLIP [198] enhance cross-embodiment generalization by generating human-executed goal videos rather than robot-specific ones, thereby reducing the reliance on robot-specific fine-tuning. Similarly, GEVRM [223] introduces an auxiliary state-alignment loss specifically designed to improve robustness against external perturbations. Strategies for Complex Long-Horizon TasksFor complex, long-horizon tasks, researchers typically employ two main approaches. One common method, exemplified by works like Gen2Act, directly leverages LLMs to decompose long-range tasks into shorter subtasks, then subsequently runs the same model for each of these shorter segments. The second approach involves using multiple candidate goal videos for improved planning. VLP [154] generates and scores multiple candidate goal videos with separate VLM, using beam-search-like algorithm to select optimal long-term strategies for subtasks. Similarly, FLIP [198] adapts language-image valuing model (LIV) [226] to evaluate candidate human-executed goal videos (synthesized by DiT network from keypoint trajectories), then uses beam-search-like algorithm to choose the best long-term option. These methods demonstrate sophisticated planning with multi-frame goals. 8.3. Advantages of Goal State Goal states offer several key advantages that significantly boost their effectiveness as action tokens. Primarily, goal states provide great data scalability. This is enabled through hindsight goal relabeling, which allows for the autonomous generation of vast training datasets by extracting single-frame and multi-frame goal states from raw robot trajectories, fundamentally bypassing the action annotation bottleneck. Moreover, using goal states unlocks access to broader training data sources and enhanced generalization capabilities. Their generators can leverage large-scale action-free video data to learn real-world dynamics, improving overall generalization. Furthermore, training on human-executed goal states (e.g., Gen2Act [222]) specifically boosts 27 Survey on Vision-Language-Action Models: An Action Tokenization Perspective their cross-embodiment generalization, enhancing knowledge transfer across different robot platforms. Beyond data, goal states also enhance task specificity. By encoding highly precise spatial and visual information, they act as clear action tokens that reduce ambiguity in complex tasks, providing lower-level policies with accurate visual instructions for fine-grained action execution. These models also boast robust interpretability; their white-box training and inference processes make human understanding, debugging, and intervention more feasible. Additionally, goal states lend themselves to straightforward evaluation. Off-the-shelf language-image valuing models, like those adapted in FLIP [198], can easily assess goal-state quality by checking their alignment with language instructions. 8.4. Limitations and Future Directions Despite their notable advantages, goal states inherently possess several limitations. Generating high-quality and consistent goal states remains challenging, often manifesting as overspecification or outright inaccuracies. Overspecification occurs when the generated goal state contains unnecessary or overly precise details. This can lead the lower-level policy to focus on trivial aspects, overconstrain its flexibility, or even make the task harder to complete if those exact details arent critical, thereby undermining the policys generalization to slight variations in the environment or task execution. To mitigate this problem, VPP [131] synthesizes goal videos by performing only single denoising step with its high-level diffusion model, conveying only coarse action and omitting some fine-grained details, thus partially alleviating overspecification. Conversely, inaccuracies imply the generated goal state is fundamentally incorrect, inconsistent with the desired outcome, physically implausible, or exhibits temporal and spatial inconsistencies due to insufficient dynamics modeling [223]. Such erroneous goals directly provide misleading guidance, inevitably causing the lower-level policy to attempt wrong actions and resulting in task failure. Additionally, generating future images or videos inherently introduces high inference latency due to significant computational overhead. For instance, AVDC [189] requires approximately 10 seconds to synthesize an 8-frame goal video. This substantial delay is further compounded by the lower-level policys need to condition on these computationally intensive goal states for action sequence generation. Some approaches, like Gen2Act [222], achieve only 3 Hz inference speed, making real-time robotic control difficult. Even VPP, which mitigates some of this by performing only single denoising step when generating goal states, can still only achieve control frequency of 7-10 Hz. Goal states as action tokens represent an auspicious direction in VLA model development, offering superior data scalability, rich visual guidance, and strong interpretability. The rapid advancement of image and video generation (exemplified by diffusion models and large-scale video generation models) provides an increasingly solid foundation for this paradigm, as higher-quality and more temporally consistent visual content will better leverage the goal-specified nature of this approach by providing embodied agents with precise and rich visual guidance. Googles recently unveiled Veo 3 video generation model demonstrates exceptional performance in both image quality and physical constraint adherence. Beyond improvements in generation quality, several key research directions warrant exploration: improving computational efficiency to enable real-time robotic control, enhancing robustness to environmental variations for deployment in real-world scenarios, and developing more efficient approaches for long-horizon task planning, as current methods either rely heavily on LLM-based task decomposition (which is limited by the quality of subtask segmentation) or employ computationally expensive beam-search-like strategies for candidate goal evaluation. Addressing these limitations will be crucial for establishing goal states as highly effective and widely applicable action token in VLA models. 9. Latent Representation as Action Tokens Embodied AI faces fundamental challenge due to the limited availability of large-scale, embodiment-specific, and action-labeled datasets. To overcome this data bottleneck, researchers have turned to more scalable data sources, such as web-scale human activity videos (e.g., Ego4D [236]) and heterogeneous cross-embodiment robot datasets. Although these sources are abundant, they often lack explicit action annotations or suffer from significant embodiment gaps, making them difficult to leverage directly. promising approach is to extract unified, embodiment-agnostic latent action representations from such data, which encode high-level semantic behaviorssuch as grasping or turning leftand effectively model real-world dynamics to support robot learning. This idea, along with its extensions and variants, has been explored in series of VLA models that employ latent representations as action tokens. Typically, these methods are realized through three-stage pipeline, as illustrated in Figure 5. The initial Latent Construction stage constructs latent action space from large dataset in an unsupervised way, Survey on Vision-Language-Action Models: An Action Tokenization Perspective Figure 5 unified visualization of representative methods (rows) that utilize latent representations as action tokens, highlighting their diverse strategies for latent space construction, training, and inference (columns). Inst.: Instruction, ùëù: Proprioception, ùëô: Language Instruction. providing pseudo-labels for the subsequent stage. Next, in the Latent Pretraining stage, VLM is adapted to predict the appropriate latent actions given the current observation and instruction. The final Action Fine-tuning stage trains the VLA to translate the predicted high-level latent actions into low-level, executable commands for the target embodiment. Based on what these latent actions represent, the approaches are broadly categorized as either vision-based, action-based, or goal-based. Table 7 provides comprehensive overview of the representative methods discussed in this section. 9.1. Vision-Based Latent Representation Vision-based latent construction primarily utilizes VQ-VAE [232] style architecture to model visual state transitions. The model learns by reconstructing future goal observation from previous observations, conditioned on sequence of latent codes ùëß1:ùëÅ from the VQ-VAEs codebook. The information bottleneck inherent to this framework compels these codes to distill the visual transformations between states, which contain information about the underlying actions. Genie [110] exemplifies this approach, training on internet game videos to produce world model controlled entirely by latent actions. These learned actions demonstrate remarkable semantic consistency, enabling coherent control not only across different games but also when generalized to real-world robotic scenarios. LAPA [39] applies this method to robotic manipulation by tokenizing the learned discrete latent actions and employing VLM for latent action prediction. This strategy demonstrates superior cross-embodiment learning capabilities, outperforming pretraining on ground-truth action labels when the agents embodiment shifts between the pretraining and fine-tuning stages. GO-1 [18] further refines this approach using ùúã0 [22]-like architecture, which integrates VLM, latent planner, and diffusion-based action head into shared backbone through causal, layer-by-layer conditioning. This unified architecture 29 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 7 Overview of VLA research using latent representation as action tokens. Paper Latent Construction Previous Module Next Module Method Encoded Information Dataset Aligned Model Training Strategy Model Training Strategy OmniJARVIS [132] FSQ [227] Goal behavior from interaction trajectory Synthetic Data by IDM and LLM QueST [194] FSQ Latent skills abstracted from raw action LIBERO, Meta-World Dataset LAPA [39] NSVQ [228] Visual difference between nearby frames (1 frame apart) BridgeData V2, SomethingSomething V2, OXE LLaVA-7B Trained on data synthesized with an IDM and LLM Transformerbased latent action decoder Trained on data synthesized with an IDM and LLM CLIP, ResNet, Transformer Decoder Trained on benchmark dataset Transformerbased latent action decoder LWM-Chat1M(7B) Trained on crossembodiment dataset (8H100) MLP head Few-shot fine-tuned on task-specific demonstrations Few-shot fine-tuned on task-specific demonstrations GROOT-2 [193] VAE [230] Goal behavior from interaction trajectory Benchmark dataset Transformer Encoder, BERT Trained on task-relevant dataset Transformer-XL model Trained on task-relevant dataset Task Embodiment Minecraft; open-ended question answering, instruction following Mouse, keyboard (virtual agent) LIBERO, Meta-World Simulated robot Language-Table, SimplerEnv [229]; Tabletop manipulation (real-world) ALE Atari [231], Minecraft SkillForge [192], Language-Table, SimplerEnv Franka Panda; 14-DOF bi-manual robot Mouse, keyboard (virtual agent) GO-1 [18] VQ-VAE [232] Visual difference between nearby frames (30 frames apart) Web-scale vision-language data, Ego4D, Cross-embodiment robot data, AgiBot World InternVL2.5-2B Trained on crossembodiment dataset Diffusion-based action head UniVLA [233] VQ-VAE Visual difference between nearby frames (1 second apart, task-centric) Data mixture from OXE, GNM [234], and Ego4D Prismatic-7B Trained on mixture dataset (A100 960h) Simple action head Pretrained on AgiBot World, Few-shot fine-tuned on task-specific data Household ( restock, fold, wipe, pour) AgiBot G1 Few-shot fine-tuned on task-specific demonstrations LIBERO, CALVIN, SimplerEnv, R2R [235]; manipulation tasks (real-world) Piper arm (from AgileX Robotics) with gripper predicts latent action and generates fine-grained, high-frequency motion for downstream tasks. Real-world experiments validate the latent planners effectiveness by demonstrating performance gains over baselines without it. However, key challenge with vision-based methods is that the resulting latent space can inadvertently capture task-irrelevant visual variations, such as background clutter or camera shakiness. UniVLA [233] mitigates this issue by first transforming raw pixels into patch-level semantic features via DINOv2 [7]. It then employs two-stage training scheme that uses language instructions to explicitly disentangle the latent space into task-centric and task-irrelevant action tokens. Ablation results show that the latent space constructed by UniVLA proves 6.4% more effective than that produced using Genies approach. 9.2. Action-Based Latent Representation Different from vision-based approaches, another line of work adopts action-based latent representation, which learns latent skill space by directly encoding and reconstructing action chunks of fixed length ùêª. For instance, QueST [194] applies FSQ [227] to these chunks from multi-task manipulation dataset, learning task-agnostic vocabulary of action primitives (e.g., reaching, grasping, or lifting). Experiments confirm the value of this approach: visualizations show that semantically similar behaviors cluster together, and the learned skills demonstrate effective few-shot transfer to new tasks. While effective, this approachs reliance on action-labeled data for the pretraining stage limits its scalability and cross-embodiment generalization. 9.3. Goal-Based Latent Representation Distinct from methods that model short-term visual transitions or action primitives, goal-based representations encode an entire tasks trajectory into latent vectors that represent the overall goal. This paradigm has proven particularly effective in virtual open-world environments, such as Minecraft [237]. Pioneering methods in this domain, such as GROOT [192] and GROOT-2 [193], employ VAE [230] to encode the observation sequences of the entire task into sequence of continuous latent vectors. Subsequently, decoder, conditioned on these latent vectors, causally reconstructs the corresponding action sequence from observations. However, as discussed in GROOT-2, this latent space is prone to two failure modesmechanical imitation of low-level trajectories and posterior collapse, leading to deviation from the intended goal information. To better align the latent space with task-relevant goals and address these issues, GROOT-2 introduces weak supervision by encouraging the encoded latent goals to match the encoded language instructions through an MLE objective. Despite these improvements, these methods lack reasoning and long-horizon planning capabilities. OmniJARVIS [132] Survey on Vision-Language-Action Models: An Action Tokenization Perspective addresses this by adapting VLM to jointly model discrete latent goals alongside vision and language tokens encompassing observation, instruction, memory, and thought. This approach ensures both strong reasoning and efficient decision-making capabilities, as demonstrated by its capacity to answer Minecraft-related questions and successfully execute complex, long-horizon tasks such as mining diamonds, which were previously unachievable. 9.4. Advantages of Latent Representation Leveraging latent representations as action tokens yields several key advantages in scalability, training efficiency, and expressive power. Primarily, vision-based latent representation enables models to scale across action-free, internet-scale human videos and cross-embodiment robot datasets, fostering an embodimentagnostic understanding of physical dynamics that enhances generalization and allows for efficient downstream embodiment-specific fine-tuning. This scalability is complemented by significant gains in training efficiency. By encoding high-level kinematic semantics into compact sequence, the latent space presents much simpler pretraining target for VLMs than raw action. For instance, UniVLA achieves performance comparable to OpenVLA [21] using only 4.45% of the training time. Finally, latent representations offer strong expressive potential due to their ability to learn more compact and efficient structures, implicitly encode task-relevant semantics that are difficult to specify through explicit formats, and support the integration of non-visual and non-linguistic modalitiessuch as tactile feedback and audiothat are typically inaccessible to languageand vision-based action tokens such as language plans or keypoints. 9.5. Limitations and Future Directions Although latent representations offer the aforementioned advantages, key limitation lies in their inherent lack of explainability and controllability, which prevents humans from intervening or correcting policy failures, as is possible in methods like RT-H [31], thereby making interpretation and debugging more difficult. Therefore, latent representations may be unsuitable in scenarios where strict safety or reliability guarantees are required. Given the inherent uninterpretability of latent representations, the properties and quality of their construction become critically important. Future research should therefore concentrate on three key directions. The first is achieving appropriate granularity: the latent space must be fine-grained enough to represent the subtle variations required for dexterous tasks, yet abstract enough to avoid unnecessary complexity and rote memorization. Current vision-based approaches often suffer from inadequate granularity and low reconstruction fidelity, limiting their effectiveness in highly dexterous tasks such as fine-grained manipulation. The second is comprehensiveness: the latent space must encompass the full spectrum of behaviors required for given task domain, as an incomplete vocabulary of behaviors will inevitably lead to policy failures when the agent encounters situations outside its learned repertoire. The third crucial focus is ensuring strong alignment with human intention. As highlighted in the discussions of UniVLA and GROOT-2, latent spaces derived from both vision and action data can inadvertently encode information irrelevant to the given instruction. Developing robust methods to disentangle task-centric signals from this noise is therefore essential. We believe that progress focusing on these three axesimproving representational granularity, comprehensiveness and strengthening alignment with human intentionwill be critical for advancing the capabilities and reliability of approaches that leverage latent representation as action tokens. 10. Raw Action as Action Tokens In the previous sections, we discussed various forms of action tokens that encode actionable guidance. These tokens typically serve as intermediate outputs of VLA modules, which are ultimately mapped to raw actions. Each form of action token exhibits distinct characteristics, making it suitable for particular domains. However, choosing an appropriate token representation can be non-trivial. In such cases, straightforward and intuitive alternative is to formulate VLA models as direct mapping from vision and language inputs to raw actions. This strategy is further motivated by the success of foundation models, which are trained on large-scale, diverse, task-agnostic datasets and are able to achieve strong performance on downstream tasks in zero-shot or few-shot manner, demonstrating generalization and scalability. Similarly, the typical approach is to collect large-scale real-world robot datasets with natural language annotations and train VLA models end-to-end to directly predict raw actions. The overarching objective is that, as the dataset grows in size and diversity and base models become more capable, the resulting VLA model can learn general-purpose robotic policy. Given the strong parallels between this training paradigm and that of foundation models, many techniques and best practices developed in the foundation model community can be inherited and adapted to this setting. 31 Survey on Vision-Language-Action Models: An Action Tokenization Perspective This section reviews the progress along this direction, with representative works summarized in Table 8. 10.1. Vision-Language Feature Fusion In the early stages, the most common approach involves fusing vision and language modules to obtain multimodal features for downstream tasks. These fused representations are then mapped to raw actions through simple layers. LangLf [224] represents one of the earliest VLA models. It uses MLP and CNN to encode the inputs and employs CVAE decoder to generate action sequences. To scale up data volume, LangLf combines 10M goal-image-conditioned state-action pairs with 10K human-labeled language-conditioned samples. BCZ [239] is one of the first works to collect large dataset (26K robot data and 19K human videos) to study how data scaling helps generalizable policy training. It utilizes ResNet [80] and multilingual sentence encoder [45] but improves the fusion process by using multi-stage FiLM conditioning [262], which dynamically modulates visual features based on language inputs. This approach allows more fine-grained instructions grounding and decodes actions with simpler MLP. 10.2. Transformer-Based Generalists Building on the success of scaling laws in LLMs, subsequent works have taken further steps to construct larger datasets, include more diverse task domains, and adopt autoregressive transformer backbones, with the goal of training generalists. VIMA [217] uses Mask R-CNN [263] and ViT [81] to extract object tokens from visual observations, which are then concatenated with language tokens and processed by pretrained T5 model [46] to produce multi-modal prompt tokens. These tokens are used as inputs to cross-attention layers for decoding robot actions. Gato [240] successfully trains large decoder-only transformer model (1.2B parameters) on the combination of 596 control tasks (totaling 1.5T tokens) and 8 vision-language datasets. The Gato model is capable of performing wide range of tasks across different domains, such as Atari games, robotic manipulation, VQA, and chatting tasks. By unifying vision, language, and action tokens, Gato demonstrates that single autoregressive model can serve as multi-modal, multi-task, and multi-embodiment generalist policy. LEO [242] extends this concept by incorporating additional 3D datasets to enhance the models 3D reasoning ability, elevating the generalist model into the 3D space. This improvement strengthens LEO in embodied reasoning and planning tasks. JARVIS-VLA [256] is Minecraft VLA model fine-tuned from pretrained VLM models (Qwen2-VL or LLaVA-NeXT). While previous VLA models typically apply imitation learning directly to fine-tune VLM on large-scale datasets for action prediction, JARVIS-VLA adopts three-stage fine-tuning strategy: (1) text-only world knowledge fine-tuning, (2) multi-modal vision-language alignment and spatial grounding, and (3) instruction-following imitation learning. 10.3. Autoregressive Robot VLA With increasing attention on robotics, RT-1 [205] introduces the largest robotic manipulation dataset at the time, featuring 130K demonstrations across over 700 tasks, and trains transformer-based model for real robots. It utilizes FiLM-conditioned EfficientNet, allowing language to modulate visual features. The transformer decoder then autoregressively generates raw actions. RT-1 demonstrates strong performance on seen tasks, generalizes well to unseen tasks, and shows robustness to distractors and varying backgrounds. Its performance further improves with the incorporation of simulation data. Moreover, incorporating data from different robotic platforms (Everyday Robots and Kuka) enables generalization across diverse embodiments. RT-2 [206] further advances this with more streamlined, end-to-end design that maximizes knowledge transfer from foundation models. It fine-tunes web-scale pretrained VLMs (PaLI-X [264] and PaLM-E [137]) into end-to-end VLAs (RT-2-PaLI-X and RT-2-PaLM-E), which directly output raw actions. The raw robot actions are discretized into action bins, enabling autoregressive inference in the same manner as VLM. Importantly, this approach mitigates the need to modify the original architecture of the foundation models. By leveraging foundation VLM as backbones and co-training on both vision-language and robot action data, RT-2 exhibits enhanced reasoning and generalization capabilities. It demonstrates emergent abilities beyond its training data during test-time inference. Moreover, RT-2 with chain-of-thought reasoning can interpret and respond to complex commands, highlighting the significant advantages of using VLMs as the backbone for VLA models. To enhance dataset scale and diversity for improved policy generalization, OXE [207] introduces unified dataset comprising over 1 million trajectories collected from 22 different robots. Empirical results of retraining RT-1/2 on this dataset show that cross-embodiment training leads to substantial performance gains, and model capacity plays critical role in data-rich settings. Although the RT-2 model has significant impact, its training code and models have not been publicly released. 32 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 8 Overview of VLA research using raw action as action tokens. Paper Action Head Type LangLf [224] CVAE Action Token Format Model Training Strategy Task Embodiment Frequency 6-DoF Cartesian position, euler angle of the end effector, 2-DoF gripper angle Language encoder: simple MLP; Vision encoder: simple CNN; Action head: CVAE Trained on 10K language-conditioned and 10M goal-image-conditioned robot data (8V100 72h) Tabletop manipulation (3D Playroom [238]) 3D Playroom 30 Hz BC-Z [239] Multi-head MLP 6-DoF Cartesian position, axis-angle of the end effector, delta form, 1-DoF gripper angle (3-DoF mobile base) Language Encoder: MUSE; Video Encoder: ResNet-18; Backbone: FiLM-conditioned ResNet-18, multi-head MLP action head Gato [240] Autoregressive transformer Task-specific text/ control action 1.2B decoder-only transformer Trained on 26K robot data and 19K human videos of 100 tasks Tabletop manipulation Everyday Robots 10 Hz Trained jointly on 1.5T tokens from 604 tasks across VQA, game, robot control using 1616 (TPU v3 96h) DM Lab, ALE Atari; RGB Stacking Benchmark (simulation, real-world) DM Lab, ALE Atari; Sawyer (real-world) 20 Hz VIMA [217] Autoregressive transformer RT-1 [205] Autoregressive transformer RT-2 [206] Autoregressive transformer Two SE(2) poses (pick/place or push start/end), discretized into bins 6-DoF end effector pose, 1-DoF gripper state, 3-DoF base, 1 mode token, 256 bins per dimension, delta, single-step 6-DoF end effector pose, 1-DoF gripper state, 1 termination command, delta, single-step Multimodal prompt encoder: Mask R-CNN, ViT, T5; Backbone: transformer Trained on 650K trajectories of 17 tasks in VIMA-Bench (8V100 24h) Tabletop manipulation (VIMA-Bench) UR5 with suction cup or spatula N/A Language encoder: USE; Vision encoder: FiLM-conditioned EfficientNet-B3; Backbone: TokenLearner, transformer Trained on RT-1 dataset consisting of 130K episodes across 700+ tasks Mobile manipulation (office kitchen) Everyday Robots 3 Hz PaLI-X (5B/55B) / PaLM-E (12B) Co-trained on RT-1 dataset and web-scale vision-language data Mobile manipulation (office kitchen) Everyday Robots 1-3 Hz (55B), 5 Hz (5B) RT-X [207] Autoregressive transformer Same as RT-1 or RT-2 RT-1/RT-2 RT-1 and RT-2 trained on OXE subset including 9 embodiments to obtain RT-1-X and RT-2-X Small-data and large-data domains within OXE Task-specific embodiments 3-10 Hz RoboFlamingo [241] LSTM, MLP 6-DoF end effector pose, 1-DoF binary gripper state, delta, single-step OpenFlamingo, action head LEO [242] Autoregressive transformer GR-1 [247] MLP Navigation: 4 discrete commands (forward, left, right, stop); Manipulation: 6-DoF pose Image encoder: OpenCLIP ConvNext; 3D encoder: PointNet++, Spatial Transformer; Backbone: Vicuna-7B 6-DoF end effector pose, 1-DoF binary gripper state, delta, single-step Language encoder: CLIP; Vision encoder: MAEpretrained ViT, perceiver resampler; Backbone: GPT-style transformer Trained on CALVIN dataset (8A100 39h for 3B version, 104h for 9B version) Trained on LEO-align (1.03M 3D-VL pairs from Objaverse [243], ScanNet [244], 3RScan [245]) and LEO-instruct dataset (505K multi-task data covering QA, planning, navigation, manipulation, etc.) CALVIN Franka Panda NR 3D VQA, captioning, dialogue and planning; object navigation (MP3D ObjNav [246]), robotic manipulation (CLIPort) Navigation: AI Habitat; Manipulation: CLIPort Video generation pretraining on 800K video clips containing 8M frames from Ego4D, robot data fine-tuning on task-specific data CALVIN, transport objects; articulated manipulation (real-world) Franka Panda (simulation) Kinova Gen2 (real-world) NR NR Octo [248] Diffusion 6-DoF end effector pose, 1-DoF gripper state, delta, action chunking Language encoder: T5-base (frozen); Vision encoder: CNN; Backbone: transformer, lightweight diffusion head Pretrained on 800K trajectories from 25 datasets in OXE (TPU v4-128 pod 14h); Fine-tuned on 100 trajectories (A5000 5h for each task) Zero-shot: in-distribution manipulation tasks; Fine-tune: new manipulation tasks on new embodiments OpenVLA [21] Autoregressive transformer 6-DoF end effector pose, 1-DoF gripper state, delta, single-step Prismatic-7B VLM Pretrained on 970K robot episodes from OXE (64A100 336h); Fine-tuned on 10150 demonstrations for each task (support parameter-efficient fine-tuning) Zero-shot: manipulation tasks in BridgeData V2 and Google Robot Evaluations; Fine-tune: Franka-Tabletop, Franka-DROID, LIBERO Zero-shot: WidowX, UR5, Everyday Robots; Fine-tune: Franka Panda, ViperX, ALOHA 5-15 Hz WidowX, Google Robot, Franka Panda 6 Hz on RTX 4090 TinyVLA [249] Diffusion 7-DoF end effector pose, 1-DoF gripper state, absolute positions, single-step Vision encoder: ViT; Backbone: Pythia; Action head: Diffusion VLM pretraining using LLaVA pipeline and dataset; Parameter-efficient fine-tuning on 100 robot data for each task Tabletop manipulation Franka Panda, Bimanual UR5 NR HiRT [250] MLP 6-DoF end effector pose, 1-DoF gripper state, delta, single-step Understanding module: InstructBLIP-7B (LoRA); Execution module: lightweight policy with visual encoder (EfficientNet-B3 for simulation, ViT-B/16 for real-world) and conditioned action head Trained on Meta-World (20 tasks, 50 demonstrations each), Franka-Kitchen (5 tasks, 100 demonstrations each), and 4 real tasks with 2000 custom trajectories; VLM fine-tuned with LoRA Meta-World, Franka-Kitchen; pick-place, press button, route cable, open drawer (real-world) Franka Panda (real-world) 9.8 Hz This table continues on the next page (Part II). 33 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 8 (continued): Part II. Paper Action Head Type Action Token Format Model Training Strategy Task Embodiment Frequency GR-2 [251] CVAE 7-DoF Cartesian space action trajectory (end effector pose, binary gripper state) RDT [23] Diffusion 128-dim unified space, 64 steps action chunking Language encoder: CLIP text encoder (frozen); Video Encoder: VQGAN (frozen); Backbone: GPT-style Transformer; Action Head: CVAE Language encoder: T5-XXL (frozen); Vision encoder: SigLIP (frozen); Backbone: DiT ùúã0 [22] Flow Matching 18-DoF unified action space, 50 steps action chunking PaliGemma-3B, action expert (300M) CogACT [252] Diffusion 6-DoF end effector pose, 1-DoF binary gripper state, delta, 15 steps action chunking Prismatic-7B VLM, DiT action module ùúã0-FAST [124] Autoregressive transformer FAST tokens, 1 second action chunking ùúã0/OpenVLA backbone Pretraining: Video generation on 38M internet and robot videos (Howto100M, Ego4D, RT-1, etc.); Fine-tuning: Joint video and action trajectory prediction on domain-specific robot data (e.g., CALVIN, 40K trajectories for multi-task tabletop manipulation) Pretrained on 1M+ trajectories from 46 diverse robot datasets (RT-1, DROID, RH20T, etc.) (48H100 720h), then on 6K+ trajectories from self-collected bimanual dataset on (48H100 72h); Fine-tuned on at least 1-5 demonstrations to learn new skill Pretrained on over 10K hours of mixed data (a subset of OXE and the ùúã dataset); Fine-tuned to follow language commands, learn new dexterous tasks or multi-stage tasks Pretrained on 22.5M frames from OXE (16A100 120h); Fine-tuned on 391 demonstrations for RealMan, 400 demonstrations for Franka Train universal tokenizer on cross-embodied dataset of 1M real robot action trajectories; Pretrained on the dataset used by ùúã0 requiring 80% less computation than ùúã CALVIN; tabletop manipulation (multi-task), pick bin (single/cluttered object, real-world) Zero-shot: wash cup, pour water, pour with specific amount or with specific hand; Few-shot: fold cloth, handover, fine-grained joystick control for robot dog Zero-shot: laundry fold, table bussing, etc.; Fine-tuned: laundry fold, box assembly, etc. SimplerEnv; pick, stack, place (real-world) Zero-shot: same tasks with ùúã0, DROID [253] tasks; zero-shot generalization to unseen environments (evaluate separate policy trained on only the DROID dataset) Kinova Gen3 with Robotiq 2F-85 gripper 200 Hz ALOHA dual-arm robot 381 Hz UR5e, bimanual UR5e, Franka, bimanual Trossen, bimanual ARX, AgileX, Mobile Trossen, ARX, Mobile Fibocom Google Robot, WidowX (SimplerEnv); RealMan, Franka Panda 50 Hz 5-30 Hz Embodiments in ùúã0; Franka Panda (DROID) 5-50 Hz UniAct [254] MLP Embodiment-specific action space OpenVLAOFT [255] Parallel decoding transformer 6-DoF end effector pose, 1-DoF gripper state (per arm), continuous, delta, action chunking (8 for LIBERO, 25 for ALOHA) Backbone: LLaVA-One-Vision-0.5B; Universal Action Space: Vector-Quantized Codebook (256128); Action Decoder: lightweight MLPs for each embodiment Pretrained on 1M trajectories from 28 embodiments (OXE, DROID, LIBERO, etc.) (64A100 240h); Fine-tune only the MLP head or adapt to ACT decoder with minimal data Fine-tuned: LIBERO; 19 real-world WidowX tasks; fast adaptation to unseen robots on complex tasks (AIRBOT stacking and bimanual manipulation) Franka Panda (LIBERO); WidowX, AIRBOT (single & bi-manual) NR OpenVLA backbone, FiLM conditioning, MLP action head Start with pretrained OpenVLA; Fine-tuning: LoRA on target tasks; LIBERO: 500 demonstrations/suite; ALOHA: 20-300 demonstrations/task LIBERO; fold cloth, tool using (real-world) Franka Panda (LIBERO); ALOHA bimanual robot (real-world) up to 109.7 Hz JARVIS-VLA [256] Autoregressive transformer 51 token bins: 22 mouse control, 29 keyboard Qwen2-VL-7B/Llava-Next8B Three-stage training on: 277K Minecraft world knowledge QA pairs; 35K VQA pairs and 404K grounding data; 7.4M frames of human/agent play data, 6.4M synthesized GUI data; Cost: stage 1&2: 128 GPU hours, stage 3: 512 GPU hours (32A800 16h) Minecraft MCU Benchmark [257] (across over 1K atomic tasks) Mouse, keyboard (virtual agent) 55 Hz HybridVLA [258] Diffusion and autoregressive transformer 6-DoF end effector pose, 1-DoF binary gripper state (per arm), delta, single state 7B: Prismatic-7B VLM; 2.7B: CLIP, Phi-2 Pretrained on 760K trajectories of 35 datasets; Fine-tuned on RLBench (simulation) or self-collected real-world data (both 100 trajectories per task) 10 tabletop tasks from RLBench; 5 single-arm and 5 dual-arm real-world manipulation tasks Franka Panda (RLBench); Franka Research 3 (single-arm); AgileX dual-arm robot 6.1 Hz GR001 N1 [259] Flow Matching Embodiment-specific action space, 16 steps action chunking System 2: Eagle-2 VLM; System 1: DiT with cross-attention; Interface: embodimentspecific MLPs N/A indicates not applicable; NR indicates not reported. Pretrained on data pyramid of: 3.3K hours of real robot data from OXE, AgiBot-Alpha, etc.; 1.7K hours of synthetic data; 2.5K hours human video data from Ego4D, EPIC-KITCHEN, etc. (50K H100 hours) Fine-tuned on 30-300 demonstrations per target task Zero-shot: bimanual handover, novel object placement; Fine-tuned: RoboCasa Kitchen [260], DexMimicGen Cross-Embodiment Suite [261], GR-1 Tabletop, Real-world GR-1 manipulation (articulated object, multi-agent coordination) Fourier GR-1 humanoid (real-world) 10 Hz (system 2); 120 Hz (system 1); 34 Survey on Vision-Language-Action Models: An Action Tokenization Perspective To advance open-source VLA models, several initiatives have been made. RoboFlamingo [241] builds upon the open-source VLM OpenFlamingo [265] and focuses on single-step vision-language comprehension. It models sequential history through an explicit policy head and is lightly fine-tuned via imitation learning on language-conditioned manipulation datasets. While RoboFlamingo focuses on cost-effective, open-source solutions, it has only been evaluated in the simulation benchmark CALVIN [145]. OpenVLA [21] is widely recognized open-source, transformer-based VLA model that has been adopted as the backbone for numerous subsequent works. It carefully curates high-quality OXE subset comprising 970K trajectories from heterogeneous robots, builds on the advanced Prismatic-7B model, and provides wellstructured, ready-to-use codebase. OpenVLA demonstrates strong zero-shot generalization after pretraining and shows that joint training on cross-embodiment datasets enables efficient robot-specific fine-tuning, requiring only 10 to 150 trajectories per robot to optimize performance. This paradigm of joint pretraining followed by lightweight adaptation has gained significant traction. Additionally, OpenVLA explores parameter-efficient fine-tuning and memory-efficient inference through quantization. By open-sourcing both its model and dataset, OpenVLA has catalyzed community-scale development and inspired wave of follow-up research. MoManipVLA [266] proposes bi-level optimization framework to adapt the stationary-based OpenVLA [21] for mobile manipulation tasks. It employs OpenVLA to generate the 3D position, posture, and state of the gripper (Œîùë•, ŒîùúÉ, Œîùê∫), using downstream search algorithm to solve for coordinated motion between the manipulator and the base. This transition requires minimal fine-tuning (200 simulation demonstrations and 50 real-world episodes), highlighting the generalization capability of VLAs across embodiments. To improve training and inference speed, boost task success rates, and identify optimal fine-tuning strategies, OpenVLA-OFT [255] conducts comprehensive empirical analysis of fine-tuning recipes and proposes integrating parallel decoding, action chunking, and continuous action representations into the fine-tuning process. TinyVLA [249] aims to develop more efficient model for both pretraining and inference. It first pretrains VLM to initialize the policy backbone, then freezes the pretrained components and applies the parameter-efficient fine-tuning method LoRA [68]. diffusion-based action decoder head is appended to the pretrained VLM via linear projection layer. To reduce inference latency caused by large model size and better handle dynamic tasks, HiRT [250] further introduces hierarchical robot transformer framework that enhances execution frequency and performance. In this design, the VLM (InstructBLIP [267]) encodes language-image inputs into latent representations at lower frequency. lightweight policy head then asynchronously generates low-level actions by conditioning this latent representation along with real-time observations. 10.4. Video Pretraining and Robot Data Fine-Tuning Another line of research has explored large-scale video generative pretraining to capture world dynamics and facilitate robot learning. For example, GR-1 [247] adopts GPT-style transformer model that learns to predict future frames during video pretraining and is subsequently fine-tuned on robot datasets to incorporate action generation. Experimental results on the CALVIN simulation benchmark and real robot demonstrate the effectiveness of video-based pretraining. Its successor GR-2 [251] scales up this approach by pretraining on much larger dataset (38 million text-video pairs compared to GR-1s 0.8 million clips) and replaces the MLP action head with CVAE. The model learns to capture essential world dynamics and semantic information from videos, which are critical for downstream policy learning. GR-2s video generation capability effectively serves as planner for action generation, with the generated videos closely aligning with real-world rollouts. 10.5. Diffusion-Based Action Chunking Although transformer-based autoregressive models have led to significant advancements, several limitations remain. First, discrete autoregressive tokenization can struggle to represent continuous or multi-modal actions, which are especially crucial for dexterous tasks. Additionally, the standard autoregressive generation process produces one action at time, limiting action inference frequency. To address these issues, new class of VLA models has emerged as an alternative to pure GPT-style architectures: using diffusion-based action heads with action chunking [268]. Diffusion policies have demonstrated superior ability to model multi-modal action distributions [269], while action chunking allows the model to output sequential actions simultaneously. This approach improves temporal consistency, reduces compounding error, and significantly boosts control frequency. Octo [248] is an early work that introduces transformer-based policy with diffusion head, trained on subset of 25 datasets from the OXE. The model processes images with CNN and ViT, while language is handled by frozen T5 model. The block-wise attention structure of the transformer allows for the addition or removal of inputs and outputs during fine-tuning, enabling adaptation to cross-embodiment action and 35 Survey on Vision-Language-Action Models: An Action Tokenization Perspective observation spaces. This design enhances flexibility in input sources and fine-tuning. more recent and influential advancement is ùúã0 [22], which combines flow matching [108] with action chunking to improve policy. The VLM backbone of ùúã0 is initialized from PaliGemma [122]. The model is pretrained on mixture of OXE Magic Soup and the ùúã dataset, covering wide range of scenes, corrective behaviors, and recovery strategies. In the post-training phase, ùúã0 is fine-tuned on smaller, task-specific dataset to adapt to particular downstream tasks. The results show that comprehensive pretraining enables strong zero-shot generalization, while requiring only minimal fine-tuning data to achieve high performance in complex, multi-stage tasks such as laundry folding, box building and egg packing. Additionally, ùúã0 supports control frequency of up to 50 Hzan order of magnitude improvement over RT-2s 5 Hz. RDT [23] further extends diffusion-based VLA models to bimanual manipulation, demonstrating impressive few-shot learning capabilities. It employs frozen SigLIP [82] and T5-XXL [46] for image and language encoding, and scales the DiT head to 1B parameters. RDT can acquire new skills from as few as 15 demonstrations, marking significant step toward highly data-efficient learning in complex robotic tasks. CogACT [252] adds diffusion-based action head to OpenVLA and introduces an ensemble strategy to mitigate inter-chunk mode shifts by aggregating chunked sequences. HybridVLA [258] integrates autoregressive and diffusion policies into unified VLA model. 10.6. Heterogeneous Datasets and Unified Action Space GR00T N1 [259] introduces the data pyramid to enhance data diversity and quantity for training robot foundation models. The pyramid comprises large-scale web and human video data, mid-scale synthetic simulation data, and small-scale real-world data. It leverages the entire pyramid by extracting latent actions from human videos and synthetic demonstrations generated via DexMimicGen [261], combined with real-world data for training. GR00T N1 adopts hierarchical architecture, where the high-level model is slow (10 Hz) autoregressive VLM (Eagle-2, 1.34B parameters) responsible for high-level contextual reasoning and planning from visual and language inputs. The low-level model is fast (120 Hz) diffusion transformer (0.86B parameters) dedicated to real-time motor control, generating smooth and responsive actions. The two models are tightly integrated and jointly trained end-to-end. To better leverage cross-embodiment datasets, UniAct [254] learns universal action space compatible across diverse embodiments, represented by vectorquantized codes, where each code encodes common atomic behaviors shared across different robots. 10.7. Recent Advancements Despite advances in diffusion-based action heads with action chunking, the inference latency problem remains, as the model requires time to generate the next action chunk. If the robot continues executing the previous chunk while the next chunk is still being inferred, the new action chunk will be based on outdated observations, lacking real-time environmental feedback. Additionally, multiple plausible action modes in the diffusion process may exist at chunk boundaries, and mode shifts can lead to discontinuities between chunks, resulting in jerky or out-of-distribution movements. Real-Time Chunking [133] shows that simple averaging-based smoothing strategies can actually degrade performance, producing trajectories worse than those of individual chunks. Instead, it frames chunk fusion as inference-time inpainting via flow matching and introduces soft masking to improve cross-chunk continuity. During inference, the model generates the next action chunk while executing the current one, freezing actions that are guaranteed to be executed and inpainting the remaining steps. Soft masking ensures that the rest part of the chunk is still considered during generation, further improving continuity across chunks. ùúã0-FAST [124], an extension of ùúã0, shows that the naive binning tokenization method would produce poor results due to strong correlation between consecutive tokens at high frequencies. To address this, it applies discrete cosine transform (DCT) to encode action chunks. The DCT-based representation provides substantial token compression (up to 13.2) across tasks while producing smoother action trajectoriesboth critical for high-precision manipulation. Another limitation in prior work is that VLM pretraining alone does not yield representations fully aligned with robotic tasks, and naive fine-tuning with action supervision can degrade previously learned knowledge. To address this, ùúã0.5 with knowledge insulation [126] proposes pretraining the VLM backbone on discretized actions and general vision-language data to develop robust, transferable representations. The action expert is trained separately using flow matching on continuous actions. To preserve the backbones pretrained knowledge, gradients from the action expert are blocked from flowing back, effectively insulating its representations. During Survey on Vision-Language-Action Models: An Action Tokenization Perspective inference, the lightweight action expert generates continuous actions, while the frozen backbone contributes broad visual-linguistic understanding derived from diverse pretraining data. 10.8. Conclusions and Discussions To sum up, raw actions serve as the most direct and executable form of action representation, making them natural choice for VLA models. This approach typically involves minimal prior human knowledge and less structural constraint, favoring end-to-end learning. As real-world data are collected in raw action format, it also requires minimal action token annotation. In line with the bitter lesson observed in LLM development, which emphasizes the power of scaling over manual engineering, raw action-based end-to-end VLA models are likely to evolve as base models grow stronger and larger datasets become available. Indeed, the evolution of VLA models using raw action tokens reflects broader trends in the foundation model erascaling up both data and model size, improving base model architectures, and transitioning from pure pretraining to post-training strategies. Recent works such as ùúã0, RDT, and GR00T N1 demonstrate that comprehensive pretraining enables both strong zero-shot generalization and efficient task-specific fine-tuning. This progression mirrors the development trajectory of LLMs. However, raw action data lacks the internet-scale accessibility of language data. It is costly to collect, often requiring teleoperation and real robot interaction, which limits scalability. Furthermore, raw actions cannot directly generalize across embodiments, and fine-tuning or post-training on downstream tasks can lead to catastrophic forgetting of pretrained visual-language knowledge. Also, directly generating raw control commands without intermediate representations is less practical for long-term control tasks, as the required context length, computational cost, and inference latency can become prohibitively high. Addressing these challenges while preserving foundation model knowledge remains critical direction for future research. 11. Reasoning as Action Tokens Embodied tasks, such as robotic manipulation and autonomous driving, often demand sophisticated cognitive abilities from AI agents. Their inherent complexity stems from the need for long-horizon reasoning, deep understanding of space, semantics, and common sense, and the ability to operate effectively within dynamic environments [270]. Even advanced foundation models face considerable challenges in these areas. While single VLA model is anticipated to address wide array of embodied tasks, simply scaling up model parameters often proves insufficient to tackle the inherent complexities of real-world scenarios, particularly those demanding robust logical and embodied reasoning. Equipping VLAs with enhanced reasoning capabilities thus emerges as promising solution. Table 9 summarizes representative works that explicitly use reasoning as action tokens. In the context of VLA, reasoning refers to deliberative thinking process that is explicitly externalized in the form of natural language and serves to enhance the generation of the target action token. Unlike other action tokens that directly represent physical movements or emphasize object interactions, these reasoning tokens serve an intermediary role, facilitating the generation of subsequent executable action tokens. This concept allows the model to think step-by-step and externalize its internal decision-making process. For instance, RAD [41] uses reasoning to produce raw actions informed by language plans, and DriveVLM [30] processes reasoning before generating vehicle motion trajectories. 11.1. Evolution of Reasoning in VLA Models The core idea of externalizing internal reasoning processes finds its roots in Chain of Thought (CoT) prompting [53]. Originally developed for LLMs to articulate intermediate steps (e.g., via the prompt think step by step) before final output, CoT has since transcended text-only domains. Its extension into visual and multi-modal contexts laid the groundwork for how reasoning could function within VLA models. For example, CoT has been applied to generate visual intermediates, such as bounding boxes of target objects, before computing final actions in visual tasks [271]. Early pioneering works in embodied reasoning often leveraged LLMs, augmented with additional modules to interpret visual scenes. notable example is Inner Monologue [136], which uses LLMs to accept human instructions, scene descriptions (generated by MDETR [272]), and action feedback (from leveraged perception models). This setup allows for recursive multi-step language planning until task is successfully completed. However, the field has rapidly evolved. Today, the mainstream approach for VLA models integrating reasoning is to utilize VLM. VLMs possess inherent and proficient multi-modal prior knowledge, simplifying the model 37 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 9 Overview of VLA research using reasoning as action tokens. Paper Model Training Details Token Format Inner Monologue [136] PALM Frozen Fixed simple reasoning Augmented Token Language Description Task Embodiment Tabletop rearrangement; mobile manipulation UR5e with gripper; Everyday Robots DriveVLM [30] Qwen-VL-7B Pretrained and co-tuned on self-constructed dataset ECoT [40] OpenVLA Trained on 60K CoT dataset constructed from BridgeData V2 CoT in three modules Trajectory Autonomous driving Li Auto autonomous vehicles Fixed CoT reasoning steps Raw Action Pick, place, move, unfold cloth WidowX with gripper RAD [41] OpenVLA Trained on mixture of robot data and 1616 action-free human demos on BridgeData V2 Toy Sink setup Fixed CoT reasoning steps Raw Action Pick, place WidowX with gripper AlphaDrive [273] Qwen2-VL-2B SFT on MetaAD Cosmos-Reason1 [274] Qwen2-VL-7B Nemotron 56B SFT on numerous self-constructed datasets CoT CoT Language Description Language Description Autonomous driving N/A Physical common sense reasoning; embodied reasoning; intuitive physics reasoning N/A These methods do not perform action grounding or execution. Included for their relevance to VLA perception and representation learning. For the same reason, these methods are N/A (not applicable) in the embodiment column. architecture by reducing the need for numerous additional modules. Their intrinsic ability to process both linguistic and visual modalities significantly enhances the reasoning process for complex embodied tasks. To tailor these VLMs to specific reasoning patterns crucial for embodied tasks, methods like fine-tuning or retraining models are commonly employed, as demonstrated by Embodied CoT (ECoT) [40] and RAD [41]. 11.2. Key Implementations and Applications ECoT [40] serves as typical example of adopting reasoning for embodied tasks. Built on Prismatic VLMs [84], an OpenVLA model [21] is specifically trained with reasoning data. significant challenge in this domain is obtaining high-quality, large-scale reasoning datasets. While human annotation yields superior quality, it is impractical at scale. ECoT introduces an automated data synthesis pipeline that structures reasoning into fixed sequence, from task decomposition to gripper position and object box prediction. Following ECoT, RAD [41] adopts similar framework but substantially extends data collection. It not only synthesizes reasoning data automatically from robot trajectories but also from easily accessible action-free human videos. The synthesis from human videos mirrors that from robot data, replacing motion primitive extraction with HaMeR [113], method for hand keypoint and pose tracking. This innovation facilitates co-training on both robot data and human videos, broadening the scope of available data. Furthermore, some VLMs are specifically trained for embodied reasoning, such as Cosmos-Reason1 [274]. This model is trained via reinforcement learning (specifically, GRPO) and supervised fine-tuning (SFT) on physical common sense, embodied reasoning, and intuitive physics, tailoring it for embodied applications. Beyond robotic manipulation, autonomous driving presents another critical application area for reasoning due to its highly complex, dynamic, interactive environment, and the paramount need for enhanced safety. DriveVLM [30] applies CoT across its three key modules: scene description, scene analysis, and hierarchical planning. The scene description module identifies critical objects in the driving environment. The scene analysis module then evaluates their characteristics and potential influences on the ego vehicle. Finally, the hierarchical planning module formulates step-by-step plans, progressing from language motions to decision descriptions and ultimately to waypoints. This demanding task requires sophisticated deduction and commonsense understanding of diverse objects and scenarios, making reasoning with VLMs particularly well-suited. Models like AlphaDrive [273], trained with SFT warm-up followed by GRPO-based RL exploration, are examples of VLMs exclusively developed for reasoning in autonomous driving contexts. 38 Survey on Vision-Language-Action Models: An Action Tokenization Perspective 11.3. Advantages of Reasoning as Action Tokens Integrating reasoning as action tokens offers several compelling advantages for VLA models: Bridging the Instruction-Action Gap and Enhanced GeneralizationReasoning significantly mitigates the gap between high-level instructions and low-level executable actions by introducing intermediate thinking steps. This allows VLMs to leverage their prior knowledge to handle tasks involving various scenes and objects to enhance the generalization ability and performance in complex long-horizon tasks. For instance, ECoT [40] demonstrates substantial performance improvements in complex manipulation tasks like put the edible object in the bowl. This task requires sophisticated reasoning, including identifying the bowl, checking all existing objects, and selecting edible ones based on common sense. ECoT also shows enhanced generalization capabilities to unseen objects and scenes, demonstrating the power of reasoning. Improved Interpretability and Human-in-theLoop CapabilitiesBy externalizing the agents thought process, reasoning enhances the interpretability of the model. Humans can clearly review the agents decisions, trace failures to specific points in the reasoning chain, and even intervene in real-time when errors are detected. This transparency also facilitates human-in-the-loop interactions, allowing flexible processing of uncertain human input for subsequent actions, as exemplified by Inner Monologues ability to let humans select objects in real-time. Enabling Cross-Embodiment Capability While different embodiments may have distinct architectures and action token formats, the high-level plan for completing task often remains consistent. Reasoning can extract these abstract plans, shifting the primary challenge to projecting them into final executions. The rich prior knowledge of VLMs, combined with training or fine-tuning on cross-embodiment datasets like OXE [207], can facilitate reasoning across various embodiments. ECoT validates its cross-embodiment capability, showing that fine-tuned model can perform ECoT reasoning effectively in new embodiments. 11.4. Limitations and Future Directions Despite its numerous advantages, applying reasoning in embodied tasks still faces several limitations: Increased Inference Time and Reduced Execution Speed. Reasoning often necessitates the model to generate lengthy thinking process or multiple reasoning steps, leading to high inference time and low execution speed. This is critical constraint for real-time, high-frequency tasks common in embodied AI. While solutions like ECoTs asynchronous execution can speed up inference by around 40%, further acceleration techniques are crucial. Fixed Reasoning Steps and Data Challenges. In current implementations, reasoning steps are frequently fixed manually. While this provides stability for certain tasks, it can limit the models generalization capability and hinder the exploration of potentially superior reasoning pathways. Furthermore, building high-quality, large-scale reasoning datasets remains costly and challenging. Concluding from the advantages and limitations, reasoning is particularly well-suited for complicated, longhorizon, deductive tasks that require decomposition into multiple subtasks, especially those with relatively low-frequency execution demands due to current inference speed limitations. Looking ahead, future work in this area promises exciting advancements: Improved Inference Speed and Foundation Model Capabilities. Enhancements in the inference speed and inherent reasoning capabilities of foundation models are anticipated. Better Data Collection Methods. Developing more efficient and scalable methods for collecting high-quality reasoning data is essential. Advanced Test-Time Compute. Leveraging test-time compute, which involves additional computation during inference, holds potential for boosting the performance, generalization, and robustness of reasoning models. Techniques explored in AlphaDrive and Cosmos-Reason1 are just early examples. Novel Reasoning Paradigm Design. Insights into designing the paradigm of the reasoning module in VLA are eagerly awaited. This may include multimodal forms of reasoning and eventually generalize to wider range, perhaps all, of embodied tasks and robot embodiments. 12. Scalable Data Sources The development of VLA models critically relies on learning action tokens that are grounded in multimodal observations, compositional to support skill sequencing, and executable for embodied policy control. Effective learning of such representations requires data that jointly provides visual-linguistic grounding, fine-grained action supervision, and embodiment-aligned sensorimotor control. However, individual data sources typically provide supervision signals with complementary strengths. To address this, modern VLA frameworks adopt hierarchical multi-source data paradigm, integrating web data and human video for visual-linguistic grounding, synthetic and simulation data for skill composition, and real-world embodied data for embodiment-specific control grounding. As these three types of data sources decrease in quantity and grow in embodiment 39 Survey on Vision-Language-Action Models: An Action Tokenization Perspective specificity, they constitute the bottom, middle, and top layers of the Data Pyramid [259]. This multi-layered supervision enables scalable and transferable action token learning across diverse tasks, embodiments, and control modalities. Table 10 showcases representative works for scalable data sources. 12.1. Bottom Layer: Web Data and Human Videos The bottom layer consists of large-scale web data and human video datasets to support visual-linguistic grounding [9, 290], world modeling [291, 292], and temporal prediction [293]. Since web data primarily consists of vision-language pairs and is mainly used to enhance foundation model capabilities, which falls outside the scope of our survey, we focus primarily on human video in our discussion. Representative human video datasets include Ego4D [236], EPIC-KITCHENS-100 [277], and Something-Something V2 [275]. Although these datasets do not contain action labels directly usable for policy learning, they capture diverse human-object interactions, complex manipulation skills, and rich physical common sense, which are valuable sources of world knowledge. Their scale and diversity enable pretraining of temporal visual encoders and facilitate the learning of action token representations. Recent VLA models have utilized these datasets to extract trajectory [179, 208], infer latent state transitions [39], and generate latent action [259]. The resulting pretrained modules provide temporally grounded, semantically structured, and partially embodied priors that enhance downstream policy learning across tasks and embodiments. In addition to explicit perceptual content, these videos implicitly encode mappings between visual observations and physical actions. This implicit structure allows models to acquire coarse affordance priors and latent dynamics conditioned on observed states and estimated actions [247, 251, 294]. Egocentric viewpoints reduce the embodiment gap by approximating robot perspectives, particularly for manipulation and navigation tasks. Recent works further exploit weak supervision to extract actionable representations from large-scale videos. Frame-level captioning and temporal alignment provide indirect supervision signals for generating trajectorybased and goal state action tokens. For instance, Magma [179] introduces Set-of-Mark and Trace-of-Mark abstractions to anchor action grounding within video streams. Ego-Exo4D [276] augments egocentric data with third-person views for 3D motion grounding, facilitating embodiment transfer. These approaches enable VLA models to build temporal grounding and language-conditioned policy priors in open-world settings. 12.2. Middle Layer: Synthetic and Simulation Data To provide critical bridge between the human video and the high cost of real-world data collection, VLA research extensively utilizes simulation and synthetic data. This paradigm provides scalable access to structured, task-centric data crucial for learning compositional skills and robust control policies. Two complementary methodologies are central to this approach. Synthetic Dataset Generation. The first methodology is offline synthetic data generation. It employs procedural generation pipelines like MimicGen [278], DexMimicGen [295], and RoboCasa [260] to programmatically augment or synthesize large-scale datasets from limited set of expert demonstrations. For instance, MimicGen [278] establishes paradigm of applying procedural variations, such as spatial transformations and scene reconfigurations, to existing trajectories to enhance data diversity. Building on this, RoboCasa [260] scales this process to generate over 100K trajectories for diverse manipulation tasks, while DexMimicGen [295] extends it to complex bimanual manipulation by combining kinematic retargeting with contact dynamics randomization. These methods substantially enrich the quantity and diversity of datasets at low costs, as demonstrated by GR00T N1, which leverages such data to train policies for complex bimanual assembly tasks [259]. Interactive Simulation Platforms. Complementary to synthetic datasets, the second methodology involves interactive simulation platforms like robosuite [296], Habitat [297], Isaac Gym [298], Isaac Lab [299], and others [300, 301, 145, 144, 302, 303, 202, 150, 204, 219, 140, 201, 141, 304, 302, 160, 176, 217, 200, 229, 305, 231, 306, 246, 257, 260, 138]. Data generation within these simulators follows several key paradigms. First, it involves teleoperation, where human operator uses VR controllers, keyboards, or other interfaces to control the simulated robot and perform tasks. The second method involves algorithmic solvers, such as classical motion planners, which generate successful trajectories for tasks that have clear solution paths. Third, learned policies, often trained via RL, can autonomously collect vast amounts of data. Beyond generating robot trajectories, these platforms also diversify the training environment itself. Procedural content generation systematically randomizes environmental factors, including objects, textures, and lighting conditions. Additionally, platforms like AgiBot Digital World [279] integrate realistic 3D assets with highfidelity physics simulation, facilitating exploration of rare, failure-prone, and complex interaction scenarios. Survey on Vision-Language-Action Models: An Action Tokenization Perspective Table 10 Overview of datasets used in VLA research. Category Name Description Quantity Data Structure Used in VLA Papers d m t o l S & e y a t R o - R SomethingSomething V2 [275] large collection of labeled video clips with humans performing everyday actions Ego4D [236] an egocentric dataset of daily-life activity videos EgoExo4D [276] multi-modal and multi-view video dataset of skilled human activities 220K video clips videos, labels, objects LAPA [39], Magma [179], CoT-VLA [33] 3670 hours 1286 hours audio, 3D meshes, eye gaze, stereo, synchronized videos from multiple cameras EmbodiedGPT [139], Magma [179], GR00T N1 [259] videos, 7-channel audio, IMU, eyegaze, two grayscale SLAM cameras, point clouds GR00T N1 [259] EPICKITCHENS100 [277] an egocentric dataset of daily activities in the kitchen 100 hours, 20M frames, 90K action segments, 200 participants narrations, videos, action segments ARM4R [208], Magma [179], 3D-VLA [38], CoT-VLA [33], GR00T N1 [259] MimicGen [278] RoboCasa [260] dataset generation systems to produce diverse demonstrations from few human demonstrations large-scale cross-embodiment simulation framework in kitchen environments AgiBot Digital World [279] high-fidelity digital twin-based simulation suite RT-1 [205] large, multi-embodiment robotic dataset including multiple tasks, objects and environments 50K demons generated from 200 human demos robot states, actions, scene/object configuration, camera data - 100K trajectories, 120 kitchen scenes, 2500 object categories action, images, proprioception, object states, task annotations GR00T N1 [259] 1M demonstrations, 2976 hours, 100 tasks RGB images, tactile, proprioceptive, low-level action commands - 130K demonstrations, 9 skills, 700 tasks action, images, language instruction, proprioception Gen2Act [222], RT-1 [205] OXE [207] large, multi-embodiment robotic dataset assembling 60 datasets 1M episodes, 311 scenes, 22 robots, 527 skills, 60 datasets, 5228 objects depends on the original dataset 3D-VLA [38], CoT-VLA [33], RT-X [207], Octo [248], OpenVLA [21], ùúã0 [22], RDT [23], etc. RH20T [280] multi-embodiment robotic dataset with visual, force, audio, and action information 110K robot episodes, 110K human demonstrations RGB-D images, Binocular IR images, action, audio, tactile (partly) 3D-VLA [38] RoboMIND [281] large, multi-embodiment dataset including failure cases 107K successful trajectories, 5K failed trajectories, 479 tasks, 96 objects, 4 robots HoNY [282] dataset containing interactions at home with the Stick 5K trajectories, 13 hours, 216 environments BridgeData V2 [219] large robotic dataset with the WidowX 250 robot arm 60K episodes, 24 environments, 13 skills RGB-D images, action, language instruction HybridVLA [258], AgiBot World Colosseo [18] RGB-D videos, action 3D-VLA [38] videos (an over-the-shoulder RGB-D camera, two random RGB cameras, wrist RGB camera), language instruction LAPA [39], FLIP [198], AVDC [189], HAMSTER [190], SuSIE [214], RoboDual [283] DROID [253] diverse robotic manipulation dataset of Franka Panda 7DoF robot arm 76K trajectories, 350 hours, 564 scenes, 86 tasks action, language, 3 RGB cameras, proprioception HAMSTER [190], Diffusion-VLA [284], Hi Robot [24], RoboDual [283], RAM [165] AgiBot World [285] large-scale robotic manipulation dataset with Genie-1 1M trajectories, 3K hours, 100 scenes, 5 domains, 200 tasks, 87 skills WOMD [287, 288] diverse interactive motion dataset for autonomous driving 103K segments, 20 seconds each, 574 hours nuScenes [289] large dataset for autonomous driving 1K driving scenes, 20 seconds each CoVLA [28] comprehensive Vision-Language-Action dataset for autonomous driving 80 hours, 10K video clips RGB-D videos, action, skill, proprioception FLaRe [286] ego pose, images, object tracks, 3D bounding box, Lidar data ego pose, image, Lidar, Radar, object 3D bounding box videos, frame-level language captions, future trajectory actions EMMA [29] EMMA [29], VLM-E2E [197] CoVLA [28] 41 Survey on Vision-Language-Action Models: An Action Tokenization Perspective These environments enable agents to learn through direct interaction with physics-based world, facilitating large-scale reinforcement learning and imitation learning. Simulation is particularly valuable for high-risk or safety-critical scenarios, such as tool misuse or complex contact dynamics, which are essential for robust policies with recovery capabilities. However, addressing the persistent sim-to-real gap remains essential, where discrepancies in visual fidelity and physics modeling necessitate further fine-tuning in real-world settings. 12.3. Top Layer: Real-World Robot Data Real-world robot data contains the most critical resource for training VLA models, providing direct supervision for learning physically grounded and executable policies. Unlike simulation or human video, real robot datasets capture the complex dynamics, sensory noise, and unpredictable variations inherent in physical environments. This high-fidelity information is indispensable for bridging the sim-to-real gap and instilling crucial embodimentspecific characteristics, such as kinematic constraints and contact dynamics. Consequently, real-world data is paramount for training policies to generate low-level action, which demands precise physical realism for successful execution. primary objective in VLA research is to develop generalist agents capable of operating across diverse robotic platforms. This motivates the curation of large-scale, multi-embodiment datasets that aggregate experiences from various robot morphologies and environments. For instance, OXE [207] assembles over 1 million manipulation episodes from multiple datasets spanning 22 robots, facilitating the learning of policies with significant cross-embodiment transfer capabilities. Recent efforts have further enriched these collections. RoboMIND [281], for example, uniquely incorporates negative data, providing 5K failure trajectories annotated with causal reasons to enable more robust policy learning through contrastive or corrective mechanisms. Similarly, RH20T [280] further provides multi-modal information, including force-torque and audio data, to support policies that reason about physical contact and environmental sounds. In contrast to the broad coverage of multi-embodiment datasets, single-embodiment and task-specific datasets provide complementary data for mastering complex, specialized skills. These datasets are crucial for learning fine-grained manipulation and long-horizon tasks. For instance, RT-1 [205] represents one of the earliest and most well-known efforts to collect large-scale single-embodiment datasets. Subsequently, DROID [253] introduces unified robotic platform, deploys it across multiple institutions worldwide, where researchers jointly collect large-scale dataset that spans wide range of tasks, objects, scenes, viewpoints, and interaction locations. Such unified yet diverse data facilitates the training of generalizable VLA models [124]. In addition, AgiBot World [285] provides 1M episodes with the Genie-1 robot across 5 different domains. For long-horizon planning, BridgeData V2 [219] contains 60K demonstrations of complex kitchen tasks, providing supervision for learning causal dependencies in multi-step manipulation. Datasets like HoNY [282] focus on capturing data in unstructured in-the-wild home environments, presenting challenges such as object clutter and variable lighting. This principle of data collection extends to autonomous driving, where datasets like nuScenes [289] and the Waymo Open Dataset-Motion [287, 288] integrate rich sensor suites (e.g., LiDAR, RADAR) to train safety-critical driving policies, typically using trajectory tokens as the primary action representation. Despite their indispensability, the acquisition of real-world robot data remains significant bottleneck due to high costs, operational complexity, and the slow pace of teleoperation or portable motion capture devices. This scalability challenge fundamentally shapes the data strategy for most state-of-the-art VLA models. prevalent and effective paradigm involves large-scale pretraining on abundant simulation or web-scraped data to learn generalizable visual, linguistic, and semantic representations. Subsequently, models are fine-tuned on smaller, high-quality, real-world datasets to adapt these general representations to specific physical embodiments and task requirements. This hierarchical approach strategically balances the need for broad world knowledge with the precise physical grounding required for reliable real-world execution, effectively mitigating the data scarcity problem while maximizing performance. 13. General Discussions and Future Directions The preceding sections reveal that each category of action tokens has been explored through series of influential papers. These studies have uncovered the expressive capacity of different action tokens, effectively leveraged the strengths of foundation models, and developed scalable data strategies, culminating in VLA models that demonstrate promising empirical performance. Clearly, each type of action token comes with its own strengths and limitations and remains in an early stage of exploration, holding significant potential for future development. At present, no single type exhibits absolute dominance or clear inferiority, nor has the 42 Survey on Vision-Language-Action Models: An Action Tokenization Perspective research community converged on dominant action token paradigm, making definitive recommendations challenging. As such, we provide our assessment of future trends in action tokens and the development of VLA models in Section 13.1. In Sections 13.2 to 13.6, we further present set of general observations and reflections, identifying underexplored areas in VLA research to inform and guide future directions. 13.1. Trends of Action Tokens and VLA Models Based on the summarized advantages and limitations of each token in Table 1, we observe that different action tokens exhibit complementary strengths and are best suited to different levels within VLA model. This suggests that the future of VLA lies not in single dominant token type, but in their strategic combination, motivating hierarchical architecture. Language plans and code offer unique advantages in long-horizon planning and logical controlcapabilities that are difficult to substitute with other action token typesmaking them ideal for the top layer. For subtasks derived from these high-level plans, combination of 3D affordance, trajectory modeling, and goal video prediction can provide precise and interpretable motion representations, making them well-suited for the intermediate layer. In contrast, language motion and API-based code are comparatively less expressive and can generally be replaced by the former three. Finally, policy module can be trained to map these vision-based representations into raw actions. While latent representations hold strong potential, we do not include them in our proposed architecture due to current training challengesparticularly in achieving appropriate granularity, semantic comprehensiveness, and task-centric alignment. These limitations are not easily addressed and may compromise reliability in practical applications. As such, we currently favor more explicit forms of action tokens, which are generally easier to train and inspect, and offer greater interpretability and control. Nonetheless, we remain optimistic about future advances in latent representations and their eventual integration as the field evolves. An end-to-end low-level policy that directly maps subtasks to raw actions offers fundamental scalability, though it remains constrained by limited data availability. In the short term, the aforementioned hierarchical design facilitates data collection to achieve data flywheel effect; in the long run, it could enable fully end-to-end controller learning that bypasses intermediate tokens and directly predicts raw actions from subtasks. Reasoning features crucial action token in VLA models. While reasoning has been incorporated into current VLA models, it is generally rudimentary and applied only to relatively simple tasks. As discussed in Section 2.4, action tokens in VLA models play role analogous to that of language tokens in LLMs. It is therefore natural to envision reasoning processes in VLA models being constructed not from language tokens, but from action tokens. This mirrors how humans solve complex tasksnot only through linguistic planning and reflection, but also by engaging in physical-world grounding and imagination. Furthermore, action-token-based reasoning should be designed to leverage test-time computation adaptively, adjusting its length according to task complexity, as is commonly done in language-based reasoning. Such reasoning should be integrated throughout the VLA hierarchy as needed to enhance the generation of all other action tokens, offering promising path toward more general and human-like intelligence. The above analysis presents our perspective on the future development of VLA models from an action tokenization perspective. Fundamentally, the existence of current action tokens stems from the capabilities of foundation models to generate and interpret them. As foundation models continue to evolve and new modalities (e.g., audio, tactile) become increasingly accessible, we anticipate the emergence of new types and subtypes of action tokens that will further expand the expressiveness and effectiveness of VLA models. Continued investigation and thoughtful integration of all action tokens will be essential to fully harness their complementary strengths and advance toward more capable, general-purpose embodied intelligence. 13.2. From VLA Models to VLA Agents natural next step is to consciously evolve beyond VLA models toward VLA agents, complementing core capabilities through an agent-centric paradigm. While current VLA models primarily focus on learning an effective mapping from vision-language inputs to action outputs, building more general and robust embodied intelligence likely necessitates agent-level systems with comprehensive and integrated functionalities. Most existing VLA models lack mechanisms for incorporating history. Even when present, such context is typically limited to few frames or simple language-based planning. This is insufficient for long-horizon tasks in the real world, particularly those involving progress tracking, subtask dependencies, or online exploration. Addressing these challenges calls for robust and structured mechanisms for memory, planning, and reflectioncomponents that have been extensively studied in the broader agent research community [307] and can be effectively 43 Survey on Vision-Language-Action Models: An Action Tokenization Perspective integrated into VLA. Preliminary efforts such as RoboOS [308] represent an early step in this direction, though the current design remains relatively simple. Additionally, planning and online exploration in VLA agents could also be substantially enhanced by integrating advances in world models. While existing research can generally be described within our proposed framework of chain of interleaved VLA modules and action tokens, future agent systems should not be constrained to linear architectures. Instead, modules and generated action tokens should be adaptively invoked and managed by the agent to fully process information and generate effective outputs. Finally, the evolution toward VLA agentsand the broader vision of deploying embodied agents in real-world environmentsalso calls for increased attention to multi-agent systems and human-agent co-existence, both of which are essential for the future integration of robots into everyday human life. 13.3. From Imitation Learning to Reinforcement Learning Our third observation centers on the training paradigm of VLA models. Currently, the vast majority of VLA models are trained using imitation learning, which presents several limitations. These include an inherent upper bound imposed by the capabilities of human demonstrators, lack of goal-conditioned execution mechanisms, and difficulties in achieving consistent, near-perfect performance. Even worse, human demonstrations are often suboptimal and may lack dexterity due to factors such as fatigue, inattentiveness, individual idiosyncrasies, and the technical constraints of data collection devicessuch as sensor imprecision and latency. These limitations of imitation learning naturally prompt reflection: much of human learning does not arise merely from observation or instruction, but instead depends fundamentally on hands-on trial-and-error and self-guided exploration. This suggests promising direction for future research: applying reinforcement learning (RL) to optimize VLA models. By enabling models to learn directly from goal feedback and autonomously explore the environment, such approaches could yield more robust, dexterous, and high-success-rate behaviors. Reinforcement learning thus offers pathway toward more human-like learning processes and capabilities in VLA models. While the promise of RL for VLA models is clear [286, 309, 310, 311, 312, 313], its direct application to realworld scenarios presents significant challenges. Deploying VLA models in the physical world often incurs high reset cost, demanding substantial time and resources to reset the environment after each trial. Furthermore, the low interaction efficiency of real-world environments means that models require vast number of interactions to learn effectively, which is often impractical. Safety concerns also loom large, as exploratory actions during RL training could lead to damage to the robot or its surroundings. These challenges highlight the critical need for developing more efficient RL algorithms that can enable VLA models to be grounded on real machines with minimal interaction. This could involve techniques such as in-context reinforcement learning [314], which leverages large, pre-trained models to learn new tasks with limited data by adapting to new contexts. Another crucial area for future research lies in automating the design of dense reward functions by leveraging existing VLMs [315, 316]. Crafting effective reward functions for complex robotic tasks is notoriously difficult and often requires significant manual effort and domain expertise. VLMs, with their impressive understanding of visual and textual information, hold the potential to interpret high-level task descriptions and automatically generate fine-grained, dense reward signals that guide the RL agent towards successful completion. This approach could significantly reduce the burden of reward engineering, accelerating the development and deployment of RL-driven VLA models in diverse real-world applications. 13.4. From Restrictive Hardware to Full Dexterity and Modalities Another critical limitation of current VLA models lies in their underlying hardware configurations. While in daily life, most complex and fine-grained manipulation tasks are performed using human hands, the vast majority of existing VLA research relies solely on simple grippers, which severely restricts the action space and dexterity of manipulation. To advance toward more capable VLA models, future research must incorporate dexterous hands as central component. Moreover, existing work primarily focuses on three common modalities: vision, language, and action. However, such sensor configurations are insufficient for developing truly general-purpose agents. Broader sensory modalitiesincluding tactile sense, audition, olfaction, and even gustationare essential for enabling agents to handle wider range of real-world tasks with the robustness and adaptability required for general intelligence. 44 Survey on Vision-Language-Action Models: An Action Tokenization Perspective 13.5. From Capability-Centric to Safety-Aware VLA models must also place greater emphasis on safety considerations [317]. Embodied intelligence not only inherits many of the alignment [52] and safety challenges present in digital AI systems, but also introduces additional riskssuch as physical damage to hardware and even potential harm to humansdue to its interaction with the real world. These high-stakes consequences demand that safety be treated as first-class concern in algorithm design. However, this remains an underexplored area in current research and calls for more systematic investigation and proactive development of safety-aware methodologies. 13.6. From Data Scarcity to Data Scalability The history of deep learning has repeatedly demonstrated that data serves as the fossil fuel powering the development of powerful models. However, current efforts in robot data collection have made it clear that in the near term data availability will remain insufficient in several critical dimensions. First, the overall quantity of robot data is severely limited. In contrast to language and vision data, which benefit from massive and continuously expanding internet-scale corpora, robot data must be collected manually through labor-intensive processes. Despite significant efforts from the community, the quantity of available robot data still falls short of vision-language data by orders of magnitude and is unlikely to reach comparable levels in the short term. In fact, the total number of tokens in OXE datasets is estimated to be only about 1/200,000 of that in large-scale language model corpora, further highlighting the scarcity of robot data. Second, robot data lacks sufficient modality coverage. Most existing datasets are limited to vision, language, and action, while other important sensory modalities such as tactile sense, audition, olfaction, and gustation remain largely unrepresented. These gaps are difficult to fill shortly due to hardware constraints. Third, robot embodiments are diverse and often incompatible with each other. Although large amount of data has been collected on different platforms, these datasets are fragmented across various embodiments and are not easily shared or reused, which further reduces the amount of usable data. Fourth, the quality of robot data is often inadequate, especially in scenarios involving dexterous manipulation. Existing data collection devices for dexterous hands are not yet sufficiently advanced in terms of precision, responsiveness, and reliability. As result, it remains difficult to acquire high-quality data for complex tasks. This challenge is even more pronounced for high-degree-of-freedom hands with force feedback. Due to these limitations, VLA models, which may ultimately require far greater data volume than digital AI systems, face significant bottleneck in terms of data availability. Future research should address these challenges along two key directions. On one hand, simulation and internet-scale resources should be better leveraged to provide scalable supervision. On the other hand, its crucial to develop more versatile, reliable, multimodal, and in-the-wild data collection systems [318, 319, 320] that can operate effectively in real-world environments. These efforts are essential to support the continued progress and scalability of VLA models. 14. Conclusion This survey positions VLA models as central pathway toward embodied AI and presents comprehensive review of existing research from an action tokenization perspective. For each category of action tokens, we systematically examine representative VLAs, analyze their strengths and limitations, and highlight potential directions for future investigation. We further summarize major efforts in scalable data sources, aiming to inform and support ongoing research. Finally, grounded in the current state of VLA development, we outline future trends and underexplored areas to help guide the next stages of progress. As vision and language foundation models continue to thrive, research in VLA is gaining momentum and holds immense promise. We hope this survey helps clarify the fields evolution, map out its trajectory, and contribute meaningfully to its growthultimately bringing us closer to the pursuit of Artificial General Intelligence. 45 Survey on Vision-Language-Action Models: An Action Tokenization Perspective"
        },
        {
            "title": "References",
            "content": "[1] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [2] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al. comprehensive survey on pretrained foundation models: history from bert to chatgpt. International Journal of Machine Learning and Cybernetics, pages 165, 2024. [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [7] Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. Featured 2024. Certification. [8] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In International Conference on Computer Vision (ICCV), pages 40154026, 2023. [9] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R√§dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [10] OpenAI. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [11] Gemini team. Gemini 2.5: Our most intelligent ai model, 2025. URL https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025. [12] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [13] brian ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as can, not as say: Grounding language in robotic affordances. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=bdHkMjBJG_w. 46 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [14] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-e: An embodied multimodal language model. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 84698488. PMLR, 2329 Jul 2023. URL https: //proceedings.mlr.press/v202/driess23a.html. [15] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 94939500. IEEE, 2023. [16] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Proceedings of The 7th Conference on Robot Learning, volume 229 of Proceedings of Machine Learning Research, pages 540562. PMLR, 0609 Nov 2023. URL https://proceedings. mlr.press/v229/huang23b.html. [17] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, and Ted Xiao. RT-trajectory: Robotic task generalization via hindsight trajectory sketches. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=F1TKzG8LJO. [18] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xu Huang, Shu Jiang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [21] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An open-source vision-language-action model. In 8th Annual Conference on Robot Learning, 2024. URL https:// openreview.net/forum?id=ZMnD6QZAE6. [22] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. ùúã0: vision-language-action flow model for general robot control, 2024. URL https://arxiv. org/abs/2410.24164, 2024. [23] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. [24] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, and Chelsea Finn. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. CoRR, abs/2502.19417, February 2025. URL https://doi.org/10.48550/arXiv.2502. 19417. 47 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [25] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [26] Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigational reasoning capability for large vision-language models. In European Conference on Computer Vision, pages 260278. Springer, 2024. [27] Valerii Serpiva, Artem Lykov, Artyom Myshlyaev, Muhammad Haris Khan, Ali Alridha Abdulkarim, Oleg Sautenkov, and Dzmitry Tsetserukou. Racevla: Vla-based racing drone navigation with human-like behaviour. arXiv preprint arXiv:2503.02572, 2025. [28] Hidehisa Arai, Keita Miwa, Kento Sasaki, Kohei Watanabe, Yu Yamaguchi, Shunsuke Aoki, and Issei Yamamoto. Covla: Comprehensive vision-language-action dataset for autonomous driving. In Proceedings of the Winter Conference on Applications of Computer Vision (WACV), pages 19331943, February 2025. [29] Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, et al. Emma: End-to-end multimodal model for autonomous driving. arXiv preprint arXiv:2410.23262, 2024. [30] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, XianPeng Lang, and Hang Zhao. DriveVLM: The convergence of autonomous driving and large vision-language models. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum? id=928V4Umlys. [31] Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong, Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. In https://arxiv.org/abs/2403.01823, 2024. [32] Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, and Yuanpei Chen. Dexgraspvla: vision-language-action framework towards general dexterous grasping. arXiv preprint arXiv:2502.20900, 2025. [33] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Tsung-Yi Lin, Gordon Wetzstein, Ming-Yu Liu, and Donglai Xiang. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 17021713, June 2025. [34] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 1152311530, 2022. URL https://api.semanticscholar.org/CorpusID:252519594. [35] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In 2nd CoRL Workshop on Learning Effective Abstractions for Planning, 2024. URL https://openreview.net/forum?id=ZGbWq3VqrO. [36] Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, et al. Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. In The Twelfth International Conference on Learning Representations, 2024. [37] Chuan Wen, Xingyu Lin, John Ian Reyes So, Kai Chen, Qi Dou, Yang Gao, and Pieter Abbeel. Anypoint Trajectory Modeling for Policy Learning. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, July 2024. doi: 10.15607/RSS.2024.XX.092. [38] Haoyu Zhen, Xiaowen Qiu, Peihao Chen, Jincheng Yang, Xin Yan, Yilun Du, Yining Hong, and Chuang Gan. 3D-VLA: 3D vision-language-action generative world model. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 6122961245. PMLR, 2127 Jul 2024. URL https://proceedings.mlr. press/v235/zhen24a.html. 48 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [39] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Se June Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, Lars Liden, Kimin Lee, Jianfeng Gao, Luke Zettlemoyer, In CoRL 2024 Workshop on Dieter Fox, and Minjoon Seo. Latent action pretraining from videos. Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond, 2024. URL https://openreview.net/forum?id=NCOP0KYb0u. [40] Micha≈Ç Zawalski, William Chen, Karl Pertsch, Oier Mees, Chelsea Finn, and Sergey Levine. Robotic control via embodied chain-of-thought reasoning. In 8th Annual Conference on Robot Learning, 2024. URL https://openreview.net/forum?id=S70MgnIA0v. [41] Jaden Clark, Suvir Mirchandani, Dorsa Sadigh, and Suneel Belkhale. Action-free reasoning for policy generalization. arXiv preprint arXiv:2502.03729, 2025. [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [43] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [44] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175, 2018. [45] Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernandez Abrego, Steve Yuan, Chris Tar, Yun-Hsuan Sung, et al. Multilingual universal sentence encoder for semantic retrieval. arXiv preprint arXiv:1907.04307, 2019. [46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [47] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [49] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [50] Richard Sutton. The bitter lesson. Incomplete Ideas (blog), 13(1):38, 2019. [51] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [52] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: comprehensive survey. arXiv preprint arXiv:2310.19852, 2023. [53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [54] Alexander Novikov, Ng√¢n Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. Technical report, Technical report, Google DeepMind, 05 2025. URL https://storage. googleapis . . . , 2025. Survey on Vision-Language-Action Models: An Action Tokenization Perspective [55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [57] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [58] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [59] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L√©onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram√©, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [60] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram√©, Morgane Rivi√®re, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [61] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L√©lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. [62] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 45824597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology. org/2021.acl-long.353/. [63] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 5:208215, 2024. [64] Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 6168, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.8. URL https://aclanthology. org/2022.acl-short.8/. [65] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 30453059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10. 18653/v1/2021.emnlp-main.243. URL https://aclanthology.org/2021.emnlp-main.243/. [66] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 27902799. PMLR, 2019. [67] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. [68] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 50 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [69] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [70] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [71] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. [72] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [73] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [74] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J. Shekita, and Bor-Yiing Su. Scaling distributed machine learning with the parameter server. In OSDI, pages 583598. USENIX Association, 2014. [75] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marcaurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. Advances in neural information processing systems, 25, 2012. [76] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Gregory Ganger, Phillip Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM symposium on operating systems principles, pages 115, 2019. [77] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [78] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541551, 1989. [79] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [80] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [81] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=YicbFdNTTy. [82] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [83] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [84] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. 51 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [85] Timoth√©e Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need In The Twelfth International Conference on Learning Representations, 2024. URL https: registers. //openreview.net/forum?id=2dnO3LLiJ1. [86] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [87] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. [88] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, and Alexander Schwing. Putting the object back into video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31513161, 2024. [89] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. [90] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. In European Conference on Computer Vision, pages 1835. Springer, 2024. [91] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1096510975, 2022. [92] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [93] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the\" edge\" of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. [94] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [95] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [96] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= St1giarCHLP. [97] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=PxTIG12RRHS. [98] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pages 1678416804. PMLR, 2022. [99] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 52 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [100] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [101] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [102] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [103] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [104] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [105] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73107320, 2024. [106] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [107] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video URL https://openai.com/research/ generation models as world simulators. video-generation-models-as-world-simulators. 2024. [108] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [109] Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky TQ Chen, David Lopez-Paz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and code. arXiv preprint arXiv:2412.06264, 2024. [110] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [111] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rockt√§schel. Genie 2: large-scale foundation world model. 2024. URL https://deepmind.google/discover/blog/ genie-2-a-large-scale-foundation-world-model/. [112] Bowen Wen, Wei Yang, Jan Kautz, and Stan Birchfield. Foundationpose: Unified 6d pose estimation and tracking of novel objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1786817879, 2024. [113] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3d with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98269836, 2024. 53 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [114] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training In International conference on machine for unified vision-language understanding and generation. learning, pages 1288812900. PMLR, 2022. [115] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [116] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [117] Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. [118] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [119] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. URL https://arxiv.org/abs/2308.12966. [120] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [121] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [122] Lucas Beyer, Andreas Steiner, Andr√© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [123] Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. Advances in Neural Information Processing Systems, 36: 1640616425, 2023. [124] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models, 2025. URL https://arxiv.org/abs/2501.09747. [125] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. ùúã0.5: vision-language-action model with open-world generalization, 2025. URL https://arxiv.org/abs/2504.16054. [126] Danny Driess, Jost Tobias Springenberg, Brian Ichter, Lili Yu, Adrian Li-Bell, Karl Pertsch, Allen Ren, Homer Walke, Quan Vuong, Lucy Xiaoyang Shi, et al. Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. arXiv preprint arXiv:2505.23705, 2025. 54 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [127] Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Jiao Qiao, Peng Gao, and Hongsheng Li. Instruct2act: Mapping multi-modality instructions to robotic actions with large language model. ArXiv, abs/2305.11176, 2023. URL https://api.semanticscholar.org/CorpusID:258762636. [128] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. In 8th Annual Conference on Robot Learning, 2024. [129] Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, and Yitao Liang. Rocket-1: Mastering open-world interaction with visual-temporal context prompting. arXiv preprint arXiv:2410.17856, 2024. [130] Zhixuan Xu, Chongkai Gao, Zixuan Liu, Gang Yang, Chenrui Tie, Haozhuo Zheng, Haoyu Zhou, Weikun Peng, Debang Wang, Tianrun Hu, et al. Manifoundation model for general-purpose robotic manipulation of contact synthesis with arbitrary objects and robots. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1090510912. IEEE, 2024. [131] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. In International Conference on Machine Learning (ICML), 2025. [132] Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents. Advances in Neural Information Processing Systems, 37:73278 73308, 2024. [133] Kevin Black, Manuel Galliker, and Sergey Levine. Real-time execution of action chunking flow policies. arXiv preprint arXiv:2506.07339, 2025. [134] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 91189147. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/v162/huang22a.html. [135] Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=G2Q2Mh3avow. [136] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. Inner monologue: Embodied reasoning through In 6th Annual Conference on Robot Learning, 2022. URL https: planning with language models. //openreview.net/forum?id=3R3Pz5i0tye. [137] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-e: An embodied multimodal language model. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 84698488. PMLR, 2329 Jul 2023. URL https: //proceedings.mlr.press/v202/driess23a.html. [138] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. 55 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [139] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36:2508125094, 2023. [140] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020. [141] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, Proceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pages 10251037. PMLR, 30 Oct01 Nov 2020. URL https://proceedings.mlr.press/v100/gupta20a.html. [142] Yanjiang Guo, Yen-Jen Wang, Lihan Zha, and Jianyu Chen. Doremi: Grounding language model by detecting and recovering from plan-execution misalignment. In International Conference on Intelligent Robots and Systems (IROS), pages 1212412131. IEEE, 2024. [143] Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, and Yang Gao. Look before you leap: Unveiling the power of GPT-4v in robotic vision-language planning. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. URL https://openreview.net/forum?id=n82dpqpa7J. [144] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchmark & learning environment, 2019. [145] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. [146] Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Pengju An, Xiaoqi Li, Kaichen Zhou, Senqiao Yang, Renrui Zhang, Yandong Guo, and Shanghang Zhang. Robomamba: Efficient vision-language-action model for robotic reasoning and manipulation. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 40085 40110. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/ paper/2024/file/46a126492ea6fb87410e55a58df2e189-Paper-Conference.pdf. [147] Aoran Mei, Guo-Niu Zhu, Huaxiang Zhang, and Zhongxue Gan. Replanvlm: Replanning robotic tasks with visual language models. IEEE Robotics and Automation Letters, 2024. [148] Rutav Shah, Albert Yu, Yifeng Zhu, Yuke Zhu, and Roberto Mart√≠n-Mart√≠n. Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation. 2025 IEEE International Conference on Robotics and Automation (ICRA), 2025. [149] Yunhai Feng, Jiaming Han, Zhuoran Yang, Xiangyu Yue, Sergey Levine, and Jianlan Luo. Reflective planning: Vision-language models for multi-stage long-horizon robotic manipulation. arXiv preprint arXiv:2502.16707, 2025. [150] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bƒ±yƒ±k, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. arXiv preprint arXiv:2412.04453, 2024. [151] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. arXiv preprint arXiv:2502.21257, 2025. [152] Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl Pertsch, Jianlan Luo, Sergey Levine, and Chelsea Finn. Yell at your robot: Improving on-the-fly from language corrections. CoRR, abs/2403.12910, 2024. URL https://doi.org/10.48550/arXiv.2403.12910. 56 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [153] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: from natural language instructions to feasible plans. Autonomous Robots, 47(8):13451365, November 2023. ISSN 1573-7527. doi: 10.1007/s10514-023-10131-7. URL http://dx.doi.org/10.1007/ s10514-023-10131-7. [154] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, et al. Video language planning. arXiv preprint arXiv:2310.10625, 2023. [155] Sai H. Vemprala, Rogerio Bonatti, Arthur Fender C. Bucker, and Ashish Kapoor. Chatgpt for robotics: IEEE Access, 12:5568255696, 2023. URL https://api. Design principles and model abilities. semanticscholar.org/CorpusID:259141622. [156] Yao Mu, Junting Chen, Qinglong Zhang, Shoufa Chen, Qiaojun Yu, Chongjian GE, Runjian Chen, Zhixuan Liang, Mengkang Hu, Chaofan Tao, Peize Sun, Haibao Yu, Chao Yang, Wenqi Shao, Wenhai Wang, Jifeng Dai, Yu Qiao, Mingyu Ding, and Ping Luo. Robocodex: multimodal code generation for robotic behavior synthesis. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [157] Junting Chen, Yao Mu, Qiaojun Yu, Tianming Wei, Silang Wu, Zhecheng Yuan, Zhixuan Liang, Chao Yang, Kaipeng Zhang, Wenqi Shao, Yu Qiao, Huazhe Xu, Mingyu Ding, and Ping Luo. Roboscript: Code generation for free-form manipulation tasks across real and simulation. ArXiv, abs/2402.14623, 2024. URL https://api.semanticscholar.org/CorpusID:267782428. [158] Chen Wang, Fei Xia, Wenhao Yu, Tingnan Zhang, Ruohan Zhang, C. Karen Liu, Fei-Fei Li, Jie Tan, and Jacky Liang. Chain-of-modality: Learning manipulation programs from multimodal human videos with vision-language-models. In ICRA, 2025. URL https://api.semanticscholar.org/CorpusID: 277940198. [159] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [160] S. Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In International Symposium on Field and Service Robotics, 2017. URL https://api.semanticscholar.org/CorpusID:20999239. [161] Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, et al. Open-world object manipulation using pre-trained vision-language models. In Conference on Robot Learning, pages 33973417. PMLR, 2023. [162] Siyuan Huang, Haonan Chang, Yuhan Liu, Yimeng Zhu, Hao Dong, Abdeslam Boularias, Peng Gao, and Hongsheng Li. A3vlm: Actionable articulation-aware vision language model. In 8th Annual Conference on Robot Learning, 2024. [163] Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, et al. Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. arXiv preprint arXiv:2502.13143, 2025. [164] Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, and Jeannette Bohg. Kite: Keypoint-conditioned policies for semantic manipulation. In 7th Annual Conference on Robot Learning, 2023. [165] Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, and Yue Wang. Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. In 8th Annual Conference on Robot Learning, 2024. [166] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration. IEEE Robotics and Automation Letters, 2024. 57 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [167] Haichao Liu, Sikai Guo, Pengfei Mai, Jiahang Cao, Haoang Li, and Jun Ma. Robodexvlm: Visual language model-enabled task planning and motion control for dexterous robot manipulation. arXiv preprint arXiv:2503.01616, 2025. [168] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1806118070, 2024. [169] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. [170] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction in robotics. In 8th Annual Conference on Robot Learning, 2024. [171] Haoxu Huang, Fanqi Lin, Yingdong Hu, Shengjie Wang, and Yang Gao. Copa: General robotic manipulation through spatial constraints of parts with foundation models. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 94889495. IEEE, 2024. [172] Mingjie Pan, Jiyao Zhang, Tianshu Wu, Yinghao Zhao, Wenlong Gao, and Hao Dong. Omnimanip: Towards general robotic manipulation via object-centric interaction primitives as spatial constraints. arXiv preprint arXiv:2501.03841, 2025. [173] Zixian Liu, Mingtong Zhang, and Yunzhu Li. Kuda: Keypoints to unify dynamics learning and visual prompting for open-vocabulary robotic manipulation. arXiv preprint arXiv:2503.10546, 2025. [174] Shaofei Cai, Zihao Wang, Kewei Lian, Zhancun Mu, Xiaojian Ma, Anji Liu, and Yitao Liang. Rocket1: Mastering open-world interaction with visual-temporal context prompting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1212212131, 2025. [175] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In CoRL, pages 894906, 2021. URL https://proceedings.mlr.press/v164/ shridhar22a.html. [176] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Rearranging the visual world for robotic manipulation. In Conference on Robot Learning, pages 726747. PMLR, 2021. [177] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. pages 87488763, 2021. [178] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, Yen-Sung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross B. Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. CoRR, abs/2409.17146, 2024. URL https://doi.org/10.48550/arXiv.2409.17146. [179] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, and Jianfeng Gao. Magma: foundation model for multimodal In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages ai agents. 1420314214, June 2025. 58 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [180] Hanzhi Chen, Boyang Sun, Anran Zhang, Marc Pollefeys, and Stefan Leutenegger. VidBot: Learning generalizable 3d actions from in-the-wild 2d human videos for zero-shot robotic manipulation. Proceedings of the Computer Vision and Pattern Recognition Conference, 2025. [181] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding DINO: marrying DINO with grounded pre-training for open-set object detection. In ECCV (47), volume 15105 of Lecture Notes in Computer Science, pages 3855. Springer, 2024. [182] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr√§henb√ºhl, and Ishan Misra. Detecting twentythousand classes using image-level supervision. In ECCV (9), volume 13669 of Lecture Notes in Computer Science, pages 350368. Springer, 2022. [183] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple openIn European conference on computer vision, pages 728755. Springer, vocabulary object detection. 2022. [184] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. [185] Hao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao Gou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie, and Cewu Lu. Anygrasp: Robust and efficient grasp perception in spatial and temporal domains. IEEE Transactions on Robotics, 39(5):39293945, 2023. [186] Dayou Li, Chenkun Zhao, Shuo Yang, Lin Ma, Yibin Li, and Wei Zhang. Learning instruction-guided manipulation affordance via large models for embodied robotic tasks. In 2024 International Conference on Advanced Robotics and Mechatronics (ICARM), pages 662667. IEEE, 2024. [187] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Computer Vision ECCV 2020, pages 405421, 2020. [188] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4), 2023. [189] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Joshua B. Tenenbaum. Learning to act from actionless videos through dense correspondences. In The Twelfth International Conference on Learning Representations, 2024. [190] Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Caelan Reed Garrett, Fabio Ramos, Dieter Fox, Anqi Li, Abhishek Gupta, and Ankit Goyal. HAMSTER: Hierarchical action models for open-world robot manipulation. In The Thirteenth International Conference on Learning Representations, 2025. [191] Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, and Shuran Song. Flow as the cross-domain manipulation interface. In 8th Annual Conference on Robot Learning, 2024. [192] Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang. GROOT: Learning to follow instructions by watching gameplay videos. In The Twelfth International Conference on Learning Representations, 2024. [193] Shaofei Cai, Bowei Zhang, Zihao Wang, Haowei Lin, Xiaojian Ma, Anji Liu, and Yitao Liang. GROOT-2: Weakly supervised multimodal instruction following agents. In The Thirteenth International Conference on Learning Representations, 2025. [194] Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, and Animesh Garg. Quest: Self-supervised skill abstractions for learning continuous control. Advances in Neural Information Processing Systems, 37: 40624089, 2024. Survey on Vision-Language-Action Models: An Action Tokenization Perspective [195] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning ùëò modes with one stone. Advances in neural information processing systems, 35:2295522968, 2022. [196] Seungjae Lee, Yibin Wang, Haritheja Etukuru, H. Jin Kim, Nur Muhammad Mahi Shafiullah, and Lerrel Pinto. Behavior generation with latent actions. In ICML, 2024. [197] Pei Liu, Haipeng Liu, Haichao Liu, Xin Liu, Jinxin Ni, and Jun Ma. Vlm-e2e: Enhancing end-to-end autonomous driving with multimodal driver attention fusion. arXiv preprint arXiv:2502.18042, 2025. [198] Chongkai Gao, Haozhuo Zhang, Zhixuan Xu, Zhehao Cai, and Lin Shao. Flip: Flow-centric generative planning for general-purpose manipulation tasks. arXiv preprint arXiv:2412.08261, 2024. URL https: //arxiv.org/abs/2412.08261. [199] Yuanqi Yao, Siao Liu, Haoming Song, Delin Qu, Qizhi Chen, Yan Ding, Bin Zhao, Zhigang Wang, Xuelong Li, and Dong Wang. Think small, act big: Primitive prompt learning for lifelong robot manipulation, 2025. URL https://arxiv.org/abs/2504.00420. [200] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets, 2021. URL https://arxiv.org/abs/2109.13396. [201] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, Aniruddha Kembhavi, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai, 2022. [202] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, qiang liu, Yuke Zhu, and Peter Stone. LIBERO: Benchmarking knowledge transfer for lifelong robot learning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [203] Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, and Roei Herzig. LLARVA: Vision-action instruction tuning enhances robot learning. In 8th Annual Conference on Robot Learning, 2024. [204] Jianlan Luo, Charles Xu, Liam Tan, Jeffrey Wu, Zipeng Lin, and Sergey Levine. FMB: functional manipulation benchmark for generalizable robotic learning. In Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions, 2024. [205] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, K. Gopalakrishnan, Karol Hausman, Alexander Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, A. Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J. Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, S. Levine, Yao Lu, U. Malla, D. Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, M. Ryoo, Grecia Salazar, Pannag R. Sanketi, Kevin Sayed, Jaspiar Singh, S. Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Q. Vuong, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. ArXiv, abs/2212.06817, 2022. [206] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, K. Choromanski, Tianli Ding, Danny Driess, Kumar Avinava Dubey, Chelsea Finn, Peter R. Florence, Chuyuan Fu, Montse Gonzalez Arenas, K. Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, A. Irpan, Nikhil J. Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, S. Levine, H. Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, M. Ryoo, Grecia Salazar, Pannag R. Sanketi, P. Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Q. Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Ted Xiao, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-language-action models transfer web knowledge to robotic control. In 7th Annual Conference on Robot Learning, 2023. [207] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alexander Herzog, 60 Survey on Vision-Language-Action Models: An Action Tokenization Perspective Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch√∂lkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter B√ºchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Paul Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Haoshu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Jo√£o Silv√©rio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J. Joshi, Niko S√ºnderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R. Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart√≠n-Mart√≠n, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Liangwei Xu, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, and Zipeng Lin. Open x-embodiment: Robotic learning datasets and rt-x models : Open x-embodiment collaboration. In ICRA, pages 68926903, 2024. URL https://doi.org/10.1109/ICRA57147.2024.10611477. [208] Dantong Niu, Yuvan Sharma, Haoru Xue, Giscard Biamby, Junyi Zhang, Ziteng Ji, Trevor Darrell, and In Forty-second Roei Herzig. Pre-training auto-regressive robotic models with 4d representations. International Conference on Machine Learning, 2025. [209] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, , Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision. CoRR, abs/2006.13256, 2020. [210] Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, and Andrew Zisserman. TAPIR: Tracking any point with per-frame initialization and temporal refinement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1006110072, 2023. [211] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020. URL https://arxiv.org/abs/2003.12039. 61 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [212] R.E. Kalman. new approach to linear filtering and prediction problems. Journal of Basic Engineering, 82(1):3545, 1960. [213] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [214] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Rich Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pre-trained image-editing diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [215] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The Something Something Video Database for Learning and Evaluating Visual Common Sense . In 2017 IEEE International Conference on Computer Vision (ICCV), pages 58435851, Los Alamitos, CA, USA, October 2017. IEEE Computer Society. doi: 10.1109/ ICCV.2017.622. URL https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.622. [216] Fei Ni, Jianye Hao, Shiguang Wu, Longxin Kou, Jiashun Liu, Yan Zheng, Bin Wang, and Yuzheng Zhuang. Generate subgoal images before act: Unlocking the chain-of-thought reasoning in diffusion model for robot manipulation with multimodal prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1399114000, June 2024. [217] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. In ICML, 2023. [218] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, and Tsung-Yi Lin. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models, 2025. URL https://arxiv.org/abs/2503.22020. [219] Homer Rich Walke, Kevin Black, Tony Z. Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, Abraham Lee, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: dataset for robot learning at scale. In 7th Annual Conference on Robot Learning, 2023. [220] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [221] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. Gmflow: Learning optical flow via global matching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81218130, 2022. [222] Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation, 2024. URL https://arxiv.org/abs/2409. 16283. [223] Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang. GEVRM: Goal-expressive video generation model for robust visual manipulation. In The Thirteenth International Conference on Learning Representations, 2025. [224] Corey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. Robotics: Science and Systems, 2021. URL https://arxiv.org/abs/2005.07648. [225] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, Song Han, and Yao Lu. VILA-u: unified foundation model integrating visual understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. 62 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [226] Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. LIV: Languageimage representations and rewards for robotic control. In Workshop on Reincarnating Reinforcement Learning at ICLR 2023, 2023. [227] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In The Twelfth International Conference on Learning Representations, 2024. [228] Mohammad Hassan Vali and Tom B√§ckstr√∂m. Nsvq: Noise substitution in vector quantization for machine learning. IEEE Access, 10:1359813610, 2022. [229] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation., 2024. URL https: //arxiv.org/abs/2405.05941. [230] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6114. [231] Marc Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of artificial intelligence research, 47:253279, 2013. [232] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [233] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions. Robotics: Science and Systems, 2025. [234] Dhruv Shah, Ajay Sridhar, Arjun Bhorkar, Noriaki Hirose, and Sergey Levine. Gnm: general navigation In ICRA, pages 72267233, 05 2023. doi: 10.1109/ICRA48891.2023. model to drive any robot. 10161227. [235] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments . In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 36743683, Los Alamitos, CA, USA, June 2018. IEEE Computer Society. doi: 10.1109/CVPR.2018.00387. URL https://doi.ieeecomputersociety.org/10. 1109/CVPR.2018.00387. [236] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the World in 3,000 Hours of Egocentric Video. In IEEE/CVF Computer Vision and Pattern Recognition (CVPR), 2022. [237] Shaofei Cai, Zhancun Mu, Kaichen He, Bowei Zhang, Xinyue Zheng, Anji Liu, and Yitao Liang. Minestudio: streamlined package for minecraft ai agent development. arXiv preprint arXiv:2412.18293, 2024. 63 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [238] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pages 11131132. PMLR, 2019. URL http://proceedings.mlr.press/v100/lynch20a.html. [239] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. BC-z: Zero-shot task generalization with robotic imitation learning. In 5th Annual Conference on Robot Learning, 2021. [240] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G√≥mez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gim√©nez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. generalist agent. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. Featured Certification, Outstanding Certification. [241] Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Vision-language foundation models as effective In The Twelfth International Conference on Learning Representations, 2024. URL robot imitators. https://openreview.net/forum?id=lFYj0oibGR. [242] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. [243] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsanit, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: Universe of Annotated 3D Objects . In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1314213153, Los Alamitos, CA, USA, June 2023. IEEE Computer Society. doi: 10.1109/CVPR52729.2023.01263. URL https://doi.ieeecomputersociety.org/10.1109/CVPR52729.2023.01263. [244] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes . In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 24322443, Los Alamitos, CA, USA, July 2017. IEEE Computer Society. doi: 10.1109/CVPR.2017.261. URL https://doi.ieeecomputersociety. org/10.1109/CVPR.2017.261. [245] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Niessner. Rio: 3d object instance re-localization in changing indoor environments. In Proceedings IEEE International Conference on Computer Vision (ICCV), 2019. [246] Ram Ramrakhya, Eric Undersander, Dhruv Batra, and Abhishek Das. Habitat-web: Learning embodied object-search strategies from human demonstrations at scale. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51735183, 2022. [247] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In ICLR, 2024. [248] Dibya Ghosh, Homer Rich Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Quan Vuong, Ted Xiao, Pannag R. Sanketi, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Robotics: Science and Systems, 2024. [249] Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, and Jian Tang. Tinyvla: Toward fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 10(4): 39883995, 2025. doi: 10.1109/LRA.2025.3544909. 64 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [250] Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. HiRT: Enhancing robotic control with hierarchical robot transformers. In 8th Annual Conference on Robot Learning, 2024. [251] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation, 2024. URL https://arxiv.org/abs/ 2410.06158. [252] Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, et al. Cogact: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024. [253] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Donovon Jackson, Charlotte Le, Yunshuang Li, Xinyu Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Khuong Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Mart√≠n-Mart√≠n, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. DROID: large-scale in-the-wild robot manipulation dataset. In RSS 2024 Workshop: Data Generation for Robotics, 2024. [254] Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, and Xianyuan Zhan. Universal actions for enhanced embodied foundation models. In 7th Robot Learning Workshop: Towards Robots with Human-Level Abilities, 2025. [255] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. Robotics: Science and Systems, 2025. [256] Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, and Yitao Liang. Jarvis-vla: Post-training largescale vision language models to play visual games with keyboards and mouse, 2025. URL https: //arxiv.org/abs/2503.16365. [257] Haowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: task-centric framework for open-ended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023. [258] Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, and Shanghang Zhang. Hybridvla: Collaborative diffusion and autoregression in unified vision-languageaction model, 2025. URL https://arxiv.org/abs/2503.10631. [259] NVIDIA, :, Johan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. Gr00t n1: An open foundation model for generalist humanoid robots, 2025. URL https://arxiv.org/abs/2503.14734. [260] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. In RSS 2024 Workshop: Data Generation for Robotics, 2024. 65 Survey on Vision-Language-Action Models: An Action Tokenization Perspective [261] Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. In CoRL Workshop on Learning Robot Fine and Dexterous Manipulation: Perception and Control, 2024. URL https://openreview.net/forum?id=KgUgavAl6Y. [262] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), April 2018. doi: 10.1609/aaai.v32i1.11671. URL https://ojs.aaai.org/index.php/ AAAI/article/view/11671. [263] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. [264] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up multilingual vision and language model, 2023. URL https://arxiv.org/abs/2305.18565. [265] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. [266] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Haibin Yan, and Yuheng Zhou. Momanipvla: Transferring visionlanguage-action models for general mobile manipulation, 2025. URL https://arxiv.org/abs/ 2503.13446. [267] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [268] Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. In ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems, 2023. [269] Cheng Chi, S. Feng, Yilun Du, Zhenjia Xu, Eric A. Cousineau, B. Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. ArXiv, abs/2303.04137, 2023. [270] Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Qinghui Gao Yuan Li, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, and Xipeng Qiu. Vlabench: large-scale benchmark for language-conditioned robotics manipulation with long-horizon reasoning tasks, 2024. URL https://arXiv.org/abs/ 2412.18194. [271] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [272] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17801790, 2021. [273] Bo Jiang, Shaoyu Chen, Qian Zhang, Wenyu Liu, and Xinggang Wang. Alphadrive: Unleashing the power of vlms in autonomous driving via reinforcement learning and reasoning, 2025. URL https: //arxiv.org/abs/2503.07608. Survey on Vision-Language-Action Models: An Action Tokenization Perspective [274] A. Azzolini, H. Brandon, P. Chattopadhyay, H. Chen, J. Chu, Y. Cui, J. Diamond, Y. Ding, F. Ferroni, and R. Govindaraju et al. Cosmos-reason1: From physical common sense to embodied reasoning, 2025. URL https://arxiv.org/abs/2503.15558. [275] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. [276] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. [277] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, , Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. International Journal of Computer Vision (IJCV), 2021. URL https://doi.org/10.1007/s11263-021-01531-2. [278] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. In 7th Annual Conference on Robot Learning, 2023. [279] Jiyao Zhang, Mingjie Pan, Baifeng Xie, Yinghao Zhao, Wenlong Gao, Guangte Xiang, Jiawei Zhang, Dong Li, Zhijun Li, Sheng Zhang, Hongwei Fan, Chengyue Zhao, Shukai Yang, Maoqing Yao, Chuanzhe Suo, and Hao Dong. Agibot digitalworld. https://agibot-digitalworld.com/, 2025. [280] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Junbo Wang, Haoyi Zhu, and Cewu Lu. Rh20t: robotic dataset for learning diverse skills in one-shot. In RSS 2023 Workshop on Learning for Task and Motion Planning, 2023. [281] Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation. arXiv preprint arXiv:2412.13877, 2024. [282] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja Etukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, and Lerrel Pinto. On bringing robots home. arXiv preprint arXiv:2311.16098, 2023. [283] Qingwen Bu, Hongyang Li, Li Chen, Jisong Cai, Jia Zeng, Heming Cui, Maoqing Yao, and Yu Qiao. Towards synergistic, generalized, and efficient dual-system for robotic manipulation. arXiv preprint arXiv:2410.08001, 2024. [284] Junjie Wen, Minjie Zhu, Yichen Zhu, Zhibin Tang, Jinming Li, Zhongyi Zhou, Chengmeng Li, Xiaoyu Liu, Yaxin Peng, Chaomin Shen, et al. Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. arXiv preprint arXiv:2412.03293, 2024. [285] AgiBot World Colosseum contributors. OpenDriveLab/AgiBot-World, 2024. Agibot world colosseum. https://github.com/ [286] Jiaheng Hu, Rose Hendrix, Ali Farhadi, Aniruddha Kembhavi, Roberto Mart√≠n-Mart√≠n, Peter Stone, KuoHao Zeng, and Kiana Ehsani. FLare: Achieving masterful and adaptive robot policies with large-scale reinforcement learning fine-tuning. In 1st Workshop on X-Embodiment Robot Learning, 2024. [287] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, Yuning Chai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aurelien Chouard, Pei Sun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motion forecasting for autonomous driving: The waymo open motion dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 97109719, October 2021. Survey on Vision-Language-Action Models: An Action Tokenization Perspective [288] Kan Chen, Runzhou Ge, Hang Qiu, Rami Ai-Rfou, Charles R. Qi, Xuanyu Zhou, Zoey Yang, Scott Ettinger, Pei Sun, Zhaoqi Leng, Mustafa Mustafa, Ivan Bogun, Weiyue Wang, Mingxing Tan, and Dragomir Anguelov. Womd-lidar: Raw sensor dataset benchmark for motion forecasting. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), May 2024. [289] Whye Kit Fong, Rohit Mohan, Juana Valeria Hurtado, Lubing Zhou, Holger Caesar, Oscar Beijbom, and Abhinav Valada. Panoptic nuscenes: large-scale benchmark for lidar panoptic segmentation and tracking. IEEE Robotics and Automation Letters, 7(2):37953802, 2022. doi: 10.1109/LRA.2022. 3148457. [290] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. [291] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. In Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. [292] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. [293] Angel Villar-Corrales, Ani Karapetyan, Andreas Boltres, and Sven Behnke. Mspred: Video prediction at multiple spatio-temporal scales with hierarchical recurrent networks. In British Machine Vision Conference BMVC, 2022. [294] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured world models from human videos. In RSS, 2023. [295] Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. In 2025 IEEE International Conference on Robotics and Automation (ICRA), 2025. [296] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Mart√≠n-Mart√≠n, Abhishek Joshi, Soroush Nasiriany, and Yifeng Zhu. robosuite: modular simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293, 2020. [297] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: platform for embodied ai research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 93399347, 2019. [298] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles MackIsaac gym: lin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. High performance gpu based physics simulation for robot learning. In J. Vanschoren and S. Yeung, editors, Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS), volume 1, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper_ files/paper/2021/file/28dd2c7955ce926456240b2ff0100bde-Paper-round2.pdf. [299] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit: unified simulation framework for interactive robot learning environments. IEEE Robotics and Automation Letters, 8(6):37403747, 2023. doi: 10.1109/LRA.2023.3270034. [300] Kiana Ehsani, Tanmay Gupta, Rose Hendrix, Jordi Salvador, Luca Weihs, Kuo-Hao Zeng, Kunal Pratap Singh, Yejin Kim, Winson Han, Alvaro Herrasti, et al. Spoc: Imitating shortest paths in simulation enables effective navigation and manipulation in the real world. arXiv preprint arXiv:2312.02976, 2023. [301] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. Homerobot: Openvocabulary mobile manipulation. arXiv preprint arXiv:2306.11565, 2023. Survey on Vision-Language-Action Models: An Action Tokenization Perspective [302] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [303] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Oier Mees, Karl Pertsch, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. In 8th Annual Conference on Robot Learning, 2024. [304] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart√≠n-Mart√≠n, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on robot learning, pages 477490. PMLR, 2022. [305] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K√ºttler, Andrew Lefrancq, Simon Green, V√≠ctor Vald√©s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab, 2016. URL https://arxiv.org/abs/1612.03801. [306] Alex Lee, Coline Manon Devin, Yuxiang Zhou, Thomas Lampe, Konstantinos Bousmalis, Jost Tobias Springenberg, Arunkumar Byravan, Abbas Abdolmaleki, Nimrod Gileadi, David Khosid, et al. Beyond pick-and-place: Tackling robotic stacking of diverse shapes. In 5th Annual Conference on Robot Learning, 2021. [307] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [308] Huajie Tan, Xiaoshuai Hao, Cheng Chi, Minglan Lin, Yaoxu Lyu, Mingyu Cao, Dong Liang, Zhuo Chen, Mengsi Lyu, Cheng Peng, et al. Roboos: hierarchical embodied framework for cross-embodiment and multi-agent collaboration. arXiv preprint arXiv:2505.03673, 2025. [309] Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. [310] Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. Conrft: reinforced fine-tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450, 2025. [311] Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What can rl bring to vla generalization? an empirical study. arXiv preprint arXiv:2505.19789, 2025. [312] SimpleVLA-RL Team. Simplevla-rl: Online rl with simple reward enables training vla models with only one trajectory. https://github.com/PRIME-RL/SimpleVLA-RL, 2025. GitHub repository. [313] Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. Improving vision-language-action model with online reinforcement learning. arXiv preprint arXiv:2501.16664, 2025. [314] Amir Moeini, Jiuqi Wang, Jacob Beck, Ethan Blaser, Shimon Whiteson, Rohan Chandra, and Shangtong Zhang. survey of in-context reinforcement learning. arXiv preprint arXiv:2502.07978, 2025. [315] Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia. RL-GPT: Integrating reinforcement learning and code-as-policy. arXiv preprint arXiv:2402.19299, 2024. [316] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=rc8o_j8I8PX. Survey on Vision-Language-Action Models: An Action Tokenization Perspective [317] Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, and Yaodong Yang. Safevla: Towards safety alignment of vision-language-action model via constrained learning. arXiv preprint arXiv:2503.03480, 2025. [318] Cheng Chi, Zhenjia Xu, Chuer Pan, Eric Cousineau, Benjamin Burchfiel, Siyuan Feng, Russ Tedrake, and Shuran Song. Universal manipulation interface: In-the-wild robot teaching without in-the-wild robots. Robotics: Science and Systems, 2024. [319] Mengda Xu, Han Zhang, Yifan Hou, Zhenjia Xu, Linxi Fan, Manuela Veloso, and Shuran Song. Dexumi: Using human hand as the universal manipulation interface for dexterous manipulation. In 1st Workshop on Robot Hardware-Aware Intelligence. [320] Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, and Karen Liu. Dexcap: Scalable and portable mocap data collection system for dexterous manipulation. In 2nd Workshop on Dexterous Manipulation: Design, Perception and Control (RSS), 2024. URL https://openreview.net/forum? id=EiarCRjOm7."
        }
    ],
    "affiliations": [
        "Institute for AI, Peking University",
        "PKU-PsiBot Joint Lab",
        "School of Computer Science, Peking University"
    ]
}