{
    "paper_title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
    "authors": [
        "Yueqian Lin",
        "Zhengmian Hu",
        "Jayakumar Subramanian",
        "Qinsi Wang",
        "Nikos Vlassis",
        "Hai \"Helen\" Li",
        "Yiran Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the model's process, not just receive an output. However, the monolithic text from methods like Chain-of-Thought (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, a system whose asynchronous architecture decouples a streaming LLM backend from a conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the model's reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600x compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling a two-way dialogue with a model's thought process, AsyncVoice Agent offers a new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks."
        },
        {
            "title": "Start",
            "content": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning Yueqian Lin1,2, Zhengmian Hu2, Jayakumar Subramanian2, Qinsi Wang1,2 Nikos Vlassis2, Hai Helen Li1, Yiran Chen1 1Duke University, Durham, NC, USA 2Adobe Research, San Jose, CA, USA 5 2 0 2 7 ] . e [ 1 6 5 1 6 1 . 0 1 5 2 : r from methods like Chain-of-Thought AbstractEffective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the models process, not just receive an output. However, the monolithic text (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, system whose asynchronous architecture decouples streaming LLM backend from conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the models reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600 compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling two-way dialogue with models thought process, AsyncVoice Agent offers new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks.1 Index TermsReal-Time Interaction, Asynchronous Agents, Human-AI Collaboration, Planning, Reasoning. of an agents internal reasoning stream, and allowing for user interruption during this process, remains significant, unaddressed gap. To fill this gap, this paper presents the architecture and demonstration of AsyncVoice Agent, system designed to serve as an interactive, real-time voice interface for explaining an LLMs ongoing reasoning processes. Rather than verbalizing static block of text, our system narrates each thought process as it streams from the model. The primary contribution is system that enables truly asynchronous dialogue about live reasoning process, allowing user to seamlessly interrupt the agents explanation to ask questions or provide feedback. This capability transforms the interaction from passive user experience into collaborative dialogue, filling crucial gap for explainability and trust. This paper details the systems architecture and its novel components. I. INTRODUCTION II. ASYNCVOICE AGENT ARCHITECTURE Large Language Models (LLMs) increasingly tackle complex tasks by generating step-by-step reasoning, often in Chain-of-Thought (CoT) format [1]. While intended to provide transparency, these reasoning traces are typically delivered as monolithic, text-based outputs. This presentation format is particularly ill-suited for spoken language interfaces, as it forces users into passive listening role for potentially long, uninterrupted monologues, hindering comprehension and preventing real-time interaction [2][4]. This fundamental mismatch between the static output of reasoning models and the dynamic nature of human conversation limits the potential for fluid, collaborative human-agent partnerships. Consequently, enhancing the interpretability of these reasoning chains has become significant focus of ongoing research. While raw CoT aims for transparency, its verbosity can be significant barrier [2]. Some approaches use textbased summarization or structuring techniques to offer posthoc digestibility [5], and dialogue systems have been proposed to improve explanations [6]. However, these typically do not operate on the live, streaming internal state of separate reasoning LLM. More recent work has explored asynchronous AI agents for real-time tool use with voice interaction, focusing on the agents task execution (e.g., booking flight while conversing) [7]. While this addresses the agents external actions, the challenge of providing real-time, interactive explanations The AsyncVoice Agent is real-time voice interface system designed to provide interactive explanation of an LLMs reasoning process. Our implementation builds upon the foundational RealtimeVoiceChat project2, adapting its core real-time ASR and WebSocket communication layer. Our primary contributions are the design and integration of several novel components that enable the explanation of an external LLMs reasoning stream. As illustrated in Figure 1, our architecture adds new backend reasoning modules and modifies the core agent pipeline. The complete system comprises three core subsystems: (1) the adapted WebSocket layer for bidirectional audio streaming, (2) our novel modular Model Context Protocol (MCP) servers for specialized reasoning tasks, and (3) multi-threaded speech processing pipeline featuring Azure TTS integration for low-latency voice interaction. A. WebSocket Communication Infrastructure The system employs FastAPI-based WebSocket server to handle concurrent bidirectional data streams. The protocol supports two distinct message types: JSON-formatted text messages for control commands and transcription updates, and binary messages containing raw PCM audio data. This dualchannel approach enables sub-second latency for voice interactions while maintaining reliable command transmission. To manage connection-specific conversation context and prevent 1This work was supported by NSF 2112562 and ARO W911NF-23-2-0224. 2https://github.com/KoljaB/RealtimeVoiceChat Fig. 1: The high-level architecture of the AsyncVoice Agent. The system features two primary components: the Backend MCP Servers that generate Continuous Stream of reasoning steps, and the AsyncVoice Agent pipeline, which verbalizes these steps and manages asynchronous user interruptions via Stop Signal. resource leaks, the server uses dedicated context manager that tracks active connections. Crucially, this handler maintains the conversational state by chronologically integrating all events, such as streamed backend reasoning steps and user voice interruptions, into single, unified session context. This design also supports potential multi-user and multi-agent collaborative sessions. B. MCP-Based Modular Reasoning Architecture The backend reasoning capabilities are provided by the specialized Backend MCP Servers shown in Figure 1. These servers are highly modular, allowing different underlying large language models to be configured for specific tasks. For instance, the Travel Planner is configured to use specialized reasoning model for its complex planning capabilities, while the Math Solver leverages the precision of GPT-4o [8]. Regardless of the reasoning model used, each server adheres to the standardized MCP. They emit progress updates and final answers using consistent notification format (ctx.notification). The agents MCP Client subscribes to this stream and processes updates based on semantic prefixes: Thinking: for an intermediate reasoning step, Content: for status update, and Answer: for the final response. This protocol standardization allows the AsyncVoice Agent to adapt to any MCP-compliant backend with minimal effort. final COMPLETE signal is used to trigger UI state transitions on the front-end and update the state manager accordingly. C. Multi-Threaded Speech Processing Pipeline To minimize response latency, the core of the agent orchestrates four concurrent processing threads. This pipeline manages the data flow from the Response Generator (LLM) to the final Voice Output (as shown in Figure 1). Request Processing Thread handles incoming user queries. An LLM Inference Thread processes text through the agents explainer LLM (powered by GPT-4o) to formulate naturallanguage explanations of the backends reasoning or to respond directly to user questions. For audio generation, our system integrates Azure TTS in dual-stage process: Quick Thread synthesizes the first part of response for immediate playback, and Final Thread generates the complete audio. crossfade mechanism ensures seamless transition, creating continuous, natural-sounding speech output. D. Turn Detection and Interruption Handling The system implements turn detection using DistilBERTbased transformer that performs binary sentence completion classification. The model processes text segments with maximum sequence length of 128 tokens and outputs probability scores indicating whether sentence is complete or incomplete. These probability scores are mapped to dynamic pause durations through configurable anchor points, enabling natural conversation flow based on semantic context. The interruption handling mechanism operates across multiple system layers to ensure responsive interaction. Audio worklet processors in the browser continuously monitor microphone input and can trigger interruption signals with sub-100ms latency. Upon detecting user speech, the system immediately issues stop signal that propagates through the audio pipeline: TTS synthesis is aborted with 100ms protection window to prevent abrupt cutoffs, active audio playback queues are cleared, and the speech processing pipeline transitions to listening mode. This multi-level architecture supports natural barge-in conversation patterns, allowing users to interrupt the agents explanation at any point and resume seamlessly after their input is processed. E. Frontend Real-Time Audio Processing The web client, whose interface is depicted in Figure 2, implements modern audio processing pipeline using the Web Audio API with AudioWorklet processors. One processor handles microphone input, while another manages the audio output queue with interruption support. The interface provides the user with real-time transcription display and visualization of the CoT reasoning in the Planning Process Fig. 2: The front-end web interface for the AsyncVoice Agent. The user can enter query to begin task. The interface displays the live Planning Process and indicates that the microphone is always active for interruptions. panel. State management covers WebSocket connection status, audio session states (e.g., listening, processing), and progress tracking, ensuring the user always has clear visual feedback on the systems status. III. EVALUATION To objectively validate the benefits of our proposed architecture, we present rigorous evaluation framework designed to quantify the performance of the AsyncVoice Agent. To ensure objective and reproducible results, the evaluation is conducted using an automated framework. This approach focuses on quantifiable metrics: responsiveness, reasoning quality, and process fidelity. We benchmark our system against two baselines across three distinct, pre-defined operational scenarios. A. Experimental Setup Our evaluation compares the performance of the following three systems: AsyncVoice Agent (Proposed): Our full system, featuring backend MCP Server for reasoning and GPT-4o explainer model for real-time verbalization. Monolithic Agent (Baseline 1): system that uses the same backend MCP Server but verbalizes the entire reasoning chain only after its complete generation. Explainer-Only Agent (Baseline 2): standard conversational system using GPT-4o for both reasoning and verbalization, without decoupled backend. B. Evaluation Scenarios and Metrics We evaluate across three diverse reasoning scenarios using 100 queries for each: Math Solver problems are drawn from the GSM8K benchmark [9] and require multi-step arithmetic; for Travel Planner and Deep Research, we used GPT-4o to generate scenarios with explicit constraints, such as budget limits for travel or minimum citation counts for research, that demand multi-round planning and analysis. Our evaluation assesses performance using three core metrics. The first is responsiveness, measured by Time to First Audio (TTFA), which is the latency from user query submission to the start of audible output. The second is reasoning quality, assessed through hybrid frameworks combining automated scoring with GPT-4o assessment. For the Math Solver, this blends exact numerical accuracy (70%) with reasoning methodology evaluation (30%), while for the integrates Travel Planner and Deep Research scenarios, constraint satisfaction with solution quality assessment. The third metric, process fidelity, is applied exclusively to our AsyncVoice Agent; here, GPT-4o scores the semantic and logical consistency between streamed explanations and the backend reasoning texts on 1-to-5 scale. To ensure fair comparison, complete reasoning traces for all systems are included in the evaluation. it C. Results As summarized in Table I, the AsyncVoice Agent demonstrates fundamental advantages in responsiveness, achieving"
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, Chain-of-thought prompting elicits reasoning in large language models, in Advances in Neural Information Processing Systems, 2022. [2] Y. Zhang, S. S. S. Das, and R. Zhang, Verbosity = veracity: Demystify verbosity compensation behavior of large language models, arXiv preprint arXiv:2402.12883, 2024. [3] Y. Lin, Y. Fu, J. Zhang, Y. Liu, J. Zhang, J. Sun, H. Li, and Y. Chen, Speechprune: Context-aware token pruning for speech information retrieval, in 2025 IEEE International Conference on Multimedia and Expo (ICME), 2025. [4] Y. Lin, Z. Hu, Q. Wang, Y. Liu, H. Zhang, J. Subramanian, N. Vlassis, H. H. Li, and Y. Chen, Voice evaluation of reasoning ability: Diagnosing the modality-induced performance gap, arXiv preprint arXiv:2509.26542, 2025. [5] D.-H. Zhu, Y.-J. Xiong, J.-C. Zhang, X.-J. Xie, and C.-M. Xia, Understanding before reasoning: Enhancing chain-of-thought with iterative summarization pre-prompting, arXiv preprint arXiv:2501.04341, 2025. [6] L. Ho and S. Schlobach, Dialogue-based explanations for logical reasoning using structured argumentation, arXiv preprint arXiv:2502.11291, 2025. [7] A. A. Ginart, N. Kodali, J. Lee, C. Xiong, S. Savarese, and J. Emmons, Asynchronous tool usage for real-time agents, arXiv preprint arXiv:2410.21620, 2024. [8] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [9] K. Cobbe, V. Kosaraju, M. Bavarian, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. TTFA of approximately 15ms across all scenarios, 6001,800 reduction compared to baseline approaches that enables genuine real-time interaction. While baselines occasionally achieve higher reasoning quality scores, the AsyncVoice Agent remains competitive. It is worth noting that for this evaluation, the backend MCP servers were configured for single-pass reasoning process; enabling their native multiround capabilities would likely improve reasoning scores further, but our focus is on interaction fidelity, not task-specific score optimization. This suggests that the streaming benefits do not significantly compromise reasoning accuracy. Critically, the high Process Fidelity scores validate that real-time explanations faithfully represent backend reasoning processes, addressing essential requirements for trustworthy human-AI collaboration. TABLE I: Objective evaluation results summary Scenario Math Solver Travel Planner Deep Research Metric TTFA (s) Score TTFA (s) Score TTFA (s) Score Monolithic Explainer-Only AsyncVoice 9.480 96.36 26.907 96.40 27.184 81. 4.618 90.60 12.089 96.70 13.980 72.60 N/A 0.015 92.20 0.015 91. 0.014 79.50 4.73 All Scenarios Process Fidelity N/A D. Discussion and Limitations The performance gap versus monolithic baselines results from our one-way streaming design where explainabilityoptimized outputs and token limits constrain reasoning depth. We consider this an acceptable trade-off for achieving 6001800 latency reduction. Our automated evaluation ensures reproducibility (using TTS-generated queries for audio consistency), while the system fully supports natural voice input with sub-100ms interruption handling and configurable explanation styles. Future work will propagate user feedback to upstream MCP servers, enabling human-in-the-loop refinement that could surpass static approaches. Current limitations include TTS prosody and unidirectional reasoning flow; however, these represent engineering challenges rather than fundamental architectural barriers. IV. CONCLUSION We introduced AsyncVoice Agent, the first system to enable real-time, interruptible dialogue with an LLMs live reasoning stream. By decoupling the reasoning backend from the explanation interface through asynchronous architecture, we transform passive CoT consumption into active collaboration. This work establishes new paradigm for human-AI interaction in complex reasoning tasks, demonstrating that sub-second responsiveness fundamentally changes how users engage with AI reasoning processes. We believe this approach will inspire further research into interactive AI systems where human expertise guides model computation in real-time."
        }
    ],
    "affiliations": [
        "Adobe Research, San Jose, CA, USA",
        "Duke University, Durham, NC, USA"
    ]
}