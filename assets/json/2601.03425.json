{
    "paper_title": "The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models",
    "authors": [
        "Yan Wang",
        "Yitao Xu",
        "Nanhan Shen",
        "Jinyan Su",
        "Jimin Huang",
        "Zining Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, a post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover a domain-invariant Standing Committee. This is a compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal a strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. This inherent bias also indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the model's natural optimization path, thereby limiting training efficiency and performance."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 5 2 4 3 0 . 1 0 6 2 : r The Illusion of Specialization: Unveiling the Domain-Invariant \"Standing Committee\" in Mixture-of-Experts Models Yan Wang1, Yitao Xu2, Nanhan Shen2, Jinyan Su3, Jimin Huang1, Zining Zhu4*, 1The Fin AI, 2Georgia Institute of Technology, 3Cornell University, 4Stevens Institute of Technology Correspondence: zzhu41@stevens.edu"
        },
        {
            "title": "Abstract",
            "content": "Mixture of Experts models are widely assumed to achieve domain specialization through sparse routing. In this work, we question this assumption by introducing COMMITTEEAUDIT, post hoc framework that analyzes routing behavior at the level of expert groups rather than individual experts. Across three representative models and the MMLU benchmark, we uncover domain invariant Standing Committee. This is compact coalition of routed experts that consistently captures the majority of routing mass across domains, layers, and routing budgets, even when architectures already include shared experts. Qualitative analysis further shows that Standing Committees anchor reasoning structure and syntax, while peripheral experts handle domain-specific knowledge. These findings reveal strong structural bias toward centralized computation, suggesting that specialization in Mixture of Experts models is far less pervasive than commonly believed. Crucially, this inherent bias indicates that current training objectives, such as load-balancing losses that enforce uniform expert utilization, may be working against the models natural optimization path, thereby limiting training efficiency and performance."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning and understanding tasks (Qian et al., 2025; Wang et al., 2025a,b,c). To further scale these models without incurring proportional computational costs, the Mixture-of-Experts (MoE) architecture has emerged as dominant approach. By activating only sparse subset of parameters for each input token, MoE models promise to decouple model capacity from inference latency. This conditional computation paradigm is particularly appealing for general-purpose LLMs because different domains * Corresponding author 1 (a) Conventional domainspecific intuition. (b) Empirically observed expert sharing. Figure 1: From domain-specific intuition to empirically observed expert sharing in Mixture-of-Experts models. (a) The Intuition: The ideal Divide-and-Conquer strategy assumes disjoint sets of experts for different domains. (b) The Observation: Empirical routing patterns reveal Standing Committee (e.g., Experts E4 and E5) that is consistently activated across disparate domains (Math, Legal, Biology), acting as generalist core hidden within the routed experts. exhibit heterogeneous computational patterns that theoretically benefit from expert specialization. The design philosophy of MoE models often follows divide-and-conquer principle where experts are expected to specialize by domain. Under this view, sparsity arises when the model routes inputs to distinct expert groups, such as dedicated set for mathematics and another for legal tasks, as illustrated in Figure 1a. However, the optimization dynamics of sparse routing frequently contradict this ideal separation. Prior research on Representation Collapse (Chi et al., 2022; Do et al., 2025) warns of pathological state where gating networks fail to optimize effectively, causing experts to become redundant or completely inactive. Recognizing that natural language relies heavily on high-frequency and domain-agnostic patterns, recent state-of-theart (SOTA) architectures have moved to institutionalize centralized processing unit. For instance, DeepSeek (Dai et al., 2024b; DeepSeek-AI et al., 2024, 2025) introduces Shared Experts that are always activated to isolate common knowledge from the routed experts. The prevailing assumption is that by architecturally separating these generalists, the remaining routed experts are free to become Table 1: Comparison with existing MoE interpretability works. While prior studies focus on individual expert specialization, internal representations, or frequencybased importance, our work uniquely identifies stable, domain-invariant expert committees. Research Focus Representative Works Routing & Behavior Analysis Closer Look into MoE (Lo et al., 2025) Probing Semantic Routing (Olson et al., 2025) Context Faithfulness (Bai et al., 2025) Unit of Analysis Individual Individual Individual Focus: Which individual expert activates for specific token? Dominant Assumption Captures Co-activation General Polysemy Domain-Specific Context-Dependent Representation & Intrinsic Mechanisms Secretly Embedding Model (Li and Zhou, 2024) Global/Layer MoE-X (Yang et al., 2025b) Intrinsic User-Centric (Swamy et al., 2024) Individual Individual Latent Space Modular User-Aligned Criticality Analysis Focus: What information is encoded within experts? Unveiling Super Experts (Su et al., 2025) Individual Pareto Principle Focus: Which single experts are dominant/critical? Structural Organization Standing Committee Analysis (Ours) Focus: How stable groups of experts dominate computation across domains? Committee Domain-Invariant true specialists. Yet, our empirical analysis reveals that this architectural fix does not fully suppress the drive toward centralization. As shown in Figure 1b, we observe substantial cross-domain sharing even among the experts explicitly designated for specialization. Unlike representation collapse, where experts die out due to optimization failure, these shared experts are highly active and functionally competent but simply refuse to specialize. This evidence suggests that the formation of generalist core is not merely an architectural choice but an inevitable emergent property of sparse computation. However, this emergent structural bias poses significant challenge to conventional MoE training: standard load-balancing auxiliary losses, which are designed to prevent expert idle-out by encouraging uniform selection, may inadvertently suppress this natural computational hierarchy. This conflict potentially leads to suboptimal convergence and wasted FLOPs on peripheral experts that lack fundamental reasoning capabilities. Recent advances in MoE interpretability have largely focused on the properties of individual experts. Prior work has examined semantic routing patterns (Olson et al., 2025; Lo et al., 2025; Bai et al., 2025), analyzed internal representations (Li and Zhou, 2024; Yang et al., 2025b), and most recently identified frequency-based Super Experts (Su et al., 2025). However, these studies predominantly treat experts as independent computational units, where importance is quantified through isolated activation statistics. Consequently, they overlook the potential for higher-level structural organization within the routing mechanism. While the Super Expert phenomenon highlights the Pareto distribution of individual criticality, it fails to capture the relational stability between experts across varying contexts. This leaves critical gap in 2 our understanding: do experts function as isolated specialists whose prominence is mere statistical byproduct, or do they spontaneously organize into stable, domain-invariant coalitions? To resolve this discrepancy between domainspecific intuition and observed expert sharing, we propose COMMITTEEAUDIT, post-hoc analytical framework designed to audit the group-level structural organization of pre-trained MoE models. Unlike prior works that focus on individual expert statistics, our framework quantifies the stability and intersection of expert coalitions across divergent tasks. We apply this auditing process to three representative models (OLMoE (Muennighoff et al., 2025), Qwen3-30B-A3B (Yang et al., 2025a), and DeepSeek-V2-Lite (DeepSeek-AI et al., 2024)), covering both standard routing and architectures with explicit shared experts. Guided by this structural perspective, our study addresses three fundamental questions regarding the hidden organization of MoE computation: Existence: Do routed experts naturally self-organize into stable, domain-invariant groups that dominate computation, or do they remain specialized by task? Dynamics: How does this group-level organization evolve across network depths? Is the centralization of experts an inevitable emergent property of sparse routing? Functionality: What functional roles do these stable groups play? Specifically, does the model rely on them for general reasoning while relegating specific knowledge to fringe of other experts? Our contributions answer these questions and challenge the prevailing view of MoE specialization: (1) We provide systematic evidence of domaininvariant Standing Committee, compact expert coalition that emerges regardless of shared-expert architectures, revealing structural bias that challenges fairness-oriented load balancing. (2) We introduce model-agnostic framework, COMMITTEEAUDIT, that utilizes Paretooptimal ranking and stability diagnostics to quantify group-level expert organization beyond individual activation statistics. (3) Through qualitative analysis, we uncover core-periphery organization where committee members anchor logical and syntactic structures, while peripheral experts manage domain-specific knowledge."
        },
        {
            "title": "2 Related Works",
            "content": "Expert Specialization and Routing Analysis MoE models are commonly motivated by divideand-conquer intuition: sparsity arises when tokens are routed to domain-specialized experts (Xue et al., 2024; Jiang et al., 2024; Zoph et al., 2022; Dai et al., 2024a; Fan et al., 2024). Early multilingual studies supported this view, reporting experts that preferentially served specific languages (Lepikhin et al., 2020a; Zheng et al., 2024). However, recent analyses of general-purpose LLMs reveal more nuanced picture. Experts often behave polysemously rather than strictly specializing (Lo et al., 2025), and routing only weakly aligns with human semantic domains (Olson et al., 2025). Other work shows that specialization is modulated by context rather than being an intrinsic property of an expert (Bai et al., 2025). In parallel, studies on super experts highlight small set of disproportionately active experts (Su et al., 2025), shifting attention from specialization to expert criticality. Internal Representations and Intrinsic Interpretability complementary line of work examines the internal representations of MoE systems. Evidence suggests that experts contribute to shared latent space rather than operating as isolated modules (Li and Zhou, 2024). To improve interpretability, architectural interventions have been proposed, including constraints that encourage interpretable expert roles (Yang et al., 2025b) and routing mechanisms designed to align usage with higher-level semantic concepts (Swamy et al., 2024). From Individual Experts to Collective Structure Despite these advances, most prior studies analyze experts as independent computational units, focusing on activation patterns or internal states (Ghandeharioun et al., 2024). What remains unclear is whether experts organize into stable, co-activated groups that persist across tasks. Our work addresses this gap by shifting the lens from individual experts to structured collectives, standing committees, and shows that such committees emerge in domain-invariant manner, challenging the conventional assumption of purely domain-specific routing."
        },
        {
            "title": "3 COMMITTEEAUDIT",
            "content": "3.1 Preliminaries Mixture-of-Experts Architecture. Mixture-ofExperts (MoE) models extend the Transformer by replacing the feed-forward network with set of parallel experts {Ei}E i=1 (Shazeer et al., 2017; Lepikhin et al., 2020b; Fedus et al., 2022). For token Rd at layer ℓ, gating network produces routing vector G(ℓ)(x). Under Top-k routing, the layer output is the weighted sum of activated experts: (cid:88) = G(ℓ)(x)iEi(x). (1) iTop-k While token-level routing is sparse, aggregating decisions over corpus reveals structural regularities. Expert Contribution Index (ECI). To quantify expert importance at the domain task level, we define the Expert Contribution Index (ECI). Given corpus partitioned into domain tasks = {τ }, we denote Dτ as the subset for domain task τ . For expert at layer ℓ, the ECI is the expected routing weight: c(ℓ) i,τ = ExDτ (cid:104) G(ℓ)(x)i (cid:105) (2) Unlike activation frequency, ECI preserves the magnitude of router preference, providing more informative signal for ranking. ECI serves as the building block for analyzing cross-task invariants. 3.2 Framework Description COMMITTEEAUDIT, as shown in Figure 2, is domain-conditioned routing analysis framework that (i) extracts domain-level routing profiles, (ii) quantifies inter-domain routing divergence, and (iii) explores Standing Committees. In high-capacity MoEs (E 64), while single experts may occasionally dominate, activation is generally too distributed for individual-centric analysis. We hypothesize that specialization is expressed through structured distribution over subset of experts, referred to as committee. Stage I: Task-conditioned routing profiles. Before constructing committees, we first extract routing representations from the MoE model. For every sample Dτ and MoE layer ℓ, we run the model and record the full routing vector G(ℓ)(x) taken at the last token unless otherwise specified: G(ℓ)(x) = softmax(z(ℓ)(x)) E1, (3) where E1 denotes the probability simplex over experts, that is E1 = {p RE : pi 0, Σipi = 1}. 3 Stage III: Standing Committee exploration. We finally identify the domain-invariant backbone for well-structured tasks. Given c(ℓ) i,τ computed in Stage I, which corresponds to the average routing weight assigned to expert when processing samples from domain task τ . Experts are then ranked within each domain task, and we denote the resulting rank by R(i, τ )(ℓ), (8) assigning penalty rank k+1 to experts that do not appear in the Top-k. Here, corresponds to the models routing sparsity. To distinguish experts that are globally useful from those that are merely task-specific, we measure how often each expert appears among the Topk across domain tasks: (ℓ) = 1 (cid:88) (cid:16) τ R(i, τ )(ℓ) (cid:17) . (9) Experts that occur sufficiently frequently form the set of consensus candidates E: (cid:110) (ℓ) = : (ℓ) γ (cid:111) , (10) where γ controls the threshold for cross-domain agreement. To ensure committee members are active in nearly all observed domain tasks, in this paper, we use γ > 0.8 to set up our experiment. For each candidate expert, we further characterize stability across domain tasks: R(i, τ )(ℓ)(cid:105) , R(i, τ )(ℓ)(cid:105) (cid:104) µ(ℓ) = Eτ σ(ℓ) = Varτ (cid:104) (11) . Here, µ(ℓ) on average, while σ(ℓ) it holds that position. captures how highly the expert is ranked quantifies how consistently Finally, the Standing Committee at layer ℓ is defined as the Pareto-optimal set: (cid:16) C(ℓ) = Pareto {(µ(ℓ) , σ(ℓ) )}iE (ℓ) (cid:17) , (12) Figure 2: Overview of the COMMITTEEAUDIT framework. We use the full routing distribution (rather than discrete Top-k activations) because it preserves the complete preference structure over experts. the We then aggregate routing behavior at domain-task level. For each expert i, we compute its ECI c(ℓ) i,τ used Eq (2) and collect all expert contributions into task-conditioned profile Gℓ,τ = [c(ℓ) 1,τ , . . . , c(ℓ) i,τ , . . . , c(ℓ) E,τ ], (4) This profile serves as the building block for both task-specificity analysis (Stage II) and expert contribution estimation (Stage III). Stage II: Quantifying task-specificity. We next assess the degree of routing specialization per domain task τ using silhouette-based score. Stage II determines whether tasks routing is sufficiently distinctive. Let d(, ) denote the cosine distance between routing vectors (as defined in Eq (3)). For each xi Dτ , define ai = 1 Dτ 1 (cid:88) (cid:16) G(ℓ)(xi), G(ℓ)(xj) (cid:17) , xj Dτ j=i (cid:88) bi = min τ =τ 1 Dτ (cid:16) (cid:17) G(ℓ)(xi), G(ℓ)(xj) , xj Dτ (5) and compute the sample-level silhouette si = bi ai max(ai, bi) [1, 1]. (6) The task-specificity score is the mean silhouette: Sℓ(τ ) = 1 Dτ (cid:88) si. xiDτ (7) This selection favors experts which achieve favorable trade-off between high average rank and low cross-domain variability. where high Sℓ(τ ) indicates that routing vectors within domain task τ form compact and clearly separated cluster. Only tasks with sufficiently high Sℓ(τ ) proceed to Standing Committee construction in Stage III. 3.3 Experimental Setup 3.3.1 Dataset We evaluate COMMITTEEAUDIT on the Massive Multitask Language Understanding (MMLU) Table 2: Aggregation of MMLU subjects into nine domain tasks. Domain Representative Subjects STEMMath STEMPhysics STEMChemistry STEMBioMed CSEng SocSci Humanities LangLing BizLaw algebra, geometry, probability, statistics, college/high-school mathematics high-school/college physics, astronomy, conceptual physics high-school and college chemistry biology, anatomy, genetics, clinical knowledge, virology, nutrition, aging, medicine computer science, security, operating systems, ML, electrical engineering economics, econometrics, sociology, psychology, political science, public relations history, philosophy, ethics, religion, art history, world facts English, literature, linguistics business, management, accounting, marketing, law Table 3: MoE configurations of evaluated models. Model Experts (E) Top-k Shared Size DeepSeek-V2-Lite Qwen3-30B-A3B OLMoE-1B-7B 64 128 64 6 8 8 2 0 0 16B 30B 7B benchmark (Hendrycks et al., 2021), which contains 57 multiplechoice subjects spanning science, humanities, social science, and professional domains. To study domain-conditioned routing rather than per-subject idiosyncrasies, we reorganize all subjects into nine semantically coherent domains  (Table 2)  : Formally, each subject is mapped to domain task τ , yielding domain-specific subsets {Dτ }. All routing analyses in this paper, including taskspecificity and Standing Committee extraction, are conducted at the domain level. 3.3.2 Model We evaluate COMMITTEEAUDIT on three representative MoE language models that differ in expertpool size and routing configuration. All models are used in inference-only mode, and we extract routing weights from every MoE layer for analysis. shown in Table al., 2024) 3, DeepSeek-V2As et includes Lite (DeepSeek-AI two shared experts that are always active, forming centralized processing path, whereas Qwen3-30B-A3B (Yang et al., 2025a) and OLMoE-1B-7B (Muennighoff et al., 2025) rely purely on routed experts. The variation in (E, k) and shared-expert usage allows us to probe whether Standing Committees are an architectural artifact or persistent phenomenon across MoE designs (details are in Appendix A). 3.4 Metrics 3.4.1 Jaccard Similarity (Cross-Domain Expert Sharing) To quantify how much different domains reuse the same experts, we compute the Jaccard similarity between domain-level Top-k expert sets. For layer ℓ and domains τ1, τ2, let Eℓ,τ denote the Top-k experts; then Jaccardℓ(τ1, τ2) = Eℓ,τ1 Eℓ,τ2 Eℓ,τ1 Eℓ,τ2 . (13) Values near 0 indicate domain-specific routing, whereas larger values reflect substantial crossdomain expert sharing. 3.4.2 Gini Coefficient (Expert Concentration) To quantify the inequality of expert contributions, we compute the Gini coefficient over the distribution of the Expert Contribution Index (ECI) at each layer ℓ. Let c(ℓ) = (c(ℓ) ) denote the global contribution vector, where c(ℓ) = Eτ [c(ℓ) i,τ ] represents the average ECI for expert across all tasks. The Gini coefficient is defined as: 1 , . . . , c(ℓ) Gini(c(ℓ)) = (cid:80)E i=1 c(ℓ) (cid:80)E j=1 c(ℓ) i=1 c(ℓ) 2E (cid:80)E . (14) In this context, Gini coefficient approaching 0 indicates uniform utilization of experts, where each expert provides an equal contribution to the models computation. Conversely, high Gini coefficient (approaching 1) signals extreme contribution inequality, where the total routing mass is monopolized by small subset of experts, providing macroscopic evidence for the existence of standing committee."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we present series of experiments to address the three questions introduced in Section 1. These experiments allow us to determine whether standing committees actually emerge, how they evolve across depth, and what functional role they play in model behavior. 4.1 Existence and Stability: The Standing Committee Phenomenon Question 1: Do routed experts converge into stable, domain-invariant groups? 5 Table 4: Cross-domain sharing (Jaccard) and expert concentration (Gini) across models. Metric Statistic OLMoE DeepSeek-V2-Lite Qwen3-30B-A3B Jaccard Similarity Gini Coefficient Max Min Overall Max Min Overall 1.0000 0.7963 0.8735 0.9082 0.8814 0.8957 1.0000 0.7103 0.8670 0.9360 0.9092 0.9207 1.0000 0.5300 0.8670 0.9605 0.9405 0. 4.1.1 JaccardGini Analysis of Expert Sharing and Concentration Table 4 evaluates the standing-committee hypothesis from two complementary perspectives: (i) whether the same experts tend to reappear across domains, and (ii) how unevenly routing mass is distributed among them. Despite substantial differences in expert capacity (E) and routing sparsity (k), all three models display high overlap and concentration, indicating that MoE routing tends to self-organize into standing committees rather than task-specific specialization. Membership Stability (Jaccard Similarity). The Jaccard index captures whether the same experts repeatedly appear among the top-k routed set across domains. OLMoE (E = 64, = 8) achieves the highest mean overlap (0.8735) and the strongest minimum stability (0.7963), suggesting that the model frequently reuses common subset of experts. Qwen3 (E = 128, = 8) shows greater local variability (Min: 0.5300), yet its high global average (0.8670) indicates that such deviations occur on top of largely stable routing structure rather than replacing it entirely. Contribution Concentration (Gini Coefficient). The Gini coefficient quantifies the inequality of ECI across the expert population. All models exhibit extreme values (> 0.88), meaning that small fraction of experts absorbs most of the routing mass. Interestingly, concentration correlates with expert capacity: Qwen3 (E = 128) attains the highest overall Gini (0.9465). Rather than distributing computation more broadly, larger pools appear to amplify the dominance of compact set of frequently selected experts. Figure 3 links these statistics to routing behavior. In Panel (a), the mean normalized weight assigned to the routed top-k experts remains both high and stable across layers. If experts were mainly taskspecialized, different domains would activate different experts, and the variance bands would widen. Instead, we observe persistent dominance by the (a) Standing Committee Stability Across Layers. (b) OLMoE expert concentration. (c) DeepSeek expert concentration. (d) QWen expert concentration. Figure 3: Evidence of standing committees in MoE models. (a) Layer-wise concentration of top-k experts across tasks. For each model, the solid line shows the mean normalized weight assigned to the top-k experts at each layer, and the shaded region denotes one standard deviation. High and stable values indicate small subset of experts (standing committees) consistently absorbs most routing mass. (bd) Lorenz curves reveal that only small subset of experts accounts for most contributions (other Lorenz curves are shown in Appendix C), showing that these committees are highly centralized rather than uniformly shared. same routed subset, indicating domain-invariant backbone. Panels (bd) arrive at the same conclusion from distributional view: Lorenz curves show that only tiny fraction of experts accounts for most ECI, confirming strongly centralized allocation of computation. Together, the two views support the Standing Committee hypothesis: MoE models concentrate routing mass onto small, persistent core, while most experts operate only peripherally. 4.1.2 Analyzing the Stability and Contribution of Standing Committees Table 5 summarizes representative standing committees based on the Pareto-optimal set across depth. Committee members consistently occupy very high routing positions (Avg. µ 3.13.8) with low rank variability (σ2 3.44). For example, the middle-layer committee in OLMoE exhibits variance of only 0.49, indicating that these experts remain near the top of the routing hierarchy regardless of domain. Rather than transient specialists, they function as de-facto backbone that the model repeatedly relies on. Although remains small (2-5) across all modTable 5: An audit of Standing Committees (C) across network phases. The details are shown in Appendix B. Avg. µ and Avg. σ2 represent the mean and the variance of ranks of the committee members across domains. ECI Cov. is the cumulative contribution, and Ratio indicates the influence density vs. uniform baseline. Model Phase Layer (ℓ) Committee Members (C) Avg. µ Avg. σ2 ECI Cov. Ratio () DeepSeek-V2-Lite OLMoE Qwen3-30B-A3B Shallow Middle Deep Shallow Middle Deep Shallow Middle Deep 3 11 2 8 16 3 33 46 {7, 13, 22, 42} {28, 43, 54} {4, 14, 47, 61} {30, 58, 63} {13, 45} {17, 52, 60} {38, 40, 80, 93} {16, 26, 57, 116, 121} {94, 101, 107} 4 3 3 2 3 4 5 3 3.36 3.15 3.11 3.41 3.28 3.19 3.61 3.82 3.15 1.81 1.98 0. 2.15 0.49 1.52 3.44 2.16 1.59 66.3% 60.7% 70.5% 43.9% 29.7% 44.0% 54.0% 67.0% 50.9% 29.5 31.4 35.8 15.9 13.1 16.0 36.4 49.9 43.3 (a) Committee coverage across layers and routing budgets k. Figure 4: Cross-layer stability of routed experts across models, measured by Jaccard similarity between topk expert sets over domains. All three MoE models maintain high overlap ( 0.8 for most layers), showing that the same experts are repeatedly selected despite changes in input domain and network depth. els, these groups capture up to 70.5% of total routing mass. Importantly, the size of remains stable even as capacity increases from = 64 to = 128. This suggests that committee-like behavior is not an artifact of particular architecture, but an emergent pattern of sparse routing optimization. 4.2 Variation and Sensitivity: Structural Dynamics Question 2: How does group structure evolve with depth, and is centralization inevitable under sparse routing? 4.2.1 Analyzing the robustness of this committee We begin by examining whether routed experts actually form persistent groups. Figure 4 reports the Jaccard similarity of top-k expert sets across domains for each layer. Across all three MoE models, the similarity remains consistently high (often 0.85), showing that the same experts are repeatedly activated across tasks and depths. Rather than rotating specialists, the routing network converges to shared backbone of experts that is largely invariant to both input domain and layer position. Having established the existence of persistent backbone, we next ask how it evolves with depth 7 (b) Committee size across layers and routing budgets k. Figure 5: Dynamics of standing committees in OLMoE under different routing budgets. We show all 16 layers. (a) Relative contribution of committee members remains high and does not vanish when increases. (b) The size of the identified committees stays small and changes only mildly with depth and k, indicating compact but persistent core of experts. and routing sparsity. Figure 5 analyzes OLMoE under different routing budgets k. In Figure 5(a), committee coverage remains high across layers, and increasing does not reduce concentration. Additional capacity mainly introduces low-coverage experts, while small subset continues to absorb most of the routing mass. Figure 5(b) shows that the committee size itself is small (typically 14 experts) and changes only mildly with depth. Thus, MoE models do not expand the committee as depth grows; they instead rely on compact, persistent core whose influence is largely insensitive to k. Together with Figure 4, these findings indicate that centralization is not accidental but an emergent structural property of sparse routing. 4.2.2 Top-k Sensitivity Sweep To probe how sensitive the standing committee is to the routing budget, we perform top-k sensitivity sweep on OLMoE. For each setting of {4, 6, 8, 12, 16}, we re-identify the standing committee based on the Pareto-optimal set and compute the retention rate with respect to the reference committee at k=8, i.e., the fraction of k=8 (a) OLMoE: functional tokens repeatedly map to fixed subset of committee experts. (b) Qwen3-30B-A3B shows similar convergence despite larger expert capacity. Figure 7: Case study of token-level routing. Rows denote functional tokens and columns denote members of the Standing Committee. cell is marked when token reliably activates an expert across domains. Anchor 2: Domain-invariant syntactic backbone. High-frequency structural tokens, including the, a, and in, also converge to overlapping committee members across domains, indicating that the committee maintains stable syntactic layer independent of content. Peripheral specialization. By contrast, domainspecific terminology rarely stabilizes: chemical symbols, biomedical identifiers, and financial jargon are distributed across many experts depending on context. This pattern supports core-periphery organization in which the committee anchors reasoning and syntax, while peripheral experts are recruited on demand for specialized knowledge. Taken together, Standing Committees function as domain-invariant control layer, coordinating logical structure and grammar while delegating domain knowledge to peripheral experts."
        },
        {
            "title": "5 Conclusion",
            "content": "This work introduces COMMITTEEAUDIT, showing that MoE models rely on domaininvariant Standing Committee that anchors reasoning and syntax, while peripheral experts handle domain knowledge. Our cross-model analysis indicates that this centralized computation emerges from sparse routing itself, rather than from architectural design choices. The resulting structural bias highlights tension with current training objectives: load-balancing losses that push toward uniform expert usage may counter the models natural optimization behavior. These results motivate function-aware routing and architectures that explicitly support coreperiphery organization of expertise. Figure 6: Top-k sensitivity sweep for OLMoE. We take the standing committee identified at = 8 as the reference core and measure, for each routing budget k, the fraction of its members that are still present in the new committee (averaged over layers). The dashed line marks the = 8 baseline (retention = 1.0). core members that remain in the new committee. Figure 6 shows that retention peaks at the nominal configuration k=8 and drops on both sides: it falls to 0.39 at k=6, 0.17 at k=4, and below 0.3 once is expanded to 12 or 16. This pattern suggests that the core experts are not an artifact of single choice, but they are also not completely rigid. When is too small, the gate is forced to exclude part of the original core; when is too large, the gate dilutes its attention and recruits additional experts, replacing some core members. Overall, the sweep indicates that sparse routing induces centralized committee around k=8, but that committee can partially reorganize as the routing budget becomes substantially more aggressive or more constrained. 4. Interaction and Behavior: Functional Interpretation Question 3: What roles do stable expert groups play in reasoning versus domain knowledge? We illustrate the functional roles of Standing Committees using qualitative case study. Figure 7 shows an activation matrix between committee experts (columns) and functional tokens (rows). cell is marked when token repeatedly activates an expert in at least three domains. Two consistent behaviors emerge. Anchor 1: Logical framing and reasoning control. Across OLMoE and Qwen3-30B-A3B, abstract reasoning triggers such as Which, What, Suppose, and question marks are routed to the same subset of committee experts. These tokens define the logical scaffolding of the prompt, suggesting that the committee acts as reasoning controller."
        },
        {
            "title": "Limitations",
            "content": "This work introduces COMMITTEEAUDIT and reports evidence for domain-invariant Standing Committee in Mixture-of-Experts models. However, several limitations remain. First, our analysis covers only small set of representative MoE architectures and settings, and does not span hybrid, hierarchical, or dynamically adaptive routing designs. Whether similar organizational patterns persist in broader systems remains an open question. Second, our study is observational and inferenceonly. We do not directly intervene in routing or measure causal effects of modifying committee members. Future work should incorporate targeted ablations and routing perturbations. Third, our evaluation primarily relies on domainlevel analyses over MMLU. While this reduces subject-level noise, it may not fully capture behavior in conversational, multi-step reasoning, coding, or tool-augmented scenarios. Finally, COMMITTEEAUDIT focuses on routing statistics rather than training dynamics. Understanding when and how Standing Committees emerge during optimization remains an important direction for future work."
        },
        {
            "title": "Potential Risks",
            "content": "Potential Positive Impacts. By revealing grouplevel routing structure, this work may inform more transparent and efficient MoE design, support diagnosis of routing failures, and encourage principled interpretability research for sparse models. Potential Negative Impacts. However, several risks remain. First, insights into centralized computation could be misinterpreted as evidence of inherent safety or robustness, leading to overreliance in deployment. Second, identifying persistent expert coalitions may enable adversarial targeting of critical routing pathways. Third, benchmarks may become over-optimized toward interpretability metrics without improving real-world safety. These findings should therefore be used as analytical tools rather than deployment guarantees."
        },
        {
            "title": "Ethical Considerations and Usage\nDisclaimer",
            "content": "purposes only and does not constitute guidance for production deployment. The framework exposes structural properties of MoE systems, but does not certify fairness, safety, robustness, or regulatory compliance. The authors make no warranties regarding completeness or suitability for downstream use, and any application to high-stakes settings should involve domain experts, risk assessment, and human oversight. We acknowledge that AI-assisted tools were used during writing and editing (e.g., grammar checking, phrasing refinement, and formatting suggestions). These tools were not used to generate research ideas, experimental results, model outputs, or claims, and all technical content, analyses, and conclusions were designed, verified, and interpreted by the authors."
        },
        {
            "title": "Licenses and Terms of Use",
            "content": "All datasets and pre-trained models used in this work are publicly available and redistributed under their respective licenses. We respect the original terms of use for each artifact. MMLU is used under its public research license, and all evaluated MoE models (OLMoE, Qwen3-30B-A3B, and DeepSeek-V2-Lite) are accessed and used in compliance with their published licenses. We do not redistribute any third-party artifacts."
        },
        {
            "title": "References",
            "content": "Jun Bai, Minghao Tong, Yang Liu, Zixia Jia, and Zilong Zheng. 2025. Understanding and leveraging the expert specialization of context faithfulness in mixture-of-experts llms. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2193821953. Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, and 1 others. 2022. On the representation collapse of sparse mixture of experts. Advances in Neural Information Processing Systems, 35:3460034613. Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024a. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. Preprint, arXiv:2401.06066. All experiments use publicly available models and datasets without personal or sensitive information. This work is intended for academic and educational Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, and 1 others. 2024b. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066. DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, and 138 others. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. Preprint, arXiv:2405.04434. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Giang Do, Hung Le, and Truyen Tran. 2025. Simsmoe: Toward efficient training mixture of experts via solving representational collapse. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 20122025. Dongyang Fan, Bettina Messmer, and Martin Jaggi. 2024. Towards an empirical understanding of moe design choices. Preprint, arXiv:2402.13089. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. 2024. Patchscopes: unifying framework for inspecting hidden representations of language models. arXiv preprint arXiv:2401.06102. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 otharXiv preprint ers. 2024. Mixtral of experts. arXiv:2401.04088. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020a. Gshard: Scaling giant models with conditional Preprint, computation and automatic sharding. arXiv:2006.16668. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020b. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668. Ziyue Li and Tianyi Zhou. 2024. Your mixture-ofexperts llm is secretly an embedding model for free. arXiv preprint arXiv:2410.10814. Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, and Jie Fu. 2025. closer look into mixture-of-experts in large language models. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 44274447. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, and 5 others. 2025. Olmoe: Open mixture-of-experts language models. Preprint, arXiv:2409.02060. Matthew Lyle Olson, Neale Ratzlaff, Musashi Hinck, Man Luo, Sungduk Yu, Chendi Xue, and Vasudev Lal. 2025. Probing semantic routing in large mixtureof-expert models. arXiv preprint arXiv:2502.10928. Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Han Yi, Yilun Zhao, Jimin Huang, Qianqian Xie, and Jian yun Nie. 2025. Fino1: On the transferability of reasoning-enhanced llms and reinforcement learning to finance. Preprint, arXiv:2502.08127. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Zunhai Su, Qingyuan Li, Hao Zhang, Weihao Ye, Qibo Xue, YuLei Qian, Yuchen Xie, Ngai Wong, and Kehong Yuan. 2025. Unveiling super experts in mixture-of-experts large language models. Preprint, arXiv:2507.23279. Vinitra Swamy, Syrielle Montariol, Julian Blackwell, Jibril Frej, Martin Jaggi, and Tanja Käser. 2024. Intrinsic user-centric interpretability through global mixture of experts. arXiv preprint arXiv:2402.02933. Yan Wang, Yueru He, Ruoyu Xiang, and Jeff Zhao. 2025a. Rkefino1: regulation knowledge-enhanced large language model. In 2025 IEEE 11th International Conference on Intelligent Data and Security (IDS), pages 4951. IEEE. Yan Wang, Yang Ren, Lingfei Qian, Xueqing Peng, Keyi Wang, Yi Han, Dongji Feng, Fengran Mo, Shengyuan Lin, Qinchuan Zhang, Kaiwen He, Chenri Luo, Jianxing Chen, Junwei Wu, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, Qianqian Xie, and JianYun Nie. 2025b. Fintagging: Benchmarking llms for extracting and structuring financial information. Preprint, arXiv:2505.20650. Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, and Jian-Yun Nie. 2025c. Finauditing: financial 10 taxonomy-structured multi-document benchmark for evaluating llms. Preprint, arXiv:2510.08886. Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open 2024. mixture-of-experts language models. arXiv preprint arXiv:2402.01739. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025a. Qwen3 technical report. Preprint, arXiv:2505.09388. Xingyi Yang, Constantin Venhoff, Ashkan Khakzar, Christian Schroeder de Witt, Puneet Dokania, Adel Bibi, and Philip Torr. 2025b. Mixture of experts made intrinsically interpretable. arXiv preprint arXiv:2503.07639. Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, and Benyou Wang. 2024. Efficiently democratizing medical llms for 50 languages via mixture of language family experts. arXiv preprint arXiv:2410.10626. Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906."
        },
        {
            "title": "Infrastructure",
            "content": "All experiments are implemented in Python using PyTorch and the HuggingFace Transformers library, and are conducted in inference-only mode. We evaluate each model on the full MMLU benchmark using two NVIDIA A100 (80 GB) GPUs. OLMoE-1B-7B, DeepSeek-V2-Lite, and Qwen330B-A3B contain approximately 7B, 16B, and 30B parameters, respectively. The total computational cost of the routing analyses is about 40 GPU-hours, including forward passes and the collection of routing statistics."
        },
        {
            "title": "B Standing Committees across all layers",
            "content": "Full-layer analysis of standing committees. To move beyond representative snapshots, we perform layer-by-layer audit of all identified standing committees in OLMoE, DeepSeek-V2-Lite, and Qwen3-30B-A3B, as shown in Table 6, 7, and 8. The resulting tables reveal consistent organizational pattern that is obscured when only few layers are examined. Across models, standing committees emerge early, consolidate in the middle layers, and persist into the deepest layers, while their composition changes only gradually. Small, persistent coalitions. Despite large expert pools, the size of each committee remains compact: typically [1, 4] for OLMoE and DeepSeek-V2-Lite, and occasionally up to five members in Qwen. Increasing expert capacity does not diversify routing. Instead, optimization repeatedly converges onto small coalition of experts that are selected across domains and prompts. This suggests that sparse routing does not primarily allocate experts by domain; rather, it reinforces stable computational core. Early centralization. striking finding is that centralization appears already in shallow layers. Even in the first few layers, standing committees capture non-trivial proportion of routing mass (often 2040%). This indicates that MoE models commit to shared processing pathways almost immediately, likely encoding high-frequency patterns such as token normalization, shallow syntactic cues, and generic lexical regularities. Contrary to the intuition that specialization gradually emerges with depth, the router begins consolidating computation from the outset. Table 6: Comprehensive audit of Standing Committees for OLMoE. Layer (ℓ) Committee Members (C) Avg. µ Avg. σ2 ECI Cov. Ratio () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 49 58, 63, 30 60, 14 9, 56 27 53 1 13, 45 27 8, 12 30, 6 33 30, 55 46, 4 1 60, 52, 17 1 3 2 2 1 1 1 2 1 2 2 1 2 2 1 3 1.00 3.41 3.17 1.67 3.11 1.78 1.89 3.28 1.33 3.78 4.28 2.00 2.50 2.22 1.78 3.19 0.00 2.15 1.17 0.33 0.99 1.06 0.54 0.49 0.22 0.49 1.05 2.00 1.95 0.73 0.62 1.52 21.5% 43.9% 30.9% 34.4% 14.5% 16.3% 16.4% 29.7% 17.3% 28.3% 26.8% 17.1% 30.6% 31.3% 16.9% 44.0% 17.25 15.94 13.84 16.24 10.68 12.27 12.38 13.10 13.15 12.27 11.35 12.96 13.69 14.14 12.78 15.99 Middle-layer consolidation. The middle layers display the clearest standing-committee behavior. Committees often grow slightly larger while their rank variance decreases, and their cumulative contribution increases sharply (frequently exceeding 5065% in DeepSeek-V2-Lite and Qwen). These layers appear to implement domain-agnostic abstractions, reasoning templates, discourse structure, and general semantic scaffolding, that are shared across inputs. The router does not allocate different domains to distinct experts; instead, it repeatedly routes through the same committee. Deep-layer bottlenecks. In deep layers, both DeepSeek-V2-Lite and Qwen exhibit strong bottleneck effects: small committee controls 5070% of routing mass, often with high influence density. Rather than distributing final computation across diverse experts, the network funnels decision-making through narrow coalition. This pattern challenges the traditional divide-and-conquer view of MoE systems, suggesting that final reasoning is centralized rather than decomposed. Architectural variability, consistent behavior. Although OLMoE shows weaker committees than Qwen and DeepSeek-V2-Lite, the qualitative trend is remarkably stable across architectures. Even with different routing designs and training recipes, all three models converge toward small, domaininvariant committees that repeatedly dominate computation. Taken together, these findings indicate that standing committees are not an artifact of any particular implementation. Instead, they appear to be an emergent consequence of sparse routing optimization, reflecting strong inductive bias toward centralization in modern MoE language models. 12 Table 7: Comprehensive audit of Standing Committees for DeepSeek-V2-Lite. Table 8: Comprehensive audit of Standing Committees for Qwen3-30B-A3B. Layer (ℓ) Committee Members (C) Avg. µ Avg. σ2 ECI Cov. Ratio () Layer (ℓ) Committee Members (C) Avg. µ Avg. σ2 ECI Cov. Ratio () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 25, 57 19, 51, 46 42, 7, 13, 22 25, 59, 13 38 35, 46 50, 17 45, 46 38, 46, 41 60 54, 43, 28 30 29, 6 5, 28, 33 8, 6, 0 24 40, 25, 31 51, 46, 53, 0 61, 14, 47, 4 44, 7 48 40, 21 23, 6, 38 60, 61 44, 1 36, 56 2 3 4 3 1 2 2 2 3 1 3 1 2 3 3 1 3 4 4 2 1 2 3 2 2 2 3.33 3.00 3.36 2.44 1.22 3.83 3.33 2.33 2.70 1.22 2.59 1.33 2.39 2.37 3.04 1.33 3.89 3.67 3.11 3.33 1.67 3.33 3.30 2.00 1.83 3.17 0.58 1.98 1.81 1.72 0.17 1.00 0.33 1.51 1.43 0.40 0.35 0.44 0.46 1.43 1.22 0.22 0.91 0.65 0.76 0.47 0.67 1.32 0.58 1.06 0.32 0.56 34.8% 52.0% 66.3% 57.3% 22.0% 31.5% 34.5% 37.9% 56.7% 23.6% 60.7% 21.9% 40.6% 58.7% 56.1% 20.4% 44.3% 65.5% 70.5% 34.3% 22.1% 35.4% 52.1% 42.7% 46.3% 34.6% 16.58 22.02 29.46 27.25 17.74 14.25 16.36 18.88 26.66 19.41 31.36 17.63 21.17 28.93 26.00 16.18 16.18 28.43 35.78 16.17 17.85 17.00 22.14 23.08 26.74 16."
        },
        {
            "title": "C Contribution Concentration Analysis",
            "content": "C.1 Lorenz Curve for OLMoE Model The Lorenz curves for OLMoE, as shown in Figure 8, reveal similarly concentrated contribution pattern, despite its smaller scale and more conservative routing design. Across layers, the curves bend sharply away from the equality baseline, with Gini coefficients consistently around 0.880.90. This indicates that only small subset of experts receives the majority of effective routing mass. Even when the nominal expert pool is relatively modest, the allocation of computation remains far from uniform. Layer-wise inspection shows that this concentration is remarkably stable. Early layers, middle layers, and deep layers all display nearly identical Lorenz profiles, suggesting that specialization does not gradually diversify as representations become more abstract. Instead, OLMoE repeatedly falls back on the same compact subset of experts, while the remaining experts contribute minimally. Interestingly, the fraction of used experts typically lies between 12% and 20%, even though the router is free to assign mass more broadly. This implies that sparsity is driven less by necessity and more by the optimization dynamics of the gating network. Rather than distributing computation in balanced way, the router converges toward persistent core of high-traffic experts that dominate inference across inputs. Taken together, the Lorenz curves demonstrate that contribution inequality is not merely byproduct of model scale. Even in OLMoE, contribution is highly centralized, reinforcing the broader Standing Committee pattern: most experts exist on the 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 114 119 40, 93, 80, 38 34, 120, 84 104, 63, 81 68, 1, 37, 66 56, 71, 78 112, 101 26 114, 84 98, 53, 112 60, 125, 7 65, 78, 56 122 20, 90, 17 116 70, 34, 83 61, 31, 77 6, 105, 21 121, 92, 17 93, 106, 63 99, 106 37, 44, 65 121, 86, 36 52, 35, 24 113, 109, 33 31, 123 78, 73, 62 17, 47, 49 116, 65, 40 57, 24, 92 83, 9, 32 57, 121, 16, 26, 116 9, 96, 110, 64 105, 56 63, 23 96 0 20, 48, 86 85, 49 81, 17, 87 55, 21 31, 6, 56 71, 31 90 107, 94, 101 38, 34 1 1 4 3 3 4 3 2 1 2 3 3 3 1 3 1 3 3 3 3 3 2 3 3 3 3 2 3 3 3 3 3 5 4 2 2 1 1 3 2 3 2 3 2 1 3 2 1 2.56 2.78 3.61 3.63 3.22 4.25 3.37 3.67 1.33 4.39 4.19 3.15 3.59 1.78 4.04 3.33 3.48 3.59 3.04 3.89 3.59 2.39 3.78 3.63 3.22 3.81 2.78 3.78 3.74 3.81 3.56 3.63 3.82 3.86 3.11 3.00 2.33 1.11 4.15 2.33 4.30 2.17 3.74 3.33 2.11 3.15 2.39 2.89 0.91 2.17 3.44 3.78 2.49 2.95 4.05 0.77 0.44 0.68 1.28 1.98 1.67 0.62 3.21 1.33 1.74 2.44 1.54 1.83 2.65 1.21 2.71 2.46 2.27 2.13 1.38 2.61 1.87 2.24 2.32 2.49 2.16 2.83 1.53 2.99 0.89 0.10 1.96 1.32 1.07 2.14 2.46 0.84 0.54 1.59 1.21 0.99 14.9% 14.4% 54.0% 40.2% 43.6% 52.6% 41.9% 27.1% 17.6% 26.7% 40.1% 43.3% 43.2% 17.1% 38.9% 20.6% 39.4% 46.7% 41.2% 42.1% 41.9% 27.7% 39.2% 42.4% 39.7% 44.5% 28.4% 41.3% 39.4% 41.7% 41.1% 43.6% 67.0% 54.0% 29.0% 28.7% 14.7% 17.2% 40.0% 30.9% 38.9% 30.6% 41.8% 29.6% 16.1% 50.9% 36.8% 14.2% 22.26 21.40 36.43 28.02 32.16 34.39 30.03 23.36 27.20 22.96 27.89 31.83 31.64 26.25 26.47 24.64 27.05 32.06 30.50 28.01 27.91 25.04 27.69 29.03 27.28 31.03 24.77 28.77 27.54 28.96 29.18 30.14 49.88 36.40 25.75 25.35 21.84 26.40 27.80 28.16 26.51 27.80 29.91 26.53 24.40 43.26 36.65 20.99 periphery, while small, repeatedly selected core absorbs the majority of computational responsibility. C.2 Lorenz Curve for DeepSeek-V2-Lite Model DeepSeek-V2-Lite exhibits an even sharper form of contribution concentration, as shown in Figure 9. Across layers, the Lorenz curves bend aggressively toward the lower-right corner, with Gini coefficients consistently around 0.910.92. This indicates that routing mass is dominated by very small subset of experts. In several layers, fewer than 15% of experts account for nearly all effective contributions, while the remaining experts receive negligible traffic. Unlike what one might expect from lightweight architecture optimized for efficiency, the inequality pattern does not relax as depth increases. Early, middle, and late layers display almost indistinguishable Lorenz shapes. The router repeatedly converges to the same compact set of high-traffic experts, rather than distributing load adaptively as representations evolve. notable pattern is the oscillation in the proportion of used experts. Some layers activate (a) Layer 0 (b) Layer 1 (c) Layer 2 (d) Layer 7 (e) Layer 8 (f) Layer (g) Layer 13 (h) Layer 14 (i) Layer 15 Figure 8: Expert Lorenz Curves across layers for OLMoE model. roughly 20% of the pool, while others rely on as little as 9%. However, even in layers with broader activation, the cumulative share of contribution remains steeply skewed. This suggests that the increase in participation does not meaningfully change who dominates, but merely introduces additional peripheral experts who play marginal roles. Taken together, these curves reinforce the central observation: contribution concentration is not mitigated by architectural simplification. DeepSeekV2-Lite still organizes computation around small, persistent Standing Committee, while most experts remain structurally available but functionally underutilized. C.3 Lorenz Curve for Qwen3 Model As shown in Figure 10, Qwen-30B-A3B shows one of the strongest forms of contribution inequality across all models we study. The Lorenz curves are almost vertical near the right edge, yielding Gini coefficients around 0.94 across layers. This indicates that routing mass is funneled into an extremely small subset of experts. In several layers, fewer than 10% of experts account for nearly all effective contribution, leaving the majority effectively idle. The pattern is also highly consistent across depth. Early layers, mid-depth layers, and late layers exhibit nearly identical Lorenz profiles, suggesting that the model does not gradually diversify expert usage as representations become more abstract. Instead, the router repeatedly returns to the same high-traffic subset, which acts as default computational pathway for most inputs. striking observation is that the proportion of active experts fluctuates between 6% and 12%, yet the inequality curve barely changes. Even when more experts are nominally activated, the cumulative contribution remains concentrated in tiny elite group. Additional experts merely contribute marginal amounts, without altering the dominance structure. These results indicate that contribution centralization intensifies as model capacity increases. In Qwen-30B-A3B, large pool of experts does not translate into broader participation. Rather, the gating dynamics amplify the emergence of persistent Standing Committee, while most experts remain structurally available but functionally peripheral. 14 (a) Layer (b) Layer 3 (c) Layer 6 (d) Layer 9 (e) Layer 12 (f) Layer 15 (g) Layer (h) Layer 21 (i) Layer 24 Figure 9: Expert Lorenz Curves across layers for DeekSeek-V2-Lite model. inated by the same small core. Additional experts tend to act as low-impact auxiliaries rather than genuine participants in computation. Taken together, these results suggest that contribution concentration is not merely an artifact of scale, architecture, or routing hyperparameters. Rather, it reflects robust inductive tendency of sparse MoE optimization. The models converge toward Standing Committee structure, in which persistent core of experts monopolizes computation while most experts operate peripherally. C.4 Cross-Model Synthesis. Across architectures of very different sizes and routing designs, we observe consistent pattern of extreme contribution concentration. OLMoE, DeepSeek-V2-Lite, and Qwen-30B-A3B all display Lorenz curves that deviate sharply from the equality baseline, with Gini coefficients typically above 0.88 and often exceeding 0.94. In every case, only small fraction of experts accounts for the vast majority of effective routing mass, while the remaining experts make negligible contributions. Importantly, this phenomenon persists across depth. Early layers, middle layers, and late layers show nearly identical inequality profiles, indicating that expert participation does not broaden as representations become more abstract. Instead, the gating networks repeatedly allocate computation to compact, stable subset of experts that serve as default processing routes, regardless of layer position or domain. At the same time, fluctuations in the proportion of used experts do not materially change this distribution. Even when more experts are nominally activated, the cumulative contribution remains dom15 (a) Layer 0 (b) Layer (c) Layer 17 (d) Layer 18 (e) Layer 27 (f) Layer 35 (g) Layer 36 (h) Layer (i) Layer 47 Figure 10: Expert Lorenz Curves across layers for QWen3-30B-A3B model."
        }
    ],
    "affiliations": [
        "Cornell University",
        "Georgia Institute of Technology",
        "Stevens Institute of Technology",
        "The Fin AI"
    ]
}