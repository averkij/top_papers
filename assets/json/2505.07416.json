{
    "paper_title": "ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation",
    "authors": [
        "Truc Mai-Thanh Nguyen",
        "Dat Minh Nguyen",
        "Son T. Luu",
        "Kiet Van Nguyen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 6 1 4 7 0 . 5 0 5 2 : r ViMRHP: Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation Truc Mai-Thanh Nguyen1,2, Dat Minh Nguyen1,2, Son T. Luu1,2, and Kiet Van Nguyen1,"
        },
        {
            "title": "1 Faculty of Information Science and Engineering,\nUniversity of Information Technology, Ho Chi Minh City, Vietnam\n2 Vietnam National University, Ho Chi Minh City, Vietnam\n{21522721,21521937}@gm.uit.edu.vn, {sonlt,kietnv}@uit.edu.vn",
            "content": "Abstract. Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90120 seconds/task 2040 seconds/task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at 3."
        },
        {
            "title": "Introduction",
            "content": "Review Helpfulness Prediction (RHP) has become crucial research topic in E-commerce due to its role in assessing the helpfulness of user-generated reviews and their impact on consumer purchasing decisions. However, the large volume of reviews poses significant challenges in identifying helpful reviews. Previous methods primarily relied on semantic features, argument mining, and classification tasks [13], while recent multimodal RHP (MRHP) approaches [47] integrate Corresponding author: kietnv@uit.edu.vn 3 https://github.com/trng28/ViMRHP 2 Truc Nguyen et al. text and images to enable more comprehensive assessment of review helpfulness and enhance predictive performance, focus on ranking reviews based on their helpfulness by using both product information and user-generated reviews. Product Information Set 2 sữa rửa mặt Good morning COSRX độ ph thấp dạng gel chiết xuất trà xanh - 150ml/tuýp Review 1 Helpfulness Score 4.0 2 chai này là chai thứ 3 thứ 4 đó DÙNG MÊ ĐIÊN ĐẢO!!! ẻm không làm khô mặt mình sau khi rửa + dịu nhẹ nên mình hay dùng lắm!! Da mình cũng nhạy cảm nhma dùng tới chai thứ 3 thứ 4 là hiểu đó, MUA LIỀN ĐIII Review 2 Helpfulness Score 3.0 Nhãn ngoài hộp serum nổi bong bóng. Không biết có phải hàng chính hãng không Review 3 Helpfulness Score 2.0 Sữa rửa mặt thơm, rửa mặt sạch, shop giao hàng nhanh, đóng gói hàng cẩn thận Table 1: An illustrative example of the MRHP task and our ViMRHP dataset. Most existing studies focus on English, and research on MRHP in Vietnamese remains limited due to the scarcity of quality annotated datasets. This gap motivates our effort to propose benchmark dataset for MRHP task in Vietnamese, formulated as ranking task following the study of Liu et al. [4], where reviews are ranked based on their helpfulness score, as detailed in Table 1. However, data annotation for high-quality dataset is often time-consuming and costly, requiring meticulous verification. To address this challenge, we leverage Large Language Models (LLMs) to assist in annotation, reducing manual effort while maintaining data quality. Despite their advantages in optimizing time and cost, LLMs still have limitations, requiring human verification to ensure accuracy and consistency. Therefore, we use human-AI collaborative annotation framework that integrates LLMs with human verification. Additionally, we demonstrate the advantages and limitations of LLMs and human annotators in the ViMRHP dataset construction process through our evaluation metrics and experimental results. In summary, our main contributions are three-fold: 1. ViMRHP Dataset. We introduce the ViMRHP dataset, Vietnamese Multimodal Review Helpfulness Prediction dataset. To the best of our knowledge, there are no large-scale datasets for the MRHP task in Vietnamese. ViMHRP via Human-AI Collaborative Annotation 3 2. Human-AI Collaborative Annotation. Annotation process of our ViMRHP dataset implements Human-AI Collaborative Annotation framework via two-step procedure: (1) AI annotation and (2) Human verification and refinement, ensuring time-efficiency, cost, and data quality. 3. Human-Verified versus AI-Annotated Data Quality. We evaluate ViMRHP with baseline models to compare human-verified and AI-annotated data, highlighting differences in quality, consistency, and biases."
        },
        {
            "title": "2.1 Review Helpfulness Prediction",
            "content": "Previous work, Review Helpfulness Prediction (RHP) has explored various tasks and approaches [3]. In the context of text reviews, the impact of structural, lexical, syntactic, semantic, and meta-data features on the helpfulness of usergenerated reviews has been investigated [8], as well as semantic features [1] for predicting review helpfulness using Amazon votes as ground truth and validating findings with the human-annotated label. Argument mining has also been studied separately, with the AM2 [2] dataset focusing on its role in determining the helpfulness of text review. Reviewer expertise and temporal dynamics [9] have also been incorporated to enhance helpfulness prediction with dataset creation from user-generated reviews on TripAdvisor."
        },
        {
            "title": "Datasets",
            "content": "RHP [9] Annotation Method Vote Mapping TripAdvisor Data Source Multimodal Language English Task No. of Reviews No. of Products 161K N/A AM2 [2] Human Amazon English"
        },
        {
            "title": "878\nN/A",
            "content": "Multi-class CLS Argument Mining"
        },
        {
            "title": "Headphones",
            "content": "Amazon-MRHP [4] Lazada-MRHP [4] ViMRHP (Ours) Vote Mapping Amazon English Ranking 1414K 59K Vote Mapping Lazada Indonesian Ranking 287K 21K Clothing, Shoes & Jewelry Clothing, Shoes & Jewelry Electronics Home & Kitchen Electronics Home & Kitchen Human-AI Shopee Vietnamese Ranking 46K 2K Fashion Electronics Home & Lifestyle Health & Beauty Table 2: Comparison of ViMRHP with notable Review Helpfulness Prediction (RHP) datasets. In recent years, RHP has advanced by integrating textual and visual information [47]. Amazon-MRHP [4] and Lazada-MRHP [4] sourced from two major E-commerce platforms, Amazon and Lazada, serve as benchmarks for the MRHP task and have been utilized in MCR [4] Multi-perspective Coherence Reasoning is the first baseline for MRHP, SANCL [5] Selective Attention and Natural Contrastive Learning model, which reduces GPU memory usage during training, Thong Nguyen et al. [6] propose Multimodal Contrastive Learning method achieved state-of-the-art results and PRR-LI [7] Large language model driven Personalized Review Recommendation model based on Implicit dimension 4 Truc Nguyen et al. mining for MRHP. However, due to the limited availability of labeled data, most existing datasets [4, 9] for this task defined ground truth by mapping helpful votes into helpfulness scores. We compare the proposed ViMRHP dataset with existing datasets in Table 2."
        },
        {
            "title": "2.2 Human-AI Collaborative Annotation Framework",
            "content": "Reducing costs and ensuring high-quality annotations are two critical factors in NLP dataset construction. Recently, leveraging LLMs in annotation has become an optimized solution for time and cost efficiency [1013]. However, balancing time, cost, and data quality remains challenging for complex annotation tasks and datasets. Therefore, recent studies have explored the human-AI collaborative annotation framework to address this issue. Notable works such as CoAnnotating [14], Wang et al. [15], and various datasets with diverse tasks have also adopted this framework, such as Value FULCRA [16], VIVA [17]. These works highlight the effectiveness of human-AI collaboration in enhancing annotation efficiency while maintaining quality."
        },
        {
            "title": "3.1 Overview",
            "content": "Figure 1: ViMRHP benchmark dataset annotation overview. The Human-AI collaborative annotation framework workflow includes two steps: (1) AI Annotation (2) Human Verification and Refinement. First, AI extracts the relevant context or gives reason from the review based on the given instruction criteria and assigns score. Then, human annotators verify and refine the final score to ensure data quality. ViMHRP via Human-AI Collaborative Annotation 5 Data Collection. To construct the ViMRHP dataset, we collected product information and user-generated reviews, including both text and images, from the Shopee4 platform in Vietnam. For user-generated reviews, an average of 21-24 reviews were collected per product, spanning the period from 2019 to 2024. All user-related information was strictly removed before use to ensure privacy and compliance with ethical standards. Task Annotation. To ensure objectivity in evaluating review helpfulness, we define three key criteria for each sample in ViMRHP dataset: {(Ki, Di, Ii)}N , i=1 where Ki is key aspects, Di is decision-making advice, and Ii is image-helpfulness, scoring on scale from 1 to 5, where 1 is lowest and 5 is highest. The ground truth, referred to as the Helpfulness Score, is computed as Hi = 1 (Ki + Di + Ii). 3 The definition of each criterion and score is provided in 3.2. Annotation Workflow. We implemented human-AI collaborative annotation framework for the ViMRHP dataset, which consists of two steps: Step 1 - AI Annotation (3.3) and Step 2 - Human Verification and Refinement (3.4). Details of our annotation process are illustrated in Figure 1."
        },
        {
            "title": "3.2 Annotation Scheme and Guideline",
            "content": "The annotation guideline for constructing the ViMRHP dataset ensures that each sample is individually labeled. This approach addresses the limitations of automated ground truth generation based on mapping the number of helpful votes, which may introduce bias, common issue in previous studies, since not all reviews receive upvotes [9]."
        },
        {
            "title": "Description",
            "content": "Key-aspects: The number of key aspects of the product mentioned in the user-generated review. Decision-making advice: The recommendation to purchase the product mentioned in the user-generated review. Image-helpfulness: The level of usefulness of the product images provided by the user in the user-generated review. 1.0: Not mention any key aspects of the product. 2.0: Mentions one key aspect. 3.0: Mentions two key aspects. 4.0: Mentions three key aspects. 5.0: Mentions four or more key aspects. 1.0: Describes an ambiguous experience without giving any purchase advice. 2.0: Clearly describes the experience but does not provide purchase advice. 3.0: Implicitly suggests whether the product is worth buying. 4.0: Strongly implies whether the product is worth buying. 5.0: Clearly recommends the product, specifying the target users or suitable situations. 1.0: Does not meet any criteria for Relevance, Clarity, Illustrative Value, or Engagement. 2.0: Meets one criterion for Relevance, Clarity, Illustrative Value, or Engagement. 3.0: Meets two criteria for Relevance, Clarity, Illustrative Value, or Engagement. 4.0: Meets three criteria for Relevance, Clarity, Illustrative Value, or Engagement. 5.0: Meets four criteria for Relevance, Clarity, Illustrative Value, and Engagement. Helpfulness Score = (Key-aspect + Decision-making advice + Image-helpfulness) / 3 Table 3: Labeling Criteria for ViMRHP Dataset Based on the work of Chua et al. [18], we establish structured annotation framework using three key criteria: key aspects, decision-making advice, and 4 https://shopee.vn Truc Nguyen et al. image helpfulness. We assign score to each criterion for each sample in our ViMRHP dataset. Table 3 presents detailed labeling criteria, with the task design outlined below. Key-aspects. Identify the context in which the review mentions aspects of the product (e.g., features, benefits, usage, durability, design, etc.) and assign score accordingly, following the criteria in Table 3. Decsion-making advice. Identify whether the review provides purchasing recommendations (e.g., recommending to buy, not to buy, specifying target buyers, or advising against certain users, etc.) and assign score based on its clarity and strength, following the criteria in Table 3. Image-helpfulness. Evaluate the helpfulness of user-uploaded images based on predefined criteria, including: Relevance: The image accurately represents the reviewed product. Clarity: The image is clear and easily interpretable. Illustrative Value: The image effectively demonstrates key product features, benefits, or real-life usage. Engagement: The image captures user interest and enhances the reviews informativeness. Assign score based on how well the image meets these criteria, following Table 3. An example annotation with detailed scoring for each criterion is shown in Table 4. For key aspects, relevant contexts are highlighted and marked as (K1), (K2), (K3), (K4), and (K5). Similarly, for decision-making advice, key statements are identified and marked as (D1). For image-helpfulness, review images are assessed based on predefined criteria, ensuring comprehensive evaluation. \"ly khá là okla, bị đọng nước ở ngoài(K1), thiết kế đơn giản thanh lịch (K2), quá trơn(K3), nắp đậy bị đổ(K4), với giá vậy là rất rất rất tuỵt zời, còn về mức độ giữ nhiệt thì mình thấy giữ được khoảng 6h (K5), rcm nên muaa(D1) Translation: The cup is ok, doesnt get condensation on the outside(K1). Design is simple elegant(K2), not too slippery(K3) The lid doesnt leak(K4). For this price, its absolutely great. As for heat retention, found it keeps warm for about 6 hours. (K5) Recommend buying it(D1). Key-aspects Decision-making advice Image-helpfulness Mentions four or more key aspects: (K1),(K2),(K3),(K4),(K5) Score 5.0 Strongly implies whether the product is worth buying: (D1) Score 4.0 Meets two criteria. Score 3.0 Helpfulness Score: 4. Table 4: Example annotation for ViMRHP Dataset ViMHRP via Human-AI Collaborative Annotation"
        },
        {
            "title": "3.3 Step 1 - AI Annotation",
            "content": "We use LLM (i.e., gpt-4o-mini version)5 [19] to automatically annotate approximately 46K review samples at total cost of 150 - 170 USD, significantly reducing expenses and time compared to manual annotation. Our annotation task involves two main challenges: (1) Extracting the review context that mentions aspects of the product for key-aspects, explanation for decision-making advice and imagehelpfulness. (2) Assign scores to each criterion. These are illustrated in the LLM response in Figure 1 and detailed instructions are provided in Appendix B."
        },
        {
            "title": "3.4 Step 2 - Human Verification and Refinement",
            "content": "In this step, to construct the ViMRHP dataset, we recruited three annotators, undergraduate students at our institution, with Data Science background and Vietnamese proficiency. We paid 50 USD per annotator for modifying the entire dataset that AI-generated annotation in Step 1. Annotators were required to complete training phase before verifying and refining AI-annotated data, during which they performed manual annotation on 100 samples. The inter-annotator agreement, measured using Fleisss κ, was assessed across three criteria: key aspects, decision-making advice, and image helpfulness. Their corresponding κ values were 0.6341, 0.5944, 0.2107. and 0.4484 for Helpfulness Score. Our labeling UI is detailed in the Appendix A."
        },
        {
            "title": "4 ViMRHP Dataset",
            "content": "Dataset Statistics. The statistical overview of the ViMRHP dataset is presented, details in Table 5. The statistics comprehensively analyze the scale and features relevant to our multimodal ranking task, including Product Information (P) and Reviews (R). In addition, we provide details on the number of reviews per product (R/P), with our review list length ranging between 21 and 23 reviews per product. Furthermore, the number of images per review (Rimg/R) and the number of images per product (Pimg/P) provide valuable insights into multimodal aspects. avg. avg. len max. len total"
        },
        {
            "title": "Domain",
            "content": "R/P Pimg/P Rimg/R Ptext Rtext Ptext Rtext Pimg Rimg"
        },
        {
            "title": "22.4\nFashion\nElectronic\n21.9\nHome & Lifestyle 22.5\nHealth & Beauty 22.9",
            "content": "8.2 7.4 7.6 7.7 2.2 1.9 2.0 2.4 82.7 145.6 85.4 111.4 86.6 118.2 79.6 129.2 196 212 157 206 1782 4175 24857 1292 3529 19289 2170 3391 20817 2202 4794 34463 Table 5: Statistical overview of the ViMRHP dataset. 5 https://platform.openai.com/docs/models/gpt-4o-mini 8 Truc Nguyen et al. Following the study of Liu et al. [4], we split the ViMRHP dataset into Train, Dev, and Test sets with ratio of 70:10:20, as detailed in Table 6, with the distribution of Products and Reviews splits also presented."
        },
        {
            "title": "Criteria",
            "content": "%Agree Cκ - 356/7812 50/1065 103/2132 Fashion Electronic 332/7274 47/1010 96/2101 Home&Lifestyle 313/7153 44/1015 91/2057 Health&Beauty 433/9956 61/1403 125/2832 Table 6: Train, Dev, Test distribution across domains (Products / Reviews) Key-aspects 81.29 Decision-making advice 64.00 Image-helpfulness 64.20 Helpfulness Score 53.64 Table 7: Agreement evaluation between Human vs AI (%) 40.34 22.96 52.93 34.65 41.65 57.31 53.59 31.34 PN In-depth Analysis - Human vs AI. After verifying and refining the entire ViMRHP dataset, we compare the labels assigned by human annotators with AI-generated annotations using agreement metrics to assess consistency and accuracy, as detailed in Table 7. Specifically, we use three evaluation metrics: %Agree - Human Agreement (Yes/No) [20], Cκ - Cohens Kappa [21], and A = 1 i=1 Hi Ai, which quantifies the deviation in scores between human annotations and AI-generated labels across different criteria, providing comprehensive measure of annotation reliability. Our analysis reveals varying agreement metrics, with Human Agreement ranging from 40.34% to 57.31% meaning human annotators manually refined approximately 50% of the dataset to ensure data quality. Cohens Kappa scores between human vs AI for ground truth Helpfulness Score only 31.34% indicating Fair Agreement [22]. The high A deviation in key-aspects 81.29% suggests significant gap in contextual understanding between AI and human annotators, limitations in accurately identifying contextual information. Distributions Analysis. In ranking tasks, determining the distribution to select thresholds for experiments is crucial in evaluating dataset. We analyze the distribution of common score ranges in each domain and highlight the scoring differences between human annotators and AI, detail in Figure 2 (2a, 2b, 2c, 2d). The most common score range assigned is 34, reflecting tendency toward neutral or safe reviews. This also serves as the basis for selecting our dataset evaluation threshold on the average length of the review list per product (R/P) is 2123, as detailed in Table 5. ViMHRP via Human-AI Collaborative Annotation 9 (a) Fashion Category (b) Electronic Category (c) Home & Lifestyle Category (d) Health & Beauty Category Figure 2: Helpfulness Score distribution across categories between Human vs AI in ViMRHP dataset"
        },
        {
            "title": "5 Experimental Setup",
            "content": "In our experiment, we implement various baseline models for our ViMRHP dataset, including text-only and multimodal approaches, to evaluate the contribution of each data modality to the review helpfulness prediction task."
        },
        {
            "title": "5.1 Baselines",
            "content": "Text-only BiMPM [23] Bilateral Multi-Perspective Matching (BiMPM) uses BiLSTMs to encode and compare product information and reviews, capturing semantic relationships and aggregating relevance scores through second BiLSTM. Conv-KRMN [24] is neural ranking model designed to enhances ad-hoc search using CNNs to capture soft matches between query and document n-grams, which are mapped into shared embedding space and processed through kernel-based pooling for relevance scoring. DUET [25] is neural ranking model that combines local interactions and distributed representations using two jointly trained deep networks to enhance lexical and semantic similarity in document ranking. 10 Truc Nguyen et al. Match-Pyramid [26] is CNN-based model for text matching that represents word-level similarities as matching matrix. CNNs hierarchically extract complex patterns from this matrix, capturing key signals like n-gram and n-term matching."
        },
        {
            "title": "Multimodal",
            "content": "MCR [4] Multi-perspective coherence reasoning is Multimodal Review Helpfulness Prediction baseline that integrates text and images from products and reviews. It includes two modules: one assessing intraand inter-modal consistency between the product and review, and another ensuring coherence within the review by aligning textual and visual content."
        },
        {
            "title": "5.2 Evaluation Metrics",
            "content": "Following the study by Liu et al. [4] on the Amazon-MRHP and Lazada-MRHP datasets, we adopt two evaluation metrics frequently used in recommendation: MAP (Mean Average Precision) and NDCG@K (Normalized Discounted Cumulative Gain), where {1, 3, 5}, to assess the ViMRHP dataset. These specific values are chosen because users typically base their purchase decisions on the top few reviews, often only reading the first 1-5 reviews."
        },
        {
            "title": "5.3 Implementation Details",
            "content": "Baseline models in our ViMRHP experiments are based on MatchZoo library [27], with all evaluations using threshold of 3.0 for NDCG@K and MAP of MatchZoo [27] metrics. All baselines, including text-only (BiMPM, Conv-KRMN, DUET, Match-Pyramid) and multimodal (MCR), word embedding layer are used with FastText, batch_size 32, learning_rate 0.001, Adam optimizer and executed on single GPU Tesla-T4-15GB. The text-only models are trained for 5 epochs per domain. The MCR model is trained for 20 epochs per domain. Before training, all product and review images are extracted with pre-trained Faster R-CNN [28] for RoI feature extraction."
        },
        {
            "title": "6.1 Human-Verified versus AI-Annotated Data Quality.",
            "content": "We present the experimental results from our ViMRHP dataset in Table 8. The experiments were conducted using two types of annotated data: human-verified and AI-generated annotations. The results in Table 8 show that multimodal approaches outperform text-only methods in the MCR baseline, especially with human-verified annotations in ViMRHP across Fashion, Electronics, Home & Lifestyle, and Health & Beauty, demonstrating the effectiveness of multimodal learning. Additionally, leveraging LLMs can reduce costs and annotation time. However, human-verified annotations perform better across baseline models, underscoring the need for human verification to ensure data quality in complex annotation tasks. ViMHRP via Human-AI Collaborative Annotation 11 Domain Modality Method Human Verification AI Annotation N@1 N@3 N@5 MAP N@1 N@3 N@5 MAP Fashion Text-only BiMPM DUET 64.801.51 68.133.04 71.150.70 71.500.50 63.29 65.09 70.45 71.00 34.730.87 34.560.97 38.030.87 45.740.10 33.86 33.59 37.16 45.64 Conv-KRMN 63.330.94 63.510.68 65.572.02 67.861.48 62.39 62.83 63.55 66.38 Match-Pyramid 63.899.74 63.125.48 65.015.49 67.494.76 54.15 57.64 59.52 62.73 Multimodal MCR 74.045.36 74.342.48 74.692.16 74.471.27 68.68 71.86 72.53 73.20 Electronic Text-only BiMPM DUET 62.3316.09 61.5111.25 63.1010.54 63.149.03 46.24 50.26 52.56 54.11 44.6618.18 51.5925.97 56.3827.81 57.7822.32 26.48 25.62 28.57 35.46 Conv-KRMN 55.5810.42 56.1310.71 60.4811.80 61.5110.52 45.16 45.42 48.68 50. Match-Pyramid 44.664.20 49.537.62 50.836.99 53.275.61 40.46 41.91 43.92 47.66 Multimodal MCR 69.1516.13 65.2514.19 66.1612.55 66.0011.01 53.02 51.06 53.61 54. Home & Lifestyle Text-only BiMPM DUET 68.3111.59 73.6513.17 73.6912.42 75.1810.08 56.72 60.48 61.27 65.10 35.344.59 40.724.19 43.304.19 53.364.65 38.75 36.53 39.11 48.71 Conv-KRMN 50.766.66 50.924.64 54.325.44 62.194.60 44.10 46.28 48.88 57.59 Match-Pyramid 62.788.22 62.197.29 62.347.34 67.244.69 54.56 57.88 58.00 62. Multimodal MCR 72.7010.98 75.2513.07 75.2311.63 76.549.72 61.72 62.18 63.60 66.82 Health & Beauty Text-only BiMPM DUET 69.966.76 71.307.05 72.607.40 77.367.98 63.20 64.59 65.20 70.38 44.461.20 47.886.00 50.346.62 60.844.60 43.20 41.88 43.72 56. Conv-KRMN 69.264.90 69.705.58 69.605.72 74.054.35 64.36 64.12 64.46 69.70 Match-Pyramid 65.9610.79 65.8816.13 67.6718.02 73.317.55 55.17 56.75 57.65 65. Multimodal MCR 76.325.07 67.64 65.07 64.84 71.25 Table 8: Performance comparison for data quality in the ViMRHP dataset. Comparing human-verified data (Human Verification) with AI-generated annotations (AI Annotation). denotes the percentage increase in performance. 71.593.95 73.228.15 72.477."
        },
        {
            "title": "6.2 Cost, Time-Efficiency and Quality Comparison",
            "content": "We compare different annotation methods in Table 9 for evaluating cost, time efficiency, and data quality. The annotation process of ViMRHP dataset collaborates with humans and LLMs (Section 3) costs around 300-320 USD. This cost achieves balance, not as low as AI annotation, it remains significantly more affordable than the estimated cost of human annotation. With the support of LLMs, annotators achieved an annotation speed of 20-40 seconds per task, enabling the dataset to be completed within 3 weeks for approximately 46K multimodal reviews."
        },
        {
            "title": "Human",
            "content": "AI Human-AI Cost Time-consume Efficiency No. Annotators Quality Table 9: Comparison of annotation methods on the ViMRHP Dataset 800 - 900 USD 2 - 3 months 90 - 120s/Task 9 - 12 annotators 150 - 170 USD N/A 1 - 2s/Task N/A 300 - 320 USD 3 weeks 20 - 40s/Task 3 annotators 12 Truc Nguyen et al. Moreover, based on the evaluation metrics between human and AI annotation in Table 7 and the experimental results on ViMRHP dataset baselines in Table 8, we demonstrate that human-verified data achieves higher quality. To this end, human-AI collaboration is more efficient than traditional crowdsourced dataset creation in terms of data quality and reducing cost with time efficiency."
        },
        {
            "title": "7 Conclusions",
            "content": "This paper introduces ViMRHP, Vietnamese dataset for MRHP tasks, covering four domains that reflect diverse user behaviors and data modalities. The ViMRHP dataset is valuable resource for future research on MRHP tasks in Vietnamese, providing quality dataset with human-verified annotations. Additionally, ViMRHP demonstrates balance between cost, efficiency, and annotation quality with Human-AI collaborative annotation framework to dataset construction. Furthermore, it highlights the limitations of LLMs in dataset creation, paving the way for more effective hybrid data annotation methods in the future. Acknowledgments. This research was supported by The VNUHCM - University of Information Technologys Scientific Research Support Fund."
        },
        {
            "title": "A Labeling UI",
            "content": "We utilize HumanSignal6 [29] as the labeling tool for the ViMRHP dataset shown in Figure 3 below. Figure 3: Labeling UI for ViMRHP dataset"
        },
        {
            "title": "6 Label Studio Enterprise Supported by HumanSignal Label Studio Academic Program",
            "content": "ViMHRP via Human-AI Collaborative Annotation"
        },
        {
            "title": "B Instruction",
            "content": "Instruction (Vietnamese). Dựa vào thông tin bài đánh giá sản phẩm bao gồm văn bản và hình ảnh đã được cung cấp. Hãy phân tích bài đánh giá theo các tiêu chí sau: Key-aspects: Đưa ra các khía cạnh chính của sản phẩm. {key_aspects} 1.0 - Không đề cập đến khía cạnh cụ thể nào của sản phẩm. 2.0 - Đề cập đến một khía cạnh cụ thể của sản phẩm. 3.0 - Đề cập đến hai khía cạnh cụ thể của sản phẩm. 4.0 - Đề cập đến ba khía cạnh cụ thể của sản phẩm. 5.0 - Đề cập đến nhiều hơn bốn khía cạnh cụ thể của sản phẩm. Decision-making advice: Khuyến nghị mua hàng. {decision_making_advice} 1.0 - Mô tả trải nghiệm cá nhân mơ hồ, không đưa ra khuyến nghị. 2.0 - Mô tả trải nghiệm cá nhân rõ ràng, nhưng không có khuyến nghị. 3.0 - Ngầm đưa ra lời khuyên về việc có nên mua sản phẩm hay không. 4.0 - Đưa ra lời khuyên rõ ràng về quyết định mua hàng. 5.0 - Đưa ra lời khuyên cụ thể cho từng đối tượng khách hàng. Image-helpfulness: Mức độ hữu ích của hình ảnh sản phẩm dựa theo các tiêu chí. {image_helpfulness} Mức độ liên quan (Relevance)..., Độ rõ ràng (Clarity)..., Giá trị minh họa (Illustrative Value)..., Tính thu hút (Engagement)... 1.0 - Không thỏa mãn tiêu chí nào. 2.0 - Thỏa mãn một tiêu chí. 3.0 - Thỏa mãn hai tiêu chí. 4.0 - Thỏa mãn ba tiêu chí. 5.0 - Thỏa mãn cả bốn tiêu chí. Trả về Helpfulness Score bằng trung bình điểm số ba tiêu chí Key-aspects, Decision-making advice, Image-helpfulness: {Helpfulness_Score} Instruction (English). Based on the provided product review, including both text and images, analyze the review according to the following criteria: Key-aspects: Extract the main aspects of the product. {key_aspects} 1.0 - Does not mention any specific aspect of the product. 2.0 - Mentions one specific aspect of the product. 3.0 - Mentions two specific aspects of the product. 4.0 - Mentions three specific aspects of the product. 5.0 - Mentions more than four specific aspects of the product. Decision-making advice: Purchase recommendation. {decision_making_advice} 1.0 - Describes an ambiguous experience without giving any purchase advice. 2.0 - Clearly describes the experience but does not provide purchase advice. 3.0 - Implicitly suggests whether the product is worth buying. 4.0 - Strongly implies whether the product is worth buying. 5.0 - Clearly recommends the product, specifying the target users or suitable situations. Image-helpfulness: The helpfulness of product images based on the following criteria. {image_helpfulness} Relevance..., Clarity..., Illustrative Value..., Engagement... 1.0 - Does not meet any criteria. 2.0 - Meets one criterion. 3.0 - Meets two criteria. 4.0 - Meets three criteria. 5.0 - Meets all four criteria. Return Helpfulness Score is calculated as the average score of the three criteria: Key-aspects, Decision-making advice, and Image-helpfulness. {Helpfulness_Score} Table 10: Prompt for AI-based Explanation Score 14 Truc Nguyen et al."
        },
        {
            "title": "References",
            "content": "1. Yang, Y., Yan, Y., Qiu, M., Bao, F.: Semantic analysis and helpfulness prediction of text for online product reviews. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). pp. 3844 (2015) 2. Chen, Z., do Amarante, D.V., Donaldson, J., Jo, Y., Park, J.: Argument mining for review helpfulness prediction. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. pp. 89148922 (2022) 3. Diaz, G.O., Ng, V.: Modeling and prediction of online product review helpfulness: survey. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 698708 (2018) 4. Liu, J., Hai, Z., Yang, M., Bing, L.: Multi-perspective coherent reasoning for helpfulness prediction of multimodal reviews. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 59275936 (2021) 5. Han, W., Chen, H., Hai, Z., Poria, S., Bing, L.: Sancl: Multimodal review helpfulness prediction with selective attention and natural contrastive learning. In: Proceedings of the 29th International Conference on Computational Linguistics. pp. 56665677 (2022) 6. Nguyen, T., Wu, X., Luu, A.T., Hai, Z., Bing, L.: Adaptive contrastive learning on multimodal transformer for review helpfulness prediction. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. pp. 1008510096 (2022) 7. Xu, B., Xu, Y.: Personalized review recommendation based on implicit dimension mining. In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers). pp. 8691 (2024) 8. Kim, S.M., Pantel, P., Chklovski, T., Pennacchiotti, M.: Automatically assessing review helpfulness. In: Proceedings of the 2006 Conference on empirical methods in natural language processing. pp. 423430 (2006) 9. Nayeem, M.T., Rafiei, D.: On the role of reviewer expertise in temporal review helpfulness prediction. In: Findings of the Association for Computational Linguistics: EACL 2023. pp. 16841692 (2023) 10. Wang, S., Liu, Y., Xu, Y., Zhu, C., Zeng, M.: Want to reduce labeling cost? gpt-3 can help. In: Findings of the Association for Computational Linguistics: EMNLP 2021. pp. 41954205 (2021) 11. Ding, B., Qin, C., Liu, L., Chia, Y.K., Li, B., Joty, S., Bing, L.: Is gpt-3 good data annotator? In: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 1117311195 (2023) 12. Tan, Z., Li, D., Wang, S., Beigi, A., Jiang, B., Bhattacharjee, A., Karami, M., Li, J., Cheng, L., Liu, H.: Large language models for data annotation and synthesis: survey. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. pp. 930957 (2024) 13. He, X., Lin, Z., Gong, Y., Jin, A.L., Zhang, H., Lin, C., Jiao, J., Yiu, S.M., Duan, N., Chen, W.: AnnoLLM: Making large language models to be better crowdsourced annotators. In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track). pp. 165190 (2024) ViMHRP via Human-AI Collaborative Annotation 15 14. Li, M., Shi, T., Ziems, C., Kan, M.Y., Chen, N., Liu, Z., Yang, D.: CoAnnotating: Uncertainty-guided work allocation between human and large language models for data annotation. In: Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. pp. 14871505 (2023) 15. Wang, X., Kim, H., Rahman, S., Mitra, K., Miao, Z.: Human-llm collaborative annotation through effective verification of llm labels. In: Proceedings of the CHI Conference on Human Factors in Computing Systems. pp. 121 (2024) 16. Yao, J., Yi, X., Gong, Y., Wang, X., Xie, X.: Value fulcra: Mapping large language models to the multidimensional spectrum of basic human value. In: Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 87548777 (2024) 17. Hu, Z., Ren, Y., Li, J., Yin, Y.: Viva: benchmark for vision-grounded decisionmaking with human values. In: Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. pp. 22942311 (2024) 18. Chua, A.Y., Banerjee, S.: Helpfulness of user-generated reviews as function of review sentiment, product type and information quality. Computers in Human Behavior 54, 547554 (2016) 19. OpenAI: Gpt-4o mini: advancing cost-efficient intelligence (2024), https://openai. com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ 20. Chen, D., Chen, R., Zhang, S., Wang, Y., Liu, Y., Zhou, H., Zhang, Q., Wan, Y., Zhou, P., Sun, L.: Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In: Forty-first International Conference on Machine Learning (2024) 21. Cohen, J.: coefficient of agreement for nominal scales. Educational and psychological measurement 20(1), 3746 (1960) 22. Landis JRKoch, G.: The measurement of observer agreement for categorical data. Biometrics 33(1), 159174 (1977) 23. Wang, Z., Hamza, W., Florian, R.: Bilateral multi-perspective matching for natural language sentences. In: International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence (2017) 24. Dai, Z., Xiong, C., Callan, J., Liu, Z.: Convolutional neural networks for softmatching n-grams in ad-hoc search. In: Proceedings of the eleventh ACM international conference on web search and data mining. pp. 126134 (2018) 25. Mitra, B., Diaz, F., Craswell, N.: Learning to match using local and distributed representations of text for web search. In: Proceedings of the 26th international conference on world wide web. pp. 12911299 (2017) 26. Pang, L., Lan, Y., Guo, J., Xu, J., Wan, S., Cheng, X.: Text matching as image recognition. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 30 (2016) 27. Guo, J., Fan, Y., Ji, X., Cheng, X.: Matchzoo: learning, practicing, and developing system for neural text matching. In: Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. pp. 12971300 (2019) 28. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang, L.: Bottom-up and top-down attention for image captioning and visual question answering. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 60776086 (2018) 29. Tkachenko, M., Malyuk, M., Holmanyuk, A., Liubimov, N.: Label Studio: Data labeling software (2020-2024), https://humansignal.com/"
        }
    ],
    "affiliations": []
}