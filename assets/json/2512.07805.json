{
    "paper_title": "Group Representational Position Encoding",
    "authors": [
        "Yifan Zhang",
        "Zixiang Chen",
        "Yifeng Liu",
        "Zhen Qin",
        "Huizhuo Yuan",
        "Kangping Xu",
        "Yang Yuan",
        "Quanquan Gu",
        "Andrew Chi-Chih Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,ω\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE."
        },
        {
            "title": "Start",
            "content": "Yifan Zhang1 Zhen Qin Huizhuo Yuan2 Kangping Xu3 Yang Yuan3 Quanquan Gu2 Andrew Chi-Chih Yao3 Zixiang Chen2 Yifeng Liu2 5 2 0 2 8 ] . [ 1 5 0 8 7 0 . 2 1 5 2 : r 1Princeton University 2University of California, Los Angeles 3IIIS, Tsinghua University yifzhang@princeton.edu qgu@cs.ucla.edu andrewcyao@tsinghua.edu.cn December 9, 2025 Abstract We present GRAPE (Group RepresentAtional Position Encoding), unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in SO(d) and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group GL. In Multiplicative GRAPE, position (or R) acts as G(n) = exp(n ω L) with rank-2 skew generator Rdd, yielding relative, compositional, norm-preserving map with closed-form matrix exponential. RoPE is recovered exactly when the d/2 planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at O(d) and O(rd) cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE"
        },
        {
            "title": "1 Introduction",
            "content": "Positional information is essential for sequence modeling with Transformers (Vaswani et al., 2017), whose self-attention is otherwise permutation-invariant. Early work injected absolute positional codes (sinusoidal or learned) into token representations (Vaswani et al., 2017). Later, relative encodings depending on offsets (Shaw et al., 2018) and linear logit biases such as ALiBi (Press et al., 2021) were introduced, the latter offering strong length extrapolation with negligible overhead. Rotary Position Embedding (RoPE) (Su et al., 2021) realizes relative positions as orthogonal planar rotations of queries and keys, preserving norms and yielding exact origin invariance of attention scores. Despite its appeal, RoPE fixes coordinate planes and typically log-uniform spectrum, limiting cross-subspace coupling and contextual warping of phase. More broadly, absolute codes break translation equivariance; table-based relatives add window-dependent overhead. new Core contribution; Corresponding authors."
        },
        {
            "title": "GRAPE",
            "content": "Group Representational Position Encoding General Relative Law: G(t s) = G(s)1G(t) G(n) = exp(n ω Generator)"
        },
        {
            "title": "Additive GRAPE",
            "content": "Operation: Rotation Manifold: SO(d) Generator: (Rank-2 Skew) = ab ba exp(L) = + sin + 1cos s2 L2 xj θij xi Norm-preserving Isometry Fast Matrix Exponentials Operation: Translation Manifold: GL(d+k) (Unipotent Lift) Generator: (Low-rank Nilpotent) A2 = 0 = exp(A) = + bias Translation pos Homogeneous Space Lift Additive Translations Recovers: RoPE Extensions: Learned Basis Recovers: ALiBi, FoX Extensions: Path Integral Also extends to contextual forms via state-dependent generators Figure 1 Overview of the GRAPE Framework. We unify positional encodings via group actions G(n) = exp(nωL). Left: Multiplicative GRAPE recovers RoPE via rank-2 skew generators in SO(d). Right: Additive GRAPE recovers ALiBi and FoX via low-rank nilpotent generators in the unipotent subgroup of GL(d + k) (k = 1 or 2). formulation is needed because current methods isolate the essential properties of stability, monotonic distance penalty, and expressivity. These observations motivate unified formulation that (i) preserves RoPEs orthogonality and exact relativity when desired, (ii) also covers additive/forgetting mechanisms such as ALiBi (Press et al., 2021) and Forgetting Transformer (FoX) (Lin et al., 2025), and (iii) admits learned and contextual generalizations with clean streaming. We therefore propose Group RepresentAtional Position Encoding (GRAPE), group-theoretic framework that unifies two complementary families of positional mechanisms (see Figure 1 for an overview). The multiplicative family (Multiplicative GRAPE) models positions as norm-preserving rotations in SO(d) acting on (q, k); the additive family (Additive GRAPE/Path-Integral Additive GRAPE) models positions as unipotent actions in the general linear group GL that yield linear-in-offset logit biases (including content-gated and path-integral forms). This perspective recovers RoPE and ALiBi as exact special cases, proves that FoX is an exact instance of Additive GRAPE, and supplies principled, streaming-friendly contextual extensions on both sides. Concretely: (a) Multiplicative GRAPE (GRAPE-M) encodes (or R) as an element of SO(d) via rank-2 skew generator; and (b) Additive GRAPE (GRAPE-A) and Path-Integral Additive GRAPE (GRAPE-AP) lifts to the general linear group GL using homogeneous coordinates to produce linear-in-offset logit biases (recovering ALiBi and FoX). For Multiplicative GRAPE, positions are mapped as G(n) = exp (cid:0)n ω L(cid:1) SO(d), = ab ba so(d), where a, Rd define rank-2 skew generator and ω > 0 is frequency. The action is an isometry, and G(n + m) = G(n)G(m) guarantees exact origin invariance of attention logits. We derive closed-form Rodrigues-type formula (Rodrigues, 1840; Hall, 2013), enabling fast linear-time application with stable derivatives and no explicit matrix materialization. RoPE is recovered when d/2 commuting rank-2 generators act on disjoint coordinate planes with prescribed frequencies. For Additive GRAPE, positions are mapped via the matrix exponential Gadd(n) = exp(nωA) = + nωA in lifted homogeneous space. Here, the generator gl(d + 1) is nilpotent matrix of rank one. While this additive transformation is not an isometry, it preserves the exact relative law, ensuring attention scores depend only on position offsets. This formulation provides rigorous group-theoretic foundation for additive biases, recovering ALiBi and FoX as exact instances. Our contributions are highlighted as follows: 1. We propose GRAPE as unified group-theoretic view that subsumes multiplicative orthogonal rotations in SO(d) and additive unipotent (all eigenvalues equal to 1) mechanisms in general linear group GL, recovering RoPE and ALiBi as exact special cases and proving FoX is an exact instance (Appendix B). 2. Multiplicative GRAPE. We derive closed-form rank-2 matrix exponential with fast application and stable differentiation; we show RoPE is special multiplicative GRAPE in possibly learned orthogonal basis. 3. Additive GRAPE. We show that linear-in-offset logit biases arise from rank-1 (or low-rank) unipotent actions in the general linear group GL with an exact relative law and streaming cacheability. This includes queryor key-gated slopes, commuting dictionary of additive components, and exact recoveries of ALiBi and FoX in closed form (Sections 4, 4.2, Appendix B). We also formalize path-integral additive biases that remain causal and support efficient training. (Section 5)."
        },
        {
            "title": "2 Multiplicative Group Representational Position Encoding",
            "content": "We propose the Multiplicative GRAPE, as Lie-group positional map with closed-form rank-2 matrix exponential, an exact relative law, and streaming/cache methodology. The core intuition is to encode position as norm-preserving rotation in the special orthogonal group SO(d) 1(Hall, 2013). single skew-symmetric generator so(d) produces the entire family of rotations via the matrix exponential. We begin with notation and the rank-2 generator. 1Definitions of SO(d) and other mathematical terms are postponed to Table 3 in the Appendix."
        },
        {
            "title": "2.1 Preliminaries and Rank-2 Generator",
            "content": "The generator is formally defined as an element of the corresponding Lie algebra, so(d). Let so(d) = {L Rdd : = L} denote the Lie algebra of SO(d). The simplest non-trivial generator defines rotation within single 2D plane. We construct such rank-2 generator from two vectors, and b, that span this plane of action. For a, Rd, define the rank-2 generator L(a, b) as L(a, b) = ab ba, α = a2, β = b2, γ = ab, = αβ γ2 0, = . (2.1) Rank-2 structure. Let = span{a, b}. The rank-2 generator has useful geometric property: applying it twice projects onto the action plane and scales. direct calculation shows L2 = s2 PU , where PU is the orthogonal projector to the space U. Hence spectrum of (the set of its eigenvalues), denoted σ(L), is {is, 0, . . . , 0} and the minimal polynomial is λ(λ2 + s2). detailed derivation is given in Appendix H. (cid:1) so that = AJA. For any SL(2) Initialization. Write [a b] Rd2 and = (cid:0) 0 1 1 0 (the 2 2 real matrices with determinant 1, see Table 3), MJM = and thus (cid:55) AM leaves invariant; for general GL(2) the group of invertible 2 2 matrices), scales by det(M). Therefore the oriented plane = span{a, b} and the scalar = (cid:112)αβ γ2 determine the action. We fix gauge at initialization by = = 1 and ab = 0 (absorbing scale into ω). Canonical 90 rotation operator. Fix block-diagonal complex structure so(d) with = and 2 = (for odd d, act on the top-left 2d/2 coordinates and leave the final coordinate (cid:1). For any Rd, write := a, which equals unchanged). Concretely, = (cid:76)d/2 i=1 rotated by 90 within the canonical 2D blocks and satisfies aa = 0 and = a. (cid:0) 0 1"
        },
        {
            "title": "2.2 Exact relative law",
            "content": "For fixed so(d), define G(n) = exp(nL) SO(d), which forms one-parameter subgroup. The exact relative law property for positional encoding implies: G(ts) = G(s)G(t), G(n)G(n) = I. Here G(n) SO(d), so the transpose coincides with the group inverse, G(n) = G(n)1; the identity above is exactly the relative-position law for one-parameter subgroup. concise summary of SO(d), GL(d) and SL(d) is collected in Table 3. This algebraic property enables relative positional encoding: interactions depend only on offsets. G(n) = exp(nωL), G(n + m) = G(n)G(m), G(0) = I, and G(n) = G(n). Crucially, this exact relative property relies solely on the one-parameter subgroup structure (G(n + m) = G(n)G(m)), holding true regardless of whether the generator implies commuting or coupled non-commuting subspaces."
        },
        {
            "title": "2.3 Closed-form fast matrix exponential",
            "content": "Based on the minimal polynomial mentioned in Section 2.1, the exponential map exp(L) for rank-2 generator can be expressed as quadratic in L. This yields convenient closed-form solution, often referred to as Rodrigues-type formula (Rodrigues, 1840; Hall, 2013): exp(L) = + sin + 1 cos s2 L2. Geometrically, the formula is best understood via L2 as projector onto U. Since L2 = s2PU , the exponential can be written as exp(L) = (1 cos s) PU + sin L, which reveals its action explicitly: it is rotation by angle within the plane = span{a, b} and the identity on the orthogonal complement . The vectors and thus define the plane of action for the positional rotation. Cost of application. For single rank-2 plane, computing = G(n)x requires two inner products = a, x, = b, x, followed by = + f1(n)(av bu) + f2(n) [γ(av + bu) βau αbv], where (α, β, γ) are plane scalars and f1,2 are trigonometric scalars (with series guards as 0). This is O(d) flops with small constant and no materialization of G(n); derivative expressions are in Appendix H."
        },
        {
            "title": "2.4 The b = J a constraint",
            "content": "We now consider an important special case by setting = a. This constraint, which makes the plane vectors and orthogonal and equal in norm, significantly simplifies the generators structure and reveals direct connection to the canonical RoPE formulation. With this constraint, the scalars simplify: γ = ab = aJ = 0, β = b2 = a2 = α, and hence = (cid:112)αβ γ2 = α. Moreover, on the 2D subspace = span{a, a} one has L(a, a)a = (J a)α, L(a, a) = α a, so L(a, a)U = α and L(a, a)U = 0. Therefore exp (cid:0)nωL(a, a)(cid:1) = (cid:0)1 cos(nωα)(cid:1)PU sin(nωα) PU , This expression follows by substituting LU = α and L2 = α2PU into the Rodrigues formula exp(nωL) = + sin(nωs) L2 with = α; see Appendix for the algebraic steps. It is s2 pure planar rotation by angle nωα on and the identity on . + 1cos(nωs) Corollary 2.1 (Frequencynorm coupling). If = 1, the rotation angle reduces to nω. Without normalization, the effective frequency is ωeff = ωa2, so the scale of can be absorbed into ω."
        },
        {
            "title": "2.5 Application to relative encoding and equivariance",
            "content": "We now demonstrate how the GRAPE-M operator G(n) is applied in practice. As established in Section 2.2, the operators group structure guarantees the exact relative law. We first transform the query and key vectors, qi and kj, into position-aware representations, (cid:101)qi and (cid:101)kj: (cid:101)qi := G(i)qi, (cid:101)kj := G(j)kj. 5 It follows from the exact relative law established in Section 2.2 that the attention score between these position-aware vectors simplifies to: (cid:101)kj = (cid:101)q G(i)G(j)kj = i G(j i)kj. Hence, the attention score depends solely on the relative offset i, not on the absolute positions. Streaming and caching. At inference, cache = G(j)kj once when token arrives. At step t, form (cid:101)qt = G(t)qt and compute logits (cid:101)q . No cache rotation is needed when increments; complexity matches RoPE. full integration into multi-head attention (per-head formulation, logits, and streaming) is detailed in Section A. k"
        },
        {
            "title": "3 Multi-Subspace Multiplicative GRAPE",
            "content": "A single rank-2 generator acts on 2D subspace, leaving the rest of the d-dimensional space untouched. To encode position across the entire hidden dimension, we can combine multiple generators. This leads to the Multi-Subspace (MS) Multiplicative GRAPE (GRAPE-M) model, which forms the basis for both RoPE and more expressive types. Detailed rank-2 algebra appears in Appendix H."
        },
        {
            "title": "3.1 Multi-Subspace GRAPE-M and RoPE as a Special Case",
            "content": "The simplest way to combine generators is to ensure they act on mutually orthogonal subspaces, which guarantees they commute. Let be even. For = 1, . . . , d/2, we can define set of rank2 generators {Li}, each acting on distinct 2D plane. RoPE is the canonical example of this construction. We further discussed non-commuting multiplicative GRAPE in Appendix C. Let the 2 2 canonical skew matrix be = (cid:0) 0 1 1 0 Rd2. We set the rank-2 generators as Li = UiJU θi > 0. The total generator is the commuting sum: (cid:1) and the coordinate selector be Ui = [e2i1 e2i] = L(e2i1, e2i) and assign per-plane frequencies LRoPE = d/2 (cid:88) i=1 θiLi with [Li, Lj] = 0 for = j."
        },
        {
            "title": "Then",
            "content": "G(n) = exp (cid:0)nLRoPE (cid:1) = d/2 (cid:89) i=1 exp(nθiLi) = blockdiag (cid:0)R2(nθ1), . . . , R2(nθd/2)(cid:1), (3.1) where R2(θ) denotes the standard 2 2 rotation matrix introduced in Table 3, and the last equality holds because each term exp(nθiLi) is identity except for single 22 rotation block on its diagonal. Eq. (3.1) is precisely the RoPE mapping: block-diagonal product of planar rotations with per-subspace angles nθi. Equality holds when the planes {Ui} are the coordinate 2D blocks and {θi} follow the canonical log-uniform spectrum. Proposition 3.1 (RoPE is multiplicative GRAPE). Choose d/2 mutually orthogonal vectors {ai} and set bi = ai with per-plane angles θi. Then the commuting MS-GRAPE G(n) = 6 (cid:81)d/ i=1 exp(nθiL(ai, ai)) equals the standard RoPE map in (possibly learned) orthogonal basis. If the planes are the canonical coordinate pairs and {θi} follow the log-uniform spectrum, we recover the canonical RoPE exactly. Spectral parameterization. Classical RoPE chooses θi on log-uniform grid across i. In GRAPE, θi can be learned or shared/tied across heads or layers. The MS-GRAPE view also allows replacing the coordinate selectors Ui by learned orthogonal basis SO(d) so that = (cid:80) B, preserving commutativity while learning subspaces. Multimodal GRAPE. Please refer to Appendix for 2D and 3D GRAPE for Vision and Multimodal Position Encoding. θiBUiJU"
        },
        {
            "title": "4 Additive Group Representational Position Encoding",
            "content": "This section shows that additive positional mechanisms (absolute shifts of features and additive logit biases, including ALiBi (Press et al., 2021)) also admit group-theoretic formulation. The key is homogeneous lift to an augmented space and one-parameter subgroup of the general linear group GL that acts by unipotent (all eigenvalues equal to 1) transformations. This yields an exact relative law and streaming/cache rules analogous to Section 2.5."
        },
        {
            "title": "4.1 Homogeneous lift and a unipotent action",
            "content": "To produce additive biases from multiplicative group action, we employ the homogeneous lift. This is standard method in linear algebra for representing affine transformations (such as translations) as linear transformations in higher-dimensional space. Let (cid:98)x := [x; 1] Rd+1 denote homogeneous augmentation of Rd. We now work within the general linear group GL(d+1) and its corresponding Lie algebra gl(d + 1), which is the set of all (d + 1) (d + 1) real matrices. Fix generator = (cid:21) (cid:20)0dd 0 01d gl(d+1), A2 = 0, (4.1) where Rd. Its exponential is unipotent: Gadd(n) := exp(n ω A) = Id+1 + ω = (cid:20) Id ω 0 1 (cid:21) GL(d+1), Gadd(n+m) = Gadd(n)Gadd(m). Application and exact relative law in GL. For queries/keys augmented as (cid:98)qi = [qi; 1] and (cid:98)kj = [kj; 1], define (cid:101)qi := Gadd(i) (cid:98)qi, (cid:101)kj := Gadd(j) (cid:98)kj, (4.2) We use the shorthand Gadd(j) := (Gadd(j)1) to emphasize that we first take the group inverse in GL(d+1) and then transpose it. and score with the standard inner product on Rd+1. The key is transformed using the inverse transpose (Gadd(j)). This is necessary because for general linear group GL, the simple transpose is no longer the inverse (unlike in SO(d)), and the inverse 7 transpose is required to recover the exact relative law: Gadd(i)Gadd(j) = Gadd(ji) for any one-parameter subgroup in GL. This composition results in the final form: (cid:101)kj = (cid:98)q (cid:101)q Gadd(ji)(cid:98)kj, depending only on ji. (4.3) Streaming matches Section 2.5: cache (cid:98)k compute (cid:101)q Closed form and content-gated additive term. Since = (cid:0) 0 0 0 (cid:98)k . = Gadd(j)(cid:98)kj once; at step form (cid:101)qt = Gadd(t)(cid:98)qt and (cid:1) and (A)2 = 0, Gadd(m) = Id+1 ω = (cid:20) Id 0 ω 1 (cid:21) , = ji, (4.4) whence (cid:101)kj = (cid:101)q kj + 1 (ji) ω ukj. (4.5) The constant +1 is softmax-shift invariant; the final term is an additive, linear-in-offset bias whose slope is key-gated by ukj. symmetric generator for the query, Aqry = (cid:0) 0 0 (cid:1) applied 0 analogously produces query-gated slope (ji) ω vqi. Using both the key-gated and query-gated components yields combined bias of the form (ji) ω (vqi ukj), still obeying the exact relative law Eq. (4.3)."
        },
        {
            "title": "4.2 Exact ALiBi as a Rank-1 unipotent in GL(d+2)",
            "content": "ALiBi adds head-specific scalar slope βh(ji) to the logits that is independent of content. This is captured exactly by augmenting with two constant coordinates: (cid:98)qi = [qi; 1; 0] Rd+2, (cid:98)kj = [kj; 0; 1] Rd+2, and choosing the rank-1 nilpotent generator = βh ed+1 d+2 Ah = βh ed+2 d+1, (A )2 = 0. (4.6) Then Gadd,h(m) = h and Gadd,h(ji)(cid:98)kj = (cid:98)q kj (ji) βh, i.e., the ALiBi term emerges as unipotent GL(d+2) action with exact relative composition. FoX as GRAPE-A. Let ft (0, 1] be per-token forget scalars and set ωt := log ft. Using the rank-1 generator of Section 4.2, the resulting additive bias is b(t, j) = (cid:80)t ℓ=j+1 ωℓ, which coincides with FoXs forgetting bias Dij. full derivation and the unipotent path product are given in Appendix B."
        },
        {
            "title": "5 Path Integral Additive GRAPE",
            "content": "Additive GRAPE (GRAPE-A) realizes exactly relative additive logits via one-parameter unipotent action in the general linear group GL; the bias depends only on an offset = ji (or contextual phase difference ΦjΦi when using cumulative phases). Here the phase Φt is scalar path variable, typically defined as cumulative sum Φt = (cid:80) ℓ<t ωℓ of per-token frequencies ωℓ, so that 8 Φj Φi plays the role of an effective relative position. In practice, we sometimes want the amount of additive encouragement/suppression between key at and query at to depend on the endpoint (e.g., the current syntactic or semantic needs of the query token), while preserving causality, boundedness, and clean composition with the orthogonal GRAPE acting on (q, k). We formalize this by rigorously defined path-integral sum, deriving conditions under which the exact relative law of Additive GRAPE is recovered. Definition (Path-integral bias). Fix head and per-head scale αh > 0. For each time u, let pu,h Rd be positional embedding obtained from token-local features (a linear projection followed by RMS normalization in our implementation). Let be the canonical block-diagonal 90 operator (Section 2.4), and define Rℓ := exp(ℓ ) (a fixed commuting rotation). For link function : (, 0) that is monotone increasing and 1-Lipschitz2, define the edge potential ψh(t, ℓ) := αh (cid:10)pt,h, Rℓ pℓ,h (cid:19) (cid:11) (cid:18) 1 0, ℓ < t. (5.1) The vectors pt,h and pℓ,h here are the positional embedding. The path-integral additive bias from key position to query position is the causal sum bh(t, j) := (cid:88) ℓ=j+1 ψh(t, ℓ) 0. (5.2) The attention logit combines this additive term with either the raw or orthogonally-rotary bilinear part: ℓt,j,h = t,hkj,h + bh(t, j) or ℓt,j,h = t,hGh(jt)kj,h + bh(t, j). (5.3) 1 1 Group-theoretic formalization and path composition. Let R(d+2)(d+2) be fixed rank-1 nilpotent with E2 = 0 (e.g., = ed+2e d+1 as in Section 4.2). For each fixed endpoint t, define endpoint-indexed unipotent factors (ℓ) := + ψh(t, ℓ) E. Since E2 = 0, the path product along (j, t] collapses additively: H(t) (cid:89) ℓ=j+1 H(t) (ℓ) = + (cid:18) (cid:88) ℓ=j+1 (cid:19) ψh(t, ℓ) = + bh(t, j) E. (5.4) Scoring in homogeneous coordinates as in Section 4 with the paired inverse-transpose removes multiplicative anisotropy and yields exactly the additive term bh(t, j), cf. Eq. (4.3). The rowwise semigroup law is preserved (Eq. (5.4)), while the t-dependence of the factors intentionally relaxes the global one-parameter group law. Relation to GRAPE-A. GRAPE-AP strictly contains GRAPE-A as the special case in which edge potentials do not depend on the endpoint: ψh(t, ℓ) θh aℓ = bh(t, j) = θh (cid:88) ℓ=j+1 aℓ = θh (cid:0)At Aj (cid:1), Au := (cid:88) ℓ<u aℓ. Two important instances follow directly: 2Our experiments take g(z) = log(Sigmoid(z)); then g(z) = 1 Sigmoid(z) (0, 1), ensuring 1-Lipschitzness. Exact ALiBi. aℓ 1 gives bh(t, j) = θh(tj); this is exactly the ALiBi term recovered via the rank-1 unipotent lift in Section 4.2. Phase-modulated Additive GRAPE. If aℓ = ωℓ with ωℓ = g(xℓ) 0, then bh(t, j) = θh(Φt Φj) with Φu = (cid:80) ℓ<u ωℓ. In both cases, bh(t, j) depends only on (possibly contextual) phase difference and thus obeys the exact relative law with the same streaming/cache policy as Section 4. Outside these endpointindependent regimes, GRAPE-AP provides strictly more expressive, path-integral biases while preserving row-wise path composition (Eq. (5.4)). Computation and streaming. For each head and decoding step t, compute the row {ψh(t, ℓ)}ℓt by single similarity sweep ℓ (cid:55) pt,h, Rℓpℓ,h (the rotated probes Rℓpℓ,h can be cached on arrival), apply the link g, and take prefix sum to obtain (cid:55) bh(t, j). This yields O(t) per-step overhead with O(1) recomputation per cached key; memory is O(L) per head for the cached probes (or O(d) if the per-ℓ rotations are recomputed on the fly). Spectral and stability. Each factor H(t) (ℓ) = + ψh(t, ℓ)E is unipotent with all eigenvalues 1 and at most two singular values deviating from 1; the full path product equals + bh(t, j)E (Eq. (5.4)). As in Appendix I.3, the paired inverse-transpose used for scoring cancels multiplicative distortions and delivers exactly the additive bias bh(t, j); operator norms remain controlled linearly in bh(t, j). more extensive spectral analysis, including eigenvalue structure and singular-value behavior across GRAPE variants, is provided in Appendix I. There, we also give an explicit comparison to PaTH Attention (Yang et al., 2025b), which is shown to be contractive and near singular. These properties may impair PaTHs effectiveness in long-context modeling."
        },
        {
            "title": "6 Experiments",
            "content": "In this section, we evaluate the performance of GRAPE on the language modeling task in comparison with baseline positional encoding mechanisms, including RoPE (Su et al., 2021), AliBi (Press et al., 2021), as well as Forgetting Transformer (FoX) (Lin et al., 2025). 6."
        },
        {
            "title": "Implementation Details",
            "content": "Based on the nanoGPT codebase (Karpathy, 2022), our experiments are implemented based on the Llama model (Touvron et al., 2023a). We only change the positional encoding mechanism and keep the rest of the model architecture the same as Llama. We choose FineWeb-Edu 100B dataset (Lozhkov et al., 2024), which contains 100 billion training tokens and 0.1 billion validation tokens, and we randomly choose 50B tokens for training. Our models are with 36 layers and 10 heads, with hidden size of 1280 and head dimension of 128. We applied QK RMSNorm for training stability (Yang et al., 2025a). The context length is set to 4,096, and the batch size is 480. All the models are optimized by AdamW optimizer (Loshchilov and Hutter, 2019), with maximum learning rate of 2 104, (β1, β2) = 0.9, 0.95, and weight decay of 0.1. We use cosine learning rate scheduler with 2,000 warm-up iterations, and the minimum learning rate is 1 105. We also clip the gradient to 1.0 for stabler training. The frequency of RoPE is set to 10,000. Moreover, for fair comparison, we do not use FoX-Pro and disabled the KV-shift module within it."
        },
        {
            "title": "6.2 Result Analysis",
            "content": "The curves for training and validation loss of models with variant positional encoding mechanism are displayed in Figures 2 and 3. This analysis provides specific insight into the source of the frameworks stability and performance. It can be observed that GRAPE can keep persistent edge over other mechanisms, including RoPE and FoX. Moreover, the model with RoPE suffers from training instability shown in Figure 3 (a), while the model with GRAPE embedding steadily improves during the training process. (a) Training Loss (b) Validation Loss Figure 2 The training and validation loss of medium-size models (355M), with different positional encoding mechanisms on the FineWeb-Edu 100B dataset. (a) Training Loss (b) Validation Loss Figure 3 The training and validation loss of large-size models (770M), with different positional encoding mechanisms on the FineWeb-Edu 100B dataset. 11 Table 1 The evaluation results of medium models with different positional encoding mechanisms pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Method ARC-E ARC-C BoolQ HellaSwag OBQA PIQA WinoGrande SciQ Avg. RoPE AliBi FoX FoX (w/ KV-shift) GRAPE-A GRAPE-M (Ctx) GRAPE-M (nonCtx) 59.34 57.07 56.78 57.11 59.68 56.02 56.31 30.89 30.80 29.01 30.55 31.91 29.35 30.55 61.22 61.16 59.11 60.34 60.06 58.81 61.77 45.46 46.98 43.07 44.32 46.27 44.88 44. 34.00 34.60 32.80 33.80 35.00 35.00 34.40 69.42 69.48 67.74 69.31 69.64 68.61 68.44 52.49 52.96 51.07 52.17 53.83 52.09 53.67 74.70 79.70 76.10 78.40 79.90 76.50 75.20 53.44 54.09 51.96 53.25 54.54 52.66 53.15 Table 2 The evaluation results of large models with different positional encoding mechanisms pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Method ARC-E ARC-C BoolQ HellaSwag OBQA PIQA WinoGrande SciQ Avg. RoPE AliBi FoX FoX (w/. KV-shift) GRAPE-A 62.25 63.43 59.22 60.77 62. 33.02 34.81 32.00 32.85 33.19 58.23 59.69 59.69 62.51 59.11 50.92 52.88 49.78 49.38 53.18 37.60 36.80 38.00 38.00 36.00 70.89 71.33 71.00 70.62 71.98 55.88 56.20 54.62 54.78 57. 80.50 82.40 79.20 81.40 84.10 56.16 57.19 55.44 56.29 57."
        },
        {
            "title": "7 Related Work",
            "content": "Positional information in Transformers mainly can be categorized into these classes: (a) absolute encodings (sinusoidal or learned) (Vaswani et al., 2017; Devlin et al., 2019; Neishi and Yoshinaga, 2019; Kiyono et al., 2021; Likhomanenko et al., 2021; Wang et al., 2020; Liu et al., 2020; Wang et al., 2021; Sinha et al., 2022; Wennberg and Henter, 2021; Ke et al., 2020); (b) relative encodings that depend on offsets (Shaw et al., 2018; Dai et al., 2019; Raffel et al., 2020; He et al., 2020); and (c) linear logit biases with strong length extrapolation (Press et al., 2021; Chi et al., 2022a,b; Li et al., 2023; Ruoss et al., 2023), all shaping recency/extrapolation behavior (Haviv et al., 2022; Kazemnejad et al., 2023). Multiplicative position encoding. RoPE realizes offsets as block-diagonal planar rotations of queries/keys, preserving norms and exact origin invariance; it is widely deployed across LLMs and modalities (Su et al., 2021; Touvron et al., 2023a,b; Heo et al., 2024). Angle/spectrum designs improve long-context fidelity (e.g., xPos) (Sun et al., 2022); LRPE formalizes separable relative transforms for linear attention models (Qin et al., 2023); mechanistic work analyzes frequency usage (Barbero et al., 2025). These methods are also compatible with sparse/linear attentions (Beltagy et al., 2020; Zaheer et al., 2020; Katharopoulos et al., 2020; Choromanski et al., 2020) and with context-scaling procedures (Xiong et al., 2023; Chen et al., 2023; Peng et al., 2023; Zhu et al., 2023; Jin et al., 2024). Beyond 1D language modeling, 2D RoPE and variants adapt rotary encodings to 2D grids by applying rotations along spatial axes, and have been shown to improve high-resolution extrapolation in Vision Transformers and related vision models (Heo et al., 2024). Recently, LieRE (Ostmeier et al., 2025) learns dense skew-symmetric generators whose exponentials produce high-dimensional rotations for multi-modal, n-dimensional inputs, while STRING (Schenck et al., 2025) designs separable, translation-invariant RoPE-style encodings that scale to 2D and 3D 12 coordinates in vision and robotics settings (Ostmeier et al., 2025; Schenck et al., 2025). GRAPE-M identifies RoPE as commuting rank-2 exponentials in SO(d) and extends it to learned subspaces and compact non-commuting mixtures in closed form and much faster way. Compared with LieRE, which parameterizes dense skew-symmetric generator and applies numerical matrix exponential (e.g., torch.matrix exp) with O(d3) time and O(d2) parameters per head, Multiplicative GRAPE decomposes the action into rank-2 subspaces and uses the closed-form Rodrigues-type formulas from Section 2.3, so we only need vectorvector operations with O(d) cost per head (a detailed comparison between LieRE and GRAPE is presented in Appendix E.) Additive position encoding and forgetting mechanisms. Additive schemes such as ALiBi (Press et al., 2021) and related kernelized/randomized forms (Chi et al., 2022a,b; Li et al., 2023; Ruoss et al., 2023) are captured exactly by GRAPE-A as unipotent actions in the general linear group GL that preserve the same relative law and streaming cacheability. Importantly, forgetting mechanisms are additive: the Forgetting Transformer (FoX) implements learnable per-head exponential decay in the attention logits and is specific GRAPE-A / GRAPE-AP instance imposing distance-dependent attenuation (Lin et al., 2025). FoXs data-dependent forget gates yield path-additive bias that we show is exactly the endpoint-independent GRAPE-AP case; see Appendix for constructive equivalence and its streaming implementation (Lin et al., 2025). Contextual position encoding. Content-adaptive position modulates effective phase or distance via token features through gating/scaling and algebraic parameterizations (Wu et al., 2020; Zheng et al., 2024; Kogkalidis et al., 2024), and contextual counting (CoPE) (Golovneva et al., 2024). GRAPE introduces phase-modulated and dictionary-based contextual variants that replace linear phase with cumulative token-adaptive phases (single or multi-subspace) while retaining exact headwise relativity and streaming caches. Finally, models can length-generalize without explicit encodings (NoPE) under suitable training (Wang et al., 2024), which corresponds to the trivial generator = 0 in our view."
        },
        {
            "title": "8 Conclusion",
            "content": "GRAPE provides general framework for positional encoding based on group actions, unifying multiplicative and additive mechanisms. Multiplicative GRAPE offers closed-form, rank-2 exponential that is relative, compositional, and norm-preserving; it recovers RoPE and yields learned-basis and non-commuting extensions at controlled cost. Additive GRAPE realizes ALiBi and FoX exactly via unipotent general linear group GL lifts with the same streaming/cache policy. The GRAPE framework integrates seamlessly with existing Transformer models and offers principled, extensible design space for future architectures."
        },
        {
            "title": "References",
            "content": "Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veliˇckovic. Round and round we go! what makes rotary positional encodings useful? In International Conference on Learning Representations (ICLR 2025), 2025. URL https://arxiv.org/abs/ 2410.06205. Also arXiv:2410.06205. 13 Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via position interpolation. arXiv preprint arXiv:2306.15595, 2023. URL https://arxiv.org/abs/2306.15595. Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. Kerple: Kernelized relative positional embedding for length extrapolation. Advances in Neural Information Processing Systems, 35:83868399, 2022a. Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer length extrapolation via the lens of receptive field analysis. arXiv preprint arXiv:2212.10356, 2022b. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Zihang Dai, Zhilin Yang, Yiming Yang, William Cohen, Ruslan Salakhutdinov, and Jaime Carbonell. Transformer-XL: Attentive language models beyond fixed-length context. In Proceedings of ACL, pages 29782988, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 41714186, 2019. Olga Golovneva, Jackie Lou, Daniel Holtmann-Rice, Aditya Kusupati, Chengeng Cai, Zijian Hu, Prateek Vijay Kumar, Tim Dettmers, Pratyusha Sharma, Behnam Neyshabur, Jason D. Lee, and Mohammad Bavarian. Contextual position encoding: Learning to count whats important. arXiv preprint arXiv:2405.18719, 2024. URL https://arxiv.org/abs/2405.18719. Brian Hall. Lie groups, lie algebras, and representations. In Quantum Theory for Mathematicians, pages 333366. Springer, 2013. Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without positional encodings still learn positional information. arXiv preprint arXiv:2203.16634, 2022. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020. Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In European Conference on Computer Vision, pages 289305. Springer, 2024. Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. In Proceedings of the 41st International Conference on Machine Learning (ICML 2024), volume 235, pages 2209922114. PMLR, 2024. Andrej Karpathy. NanoGPT. https://github.com/karpathy/nanoGPT, 2022. 14 Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. arXiv preprint arXiv:2006.15595, 2020. Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. SHAPE: Shifted absolute position embedding for transformers. In Proceedings of EMNLP, pages 33093321, 2021. Konstantinos Kogkalidis, Jean-Philippe Bernardy, and Vikas Garg. Algebraic positional encodings. Advances in Neural Information Processing Systems, 37:3482434845, 2024. Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions improves long context transformers. arXiv preprint arXiv:2310.04418, 2023. Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. CAPE: Encoding relative positions with continuous augmented positional embeddings. In Advances in Neural Information Processing Systems (NeurIPS), volume 34, pages 1607916092, 2021. Mengtian Lin, Ji Lin, Wei-Ming Chen, and Yonglong Tian. Forgetting transformer: Softmax attention with forget gate. arXiv preprint arXiv:2503.02130, 2025. URL https://arxiv.org/ abs/2503.02130. Xuanqing Liu, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Learning to encode position for transformer with continuous dynamical model. In International conference on machine learning, pages 63276335. PMLR, 2020. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. Masato Neishi and Naoki Yoshinaga. On the relation between position information and sentence length in neural machine translation. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 328338, 2019. Sophie Ostmeier, Brian Axelrod, Maya Varma, Michael Moseley, Akshay Chaudhari, and Curtis Langlotz. Liere: Lie rotational positional encodings. In Forty-second International Conference on Machine Learning, 2025. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. 15 Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Zhen Qin, Weixuan Sun, Kaiyue Lu, Hui Deng, Dongxu Li, Xiaodong Han, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Linearized relative positional encoding. arXiv preprint arXiv:2307.09270, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. Olinde Rodrigues. Des lois geometriques qui regissent les deplacemens dun syst`eme solide dans lespace, et de la variation des coordonnees provenant de ces deplacemens consideres independamment des causes qui peuvent les produire. Journal de Mathematiques Pures et Appliquees, 5:380440, 1840. Anian Ruoss, Gregoire Deletang, Tim Genewein, Jordi Grau-Moya, Robert Csordas, Mehdi Bennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. arXiv preprint arXiv:2305.16843, 2023. Connor Schenck, Isaac Reid, Mithun George Jacob, Alex Bewley, Joshua Ainslie, David Rendleman, Deepali Jain, Mohit Sharma, Avinava Dubey, Ayzaan Wahid, et al. Learning the ropes: Better 2d and 3d position encodings with string. arXiv preprint arXiv:2502.02562, 2025. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018. Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy, Joelle Pineau, Dieuwke Hupkes, and Adina Williams. The curious case of absolute position embeddings. arXiv preprint arXiv:2210.12574, 2022. Jianlin Su, Yuancheng Zhang, Shengfeng Pan, Shengyu Ge, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, (cid:32)Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 16 Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. Encoding word order in complex embeddings. In Proceedings of ICLR, 2020. Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and Jakob Grue Simonsen. On position embeddings in BERT. In Proceedings of ICLR, 2021. Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1402414040, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.834. URL https://aclanthology.org/2024.findings-acl.834/. Ulme Wennberg and Gustav Eje Henter. The case for translation-invariant self-attention in transformer-based language models. arXiv preprint arXiv:2106.01950, 2021. Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. Da-transformer: Distance-aware transformer. arXiv preprint arXiv:2010.06925, 2020. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Songlin Yang, Yikang Shen, Kaiyue Wen, Shawn Tan, Mayank Mishra, Liliang Ren, Rameswar Panda, and Yoon Kim. Path attention: Position encoding via accumulating householder transformations. arXiv preprint arXiv:2505.16381, 2025b. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, et al. Dape: Data-adaptive positional encoding for length extrapolation. Advances in Neural Information Processing Systems, 37:2665926700, 2024. Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400, 2023."
        },
        {
            "title": "Appendix",
            "content": "A Application in Multi-Head Attention Forgetting Transformer as Special Additive GRAPE Non-Commuting Multiplicative GRAPE Composition of Additive GRAPE and Multiplicative GRAPE Comparison with LieRE 2D and 3D GRAPE for Vision and Multimodal Position Encoding Algorithmic Details and Pseudo Code Differentiation and Fast Application of Rank-2 Matrix Exponential 19 20 21 22 22"
        },
        {
            "title": "25\nI.1 Rank-2 Plane: Exact Spectrum and Geometric Interpretation . . . . . . . . . . . . . 25\nI.2 Multi-subspace GRAPE-M and RoPE . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nI.3 Additive GRAPE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nI.4 Comparison to PaTH Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28",
            "content": "18 Table 3 Summary of Notation and Definitions. Symbol Definition GL(d) SO(d) SL(d) gl(d) so(d) exp() R2(θ) General Linear Group: The group of all invertible matrices. Special Orthogonal Group: The group of orthogonal matrices with determinant 1 (RR = I, det(R) = 1). Special Linear Group: The group of matrices with determinant 1. general linear algebra: The Lie algebra of GL(d), consisting of all matrices. special orthogonal algebra: The Lie algebra of SO(d), consisting of all skew-symmetric matrices (L = L). Exponential Map: map from Lie algebra (generator) to Lie group (operator). (cid:18)cos θ sin θ cos θ 2D Rotation Matrix: The matrix (cid:19) . sin θ G(n) Transpose (in SO(d)): For SO(d), the transpose is the group inverse (G = G1). G(n) Inverse Transpose (in GL(d)): The transpose of the matrix inverse, (G1). Unipotent Unipotent Transform: linear transformation whose eigenvalues are all 1. pu,h Positional Embedding/Representation: vector derived from token-local features, obtained via linear projection followed by RMS normalization. Application in Multi-Head Attention Building upon the algebraic foundation for relative encoding established in Section 2.5, this section details the concrete integration of the rotational map G(n) into the full Multi-Head Attention (MHA) architecture, covering the per-head formulation, streaming policy, and implementation complexity. Per-head formulation. Let be the number of heads and the per-head width. For head [H], let (qt,h, kt,h, vt,h) Rd denote the query/key/value at position t. GRAPE-M position map is realized as an orthogonal operator Gh,t SO(d) applied to (qt,h, kt,h): (cid:101)qt,h = Gh,t qt,h, (cid:101)kt,h = Gh,t kt,h, (cid:101)vt,h = vt,h."
        },
        {
            "title": "The headwise attention logits and outputs are then",
            "content": "ℓt,j,h = (cid:101)q t,h(cid:101)kj,h t,h = (cid:0)G h,tGh,j (cid:1)kj,h , yt,h = (cid:88) jt softmax(cid:0)ℓt,,h (cid:1) (cid:101)vj,h, (A.1) with the usual output projection applied after concatenation across heads. Exact relative law. If Gh,t arises from one-parameter subgroup Gh(n) = exp(n Lh) (commuting MS-GRAPE-M, including RoPE and learned commuting bases), then h,tGh,j = Gh(jt) = ℓt,j,h = t,hGh(jt) kj,h , 19 so logits depend only on the offset jt (exact origin invariance). Streaming cache. Applying the rotational map G(t) independently to each query and key vector is the core property that enables an efficient streaming cache policy. For any type where Gt is known at token arrival (non-contextual and phase-modulated), cache (cid:101)kj,h = Gh,jkj,h once and never rewrite it; at step t, compute (cid:101)qt,h = Gh,tqt,h and use logits ℓt,j,h = (cid:101)q t,h(cid:101)kj,h/ d."
        },
        {
            "title": "B Forgetting Transformer as a Special Additive GRAPE",
            "content": "The Forgetting Transformer (FoX) introduces scalar forget gate ft (0, 1] per head and timestep and adds the cumulative log-gate as an additive bias in the attention logits. Concretely, for head h, ft,h = σ(w f,hxt + bf,h), Fij,h = (cid:89) ℓ=j+ fℓ,h, Dij,h = log Fij,h = (cid:88) ℓ=j+1 log fℓ,h, and the attention is Oh = softmax (cid:16) 1 QK + Dh (cid:17) V. (FoX) We now show that Eq. (FoX) is exactly realized by our GRAPE-A framework using the endpoint-independent path-additive specialization of Section 5. FoX as GRAPE-AP with endpoint-independent edges. In GRAPE-AP (Section 5), head-wise additive logit bh(t, j) arises as causal path sum bh(t, j) = (cid:88) ℓ=j+1 ψh(t, ℓ). If the edge potentials do not depend on the endpoint, i.e. ψh(t, ℓ) aℓ,h, then bh(t, j) reduces to difference of per-time potentials: bh(t, j) = (cid:88) ℓ=j+1 aℓ,h = Ut,h Uj,h, Uu,h := (cid:88) ℓ<u aℓ,h. FoX corresponds to the choice aℓ,h = log fℓ,h 0, yielding bh(t, j) Dij,h = (cid:88) ℓ=j+1 log fℓ,h. Thus, the FoX forgetting bias Dh is precisely the GRAPE-AP path-integral additive bias with endpoint-independent edges. Unipotent GL lift (GRAPE-A view). Let := ed+2e For fixed head and endpoint t, define per-link unipotent factors d+1 be the rank-1 nilpotent used in Section 4.2. H(t) (ℓ) = + ψh(t, ℓ) E, ψh(t, ℓ) = log fℓ,h. Since E2 = 0, the path product collapses: (cid:89) ℓ=j+1 H(t) (ℓ) = + (cid:16) (cid:88) (cid:17) log fℓ,h = + Dij,h E. ℓ=j+1 Scoring in homogeneous coordinates as in Section 4 with the paired inverse-transpose, t,h (cid:101)kj,h = (cid:98)q (cid:101)q t,h (cid:16) + Dij,h (cid:17) (cid:98)kj,h = t,hkj,h + Dij,h, factor we include throughout). Hence recovers Eq. (FoX) exactly (up to the standard 1/ FoX is an exact GRAPE-A / GRAPE-AP instance realized by rank-1 unipotent path with endpoint-independent edges. Streaming and complexity. Compute prefix sums Ut,h = (cid:80) ℓ<t log fℓ,h once per step; then Dij,h = Ui,h Uj,h is obtained by subtraction, preserving the O(L) rowwise cost and the streaming cache policy from Section 4Section 5. The headwise gates ft,h add O(1) parameters and negligible computation. Special cases and composition. If ft,h eβh (constant per head), then Dij,h = βh(ij) and FoX reduces to exact ALiBi (Section 4.2). More generally, FoX composes additively with the multiplicative (orthogonal) GRAPE acting on (q, k) as in Eq. (5.3), preserving norm-preservation of the rotational part while adding bounded, non-positive, content-adaptive path biases. Non-Commuting Multiplicative GRAPE Consider the thin compression = ELrE with Rdr orthonormal and Lr so(r). Then σ(L) = σ(Lr) {0}dr, σ(cid:0) exp(nL)(cid:1) = σ(cid:0) exp(nLr)(cid:1) {1}dr. t=1 θtJ)T is the real-Schur form, then the nontrivial eigenvalues are {iθt}r/2 If Lr = T((cid:76)r/2 t=1 and einθt for the exponential. Thus, the expressive power of non-contextual non-commuting MS-GRAPE is fully captured by the r/2 mode angles {θt}; the ambient lifting via preserves the spectrum."
        },
        {
            "title": "D Composition of Additive GRAPE and Multiplicative GRAPE",
            "content": "For the unipotent forms of Additive GRAPE, applying Gadd(m) requires one inner product and one scalar-vector multiplication per active component. Thus, the per-head overhead is O(d) and typically negligible relative to attention matmuls. Multiplicative GRAPE (Section 3) and Additive GRAPE (Section 4) compose naturally, either additively at the logit level ℓt,j,h = 1 t,hGh(jt)kj,h + (cid:104) t,hGadd,h(jt)(cid:98)kj,h (cid:98)q t,hkj,h (cid:105) , or as single block-upper-triangular GL action in homogeneous coordinates. Concretely, define the joint lift (cid:98)q = [q; 1], (cid:98)k = [k; 1], (cid:98)G(m) = (cid:21) (cid:20)exp(m L) ω 0 1 GL(d+1), which combines the orthogonal rotation exp(mL) on features with unipotent translation along the homogeneous axis. Scoring with the paired inverse-transpose as in Eq. (4.2) yields (cid:98)q (cid:98)G(m)(cid:98)k+ = exp(mL)k ω uk + const, exactly reproducing the sum of multiplicative (rotary) and additive (bias) components up to softmax-invariant constant. In both formulations, exact relativity and streaming caches are retained."
        },
        {
            "title": "E Comparison with LieRE",
            "content": "Lie Rotational Position Encodings (LieRE) (Ostmeier et al., 2025) encode positional information by learning skew-symmetric generator in SO(d). The method then applies the matrix exponential of this generator to get rotational position map. For each attention head, the method learns one skew matrix. Its exponential gives dense orthogonal operator on queries and keys. Positions then match elements of one-parameter subgroup on the rotation manifold. This picture is compact Lie theoretic version of RoPE style encodings. Different heads can learn distinct rotational geometries and the map keeps the norm and an exact relative position law. Formally, for head the generator is Gh so(d). The positional map is (cid:55) exp(nωhGh)x. direct implementation has cost TLieRE(d) = Θ(d3) per head for the matrix exponential and needs Θ(d2) parameters and the same order of memory. Multiplicative GRAPE and LieRE both use rotations in SO(d) that come from skew-symmetric generators. LieRE gives each head dense or block skew matrix. It forms the positional operator with the full matrix exponential exp(G). This creates very rich rotations but needs O(d3) time for the exponential and O(d2) parameters and memory per head. GRAPE-M restricts the generator to sum of rank 2 planes and uses closed form Rodrigues-type formula for the exponential (Section 2). For one token, the positional mapping then reduces to few inner products and vector updates. So the cost is O(d) time and O(d) memory per head. This choice of parametrization has two main effects in practice. First, the GRAPE-M scale cleanly translates to contextual versions where frequencies or phases depend on the token content. The closed-form expression can be computed quickly for each token, and there is no large matrix exponential. In the LieRE setup, one needs new dense matrix exponential for each contentdependent generator. This step is much more costly and makes such contextual use harder to deploy in real models. Second, GRAPE gives single group-theoretic picture for multiplicative and additive mechanisms. The multiplicative part lives in SO(d) and additive or forgetting style terms (ALiBi, FoX, GRAPE-A, GRAPE-AP) come from unipotent actions in GL with the same relative law and the same streaming cacheability (Sections 4-5). LieRE only targets rotational encodings and does not model additive logit biases or forgetting terms. 2D and 3D GRAPE for Vision and Multimodal Position Encoding Extending GRAPE beyond one-dimensional token positions is easy. The construction only needs chosen group action on coordinates. For images with integer pixel coordinates (u, v) Z2 we pick two generators L(x) and L(y). token at (u, v) then gets the encoding G2D(u, v) = exp(cid:0)u ωxL(x)(cid:1) exp(cid:0)v ωyL(y)(cid:1) SO(d). The two generators act on 2D planes that can be disjoint in the base design. In that case, the map reduces to RoPE-style separable encoding. learned choice of planes inside Rd gives the GRAPE-M variant again. For 3D coordinates (u, v, w) that mark video space time tokens or point clouds, we follow the same pattern. We introduce three commuting generators and define G3D(u, v, w) = exp(cid:0)u ωxL(x)(cid:1) exp(cid:0)v ωyL(y)(cid:1) exp(cid:0)w ωzL(z)(cid:1). 22 In the non-commuting case, we use the thin Schur mode compression from Appendix C. The closed-form rank 2 matrix exponential from the main text still applies. The per token cost stays O(d) even for higher-dimensional coordinate spaces. On the additive side, GRAPE-A and GRAPE-AP handle 2D or 3D structures through the scalar offset m. The value can be any function of coordinate differences. For an image, we can take and this keeps the same algebraic template. For 3D settings, we can set = αx(ut uj) + αy(vt vj), = rt rj with rt and rj in R3. The update matrix then stays unipotent, and the exact relative composition law still holds. This gives clear way to impose axis-aligned or radial recency bias in vision and multimodal models."
        },
        {
            "title": "G Algorithmic Details and Pseudo Code",
            "content": "This appendix contains the detailed pseudocode. Algorithm 1 Commuting Multi-Subspace GRAPE-M Require: Q, RBLHd, orthogonal Rdd, frequencies {ωh,j}d/2 1: for = 1 to do 2: Q[:, :, h, :] Q[:, :, h, :] E; K[:, :, h, :] K[:, :, h, :] for ℓ = 0 to 1 do j=1, positions ZL for = 1 to d/2 do θ nℓ ωh,j; apply 2 2 rotation G2(θ) to coords (2j1, 2j) of Q[:, ℓ, h, :] and K[:, ℓ, h, :] end for 3: 4: 5: 6: 7: end for (cid:101)Q[:, :, h, :] E; 8: 9: end for 10: return ( (cid:101)Q, (cid:101)K) (cid:101)K[:, :, h, :] E 23 3: 4: 10: 11: 12: 13: 14: 15: 4: 9: 10: Algorithm 2 Fast Contextual Non-commuting MS-GRAPE-M via Schur-Mode Rotation Require: Q, RBLHd; planes {(ah,j, bh,j, ωh,j)}m 1: for = 1 to do 2: j=1; positions Build Uh = span{ah,j, bh,j}; orthonormalize bh Rdrh LU,h (cid:17) j=1 ωh,jL(ah,j, bh,j) bh so(rh) (cid:16) (cid:80)m one-time per head Orthogonally Schur-decompose: LU,h = Th Eh bhTh Rdrh; precompute (ch,t, sh,t) = (cos θh,t, sin θh,t) t=1 θh,tJ (cid:16)(cid:76)rh/2 (cid:17) 5: 6: end for 7: for ℓ = 0 to 1 do for = 1 to do 8: yQ Q[:, ℓ, h, :]; for = 1 to rh/2 do 9: yK K[:, ℓ, h, :] token loop (Ch,t, Sh,t) PhaseTo(nℓ; ch,t, sh,t) (C, S) from (cos θ, sin θ) via angle-addition or binary exponentiation Apply (cid:16) Ch,t Sh,t Sh,t Ch,t (cid:17) to coordinates (2t1, 2t) of yQ, yK end for (cid:101)Q[:, ℓ, h, :] Q[:, ℓ, h, :] + Eh(yQ (cid:101)K[:, ℓ, h, :] K[:, ℓ, h, :] + Eh(yK Q[:, ℓ, h, :]) K[:, ℓ, h, :]) end for 16: 17: end for 18: return ( (cid:101)Q, (cid:101)K) Algorithm 3 Additive GRAPE (GRAPE-A) with streaming cache Require: Q, RBLHd; per-head additive generators {Ah} with A2 1: Augment: (cid:98)Q [Q; 1; 0], (cid:98)K [K; 0; 1] as needed (Section 4.2) 2: for = 0 to L1 do 3: for = 1 to do (cid:98)K[:, j, h, :] (cid:0)I nj (cid:1) (cid:98)K[:, j, h, :] = 0; positions ZL cache once on arrival end for 5: 6: end for 7: for = 0 to L1 do 8: for = 1 to do (cid:101)Q[:, t, h, :] (cid:0)I + nt Ah Compute additive logits: λt,j,h (cid:101)Q[:, t, h, :] (cid:98)K[:, j, h, :] (cid:98)Q[:, t, h, :] (cid:1) end for 11: 12: end for 13: return {λt,j,h} (to be added to orthogonal GRAPE/RoPE logits) 24 Differentiation and Fast Application of Rank-2 Matrix Exponential Differentiation and stability. Let f1(z) = sin z2 with = nωs. Then and f2(z) = 1cos exp(nωL) = + f1(z)L + f2(z)L2. For any scalar parameter θ {ω} {entries of a, b}, θ exp(nωL) = f1(z) θL + f2(z) (L θL + θL L) + θz (cid:0)f 1(z)L + 2(z)L2(cid:1), θz = nω θs + ns θω, θs = 1 6 + O(z4) and f2(z) = 1 24 + O(z4). These formulas enable 2 s1θ(αβ γ2). 2 Use series for < ε: f1(z) = 1 z2 mixed-precision backprop with small-s guards. Fast application. For any Rd, Lx = ab, ba, x, L2x = γ(ab, + ba, x) β aa, α bb, x. Thus G(n)x = + f1Lx + f2L2x with f1 = sin(nωs) , which is evaluable in O(d) time via few inner products. By the minimal polynomial λ(λ2 + s2), L3 = s2L; expanding exp(ηL) and regrouping yields the rank-2 update form used throughout and f2 = 1cos(nωs) s"
        },
        {
            "title": "I Spectral Analysis of GRAPE and Other Methods",
            "content": "In this section, we discuss eigenvalue-level results for GRAPE-M generators/exponentials and summarize the unipotent spectra of GRAPE-A/GRAPE-AP. Throughout, L(a, b) = ab ba so(d), and α = a2, β = b2, γ = ab, = αβ γ2, = as in Section 2. I.1 Rank-2 Plane: Exact Spectrum and Geometric Interpretation Lemma I.1 (Rank-2 spectrum). For = L(a, b), the eigenvalues are {is} {0}d2, and there exists SO(d) such that BLB = (cid:20)sJ 0 (cid:21) , 0 0d2 = (cid:0) 0 1 1 0 (cid:1). Moreover, = ab sin ϕ, where ϕ [0, π] is the angle between and b. Proof. From Section 2, L2 = s2PU with = span{a, b}, whence the minimal polynomial is λ(λ2 + s2) and σ(U) = {is, 0}. Choosing an orthonormal basis aligned with yields the claimed form. Finally, = αβ γ2 = a2b2(1 cos2 ϕ) = (ab sin ϕ)2. Corollary I.2 (Phase bounds and orthogonality). The per-step rotation angle of exp(ηK) on equals θ = ηs and satisfies 0 θ ηab, with equality when b. If = (Section 2.4) and = 1, then = 1 and θ = η. Exponential spectrum. For any Z, σ(cid:0) exp(nL)(cid:1) = {eins} {1}d2. Hence ρ(exp(nL)) = 1, the map is unitary (orthogonal), and all Lyapunov exponents are zero. Periodicity holds with fundamental period = 2π/s when s/π Q; otherwise, the trajectory is quasi-periodic on the unit circle. 25 I.2 Multi-subspace GRAPE-M and RoPE Let = (cid:80)m Then j=1 θjLj with mutually orthogonal planes (hence [Li, Lj] = 0 for = j) and Lj = UjJU . (cid:77) BLB = θjJ 0d2m, σ(L) = {iθj}m j=1 {0}d2m, for some SO(d). Consequently, j=1 σ(cid:0) exp(nL)(cid:1) = {einθj }m j=1 {1}d2m. This recovers RoPE when the planes are the coordinate pairs and {θj} follow the canonical loguniform spectrum (Proposition 3.1). I.3 Additive GRAPE We now analyze the spectral properties of the additive lifts in GL introduced in Sections 4 and 5. The key structural fact is unipotency: all per-step factors are identity plus rank-1 (or few-rank) nilpotent update of index 2. Setup. Let gl(d+1) (or gl(d+2) for ALiBi) satisfy A2 = 0 as in (4.1) and (4.6). For scalar path parameter R, define the unipotent factor H(s) := exp(sA) = + A, H(s)1 = A, det H(s) = 1. For Additive GRAPE (GRAPE-A) with offset = ji, = ω; for GRAPE-PA, = sh(t, j) := (cid:80)t ℓ=j+1 ψh(t, ℓ) from Eq. (5.2). Proposition I.3 (Eigenvalues and Jordan structure of additive lifts). Let gl(D) satisfy A2 = 0 and = 0. Then for every = 0, σ(cid:0)H(s)(cid:1) = {1}D, (H(s) I)2 = 0, det H(s) = 1, ρ(H(s)) = 1. Hence, the minimal polynomial of H(s) is (λ 1)2, and the Jordan form consists of size-2 Jordan blocks for the 1-eigenspace, with the number of nontrivial blocks equal to rank(A). Proof. Since A2 = 0, exp(sA) = + sA and (H(s) I)2 = s2A2 = 0. The characteristic polynomial is (λ 1)D for H(s), so all eigenvalues equal 1. The determinant equals the product of eigenvalues, hence 1; the spectral radius is therefore 1. Dictionary closure. If {Ar}R r=1 satisfy A2 = 0 and ArAs = 0 for all r, s, then (cid:16) (cid:88) (cid:17) (cid:88) = θrAr A2 θ2 + r (cid:88) r=s θrθsArAs = 0, so the combined generator is also index-2 nilpotent and yields the same unipotent spectrum. Singular values. Although H(s) is not orthogonal, its deviation from is rank-limited and exactly analyzable. We first give sharp, explicit formula for the canonical rank-1 case (ALiBi block), then general bound. Lemma I.4 (Exact singular-value pair for canonical rank-1 unipotent). Let := ep with = and define H(s) := + sE RDD. Then 2 singular values equal 1, and the remaining two are σ(H(s)) = (cid:114) 1 + s2 2 1 + 4 , (cid:113) σ+(H(s)) σ(H(s)) = 1. (I.1) In particular, κ2(H(s)) = σ+(H(s))/σ(H(s)) = 1 + 2s + O(s2) as 0. Proof. The action of H(s)H(s) is identity on span{ep, eq}. In the basis {eq, ep} it equals (cid:0) 1+s2 whose eigenvalues are 1 + s2 (cid:112)det(H H) = det = 1. 4 . Taking square roots yields (I.1). The product equals 2 1 + s2 (cid:113) 1 (cid:1), Corollary I.5 (ALiBi and Additive GRAPE (GRAPE-A) conditioning numbers). For the exact ALiBi generator in Eq. (4.6), = ed+2e d+1 and = βh, so the only nontrivial singular values of Gadd,h(m) = + sE are given by Eq. (I.1). For the single-vector additive lift Eq. (4.1) with (cid:1) and = 1, the same formula holds with replaced by an orthogonally similar rank-1 = (cid:0) 0 0 0 update and = ω. Lemma I.6 (General operator-norm bounds for index-2 unipotents). For any with A2 = 0 and any R, 1 A2 σmin(I + sA) σmax(I + sA) 1 + A2. When rank(A) = 1 and A2 = 1, these bounds are tight and coincide with Lemma I.4 at first order in s. Proof. Use the triangle inequality (I + sA)x2 x2 + A2x2 and its reverse form applied to (I + sA)1 = sA; see also Weyl inequalities for singular values under rank-1 perturbations. Cancellation in the relative logit. While H(s) can be anisotropic (Lemma I.4), the Additive GRAPE (GRAPE-A) scoring uses paired inverse-transpose (Eq. (4.2)), which cancels all multiplicative distortions and yields pure additive term: (cid:101)kj = (cid:98)q (cid:101)q (cid:0)I + A(cid:1)(cid:0)I A(cid:1) (cid:98)kj = (cid:98)q (cid:0)I + (ij) A(cid:1) (cid:98)kj = (cid:98)q Gadd(ji)(cid:98)kj, since (A)2 = 0. This reproduces the exact relative law Eq. (4.3) and the closed form Eq. (4.4) (e.g. Eq. (4.5)), independently of σ(H(s)). GRAPE-AP as path-integral unipotent. Fix head and endpoint t. The per-row path product in Section 5 is (cid:89) ℓ=j+1 (cid:0)I + ψh(t, ℓ) E(cid:1) = + (cid:16) (cid:88) ℓ=j+1 (cid:17) ψh(t, ℓ) = + sh(t, j) E, because E2 = 0. Thus GRAPE-AP inherits the unipotent spectrum of Prop. I.3 with row-dependent Its only two nontrivial singular values are = sh(t, j) 0 (since ψh 0 by construction). exactly (I.1) with (cid:55) sh(t, j); the rest equal 1. Consequently, (cid:0)PA factor(cid:1) = κ2 σ+ σ (cid:0)sh(t, j)(cid:1) (cid:0)sh(t, j)(cid:1) = 1 + 2 sh(t, j) + O(cid:0)sh(t, j)2(cid:1), 27 while the determinant remains 1 and eigenvalues are all 1. As in Additive GRAPE (GRAPE-A), the paired inverse-transpose used in the bilinear scoring removes any multiplicative anisotropy, leaving the bounded additive term bh(t, j) in Eq. (5.2). Implications. Now we summarize the implications of previous results. For all s, H(s) is invertible with H(s)1 = sA; eigenvalues do not grow with offset length (spectral radius = 1). The operator norm grows at most linearly in (Lemma I.6) and is exactly characterized in the rank-1 canonical cases (Lemma I.4). Secondly, det H(s) = 1 implies no net volume change; any expansion along one direction is exactly balanced by contraction along its paired direction (product σ+σ = 1). Despite anisotropy, the GRAPE-A and GRAPE-AP logits remain exactly relative because the key transform uses H(s), algebraically eliminating multiplicative distortion and yielding the closed-form additive bias (Eqs. (4.3), (4.4), (5.2)). I.4 Comparison to PaTH Attention PaTH Attention (Yang et al., 2025b) proposes contextual multiplicative position map given by cumulative product of identity-plus-rank-one matrices Ht = βt wtw , βt (0, 2), applied along the path between key position and query position as (cid:81)i s=j+1 Hs (see Section 2 of the PaTH paper). In contrast to GRAPE-M factors, each Ht is not orthogonal unless βt {0, 2}. This has immediate spectral consequences. Per-step spectrum. Since Ht is symmetric rank-1 perturbation of the identity with projector Pt := wtw , wt2 = 1, σ(Ht) = { 1 βt, 1, . . . , 1 (cid:124) (cid:123)(cid:122) (cid:125) d1 }, det(Ht) = 1 βt, Ht2 = max{1, 1 βt} = 1. Thus Ht is norm nonexpansive (operator norm 1) but not norm-preserving unless βt {0, 2}. Singular values equal the absolute eigenvalues because Ht is symmetric; the component along wt is scaled by 1 βt < 1 for any βt (0, 2) {0, 2}, and flips sign when βt > 1 (a design choice in PaTH to allow negative eigenvalues for state-tracking). Path product is contractive and near-singular. Let Pji = (cid:81)i singular values gives s=j+1 Hs. Submultiplicativity of σmax(Pji) (cid:89) s=j+1 Hs2 = 1, σmin(Pji) (cid:89) s=j+1 σmin(Hs) = (cid:89) s=j+1 1 βs. Hence Pji is (at best) nonexpansive, with worst-case exponential lower bound on the smallest singular value governed by the path-length product of 1 βs. Whenever some βs is close to 1, Hs is nearly singular (and exactly singular if βs = 1), driving σmin(Pji) toward zero. Volume contraction is quantified by det(Pji) = (cid:89) (1 βs), s=j+1 which typically decays exponentially in unless βs concentrates at the orthogonal endpoints {0, 2}. Aligned-plane special case. If the directions are time-invariant, ws w, then Pt = ww is an idempotent projector and the factors commute: (cid:89) Hs = (cid:89) (cid:0)I βsP(cid:1) = (cid:16) 1 (cid:89) (1 βs) (cid:17) P, s=j+ s=j+1 s=j+1 s=j+1(1 βs), making the contraction along explicit and so the eigenvalue along is exactly (cid:81)i exponential in path length unless βs {0, 2}. Implications for long-context modeling. Because the PaTH transport multiplies the Q/K bilinear by Pji, any persistent deviation of βt from {0, 2} yields cumulative energy loss along moving one-dimensional subspace. This concentrates mass in progressively fewer directions and can flatten or attenuate long-range logits Pjikj as grows, unless additional renormalizations or forget-gates are introduced. In contrast, GRAPE-M maps lie in SO(d), so for both non-contextual and contextual types, all singular values are 1; volumes and norms are preserved, and Lyapunov exponents are 0, avoiding contraction-induced degradation of long-range interactions. Lemma I.7 (Orthogonality condition for PaTH factors). For Ht = βtwtw with wt = 1, Ht is orthogonal iff βt {0, 2}. For βt (0, 2) {0, 2}, Ht is symmetric, diagonalizable with eigenvalues in (1, 1] {1}, and strictly contractive on span{wt}."
        }
    ],
    "affiliations": [
        "IIIS, Tsinghua University",
        "Princeton University",
        "University of California, Los Angeles"
    ]
}