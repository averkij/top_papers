{
    "paper_title": "WideSeek: Advancing Wide Research via Multi-Agent Scaling",
    "authors": [
        "Ziyang Huang",
        "Haolin Ren",
        "Xiaowei Yuan",
        "Jiawei Wang",
        "Zhongtao Jiang",
        "Kun Xu",
        "Shizhu He",
        "Jun Zhao",
        "Kang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm."
        },
        {
            "title": "Start",
            "content": "WideSeek: Advancing Wide Research via Multi-Agent Scaling Ziyang Huang * 1 2 Haolin Ren * 1 2 Xiaowei Yuan 1 2 Jiawei Wang 3 Zhongtao Jiang Kun Xu Shizhu He 1 2 Jun Zhao 1 2 Kang Liu 1 2 https://wideseek-ai.github.io 6 2 0 2 2 ] . [ 1 6 3 6 2 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Search intelligence is evolving from Deep Research to Wide Research, paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, General Broad Information Seeking (GBIS) benchmark constructed via rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is promising direction for advancing the Wide Research paradigm. 1. Introduction Search Intelligence constitutes the cornerstone of Agentic AI (Shi et al., 2025; Abou Ali et al., 2025). Moving beyond mere substitute for conventional search engines, it serves as an essential module for complex, real-world applications, including repository-level code generation (Jimenez et al., 2024), enterprise data intelligence (Lei et al., 2025), and Equal contribution. 1Institute of Automation, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3University of Science and Technology of China. Corresponding author: Kang Liu . Preprint. Emali: huangzy0312@gmail.com 1 Figure 1. Deep Research paradigm vs. Wide Research paradigm. general GUI manipulation (Xie et al., 2024). Existing research has predominantly focused on Deep Research (Wei et al., 2025), which employs complex, multistep reasoning and action sequences to locate single hard-to-find piece of information. As AI enters its Second Half (Yao, 2025), the research community is increasingly shifting its focus toward real-world and utility scenarios. This transition necessitates move toward Wide Research (Manus, 2025), as shown in Figure 1, which replaces sequential reasoning with parallel orchestration paradigm. By prioritizing high-breadth synthesis and structural comprehensiveness, Wide Research enhances productivity and scales the effectiveness of industrial AI deployment. Wide Research focuses on systematic retrieval across expansive search spaces, transitioning from deep-but-narrow chains to high-breadth parallelized frameworks. Aligning with Kimi Agent-Swarm (Moonshot AI, 2026), this WideSeek: Advancing Wide Research via Multi-Agent Scaling paradigm employs sophisticated orchestrator to decompose complex global objectives into granular, parallel subtasks, which are then concurrently executed by autonomous agents capable of iterative deep research and mutual crossvalidation. representative application is the generation of Competitor Analysis Tables, as exemplified by systems such as Manus (Manus, 2025), which synthesize information from thousands of sources into comprehensive comparative tables, substantially reducing labor costs of Human Data Analyst while enhancing productivity at scale. Despite its promise, the advancement of Wide Research is hindered by three primary challenges: (1) Limitations in Benchmarks: Existing benchmarks (Wong et al., 2025; Lan et al., 2025) are largely constructed by human experts, which limits their scale, diversity, and categorization depth. Furthermore, they typically provide only test sets, lacking the training data necessary for model optimization; (2) Deficiencies in Data Synthesis: Current data synthesis methods for search agents focus on sampling complex graph topologies to simulate multi-step reasoning paths (Li et al., 2025; Tao et al., 2025). While these approaches effectively optimize for search depth, they lack the capacity to efficiently synthesize large scale of atomic information under complex constraint, which is critical for search width; and (3) Optimization Gaps: Previous approaches often rely on closed-source models within static multi-agent frameworks (Roucher et al., 2025), or concentrates on enhancing the depth of single-agent reasoning (Lu et al., 2025). There is notable lack of exploration into the end-to-end optimization of systems capable of autonomously broadening their search paths. To address these challenges, we investigate the Wide Research paradigm through two perspectives: data pipeline construction and agent optimization. Data Pipeline & Benchmark. While conventional methods construct information graphs from web pages to emulate reasoning paths toward single answer, our approach utilizes large-scale Knowledge Graphs (KGs) (Schmelzeisen et al., 2021) to extract clusters of interconnected world knowledge. Specifically, we initialize the process with seed entities and set of sampled seed constraints. By applying formal set operations (including intersection, union, and difference), we construct complex constraints that resolve into target entity set. Simultaneously, we sample high-coverage attributes of these entities to define the target attribute set. Next, we fetch all atomic information from Knowledge Graph to form the answer table and construct the input task based on the complex constraints. For convenient evaluation, this pipeline produces column-wise rubrics for reward system. To ensure the quality of data, all tasks will be evaluated by hybrid filtering system. Based on this pipeline, we introduce WideSeekBench, benchmark for General Broad Information Seeking (GBIS) comprising both training and test sets. To ensure rigorous and multi-dimensional evaluation, the test set is strictly sampled and balanced across target information volume, operator complexity, and domains. Agent Optimization. The Wide Research paradigm requires agents to acquire and synthesize target information from large volume of sources. This necessitates reasoning architecture that supports both parallel and serial execution, typically involving ultra-long-context reasoning and extensive tool invocation. To expand the search scope, enable robust cross-validation, and reduce execution complexity, we propose WideSeek, system built on dynamic multi-agent architecture. Following Planner-Executor pattern, the main agent is responsible for planning, task decomposition, and self-reflection, while sub-agents reason and execute tool calls to complete the sub-task. In contrast to previous methods that pre-define the roles and quantity of agents, which often degenerate into rigid workflows, WideSeek empowers the main agent with complete autonomy. It allows the system to dynamically instantiate any number of sub-agents at any step based on task requirements. Building on this flexible architecture, we collect all trajectories of the main agent and sub-agents and linearize them into unified trajectory. Based on this, we optimize the system using end-to-end Reinforcement Learning (RL). In conclusion, our experiments and analysis demonstrate that the transition from Deep to Wide Research requires fundamental shift in agentic design, transitioning from sequential to dynamic, parallel orchestration. Moreover, our work not only establishes rigorous benchmark for the field but also provides compelling evidence that specialized end-to-end multi-agent optimization can enable models to search at scale in complex scenarios. 2. Data Pipeline & Benchmark In contrast to Deep Research, Wide Research represents an application that is more closely aligned with real-world productivity scenarios. It aims to retrieve collection of relevant information that satisfies complex constraints. Specifically, we can compile all relevant information into table for comparative analysis. We define this task as General Broad Information Seeking (GBIS). To systematically evaluate models Wide Research capabilities and to further investigate how post-training can enhance these capabilities in base models, we propose rigorous multi-stage data pipeline and thus construct the WideSeekBench. 2.1. Task Definition We define the GBIS task over universe of entities within world knowledge space W. task instance is formally defined as tuple = (Q, A), where is WideSeek: Advancing Wide Research via Multi-Agent Scaling Figure 2. The data pipeline of WideSeekBench construction, which mines set of target information under complex constraints. , vi,j = Value(ei, aj) (1) Φ(e) := c(e) Φ(e) Φ1(e)Φ2(e) Φ1(e)Φ2(e) (2) task query encoding complex semantic constraint, and = {a1, a2, . . . , am} is the set of required attributes. The query maps to latent semantic filter function Φ : {0, 1}. The objective is to construct ground truth table corresponding to the target entity set = {e Φ(e) = 1}. Formally, is table of size m: = v1,1 ... vE,1 . . . v1,m ... vE,m GBIS requires the agent to comprehensively synthesize T. This requires not only the precision of the search but also the recall. 2.2. Data Pipeline We employ multi-phase approach on knowledge graph to synthesize complete benchmark instances of the form (Q, A, T, R), where denotes the evaluation rubrics. We provide more details in Appendix A. Phase 1: Seed Constraint Construction. To ensure comprehensive coverage and diversity, we adopt top-down sampling strategy. (a) Domain Definition & Sampling: We start with human-defined set of high-level domains Ddomain (e.g., Education, Sports). From each high-level domain, we sample specific sub-domains Dsub (e.g., University, Basketball). (b) Seed Sampling: Within each sub-domain, we sample seed entities eseed and extract their relations (triples) 3 Rseed = {(eseed, p, v)} from K. This process yields diverse pool of atomic constraints C(eseed) atom = {(p, v)} associated with each seed entity. Phase 2: Logical Composition & Schema Extension. We compose atomic constraints into complex constraints and extend the attribute schema. (a) Logical Composition: Using operators = {, , }, we recursively define the composite filter Φ as: where c() denotes boolean predicate induced by an atomic constraint (p, v) C(eseed) atom , and c(e) = 1 if entity satisfies property with value v. We execute Φ over to retrieve the target entity set E. (b) Schema Extension: Given the validated entity set E, we construct candidate attribute set Acand = (cid:83) eE Attributes(e), from which we select target attributes Acand by enforcing entity coverage and sufficient value diversity, and retrieve all corresponding values to populate T. This phase yields approximately 30,000 candidate tasks. Phase 3: Agent Task Synthesis. This phase converts complex constraints and target attributes into user-facing tasks using LLMs. (a) Self-Refining Query Synthesis: We treat query generation as an iterative, self-refining process. An LLM generator Mgen converts Φ into query Q, while LLM verifier Mver extracts logic ˆΦ back from Q. Discrepancies ( ˆΦ Φ) trigger feedback loops for Mgen to regenerate until consistency is achieved. The consistency is also evaluated by Mver. (b) Column-wise Rubric GeneraWideSeek: Advancing Wide Research via Multi-Agent Scaling tion: For each attribute aj, we generate specific evaluation rubric Rj based on column semantics and cell values ,j, defining acceptance criteria for formats and tolerances. This phase yields approximately 15,000 candidate tasks. Phase 4: Multi-Stage Filtering. To ensure high quality, we apply three-level filtering protocol: (a) Rule-based Filter: We perform web searches to discard tasks where entities in are not grounded in web page. Moreover, we discard tasks where some cells lack natural language descriptions or is sparse (> 50% empty cells). (b) LLMbased Filter: An LLM scores tasks against five dimensions: Human-Likeness, Solvability, Common Sense, Temporal Stability, and Rubric Rationality. final task passes all these standards. (c) Human Verification: final manual review removes subtle semantic irrationalities. This phase yields 5156 final tasks. 2.3. WideSeekBench We introduce WideSeekBench, comprehensive benchmark designed to evaluate Wide Research capabilities. The dataset comprises total of 5,156 tasks, which are partitioned into training set of 4,436 tasks Dtrain and heldout test set of 720 tasks Dtest. The comparison of different search agent benchmarks is shown in Table 3. To enable fine-grained evaluation, we meticulously controlled the distribution of the test set. This allows for multi-dimensional task classification and detailed analysis. Specifically, the test tasks are categorized based on three distinct dimensions: (1) Volume of Target Information: We quantify the volume based on the total number of cells in the ground truth table. Based on this, tasks are divided into 10 distinct intervals to assess performance across varying information volume. The specific distribution is illustrated in Figure 7b. (2) Constraint Complexity: To evaluate how agents handle complex tasks, we classify the tasks into 7 types based on the nature of the constraints involved. The distribution of these constraint types is presented in Table 7. (3) Domain Diversity: We categorize the tasks into 18 distinct domains to ensure broad topical coverage. The domain-wise distribution is shown in Figure 7d. Furthermore, we ensure that all entities in ground truth tables correspond to existing real-world web pages via search. To guarantee fair, transparent, and reproducible evaluation, we constructed standalone Simulated Environment. This environment includes local document corpus and local search engine. Detailed specifications of the simulated environment are provided in the Appendix A.8. Following WideSearch (Wong et al., 2025), we use Success Rate, Row F1, and Item F1 as the evaluation metrics. We show the details of evaluation in Appendix A.9. 3. WideSeek Given task (Q, A), the objective is to retrieve related information to construct structured table ˆT containing set of entities ˆE = {e1, e2, . . . , eN } and their corresponding attribute values v( ˆE, A), satisfying complex semantic constraint Φ derived from Q. To address the complexity of this task, which often exceeds the context and reasoning limits of single serial trajectory, we propose WideSeek. WideSeek operates as dynamic, hierarchical multi-agent system governed by unified policy πθ. 3.1. Multi-Agent Rollout The inference process, as shown in the left of Figure 3, is modeled as hierarchical Markov Decision Process (MDP) (Luo et al., 2025). Unlike static multi-agent architectures with fixed roles, WideSeek employs centralized Main Agent (Planner) that dynamically forks variable instances of Sub-Agents (Executors) at any step. Hierarchical State Transition. At the top level, the Main Agent operates at time steps t. Let smain denote the global state, encompassing the user query and the history of highlevel thoughts and sub-results. The Main Agents policy πθ(amain ) selects an action amain from hierarchit cal action space = Aplanning Atermination. smain t Aplanning, If amain the agent invokes the function create sub agent(q(1) sub, . . . , q(k) sub). This action triggers the parallel instantiation of Sub-Agents, where is dynamically determined by the policy rather than hyperparameter. Each Sub-Agent (j {1, . . . , k}) operates in its own local MDP defined by the sub-task q(j) sub. It generates trajectory (j) 0, sj 1 . . . )1 using the same unified policy πθ, utilizing atomic search tools (e.g., search, open page). Each action execution receives an observation oj from the environment and updates the sub-agent state: sj t+1 sj . Upon completion, the sub-agent returns textual sub-result rj, which updates the global state: smain {r1, . . . , rk}. If amain t+1 smain Atermination, the agent synthesizes the accumulated information in smain to produce the final answer Tans and terminates the rollout. sub = (sj oj 0, aj This hierarchical execution generates composite trajectory that interleaves the planners reasoning traces with the execution traces of all dynamically created sub-agents. 3.2. Cold Start Given the complexity of the task, we distill high-quality trajectories from multiple teacher models and fine-tune the policy via SFT (Supervised Fine-Tuning). Further details are provided in the Appendix B.1. 1We reuse to represent trajectories. 4 WideSeek: Advancing Wide Research via Multi-Agent Scaling Figure 3. An illustration of Multi-Agent Reinforcement Learning. As shown on the left, the main agent can fork any number of sub-agents at any step. The trajectories of the main agent and sub-agents are unified for RL training. 3.3. Multi-Agent Reinforcement Learning Standard single-agent RL optimizes sequential trajectory. However, WideSeeks execution graph is dynamic tree structure. We propose Unified Multi-Agent RL framework that models the entire system as single generative process optimized via Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Unified Trajectory Modeling. We model the multi-agent interaction as unified joint distribution. Since all agents share the same LLM checkpoint πθ, we linearize the hierarchical execution trace into single sequence. First, we define the trajectory of the j-th Sub-Agent forked at the Main Agents time step as complete sequence of local state-action pairs: define comprehensive global reward R(T ) that serves as the sparse training signal. The reward is composed of correctness score based on Item-F1 and penalty for format violations. To discourage structural degradation, we impose format penalty. Let nerr be the total count of format errors (e.g., invalid tool calls) in trajectory , and Nmax be predefined maximum tolerance for errors. The final reward function is defined as: R(T ) = Item-F1(Tans, T) λ (cid:19) (cid:18) nerr Nmax (cid:124) (cid:125) (cid:123)(cid:122) Format Penalty (5) (cid:104) (t,j) sub = (st,j 0 , at,j 0 ), (st,j 1 , at,j 1 ), . . . , (st,j (cid:105) , rt,j) where λ is balancing coefficient. This ensures that the agent is penalized proportionally to the frequency of format hallucinations relative to the tolerance threshold. (3) The global unified trajectory is then constructed by interleaving each Main Agent step (smain ) with the set of trajectories from all Kt Sub-Agents forked at that step: , amain (cid:34) = (smain , amain 0 ), K0(cid:91) j=1 (0,j) sub , . . . , (smain , amain ), Kt(cid:91) (t,j) sub , . . . , (smain (cid:35) (4) , ) j=1 (cid:124) (cid:123)(cid:122) Executors at step (cid:125) Optimization via Unified GRPO. We optimize πθ to maximize the expected reward of the unified trajectory. For each query Q, we sample group of unified trajectories {T 1, . . . , G}. The Global GRPO objective is formally defined as: (θ) = EQD,{T g}πθold (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) g=1 1 g g (cid:88) u=1 1 au,k (cid:16) min au,k (cid:88) k=1 ρg,u,k ˆAg, clip(ρg,u,k, 1 ϵ, 1 + ϵ) ˆAg (cid:35) (cid:17) (6) Reward Function Design. To guide the policy toward both accurate information retrieval and robust tool usage, we Here, indexes the action tokens generated by the model across the each step in linearized unified trajectory g, cov5 WideSeek: Advancing Wide Research via Multi-Agent Scaling Table 1. Experiment results on WideSeekBench. We run each task for 4 times. Success Rate (%) Row F1 Score (%) Item F1 Score (%) Pass@4 Mean@ Max@4 Mean@4 Max@4 # Sub-Agents # Tool Calls 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.14 0.00 4.45 4.11 4.34 3.17 3.44 6.75 6.75 6.85 5.86 5.92 21.03 20.44 20.51 17.48 17.88 26.88 27.88 27.09 25.19 25.23 11.21 6.02 31.25 8.74 7. 408.64 121.36 326.41 85.36 88.36 0.53 1.26 1.09 (+0.56) 1.74 (+1.21) 1.95 (+1.42) 1.51 3.00 2.59 (+1.08) 3.66 (+2.15) 3.88 (+2.37) 7.37 10.11 10.86 (+3.49) 11.35 (+3.98) 12.87 (+5.50) 12.71 16.51 16.61 (+3.90) 18.92 (+6.21) 19.73 (+7.02) 4.18 7.53 9.57 (2.29) 13.16 (3.15) 26.60 (6.36) 9.50 17.15 41.09 (4.33) 121.98 (12.84) 273.75 (28.82)"
        },
        {
            "title": "Model",
            "content": "Proprietary Models GPT-5.2 GPT-5.1 DeepSeek-v3.2 Kimi-K2-Thinking Seed-1.8 Open-Sourced Models Qwen3-8B-Thinking Qwen3-30B-A3B-Thinking WideSeek-8B-RL WideSeek-8B-SFT WideSeek-8B-SFT-RL ering both Main Agent planning steps and Sub-Agent execution steps. The term ρg,u,k = πθ(au,ksu,au,<k) πθold (au,ksu,k,au,<k) represents the importance sampling ratio for the k-th token in the u-th action. The group-relative advantage ˆAg is computed using the global reward R(T g) as ˆAg = (R(T g)µR)/σR, where µR and σR are the mean and standard deviation of rewards within the sampled group, respectively. 4. Experiment 4.1. Setting We test proprietary models and open-sourced models on WideSeekBench. We use Qwen3-8B (Yang et al., 2025) as the base for agent optimization. For more training settings, we show in Appendix B.3. To test the generalization to the Deep Research dataset, we test the agent on Browsecompplus (Chen et al., 2025). We also show WideSeek trajectory example in Appendix B.4 for better understanding. 4.2. Main Results Scalability Gaps. As shown in Table 1, current state-of-theart proprietary models, including GPT-5.2, exhibit limited success on the challenging WideSeekBench, with Mean@4 Item-F1 remaining only 21.03. This underscores the difficulty of conducting search at scale. Moreover, distinct behavioral gap exists between proprietary and open-sourced models. Proprietary models spontaneously instantiate more sub-agents (e.g., DeepSeek-v3.2 forks 31.25) and execute significantly more tool calls (e.g., GPT-5.2 executes 408). This suggests that while current frontier models possess the potential for parallel task orchestration, they fail to effectively coordinate these actions to satisfy complex, highbreadth constraints without specialized optimization. Efficacy of WideSeek Optimization. We analyze the impact of our optimization method on the Qwen3-8B-Thinking as presented in Table 1. Distilling high-quality trajectoTable 2. Browsecomp-Plus performance. We test the generalization of WideSeek to Deep Research dataset."
        },
        {
            "title": "Model",
            "content": "Gemini-2.5-Pro GPT-OSS-120B-Low DeepSeek-R1-0528 Search-R1-32B Qwen3-32B"
        },
        {
            "title": "Acc",
            "content": "29.52 25.54 16.39 11.08 10.72 WideSeek Qwen3-30B-A3B WideSeek Qwen3-8B WideSeek-8B-SFT WideSeek WideSeek-8B-SFT-RL WideSeek WideSeek WideSeek-8B-RL 14.82 14.22 23.61 23.61 26.42 (+12.20) ries via SFT results in strong performance boost, with WideSeek-8B-SFT achieving 12.84 increase in tool usage and 3.15 increase in sub-agent instantiation compared to the base model, indicating successful learning of multi-agent scaling. Further end-to-end optimization via RL yields the highest performance, where WideSeek-8BSFT-RL achieves an Item F1 score of 12.87% (+5.50% over base) and Max Row F1 of 3.88%. The system learns to scale its search effort aggressively, increasing tool calls by factor of 28.82 and sub-agents by 6.36. RL from scratch (WideSeek-RL) also learns to scale the number of sub-agents and tool calls, thus yielding better performance. While performance gains are substantial, they remain bounded by the 8B parameter size, suggesting that the reasoning bottleneck persists even with extensive retrieval. Additionally, Figure 9 illustrates the training dynamics, revealing strong correlation between the rising reward curve and increasing tool calls, confirming that the model discovers broader information seeking as the optimal policy. Generalization to Deep Research. To assess whether the capabilities transfer to deep research tasks, we evaluate our WideSeek: Advancing Wide Research via Multi-Agent Scaling Figure 4. Item-F1 score, the number of sub-agents, and the number of tool calls on different task sets with different volume of target information. models on the BrowseComp-Plus  (Table 2)  . Even without any training, the WideSeek scaffold provides structural advantage; the base Qwen3-8B utilizing WideSeeks dynamic multi-agent framework (14.22%) outperforms significantly larger models like Qwen3-32B (10.72%) that rely on ReAct. This suggests that decomposing complex queries into parallel sub-tasks effectively mitigates the context management burden. Furthermore, training on WideSeekBench confers robust generalization capabilities, with WideSeek-8B-RL achieving an accuracy of 26.42%, +12.20% improvement over the base model. Despite being trained solely on wide research tasks, the agents ability transfers effectively to deep research tasks. 5. Analysis WideSeekBench facilitates granular evaluation of agent capabilities through multi-dimensional task classification. Overall, our experimental results indicate that multi-agent RL consistently enhances performance across all analyzed dimensions, demonstrating the robustness of our method. Volume of Target Information. We categorize tasks based on the total count of atomic information in the ground truth table, ranging from small-scale intervals ([4, 16]) to massivescale intervals ([2048, 4096]). As shown in Figure 4, across all intervals, consistent performance hierarchy is observed: WideSeek-8B-SFT-RL > WideSeek-8B-SFT > WideSeek8B-RL. In the lower volume range ([4, 128]), performance gaps are minimal as the retrieval load remains manageable. However, in the range of [128, 4096], performance significantly degrades as the volume increases, confirming that massive-scale information seeking remains formidable challenge. Notably, in the extreme interval ([2048, 4096]), both WideSeek-8B-SFT and WideSeek-8B-SFT-RL exhibit counter-intuitive drop in tool call frequency alongside low success rates. This phenomenon suggests an early stopping behavior, likely stemming from the refusal tendencies distilled from the teacher model (frontier LLMs), which often assess such high-volume tasks as infeasible and reject them. Conversely, the WideSeek-8B-RL model, trained from scratch without SFT initialization, does not exhibit this bias; instead, its tool usage scales positively with atomic information volume, indicating that the agent has autonomously learned to deploy more extensive search actions to maximize recall in data-heavy scenarios. Figure 5. Item-F1 score on different constraint types. 7 WideSeek: Advancing Wide Research via Multi-Agent Scaling Constraint Type. We classify tasks into seven distinct logical constraint types corresponding to set operations in SPARQL (e.g., AND, OR, NOT), which represent the logic required to filter information sets (see Appendix A.4). As illustrated in Figure 5, our analysis reveals that models generally achieve higher performance on OR type constraints. This is likely because disjunctive logic inherently aligns with parallel execution, allowing the system to easily decompose the query into independent sub-agents for concurrent search. In contrast, the NOT constraint type yields the lowest performance. Furthermore, compounding other constraints with negation (e.g., OR NOT) invariably leads to significant performance drops. This highlights that set difference operations (requiring the agent to exclude specific entity set from the results) constitute distinct reasoning bottleneck for current search agents. et al., 2025; Team et al., 2025). Early synthesis efforts predominantly adopted an information-driven paradigm, focusing on simulating web navigation paths. For instance, WebWalkerQA (Wu et al., 2025b) constructs linear information chains to emulate human browsing, while WebDancer (Wu et al., 2025a) and WebSailor (Li et al., 2025) leverage external information aggregation and entity coreference networks to generate complex QA pairs. However, these methods primarily optimize for search depth, focusing on the retrieval of specific reasoning paths to reach single answer. To enhance structural consistency and logical rigour, formalization-driven synthesis has gained attention, especially in the mathematical domain (Xin et al., 2024; Ren et al., 2025) and the knowledge base question answering domain (Xia et al., 2025). Most recently, WebShaper (Tao et al., 2025) pioneered the use of set-theoretic constructs (Knowledge Projections) to model informationseeking tasks. However, WebShaper still focuses on augmenting the reasoning structure to handle complex multistep depth. In contrast, our work introduces formalization grounded in set theory specifically designed for search width. Unlike path-based or reasoning-oriented methods, we use Knowledge Graphs to extract clusters of interconnected world knowledge and define target entity sets within expansive search spaces using set operators. This allows us to precisely regulate task breadth and constraint complexity, addressing the Wide Research requirements that traditional information-driven (Wu et al., 2025b; Li et al., 2025) or depth-oriented formalization (Tao et al., 2025) paradigms do not fully cover. Figure 6. Item-F1 score on different domains. 6.2. LLM-based Multi-Agent Reinforcement Learning Domain. We evaluate agent performance across 18 distinct domains. As shown in Figure 6, the results demonstrate that our agent optimization strategy yields robust improvements universally, maintaining the trend WideSeek-8B-SFT-RL > WideSeek-8B-SFT > WideSeek-8B-RL across all categories. This validates the effectiveness of our method in enabling models to learn superior multi-agent coordination strategies during exploration to retrieve more comprehensive information. Simultaneously, the models exhibit consistent domain sensitivity; for instance, performance is notably higher in Infrastructure compared to Education & Academia. 6. Related Work 6.1. Data Synthesis for Search Agent The training of search agents has shifted towards highquality synthetic data to overcome the scale and diversity limits of human-curated benchmarks (Li et al., 2025; Tao Traditional Large Language Model (LLM)-based multiagent systems primarily rely on static, heuristic-driven architectures with pre-defined roles, often lacking parameterlevel optimization for specific collaborative tasks (Qian et al., 2024; Hong et al., 2023). Recently, the research community has shifted toward cooperative MARL to enable more effective coordination. For instance, MAGRPO (Liu et al., 2025) introduces multi-agent group relative policy optimization to fine-tune multiple LLMs for writing and coding tasks, moving beyond individual rewards toward collective efficiency. Similarly, the Optimized Workforce Learning (OWL) framework (Hu et al., 2025) utilizes reinforcement learning to optimize domain-agnostic planner for complex task decomposition. While these works demonstrate the potential of RL in multi-agent coordination, they either focus on general-purpose cooperation or decouple planning from execution to maintain transferability, often leaving the specialized executors as black-box modules. MGRPO (Hong et al., 2025) and Fold-GRPO (Sun et al., 2025) use the branch-return paradigm, but they usually fork fixed 8 WideSeek: Advancing Wide Research via Multi-Agent Scaling number of sub-agents (i.e., 1) for sub-tasks execution at each step of the main agent. The industry has also seen the emergence of advanced agentic products, such as Kimi K2.5 Agent Swarm (Moonshot AI, 2026), which achieves impressive performance by optimizing the orchestrator while treating sub-agents as static parameters. However, such orchestration-only optimization may limit the systems ability to refine the interaction granularity between the planner and executors. In contrast, we propose an end-to-end reinforcement learning approach that simultaneously optimizes both the main planner agent and the sub-agents (executors). Unlike OWLs decoupling or Kimi 2.5s static sub-agent paradigm, our work enables the entire system to co-evolve, allowing the main agent to autonomously broaden search paths while the sub-agents adapt their retrieval and synthesis strategies for industrialscale Wide Research. This joint optimization ensures that the planning of breadth and the execution of tool-calling are aligned toward maximizing final search utility. 7. Conclusion To address the paradigm shift from Deep to Wide Research, we introduce WideSeekBench to formalize the General Broad Information Seeking (GBIS) task. We construct it via rigorous multi-phase data pipeline that mines intersected world knowledge from KGs. We propose WideSeek, dynamic hierarchical multi-agent architecture optimized via an end-to-end reinforcement learning framework. Our results demonstrate that WideSeek effectively leverages agent scaling to solve complex, parallel retrieval tasks, significantly advancing Wide Research capabilities."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Abou Ali, M., Dornaika, F., and Charafeddine, J. Agentic ai: comprehensive survey of architectures, applications, and future directions. Artificial Intelligence Review, 59 (1), November 2025. ISSN 1573-7462. doi: 10.1007/ s10462-025-11422-4. URL http://dx.doi.org/ 10.1007/s10462-025-11422-4. Bast, H. and Buchhold, B. Qlever: query engine for efficient sparql+text search. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, CIKM 17, pp. 647656, New York, NY, USA, 2017. Association for Computing ISBN 9781450349185. Machinery. doi: 10.1145/ 3132847.3132921. URL https://doi.org/10. 1145/3132847.3132921. Chen, Z., Ma, X., Zhuang, S., Nie, P., Zou, K., Liu, A., Green, J., Patel, K., Meng, R., Su, M., Sharifymoghaddam, S., Li, Y., Hong, H., Shi, X., Liu, X., Thakur, N., Zhang, C., Gao, L., Chen, W., and Lin, J. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent, 2025. URL https://arxiv.org/abs/2508.06600. Hong, H., Yin, J., Wang, Y., Liu, J., Chen, Z., Yu, A., Li, J., Ye, Z., Xiao, H., Chen, Y., Zhou, H., Yue, Y., Yang, M., Guo, C., Liu, J., Wei, P., and Gu, J. Multi-agent deep research: Training multi-agent systems with m-grpo, 2025. URL https://arxiv.org/abs/2511.13288. Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Wang, J., Zhang, C., Wang, Z., Yau, S. K. S., Lin, Z., et al. Metagpt: Meta programming for multi-agent collaborative framework. In The twelfth international conference on learning representations, 2023. Hu, M., Zhou, Y., Fan, W., Nie, Y., Xia, B., Sun, T., Ye, Z., Jin, Z., Li, Y., Chen, Q., et al. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation. arXiv preprint arXiv:2505.23885, 2025. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=VTF8yNQM66. Lan, T., Zhu, B., Jia, Q., Ren, J., Li, H., Wang, L., Xu, Z., Luo, W., and Zhang, K. Deepwidesearch: Benchmarking depth and width in agentic information seeking, 2025. URL https://arxiv.org/abs/2510.20168. Lei, F., Meng, J., Huang, Y., Zhao, J., Zhang, Y., Luo, J., Zou, X., Yang, R., Shi, W., Gao, Y., He, S., Wang, Z., Liu, Q., Wang, Y., Wang, K., Zhao, J., and Liu, K. Dacomp: Benchmarking data agents across the full data intelligence lifecycle, 2025. URL https://arxiv. org/abs/2512.04324. Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., Shen, W., Zhang, J., Zhang, D., Wu, X., Jiang, Y., Yan, M., Xie, P., Huang, F., and Zhou, J. Websailor: Navigating super-human reasoning for web agent, 2025. URL https://arxiv.org/ abs/2507.02592. Liu, S., Chen, T., Liang, Z., Lyu, X., and Amato, C. Llm collaboration with multi-agent reinforcement learning. arXiv preprint arXiv:2508.04652, 2025. 9 WideSeek: Advancing Wide Research via Multi-Agent Scaling Lu, R., Hou, Z., Wang, Z., Zhang, H., Liu, X., Li, Y., Feng, S., Tang, J., and Dong, Y. Deepdive: Advancing deep search agents with knowledge graphs and multi-turn rl, 2025. URL https://arxiv.org/abs/2509. 10446. Luo, X., Zhang, Y., He, Z., Wang, Z., Zhao, S., Li, D., Qiu, L. K., and Yang, Y. Agent lightning: Train any ai agents with reinforcement learning, 2025. URL https: //arxiv.org/abs/2508.03680."
        },
        {
            "title": "Introducing wide",
            "content": "Manus. URL introducing-wide-research. 2025. https://manus.im/blog/ research, Moonshot AI. Kimi k2.5: Visual agentic intelliURL https://www.kimi.com/ gence, 2026. blog/kimi-k2-5.html. Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1517415186, 2024. Ren, Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., Zhang, L., Fu, Z., Zhu, Q., Yang, D., et al. Deepseekprover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. Roucher, A., del Moral, A. V., Wolf, T., von Werra, L., and Kaunismaki, E. smolagents: smol library to build great agentic systems. https://github.com/ huggingface/smolagents, 2025. Schmelzeisen, L., Dima, C., and Staab, S. Wikidated 1.0: An evolving knowledge graph dataset of wikidatas revision history, 2021. URL https://arxiv.org/ abs/2112.05003. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, pp. 12791297. ACM, March 2025. doi: 10.1145/ 3689031.3696075. URL http://dx.doi.org/10. 1145/3689031.3696075. 10 Shi, Z., Chen, Y., Li, H., Sun, W., Ni, S., Lyu, Y., Fan, R.-Z., Jin, B., Weng, Y., Zhu, M., Xie, Q., Guo, X., Yang, Q., Wu, J., Zhao, J., Tang, X., Ma, X., Wang, C., Mao, J., Ai, Q., Huang, J.-T., Wang, W., Zhang, Y., Yang, Y., Tu, Z., and Ren, Z. Deep research: systematic survey, 2025. URL https://arxiv.org/abs/2512.02038. Sun, W., Lu, M., Ling, Z., Liu, K., Yao, X., Yang, Y., and Chen, J. Scaling long-horizon llm agent via contextfolding, 2025. URL https://arxiv.org/abs/ 2510.11967. Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., Xie, P., Huang, F., and Zhou, J. Webshaper: Agentically data synthesizing via information-seeking formalization, 2025. URL https: //arxiv.org/abs/2507.15061. Team, T. D., Li, B., Zhang, B., Zhang, D., Huang, F., Li, G., Chen, G., Yin, H., Wu, J., Zhou, J., et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents, 2025. URL https://arxiv.org/ abs/2504.12516. Wong, R., Wang, J., Zhao, J., Chen, L., Gao, Y., Zhang, L., Zhou, X., Wang, Z., Xiang, K., Zhang, G., Huang, W., Wang, Y., and Wang, K. Widesearch: Benchmarking agentic broad info-seeking, 2025. URL https: //arxiv.org/abs/2508.07999. Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Fu, G., Jiang, Y., et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. Wu, J., Yin, W., Jiang, Y., Wang, Z., Xi, Z., Fang, R., Zhang, L., He, Y., Zhou, D., Xie, P., et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025b. Xia, T., Ding, L., Wan, G., Zhan, Y., Du, B., and Tao, D. Improving complex reasoning over knowledge graph with logic-aware curriculum tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 1288112889, 2025. Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., Liu, Y., Xu, Y., Zhou, S., Savarese, S., Xiong, C., Zhong, V., and Yu, T. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. In The Thirty-eight Conference on Neural Information Processing Systems WideSeek: Advancing Wide Research via Multi-Agent Scaling Datasets and Benchmarks Track, 2024. URL https: //openreview.net/forum?id=tN61DTr4Ed. Xin, H., Guo, D., Shao, Z., Ren, Z., Zhu, Q., Liu, B., Ruan, C., Li, W., and Liang, X. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yao, S. The second half, 2025. URL https://ysymyth. github.io/The-Second-Half/. WideSeek: Advancing Wide Research via Multi-Agent Scaling A. The Details of WideSeekBench A.1. Benchmark Comparison Table 3. Comparison of WideSeekBench with existing information-seeking benchmarks. Task Type distinguishes between finding specific hidden info (Deep) vs. collecting broad structured info (Wide). Auto Gen. indicates if the data pipeline is automated. Multi-dim. indicates if tasks are classified by fine-grained dimensions (i.e., constraints, domains.)."
        },
        {
            "title": "Size",
            "content": "Domains Task Type Train Set Auto Gen. Multi-dim Class. GAIA (Text-Only Split) BrowseComp BrowseComp-ZH WideSearch DeepWideSearch xbench-DeepSearch WebShaper 103 1,266 289 200 220 100 5,000 WideSeekBench (Ours) 5,156 - 9 11 14 15 - -"
        },
        {
            "title": "Wide",
            "content": "MIX A.2. Knowledge Graph Source and Infrastructure We ingest the Wikidata Truthy Dump (October 1, 2025) into local QLever (Bast & Buchhold, 2017) SPARQL engine to support efficient, rate-limit-free execution of complex SPARQL queries over the full knowledge graph. A.3. Seed Constraint Construction We construct diverse set of seed entities to serve as the semantic basis for downstream constraint construction and task synthesis. Domain Taxonomy. We define 18 high-level domains (e.g., Computer Science, Life Sciences, Governance). Each domain is mapped to set of Wikidata classes, which are treated as domain-specific sub-domains. In total, this mapping yields 200 sub-domains across all domains. These sub-domains jointly define controlled search scope Ssubdomain (refer to Appendix A.6 for details). Retrieval and Ranking. For each sub-domain, we identify 80 informative seed entities from the knowledge base using three-stage SPARQL-based workflow. (1) Retrieval: Given sub-domain class, we retrieve candidate entity set Ecand by recursively querying the class and all its subclasses via the transitive closure of the wdt:P279 (subclass of) relation (Listing 1). (2) Ranking: Each candidate entity Ecand is ranked by its information density, approximated by the number of outgoing RDF triples associated with (Listing 2). Entities with higher information density are preferred, as they support the construction of richer constraints and attribute schemas. (3) Filtering: We remove non-entity artifacts and structurally uninformative entries, including entities whose labels begin with \"List of\" or \"Category:\". The remaining entities constitute the seed entity set Eseed. Listing 1. Candidate entity retrieval via recursive subclass matching, where wdt:P31 and wdt:P279 represent the instance of and subclass of relations in Wikidata, respectively. SELECT DISTINCT ?entity WHERE { ?entity (wdt:P31/wdt:P279*) wd:TARGET_ID . } Listing 2. Ranking entities by information density (triple count). SELECT ?entity ?label (COUNT(?p) AS ?count) WHERE { VALUES ?entity { wd:Q_CANDIDATE_1 ... } ?entity ?p ?o . OPTIONAL { ?entity rdfs:label ?label . FILTER(LANG(?label) = \"en\") } } 12 WideSeek: Advancing Wide Research via Multi-Agent Scaling GROUP BY ?entity ?label ORDER BY DESC(?count) A.4. Logical Composition and Task Synthesis We describe the procedures for composing atomic constraints into executable queries, executing and validating the resulting retrievals, and constructing bounded tables. For each seed entity, we generate up to 200 composite constraints. To control redundancy and dataset balance, each seed contributes at most 4 validated tables. Query Formulation. Given sampled seed entity eseed Eseed and its associated relations Rseed = {(eseed, p, v)}, retrieved from via property-seeking SPARQL queries, we define the associated atomic constraint set C(eseed) atom = {(p, v)}. We then sample atomic constraints C(eseed) atom and compose them into composite SPARQL filters using seven predefined logical patterns  (Table 4)  , yielding composite constraint Φ. Apart from the domain constraint, each composite constraint Φ is required to contain at least 1 and at most 8 atomic constraints. Execution and Verification. Each composite filter Φ is executed against the knowledge base to retrieve candidate entity set E. We restrict the cardinality of the candidate entity set to the interval [1, 1024]. As shown in Listing 3, verification step enforces this constraint prior to attribute retrieval, discarding any queries where falls outside the bound. Table Construction and Quality Control. Given the validated entity set E, we first collect candidate attribute set Acand = (cid:83) eE Attributes(e), and dynamically select target attributes Acand by retaining only those with at least 50% coverage across entities and sufficient value diversity (Listing 4). Next, we compute the potential table size Ncells = A, and retain tasks satisfying Ncells [8, 8192]. Entities that fail to resolve to valid labels are removed, resulting in the cleaned entity set Eclean. Finally, we perform batch SPARQL queries to retrieve all cell values (Listing 5) and populate the table T. To avoid redundancy, we further deduplicate tables by discarding those with identical entity sets and attribute schemas, retaining only one representative table per equivalence class. SELECT (COUNT(DISTINCT ?item) AS ?count) WHERE { # Synthesized Logical Constraints (see Table 4) ?item wdt:P31 wd:Q_domain . ... } Listing 3. Pre-flight cardinality check. Listing 4. Attribute frequency analysis. SELECT ?prop (COUNT(DISTINCT ?item) AS ?cnt) WHERE { VALUES ?item { wd:Q_sample1 ... } ?item ?prop ?value . FILTER(STRSTARTS(STR(?prop), \"http://www.wikidata.org/prop/direct/\")) } GROUP BY ?prop Listing 5. Batch value retrieval. SELECT ?item ?prop ?value ?valueLabel WHERE { VALUES ?item { wd:Q_e1 ... } VALUES ?directProp { wdt:P1 ... } ?item ?directProp ?value . ?prop wikibase:directClaim ?directProp . # Map back to direct predicate OPTIONAL { ?value rdfs:label ?valueLabel . FILTER(LANG(?valueLabel) = \"en\") } } A.5. Agent Task Synthesis and Multi-Stage Filtering We implement cyclic generation-verification pipeline to transform structured logical filters Φ into diverse, human-like search tasks Q, followed by rigorous quality assurance protocol. In this subsection, all LLM-based operations are powered by GPT-5. WideSeek: Advancing Wide Research via Multi-Agent Scaling Table 4. Logical patterns in WideSeekBench. denotes the domain constraint."
        },
        {
            "title": "Pattern",
            "content": "Prob. Formulation Φ(e)"
        },
        {
            "title": "AND",
            "content": "OR"
        },
        {
            "title": "AND OR",
            "content": "20% D(e) ((cid:86) ci(e)) 20% D(e) ((cid:87) ci(e)) 15% D(e) cbase(e) ((cid:87) cexi (e)) 15% D(e) [((cid:86) ci(e)) ((cid:86) cj(e))]"
        },
        {
            "title": "AND NOT",
            "content": "15% D(e) ((cid:87) cexj (e)) ((cid:86) cini (e))"
        },
        {
            "title": "OR NOT",
            "content": "10% D(e) ((cid:87) cexj (e)) ((cid:87) cini (e))"
        },
        {
            "title": "AND OR NOT",
            "content": "5% D(e) [((cid:86) ((cid:87) cexk (e)) ci(e)) ((cid:86) cj(e))] SELECT ?item WHERE { ?item wdt:P31 wd:Q_dom . ?item wdt:P1 wd:Q1 . ?item wdt:P2 wd:Q2 . } SELECT ?item WHERE { ?item wdt:P31 wd:Q_dom . { { ?item wdt:P1 wd:Q1 } UNION { ?item wdt:P2 wd:Q2 } } } SELECT ?item WHERE { ?item wdt:P31 wd:Q_dom . ?item wdt:P_base wd:Q_base . FILTER NOT EXISTS { { ?item wdt:P_ex1 wd:Q_ex1 } UNION { ?item wdt:P_ex2 wd: Q_ex2 } } } SELECT ?item WHERE { ?item wdt:P31 wd:Q_dom . { { ?item wdt:P1 wd:Q1 . ?item wdt:P2 wd:Q2 } UNION { ?item wdt:P3 wd:Q3 . ?item wdt:P4 wd:Q4 } } } SELECT ?item WHERE { ?item wdt:P31 wd:Q_dom . ?item wdt:P1 wd:Q1 . ?item wdt:P2 wd:Q2 . FILTER NOT EXISTS { { ?item wdt:P_ex1 wd:Q_ex1 } UNION { ?item wdt:P_ex2 wd: Q_ex2 } } } SELECT ?item WHERE { ?item wdt:P31 wd:Q_dom . { { ?item wdt:P1 wd:Q1 } UNION { ?item wdt:P2 wd:Q2 } } FILTER NOT EXISTS { { ?item wdt:P_ex1 wd:Q_ex1 } UNION { ?item wdt:P_ex2 wd: Q_ex2 } } } SELECT ?item WHERE { ?item wdt:P31 wd:Q_dom . { { ?item wdt:P1 wd:Q1 . ?item wdt:P2 wd:Q2 } UNION { ?item wdt:P3 wd:Q3 . ?item wdt:P4 wd:Q4 } } FILTER NOT EXISTS { { ?item wdt:P_ex1 wd:Q_ex1 } UNION { ?item wdt:P_ex2 wd: Q_ex2 } } } Self-Refining Query Synthesis. The transformation process employs dual-model architecture to ensure both linguistic diversity and logical fidelity. First, raw constraints are mapped into structured f-string template (e.g., \"Find all {sub-domain} that {prop} is {val}...\"). generator model Mgen then transforms this template into natural language using style randomization protocol, sampling syntactic mode (1, 10) from predefined set (Action, Question, Imperative, Need, Context, Interest, Description, Casual, Professional and Task) for each task. To ensure semantic accuracy, critic model Mver extracts the logic ˆΦ back from the generated query and performs constraint-by-constraint equivalence check S(Φ, ˆΦ). The verifier rigorously compares entity preservation, operator logic (, , ), filtering scope, and output schema consistency. Any discrepancy triggers feedback loop with specific error correction instructions, capped at = 5 iterations. Data-Driven Rubric Synthesis. We leverage an LLM to synthesize adaptive evaluation criteria Rj by analyzing the data distribution of each ground truth column ,j. Unlike rigid string matching, the model generates semantic compliance standards tailored to the specific data type: (1) Entities explicitly accept aliases and naming variations; (2) Dates enforce semantic exactness regardless of format; (3) Numerics require value equality within defined tolerances; and (4) Sets enforce equality independent of item order. Quality Assurance Protocol. We apply three-tier filtering mechanism. (1) Rule-Based Filtering discards tasks with sparse ground truth (> 50% empty cells) or weak web grounding where target entities lack verifiable search API hits, as 14 WideSeek: Advancing Wide Research via Multi-Agent Scaling Table 5. Property ID Blacklist."
        },
        {
            "title": "Property ID Label",
            "content": "P31 P106 P108 P248 P279 P361 P373 P460 P527 P646 P910 P1001 P1343 P1709 P1754 P1889 P2671 P3876 P6104 Instance of Occupation Employer Stated in Subclass of Part of Commons category Said to be the same as Has part Freebase ID Topics main category Applies to jurisdiction Described by source Equivalent class Category related to list Different from Google Knowledge Graph ID Category for alumni of educational institution Maintained by WikiProject determined by their English sitelink counts in Wikidata, where entities with zero English sitelinks are strictly filtered out. (2) LLM-Based Filtering employs judge model to evaluate tasks on 5-point scale across Human-Likeness, Solvability, Common Sense, Temporal Stability, and Rubric Rationality, violation in any category results in immediate rejection.(3) Human Verification removes subtle semantic irrationalities (e.g., logical contradictions) that automated filters might overlook. Prompt: LLM-Based Filtering You are an expert Data Quality Auditor for Search Agent Benchmark. Your goal is to validate synthesized widesearch tasks to ensure they are high-quality, solvable, and fairly evaluated. ## Input Data provided for each request: 1. Task Info: The natural language user query and basic metadata. 2. Rubrics: The evaluation criteria for each column (preprocess, metric, criterion). 3. Ground Truth Stats & Sample: Statistics of the expected answer table and preview of the data. 4. Intermediate Logic: The SPARQL query and constraints used to generate the task. ## Evaluation Criteria (Pass/Fail) Analyze the task against the following 5 dimensions. If it fails on ANY dimension, mark as INVALID. ### 1. Human-Likeness & Phrasing * Goal: Ensure the query sounds like real human request. * Reject if phrasing is unnatural, overly pedantic, or raw SPARQL translation. * Fail: \"Find entities where mass is exactly 70.53 kg.\" * Fail: \"Find items with property P31 equal to Q5.\" (Leaked IDs). * Pass: \"Find all public hospitals in Portugal founded before 1950.\" ### 2. Task Solvability * Goal: Ensure the task is logically possible. * Reject if logic contains contradictions (e.g., \"born > 2000 AND died < 1999\"). * Reject if it asks for private/internal info not publicly available. ### 3. Answer Common Sense * Goal: Detect \"Hallucinated\" or \"Incomplete\" Ground Truth. * Reject if GT clearly contradicts general world knowledge. * Fail: \"All US States\" -> GT has only 5 rows (Recall too low). * Fail: \"Current UK Monarchs\" -> GT has 10 rows (Recall too high/Wrong). * Fail: GT has >50% empty cells for key attributes. ### 4. Temporal Stability & Wiki-Fit * Goal: Ensure facts are static and suitable for Wikipedia search. * Reject if query relies on \"Current/Now\" without time anchor (e.g., \"Current CEO\") UNLESS the GT explicitly handles specific dates. 15 WideSeek: Advancing Wide Research via Multi-Agent Scaling * Reject if attributes are purely database metadata (e.g., \"Wikidata QID\", \"GND ID\") unlikely to be found in Wikipedia text. ### 5. Rubric Rationality (Criterion Quality Check) * Goal: Ensure the criterion explicitly instructs the judge to handle the variability of web data (formats, aliases, precision). * Reject if the criterion enforces rigid constraints that are unrealistic for web extraction. * Fail (False Strictness on Non-Strings): The criterion demands \"exact string match\" or \"no formatting differences\" for Dates, Coordinates, or Floating Point Numbers. * Bad: \"Accept only exact string match.\" (for Date like 2020-01-01). * Good: \"Accept dates representing the same day, regardless of format (e.g., Jan 1st, 2020).\" * Fail (Unrealistic Precision): The criterion for Coordinates or Statistics requires matching to extreme * Fail (Missing Alias Logic): The criterion for Entities (e.g., Universities, Journals) fails to mention precision (e.g., >4 decimal places) which rarely appears in web text. * Bad: \"Reject if coordinate differs by 0.000001.\" * Good: \"Accept if coordinates are geographically close (within 1km).\" allowing \"Aliases\" or \"Common Abbreviations\". * Bad: \"Accept only University of California, Los Angeles.\" * Good: \"Accept UCLA or Univ. of Calif. LA as valid matches.\" * Fail (Vague Instructions): The criterion is too generic to ensure consistency. * Bad: \"Check if the answer is correct.\" (Too open-ended). * Good: \"Accept exact matches or minor spelling variations; reject completely different entities.\" ## Output Format Output JSON object ONLY: json { \"status\": \"VALID\" \"INVALID\", \"reasoning\": \"Concise explanation...\", \"flags\": { \"unnatural_phrasing\": boolean, \"logically_unsolvable\": boolean, \"gt_contradicts_commonsense\": boolean, \"temporal_or_wiki_issue\": boolean, \"bad_rubric\": boolean }, \"suggestion\": \"Optional fix\" } A.6. WideSeekBench Statistics Scale of Target Information Figure 7 depicts the scale of target information across diverse top-level domains. The dataset contains 4,436 training instances and 720 test instances, covering 18 domains. Tables 6 and 7 provide detailed distributions of subdomains. High-frequency categories in the training set include film (252), video game (197), and airport (176). The test set preserves similar distribution (e.g., film 41, video game 38). Constraint Complexity Table 8 shows the distribution of logical patterns in the dataset, which directly reflects the distribution of constraints. The training set is dominated by single-type patterns, with pure conjunctions (AND) accounting for 37.8%, followed by AND NOT (19.5%). The test set exhibits more balanced distribution, with simple AND patterns reduced to 20.0% and complex composite patterns substantially increased. The most complex combination, AND OR NOT, constitutes 11.5% of the test set (compared to 5.1% in training), and other high-complexity patterns such as AND OR and OR NOT are also more evenly represented. Domain Diversity Figure 8 shows the distribution of topics in WideSeekBench across training and test sets. Dominant domains such as Screen & Print Media and Gaming are represented by subdomains including film and video game. Scientific and technical sectors are also covered, notably Space (e.g., airport, space mission) and Infrastructure (e.g., metro station). The dataset exhibits long-tailed distribution that includes specialized concepts ranging from Life Sciences (e.g., protein family, enzyme) to Natural Geography features (e.g., planetary nebula, glacier). The test set (Figures 8c and 8d) maintains similar distribution across domains. WideSeek: Advancing Wide Research via Multi-Agent Scaling (a) Train Set Scale (b) Test Set Scale (c) Train Set Domain Distribution (d) Test Set Domain Distribution Figure 7. Comprehensive statistics of WideSeekBench. Top row (a, b) illustrates the scale of target information. Bottom row (c, d) depicts the distribution of domains across Training and Test sets. Table 6. Training set domain and sub-domain summary. Domain Subdomain Count Domain Subdomain Count Screen & Print Media Audio Business & Economy Education & Academia Space film short film television series literary work television program publisher comics magazine episode periodical poem single album rock band song musical group orchestra Musical Work concert rock bank public company goods manufactured good enterprise stock exchange business brewery brand company trademark currency farm university college scientific journal research institute academic journal educational institution laboratory school library airport space mission artificial satellite rocket launch asteroid aircraft model exoplanet variable star 252 109 92 32 31 7 6 5 5 2 2 146 89 86 74 40 18 3 2 1 73 72 70 56 18 17 17 13 11 9 8 8 1 143 117 25 25 13 6 6 6 5 176 47 34 31 25 10 8 3 Space Governance Sports Computer Science Infrastructure planetary nebula political party charitable organization non-governmental organization polity government agency armed organization political organization battle international organization war former administrative territorial entity treaty legal case organization administrative territorial entity firearm public election executive branch conflict association legal norm crime sporting event sports season competition stage association football club competition recurring sporting event edition recurring sporting event racing Olympic Games physical activity sports venue sports competition association football match tennis tournament sport nation at sport competition baseball player sports club programming language operating system free software computer computer network protocol software database metro station dam 1 122 28 18 17 15 14 13 13 12 11 10 8 7 6 5 5 5 3 3 3 3 2 76 64 36 28 22 17 16 12 11 7 5 5 5 5 4 4 1 1 114 94 35 28 10 7 3 146 42 Subdomain Count railway station controlled-access highway lighthouse hotel road power station wind farm house building industrial building historical country tomb ceremony church building cultural heritage museum archaeological site heritage cultural property architectural heritage monument shrine heritage site temple location of worship funerary structure structure of worship chapel cemetery video game electronic game board game national park mountain island lake protected area canal park disaster glacier landform earthquake natural heritage hill valley forest nature reserve watercourse mineral machine 26 21 19 13 10 5 3 2 1 1 56 40 15 15 14 12 10 10 10 9 9 9 6 6 4 3 1 1 197 12 1 38 32 27 18 18 14 11 7 7 6 6 5 3 3 3 3 2 1 Domain Machinery Settlement Life Sciences & Medicine Language Others People & Society Subdomain Count vehicle vehicles and vehicle parts product tool equipment automobile model ship physical tool town municipality village city neighborhood district province human settlement region taxon protein family hospital mammal Chordata fungi Vertebrata medication anatomical structure bird disease enzyme insect plant anomaly language languoid language variety visual artwork flag dish data artificial physical object physical process philosophy knowledge organization system sculpture communications media assembly chemical process disposable product human ethnic group occupation 42 26 17 11 9 5 4 39 32 22 20 10 9 8 3 1 17 9 8 6 6 6 4 3 2 2 1 1 1 1 1 43 9 1 13 11 8 4 3 2 2 2 2 2 1 1 1 30 9 1 Domain Infrastructure Cultural & Historical Heritage Gaming Natural Geography Machinery Total: 17 WideSeek: Advancing Wide Research via Multi-Agent Scaling Table 7. Test set domain and sub-domain summary. Domain Subdomain Count Domain Subdomain Count Education & Academia Screen & Print Media Space Infrastructure Governance college university research institute laboratory school academic journal educational institution scientific journal library film short film television series television program literary work photograph magazine airport space mission artificial satellite aircraft model asteroid exoplanet rocket launch astronomical object metro station railway station controlled-access highway hotel dam power station lighthouse wind farm road political party armed organization polity 34 26 5 4 3 3 3 1 1 41 16 12 6 2 1 1 31 12 7 3 3 2 1 1 35 7 5 4 3 2 1 1 1 31 4 3 Governance Business & Economy Sports Gaming Settlement former administrative territorial entity charitable organization battle political organization government agency non-governmental organization conflict international organization war bank public company manufactured good stock exchange brewery goods enterprise company currency business brand sports season association football club sporting event competition stage recurring sporting event association football match recurring sporting event edition sports competition racing Olympic Games sports venue video game board game electronic game town municipality 3 2 2 2 2 1 1 1 1 9 8 7 5 4 4 3 2 1 1 1 16 6 6 3 3 2 2 2 1 1 1 38 3 1 19 6 Domain Settlement Cultural & Historical Heritage People & Society Audio Natural Geography Total: Subdomain Count Domain Subdomain Count human settlement village city province neighborhood region district historical country church building ceremony historical event architectural heritage monument cultural heritage tomb heritage site museum heritage cultural property human ethnic group musical group album rock band single song orchestra musician national park island mountain canal hill landform protected area watercourse park 4 3 3 2 2 2 1 11 6 4 3 2 2 2 2 1 1 1 31 3 7 6 6 6 5 3 1 6 5 5 3 2 2 2 1 1 Natural Geography Computer Science Life Sciences & Medicine Machinery Language Others natural heritage lake forest operating system programming language free software computer network protocol computer archive hospital protein family fungi symptom Vertebrata taxon plant mammal bird automobile model vehicle machine ship equipment vehicles and vehicle parts product tool language human language language variety visual artwork dish flag unit of measurement science artificial physical object 1 1 1 11 8 1 1 1 1 6 4 2 2 1 1 1 1 1 6 3 3 2 2 1 1 8 4 1 4 2 2 1 1 1 Table 8. Distribution of logical patterns in WideSeekBench."
        },
        {
            "title": "Percentage",
            "content": "1,676 866 704 502 233 229 226 4,436 37.8% 19.5% 15.9% 11.3% 5.3% 5.2% 5.1% 100.0% 144 119 104 93 94 83 83 20.0% 16.5% 14.4% 12.9% 13.1% 11.5% 11.5% 100.0% 18 WideSeek: Advancing Wide Research via Multi-Agent Scaling (a) Train: Domain Distribution (b) Train: Subdomain Distribution (c) Test: Domain Distribution (d) Test: Subdomain Distribution Figure 8. Word clouds illustrating the diversity of WideSeekBench. Top row (a, b) shows the Training set, and bottom row (c, d) shows the Test set. The size of each term corresponds to its frequency. 19 WideSeek: Advancing Wide Research via Multi-Agent Scaling A.7. Task Cases Example 1: AND Domain: Education & Academia Sub-domain: scientific journal Query: Curious about scientific journals published by Wiley that focus on geography and either use English as their work language or have an English name. Please find scientific journal that meet these conditions, and provide the following attributes for these scientific journal: scientific journal, inception, country of origin, indexed in bibliographic review, Danish Bibliometric Research Indicator level **Column Specifications:** 1. **scientific journal** geography * Precise Explanation: periodical journal that publishes peer-reviewed scientific research focused on * Format Specification: Full official title of the journal in standard title case * Example: \"Journal of Coastal Geographic Studies\" (15 lines skipped) 5. **Danish Bibliometric Research Indicator level** * Precise Explanation: The official ranking level assigned to the journal under the Danish Bibliometric"
        },
        {
            "title": "Research Indicator system for academic publication channels",
            "content": "* Format Specification: Single integer value (either 1 or 2) * Example: \"1\" Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: scientific journal, inception, country of origin, indexed in bibliographic review, Danish Bibliometric Research Indicator level. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily . The output format is markdown {data_content} . SPARQL Query: PREFIX wd: <http://www.wikidata.org/entity/> PREFIX wdt: <http://www.wikidata.org/prop/direct/> SELECT DISTINCT ?item WHERE { ?item wdt:P31/(wdt:P279wdt:P1647)* wd:Q5633421 . ?item wdt:P123/(wdt:P279wdt:P1647)* wd:Q1479654 . ?item wdt:P407/(wdt:P279wdt:P1647)* wd:Q1860 . ?item wdt:P921/(wdt:P279wdt:P1647)* wd:Q1071 . } Table: 20 WideSeek: Advancing Wide Research via Multi-Agent Scaling scientific journal inception country of origin indexed in bibliographic review Danish Bibliometric Research The Geographical Journal 1831-01-01T00:00:00Z United Kingdom The Canadian Geographer 1950-01-01T00:00:00Z Canada Geographical Analysis 1969-01-01T00:00:00Z United States Journal of Biogeography 1974-01-01T00:00:00Z United Kingdom Transactions of the Institute of British Geographers Antipode 1935-01-01T00:00:00Z United Kingdom 1969-01-01T00:00:00Z United Kingdom Area 1969-01-01T00:00:00Z United Kingdom International Journal of Urban and Regional Research International Migration 1977-01-01T00:00:00Z United Kingdom 1961-01-01T00:00:00Z United Kingdom Natural Resources Forum 1976-01-01T00:00:00Z Kingdom of the Netherlands Social Sciences Citation Index; Scopus Social Sciences Citation Index; Scopus Social Sciences Citation Index; Scopus Science Citation Index Expanded; Scopus Scopus Social Sciences Citation Index; Scopus Social Sciences Citation Index; Scopus Social Sciences Citation Index; Scopus Social Sciences Citation Index; Scopus Science Citation Index Expanded; Scopus Indicator level 1 1 1 2 2 1 2 1 1 (showing first 10 rows, total: 11 rows) Criterion Accept exact journal names, common recognized abbreviations, and minor title variations (such as omitting leading articles like The or adjusting capitalization) as long as they refer to the same published journal. Reject completely distinct journal titles that do not correspond to the same publication. Accept dates that represent the same year, month, and day regardless of string format, including ignoring timestamp suffixes like T00:00:00Z which do not alter the core date value (e.g., 1831-01-01T00:00:00Z matches January 1, 1831 or 1831). Reject dates referring to different year, month, or day. Accept the exact official country name, common colloquial aliases, and official alternative names (e.g., United Kingdom matches UK, Kingdom of the Netherlands matches Netherlands or Holland). Reject country names that refer to an entirely different sovereign nation. Accept lists that contain the same set of indexing services regardless of order, punctuation separators (semicolon vs comma), or minor spacing differences around separators. For example, Social Sciences Citation Index; Scopus matches Scopus, Social Sciences Citation Index. Reject lists that have missing or additional indexing services not present in the reference value. Accept values that are numerically equivalent to the reference level, regardless of formatting (e.g., the string 2 matches the number 2 or the phrase Level 2) as long as they refer to the correct indicator level. Reject values that correspond to different integer level entirely. Rubrics: Column scientific journal"
        },
        {
            "title": "Metric\nllm judge",
            "content": "inception llm judge country of origin llm judge indexed in bibliographic review llm judge Danish Bibliometric Research Indicator level llm judge Example 2: AND NOT Domain: Natural Geography Sub-domain: lake Query: What are all the lakes located in the Northwest Territories, with basin country of Canada, excluding those that outflow into the Buffalo River? Please find lake that meet these conditions, and provide the following attributes for these lake: lake, elevation above sea level, coordinate location, country **Column Specifications:** 21 WideSeek: Advancing Wide Research via Multi-Agent Scaling 1. **lake** - Precise Explanation: The formal, titled name of contained body of still water situated within natural basin - Format Specification: Proper noun written in title case, may include hyphens for connected lake systems - Example: \"Silvermoss Lake\" (10 lines skipped) 4. **country** - Precise Explanation: The sovereign state or recognized territorial entity containing the lake; multiple entries separated by semicolons for cross-border lakes - Format Specification: Proper noun country or territorial entity names, separated by semicolons when applicable - Example: \"Canada; Deninu First Nation\" Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: lake, elevation above sea level, coordinate location, country. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily . The output format is markdown {data_content} . SPARQL Query: PREFIX wd: <http://www.wikidata.org/entity/> PREFIX wdt: <http://www.wikidata.org/prop/direct/> SELECT DISTINCT ?item WHERE { ?item wdt:P31/(wdt:P279wdt:P1647)* wd:Q23397 . ?item wdt:P131/(wdt:P279wdt:P1647)* wd:Q2007 . ?item wdt:P205/(wdt:P279wdt:P1647)* wd:Q16 . FILTER NOT EXISTS { ?item wdt:P201/(wdt:P279wdt:P1647)* wd:Q22605327 . } } Table: lake Clinton-Colden Lake Colville Lake Point Lake Dubawnt Lake Keller Lake Artillery Lake Faber Lake Lac La Martre Wholdaia Lake MacKay Lake Rubrics: elevation above sea level 375.0 244.0 375.0 236.0 255.0 365.0 213.0 265.0 364.0 431.0 coordinate location POINT(-107.466667 63.966667) POINT(-126.000000 67.166667) POINT(-113.067962 65.250299) POINT(-102.096400 POINT(-101.533333 63.100000) POINT(-121.584942 63.950142) POINT(-107.866667 63.166667) POINT(-117.466667 63.883333) POINT(-117.950000 63.316667) POINT(-104.167297 60.716880) POINT(-111.033333 63.916667) 63.062790); country Canada Canada Canada Canada Canada Canada Canada Tlicho Nation; Canada Canada Canada (showing first 10 rows, total: 34 rows) 22 WideSeek: Advancing Wide Research via Multi-Agent Scaling"
        },
        {
            "title": "Metric\nllm judge",
            "content": "elevation above sea level llm judge coordinate location llm judge country llm judge Criterion Accept the exact lake name, minor formatting variations like hyphen or spacing changes, and established common aliases for the lake. Reject names referring to an entirely different lake. Accept elevation values that are numerically equivalent or very close, allowing for reasonable rounding differences or minor measurement variations within 1 meter or 1% difference. Ignore formatting differences like trailing .0 or string versus numeric representation. Accept coordinate values that represent the same general locations, allowing minor positional deviations within approximately 1km. Ignore formatting differences like POINT() wrapping or separator type (comma vs semicolon), and accept sets of coordinates in any order as long as they refer to the same key locations for the lake. Accept country or recognized indigenous territorial entity names that correctly represent the lakes location, including combined entries separated by any separator in any order. Allow common aliases for countries or associated indigenous nations, and reject entirely unrelated country or territorial names. Example 3: AND OR Domain: Screen & Print Media Sub-domain: television series Query: What are all the television series that are either science fiction programs set on an island, or television series that are followed by Ashes to Ashes and have Ashes to Ashes as their spin-off? Please find television series that meet these conditions, and provide the following attributes for these television series: television series, original broadcaster, start time, end time, country of origin, number of seasons, number of episodes Precise Explanation: connected collection of episodes released under one unified official title Format Specification: Full official title of the television series Example: \"Island of Echoes\" Precise Explanation: Total count of episodes that officially aired as part of the television series Format Specification: Decimal numeric value Example: \"40.0\" Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: television series, original broadcaster, start time, end time, country of origin, number of seasons, number of episodes. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily . The output format is markdown {data_content} . SPARQL Query: PREFIX wd: <http://www.wikidata.org/entity/> PREFIX wdt: <http://www.wikidata.org/prop/direct/> SELECT DISTINCT ?item WHERE { ?item wdt:P31/(wdt:P279wdt:P1647)* wd:Q5398426 . { { **Column Specifications:** 1. **television series** (20 lines skipped) 7. **number of episodes** * * * * * * WideSeek: Advancing Wide Research via Multi-Agent Scaling ?item wdt:P136/(wdt:P279wdt:P1647)* wd:Q336059 . ?item wdt:P8411/(wdt:P279wdt:P1647)* wd:Q23442 . } UNION { ?item wdt:P156/(wdt:P279wdt:P1647)* wd:Q725195 . ?item wdt:P2512/(wdt:P279wdt:P1647)* wd:Q725195 . } } } Table: television series Intergalactic Life on Mars Department The Hoobs Survivors Summer in Transylvania Chocky Agent Carter Sense8 Alice start time 2021-04-30T00:00:00Z 2006-01-09T00:00:00Z 1969-03-09T00:00:00Z 2001-01-15T00:00:00Z; 05T00:00:00Z 1975-04-16T00:00:00Z 2010-10-25T00:00:00Z 1984-01-01T00:00:00Z 2015-01-06T00:00:00Z 2015-06-05T00:00:00Z 2009-12-06T00:00:00Z country of origin United Kingdom United Kingdom United Kingdom United Kingdom; United States 2009-10number of episodes 8.0 16.0 28.0 / United Kingdom United Kingdom United Kingdom United States United States United Kingdom; Canada 38.0 20.0 18.0 18.0 24.0 2.0 (showing 4 of 7 columns), (showing first 10 rows, total: 41 rows) Criterion Accept the exact show title, widely recognized alternate spellings, official abbreviations, and titles with minor qualifying additions (like (TV series)) that clearly refer to the same show. Reject completely unrelated television series names. Accept exact broadcaster names, common official aliases and abbreviations (e.g., ABC = American Broadcasting Company), lists of broadcasters in any order regardless of separator punctuation, and ignore duplicate entries in the list. Confirm all key original broadcasters are present in the prediction; reject predictions missing major broadcasters or listing entirely unrelated ones. Accept dates representing the exact same day, month, and year regardless of string format (e.g., ISO 8601 format like 2007-04-10T00:00:00Z matches April 10, 2007). For entries with multiple end times, accept lists in any order regardless of separator punctuation, and confirm all listed end dates are present. Ignore time zone suffixes if the core date is accurate. Reject predictions with significantly mismatched end dates. Accept numeric values that are equivalent to the reference count, whether formatted as integer or decimal (e.g., 1 == 1.0). Allow for minor discrepancies only if they correspond to widely cited alternative season counts for the specific series; otherwise, reject values that differ by more than one season from the reference. Accept numeric values that are equivalent to the reference count, whether formatted as integer or decimal (e.g.,8 ==8.0). Allow for minor discrepancies only if they correspond to widely cited alternative episode counts for the specific series; otherwise, reject values that differ by more than five episodes from the reference. (showing 5 of 7 columns) Rubrics: Column television series"
        },
        {
            "title": "Metric\nllm judge",
            "content": "original broadcaster llm judge end time llm judge number of seasons llm judge number of episodes llm judge Example 4: AND OR NOT Domain: Business & Economy Sub-domain: public company 24 **Column Specifications:** 1. **public company** (15 lines skipped) 5. **inception** * * * * * * WideSeek: Advancing Wide Research via Multi-Agent Scaling Query: Curious about all public companies that either operate in the retail industry with Derica W. Rice serving as board member, or are based in the United States and are members of the Linux Foundation, excluding any companies that had the Target Corporation 2013 data breach as significant event or have Bullseye as their mascot. Please find public company that meet these conditions, and provide the following attributes for these public company: public company, headquarters location, stock exchange, legal form, inception Precise Explanation: company that offers its securities for sale to the general public, allowing members of the public to invest and own shares in the business Format Specification: Properly formatted official company name, following standard business naming conventions Example: Northwood Retail Group Precise Explanation: The official date when the public company first came into existence, distinct from official opening or launch dates Format Specification: ISO 8601 date format (yyyy-mm-ddTHH:MM:SSZ), using semicolons to separate multiple inception dates if applicable Example: 2001-05-15T00:00:00Z; 2005-09-02T00:00:00Z Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: public company, headquarters location, stock exchange, legal form, inception. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily . The output format is markdown {data_content} . SPARQL Query: PREFIX wd: <http://www.wikidata.org/entity/> PREFIX wdt: <http://www.wikidata.org/prop/direct/> SELECT DISTINCT ?item WHERE { ?item wdt:P31/(wdt:P279wdt:P1647)* wd:Q891723 . { { ?item wdt:P452/(wdt:P279wdt:P1647)* wd:Q126793 . ?item wdt:P3320/(wdt:P279wdt:P1647)* wd:Q109743489 . } UNION { ?item wdt:P17/(wdt:P279wdt:P1647)* wd:Q30 . ?item wdt:P463/(wdt:P279wdt:P1647)* wd:Q858851 . } } FILTER NOT EXISTS { { ?item wdt:P793/(wdt:P279wdt:P1647)* wd:Q76313694 . } UNION { ?item wdt:P822/(wdt:P279 wdt:P1647)* wd:Q4997048 . } } } Table: 25 WideSeek: Advancing Wide Research via Multi-Agent Scaling public company CME Group Capital One Juniper Networks Ciena Nuance Communications VMware Adobe Netflix, Inc. headquarters location Chicago; London McLean stock exchange Nasdaq New York Stock Exchange Sunnyvale Hanover Burlington Palo Alto San Jose Los Gatos New York Stock Exchange New York Stock Exchange; Nasdaq Nasdaq New York Stock Exchange London Stock Exchange; Nasdaq Nasdaq Micron Technology EPAM Systems Boise Newtown; Pennsylvania Nasdaq New York Stock Exchange legal form Delaware corporation private limited company; Commercial Bank; Delaware corporation Delaware corporation Delaware corporation Delaware corporation corporation Delaware corporation Delaware corporation; public company Delaware corporation Delaware corporation inception 2007-07-12T00:00:00Z 1994-01-01T00:00:00Z; 199407-21T00:00:00Z 1996-02-06T00:00:00Z 1999-01-01T00:00:00Z 1992-01-01T00:00:00Z 1998-01-01T00:00:00Z 1982-02-28T00:00:00Z 1997-08-29T00:00:00Z 1978-10-01T00:00:00Z 1993-01-01T00:00:00Z (showing first 10 rows, total: 63 rows) Criterion Accept the exact company name, common aliases, standard suffix variations (e.g., Netflix matches Netflix, Inc.), and minor punctuation differences. Reject any entry referring to completely unrelated public company. Accept if the set of headquarters locations matches, regardless of order or separator (comma vs semicolon). Allow common location aliases (e.g., PA for Pennsylvania) and accept entries referring to the same general metropolitan or geographic area. Reject entries that list entirely unrelated locations. Accept if the set of stock exchanges matches, regardless of order or separator (comma vs semicolon). Allow standard abbreviations (e.g., NYSE matches New York Stock Exchange, LSE matches London Stock Exchange). Reject entries that list entirely different stock exchanges. Accept semantic equivalents of the legal form(s), regardless of order or separator (comma vs semicolon). Allow abbreviations (e.g., Delaware corp matches Delaware corporation) and equivalent terms (e.g., corporation matches Delaware corporation where the jurisdiction is implied). Reject entries that describe an entirely different legal structure unrelated to the reference. Accept dates representing the same day, month, and year, regardless of string format (e.g., 2007-07-12 matches July 12, 2007). Ignore trailing timestamp suffixes like T00:00:00Z as non-essential. If multiple inception dates are listed, accept any matching equivalent date from the reference set. Reject entries with completely different inception dates. Rubrics: Column public company"
        },
        {
            "title": "Metric\nllm judge",
            "content": "headquarters tion locallm judge stock exchange llm judge legal form llm judge inception llm judge Example 5: NOT Domain: Gaming Sub-domain: video game Query: Curious about video games that have been nominated for The Game Awards - Most Anticipated Game, excluding any that have GRAC rating of 15+ Please find video games that meet these conditions, and provide the following attributes for these video games: video game, review score, input device, platform, distributed by, publication date, distribution format, country of origin, developer, publisher, language of work or name, genre, announcement date, game mode **Column Specifications:** 1. **video game** - Precise Explanation: The official title of an electronic game that features user interface and provides visual feedback to players during gameplay 26 WideSeek: Advancing Wide Research via Multi-Agent Scaling - Format Specification: Properly capitalized official game title matching standard marketing punctuation and spelling - Example: \"Lunar Frontier: Operation Genesis\" (60 lines skipped) 14. **game mode** - Precise Explanation: The official playing modes available for the video game, including single-player and multiplayer options - Format Specification: Standard industry terms for video game playing modes - Example: \"single-player video game; competitive multiplayer\" Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: video game, review score, input device, platform, distributed by, publication date, distribution format, country of origin, developer, publisher, language of work or name, genre, announcement date, game mode. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily . The output format is markdown {data_content} . SPARQL Query: PREFIX wd: <http://www.wikidata.org/entity/> PREFIX wdt: <http://www.wikidata.org/prop/direct/> SELECT DISTINCT ?item WHERE { ?item wdt:P31/(wdt:P279wdt:P1647)* wd:Q7889 . ?item wdt:P1411/(wdt:P279wdt:P1647)* wd:Q68094302 . FILTER NOT EXISTS { ?item wdt:P9866/(wdt:P279wdt:P1647)* wd:Q23005410 . } } Table: video game Resident Evil 4 Final Fantasy VII Rebirth Hades II Death Stranding 2: On the Beach Like Dragon: Infinite Wealth Star Wars Outlaws Monster Hunter Wilds Ghost of Yotei Quantum Break No Mans Sky Rubrics: publisher Capcom Square Enix Supergiant Games distributed by PlayStation Store; Microsoft Store; Steam; ... PlayStation Store; Steam Nintendo eShop; Steam; Epic Games Store PlayStation Store PlayStation Store; Microsoft Store; Steam PlayStation Store; Microsoft Store; Steam; ... PlayStation Store; Microsoft Store; Steam PlayStation Store Microsoft Store; Steam; Humble Store Xbox Game Studios PlayStation Store; Microsoft Store; GOG.com; ... Capcom Ubisoft Sony Interactive Entertainment Sega"
        },
        {
            "title": "Sony Interactive Entertainment",
            "content": "Sony Interactive Entertainment; Hello Games game mode single-player video game single-player video game single-player video game single-player video game single-player video game single-player video game single-player video game single-player video game single-player video game co-op mode; single-player video game; multiplayer video game (showing 4 of 14 columns), (showing first 10 rows, total: 34 rows) 27 WideSeek: Advancing Wide Research via Multi-Agent Scaling"
        },
        {
            "title": "Metric\nllm judge",
            "content": "review score llm judge country of origin llm judge announcement date llm judge game mode llm judge Criterion Accept exact official game titles and widely recognized common aliases (e.g., RE4 matches Resident Evil 4). Reject entirely unrelated game titles that do not reference the same core game. Accept lists of scores that contain the same core numerical values regardless of formatting (percentages vs integer scores, semicolon vs comma separators). Allow minor rounding differences and ignore trivial punctuation variations. Reject scores that are significantly different in magnitude from the reference. Accept exact country names and standard common aliases (e.g., US matches United States, UK matches United Kingdom). Reject countries that are not the official origin of the games development. Accept dates that represent the exact same day, month, and year regardless of string format (e.g., 2022-06-02 matches June 2, 2022). Ignore list order and separator punctuation. Reject dates that refer to different official announcement day. Accept game mode names and common shorthand equivalents (e.g., co-op matches co-op mode, multiplayer matches multiplayer video game). Ignore list order and separator punctuation. Reject game modes that are not officially supported by the game. (showing 5 of 14 columns) Example 6: OR Domain: Space Sub-domain: airport Query: Please find all airports that have Paris-Charles de Gaulle Airport as scheduled service destination, or are located on Crete, or are owned by Greece, or are located in Heraklion. Please find airport that meet these conditions, and provide the following attributes for these airport: airport , IATA airport code, elevation above sea level, coordinate location, place served by transport hub, ICAO airport code, aerodrome reference point, date of official opening, named after, patronage, runway, country , located in time zone **Column Specifications:** 1. **airport** * Precise Explanation: location where aircraft take off and land with extended support facilities, mostly for commercial air transport * Format Specification: Full official airport name including any honorific or nickname enclosed in quotes * Example: \"Thessaloniki Airport \"Makedonia\"\" (55 lines skipped) 13. **located in time zone** * Precise Explanation: Official time zone the airport operates within per the tz database * Format Specification: Tz database time zone identifier string * Example: \"Europe/Athens\" Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: airport, IATA airport code, elevation above sea level, coordinate location, place served by transport hub, ICAO airport code, aerodrome reference point, date of official opening, named after, patronage, runway, country, located in time zone. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily . The output format is markdown {data_content} . SPARQL Query: WideSeek: Advancing Wide Research via Multi-Agent Scaling PREFIX wd: <http://www.wikidata.org/entity/> PREFIX wdt: <http://www.wikidata.org/prop/direct/> SELECT DISTINCT ?item WHERE { ?item wdt:P31/(wdt:P279wdt:P1647)* wd:Q1248784 . { { ?item wdt:P521/(wdt:P279wdt:P1647)* wd:Q46280 . } UNION { ?item wdt:P706/(wdt:P279wdt:P1647)* wd:Q34374 . } UNION { ?item wdt:P127/(wdt:P279wdt:P1647)* wd:Q41 . } UNION { ?item wdt:P131/(wdt:P279wdt:P1647)* wd:Q160544 . } } } Table: airport Heraklion International Airport Nikos Kazantzakis Chania International Airport Thessaloniki Airport Makedonia Ottawa Macdonald-Cartier International Airport Roland Garros Airport Athens International Airport Eleftherios Venizelos Toronto Pearson International Airport Vancouver International Airport Guiyang Longdongbao International Airport Warsaw Chopin Airport Rubrics: Column airport"
        },
        {
            "title": "IATA airport code",
            "content": "llm judge aerodrome ence point referllm judge country llm judge located in time zone llm judge place served by transport hub Heraklion named after Heraklion; Nikos Kazantzakis located in time zone Europe/Athens Chania Thessaloniki Ottawa Saint-Denis Athens Toronto Vancouver Guiyang Warsaw Daskalogiannis; Chania Macedonia; Thessaloniki; Macedonia John A. Macdonald; Ottawa; GeorgeEtienne Cartier Reunion; Roland Garros; Saint-Denis Athens; Eleftherios Venizelos Europe/Athens Europe/Athens Eastern Time Zone Indian/Reunion Europe/Athens Lester B. Pearson; Malton; Toronto Vancouver / Eastern Time Zone America/Vancouver Asia/Shanghai Frederic Chopin; Warsaw; Okecie Europe/Warsaw (showing 4 of 13 columns), (showing first 10 rows, total: 21 rows) Criterion Accept the exact airport name, shortened variations (e.g., omitting International), title case or case-insensitive matches, and common aliases that clearly refer to the same airport. Reject names of entirely different airports. Accept the standard 3-letter IATA code regardless of case (e.g., HER matches her or Her). Reject any non-matching 3-letter code that refers to different airport. Accept reference point coordinates that match the airports official reference location, allowing minor decimal deviations (within 100m), and any valid coordinate format regardless of string structure. Reject points that do not correspond to the airports reference point. Accept the exact country name or common aliases (e.g., China matches Peoples Republic of China or PRC), case variations, and abbreviations that are widely recognized. Reject names of entirely different countries. Accept standard time zone names, common abbreviations (e.g., EST matches Eastern Time Zone), and semantic equivalents that refer to the same time zone. Reject any time zone that does not correspond to the airports geographic location. (showing 5 of 13 columns) Example 7: OR NOT Domain: Sports Sub-domain: recurring sporting event edition 29 WideSeek: Advancing Wide Research via Multi-Agent Scaling Query: Curious about all recurring sporting event editions that either have Germany at the 2016 Summer Olympics as participant, are part of the Summer Olympic Games sports season, or include Olympic sporting events as part of their programming, excluding any editions located in Brazil or hosted at Maracana Stadium. Please find recurring sporting event edition that meet these conditions, and provide the following attributes for these recurring sporting event edition: recurring sporting event edition, number of participants, location, start time, end time, officially opened by **Column Specifications:** 1. recurring sporting event edition Precise Explanation: specific yearly or regularly scheduled run of recurring sporting event, representing one distinct iteration of the competition Format Specification: Formal proper noun title including the event name and associated year Example: 2032 Winter Paralympic Games (20 lines skipped) 6. officially opened by Precise Explanation: The public figure or official who performed the formal opening ceremony for the event Format Specification: Full formal name of the individual who conducted the opening ceremony Example: Charles III of the United Kingdom Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: recurring sporting event edition, number of participants, location, start time, end time, officially opened by. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily . The output format is markdown {data_content} . SPARQL Query: PREFIX wd: <http://www.wikidata.org/entity/> PREFIX wdt: <http://www.wikidata.org/prop/direct/> SELECT DISTINCT ?item WHERE { ?item wdt:P31/(wdt:P279wdt:P1647)* wd:Q114609228 . { { ?item wdt:P710/(wdt:P279wdt:P1647)* wd:Q17739057 . }"
        },
        {
            "title": "UNION",
            "content": "{ ?item wdt:P3450/(wdt:P279wdt:P1647)* wd:Q159821 . }"
        },
        {
            "title": "UNION",
            "content": "{ ?item wdt:P2670/(wdt:P279wdt:P1647)* wd:Q18536594 . } } FILTER NOT EXISTS { { ?item wdt:P17/(wdt:P279wdt:P1647)* wd:Q155 . } UNION { ?item wdt:P115/(wdt:P279wdt: P1647)* wd:Q155174 . } } } Table: recurring sporting event edition 2028 Summer Olympics 2020 Summer Olympics 1896 Summer Olympics 1900 Summer Olympics 1904 Summer Olympics 1908 Summer Olympics 1912 Summer Olympics 1916 Summer Olympics 1920 Summer Olympics 1924 Summer Olympics number of participants location start time end time officially opened by / 206.0; 11420.0 14.0; 241.0 26.0; 1226.0 12.0; 651.0 22.0; 2008.0 28.0; 2407.0 / 29.0; 2622.0 44.0; 3088.0 Los Angeles Tokyo Athens Paris St. Louis London Stockholm Berlin Antwerp Paris 2028-07-14T00:00:00Z 2021-07-24T00:00:00Z 1896-04-06T00:00:00Z 1900-05-14T00:00:00Z 1904-07-01T00:00:00Z 1908-04-27T00:00:00Z 1912-05-05T00:00:00Z / 1920-04-20T00:00:00Z 1924-05-04T00:00:00Z 2028-07-30T00:00:00Z 2021-08-08T00:00:00Z 1896-04-15T00:00:00Z 1900-10-28T00:00:00Z 1904-11-23T00:00:00Z 1908-10-31T00:00:00Z 1912-07-27T00:00:00Z / 1920-09-12T00:00:00Z 1924-07-27T00:00:00Z / Naruhito George of Greece / David R. Francis Edward VII Gustaf of Sweden / Albert of Belgium Gaston Doumergue (showing first 10 rows, total: 27 rows) 30 WideSeek: Advancing Wide Research via Multi-Agent Scaling Rubrics: Column recurring sporting event edition"
        },
        {
            "title": "Metric\nllm judge",
            "content": "number of participants llm judge start time llm judge end time llm judge officially opened by llm judge Criterion Accept any reference to the exact same edition of the recurring sporting event, including common aliases and formatting variations (e.g., 1896 Summer Olympics matches Athens 1896 Olympics or 1st Summer Olympic Games). Reject references to different year, different recurring sporting event, or an incorrect edition entirely. Accept participant counts that match the reference values within 5% tolerance for each numerical entry, regardless of separator used (semicolon, comma, space, etc.). Allow omission of trailing decimal zeros (e.g., 206 matches 206.0) and accept the two counts presented in either order as long as both values are correct within tolerance. Reject counts that differ by more than 10% or are entirely unrelated to the events participant numbers. Accept dates that represent the exact same calendar day, month, and year for the events start, regardless of string format or time zone notation (e.g., 2028-07-14T00:00:00Z matches July 14, 2028 or 14/07/2028). Reject any date that does not correspond to the correct start date of the event edition. Accept dates that represent the exact same calendar day, month, and year for the events end, regardless of string format or time zone notation (e.g., 2028-07-30T00:00:00Z matches July 30, 2028 or 30/07/2028). Reject any date that does not correspond to the correct end date of the event edition. Accept the exact name of the dignitary, standard title variations, common aliases, and omissions of non-essential title details (e.g., George of Greece matches King George or George I). Ignore minor variations in name formatting like spacing or punctuation. Reject any person who did not officially open the specified event edition. (showing 5 of 6 columns) A.8. Simulated Environment To facilitate training and validation, we construct stable and realistic simulated search engine, utilizing snapshot of WikiPedia 2025 as the corpus. To guarantee task solvability, we verified that all entities appearing in the answer tables possess corresponding Wikipedia pages and are contained within the utilized dump. We employ Qwen3-0.6B-Embedding2 to extract features from all text data, converting them into corresponding embeddings.This environment exposes two functions: search: Computes the query embedding on the fly, retrieves the top-k nearest documents from the corpus, and returns their URLs and abstracts. open page: Retrieves the full content of specific page given its DocID or URL. We show the schema of these tools as below:"
        },
        {
            "title": "Schema of Tools",
            "content": "SEARCH = { type: function, function: { \"name\": \"search\", \"description\": \"Performs web search: supply string query and optional topk. The tool retrieves the top topk results (default 10) for the query, returning their docid, url, and document content (may be truncated based on token limits).\", \"parameters\": { \"type\": \"object\", \"properties\": { \"query\": { \"type\": \"string\", \"description\": \"The query string for the search.\" }, \"topk\": { \"type\": \"integer\", \"description\": \"Return the top pages.\", } }, \"required\": [ 2https://huggingface.co/Qwen/Qwen3-Embedding-0.6B 31 WideSeek: Advancing Wide Research via Multi-Agent Scaling \"query\" ] } } } OPEN_PAGE = { type: function, function: { name: open_page, description: ( \"Open page by docid or URL and return the complete content. \" \"Provide either docid or url; if both are provided, prefer docid. \" \"The docid or URL must come from prior search tool results.\" ), parameters: { type: object, properties: { docid: { type: string, description: Document ID from search results to resolve and fetch., }, url: { type: string, description: Absolute URL from search results to fetch., }, }, required: [], }, }, } A.9. Evaluation To comprehensively assess the quality of the generated tables across different granularities, we employ three evaluation metrics: Success Rate, Row F1, and Item F1. These metrics evaluate the performance at the table, row, and cell levels, respectively. Specifically, we use the LLM-based judge with column-wise rubrics to evaluate whether each generated cell is aligned with the corresponding ground truth cell. We use the GPT-4.1 as the default judge LLM. Success Rate: This is the strictest metric, operating at the table level. sample is considered success only if the answer table exactly matches the ground truth in terms of both content and structure, without any errors. Row F1: This metric evaluates the retrieval and generation accuracy at the row level. We calculate the precision and recall of the generated rows against the ground truth rows to compute the F1 score. predicted row is considered correct match only if all the cells within that row are perfectly consistent with the corresponding ground truth row. Item F1: To provide fine-grained assessment, Item F1 evaluates performance at the cell level. It calculates the F1 score based on the individual data items (cells) within the table. This metric focuses on the models ability to extract or generate specific details correctly, regardless of whether the entire row is perfect. B. Experiments B.1. Cold Start To bootstrap the unified policy πθ with the capability to perform complex task decomposition and robust information seeking, we employ Cold Start phase via Supervised Fine-Tuning (SFT). Trajectory Collection and Filtering. We utilize multiple teacher policies (e.g., DeepSeek-V3.2, Kimi-K2) to generate diverse set of rollout trajectories on the training set Dtrain. For each query Qi, we collect set of candidate trajectories {T i,m}M m=1. To ensure the quality of the training signal, we introduce strict filtering mechanism based on the Item-level F1 score (F 1item) against the ground truth table . trajectory is retained for the SFT dataset DSF if and only if its performance exceeds threshold η: DSF = {T i,m Item-F1(Answer(T i,m), ) > η} (7) We set the η as 0.6. 32 WideSeek: Advancing Wide Research via Multi-Agent Scaling SFT Optimization. The policy πθ is initialized by minimizing the standard negative log-likelihood loss over the filtered high-quality trajectories. Let be represented as sequence of tokens (x1, x2, . . . , xL). The SFT objective is defined as: LSF (θ) = ET DSF (cid:88) log πθ(xt x<t) (8) The loss is only computed on the tokens generated by models itself (the thoughts and actions). t=1 B.2. Training Dynamics Figure 9. The trianing dynamics of WideSeek-8B-RL. We present the evolution of training rewards and the times of tool calls throughout the entire training process. B.3. Setting We use VERL (Sheng et al., 2025) and AgentLightning (Luo et al., 2025) as the RL training framework. We use Qwen3-8B (Yang et al., 2025) as the base model. The RL hyper parameters are shown in Table 9. We use 64 H100 GPUs for RL Training. We use function calling to create sub-agent for main agent. The corresponding tool schema is as below. We use the GPT-4.1 as the default judge LLM."
        },
        {
            "title": "Schema of Tools",
            "content": "CREATE_SUB_AGENT = { \"type\": \"function\", \"function\": { \"name\": \"create_sub_agent\", \"description\": \"Create more sub-agents to gather information in parallel.\", \"parameters\": { \"type\": \"object\", \"properties\": { \"tasks\": { \"type\": \"array\", 33 WideSeek: Advancing Wide Research via Multi-Agent Scaling"
        },
        {
            "title": "Value",
            "content": "64 6 32768 8192 1e-6 0.28 0.2 80 Table 9. The hyper parameters for RL traning. \"description\": \"A list of tasks, each for new sub-agent. The number of subtasks created at one time can be large or small.\", \"items\": { \"type\": \"object\", \"properties\": { \"agent_id\": {\"type\": \"string\", \"description\": \"A unique identifier for the subagent, e.g., agent_001.\"}, \"task\": {\"type\": \"string\", \"description\": \"The specific task this sub-agent must perform.\"} }, \"required\": [\"agent_id\", \"task\"] } } }, \"required\": [\"tasks\"] } } } B.4. Case Study We illustrate the unified trajectory of the same task query produced by 4 models in Figure 10: Qwen3-30B-A3B-Thinking, WideSeek-8B-SFT-RL, WideSeek-8B-SFT, WideSeek-8B-RL. And for better understanding, we show case trajectory of WideSeek-8B-RL as follows."
        },
        {
            "title": "Case trajectory on WideSeekBench",
            "content": "Question: Please find all films that were nominated for the Academy Award for Best Sound and received the National Board of Review: Top Ten Films award, excluding any films that feature Constantin Alexandrov as cast member or have Anna Hamilton Phelan as screenwriter. Please find film that meet these conditions, and provide the following attributes for these film: film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language of film or TV show, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating *Column Specifications:** 1. **film** Precise Explanation: sequence of moving images stored on film stock, released as feature-length motion picture. Format Specification: Full official title of the film as single string. Example: Midnight Tide 34 WideSeek: Advancing Wide Research via Multi-Agent Scaling Figure 10. Multi-Agent Trajectory 2. **color** Precise Explanation: The color format used to produce the film. Format Specification: Single term indicating the color format, typically color or black and white. Example: black and white 3. **country of origin** Precise Explanation: The country or countries where the film was primarily produced and created. Format Specification: List of country names separated by semicolons if multiple countries apply. Example: United Kingdom; Canada 4. **producer** Precise Explanation: The primary person or persons responsible for overseeing the films core production work, excluding executive or associate producers. Format Specification: List of producer full names separated by semicolons if multiple producers are credited. Example: Elara Voss; Marcus Hale 5. **review score** Precise Explanation: Aggregate or individual critical review scores the film received from rating platforms or professional critics. Format Specification: Multiple score values (percentages, numerical ratings) separated by semicolons. Example: 88%; 9.2; 95 6. **CNC film rating (France)** Precise Explanation: Age restriction or content warning category assigned to the film by Frances CNC Board of Film Classification. Format Specification: Rating category label, with multiple categories separated by semicolons if applicable. Example: 12 years and over; violence warning WideSeek: Advancing Wide Research via Multi-Agent Scaling 7. **assessment** Precise Explanation: List of social representation or media diversity tests the film has been evaluated against. Format Specification: Names of assessment tests separated by semicolons. Example: Bechdel test; Mako Mori test 8. **narrative location** Precise Explanation: The fictional or real-world settings where the films plot and story take place. Format Specification: List of location names separated by semicolons if multiple settings are featured. Example: Iceland; Reykjavık; Northern Fjords 9. **distributed by** Precise Explanation: The companies responsible for distributing the film to audiences via theatrical, streaming, or home video channels. Format Specification: List of distribution company names separated by semicolons if multiple distributors are involved. Example: Warner Bros. Pictures; Max 10. **original language of film or TV show** Precise Explanation: The primary language used during the original production and recording of the films dialogue and core audio. Format Specification: Single language name or list separated by semicolons if multiple original languages are used. Example: Spanish; Catalan 11. **composer** Precise Explanation: The person or persons who created the original musical score for the film, separate from lyricists. Format Specification: List of composer full names separated by semicolons if multiple composers are credited. Example: Lila Marquez; Theo Carter 12. **distribution format** Precise Explanation: The primary method used to distribute the film to viewers. Format Specification: Single term describing the distribution format, such as theatrical release or video on demand. Example: theatrical release 13. **box office** Precise Explanation: The total ticket sales revenue earned by the film, potentially including domestic and international totals. Format Specification: Numeric box office values separated by semicolons if multiple totals are provided. Example: 78245100.0; 195320000.0 14. **director** Precise Explanation: The person or persons responsible for overseeing the films creative direction and artistic vision throughout production. Format Specification: List of director full names separated by semicolons if multiple directors are credited. Example: Jasper Reed 15. **Kijkwijzer rating** Precise Explanation: The age-based content rating assigned to the film under the Dutch Kijkwijzer media rating system. Format Specification: Numeric rating value, with multiple ratings separated by semicolons if applicable. Example: 9; 12 36 WideSeek: Advancing Wide Research via Multi-Agent Scaling 16. **production designer** Precise Explanation: The person responsible for creating the films overall visual aesthetic, including set design, props, and visual conceptualization. Format Specification: List of production designer full names separated by semicolons if multiple designers are credited. Example: Zara Bennett 17. **duration** Precise Explanation: The total runtime of the film measured in minutes. Format Specification: Numeric value representing the total runtime in minutes. Example: 122.0 18. **filming location** Precise Explanation: The actual real-world locations where the film was physically shot, distinct from its narrative setting. Format Specification: List of filming location names separated by semicolons if multiple locations were used. Example: Vancouver; British Columbia; Stanley Park 19. **film editor** Precise Explanation: The person or persons responsible for assembling raw footage into the final finished cut of the film. Format Specification: List of film editor full names separated by semicolons if multiple editors are credited. Example: Noah Brooks; Sienna Torres 20. **production company** Precise Explanation: The companies that funded and oversaw the core production process of the film. Format Specification: List of production company names separated by semicolons if multiple companies were involved. Example: A24; Reel Pictures Studio 21. **FSK film rating** Precise Explanation: The age restriction rating assigned to the film under the German FSK film classification system. Format Specification: Rating in the standard format FSK [age]. Example: FSK 12 Try to provide all information. Only use / when you believe the information is truly unavailable or cannot be obtained. Please organize the results in one Markdown table with the following columns: film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language of film or TV show, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating. Dont ask me any questions, just output the results according to the columns without omitting cells arbitrarily. The output format is markdown {data content} . Round 1 (Main Agent + Sub Agents: agent 001, agent 002)"
        },
        {
            "title": "Main Agent",
            "content": "37 WideSeek: Advancing Wide Research via Multi-Agent Scaling <think> Okay, lets tackle this query step by step. The user wants all films that were nominated for the Academy Award for Best Sound and received the National Board of Review: Top Ten Films award, but exclude those with Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Then, they need list of attributes for each film. First, need to find films that are in both the Academy Award Best Sound nominees and the National Board of Review Top Ten Films. Then, filter out any films that have either of the two excluded people. But the user also wants lot of specific details about each film, which might not be straightforward to get. Since dont have access to external databases like Wikidata, Ill need to create sub-agents to search for this information. Let me start by creating sub-agents to get the list of Academy Award Best Sound nominees and the National Board of Review Top Ten Films. Then, can cross-reference these lists to find overlapping films. Next, check each film in the overlap for the presence of Constantin Alexandrov or Anna Hamilton Phelan, and exclude those that do. Finally, collect all the required attributes for each remaining film. But how to structure this? Let me first split the tasks. First, get the Academy Award nominees for Best Sound. Then, get the National Board of Review Top Ten Films. Then, find the intersection. Then, for each of those, check for the two excluded... </think> <tool call> {name:create sub agent,arguments:{tasks:[{agent id:agent 001,task:List all films nominated for the Academy Award for Best Sound.},{agent id:agent 002,task:List all films that received the National Board of Review: Top Ten Films award.}]}} </tool call> <tool response> Sub-agents for call chatcmpl-tool-7f2e53c1e9bd41d6bf46824416e0be1f completed. Reports: Report from agent 001 Here is list of films nominated for the Academy Award for Best Sound, based on the historical data available in the provided Wikipedia content (up to the 1970s): ### **1930s** - **19291930 (3rd)**: *The Big House*, *The Case of Sergeant Grischa*, *The Love Parade*, *Raffles*, *Song of the Flame* - **19301931 (4th)**: *Paramount Publix Studio Sound Department*, *Metro-Goldwyn-Mayer Studio Sound Department*, *RKO Radio Studio Sound Department*, *Samuel Goldwyn-United Artists Studio Sound Department* - **19311932 (5th)**: *Paramount Publix Studio Sound Department*, *Metro-Goldwyn-Mayer Studio Sound Department*, *RKO Radio Studio Sound Department*, *Warner Bros.-First National Studio Sound Department* - **19321933 (6th)**: *A Farewell to Arms*, *42nd Street*, *Gold Diggers of 1933*, *I Am Fugitive from Chain Gang* - **1934 (7th)**: *One Night of Love*, *The Affairs of Cellini*, *Cleopatra*, *Flirtation Walk*, *The Gay Divorcee*, *Imitation of Life*, *Viva Villa!*, *The White Parade* - **1935 (8th)**: *Naughty Marietta*, *$1,000 Minute*, *Bride of Frankenstein*, *Captain Blood*, *The Dark Angel*, *I Dream Too Much*, *Lost Horizon*, *Maytime*, *Thanks Million* - **1936 (9th)**: *San Francisco*, *Banjo on My Knee*, *The Charge of the Light Brigade*, *Dodsworth*, *General Spanky*, *Mr. Deeds Goes to Town*, *The Texas Rangers*, *That Girl from Paris*, *You Cant Take It with You* - **1937 (10th)**: *The Hurricane*, *The Girl Said No*, *Hitting New High*, *In Old Chicago*, *The Life of Emile Zola*, *Lost Horizon*, *Maytime*, *Topper*, *Wells Fargo* ### **1940s** - **1940 (13th)**: *Strike Up the... </tool response> 38 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 001 Task: List all films nominated for the Academy Award for Best Sound. Tool Call 1: <think> Okay, need to list all films that were nominated for the Academy Award for Best Sound. Let me think about how to approach this. First, the user is asking... </think> <tool call> {name:search,arguments:{query:Academy Award for Best Sound nominations,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user is asking for list of all films nominated for the Academy Award for Best Sound. Let me start by understanding the search results they provided.... </think> <tool call> {name:open page,arguments:{docid:enwiki/99028}} </tool call> <tool response> ... </tool response> Response <think> Okay, so the user wants list of all films nominated for the Academy Award for Best Sound. Let me start by understanding the information have from the search... </think> <agent response> Here is list of films nominated for the Academy Award for Best Sound, based on the historical data available in the provided Wikipedia content (up to the 1970s): ###... </agent response> Sub Agent: agent 002 Task: List all films that received the National Board of Review: Top Ten Films award. Tool Call 1: <think> Okay, the user wants list of all films that have won the National Board of Reviews Top Ten Films award. First, need to figure out how to get... </think> <tool call> {name:search,arguments:{query:National Board of Review Top Ten Films award winners,topk:20}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user wants list of all films that received the National Board of Review: Top Ten Films award. Let me check the search results they provided. Looking... </think> <tool call> {name:open page,arguments:{docid:enwiki/8644529}} </tool call> <tool response> ... </tool response> Response <think> Okay, the user is asking for list of all films that received the National Board of Review: Top Ten Films award. Let me look through the information have.... </think> <agent response> Here is list of all films that received the **National Board of Review: Top Ten Films** award, organized by year (19292024): ### **1920s** - **1929**: *Applause*, *Broadway*, *Bulldog Drummond*,... </agent response>"
        },
        {
            "title": "Main Agent",
            "content": "Round 2 (Main Agent + Sub Agents: agent 003, agent 004) 39 WideSeek: Advancing Wide Research via Multi-Agent Scaling <think> Okay, so the user wants list of films that were nominated for the Academy Award for Best Sound and also received the National Board of Review: Top Ten Films award. But theres catch: we need to exclude any films that have Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Then, for each qualifying film, we need to gather bunch of attributes like color, country of origin, producer, etc. First, need to find the intersection of the two lists. The first sub-agent gave me the list of Best Sound nominees from the 1930s to 1970s, and the second sub-agent provided the Top Ten Films from the National Board of Review from 1929 to 2024. So, need to compare these two lists to find films that are present in both. But looking at the reports, the Best Sound list from the first agent has some entries that are studio sound departments, like Paramount Publix Studio Sound Department. Those might not be actual films, so should probably ignore those. The user mentioned films, so maybe those entries are errors or not applicable. should check each entry to see if they are actual films. Next, need to cross-reference the two lists. Let me start by looking for common films in both lists. For example, in the 1930s, the Best Sound list includes The Love Parade and Viva Villa! which also appear in the National Board of Review list. But need... </think> <tool call> {name:create sub agent,arguments:{tasks:[{agent id:agent 003,task:Identify films that are common between the Academy Award for Best Sound nominees and the National Board of Review: Top Ten Films award list, from 1930s to 1970s.},{agent id:agent 004,task:Check if any of the common films listed in agent 003s report include Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter.}]}} </tool call> <tool response> Sub-agents for call chatcmpl-tool-a9815765f0ec4795b345b11a27db9fbd completed. Reports: Report from agent 003 The films common between the Academy Award for Best Sound nominees and the National Board of Reviews Top Ten Films from the 1930s to 1970s are as follows: ### **1930s** - **1930**: - *The Love Parade* (National Board) Academy Award nominee (1930). - **1931**: - *City Lights* (National Board) Academy Award nominee (1931). - **1932**: - *I Am Fugitive from Chain Gang* (National Board) Academy Award nominee (1932). - **1933**: - *Topaze* (National Board) Academy Award nominee (1933). - **1934**: - *It Happened One Night* (National Board) Academy Award nominee (1934). - **1935**: - *The Informer* (National Board) Academy Award nominee (1935). - **1936**: - *Mr. Deeds Goes to Town* (National Board) Academy Award nominee (1936). - **1937**: - *A Star Is Born* (National Board) Academy Award nominee (1937). - **1938**: - *Snow White and the Seven Dwarfs* (National Board) Academy Award nominee (1939). - **1939**: - *Stagecoach* (National Board) Academy Award nominee (1939). ### **1940s** - **1940**: - *The Grapes of Wrath* (National Board) Academy Award nominee (1940). - **1941**: - *Citizen Kane* (National Board) Academy Award nominee (1941). - **1942**: 40 WideSeek: Advancing Wide Research via Multi-Agent Scaling - *Casablanca* (National Board) Academy Award nominee (1942). - **1943**: - *Casablanca* (National Board) Academy Award nominee (1943). - **1944**: - *The Miracle of Morgans Creek* (National Board) Academy Award nominee (1944). - **1945**: - *The Lost Weekend* (National Board)... </tool response> Sub Agent: agent"
        },
        {
            "title": "Identify films that are common between the Academy Award for Best Sound nominees and the",
            "content": "Task: National Board of Review: Top Ten Films award list, from 1930s to 1970s. Tool Call 1: <think> Okay, so need to find films that are common between the Academy Award for Best Sound nominees and the National Board of Reviews Top Ten Films list from the... </think> <tool call> {name:search,arguments:{query:National Board of Review Top Ten Films 1930s to 1970s,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user wants to find films that are common between the Academy Award for Best Sound nominees and the National Board of Reviews Top Ten Films list from... </think> <tool call> {name:open page,arguments:{docid:enwiki/99028}} </tool call> <tool response> ... </tool response> Response <think> Okay, so the user wants to find films that are common between the Academy Award for Best Sound nominees and the National Board of Reviews Top Ten Films list from... </think> <agent response> The films common between the Academy Award for Best Sound nominees and the National Board of Reviews Top Ten Films from the 1930s to 1970s are as follows: ### **1930s** </agent response> 41 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 004 Task: Check if any of the common films listed in agent 003s report include Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Tool Call 1: <think> Okay, need to figure out if Constantin Alexandrov is cast member in any of the common films listed in agent 003s report or if Anna Hamilton Phelan is a... </think> <tool call> {name:search,arguments:{query:agent 003s report common films,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user wants to check if any of the common films listed in agent 003s report include Constantin Alexandrov as cast member or Anna Hamilton Phelan as a... </think> <tool call> {name:search,arguments:{query:Constantin Alexandrov filmography Anna Hamilton Phelan screenwriting credits,topk:5}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, lets see. The user wants to check if any of the common films listed in agent 003s report include Constantin Alexandrov as cast member or Anna Hamilton Phelan as... </think> <tool call> {name:search,arguments:{query:Constantin Alexandrov phy,topk:5}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, so the task is to check if any of the common films listed in agent 003s report include Constantin Alexandrov as cast member or Anna Hamilton Phelan as a... </think> <tool call> {name:open page,arguments:{docid:enwiki/18716679}} </tool call> <tool response> ... </tool response> Response <think> Okay, so the task is to check if any of the common films listed in agent 003s report include Constantin Alexandrov as cast member or Anna Hamilton Phelan as a... </think> <agent response> After reviewing the search results and full content of the films listed in agent 003s report, there is no evidence that **Constantin Alexandrov** appears as cast member or **Anna Hamilton... </agent response> filmograRound 3 (Main Agent + Sub Agents: agent 005, agent 006, agent 007, agent 008)"
        },
        {
            "title": "Main Agent",
            "content": "<think> Okay, so the user wants all films that were nominated for the Academy Award for Best Sound and also received the National Board of Review: Top Ten Films award, but exclude any that have Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Then, they need bunch of attributes for each film. First, need to find the intersection of the two lists. From the first sub-agent, agent 001 gave the Best Sound nominees, and agent 002 gave the National Board Top Ten Films. Then agent 003 found the common films between those two lists. The result was list of films from 1930s to 1970s. But then agent 004 checked if any of those films include Constantin Alexandrov or Anna Hamilton Phelan. The report said no, so those films are safe. But should make sure that agent 004s conclusion is accurate. Maybe double-check some of the films mentioned. For example, *Casablanca* is in both lists. Let me verify if Constantin Alexandrov was in *Casablanca*. Wait, the report says agent 004 checked and found no mention of him. Similarly, Anna Hamilton Phelan isnt listed as screenwriter for any of them. Now, the next step is to gather all the required attributes for each of these films. The columns are film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, 42 WideSeek: Advancing Wide Research via Multi-Agent Scaling duration, filming location, film editor, production company, FSK film rating. But... </think> <tool call> {name:create sub agent,arguments:{tasks:[{agent id:agent 005,task:Retrieve detailed attributes for *It Happened One Night* (1934): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.},{agent id:agent 006,task:Retrieve detailed attributes for *Casablanca* (1942): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.},{agent id:agent 007,task:Retrieve detailed attributes for *The Grapes of Wrath* (1940): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.},{agent id:agent 008,task:Retrieve detailed attributes for *Citizen Kane* (1941): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.}]}} </tool call> <tool response> Sub-agents for call chatcmpl-tool-b83f2882255149a29a02640892fa7924 completed. Reports: Report from agent 005 The detailed attributes for *It Happened One Night* (1934) are as follows: ### **Film Attributes** - **Title**: *It Happened One Night* - **Color**: Black-and-white (typical for 1934 films). - **Country of Origin**: United States. - **Producer**: Frank Capra and Harry Cohn. - **Review Score**: - **Rotten Tomatoes**: 98% score, 9.1/10 average. - **Metacritic**: 87/100. - **CNC Film Rating (France)**: Not explicitly mentioned in available sources. Frances CNC (Commission Nationale du Cinema) ratings may require checking French-specific databases or archives. - **Assessment**: Widely regarded as one of the greatest films ever made, winning all five major Academy Awards (Best Picture, Best Director, Best Actor, Best Actress, Best Adapted Screenplay). - **Narrative Location**: Florida (Ellies escape), New York City, and cross-country bus journey. - **Distributed By**: Columbia Pictures. - **Original Language**: English. - **Composer**: Howard Jackson and Louis Silvers. - **Distribution Format**: Theatrical release (standard for 1934). - **Box Office**: $2.5 million (worldwide rentals). - **Director**: Frank Capra. - **Kijkwijzer Rating (Netherlands)**: Not found in available sources. Dutch ratings would need specialized database. - **Production Designer**: Not explicitly listed in the Wikipedia infobox. Research into production design credits or industry archives may reveal this. - **Duration**: 105 minutes. - **Filming Location**: Part of the film was shot on Thousand Oaks Boulevard in Thousand Oaks, California. - **Film Editor**: Gene Havlick. - **Production Company**: Columbia Pictures. - **FSK Film Rating (Germany)**: Not specified in sources. Germanys FSK ratings would require checking... </tool response> 43 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 005 Task: Retrieve detailed attributes for *It Happened One Night* (1934): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating. Tool Call 1: <think> Okay, so the user wants detailed attributes for the 1934 film It Happened One Night. Let me start by figuring out what information need to retrieve. The task includes... </think> <tool call> {name:search,arguments:{query:It Happened One Night 1934 detailed attributes,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user wants detailed attributes for the 1934 film It Happened One Night. Let me look through the search results they provided. The first result is the Wikipedia... </think> <tool call> {name:open page,arguments:{docid:enwiki/44696}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, let me start by looking at the users request. They want detailed attributes for the 1934 film It Happened One Night. The attributes listed include film, color, country of... </think> <tool call> {name:search,arguments:{query:It Happened One Night 1934 FSK rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, let me figure out how to approach this. The user wants detailed attributes for the 1934 film It Happened One Night. So far, Ive used the search tool to... </think> <tool call> {name:search,arguments:{query:It Happened One Night 1934 production designer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 5: <think> Okay, so the user is asking for detailed attributes of the 1934 film It Happened One Night. Let me go through what they need. The attributes listed include film, color,... </think> <tool call> {name:search,arguments:{query:It Happened One Night 1934 production designer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 6: <think> Okay, lets see. The user is asking for detailed attributes of the 1934 film It Happened One Night. They want info on film, color, country of origin, producer, review score,... </think> <tool call> {name:open page,arguments:{docid:enwiki/44696}} </tool call> <tool response> ... </tool response> Response <think> Okay, let me try to figure out how to get all the detailed attributes for It Happened One Night (1934). The user is asking for bunch of specific info:... </think> <agent response> The detailed attributes for *It Happened One Night* (1934) are as follows: ### **Film Attributes** - **Title**: *It Happened One Night* - **Color**: Black-and-white (typical for 1934 films). - **Country... </agent response> WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 006 at1942 1942 detailed {name:search,arguments:{query:Casablanca {name:search,arguments:{query:Casablanca Task: You have reached the token limit. Please summarize your relevant findings immediately based on existing information. Tool Call 1: <think> Okay, need to retrieve detailed attributes for the 1942 film Casablanca. Let me start by breaking down the users request. They want information on various aspects like film, color,... </think> <tool call> tributes,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user wants detailed attributes for the 1942 film Casablanca. Let me look through the search results they provided. The first result is about 1942 in film, which... </think> <tool call> {name:open page,arguments:{docid:enwiki/57585}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, lets see. The user wants detailed attributes for the 1942 film Casablanca. The search results gave me few documents, but the most relevant one is the Wikipedia page... </think> <tool call> CNC France,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, lets tackle this task step by step. The user wants detailed attributes for the 1942 film Casablanca, including things like color, country of origin, producer, review score, CNC film... </think> <tool call> {name:search,arguments:{query:Casablanca 1942 Kijkwijzer rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 5: <think> Okay, lets tackle this task step by step. The user wants detailed attributes for the 1942 film Casablanca. So far, Ive used the search and open page tools to get information... </think> <tool call> France,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 6: <think> Okay, lets see. The user is asking for detailed attributes of the 1942 film Casablanca. They want information on film, color, country of origin, producer, review score, CNC film rating... </think> <tool call> {name:search,arguments:{query:Casablanca 1942 FSK film rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 7: <think> Okay, lets see. The user wants detailed attributes for the 1942 film Casablanca. They listed bunch of specific fields: film, color, country of origin, producer, review score, CNC film... </think> <tool call> {name:search,arguments:{query:Casablanca France,topk:1}} </tool call> <tool response> ... </tool response> {name:search,arguments:{query:Casablanca 1942 CNC film rating rating rating"
        },
        {
            "title": "CNC",
            "content": "1942 45 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 006 (continued) 1942 production {name:search,arguments:{query:Casablanca Tool Call 8: <think> Okay, lets see. The user wants detailed attributes for the 1942 film Casablanca. They listed bunch of specific pieces of information: film, color, country of origin, producer, review score,... </think> <tool call> designer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 9: <think> Okay, lets tackle this task step by step. The user is asking for detailed attributes of the 1942 film Casablanca. They want information on film, color, country of origin, producer,... </think> <tool call> {name:open page,arguments:{docid:enwiki/57585}} </tool call> <tool response> ... </tool response> Tool Call 10: <think> Okay, let me tackle this task. The user wants detailed attributes for the 1942 film Casablanca. They listed bunch of categories like film, color, country of origin, producer, review... </think> <tool call> {name:search,arguments:{query:Casablanca production signer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 11: <think> Okay, lets see. The user wants detailed attributes for the 1942 film Casablanca. They listed bunch of categories: film, color, country of origin, producer, review score, CNC film rating... </think> <tool call> designer,topk:1}} </tool call> <tool response> reached the token limit </tool response> Response <think> Okay, lets tackle this. The user wants detailed attributes for the 1942 film Casablanca. They listed bunch of categories like film, color, country of origin, producer, etc. First, I... </think> <agent response> Based on the available information from the Wikipedia page for *Casablanca* (1942), here is comprehensive summary of its detailed attributes: {name:search,arguments:{query:Casablanca production 1942 de- ### **Film Details** - **Title**: Casablanca - **Type**: American... </agent response> 46 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 007 {name:search,arguments:{query:The Grapes of Wrath 1940 FSK ratTask: You have reached the token limit. Please summarize your relevant findings immediately based on existing information. Tool Call 1: <think> Okay, need to find detailed attributes for the 1940 film The Grapes of Wrath. Let me start by breaking down the task. The user is asking for lot... </think> <tool call> {name:search,arguments:{query:The Grapes of Wrath 1940,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, lets tackle this task. The user wants detailed attributes for the 1940 film The Grapes of Wrath. First, need to figure out which of the search results are... </think> <tool call> {name:open page,arguments:{docid:enwiki/4350811}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, lets see. The user wants detailed attributes for the 1940 film The Grapes of Wrath. They listed bunch of specific fields like film, color, country of origin, producer,... </think> <tool call> ing,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, so the user wants detailed attributes for the 1940 film The Grapes of Wrath. Let me start by going through the information have. First, from the initial search... </think> <tool call> {name:search,arguments:{query:The Grapes of Wrath 1940 production designer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 5: <think> Okay, so need to retrieve detailed attributes for the 1940 film The Grapes of Wrath. Let me start by recalling the information already have from the previous searches.... </think> <tool call> ing,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 6: <think> Okay, so the user is asking for detailed attributes of the 1940 film The Grapes of Wrath. Let me see what information have so far. From the previous search... </think> <tool call> {name:search,arguments:{query:The Grapes of Wrath 1940 production designer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 7: <think> Okay, so the user is asking for detailed attributes of the 1940 film The Grapes of Wrath. Let me start by going through the information have. First, from the... </think> <tool call> {name:search,arguments:{query:The Grapes of Wrath 1940 production designer,topk:1}} </tool call> <tool response> ... </tool response> {name:search,arguments:{query:The Grapes of Wrath 1940 CNC rat47 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 007 (continued) Tool Call 8: <think> Okay, lets see. The user wants detailed attributes for the 1940 film The Grapes of Wrath. Theyve already done search and opened the Wikipedia page for the film. The... </think> <tool call> {name:search,arguments:{query:The Grapes of Wrath 1940 production designer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 9: <think> Okay, lets tackle this task step by step. The user wants detailed attributes for The Grapes of Wrath (1940), including film, color, country of origin, producer, review score, CNC film... </think> <tool call> {name:open page,arguments:{docid:enwiki/4350811}} </tool call> <tool response> ... </tool response> Tool Call 10: <think> Okay, let me see. The user is asking for detailed attributes of the 1940 film The Grapes of Wrath. They listed bunch of categories like film, color, country of... </think> <tool call> {name:search,arguments:{query:The Grapes of Wrath 1940 production designer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 11: <think> Okay, so need to retrieve detailed attributes for The Grapes of Wrath (1940) film. Let me start by recalling the users query. They want information on film, color, country... </think> <tool call> {name:search,arguments:{query:The Grapes of Wrath 1940 production designer,topk:1}} </tool call> <tool response> reached the token limit </tool response> Response <think> Okay, so the user wants detailed attributes for the 1940 film The Grapes of Wrath. Let me start by recalling the information have from the previous searches and the... </think> <agent response> ### Detailed Attributes for *The Grapes of Wrath* (1940) 1. **Film**: - **Title**: *The Grapes of Wrath* (1940) - **Type**: Drama / Adaptation of John Steinbecks novel. - **Country of... </agent response> 48 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 008 {name:search,arguments:{query:Citizen Kane Task: Retrieve detailed attributes for *Citizen Kane* (1941): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating. Tool Call 1: <think> Okay, need to retrieve detailed attributes for the film *Citizen Kane* (1941). The user has listed bunch of specific pieces of information they want, so should start... </think> <tool call> </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, let me start by understanding the task. The user wants detailed attributes for the film Citizen Kane (1941). They listed bunch of specific pieces of information: color, country... </think> <tool call> {name:open page,arguments:{docid:enwiki/5224}} </tool call> <tool response> ... </tool response> Response <think> Okay, let me start by figuring out what the user needs. They want detailed attributes for the film *Citizen Kane* (1941), including things like color, country of origin, producer, review... </think> <agent response> Here is the detailed information about *Citizen Kane* (1941): 1941,topk:10}} 1. **Film**: *Citizen Kane* 2. **Color**: Black and white (though the innovative 1940 trailer was in color). 3. **Country of Origin**:... </agent response>"
        },
        {
            "title": "Main Agent",
            "content": "Round 4 (Main Agent + Sub Agents: agent 009, agent 010) <think> Okay, so the user is asking for all films that were nominated for the Academy Award for Best Sound and received the National Board of Review: Top Ten Films award, excluding any films with Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Then, they want all the specified attributes for those films. First, need to find the intersection of the two lists: Academy Award Best Sound nominees and National Board of Review Top Ten Films. The previous agents did that for the 1930s to 1970s and found some common films like *It Happened One Night*, *Casablanca*, *The Grapes of Wrath*, and *Citizen Kane*. Agent 004 checked if any of these have the excluded names and found none, so those four films are still in the running. Now, the user wants detailed attributes for each of these films. Agents 005, 006, 007, and 008 retrieved info for each, but some details are missing, like regional ratings (CNC, Kijkwijzer, FSK) and production designers. The user wants all information, using / for unavailable data. need to compile the data from these reports into the required markdown table. Let me check each films details: For *It Happened One Night* (1934): - Color: Black-and-white - Country of Origin: United States - Producer: Frank Capra and Harry Cohn - Review Score: 98% on Rotten Tomatoes, 9.1/10 average - CNC Film Rating: Not found - Assessment: Won 5 Academy Awards - Narrative Location: Florida, New York City, cross-country bus journey -... </think> <tool call> {name:create sub agent,arguments:{tasks:[{agent id:agent 009,task:Retrieve detailed attributes for *The Life of Emile Zola* (1937): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.},{agent id:agent 010,task:Retrieve detailed attributes for *The Best Years of Our Lives* (1946): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer 49 WideSeek: Advancing Wide Research via Multi-Agent Scaling rating, production designer, duration, filming location, film editor, production company, FSK film rating.}]}} </tool call> <tool response> Sub-agents for call chatcmpl-tool-82a2e428e1c443c29b5e1a8683cbeb7b completed. Reports: Report from agent 009 ### Detailed Attributes for *The Life of Emile Zola* (1937): 1. **Film**: Biographical drama based on Emile Zolas life, focusing on the Dreyfus Affair and his literary career. 2. **Color**: Black-and-white (standard for 1937 films). 3. **Country of Origin**: United States. 4. **Producer**: Henry Blanke (produced by Warner Bros.). 5. **Review Score**: - **Rotten Tomatoes**: 92% from 72 reviews (average score: 7.5/10). - **Critics Consensus**: Praised for its historical significance, Paul Munis performance, and thematic relevance to 1930s political tensions. 6. **CNC Film Rating (France)**: Not explicitly mentioned in available sources. The films connection to the Dreyfus Affair may have influenced its reception in France, but no official CNC rating is listed. 7. **Assessment**: Regarded as landmark biographical film, winning three Oscars (Best Picture, Best Supporting Actor, Best Screenplay). Critics highlighted its moral urgency and Paul Munis portrayal of Zola. 8. **Narrative Location**: Set in 19th-century France, spanning Paris, French Guiana (Devils Island), and other locations tied to Zolas life and the Dreyfus Affair. 9. **Distributed By**: Warner Bros. Pictures. 10. **Original Language**: English (though the story is about French author and historical events). 11. **Composer**: Max Steiner (score awarded to Leo F. Forbstein). 12. **Distribution Format**: Theatrical release (no mention of home media formats). 13. **Box Office**: Specific revenue figures are not provided in available sources. 14. **Director**: William Dieterle. 15. **Kijkwijzer Rating (Netherlands)**: Not mentioned in sources. 16. **Production Designer**: Not explicitly listed in the... </tool response> 50 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 009 {name:search,arguments:{query:The Life of Emile Zola 1937 film deTask: Retrieve detailed attributes for *The Life of Emile Zola* (1937): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating. Tool Call 1: <think> Okay, need to find detailed attributes for the 1937 film The Life of Emile Zola. Lets start by breaking down the task. The user is asking for lot... </think> <tool call> tails,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user is asking for detailed attributes of the 1937 film The Life of Emile Zola. Let me start by looking at the search results they provided. The... </think> <tool call> {name:open page,arguments:{docid:enwiki/44210}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, let me start by going through the information provided from the open page tool. The user wants detailed attributes for the 1937 film The Life of Emile Zola. The first... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola 1937 CNC rating FSK rating box office,topk:3}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, so need to find detailed attributes for the 1937 film The Life of Emile Zola. The user is asking for lot of information: film, color, country of... </think> <tool call> {name:open page,arguments:{docid:enwiki/44210}} </tool call> <tool response> ... </tool response> Tool Call 5: <think> Okay, lets see. The user is asking for detailed attributes of the 1937 film The Life of Emile Zola. They want information on film, color, country of origin, producer, review... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola 1937 box office revenue CNC rating FSK rating Kijkwijzer rating,topk:3}} </tool call> <tool response> ... </tool response> Tool Call 6: <think> Okay, let me tackle this task step by step. The user is asking for detailed attributes of the 1937 film The Life of Emile Zola. They want information on film,... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola 1937 box office revenue FSK rating CNC rating Kijkwijzer rating,topk:3}} </tool call> <tool response> ... </tool response> Tool Call 7: <think> Okay, lets tackle this query. The user wants detailed attributes for the 1937 film The Life of Emile Zola. They listed bunch of specific fields: film, color, country, producer,... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola 1937 box office revenue FSK rating CNC rating Kijkwijzer rating,topk:3}} </tool call> <tool response> ... </tool response> 51 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 009 (continued) Tool Call 8: <think> Okay, so the user is asking for detailed attributes of the 1937 film The Life of Emile Zola. They want information on film, color, country of origin, producer, review score,... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola 1937 box office revenue FSK rating CNC rating Kijkwijzer rating,topk:3}} </tool call> <tool response> ... </tool response> Tool Call 9: <think> Okay, so need to find detailed attributes for the 1937 film The Life of Emile Zola. The user is asking for bunch of specific info: film, color, country... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola 1937 box office revenue FSK rating CNC rating Kijkwijzer rating,topk:3}} </tool call> <tool response> ... </tool response> Response <think> Okay, so need to retrieve detailed attributes for the 1937 film The Life of Emile Zola. The user is asking for list of specific information including film, color,... </think> <agent response> ### Detailed Attributes for *The Life of Emile Zola* (1937): 1. **Film**: Biographical drama based on Emile Zolas life, focusing on the Dreyfus Affair and his literary career. 2. **Color**:... </agent response> 52 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 010 Task: You have reached the token limit. Please summarize your relevant findings immediately based on existing information. Tool Call 1: <think> Okay, need to retrieve detailed attributes for the movie The Best Years of Our Lives (1946). Let me start by breaking down the task. The user is asking for... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user is asking for detailed attributes of the film The Best Years of Our Lives from 1946. Let me start by looking at the search results they... </think> <tool call> {name:open page,arguments:{docid:enwiki/42159}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, so the user wants detailed attributes for The Best Years of Our Lives (1946). Let me start by going through the information have from the search results and... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, let me see. The user wants detailed attributes for The Best Years of Our Lives (1946). They listed bunch of categories like film, color, country of origin, producer,... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 5: <think> Okay, lets tackle this query step by step. The user wants detailed attributes for The Best Years of Our Lives (1946). They listed bunch of categories like film, color,... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 6: <think> Okay, so the user is asking for detailed attributes of the 1946 film The Best Years of Our Lives. Let me start by going through the information have from... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 7: <think> Okay, lets tackle this task step by step. The user wants detailed attributes for the film The Best Years of Our Lives (1946). They listed bunch of specific pieces... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> 53 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 010 (continued) Tool Call 8: <think> Okay, so the user is asking for detailed attributes of the film The Best Years of Our Lives from 1946. They listed bunch of specific pieces of information they... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 9: <think> Okay, lets tackle this query step by step. The user wants detailed attributes for The Best Years of Our Lives (1946). From the previous search results, found the Wikipedia... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 10: <think> Okay, so need to retrieve detailed attributes for The Best Years of Our Lives (1946). Lets start by looking at the information have from the previous searches. First,... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 11: <think> Okay, so need to retrieve detailed attributes for The Best Years of Our Lives (1946). Let me start by going through the information have so far. First, the... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 CNC rating,topk:1}} </tool call> <tool response> reached the token limit </tool response> Response <think> Okay, let me try to figure this out. The user is asking for detailed attributes of the 1946 film The Best Years of Our Lives. They listed bunch of... </think> <agent response> ### Summary of *The Best Years of Our Lives* (1946) *Film**: *The Best Years of Our Lives* (also known as *Glory for Me* and *Home Again*). *Color**: Color. *Country of... </agent response>"
        },
        {
            "title": "Main Agent",
            "content": "Round 5 (Main Agent + Sub Agents: agent 011, agent 012) <think> Okay, let me start by understanding the users question again. They want all films that were nominated for the Academy Award for Best Sound and received the National Board of Review: Top Ten Films award, excluding any films that feature Constantin Alexandrov as cast member or have Anna Hamilton Phelan as screenwriter. Then, they need specific attributes for each of these films. First, need to find the intersection of the two lists: Academy Award Best Sound nominees and National Board of Review Top Ten Films. From the previous reports, agent 003 provided list of common films between these two categories from the 1930s to 1970s. That list includes films like *It Happened One Night*, *Casablanca*, *The Grapes of Wrath*, *Citizen Kane*, and others. Next, have to check if any of these films include Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Agent 004 checked this and found no evidence of either name in the listed films. So, the initial list seems okay, but maybe should verify couple of films just to be thorough, especially if theres any chance of misspelling or alternative names. However, since the report says no matches, can proceed. Now, the user wants detailed attributes for each film. Agents 005 to 010 have retrieved information for *It Happened One Night*, *Casablanca*, *The Grapes of Wrath*, *Citizen Kane*, and *The Life of Emile Zola* and *The Best Years of Our 54 WideSeek: Advancing Wide Research via Multi-Agent Scaling Lives*. Each of these reports provides... </think> <tool call> {name:create sub agent,arguments:{tasks:[{agent id:agent 011,task:Retrieve detailed attributes for *The Best Years of Our Lives* (1946): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.},{agent id:agent 012,task:Retrieve detailed attributes for *The Life of Emile Zola* (1937): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.}]}} </tool call> <tool response> Sub-agents for call chatcmpl-tool-989f200e99584922b8e6826a3e5e742c completed. Reports: Report from agent 011 The Best Years of Our Lives (1946) is 172-minute American drama film directed by William Wyler. Here are the detailed attributes based on available information: 1. **Film**: Drama 2. **Color**: Color (as cinematographer Gregg Toland used deep focus and color photography). 3. **Country of Origin**: United States 4. **Producer**: Samuel Goldwyn 5. **Review Score**: 97% Fresh rating on Rotten Tomatoes (8.9/10 average). 6. **CNC Film Rating (France)**: Not explicitly mentioned in available sources. 7. **Assessment**: Critically acclaimed as masterpiece, praised for its realistic portrayal of WWII veterans struggles and ensemble casting. Roger Ebert called it modern, lean, and honest. 8. **Narrative Location**: Boone City (fictional, based on Cincinnati, Ohio). 9. **Distributed By**: RKO Radio Pictures 10. **Original Language**: English 11. **Composer**: Hugo Friedhofer (with Emil Newman as musical director). 12. **Distribution Format**: Theatrical release (initially limited to six shows per day due to length). 13. **Box Office**: $23.7 million (adjusted for inflation, one of the top 100 grossing films in U.S. history). 14. **Director**: William Wyler 15. **Kijkwijzer Rating (Dutch)**: Not found in available sources. 16. **Production Designer**: Not explicitly listed in the text, but Wyler emphasized realistic sets and life-size constructions. 17. **Duration**: 172 minutes 18. **Filming Locations**: Los Angeles County Arboretum, Ontario International Airport (California), Raleigh Studios, Samuel Goldwyn/Warner Hollywood Studios. 19. **Film Editor**: Daniel Mandell 20. **Production Company**: Samuel Goldwyn Productions 21. **FSK Film Rating (Germany)**: Not mentioned in available sources. **Notes**: - The... </tool response> 55 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent Task: Retrieve detailed attributes for *The Best Years of Our Lives* (1946): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating. Tool Call 1: <think> Okay, need to retrieve detailed attributes for the film The Best Years of Our Lives from 1946. Let me start by breaking down the users query. They want information... </think> <tool call> {name:search,arguments:{query:The Best Years of Our Lives 1946 detailed attributes,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user wants detailed attributes for the film The Best Years of Our Lives from 1946. Let me start by looking through the search results provided. The first... </think> <tool call> {name:open page,arguments:{docid:enwiki/42159}} </tool call> <tool response> ... </tool response> Response <think> Okay, lets start by looking at the users query. They want detailed attributes for The Best Years of Our Lives (1946). The attributes listed include film, color, country of origin,... </think> <agent response> The Best Years of Our Lives (1946) is 172-minute American drama film directed by William Wyler. Here are the detailed attributes based on available information: 1. **Film**: Drama 2.... </agent response> 56 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 012 {name:search,arguments:{query:The Life of Emile Zola 1937 film deTask: You have reached the token limit. Please summarize your relevant findings immediately based on existing information. Tool Call 1: <think> Okay, need to find detailed attributes for the 1937 film The Life of Emile Zola. Lets start by breaking down the users query. They want information on various aspects... </think> <tool call> tails,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user is asking for detailed attributes of the 1937 film The Life of Emile Zola. Let me start by looking at the search results they provided. The... </think> <tool call> {name:open page,arguments:{docid:enwiki/44210}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, lets tackle this task. The user wants detailed attributes for the 1937 film The Life of Emile Zola. First, need to go through the information provided in the... </think> <tool call> {name:open page,arguments:{docid:enwiki/4970252}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, lets see. The user wants detailed attributes for the 1937 film The Life of Emile Zola. So far, Ive used the search tool and found the Wikipedia page for... </think> <tool call> France,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 5: <think> Okay, lets see. The user is asking for detailed attributes of the 1937 film The Life of Emile Zola. So far, Ive used the search and open page tools to get... </think> <tool call> France,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 6: <think> Okay, lets tackle this step by step. The user wants detailed attributes for the 1937 film The Life of Emile Zola. So far, Ive used the search and open page tools... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola FSK rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 7: <think> Okay, let me try to figure out how to get all the details for The Life of Emile Zola (1937). The user is asking for bunch of attributes like... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola Kijkwijzer rating,topk:1}} </tool call> <tool response> ... </tool response> {name:search,arguments:{query:The Life of Emile Zola CNC rating {name:search,arguments:{query:The Life of Emile Zola CNC rating 57 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 012 (continued) {name:search,arguments:{query:The Life of Emile Zola CNC rating Tool Call 8: <think> Okay, lets tackle this task step by step. The user is asking for detailed attributes of the 1937 film The Life of Emile Zola. They want information on film, color,... </think> <tool call> France,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 9: <think> Okay, lets tackle this task step by step. The user wants detailed attributes for the 1937 film The Life of Emile Zola. So far, Ive used the search and open page... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola box office,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 10: <think> Okay, lets tackle this task step by step. The user wants detailed attributes for the 1937 film The Life of Emile Zola. So far, Ive used the search and open page... </think> <tool call> {name:search,arguments:{query:The Life of Emile Zola FSK rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 11: <think> Okay, so need to retrieve detailed attributes for the 1937 film The Life of Emile Zola. Let me start by listing all the required information: film, color, country of... </think> <tool call> France,topk:1}} </tool call> <tool response> reached the token limit </tool response> Response <think> Alright, lets tackle this task step by step. The user wants detailed attributes for the 1937 film The Life of Emile Zola. So far, Ive used the search and open page... </think> <agent response> ### Summary of **The Life of Emile Zola** (1937) Attributes: 1. **Film**: Biographical drama based on the life of 19th-century French author Emile Zola. 2. **Color**: Black-and-white (as it was... </agent response> {name:search,arguments:{query:The Life of Emile Zola CNC rating"
        },
        {
            "title": "Main Agent",
            "content": "Round 6 (Main Agent + Sub Agents: agent 013, agent 014) <think> Okay, so the user is asking for list of films that were nominated for the Academy Award for Best Sound and also won the National Board of Review: Top Ten Films award, but they need to exclude any films that have Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Then, they want all those films with specific attributes like color, country of origin, etc. First, need to make sure have the correct list of films that meet both awards. From the previous reports, agents 003, 005, 006, 007, 008, 010, and 011 have provided some of these films. Let me check if theres any overlap between the Academy Award Best Sound nominees and the National Board of Reviews Top Ten Films. Looking at the reports, agent 003s list includes films like *It Happened One Night*, *Casablanca*, *The Grapes of Wrath*, *Citizen Kane*, *The Best Years of Our Lives*, and *The Life of Emile Zola*. These are the ones that are common in both lists. Now, need to check if any of these films include Constantin Alexandrov or Anna Hamilton Phelan. Agent 004s report says theres no mention of these names in the films listed, so thats relief. But should still double-check each films cast and crew to be thorough. Next, for each film, need to gather all the specified attributes. Agents 005, 006, 007, 008, 010, and 011 have provided detailed info on these films, but some details are missing... </think> 58 WideSeek: Advancing Wide Research via Multi-Agent Scaling <tool call> {name:create sub agent,arguments:{tasks:[{agent id:agent 013,task:Retrieve detailed attributes for *A Star Is Born* (1937): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.},{agent id:agent 014,task:Retrieve detailed attributes for *The Hurricane* (1937): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating.}]}} </tool call> <tool response> Sub-agents for call chatcmpl-tool-0a2be8050ff54e8a8bd2deb5ac4f7056 completed. Reports: Report from agent 013 ### Detailed Attributes for *A Star Is Born* (1937) - **Film**: Drama/musical - **Color**: Technicolor - **Country of Origin**: United States - **Producer**: David O. Selznick - **Review Score**: 100% on Rotten Tomatoes (based on 23 reviews, average 7.7/10). - **CNC Film Rating (France)**: **Not found** in available sources. - **Kijkwijzer Rating (Netherlands)**: **Not found** in available sources. - **FSK Film Rating (Germany)**: **Not found** in available sources. - **Assessment**: Praised as groundbreaking color film and powerful human interest drama. Won the Academy Award for Best Original Story. - **Narrative Location**: Hollywood, California. - **Distributed By**: United Artists - **Original Language**: English - **Composer**: Max Steiner - **Distribution Format**: Blu-ray (restored 4K version released in 2022). - **Box Office**: $2.37 million in worldwide rentals. - **Director**: William A. Wellman - **Production Designer**: **Not explicitly mentioned** in the source material. - **Duration**: 111 minutes - **Filming Location**: Hollywood, California (implied by the plot and production details). - **Film Editor**: James E. Newcom and Anson Stevenson (uncredited). - **Production Company**: Selznick International Pictures - **FSK Rating**: **Not found**; the films content likely aligns with general theatrical standards but lacks specific classification. ### Notes: - **CNC/FSK/Kijkwijzer Ratings**: No direct information was found in accessible sources. These ratings may not have been assigned or documented for this film. - **Production Designer**: The Wikipedia page does not specify this detail, and further research beyond the current tools would be required. - **Remakes**:... </tool response> 59 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 013 Is Born 1937 detailed at- {name:search,arguments:{query:A Star Task: Retrieve detailed attributes for *A Star Is Born* (1937): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating. Tool Call 1: <think> Okay, need to find detailed attributes for the 1937 film Star Is Born. Let me start by breaking down the task. The user is asking for lot... </think> <tool call> tributes,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, let me start by understanding the users task. They want detailed attributes for the 1937 film Star Is Born. The attributes listed include film, color, country of origin,... </think> <tool call> {name:open page,arguments:{docid:enwiki/3450569}} </tool call> <tool response> ... </tool response> Tool Call 3: <think> Okay, lets tackle this query. The user wants detailed attributes for the 1937 film Star Is Born. First, need to go through the information provided from the search... </think> <tool call> {name:search,arguments:{query:A Star Is Born 1937 FSK rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 4: <think> Okay, so need to find detailed attributes for the 1937 film Star Is Born. Lets start by recalling the information already have from the previous search results.... </think> <tool call> {name:search,arguments:{query:A Star Is Born 1937 FSK rating,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 5: <think> Okay, so the user is asking for detailed attributes of the 1937 film Star Is Born. Let me start by going through the information have. First, from the... </think> <tool call> {name:search,arguments:{query:A Star signer,topk:1}} </tool call> <tool response> ... </tool response> Tool Call 6: <think> Okay, so the user is asking for detailed attributes of the 1937 film Star Is Born. Let me start by going through the information have. First, from the... </think> <tool call> {name:open page,arguments:{docid:enwiki/3450569}} </tool call> <tool response> ... </tool response> Response <think> Okay, so the user is asking for detailed attributes of the 1937 film Star Is Born. Let me start by going through all the information have gathered so... </think> <agent response> ### Detailed Attributes for *A Star Is Born* (1937) - **Film**: Drama/musical - **Color**: Technicolor - **Country of Origin**: United States - **Producer**: David O. Selznick - **Review Score**: 100%... </agent response> Is Born 1937 production de60 WideSeek: Advancing Wide Research via Multi-Agent Scaling Sub Agent: agent 014 Task: Retrieve detailed attributes for *The Hurricane* (1937): film, color, country of origin, producer, review score, CNC film rating (France), assessment, narrative location, distributed by, original language, composer, distribution format, box office, director, Kijkwijzer rating, production designer, duration, filming location, film editor, production company, FSK film rating. Tool Call 1: <think> Okay, need to find detailed attributes for the 1937 film The Hurricane. Let me start by breaking down the users query. They want information on various aspects like film,... </think> <tool call> {name:search,arguments:{query:The Hurricane 1937 film details,topk:10}} </tool call> <tool response> ... </tool response> Tool Call 2: <think> Okay, so the user is asking for detailed attributes of the 1937 film The Hurricane. Let me start by looking at the search results they provided. The first result, DocID... </think> <tool call> {name:open page,arguments:{docid:enwiki/1364234}} </tool call> <tool response> ... </tool response> Response <think> Okay, lets start by looking at the users task. They want detailed attributes for the 1937 film The Hurricane. The attributes include film, color, country of origin, producer, review score,... </think> <agent response> The Hurricane (1937) film details are as follows: - **Film**: The Hurricane (1937 film) - **Color**: Black-and-white (implied by absence of color mention in sources). - **Country of Origin**: United... </agent response>"
        },
        {
            "title": "Main Agent",
            "content": "Round 7 (Main Agent) <think> Okay, let me see. The user wants all films that were nominated for the Academy Award for Best Sound and received the National Board of Review: Top Ten Films award, excluding any films with Constantin Alexandrov as cast member or Anna Hamilton Phelan as screenwriter. Then, they need detailed attributes for each of those films. First, need to find the intersection of the two lists: Academy Award Best Sound nominees and National Board of Review Top Ten Films. From the previous reports, agent 003 provided list of common films between the two, such as *It Happened One Night*, *Casablanca*, *The Grapes of Wrath*, *Citizen Kane*, *The Life of Emile Zola*, *The Best Years of Our Lives*, and *A Star Is Born* (1937), *The Hurricane* (1937). Next, have to check if any of these films include Constantin Alexandrov or Anna Hamilton Phelan. Agent 004s report said theres no evidence of either name in the listed films. But just to be thorough, maybe should cross-verify couple of the films mentioned. For example, *Casablanca* has stellar cast like Bogart and Bergman, but no Alexandrov. *The Grapes of Wrath* has Henry Fonda, Jane Darwell, etc., also no Alexandrov. *Citizen Kane* has Orson Welles, but again, no mention of the names. So it seems safe to proceed without excluding any films. Now, the next step is to collect all the detailed attributes for each of these films. Agents 005 to 014 have provided information for *It Happened One Night*,... </think> <answer> Color Film B&W It Happened One Night B&W Casablanca Color* The Grapes of Wrath B&W Citizen Kane The Life of Emile Zola B&W Best Years of Our Lives Color Star Is Born (1937) The Hurricane Origin USA USA USA USA USA USA Technicolor USA USA B&W Producer F. Capra, H. Cohn Hal B. Wallis D.F. Zanuck, N. Johnson Orson Welles Henry Blanke Samuel Goldwyn David O. Selznick Samuel Goldwyn Score 98%; 9.1; 87 / 96%; 9/10 / 92%; 7.5 97%; 8.9 100%; 7.7 / CNC Assessment / / / / / / / / Narrative Loc Distributor FL, NYC Cultural milestone, 5 Oscars Cultural icon, 3 Oscars Morocco Social commentary, 3 Oscars OK, CA Innovation, 1 Oscar Historical, 3 Oscars Post-war, 7 Oscars Color film VFX, 1 Oscar Columbia Warner Bros. 20th Century RKO Radio Warner Bros. RKO Radio United Artists United Artists Xanadu, FL France Ohio CA South Seas Lang Composer H. Jackson En M. Steiner En A. Newman En B. Herrmann En M. Steiner En H. Friedhofer En M. Steiner En A. Newman En Format theatrical theatrical theatrical theatrical theatrical theatrical theatrical theatrical Box Office Director F. Capra $2.5m M. Curtiz $3.7m J. Ford $1.6m O. Welles $1.8m W. Dieterle / W. Wyler $23.7m W. Wellman $2.37m J. Ford $3.2m Kijk Designer / / / / / / P. Ferguson / A. Grot / / / / / / / Dur 105 102 129 119 116 172 111 110 61 Filming Loc Editor CA CA OK, CA CA USA CA CA / Company Columbia Warner Bros. 20th Century RKO Radio Warner Bros. Goldwyn Prod. G. Havlick O. Marks R. Simpson R. Wise W. Low D. Mandell J.E. Newcom Selznick Int. L. Nosler Goldwyn Prod. FSK / / / / / / / / WideSeek: Advancing Wide Research via Multi-Agent Scaling ### e : n1 . ** s R n * * : CNC, FSK , and k z a g n2 . ** d i a o a b n l h D g * * : * Happened One h * , * a n * , * The p f Wrath * , * The e m Z * , * The t r Our e * , *A r r f s . r Born * ( 1 9 3 7 ) , and * The r n * ( 1 9 3 7 ) , l t * and * The e m Z * k c l e s c . Our e * o a o s g m . Born * ( 1 9 3 7 ) f e T n l ; e s e i names were n3 . **Box i * * : * The p f Wrath u , * The t r n4 . ** o * * : *A r assumed c n5 . ** o ) . and t l n d ( . . , * The p f Wrath * x s s * * : No m m o e . t n d n n A a o Anna </answer>"
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "University of Science and Technology of China"
    ]
}