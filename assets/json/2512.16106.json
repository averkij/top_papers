{
    "paper_title": "ModelTables: A Corpus of Tables about Models",
    "authors": [
        "Zhengyuan Dong",
        "Victor Zhong",
        "Ren√©e J. Miller"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 6 0 1 6 1 . 2 1 5 2 : r ModelTables: Corpus of Tables about Models Zhengyuan Dong University of Waterloo Waterloo, ON, Canada zhengyuan.dong@uwaterloo.ca Victor Zhong University of Waterloo Waterloo, ON, Canada victor.zhong@uwaterloo.ca Ren√©e J. Miller University Waterloo Waterloo, ON, Canada rjmiller@uwaterloo.ca ABSTRACT We present ModelTables, benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text-only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open datalake tables, model tables are smaller yet exhibit denser inter-table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct multi-source ground truth using three complementary signals: (1) paper citation links, (2) explicit model-card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union-based semantic table retrieval attains 54.8% P@1 overall (54.6% on citation, 31.3% on inheritance, 30.6% on shareddataset signals); table-based dense retrieval reaches 66.5% P@1, and metadata-hybrid retrieval achieves 54.1%. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large-scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables."
        },
        {
            "title": "1 INTRODUCTION\nMany table corpora have been created and shared by researchers.\nThese include Webtables [6, 14, 30, 32, 47], Wikitables [3],\nVizNet [24], GitTables [25], and several Open Data corpora [43, 62]\ncontaining tables crawled from Open Data Portals. These table\ncollections have been used to study a variety of data management\ntasks including semantic type annotation (determining the semantic\ntype of an attribute), schema completion, table annotation, schema\nmatching, entity matching, and table search (finding related tables).\nLarge corpora like GitTables are designed to be used in machine\nlearning for table interpretation or understanding tasks [25].",
            "content": "In contrast, in this work, we present ModelTables, the first corpus of tables that describe AI models. Unlike previous table corpora that have broad topical coverage and content, we focus on tables describing models. To create our corpus, we make use of three types of resources. First, model repository (or model lake [45]) containing large collection of models described by model cards [41, 54] and data cards which describe training data and benchmarks used in model development [20, 33]. Second, code repository containing code related to the models. Typically model card will link to code repository. Third, paper repository (or to be more specific for our benchmark, two paper repositories) containing scholarly papers that describe the models. Model and data cards, along with code repositories can link to these papers. Note that in our corpus, every table is associated with model (or in the case of duplicates, table may be associated with more than one model)."
        },
        {
            "title": "1.1 From a Corpus to a Benchmark\nNotice that most of the corpora mentioned are not benchmarks per\nse, in that they generally do not contain ground truth for any of\nthe tasks they are designed to study. Rather, researchers generally\nhand-curate subsets of these corpora or manually manipulate tables\nwithin them to create label benchmarks with ground truth.",
            "content": "Consider, for example, the well-studied related table search. Related tables are typically defined as tables that \"include content that conceivably could have been in single table\" [51], and with this idea in mind most work on related table search focuses on finding tables that can be unioned or joined [4, 12, 43, 61]. For benchmarking, set of labeled tables is created (containing known related and non-related tables), for example the benchmarks for table union search TUS [43] and SANTOS [27] where labels are hand-curated or UGEN [46] where labels are LLM generated. In contrast, we are publishing ModelTables with several possible baseline ground-truth annotations for table relatedness that are generated by model developers themselves. And in contrast to other corpora, we propose using ModelTables to study thematic notion of table relateness, rather than structural one [51]. Our relatedness ground truth comes from three sources. First, model cards link to each other. The first measure of relatedness is model card relatedness: model is related to any model that its model card links (via URL or tag like base_model). Second, data cards also link to each other and other models. Third, paper relatedness is very well studied through the citations in papers and model cards link to papers."
        },
        {
            "title": "1.2 Benchmark Specifics\nWe build a large-scale benchmark over the Hugging Face Model\nLake1 and extract all tables from model cards or data cards. Hug-\nging Face is today, the largest open model repository [20, 33, 41, 54].\nModel (data) cards often refer to GitHub sites and we use GitHub\nas our code repository. From GitHub README files (that are refer-\nenced by a model or data card), we also extract tables. Finally, we\nextract tables from scholarly papers that are referenced in a model\n(or data) card or in a corresponding GitHub site. In our work, we",
            "content": "1huggingface.co Zhengyuan Dong, Victor Zhong, and Ren√©e J. Miller Figure 1: The Model Lake Benchmark Setup Pipeline. This figure outlines our end-to-end, automated workflow for establishing the Model Lake benchmark. It illustrates the general-purpose pipeline for collecting, cleaning, and linking diverse tables, including the integration of paper-level citation graph. Furthermore, it details how we construct multi-level ground-truth for model relatedness by combining signals from paper citations, model lineage from model cards, and shared dataset metadata, providing robust basis for evaluation. found many links to arXiv papers2 along with bibtex entries and simple paper titles (often found in table captions). For papers not in arXiv we tried to find the paper source in semantic scholar. We make use of the official Semantic Scholar Corpus3, resource which is updated weekly and is accessible through Semanti Scholar API. Its metadata and citation extraction follow the S2ORC pipeline [38], with commercial-scale refinements in the current deployed version."
        },
        {
            "title": "1.3 Contributions\nThis work makes several contributions. (1) We present the first\nlarge-scale benchmark of tables that describe AI Models. Unlike pre-\nvious table benchmarks that are designed to have broad thematic\ncontent, our tables all describe models and therefore can be used\nin model understanding tasks. (2) We introduce a set of relatedness\nground-truths that are based on verifiable relatedness signals from\nscholarly papers, model cards, and datasets, enabling our bench-\nmark to be used for a variety of relatedness search tasks. (3) We\ndescribe a reproducible pipeline for benchmark creation. While our\nbenchmark uses open models, code, and papers, our pipeline can\nalso be used in-house to create a resource for private model lakes. In\nsuch a scenario, the paper-relatedness would be replaced by project\nor team relatedness.4 To date, there has been little work to help\ndata-driven organizations manage and understand their own rich\nprivate model lakes. Ours is a first step in demonstrating how struc-\ntured data in a model lake can be used for model understanding. (4)\nWe analyze the benchmark and relatedness graphs and compare\nthem to state-of-the-art table search corpora and benchmarks. Our\nbenchmark contains more related tables, with richer relatedness\ndistributions that reflect the real nature of models (some models\nare hubs and related to many models and there is a skewed dis-\ntribution with many models having few models they are related\nto). (5) We empirically show the performance of existing related\ntable search methods (from IR and data lakes) on model tables. Our\nresults indicate a research gap in finding thematically related tables.\nWe show how our benchmark can be used to study new forms of\ntable relatedness (beyond the typical structural or IR notions). (6)",
            "content": "2arxiv.org 3https://www.semanticscholar.org/ 4LinkedIn Engineering Blog Scaling Machine Learning Productivity at LinkedIn; Netflix TechBlog Supporting Diverse ML Systems at Netflix. We discuss several use cases for the benchmark in model and table understanding. Our results show that semantic search and table discovery uncover table relatedness from complementary perspectives, enabling richer understanding of model similarity. Paper Organization. In Section 3, we describe our benchmark construction how we find and extract tables. In Section 4, we describe our ground truth extraction. We define three levels of ground truth all constructed from different signals provided by model developers. In Section 5, we analyze the benchmark and provide statistics on the tables, the different relatedness graphs and compare them to other table benchmarks (containing ground truth). In Section 6, we consider related table search as an application for our benchmark. We show that existing IR/DB table search approaches do not capture table relatedness well and point to new areas for research. In Section 7, we discuss other applications of our benchmark."
        },
        {
            "title": "2 RELATED WORK\n2.1 Data Discovery\nRetrieving structured tables in data lakes is typically posed as fol-\nlows [7, 59]: given a query table, the system must discover rele-\nvant tables in a large, heterogeneous repository based on schema\ncompatibility, content overlap, or semantic similarity. Early work\nfocused on joinable table discovery, where DeepJoin [12] uses pre-\ntrained language models to embed column values and perform\nnearest-neighbor search for both syntactic and semantic joins, while\nLSHEnsemble [62] and Josie [61] applies overlap-set indexing to\nefficiently identify tables sharing join keys. Unionable table search\nwas addressed proposed by Nargesian et al. [43]. SANTOS extends\nthis work [27] using knowledge-base-driven semantic relations\nto improve union prediction and D3L by using regular expres-\nsions [4] In parallel, keyword-based table search methods index\ncolumn names and metadata to support ad-hoc textual queries over\nthe lake. Blend [15] unifies and optimizes these classical data lake\ntasks. Additionally, table relatedness techniques quantify seman-\ntic and structural affinities between tables by analyzing attribute\nco-occurrence statistics, thereby enhancing the precision of related-\ntable retrieval [6].",
            "content": "More recent research has introduced tasks that go beyond simple retrieval. Table reclamation [16] aims to recover missing rows ModelTables: Corpus of Tables about Models or columns in query table by selectively merging content from related tables, effectively healing incomplete datasets. Column disambiguation [2] tackles inconsistent or ambiguous header names by linking them to canonical types or entities, improving interoperability across sources. Table augmentation [57] enriches an existing table with complementary attributessuch as geographic or demographic datapulled from external repositories. By extending classical join, union, and keyword search with these new capabilities, data lakes become far more powerful and flexible for downstream analytics and AI workflows."
        },
        {
            "title": "2.2 Model Lake\nModel Lakes are still in an early, emerging phase [19, 45]. Pal et al.\n[44] present a vision for model lakes which includes four core\ntasks: attribution, which traces a model‚Äôs outputs back to influential\ntraining data or internal features; versioning, which reconstructs\ndirected model trees to reveal parent‚Äìchild derivations; search,\nwhich locates models by intrinsic properties, output behavior, or\nmetadata; and benchmarking, which evaluates models at scale using\nunified suites. Garouani et al. [19] propose a three-zone architecture\n(ingestion, analysis, governance) with a rich metadata model for\nboth data and models.",
            "content": "Hugging Face is perhaps the largest open model lake currently. Hugging Face supports keyword search over model names and fulltext search over textual metadata. Recently, they added semantic search that uses LLM generated summaries of datasets and models and similarity search over these summaries.5 This is black-box search and could not be included in our comparison. Our work complements this by providing an open benchmark for evaluating their search and other semantic model and data set search approaches."
        },
        {
            "title": "3.1 Table Extraction\nWe construct our benchmark by collecting tables included in model\ncard READMEs and additional tables from other resources refer-\nenced in the model cards. We use the HuggingFace Model Card\ndataset6 as our primary source, HuggingFace is a large and popular\nmodel lake (with over 1M models at the time of our crawl). It offers\ncentralized hosting of model artifacts and rich metadata. From each\nmodel card, we extract three core elements: embedded tables, URLs,\nand BibTeX references. Then, for each URL and BibTeX entry, we\ndetect its source platform (arXiv GitHub, another Hugging Face\nmodel card) and apply tailored table scraping strategies. The result-\ning benchmark contains table from four resources. This extraction\npipeline is formalized in Algorithm 1 and described below.",
            "content": "Model Card Tables. We first parse all tables directly embedded within model cards. These tables are easy to parse and therefore 5https://huggingface.co/spaces/librarian-bots/huggingface-semantic-search 6https://huggingface.co/datasets/librarian-bots/model_cards_with_metadata 250118 Algorithm 1: Table Corpus Construction from Model Lake Input: Set of all model cards Cùëéùëôùëô from Hugging Face Output: Curated set of tables T, linked to its source 1 2 Cùë£ùëéùëôùëñùëë Filter Cùëéùëôùëô for cards with paper and tables 3 foreach ùëöùëê Cùë£ùëéùëôùëñùëë do ùëáùëêùëéùëüùëë ExtractTables(ùëöùëê.readme) Add all tables in ùëáùëêùëéùëüùëë to T, linking them to ùëöùëê ùëÉùëüùëí ùëì ùë† ExtractPaperReferences(ùëöùëê.readme) foreach ùëù ùëÉùëüùëí ùëì ùë† do ùëáùëéùëüùë•ùëñùë£ ExtractTables(GetArxivContent(ùëù ) ) Add all tables in ùëáùëéùëüùë•ùëñùë£ to T, linking them to ùëöùëê ùëáùë†2ùëúùëüùëê_ùëüùëéùë§ GetS2ORCTextTables(ùëù ) ùëáùë†2ùëúùëüùëê_ùëùùëüùëúùëê RecoverTableStructure(ùëáùë†2ùëúùëüùëê_ùëüùëéùë§ ) Add all tables in ùëáùë†2ùëúùëüùëê_ùëùùëüùëúùëê to T, linking them to ùëöùëê ùê∫ùë¢ùëüùëôùë† ExtractGitHubURLs(ùëöùëê ) foreach ùëî ùê∫ùë¢ùëüùëôùë† do ùëáùëîùëñùë°‚Ñéùë¢ùëè ExtractTables(GetReadme(ùëî) ) Add all tables in ùëáùëîùëñùë°‚Ñéùë¢ùëè to T, linking them to ùëöùëê 5 7 8 9 10 11 13 14 15 16 17 return very clean tables. The tables are often configuration tables or performance tables, but we include all tables within the model cards. GitHub Tables. If model card links to GitHub repository, we download the README file and extract any tables found. Like model card tables, these tables are easy to extract and also quite clean. Some GitHub URLs point to large checkpoints or code repositories lacking README files or reliable places to find structured tables, and as result do not contribute to our benchmark. arXiv Tables. If model card links to one or more arXiv paper (or has BibTex file for an arXiv paper), we would like to use the tables contained in these papers. If the arXiv link has an HTML version suitable for reliable table extraction, then we extract and use these tables from the markup. At this time, we found that table extraction directly from PDFs did not yield high quality tables, so our benchmark does not include these. To avoid including irrelevant papers and tables, we explicitly exclude the placeholder paper link7, found in the Hugging Faces template model card that many people use as starting point and forget to delete8. Semantic Scholar Tables. If model card links to paper in Semantic Scholar9, we leverage the Semantic Scholar Open Research Corpus (S2ORC) [38]. S2ORC processes PDF and LaTeX files through its pipeline and saves identified tables, figures, and text sections in its public data dump. However, these extracted tables are unstructured raw text, lacking row-column structure. Therefore, after obtaining this raw table text from the S2ORC data, we pipe it into Large Language Model (LLM) for parsing and refinement. The LLMs task is to convert these unstructured text blocks into clean, uniformly structured table format. 7https://arxiv.org/abs/1910.09700 8https://huggingface.co/templates/model-card-example 9https://www.semanticscholar.org Zhengyuan Dong, Victor Zhong, and Ren√©e J. Miller"
        },
        {
            "title": "3.2 Extraction Quality Control\nOur initial collection of tables, sourced from diverse platforms\npresents significant challenges in terms of quality and consistency.\nTo create a high-quality and robust benchmark, our post-extraction\npipeline refines the data through three key stages. We begin with\nQuality Control to correct low-level parsing patterns. This is\nfollowed by Strategic Filtering to remove entire tables unsuitable\nfor analysis. Finally, the refined dataset undergoes systematic Data\nAugmentation to handle representational heterogeneity.",
            "content": "Quality Control on Tables. Tables sourced from diverse platforms exhibit significant quality variance. These tables exhibit significant structural heterogeneity. Originating from formats like HTML and Markdown, their underlying structure is analogous to that of common web tables and CSV files and may exhibit complex layouts with multi-level headers or merged cells. Our extract relies on widely-used SOTA table extraction methods [8, 21] designed explicitly for processing highly heterogeneous tables. Specifically, our pipeline handles several challenging edge cases. It resolves Column Alignment Issues from merged or empty cells. It handles Internal Delimiter Conflicts by specially parsing Markdown cells that contain the pipe character (), pattern that exists in some Hugging Face Model Cards. 10 The pipeline preserves context from Cell-Level Annotations by merging footnote text directly into its corresponding cell. It also stitches together fragmented MultiPage Tables and prunes Visual Formatting Artifacts like empty rows. Finally, for Special Characters and Symbols (e.g., lambda, cdot), our approach is to preserve them as-is. Given the vast number of domain-specific notations, preserving the original representation is more robust for maintaining semantic integrity than attempting comprehensive normalization. These targeted corrections ensure all tables are structurally sound. Strategic Filtering. After correcting parsing errors, we apply strategic filtering process to remove tables with significant structural issues or those unsuitable for column-wise analysis. Specifically, our LLM-based recovery over Semantic Scholar extracts yields tables of variable quality, making them appropriate only for an ablation study rather than our main results."
        },
        {
            "title": "3.3 Table Augmentation\nA key challenge in model lakes is the significant heterogeneity in\nhow tables are represented across sources. Scientific tables vary\nwidely in orientation, formatting, and content encoding, which\nhinders straightforward matching and retrieval. To address this,\nwe apply two augmentation techniques designed to enhance table\nretrieval accuracy. Tables created by these augmentions can be\noptionally be added to the benchmark, depending on the use case.",
            "content": "Transpose Augmentation. In model lakes, or more generally in scientific tables, authors may present information horizontally, for example, table with column for each of set of benchmark datasets, where the column value is the performance of configuration on that dataset. Conversely, vertical layouts are frequently used when comparing different metrics for single dataset, with metrics arranged as row labels. These stylistic variations mean that 10https://huggingface.co/spacy/nb_core_news_lg ModelTables: Corpus of Tables about Models Figure 2: Tables from RoBERTa [37] and DPS [58] motivate augmentations: transposition for alignment, and headercell fusion for semantic normalization tables representing similar information may have headers and key labels oriented differently. For instance, as shown in Figure 2, what appears as column header in one table might be row header in another. Given the tables from each of our four sources, we create set of tables containing the transpose of each of these tables. Header-to-Cell Augmentation. In addition, we observe that some tables embed header semantics directly into the cell values. typical case is columns that combine categorical labels with numerical data for example, Training Epochs/Steps column may contain entries like 3 epochs or 1500 steps, merging quantity with unit in one cell. This practice, while enhancing human readability, introduces challenges for automated parsing and semantic alignment, as the same conceptual attribute might be split across different structural elements in different tables. We can alleviate this and provide more semantic context for interpreting numeric values in tables but concatenating values with header names. Given the set of tables from any of our four sources, we create header augmentation of this set where for each table every value in the table is concatenated with its attribute header name."
        },
        {
            "title": "4 MODEL AND TABLE RELATEDNESS\nIn addition to extracting tables, we also extract model ids and paper\nids. Hence, our corpus contains a heterogeneous graph contain-\ning tables, models and papers as nodes. All tables are associated\n(conceptually via an edge) with at least one model (more if there\nare duplicate tables). A model may be associated with one or more\npapers (through its model card, associated data card, or associated\ncode repo). And every paper is associated with at least one model.\nThese direct \"extraction\" links or edges provide a way of relating\ntables and models. In this section, we explore additional semantic\nfor relatedness. Specifically, from this heterogeneous graph, we\nwish to exploit both the direct associations and possibly indirect\nassociations to build several relatedness graphs that may be used\nin table and model search and understanding tasks.",
            "content": "Our central hypthesis is that ModelTables can be used for variety of table and model understanding tables. For example, by finding and integrating tables related to model, we can better answer complex data science questions such as \"What is the best model for my task?\" Hence, key challenge is to find related tables and related models within our benchmark. This raises fundamental question: how do we determine if two models are truly related? To answer this, we construct few possible model relatedness graphes. The core concepts guiding the construction of these graphs are formally defined in Table 1. Table 1: Definitions for Model and Table Relatedness. Concept, Definition, and Detection Model Relatedness: connection between models based on their model card or training data. It is modeled via signals at the paper level (citations, shared references), model level (direct model card links or lineage indicated by model tags), and dataset level (common datasets). Table Relatedness: An inferred relationship between tables. It is modeled using the location of extraction and by inheriting the relatedness status from the source models or papers associated with each table. We assess model relatedness across three key levels, as illustrated in Figure 1, paper, model, and dataset."
        },
        {
            "title": "4.1 Paper Relatedness\nFirst, at the paper level, we obtain each paper‚Äôs references and\ncitations from Semantic Scholar data. Based on this information,\nwe identify whether two papers directly cite each other or share\noverlapping reference lists to determine their relatedness.",
            "content": "More formally, two models ùëöùëñ and ùëö ùëó are paper related (ùëÖpaper) if any of their associated papers are related through: (a) Direct: Direct Citation one paper directly cites the other; (b) Overlap: Reference Overlap their reference lists share at least one common cited work. Reference overlap is computed over reference lists to capture both direct and latent citation relationships. As an example two papers published almost concurrently may be very related but may not cite each other. However, reference overlap will reveal the relationship. Citation metadata are retrieved from the Semantic Scholar API11 and include two key attributes: citation intent and isInfluential [9, 52]. The intent citations come from Methodology or Results sections of papers and are considered more meaningful than (for example) citations contained in background or related work section, which is derived from multi-class classification model that categorizes the rhetorical role of each citation. The isInfluential is binary indicator to detect if the citation appears to be important, produced by binary classification model that integrates multiple textual and citation-level factors, such as citation frequency, section placement, and contextual emphasis [9, 52]. Of the various types of citation relationships, direct citations are the stricter is some sense than overlap citations, but these two types of relationships will form different relatedness graphs between models (or their tables). Within each set (direct vs. overlap) citations, we 11https://www.semanticscholar.org/product/api/tutorial can restrict our consideration to citations that have both the isInfluential and Intent attributes, have one of these, or consider any citation. Hence, the paper-related edges of our model graph can be used to form eight possible relatedness graphs based on the citation type: Direct (Intent Influential); Direct (Intent); Direct (Influential); Direct; Overlap (Intent Influential); Overlap (Intent); Overlap (Influential); Overlap. When using paper relatedness (ùëÖpaper) we refer to the use of all (Direct Overlap) edges, unless otherwise indicated. In practice, we expect use case will only use subset of these."
        },
        {
            "title": "4.2 Model Card Relatedness\nSecond, at the model card level, we consider the base model tags\nprovided in model cards to determine whether one model is an\nancestor of another, as well as whether two models share the same\nancestor. We will consider such models related. Additionally, if one\nmodel card contains a direct link to another model card, these two\nmodels are regarded as related.",
            "content": "More formally, two models are model related (ùëÖmodel) if either: (a) Explicit Model Reference one model card explicitly links to another model card; or (b) Base Model Inheritance model declares another as its base_model (or equivalent) in the metadata, indicating lineage through fine-tuning, adapter injection, quantization, or model merging; or (c) Shared Ancestor - two models share the same base_model. Hyperlinks are resolved to canonical identifiers following the pattern https://huggingface.co/{org_id}/{repo_id}, and shorthand references (e.g., gpt2 instead of openai-community/gpt2) are normalized using download frequency statistics. level,"
        },
        {
            "title": "4.4 Model and Table Relatedness Graphs\nEach table ùë°ùëñ inherits the relatedness of its associated model(s) ùëÄ (ùë°ùëñ );\nhence, table-level relationships are defined as:",
            "content": "ùëÖùëá (ùë°ùëñ, ùë° ùëó ) = ùëÖpaper (ùëÄ (ùë°ùëñ ), ùëÄ (ùë° ùëó )) ùëÖmodel (ùëÄ (ùë°ùëñ ), ùëÄ (ùë° ùëó )) ùëÖdataset (ùëÄ (ùë°ùëñ ), ùëÄ (ùë° ùëó )) (1) However, the benchmark is constructed so that the different types of relatedness may be used in concert or individually. We then operationalize these definitions when constructing the ground-truth relatedness matrices (the various ùëÖ), as detailed in Algorithm 2. 12https://huggingface.co/datasets/librarian-bots/dataset_cards_with_metadata, version 250428 Zhengyuan Dong, Victor Zhong, and Ren√©e J. Miller Algorithm 2: Table Ground Truth Inference Input: Tables T; Citation graph ùê∫ùëêùëñùë°ùëí ; Filter config ùêπùëêùëúùëõùëì ùëñùëî Output: Binary relatedness matrices {ùëÖùëùùëéùëùùëíùëü , ùëÖùëëùëéùë°ùëéùë†ùëíùë° } , ùëÖùëöùëúùëëùëíùëô T 1 Function ArePapersRelated(ùëöùëé, ùëöùëè, ùê∫ùëêùëñùë°ùëí, ùêπùëêùëúùëõùëì ùëñùëî): 2 foreach (ùëùùëé, ùëùùëè ) GetPapers(ùëöùëé ) GetPapers(ùëöùëè ) do 3 4 6 7 8 9 10 12 (ùëÖùëé, ùëÖùëè ) GetRefs(ùëùùëé, ùëùùëè, ùê∫ùëêùëñùë°ùëí ) (ùëÖùëé, ùëÖùëè ) FilterByIntent(ùëÖùëé, ùëÖùëè, ùêπùëêùëúùëõùëì ùëñùëî.intent) if ùêπùëêùëúùëõùëì ùëñùëî.influence then (ùëÖùëé, ùëÖùëè ) FilterByInfluence(ùëÖùëé, ùëÖùëè ) if ùêπùëêùëúùëõùëì ùëñùëî.type = Direct then if (ùëùùëè ùëÖùëé) or (ùëùùëé ùëÖùëè ) then return true else if ùêπùëêùëúùëõùëì ùëñùëî.type = Overlap then if HasOverlap(ùëÖùëé, ùëÖùëè ) then return true return false 13 14 Function AreModelsRelated(ùëöùëé, ùëöùëè ): 15 16 Function AreDatasetsRelated(ùëöùëé, ùëöùëè ): return HasSharedDataset(ùëöùëé, ùëöùëè ) 17 return HasLineage(ùëöùëé, ùëöùëè ) or HasSharedBaseModel(ùëöùëé, ùëöùëè ) 18 Procedure Main: Initialize ùëÖùëùùëéùëùùëíùëü foreach (ùë°ùëé, ùë°ùëè ) do , ùëÖùëöùëúùëëùëíùëô , ùëÖùëëùëéùë°ùëéùë†ùëíùë° as zero matrices foreach (ùëöùëé, ùëöùëè ) GetModels(ùë°ùëé ) GetModels(ùë°ùëè ) do if ArePapersRelated(ùëöùëé, ùëöùëè, ùê∫ùëêùëñùë°ùëí, ùêπùëêùëúùëõùëì ùëñùëî) then ùëÖùëùùëéùëùùëíùëü (ùë°ùëé, ùë°ùëè ) 1 if AreModelsRelated(ùëöùëé, ùëöùëè ) then (ùë°ùëé, ùë°ùëè ) 1 ùëÖùëöùëúùëëùëíùëô if AreDatasetsRelated(ùëöùëé, ùëöùëè ) then (ùë°ùëé, ùë°ùëè ) 1 , ùëÖùëëùëéùë°ùëéùë†ùëíùë° ùëÖùëëùëéùë°ùëéùë†ùëíùë° } return {ùëÖùëùùëéùëùùëíùëü , ùëÖùëöùëúùëëùëíùëô 19 20 21 22 23 25 26"
        },
        {
            "title": "5.1 Table Statistics",
            "content": "Model-Level Quality Filtering. We began our benchmark construction from model lake collection of over 1.1M models and their associated model cards. As shown in Figure 3(top), about 68% of these model cards are non-empty, containing at least minimal documentation. Among them, roughly 17% include extractable tables that summarize model characteristics or results. To create high quality corpus of models with scholarly publications, we further restrict to model cards that reference valid research papers. This step substantially narrows the pool, as many community-submitted cards lack formal paper links. To mitigate this sparsity, we integrate other resources, including GitHub READMEs, arXiv, and Semantic Scholar, to recover missing table or citation information. After this augmentation, around 60K models satisfy all quality criteria and form the core of our benchmark. Table-Level Duplication Analysis. The bottom of Figure 3 analyzes the resulting table corpus by showing the frequency of table occurrences after extraction. We observe pronounced long-tailed distribution: most tables appear only once, while few popular or widely reused ones recur many times across model cards. This ModelTables: Corpus of Tables about Models Figure 3: Modeland table-level filtering of the model lake. Top: Stepwise filtering of models by card completeness, table presence, and valid references, yielding the final benchmark subset. Bottom: Table frequency distribution showing longtailed reuse pattern that motivates deduplication. Figure 4: Our benchmark contains larger quantity of smaller tables compared to data lakes, structural profile driven by the distinct characteristics of its academic sources. aligns with our intuition that valid model cards typically contain one or several unique tables, while occasional anomalies correspond to reused or mis-parsed tables. Such evidence validates our deduplication strategy, ensuring that the benchmark retains representative yet non-redundant tables. Benchmark Filtering Overview. Our goal is to refine the raw corpus of extracted tables into high-quality benchmark suitable for computing tableand model-level relatedness. Figure 4 (blue/green shades on the right) summarizes how the dataset contents changed with respect to some table filters. Stage - All -> Dedup (Deduplication insights). Removing redundant tables eliminates the long tail of boilerplate or templatederived tables and retains roughly 17% of the original corpus. Average column counts remain nearly unchanged, while the average number of rows increases slightly, evidence that duplicates are predominantly small, repetitive tables rather than structurally rich ones. Duplication intensity differs strongly by source: the code resource GitHub shows heavy reuse, whereas our two paper sources like arXiv and Semantic Scholar exhibit much less redundancy. Stage - Dedup -> Title (Contain references). The next step links each table to at least one textual anchor, such as modelcard title, GitHub README heading, or paper title. This constraint mainly removes code repository tables lacking citations or bibliographic context while leaving most paper tables intact. Conceptually, the corpus transitions from raw structural artifacts to citation-anchored tables, enabling downstream computation of paperand model-level relationships. Stage - Title -> Valid-Title (Scholarly validation). Titles are then validated against an external scholarly index (Semantic Scholar) to confirm that each refers to verifiable research artifact. After this filtering, the benchmark stabilizes at roughly 105 tables. Within this subset, Model-Card tables that pass validation become noticeably larger (more rows), but narrower (fewer attributes) as larger performance tables predominate. Roughly two-thirds of the model card titles and about one-third of GitHub references fail this match, whereas nearly all papers titles resolve cleanly as expected. The surviving corpus thus reflects academically grounded, structurally consistent content suited for quantitative relatedness analysis. Comparison to existing Corpora and Benchmarks. Figure 4 (orange/red shades on the left) contains several popular table corpora and few data lake benchmarks (the later containing ground truth). Our benchmark exhibits distinct structural and semantic profile compared to classic data-lake benchmarks such as SANTOS [27] and TUS [43] and to newer LLM generated benchmarks UGEN V1/2 [46]. We obtain many more, but smaller tables (comparable in size to the smaller UGEN tables). While the overall statistics differ, the two types of benchmarks remain complementary. Recall the data lake benchmarks are either hand-curated from open data so that ground-truth relatedness is know (TUS/SANTOS) or LMM generated (UGEN). As we report below these benchmarks differ more in the characteristics of the ground truths (relatedness distributions), see Section 5.2. Traditional data lakes capture large-scale structural diversity, whereas our benchmark emphasizes academically structured precision. Together, they offer broader landscape for exploring table discovery, integration, and evaluation methods across heterogeneous domains. In comparison, the (unlabeled) larger Web corpora (WDC, WikiTables, GitTables) contain many more tables. Excluding GitTables, these are typically smaller tables as these come from web pages, and mass-collaboration pages."
        },
        {
            "title": "5.2 Table Relatedness Statistics\nWe have already examined the filtering quality of models and tables\n(Figure 3, 4), ensuring that each retained table is linked to a valid",
            "content": "Zhengyuan Dong, Victor Zhong, and Ren√©e J. Miller benchmark exhibits broader and more variable long-tailed pattern, with related-table counts ranging from only few to well over thousand. This pattern arises from hub effects, highly cited papers or widely reused base models creating dense clusters of connections. The resulting variability reflects the realistic, heterogeneous nature of the scientific table ecosystem, providing challenging yet representative ground truth for evaluating retrieval and discovery methods. Figure 5: The pronounced long-tail distribution in our ground truth arises from influential models acting as hubs, which link to many model cards."
        },
        {
            "title": "EVALUATION OF TABLE SEARCH USING\nMODELTABLES",
            "content": "We now present detailed experimental result developed with our benchmark. In this study, we evaluate to what extend existing related-table search methods can find tables that are related using any of our ground truth measures (model, data, or paper). paper. In this section, we turn to the ground-truth relatedness statistics derived from our three types of relatedness. Paper Relatedness Ground Truth Density. We begin by quantifying the density of the paper relatedness (ùëÖpaper) of tables for the eight different subsets of ùëÖpaper. Table 2. The raw overlap graph connects over 40% of all table pairs, reflecting broad, but noisy linkage. Applying influence or intent filters reduces this density to 1025%, and combining both yields compact around 8% high-confidence core. In contrast, direct citation graphs remain substantially sparser (around 38%), indicating higher precision, but narrower coverage. These statistics confirm that our ground-truth graph preserves sufficient connectivity for meaningful discovery tasks without collapsing into an unrealistically dense structure. The contrasting densities highlight two complementary capabilities of the benchmark. Overlap-based graphs recover indirect semantic relationship. While direct graphs represent explicit citation lineage. Together they form precisionrecall trade-off: overlap offers breadth and higher recall; direct citations offer precision and reliability. Table 2: Paper Relatedness Density across Citation Types. Applying influence and intent filters sparsifies the graph, while overlap-based methods capture indirect relationships resulting in much denser graph. Citation Type Overlap Overlap (Intent) Overlap (Influential) Overlap (Intent Influential) Direct Direct (Intent) Direct (Influential) Direct (Intent Influential) Model Dataset All (Direct Model Dataset) Nonzero Edges Density 43.03% 25.83% 9.94% 8.04% 8.17% 5.76% 3.48% 3.09% 0.19% 0.41% 8.31% 3.71 109 2.23 109 8.59 108 6.95 108 7.06 108 4.98 108 3.01 108 2.67 108 1.71 107 3.51 107 7.18 108 Model and Dataset Density Patterns. The bottom of Table 2, contains the density of the Model and Dataset relatedness over our table graph. The Model relatedness is the sparsest, as expected, and the most precise in terms of semantics. The Dataset relatedness is also sparse, but represents different semantics of relatedness that should be included depending on the task. Table Relatedness Distribution. Using the table-as-query paradigm common in data lakes [42], we further analyze the distribution of related-table counts per query, i.e., how many related tables each query table has (Figure 5). Unlike existing benchmarks such as SANTOS [27] and TUS [43], which define fixed set of queries, our benchmark treats every table as potential query.13 This design yields substantially larger query space, though it does not imply that the underlying distribution is entirely different. Indeed, our 13UGEN is excluded because every query has exactly 10 related tables. Figure 6: Grounding the table discovery task in the Model Lake context. To give meaning to traditional table search (left), we establish ground truth for relatedness (right) by leveraging the rich connections between associated papers, models, and tables. This section presents comprehensive evaluation of multiple table discovery methods on our newly constructed ModelTables benchmark. We investigate how well existing techniquesranging from advanced table discovery techniques to traditional embeddingbased and metadata-driven retrieval from IRperform over model tables."
        },
        {
            "title": "6.1 Experimental Methods\nWe evaluate state-of-the-art table search and retrieval methods on\nour benchmark. We consider data discovery methods including",
            "content": "ModelTables: Corpus of Tables about Models Table 3: Comparative Implementation and Settings for Table Discovery and Retrieval Methods Table Discovery Methods Retrieval Methods Method Keyword Search Joinable Search Unionable Search Dense Retrieval Sparse Retrieval Hybrid Retrieval Content Tool Headers Table Blend [15] Yes Table Blend [15] No Table Starmie [17] No Table FAISS [13] Yes Metadata Pyserini [34] - Metadata Pyserini [34] + FAISS [13] - joinable and unionable table search. We also consider state-of-theart table retrieval methods. Key implementation characteristics distinguishing these methods are summarized in Table 3. top-1 result from these candidates. This dual approach allows us to compare pure sparse retrieval against the benefits of the cascade hybrid approach. Keyword Search. We use Blends implementation of keyword search [15]. Blend is recent unified table discovery system with collection of discovery operators that include keyword search as well as join, correlation, and union discovery. For Keyword Search, we retrieve tables by matching query table header tokens against candidate table headers and cells. We consider table relevant if query token is contained within candidates content, with candidates then ranked by hit count. Join Search. We use Blends implementation of join search [15]. Given query table, we search for tables that join on the right-most column (typically key column in model lake tables). Following the literature, we do not use headers in this search as the attributes may have different names and still be joinable. Union Search. We use Starmies implementation of union search [43]. Unlike Blend which only considers exact matching of table values, Starmie considers semantic matching and out-performs Blend on this task. Dense Retrieval. We create table embedding that encodes each table (values and headers) as single vector using pretrained Sentence-BERT encoder [50]. Candidate tables are then ranked by the cosine similarity of their embeddings, efficiently performed using FAISS [13]. Sparse Retrieval. Sparse retrieval is traditional IR technique that relies on term-matching and has been used for table retrieval by applying matching on the metadata describing table [40, 56]. Such metadata retrieval leverages the textual context surrounding each table rather than its internal structure. Our preprocessing aggregates raw README text (for Markdown tables) or captions combined with in-text mentions (for arXiv tables) to form local \"in-context\" metadata corpus per table. Notably, when table is associated with multiple metadata texts (due to duplication), all these texts are concatenated to create an augmented corpus for indexing. Queries are generated by serializing these metadata texts into prompts. For sparese retrieval, we use Pyserini [34] sparse-term index with 1024-clause truncation, and Hybrid Retrieval, which first retrieves the top-100 candidates via sparse search and then applies dense Sentence-BERT re-ranking. Hybrid Retrieval. Hybrid retrieval combines the sparse retrieval with dense retrieval based on metadata in cascaded manner. Specifically, we first apply sparse metadata retrieval to obtain the top-100 candidates, and then use dense retrieval to select the"
        },
        {
            "title": "6.2 Evaluation Results",
            "content": "Setting. In our evaluation, we use the original ModelTable corpus as the default retrieval candidate pool. In the augmentation ablation study, we union these table with both the transpose augmentation and the header augmentation (Section 3.3). Hence, for each query table in these ablation settings, we (in parallel) search for the table, its transpose, and its header augmentation. retrieval is considered success if any of these searches yields top-1 result that corresponds to correct ground truth table. In this study, we use Direct paper relatedness. This optimistic evaluation setting is designed to estimate the potential upper bound of performance achievable by each of the current state-of-the-art methods. Findings. This section presents the baseline retrieval performance of different methods. The goal is to establish reference points for future improvements and to understand how different data characteristics impact retrieval effectiveness. F1: While All Modern Methods Significantly Outperform Keyword Search, IR methods Outperform Data Lake Methods except Union Search. Our findings  (Table 4)  demonstrate that modern semantic retrieval methodsencompassing approaches based on table content, contextual metadata, and schemaawarenessconsistently and substantially outperform traditional keyword search across all defined ground truths (GTs). Nevertheless, their efficacy is nuanced, varying significantly with the specific semantic signals each method prioritizes. Dense retrieval on tables proved highly effective within the Retrieval Methods, achieving top performance on the Paper GT and the best result overall, which underscores the importance of capturing the semantics of tables. In turn, contextual metadata demonstrated its criticality for specific tasks, with Hybrid retrieval excelling on both Model GT and Dataset GT by leveraging essential contextual information beyond raw table data. While unionable method Starmie demonstrated strong performance, leading the Table Discovery Methods across all GTs. Notably, Starmie was second best for the Paper and All GT indicating unionability is strong signal for Direct paper relatedness. However, it was generally surpassed by Dense and Hybrid retrieval when considering overall or other specific GTs, suggesting potential for enhancement by integrating these semantic signals. And importantly, the tables retrieve by these approaches are different as we show later, further indicating an opportunity for additional research to combine their strengths. Zhengyuan Dong, Victor Zhong, and Ren√©e J. Miller Table 4: Precision@1 of retrieval and table discovery methods across different ground truths (GTs) Retrieval Methods Table Discovery Methods Method / GT Paper Model Dataset All Method / GT Paper Model Dataset All Dense Retrieval Sparse Retrieval Hybrid Retrieval 0.6651 0.5131 0.5410 0.4175 0.3724 0. 0.4143 0.3749 0.4592 0.6672 Keyword Search 0.5157 Joinable Search 0.5428 Unionable Search 0.2043 0.2742 0.5465 0.0472 0.0694 0.3133 0.0460 0.0723 0.3068 0.2060 0.2755 0. F2: Significant Performance Variation Arises from Differences in Table Source Quality. The source of table significantly impacts its style and data quality. For instance, tables from GitHub and model cards are often rich with developer-curated metadata, while those from academic papers arXiv and Semantic Scholar are more structurally diverse. To analyze this, we evaluated the performance of Unionable search on each source individually and in combination  (Table 5)  . Our analysis reveals dramatic variation in precision based on these sources. Model cards and GitHub tables consistently yield high precision, likely benefiting from their well-curated Markdown format. Note that the Semantic Scholar tables consists of LLMrefined plain text rather than structurally rich HTML or Markdown tables. Hence, the tables are noisier and this dramatically effects the quality of retrieval. This principle of the impact of the source characteristics is further confirmed by our combined-source experiments, which underscore that data homogeneity is critical for achieving high precision. Performance is strongest for the model cards and GitHub (M+G) subset, where tables share consistent Markdown format. Accuracy progressively declines as less-structured arXiv tables are introduced (in M+G+A), and further diminishes with the inclusion of the even more varied Semantic Scholar tables (in M+G+A+S). F3: Domain-Specific Augmentations Enhance the Searchability of Tables. To improve retrieval (especially for numerical tables, which often lack explicit semantic context and have layout variations), we evaluated separately our two data augmentation strategies: adding semantic context (Header Augmentation) and normalizing the layout (Transpose Augmentation). Note that metadata-based retrieval methods are excluded from this analysis, as they do not benefit from table augmentation. Unionable search is excluded, as Starmies runtime depends on column count, which Table 5: Precision@1 of table union search across table resources on different GTs. Resource / GT Paper Model Dataset All ModelCard (M) GitHub (G) ArXiv (A) Semantic Scholar (SS) M+G M+G+A M+G+A+SS 0.7857 0.8177 0.4504 0.3059 0.7801 0.5465 0. 0.2599 0.7381 0.3545 0.2630 0.2766 0.3133 0.2700 0.2142 0.7366 0.3607 0.2637 0.2358 0.3068 0.2657 0.7873 0.8171 0.4517 0.3062 0.7816 0.5487 0. explodes under the transpose augmentation. Our analysis  (Table 6)  shows the effectiveness of each augmentation. Adding semantic context proved highly effective as it improved precision across all ground truths relatedness (paper, model, and dataset). This confirms that making the implicit meaning of numerical cells explicit is valuable for model lake table retrieval. In contrast, the Transpose Augmentation consistently improves performance only slightly. This demonstrates that the best augmentation strategies for model lake tables is to enrich the tables existing semantics. F4: Robustness and Consistent Performance are Maintained Across Various Ground-Truths. Our benchmark permits the use of different citation semantics in establishing paper relatedness  (Table 2)  . Table 7 presents the precision of union search over eight different citation definitions. Precision predictably drops when stricter GT criteria are applied (meaning fewer citation relationships), direct consequence of the ground truth becoming sparser. This inverse relationship is particularly evident with Paper GT, where precision drops as citation criteria become more stringent. Interestingly, when comparing GTs, those defined by overlap consistently yield higher precision than GTs based on direct citation criteria, suggesting overlap inherently captures broader, denser set of relevant connections. These findings underscore that while retrieval performance metrics are highly sensitive to the definition and statistical properties of the ground truth itself, the methods exhibit consistent behavior even as ground truth criteria become more stringent. This highlights the importance of understanding how GT selection influences reported performance. F5: Structural augmentations are effective in improving retrieval robustness alongside semantic augmentations. In addition to semantic and citation augmentations, we investigate the impact of table structure perturbations, including column and row shuffling. These augmentations are evaluated using Starmie only, as its encoder is specifically designed for structural robustness. Column shuffling at inference yields the largest gains, highlighting that augmenting table structure is practical strategy for robust retrieval across heterogeneous model lakes. We observe that all structural augmentation tricks improve retrieval performance, confirming substantial variation in table styles across authors and sources. Column shuffling provides the largest gains, highlighting that column order is especially arbitraryparticularly in academic tables with semi-structured formats, where unionability is difficult to detect. Structural augmentations, especially at the column level, help the encoder focus on semantic and schema similarities rather than layout, resulting in more robust retrieval in heterogeneous model lakes. ModelTables: Corpus of Tables about Models Table 6: Precision@1 of methods on augmented tables across different GTs. Header-to-Cell Augmentation Transpose Augmentation Both Augmentation Method / GT Paper Model Dataset All Paper Model Dataset All Paper Model Dataset All Dense Retrieval Keyword Search Joinable Search 0.7778 0.2738 0.4569 0.5121 0.0512 0.1541 0.5042 0.0502 0.1555 0.7798 0.2761 0.4593 0.7602 0.2989 0. 0.4950 0.0785 0.0527 0.4866 0.0782 0.0592 0.7623 0.3012 0.2776 0.8142 0.4653 0.5056 0.5459 0.2365 0.1617 0.5358 0.2473 0. 0.8160 0.4688 0.5084 Table 7: Precision@1 of unionable search for citation augmentation on Paper GT. Citation Type Direct Overlap All Intent Influential Intent Influential 0.5465 0.5278 0.5097 0. 0.7487 0.6471 0.5533 0.5382 Table 8: Precision@1 of Starmie with structural augmentations across different GTs. Augmentation / GT Paper Model Dataset All Base Drop Cell Shuffle Col Shuffle Row 0.4119 0.4607 0.5465 0. 0.1899 0.2323 0.3133 0.2031 0.1806 0.2240 0.3068 0.1979 0.4133 0.4627 0.5487 0."
        },
        {
            "title": "6.3 Example Studies\nNone of the known retrieval methods has great performance on\nModelTables even for the simple precision1 task. To shed better\nlight on why, we give two examples of tables retrieved using a\nstructural technique (unionable search) and tables retrieved using\ntable-embeddings (dense retrieval). We use these examples to moti-\nvate intuitively why we believe our study motivates the need for\ncareful combination of structural and semantic signals to improve\nrelated table search.",
            "content": "Example I: Unionable Search Example. Here we demonstrate using unionable search for GLUE-style performance tables, showcasing the searchs ability to retrieve tables that describe related models. Figure 7 (a) illustrates BERT query table and the Top-2 retrieved tables using Starmie (union table search. these search results are clearly relevant and enable direct, side-by-side comparison of both baseline and advanced models (e.g., BERT [11], RoBERTa [37], MUPPET [1], Uni-Perceive [63]) across consistent set of NLP tasks and evaluation metrics such as MNLI, QNLI, MRPC, SST-2, and SQuAD [49]. These tables exhibit clear structural and semantic alignment, reporting results on overlapping benchmarks that make them unionable. But they are also clearly relevant to data scientist analyzing model behavior. Our observations highlight the limitations of traditional textbased information retrieval methods, which can only address coarsegrained questions such as Which models are available for given task?, falling short on fine-grained performance comparisons at the metric level. Using table search, we can retrieve related and integratable tables that expand the analysis that can be done with the query table alone. This approach enables users to find tables that help them to make precise, context-aware decisions grounded in detailed performance data from multiple models and benchmarks. Example II: Dense Retrieval Example. As shown in Figure 7(b), dense retrieval uncovers tables that, while still centered on BERT variants and key NLP benchmarks such as GLUE and SQuAD, exhibit much greater heterogeneity in schema and reporting style compared to union-based results. Unlike the more schema-aligned tables surfaced by union search, dense retrieval often returns tables with alternative column organizations, extra metrics, or less conventional formattingreflecting the diverse ways authors present experimental results. This flexibility can be seen as major advantage: dense retrieval is able to connect tables that are semantically related but do not share structure or use differing conventions. As result, it can successfully retrieve relevant data even when column names, task sets, or layouts diverge from the query. This capability is especially valuable for scientific corpora where reporting standards vary widely and metadata may be noisy or incomplete. However, dense retrieval is not without limitations. Because it relies on semantic similarity, it tends to retrieve tables with highly overlapping baselines or benchmarksmost notably, results focused on \"BERT-base\"rather than broader variety of model variants or experimental settings. This may result in concentration of retrieved tables that report on the same baselines, potentially missing tables that, while structurally unionable, cover wider range of models or alternative approaches. In contrast, union-based methods, are more likely to surface diverse model variants and experimental configurations, supporting broader comparison across methods. In summary, dense retrieval effectively complements unionbased methods by bridging gaps in schema alignment and surfacing structurally diverse, semantically relevant tables. However, it is less effective in promoting methodological or model diversity, often prioritizing tables with significant overlap in baselines and benchmarks. This suggests there are open research challenges in combining the strengths of both approaches to achieve better model lake performance."
        },
        {
            "title": "7 APPLICATIONS\nBeyond related table search, our benchmark has numerous applica-\ntions. We elaborate on just a few here.\nModel Search Unlike previous table corpora, ModelTables can be\nused for model understanding. One possible line of inquiry is to",
            "content": "Zhengyuan Dong, Victor Zhong, and Ren√©e J. Miller Figure 7: Example of table discovery using (a) union-based and (b) dense retrieval for BERT query. (a) Union-based retrieval returns schema-aligned tables with consistent reporting of GLUE [53] and SQuAD [49] results. (b) Dense retrieval yields more diverse and semi-structured tables, reflecting varied author practices in reporting and layout. see if table relatedness can be used as proxy for model relatedness. Currently, model lakes rely on keyword search over names and metadata. Semantic search is only just emerging and relies on LLM Q&A over open models. We hypothesize that tables describing related models exhibit high structural and semantic similarity, allowing table discovery to serve as proxy for identifying related models. Scientific tables have been shown to encode rich semantics useful for reasoning and retrieval [10, 31, 39, 55, 60]. Validating this hypothesis requires (1) robust table-retrieval methods and (2) principled ground-truth definition of model relatedness. Model Table Integration While the problem of integrating set of data lake tables has been studied [28], the problem of integrating inconsistent data lake tables, including tables with many numeric values has not received sufficient attention. Using ModelTables, we can identify set of tables about model. We can characterize them into configuration tables, performance tables, and other types of tables. Within each type of table, we can identify inconsistencies in the tables and use integration techniques to resolve inconsistencies and define best integration. We speculate that such research could have implications not only for data integration research, but for research on model card verification and generation [36]. Model Understanding One of the holy grails in model lake research is understanding which model best suits task [45]. To this end, we speculate that using principled search over Model Tables, we may be able to understand which model performs best on different benchmarks and synthesize this rich structured information to do better query answering over models."
        },
        {
            "title": "8 CONCLUSION & FUTURE WORK\nWe presented a new benchmarks, ModelTables. We presented robust\nand repeatable methods to generate our benchmark and ground\ntruth. The benchmark itself was created with a January 2025 crawl\nof Huggingface and related resources. As shown in Figure 8, the\nnumber of tables and models are growing exponentially over time,\nsomething that will allow the generation of a significantly larger",
            "content": "ModelTables V2 using our reproducible and open benchmark creation pipeline and code. We presented characteristics of the benchmark tables, and showed that the ground-truth, derived from real reliable signals created by model developers, shows realistic characteristics (e.g., hub popular models and skewed relatedness distribution) that is unique among all table benchmarks surveyed (which use either human-labels or LLM generated labels). We presented one extensive use case of the benchmark to evaluate related table search methods on model tables where unlike in web tables or data lakes, relatedness cannot be defined solely based on structure [51]. We showed how our benchmark can give insight into research gaps and new areas for innovation. We also speculate on several other applications in both table understanding and model understanding. Figure 8: Cumulative counts of Huggingface models and tables over time. ACKNOLWEDGEMENTS We acknowledge the support of the Canada Excellence Research Chairs (CERC) program. Nous remercions le Chaires dexcellence en recherche du Canada (CERC) de son soutien. ModelTables: Corpus of Tables about Models REFERENCES [1] Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive Multi-task Representations with Pre-Finetuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 57995811. [2] Mehdi Akbarian Rastaghi, Ehsan Kamalloo, and Davood Rafiei. 2022. Probing the Robustness of Pre-Trained Language Models for Entity Matching. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA) (CIKM 22). Association for Computing Machinery, New York, NY, USA, 37863790. https://doi.org/10.1145/3511808. [3] Chandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2013. Methods for exploring and mining tables on Wikipedia. In SIGKDD Workshop on Interactive Data Exploration and Analytics, IDEA@KDD. ACM, 1826. https://doi.org/10.1145/2501511.2501516 [4] Alex Bogatu, Alvaro A. A. Fernandes, Norman W. Paton, and Nikolaos Konstantinou. 2020. Dataset Discovery in Data Lakes. In ICDE. 709720. [5] Peter Buneman, Dennis Dosso, Matteo Lissandrini, and Gianmaria Silvello. 2022. Data citation and the citation graph. Quantitative Science Studies 2, 4 (2022), 13991422. [6] Michael Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. Webtables: exploring the power of tables on the web. Proceedings of the VLDB Endowment 1, 1 (2008), 538549. [7] Adriane Chapman, Elena Simperl, Laura Koesten, George Konstantinidis, LuisDaniel Ib√°√±ez, Emilia Kacprzak, and Paul Groth. 2020. Dataset search: survey. VLDB J. 29, 1 (2020), 251272. https://doi.org/10.1007/S00778-019-00564-X [8] Christina Christodoulakis, Eric Munson, Moshe Gabel, Angela Demke Brown, and Ren√©e Miller. 2020. Pytheas: pattern-based table discovery in CSV files. Proceedings of the VLDB Endowment 13, 12 (2020), 20752089. [9] Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural Scaffolds for Citation Intent Classification in Scientific Publications. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 35863596. [10] Harsh Desai, Pratik Kayal, and Mayank Singh. 2021. TabLeX: benchmark dataset for structure and content information extraction from scientific tables. In Document Analysis and RecognitionICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 510, 2021, Proceedings, Part II 16. Springer, 554569. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers). 41714186. [11] [12] Yuyang Dong, Chuan Xiao, Takuma Nozawa, Masafumi Enomoto, and Masafumi Oyamada. 2023. DeepJoin: Joinable Table Discovery with Pre-trained Language Models. Proc. VLDB Endow. 16, 10 (2023), 2458 2470. https://doi.org/10.14778/ 3603581.3603587 [14] [13] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar√©, Maria Lomeli, Lucas Hosseini, and Herv√© J√©gou. 2024. The Faiss library. (2024). arXiv:2401.08281 [cs.LG] Julian Eberius, Katrin Braunschweig, Markus Hentsch, Maik Thiele, Ahmad Ahmadov, and Wolfgang Lehner. 2015. Building the dresden web table corpus: classification approach. In 2015 IEEE/ACM 2nd International Symposium on Big Data Computing (BDC). IEEE, 4150. [15] Mahdi Esmailoghli, Christoph Schnell, Ren√©e Miller, and Ziawasch Abedjan. 2025. BLEND: Unified Data Discovery System. In 2025 IEEE 41st International Conference on Data Engineering (ICDE). IEEE Computer Society, 737750. [16] Grace Fan, Roee Shraga, and Ren√©e Miller. 2024. Gen-T: Table Reclamation in Data Lakes. In 2024 IEEE 40th International Conference on Data Engineering (ICDE). IEEE, 35323545. [17] Grace Fan, Jin Wang, Yuliang Li, Dan Zhang, and Ren√©e J. Miller. 2023. Semanticsaware Dataset Discovery from Data Lakes with Contextualized Column-based Representation Learning. PVDLB 16, 7 (2023), 17261739. [18] Suzanne Fricke. 2018. Semantic scholar. Journal of the Medical Library Association: JMLA 106, 1 (2018), 145. [19] Moncef Garouani, Franck Ravat, and Nathalie Vall√®s-Parlangeau. 2024. Model Lake : New Alternative for Machine Learning Models Management and Governance. In Web Information Systems Engineering - WISE 2024 - 25th International Conference, Doha, Qatar, December 2-5, 2024, Proceedings, Part IV (Lecture Notes in Computer Science), Mahmoud Barhamgi, Hua Wang, and Xin Wang (Eds.), Vol. 15439. Springer, 133144. https://doi.org/10.1007/978-981-96-0573-6_10 [20] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum√© Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 8692. [21] Mazhar Hameed, Gerardo Vitagliano, Fabian Panse, and Felix Naumann. 2025. Repairing Raw Data Files with TASHEEH. SIGMOD Rec. 54, 1 (2025), 9099. https://doi.org/10.1145/3733620.3733639 [22] Jilin He, Ekaterina Borisova, and Georg Rehm. 2024. Towards Novel Classification of Table Types in Scholarly Publications. In International Workshop on Natural Scientific Language Processing and Research Knowledge Graphs. Springer, 3148. [23] Ginny Hendricks, Dominika Tkaczyk, Jennifer Lin, and Patricia Feeney. 2020. Crossref: The sustainable source of community-owned scholarly metadata. Quantitative Science Studies 1, 1 (2020), 414427. [24] Kevin Zeng Hu, Snehalkumar (Neil) S. Gaikwad, Madelon Hulsebos, Michiel A. Bakker, Emanuel Zgraggen, C√©sar A. Hidalgo, Tim Kraska, Guoliang Li, Arvind Satyanarayan, and √áagatay Demiralp. 2019. VizNet: Towards Large-Scale Visualization Learning and Benchmarking Repository. In CHI. ACM, 662. https: //doi.org/10.1145/3290605.3300892 [25] Madelon Hulsebos, √áaƒüatay Demiralp, and Paul Groth. 2021. Gittables: largescale corpus of relational tables. arXiv preprint arXiv:2106.07258 (2021). [26] Madelon Hulsebos, √áagatay Demiralp, and Paul Groth. 2023. Gittables: largescale corpus of relational tables. Proceedings of the ACM on Management of Data 1, 1 (2023), 117. [27] Aamod Khatiwada, Grace Fan, Roee Shraga, Zixuan Chen, Wolfgang Gatterbauer, Ren√©e J. Miller, and Mirek Riedewald. 2023. SANTOS: Relationship-based Semantic Table Union Search. In Accepted to appear in SIGMOD Conference. ACM. https://arxiv.org/pdf/2209.13589.pdf [28] Aamod Khatiwada, Roee Shraga, Wolfgang Gatterbauer, and Ren√©e Miller. 2022. Integrating Data Lake Tables. Proceedings of the VLDB Endowment 16, 4 (2022), 932945. [29] Seongchan Kim, Keejun Han, Soon Young Kim, and Ying Liu. 2012. Scientific table type classification in digital library. In Proceedings of the 2012 ACM symposium on Document engineering. 133136. [30] Keti Korini, Ralph Peeters, and Christian Bizer. 2022. SOTAB: The WDC Schema. org table annotation benchmark. In CEUR Workshop Proceedings, Vol. 3320. RWTH Aachen, 1419. [31] Benno Kruit, Hongyu He, and Jacopo Urbani. 2020. Tab2know: Building knowledge base from tables in scientific papers. In International Semantic Web Conference. Springer, 349365. [32] Oliver Lehmberg, Dominique Ritze, Robert Meusel, and Christian Bizer. 2016. large public corpus of web tables containing time and context metadata. In Proceedings of the 25th international conference companion on world wide web. 7576. [34] [33] Quentin Lhoest, Albert Villanova Del Moral, Yacine Jernite, Abhishek Thakur, Patrick Von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. 2021. Datasets: community library for natural language processing. arXiv preprint arXiv:2109.02846 (2021). Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: Python Toolkit for Reproducible Information Retrieval Research with Sparse and Dense Representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021). 23562362. [36] [35] Zihang Lin, Yian Yin, Lu Liu, and Dashun Wang. 2023. SciSciNet: large-scale open data lake for the science of science research. Scientific Data 10, 1 (2023), 315. Jiarui Liu, Wenkai Li, Zhijing Jin, and Mona T. Diab. 2024. Automatic Generation of Model and Data Cards: Step Towards Responsible AI. In ACL, Kevin Duh, Helena G√≥mez-Adorno, and Steven Bethard (Eds.). Association for Computational Linguistics, 19751997. https://doi.org/10.18653/V1/2024.NAACL-LONG. [37] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). [38] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 49694983. https://doi.org/10.18653/v1/2020. acl-main.447 [39] Xinyuan Lu, Liangming Pan, Qian Liu, Preslav Nakov, and Min-Yen Kan. [n.d.]. SCITAB: Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. In The 2023 Conference on Empirical Methods in Natural Language Processing. Joel Mackenzie, Andrew Trotman, and Jimmy Lin. 2023. Efficient document-ata-time and score-at-a-time query evaluation for learned sparse representations. ACM Transactions on Information Systems 41, 4 (2023), 128. [40] [41] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* 19). Association for Computing Machinery, New York, NY, USA, 220229. https://doi.org/10.1145/ 3287560.3287596 [42] Fatemeh Nargesian, Erkang Zhu, Ren√©e J. Miller, Ken Q. Pu, and Patricia C. Arocena. 2019. Data Lake Management: Challenges and Opportunities. PVLDB 12, 12 (2019), 19861989. [43] Fatemeh Nargesian, Erkang Zhu, Ken Pu, and Ren√©e Miller. 2018. Table union search on open data. PVLDB 11, 7 (2018), 813825. [44] Koyena Pal, David Bau, and Ren√©e J. Miller. 2024. Model Lakes. CoRR https://doi.org/10.48550/ARXIV.2403.02327 abs/2403.02327 (March 2024). arXiv:2403.02327 [45] Koyena Pal, David Bau, and Ren√©e J. Miller. 2025. Model Lakes. In Proceedings 28th International Conference on Extending Database Technology, EDBT 2025, Barcelona, Spain, March 25-28, 2025, Alkis Simitsis, Bettina Kemme, Anna Queralt, Oscar Romero, and Petar Jovanovic (Eds.). OpenProceedings.org, 985995. https: //doi.org/10.48786/EDBT.2025.81 [46] Koyena Pal, Aamod Khatiwada, Roee Shraga, and Ren√©e J. Miller. 2023. Generative Benchmark Creation for Table Union Search. arXiv:2308.03883 [cs.DB] [47] Ralph Peeters, Alexander Brinkmann, and Christian Bizer. 2024. The Web Data Commons Schema. org Table Corpora. In Companion Proceedings of the ACM Web Conference 2024. 10791082. Jason Priem, Heather Piwowar, and Richard Orr. 2022. OpenAlex: fully-open index of scholarly works, authors, venues, institutions, and concepts. arXiv e-prints (2022), arXiv2205. [48] [49] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 23832392. [50] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 39823992. [51] Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Y. Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu. 2012. Finding related tables. In SIGMOD, K. Sel√ßuk Candan, Yi Chen, Richard T. Snodgrass, Luis Gravano, and Ariel Fuxman (Eds.). ACM, 817828. https://doi.org/10.1145/2213836.2213962 [52] Marco Valenzuela, Vu Ha, and Oren Etzioni. 2015. Identifying Meaningful Citations. [53] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. [n.d.]. GLUE: Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In International Conference on Learning Representations. Zhengyuan Dong, Victor Zhong, and Ren√©e J. Miller [54] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations. 3845. Junyi Wu, Chen Ye, Haoshi Zhi, and Shihao Jiang. 2023. Column-Type Prediction for Web Tables Powered by Knowledge Base and Text. Mathematics 11, 3 (2023), 560. [55] [56] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking baselines using Lucene. Journal of Data and Information Quality (JDIQ) 10, 4 (2018), 120. [57] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. 2020. Generative data augmentation for commonsense reasoning. arXiv preprint arXiv:2004.11546 (2020). [58] Haojie Zhang, Ge Li, Jia Li, Zhongjin Zhang, Yuqi Zhu, and Zhi Jin. 2022. Finetuning pre-trained language models effectively by optimizing subnetworks adaptively. Advances in neural information processing systems 35 (2022), 2144221454. [59] Shuo Zhang and Krisztian Balog. 2021. Semantic Table Retrieval Using Keyword and Table Queries. ACM Trans. Web 15, 3 (2021), 11:111:33. https://doi.org/10. 1145/3441690 [60] Xuanliang Zhang, Dingzirui Wang, Baoxin Wang, Longxu Dou, Xinyuan Lu, Keyan Xu, Dayong Wu, Qingfu Zhu, and Wanxiang Che. 2024. SCITAT: Question Answering Benchmark for Scientific Tables and Text Covering Diverse Reasoning Types. arXiv preprint arXiv:2412.11757 (2024). [61] Erkang Zhu, Dong Deng, Fatemeh Nargesian, and Ren√©e Miller. 2019. Josie: Overlap set similarity search for finding joinable tables in data lakes. In SIGMOD. 847864. [62] Erkang Zhu, Fatemeh Nargesian, Ken Q. Pu, and Ren√©e J. Miller. 2016. LSH Ensemble: Internet-Scale Domain Search. Proc. VLDB Endow. 9, 12 (2016), 1185 1196. https://doi.org/10.14778/2994509.2994534 [63] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. 2022. Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1680416815."
        }
    ],
    "affiliations": [
        "University of Waterloo"
    ]
}