{
    "paper_title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
    "authors": [
        "Ziheng Ouyang",
        "Yiren Song",
        "Yaoli Liu",
        "Shihao Zhu",
        "Qibin Hou",
        "Ming-Ming Cheng",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 4 1 6 0 2 . 1 1 5 2 : r The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment Ziheng Ouyang1, Yiren Song2, Yaoli Liu3, Shihao Zhu1, Qibin Hou1, Ming-Ming Cheng1, Mike Zheng Shou2 1VCIP, Nankai University 2Show Lab, National University of Singapore 3State Key Laboratory of CAD&CG, Zhejiang University Corresponding author. Abstract: Previous works have explored various customized generation tasks given reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying reference-guided post-editing approach and present our ImageCritic. We first construct dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on thorough examination of the models attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods. Project Page: https://ouyangziheng.github.io/ImageCritic-Page/ HVision@Nankai"
        },
        {
            "title": "Introduction",
            "content": "In recent years, image generation systems based on diffusion models have evolved from UNet architectures to Transformerbased Diffusion Transformer (DiT) models [70, 6, 24], with reference-guided generation becoming one of the key research trends, powering applications such as virtual try-on [36, 34, 15], image editing [28, 17, 61, 62, 63, 21, 64, 8, 54, 64], and subject customization [4, 50, 17, 44, 3, 60, 74, 75, 37]. However, due to discrepancies introduced by VAE-based encoding and decoding in encoderdecoder frameworks and the loss of shallow-layer information in decoder-only structures, existing models often suffer from inconsistent and inaccurate fine-grained details between the generated image and the reference image. As illustrated in Fig. 1, images generated using current state-of-the-art [1, 10, 58, 60] customized generation methods commonly exhibit inconsistencies or blurriness in textual and logo regions. One naive approach that aims to solve this inconsistency problem between generated images and reference images is reference-based super-resolution models, such as ReFIR [14], which refine blurry regions to improve alignment with the reference. However, as shown in Fig. 1(a), the inconsistencies in the generated images remain uncorrected, and in some local regions the errors become even more severefalling short of the requirements for consistency-aware correcting. Another way is to use multi-image editing models, like QwenImage [58], to repair local details through text instructions. As shown in Fig. 1(a), the model fails to accurately locate and correct the target details. These demonstrate that, even with reference image, enhancing the consistency of existing customized generation models remains challenging problem. We summarize the failure of the aforementioned models into two main issues. First, there is lack of high-quality data that is capable of focusing on and improving fine-grained details. For example, datasets such as Subjects200K [50], UNO1M [60], and X2I2 [59] mainly focus on maintaining the overall object consistency while neglecting subtle local details. To address this, we construct referencetarget pairs that maintain strong consistency and simultaneously capture the inherent generation issues of current models. Specifically, we employ VLM-based selection strategy to build the referencetarget pairs and further introduce Flux-Fill [24] to explicitly degrade local regions of the generated images, thus simulating inconsistency in details and completing our dataset construction. In addition, existing models often fail to attend to, localize, and align fine-grained regions for precise correction. To tackle this problem, we visualize the attention maps of the reference and input images within the noise regions and observe strong coupling between them. To mitigate this, we introduce an attention alignment loss, which explicitly supervises the attention maps using region masks, enabling effective decoupling and automatic localization of to be corrected regions. Fur1 Figure 1 Visual illustrations. (a) illustrates that we first conduct customized generation using GPT-4o [1], and then apply different methods for the post editing. Edit method [58] struggle to achieve fine-grained consistent generation, while images processed by super-resolution methods [14] often exhibit noticeable detail inaccuracies. In contrast, our proposed ImageCritic corrects local details to ensure text and logo consistency while maintaining accurate spatial alignment, significantly improving the overall coherence of generated images. (b) We further apply our method to customized results generated by both state-of-the-art closed-source [10] and open-source models [58, 60]. After performing our correction, the fine-grained details of the generated images align precisely with those of the original objects, demonstrating the superior performance of our approach. thermore, we analyze the intrinsic properties of our chosen base model. To improve generalization and reference-image understanding, we proposed detail encoder, which explicitly embeds features for both reference and input images, fully leveraging the strengths of the pre-trained editing model and further improving the consistency between generated outputs and inputs. To improve usability, we further design an agent-based processing chain that automates the entire workflow, including consistency evaluation, discrepancy localization, reference retrieval, and image refinement. This enables multi-round humanagent interaction and fully automated one-click refinement process, greatly enhancing the models ability to handle complex real-world scenarios. As shown in Fig. 1, our method significantly improves the consistency of generated images. The refined results preserve the physical priors of relative spatial relationships between objects while maintaining environmental and lighting consistency. Meanwhile, local regions such as the wheel hub in Fig. 1(a) demonstrate remarkable improvement in structural fidelity and fine-grained alignment. Our contributions can be summarized as follows. To further enhance the consistency of existing generative models, we construct reference-degraded-target dataset and establish benchmark for complex object customization, providing valuable resources for high-consistency generation and editing research. To address the common issue of detail inconsistency and inaccuracy for existing customized generation models, we introduce an attention alignment loss for regionaware reference perception and detail encoder for precise task understanding. We design an agent system that seamlessly integrates the critic model, enabling automated and intuitive multiround editing while ensuring robust performance."
        },
        {
            "title": "2 Related Work",
            "content": "Conditional image generation. Conditional image generation aims to improve controllability in diffusion models by introducing external conditions. Existing conditionguided diffusion models can be broadly categorized into two types of tasks: condition-aligned generation and condition-referenced generation. Condition-aligned generation focuses on abstract structural control, such as pose or shape alignment [69, 31, 33, 32]. In this setting, conditional inputs are strictly aligned with the output at the pixel level, achieving fine-grained spatial control. In contrast, condition-referenced generation emphasizes high-level semantic and object-specific control rather than pixel-level correspondence. Representative works, such as IP-Adapter [66], Textual inversion [12], and others, introduce adapters or encoders [23, 55, 72, 43, 27, 13] to inject subject-related conditional information into the diffusion process, enabling generation that preserves reference subject consistency. Unlike previous approaches that inject conditional signals directly at the feature level, DiT-based methods [26, 53, 61, 62, 59, 73, 48, 46, 47, 52, 57] unify semantic and spatial conditions into token sequences and integrate them with text tokens through multimodal attention or token concatenation mecha2 Figure 2 Data curation pipeline. (a) illustrates the complete pipeline of our approach, which involves generating customized images using existing state-of-the-art models, applying VLM-based filtering, and performing degradation. (b) shows local regions from our dataset, where the target patch aligns well with the input patch, and the degraded patch effectively simulates fine-grained inconsistencies in text and logo areas commonly seen between the input patch and the generated patch. nisms. To enhance subject consistency, DreamO [35] designs router mechanism to focus attention on target subjects, and XVerse [3] adopts text-stream modulation to transform reference images into token-specific offsets. MOSAIC [44] further introduces fine-grained regional constraints via explicit semantic point correspondences. Despite these advances, inconsistency in fine-grained details remains significant limitation. To address these issues, we propose unified postediting correction framework that substantially enhances the overall consistency of generated images. LLM for Generation. Large Language Models (LLMs), such as ChatGPT [1] and Llama [51, 51], have demonstrated outstanding capabilities in the field of natural language processing. Meanwhile, Multimodal Large Language Models (MLLMs), including LLaVA [29], Claude, and GPT-4, further integrate visual understanding, significantly enhancing the ability to process multimodal data. LLMs have now become core component of vision-language tasks, where their powerful language understanding and reasoning abilities have driven advances in cross-modal semantic alignment and visual reasoning, as demonstrated in works such as VisProg [16] and ViperGPT [49]. At the same time, LLMs are widely adopted within agent frameworks [65, 45, 30, 5, 67], where they learn to invoke external tools to accomplish complex tasks such as visual interaction, speech processing [16], software development [39], and game operation [7]. With the deepening of multimodal training, the interconnections among generative agents have become increasingly integrated. LayoutGPT [9] can generate spatial layouts from textual prompts, GenArtist [56] and LLM Blueprints employ generate-thenedit paradigm to optimize outputs, and LayerCraft [71] introduces an integrated multi-agent framework that unifies layout planning and object integration within single system."
        },
        {
            "title": "3 Data Curation",
            "content": "Under our problem setup, the input consists of the customized generated image to be repaired and reference image. Our goal is to enhance the fine-grained consistency of the generated image. However, given that existing models often suffer from detail misalignment issues, identifying more consistent inputoutput pairs to achieve substantial performance improvement over current models remains challenging problem. We decompose the problem into two main components: constructing consistent referencetarget pairs and generating images that reflect the inherent issues present in existing generative models. First, we employ VLM-based filtering strategy to leverage the complementary strengths of both inputs, thereby constructing the most ideal ground-truth supervision. Then, we simulate common artifacts observed in current generative models, such as text rendering errors and logo mismatches, through controlled degradation. As shown in Fig. 2(a), we begin by collecting set of high-quality images and generating diverse prompts that include various objects, scenes, humanobject interactions, and lighting conditions. Then, by using state-of-the-art text-toimage models, like Flux Kontext [25], GPT-4o [1], and NanoBanana [10], we produce scene-rich images. These images are evaluated by Qwen-VL [2], which scores them in terms of visual quality and clarity. We retain only the top-performing samples. Next, Qwen is used again to annotate each image, followed by Grounding SAM [42] for object detection and segmentation. To mitigate potential mask errors from SAM, we further use Qwen to recheck and filter the outputs, ensuring consistency while preserving segmentation accuracy. This process yields set of high-quality referencetarget pairs. To simulate real-world degradation, we apply the Flux-Fill [24] model to actively corrupt the filtered high-quality images. Specifically, we randomly select subsets of the masked regions as inputs to Flux-Fill, prompting it to inpaint regions or alter elements, such as text or logos. The resulting degraded samples are then screened by Qwen to remove those with severe visual errors. Through this data construction pipeline, we effectively address the two challenges described above and acquire 10k high-quality reference-degraded-target triplets. As illustrated in Fig. 2(b), by comparing the input image patch and the target patch, one can observe that the filtered targets exhibit high degree of consistency with the original input. Mean3 while, when comparing the models generated patch and the degraded patch, it can be seen that the degraded patches effectively reproduce common artifacts found in current generative models, including logo misalignment and text generation errors, which provide realistic supervision signals and solid basis for subsequent model training."
        },
        {
            "title": "4 Methodology",
            "content": "Our overall pipeline is illustrated in Fig. 3. As shown, the reference image and input image are fed into the detail encoder together with the prompt to obtain text tokens. These tokens are then concatenated with the image tokens encoded by the VAE and subsequently fed into the DiT for denoising. During training, we introduce an Attention Alignment Loss to ensure both the disentanglement and alignment of attentions between the condition inputs and the noisy target. During inference, we design an agent chain that, given reference image and an input image to be corrected, automatically identifies the required patches and organizes the prompt. Both training and inference follow the same prompt format: Use the object in IMG1 as reference to be corrected, replace, or enhance the object in IMG2. Here, IMG1 and IMG2 are trigger tokens corresponding to the reference image and the input image, respectively, while {object} denotes simplified description of the target object identified from the reference image."
        },
        {
            "title": "4.1 Preliminaries",
            "content": "The DiT architecture employs multi-modal attention mechanism by concatenating text tokens cT RM and noisy image tokens ztgt RN as input, where denotes the embedding dimension, and and are the numbers of image and text tokens, respectively. It projects the hidden states of these tokens into query Q, key K, and value representations and perform attention: Attention(Z) = softmax (cid:19) (cid:18) QK V, (1) where = [ztgt, cT ] denotes the concatenation of the text tokens and the noisy image tokens. Recent models, such as the Flux Kontext image editing model [25], extend this formulation to = [ztgt, cT , zc], which introduces an additional control token zc to incorporate image-level conditioning. Here, zc represents an image token corresponding to the image to be edited. We adopt Flux Kontext [25] as our base model to leverage its image control capability for fine-grained consistency correcting. fine-tuning, the model is able to understand our intention and perform local correcting. However, for small text or fine-grained regions, the generated images still retain former elements from the input image, indicating that the overall consistency remains insufficient. As observed in [18, 19, 35], the image-to-image attention mechanism is capable of capturing spatial patterns during generation. Motivated by this, we further visualize the attention maps between the condition inputs and the generation target, as shown in Fig. 4. Specifically, the attention map is formulated as = QciK tgt , {R, I}, (2) where and denote the reference and input images, respectively, Qci represents the condition tokens projected into the query space, and Ktgt denotes the noisy tokens projected into the key space. By visualizing the influence of tokens from the reference and input branches on the noise latent, we observe that in our fine-tuned model, the attentions from these two branches are strongly coupled within the noise region. As result, the input and reference tokens convey conflicting cues for local generation, preventing the model from accurately identifying the reference details necessary for precise correction. Consequently, the model either neglects modifications in these regions or performs inaccurate edits. To achieve the goal of consistency enhancement, we expect the input tokens to provide guidance on object-level details, while the reference tokens focus on background, lighting, and other global aspects. To realize this effect, we introduce an Attention Alignment Loss that enforces alignment at the attention level, explicitly guiding the model to query and reference the target regions appropriately. Specifically, we first define binary object mask as: (cid:40) B(p) = 1, background, 0, subject, (3) Subsequently, we incorporate an MSE-based alignment term into the DiT framework to optimize the attention distributions of both the reference and input branches with respect to the noise regions, defined as: LG = LR = 1 nl 1 nl nl1 (cid:88) j=0 nl1 (cid:88) j=0 (cid:13) (cid:13) (cid:13)B (cid:16)"
        },
        {
            "title": "M j\nG",
            "content": "(cid:17)(cid:13) 2 (cid:13) (cid:13) 2 , (cid:13) (cid:13) (cid:13)B (cid:16)"
        },
        {
            "title": "M j\nR",
            "content": "(cid:17)(cid:13) 2 (cid:13) (cid:13) 2 , (4) (5)"
        },
        {
            "title": "4.2 Consistency Critic",
            "content": "Attention alignment. Given the dataset obtained from Sec. 1, straightforward idea is to fine-tune the base model using LoRA [20]. As shown in the first row of Fig. 4, after LoRA where denotes the complement of B, represents elementwise multiplication, is the layer index, nl is the total number of layers, and () denotes min-max normalization operator, which normalizes the input values to the range between zero 4 Figure 3 Overview of the proposed ImageCritic. We propose ImageCritic, which employs Detail Encoder and an Attention Alignment Loss to enable the model to localize regions requiring restoration, thereby achieving high-quality and consistent image correcting. Furthermore, we develop fully automated agent framework that supports both local patch restoration and multi-round correcting processes. Figure 5 Effect of the detail encoder. We find that when the input image exhibits structural differences from the reference image, the model fails to correctly identify the intended reference object, leading to inconsistent generation results. Figure 4 Attention visualization. We separately extract the noise attention maps with respect to the reference image and the input image to be corrected, denoted as MR and MI , respectively. The first row shows the results of the LoRA-finetuned base model, while the second row presents the results after applying the attention alignment loss. The first two columns correspond to the attention map of the double stream layer, and the last two columns correspond to the single stream layer. It can be observed that the attention alignment loss effectively promotes attention disentanglement. and one. Finally, the overall training objective is formulated as: = Ldif + LR + LG. (6) Here, Ldif represents the rectified flow-matching loss originally adopted by the base model. This design encourages the reference patches to focus primarily on the object regions rather than the background, while preventing the input patches from exerting excessive influence on the regions that require correction. Moreover, as observed in Fig. 4, in the double stream layers, the attentions from both the reference and input branches are concentrated and exhibit strong responses to the noise regions, while in the single block, the attention from the condition branch is more dispersed, lacking distinct regional features. Therefore, we apply the Attention Alignment Loss only at the double stream blocks of the model, and nl is set to the number of double-stream block layers only. Detail encoder. During training, we provide the model with two image inputs simultaneously and set the triggers IMG1 and IMG2\" to represent each image, enabling precise prompt description. However, for the T5 text encoder, the latent representation derived from the same trigger word remains identical across different inputs, which causes the model to struggle in associating different image inputs with their respective triggers, leading to incorrect outputs. As shown in Fig. 5, when the input image slightly differs in shape from the reference image, the model struggles to accurately identify the corresponding part of both condition images within the prompt, leading to erroneous generation. Inspired by works such as PhotoMaker [27] and Dreamo [35], we design Detail Encoder that couples the latent representation of the trigger word with that of the image, enhancing the models understanding of the reference image. This coupling Figure 6 Visual Results. We present the results obtained by applying our method to the generated outputs of existing state-of-the-art customized generation models, including both open-source and closed-source variants. As shown, our method substantially enhances the consistency of the generated images. mechanism allows the model to link textual triggers and visual content more effectively, thereby improving reference consistency in the generated results. Specifically, let the prompt, after being processed by the T5 encoder, produce token representations RM dt, and let the flattened hidden representation of the image obtained from CLIP [40] be Ci R1dc. From these tokens, we extract the hidden states PR, PI R1dt, corresponding to the trigger words IMG1 and IMG2, which refer to the reference image and the input image to be corrected, respectively. Here, dt and dc denote the hidden dimensionalities of the T5 and CLIP models, respectively. We then construct fused hidden representation: = [Pi; Ci] R1(dt+dc), {R, I}. (7) Subsequently, is passed through two-layer MLP and projected back to the original dimension, yielding Pi, which is then used to update the corresponding hidden states of the trigger words PR and PI in the prompt representation, respectively."
        },
        {
            "title": "4.3 Agent Chain",
            "content": "In our experiments across various generative models, we observe that they tend to compress the reference image, resulting in local regions where fine textual details become too small or even fail to render properly. However, in real-world scenarios, users often provide high-resolution reference images that are not fully utilized by such models. Moreover, existing models sometimes produce results in which the reference image occupies only small portion of the generated output, which can negatively affect the correcting quality. To better preserve fine details, fully leverage high-resolution guidance, and enable more intuitive interaction, we develop an agent chain framework. Our system integrates multiple specialized agents through carefully designed prompts, with each agent responsible for specific subtaskssuch as assessing content consistency, identifying inconsistent regions, selecting rele6 Table 1 Quantitative comparison on CriticBench. Evaluation of consistency correcting across different generation models on our proposed CriticBench. Method CLIP-I DINO DreamSim Sora [1] Nano-Banana [10] 78.7 79.6 +0.9 79.2 79.8 +0.6 68.4 69.2 +0.8 66.5 66.9 +0.4 29.1 28.7 -0.4 32.0 31.8 -0.2 Xverse [3] DreamO [35] MOSAIC [44] OmniGen2 [59] UNO [60] Qwen-Image [58] 76.5 79.9 +3.4 77.8 78.1 +0.3 74.6 77.1 +2.5 78.8 79.3 +0.5 77.6 78.9 +1.3 77.9 78.2 +0.3 68.8 71.9 +3.1 67.7 68.2 +0.5 62.6 65.0 +2.4 70.0 70.8 +0.8 68.4 69.3 +0.9 69.2 69.4 +0.2 34.3 31.4 -2.9 29.6 29.2 -0.4 35.2 31.4 -3.8 27.7 27.0 -0.7 33.6 32.1 -1.5 30.3 30.1 -0.2 Table 2 Quantitative comparison on DreamBench++. Evaluation of consistency correcting across different generation models on DreamBench++. Method CLIP-I DINO DreamSim Xverse [3] Dreamo [35] MOSAIC [44] OmniGen2 [59] UNO [60] Qwen-Image [58] 81.5 82.3 +0.8 83.3 83.4 +0.1 77.5 78.3 +0.8 78.5 79.2 +0.7 78.7 79.0 +0.3 78.8 79.7 +0.9 64.5 66.3 +1.8 72.5 72.7 +0.2 57.6 59.2 +1.6 62.0 64.5 +2.5 62.7 63.2 +0.5 61.8 64.9 +3.1 36.9 34.9 -2.0 27.2 27.1 -0.1 42.7 41.1 -1.6 38.6 36.3 -2.3 37.7 37.1 -0.6 36.9 34.5 -2. proposed method to enhance the consistency of the generated results. As illustrated in Fig. 6, existing approaches can generally preserve the primary subject; however, they often struggle to handle fine-grained local details, which limits their overall applicability. In contrast, our method achieves significant improvement in consistency while maintaining the original lighting, texture, and background fidelity of the generated images. In addition to these advantages, our approach further demonstrates strong robustness and generalization across diverse scenarios. As shown in Fig. 7, our method achieves highly consistent correcting across varying viewpoints, object categories, and diverse stylization settings, while faithfully preserving the background, illumination, and stylistic characteristics of the input images."
        },
        {
            "title": "5.3 Quantitative comparisons.",
            "content": "Benchmark. We first evaluate our model on Dreambench++ [38]. Subsequently, to further examine the models performance on more challenging cases involving text, logos, and other complex visual details, we introduce new benchmark, CriticBench, which specifically focuses on fine-grained detail preservation. Our benchmark consists of 300 images, including 200 intricate multilingual product images and 100 images of apparel and accessories. For each image, we utilize the Qwen-VL [2] model to generate editing instructions that most accurately align with the intended scene of the input image. Subsequently, we apply our method to these results and evaluate the quality of the final generated images. Evaluation Metrics. We evaluate our approach on the aforementioned DreamBench++ [38] and CriticBench benchmarks, leveraging the methods introduced earlier for comparison. For each method, we compare each generated image with its corresponding reference image and compute metrics, Figure 7 Robust Generalization. As shown in the clocks numerals and hands and the robots panels, our proposed ImageCritic effectively preserves style and illumination consistency rather than relying on naive copy-paste patterns, achieving detailed and coherent corrections across varying viewpoints, cross-category objects, and diverse stylization settings. vant reference patches, summarizing prompts, and performing final image correction (handled by the critic model). Specifically, we employ the Qwen-Agent [2] as coordinator, which supervises the overall workflow, facilitates smooth communication between users and agents, and orchestrates cooperation among all components. While the system can operate autonomously, users can also interactively adjust input patches. For instance, if user wishes to regenerate small textual details within product region, the coordinator delegates this request to the corresponding agent for reprocessing, ensuring that the input patches align precisely with the users intent."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation details. We adopt Flux.1-Kontext-dev [25] as our base model, upon which we conduct LoRA fine-tuning with learning rate of 1 104 and rank of 128, training on 2 GPUs with batch size of 4 per GPU (total batch size = 8) for total of 20,000 steps. After training, we enhance subset of the generation targets with low consistency in the dataset using our proposed ImageCritic to further improve overall generation consistency. Details are provided in the Supplementary Material. The model was then retrained following the same settings described above."
        },
        {
            "title": "5.2 Qualitative comparisons.",
            "content": "We employ both open-source models XVerse [3], Dreamo [35], MOSAIC [44], OmniGen2 [59], UNO [60], and Qwen-Image [58] as well as the closed-source models NanoBanana [10] and GPT-Image [1], to evaluate single subject customization performance. Subsequently, we apply our 7 Table 3 Ablation study. Quantitative evaluation of the contribution of each component in our proposed model. AAL DE CLIP-I DINO DreamSim 77.9 +0.3 78.3 +0.7 78.3 +0.7 78.9 +1.3 68.1 +0.4 68.4 +0.7 68.6 +0.9 68.9 +1. 31.3 -0.2 30.6 -0.9 30.6 -0.9 29.8 -1.7 including CLIP Image Score [40], DINO Score [68], and DreamSim [11]. Specifically, on our proposed CriticBench, for each method, we utilize Qwen-VL [2] to perform semantic grounding on the corresponding objects within the generated images to enhance the evaluation of fine-grained detail consistency. The evaluation results on DreamBench++ [38] and CriticBench are presented in Tab. 1 and Tab. 2, respectively. As shown, our method substantially improves the consistency between the generated images and their corresponding reference images, demonstrating strong capability in recovering details."
        },
        {
            "title": "5.4 Ablation analysis",
            "content": "We validate the effectiveness of our proposed dataset and conduct ablation studies on two key design components: Attention Alignment Loss (AAL) and Detail Encoder (DE). Qualitative comparisons. As presented in Fig. 4 and Fig. 5, We compare the generation results of the model before and after adding our proposed module. As shown in Fig. 4, the results of the fine-tuned base model validate the effectiveness of our proposed dataset. However, simple fine-tuning still leads to deficiencies in subtle local details. After introducing the AAL, the attentions from the input and reference branches become concentrated on the required object and background regions, achieving attention disentanglement and significantly improving the models ability to correct local details. Meanwhile, as shown in Fig. 5, after incorporating the DE, the models capability to interpret complex inputs is enhanced, enabling it to accurately locate the target regions. Quantitative evaluation, Following the setup in Sec. 5.3, we perform consistency correcting on the generated results of different methods using CriticBench, and report the average score improvements across all methods. As shown in Tab. 3, fine-tuning the base model on our dataset alone already improves the consistency of generated images, verifying the effectiveness of our proposed dataset. Moreover, introducing either component individually leads to additional performance gains, while combining both yields further and more substantial improvement. These results confirm the effectiveness of our proposed modules and their synergistic effect."
        },
        {
            "title": "5.5 Discussion",
            "content": "To evaluate the effectiveness of our proposed agent chain in locating input and reference patches, we manually annotated the target regions requiring correction along with their correTable 4 Agent chain discussions. Comparison between our proposed agent chain and human annotations in locating required input and reference patches. Metric Mean IoU (%) mAP@50 (%) Score 75.3 88.4 sponding reference regions within the proposed benchmark. We then compared these human-defined bounding boxes with those automatically predicted by our agent, and quantified their alignment using the Intersection over Union (IoU): IoU = Bagent Bhuman Bagent Bhuman , (8) where Bagent and Bhuman represent the bounding boxes predicted by the agent and human annotators, respectively. Additionally, we also compute the mean Average Precision at 50% IoU threshold (mAP@50), which assesses how accurately the agent detects and localizes regions whose overlap with the ground truth exceeds 50%. As shown in Tab. 4, our agent achieves mean IoU of 75.3% and mean mAP@50 of 88.4%, indicating its strong capability in accurately querying and localizing fine-grained details."
        },
        {
            "title": "6 Conclusions",
            "content": "In this paper, we introduce ImageCritic, novel post-editing correction framework designed to enhance the consistency of customized image generation, and construct referencedegraded-target triplet dataset tailored to this task, enabling more accurate correcting. Extensive experimental results demonstrate that ImageCritic effectively mitigates inconsistencies in generated images, significantly improving detail consistency across both open-source and closed-source models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2, 3, 7, 13 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 7, 8, 12 [3] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. 1, 3, 7, 13 8 [4] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, et al. Unireal: Universal image generation and editing via learning real-world dynamics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1250112511, 2025. 1 [5] Yuming Chen, Jiangyan Feng, Haodong Zhang, Lijun Gong, Feng Zhu, Rui Zhao, Qibin Hou, Ming-Ming Cheng, and Yibing Song. Re-aligning language to visual objects with an agentic workflow. In ICLR, 2025. [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1 [7] Meta Fundamental AI Research Diplomacy Team (FAIR), Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378(6624):10671074, 2022. 3 [8] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: Diffusion transformer for image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2969 2977, 2025. 1 [9] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. 3 [10] Alisa Guillaume Vernade, Kat Kampf, Fortin, 2.5 and Ammaar Reshi. flash image model. image, https://developers.googleblog.com/en/introducing-gemini-25-flash-image/, 2025. 1, 2, 3, 7, 13 state-of-the-art Introducing gemini our [11] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 8 [12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [13] Yan Gong, Yiren Song, Yicheng Li, Chenglin Li, and Yin Zhang. Relationadapter: Learning and transferring visual relation with diffusion transformers. arXiv preprint arXiv:2506.02528, 2025. 2 [14] Hang Guo, Tao Dai, Zhihao Ouyang, Taolin Zhang, Yaohua Zha, Bin Chen, and Shu-Tao Xia. Refir: Grounding large restoration models with retrieval augmentation. arXiv preprint arXiv:2410.05601, 2024. 1, 2 [15] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks. arXiv preprint arXiv:2501.15891, 2025. 1 [16] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In 9 Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1495314962, 2023. 3 [17] Zhen Han, Zeyinzi Jiang, Yulin Pan, Jingfeng Zhang, Chaojie Mao, Chenwei Xie, Yu Liu, and Jingren Zhou. Ace: Allround creator and editor following instructions via diffusion transformer. arXiv preprint arXiv:2410.00086, 2024. 1 [18] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving single-and multi-human image personalization. arXiv preprint arXiv:2408.05939, 2024. [19] Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, and Liefeng Bo. Anystory: Towards unified single and multiple subject personalization in text-to-image generation. arXiv preprint arXiv:2501.09503, 2025. 4 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1:3, 2022. 4 [21] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu. Photodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397, 2025. 1 [22] Mi Jian, Yumeng Li, Bowen Wang, Xiaomin He, Zheyuan Gu, Qing Yan, Colin Zhang, and Lei Zhang. dots.ocr: Multilingual document layout parsing in single vision-language model. https://github.com/rednote-hilab/dots. ocr/blob/master/assets/blog.md, 2025. 13, 17, 18, 19 [23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-toimage diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. 2 [24] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 3, 12 [25] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 3, 4, 7 [26] Duong Le, Tuan Pham, Sangho Lee, Christopher Clark, Aniruddha Kembhavi, Stephan Mandt, Ranjay Krishna, and Jiasen Lu. One diffusion to generate them all. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26712682, 2025. 2 [27] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86408650, 2024. 2, 5 [28] Zhong-Yu Li, Ruoyi Du, Juncheng Yan, Le Zhuo, Zhen Li, Peng Gao, Zhanyu Ma, and Ming-Ming Cheng. Visualcloze: universal image generation framework via visual in-context learning. arXiv preprint arXiv:2504.07960, 2025. 1 [29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [30] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Interngpt: Solving vision-centric tasks Qingyun Li, et al. by interacting with chatgpt beyond language. arXiv preprint arXiv:2305.05662, 2023. 3 [31] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Follow your Siran Chen, Xiu Li, and Qifeng Chen. pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 41174125, 2024. 2 [32] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 2 [33] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. 2 [34] Ming Meng, Qi Dong, Jiajie Li, Zhe Zhu, Xingyu Wang, Zhaoxin Fan, Wei Zhao, and Wenjun Wu. Hf-vton: Highfidelity virtual try-on via consistent geometric and semantic alignment. arXiv preprint arXiv:2505.19638, 2025. 1 [35] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. 3, 4, 5, 7, 13 [36] Haifeng Ni, Ming Xu, and Faming Fang. Itvton: Virtual tryon diffusion transformer based on integrated image and text. arXiv preprint arXiv:2501.16757, 2025. [37] Ziheng Ouyang, Zhen Li, and Qibin Hou. K-lora: Unlocking training-free fusion of any subject and style loras. arXiv preprint arXiv:2502.18461, 2025. 1 [38] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. arXiv preprint arXiv:2406.16855, 2024. 7, 8 [39] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. 3 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6, 8 [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollár, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 12 [42] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2 [44] Dong She, Siming Fu, Mushui Liu, Qiaoqiao Jin, Hualiang Wang, Mu Liu, and Jidong Jiang. Mosaic: Multi-subject personalized generation via correspondence-aware alignment and disentanglement. arXiv preprint arXiv:2509.01977, 2025. 1, 3, 7, 13 [45] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. 3 [46] Yiren Song, Danze Chen, and Mike Zheng Shou. Layertracer: Cognitive-aligned layered svg synthesis via diffusion transformer. arXiv preprint arXiv:2502.01105, 2025. 2 [47] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanything: Harnessing diffusion transformers for multi-domain procedural sequence generation. arXiv preprint arXiv:2502.01572, 2025. 2 [48] Yiren Song, Cheng Liu, and Mike Zheng Shou. Omniconsistency: Learning style-agnostic consistency from paired stylization data. arXiv preprint arXiv:2505.18445, 2025. [49] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1188811898, 2023. 3 [50] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. 1, 13, 16 [51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 [52] Cong Wan, Xiangyang Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, and Yihong Gong. Grid: Visual layout generation. arXiv preprint arXiv:2412.10718, 2024. 2 [53] Haoxuan Wang, Jinlong Peng, Qingdong He, Hao Yang, Ying Jin, Jiafu Wu, Xiaobin Hu, Yanjie Pan, Zhenye Gan, Mingmin Chi, et al. Unicombine: Unified multi-conditional combination with diffusion transformer. arXiv preprint arXiv:2503.09277, 2025. 2 [54] Jiangshan Wang, Yue Ma, Jiayi Guo, Yicheng Xiao, Gao Huang, and Xiu Li. Cove: Unleashing the diffusion feature correspondence for consistent video editing. arXiv preprint arXiv:2406.08850, 2024. 1 [55] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and imHao Jiang. Ms-diffusion: Multi-subject zero-shot age personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. [56] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems, 37:128374128395, 2024. 3 [57] Zitong Wang, Hang Zhao, Qianyu Zhou, Xuequan Lu, Xiangtai Li, and Yiren Song. Diffdecompose: Layer-wise decomposi10 net: Captioning with adaptive attention on visual and nonvisual words. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1546515474, 2021. 1 [71] Yuyao Zhang, Jinghao Li, and Yu-Wing Tai. Layercraft: Enhancing text-to-image generation with cot reasoning and layered object integration. arXiv preprint arXiv:2504.00010, 2025. 3 [72] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject repreIn Proceedings of sentation for subject-driven generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. [73] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. 2 [74] Dewei Zhou, Mingwei Li, Zongxin Yang, and Yi Yang. Dreamrenderer: Taming multi-instance attribute control in large-scale text-to-image models. arXiv preprint arXiv:2503.12885, 2025. 1 [75] Shihao Zhu, Bohan Cao, Ziheng Ouyang, Zhen Li, PengTao Jiang, and Qibin Hou. Agebooth: Controllable facial aging and rejuvenation via diffusion models. arXiv preprint arXiv:2510.05715, 2025. 1 tion of alpha-composited images via diffusion transformers. arXiv preprint arXiv:2505.21541, 2025. 2 [58] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 1, 2, 7, 13 [59] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 1, 2, 7, [60] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 1, 2, 7, 13, 16 [61] Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, and Jiaya Jia. Dreamomni: Unified image generation and editing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2853328543, 2025. 1, 2 [62] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025. 1, 2 [63] Ming Xie, Chenjie Cao, Yunuo Cai, Xiangyang Xue, Yu-Gang Jiang, and Yanwei Fu. Anyrefill: unified, data-efficient framework for left-prompt-guided vision tasks. arXiv preprint arXiv:2502.11158, 2025. 1 [64] Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, and Linfeng Zhang. Eedit: Rethinking the spatial and temporal redundancy for efficient image editing. arXiv preprint arXiv:2503.10270, 2025. 1 [65] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023. [66] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2 [67] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, PengTao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. In NeurIPS, 2025. 3 [68] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection, 2022. 8 [69] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. 2 [70] Xuying Zhang, Xiaoshuai Sun, Yunpeng Luo, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Feiyue Huang, and Rongrong Ji. Rst-"
        },
        {
            "title": "Supplementary Material",
            "content": "The supplementary material is structured as follows: 1. We provide additional details on the construction of our dataset in Sec. A. 2. We compared additional existing editing methods on the restoration task in Sec. B. 3. We provide representative cases from our dataset and compare them with those from other existing datasets in Sec. C. 4. We present the correction results of our proposed ImageCritic across multiple languages on images generated by different models in Sec. D. 5. We present an example of using the agent chain and the prompts used by each of our agent components in Sec. E."
        },
        {
            "title": "A Data Curation details",
            "content": "As illustrated in Fig. 8, our data construction pipeline proceeds sequentially as follows. Step 1: Reference-target pairs generation We begin by collecting large-scale product dataset through web crawling and downloading and employ state-of-the-art generative models to produce synthetic image variants for each product sample. Step 2: Generation quality filtering We then used Qwen3-vl [2] to perform quality filtering with prompt: Determine whether the text details in the input image are both strictly clearly visible and fully readable. If any part of the text is blurred, low-resolution, or difficult to recognize, answer No. Otherwise, answer Yes. Respond with only Yes or No, followed by brief reason.. Step 3: Semantic tags generation Next, we employed Qwen [2] to assign semantic tags to the generated images in order to extract object categories for training purposes with the prompt Given the image of an object, return only the most general category of the object using exactly 1 to 3 words. Strictly avoid any additional details or descriptions.\". Step 4: Image Grounding To ensure that the model correctly interprets semantics in complex scenes, we utilized the Qwen [2] for grounding. The prompting strategy was as follows: Given image1 as reference, detect the same object in image2 and output the bounding box coordinates strictly in the format [x1, y1, x2, y2], where x1,y1 are the top-left integers and x2,y2 are the bottom-right integers. You must return only the coordinates in that exact format with no extra text. where image1 and image2 referred to the input image and the generated image, respectively. Step 5: Consistency Verification Based on the bounding boxes predicted by Qwen [2], we adopted SAM [41] to detect and extract precise object masks. To further guarantee the accuracy of these masks and enhance generative consistency, we invoked the Qwen again to compare the full-object masks against the product references, using the following prompt: Please analyze if the extracted region in Image 1 corresponds to the product in Image 2. Ignore the background and focus only on the main object. 1. Are the objects in both images the same product? If Image 2 contains only small portion of local region from Image 1, consider it as No. (Yes/No). 2. Explain based on visual features such as shape, color, texture, and context. If the object in Image 2 contains the mask region, describe the match. where image1 and image2 referred to the input image and the generated image, respectively. Step 6: Image degradation From each verified full-object mask, we sampled 20%70% of the region to obtain random local rectangular mask used as the inpainting region for the Flux-Fill [24] model. For FluxFill, we designed three types of promptsEnglish words, Chinese characters, logosas well as an empty-prompt setting to accommodate different scenarios. Step 7: Degradation quality filtering After generation, we again used Qwen [2] to filter out severely degraded results produced by Flux-Fill, with the following prompt: Is there is obvious distorted text or mismatched elements in the image. Answer with Yes or No, followed by brief explanation of the reason. Step 8: Final Image Composition Let denote binary full-object mask generated by SAM [41], the degraded image, and the generated image. We construct the final degraded sample by combining the degraded content inside the object region with the generated content outside it: Ifinal = + (1 ) G. (9) This procedure effectively suppresses artifacts that may arise in background areas during the filling process, ensuring that degradation occurs strictly within the object region without affecting the original background environment. 12 Figure 8 Data curation details. We present comprehensive account of our dataset construction methodology, encompassing data generation, data annotation, and data augmentation procedures. Step 9: Iterative Data Enhancement Strategy After following the training procedure described above, our model is able to perform local region restoration. To further improve the consistency and quality of the generated outputs, we apply additional enhancement to the generated image with Chinese characters. Specifically, since we obtained grounding masks for objects during data construction, we crop the corresponding regions and feed themtogether with the original input imageinto our method for consistency correction. The corrected images are then used as new targets to update the dataset, thereby further improving overall data quality."
        },
        {
            "title": "B Image Correction Comparison",
            "content": "We further compared additional multimodal large language models, such as Nano-banana [10] and GPT-Image 1 [1], as well as editing models like Omnigen2 [59] and Qwenimage [58], evaluating their capabilities on the task of consistency correction in generated images. To help the models better understand the task, we designed the following prompt: Use the product in the left first reference image as reference to refine, replace, and enhance the product in the right second to-be-refined image, matching their texture, details, color, logo, and texts, while preserving everything else in the right second to-be-refined image untouched. As shown in the Fig. 9, existing methods fail to perform consistent correction on generated images while maintaining background fidelity. In contrast, our method substantially improves image consistency while preserving the original background."
        },
        {
            "title": "C Dataset Comparison",
            "content": "As shown in the figure, we present several pairs from our dataset and compare our referencetarget pairs with those from other datasets. From the visualizations, it is evident that our paired data maintain high global consistency and rich local details, even under diverse languages, scenes, generation sources, and viewpoints. In contrast, datasets such as Subjects-200k [50] and UNO-1M [60] do not strictly preserve local consistency and often exhibit region-level blurring or mismatches."
        },
        {
            "title": "D Additional Visual Results",
            "content": "To further demonstrate the robustness of our method across different languages and scenarios, we first used GPT-Image 1 to generate images with diverse categories in multiple languages, and then performed customized generation using the open-source models XVerse [3], Dreamo [35], MOSAIC [44], OmniGen2 [59], UNO [60], and Qwen-Image [58], as well as the closed-source models NanoBanana [10] and GPTImage [1]. We then applied our method for consistency correction and used an OCR model [22] to detect the text in the generated images. As shown in Figures Fig. 11, Fig. 12, and Fig. 13, our method delivers effective consistency restoration, and the OCR results further underscore its robustness across diverse linguistic and contextual settings."
        },
        {
            "title": "E Agent Chain Details",
            "content": "As shown in Figure 14, the system progressively corrects the generated image through the coordinated operation of multiple specialized agents. After the user provides reference image and generated image, the inconsistency detector, reference finder, and ImageCritic agents sequentially perform comparison, cropping, and correction until the user is satisfied with the result. During this process, the user may choose whether to accept each proposed patch or provide new target region or description to guide further correction by the 13 agent chain. This iterative loop continues until the generated image aligns with the users intent. The prompts for the Inconsistency Detector, Reference Finder, TagGrounder, and Coordinator are configured in List. 1, List. 2, List. 3, and List. 4, respectively. 14 Figure 9 Comparisons with multimodal and editing model. We evaluated existing multimodal and editing methods. As shown, current models exhibit issues in preserving global coherence or performing localized corrections, which affects their practical applicability. In contrast, our approach maintains local consistency in the generated content while ensuring overall background coherence. Figure 10 Comparion with dataset. Comparison of data samples from our Critic-10k dataset, the Subjects-200K [50], and the UNO-1M [60] dataset. 16 Figure 11 Additional visual result. We present the visual results of our proposed ImageCritic under multilingual, multi-view, and multi-scene settings. By applying an OCR [22] model to the generated images, we observe that after correction with our method, all recognized text perfectly matches the reference images. This demonstrates that our approach achieves precise and comprehensive detail correction without disrupting the overall structural or contextual integrity of the images. 17 Figure 12 Additional visual result. We present the visual results of our proposed ImageCritic under multilingual, multi-view, and multi-scene settings. By applying an OCR [22] model to the generated images, we observe that after correction with our method, all recognized text perfectly matches the reference images. This demonstrates that our approach achieves precise and comprehensive detail correction without disrupting the overall structural or contextual integrity of the images. Figure 13 Additional visual result. We present the visual results of our proposed ImageCritic under multilingual, multi-view, and multi-scene settings. By applying an OCR [22] model to the generated images, we observe that after correction with our method, all recognized text perfectly matches the reference images. This demonstrates that our approach achieves precise and comprehensive detail correction without disrupting the overall structural or contextual integrity of the images. 19 Figure 14 Illustration of multi-agent image correcting workflow. The system performs localized detection, reference matching, and iterative region-level corrections driven by user feedback, progressively correcting the generated image until it is accepted. 20 prompt = ( Listing 1 The prompt for Inconsistency Detector. \"Carefully compare the two images. Image 1 is the reference image (correct version), and Image 2 is the target image that may contain defects. Focus only on the main subject of the image, ignoring any differences in the background. Identify the region in Image 2 that differs from the corresponding area in Image 1. Differences may include blur, illegible text, texture inconsistency, artifacts, missing parts, or any other visual discrepancies.Return ONLY the bounding box of the different region in Image 2 in the strict format:[xmin, ymin, xmax, ymax]\" ) messages = [ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Reference Image1 (correct version)\"}, {\"type\": \"image\", \"image 1\": image_A_path}, {\"type\": \"text\", \"text\": \"Target Image2 (may have defects)\"}, {\"type\": \"image\", \"image 2\": image_B_path}, {\"type\": \"text\", \"text\": prompt}, ], } ] prompt = ( Listing 2 The prompt for Reference Finder. \"I will show you problematic region from image1 and reference image2. \" \"Please find the corresponding region in image2 that matches the same area as the \" \"problematic region from image1. Return only the bounding box coordinates in the \" \"format [xmin, xmax, ymin, ymax], No additional text, just the coordinates inside []\" ) messages = [ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"image1 (problem region)\"}, {\"type\": \"image\", \"image\": problem_crop_path}, {\"type\": \"text\", \"text\": \"Reference image2\"}, {\"type\": \"image\", \"image\": image_A_path}, {\"type\": \"text\", \"text\": prompt}, ], } ] 21 Listing 3 The prompt for TagGrounder. prompt = ( f\"Find the region in this image that best matches the product tag: \"{tag}\". \" \"Return ONLY the bounding box in Image in the strict format: \" \"[xmin, ymin, xmax, ymax]. No extra text.\" ) messages = [ { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": f\"Image to search product tag: {tag}\"}, {\"type\": \"image\", \"image\": image_path}, {\"type\": \"text\", \"text\": prompt}, ], } ] 22 \"\"\" You are the Coordinator Agent for an image restoration workflow. Listing 4 The prompt for Coordinator. The workflow has three sequential steps: 1. Inconsistency Detector: compare images and detect the difference region - Input: image_A (reference image path), image_B (target image path) - Output: bbox_B (difference region), prompt (problem description) 2. Reference Finder: find clean reference region - Input: image_A (reference), image_B (target), bbox_B (problem region) - Output: bbox_A (reference region), cropped reference region, object_tag 3. ImageCritic: perform correction - Input: prompt, image_A, image_B, bbox_A, bbox_B, object_tag - Output: image_path (final corrected image), patch info Additionally, there is an auxiliary specialist: - TagGrounder: given an image and user-provided product tag, locate bbox in that image. Workflow rules: - Execute strictly in the order 1 - 2 - 3. - After each step, you MUST ask the user: \"Accept [STEP_NAME] result? (yes/no):\" - If the user answers \"yes\" or \"y\", continue to the next step. - If the user answers \"no\": - You must wait for the user to provide either: (a) new bbox in the format [xmin, ymin, xmax, ymax], or (b) product tag (e.g. shoe, bag, logo) to re-locate the region. - If the user gives bbox, this bbox has the highest priority and must override previous bbox_B or bbox_A. - If the user gives product tag, you MUST call the TagGrounder specialist via the delegate_to_specialist tool to convert this tag into bbox, then use this bbox in the current or next step. - Always delegate concrete image-processing work to the appropriate specialist via the delegate_to_specialist tool. - Maintain data flow between steps (outputs from previous step feed into the next). - When all three steps are done and the final restored image is produced, output: \"Image restoration workflow completed!\" and then stop. Current task: The user provides image_A and image_B paths. Please run the three-step correction workflow. \"\"\""
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore",
        "State Key Laboratory of CAD&CG, Zhejiang University",
        "VCIP, Nankai University"
    ]
}