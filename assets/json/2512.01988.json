{
    "paper_title": "Artemis: Structured Visual Reasoning for Perception Policy Learning",
    "authors": [
        "Wei Tang",
        "Yanpeng Sun",
        "Shan Zhang",
        "Xiaofan Li",
        "Piotr Koniusz",
        "Wei Li",
        "Na Zhao",
        "Zechao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in a spatial and object-centric space. In response, we introduce Artemis, a perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as a (label, bounding-box) pair capturing a verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides a principled route toward scalable and general perception policies."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 8 8 9 1 0 . 2 1 5 2 : r Artemis: Structured Visual Reasoning for Perception"
        },
        {
            "title": "Policy Learning",
            "content": "Wei Tang1 Yanpeng Sun2 Shan Zhang3,5 Xiaofan Li4 Piotr Koniusz5 Wei Li6 Na Zhao2 Zechao Li1 1NJUST IMAG 2 SUTD IMPL 3Adelaide AIML 4 Baidu Inc. 5Data61 CSIRO 6 SenseTime"
        },
        {
            "title": "Abstract",
            "content": "Recent reinforcement-learning frameworks for visual perception policy have begun to incorporate intermediate reasoning chains expressed in natural language. Empirical observations indicate that such purely linguistic intermediate reasoning often reduces performance on perception tasks. We argue that the core issue lies not in reasoning per se but in the form of reasoning: while these chains perform semantic reasoning in an unstructured linguistic space, visual perception requires reasoning in spatial and object-centric space. In response, we introduce Artemis, perception-policy learning framework that performs structured proposal-based reasoning, where each intermediate step is represented as (label, bounding-box) pair capturing verifiable visual state. This design enables explicit tracking of intermediate states, direct supervision for proposal quality, and avoids ambiguity introduced by language-based reasoning. Artemis is built on Qwen2.5-VL-3B, achieves strong performance on grounding and detection task and exhibits substantial generalization to counting and geometric-perception tasks. The consistent improvements across these diverse settings confirm that aligning reasoning with spatial representations enhances perception-policy learning. Owing to its strengthened visual reasoning, Artemis also achieves competitive performance on general MLLM benchmarks, illustrating that spatially grounded reasoning provides principled route toward scalable and general perception policies. Date: December 2, 2025 Connection Email: yanpeng_sun@sutd.edu.sg, shan.zhang@adelaide.edu.au, weitang@njust.edu.cn Project Page: https://vi-ocean.github.io/projects/artemis/"
        },
        {
            "title": "1 Introduction",
            "content": "The eye sees only what the mind is prepared to comprehend. Henri Bergson Large language models (LLMs) [14] have recently entered new stage of reasoning development. Rule-based reinforcement learning (RL) [3, 57] has been shown to elicit multi-step reasoning in large language models through explicit, verifiable rewards. Models such as DeepSeek-R1 [8] and OpenAI-o1 [5] demonstrate that RL can enhance reasoning ability beyond supervised learning. Most of these advances are achieved within the language modality [2, 4, 6, 7]. Researchers collaborating on the ViOcean initiative. Corresponding Author. Project Leader. Encouraged by these advances, recent studies have begun to apply rule-based RL in multimodal large language models (MLLMs) [912] during post-training to improve visual perception. Early works [11, 13, 14] followed the same design as reasoning-oriented LLMs, inducing models to explicitly think through language during training. Although this strategy brings moderate gains on reasoning-like tasks, it performs poorly on most perception benchmarks [1517]. As shown in Fig 1 (b), the perception-policy model generates verbose textual thoughts unrelated to the query, leading to incorrect spatial grounding. The underlying cause is that the thinking process remains confined to linguistic search space, disconnected from spatial and object-centric representations. Figure 1 Motivation of Artemis. Comparison between current perception-policy models and human perception. (a) Query: find the shortest player. (b) Perceptionpolicy models depend on ungrounded language reasoning, leading to wrong localization. (c) Humans perform structured visual reasoning, progressively refining attention to identify the correct player. To address this limitation, recent studies explore no-think [18, 19] and adaptive-think [20, 21] strategies that reduce or skip explicit reasoning during reinforcement learning. Both aim to mitigate the drawbacks of language-centric thinking and have shown improvements on several perception-related benchmarks. Although effective within specific tasks, current implementations remain limited in generalization, as most are designed for isolated perception settings [13, 15, 16, 18, 19, 22]. However, this task-bound design contrasts with the goal of general perception, where single policy should generalize from grounding to counting and from natural images to diagrams. These limitations reveal deeper issue in perception-policy learning. What factors fundamentally degrade this process? Instead of removing thinking altogether, we must ask: what form of thinking truly benefits perception? Inspired by findings in cognitive science [23, 24], we rethink how humans perceive complex scenes. As shown in Fig. 1 (c), humans perceive complex scenes through global-to-local visual reasoning process that scans the scene, locates relevant regions, and refines attention to targets before answering. Unlike linguistic reasoning that operates in the semantic space, this process grounds reasoning in spatial and object-centric representations. we refer to this process as structured visual reasoning. Toward this end, we propose Artemis, framework that applies rule-based RL in MLLM post-training for structured visual reasoning in perceptionpolicy learning. Specifically, we implement structured visual reasoning by representing thinking step as set of (labelbounding-box) pairs that explicitly capture visual evidence. This explicit formulation grounds the thinking process in spatial space, enabling direct supervision on both intermediate reasoning and final answer. To effectively learn structured visual reasoning, Artemis builds upon the strong semantic understanding already encoded in the base MLLM and focuses on teaching the model how to see and ground what it can already comprehend. Rather than optimizing linguistic reasoning, the policy learns to translate semantic concepts into object-centric visual evidence. To ground this reasoning in spatial representations, the model must learn from tasks that explicitly couple language with perception. We therefore adopt visual grounding, which requires both linguistic understanding and precise spatial localization, and has been widely recognized as one of the most comprehensive tasks for evaluating multimodal perception and reasoning. We further incorporate object detection to provide dense scene-level supervision that strengthens the models perceptual foundation. Both tasks are optimized under unified rule-based reinforcement framework built upon group relative policy optimization (GRPO) [8]. We redesign the rewards to evaluate not only final outcomes but also intermediate visual reasoning steps. We validate Artemis through extensive experiments across 7 perception and 10 general multimodal benchmarks. Experimental results show that Artemis generalizes robustly across perception tasks and visual domains, adapting from grounding to counting and from natural images to mathematical diagrams. Remarkably, it also improves performance on general MLLM benchmarks, indicating that stronger perception enhances overall multimodal capability. Unlike prior approaches that require dedicated model for each perception task, single Artemis policy performs consistently across all settings. These findings highlight Figure 2 Overview of the Artemis framework for RL-based perception-policy learning. Rollouts generated by MLLM are encouraged to perceive structured visual evidence before decision-making, guided by the structured visual reasoning reward, while the outcome rewards supervise the format and answer generation. GRPO is employed to optimize the unified perception-policy learning framework. paradigm shift: perception policies benefit most not from eliminating thinking, but from structuring it spatially."
        },
        {
            "title": "2 Related Works",
            "content": "RL-based Post-training for Reasoning. Reinforcement learning (RL) becomes key post-training strategy for enhancing reasoning ability in LLMs [3, 57]. Reinforcement learning from human feedback (RLHF) uses human preference data to optimize the model policy [25], while direct preference optimization (DPO) [1, 2, 26] and proximal policy optimization (PPO) [27, 28] update model policies based on preference or reward models. More recently, reinforcement learning with verifiable rewards (RLVR) attracts significant attention, with group relative policy optimization (GRPO) [8, 29] as representative method that uses relative rewards among sampled reasoning trajectories without an explicit reward model to directly and verifiably supervise reasoning. These advances inspire similar efforts in MLLMs, such as the Qwen series [2, 10] and DeepSeek-VL2 [11]. However, these approaches mainly target task-specific visual reasoning, e.g., visual math reasoning [13, 30] or document understanding [31, 32], with limited focus on perception-driven reasoning [18]. Visual Perception in Multimodal Models. Visual perception has long been studied in computer vision [3337], where models learn to recognize objects and reason about their spatial layout and relations [3841]. Recently, MLLMs achieve notable progress in visual perception [10, 28, 42, 43]. To further improve perception, recent works apply RL for perception-policy learning, transferring reasoning-oriented strategies from LLMs and encouraging models to think explicitly in natural language. Early perception-policy works [13, 15, 17, 22, 44, 45] follow this approach, improving both reasoning and perception abilities in task-specific settings, but showing limited gains on general perception tasks due to their linguistic search space. More recent methods [1821] reduce or skip explicit linguistic reasoning during RL to mitigate language-centric limitations, yet improvements remain limited and largely task-specific. From the perspective of cognitive science on how humans perceive and reason about complex scenes [23, 24], we rethink perception-policy learning with RL to learn unified perception-policy that generalizes across visual perception tasks."
        },
        {
            "title": "3 Artemis Framework",
            "content": "In this section, we introduce Artemis, unified framework for perception-policy learning. We begin with brief overview of the GRPO algorithm in Section 3.1. We then rethink existing perception-policy rewards in Section 3.2, highlighting their limitations, and finally present our reward design in Section 3.3."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Reinforcement learning with RLVR has recently gained attention as post-training method for LLMs. Unlike PPO, which relies on reward model, GRPO eliminates the need for separate evaluation model and uses rule-based rewards to directly and verifiably guide policy optimization. Let denote the models output. For each input, the model samples group of outputs {oi}G , where oi O. Each output oi receives scalar reward ri. In GRPO, the reward for each output oi is compared against the mean reward of all outputs in the group. The advantage Ai is calculated as: i=1 4 Ai = ri mean(r1, . . . , rG) std(r1, . . . , rG) . The objective function for GRPO is then: JGRPO(θ) = (cid:104) min θ ˆAi clip( ˆAi, 1 ϵ, 1 + ϵ) β KL[πθπref] (cid:105) (1) (2) Here, ˆAi denotes the normalized advantage, ϵ controls clipping to stabilize updates, β weights the KL regularization, and πref is reference policy. GRPO leverages rule-based rewards that are directly verifiable, making it well-suited for visual perception tasks with explicit evaluation rules (e.g., grounding, detection). This allows effective post-training with minimal data, outperforming standard supervised fine-tuning (SFT) in both efficiency and generalization [15, 18, 20]."
        },
        {
            "title": "3.2 Rethinking Rewards for Perception Policies",
            "content": "Rewards are the foundation of perception-policy reinforcement learning [15, 16, 18, 19]. They define the learning objective and influence how the model learns to reason. In this context, the reward is not performance metric but learning signal that guides how the policy improves. In most current perception-policy reinforcement learning setups, the reward targets only the final outcome [1820]. Such outcome-only reward design provides sparse and delayed feedback, giving no guidance on whether the model gathers or verifies the right visual evidence. Consequently, models tend to produce superficially plausible linguistic rationales instead of engaging in spatially grounded reasoning [15, 16]. This reveals deeper issue: the misalignment between the reward objective and the perceptual process limits generalization. When the task demands spatial reasoning but the reward evaluates only semantic correctness, the learning signal encourages the wrong behavior. Evidence from process supervision studies in language models (e.g., OpenAI PRM [46], DeepSeek-R1 [8]) supports this view: supervising intermediate steps rather than final answers yields more robust reasoning behaviors. Conversely, forcing language-based thinking in perception tasks without grounding often degrades performance because the reward reinforces linguistic fluency rather than perceptual correctness. These findings indicate that the design of reward functions directly shapes the learning trajectories of perception policies. As most base models are pretrained in semantic reasoning spaces [4, 6, 7, 10, 11, 28], outcome-only rewards leave exploration unconstrained and tend to reinforce semantic priors, producing reasoning that appears coherent in language but remains visually ungrounded. In contrast, object-centric intermediate rewards balance exploration and exploitation, stabilize learning, and encourage perceptual reasoning that generalizes across tasks."
        },
        {
            "title": "3.3 Artemis Reward Design",
            "content": "Based on the previous analysis, improving perception requires reward that guides both the reasoning process and the final outcome. We therefore develop unified reward framework that evaluates not only final predictions but also intermediate object-centric reasoning outputs  (Fig. 2)  . Each rollout contains structured (label, bounding-box) pairs representing visual evidence supervised by the Structured Visual Reasoning Reward, while other components such as the final answer or output format are optimized with the outcome rewards. 5 Structured Visual Reasoning Reward. The Structured Visual Reasoning reward provides direct supervision to the models reasoning process. It encourages explicit reasoning in the spatial domain by requiring the model to identify relevant objects and regions before producing the final answer. This object-centric formulation ensures that each reasoning step corresponds to verifiable entities in the scene rather than ambiguous linguistic descriptions. Formally, the model is prompted to output structured visual evidence within the <think> block: <think>...</think> (cid:104) { ˆC, ˆB} (cid:105) . Here, ( ˆC, ˆB) denote the predicted label and bounding boxes in the <think> </think> output. Correspondingly, the ground-truth annotations adopt the same structured format, where each reasoning object is represented as (label, bbox) pair. For instance, the ground-truth annotations in Fig. 2, where each purple bounding box is associated with its corresponding label: \" eas n _o jec ts \" : { { \" label \" : \" tiger \" , \" bbox \" : [450 , 467 , 1173 , 791]} , { \" label \" : \" elephant \" , \" bbox \" : [745 , 236 , 1128 , 710]} , { \" label \" : \" zebra \" , \" bbox \" : [265 , 314 , 493 , 768]} , { \" label \" : \" giraffe \" , \" bbox \" : [45 , 33 , 360 , 782]} } To encourage the model to generate informative structured evidence within think block, we introduce the concept of key object, which corresponds to the target object in the answer (i.e. the tiger in Fig. 2). Predicted boxes that match this key object are assigned the highest reward, as correctly identifying it provides direct support for producing correct final answer. At the same time, other predicted objects (i.e., the elephant, the zebra, and the giraffe in Fig. 2) are also assigned positive, smaller weights, ensuring that the model perceives contextual information that may be spatially or semantically related to the key object. In practice, objects whose positions and categories are similar to the key object are more likely to be relevant to the answer, so including these additional evidence objects encourage the model to produce richer, context-aware reasoning without overwhelming the supervision signal. Formally, let η (0, 1) control the importance of the key object. The weight assigned to each ground-truth box is defined as η, 1 η 1 where is the total number of annotated objects in the scene. Empirically, we set η = 0.8. otherwise, if Bj is the key object, w(Bj) = , (3) Since the structured visual reasoning output contains multiple predicted objects, we need to establish one-toone correspondences with the ground-truth boxes to compute the reward accurately. Inspired by previous work involving multi-object matching [18, 19], this is performed using the Hungarian algorithm [47], aligning each predicted box ˆB with ground-truth box and each predicted label ˆC with the corresponding ground-truth label C. The structured visual reasoning reward is then defined as weighted sum over the matched pairs: rrsn = (cid:88) w(B) 1 (cid:104) IoU( ˆB, B) τIoU (cid:105) (cid:104) sim( ˆC, C) τsim (cid:105) , (4) where, empirically, we set τIoU = 0.8 and compute sim() using Rouge-1 with τsim = 0.9. Outcome Rewards. To guide the model toward correct and structured outputs, we define outcome rewards composed of two components: the format reward rformat and the answer reward rans. The format reward rformat enforces structural validity of the outputs. It checks whether the model produces complete <think></think> block for intermediate reasoning (when applicable) and valid <answer></answer> block for the final prediction. This reward is task-agnostic and applies to any setting that adopts structured visual reasoning. The answer reward rans measures the perceptual accuracy of the models final prediction. We use object detection to provide dense spatial supervision over the scene, while visual grounding ensures that intermediate reasoning aligns with the target object. The reward jointly evaluates localization accuracy using GIoU and Table 1 Visual grounding accuracy on the RefCOCO/+/g datasets, including three IoU threshold of accuracy metrics (e.g., @50 indicates threshold of 0.5). Models marked with denote results from our own inference, and those highlighted in gray indicate expert models. 6 val@50 testA@50 testB@50 val@75 testA@ testB@75 val@95 testA@95 testB@95 valAvg testAAvg testBAvg RefCOCO Method Size Expert Models MDETR [39] OFA [48] General MLLMs LLaVA-v1.5 [43] LLaVA-OV [42] Qwen2-VL [2] Qwen2.5-VL [10] DeepSeek-VL2-Tiny [11] RL-based MLLMs Perception-R1 [18] Vision-R1 [19] VLM-R1 [15] Artemis - - 7B 7B 2B 3B 3B 2B 7B 3B 3B 87.5 88.4 49.1 73.0 86.8 88.6 83.5 89.1 89.6 90.7 91.3 90.4 90. 54.9 82.3 89.6 91.7 86.7 91.4 92.9 92.8 93.4 82.6 83.3 43.3 63.5 82.0 84.0 77.9 84.5 84.9 85.9 87.0 - - 10.7 24.2 77.2 79.1 69.7 79.5 80.0 81.6 83.6 - - 13.6 29.6 80.6 83.5 74.1 83.6 84.7 84.7 86.4 Method Size val@50 testA@50 testB@50 val@75 testA@ Expert Models MDETR [39] OFA [48] General MLLMs LLaVA-v1.5 [43] LLaVA-OV [42] Qwen2-VL [14] Qwen2.5-VL [10] DeepSeek-VL2-Tiny [11] RL-based MLLMs Perception-R1 [18] Vision-R1 [19] VLM-R1 [15] Artemis - - 7B 7B 2B 3B 3B 2B 7B 3B 3B 81.1 81.3 42.4 65.8 77.1 81.9 73. 81.7 83.0 84.2 85.3 85.5 87.1 49.7 79.0 82.5 87.3 81.3 86.8 89.0 89.3 89.9 72.9 74.2 36.4 57.2 70.1 74.7 63. 74.3 75.3 76.6 77.8 - - 9.8 23.6 68.7 73.2 61.9 73.6 74.7 76.1 78.3 - - 12.4 28.8 73.8 79.3 70. 79.3 81.7 81.2 82.9 Method Size val@50 test@50 val@ test@75 - - 6.9 15.9 70.1 71.2 60.0 - - 0.4 0.5 33.0 34.6 24.6 35.0 72.4 33.6 72.6 35.6 73.5 76.5 40.1 RefCOCO+ val@ testB@75 - - 6.4 15.3 60.0 63.9 49.4 - - 0.5 0.6 29.4 32.3 22.1 64.2 32.6 64.1 31.5 65.7 33.4 38.3 68.7 RefCOCOg val@ - - 0.3 0.5 35.7 37.9 29.2 38.5 36.8 37.9 42.8 - - 0.3 0.5 26.9 27.8 19.3 28.8 28.6 27.7 33. - - 20.1 32.6 65.7 67.4 59.3 67.9 67.7 69.3 71.7 - - 22.9 37.5 68.6 71.0 63.3 71.2 71.5 71.8 74. - - 16.8 26.6 59.7 61.0 52.4 61.9 62.0 62.4 65.6 testA@95 testB@95 valAvg testAAvg testBAvg - - 0.5 0.6 32.3 35.8 27.3 36.9 35.2 36.4 41.7 - - 0.2 0.4 23.0 25.4 16.1 26.7 25.6 25.9 30.0 - - 17.6 30.0 58.4 62.5 52.4 62.6 63.1 64.6 67.3 - - 20.8 36.1 62.9 67.5 59.6 67.7 68.6 69.0 71.5 - - 14.3 24.3 51.0 54.7 43.0 55.1 55.0 56.1 58.7 test@ valAvg testAvg Expert Models MDETR [39] OFA [48] General MLLMs LLaVA-v1.5 [43] LLaVA-OV [42] Qwen2-VL [14] Qwen2.5-VL [10] DeepSeek-VL2-Tiny [11] RL-based MLLMs Perception-R1 [18] Vision-R1 [19] VLM-R1 [15] Artemis - - 7B 7B 2B 3B 3B 2B 7B 3B 3B 83.3 82.2 43.2 70.8 83.3 85.1 75.7 85.7 86.4 86.0 87.3 83.3 82.3 45.1 70.8 83.1 85.7 79.2 85.4 86.9 86.7 87. - - 8.5 23.3 72.7 74.4 60.4 75.7 76.4 75.1 77.7 - - 9.3 23.6 73.0 75.8 63.1 76.0 77.8 76.8 79. - - 0.3 0.6 28.9 32.1 19.1 32.1 32.4 32.7 36.3 - - 0.3 0.7 27.9 33.1 21.0 33.1 33.1 32.9 37. - - 17.3 31.6 61.6 63.9 38.8 64.5 65.1 64.6 67.1 - - 18.2 31.7 61.3 64.9 54.4 64.8 65.9 65.5 68. label consistency to ensure correct object identification. Implementation details for both components are provided in the Appendix A. Overall Reward. The total reward of Artemis for GRPO post-training is the sum of the three components: Each component contributes equally, encouraging correctly formatted outputs, accurate final predictions, and high-quality intermediate object-centric reasoning evidence. rtotal = rformat + rans + rrsn. (5)"
        },
        {
            "title": "4 Experiments",
            "content": "We first introduce the settings of Artemis in Section 4.1. To assess its improvement of perceptual capabilities, we validate Artemis through extensive experiments including 7 perception benchmarks and 10 general Table 2 Object detection evaluation on COCO2017 val. Models marked with denote results from our own inference. Gray rows indicate expert models (from MMDetection [49]). Table 3 Zero-shot visual counting on Pixmo-Count and reasoning grounding on LISA test. Models marked with denote results from our own inference. Models marked with follow detect-then-count paradigm, deriving counts from predicted boxes. 7 Method Size Epoch Object detection COCO2017 Val mAP AP50 AP75 AR100 Method Size Expert Models YOLOv3 [50] Faster-RCNN [51] General MLLMs Qwen2.5-VL [10] Griffon [52] RL-based MLLMs VLM-R1 [15] Vision-R1 [19] 3B 13B 3B 7B Perception-R1 [18] 3B Artemis 3B - - 273 27.9 49.2 28.3 35.6 55.7 37.9 1 1 1 1 1 15.4 22.5 15.9 24.8 40.6 25.1 21.6 35.6 21.7 26.6 40.0 27.8 31.9 46.7 33. 31.0 48.0 31.9 - - 29.8 - 33. - 41.2 46.6 General MLLMs LLaVA-v1.5 [43] LLaVA-OV [42] Qwen2-VL [14] Qwen2.5-VL [10] 7B 7B 2B 3B RL-based MLLMs VisionReasoner[16] 7B Perception-R1 [18] UniVG-R1 [17] 7B No-Thinking-RL [20] 2B VLM-R1 [15] 2B 3B Artemis 3B Visual Counting Reasoning Grounding Pixmoval Pixmotest LISAtest 33.3 55.8 60.2 58.0 70.1 78.1 - - - 81.4 31.0 53.7 50.5 57.8 69.5 75.6 - - - 76.9 - - - 67.4 - - 59.7 61.8 63. 78.3 multimodal benchmarks in Section 4.2. We further perform in-depth ablation studies in Section 4.3 to provide deeper understanding of the key components and strategies in our method."
        },
        {
            "title": "4.1 Implementation Details",
            "content": "Datasets-Training. To supervise structured visual reasoning in our unified Artemis framework, we construct the Artemis-RFT dataset from MS-COCO [53], resulting in roughly 77k post-training instances. Grounding and detection data are roughly balanced. For the grounding data, we build upon the answers from RefCOCO/+/g series, and additionally provide ground-truth annotations specifically for computing structured visual reasoning; sample examples are shown in Section 3.3. For the detection data, we adopt the COCO detection annotations with minor modifications. We also use 80k COCO detection instances as cold-start data; these do not overlap with Artemis-RFT. The detailed construction process and statistics of Artemis-RFT are provided in the Appendix B. Datasets-Evaluation. To verify improvements in perception, we test our models on diverse downstream tasks and benchmarks. In-domain benchmarks include RefCOCO/+/g for visual grounding [38, 39] and COCO2017 val for object detection [51, 53]; zero-shot out-of-domain tasks and benchmarks include Lisa-grounding [54], counting (Pixmo-Count [55]), mathematical diagrams perception (MATHGLANCE [56]), and general ability benchmarks of MLLMs from VLMEvalKit [57] such as MMBench [58], MMVet [59], SEEDBench [60], OCRBench [61], etc. Model and Training Settings: We use Qwen2.5-VL-3B-Instruct [10] as our baseline. Training is conducted on 4 NVIDIA A100 80GB GPUs. Cold-start. To help the model adapt to the prompt format at early stages, and improve performance, we first perform 1-epoch cold-start on the 80k COCO detection instances. GRPO post-training. We then train 1 epoch on the Artemis -RFT dataset. Other GRPO optimization settings follow VLM-R1 [15]: rollout = 8, max response length = 2048, temperature = 1.0, KL coefficient β = 0.04, initial learning rate = 1e6, and global batch size = 128. Prompt details are provided in the Appendix C."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "Results of Visual Grounding. Table 1 reports top-1 accuracy under different IoU thresholds on RefCOCO/+/g. Compared with the base Qwen2.5-VL-3B, Artemis consistently improves performance across all splits, highlighting enhanced perception. Compared to RL-based MLLMs, including VLM-R1, which performs reasoning in task-specific models over linguistic reasoning, as well as Perception-R1 and Vision-R1, which are also task-specific but skip reasoning by Table 4 Zero-shot accuracy on MATHGLANCE benchmark. The evaluation covers four types of questions: cls (shape classification), cnt (object counting), grd (object grounding), and rlat (relationship identification). all indicates the overall accuracy for each subject, while Avg. reports the mean all score across all three subjects. Models marked with denote results from our own inference. 8 Method Size Avg. General MLLMs G-LLaVA [62] DeepSeek-VL2-Tiny [11] Qwen2.5-VL [10] LLaVA-v1.5 [43] RL-based MLLMs VLM-R1 [15] No-Thinking-RL [20] Perception-R1 [18] Artemis 7B 3B 3B 7B 3B 2B 2B 3B 30.3 32.6 33.1 33.3 34.4 45.3 45.3 49.3 all 25.6 29.5 31.0 29.2 27.8 45.2 42.5 29.0 26.9 38.0 33.2 44.3 42.5 29.7 39.2 50.6 Plane Geometry grd cnt cls rlat all Solid Geometry grd cnt cls rlat all cls Graphs cnt grd rlat 41.2 34.4 22.0 39.6 27.6 44.8 34.0 30.4 0.4 4.6 20.3 14.2 38.0 32.0 38.0 37.5 32.3 39.0 37.1 31. 45.4 76.7 75.6 43.0 38.1 32.0 33.0 42.3 4.8 0.0 0.0 0.0 32.5 37.5 30.0 31.3 33.9 29.4 31.2 39.0 0.7 44.0 0.4 46.0 38.5 4.3 24.2 52. 50.5 57.7 72.1 96.5 0.0 43.6 56.4 0.0 59.5 96.5 67.0 6.4 56.4 38.8 32.6 56.3 46.3 46.8 52.5 6.4 60.0 52.3 93.0 53. 58.0 39.1 71.0 76.8 73.9 65.2 66.7 65.2 37.0 57.4 29.6 35.2 0.0 0.0 0.0 0.0 42.4 18.2 9.1 39.4 29.6 0.0 12.1 61.1 0.0 69.7 78.8 0.0 55.6 75.9 1.61 81. directly supervising final answers, our structured visual reasoning within unified model yields larger gains across all splits, especially at higher IoU thresholds. For instance, on RefCOCO testB, Artemis improves over VLM-R1 by 1.1 at IoU@50, over Vision-R1 by 3.9 at IoU@75, and over Perception-R1 by 4.6 at IoU@95, demonstrating that Artemis produces highly accurate bounding boxes. Results of Object Detection. We report object detection results on COCO2017 val  (Table 2)  . Object detection remains one of the most demanding tasks in visual perception due to its requirement for accurate localization and comprehensive scene understanding. Compared with the Qwen2.5-VL-3B baseline, Artemis achieves substantial improvement across all metrics (e.g. mAP 31.0 vs. 15.4 and AP50 48.0 vs. 22.5), representing qualitative leap in perception capability. Compared with other RL-based methods, Artemis demonstrates stronger overall scene perception and more complete detection, achieving higher AP50 and AR100 (48.0 and 46.6) than Perception-R1 (46.7 and 41.2). To the best of our knowledge, Artemis is the only RL-based model besides Perception-R1 that surpasses an mAP of 30 within the 3B scale, while showing notable advantages in both AP50 and AR100. Zero-shot Visual Perception on Natural Scenes. We report zero-shot visual counting accuracy on the PixmoCount dataset  (Table 3)  . Even under fully zero-shot conditions, without any counting-specific training, our model achieves strong performance by internally enumerating instances during the <think> phase and directly producing numeric counts (For more details, please refer to the Appendix D.2). This emergent ability offers clear and intuitive improvement in perceptual reasoning. In contrast, Perception-R1, which skips the reasoning process, and VisionReasoner, which adopts language-based reasoning, both follow detect-then-count paradigm with post-hoc aggregation of predicted boxes. Even though these methods are trained on the Pixmo dataset, making our zero-shot setting more challenging, Artemis still outperforms them. We further show reasoning visual grounding accuracy on the LISA test set in Table 3. Our model achieves 78.3 accuracy, substantially outperforming prior works such as UniVG-R1 (59.7), and No-Thinking-RL (61.8). This improvement demonstrates that structured visual reasoning enhances not only spatial understanding but also reasoning-dependent perception in natural scenes. This improvement demonstrates that explicitly associating spatial reasoning with the thinking process enhances grounding accuracy on contextual objects and reduces linguistic hallucinations through verifiable visual supervision, improving overall perceptual capabilities in complex natural scenes. Zero-shot Visual Perception on Math Scene. Table 4 reports the zero-shot results on the MATHGLANCE benchmark, which evaluates visual mathematical perception across plane geometry, solid geometry, and graph-based problems. All models are evaluated using the official MATHGLANCE prompts. Artemis achieves the best overall average score (49.3), substantially outperforming both general MLLMs (e.g., Qwen2.5-VL, LLaVA-v1.5) and recent R1-based models (e.g., Perception-R1, No-Thinking-RL). The improvement reflects an overall enhancement of perceptual capabilities. Importantly, the structured visual reasoning learned from natural images transfers to math-related visual scenes, demonstrating that Artemis effectively enhances Table 5 Zero-shot comprehensive evaluation of visual perception across multiple mainstream multimodal benchmarks. Models marked with denote results from our own inference. MMBench MMVet MMStar Avg. Avg. Avg. ScienceQA SeedBench Avg. Avg. MME Sum Avg. AI2D OCRBench Method Size General MLLMs LLaVA-v1.5 [43] Qwen2-VL [14] DeepSeek-VL2-Tiny [11] Qwen2.5-VL [10] RL-based MLLMs VLM-R1 [15] Perception-R1 [18] Artemis 7B 2B 3B 3B 3B 2B 3B 62.8 71.9 74.6 79.1 70.7 71.8 79.3 32.8 45.6 52.5 60. 58.8 48.9 61.4 32.6 46.3 - 53.8 53.1 45.7 55.9 65.4 74.0 - 79. 69.4 73.4 79.6 60.1 72.7 - 74.0 68.8 73.0 74.3 1338.3 1471.1 1905.5 2200. 2156.2 1903.9 2229.7 51.9 71.6 - 78.3 73.3 71.8 78.2 Avg. - - 805 826 774 - 828 POPE BLINK Avg. Avg. - - - 85.9 79.3 - 88.6 - - - 48.8 46.9 - 48.5 perceptual capabilities with strong generalization and robustness across different types of visual tasks. Zero-shot General Visual Comprehension. Table 5 reports zero-shot results on wide range of multimodal benchmarks for general visual understanding and reasoning. After training with our structured visual reasoning, Artemis demonstrates consistent improvement in overall performance, indicating that the models perception ability has been strengthened and more uniformly aligned across diverse visual tasks. These results further demonstrate that our method effectively enhances the models general visual comprehension without task-specific tuning."
        },
        {
            "title": "4.3 Ablation Study of Artemis",
            "content": "Rewards Composition. We conduct ablations to evaluate the contributions of different reward components on both in-domain (RefCOCOg, COCOmAP) and out-of-domain (MATHGLANCE) perception, as shown in Table 7. Adding the grounding reward (Grd. in the table, indicates format and answer rewards for grounding task) alone improves the models perception, particularly on out-of-domain MATHGLANCE, indicating better generalization. Using only the detection reward (Det. in the table, indicates format and answer rewards for detection task) enhances holistic scene perception (COCOmAP) but substantially reduces grounding accuracy. Table 6 Ablation study on the contributions of Grd. (Reward for Grounding), Det. (Reward for Detection), and S.V. Rsn. (Structured Visual Reasoning Reward). The last column shows the average MATHGLANCE accuracy. Incorporating structured visual reasoning with grounding further improves performance across all metrics, confirming that structured intermediate reasoning enhances both in-domain and out-of-domain perception. In contrast, joint training without structured visual reasoning guidance may achieve reasonable in-domain performance, but its generalization to out-of-domain scenarios is limited. Enabling all three components achieves the best balanced performance, demonstrating the complementarity of the rewards in strengthening general visual understanding. MATHGLANCE 33.1 43.1 44.0 44.2 43.7 49.3 RefCOCOgval 85.1 86.6 51.6 86.8 87.1 87.3 COCOmAP 15.4 27.5 30.6 28.0 30.7 31. S.V. Rsn. Grd. Det. - - - Further Analysis of Reasoning Forms. To further understand how different reasoning forms influence perception, we conduct an additional ablation comparing None reasoning, linguistic reasoning, and our proposed structured visual reasoning. Although we already analyzed the impact of reasoning forms on perception tasks in the main paper (e.g., VLM-R1 uses linguistic reasoning, Perception-R1 skips reasoning and supervises only outcome rewards), here we perform an additional controlled ablation on our model to rigorously isolate and validate these effects. For the None reasoning setting, we adjust the training prompt to make the model output only the final answer without any reasoning. For Ling. Rsn., we adjust the training prompt so that the model produces linguistic reasoning. For both variants, we only supervise the final answer, following similar setup to Perception-R1 and VLM-R1. S.V. Rsn. is our default setting, which applies structured visual reasoning supervision. This study isolates the impact of reasoning structure by keeping all other training conditions identical, including the same cold-start model and hyperparameters. From Table 7, we derive several observations. Table 7 Ablation study comparing different forms of reasoning on our model with cold-start initialization. None indicates no reasoning, Ling. Rsn. denotes linguistic reasoning, and S.V. Rsn. denotes structured visual reasoning. M.G. represents MATHGLANCE benchmark. Table 8 Ablation study comparing different training orders for grounding (Grd.) and detection (Det.) tasks. Det. Grd. means training detection first, then grounding; Grd. Det. means training grounding first, then detection; Joint means training both tasks simultaneously. M.G. represents MATHGLANCE benchmark. 10 Forms RefCOCOgval COCOmAP M.G. Pixmoval LISAtest None Ling. Rsn. S.V. Rsn. 87.5 86.4 87. 30.6 30.4 31.0 44.3 47.3 65. 13.4 49.3 81.4 71.7 66.5 78. Training Order RefCOCOgval COCOmAP M.G. Pixmoval LISAtest Det. Grd. Grd. Det. Joint 77.1 79.2 68.8 67.8 46.7 42.7 86.9 87.2 28.4 30. 49.3 81.4 78.3 31.0 87.3 None reasoning. Removing reasoning entirely gives surprisingly strong in-domain grounding score, but leads to severe degradation on out-of-domain benchmarks such as MATHGLANCE, Pixmo, and LISA. These results reveal structural misalignment between the reward objective and the perceptual process. Supervising only outcome rewards encourages the model to overfit to local bounding-box objectives, so it focuses on matching training boxes rather than developing general perceptual skills. This leads to high in-domain grounding accuracy but poor performance on out-of-domain perceptual benchmarks. Linguistic reasoning. Introducing linguistic rationales causes performance to drop sharply across most perception benchmarks, particularly on visual counting. This supports our earlier argument in the main paper that models using linguistic reasoning tend to produce superficially plausible linguistic rationales, which do not contribute to and often interfere with the underlying perceptual process. This experimental observation aligns with our rethink, further validating the effectiveness of our method and clarifying the underlying motivation. Structured visual reasoning. Our structured visual reasoning maintains competitive in-domain grounding performance while clearly improving downstream perception benchmarks. We attribute these gains to the structured visual reasoning rewards, which shift the reasoning process to the object-centric level. Rather than optimizing linguistic reasoning, the policy learns to translate semantic concepts into object-centric visual evidence, leading to more stable optimization and perceptual representations that generalize across tasks. Effect of Joint Training on Perception. To investigate the effect of task ordering on the perceptual gains of our model, we conduct an additional experiment comparing joint training with sequential task optimization. Specifically, we contrast our default joint optimization with two sequential alternatives: training detection first and then grounding (Det. Grd.), and training grounding first and then detection (Grd. Det.). As shown in Table 8, the sequential strategies yield competitive performance on in-domain visual grounding (RefCOCOgval), with Grd. Det. slightly outperforming Det. Grd. (87.2 vs. 86.9). However, joint training consistently achieves the strongest results across all out-of-domain perception benchmarks. This indicates that while sequential training can handle individual tasks effectively, jointly optimizing grounding and detection allows the model to learn more generalizable perceptual representations. In particular, joint training balances the supervision signals across tasks, preventing overfitting to single objective and promoting transfer to diverse downstream perception challenges. Effect of SFT and RL Training Strategies. We ablate the impact of different training strategies on visual perception as shown in Fig. 3. We use the same set of samples for SFT and RL, converted into the SFT format for SFT training. The baseline Qwen2.5VL-3B without SFT or RL exhibits low scene perception, especially in detection. Applying SFT only on grounding (SFT-Grd.) decreases performance, likely due to insufficient data for learning structured visual reasoning. In contrast, SFT on detection (SFT-Det.) improves overall scene perception but significantly harms grounding, highlighting the challenge of balancing fine-grained perception in single-task SFT. Mixing SFT for both grounding and detection (SFT-Full) partially recovers performance, yet data scarcity still limits gains. Only with GRPO-based RL can the model effectively leverage structured 11 Figure 3 Impact of SFT and RL training strategies on COCO detection (bars) and visual grounding (lines). Figure 4 Impact of cold start strategies on COCO detection (bars) and visual grounding (lines). visual reasoning to enhance both detection and grounding under small data regimes. For fairness, the RL model does not use any cold-start initialization. Cold-Start Analysis. We conduct ablation study to invest the effect of cold-start strategies and the proportion of grounding and detection data on the models perception performance  (Fig. 4)  . Without cold-start data, the model struggles to detect contextual objects. This is likely due to insufficient exposure to diverse scene contexts, which results in reduced scene perception. Such deficiencies may compromise downstream tasks that rely on comprehensive scene understanding. Introducing only small amount of grounding data may provide initial guidance, but it can also disrupt the models pre-existing capabilities learned from baseline training, and may even negatively affect other perceptual abilities, such as detection. Adding balanced proportion of detection data significantly recovers performance, while applying cold start directly to detection achieves the best overall results. This indicates that improving scene-level perception not only enhances structured visual reasoning reasoning but also boosts the models overall perception capabilities."
        },
        {
            "title": "4.4 More In-depth Analysis of Artemis",
            "content": "Here we provide an in-depth analysis of why Structured Visual Reasoning are fundamentally better aligned with perception-policy learning than conventional linguistic reasoning. As noted in the Section 1, our design is grounded in the cognitive intuition of how humans perceive and reason about visual scenes [23, 24]. Human perception operates through coordinated spatial attention, object localization, and semantic integration. Neuroscience studies show that humans process complex scenes by sequentially directing attention across spatially relevant regions [63]. The posterior parietal cortex maintains spatial priority map that enables rapid localization of task-relevant objects before any linguistic reasoning takes place [64]. In other words, the reasoning process begins with where to look and what is present, rather than with holistic linguistic descriptions. In our proposed Structured Visual Reasoning, the computational analogue of this moving spotlight is the (labelbounding-box) pair, where bounding box coordinates provide precise spatial localization and category labels indicate object identity. Linguistic reasoning, by contrast, are free-form and inherently uncertain: they tend to summarize the entire scene using broad captions that are vulnerable to hallucinated visual cues, irrelevant details (e.g., color or background descriptions), and the loss of fine-grained spatial constraints. Structured Visual Reasoning instead enforce explicit, spatially grounded reasoning that remains tightly coupled to the policys perceptual state. Modeling the reasoning process directly in visual space and object-centric ensures that intermediate thoughts propagate in predictable and interpretable manner, with each step anchored to specific region of the visual input. This mismatch between linguistic reasoning and visual 12 priority undermines the policys stability and generalizability, further underscoring the advantage of structured visual reasoning for perception-policy learning. We conducted comprehensive experiments to validate the effectiveness of Artemis, unified rule-based RL framework for perceptionpolicy learning equipped with structured visual reasoning, as reported in Section 4.2. We further provide controlled experiment using the same training setup as ours but varying only the form of the thinking process, as shown in Tabel 7, with detailed analysis in the related section. key finding from these evaluations concerns zero-shot generalization: structured object-centric thoughts enable the policy model to generalize to out-of-domain tasks and transition smoothly from natural images to diagrammatic inputs. Table 3 shows that, without any exposure to the counting task during training and without any deliberate post-hoc processing, Artemis achieves significant improvement of +11.3 over VisionReasoner (which relies on language-based reasoning). We further provide qualitative visualizations in Fig. 10Fig. 11. For example, in Fig. 10, our model reasons in visual format, explicitly listing target objects and their locations, which naturally leads to the correct answer of 6. By contrast, the base Qwen2.5-VL model hallucinates multiple bounding boxes for the same balloon while missing others, yielding an incorrect count. Notably, Qwen2.5-VL is not able to directly infer the numeric quantity; instead, it relies on post-processing heuristics (len(list)) to obtain the final number, highlighting its lack of grounded perceptual reasoning while merely following the counting prompt to list potential targets. From Natural Images to Mathematical Diagrams. Unlike natural images, mathematical diagrams are inherently semantically sparse yet structurally rich, requiring genuine understanding of geometric primitives rather than superficial pattern recognition to complete the perception tasks [56]. Prior work shows that even advanced general MLLMs (e.g., GPT-4o) struggle with planar geometry, especially in fine-grained grounding tasks. In contrast, Artemis naturally transfers its strong perceptual ability to mathematical understanding in diagrammatic settings. For example, we achieve 24.2% accuracy on the grounding task, whereas other RL-based policy models remain below 5%. In shape classification and relationship identification, Artemis shows over 10% improvements compared with both general-purpose MLLMs and RL-based MLLMs  (Table 4)  . This zero-shot transfer improvement is non-trivial: in the tasks of MATHGLANCE benchmark, shape classification requires distinguishing highly similar polygon families (e.g., equilateral vs. right triangles), and relationship identification involves mathematical relations such as perpendicularity and parallelism that differ fundamentally from natural-image categories. Artemiss ability to perform these tasks in zero-shot manner reinforces our claim: stronger perceptual model should handle diverse perception tasks within unified framework, rather than relying on separately trained task-specific models for each individual task. Moreover, the Structured Visual Reasoning design provides task-agnostic and generalizable format for perception, enabling robust transfer across domains."
        },
        {
            "title": "5 Conclusion",
            "content": "This work rethinks perception-policy learning inspired by how human perception of complex scenes, emphasizing the importance of structured visual reasoning. Building on this insight, we present Artemis, unified rule-based RL framework that guides visual perception learning through structured visual reasoning. By supervising reasoning in structured and verifiable manner, the model develops stronger perception and demonstrates substantial generalization across across broad range of in-domain and out-of-domain visual perception tasks. Our findings confirm that aligning intermediate reasoning with spatial and object-centric manner empowers perception-policy learning."
        },
        {
            "title": "References",
            "content": "[1] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models, 2024. 13 [2] Qwen Team et al. Qwen2 technical report, 2024. [3] OpenAI. Gpt-4 technical report, 2023. [4] OpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022. [5] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card, 2024. [6] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models, 2023. [7] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size, 2024. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [9] Yanpeng Sun, Huaxin Zhang, Qiang Chen, Xinyu Zhang, Nong Sang, Gang Zhang, Jingdong Wang, and Zechao Li. Improving multi-modal large language model through boosting vision capabilities, 2024. [10] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report, 2025. [11] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. [12] Wei Tang, Yanpeng Sun, Qinying Gu, and Zechao Li. Visual position prompt for mllm based visual grounding. IEEE Trans. Multimedia, 2025. [13] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. [14] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [15] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model, 2025. [16] Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning, 2025. [17] Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning, 2025. [18] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yanwoa Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. In Proceedings of Advances in Neural Information Processing Systems, 2025. [19] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning, 2025. [20] Ming Li, Jike Zhong, Shitian Zhao, Yuxiang Lai, Haoquan Zhang, Wang Bill Zhu, and Kaipeng Zhang. Think or not think: study of explicit thinking in rule-based visual reinforcement fine-tuning, 2025. [21] Zirun Guo, Minjie Hong, and Tao Jin. Observe-r1: Unlocking reasoning abilities of mllms with dynamic progressive reinforcement learning, 2025. [22] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning, 2025. [23] Antonio Torralba, Aude Oliva, Monica Castelhano, and John Henderson. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search. Psychological review, 113(4):766, 2006. [24] Jeremy Wolfe and Todd Horowitz. Five factors that guide attention in visual search. Nature human behaviour, 1(3):0058, 2017. [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In Proceedings of Advances in Neural Information Processing Systems, volume 35, pages 2773027744, 2022. [26] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Proceedings of Advances in Neural Information Processing Systems, volume 36, pages 5372853741, 2023. [27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [28] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report, 2024. [29] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [30] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-of-thought reasoning. In Proceedings of Advances in Neural Information Processing Systems, 2025. [31] Junyu Xiong, Yonghui Wang, Weichao Zhao, Chenyu Liu, Bing Yin, Wengang Zhou, and Houqiang Li. Docr1: Evidence page-guided grpo for multi-page document understanding, 2025. [32] Wenwen Yu, Zhibo Yang, Yuliang Liu, and Xiang Bai. Docthinker: Explainable multimodal large language models with rule-based reinforcement learning for document understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 837847, 2025. [33] Yanpeng Sun, Jing Hao, Ke Zhu, Jiang-Jiang Liu, Yuxiang Zhao, Xiaofan Li, Gang Zhang, Zechao Li, and Jingdong Wang. Descriptive caption enhancement with visual specialists for multi-modal perception. arXiv preprint arXiv:2412.14233, 2025. [34] Yanpeng Sun, Qiang Chen, Jian Wang, Jingdong Wang, and Zechao Li. Exploring effective factors for improving visual in-context learning. IEEE Trans. Image Process., 34:21472160, 2025. [35] Gaoxiang Cong, Liang Li, Zhenhuan Liu, Yunbin Tu, Weijun Qin, Shenyuan Zhang, Chengang Yan, Wenyu Wang, and Bin Jiang. LS-GAN: iterative language-based image manipulation via long and short term consistency reasoning. In Proceedings of ACM Int. Conf. Multimedia, pages 44964504, 2022. [36] Yunbin Tu, Liang Li, Li Su, Zheng-Jun Zha, and Qingming Huang. SMART: syntax-calibrated multi-aspect relation transformer for change captioning. IEEE Trans. Pattern Anal. Mach. Intell., 46(7):49264943, 2024. [37] Zechao Li, Yanpeng Sun, Liyan Zhang, and Jinhui Tang. Ctnet: Context-based tandem network for semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 44(12):99049917, 2022. [38] Wei Tang, Liang Li, Xuejing Liu, Lu Jin, Jinhui Tang, and Zechao Li. Context disentangling and prototype inheriting for robust visual grounding. IEEE Trans. Pattern Anal. Mach. Intell., 46(5):32133229, 2024. [39] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr: Modulated eetection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17601770, 2021. [40] Yanpeng Sun, Jiahui Chen, Shan Zhang, Xinyu Zhang, Qiang Chen, Gang Zhang, Errui Ding, Jingdong Wang, and Zechao Li. Vrp-sam: Sam with visual reference prompt. In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 15 [41] Yanpeng Sun, Qiang Chen, Xiangyu He, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jian Cheng, Zechao Li, and Jingdong Wang. Singular value fine-tuning: Few-shot segmentation requires few-parameters fine-tuning. In Proceedings of Advances in Neural Information Processing Systems, volume 35, pages 3748437496, 2022. [42] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. [43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [44] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization, 2025. [45] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization, 2025. [46] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In Proceedings of Int. Conf. Learn. Represent., 2023. [47] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83 97, 1955. [48] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In Proceedings of International Conference on Machine Learning, pages 2331823340, 2022. [49] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark, 2019. [50] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement, 2018. [51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell., 39(6):11371149, 2016. [52] Yufei Zhan, Yousong Zhu, Zhiyang Chen, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon: Spelling out all object locations at any granularity with large language models. In Proceedings of European Conference on Computer Vision, pages 405422, 2024. [53] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Proceedings of European Conference on Computer Vision, pages 740755, 2014. [54] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [55] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-ofthe-art vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 91104, 2025. [56] Yanpeng Sun, Shan Zhang, Wei Tang, Aotian Chen, Piotr Koniusz, Kai Zou, Yuan Xue, and Anton van den Hengel. Mathglance: Multimodal large language models do not know where to look in mathematical diagrams, 2025. [57] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of ACM Int. Conf. Multimedia, pages 1119811201, 2024. [58] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?, 2023. 16 [59] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023. [60] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023. [61] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [62] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal large language model, 2023. [63] James Bisley and Michael Goldberg. Attention, intention, and priority in the parietal lobe. Annual Review of Neuroscience, 33:121, 2010. [64] Trenton Jerde, Elisha Merriam, Adam Riggall, James Hedges, and Clayton Curtis. Prioritized maps of space in human frontoparietal cortex. Journal of Neuroscience, 32(48):1738217390, 2012. [65] Zilun Zhang, Zian Guan, Tiancheng Zhao, Haozhan Shen, Tianyu Li, Yuxiang Cai, Zhonggen Su, Zhaojun Liu, Jianwei Yin, and Xiang Li. Geo-r1: Improving few-shot geospatial referring expression understanding with reinforcement fine-tuning, 2025. 17 Artemis: Structured Visual Reasoning for Perception Policy Learning Appendix"
        },
        {
            "title": "A Details of Outcome Rewards",
            "content": "As described in the main paper, we teach the model structured visual reasoning by relying on tasks that explicitly couple language with perception. Visual grounding provides fine-grained supervision that links semantic concepts to precise spatial locations, while object detection offers dense scene-level signals that strengthen the models perceptual foundation. These tasks guide the policy to translate semantic understanding into object-centric visual evidence. Under our unified rule-based reinforcement framework built upon GRPO, we redesign the rewards to evaluate not only final outcomes but also intermediate visual reasoning steps. Below, we provide the details of the outcome rewards. A.1 Format Rewards We define format reward rformat to ensure that model outputs follow the expected structure for rule-based evaluation. Visual Grounding. The output must include correctly structured <think> </think> block followed by an <answer> </answer> block containing at least one bounding box. Let denote the models output. The format reward is defined as (cid:40) rVG format = 1, 0, if is valid otherwise (6) Object Detection. Unlike grounding tasks, which require interpreting user prompt to locate specific target within the image, pure detection tasks aim to identify and localize all foreground objects based solely on their visual features, without reasoning over candidate regions guided by language. We therefore omit the <think> </think> block in this setting. The format reward is 1valid(O) = (cid:40) 1, is valid, otherwise, 0, rDET format = 1think(O) = 0 and 1valid(O) = 1, 1, 0.5, 1think(O) = 1 and 1valid(O) = 1, 0, otherwise. (7) (8) where 1think(O) indicates the presence of <think> block in O, and 1valid(O) indicates whether is valid. Finally, we can define the overall format reward across tasks as: This unified formulation allows us to handle both tasks in single framework while respecting their individual format requirements. rformat = rVG format + rDET format. (9) 18 A.2 Answer Rewards Building on the format rewards, the answer rewards directly evaluate the perceptual correctness of model predictions. Visual Grounding. For visual grounding, each sample contains exactly one target region. Given the predicted bounding box ˆB and the ground-truth bounding box B, the answer reward measures localization quality using the GIoU [65]: ans = GIoU( ˆB, B). rVG (10) Object Detection. Object localization often involves multiple objects, requiring one-to-one correspondences between predictions and ground-truth boxes to accurately compute rewards. Following Vision-R1 [19], we use the Hungarian algorithm [47] to establish these correspondences between predicted bounding boxes ˆB with predicted categories ˆC and ground-truth boxes with categories C. Based on these matched pairs, the base detection reward is defined as weighted sum of three components: base = λ1 GIoU( ˆB, B) + λ2 1{ ˆC = C} + λ3 F1( ˆB, B), rDET where the first term measures localization quality, the second term checks exact label match, and the third term captures coverage completeness over all predicted and ground-truth boxes. The weights are set empirically as λ1 = 0.4, λ2 = 0.3, and λ3 = 0.3. To improve detection quality, we apply two multiplicative penalties to the base detection reward. The quantity penalty encourages the number of predicted bounding boxes to match the number of ground-truth boxes: (11) ρnum( ˆB, B) = min( ˆB, B) max( ˆB, B) , (12) and the missing penalty penalizes each ground-truth box that is not matched by any prediction ˆb ˆB: ρmiss( ˆB, B) = 1 {b is unmatched by any ˆb ˆB} . The final detection answer reward is then computed as ans = rDET rDET base ρnum( ˆB, B) ρmiss( ˆB, B), and is clipped to lie within [0, 1] for stability. Similarly, we can define the overall answer reward across tasks as the sum of the task-specific rewards:"
        },
        {
            "title": "B Dataset Construction",
            "content": "rans = rVG ans + rDET ans . (13) (14) (15) As described in the main paper, we supervise structured visual reasoning using dedicated post-training dataset. To support this objective within our unified Artemis framework, we construct the Artemis-RFT dataset from MS-COCO [53], yielding roughly 77k post-training instances. Table 9 summarizes the dataset, showing the number of samples, images, and average samples per image for the grounding and detection subsets. The grounding (Grd.) subset contains 39,651 samples from 7,037 images, averaging 5.6 samples per image, designed to teach precise spatial localization. The detection (Det.) subset contains 37,446 samples from 37,446 images, averaging 1 sample per image, providing dense scene-level supervision. Specifically, the dataset is constructed using MS-COCO images together with grounding-related annotations from LLaVA-665k [43] (including RefCOCO/+/g). From these annotations, we extract referring expression queries and their associated bounding boxes. Region-caption annotations associated with localized regions are also collected and used as visual evidence for constructing reasoning traces. When multiple captions describe 19 Figure 5 data example from Artemis-RFT. Our dataset contains two task types, visual grounding and object detection, and our unified Artemis perception policy learning framework is jointly trained on both. For the grounding example, purple boxes denote the reasoning objects; the numeric labels are added only for illustration and do not exist in the raw data. The solution is indicated by green box. In this example, box ➂ is the solution and also serves as the key reasoning object, so only the green box is shown where the two overlap. For the detection example, we display only the green box that corresponds to the solution. the same region, they are merged into single concise description. All bounding boxes are converted from normalized padded-square coordinates back to the original image resolution and unified under consistent coordinate convention of Qwen2.5-VL [10]. To further enrich the object-centric evidence, we incorporate region annotations from Vision-R1 [19] that correspond to the same images, standardizing their labels and coordinates to match our representation. During this process, multiple bounding boxes that refer to the same image region are merged to avoid redundant annotations while preserving all valid visual evidence. To ensure clear correspondence between reasoning process and final answers, we make sure that every answer corresponds to region included in the reasoning objects, which also enables the identification of key objects. For the detection subset, we randomly sample 80k COCO detection annotations as cold-start data and incorporate the remaining annotations in the same unified box format, providing additional scene-level supervision. data example of Artemis-RFT is shown in Fig. 5. Table 9 Statistics of the Artemis-RFT dataset, including the grounding (Grd.) and detection (Det.) subsets. We also report average samples per image. #Samples #Images Avg. Samples per Image Grd. subset Det. subset 39,651 37,446 7,037 37,446 Partition 5.6 1 77, 44,483 Total"
        },
        {
            "title": "C Prompt Settings",
            "content": "For reproducibility, we provide the complete prompts used in our experiments. Table 10, Table 11, and Table 12 show the prompts for visual grounding, object detection, and visual counting, respectively. For LISA grounding, we use the same prompt as in the visual grounding task. For MATHGLANCE and Visual Grounding (Training & Testing) User: Analyze the image and answer the following question:What region is described as: <description>? Think step by step: 1. Identify small set of key objects or regions that help locate the described target. 2. These may include the target object itself and/or contextual objects (e.g., nearby items, reference persons, background cues). 20 3. Estimate their bounding box(es) in [x1, y1, x2, y2] format (pixel coordinates). Output your reasoning in <think> tags as JSON list of key objects: <think> [{\"label\": \"<object_label_1>\", \"bbox\": [x1, y1, x2, y2]}, {\"label\": \"<object_label_2>\", \"bbox\": [x1, y1, x2, y2]}] </think> Then output the final answer in <answer> tags: <answer>{\"label\": \"<description>\", \"bbox\": [x1, y1, x2, y2]}</answer> Table 10 Default prompt used for visual grounding in Artemis. general visual comprehension MLLM evaluation benchmarks, such as MMStar, we use the official prompts provided by the evaluation kits (e.g., VLMEVALkit [57]) to ensure fair comparison. For the other comparison methods included in the main paper, i.e., models marked with , the results are generated using our own inference. We use the official checkpoints corresponding to the most suitable task (when available) and follow the prompt settings recommended in their original papers or GitHub repositories (if provided). For full details on the inference procedures, please refer to the original publications and codebases."
        },
        {
            "title": "D Qualitative Results",
            "content": "In this section, we provide qualitative visualizations to complement the quantitative results. We illustrate Artemis performance on visual grounding (Section D.1), visual counting (Section D.2), and additional tasks including MATHGLANCE, COCO Val 2017, and LISA grounding (Section D.3). D.1 Qualitative Comparisons on RefCOCO/+/g In Fig. 6Fig. 9, we present several visual grounding examples frome RefCOCO/+/g. For the Perception-R1 model that skips the reasoning process, its grounding performance is noticeably inaccurate across these cases. This suggests that removing the reasoning stage may hinder the models ability to further learn and perceive complex scenes, as the model is supervised only through answer rewards. Consequently, it tends to optimize toward matching annotated boxes rather than developing general perceptual skills. As discussed in Section 4.3 (Further Analysis of Reasoning Forms), such training may yield reasonable in-domain performance but fails to substantially enhance the underlying perception capability, resulting in poor generalization to out-of-domain perception data or tasks. In essence, this approach does not fully leverage the strong reasoning potential brought by MLLMs and GRPO-based reinforcement learning, making it fundamentally similar to previous grounding models that directly optimize the final outputs. For linguistic reasoning based VLM-R1, the main limitation lies in the difficulty of supervising perception oriented reasoning within the linguistic space, which often leads to mismatches between the reasoning and the final answer. For example, in Case 1  (Fig. 6)  , the query is the person riding the horse in the center front area. Although the reasoning appears to correctly describe the image, the answer incorrectly shifts the target from the person to the horse. In Case 2  (Fig. 7)  , the reasoning is already incorrect at the start, stating that there are two zebras while the image actually contains three. In Case 3  (Fig. 8)  and Case 4  (Fig. 9)  , the model provides the correct descriptions of the scenes during the reasoning stage, yet the final answer still selects an Object Detection (Training & Testing) User: Analyze the image and answer: List all visible objects in the image with their bounding boxes.\" Think step by step: 1. Identify all major visible objects in the image from the following category set: [category str of coco]. 2. For each object, estimate its category and bounding box [x1, y1, x2, y2] (pixel coordinates). 3. Avoid duplicates and irrelevant objects. 4. Directly output the final answer in <answer> tags as JSON list of all detected objects, and do not ouput 21 any <think>: Directly output the final answer in <answer> tags: <answer> [{\"label\": \"<object_label_1>\", \"bbox\": [x1, y1, x2, y2]}, {\"label\": \"<object_label_2>\", \"bbox\": [x1, y1, x2, y2]}] </answer> Table 11 Default prompt used for object detection in Artemis. Figure 6 Qualitative visual grounding results (Case 1) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted. Green: ground truth; Purple: Artemis reasoning boxes; Red: Artemis answer; Blue: Perception-R1; Pink: VLM-R1. incorrect or imprecise object. These reasoning and answer mismatches, together with the quantitative results, indicate that linguistic reasoning may not provide an effective or reliable reasoning mechanism for perception driven grounding tasks. Together with the quantitative results, the visualization cases verify that structured visual reasoning rewards provide more suitable training signal for perception tasks. By guiding the model to reason in an object-centric manner, as shown by the purple reasoning bounding boxes and red answer bounding boxes in the cases, our Artemis produces more precise and correct grounding results compared with other methods like Perception-R1 Visual Counting (Zero-shot Testing Only) User: Analyze the image and answer: \"How many <description> are in the image?\" Step 1: Examine the image carefully and list **all visible instances** of <description> in <think> tags. Step 2: Each instance should be dictionary with \"label\" and \"bbox\" if possible. Step 3: After listing all instances, count them. Step 4: Output the total count in <answer> tags. **The number must exactly equal the number of items in <think>. Do NOT estimate or guess.** Example: <think> [{\"label\": \"<description> #1\", \"bbox\": [x1, y1, x2, y2]}, {\"label\": \"<description> #2\", \"bbox\": [x1, y1, x2, y2]}] </think> <answer> {\"<description> count\": <number_of_items_in_think>} </answer> Table 12 Default prompt used for zero-shot visual counting in Artemis. Figure 7 Qualitative visual grounding results (Case 2) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted. Green: ground truth; Purple: Artemis reasoning boxes; Red: Artemis answer; Blue: Perception-R1; Pink: VLM-R1. and VLM-R1. D.2 Qualitative Comparisons on Visual Counting In Fig. 10 and Fig. 11, we present zero-shot visual counting qualitative results of Artemis on the Pixmo dataset, compared with Perception-R1 (trained on Pixmo as described in the main paper) and the baseline 23 Figure 8 Qualitative visual grounding results (Case 3) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted. Green: ground truth; Purple: Artemis reasoning boxes; Red: Artemis answer; Blue: Perception-R1; Pink: VLM-R1. Figure 9 Qualitative visual grounding results (Case 4) of Artemis compared with Perception-R1 and VLM-R1 on the RefCOCO/+/g datasets. For brevity, full prompts are omitted. Green: ground truth; Purple: Artemis reasoning boxes; Red: Artemis answer; Blue: Perception-R1; Pink: VLM-R1. Qwen2.5-VL-3B. As discussed in Section 4.2.3 (Main paper), although Artemis has not been trained on counting tasks or related data, it is able to internally enumerate instances during the <think> phase and directly 24 Figure 10 Qualitative visual counting results (Case 1) of Artemis compared with Perception-R1 and Qwen2.5-VL on the Pixmo datasets. For brevity, full prompts are omitted. Green: ground truth; Purple: Artemis reasoning boxes; Red: Artemis answer; Blue: Perception-R1; Cyan: Qwen2.5-VL. Figure 11 Qualitative visual counting results (Case 2) of Artemis compared with Perception-R1 and Qwen2.5-VL on the Pixmo datasets. For brevity, full prompts are omitted. Green: ground truth; Purple: Artemis reasoning boxes; Red: Artemis answer; Blue: Perception-R1; Cyan: Qwen2.5-VL. produce accurate numeric counts in these cases. In contrast, Perception-R1 and Qwen2.5-VL-3B rely on post-processing, where counts are derived from detected objects. Such statistical counting does not reflect true learning of counting as perceptual ability, but rather represents simplified variant of detection-based 25 Figure 12 Qualitative mathematical perception results of Artemis compared with Qwen2.5-VL on MATHGLANCE, showing examples from the mathematical shape Classification and mathematical shape Counting tasks. For brevity, full prompts are omitted. Green: ground truth; Cyan: Qwen2.5-VL; Red: Artemis answer. perception. For example, in Case 1  (Fig. 10)  , Perception-R1 over-detects one bounding box (box 7), while Qwen2.5-VL-3B outputs repeated boxes (boxes 25 and 69 are duplicates). In Case 2  (Fig. 11)  , both methods fail to detect some instances. Taken together with the quantitative results in the main paper, these visualizations suggest that Artemis develops enhanced perception capabilities. By leveraging structured visual reasoning, the model acquires perceptual skills that seamlessly transfer to counting tasks, resulting in counting behavior remarkably similar to human intuition. D.3 Additional Qualitative Results Futhermore, we present visualizations of Artemis on the benchmarks of MATHGLANCE (Fig. 12 and Fig. 13), COCO Val 2017  (Fig. 14)  , and the LISA grounding test  (Fig. 15)  . These cases provide several observations that complement the quantitative results reported in the main paper. First, the perception capabilities learned from natural images can transfer to visually distinct domains, such as mathematical diagrams, improving performance on related perception tasks. For example, in the MATHGLANCE counting problem  (Fig. 12)  , Artemis correctly identifies the only quadrilateral in the scene, whereas Qwen2.5-VL incorrectly selects option D, whose referenced vertices do not even form valid shape Figure 13 Qualitative mathematical perception results of Artemis compared with Qwen2.5-VL on MATHGLANCE, showing examples from the mathematical Visual Grounding and mathematical Relation Identification tasks. For brevity, full prompts are omitted. Green: ground truth; Cyan: Qwen2.5-VL; Red: Artemis answer. in the diagram. In the MATHGLANCE relation problem  (Fig. 13)  , Artemis also succeeds in selecting the correct relational configuration by accurately perceiving the relative positions of the relevant shapes, while Qwen2.5-VL again makes an incorrect choice. Second, for detection and reasoning grounding, we observe an overall enhancement of scene-level perceptual capabilities. In detection cases  (Fig. 14)  , Artemis does not miss any ground-truth objects, suggesting improved perception in natural scenes. For LISA grounding test, which requires knowledge-intensive reasoning, Artemis is able to locate relevant visual evidence to support its answers. For instance, in the third case of Fig. 15, the model first perceives the relevant visual evidence, identifying the person mentioned in the query as well as the ladder, and then uses this evidence to infer the final answer. The visualizations illustrate that structured visual reasoning training leads to stronger object-centric perception and more accurate reasoning-based grounding across diverse tasks. 27 Figure 14 Qualitative object detection results of Artemis on COCO Val 2017. Green: ground truth; Red: Artemis answer; Figure 15 Qualitative reasoning-based grounding results of Artemis on the LISA grounding test set. Green: ground truth; Purple: Artemis reasoning boxes; Red: Artemis answer."
        }
    ],
    "affiliations": [
        "Adelaide AIML",
        "Baidu Inc.",
        "Data61 CSIRO",
        "NJUST IMAG",
        "SUTD IMPL",
        "SenseTime"
    ]
}