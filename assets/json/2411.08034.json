{
    "paper_title": "Scaling Properties of Diffusion Models for Perceptual Tasks",
    "authors": [
        "Rahul Ravishankar",
        "Zeeshan Patel",
        "Jathushan Rajasegaran",
        "Jitendra Malik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we argue that iterative computation with diffusion models offers a powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and amodal segmentation under the framework of image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perceptual tasks. Through a careful analysis of these scaling properties, we formulate compute-optimal training and inference recipes to scale diffusion models for visual perception tasks. Our models achieve competitive performance to state-of-the-art methods using significantly less data and compute. To access our code and models, see https://scaling-diffusion-perception.github.io ."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 2 4 3 0 8 0 . 1 1 4 2 : r a"
        },
        {
            "title": "SCALING PROPERTIES OF DIFFUSION MODELS\nFOR PERCEPTUAL TASKS",
            "content": "Rahul Ravishankar*, Zeeshan Patel*, Jathushan Rajasegaran, Jitendra Malik University of California, Berkeley {rravishankar, zeeshanp}@berkeley.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "In this paper, we argue that iterative computation with diffusion models offers powerful paradigm for not only generation but also visual perception tasks. We unify tasks such as depth estimation, optical flow, and amodal segmentation under the framework of image-to-image translation, and show how diffusion models benefit from scaling training and test-time compute for these perceptual tasks. Through careful analysis of these scaling properties, we formulate computeoptimal training and inference recipes to scale diffusion models for visual perception tasks. Our models achieve competitive performance to state-of-the-art methods using significantly less data and compute. We release code and models at scaling-diffusion-perception.github.io."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have emerged as powerful techniques for generating images and videos, while showing excellent scaling behaviors. However, diffusion models can also be utilized for downstream prediction for inverse vision problems. In this paper, we present unified framework to perform depth estimation, optical flow estimation, and amodal segmentation with single diffusion model, as illustrated in Figure 1. Previous works such as Marigold (Ke et al., 2024), FlowDiffuser (Luo et al., 2024), and pix2gestalt (Ozguroglu et al., 2024) demonstrate the potential of repurposing image diffusion models for various inverse vision tasks individually. Building on this foundation, we perform an extensive empirical study, establishing scaling power laws for depth estimation, and display their transferability to other perceptual tasks. Using insights from these scaling laws, we formulate compute-optimal recipes for diffusion training and inference. We are the first to show that efficiently scaling compute for diffusion models leads to significant performance gains in downstream perceptual tasks. Recent works have also focused on scaling test-time compute to enhance the capabilities of modern LLMs, as demonstrated by OpenAIs o1 model (OpenAI, 2024). As Noam Brown noted, It turned out that having bot think for just 20 seconds in hand of poker got the same boosting performance as scaling up the model by 100,000x and training it for 100,000 times longer. In our experiments, we realize similar trade off between allocating more compute during training versus test-time for diffusion models. We scale test-time compute by exploiting the iterative and stochastic nature of diffusion to increase the number of denoising steps. By allocating more compute to early denoising steps, and ensembling multiple denoised predictions, we consistently achieve higher accuracy on these perceptual tasks. Our results provide evidence of the benefits of scaling test-time compute for inverse vision problems under constrained compute budgets, bringing new perspective to the conventional paradigm of training-centric scaling for generative models. *Equal Contribution Figure 1: Unified Framework: We fine-tune pre-trained Diffusion Model (DM), for visual perception tasks. We take RGB image, and conditional image (i.e. next video frame, occlusion mask, etc.), along with the noised image of the ground truth prediction. Our model generates predictions for visual tasks such as depth estimation, optical flow prediction, and amodal segmentation, based on the conditional task embedding. We train generalist model that can perform all three tasks with exceptional performance."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Generative Modeling: Generative modeling has been studied under various methods, including VAEs (Kingma, 2013), GANs (Goodfellow et al., 2014), Normalizing Flows (Rezende & Mohamed, 2015), Autoregressive models (van den Oord et al., 2016), and Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020). Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020) have shown impressive scaling behaviors for many image and video generation models. Notable examples include Latent Diffusion Models (Rombach et al., 2022), which enhanced efficiency by operating in compressed latent space, Imagen (Saharia et al., 2022), which generates samples in pixel space with increasing resolution, and Consistency Models (Song et al., 2023), which aim to accelerate sampling while maintaining generation quality. Recent methods like Rectified Flow (Liu et al., 2022) and Flow Matching (Lipman et al., 2023) employ training objectives inspired by optimal transport to model continuous vector fields that map data to target distributions, eliminating the discrete formulation of diffusion models. Rectified Flow mitigates numerical issues in training by applying flow regularization, and Flow Matching offers efficient sampling with fewer discretization artifacts, making them promising alternatives to diffusion for high-quality generation. Apart from diffusion models, Parti (Yu et al., 2022) and MARS (He et al., 2024) showcased the potential of autoregressive models for image generation, and the Muse architecture (Chang et al., 2023) introduced masked image generation approach using transformers. Scaling Diffusion Models: Diffusion modeling has shown impressive scaling behaviors in terms of data, model size, and compute. Latent Diffusion Models (Rombach et al., 2022) first showed that training with large-scale web datasets can achieve high quality image generation results with U-Net backbone. DiT (Peebles & Xie, 2023) explored scaling diffusion models with the transformer architecture, presenting desirable scaling properties for class-conditional image generation. Later, Li et al.(Li et al., 2024) studied alignment scaling laws of text-to-image diffusion models. Recently, Fei et al.(Fei et al., 2024a) trained mixture-of-experts DiT models up to 16B parameters, achieving high-quality image generation results. Upcycling is another way to scale transformer models. Komatsuzaki et al. (Komatsuzaki et al., 2022) used upcycling to convert dense transformerbased language model to mixture-of-experts model without pre-training from scratch. Similarly, EC-DiTSun et al. (2024) explores how to exploit heterogeneous compute allocation in mixture-ofexperts training for DiT models through expert-choice routing and learning to adaptively optimize the compute allocated to specific text-image data samples. Diffusion Models for Perception Tasks: Diffusion models have also been used for various downstream visual tasks such as depth estimation (Ji et al., 2023; Duan et al., 2023; Saxena et al., 2023; 2024; Zhao et al., 2023). More recently, Marigold (Ke et al., 2024) and GeoWizard (Fu et al., 2024) displayed impressive results by repurposing pre-trained diffusion models for monocular depth estimation. Diffusion models with few modifications are used for semantic segmentation for categorical distributions (Hoogeboom et al., 2021; Brempong et al., 2022; Tan et al., 2022; Amit et al., 2021; Baranchuk et al., 2021; Wolleb et al., 2022), instance segmentation (Gu et al., 2024), and panoptic segmentation (Chen et al., 2023). Diffusion models are also used for optical flow (Luo et al., 2024; Saxena et al., 2024) and 3D understanding (Liu et al., 2023; Jain et al., 2022; Poole et al., 2022; Wang et al., 2023; Watson et al., 2022)."
        },
        {
            "title": "3 GENERATIVE PRE-TRAINING",
            "content": "We first explore how to efficiently scale diffusion model pre-training. We pre-train diffusion models for class-conditional image generation using diffusion transformer (DiT) backbone and follow the original model training recipe (Peebles & Xie, 2023). Starting with target RGB image Ruu3, where the resolution of the image is u, our pretrained, frozen Stable Diffusion variational autoencoder (Rombach et al., 2022) compresses the target to latent z0 Rww4, where = u/8. Gaussian noise is added at sampled time steps to obtain noisy target latent. Noisy samples are generated as: zt = αt z0 + 1 αt ϵt (1) s=1(1 βs), with {β1, . . . , βT } as the variance schedule of process. for timestep t. The noise is distributed as ϵ (0, I), Uniform(T ), with = 1000 and αt := (cid:81)t In the denoising process, the class-conditional DiT fθ(), parameterized by learned parameters θ, gradually removes noise from zt to obtain zt1. The parameters θ are updated by noising z0 with sampled noise ϵ at random timestep t, computing the noise estimate, and optimizing the mean squared loss between the generated noise and estimated noise in an batch size sample. We formally represent this as the following minimization problem: θ = arg min θ Lθ(zt, ϵi) = arg min θ 1 (cid:88) (ϵi ˆϵi)2, i=1 (2) where θ are the DiT learned parameters and ˆϵi is the DiT noise prediction for sample i. 3.1 MODEL SIZE We pre-train six different dense DiT models as in Table 1, increasing model size by varying the number of layers and hidden dimension size. We use Imagenet-1K (Russakovsky et al., 2015) as our pre-training dataset and train all models for 400k iterations with fixed learning rate of 1e-4 and batch size of 256. Fig. 2 shows that larger models converge to lower loss with clear power law scaling behavior. We show the train loss as function of compute (in MACs), and our predictions indicate power law relationship of L(C) = 0.23 0.0098. Our pre-training experiments display the ease of scaling DiT with small training dataset, which translates directly to efficiently scaling downstream model performance. 3.2 MIXTURE OF EXPERTS Figure 2: Scaling at Model Size: For generative pre-training of DiT models, we observe clear power law scaling behavior as we increase the model size. We also pre-train Sparse Mixture of Experts (MoE) models (Shazeer et al., 2017), following the S/2 and L/2 model configurations in (Fei et al., 2024b). We use three different MoE configurations listed in Table 2, scaling the total parameter count by increasing hidden size, number of experts, layers, and attention heads. Each MoE block activates the top-2 experts per token and has shared expert that is used by all tokens. To alleviate issues with expert balance, we use the proposed expert balance loss function from (Fei et al., 2024b) which distributes the load across experts more efficiently. Sparse MoE pre-training allows for higher parameter count while increasing throughput, making it more compute efficient than training dense DiT model of the same size. We train our DiT-MoE models with the same training recipe as the dense DiT model using ImageNet-1K. Our approach enables training DiT-MoE models to increase model capacity without increasing compute usage by another order of magnitude, which would be required to train dense models of similar sizes. 3 Model Params Dimension Heads Layers a1 a2 a3 a4 a5 a6 14.8M 77.2M 215M 458M 1.2B 1.9B 256 512 768 1024 1536 1792 16 16 16 16 16 12 16 20 24 28 32 Model Active / Total Dim Heads Layers S/2-8E2A 71M / 199M 384 S/2-16E2A 71M / 369M 384 L/2-8E2A 1.0B / 2.8B 1024 6 6 16 12 12 Table 1: Dense DiT Models: We scale dense DiT model size by increasing hidden dimension and number of layers linearly while keeping number of heads constant following (Yang et al., 2022; Touvron et al., 2023; ?). Table 2: MoE DiT Models: We scale the MoE DiT models by increasing dimension size, number attention heads, layers, and experts following (Fei et al., 2024b)."
        },
        {
            "title": "4 FINE-TUNING FOR PERCEPTUAL TASKS",
            "content": "In this section, we explore how to scale the fine-tuning of the pre-trained DiT models to maximize performance on downstream perception tasks. During fine-tuning, we utilize the image-to-image diffusion process from (Ke et al., 2024) and (Brooks et al., 2023) as our training recipe. We pose all our visual tasks as conditional denoising diffusion generation. Give an RGB image Ruu3 and its pair ground truth image Ruu3, we first project them to the latent space, i0 Rww4 and d0 Rww4, respectively. We only add noise to the ground truth latent to obtain dt and concatenate it with the RGB latent which results in tensor zt = {i0, dt}. The first convolutional layer of the DiT model is modified to match the doubled number of input channels, and its values are reduced by half to make sure the predictions are the same if the inputs are just RGB images (Ke et al., 2024). Finally, we perform diffusion training by denoising the ground truth image. We ablate several fine-tuning compute scaling techniques on the monocular depth estimation task and report Absolute Relative error and Delta1 error. We transfer the best configurations from the depth estimation ablation study to fine-tune for other visual perception tasks. 4.1 EFFECT OF MODEL SIZE We fine-tune the pre-trained a1-a6 dense models on the depth estimation task to study the effect of model size. We scale model size as shown in as described in Section 3.1. Fig. 3 shows that larger dense DiT models predictably converge to lower fine-tuning loss, presenting clear power law scaling behavior. We plot the train loss and validation metrics as function of compute (in MACs). Our fine-tuned model predictions show power law relationship in both depth Absolute Relative error and depth Delta1 error. These experiments provide strong signal on how model performance will scale as we increase fine-tuning compute by scaling model size. Figure 3: Effect of Model Size: We fine-tune a1-a6 models on the Hypersim dataset for 30K iterations with an exponential decay learning rate schedule from 3e-5 to 3e-7. We observe strong correlation between the fine-tuning loss scaling law and validation metric scaling laws."
        },
        {
            "title": "4.2 EFFECT OF PRE-TRAINING COMPUTE",
            "content": "We also investigate the behavior of fine-tuning as we scale the number of pre-training steps for the DiT backbone. We train four models with the a4 configuration using varied number of pre-training steps, keeping all other hyperparameters constant. We then fine-tune these four models on the same depth estimation dataset.Fig. 4 displays the power law scaling behavior of the validation metrics for depth estimation as we increase DiT pre-training steps. Our experiments show that having stronger pre-trained representations can be helpful when scaling fine-tuning compute. Figure 4: Effect of Scaling Model Pre-training Compute on Depth Estimation: (a) Depth Absolute Relative Error vs. MACs. (b) Depth Delta1 Error vs. MACs. We pre-train four a4 models with 60K, 80K, 100K, and 120K steps. These models are then fine-tuned for 30K steps on the Hypersim depth estimation dataset. We observe clear power law as we increase the DiT pre-training compute across depth estimation validation metrics. 4.3 EFFECT OF IMAGE RESOLUTION The sequence length of each image also affects the total compute spent during training. For each forward pass, we can scale the amount of compute used by simply increasing the resolution of the image, which will increase the number of tokens in the image embedding. By increasing the number of tokens, we can increase the amount of information the model can learn from at training time to build stronger internal representations, which can in turn improve downstream performance. We use dense DiT-XL models with resolutions of 256 256 and 512 512 from (Peebles & Xie, 2023) and we pre-train DiT-MoE L/2-8E2A models with 256 256 and 512 512 resolutions following the recipe in (Fei et al., 2024b). We then fine-tune each of these models with the corresponding resolution for the depth estimation task. Fig. 5 displays that increasing image resolution to scale fine-tuning compute can provide significant gains on downstream depth estimation performance. Figure 5: Effect of Image Resolution. We fine-tune DiT-XL and DiT-MoE L/2 models with resolutions of 256 256 and 512 512. We observe power law when increasing image resolution during training. By scaling the number of tokens per image by 4, we achieve strong performance on Depth Absolute Error, displaying the effect of increasing total dataset tokens for dense visual perception tasks such as depth estimation."
        },
        {
            "title": "4.4 EFFECT OF UPCYCLING",
            "content": "Sparse MoE models are efficient options for increasing the capacity of model, but pre-training an MoE model from scratch can be expensive. One way to alleviate this issue is Sparse MoE Upcycling (Komatsuzaki et al., 2023). Upcycling converts dense transformer checkpoint to an MoE model by copying the MLP layer in each transformer block times, where is the number of experts, and adding learnable router module that sends each token to the top-k selected experts. The outputs of the selected experts are then combined in weighted sum at the end of each MoE block. We upcycle various dense DiT models after they are fine-tuned for depth estimation and then continue fine-tuning the upcycled model. Fig. 6 displays the scaling laws for upcycling, providing an average improvement of 5.3% on Absolute Relative Error and 8.6% on Delta1 error. Figure 6: Effect of Upcycling. We upcycle a2, a3, and a4 models fine-tuned for depth estimation with varying number of total/active model experts. We continue fine-tuning each upcycled model for 15K iterations on the Hypersim depth estimation dataset. We observe clear scaling law in the validation metrics as we increase fine-tuning compute with upcycling. The upcycled models can also achieve equivalent or superior performance to our dense a5 and a6 checkpoints, each of which utilize more compute during pre-training and fine-tuning. Increasing the total model experts and total active experts can also improve the downstream performance."
        },
        {
            "title": "5 SCALING TEST-TIME COMPUTE",
            "content": "Scaling test-time compute has been explored for autoregressive Large Language Models (LLMs) to improve performance on long-horizon reasoning tasks (Brown et al., 2024; Snell et al., 2024; El-Refai et al., 2024; OpenAI, 2024). In this section, we show how to reliably improve diffusion model performance for perceptual tasks by scaling test-time compute. We summarize our approach in Fig. 7. We use the Stable-Diffusion VAE to encode the input image into latent space (Rombach et al., 2022). Then, we sample target noise latent from standard Gaussian distribution, which is iteratively denoised with DDIM (Song et al., 2021) to generate the downstream prediction. Figure 7: Inference Scaling: Diffusion models by design allow efficient scaling of test-time compute. First, we can simply increase the number of denoising steps to increase the compute spent at inference. Since we are estimating deterministic outputs, we can then initialize multiple noise latents and ensemble the predictions to get better estimation. Finally, we can also reallocate our test-time compute budget for low and high frequency denoising by modifying the noise variance schedule."
        },
        {
            "title": "5.1 EFFECT OF SCALING INFERENCE STEPS",
            "content": "The most natural way of scaling diffusion inference is by increasing denoising steps. Since the model is trained to denoise the input at various timesteps, we can scale the number of diffusion denoising steps at test-time to produce finer, more accurate predictions. This coarse-to-fine denoising paradigm is also reflected in the generative case, and we can take advantage of it for the discriminative case by increasing the number of denoising steps. In Fig. 8, we observe that increasing the total test-time compute by simply increasing the number of diffusion sampling steps provides substantial gains in depth estimation performance. Figure 8: Effect of Number of Sampling Steps. (a) Delta1 Error vs. Number of Steps. (b) Absolute Relative Error vs. Number of Steps. For each model, we sample for [1, 2, 5, 10, 20, 50, 100] steps with the DDIM sampler. We show clear power law scaling behavior in (a) and (b), displaying the effectiveness of scaling test-time compute by increasing the number of diffusion sampling steps. 5.2 EFFECT OF TEST TIME ENSEMBLING We also explore scaling inference compute with test-time ensembling. We exploit the fact that denoising different noise latents will generate different downstream predictions. In test-time ensembling, we compute forward passes for each input sample and reduce the outputs through one of two methods. The first technique is naive ensembling where we use the pixel-wise median across all outputs as the prediction. The second technique presented in Marigold (Ke et al., 2024) is median compilation, where we collect predictions { ˆd1, . . . , ˆdN } that are affine-invariant, jointly estimate scale and shift parameters ˆsi and ˆti, and minimize the distances between each pair of scaled and shifted predictions ( ˆd j) where ˆd = ˆd ˆs + ˆt. For each optimization step, we take the pixelˆd wise median m(x, y) = median( (x, y)) to compute the merged depth m. Since it requires no ground truth, we scale ensembling by increasing to utilize more test-time compute. ˆd 1(x, y), . . . , i, ˆd Figure 9: Effect of Test Time Ensembling. (a) Delta1 Error vs. Number of Forward Passes. (b) Absolute Relative Error vs. Number of Forward Passes. Ensembling multiple predictions from distinct noise initializations displays power law scaling behavior. We apply test-time ensembling values of [1, 2, 5, 10, 15, 20]."
        },
        {
            "title": "5.3 EFFECT OF NOISE VARIANCE SCHEDULE",
            "content": "We can also scale test-time compute by increasing compute usage at different points of the denoising process. In diffusion noise schedulers, we can define schedule for the variance of the Gaussian noise applied to the image over the total diffusion timesteps . Tuning the noise variance schedule allows for reorganizing compute by allocating more compute to denoising steps earlier or later in the noise schedule. We experiment with three different noise level settings for DDIM: linear, scaled linear, and cosine. Cosine scheduling from (Nichol & Dhariwal, 2021) linearly declines from the middle of the corruption process, ensuring the image is not corrupted too quickly as in linear schedules. Fig. 10 shows that the cosine noise variance schedule outperforms linear schedules for DDIM on the depth estimation task under fixed compute budget. Figure 10: Effect of Noise Variance (Beta) Schedule. We fine-tune a4 models with three different beta schedules: linear, scaled linear, cosine. Reallocating compute with the cosine schedule to spend more time denoising at earlier timesteps significantly improved Delta1 and Absolute Relative Error rates."
        },
        {
            "title": "6 PUTTING IT ALL TOGETHER",
            "content": "Using the lessons from our scaling experiments on depth estimation, we train diffusion models for optical flow prediction and amodal segmentation. We show that using diffusion models while considering efficient methods to scale training and test-time compute can provide substantial performance gains on visual perception tasks, achieving improved or similar performance as current state-of-the-art techniques. Our experiments provide insight on how to efficiently apply diffusion models for these visual perception tasks under limited compute budgets. Finally, we train unified expert model, capable of performing all three visual perception tasks previously mentioned, displaying the generalizability of our method. Our results prove the effectiveness of our training and test-time scaling strategies, removing the need to use pre-trained models trained on internet-scale datasets to enable high-quality visual perception in diffusion models. Fig. 11 displays the predicted samples from our models. 6.1 DEPTH ESTIMATION We combine our findings from the ablation studies on depth estimation to create model with the best training and inference configurations. We train DiT-XL model from (Peebles & Xie, 2023) on depth estimation data from Hypersim for 30K steps with batch size of 1024, resolution of 512 512, and learning rate exponentially decaying from 1.2e-4 to 1.2e-6. We use median compilation ensembling with cosine noise variance schedule. From our scaling experiments, we found the optimal configuration for inference to be 200 denoising steps with = 5 samples for ensembling. As shown in Table 3, our model achieves the same validation performance as Marigold on the Hypersim dataset and better performance on the ETH3D test set while being trained with lower resolution images and approximately three orders of magnitude less pre-training data and compute. 8 Method DiverseDepth MiDaS LeReS Omnidata HDN DPT Marigold Ours Hypersim NYUv2 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 ScanNet ETH3D DIODE 13.5 13. 87.5 87.6 22.8 18.4 17.1 16.6 12.1 7.8 6.5 4.8 69.4 75.2 77.7 77.8 83.3 94.6 96.0 97.8 11.7 11.1 9.0 7.4 6.9 9.8 5.5 6.8 87.5 88.5 91.6 94.5 94.8 90.3 96.4 95.0 10.9 12.1 9.1 7.5 8.0 8.2 6.4 7. 88.2 84.6 91.7 93.6 93.9 93.4 95.1 93.7 37.6 33.2 27.1 33.9 24.6 18.2 30.8 31.0 63.1 71.5 76.6 74.2 78.0 75.8 77.3 77.2 Table 3: Depth Estimation Performance Comparison on Multiple Datasets. We achieve stateof-the-art performance on the ETH3D dataset and competitive performance across all other benchmarks. Notably, we closely match the performance of Marigold across all datasets with significantly less training compute. 6.2 OPTICAL FLOW PREDICTION Optical flow estimation involves predicting the motion of objects between consecutive frames in video, represented as dense vector field indicating pixel-wise displacement. We use similar configuration as the depth estimation model for optical flow training. We train DiT-XL model on the FlyingChairs dataset for 40K steps with batch size of 1024, resolution of 512 512, and learning rate exponentially decaying from 1.2e-4 to 1.2e-6. We compare our models performance with other specialized optical flow prediction techniques in Table 4. Method FlyingChairs EPE DeepFlow FlowNetS FlowNetS+v FlowNetS+ft FlowNetS+ft+v FlowNetC FlowNetC+v FlowNetC+ft FlowNetC+ft+v Ours (w/o ensembling) Ours (w/ ensembling) 3.53 2.71 2.86 3.04 3.03 2.19 2.61 2.27 2.67 3.45 3.08 Table 4: Optical Flow Comparison with Specialized Techniques. We evaluate our optical flow model on the FlyingChairs validation set. Our model achieves similar endpoint error as specialized methods, including DeepFlow (Weinzaepfel et al., 2013) and FlowNet (Fischer et al., 2015). We train with significantly less data compared to other specialized methods, which use several optical flow datasets. We generate predictions with and without test-time ensembling."
        },
        {
            "title": "6.3 AMODAL SEGMENTATION",
            "content": "Amodal segmentation is the process of predicting the complete shape and extent of objects in an image, including the portions that are occluded or not directly visible, which can require higher-level reasoning for complex scenes. We fine-tune DiT-XL model on the pix2gestalt dataset (Ozguroglu et al., 2024) for 6K steps with batch size of 4096, resolution of 256 256, and learning rate exponentially decaying from 1.2e-4 to 1.2e-6. We compare our model with other methods in Table 5. Method COCO-A P2G MP3D PCNet PCNet-Sup SAM SD-XL Inpainting pix2gestalt Ours 81.35 82.53 67.21 76.52 82.9 82.9 88.7 88. 61.5 63.9 Table 5: Amodal Segmentation Performance (mIOU) Comparison Across Different Datasets. This table compares mIOU performance across COCO-A, Pix2Gestalt, and MP3D datasets, showing the effectiveness of various methods. Our method is able to achieve competitive performance across all tasks, while training only on Pix2Gestalt."
        },
        {
            "title": "6.4 ONE MODEL FOR ALL",
            "content": "We train unified DiT-XL model for each of the different tasks. We train this model on mixed dataset consisting of all three tasks. To train this generalist model, we modify the DiT-XL architecture by replacing the patch embedding layer with separate PatchEmbedRouter module, which routes each VAE embedding to specific input convolutional layer based perception task. This ensures the DiT-XL model is able to distinguish between the task-specific embeddings during finetuning. We use similar training recipe as the previous experiments, using images with 512 512 resolution and learning rate exponentially decaying from 1.2e-4 to 1.2e-6. Then, we upcycle the fine-tuned DiT-XL checkpoint to an DiT-XL-8E2A model, and continue fine-tuning for another 4K iterations. We display the generated predictions in Fig. 11 which exemplify the generalizability and transferability of our scaling techniques across variety of perception tasks. Figure 11: Depth Estimation, Optical Flow Estimation, and Amodal Segmentation Examples: Each row showcases results from our models for different tasks. (a) Depth estimation, with relative scale and shift. (b) Optical flow, with scale and shift. (c) Amodal segmentation, where the model sees an RGB image and segmentation of the occluded object; the task is to predict the amodal image."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In our work, we examine the scaling properties of diffusion models for visual perception tasks. We explore various approaches to scale diffusion training, including increasing model size, mixtureof-experts models, increasing image resolution, and upcycling. We also efficiently scale test-time compute by exploiting the iterative nature of diffusion, which significantly improves downstream performance. Our experiments provide strong evidence of scaling, uncovering power laws across various training and inference scaling techniques. We hope to inspire future work in scaling training and test-time compute for iterative generative paradigms such as diffusion for perception tasks."
        },
        {
            "title": "8 ACKNOWLEDGMENTS",
            "content": "We thank Alexei Efros for helpful discussions. We also thank Xinlei Chen, Amil Dravid, Neerja Thakkar for their valuable feedback on the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf. Segdiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390, 2021. Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Labelefficient semantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. Emmanuel Asiedu Brempong, Simon Kornblith, Ting Chen, Niki Parmar, Matthias Minderer, and Mohammad Norouzi. Denoising pretraining for semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 41754186, 2022. Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023. URL https://arxiv.org/abs/2211.09800. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers, 2023. URL https:// arxiv.org/abs/2301.00704. Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, and David Fleet. generalist framework for panoptic segmentation of images and videos. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 909919, 2023. Yiqun Duan, Xianda Guo, and Zheng Zhu. Diffusiondepth: Diffusion denoising approach for monocular depth estimation. arXiv preprint arXiv:2303.05021, 2023. Karim El-Refai, Zeeshan Patel, Jonathan Pei, and Tianle Li. Swag: Storytelling with action guidance. 2024. Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint arXiv:2407.11633, 2024a. Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Jusnshi Huang. Scaling diffusion transformers to 16 billion parameters. arXiv preprint, 2024b. Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Hausser, Caner Hazırbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks, 2015. URL https://arxiv.org/abs/1504.06852. Xiao Fu, Wei Yin, Mu Hu, Kaixuan Wang, Yuexin Ma, Ping Tan, Shaojie Shen, Dahua Lin, and Xiaoxiao Long. Geowizard: Unleashing the diffusion priors for 3d geometry estimation from single image, 2024. URL https://arxiv.org/abs/2403.12013. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Zhangxuan Gu, Haoxing Chen, and Zhuoer Xu. Diffusioninst: Diffusion model for instance segmentation. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 27302734. IEEE, 2024. 11 Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, Ziwei Huang, LeiLei Gan, and Hao Jiang. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis, 2024. URL https://arxiv. org/abs/2407.07614. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:1245412465, 2021. Ajay Jain, Ben Mildenhall, Jonathan Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 867876, 2022. Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp: Diffusion model for dense visual prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2174121752, 2023. Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94929502, 2024. Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055, 2022. Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-experts from dense checkpoints, 2023. URL https://arxiv.org/abs/2212. 05055. Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-based text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94009409, 2024. Lipman et al. Flow matching: Symmetrizing optimal transport and generative modeling. arXiv preprint arXiv:2301.13003, 2023. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 92989309, 2023. Liu et al. Rectified flow: unified approach for free-form generative models. arXiv preprint arXiv:2209.07953, 2022. Ao Luo, Xin Li, Fan Yang, Jiangyu Liu, Haoqiang Fan, and Shuaicheng Liu. Flowdiffuser: Advancing optical flow estimation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1916719176, 2024. Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021. URL https://arxiv.org/abs/2102.09672. OpenAI. Learning to reason with llms. https://openai.com/index/ learning-to-reason-with-llms/, September 2024. Ege Ozguroglu, Ruoshi Liu, Dıdac Surs, Dian Chen, Achal Dave, Pavel Tokmakov, and Carl Vondrick. pix2gestalt: Amodal segmentation by synthesizing wholes. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 12 William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International conference on machine learning, pp. 15301538. PMLR, 2015. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li FeiFei. Imagenet large scale visual recognition challenge, 2015. URL https://arxiv.org/ abs/1409.0575. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023. Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. Advances in Neural Information Processing Systems, 36, 2024. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017. URL https://arxiv.org/abs/1701.06538. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/ abs/2408.03314. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=St1giarCHLP. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, and Nan Du. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing, 2024. URL https://arxiv.org/abs/2410.02098. Haoru Tan, Sitong Wu, and Jimin Pi. Semantic diffusion network for semantic segmentation. Advances in Neural Information Processing Systems, 35:87028716, 2022. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, 13 Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural netIn Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd works. International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 17471756, New York, New York, USA, 2022 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/oord16.html. Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1261912629, 2023. Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628, 2022. Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepflow: Large displacement optical flow with deep matching. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2013. Julia Wolleb, Robin Sandkuhler, Florentin Bieder, Philippe Valmaggia, and Philippe Cattin. Diffusion models for implicit image segmentation ensembles. In International Conference on Medical Imaging with Deep Learning, pp. 13361348. PMLR, 2022. Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer, 2022. URL https://arxiv.org/abs/ 2203.03466. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing textto-image diffusion models for visual perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 57295739, 2023."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}