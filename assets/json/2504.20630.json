{
    "paper_title": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting",
    "authors": [
        "Yu Zhang",
        "Wenxiang Guo",
        "Changhao Pan",
        "Zhiyuan Zhu",
        "Tao Jin",
        "Zhou Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo."
        },
        {
            "title": "Start",
            "content": "ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Yu Zhang * 1 Wenxiang Guo * 1 Changhao Pan * 1 Zhiyuan Zhu * 1 Tao Jin 1 Zhou Zhao"
        },
        {
            "title": "Abstract",
            "content": "Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo. 5 2 0 2 9 2 ] . e [ 1 0 3 6 0 2 . 4 0 5 2 : r 1. Introduction Binaural hearing provides localization cues through sound fields, enhancing the human spatial perception of the environment. This capability is critical for applications requiring deep immersion, like movies, VR, and AR (Fitria, *Equal contribution Correspondence to: Zhou Zhao <zhaozhou@zju.edu.cn>, Tao Jin <jintao@zju.edu.cn>. 1Zhejiang Univeristy. Preprint. Under review. Figure 1. ISDrama uses scripts as content, prompt audio to guide timbre, and spatial information from multimodal prompts, to create continuous multi-speaker binaural speech with dramatic prosody. 2023). Compared to non-linguistic binaural audio (Sun et al., 2024), generating binaural speech is more challenging yet promising. Specifically, based on multimodal prompts from diverse contexts, generating continuous multi-speaker binaural speech with dramatic prosody can create immersive spatial drama. This new task enhances storytelling, offering an immersive emotional and spatial engagement, and an integration of virtual and reality. With deep learning advancements, progress has been made in synthesizing speech with prosody modeling (Ju et al., 2024; Du et al., 2024) and generating binaural audio from monaural audio based on input multimodal prompts (Li et al., 2024d; Parida et al., 2022). Cascading these methods to generate continuous multi-speaker binaural speech with prosody modeling seems to be viable approach. However, this cascading method will disrupt the integration of prosody and spatial modeling, leading to mismatches and error accumulation. This highlights the need for unified multimodal immersive spatial drama generation framework. To date, one-stage multimodal immersive spatial drama generation remains an unexplored domain, focusing on creating continuous multi-speaker binaural speech with dramatic prosody. As shown in Figure 1, this task requires inputting ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting scripts as content and prompt audio to guide timbre, along with extracting spatial information in pose (including position, orientation, and movement speed) and scene from multimodal prompts (e.g., silent video, textual prompts, geometric poses) to cover wider range of applications. Therefore, this task extends beyond video dubbing, enabling precise and flexible spatial control. Additionally, drama demands prosodic expressiveness that far exceeds normal speech. Not only does dramatic prosody require learning accent and articulation from prompt audio, but it also integrates semantics to enhance the modeling of semantically aligned emotion and rhythm. Currently, multimodal immersive spatial drama generation encounters three major challenges: 1) Lack of high-quality annotated recorded data. Simulated data fails to capture the complex, dramatic prosody and the precise effects of real-world spatial scenes, positions, and orientations. While some datasets (Richard et al., 2021) use binaural devices to simulate human interaural phase difference (IPD) and interaural level difference (ILD), they suffer from limitations in scale, dramatic prosody, and multimodal prompts. 2) Challenges in extracting unified pose representations from multimodal prompts. Silent video, geometric pose, and textual prompts provide spatial information, including positions, orientations, and movement speed for various scenarios. While some methods extract positional information from visual or positional inputs (Leng et al., 2022), they cannot learn unified pose representations across more diverse scenarios. 3) Difficulty in one-stage modeling dramatic prosody and spatial immersion. Existing monaural speech models (Du et al., 2024) struggle to simultaneously model dramatic prosody, which requires semantic alignment in the temporal-frequency domain, and spatial information, which spans the spatial-temporal dimensions. To address these challenges, we first introduce MRSDrama, the first multimodal recorded spatial drama dataset, comprising binaural drama audios, scripts, videos, geometric poses, and textual prompts. The dataset includes 97.82 hours of speech data recorded by 21 speakers across three scenes. Next, we propose ISDrama, the first immersive spatial drama generation model based on multimodal prompting. ISDrama generates high-quality, continuous, multi-speaker binaural speech with dramatic prosody and spatial immersion, driven by multimodal prompts. To extract unified pose representation from multimodal prompts, we design the Multimodal Pose Encoder, contrastive learning-based framework that encodes not only position and head orientation but also radial velocity, accounting for the Doppler effect caused by moving speakers. Meanwhile, we develop the Immersive Drama Transformer, flow-based MambaTransformer model capable of generating immersive spatial drama effectively and stably. Within this model, we introduce Drama-MOE (Mixture of Experts), which selects the appropriate experts to enhance prosodic expressiveness and improve pose control. Then, we adopt context-consistent classifier-free guidance (CFG) strategy to ensure the quality and coherence of complete drama generation. We evaluate ISDrama on quality, speaker similarity, prosodic expressiveness, pose, angle, distance, IPD, and ILD. Experimental results show that ISDrama outperforms baseline models on both objective and subjective metrics, demonstrating its ability to generate immersive spatial drama that adheres to physical principles while exhibiting rich prosodic variation. Overall, our main contributions can be summarized as: We develop MRSDrama, the first multimodal recorded spatial drama dataset, accompanying videos, scripts, alignments, positions, and textual prompts. We introduce ISDrama, the first immersive spatial drama generation model through multimodal prompting. We design the Multimodal Pose Encoder to extract pose from multimodal inputs, while the Immersive Drama Transformer produces binaural speech. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. 2. Related Work Audio Spatialization with Multimodal Cues. In recent years, the development of deep learning has significantly advanced the exploration of spatial audio. Substantial progress has been made in sound source localization for binaural audio (Krause et al., 2023; Yang & Zheng, 2022; Shimada et al., 2022; GarcÄ±a-Barrios et al., 2022). At the same time, the rise of multimodal research has spurred innovations in spatial audio synthesis. Mono2Binaural (Gao & Grauman, 2019) devises deep convolutional neural network that leverages visual cues to convert monaural audio into binaural audio. Sep-Stereo (Zhou et al., 2020) enhances stereo audio by integrating audio-visual features. Garg et al. (2021) proposes multi-task framework that learns geometry-aware features for generating binaural audio. CLUP (Li et al., 2024d) jointly learns to localize visually sounding objects and generate binaural audio. Beyond visual modalities, BinauralGrad (Leng et al., 2022) employs positional information for monaural-to-stereo audio generation, while scene depth maps have also been utilized (Parida et al., 2022). However, these methods rely on monaural audio input, limiting their applicability for complex and flexible generation tasks. Recently, SpatialSonic (Sun et al., 2024) employs Diffusion Transformer to generate binaural audio from both text and image prompts. VISAGE (Kim et al., 2025) leverages CLIP visual features to generate first-order ambisonics directly from silent video frames. However, these tasks focus on generating short audio clips lacking actual linguistic information. ImmerseDiffusion (Heydari et al., 2025) 2 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Table 1. Comparison of current open-source recorded spatial speech datasets. Multi-channel speech does not account for the intricate structures of human ears, whereas binaural speech incorporates natural IPD and ILD, ensuring realistic and immersive hearing perception. Dataset Hours Speakers Audio Paired Type SWEET-HOME (VACHER ET AL., 2014) DIRHA (RAVANELLI ET AL., 2015) VOICE-HOME (BERTIN ET AL., 2016) BINAURAL(RICHARD ET AL., 2021) MRSDRAMA (OURS) 47.3 11 2.5 2 97.82 71 24 3 8 21 MULTI-CHANNEL MULTI-CHANNEL MULTI-CHANNEL BINAURAL TEXT, PROMPT TEXT, POSE TEXT, POSE, PROMPT POSE BINAURAL TEXT, VIDEO, POSE, PROMPT Figure 2. The pipeline of MRSDrama data collection. Human double-checks exist in each process. Notably, all data are desensitized. proposes an end-to-end generative audio model conditioned on textual prompts of spatial, temporal, and environmental factors. However, it lacks support for multimodal prompts, binaural scenes, and expressive prosody modeling. Generating continuous linguistic speech with unified scene and pose information extracted from more diverse prompt modalities remains an unresolved challenge. Speech Synthesis with Prosody Modeling. Prosody modeling aims to synthesize target speech with natural or emotional prosody, which is essential to generate expressive speech in controlled manner and typically involves transferring prosody from prompt audio (Wagner & Watson, 2010). Skerry-Ryan et al. (2018) is the first to integrate prosody reference encoder into Tacotron-based TTS system, enabling the transfer of prosody for similar-text speech. Attentron (Choi et al., 2020) introduces an attention mechanism to extract prosody from reference samples. ZSM-SS (Kumar et al., 2021) proposes Transformer-based architecture with an external speaker encoder using wav2vec 2.0 (Baevski et al., 2020). Li et al. (2021) incorporates global utterance-level and local phoneme-level prosodic features in target speech. Daft-Exprt (ZaÄ±di et al., 2021) employs gradient reversal layer to enhance target speaker fidelity in prosody transfer. Generspeech (Huang et al., 2022) incorporates the attention mechanism to capture multi-level prosody. HierSpeech++ (Lee et al., 2023) generates F0 representation based on text representations and prosody prompts, while StyleTTS 2 (Li et al., 2024c) predicts pitch and energy based on prosody predictor (Li et al., 2022). Mega-TTS 2 (Jiang et al., 2024) employs clustering VQ for prosody encoding and Prosody Latent LM for predicting. NaturalSpeech 3 (Ju et al., 2024) employs factorized vector quantization to disentangle prosody. CosyVoice (Du et al., 2024) incorporates x-vectors into an LLM to disentangle and model prosody. FireRedTTS (Guo et al., 2024) employs semantic-aware speech tokenizer to encode speech style and conduct style control. However, these models only focus on monaural speech synthesis and cannot simultaneously model dramatic prosody, which requires semantic alignment in the temporalfrequency domain, and spatial information, which spans the spatial-temporal dimensions. 3. Dataset: MRSDrama Due to the high costs associated with multimodal spatial speech recording and annotation, as shown in Table 1, existing open-source recorded datasets are inadequate for supporting the generation of multimodal immersive spatial drama. Meanwhile, simulated data cannot reflect the nuanced prosody or subtle acoustic effects of real-world spatial environments. Therefore, we propose MRSDrama, the first multimodal recorded spatial drama dataset containing binaural drama audios, videos, scripts, geometric poses, and textual prompts. Our dataset consists of 97.82 hours of speech data recorded by 21 speakers across 3 scenes. Figure 2 shows the construction pipeline. Recording. To construct ISDrama, we first source Chinese translations of renowned theatrical scripts through au3 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Figure 3. The statistics of MRSDrama. The position distribution is plotted on the plane defined by the listeners forward direction and ears. thorized channels, including Hamlet, Waiting for Godot, Thunderstorm, Hiroshima Mon Amour, Offending the Audience, etc. After meticulous selection process, we enlist 21 expressive speakers, each provides consent to record for research purposes. Additionally, we select three scenes with varying sizes and acoustical effects to ensure diverse spatial effects. During the recording process, speakers are instructed to read the script expressively while remaining in character and occasionally moving at constant speed to create dynamic spatial effects. We use professional binaural recording equipment and sound cards to capture the audio, while synchronized video footage is recorded using cameras. All audio files are saved in WAV format with 48 kHz sampling rate, and videos are captured at 24 fps. Annotation. We separately annotate the audio and video components. 1) For Audio Annotation, We first perform denoising using FRCRN (Zhao et al., 2022). Subsequently, MFA (McAuliffe et al., 2017) is employed for coarse phoneme-to-audio alignment between the original script and audio. Chinese phonemes are extracted by pypinyin. Next, annotators are asked to use Praat (Boersma, 2001) to refine the rough alignment, focusing on correcting word and phoneme boundaries and addressing erroneous words based on auditory perception. 2) For Video Annotation, Speakers occasionally move between many fixed points in each scene. Annotators are asked to record speakers arrival times and point coordinates of each movement. They then measure these speakers head orientation and mouth height while standing and sitting in each frame, extracting 3D position coordinates and quaternion orientation to form frame-level sound source poses. Based on the annotated pose data, we then use GPT-4o (Achiam et al., 2023) to generate textual prompts for each actors line by combining the orientation, endpoint, direction, speed, and start time of each motion. Meanwhile, annotators also label the camera pose (position and orientation) and scene prompts, including descriptions about room sizes and acoustical effects. Post-Processing. To ensure data quality, we inspect the entire dataset, including the script, alignment, poses, and prompts. Next, we segment the 97.82 hours of speech into 47,958 segments based on speaker transitions of each drama and maximum duration setting of 16 seconds. MRSDrama is the largest recorded spatial speech corpus to date and the first spatial drama dataset with multimodal annotations. It features continuous multi-speaker binaural speech with dramatic prosody accompanying rich modalities, making it suitable for various tasks like binaural localization and drama generation. Figure 3 presents the statistics of MRSDrama, showcasing the diversity across theatrical scripts, speakers, phonemes, and positions. This highlights the datasets variety in content, timbre, and spatial information, demonstrating its potential for generalization. For more details about MRSDrama, please refer to Appendix B. 4. Method: ISDrama 4.1. Overview We aim to generate immersive spatial drama based on scripts, prompt audio for each speaker, and spatial information from multimodal prompts. Let ygt represent one of the binaural speech in the ground truth drama, and R280T represent the mel spectrogram, where mgt denotes the target length. Typically, we segment target drama based on speaker transitions in the script. As shown 4 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Figure 4. The architecture of ISDrama. In Figure (a), DP & LR means duration predictor and length regulator. In Figure (b), the ODE Solver, autoencoder decoder, and vocoder generate predicted audio from Gaussian noise with conditions during inference. In Figure (c), MPE is the Multimodal Pose Encoder, while IDT denotes the Immersive Drama Transformer. MPE predicts content duration and segments the inputs. Then IDL coherently generates the complete drama. In Figure (d), FAN denotes Fourier Analysis Networks. in Figure 4, since the autoencoder compresses mgt into Ëmgt, the generation process is Ëmpr = G(Ïµ C), where Ïµ is Gaussian noise and represents the conditions. includes the corresponding content c, scene s, pose from multimodal prompts, and specified prompt audio a. To encode unified pose embedding zp from multimodal prompts, we design the Multimodal Pose Encoder based on contrastive learning, accounting for the Doppler effect of moving speakers. In addition to the pose corresponding to ygt, the content is segmented from the complete drama script. The pronunciation and semantics of are encoded as zc. Next, zp and zc are fed into the Immersive Drama Transformer, along with scene information (a video frame or textual description) and prompt audio for timbre, to generate the predicted binaural speech ypr through spatial and prosodic modeling. Then, by combining different segments of ypr, as shown in Figure 4 (c), we can coherently generate the complete drama through context-consistent CFG strategy. 4.2. Multimodal Pose Encoder To support broader range of application scenarios, the generation of spatial drama must not only accommodate video dubbing but also offer flexibility in textual prompts and precise control over geometric poses, including positions, orientations, and movement speed. To address this, we design the Multimodal Pose Encoder, which predicts unified pose embedding zp from multimodal prompts. As shown in Figure 4(a), our model encodes three types of multimodal prompts and embeds them into unified space. The correct phase estimation is crucial for binaural audio (Richard et al., 2022). Therefore, for geometric pose, we not only encode the head orientation ori and 3D coordinates of the sound source relative to left and right ears, posl and posr but also add the radial relative velocity according to Doppler effect (Gill, 1965) for phase estimation (Liu et al., 2022a). Specifically, we calculate the 3D velocity vector of the moving sound source in the Cartesian coordinate system, then decompose into radial velocity components vradl and vradr in the spherical coordinate systems of left and right ears respectively by vrad = posv Ër, where 1 is the radial unit vector. Finally, we can encode and Ër concatenate ( posl, posr, ori, vradl, vradr) into zpgeo. pos To determine the target embedding length and considering the relationship between semantics, spoken speed, and motion, we encode the pronunciation and semantics of content and expand it with the predicted phoneme duration as zc. 5 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Since the input script is organized in speaker transitions, we can compute predicted phoneme durations to segment the length for each target speech as shown in Figure 4 (c). Then we employ Transformer to predict zptxt and zpvid, where zc serves as the input and leverages different modal conditions. For textual prompts, we use FLAN-T5 (Chung et al., 2024) to encode text as the condition. For silent video, we concatenate camera pose, mouth pixel sequences from Co-Tracker3 (Karaev et al., 2024), and embedding from CLIP (Radford et al., 2021) as the condition. After obtaining pose embeddings of these three modalities, we design three types of contrasts for contrastive learning, each focusing on different aspects of the pose to explore diverse physical and spatial features. These include dynamic features (mobility, movement speeds, and movement direction), postural features (posture and orientation), and positional features (distance and angle). We then use the contrastive objective (Radford et al., 2021) for training: p1,p2 = 1 2N (cid:88) (log + log i=1 exp(sim(zp2 j=1 exp(sim(zp2 i, zp1 (cid:80)N (cid:80)N exp(sim(zp1 j=1 exp(sim(zp1 i)/Ï ) ), i, zp1 j)/Ï ) i, zp2 i)/Ï ) i, zp j)/Ï ) pgeo,pvid + contras will be where sim( (1) ) denotes cosine similarity. The final total loss pgeo,ptxt + pvid,ptxt . After training, these three embeddings are aligned in the same R2HT , where despace. The resulting unified zp notes the hidden size, contains pose information, helping to model natural IPD and ILD of binaural speech. For more details, please refer to Appendix C.4. L 4.3. Immersive Drama Transfomer Flow-based Mamba-Transformer. Spatial information and prosody in binaural speech are related to pose, scene, emotion, and rhythm. Their interaction necessitates the simultaneous modeling of both temporal-frequency and spatial-temporal, which becomes more complex with long sequences. The flow matching method facilitates smooth transformations, promoting stable and rapid generation, while the Transformer architecture excels in sequence generation, making the flow-based Transformer particularly suitable for this task. Meanwhile, since Transformer tends to be computationally expensive for long sequences, we add Mamba blocks (Gu & Dao, 2023) at an Attention-to-Mamba ratio of 1:K to balance memory usage, efficient training, and the ability to handle long sequences. As shown in Figure 4 (b), we add Gaussian noise Ïµ to the autoencoder output Ëmgt to obtain xt at timestep t. We then add content embedding zc and pose embedding zp to xt and concatenate it with prompt audio embedding za. Therefore, using the self-attention mechanism and capability of Mamba blocks to capture long-range dependencies, Immersive Drama Transformer can effectively model content, pose, timbre, accent, and articulation. Scene information (e.g., video frame in silent video or textual description for other inputs) is encoded and processed using the cross-attention mechanism to simulate the rooms acoustic properties, such as the differences in acoustic effects caused by varying room sizes and acoustical effects. Notably, we use the first blocks output to predict binaural F0, providing extra supervision and additional input for subsequent blocks, helping to model dramatic prosody. Furthermore, we employ RMSNorm (Zhang & Sennrich, 2019) and design global adapters with AdaLN (Peebles & Xie, 2023) to ensure training stability and global consistency in timbre and scene. The final output vector field is trained with the flow-matching objective: low = Et,pt(xt) C; Î¸) vt(xt, where pt(xt) represents the distribution of xt at timestep t. For more details, please refer to Appendix and C.6. 2 , ( Ëmgt (2) Ïµ) Drama-MOE. To enhance prosodic expressiveness and pose control, we propose Drama-MOE (Mixture of Experts), selecting suitable experts based on various input conditions. As shown in Figure 4 (d), Drama-MOE consists of two expert groups, each focusing on dramatic prosody and spatial information. Prosodic MOE leverages prosody in aligned prompt audio embedding za and semantics in content embedding zc to select suitable experts for fine-grained dramatic prosody modeling, such as an expert specialized in happy, fast, high-pitched tone. za is aligned with the inputs by cross-attention model. The Spatial MOE conditions on za, adjusting inputs to match the corresponding spatial information, like sound source pose-induced changes and binaural differences in phase and loudness. It selects experts based on inputs, like one specialized in sound source slowly approaching the listener from the far left direction. Each expert in our design leverages Fourier Analysis Networks (FAN) (Dong et al., 2024) to decompose frequency components, enabling explicit modeling of binaural spatial dynamics while capturing speech periodicity, rhythm, and intonation. The FAN layer is defined as: Ï(x) [cos(Wpx) sin(Wpx) Ï(B + px)], (3) where Wp, p, and are learnable parameters. The output of FAN combines sinusoidal functions and nonlinear activation, capturing periodic and non-periodic variations. This enhances the expressive power for spatial-temporal and time-frequency auditory modeling, aiding in the synchronized modeling of speech prosody and spatial information. Our routing strategies in Drama-MOE use dense-to-sparse Gumbel-Softmax (Nie et al., 2021), enabling dynamic and efficient expert selection for each group. Let be the hidden 6 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting representation, and g(h)i denote the routing score for expert i. To prevent overloading and ensure balance, we apply load-balancing loss (Fedus et al., 2022): balance = Î±N (cid:32) (cid:88) i="
        },
        {
            "title": "1\nB",
            "content": "(cid:88) hB (cid:33) g(h)i , (4) where is the batch size, is the number of experts, and Î± hyperparameter controlling regularization strength. For more details, please refer to Appendix C.7. 4.4. Complete Drama Inference Procedure For inference, users can input complete script and specify each speakers timbre with prompt audio a. We typically segment the target drama based on speaker transitions in the script. As shown in Figure 4 (c), the Multimodal Pose Encoder predicts content duration and provides each targets embedding of pose zp and content zc. Then, the Immersive Drama Transformer generates each target speech ypr from Gaussian noise Ïµ, conditioned on zc, zp, scene s, and a. To enhance generation quality and ensure contextual prosodic consistency, we design the context-consistent classifier-free guidance (CFG) strategy, which uses both prompt audio and the last predicted audio from the same speaker yprlast. During inference, we modify the output vector field as: Î³)vt(x, Î±)vt(x, a, C; Î¸)+ zp; Î¸) = Î³Î±vt(x, vcf g(x, Î³(1 yprlast, C; Î¸) + (1 , C; Î¸), (5) where Î³ is the CFG scale that balances creativity and controllability, while Î± balances context consistency and input controllability. Setting Î³ = 3 and Î± = 0.4, we improve generation quality and incorporate previously generated audio to enhance the prosody consistency of the same speaker within drama act. This ensures coherence while preserving the timbre and accent of the original prompt audio. Moreover, as prosody can be learned from previous prompt audio in the same context, this approach also improves the expressiveness of semantically aligned prosody. Additionally, by leveraging the accelerated inference capabilities of flow matching and Mamba blocks, our model can efficiently generate immersive spatial drama. For more details, please refer to Appendix C.2 and C.3. 5. Experiments 5.1. Experimental Setup Implementation Details. Mel-spectrograms are derived from raw binaural waveforms with 48 kHz sample rate, 1024 window size, 256 hop size, and 80 mel bins. We use four Mamba-Transformer blocks. The flow-matching timestep is 1000 for training and 25 for inference with the Euler ODE solver. For the training procedure of Immersive 7 Drama Transformer, we use 8 NVIDIA RTX-4090 GPUs with batch size of 12K frames per GPU for 100K steps. The Adam optimizer is applied with learning rate of 5 105, Î²1 = 0.9, Î²2 = 0.999, and 10K warm-up steps. Please refer to Appendix C.1 for more details. Evaluation Metrics. We perform both subjective and objective evaluations on the generated samples. To ensure fair comparison with existing monaural speech synthesis models, we adopt monaural evaluation metrics and then evaluate binaural metrics for cascade-generated binaural speech with BinauralGrad (Leng et al., 2022). 1) For objective evaluation, we use Character Error Rate (CER) and Cosine Similarity (SIM) to assess the content accuracy and speaker similarity with prompt audio. F0 Frame Error (FFE) is used to evaluate the quality of prosody modeling. Since existing binaural metrics are scarce and not suitable for our one-stage binaural speech generation task, we have designed several new metrics. We extract Interaural Phase Difference (IPD) and Interaural Level Difference (ILD) from the binaural melspectrograms and compute MAE with GT. We also compute cosine similarity of angle and distance embedding extracted from SPATIAL-AST (Zheng et al., 2024) with GT for spatial evaluation. 2) For subjective evaluation, we conduct Mean Opinion Score (MOS), which is rated from 1 to 5 and reported with 95% confidence intervals. MOS-Q evaluates the synthesized quality (like naturalness, spatial perception, and coherence), MOS-S assesses speaker similarity in timbre and accent, and MOS-E measures the expressiveness of semantically aligned prosody. For binaural metrics, we employ MOS-P to evaluate the pose consistency between the multimodal prompt and the generated audio. In the ablation study, we conduct Comparative Mean Opinion Score (CMOS). Please refer to Appendix for more details, 5.2. Results Comparison of baseline models for monaural speech. To ensure fair evaluation of the synthesized quality, speaker similarity to the prompt audio, and prosodic expressiveness, we compute monaural speech metrics after averaging the binaural speech generated by ISDrama across channels. For the baseline models, we employ several strong popular opensource speech synthesis models, including: 1) Uniaudio (Yang et al., 2023), 2) StyleTTS 2 (Li et al., 2024c), 3) CosyVoice (Du et al., 2024), 4) FireRedTTS (Guo et al., 2024), and 5) F5-TTS (Chen et al., 2024). We use their open-source codes on GitHub and train them using our MRSDrama dataset in monaural speech format. We also provide detailed introduction for each baseline model in Appendix E.1. To achieve more precise evaluation of quality and speaker similarity, we conduct the assessment using only single sentences for all models. Table 2 shows that ISDrama performs well across all monaural metrics. In synthesized quality (CER, MOS-Q) and speaker similarity ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Table 2. Monaural speech quality comparison. For testing quality and speaker similarity, we use single-sentence speech for evaluation. Method Objective Metrics Subjective Metrics CER SIM FFE MOS-Q MOS-S MOS-E GROUND TRUTH 2.58% - - 4.43 0.12 4.41 0. 4.26 0.11 UNIAUDIO STYLETTS 2 COISYVOICE FIREREDTTS F5-TTS 0.94 4.21% 0.93 4.19% 3.95% 0.96 3.07 % 0.95 0.96 3.13% ISDRAMA (OURS) 3.31% 0. 0.68 0.60 0.56 0.60 0.55 0.34 3.93 0.12 3.89 0.04 4.05 0.06 4.01 0.17 4.12 0.16 4.06 0.05 4.02 0.10 4.19 0.09 4.11 0.11 4.21 0.08 3.65 0.18 3.72 0.09 3.81 0.21 3.77 0.10 3.86 0.06 4.06 0. 4.18 0.11 4.01 0.09 Table 3. Binaural speech quality comparison. We use complete drama for spatial evaluation. ANG and Dis denote angle and distance. Spatialization refers to the generation of binaural audio directly from the GT monaural audio based on the geometric pose. Method SPATIALIZATION UNIAUDIO STYLETTS2 COISYVOICE FIREREDTTS F5-TTS ISDRAMA (GEOMETRIC) ISDRAMA (VIDEO) ISDRAMA (TEXTUAL) Objective Metrics Subjective Metrics IPD MAE ILD MAE ANG COS DIS COS MOS-Q MOS-P 0.007 0.012 0.011 0.011 0.010 0.009 0.008 0.009 0.011 0.043 0.060 0.064 0.055 0.051 0. 0.046 0.051 0.055 0.58 0.38 0.33 0.44 0.42 0.45 0.51 0.48 0.43 0.79 0.64 0.61 0.68 0.65 0. 0.75 0.73 0.68 4.09 0.08 4.26 0.16 3.65 0.16 3.63 0.05 3.72 0.13 3.78 0.19 3.66 0.14 4.01 0.14 3.97 0.11 3.95 0.13 3.89 0.11 3.81 0.16 4.02 0.09 3.97 0.17 4.11 0. 4.18 0.10 4.09 0.08 4.41 0.06 (SIM, MOS-S), it is comparable to the performance of the best speech synthesis baseline model, while it surpasses all baseline models in prosodic expressiveness (FFE, MOSE). These well-performing results can be attributed to the integration of semantics to model dramatic prosody with semantically aligned emotion and rhythm in Drama-MOE, as well as F0 supervision during training. Comparison of Baseline Models for Binaural Speech. Based on the subjective and objective spatial metrics we designed, we train commonly used monaural-to-binaural spatialization model, BinauralGrad (Leng et al., 2022), on our MRSDrama dataset to convert the generated monaural speech from baseline models, conditioned on geometric poses, into binaural speech for evaluating spatial performance. To better test the coherence of spatial information and the consistency of prosody, we concatenate sentences from the same generated drama for evaluation. As shown in Table 3, with the same geometric pose input, the binaural speech generated by ISDrama outperforms all two-stage models across all spatial metrics, both subjective and objective. The high MOS-Q shows coherence in continuous multi-speaker speech generation, which can be attributed to the effectiveness of our context-consistent CFG in generating complete drama. In more flexible application scenarios, including video and textual prompt inputs, ISDrama continues to deliver excellent performance, thanks to the unified pose representation extracted by the Multimodal Pose Encoder. Although the textual prompt leads to lower objective metrics, it achieves strong MOS-P score, which can be attributed to the general nature of the text descriptions. Specifically, the same textual description can generate multiple different pose sequences, resulting in spatial information not fully aligned with the ground truth. When listening to the demos and considering the pose cues provided by multimodal inputs, it is evident that ISDrama can maintain consistency with the poses of the sound source and demonstrate coherent and expressive prosody aligned with semantics. Overall, ISDrama effectively models spatial information and prosodic expressiveness, achieving wellbalanced outcome. These results can be attributed to the exceptional pose modeling capabilities of our Multimodal Pose Encoder, as well as the enhanced dramatic prosody and pose control facilitated by Immersive Drama Transformer. 5.3. Ablation Study Multimodal Pose Encoder. Tables 3 and 4 present the results of feeding pose embeddings of different prompts into the Immersive Drama Transformer, with or without con8 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Table 4. Ablation studies on Immersive Drama Transformer. MPE denotes Multimodal Pose Encoder. Method Objective Metrics Subjective Metrics CER FFE ANG COS DIS COS CMOS-Q CMOS-S CMOS-E CMOS-P ISDRAMA (GEOMETRIC) 3.31% GEOMETRIC W/O MPE VIDEO W/O MPE TEXTUAL W/O MPE W/O DRAMA-MOE W/O PROSODIC-MOE W/O SPATIAL-MOE W/O FAN W/O F0 PRED W/O MAMBA W/O CFG 3.35% 3.68% 3.72% 4.01% 3.55% 3.48% 3.79% 3.65% 3.84% 3.73% 0.34 0.37 0.41 0.44 0.49 0.47 0.39 0. 0.46 0.49 0.48 0.51 0.49 0.38 0.35 0.39 0.47 0.41 0.40 0.46 0.43 0.45 0. 0.73 0.63 0.61 0.65 0.71 0.66 0.66 0.69 0.65 0.68 0.00 -0.11 -0.39 -0.50 -0.46 -0.28 -0.27 -0. -0.36 -0.30 -0.38 0.00 -0.03 -0.19 -0.20 -0.27 -0.21 -0.12 -0.24 -0.19 -0.08 -0.22 0. -0.10 -0.29 -0.32 -0.40 -0.36 -0.21 -0.32 -0.41 -0.27 -0.32 0.00 -0.16 -0.52 -0.57 -0.52 -0.46 -0.28 -0. -0.48 -0.39 -0.38 trastive learning. We observe that the absence of contrastive learning has minimal impact on geometric poses. This is because the geometric pose embeddings include 3D orientation, quaternion orientation, and radial velocity, which provide sufficient information for accurate pose modeling. However, for silent video and textual prompts, omitting contrastive learning significantly reduces spatial performance. This highlights the importance of contrastive learning, as it enhances the models ability to generate unified embeddings from multimodal prompts, which is crucial for supporting diverse and flexible application scenarios. Drama-MOE. Table 4 shows the results of experiments where we removed the full Drama-MOE, eliminated individual expert groups, and replaced the FAN module with simple linear layer. The results reveal that removing the full Drama-MOE leads to decline in performance across all metrics. When examining the individual expert groups, we find that Spatial-MOE significantly affects spatial performance, while Prosodic-MOE influences both prosody expressiveness and speaker similarity. These observations suggest that Drama-MOE plays key role in enhancing the modeling of both prosody and pose by utilizing specialized experts tailored to different spatial conditions and semantically aligned prosody. Additionally, we find that the FAN module outperforms the simple linear layer in all aspects, reflecting the benefits brought by frequency decomposition in capturing more nuanced features. Immersive Drama Transformer. We evaluate the effects of removing the supervision of F0, replacing Mamba with memory-equivalent Transformer, and eliminating the context-consistent CFG. The results are shown in Table 4. It can be observed that F0 prediction is crucial for accurate prosody modeling, as its absence results in significant loss of prosodic quality. The Mamba block, being lightweight, allows stacking more layers than traditional Transformer with the same memory budget, which leads to further improvements in quality. Additionally, removing the context-consistent CFG reduces the consistency and prosodic expressiveness of the generated outputs. These findings underscore the effectiveness of these design choices in enhancing quality and prosody modeling capabilities of the Immersive Drama Transformer. For more extended results, please refer to Appendix E. 6. Conclusion In this paper, we introduce novel task Multimodal Immersive Spatial Drama Generation, focusing on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts. To support this task, we present MRSDrama, the first multimodal recorded spatial drama dataset, comprising binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model based on multimodal prompting. To extract unified pose representation from multimodal prompts, we design the Multimodal Pose Encoder, contrastive learningbased framework, accounting for the Doppler effect caused by moving speakers. To generate immersive spatial drama effectively and stably, we develop the Immersive Drama Transformer, flow-based Mamba-Transformer model, incorporating Drama-MOE, which selects proper experts to enhance prosodic expressiveness and pose control. Then, we adopt context-consistent CFG strategy to coherently generate high-quality complete drama. Experimental results show that ISDrama achieves better performance than baseline models across both objective and subjective metrics."
        },
        {
            "title": "Impact Statement",
            "content": "Large-scale multimodal datasets and generative models often pose ethical challenges. Since MRSDrama includes human voice and video data, we have implemented data desensitization measures. Specifically, we applied masks to the faces in videos to prevent the leakage of biometric 9 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting information. Additionally, ISDrama, with its immersive spatial drama generation capabilities, could be misused for applications like dubbing in entertainment short videos, potentially infringing on the copyrights of well-known actors. Furthermore, its ability to transfer and control prosody and spatial poses lowers the technical barriers to generating high-quality drama, which could lead to risks such as unfair competition and potential unemployment for professionals in related theatrical occupations. To mitigate these risks, we plan to explore methods such as audio watermarking to safeguard individual privacy and address misuse concerns."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Bachlechner, T., Majumder, B. P., Mao, H., Cottrell, G., and McAuley, J. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pp. 13521361. PMLR, 2021. Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. Bertin, N., Camberlein, E., Vincent, E., Lebarbenchon, R., Peillon, S., Lamande, E., Sivasankaran, S., Bimbot, F., Illina, I., Tom, A., et al. french corpus for distantmicrophone speech processing in real homes. In Interspeech 2016, 2016. Boersma, P. Praat, system for doing phonetics by computer. Glot. Int., 5(9):341345, 2001. Chen, J., Tan, X., Luan, J., Qin, T., and Liu, T.-Y. Hifisinger: Towards high-fidelity neural singing voice synthesis. arXiv preprint arXiv:2009.01776, 2020. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. Wavlm: Largescale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518, 2022. Chen, V. C., Li, F., Ho, S.-S., and Wechsler, H. Microdoppler effect in radar: phenomenon, model, and simulation study. IEEE Transactions on Aerospace and Electronic Systems, 42:221, 2006. Chen, Y., Niu, Z., Ma, Z., Deng, K., Wang, C., Zhao, J., Yu, K., and Chen, X. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. arXiv preprint arXiv:2410.06885, 2024. Choi, S., Han, S., Kim, D., and Ha, S. Attentron: Few-shot text-to-speech utilizing attention-based variable-length embedding. arXiv preprint arXiv:2005.08484, 2020. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Dong, Y., Li, G., Tao, Y., Jiang, X., Zhang, K., Li, J., Su, J., Zhang, J., and Xu, J. Fan: Fourier analysis networks. arXiv preprint arXiv:2410.02675, 2024. Du, Z., Chen, Q., Zhang, S., Hu, K., Lu, H., Yang, Y., Hu, H., Zheng, S., Gu, Y., Ma, Z., et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Fei, Z., Fan, M., Yu, C., Li, D., Zhang, Y., and Huang, J. Dimba: Transformer-mamba diffusion models. arXiv preprint arXiv:2406.01159, 2024. Fitria, T. N. Augmented reality (ar) and virtual reality (vr) technology in education: Media of teaching and learning: review. International Journal of Computer and Information System (IJCIS), 4(1):1425, 2023. Gao, R. and Grauman, K. 2.5 visual sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 324333, 2019. Gao, Z., Zhang, S., McLoughlin, I., and Yan, Z. Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition. arXiv preprint arXiv:2206.08317, 2022. GarcÄ±a-Barrios, G., Krause, D. A., Politis, A., Mesaros, A., Gutierrez-Arriola, J. M., and Fraile, R. Binaural source localization using deep learning and head rotation information. In 2022 30th European Signal Processing Conference (EUSIPCO), pp. 3640. IEEE, 2022. Garg, R., Gao, R., and Grauman, K. Geometry-aware multitask learning for binaural audio generation from video. arXiv preprint arXiv:2111.10882, 2021. Gill, T. P. The doppler effect: An introduction to the theory of the effect. (No Title), 1965. 10 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Guo, H.-H., Liu, K., Shen, F.-Y., Wu, Y.-C., Xie, F.-L., Xie, K., and Xu, K.-T. Fireredtts: foundation text-tospeech framework for industry-level generative speech applications. arXiv preprint arXiv:2409.03283, 2024. Guo, W., Zhang, Y., Pan, C., Huang, R., Tang, L., Li, R., Hong, Z., Wang, Y., and Zhao, Z. Techsinger: Technique controllable multilingual singing voice synthesis via flow matching. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2397823986, 2025. Heydari, M., Souden, M., Conejo, B., and Atkins, J. Immersediffusion: generative spatial audio latent diffusion model. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2025. Huang, R., Ren, Y., Liu, J., Cui, C., and Zhao, Z. Generspeech: Towards style transfer for generalizable outarXiv preprint of-domain text-to-speech synthesis. arXiv:2205.07211, 2022. Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Jiang, Z., Liu, J., Ren, Y., He, J., Ye, Z., Ji, S., Yang, Q., Zhang, C., Wei, P., Wang, C., et al. Mega-tts 2: Boosting prompting mechanisms for zero-shot speech synthesis. In The Twelfth International Conference on Learning Representations, 2024. Jiang, Z., Ren, Y., Li, R., Ji, S., Zhang, B., Ye, Z., Zhang, C., Jionghao, B., Yang, X., Zuo, J., et al. Megatts 3: Sparse alignment enhanced latent diffusion transformer for zeroshot speech synthesis. arXiv preprint arXiv:2502.18924, 2025. Ju, Z., Wang, Y., Shen, K., Tan, X., Xin, D., Yang, D., Liu, Y., Leng, Y., Song, K., Tang, S., et al. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. arXiv preprint arXiv:2403.03100, 2024. Karaev, N., Makarov, I., Wang, J., Neverova, N., Vedaldi, A., and Rupprecht, C. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. Kim, J., Yun, H., and Kim, G. Visage: Video-to-spatial audio generation. In The Thirteenth International Conference on Learning Representations, 2025. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33:1702217033, 2020. Krause, D. A., GarcÄ±a-Barrios, G., Politis, A., and Mesaros, A. Binaural sound source distance estimation and localization for moving listener. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. Kumar, N., Goel, S., Narang, A., and Lall, B. Normalization driven zero-shot multi-speaker speech synthesis. In Interspeech, pp. 13541358, 2021. Lee, S.-H., Choi, H.-Y., Kim, S.-B., and Lee, S.-W. Hierspeech++: Bridging the gap between semantic and acoustic representation of speech by hierarchical variational inference for zero-shot speech synthesis. arXiv preprint arXiv:2311.12454, 2023. Leng, Y., Chen, Z., Guo, J., Liu, H., Chen, J., Tan, X., Mandic, D., He, L., Li, X., Qin, T., et al. Binauralgrad: two-stage conditional diffusion probabilistic model for binaural audio synthesis. Advances in Neural Information Processing Systems, 35:2368923700, 2022. Li, K., Sang, W., Zeng, C., Yang, R., Chen, G., and Hu, X. Sonicsim: customizable simulation platform for speech processing in moving sound source scenarios. arXiv preprint arXiv:2410.01481, 2024a. Li, R., Zhang, Y., Wang, Y., Hong, Z., Huang, R., and Zhao, Z. Robust singing voice transcription serves synthesis. arXiv preprint arXiv:2405.09940, 2024b. Li, X., Song, C., Li, J., Wu, Z., Jia, J., and Meng, H. Towards multi-scale style control for expressive speech synthesis. arXiv preprint arXiv:2104.03521, 2021. Li, Y. A., Han, C., and Mesgarani, N. Styletts: style-based generative model for natural and diverse text-to-speech synthesis. arXiv preprint arXiv:2205.15439, 2022. Li, Y. A., Han, C., Raghavan, V., Mischler, G., and Mesgarani, N. Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. Advances in Neural Information Processing Systems, 36, 2024c. Li, Z., Zhao, B., and Yuan, Y. Cyclic learning for binaural audio generation and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2666926678, 2024d. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2022. ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Liu, J., Ye, Z., Chen, Q., Zheng, S., Wang, W., Zhang, Q., and Zhao, Z. Dopplerbas: Binaural audio synthesis addressing doppler effect. arXiv preprint arXiv:2212.07000, 2022a. Liu, X., Gong, C., et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022b. Mao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., and Paul Smolley, S. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 27942802, 2017. McAuliffe, M., Socolof, M., Mihuc, S., Wagner, M., and Sonderegger, M. Montreal forced aligner: Trainable textspeech alignment using kaldi. In Interspeech, volume 2017, pp. 498502, 2017. Nie, X., Miao, X., Cao, S., Ma, L., Liu, Q., Xue, J., Miao, Y., Liu, Y., Yang, Z., and Cui, B. Evomoe: An evolutional mixture-of-experts training framework via denseto-sparse gate. arXiv preprint arXiv:2112.14397, 2021. Parida, K. K., Srivastava, S., and Sharma, G. Beyond mono to binaural: Generating binaural audio from mono audio with depth and cross modal attention. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 33473356, 2022. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Ravanelli, M., Cristoforetti, L., Gretter, R., Pellin, M., Sosi, A., and Omologo, M. The dirha-english corpus and related tasks for distant-speech recognition in domestic environments. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pp. 275 282. IEEE, 2015. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. Fastspeech: Fast, robust and controllable text to speech. Advances in neural information processing systems, 32, 2019. Richard, A., Dodds, P., and Ithapu, V. K. Deep impulse responses: Estimating and parameterizing filters with deep networks. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 32093213. IEEE, 2022. Sarabia, M., Menyaylenko, E., Toso, A., Seto, S., Aldeneh, Z., Pirhosseinloo, S., Zappella, L., Theobald, B.-J., Apostoloff, N., and Sheaffer, J. Spatial librispeech: An augmented dataset for spatial audio learning. arXiv preprint arXiv:2308.09514, 2023. Shimada, K., Koyama, Y., Takahashi, S., Takahashi, N., Tsunoo, E., and Mitsufuji, Y. Multi-accdoa: Localizing and detecting overlapping sounds from the same class with auxiliary duplicating permutation invariant training. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 316320. IEEE, 2022. Skerry-Ryan, R., Battenberg, E., Xiao, Y., Wang, Y., Stanton, D., Shor, J., Weiss, R., Clark, R., and Saurous, R. A. Towards end-to-end prosody transfer for expressive speech synthesis with tacotron. In international conference on machine learning, pp. 46934702. PMLR, 2018. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Sun, P., Cheng, S., Li, X., Ye, Z., Liu, H., Zhang, H., Xue, W., and Guo, Y. Both ears wide open: Towards language-driven spatial audio generation. arXiv preprint arXiv:2410.10676, 2024. Vacher, M., Lecouteux, B., Chahuara, P., Portet, F., Meillon, B., and Bonnefond, N. The sweet-home speech and multimodal corpus for home automation interaction. In The 9th edition of the Language Resources and Evaluation Conference (LREC), pp. 44994506, 2014. Wagner, M. and Watson, D. G. Experimental and theoretical advances in prosody: review. Language and cognitive processes, 25(7-9):905945, 2010. Yang, D., Tian, J., Tan, X., Huang, R., Liu, S., Chang, X., Shi, J., Zhao, S., Bian, J., Wu, X., et al. Uniaudio: An audio foundation model toward universal audio generation. arXiv preprint arXiv:2310.00704, 2023. Yang, Q. and Zheng, Y. Deepear: Sound localization with IEEE Transactions on Mobile binaural microphones. Computing, 23(1):359375, 2022. Richard, A., Markovic, D., Gebru, I. D., Krenn, S., Butler, G. A., Torre, F., and Sheikh, Y. Neural synthesis of binaural speech from mono audio. In International Conference on Learning Representations, 2021. ZaÄ±di, J., Seute, H., Niekerk, B., and Carbonneau, M. Daftexprt: Robust prosody transfer across speakers for expressive speech synthesis. arXiv preprint arXiv:2108.02271, 2021. 12 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Zhang, B. and Sennrich, R. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. A. Preliminaries A.1. Doppler Effect Zhang, L., Li, R., Wang, S., Deng, L., Liu, J., Ren, Y., He, J., Huang, R., Zhu, J., Chen, X., et al. M4singer: multi-style, multi-singer and musical score provided mandarin singing corpus. Advances in Neural Information Processing Systems, 35:69146926, 2022. Zhang, Y., Huang, R., Li, R., He, J., Xia, Y., Chen, F., Duan, X., Huai, B., and Zhao, Z. Stylesinger: Style transfer for out-of-domain singing voice synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1959719605, 2024a. Zhang, Y., Jiang, Z., Li, R., Pan, C., He, J., Huang, R., Wang, C., and Zhao, Z. Tcsinger: Zero-shot singing voice synthesis with style transfer and multi-level style control. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 19601975, 2024b. Zhang, Y., Pan, C., Guo, W., Li, R., Zhu, Z., Wang, J., Xu, W., Lu, J., Hong, Z., Wang, C., et al. Gtsinger: global multi-technique singing corpus with realistic music scores for all singing tasks. arXiv preprint arXiv:2409.13832, 2024c. Zhang, Y., Guo, W., Pan, C., Zhu, Z., Li, R., Lu, J., Huang, R., Zhang, R., Hong, Z., Jiang, Z., and Zhao, Z. Versatile framework for song generation with prompt-based control. arXiv preprint arXiv:2504.19062, 2025. Zhao, S., Ma, B., Watcharasupat, K. N., and Gan, W.-S. Frcrn: Boosting feature representation using frequency recurrence for monaural speech enhancement. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 92819285. IEEE, 2022. Zheng, Z., Peng, P., Ma, Z., Chen, X., Choi, E., and Harwath, D. Bat: Learning to reason about spatial sounds with large language models. arXiv preprint arXiv:2402.01591, 2024. Zhou, H., Xu, X., Lin, D., Wang, X., and Liu, Z. Sepstereo: Visually guided stereophonic audio generation by associating source separation. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XII 16, pp. 5269. Springer, 2020. The Doppler effect (Gill, 1965) refers to the change in the frequency of wave as observed by an observer when the source of the wave is in motion relative to it. Initially applied in radar systems, this effect helps to analyze the features of moving objects, such as targets of interest (Chen et al., 2006). The Doppler effect can be expressed as: (cid:18) fo = (cid:19) fs, vrad (6) where is the wave propagation speed, vrad is the radial velocity of the moving sound source, fs is the source frequency, and fo is the frequency received by the observer. A.2. Rectified flow-matching p0(x) represents standard Gaussian, and x1 In generative modeling, the true data distribution is denoted as q(x1), which can be sampled but lacks an accessible density function. Consider probability path pt(xt), where p1(x) x0 approximates the real data distribution. The core of the flow-matching approach (Liu et al., 2022b) is to model this path directly, which is governed by the following ordinary differential equation (ODE): dx = u(x, t)dt, [0, 1], (7) where u(x, t) denotes the target vector field, and is the time index. If we have access to the vector field u, it is possible to recover realistic data by reversing the flow. To approximate u, we use vector field estimator v( ), and the flow-matching objective function is: FM(Î¸) = Et,pt(x) v(x, t; Î¸) u(x, t) 2 , (8) where pt(x) denotes the distribution of at time t. When conditioning on additional C, the objective becomes the conditional flow-matching formulation (Lipman et al., 2022): C; Î¸) v(x, CFM(Î¸) = Et,p1(x1),pt(xx1) x1, C) u(x, (9) The key idea behind flow-matching is to construct direct transformation path from Gaussian noise to real data. This is achieved by linearly interpolating between Gaussian noise x0 and the real data x1 to generate samples at time t: 2 . xt = (1 t)x0 + tx1. (10) Thus, the conditional vector field becomes u(x, x1, C) = x0, and the rectified flow-matching (RFM) loss used x1 for optimization is: v(x, C; Î¸) (x1 2 . x0) (11) ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting If the vector field is estimated correctly, realistic data can be generated by passing Gaussian noise through an ODE solver at discrete time steps. One effective method to solve the reverse flow is the Euler ODE: xt+Ïµ = + Ïµv(x, C; Î¸), (12) where Ïµ is the step size. Flow-matching models typically require hundreds to thousands of training steps. However, the efficient linear interpolation approach significantly reduces this to 25 steps or fewer during inference, greatly enhancing computational efficiency. Moreover, the seamless transition from noise to data ensures both stability and high-quality output, which is essential for generating complex data free of artifacts while maintaining consistency across diverse input conditions. A.3. Mamba Models based on Structured State Space (SSM), such as S4 and Mamba (Gu & Dao, 2023), draw inspiration from to continuous systems that map 1D sequence x(t) another sequence y(t) RN. In this system, the matrix RN1 and evolution parameter, while used for projection. The continuous system is: RNN serves as the state R1N are through hidden state h(t) h(t) = Ah(t) + Bx(t), y(t) = Ch(t). (13) S4 and Mamba are discrete-time versions of this continuous system, incorporating timescale parameter to convert the continuous parameters and into discrete counterparts and B. typical approach to perform this transformation is through zero-order hold (ZOH), which is: = exp (A), = (A)1(exp (A) (14) I) B. After discretizing and B, the discrete-time system can be written as: ht = Aht1 + Bxt, yt = Cht. (15) Finally, these models obtain the output through global convolution operation, represented as: = (CB, CAB, . . . , CA M1 B), = K, (16) where denotes the length of the input sequence x, and RM represents the structured convolutional kernel. B. Dataset Details Simulated data cannot accurately capture the intricate, dramatic prosody or the precise effects of real-world spatial 14 scenes, positions, and orientations. Meanwhile, stimulated spatial audio datasets that expand on monaural audio data also have limited effectiveness in modeling the nuances of real-world scenes, positions, and orientations (Sarabia et al., 2023; Li et al., 2024a). As shown in Figure 6, we utilize the 3Dio FS XLR Binaural Microphone1 connected to Yamaha professional sound card to record binaural audio, effectively modeling interaural phase differences (IPD) and interaural level differences (ILD). The recordings are complemented by 24fps video captured using camera. The scripts used in our study are sourced from authorized materials. For the recording process, we design 5 to 12 fixed positions in each scene, covering various distances and angles. However, the routes and speeds are not predetermined, allowing speakers to move freely. Speakers may stand and walk through these points or sit on chair at designated position, which results in height variations. This flexibility also aids in contrastive learning. Speakers read from script displayed on screen that is not visible in the camera frame, with the requirement that the script has semantically aligned dramatic prosody. The recording of both audio and video for each dramatic act typically lasts between 3 and 15 minutes. We hire the speakers at rate of $100 per hour, and they agree to make the video and audio data available for research purposes. We assure them that no facial information will be disclosed and then apply masks to their faces in the video using Adobe After Effects. Annotators label the 3D coordinates and quaternion orientations of the sound source (the speakers mouth) based on the video. Using the annotated pose data, we employ GPT-4o (Achiam et al., 2023) to generate textual prompts, such as: Speaker 1 initially stands in the front-right, then walks at slow pace to the far front-left... For binaural speech, we perform denoising using FRCRN (Zhao et al., 2022) and extract phonemes using PyPinyin. Following previous Chinese audio annotation works (Zhang et al., 2024a;b;c; Li et al., 2024b; Guo et al., 2025; Zhang et al., 2022). Coarse alignment between phonemes and audio is achieved using MFA (McAuliffe et al., 2017). Annotators then refine the rough alignment using Praat (Boersma, 2001), focusing on correcting word and phoneme boundaries and addressing erroneous words based on auditory perception. Each step is double-checked by human annotators to ensure accuracy. We hire annotators at rate of $30 per hour, and they consent to their contributions being used for research purposes. Finally, the dataset is segmented into 47,958 segments according to speaker transitions, with each segment having maximum duration of 16 seconds. Figure 5 shows more statistics of MRSDrama on movement speeds and segment 1https://3diosound.com/products/ free-space-xlr-binaural-microphone ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Figure 5. The extended statistics of MRSDrama. hop size, and 80 mel bins. We use HiFi-GAN (Kong et al., 2020) as the vocoder to synthesize waveforms from melspectrograms. Specifically, the model consists of 3 layers for both the encoder and decoder, with hidden size of 384 and Conv1D kernel size of 5. The binaural melspectrogram with dimensions B, 2, 80, is compressed into dimensions B, 40, /4, facilitating further processing by the Transformer. During training, batches of fixed length are used, consisting of 3000 mel-spectrogram frames. The 104, Adam optimizer is employed with learning rate of 1 Î²1 = 0.9, Î²2 = 0.999, and 10K warm-up steps. For the Immersive Drama Transformer, we employ four Mamba-Transformer blocks. Each block uses hidden size of 768 and 8 attention heads. The Mixture-of-Experts (MoE) module includes 4 experts per expert group. Each Transformer has three Mamba blocks. The total number of parameters is 177 M. The flow-matching training uses 1,000 timesteps, while inference employs 25 timesteps with the Euler ODE solver. During training, we use 8 NVIDIA RTX4090 GPUs with batch size of 12K frames per GPU for 100K steps. The Adam optimizer is applied with learning 105, Î²1 = 0.9, Î²2 = 0.999, and 10K warm-up rate of 5 steps. C.2. Training Procedure As detailed in Section 4.2, the final loss for the Multimodal Pose Encoder includes: 1) contras: the contrastive objecL dur: the mean squared error tive for three modalities. 2) (MSE) duration loss between the predicted and ground truth phoneme-level durations on logarithmic scale. Figure 6. The binaural recording device we used. duration. C. Model Design C.1. Model Configuration For the Multimodal Pose Encoder, we use hidden size of 768. Our semantic information is modeled using BERT (base) (Devlin et al., 2018), which preserves temporal length. The duration decoder adopts the FastSpeech architecture (Ren et al., 2019). For the geometric encoder, we employ Conv1D layer with kernel size of 5. For the video and textual encoders, the Transformer model utilizes 8 transformer layers and 8 attention heads, with hidden size of 768. The total number of parameters is 23.32M for each model. We obtain CLIP embeddings at 4 frames per second (FPS). During training, we use the Adam optimizer with 104, Î²1 = 0.9, Î²2 = 0.999, and 10K learning rate of 1 warm-up steps. For the spatial audio encoder and decoder, we use Variational Autoencoder (VAE) architecture (Kingma & Welling, 2013). Mel-spectrograms are derived from binaural waveforms with 48 kHz sample rate, 1024 window size, 256 In Appendix C.5, the final loss terms for the spatial audio rec: the encoder and accomp decoder are as follows: 1) L2 reconstruction loss of mel-spectrograms. 2) adv: the LSGAN-styled adversarial loss for the GAN discriminator. 15 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting As for Immersive Drama Transformer, the final loss terms low: the flow matching loss, during training include: 1) as described in Section 4.3. 2) pitch: the MSE pitch loss between the predicted and ground truth f0 in the log scale. 3) balance: the load-balancing loss for each expert group in Drama-MOE, as discussed in Section C.7. L C.3. Inference Procedure For enhanced generation quality and contextual prosody consistency, we utilize the prompt audio and the last predicted audio from the same speaker, yprlast, using the Classifier-Free Guidance (CFG) strategy. During training, we randomly select prompt audio from the same speaker within the same script to improve generalization, with 0.4 probability of selecting other audio from the same speaker and 0.2 probability of dropping the prompt audio entirely. During inference, we modify the output vector field as in Equation 5. When Î± = 1, vcf relies solely on the input prompt audio. Furthermore, if Î± = 1 and Î³ = 1, vcf is equivalent to the original vt(x, a, C; Î¸). Setting Î³ = 3 and Î± = 0.4, we improve generation quality and incorporate previously generated audio to enhance the prosody consistency of the same speaker within single drama act. This ensures coherence while preserving the timbre, accent, and articulation of the original prompt audio. Moreover, as prosody can be learned from previous prompt audio in the same context, this method further enhances prosodic expressiveness and alignment with the drama narrative. C.4. Multimodal Pose Encoder For inputs from video and geometric pose, we need to segment the length of each speaker transition. In the case of geometric pose, sudden jump in position indicates speaker switch, allowing us to determine the total duration of each actors line. For video inputs, we also input the pixel coordinates of the speakers lips for the onset of each speech and the corresponding starting frame. This information is utilized by Cotracker3 (Karaev et al., 2024) to track the speakers mouth movements, facilitating the modeling of 3D position coordinates and quaternion orientation. This process also helps in accurately determining the total duration of each actors line. For duration prediction, we estimate the phoneme durations within each actors line based on speaker transitions in the script. In addition to encoding semantic and phonetic information as inputs to the duration predictor, we incorporate the length from the video segment and the pose sequence as constraints for the actors line duration. This ensures highly precise and consistent duration prediction. We design three types of contrasts for contrastive learning to explore diverse physical and spatial features. Dynamic features include mobility, which differentiates between moving and stationary states, movement speed to capture variations in velocity, and movement direction to account for distinct directional shifts. Postural features focus on posture, distinguishing between standing and sitting, and orientation, which captures differences in the relative facing direction of the sound source. Finally, positional features emphasize varying distances to explore positional relationships and different angles to capture changes in the viewing perspective. Together, these dimensions comprehensively enhance the robustness of our contrastive learning framework. C.5. Spatial Audio Encoder and Decoder The Spatial Audio Encoder and Decoder are designed based on the Variational Autoencoder (VAE) model (Kingma & Welling, 2013). During pre-training, we optimize the encoder and decoder using the L2 reconstruction loss. To further enhance reconstruction quality, we integrate GAN discriminator inspired by the architecture of ML-GAN (Chen et al., 2020). Specifically, we employ the LSGAN-style adversarial loss (Mao et al., 2017), adv, which minimizes the distributional divergence between the predicted melspectrograms and the ground truth mel-spectrograms. Before encoding, we extract the mel-spectrogram using librosa 2. After generating the mel-spectrogram from the decoders output, HiFi-GAN (Kong et al., 2020) is used to convert it back into an audio waveform. To improve the quality of speech generation, we add the reconstructed F0 from our Immersive Drama Transformer during inference, applying the neural source filter (NSF) strategy for enhanced quality. C.6. Mamba-Transformer Block The integration of Transformer and Mamba layers provides flexible framework to balance the often competing objectives of low memory consumption, high computational throughput, and output quality (Fei et al., 2024). As sequence lengths increase, attention operations progressively dominate computational costs. In contrast, Mamba layers are inherently more compute-efficient, and increasing their proportion within the model improves throughput, particularly for longer sequences. After experimentation, we determine that an optimal balance is achieved with an attentionto-Mamba ratio of 1 : K, where is set to 3. To enhance training stability and prevent numerical instability caused by uncontrolled growth in absolute values, we adopt RMSNorm (Zhang & Sennrich, 2019). For encoding scene s, we encode video frame or textual scene descriptions into scene embedding zs. Subsequently, the global embedding zg is computed by averaging the prompt audio embedding za and the scene embedding zs along the temporal dimension, followed by the addition of the time step 2https://github.com/librosa/librosa ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting CrossAttention(zp(Q), za(K), za(V )) Algorithm 1 Pseudo-Code of Drama-MOE Routing Strategy Input: Input hidden representation h, content embedding zc, prompt audio embedding za, pose embedding zp, time step Output: Output with enhanced quality and control ofinal 1: Initialize Gumbel-Softmax temperature Ï , sample Gumbel noise Î¶ 2: for each time step do Prosidic MOE: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: Return ofinal Use Cross-Attention modeling prosody for alignment between za and zp: zpro Use Gumbel-Softmax for each token in the time channel to select an expert by zpro: gprosodic(h) Compute prosodic MOE output: oprosodic Spatial MOE: Use Gumbel-Softmax for each token in the time channel to select an expert by zp: gspatial(oprosodic) Compute spatial MOE output: gspatial,i ospatial ospatial as the final routed output Experti,spatial(oprosodic) GumbelSoftmax(zpro GumbelSoftmax(zp Experti,prosodic(h) Wprosodic + Î¶)/Ï Wspatial + Î¶)/Ï gprosodic,i (cid:80) (cid:80) embedding zt. This global embedding is processed through global adaptor, which modulates the latent representation via adaptive layer normalization (AdaLN) (Peebles & Xie, 2023) to ensure style consistency. The scale and shift parameters are calculated using linear regression: AdaLN (h, c) = Î³c LayerN orm(h) + Î²c, (17) where represents the hidden representation. The batch normalization scale factor Î³ is zero-initialized in each block (Peebles & Xie, 2023). Additionally, we utilize rotary positional embeddings (RoPE) (Su et al., 2024) as form of relative positional encoding, injecting temporal positional information into the model. This enhances the models ability to capture temporal relationships between sequential frames, leading to notable performance gains in the transformer. Furthermore, zero-initialized attention mechanism (Bachlechner et al., 2021) is employed. Given the queries Qh, keys Kh, and values Vh derived from the hidden states, as well as the scene keys Ks and values Vs, the final attention output is computed as: (cid:32) Qh Kh (cid:33) (cid:33) Vh+ (18) Attention = softmax tanh(Î±)softmax Vs, (cid:32) QhK s C.7. Drama-MOE Following previous works (Zhang et al., 2025), our routing mechanism employs the dense-to-sparse Gumbel-Softmax technique (Nie et al., 2021) to achieve adaptive and efficient expert selection. This method utilizes the Gumbel-Softmax trick, which reparameterizes categorical variables to make sampling differentiable, enabling dynamic routing. For hidden state h, the routing score assigned to expert i, denoted as g(h)i, is calculated as: g(h)i = exp((h j=1 exp((h (cid:80)N Wg + Î¶i)/Ï ) Wg + Î¶j)/Ï ) , (19) where Wg is the trainable gating weight, Î¶ represents noise sampled from Gumbel(0, 1) distribution (Jang et al., 2016), and Ï denotes the softmax temperature. At the beginning of training, Ï is set to high value, promoting denser routing where multiple experts may contribute to processing the same input. As training advances, Ï is gradually lowered, resulting in more selective routing with fewer experts involved. When Ï approaches zero, the output distribution becomes nearly one-hot, effectively assigning each token to the most relevant expert. Following the approach of (Nie et al., 2021), we gradually decrease Ï from 2.0 to 0.3 during training to transition from dense to sparse routing. During inference, deterministic routing mode is applied, ensuring that only one expert is chosen for each token. The complete Drama-MOE algorithm is outlined in Algorithm 1. where Qh and Kh incorporate RoPE for queries and keys, represents the dimensionality of these vectors, and Î± is zero-initialized learnable parameter that modulates the cross-attention with the scene embedding. To prevent overloading any single expert and to ensure balanced utilization, we integrate load-balancing loss for each expert group, as described in Section 4.3, following the approach in (Fedus et al., 2022). For the regularization 17 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting strength hyperparameter, we set it to 0.1 in our implementation. The load-balancing mechanism promotes more uniform allocation of tokens across experts, thereby improving training efficiency by mitigating issues such as expert underutilization or excessive workload. Consequently, our routing strategy not only facilitates dynamic and adaptive expert selection but also ensures an even distribution of computational resources. This leads to reduced training time and enhanced performance for the Drama-MOE. D. Evaluation Metrics D.1. Objective Evaluation We evaluate speech intelligibility using the Character Error Rate (CER). The CER is calculated by comparing the transcribed text from speech to the original target text by Paraformer-zh (Gao et al., 2022). To objectively evaluate speaker similarity, we employ Cosine Similarity (SIM). SIM measures the resemblance in speaker identity between the synthesized speech and the GT speech by computing the average cosine similarity between the embeddings extracted from the synthesized and GT speech, thus serving as an objective measure of speaker similarity. We use WavLM (Chen et al., 2022) model fine-tuned for speaker verification 3 to extract speaker embeddings. We employ F0 Frame Error (FFE) to evaluate the synthesis prosody of the test set objectively. FFE combines metrics for voicing decision error and F0 error, capturing essential synthesis quality information. For the objective evaluation of IPD and ILD, we first convert the time-domain signal x(n) into the frequency-domain signal X(t, ) using the short-time Fourier transform (STFT): 1 (cid:88) , ej2Ïf n, n) n=0 { w(t xi(n) Xi(t, ) = 1, 2 } (20) n) is window function, is the window where w(t length, and indicates the channel of the binaural audio. Next, we calculate the mel-spectrogram, IPD, and ILD based on the frequency-domain signals Xi(t, ). The melspectrogram for each channel is calculated as: Si(t, m) = log (cid:0) 2 Xi(t, ) melW(cid:1) , (21) where melW is an -bin mel filter bank. The IPD is derived from the phase spectrograms of the left and right channels: IP D(t, ) = X2(t, ) X1(t, ) . (22) ILD is extracted from the loudness spectrum of the left and 3https://huggingface.co/pyannote/speaker-diarization 18 right channels: ILD(t, ) = 20 log10 (cid:18) X2(t, ) X1(t, ) (cid:19) + Îµ + Îµ , Îµ = 1e10. (23) We calculate Mean Absolute Error (MAE) metrics based on the IPD and ILD extracted from the ground truth (GT) and the predicted speech. Since the IPD here is in radians and the ILD uses log10, the resulting values are quite small, especially after averaging the MAE over the time dimension. So, we multiply by 100 to make the results more intuitive. Additionally, we analyze angular and distance metrics using SPATIAL-AST (Zheng et al., 2024). SPATIAL-AST encodes an angle and distance embedding for binaural audio. We compute and average the cosine similarity for each 1-second segment based on the GT and predicted audio. D.2. Subjective Evaluation We conduct Mean Opinion Score (MOS) as subjective evaluation metric. For each task, we randomly select 40 pairs of sentences from our test set for subjective evaluation. Each pair consists of an audio prompt that provides timbre, and synthesized speech sample, both of which are evaluated by at least 15 professional listeners. In the context of MOS-Q evaluations, these listeners are instructed to concentrate on synthesis quality (including clarity, naturalness, rich stylistic details, coherence, and spatial perception for binaural audio). Conversely, during MOSS evaluations, the listeners are directed to assess speaker similarity (in terms of timbre, accent, and articulation) to the audio prompt. For MOS-E, the listeners are informed to evaluate prosodic expressiveness. For MOS-P, the listeners are instructed to evaluate the consistency between the pose information provided by the multimodal prompt and the generated binaural speech (focusing on angle, distance in 3D space, and changes caused by movement). In MOS evaluations, listeners are requested to grade various speech samples on Likert scale ranging from 1 to 5. Notably, all participants are fairly compensated for their time and effort. We compensated participants at rate of $12 per hour, with total expenditure of approximately $300 for participant compensation. Participants are informed that the results will be used for scientific research. The instructions for testers in monaural and binaural evaluation are shown in Figure 7 and Figure 8. For CMOS, listeners are asked to compare pairs of audio generated by systems and and indicate their preference between the two. They are then asked to choose one of the following scores: 0 indicating no difference, 1 indicating slight difference, 2 indicating significant difference, and 3 indicating very large difference. ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Figure 7. Screenshot of monaural speech evaluation. E. Extended Experiments E.1. Baseline Models UniAudio (Yang et al., 2023) uses LLMs to generate multiple audio types by tokenizing target audio and conditions, concatenating them into single sequence, and performing next-token prediction. multi-scale Transformer is introduced to handle long sequences caused by neural codecbased VQ. We employ their official code 4. StyleTTS 2 (Li et al., 2024c) combines style diffusion and adversarial training with large speech language models (SLMs) for high-quality text-to-speech synthesis. It models style as latent random variable using diffusion models. We employ their official code 5. CosyVoice (Du et al., 2024) represents speech with supervised semantic tokens derived from multilingual speech recognition model, using vector quantization in the encoder. It employs an LLM for text-to-token generation and con4https://github.com/yangdongchao/UniAudio 5https://github.com/yl4579/StyleTTS2 19 ditional flow matching model for token-to-speech synthesis. We employ their official code 6 FireRedTTS (Guo et al., 2024) is language-model-based TTS system that encodes speech into discrete semantic tokens via semantic-aware speech tokenizer. language model then generates these tokens from text and audio prompts, followed by two-stage waveform generator for high-fidelity synthesis. We employ their official code 7. F5-TTS (Chen et al., 2024), non-autoregressive text-tospeech system based on flow matching with Diffusion Transformer (DiT). Text input is padded with filler tokens to match the length of input speech, followed by denoising for speech generation. We employ their official code 8. 6https://github.com/FunAudioLLM/CosyVoice 7https://github.com/FireRedTeam/FireRedTTS 8https://github.com/SWivid/F5-TTS ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Figure 8. Screenshot of binaural speech evaluation. E.2. Visualization Results Figure 9 presents visualization of our binaural speech synthesis results. In Figures (a) and (b), we illustrate the mel spectrogram and F0 contour of single speech sample for the left and right audio channels. ISDrama effectively captures fine-grained mel details and generates expressive F0 variations, demonstrating high-quality synthesis and rich prosody. Figure (c) visualizes the Interaural Level Difference (ILD). The darker coloration on the left side indicates that the sound source initially originates from the back-left position, while the lighter coloration on the right side corresponds to the source moving toward the back-right. This clearly shows that ISDrama successfully models positional information, accurately reflecting spatial dynamics. E.3. Drama-MOE For the Spatial-MOE component in Drama-MOE, we find that expert selection is highly influenced by the horizontal angle (azimuth). To analyze this, we evaluate the softmax output probabilities before expert selection. As shown in Figure 10, we only present results for the Table 5. Ablation study for Drama-MOE. Expert CMOS-Q CMOS-E 1 2 3 4 5 -0.63 -0.47 -0.19 0.00 -0. -0.79 -0.44 -0.25 0.00 0.07 0180 azimuth range, as expert selection patterns for the front (0180) and back (180360) azimuths are highly similar. Thus, we aggregate them for clarity. Our results reveal distinct expert specialization based on azimuth: 1) Expert 1 primarily covers 0 to 45. 2) Expert 2 is most active in the 45 to 120 range. 3) Expert 3 focuses on 1200 to 135. 4) Expert 4 is dominant in 135 to 180. This distribution demonstrates the effectiveness of our loadbalancing strategy, ensuring that Spatial-MOE effectively selects different experts based on spatial information. Furthermore, we conduct an ablation study on the number of experts in Spatial-MOE, as shown in Table 5. To evaluate perceptual differences, we performed CMOS (Compara20 ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting Figure 9. Visualization results. Table 6. Ablation study for context-consistent CFG. Î± 1.00 0.40 0.40 1.00 0.40 0.00 0.40 Î³ CMOS-Q CMOS-S CMOS-E 1.00 1.00 1.50 3.00 3.00 3.00 5. -0.62 -0.31 -0.36 -0.28 0.00 -0.30 -0.49 -0.09 -0.16 -0.22 -0.02 0.00 -0.24 0.04 -0.78 -0.43 -0.51 -0.55 0.00 -0.22 -0.60 prosody of the prompt audio, resulting in lower CMOS-S and suboptimal CMOS-E. When Î³ ranges from 1.5 to 3, the generated speech aligns with the pronunciation and accent of the prompt audio. Within this range, if Î± is appropriately set, the model effectively utilizes the previous prompt audio to synthesize semantically aligned dramatic prosody. When Î³ > 5, the prosody adheres to the style of the prompt audio but reduces the ability to adapt to semantic learning. On the other hand, when Î± approaches 0, the CMOS-S decreases as the model over-relies on contextual cues, neglecting the finegrained details of the accent and pronunciation in the prompt audio. Conversely, when Î± approaches 1, the CMOS-E decreases because the model becomes overly dependent on the prompt audio, making it challenging to model semantically aligned dramatic prosody. By setting Î³ = 3 and Î± = 0.4, we achieve improved generation quality and incorporate previously generated audio to enhance the prosody consistency of the same speaker within single dramatic act. Figure 10. The statistics of spatial routing in Drama-MOE. tive Mean Opinion Score) test. Our results indicate consistent improvement in generation quality as the number of experts increases. However, beyond four experts, the gains become marginal or negligible. This is likely due to two factors: 1) Increased model complexity and learning burden. An excessive number of experts may introduce unnecessary redundancy, making it harder for the model to converge effectively. 2) Higher computational cost. More experts lead to significantly increased training and inference overhead without proportional gains in performance. Based on these findings, we set the number of experts per group to four, achieving balance between performance and efficiency. E.4. Context-Consistent CFG Following previous works about CFG (Jiang et al., 2025), we conduct experiments to verify the parameter settings of Equation 5, as shown in Table 6. For evaluation, we implement CMOS assessments. When Î± = 1, vcf relies solely on the input prompt audio. Moreover, if Î± = 1 and Î³ = 1, vcf becomes equivalent to the original formulation a, C; Î¸). vt(x, When Î³ increases from 1 to 1.5, the generated speech exhibits inconsistencies between the pronunciation and"
        }
    ],
    "affiliations": [
        "Zhejiang Univeristy"
    ]
}