{
    "paper_title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception",
    "authors": [
        "Yuheng Shi",
        "Xiaohuan Pei",
        "Minjing Dong",
        "Chang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire high-resolution images is computationally prohibitive. While recent methods leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present a difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the model's internal attention are computationally inefficient and less accurate, requiring either multi-pass prefill stages or reliance on the slow auto-regressive decoding process. In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around a pipeline that transforms the noisy attention maps from the MLLM's middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train a lightweight Region Proposal Network (RPN) that learns a more precise localization. This RPN is also highly efficient, predicting the RoI in a single forward pass using features from the MLLM's middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass operations.To validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over a 10% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning. Code is available at https://github.com/YuHengsss/SD-RPN."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 4 4 9 6 1 . 9 0 5 2 : r Preprint. Under review. CATCHING THE DETAILS: SELF-DISTILLED ROI PREDICTORS FOR FINE-GRAINED MLLM PERCEPTION Yuheng Shi1 Xiaohuan Pei1 Minjing Dong2 Chang Xu1 1University of Sydney yuhengshi99@gmail.com, xiaohuan.pei@sydney.edu.au, minjdong@cityu.edu.hk, c.xu@sydney.edu.au 2City University of Hong Kong"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire highresolution images is computationally prohibitive. While recent methods leverage Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the models internal attention are computationally inefficient and less accurate, requiring either multipass prefill stages or reliance on the slow auto-regressive decoding process. In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around pipeline that transforms the noisy attention maps from the MLLMs middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train lightweight Region Proposal Network (RPN) that learns more precise localization. This RPN is also highly efficient, predicting the RoI in single forward pass using features from the MLLMs middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass operations. To validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over 10% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning. Code is available at https://github.com/YuHengsss/SD-RPN."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent years have witnessed significant advancements in Multimodal Large Language Models (MLLMs), which have evolved from foundational architectures like LLaVA (Liu et al., 2023b) to more sophisticated systems (Wang et al., 2024a; Bai et al., 2023; Chen et al., 2024a;b) such as Qwen2.5-VL (Bai et al., 2025) and InternVL-3.0 (Zhu et al.). The performance of MLLMs is tied to the quality of their visual perception. In typical MLLM architecture, vision encoder, such as CLIP (Radford et al., 2021), processes visual signals and projects them into the embedding space of Large Language Model (LLM) (Touvron et al., 2023; Zheng et al., 2023) for subsequent reasoning. Consequently, the richness of these visual features is crucial for the model to achieve comprehensive and fine-grained understanding of the input. To enhance this fine-grained perception, scaling the resolution of visual inputs has emerged as direct and effective strategy. The approaches to achieve this have evolved considerably. Initial methods (Liu et al., 2023b; Dai et al., 2023; Bai et al., 2023) relied on fixed-resolution input. Subsequent research introduced more flexible techniques, such as S2 (Shi et al., 2024), Any-Resolution (Liu et al., 2023a; Chen et al., 2024b), and Naive Dynamic Resolution (Wang et al., 2024a; Bai et al., 2025) to handle higher-resolution imagery. However, processing entire high-resolution images uniformly is computationally intensive. More recently, an alternative paradigm (Yu et al., 2025; Shi 1 Preprint. Under review. (a) (b) Figure 1: (a) The Pipeline of SD-RPN. The RPN is trained with pseudo-labels to effectively predict RoIs. These RoIs are then used to crop fine-grained sub-images for the final inference stage. (b) Performance Comparison. Performance evaluation with S2 (Shi et al., 2024) and ViCrop (Zhang et al., 2025) on the LLaVA-1.5-7B baseline. Accuracy is averaged over five Document and OCR benchmarks. Our SD-RPN achieves superior trade-off between performance and throughput. et al., 2025) has gained traction: identifying Region-of-Interest (RoI) within low-resolution input and then selectively scaling up only that specific region. This RoI-centric approach has proven to be more efficient and effective means of improving visual detail perception. While the RoI paradigm marks significant step toward efficient high-resolution processing, current methodologies for identifying these regions still present notable limitations. Approaches such as VILA-HD (Shi et al., 2025), for instance, rely on large-scale pre-training with detailed annotations, process that is both data-intensive and computationally demanding. Furthermore, they often require complete prefilling stage for the initial low-resolution image, which can impede both training and inference efficiency. An alternative direction leverages the intrinsic localization capabilities of MLLMs (Kang et al., 2025b; Zhang et al., 2025) without training, specifically by computing crossattention scores between image tokens and corresponding textual description. However, leveraging this internal attention for RoI identification is often computationally inefficient in inference, as current methods typically require either complex, multi-pass operations during the prefill stage (Zhang et al., 2025) or rely on the inherently sequential and slow auto-regressive decoding stage (Wang et al., 2024b). Consequently, an effective and efficient method for RoI identification that avoids both reliance on extensive annotated data and the high latency of auto-regressive decoding or multiple forward passes remains critical, yet underexplored, challenge. In this work, we introduce SD-RPN, novel self-distillation framework (as shown in Fig. 1a) designed to overcome these limitations by efficiently harnessing the MLLMs intrinsic localization capabilities. Our approach is motivated by the insight that while an MLLMs internal attention provides strong RoI signal, it is too noisy (e.g., attention sinks, incomplete activation) for direct supervision. Previous studies reveal that using such noisy signals for dense supervision yields suboptimal results (Wang et al., 2022; Tian et al., 2021). To address this, we propose pseudo-labeling pipeline that transforms the noisy attention map into sparse and reliable supervisory signal. This pipeline first denoises the map by removing sink tokens based on feature norms. Subsequently, it employs selective classification strategy that assigns discrete labels based on confidence thresholds and minimal bounding box around high-attention tokens, which resolves the ambiguity inherent in the original noisy map. We use these pseudo labels to train lightweight RPN. By learning from this distilled knowledge, the RPN develops more precise localization function than methods relying on raw, ambiguous attention. Beyond its accuracy, the RPNs architectural design confers significant efficiency gains. Composed of few transformer blocks built upon the frozen MLLM backbone, the RPN operates on features from the models middle layers. This strategic placement allows it to predict the RoI by executing only partial forward pass up to the MLLMs middle layers, completely decoupling localization from the slow, auto-regressive generation process. The entire framework is trained end-to-end, distilling the localization knowledge from the models own response-guided attention into the efficient RPN. This enables dynamic two-stage inference process where the model first predicts salient regions and then analyzes high-resolution crops to generate its final response. 2 Preprint. Under review. Our SD-RPN achieves significant enhancement (Fig. 1b) in fine-grained perception without the cost of full-model fine-tuning or large-scale annotated datasets. Moreover, our framework demonstrates exceptional training efficiency and generalization. To validate our method, we integrate it into the widely-used LLaVA-1.5 (Liu et al., 2023a), aligning with previous works (Zhang et al., 2025; Kang et al., 2025b). When being trained on only 10K randomly selected question-answer pairs from GQA (Hudson & Manning, 2019) and OCR-VQA (Mishra et al., 2019), our method yields substantial gains on unseen data, achieving over 10% absolute accuracy improvement on benchmarks such as TextVQA (Singh et al., 2019), DocVQA (Mathew et al., 2020), and V-Star (Wu & Xie, 2023). Our contributions are twofold. First, we introduce robust pipeline to denoise the internal attention maps of an MLLM, generating high-quality pseudo-labels for supervision. Second, we propose novel, annotation-free self-distillation framework that trains lightweight RPN to predict RoIs by leveraging the MLLMs intrinsic localization knowledge."
        },
        {
            "title": "2.1 PERCEPTION IN MLLMS.",
            "content": "Recent studies have established that MLLMs often struggle with fine-grained perception, limitation rooted in the challenge of efficiently processing high-resolution visual inputs (Tong et al., 2024b; Kang et al., 2025b; Shi et al., 2024; Liu et al., 2024a; Chen et al., 2024a). One line of research has focused on enhancing the models global visual understanding. This includes developing more sophisticated vision encoders (Wang et al., 2024a; Bai et al., 2025; Zhang et al., 2024b; Ge et al., 2024; Luo et al., 2024), supplementing the LLM with full high-resolution images (Liu et al., 2024a; Zhu et al.; Huang et al., 2024; Tong et al., 2024a), and incorporating external tools (Zhao et al., 2024). More recent works Shi et al. (2025); Yu et al. (2025); Zheng et al. (2025); Shao et al. (2024) have demonstrated that identifying the RoI first in relatively low-resolution visual input and then scaling the resolution specifically for the RoI is more efficient and effective. However, they come at the expense of extensive training, requiring massive supervision and costly annotations. While recent training-free methods (Zhang et al., 2025; Wang et al., 2024b) attempt to identify RoIs by utilizing the internal perceptual capabilities of MLLMs, they are often hampered by noisy activations or require slow auto-regresssive decoding stage and multiple forward passes, which hinders both performance and efficiency. 2.2 SELF-DISTILLATION IN MLLMS. Knowledge distillation is an effective paradigm for transferring knowledge from teacher to student network (Hinton et al., 2015). Among its variants, self-distillation (Zhang et al., 2019) emerges as unique form in which the teacher and student share the same architecture, and it has been widely adopted in visionlanguage tasks. In multimodal pre-training, several works (Oquab et al., 2023; Cai et al., 2024; Dong et al., 2023; Kim et al., 2025) leverage self-distillation to improve cross-modal alignment and representation learning, where it is commonly employed to enhance feature extraction. In multimodal downstream tasks, recent efforts (Kong et al., 2024; Peng et al., 2025; Hou et al., 2024) take advantage of self-distillation to refine visiontext region alignment and thereby improve grounding and reasoning performance. As self-distillation eliminates the need for larger pre-trained teacher, it naturally offers strong potential for extracting high-resolution and fine-grained perceptual cues directly from MLLMs themselves."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PRELIMINARIES The standard architecture of Multimodal Large Language Model (MLLM) comprises three core components: vision encoder, Ev, vision-language projector, P, and Large Language Model (LLM), L. Given an image-text pair, (xv, xt), the MLLM processes the inputs to generate textual response. The vision encoder Ev extracts feature vectors from the input image xv. These visual features are then transformed by the projector into sequence of visual embeddings, H0 = P(Ev(xv)), which are aligned with the LLMs input space. The input text xt is processed by tokenizer and an embedding layer to produce sequence of text embeddings, which 3 Preprint. Under review. Figure 2: An overview of our pseudo-label generation pipeline. FG and BG denote the foreground and background respectively. Layer index is omitted for simplicity. typically includes system prompt and user query, yielding H0 user, respectively. After encoding H0 user in parallel (called the prefilling stage), the MLLM generates responses auto-regressively (called the decoding stage). Due to the lack of parallelization, the decoding stage is typically much slower than the prefilling stage when processing the same number of tokens. sys and sys, H0 v, H0 The attention scores are key to identifying image regions relevant to the text (Kang et al., 2025b). RoI RHW , which From the cross-modal attention in layer l, we could derive RoI map, Ml represents the averaged importance of each visual token across textual tokens of user query or RoI = (cid:80)Nt response. For single attention head, this map is computed as Ml i/Nt, = R(HW )d are the query and key matrisoftmax ces from the Nt response tokens and visual tokens, respectively. RNtd and Kl , where Ql i=1 Al v)T / t(Kl Ql (cid:16) (cid:17) 3.2 PSEUDO-LABEL GENERATION FOR ROI While the RoI map MRoI provides valuable signal for localizing text-relevant image regions, it is often fraught with noise. As illustrated in Fig. 2, these raw maps can exhibit erroneously high attention in background areas and incomplete activation across the true foreground object. To overcome this, we propose pseudo-label generation pipeline, which is depicted in Fig. 2, to transform the noisy RoI map into sparse and reliable supervisory signal for training our RPN. The first source of noise we address is the sink tokens. These are visual tokens that attract substantial attention despite being semantically irrelevant to the grounded object. As identified in recent studies (Darcet et al., 2023; Kang et al., 2025a), sink tokens can be identified by the high L2-norm of their corresponding feature vectors Hv. To mitigate them, we first denoise the initial RoI map MRoI by nullifying the scores of all identified sink tokens, yielding cleaner version, Figure 3: Attention magnitude VS. Localization accuracy. RoI: (M RoI)j = (cid:26)0 (MRoI)j if (Hv)j2 > τnorm otherwise , (1) where τnorm is predefined norm threshold. Label Assignment. Following the removal of sink tokens, the denoised RoI map RoI more clearly highlights the foreground area. However, it is not yet an ideal supervisory signal, as it still suffers from an obscure foreground-background margin and incomplete object activation. To empirically investigate the margin issue, we analyze the denoised attention maps RoI on the TextVQA subset (Zhang et al., 2025). Fig. 3 plots the proportion of tokens falling inside the groundtruth (GT) bounding box as function of their sample-wise relative attention score, calculated as a/amax for each attention value RoI. The histogram reveals that the proportion of tokens inside the GT box is approximately 40% for high attention scores (e.g., > 0.2) and 10% for low scores (e.g., < 0.1). However, the ambiguous middle-range of attention not only exhibit distinct localization pattern from the more decisive high and low scores, but they also vastly outnumber the high-attention tokens. Using such signal for dense supervision would inevitably force the model to 4 Preprint. Under review. Figure 4: Overview of our SDRPN framework. Our lightweight RPN (top) is initialized from and built upon frozen MLLM backbone to efficiently predict dense RoI map. It is trained via self-distillation (bottom), where pseudo-labels are generated by denoising the full MLLMs internal response-to-image attention maps. Superscripts denote layer indices; subscripts denote token sources. We omit the system prompt tokens for brevity. learn from these numerous ambiguous regions. To address this, we avoid regressing on RoI directly and instead implement selective binary classification that assigns foreground or background only to high-confidence tokens, leaving ambiguous tokens to be ignored. This approach is also designed to alleviate the incomplete activation problem. We define minimal bounding box, Bf g, which encloses identified foreground tokens. Tokens inside this box that is not classified as foreground is explicitly ignored, which prevents the RPN from receiving erroneous background signals from inactivated parts of the true object. Guided by these criteria, the final pseudo-label map, MRoI is constructed by: ( MRoI)j = 1 0 1 if token Sf g, if token Sbg, otherwise (ignored), (2) where the foreground set is Sf = {j aj τf amax} and the background set is Sbg = {j / Bf and aj τbg amax}. In Appendix E, we also provide theoretical view of why learning to predict RoI labels is superior than using the raw attention maps directly. 3.3 ROI PREDICTION VIA SELF-DISTILLATION As validated by recent studies (Kang et al., 2025b; Shao et al., 2025), the middle layers of MLLMs exhibit significant potential for RoI prediction. Given these observations, can we leverage the localization capabilities inherent in the middle layers of MLLMs to build more efficient RoI predictor in both inference and data? To answer this question, we propose lightweight and tunable SelfDistilled Region Proposal Network (SD-RPN) that consists of transformer blocks built upon the first frozen MLLM layers, which serve as the backbone. Predicting the RoI Map. The architecture of our SD-RPN is illustrated in Fig. 4. We initialize the RPNs weights using those from layers to + of the pretrained MLLM, strategy that enables the efficient transfer of learned representations (Shao et al., 2025). Instead of predicting sparse bounding boxes, our well-initialized RPN is trained to predict dense RoI map, ˆMRoI. The prediction process is triggered by specific query tokens derived from the conversational context. Concretely, for given image and conversation with turns, we first collect the sequence of hidden states, H, from the second last layer of the RPN: = [Hsys, Hv, Hu(1), Hr(1), . . . , Hu(n), Hr(n)], (3) 5 Preprint. Under review. where the subscript of and denote user and response tokens. From this sequence, we specifically isolate the hidden state corresponding to the final token of each user question, Hu(i)[1], as these tokens immediately precede the models answers and serve as the most direct prompt for generating grounded response. These query vectors are collected into single tensor, HRoI, which serves as the input for our RoI prediction head: HRoI = concat(Hu(1)[1], . . . , Hu(n)[1]), (4) where HRoI Rnd. These query vectors, along with the visual token hidden states Hv, are then projected into the query and key space using the linear layers (LPq and LPk) from the RPNs final attention block: QRoI = LPq(Norm(HRoI)), Kv = LPk(Norm(Hv)), where Norm denotes the layer normalization (Zhang & Sennrich, 2019; Ba et al., 2016). The predicted dense RoI map, ˆMRoI is then computed via simple matrix multiplication of these query and key matrices: ˆMRoI = QRoIKT . For brevity, the head dimension is omitted here; in practice, attention scores are computed per head and then averaged. Notably, this design is more efficient than exsiting frameworks (Shi et al., 2025; Zhang et al., 2025) as it only need single forward through partial LLM layers. (5) Training via Self-Distillation. The RPN is trained entirely through self-distillation to predict the pseudo-label map MRoI which is generated via the pipeline in Section 3.2 by minimizing binary cross-entropy (BCE) loss, defined as LBCE( ˆMRoI, MRoI). While the text responses used for pseudo-label generation could theoretically originate from stronger teacher model (e.g., GPT-4) or human annotations, our empirical results reveal crucial insight: using the MLLMs self-predicted responses yields superior performance (detailed in Tab. 4b). We attribute this counter-intuitive result to the principle of representational consistency. Although an external teachers response may be more accurate, the attention maps it induces can be out-of-distribution for the student RPN. In contrast, the attention maps from self-generated responses, even if imperfect, are inherently aligned with its own internal visual grounding mechanisms. This creates more consistent and attainable distillation target, which proves more effective for training the lightweight RPN, especially in our data-efficient setting. This self-sufficient framework thus removes any dependency on external models or annotated data. 3.4 TWO-STAGE INFERENCE WITH ROI The predicted RoI map, ˆMRoI, enables dynamic, two-stage inference process that significantly enhances the models fine-grained perception capabilities. The first stage involves predicting and post-processing ˆMRoI to produce clean binary foreground mask, B. To consolidate activated regions and ensure robustness against noise, the dense map ˆMRoI is reshaped into 2D map (γ), smoothed with Gaussian filter (G), and then binarized using fixed threshold (τ ): B(x, y) = (cid:40) 1, 0, if G(γ( ˆMRoI))(x, y) > τ, otherwise, (6) where (x, y) represents the spatial coordinates. With this binary mask, we proceed to the second stage to extract fine-grained visual features. We explore two different upscaling strategies for the predicted RoI. The first, which we term Box Upscaling, processes each salient region independently. We first identify all distinct connected-component regions, {Ri}k i=1, within the mask B. minimal, axis-aligned bounding box, bi, is then computed for each region. These bounding boxes are used to crop sub-images, {xvi}k i=1, from the original source image, which are then encoded to produce new, high-resolution visual embeddings H0 vbox: bi = bbox(Ri), H0 vbox = {P(Ev(xvi ))}k i=1, (7) where bbox() is the operator that returns the coordinates of the minimal bounding box enclosing the input region. Alternatively, our second strategy, which we term Masked Upscaling, takes unified approach. This method first computes single, all-encompassing bounding box, ball, that encloses the union of all connected foreground regions, (cid:83)k i=1 Ri. This unified bounding box is used to crop 6 Preprint. Under review. Table 1: Performance on Document & OCR benchmarks. Dataset subscripts denote the evaluation split. Performance subscripts show the absolute improvement () over the baseline. Throughput is relative to the baseline, measured on single NVIDIA A6000 GPU. Methods Throughput DocVQAval ChartQAtest OCRBenchtest InfoVQAval TextVQAval Ave. LLaVA-1.5-7B +S2 +ViCrop +SD-RPN(Ours) LLaVA-1.5-13B +S2 +ViCrop +SD-RPN(Ours) DeepSeek-VL-1.3B +SD-RPN(Ours) DeepSeek-VL-7B +SD-RPN(Ours) 1.0 0.70 0.42 0.62 1.0 0.71 0.39 0.51 1.0 0.47 1.0 0.40 21.5 27.1 27.0 33.912.4 23.5 30.7 30.2 39.716.2 37.0 52.915.9 50.7 66.716.0 18.1 18.9 20.0 20.12. 18.1 20.3 20.1 21.43.3 48.6 51.32.7 61.4 63.62.2 31.4 32.6 33.2 37.05.6 33.7 36.4 36.1 38.85.1 38.9 40.71. 42.4 47.95.5 20.4 22.5 21.4 22.01.6 23.4 24.7 25.9 24.71.3 21.7 24.22.5 30.4 36.25.8 46.1 52.6 57.2 58.712. 48.7 54.5 60.3 62.313.6 55.8 65.910.1 63.0 71.58.5 27.5 30.7 31.8 34.36.8 29.5 33.3 34.5 37.47.9 40.4 47.06. 49.6 57.27.6 Table 2: Performance on Vision-Centric and High-Resolution benchmarks. Methods LLaVA-1.5-7B +S2 +ViCrop +SD-RPN(Ours) LLaVA-1.5-13B +S2 +ViCrop +SD-RPN(Ours) DeepSeek-VL-1.3B +SD-RPN(Ours) DeepSeek-VL-7B +SD-RPN(Ours) V* Bench POPE HR-Bench 4K HR-Bench 8K Attr Spatial Overall F1 Acc. FSP FCP Overall FSP FCP Overall 48.7 53.0 53.9 67.8 47.0 43.5 47.8 59.1 34.8 53.0 37.4 52.2 52.6 59.1 50.0 67. 56.6 59.2 57.9 67.1 55.3 57.9 50.0 52.6 50.3 55.5 52.4 67.517.2 50.8 49.7 51.8 62.311.5 42.9 55.012. 42.4 52.410.0 85.9 87.1 88.0 87.11.2 85.9 87.3 88.0 87.31.4 86.1 87.61.5 86.0 87.61.6 87.4 87.9 88.6 87.80. 87.1 88.1 88.7 88.11.0 87.1 88.31.2 87.1 88.11.0 39.5 49.8 60.8 60.3 41.5 51.8 66.3 60.8 40.5 59. 47.5 60.5 35.5 38.3 34.8 35.8 44.5 46.5 40.3 44.8 33.0 33.5 41.5 40.0 37.5 44.0 47.8 48.010. 43.0 49.1 53.3 52.89.8 36.8 46.39.5 44.5 50.35.8 32.5 40.5 38.5 46.5 36.6 41.3 44.5 51.0 30.1 39. 37.3 53.3 33.8 36.5 33.8 31.8 38.5 44.5 37.0 36.3 29.3 31.5 41.3 38.8 33.8 38.5 36.1 39.15. 36.6 42.9 40.6 43.67.0 30.1 35.45.3 39.3 46.06.7 single sub-image, xvall , which is then passed through the vision encoder and projector to produce set of high-resolution embeddings H0 ball = bbox( Ri), vmask = P(B Ev(xvall )), (8) i=1 where represents masking operation that uses to select the foreground features from the encoders output. Box Upscaling can achieve higher effective resolution for small, individual regions and Masked Upscaling better preserves the global spatial and positional relationships between all foreground elements, which is crucial for structured data. In the second stage, the high-resolution visual tokens are inserted into the sequence immediately following the original visual tokens. The LLM then performs auto-regressive decoding on this augmented context to generate the final answer. We present empirical comparison of these two upscaling strategies in our ablation study."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT CONFIGURATIONS. Benchmark Settings. To ensure comprehensive evaluation, we test our SD-RPN framework across range of benchmarks, following the protocols of established works (Shi et al., 2025; Zhang et al., 2025; Zheng et al., 2025). Our results are presented in two parts to clearly delineate performance across different domains. The first part focuses on the Document & OCR category, which assesses performance on text-rich images, and includes five benchmarks: DocVQA (Mathew et al., 2020), ChartQA (Masry et al., 2022), OCRBench (Liu et al., 2024b), InfoVQA (Mathew et al., 2022), and TextVQA (Singh et al., 2019). The second part focuses on Vision-Centric and HighResolution tasks, including V-Star Bench (V*) (Wu & Xie, 2023), POPE (Li et al., 2023), and 7 vmask : (cid:91) Preprint. Under review. HR-Bench (Wang et al., 2024b). Notably, for TextVQA, we do not provide the model with external OCR annotations, strategy adopted from ViCrop (Zhang et al., 2025) to ensure fair evaluation of the MLLMs intrinsic perceptual capabilities. Baselines and Comparison Methods. To evaluate SD-RPN, we integrate it into two prominent MLLM families. Our primary experiments are conducted on the widely-used LLaVA-1.5 (Liu et al., 2023a) architecture, across both its 7B and 13B scales. To demonstrate the generalizability of our approach, we also apply it to the more recent DeepSeek-VL model (Lu et al., 2024). We compare our method against two representative techniques for high-resolution visual processing: S2 (Shi et al., 2024), full-tuning method, and ViCrop (Zhang et al., 2025), training-free cropping baseline. To ensure rigorous and fair comparison for full reproducibility, we have re-implemented all methods within the unified lmms-eval evaluation library (Zhang et al., 2024a). Implementation Details. To train our proposed SD-RPN, we generate pseudo-labels from combined dataset of GQA (Hudson & Manning, 2019) and OCR-VQA (Mishra et al., 2019), which provides supervision for natural scenes and text-rich images, respectively. These pseudo-labels are derived from the models internal response-to-image attention maps. Inspired by recent analyses of MLLM attention mechanisms (Kang et al., 2025b), we select attention maps from the middle layers for this process. During the pseudo-label generation pipeline, the relative attention thresholds for defining the foreground (Sf g) and background (Sbg) sets are empirically set to 0.2 and 0.1, respectively. Further details on the layer selection rationale and hyper-parameter configuration used in training can be found in the Appendix A. 4.2 MAIN RESULTS We present our main results in Tab. 1 for text-rich benchmarks and Tab. 2 for vision-centric benchmarks. Our evaluation assesses both performance and efficiency. In addition to these quantitative results, we provide qualitative comparison in Appendix D. Performance. Integrating our SD-RPN framework yields substantial performance gains across all baseline models. On the text-rich benchmarks (Tab. 1), our method improves the average score by approximately 7% over the standard baselines. The benefits are also consistent on vision-centric tasks (Tab. 2), where our approach achieves an average improvement of over 10% on V-Star Bench. It is important to note that, to ensure fair comparison on V-Star Bench, the image cropping strategy employed by ViCrop was not used in our evaluation. Efficiency. The two-stage inference process used by our method and ViCrop inherently involves trade-off, reducing throughput in exchange for higher accuracy. This additional latency stems from three primary sources: the initial RoI prediction stage, the feature extraction for the high-resolution RoI crops, and the increased number of visual tokens processed by the LLM in the final generation stage. However, our SD-RPN is architected for superior efficiency in the critical RoI prediction step. By leveraging only subset of the MLLMs layers in single forward pass, our method is significantly faster than competing two-stage approaches. For example, when integrated into the LLaVA-1.5-7B model, our RoI prediction stage is 1.5 faster than that of ViCrop, demonstrating more favorable balance between performance and computational cost. 4.3 ABLATION STUDY Table 3: Ablation study on the key components of our method. (#) Setting Throughput OCRBench TextVQA POPE (0) LLaVA-1.5-7B (Baseline) (1) Response-to-Image Attention (2) Attention Prediction (3) +Label Assignment (4) +Remove Sink Tokens (5) +Masked Upscaling 1.0 0.42 0.58 0.55 0.55 0.62 31.4 32.82.4 32.20.8 33.92.5 36.24.8 37.05.6 46.1 85. V* 50.3 Ave. 53.4 53.06.9 56.810.7 57.311.2 57.911.8 58.712.6 84.31.6 56.93.8 57.67.6 85.70.2 61.310.0 59.05.3 85.60.3 68.618.3 61.47.9 86.80.9 68.618.3 62.49.0 87.11.2 67.517.2 62.69. On Pseudo-Label Gerneration & SD-RPN. Tab. 3 presents detailed ablation study dissecting the core components of our proposed framework. We begin by evaluating two baseline strategies for 8 Preprint. Under review. Table 4: (a) Varying the number of frozen backbone layers (B) while keeping the RPN layers fixed. (b) Varying the number of training data samples. The symbol indicates that pseudo-labels were generated using ground-truth responses from the LLaVA supervised fine-tuning dataset. All scores are accuracy (%) except for POPE, which reports the F1-score. Setting Throughput OCRBench Baseline 1.0 31.4 TextVQA 46. POPE V* Ave. Setting OCRBench 85. 50.3 53.4 Baseline 31.4 TextVQA 46. POPE V* Ave. 85.9 50.3 53. B3R3 B6R3 B9R3 B12R3 B15R3 B18R3 51.04.9 53.57.4 0.71 33.92.5 56.53.1 87.01.1 0.63 34.43.0 87.71.8 58.24.8 0.66 35.84.4 56.310.2 87.11.2 59.76.3 0.60 35.54.1 57.811.7 87.41.5 67.016.7 61.98.5 0.62 37.05.6 58.712.6 87.11.2 67.517.2 62.69.2 0.52 35.54.1 57.811.7 87.31.4 66.015.7 61.78.3 53.93.6 57.16.8 59.79.4 10K 25K 50K 100K 152K 152K 87.61.7 61.311.0 60.67.2 35.23.8 60.36.9 59.79.4 87.11.2 35.84.4 37.56.1 86.91.0 67.016.7 62.49.0 86.80.9 66.015.7 61.98.5 36.55.1 87.11.2 67.517.2 62.69.2 37.05.6 35.23.8 56.9 10.8 86.60.7 63.913.6 60.77. 58.312.2 58.512.4 58.312.2 58.312.2 58.712.6 (a) (b) leveraging attention. Box upscaling is adopted as the upscaling setting except additional note. In (#1), we use the raw Response-to-Image Attention map directly for RoI identification. In (#2), our RPN is trained to regress these attention scores via Mean Squared Error (MSE) loss. While both methods improve upon the baseline, their average gains of 3.8% and 5.3% are limited, confirming our hypothesis that using noisy, unprocessed attention maps for direct supervision is suboptimal. The introduction of Label Assignment strategy (#3), which creates high-confidence labels and ignores ambiguous regions, yields significant performance jump to 7.9% average improvement. This is further enhanced by Remove Sink Tokens step (#4), which denoises the attention map to achieve 9.0% average gain. These results validate that our proposed denoising pipeline is crucial for generating high-quality supervision and enabling precise RoI prediction. Aligning with previous analysis in Sec. 3.4, masked upscaling achieves superior performance on OCRBench and TextVQA. Given its better throughput (0.62x vs. 0.55x), we adopt it as the default setting. On Backbone Layers. In Tab. 4a, we conduct an ablation study on the number of frozen backbone layers (B) that the RPN is built upon, keeping the number of trainable RPN layers fixed. We observe clear trend in performance: as the backbone depth increases, the average performance steadily improves, reaching its peak with the B15R3 configuration, which achieves 9.2% gain over the baseline. Beyond this point, performance begins to decline. The relationship between backbone depth and efficiency is more nuanced. While the inference cost of the RoI prediction stage is positively correlated with the number of backbone layers, the overall throughput is not monotonic. This is because more precise RoI prediction, often produced by deeper backbone, can reduce the number of irrelevant or noisy visual tokens that are passed to the second, more costly generation stage. This interplay between the increasing cost of the first stage and the potentially decreasing cost of the second stage explains the fluctuating throughput values. On Data Efficency. We conduct ablation studies on the size of the training set, with results presented in Tab. 4b. Performance generally improves with an increased number of training samples. Remarkably, even when trained on only 10K samples, the framework achieves substantial gains over the baseline (e.g., over 10% on improvement on both TextVQA and V-Star). Further increases in data size continue to yield performance gains. The peak performance is ultimately achieved when training on the full combined dataset of 152K samples. The final row of Tab. 4b presents crucial comparison where we generate pseudo-labels using ground-truth responses from the LLaVA supervised fine-tuning (SFT) dataset. Counter-intuitively, this approach yields notably lower average performance of 60.7% compared to the 62.6% achieved by our standard self-distillation method. This suggests that the attention maps produced by the models own generated responses provide more effective and internally consistent supervision signal for distilling localization knowledge."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We proposed SD-RPN, self-distilled region proposal framework that efficiently exploits the intrinsic localization signals of MLLMs to identify Regions of Interest without external annotations or auto-regressive decoding. By attaching lightweight RPN to frozen backbones and training it with denoised pseudo-labels, our method achieves consistent improvements in fine-grained perception while maintaining strong efficiency and generalization. Extensive experiments confirm its advantage over both full-image scaling and training-free heuristics, and ablations highlight its robustness 9 Preprint. Under review. and data efficiency. Our work establishes principled direction for scalable high-resolution perception in MLLMs, and opens avenues toward adaptive token allocation and broader multimodal applications such as video and document understanding."
        },
        {
            "title": "REFERENCES",
            "content": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Yuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He, Ao Tong, Zhenye Gan, Chengjie Wang, Zhucun Xue, Yong Liu, and Xiang Bai. Llava-kd: framework of distilling multimodal large language models. arXiv preprint arXiv:2410.16236, 2024. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF Conference on Computer for generic visual-linguistic tasks. Vision and Pattern Recognition, pp. 2418524198, 2024b. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. NeurIPS, 2023. Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et al. Maskclip: Masked self-distillation advances contrastive language-image pretraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1099511005, 2023. Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, and Bo Zheng. Convllava: Hierarchical backbones as visual encoder for large multimodal models. arXiv preprint arXiv:2405.15738, 2024. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Xiaojun Hou, Jiazheng Xing, Yijie Qian, Yaowei Guo, Shuo Xin, Junhao Chen, Kai Tang, Mengmeng Wang, Zhengkai Jiang, Liang Liu, et al. Sdstrack: Self-distillation symmetric adapter learning for multi-modal visual object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2655126561, 2024. Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, and Xiang Bai. Mini-monkey: Alleviating the semantic sawtooth effect for lightweight mllms via complementary image pyramid. arXiv preprint arXiv:2408.02034, 2024. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 10 Preprint. Under review. Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. See what you are told: Visual attention sink in large multimodal models. In ICLR, 2025a. Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. Your large vision-language model only needs few attention heads for visual grounding. In CVPR, 2025b. Sanghwan Kim, Rui Xiao, Mariana-Iuliana Georgescu, Stephan Alaniz, and Zeynep Akata. Cosmos: Cross-modality self-distillation for vision language pre-training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1469014700, 2025. Jun Kong, Jin Wang, Liang-Chih Yu, and Xuejie Zhang. Multimodality self-distillation for fast inference of vision and language pretrained models. IEEE Transactions on Multimedia, 26:8928 8940, 2024. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024a. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 2024b. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Dimosthenis Karatzas, Manmatha, and CV Jawahar. Docvqa: dataset for vqa on document images. corr abs/2007.00398 (2020). arXiv preprint arXiv:2007.00398, 2020. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In WACV, 2022. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual In 2019 international conference on document question answering by reading text in images. analysis and recognition (ICDAR). IEEE, 2019. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Yi Peng, Peiyu Wang, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, et al. Skywork r1v: Pioneering multimodal reasoning with chain-ofthought. arXiv preprint arXiv:2504.05599, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 11 Preprint. Under review. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. NeurIPS, 2024. Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei, Hongyuan Zhang, Ning Mao, Wei Chen, and Jun Yu. Growing twig to accelerate large vision-language models. In ICCV, 2025. Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. When do we not need larger vision models? In ECCV, 2024. Baifeng Shi, Boyi Li, Han Cai, Yao Lu, Sifei Liu, Marco Pavone, Jan Kautz, Song Han, Trevor Darrell, Pavlo Molchanov, et al. Scaling vision pre-training to 4k resolution. In CVPR, 2025. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen. Boxinst: High-performance instance segmentation with box annotations. In CVPR, 2021. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. NeurIPS, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024b. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. arXiv preprint, 2024b. Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz, Anima Anandkumar, Chunhua Shen, and Jose Alvarez. Freesolo: Learning to segment objects without annotations. In CVPR, 2022. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Introducing visual perception token into multimodal large language model. In ICCV, 2025. Biao Zhang and Rico Sennrich. Root mean square layer normalization. NeurIPS, 2019. Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. In ICLR, 2025. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024a. URL https://arxiv.org/abs/2407. 12772. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 37133722, 2019. Preprint. Under review. Yipeng Zhang, Yifan Liu, Zonghao Guo, Yidan Zhang, Xuesong Yang, Chi Chen, Jun Song, Bo Zheng, Yuan Yao, Zhiyuan Liu, et al. Llava-uhd v2: an mllm integrating high-resolution feature pyramid via hierarchical window transformer. arXiv preprint arXiv:2412.13871, 2024b. Xiangyu Zhao, Xiangtai Li, Haodong Duan, Haian Huang, Yining Li, Kai Chen, and Hua Yang. Mg-llava: Towards multi-granularity visual instruction tuning. arXiv preprint arXiv:2406.17770, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS, 2023. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. 13 Preprint. Under review. MORE IMPLEMENTATION DETAILS. Training Setting. The hyperparameters used to train our RPN were adapted from TwigVLM (Shao et al., 2025) and are detailed in Table 5. All models were trained on server using four NVIDIA A6000 GPUs. Our training dataset consists of 152K samples, with 72K sourced from the GQA dataset (primarily natural images) and 80K from the OCR-VQA dataset (text-rich images). As point of reference for computational cost, training the RPN for the LLaVA-1.5-7B model on our full 152K-sample dataset completes in around two hours. Table 5: Training hyperparameters for the RPN. Config Setting AdamW 0. optimizer weight decay optimizer momentum β1, β2 = 0.9, 0.98 batch size learning rate schedule peak learning rate warm-up strategy warm-up ratio training epochs 128 cosine decay 5e-5 linear 0.03 1 Table 6: Model-specific layer configurations. Selected Attention Layers refers to the layers from which attention maps were extracted for pseudo-label generation. Frozen Backbone Layers (B) refers to the number of initial layers kept frozen, upon which the trainable RPN is built. Model Selected Attention Layers Frozen Backbone Layers (B) LLaVA-1.5-7B LLaVA-1.5-13B DeepSeek-VL-1.3B DeepSeek-VL-7B 14 1316 714 917 15 15 9 12 Layer Selection Rationale. The specific layers used for pseudo-label generation and for defining the RPN backbone were empirically chosen for each model architecture, with the final configurations detailed in Table 6. For generating pseudo-labels, our selection of attention maps is inspired by recent analyses (Kang et al., 2025b) which demonstrate that the middle layers of an MLLM contain the most potent visual grounding signals. These intermediate layers represent sweet spot where visual and textual representations are sufficiently fused for localization, yet before the models final layers become overly specialized for abstract reasoning and text generation, potentially weakening the direct visual signal. When range of layers is specified, the attention maps were averaged to create more stable and robust signal. The number of frozen backbone layers, B, determines the depth and richness of the features that the trainable RPN operates on. This choice represents critical trade-off. deeper backbone (larger B) provides more semantically rich features but increases the computational cost of the RoI prediction stage and can limit the learning capacity of small RPN. Conversely, shallower backbone is faster but may not provide features that are sufficiently aligned for accurate RoI prediction. The values in Table 6 were determined through ablation studies (see Table 4a) to find the optimal balance between performance and efficiency for each model. Algorithm Details. To provide concrete and reproducible specification of our training procedure, we detail the complete end-to-end process in Algorithm 1. The algorithm consists of main training loop and the core helper routines. The main training loop iterates through the dataset, performing two key stages for each sample. First, the teacher stage (lines 3-10) uses the full, frozen MLLM to generate pseudo-label. This involves auto-regressively generating response, extracting the raw response-to-image attention map from pre-selected middle layer, and then refining this map using our RemoveSinkTokens and AssignLabels helper functions. This produces the final denoised pseudo-label, MRoI. Second, the student stage (lines 11-15) trains the lightweight RPN. The RPN 14 Preprint. Under review. Algorithm 1 Self-Distilled RPN: training and helper routines Require: Full MLLM with layers; frozen depth B; RPN depth R; pseudo-label layer l; dataset D; thresholds (τnorm, τf g, τbg); optimizer for trainable RPN parameters Ensure: Trained RPN on top of the frozen backbone 1: Initialization: Initialize RPN layers B+1:B+R from L; freeze layers 1:B; initialize optimizer. 2: for each (xv, xt) do // Teacher: generate pseudo-labels from the full MLLM Auto-regressive response 3: 4: 5: Ypred Decode(L, xv, xt) xfull Concat(xt, Ypred) r, Kl v, Ql Hl Sof tmax (cid:16) eatures(L, xv, xfull , l) (cid:17) r(Kl v)/ Ql 6: 7: MRoI eanAcrossHeadsAndResp(A) 8: 9: 10: MRoI AssignLabels(M mask ( MRoI = 1) RoI RemoveSinkT okens(MRoI, Hl RoI, τf g, τbg) v, τnorm) Softmax over visual tokens Values in {1, 0, 1} // Student: forward through frozen 1:B and trainable B+1:B+R 11: 12: 13: 14: 15: Hv, HRoI eatures(LRPN, xv, xt, B+R1) QRoI LPq(N orm(HRoI)), Kv LPk(N orm(Hv)) ˆMRoI QRoIK loss BCEW ithLogits Update(optimizer, loss) (cid:16) ˆMRoI[mask], MRoI[mask] (cid:17) Logits for dense RoI map return with entries set to 0 where Hv,j2 > τnorm Helper routines 1: function REMOVESINKTOKENS(M, Hv, τnorm) 2: 3: function ASSIGNLABELS(M, τf g, τbg) 4: 5: 6: 7: 8: 9: amax maxj Mj Sf {j : Mj τf amax} Bf inEnclosingBox(Sf g) Sbg {j / Bf : Mj τbg amax} Build with Mj=1 for jSf g, Mj=0 for jSbg, and 1 otherwise return Table 7: Ablation on the region proposal layers. Setting OCRBench TextVQA POPE LLaVA-1.5-7B 31.4 46. 85.9 V* 50.3 B15R1 B15R2 B15R3 B15R4 35.84.4 35.94.5 37.05.6 37.15.7 54.78.6 57.811.7 58.712.6 58.412. 87.11.2 87.21.3 87.1 1.2 87.11.2 56.05.7 63.413.1 67.517.2 67.517.2 Ave. 53.4 58.45.0 61.17.7 62.69.2 62.59.1 performs forward pass on the original input to predict dense RoI map ˆMRoI. masked BCEWithLogits loss is then computed between the predicted map and the pseudo-label target, ensuring that ambiguous regions (where the mask is false) do not contribute to the gradient. Finally, the RPNs parameters are updated via backpropagation."
        },
        {
            "title": "B MORE ABLATIONS",
            "content": "On Region Proposal Layers. In Table 7, we present an ablation study on the depth of the trainable Region Proposal Network, varying the number of tunable layers (R). We observe that increasing the RPNs depth from one to three layers yields significant performance improvement (from 58.4% to 62.6% average score). Beyond this point, we found that the performance gains diminished, failing to justify the additional computational cost incurred during both training and inference. Therefore, 15 Preprint. Under review. Table 8: Ablation on the setting of Pre-smoothing and Post-smoothing. Setting OCRBench TextVQA POPE LLaVA-1.5-7B 31.4 46.1 85.9 V* 50.3 Ave. 53.4 Pre-smoothing Post-smoothing 34.83.4 37.05.6 58.48.3 58.712. 87.92.0 87.1 1.2 60.710.4 67.517.2 60.57.1 62.69.2 to achieve the best balance between model capacity and efficiency, we selected R=3 as the default setting for our framework. On Pseudo-label Smoothing. Our standard pipeline applies Gaussian filter to the RPNs final predicted map, step we term post-smoothing. We investigated an alternative approach, pre-smoothing, where the filter is instead applied directly to the pseudo-labels during their generation. As shown in Table 8, the post-smoothing approach consistently outperforms pre-smoothing across most benchmarks, achieving +2.1% higher average score. We hypothesize that this performance gap stems from distributional shift induced by pre-smoothing. Applying filter to the sparse pseudo-labels makes them denser, effectively enlarging the target foreground area. While this might appear beneficial, it creates significant discrepancy between the dense, artificially softened labels and the sharper, more sparse predictions natural to the self-initilized RPN. This distribution gap complicates the training process, particularly in our data-efficient setting. This difficulty is reflected in the final convergence loss, where the pre-smoothing strategy results in 2.5x higher value than postsmoothing (0.05 vs. 0.02). We conclude that forcing the network to learn these artificially blurred targets increases ambiguity, ultimately degrading the precision of the RoI predictions."
        },
        {
            "title": "C PROMPT USAGE",
            "content": "Below are the prompts used for pseudo-label generation with LLaVA-1.5 and DeepSeek-VL. Each template follows the respective models standard input format. LLaVA-1.5 <image> USER:{question} Answer the question using single word or phrase. ASSISTANT: DeepSeek-VL <image_placeholder> USER:{question} Answer the question using single word or phrase. Assistant:"
        },
        {
            "title": "D VISUALIZATION",
            "content": "To provide deeper, qualitative understanding of our models advantages, Figure 5 presents series of challenging visual question-answering examples. These samples, drawn from the V-Star, TextVQA, and DocVQA benchmarks, were specifically chosen because they require recognizing and reasoning about fine-grained details within cluttered scenes. The baseline model, which relies on processing the entire image at global level, often fails to perceive these critical details. In contrast, our SD-RPN first employs its region proposal mechanism to identify the most salient RoI for given questionvisualized by the green bounding boxes for subsequent fine-grained perception. This two-stage approach allows our model to accurately perform tasks like detailed text extraction, object attribute identification, and spatial reasoning, achieving correct answers where the baseline consistently fails. 16 Preprint. Under review. Figure 5: Qualitative comparison of our SD-RPN against the LLaVA-1.5-7B baseline on challenging samples from the V-Star, TextVQA, and DocVQA benchmarks. For each example, the top image displays the original input with the Region-of-Interest (RoI) predicted by our method, while the image below it visualizes the dense RoI map. These examples highlight SD-RPNs ability to precisely localize and analyze fine-grained visual information (e.g., text, small objects), leading to correct answers where the baseline, which processes the full image, consistently fails. 17 Preprint. Under review."
        },
        {
            "title": "E WHY PREDICTING ROI SCORES OUTPERFORMS USING RAW ATTENTION",
            "content": "Let denote token-level features available to the region-of-interest (RoI) predictor, {0, 1} is the latent foreground (FG) indicator, and [0, 1] is the models response-to-image attention used as noisy proxy for . Define the posterior η(x) := Pr(Y = 1 = x). Regression view (continuous RoI scoring). We train predictor by minimizing the population squared loss R(h) = E(cid:2)(h(X) A)2(cid:3). By pointwise conditional minimization, the optimal predictor is the conditional expectation h(x) = E[A = x]. (9) Under standard noise models, is an affine function of η(x): E[A = x] = (cid:26)(1 ρ1 ρ0) η(x) + ρ0, µ0 + (µ1 µ0) η(x), class-conditional noise (CCN), additive activation model. In both cases, E[A = x] is strictly increasing in η(x) when ρ0 + ρ1 < 1 or µ1 > µ0, so thresholding recovers the Bayes decision boundary up to constant shift. Noise reduction via conditional averaging. The raw attention signal can be decomposed into its signal component, defined as h(X) = E[A X], and its noise component, ϵ = E[A X]. By construction, the noise is zero-mean conditional on the features (E[ϵ X] = 0). We can measure the quality of any estimator by its Mean Squared Error (MSE) with respect to this true underlying signal, h(X). The MSE of the optimal predictor h(X) with respect to the true signal is, trivially, zero: E(cid:2)(h(X) h(X))2(cid:3) = 0. In contrast, the MSE of the raw attention signal with respect to the true signal is: E(cid:2)(A h(X))2(cid:3) = E(cid:2)(A E[A X])2(cid:3) = E(cid:2)E[(A E[A X])2 X](cid:3) = E[Var(A X)]. This term, the expected conditional variance, represents the irreducible error inherent in the attention signal. As long as the attention is not perfectly deterministic function of the features (i.e., Var(A X) > 0), we have: E(cid:2)(h(X) h(X))2(cid:3) < E(cid:2)(A h(X))2(cid:3). This inequality formally proves that the optimal predictor h(X) is strictly better, denoised estimate of the underlying signal than the raw attention A. Any learned predictor ˆh that successfully approximates will therefore also be more stable and accurate predictor. Classification view (binary RoI selection). For symmetric class-conditional noise (symmetric CCN) with flip rate ρ < 1 2 , Pr(A = 1 = x) = (1 2ρ) η(x) + ρ, (10) strictly increasing transform of η(x). Minimizing classification-calibrated surrogate on yields scorer whose optimal threshold (shifted by ρ) implements the clean Bayes rule. In contrast, selecting RoIs directly via suffers from the conditional variance Var(A X), leading to higher false positives/negatives, especially in low-margin regions. Implication for RoI prediction. lightweight RoI head trained to regress learns E[A X], which (i) is order-preserving with η(x) by the affine forms above, and (ii) enjoys reduced variance. Thresholding learned predictor ˆh(X) therefore yields more accurate and stable RoI proposals than thresholding raw attention A, matching our empirical gains. Our proposed Label Assignment strategy can be seen as further refinement of this principle, where we structure the learning problem as classification task on high-confidence tokens to make the training process even more robust to the noise in A. 18 Preprint. Under review."
        },
        {
            "title": "F THE USE OF LARGE LANGUAGE MODELS",
            "content": "Throughout the preparation of this manuscript, we utilized Large Language Model (LLM) as general-purpose writing assistant to enhance the quality and clarity of our text. The LLMs role was strictly limited to that of linguistic refinement tool. We used it to help polish our initial drafts by: Rephrasing sentences and paragraphs for improved readability, conciseness, and more formal academic tone. Correcting grammatical errors, spelling, and punctuation. Improving the logical flow and transitions between sentences. Assisting with the generation of LaTeX code for tables based on our provided experimental data. It is important to state that all core research ideas, the conceptualization of the proposed framework, the experimental design, the analysis of results, and the final scientific conclusions presented in this paper were conceived and articulated entirely by the human authors. The LLM did not contribute to the scientific ideation or the results of this work. The authors have reviewed, edited, and take full responsibility for all content in this manuscript."
        }
    ],
    "affiliations": [
        "City University of Hong Kong",
        "University of Sydney"
    ]
}