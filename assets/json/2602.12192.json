{
    "paper_title": "Query-focused and Memory-aware Reranker for Long Context Processing",
    "authors": [
        "Yuqing Li",
        "Jiangnan Li",
        "Mo Yu",
        "Guoxuan Ding",
        "Zheng Lin",
        "Weiping Wang",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 2 9 1 2 1 . 2 0 6 2 : r Query-focused and Memory-aware Reranker for Long Context Processing Yuqing Li1,2* Jiangnan Li3* Mo Yu3* Guoxuan Ding1,2 Zheng Lin1,2 Weiping Wang1 Jie Zhou3 1Institute of Information Engineering, Chinese Academy of Sciences 2School of Cyber Security, University of Chinese Academy of Sciences 3Pattern Recognition Center, WeChat AI, Tencent Inc liyuqing@iie.ac.cn {jiangnanli,moyumyu}@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passagequery relevance using the attention scores of selected heads. This approach provides listwise solution that leverages the holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-ofthe-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance1."
        },
        {
            "title": "Introduction",
            "content": "Embedding Models, especially those built on top of LLMs, achieved successes and enabled generators (RAG) and agents to work with long inputs or large input corpora efficiently (Zhang et al., 2025b; Zhao et al., 2025; Babakhin et al., 2025; Li et al., 2025a). However, embeddings also have limitations, as theoretically proved and empirically illustrated by (Weller et al., 2025). They reveal \"geometric bottleneck\" where fixed-dimensional *Equal contribution. Corresponding author. 1The models are available at https://huggingface.co/ MindscapeRAG/QRRanker 1 vectors fail to encode the combinatorial complexity of query-document interactions. Furthermore, the inductive bias of the similarity measure limits the applicable domains where other types of relationships are required to recall, e.g., causality, associations, and analogy. long line research applies an additional reranker module on the shortlist returned from embedding models to resolve this challenge. The rerankers use larger models, more powerful representations (like cross-attention). The fast development of LLMs boosts many LLM-based reranker releases to benefit from the reasoning capabilities of LLMs (Zhang et al., 2025b; Sun et al., 2025; Liu et al., 2025a; Pradeep et al., 2023b). These rerankers can adopt either pointwise or listwise formulations. Pointwise lost the global view of the shortlist, but can give scores. Listwise approaches, on the other hand, directly inherit the long-context reasoning and text generation ability of the backbone LLMs, which takes holistic view of the shortlist, but the next-token prediction limits the prediction of fine-grained scores, and the predicted float numbers cannot always accurately reflect the true confidence (Liu et al., 2025b; Lin et al., 2024). As result, they adopt Likert rating regime, asking the models to output five-point or ten-point scale score for each input document. Which limited the available training data. In this work, we propose an alternative solution built upon the existing analysis of retrieval heads in LLMs (Wu et al., 2024; Zhang et al., 2025a). These works identify two related types of heads: retrieval heads and Query-focused Retrieval (QR) heads. Both refer to attention heads whose attention patterns reflect retrieval behaviors. Specifically, when concatenating long contexts of relevant and distractor passages with the query, these heads are defined as those that put significant attention weights on the relevant passages, so as the ranks of attention weights correlate with the ranks of relevance. While existing works mainly focus on probing and understanding the functions of such heads, our work moves one step further by training LLMs to optimize the ranking accuracy of small set of retrieval heads. In this way, we achieve an LLMranker that is optimized to rank passages with attention weights. This resulted listwise solution, named QRRanker, can naturally work with continuous relevance scores without the limitation of Likertscale supervision, hence can be trained on arbitrary retrieval datasets. Our QRRanker enjoys several good properties in practice. First, the retrieval heads can be effectively trained even when the backbone has relatively small scale, e.g., 4B parameters. This allows the listwise approach to run with improved efficiency. Second, it is easy to enhance the input candidate passages with their global context with efficiency, by prepending the shared contextual information to the ground of candidates during training, which is essential for long narrative understanding. Finally, we observed that our QRRanker is quite robust to the selection of heads, and training with heads from layers in the middle would result in no performance drop. This allows us to take off the higher layers of the LLMs during training and inference, which can greatly reduce the latency of the model. Experiments on various domains, including Wikipedia QA tasks (Musique (Trivedi et al., 2022), HotpotQA (Yang et al., 2018)), long narrative QA tasks (NarrativeQA (Koˇcisk`y et al., 2018), DetectiveQA (Xu et al., 2025b)) and long-context dialogue (LoCoMo (Maharana et al., 2024)), demonstrate the advantage of our QRRanker. As versatile ranking framework, our approach not only outperforms the state-of-the-art general-purpose pointwise and listwise models like Qwen-Rerank and GroupRank, but also consistently improves over the domain-specific ranking approaches, such as HippoRAG-v2 (Gutierrez et al., 2025) for Wikipedia QA and list of recent memoryenhanced approaches (Li et al., 2025b; Rasmussen et al., 2025; Hu et al., 2026a) on LoCoMo."
        },
        {
            "title": "2 Related Work",
            "content": "Reranking Ranking techniques are acceptedly constructed based on two structures: Siamese network (Bi-encoder; Koch et al. 2015) and Crossencoder (Thakur et al., 2021). Embedding models (Zhang et al., 2025b) is the first one usually used to rank the whole corpus of documents with embeddings stored for reuse. However, they are limited by the geometric bottleneck, failing to encode more fine-grained interactions between query and document. The limitation can be alleviated by cross-encoders, which score every document by cross-attention with the query. The computing burden of dedicated re-encoding every pair of querydocument narrows the way for cross-encoders only reranking top-n documents ranked by bi-encoders, which produces refined sorting of top docs. Therefore, cross-encoders are called Rerankers. In the era of LLMs, Rerankers are also deeply explored using LLMs. They can be classified into two groups: Pointwise and Listwise. Pointwise describes the paradigm of pairwise scoring for documents, which is the major direction (Qin et al., 2024; Sun et al., 2023; Liu et al., 2025a; Zhuang et al., 2025) in practice, e.g., Qwen3 (Zhang et al., 2025b), Jina, mGTE (Zhang et al., 2024), BGE-m3 (Chen et al., 2024) rerankers. Pointwise models independently encode documents, failing to grasp global information. To this end, Listwise models fully utilize LLMs generating ability. They (Pradeep et al., 2023a,b) concatenate documents as list and generate the reranking result accordingly. To step further, tuned using RL, models can first think (Sun et al., 2023; Liu et al., 2025a; Qin et al., 2025; Ma et al., 2023; Sun et al., 2025) and then give the answer, achieving significant performance. However, Listwise models require training data to provide specific ranking of docs or even scores, leading to burdens of data collection and construction. Furthermore, LLMs generation is not stable (e.g., generating bad formats), especially when introducing the thinking process. As studied by Wu et al. (2024); Zhang et al. (2025a), LLMs inherently possess the ability of retrieval, and retrieval attention heads can be extracted to rank docs, achieving competitive performance. Nevertheless, these heads may change when moving to new tasks, requiring additional seed datasets to extract them. To this end, we propose to train the selected heads, which ensures better transferability. Memory Utilization Memory construction and utilization to alleviate problems of long-context processing become hot spot nowadays. For long story understanding, Li et al. (2025a) construct global memory to enhance retrieval and generation. For dialogue management, sophisticated graphs (Jiang et al., 2026; Xu et al., 2025a; Ras2 chunks: QRScoreh = 1 (cid:88) (cid:88) (cid:88) ciG wqQ wcci AQci [wq, wc], (1) where wc and wQ are tokens in gold chunk ci and respectively. The QR score measures the extent to which focuses on gold chunks. higher value indicates that the head has the potential to identify G. The QR score will be computed and averaged on the seed dataset for every head H, sort descendingly, and then pick up the top 16 heads as the QR heads (h HQR). We select QR heads for Qwen3-4B-Instruct-2507 (Yang et al., 2025) by 1000 random samples from NarrativeQA."
        },
        {
            "title": "QR heads compute the retrieval score for a chunk",
            "content": "hHQR ci in similar way like Eq. 1 by replacing (cid:80) cG with (cid:80) . Zhang et al. (2025a) further add score calibration to mitigate intrinsic biases in attention weights, which encodes null query =N/A (cid:80) wqN AN ci. Notably, and subtracts its with our QR training, calibration becomes optional. 1 N"
        },
        {
            "title": "4 Method",
            "content": "Our QRRanker is the Listwise method that reranks all the documents in single inference pass, following the so-called \"prompt-decoders\" (Pradeep et al., 2023a). Notably, QRRanker does not involve any generation processes, but only prefills the prompt with the question and documents, and obtains the attention scores, which is more timeand resource-friendly. Though the original QR retriever (Zhang et al., 2025a) with group of precomputed QR heads can transfer to new tasks, the performance may not be that stable, as QR heads may be changed on new tasks. To this end, we propose dedicated training pipeline for QRRanker. We first construct listwise training instances and then optimize the precomputed QR heads with contrastive ranking objective."
        },
        {
            "title": "4.1.1 Listwise Training Instances",
            "content": "We build unified training set by combining MuSiQue (Trivedi et al., 2022) and NarrativeQA (Koˇcisk`y et al., 2018). We first determine evidence chunks for each question. For MuSiQue, we directly use the official supporting facts in the original annotations as evidence. For NarrativeQA, since gold chunks are not provided, we follow Li et al. (2025a) to construct silver evidence chunks. 3 Figure 1: The retrieval score and QR score are computed based on the attention score of (QR) attention head. In this figure, Doc2 is the gold document (chunk). mussen et al., 2025; Hu et al., 2026b,a), trees (Li et al., 2026), and systems (Chhikara et al., 2025; Li et al., 2025b; Nan et al., 2025; Tao et al., 2026; Zou et al., 2026) of events, personas, and chunks are designed to accurately extract related dialogue history for further use. However, better and powerful search for history, with simple memory construction, can beat complicated memory management, and we will show our solution to reach this goal."
        },
        {
            "title": "3 Preliminaries: QR-head",
            "content": "In this section, we first introduce the definition of the Query-Focused Retrieval heads (QR-head). As introduced by Wu et al. (2024) and Zhang et al. (2025a), among all heads in multi-head selfattention modules, some play crucial roles as retrievers. These heads pay more attention to the parts containing information to answer the question of the context when encoding the question. Zhang et al. (2025a) name them as QR-heads and identify them by QR score. Formally, for question Q, its corresponding context is split into chunk list [c0, c1, ..., cn], where = [cg0, ..., cgm] are gold chunks to answer the question. The attention score of an attention head between and every chunk ci when encoding the prompt with and is denoted as AQci RQci. heads QR score is comh puted by summing up the attention scores of gold Figure 2: The structure of QRRanker is illustrated in the middle, where the highlighted heads are QR heads for document scoring. As QRRanker can be aware of memory enhancement to capture more contextual information, we can construct memories for narratives and dialogues, which is shown on the left. The right part demonstrates the rank-rerank pipeline of qa for narratives/wiki/dialogues, which involves no sophisticated design. After establishing the evidence, we retrieve top50 candidate set for each question using Qwen3Embedding-8B and form listwise instance by labeling retrieved candidates that match the preconstructed evidence as positive, while treating the remaining retrieved candidates as negatives. Optionally, we construct summary prefix by mapping the retrieved chunks to their corresponding summaries, and prepend these summaries before the chunk list, i.e., = [M ; C]. Alg. 1 summarizes this construction on NarrativeQA; MuSiQue follows the same procedure except that relevant evidence directly comes from their official supporting facts. We describe how the summaries are constructed in the next subsection."
        },
        {
            "title": "4.1.2 Summary Construction\nTo provide high-level semantic guidance and sup-\nport long-context narrative understanding, we con-\nstruct summaries as auxiliary memory context.\nWhen used, summaries are prepended as a global\nprefix to the retrieved chunk list, so the model\ncan leverage both coarse-grained context and fine-\ngrained evidence. We explore two complementary\nstrategies for constructing summaries.",
            "content": "Block-based Summary. For long narrative books, we construct block-level summaries that respect the sequential nature of storytelling. Specifically, we split each book into blocks (20 consecutive chunks per block) and generate one summary per block. (see Appendix A.1) Event-centric Summary. For dialogue-based data, we extract structured events from conversations and form an event-centric summary. Each 4 event is represented by short description and is linked to its source utterances, enabling traceability to the original dialogue. (see Appendix A.2)."
        },
        {
            "title": "4.2 QR Training",
            "content": "Obtaining QR heads precomputed by the QR score mentioned in Sec. 3, our training scheme focuses on training these heads. For question and the top 50 candidate documents = [c1, ..., c50] ranked by retriever (e.g., embedding models like Qwen3-Embedding), where gold (positive) documents are = [cg0, .., cgm], the prompt input to QRRanker is constructed by concatenating and in order with some instructions: = Inst(C, Q), where the instruction template is provided in Appendix A.3. The prompt is fed into the model, and in every attention head, the attention score is computed as APP . We locate the position of and ci and take out the query-focused part AQci . The retrieval score of the passage ci computed by the QR head HQR is: sh ci = 1 (cid:88) (cid:88) ici jQ AQci [i, j], (2) hHQR where the score computing is illustrated in Fig. 1. Then, the final retrieval score is obtained by summing up all scores provided by QR heads: sci = (cid:80) ci can also be computed by aggregating the maximum attention item, like used in approaches like ColBERT (Khattab and Zaharia, 2020), which achieves similar performance, so we do not discuss it here. ci. Additionally, sh sh 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: Algorithm 1 Construct listwise training instances on NarrativeQA with optional summary prefix Require: NarrativeQA training split D; retriever R; top-K (K=50); memory flag UseMem; summary map Ensure: Training set 1: 2: for all question in do 3: SILVEREVIDENCE(Q) constructed following (Li et al., 2025a) R(Q, K) retrieve top-K candidate chunks for all ci do yi I[ci G] end for if UseMem then LOOKUPSUMMARIES(C, M) map chunks in to summaries MERGEDEDUP(S) merge & loss to simultaneously optimize them: τ (scp) 1 (cid:88) log , Lsample = τ (scp) + (cid:80) cpG cnCG τ (scn) (4) where τ denotes the exponential function. The objective above treats every positive document as an independent sub-sample and averages the loss inside the sample. For the dataset, the objective aligns with conventional contrastive loss. As our QRRanker can be made memory-aware to incorporate broader contextual information, during QR training, we optionally prepend memory prefix (e.g., summaries mapped from the retrieved chunks) before the candidate list C. The resulting prompt to QRRanker is constructed as = Inst(M, C, Q)."
        },
        {
            "title": "5 Experimental Setup",
            "content": "de-duplicate summaries"
        },
        {
            "title": "5.1 Datasets",
            "content": "else end if {(Q, M, C, {yi}K i=1)} 14: 15: end for 16: return We then optimize the document scores = [sc1, ..., sc50] utilizing the sample-level contrastive loss. In conventional contrastive scene, the score sci is stably ranged in [0, 1], while, in our case, sci can be affected by tokens in the instruction (e.g., the heads sensitivity to attention sink), which may lead to an unstable range for samples. Therefore, the temperature may not be suitable for scaling the score. To this end, we normalize the score with the max-min norm, which can be formed as: = scale (S min(S)) max(S) min(S) , (3) where scale is factor to scale the range to [0, scale] for stability. The original contrastive loss samples one positive document at time; however, the top 50 documents may contain more than one positive document. It can be suboptimal if we follow the original setting, as unselected positive documents are ignored. We propose group version of contrastive 5 To evaluate QRRanker across diverse retrieval settings, we conduct experiments on benchmarks spanning Wikipedia multi-hop QA, long-context story QA, and dialogue memory. Wikipedia Multi-hop QA For fact-based multihop retrieval, we evaluate on HotpotQA (Yang et al., 2018) and MuSiQue (Trivedi et al., 2022). To ensure fair comparison, we adopt the corpus and test splits provided by HippoRAG (Gutierrez et al., 2025), maintaining consistency in the candidate passage pool. Long-context Story QA We utilize datasets that demand complex reasoning over extended contexts, specifically: (1) NarrativeQA from the HELMET benchmark (Yen et al., 2024), which consists of 1,272 questions with the longest document reaching 518k tokens. (2) DetectiveQA (Xu et al., 2025b) is bilingual detective story dataset with an average length exceeding 100k tokens, requiring precise evidence localization across scattered plot points. Long-context dialogue memory We evaluate our model on LoCoMo (Maharana et al., 2024), large-scale benchmark designed for long-context dialogue memory. The dataset comprises 50 multi-session dialogues across 10 distinct user groups, with each dialogue averaging approximately 9,000 tokens. Following prior work, we report performance across four fine-grained categories: single-hop, multi-hop, temporal reasoning, and open-domain. Methods Wikipedia QA Story QA Musique HotpotQA NarrativeQA DetectiveQA Overall Avg@k R@3 R@5 R@10 R@3 R@5 R@10 R@3 R@5 R@10 R@3 R@5 R@10 avg@3 avg@5 avg@10 Qwen3-Embedding-4B Qwen3-Embedding-8B SFT-Embedding-8B 51.56 59.83 69.88 54.35 62.55 72.47 45.11 52.93 62.03 78.84 86.16 92.33 82.85 89.05 95.15 82.36 88.63 94.19 12.57 18.33 28.08 14.98 20.92 32.39 21.31 29.77 44. 19.25 26.17 37.04 12.84 20.00 31.17 19.84 27.59 39.00 40.56 41.25 42.16 Embedding Methods Reranking Methods HippoRAG-v1 HippoRAG-v2 Qwen-Reranker-4B (out-of-box) 57.60 66.37 74.26 61.60 69.71 77.49 Qwen-Reranker-4B (trained) GroupRank-32B 55.49 65.08 73.07 63.12 71.22 78.99 QRHeads-4B (out-of-box) 53.20 74.70 90.40 96.30 89.80 94.15 96.75 89.35 93.95 96.90 82.45 90.60 94.50 90.20 94.80 96.90 20.83 28.25 41.98 25.84 35.05 49.62 23.98 33.76 48.83 24.28 33.44 48.89 23.42 30.50 42.09 29.67 38.92 51.25 29.34 39.21 51.38 23.71 32.89 45.58 47.91 51.61 47.82 50. 47.62 48.13 49.73 54.82 59.41 57.16 58.09 56.83 57.80 59.85 63.77 68.82 66.95 67.59 Our QRRanker-4B 70.19 77.37 82. 95.05 96.90 97.70 29.11 38.89 54.93 32.22 41.32 53.76 56.64 63.62 72. Table 1: Retrieval and Rerank performance measured by Recall@{k}. indicates the metric is not reported in the corresponding paper. For Wikipedia QA, we rerank the top-50 candidates retrieved by Qwen3-Embedding-8B; for Story QA, we rerank the top-50 candidates retrieved by SFT-Embedding-8B. DetectiveQA scores are averaged over English and Chinese sets. Overall columns report avg@3/avg@5/avg@10 averaged over the four datasets. Bold numbers indicate the best result in each column. For fairness, all rerankers are evaluated with single run. Methods R@3 R@5 R@10 Qwen3-Emb-8b SFT-Emb-8b GroupRank-32B QRHeads (out-of-box) QRRanker (ours) Improvement vs. SFT-Emb 58.61 76.01 77.99 85.93 87.34 +11.33 67.67 83.10 82.94 90.35 91.32 +8.22 79.15 90.15 88.14 94.86 95.01 +4. Table 2: Retrieval and Rerank performance on LoCoMo."
        },
        {
            "title": "5.2 Baselines",
            "content": "We evaluate QRRanker against broad spectrum of retrieval and memory frameworks. For general-purpose reranking on Wikipedia QA and Long-context story tasks, we compare QRRanker against two categories of models: (1) Embedding Models: Qwen3-Embedding (4B/8B) (Zhang et al., 2025b) and SFT-Embedding8B, which is fine-tuned from Qwen3-Embedding8B on our constructed data. (2) Reranking Methods: HippoRAG (Jimenez Gutierrez et al., 2024; Gutierrez et al., 2025), GroupRank-32B (Sun et al., 2025), Qwen3-Reranker-4B (out-of-box) (Zhang et al., 2025b), and Qwen3-Reranker-4B variant trained on the same data as our QRRanker. We also include the QRHead without training as baseline. For the long-context dialogue task on LoCoMo, we compare QRRanker with range of strong baselines, including: A-Mem (Xu et al., 2025a), MemoryOS (Li et al., 2025b), Zep (Rasmussen et al., 2025), Mem0 (Chhikara et al., 2025), Nemori (Nan et al., 2025), and LightMem (Fang et al., 2025); TiMem (Li et al., 2026), Synapse (Jiang et al., 2026), Membox (Tao et al., 2026), CompassMem (Hu et al., 2026b), and ES-Mem (Zou et al., 2026); SimpleMem (Liu et al., 2026). Detailed baseline descriptions are provided in Appendix B. 5."
        },
        {
            "title": "Implementation Details",
            "content": "Our QRRanker is trained on Qwen3-4B-Instruct2507, with QR heads selected as described in Appendix C. In the training process, the scale factor in the max-min norm is set to 8; the batch size is set to 1; the gradient accumulating step is set to 4; the learning rate is set to 1e-5. We utilize the DeepSpeed ZERO2 strategy and train QRRanker using 8 H20 GPUs. For downstream QA evaluation, we use taskspecific prompting for generation; the full prompt templates for NarrativeQA, DetectiveQA, and LoCoMo are provided in Appendix A. We employ Qwen3-8B as the generator for NarrativeQA and DetectiveQA, where books are chunked into nonoverlapping passages of 200 tokens. For the LoCoMo benchmark, we utilize GPT-4o-mini and GPT-5-mini as the generators. We segment the dialogue history into small chunks, ensuring that utterance continuity is preserved, with an average chunk size of 258 tokens. When enabling the memoryaware setting, we prepend summary prefix before the ranked chunk list. We cap the summary prefix at 512 tokens and select summaries based on their coverage of the retrieved/reranked chunks. 6 LLM Method Tokens Singlehop Multihop Temporal Opendomain Overall F1 GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4.1-mini GPT-4o-mini GPT-5-mini Qwen3-Emb-8B (out-of-box) SFT-Emb-8B A-Mem (Xu et al., 2025a) MemoryOS (Li et al., 2025b) Zep (Rasmussen et al., 2025) Mem0 (Chhikara et al., 2025) Nemori (Nan et al., 2025) LightMem (Fang et al., 2025) TiMem (Li et al., 2026) Synapse (Jiang et al., 2026) Membox (Tao et al., 2026) CompassMem (Hu et al., 2026b) ES-Mem (Zou et al., 2026) SimpleMem (Liu et al., 2026) QRRanker (Ours) QRRanker (Ours) 846 841 2,712 3,874 3,911 1,764 4,767 815 511 814 2,166 20,000 2,925 531 854 854 47.95 57.22 44.65 48.62 49.56 47.65 46.33 47.64 48.90 60.09 57.36 50.07 51. 62.95 61.78 35.24 37.06 27.02 35.27 35.74 38.72 32.36 32.11 35.70 39.88 38.84 36.52 43.46 43.06 44.73 41.36 56.27 45.85 41.15 42.00 48.93 55.99 53.79 50.10 58.03 57.96 47.90 58.62 61.90 64.53 24.79 29.11 12.14 20.02 19.37 28.64 29.19 26.14 25.90 27.96 26.61 24.77 19. 29.79 31.04 42.81 51.58 39.65 42.84 43.56 45.09 44.72 44.73 54.40 40.50 53.10 52.18 45.56 43.24 57.03 57.32 Table 3: Comparison with SOTA Memory and Agent frameworks on the LoCoMo. Results marked with are derived from ES-Mem (Zou et al., 2026). For QRRanker, we rerank the top-50 chunks retrieved by SFT-Emb-8B and utilize only the top-3 chunks as context for generation, without additional memory mechanisms. indicates the metric is not reported in the corresponding paper."
        },
        {
            "title": "6.1 Main Results",
            "content": "We conduct extensive experiments spanning three distinct domains: Wikipedia multi-hop QA, longcontext story QA, and dialogue memory. These experiments cover five datasets in both English and Chinese. Tables 1 and 2 present the overall reranking performance measured by Recall@k. The results demonstrate that our proposed QRRanker consistently achieves the best performance across all datasets. It substantially outperforms embeddingonly retrieval, strong reranking baselines such as Qwen-Reranker, and the vanilla out-of-box QRHeads variant. Furthermore, we evaluate downstream generation on narrative QA and dialogue memory as reported in Tables 3 and 4. In these tasks, our QRRanker yields consistent gains and demonstrates strong generalization from retrieval reranking to end tasks Rerank Performance. We first analyze the retrieval effectiveness of QRRanker when applied to rerank the top-50 candidates retrieved by embeddings. As shown in Table 1, QRRanker establishes new state-of-the-art benchmark. It surpasses the strong baseline Qwen-Reranker-4B by substantial margin and improves the average recall significantly. On Wikipedia datasets such as Musique and HotpotQA, QRRanker outperforms complex graph-based methods like HippoRAG (Gutierrez et al., 2025). Remarkably, it also exceeds the performance of GroupRank-32B despite being significantly more lightweight. This indicates that our method captures inter-passage dependencies more effectively than simple groupwise scoring or graph traversal. The performance gap is particularly evident in the Story domain, where context tracking is critical. For instance, QRRanker achieves Recall@10 of 54.93 on NarrativeQA compared to 48.83 for GroupRank and 48.89 for the vanilla QRHeads. Finally, on LoCoMo  (Table 2)  , QRRanker maintains the same advantage, indicating its effectiveness in retrieving relevant context from long conversational histories. Long-context Story QA Performance. Highquality retrieval should translate to improved generation accuracy. We evaluate this on narrative understanding datasets. As shown in Table 4, QRRanker significantly improves downstream QA performance. On NarrativeQA, it achieves 33.61 F1, outperforming the trained Qwen3-Reranker4B (30.51). On DetectiveQA, accuracy increases from 62.85 (SFT-Embedding-8B) to 67.25 with QRRanker. These results suggest that QRRanker selects evidence that is not only semantically relevant, but also better aligned with the reasoning needed for answer generation. 7 Methods NarrativeQA DetectiveQA F1 EM ACC Embedding Methods Qwen3-Embedding-8B SFT-Embedding-8B 26.30 11.01 28.48 12. Reranking Methods Qwen3-Reranker-4B (vanilla) 29.10 12.58 Qwen3-Reranker-4B (trained) 30.51 13.52 QRRanker Series QRHeads-4B QRRanker 31.40 14.70 33.61 16.04 57.35 62.85 60.93 64.52 64.75 67. Table 4: QA performance on NarrativeQA and DetectiveQA. All methods utilize R@3 retrieved chunks as the context for generation (Qwen3-8B as Generator). Dataset QRRanker Chunk +Sum LoCoMo NarrativeQA DetectiveQA HotpotQA Musique 86.64 28.09 29.55 95.05 70.19 87.34 29.11 32.22 94.75 70.16 +0.70 +1.02 +2.67 -0.30 -0.03 Table 5: Recall@3 comparison of QRRanker with chunk-only inputs versus summary prefix (+Sum) as contextual memory. indicates the absolute change after adding the summary prefix. Dialogue Memory Performance As summarized in Table 3, QRRanker demonstrates superior efficiency on LoCoMo, achieving the best Overall F1 with highly compact input budget. By using only 854 tokens on average (top-3 chunks) directly from the raw dialogue history, our approach achieves an Overall F1 of 57.03 with GPT-4o-mini and 57.32 with GPT-5-mini. In contrast, many memory-augmented frameworks require substantially larger budgets to maintain explicit memory stores or graphs. Our approach instead reranks the top-50 chunks retrieved by the embedding retriever and feeds only small set of top-ranked raw dialogue chunks to the generator. This lightweight design preserves high inference efficiency and low system complexity while still capturing longrange dependencies, yielding the highest Overall F1 among prior reported results on LoCoMo in our comparison."
        },
        {
            "title": "6.2 Results with Contextual Information",
            "content": "As shown in Table 5, equipping QRRanker with summary prefix consistently improves ranking performance across long-dialogue and long-context story benchmarks. This suggests that the summary provides global contextual guidance, complementing the fine-grained evidence from retrieved chunks. Moreover, we test summary-based memory on Wikipedia-based multi-hop QA. We build hierarchical clustering tree over retrieved passages and use parent summaries as the prefix. However, this strategy brings no gains and can even degrade performance, suggesting that abstracted global summaries are less helpful when evidence is highly localized in Wikipedia passages."
        },
        {
            "title": "Levels",
            "content": "QRRanker uses static preset heads, which invokes our curiosity about the heads from which level of layers are suitable as starters for QR training. We propose variant that dynamically selects heads from range of continuous layers for every sample. The variant totally picks up 16 heads from layer ls to le with 16/(le ls) heads per layer, where ls-le determines the level of layers (i.e., low, middle, high). Details of the variant are elaborated in Appendix D. We train and evaluate both QRRanker and its variants on the NarrativeQA dataset. Methods R@3 R@5 R@10 QRRanker 28.87 39. 54.44 10-17 17-24 28-35 24.51 28.15 28.48 34.52 39.07 38.88 49.91 54.28 54.65 Table 6: Retrieval performance on NarrativeQA of QRRanker and its variants adapted on different levels of layers. ls le denotes the layers with head selection. As shown in Tab. 6, training models with lower layers 10-17 shows significant performance drop, while middle layers 17-24 and top layer 28-35 almost keep the same performance as QRRanker. Intuitively, lower layers truncate too much knowledge from higher layers, and heads in the middle-to-top layers are more likely to be retrievers. The outcome aligns with the phenomenon that QR heads in QRRanker are all positioned in the middle layers (17-24). Interestingly, we compare QR heads in QRRanker with those selected by the variant (17-24), and the degree of overlap is pretty low. It indicates that, with QR training, such potential is activated, which shows that our method can utilize the robustness of heads, even not QR heads, from the middle to the top. This provides way 8 Method P50 (ms) P95 (ms) TFLOPs Peak Mem (/query) (GB) Qwen3-Reranker (batch=50) 1221.59 1256.29 1895.26 1929.09 Qwen3-Reranker (batch=1) QRRanker 1095.42 1133.38 928.1 910.42 QRRanker (middle) 115.69 113.65 82.74 69.83 13.88 7.78 11.18 8.71 Table 7: Inference efficiency comparison in latency (P50/P95), compute (TFLOPs per query), and peak GPU memory. All models are evaluated under the same hardware and inference settings over 20 queries. For Qwen3-Reranker-4B, batch=50 processes 50 chunk query pairs in single forward pass, whereas batch=1 processes the 50 pairs with 50 separate forward passes. QRRanker(middle) truncates the model after layer 24. to only focus on heads in the middle and truncate the higher layers for smaller and faster ranker. We quantify the inference efficiency benefits of this middle-layer truncation in Section 6.4. 6."
        },
        {
            "title": "Inference Efficiency",
            "content": "We further investigate the computational efficiency of our approach compared to baselines on set of 20 queries. As shown in Table 7, QRRanker achieves lower P50/P95 latency than Qwen3-Reranker-4B, while also reducing compute (TFLOPs) and peak memory. Moreover, QRRanker(middle) further improves efficiency by truncating the model after layer 24, discarding higher layers. It achieves the best P50/P95 latency with additional reductions in compute and memory. For Qwen3-Reranker-4B, we report two inference settings. With batch=50, all 50 chunk query pairs are processed in single forward pass. With batch=1, the 50 pairs are processed with 50 separate forward passes, which substantially increases latency. Overall, QRRanker provides better performance and cost trade-off, and the truncated middle-layer variant offers an especially lightweight and fast option."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we present QRRanker, lightweight and efficient listwise reranking framework built on Query-focused Retrieval (QR) heads in LLMs. By explicitly training selected QR heads for ranking, QRRanker produces real-valued relevance scores and performs reranking without generation at inference time. Across five datasets spanning Wikipedia multi-hop QA, long-context story QA, and dialogue memory, QRRanker consistently improves reranking quality and downstream QA performance. QRRanker remains practical with small backbone (e.g., 4B) and offers clear inference efficiency benefits. Moreover, it supports simple extensions such as an optional summary prefix for global context and mid-layer head selection for further efficiency."
        },
        {
            "title": "References",
            "content": "Yauhen Babakhin, Radek Osmulski, Ronay Ak, Gabriel Moreira, Mengyao Xu, Benedikt Schifferer, Bo Liu, and Even Oldridge. 2025. Llama-embednemotron-8b: universal text embedding model for multilingual and cross-lingual tasks. Preprint, arXiv:2511.07025. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. M3embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfIn Findings of the Assoknowledge distillation. ciation for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 1116, 2024, volume ACL 2024 of Findings of ACL, pages 23182335. Association for Computational Linguistics. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413. Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, and 1 others. 2025. Lightmem: Lightweight and efficient arXiv preprint memory-augmented generation. arXiv:2510.18866. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1120:39. Bernal Jimenez Gutierrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From rag to memory: Non-parametric continual learning for large language models. In arXiv.org. Chuanrui Hu, Xingze Gao, Zuyi Zhou, Dannong Xu, Yi Bai, Xintong Li, Hui Zhang, Tong Li, Chong Zhang, Lidong Bing, and 1 others. 2026a. Evermemos: self-organizing memory operating system for structured long-horizon reasoning. arXiv preprint arXiv:2601.02163. Yuyang Hu, Jiongnan Liu, Jiejun Tan, Yutao Zhu, and Zhicheng Dou. 2026b. Memory matters more: Eventcentric memory as logic map for agent searching and reasoning. arXiv preprint arXiv:2601.04726. Hanqi Jiang, Junhao Chen, Yi Pan, Ling Chen, Weihang You, Yifan Zhou, Ruidong Zhang, Yohannes Abate, and Tianming Liu. 2026. Synapse: Empowering llm 9 agents with episodic-semantic memory via spreading activation. arXiv preprint arXiv:2601.02744. Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large language models. Advances in Neural Information Processing Systems, 37:5953259569. Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 39 48. Gregory Koch, Richard Zemel, Ruslan Salakhutdinov, and 1 others. 2015. Siamese neural networks for one-shot image recognition. In ICML deep learning workshop, volume 2, pages 130. Lille. Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328. Kai Li, Xuanqing Yu, Ziyi Ni, Yi Zeng, Yao Xu, Zheqing Zhang, Xin Li, Jitao Sang, Xiaogang Duan, Xuelei Wang, and 1 others. 2026. Timem: Temporal-hierarchical memory consolidation for long-horizon conversational agents. arXiv preprint arXiv:2601.02845. Yuqing Li, Jiangnan Li, Zheng Lin, Ziyan Zhou, Junjie Wu, Weiping Wang, Jie Zhou, and Mo Yu. 2025a. Mindscape-aware retrieval augmented generation for improved long context understanding. arXiv preprint arXiv:2512.17220. Zhiyu Li, Chenyang Xi, Chunyu Li, Ding Chen, Boyu Chen, Shichao Song, Simin Niu, Hanyu Wang, Jiawei Yang, Chen Tang, and 1 others. 2025b. Memos: memory os for ai system. arXiv preprint arXiv:2507.03724. Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, and Kun Gai. 2024. Just ask one more time! self-agreement improves reasoning of language In Findings of models in (almost) all scenarios. the Association for Computational Linguistics: ACL 2024, pages 38293852. Jiaqi Liu, Yaofeng Su, Peng Xia, Siwei Han, Zeyu Zheng, Cihang Xie, Mingyu Ding, and Huaxiu Yao. 2026. Simplemem: Efficient lifelong memory for llm agents. arXiv preprint arXiv:2601.02553. Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, and Hua Wei. 2025b. Uncertainty quantification and confidence calibration in large language models: survey. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pages 61076117. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023. Zero-shot listwise document reranking with large language model. CoRR, abs/2305.02156. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of llm agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851 13870. Jiayan Nan, Wenquan Ma, Wenlong Wu, and Yize Chen. 2025. Nemori: Self-organizing agent memory inspired by cognitive science. arXiv preprint arXiv:2508.03341. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023a. Rankvicuna: Zero-shot listwise document reranking with open-source large language models. arXiv preprint arXiv:2309.15088. Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. 2023b. Rankzephyr: Effective and robust zeroshot listwise reranking is breeze! arXiv preprint arXiv:2312.02724. Xubo Qin, Jun Bai, Jiaqi Li, Zixia Jia, and Zilong Zheng. 2025. Tongsearch-qr: Reinforced query reasoning for retrieval. CoRR, abs/2506.11603. Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, and Michael Bendersky. 2024. Large language models are effective text rankers with pairwise ranking prompting. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, volume NAACL 2024 of Findings of ACL, pages 15041518. Association for Computational Linguistics. Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. 2025. Zep: temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956. Duolin Sun, Meixiu Long, Dan Yang, Yihan Jiao, Zhehao Tan, Jie Feng, Junjie Wang, Yue Shen, Peng Wei, Jian Wang, and 1 others. 2025. Grouprank: groupwise reranking paradigm driven by reinforcement learning. arXiv preprint arXiv:2511.11653. Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, and Zhicheng Dou. 2025a. Reasonrank: Empowering passage ranking with strong reasoning ability. arXiv preprint arXiv:2508.07050. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language models as re-ranking agents. In Proceedings of the 2023 Conference on 10 Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1491814937. Association for Computational Linguistics. Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen, and Xi Ye. 2025a. Query-focused retrieval heads improve long-context reasoning and re-ranking. arXiv preprint arXiv:2506.09944. Dehao Tao, Guoliang Ma, Yongfeng Huang, and Minghu Jiang. 2026. Membox: Weaving topic continuity into long-range memory for llm agents. arXiv preprint arXiv:2601.03785. Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. 2021. Augmented SBERT: data augmentation method for improving bi-encoders for pairwise sentence scoring tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 296310. Association for Computational Linguistics. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Orion Weller, Michael Boratko, Iftekhar Naim, and Jinhyuk Lee. 2025. On the theoretical limitations of embedding-based retrieval. arXiv preprint arXiv:2508.21038. Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. 2024. Retrieval head mechanistically explains long-context factuality. ArXiv, abs/2404.15574. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025a. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110. Zhe Xu, Jiasheng Ye, Xiaoran Liu, Xiangyang Liu, Tianxiang Sun, Zhigeng Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu. 2025b. DetectiveQA: Evaluating long-context reasoning on In Workshop on Reasoning and detective novels. Planning for Large Language Models. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pages 23692380. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. 2024. Helmet: How to evaluate longcontext language models effectively and thoroughly. arXiv preprint arXiv:2410.02694. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. 2024. mgte: Generalized longcontext text representation and reranking models for multilingual text retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: EMNLP 2024 - Industry Track, Miami, Florida, USA, November 12-16, 2024, pages 13931412. Association for Computational Linguistics. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, and 1 others. 2025b. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176. Xinping Zhao, Xinshuo Hu, Zifei Shan, Shouzheng Huang, Yao Zhou, Xin Zhang, Zetian Sun, Zhenyu Liu, Dongfang Li, Xinyuan Wei, and 1 others. 2025. Kalm-embedding-v2: Superior training techniques and data inspire versatile embedding model. arXiv preprint arXiv:2506.20923. Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. 2025. Rank-r1: Enhancing reasoning in llm-based document rerankers via reinforcement learning. CoRR, abs/2503.06034. Huhai Zou, Tianhao Sun, Chuanjiang He, Yu Tian, Zhenyang Li, Li Jin, Nayu Liu, Jiang Zhong, and Kaiwen Wei. 2026. Es-mem: Event segmentationbased memory for long-term dialogue agents. arXiv preprint arXiv:2601.07582."
        },
        {
            "title": "A Prompt Templates",
            "content": "A.1 Block-based Summary Generation"
        },
        {
            "title": "Prompt",
            "content": "You are an expert fiction editor and continuity supervisor. You are provided with raw text segment from book (Part {sub_idx} / {total_subs}). This segment consists of approximately 20 consecutive chunks combined. <Raw_Text> {raw_text} </Raw_Text> Please generate Detailed Narrative Summary following these strict guidelines: 1. Narrative Reconstruction: Do not list events. Rewrite the content as coherent story in the third person, past tense. It should read like condensed version of the original text. 2. Detail Preservation: Preserve specific Character Names and their 11 relationships. Keep key Dialogues that drive the plot. Note specific Locations or setting changes. 3. Noise Filtering: IGNORE any copyright notices, project gutenberg headers, page numbers, or table of contents. If the text starts or ends in the middle of sentence, ignore the broken fragments and focus on the complete thoughts. 4. Style: NO meta-commentary (e.g., do NOT say The text describes..., In this chunk...). Directly tell the story. 5. Length: 50-100 words. Output the summary directly. A.2 Event-centric Summary Generation INSTRUCT: You are specialized system for extracting structured event representations from conversational data. 1. EVENT CLASSIFICATION ANCHOR Events. Anchor events are MAJOR LIFE MILESTONES that will be remembered for years. Only classify as an anchor if the event meets ALL of these criteria: Represents first-time or rare life occurrence Has lasting impact on the persons identity, relationships, or life trajectory Would be mentioned when telling someone about important moments in my life Examples of TRUE ANCHOR events: First time attending LGBTQ support group, Starting adoption process, Career change, Moving to new country, etc. EPHEMERAL Events. Most events are ephemeral. These include: Plans and intentions (I plan to..., want to...) Routine activities (exercise, hobbies, daily tasks) Casual conversations and updates Past events being recalled (unless first mention of major milestone) 4. RAW DIALOGUE REFERENCE related_line_indices: list the 2-4 most relevant line numbers (1-indexed from the dialogue) These lines will be saved as the events source evidence INPUT DIALOG: {dialog} OUTPUT EVENTS): (JSON FORMAT, { \"events\": [ EXTRACT 1-3 \"summary\": \"concise description\", \"related_line_indices\": [1, 2, 3] { } ] } A.3 QRRanker Instruction Template INSTRUCT: [Optional Memory Prefix ] Here are some session summaries that may help answer the query: {mapped summaries from top-50 chunks} [Candidate Chunks C] Here are some retrieved chunks: [1] {chunk c1} [2] {chunk c2} [3] {chunk c3} [4] . . . [5] {chunk c50} QUERY Q: {question } A.4 LoCoMo QA Prompt You are an intelligent memory assistant tasked with retrieving accurate information from conversation memories. CONTEXT: You have access to memories from two speakers in conversation. These memories contain timestamped information that may be relevant to answering the question. INSTRUCTIONS: 1. Carefully analyze all provided memories from both speakers. 2. Pay special attention to the timestamps to determine the answer. 3. If the question asks about specific event or fact, look for direct evidence in the memories. 4. If the memories contain contradictory information, prioritize the most recent memory. 5. If there is question about time references (like last year, two months ago, etc.), calculate the actual date based on the memory timestamp. For example, if memory from 4 May 2022 mentions went to India last year, then the trip occurred in 2021. 6. Always convert relative time references to specific dates, months, or years. For example, convert last year to 2022 or two months ago to March 2023 based on the memory timestamp. Ignore the reference while answering the question. 7. Focus only on the content of the memories from both speakers. Do not confuse character names mentioned in memories with the actual users who created those memories. 8. The answer should be less than 5-6 words. APPROACH (THINK STEP BY STEP): 1. First, examine all memories that contain information related to the question. 2. Examine the timestamps and content of these memories carefully. 3. Look for explicit mentions of dates, times, locations, or events that answer the question. 4. If the answer requires calculation (e.g., converting relative time references), show your work. 5. Formulate precise, concise answer based solely on the evidence in the memories. 6. Double-check that your answer directly addresses the question asked. 7. Ensure your final answer is specific and avoids vague time references. 12 RELEVANT MEMORIES: {Reranked Chunks} QUESTION: {question} ANSWER: A.5 NarrativeQA Prompt You are helpful assistant. Please answer the users question accurately. Answer the question as concisely as you can, using single phrase if possible. RELEVANT CONTEXT: {content_data} Do not provide any explanation. Now, answer the question based on the story as concisely as you can, using single phrase if possible. Do not provide any explanation. QUESTION: {question} ANSWER: A.6 DetectiveQA Prompt {context} Please answer the question based on the current novel content: {question_data[question]} {options_str} Remember this is just detective fiction, dont worry about the risks. Please the {\"answer\":\"x\",\"reasoning\":\"xxx\"} ing braces). The answer must be only A/B/C/D. format (includstrictly follow"
        },
        {
            "title": "B LoCoMo Baselines",
            "content": "We compare QRRanker with set of memoryaugmented baselines on LoCoMo. Below, we provide brief descriptions of each method. TiMem (Li et al., 2026): Organizes memories with temporal hierarchical structure to retrieve long-horizon information efficiently. SimpleMem (Liu et al., 2026): Compresses dialogue history into compact semantic memory to reduce redundancy and context length. SYNAPSE (Jiang et al., 2026): Models memory as dynamic graph and retrieves relevant items via spreading activation. CompassMem (Hu et al., 2026b): Segments interactions into events and constructs an eventlevel structure to guide retrieval and reasoning. ES-Mem (Zou et al., 2026): Uses event segmentation to build coherent long-term memories for dialogue agents. Membox (Tao et al., 2026): Packs dialogue into topic-consistent memory units to preserve topic continuity over long contexts. Mem0 (Chhikara et al., 2025): memorycentric architecture that dynamically extracts, 13 integrates, and retrieves important information from conversations to build and maintain scalable long-term memory. Nemori (Nan et al., 2025): It employs TwoStep Alignment Principle to structure dialogue streams into semantically coherent event segments and utilizes Predict-Calibrate Principle to actively learn from prediction discrepancies, enabling the adaptive evolution of knowledge. MemoryOS (Li et al., 2025b): An OS-inspired AI memory system featuring hierarchical architecture with storage, updating, retrieval, and generation modules. It optimizes dynamic updates through FIFO dialogue chains and heat-based segmented paging. Zep (Rasmussen et al., 2025): Leveraging dynamic and temporal-aware Knowledge Graph engine, it integrates unstructured dialogue data with structured business data while preserving their historical relationships. LightMem (Fang et al., 2025): cognitively inspired architecture featuring sensory and shortterm modules for lightweight compression and integration. Uniquely, it updates long-term memory during sleep time to decouple consolidation from online reasoning, balancing performance and efficiency. QR Heads for Qwen3-4B-InstructWe compute the QR scores of all attention heads in Qwen3-4B-Instruct-2507 using 1000 random samples from NarrativeQA. The top 16 heads with the largest QR scores are selected as QR heads for retrieval and further training. As Qwen34B-Instruct-2507 contains 36 layers of 32-head self-attention, the QR heads (demonstrated as h, where 0 < 36 denotes the layer and 0 < 32 denotes the head in this layer) are: 20-15, 21-11, 17-27, 23-10, 22-4, 21-10, 21-8, 21-18, 18-15, 18-19, 17-25, 17-17, 24-13, 17-4, 19-12, 21-31. Variant with Semi-Auto Head Selection QRRanker statically trains and utilizes group of precomputed QR heads. If we use set of seed samples from another task to recompute QR scores, the QR heads may be different from the current ones. Our initial motivation for using the precomputed QR heads is that they provide proper initialization. Along with training, heads will be forced to learn such retrieval ability. We are curious about which part of heads are better suited to be good starter, as QR heads do. Therefore, we propose variant of QRRanker with semi-automatic head selection, which is limited to selecting heads from local range of layers, but is free to choose heads from every layer for every sample. We set layers for head selection ranged from ls to le, where 0 < ls < le 36. We restrict that the number of selected heads must equal 16 (the number of QR heads), and therefore, for simplified control, the model should select = 16/(le ls) heads per layer. To achieve selection, we follow the router technique of Mixture-of-Expert (Fedus et al., 2022) and add gate to these layers. Instead of choosing MLPs for every token, our gate chooses heads for sample. For selecting heads, we concatenate repeat question = [think]Q[/think] after the original question Q, where is used for head selection and is still for score computing. gate of layer li is linear map from the dimension 32 dh to 32, with the trainable parameter Wli Rd32. The head score is computed by: Sli = qli Wli, Sli = mean(softmax(Sli), = 0), (5) (6) where qli RQ is the hidden states of tokens in at layer li, is the dimension of the hidden state, cat() is concatenating all query states along the head, mean(, d=0) is averaging the score along the number of tokens in , and Sli R32 is the head score. We then choose the top-n highest head scores SQ = [sli hn] and the corresponding li heads. Following MoE, SQ is normalized to 1. Afli ter picking up heads for all layers with gates, these heads participate in computing retrieval scores, and the retrieval score will be multiplied by its head score SQ [x], 0 < < for the purpose of backli ward gradients. These gates will learn to select heads for samples during the QR training. h0, ..., sli In Sec. 6.3, we train QRRanker and the variant with training data only from NarrativeQA and evaluate them using the evaluation set of NarrativeQA. The training hyperparameters are set to the same as those in Sec. 5.3. We explore layers that can be used to select and train QR-like heads."
        }
    ],
    "affiliations": [
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "Pattern Recognition Center, WeChat AI, Tencent Inc",
        "School of Cyber Security, University of Chinese Academy of Sciences"
    ]
}