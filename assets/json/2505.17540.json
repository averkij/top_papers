{
    "paper_title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning",
    "authors": [
        "Mingrui Wu",
        "Lu Wang",
        "Pu Zhao",
        "Fangkai Yang",
        "Jianjin Zhang",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Weihao Han",
        "Hao Sun",
        "Jiayi Ji",
        "Xiaoshuai Sun",
        "Qingwei Lin",
        "Weiwei Deng",
        "Dongmei Zhang",
        "Feng Sun",
        "Qi Zhang",
        "Rongrong Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, a novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains a language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without human-annotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 4 5 7 1 . 5 0 5 2 : r RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning Mingrui Wu1, Lu Wang2, Pu Zhao2, Fangkai Yang2, Jianjin Zhang2, Jianfeng Liu2, Yuefeng Zhan2, Weihao Han2, Hao Sun2, Jiayi Ji1, Xiaoshuai Sun1, Qingwei Lin2, Weiwei Deng2, Dongmei Zhang2, Feng Sun2, Qi Zhang2, Rongrong Ji1 1 Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China 2 Microsoft"
        },
        {
            "title": "Abstract",
            "content": "Despite recent progress in text-to-image (T2I) generation, existing models often struggle to faithfully capture user intentions from short and under-specified prompts. While prior work has attempted to enhance prompts using large language models (LLMs), these methods frequently generate stylistic or unrealistic content due to insufficient grounding in visual semantics and real-world composition. Inspired by recent advances in reasoning for language model, we propose RePrompt, novel reprompting framework that introduces explicit reasoning into the prompt enhancement process via reinforcement learning. Instead of relying on handcrafted rules or stylistic rewrites, our method trains language model to generate structured, self-reflective prompts by optimizing for image-level outcomes. The tailored reward models assesse the generated images in terms of human preference, semantic alignment, and visual composition, providing indirect supervision to refine prompt generation. Our approach enables end-to-end training without humanannotated data. Experiments on GenEval and T2I-Compbench show that RePrompt significantly boosts spatial layout fidelity and compositional generalization across diverse T2I backbones, establishing new state-of-the-art results. Code is available at: https://github.com/microsoft/DKI_LLM/tree/main/RePrompt."
        },
        {
            "title": "Introduction",
            "content": "Text-to-image (T2I) generation has made rapid progress with the rise of large-scale generative models [22, 8, 34, 6], yet persistent challenge remains: users typically provide concise and underspecified prompts, which often result in images that fail to reflect the intended semantics or visually coherent compositions. Generated outputs may misrepresent object counts, overlook spatial relations, or violate real-world plausibility. This misalignment arises from the gap between natural language descriptions and the structured visual logic required for faithful image generation [53]. Previous work on prompt enhancement in T2I can be divided into two main approaches. The first approach focuses on iterative refinement: an image is generated from an initial prompt, and subsequent feedback, derived either from human preference models or from automated scoring systems, is used to improve the prompt or intermediate representations over multiple rounds [56, 47, 45, 13]. Although this approach can progressively improve image quality, it suffers from high latency and computational overhead due to repeated image generation, and it rarely incorporates explicit scene semantics or compositional reasoning. The second approach enriches prompts in single pass by leveraging large language models (LLMs) to inject additional detail and context [2, 15]. While these methods produce Work done at microsoft. Project Leader. Preprint. Under review. Figure 1: Given the user prompt \"a photo of couch below vase\", existing models like DELLE3 generate rich language descriptions but often produce unrealistic or physically implausible compositions. In contrast, our RePrompt performs explicit chain-of-thought reasoning to resolve spatial relations, resulting in enhanced prompts that guide text-to-image models towards realistic and semantically aligned generations. linguistically fluent and expressive descriptions, they frequently generate prompts that produce images with semantically inconsistent or visually implausible content, such as conflicting object placements or unrealistic interactions, because the underlying LLMs lack grounding in physical reality and do not incorporate feedback from downstream visual tasks. As result, they frequently hallucinate content or miss critical spatial and attribute-level relationships (see Figure 1). In contrast, we propose RePrompt, reasoning-augmented prompt refinement framework trained via reinforcement learning. Rather than relying on stylistic rewriting or black-box completions, RePrompt trains language model to generate structured and semantically grounded prompts through selfreflection and step-by-step decomposition. Motivated by recent advances in reasoning-augmented language models [11, 41, 40, 19], RePrompt enables the model to internally simulate the visual implications of promptmuch like how humans mentally visualize scene before drawing. This structured, logic-driven process anticipates potential errors (e.g., conflicting object positions, missing entities, or spatial incoherence) during prompt construction, thereby reducing the need for multiple rounds of image generation. core component of RePrompt is T2I RePrompt Reward Model tailored for text-to-image generation. Instead of relying on pre-labeled reasoning traces or handcrafted prompt templates, RePrompt learns from downstream visual feedback by optimizing prompt generation through reinforcement learning. To capture the multifaceted nature of image quality, we design ensemble reward that evaluates generated images along three dimentions: human preference, visual realism, and semantic alignment with the input. By learning from diverse and grounded feedback signals, the model develops more robust reasoning strategy that transfers across prompt types, scene structures, and T2I backbones, enabling stronger performance on unseen inputs without overfitting to specific linguistic patterns. Experiments on GenEval [10] and T2I-Compbench [16] demonstrate that RePrompt significantly improves compositional accuracy, semantic fidelity, and spatial coherence. Notably, on the GenEval benchmark, RePrompt surpasses Qwen2.5 3B-enhanced baselines by +77.1% (FLUX [22]), +78.8% (SD3 [8]) and +122.2% (Pixart-Σ [6]) in the position category, highlighting its superior capability in grounding spatial relations. Furthermore, RePrompt achieves the best overall accuracy (0.76) among all evaluated methods while maintaining an order-of-magnitude lower latency than optimizationheavy baselines like Idea2Img (30s vs. 140s per image), offering scalable and inference-efficient solution. These findings validate the effectiveness of explicit reasoning in prompt construction for closing the semantic-visual alignment gap in text-to-image generation, without relying on larger language models or expensive optimization at inference time. 2 Figure 2: Overview of the proposed RePrompt. For each input prompt, RePrompt generates multiple reasoning trace and enhanced prompt pairs. The reasoning trace guides the model to produce more detailed, image-grounded prompts. These are used to synthesize candidate images via T2I model, which are then scored by reward model. Feedback is used to update RePrompt via GRPO."
        },
        {
            "title": "2 Related Work",
            "content": "Text to Image Generation. Recently, large-scale diffusion models [22, 8, 34, 6, 48, 27, 2, 29, 36, 37] have achieved impressive progress in generating high-resolution, photorealistic images from complex textual prompts. To enhance alignment between text and visuals, prior work has explored prompt engineering [15, 32, 57, 31, 59, 4, 35, 56, 47, 45], often relying on manual or heuristic strategies with limited generalization. We propose reinforcement learning-based framework that automatically refines prompts through iterative reasoning, achieving better semantic alignment than static or rule-based methods. Reinforcement Learning. Reinforcement learning (RL) has proven effective in scenarios where iterative feedback is essential for task optimization. In the realm of generative models, RL-based approaches [11, 42, 54, 12, 14, 24, 63, 33, 3, 26, 21, 23, 39, 61, 43, 20, 51] have been employed to fine-tune outputs by maximizing reward function that encapsulates desired attributes such as image realism, diversity, and semantic fidelity. In our framework, we adopt RL techniques to drive the automatic optimization of text prompts. By defining multi-faceted reward that not only evaluates the visual quality of the generated image but also the interpretability and relevance of the prompt, our approach enables the model to learn an optimal prompt refinement strategy over successive iterations. Notably, T2I-R1 [20] is closely related to our work, but it targets Janus-Pro [7], unified vision-language model. In contrast, our method trains an auxiliary LLM that generalizes across diverse text-to-image models, offering more flexible and model-agnostic solution. Reasoning in LLM. Reasoning in LLMs [46] improves complex task solving by decomposing problems into intermediate steps [9, 18, 55, 60, 17, 58, 30, 25, 28]. In multimodal generation, reasoning mechanisms [53, 13, 44, 62, 38, 5] enhance prompt understanding and semantic alignment. Our method integrates reasoning with reinforcement learning, enabling step-wise prompt refinement that improves interpretability, text-image alignment, and image qualityoffering new perspective for prompt optimization in T2I generation."
        },
        {
            "title": "3 Method",
            "content": "We present RePrompt, reasoning-augmented reprompting framework for text-to-image (T2I) generation. RePrompt decouples prompt generation from image generation, i.e., training language model to produce structured, semantically rich prompts, while keeping the T2I backbone fixed. We optimize RePrompt via reinforcement learning (RL) to directly improve downstream image quality, compositional correctness, and usability. 3 3.1 Framework Overview RePrompt comprises three main modules (Figure 2): 1) Prompting Policy πθ, which produces reasoning trace and an enhanced prompt ; 2) fixed T2I Synthesizer fϕ, which renders an image from ; 3) T2I RePrompt Reward Model Rtotal(I, P, ), which scores the image on realism, semantic alignment, and prompt structure. Given an input prompt , the policy samples: = (H, ) πθ(P ), the synthesizer then generates: = fϕ(P ). Since fϕ is non-differentiable, we formulate prompt generation as single-step Markov Decision Process (MDP): State: the original prompt . Action: sampling = (H, ) πθ(y ). (cid:55) = fϕ(P ). Reward r: the reasoning reward = Transition: deterministic mapping Rtotal(I, P, ). Objective: maximize EP We train πθ via reinforcement learning (Group Relative Policy Optimization, GRPO [11]) to improve downstream image quality. By keeping the T2I model fϕ fixed, RePrompt learns backbone-specific reasoning strategies that enhance semantic fidelity and visual realism without requiring any manually annotated reasoning traces. (cid:2)Eyπθ(yP )[r](cid:3). 3.2 T2I RePrompt Reward Model central component of our framework is the T2I RePrompt Reward Modelan ensemble, imagegrounded reward function specifically designed for the prompt refinement task in T2I generation. In contrast to generic reward functions used in natural language or vision tasks, our reward model is codeveloped with the objective of enhancing reasoningdriven prompt construction. It explicitly evaluates whether generated prompt yields an image that is realistic, semantically faithful to the user intent, and compositionally coherent. Figure 3: The Visual-Reasoning Reward. This reward framework is critical to training RePrompt effectively and serves three key goals: (1) Stable optimization: Each component provides dense and structured feedback, mitigating the challenges of sparse or noisy reward signals during early-stage learning. (2) Multi-faceted supervision: The reward captures complementary aspects of T2I quality, including human preference, visual quality, and semantic alignment, ensuring holistic prompt improvement. (3) Cross-model generalization: Because the reward depends only on the prompt-image output pair and not on any specific T2I architecture, it generalizes naturally across different generation backbones and unseen prompt distributions. Together, these properties enable our reward model to not only guide the training of the reprompting policy but also ensure broad applicability and stable learning across varying settings in text-to-image generation. Visual-Reasoning Reward (Rvis). This component (as shown in Figure 3) captures both useraligned preferences and semantic correctness at the image level: Rvis = α RIMG pref + γ RVLM sem . (1) Here, RIMG pref is derived from ImageReward [50], which used to evaluate whether generated images align with human preferences. RVLM sem is obtained from VLM-Reward [1], which evaluates semantic consistency and visual quality using vision-language model. The weights α and γ allow us to control the trade-off between perceived image quality and factual alignment. Structure Reward (Rstruc). To ensure that the generated output maintains clear reasoning-toprompt format, we enforce structured syntax: <reason>...</reason><prompt>...</prompt> We apply binary reward: Rstruc = (cid:26)+1, if format is correct 1, otherwise (2) 4 This reward encourages models to adhere to standardized output layout, simplifying downstream parsing and ensuring the interpretability of reasoning traces. Length Reward (Rlen). To ensure compatibility with real-world T2I models such as SDXL and FLUX.1, which impose token-length limits, we apply constraint on prompt length: Rlen = (cid:26)+1, Lmin Lmax, 1, otherwise (3) where is the token length of , and [Lmin, Lmax] is empirically set to 15 and 77 tokens. This ensures the prompt is both concise and informative. Ensemble Reward and Optimization. All reward components are normalized to unit variance and summed: The RePrompt policy πθ is trained to maximize expected downstream reward: Rtotal = Rvis + Rstruc + Rlen. θ = arg max θ EP D, yπθ(yP ) [Rtotal(I, P, )] , where = fϕ(P ) and fϕ is fixed text-to-image generator. (4) (5) Generalization and Flexibility. Since the reward depends only on the input-output behavior of the system, not the internals of the image model, our framework generalizes well across unseen prompts and new T2I backbones. This modularity also enables RePrompt to adapt to model-specific strengths and failure patterns, while maintaining stable and interpretable training signals throughout. 3.3 Reprompt Optimization Prompt generation for T2I involves non-differentiable, black-box image renderer fϕ. RL allows us to optimize πθ directly with respect to downstream image outcomes rather than proxy losses on text. Moreover, because RePrompt is individualized to each T2I backbone, it can adapt its reasoning and prompt style to the specific strengths and limitations of given modelimproving generalization and image fidelity without retraining fϕ. At each update step, for given user prompt , we sample set of candidate outputs {yi}G πθold (y ). Each candidate yi = (Hi, reward ri = Rtotal(Ii, P, ). We then compute normalized advantages: i=1 ) and receives scalar ) yields an image Ii = fϕ(P Ai = ri µr σr , µr = 1 G (cid:88) j=1 rj, σr = (cid:118) (cid:117) (cid:117) (cid:116) 1 G (cid:88) (rj µr)2. j=1 (6) We use Group Relative Policy Optimization (GRPO) [11] as practical and stable update method for training the reprompting policy based on group-wise reward comparisons. The objective is defined as: JGRPO(θ) = EP,{yi}πθold (cid:34) 1 (cid:88) i=1 (cid:16) min ρi Ai, clip(ρi, 1 ε, 1 + ε) Ai (cid:17) βKL KL(cid:0)πθ(y ) πref (y )(cid:1) (cid:35) , (7) where ρi = πθ(yiP ) πθold (yiP ) , ε and βKL are clipping and penalty coefficients, πref is reference policy (e.g., the initial or distillation policy). By fixing fϕ and optimizing only πθ, RePrompt can be applied universally to any off-the-shelf T2I model, learning backbone, specific reasoning and prompt strategies without retraining the image generator. We further validate RePrompt with variancereduction analysis, showing that structured reasoning reduces reward uncertainty and lowers the sample complexity for accurate estimation. This leads to faster and more stable GRPO training. Full analysis and proof are provided in Appendix A. 5 Table 1: Evaluation of text-to-image generation on the GenEval benchmark. Our method consistently outperforms strong baselines, achieving the best overall scores. Notably, our approach shows substantial gains in spatial position understanding over Qwen2.5 3B-enhanced baselines, demonstrating its superior capability in grounding spatial relations. Method Single object Two object Counting Colors Position Attribute Overall binding FLUX [22] +GPT4 +Deepseek-r1 +Qwen2.5 7B +Qwen2.5 3B +Ours (train w/ FLUX) Improvement over Qwen2.5 3B -1.0% +3.6% +22.2% +4.9% +77.1% +2.1% +11.8% 0.99 0.99 1.00 0.99 0.99 0.98 0.45 0.52 0.43 0.51 0.48 0.49 0.79 0.79 0.81 0.83 0.84 0. 0.75 0.68 0.56 0.62 0.63 0.77 0.78 0.84 0.78 0.84 0.81 0.85 0.18 0.51 0.47 0.36 0.35 0.62 0.66 0.72 0.67 0.69 0.68 0.76 SD3 [8] +GPT4 +Deepseek-r1 +Qwen2.5 7B +Qwen2.5 3B +Ours (train w/ FLUX) Improvement over Qwen2.5 3B -1.0% 0.0% +13.2% +2.4% +78.8% +9.1% +10.3% 0.88 0.85 0.80 0.85 0.84 0. 0.62 0.51 0.53 0.49 0.53 0.60 1.00 1.00 0.99 1.00 1.00 0.99 0.58 0.54 0.46 0.58 0.55 0.60 0.22 0.48 0.44 0.34 0.33 0.59 0.69 0.70 0.67 0.68 0.68 0.75 0.85 0.84 0.82 0.82 0.86 0. Pixart-Σ [6] 0.26 0.60 0.31 0.67 +GPT4 0.27 0.63 +Deepseek-r1 0.32 0.67 +Qwen2.5 7B 0.32 0.68 +Qwen2.5 3B 0.35 +Ours (train w/ FLUX) 0.64 Improvement over Qwen2.5 3B -1.0% -5.9% +16.7% -1.2% +122.2% +9.4% 0.99 0.96 0.99 0.96 0.99 0.98 0.81 0.84 0.78 0.83 0.82 0.81 0.10 0.36 0.24 0.20 0.18 0.40 0.47 0.48 0.43 0.43 0.48 0.56 0.54 0.60 0.56 0.57 0.58 0.62 +6.9%"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Settings Implementation Details. We use Qwen2.5-3B [52] as our base language model. For the textto-image model used for training, we use the FLUX.1-dev [22] model, which generates images at resolution of 512512 pixels. Our model is trained using the TRL 2 reinforcement learning framework for 3 epochs, with 4 outputs generated per instance, the weight of ImageReward [50] and VLM-Reward are both 0.5. The VLM used for computing VLM-Reward is GPT-4V. All experiments were conducted on 8 NVIDIA A100 (80GB) GPUs, and the entire training process required about 6 hours. More details are in the Appendix B. Training Data. Inspired by the prompt construction strategy in GenEval [10], we adapt six objectcentric templates to newly curated list of 288 common daily objects generated via GPT-4 [1]. This results in training corpus of 9,000 prompts, carefully filtered to avoid overlap with the GenEval. We use 8,000 prompts to fine-tune our RePrompt via supervised learning, and 1,000 prompts for reinforcement learning. Evaluation Setup. To assess the impact of RePrompt, we evaluate on two benchmarks: GenEval [10] and T2I-Compbench [16]. GenEval focuses on instance-level alignment with user intent using concise prompts, while T2I-Compbench measures compositional generation under complex scenarios involving multiple objects, attributes, and spatial relations. 4.2 Comparision Table 1 presents the performance comparison across various text-to-image generation models evaluated on the GenEval benchmark, which assesses six fine-grained composition capabilities: singleobject, two-object, counting, color, spatial position, and attribute binding. 2https://github.com/huggingface/trl 6 Table 2: Evaluation of text-to-image generation on the T2I-Compbench. We report the baseline results, their variants enhanced with Qwen2.5 3B, and our method trained with FLUX. Our approach consistently improves performance across most aspects, particularly in Spatial compositions. Method Color Shape Texture Spatial Numeracy Complex FLUX +Qwen2.5 3B +Ours (train w/ FLUX) SD3 +Qwen2.5 3B +Ours (train w/ FLUX) Pixart-Σ +Qwen2.5 3B +Ours (train w/ FLUX) 0.7223 0.7149 0. 0.7941 0.7227 0.7866 0.5682 0.6618 0.6665 0.4796 0.5103 0.5276 0.5812 0.5478 0.5891 0.4717 0.4814 0.5011 0.6522 0.6012 0. 0.7224 0.6581 0.7184 0.5622 0.5662 0.6190 0.2494 0.2579 0.3301 0.2815 0.2549 0.3315 0.2497 0.2481 0.2913 0.6101 0.5982 0. 0.5871 0.5934 0.6289 0.5366 0.5443 0.5716 0.3616 0.3325 0.3721 0.3714 0.3307 0.3744 0.3655 0.3335 0.3680 Table 3: Ablation study of SFT and RL on the GenEval benchmark. Method Two Counting Colors Position Attribute Overall Single object object FLUX [22] 0.99 +Qwen2.5 3B 0.99 w/ SFT w/ RL w/ SFT + RL 0.99 0.98 0. 0.79 0.84 0.83 0.83 0.87 0.75 0.63 0.64 0.71 0.77 0.78 0.81 0.81 0.87 0. 0.18 0.35 0.43 0.41 0.62 binding 0.45 0.48 0.44 0.53 0.49 0.66 0. 0.69 0.72 0.76 Notably, our method demonstrates exceptional gains in spatial layout understanding (Position). For instance, when built upon FLUX [22] , our approach achieves 0.62 score on Position, representing +77.1% relative improvement over the Qwen2.5 3B baseline. Similarly, for SD3 [8], we observe +78.8% gain, and for Pixart-Σ [6], the relative improvement reaches an impressive +122.2%. This strong enhancement highlights the strength of our compositional training strategy in explicitly grounding spatial relations between objects. Beyond the Position metric, our method also achieves substantial improvements in Counting (+22.2% for FLUX, +13.2% for SD3, +16.7% for Pixart-Σ) and moderate gains in attribute binding (+2.1% to +9.4%). As result, we observe consistent boosts in overall GenEval scores: +11.8% (FLUX), +10.3% (SD3), and +6.9% (Pixart-Σ). These results verify the effectiveness of our reinforcement learningbased reprompting strategy, which enables the language model to iteratively reason about visual composition and generate more precise, image-aligned prompts, and brings state-of-the-art advances in spatial understanding. 4.3 Generalization Performance on T2I-Compbench We further assess the robustness of our method on the T2I-Compbench benchmark, which tests compositional generalization across six dimensions: color, shape, texture, spatial reasoning, numeracy, and complex attribute combinations. As shown in Table 2, RePrompt consistently improves performance across all evaluated backbones. In particular, it significantly boosts spatial compositional scores (e.g., from 0.2494 to 0.3301 on FLUX and from 0.2815 to 0.3315 on SD3) and enhances numeracy understanding, two long-standing challenges in text-to-image generation. Moreover, our method outperforms stronger LLM-enhanced baselines, for instance, on Pixart-Σ, RePrompt achieves notable gains in both texture and spatial dimensions. These results demonstrate that our approach generalizes well across diverse models and compositional skills, validating its effectiveness as versatile and plug-and-play enhancement for real-world T2I systems. 4.4 Ablation Study Ablation on SFT and RL. Table 3 shows the effect of supervised fine-tuning (SFT) and reinforcement learning (RL) on GenEval. SFT brings modest gains (+0.01 overall), mainly improving positional understanding (0.350.43), suggesting it helps inject object-attribute knowledge but struggles with complex reasoning. RL yields larger boosts (+0.04 overall), as it directly optimizes 7 Table 4: Quantitative comparison between RePrompt and other method on image generation accuracy and latency with the subset of Geneval. All latency is measured on single NVIDIA A100 GPU. Method FLUX Show-o [49] Idea2Img [56] (w/ FLUX) PARM++ [13] (w/ Show-o) RePrompt (w/ FLUX) Accuracy Latency (s / img) 0.65 0.55 0.69 0.72 0.76 20 3 140 110 30 Figure 4: Impact of our method across different base T2I models on the GenEval benchmark. Our method consistently improves the compositional understanding across all base models. visual correctness. Combining SFT and RL achieves the best results (0.76 overall), with strong improvements in spatial reasoning (0.62) and counting (0.77). These results confirm that SFT offers useful priors, while RL is key for compositional robustness. Comparison on Accuracy and Latency. Table 4 benchmarks our RePrompt against prior methods in terms of both generation accuracy (GenEval overall) and inference latency. Among base models, FLUX.1 achieves higher accuracy (0.65) compared to Show-o (0.55) but suffers from longer latency (20s vs. 3s per image), reflecting trade-off between quality and speed. When incorporating advanced prompting techniques, Idea2Img (w/ FLUX.1) improves accuracy to 0.69 but at the cost of significant latency increase (140s per image), while PARM++ (w/ Show-o) achieves 0.72 accuracy with 110s latency. In contrast, our RePrompt achieves the best accuracy (0.76) while maintaining much lower latency (30s). This demonstrates the effectiveness of our design in enabling precise visual grounding without compromising efficiency, making it more practical for real-world deployment. Ablation Study on Trained T2I Models. Figure 4 illustrates the generalization ability of RePrompt across three diverse T2I backbones: FLUX, SD3, and PixArt-Σ. Our method consistently boosts compositional scores on GenEvalimproving FLUX from 0.66 to 0.76, SD3 from 0.69 to 0.75, and PixArt-Σ from 0.54 to 0.62outperforming both baselines and the +Qwen2.5 variants. These results demonstrate RePrompts robustness and model-agnostic design, working effectively across both large (e.g., SD3) and lightweight (e.g., PixArt-Σ) backbones. Notably, we observe that stronger T2I models used during training lead to better generalization at inference, likely due to richer reasoning patterns induced during learning. Qualitative Comparison. Figure 5 illustrates the qualitative advantage of RePrompt over existing T2I models. Baseline models often generate images with incorrect spatial relations or hallucinated objects. For example, when prompted with fire hydrant with tennis racket, DALL-E 3 produces unrealistic, stylistic blends where the objects are merged. In contrast, RePrompt accurately grounds the tennis racket \"around the base\" of the fire hydrant, respecting the intended composition. Similarly, for photo of dog above cow, our method correctly depicts the dog in the air above the cow with \"a high angle shot\", aligning with the prompt semantics. These results highlight RePrompts ability to mitigate typical failures in spatial arrangement and object interaction by generating prompts with explicit compositional cues. More cases are in the Appendix D. 8 Figure 5: Qualitative results on compositional prompts. Compared to vanilla T2I models, our RePrompt improves spatial layout and object relations by generating enhanced prompts with explicit reasoning, leading to more faithful compositions. More Ablation Study. We present additional ablation studies on reasoning, training dynamics, and reward functions in the Appendix to further validate the effectiveness of our reasoning design and reward selection, as well as the stability of the training process."
        },
        {
            "title": "5 Limitations",
            "content": "While our method consistently improves compositional quality across standard T2I benchmarks, several limitations warrant future exploration. First, the performance gains on certain fine-grained taskssuch as numeracy and object countingremain modest, suggesting potential for further enhancement in precise quantitative reasoning. Second, our approach requires fine-tuning with compositional supervision, which may introduce additional computational cost and reliance on structured training signals. However, this design is consistent with common RLHF setups and does not limit practical deployment. Third, the effectiveness of RePrompt depends on the quality of the reward model; while we demonstrate robustness across standard evaluators, improvements in reward fidelity could further amplify performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose simple yet effective method to enhance compositional understanding in text-to-image (T2I) generation models. By injecting chain-of-thought (CoT) reasoning into the prompt construction pipeline and pairing each CoT with an enhanced prompt, our approach improves the alignment between textual descriptions and generated images. Extensive experiments on the GenEval benchmark demonstrate that our method consistently improves performance across various T2I backbones, including FLUX, SD3, and Pixart-Σ, without requiring additional model retraining. These results highlight the generalizability and plug-and-play nature of our method. We believe our approach offers new perspective for improving controllability and compositional fidelity in generative models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [4] Tingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui Zhu, and Jun Huang. Beautifulprompt: Towards automatic prompt engineering for text-to-image synthesis. arXiv preprint arXiv:2311.06752, 2023. [5] Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395, 2024. [6] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [7] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [9] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [10] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Jianshu Guo, Wenhao Chai, Jie Deng, Hsiang-Wei Huang, Tian Ye, Yichen Xu, Jiawei Zhang, Jenq-Neng Hwang, and Gaoang Wang. Versat2i: Improving text-to-image models with versatile reward. arXiv preprint arXiv:2403.18493, 2024. [13] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. [14] Shashank Gupta, Chaitanya Ahuja, Tsung-Yu Lin, Sreya Dutta Roy, Harrie Oosterhuis, Maarten de Rijke, and Satya Narayan Shukla. simple and effective reinforcement learning method for text-to-image diffusion fine-tuning. arXiv preprint arXiv:2503.00897, 2025. [15] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. Advances in Neural Information Processing Systems, 36:6692366939, 2023. [16] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. 10 [17] Qihan Huang, Long Chan, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, Jingyuan Chen, Chang Yao, and Jie Song. Boosting mllm reasoning with text-debiased hint-grpo. arXiv preprint arXiv:2503.23905, 2025. [18] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [19] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [20] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, PhengAnn Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [21] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. [22] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [23] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [24] Seung Hyun Lee, Yinxiao Li, Junjie Ke, Innfarn Yoo, Han Zhang, Jiahui Yu, Qifei Wang, Fei Deng, Glenn Entis, Junfeng He, et al. Parrot: Pareto-optimal multi-reward reinforcement learning framework for text-to-image generation. In European Conference on Computer Vision, pages 462478. Springer, 2024. [25] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [26] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1940119411, 2024. [27] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. [28] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. [29] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [30] Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek Wong, Xiaoyi Feng, and Maosong Sun. Deepperception: Advancing r1-like cognitive visual perception in mllms for knowledge-intensive visual grounding. arXiv preprint arXiv:2503.12797, 2025. [31] Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-to-image consistency via automatic prompt optimization. arXiv preprint arXiv:2403.17804, 2024. [32] Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen, and Qing Yang. Dynamic prompt optimizing for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2662726636, 2024. [33] Ofir Nabati, Guy Tennenholtz, ChihWei Hsu, Moonkyung Ryu, Deepak Ramachandran, Yinlam Chow, Xiang Li, and Craig Boutilier. Personalized and sequential text-to-image generation. arXiv preprint arXiv:2412.10419, 2024. [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [35] Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei Wen. Diffusiongpt: Llm-driven text-to-image generation system. arXiv preprint arXiv:2401.10061, 2024. [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [38] Zahraa Al Sahili, Ioannis Patras, and Matthew Purver. Faircot: Enhancing fairness in diffusion models via chain of thought reasoning of multimodal language models. arXiv preprint arXiv:2406.09070, 2024. [39] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [40] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [41] Luong Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 76017614, 2024. [42] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [43] Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. [44] Yi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun Shu, Zhong Tao, Dong She, Zhelun Yu, et al. Mint: Multi-modal chain of thought in unified generative models for enhanced image generation. arXiv preprint arXiv:2503.01298, 2025. [45] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. Advances in Neural Information Processing Systems, 37:128374128395, 2024. [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [47] Tsung-Han Wu, Long Lian, Joseph Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-controlled diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63276336, 2024. 12 [48] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [49] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [50] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [51] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. [52] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [53] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. [54] Shentao Yang, Tianqi Chen, and Mingyuan Zhou. dense reward view on aligning text-toimage diffusion with preference. arXiv preprint arXiv:2402.08265, 2024. [55] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [56] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v for automatic image design and generation. In European Conference on Computer Vision, pages 167184. Springer, 2024. [57] Shih-Ying Yeh, Sang-Hyun Park, Giyeong Oh, Min Song, and Youngjae Yu. Tipo: Text to image with text presampling for prompt optimization. arXiv preprint arXiv:2411.08127, 2024. [58] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [59] Taeyoung Yun, Dinghuai Zhang, Jinkyoo Park, and Ling Pan. Learning to sample effective and diverse prompts for text-to-image generation. arXiv preprint arXiv:2502.11477, 2025. [60] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [61] Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, and Bin Cui. Itercomp: Iterative composition-aware feedback learning from model gallery for text-to-image generation. arXiv preprint arXiv:2410.07171, 2024. [62] Yuyao Zhang, Jinghao Li, and Yu-Wing Tai. Layercraft: Enhancing text-to-image generation with cot reasoning and layered object integration. arXiv preprint arXiv:2504.00010, 2025. [63] Hanyang Zhao, Haoxian Chen, Ji Zhang, David Yao, and Wenpin Tang. Scores as actions: framework of fine-tuning diffusion models by continuous-time reinforcement learning. arXiv preprint arXiv:2409.08400, 2024."
        },
        {
            "title": "A Variance Reduction via Structured Reasoning",
            "content": "In this appendix, we provide full theoretical analysis showing that conditioning prompt generation on an explicit reasoning trace strictly reduces the variance of the downstream reward estimator. This reduction in variance directly leads to improved sample efficiency for reinforcement learning. A.1 Setup and Notation Let: be the space of bare prompts . be the space of reasoning traces H. : [0, Rmax] be the reward random variable obtained by sampling π(P ) and generating an image = fϕ(P ). rH : [0, Rmax] be the reward when first sampling πH (P ), then π(P H), and finally = fϕ(P ). All expectations and variances are taken over the joint sampling of and . A.2 Law of Total Variance By the law of total variance, Var(cid:2)r(P )(cid:3) = EH Since variances are nonnegative, we immediately have: Theorem A.1 (Variance Reduction). (cid:2)Var[r(P ) H](cid:3) + VarH (cid:2)E(cid:2)r(P ) H(cid:3)] Var(cid:2)r(H, )(cid:3) = EH (cid:2)Var(cid:2)r H(cid:3)(cid:3) Var(cid:2)r(P )(cid:3). Proof. By definition, and since Var(cid:2)r(H, )(cid:3) = EH [Var[r H]] Var[r(P )] = EH [Var[r H]] + VarH [E[r H]], the nonnegativity of VarH [E[r H]] yields the result. A.3 Sample Complexity Improvement Variance directly impacts the number of samples required to estimate the expected reward within given accuracy. Consider estimating the expected reward µ = E[r] by drawing independent samples {ri}. By Chebyshevs inequality, Thus, to guarantee Pr(ˆµ µ ε) δ, we require (cid:16) Pr ˆµ µ ε (cid:17) Var[r] ε2 . Var[r] ε2 δ . Applying Theorem A.1, using reasoning reduces the required sample size: Nreasoning = Var[r(H, )] ε2δ Var[r(P )] ε2δ = Nbare. A.4 Discussion This analysis formalizes the intuition that introducing an intermediate reasoning trace conditions the policy on structured, compositional information about the scene, thereby reducing uncertainty (variance) in the reward signal. Empirically, this translates to fewer rollouts needed during GRPO training and more stable gradient estimatesaccelerating convergence without additional data or annotations."
        },
        {
            "title": "B Experiment Setting",
            "content": "Table 5 summarizes the key hyperparameters used in our experiments, including configurations for the GRPO optimization algorithm, the FLUX.1 text-to-image model, and the joint training process. For GRPO, we set the clipping range ε to 0.2 and the KL penalty coefficient β to 0.04 with group size of 4. The FLUX.1 model operates at resolution of 512 512, with 50 diffusion steps and classifier-free guidance (CFG) scale of 3.5. In joint training, we balance ImageReward and VLM-Reward with equal weights (0.5 each), and constrain prompt lengths between 15 and 77 tokens. Training is conducted using 8 devices with per-device batch size of 4, learning rate of 2e-6, gradient accumulation steps of 2, and total of 3 epochs. Table 5: Key Parameters for GRPO and T2I Model. Config Symbol Value GRPO Config"
        },
        {
            "title": "Clipping range\nKL penalty coefficient\nGroup size",
            "content": "ε β 0.2 0.04 4 FLUX.1 Config Image resolution Diffusion steps CFG scale 512 512 λcfg Joint Training Parameters ImageReward weight VLM-Reward weight Min prompts length Max prompts length Device number Per device batch size Learning rate Gradient Accumulation Epoch α γ Lmin Lmax - lr - - 50 3.5 0.5 0.5 15 77 8 4 2e-6"
        },
        {
            "title": "C Ablation Study",
            "content": "Ablation Study on Reasoning. Table 6 presents an ablation study on the GenEval benchmark to assess the impact of reasoning in our reinforcement learning framework. The FLUX baseline, integrating large language model (+Qwen2.5 3B) brings modest gains across most categories, raising the overall score to 0.68. Applying RL without reasoning achieves similar overall improvement (0.68), suggesting that reward-driven optimization alone contributes to better alignment. However, incorporating reasoning into the RL loop leads to more substantial improvement, pushing the overall score to 0.72. Notably, categories that require more complex semantic understandingsuch as \"Colors\" (from 0.83 to 0.87) and \"Attribute binding\" (from 0.46 to 0.53)benefit the most. These results demonstrate that step-by-step reasoning helps the model better decompose and interpret textual prompts, thereby enabling more accurate and faithful image generation. Training Dynamics. Figure 6 presents the reward curve during reinforcement learning of the RePrompt. We observe stable and monotonically increasing trend in the reward, demonstrating that our reward model provides reliable and effective supervision signal throughout training. The absence of sharp fluctuations or reward collapse suggests that our RL setup maintains stable policy updates. This aligns with the observed downstream improvements, confirming that reward-guided prompt refinement effectively enhances compositional alignment in generated images. Ablation Study on Visual Reasoning Rewards. To investigate the effectiveness of different visual reasoning reward signals used in our reinforcement learningbased reprompting strategy, we conduct 15 Table 6: Ablation study of reasoning on the GenEval benchmark. Method Two Counting Colors Position Attribute Overall Single object object FLUX +Qwen2.5 3B RL w/o reasoning RL w/ reasoning 0.99 0.99 1.00 0. 0.79 0.84 0.81 0.83 0.75 0.63 0.68 0.71 0.78 0.81 0.83 0. 0.18 0.35 0.33 0.41 binding 0.45 0.48 0.46 0.53 0.66 0. 0.68 0.72 Figure 6: Training curve of our RePrompt during reinforcement learning. The curve shows steady reward improvement, indicating stable training dynamics and effective reward signal. detailed ablation study on the GenEval benchmark. As shown in Table 7, our method consistently improves over the FLUX baseline and Qwen2.5 3B variant across all subcategories, demonstrating the efficacy of reward-driven learning in aligning generated images with prompt semantics. Both ImageReward and VLM-Reward show noticeable gains over the +Qwen2.5 3B baseline, particularly in Colors (0.88 vs. 0.81 with ImageReward) and Counting (0.69 vs. 0.63 with VLM-Reward), indicating that each reward captures complementary aspects of visual faithfulness. However, their standalone performances are still limited in challenging tasks like Position and Attribute Binding. Notably, our ensemble reward formulationwhich combines both ImageReward and VLM-based feedbackachieves the best overall performance (0.72), with consistent improvements in Counting (+0.08), Position (+0.06), and Attribute Binding (+0.05) compared to the +Qwen2.5 3B baseline. These are precisely the categories that require stronger compositional understanding and spatial reasoning, underscoring the strength of our composite reward design in driving semantically aligned image generation.The ablation confirms that each reward contributes uniquely, and their integration enables more holistic supervision, leading to substantial gains in compositional image-text alignment. Training Dynamics with Different Visual Reasoning Rewards. We analyze the impact of different visual reasoning rewards on training dynamics by plotting training curves under various reward configurations. As shown in Figure 7, training with single reward model leads to unstable reward fluctuations and degraded sample quality. This instability arises from the limited supervision capacity of single reward model, which may overfit to specific patterns or neglect important compositional aspects. In contrast, our proposed multi-reward formulationbalancing diverse reasoning signals, enables smoother optimization and more consistent improvements across iterations. These results Table 7: Ablation study of rewards on the GenEval benchmark. Method FLUX +Qwen2.5 3B R1-Prompter α - - γ - - 0 1 0 1 0.5 0.5 Single object object Two Counting Colors Position Attribute Overall 0.99 0. 0.99 0.99 0.98 0.79 0.84 0.84 0.82 0.83 0.75 0.63 0.63 0.69 0.71 0.78 0. 0.88 0.85 0.87 0.18 0.35 0.38 0.32 0.41 binding 0.45 0.48 0.50 0.47 0. 0.66 0.68 0.70 0.69 0.72 Figure 7: Training curve with different visual reasoning rewards. Using single reward model leads to instability and suboptimal performance during training. In contrast, our balanced reward designcombining multiple specialized reward signalsyields more stable convergence and higher final reward values. emphasize the importance of combining complementary reward models to achieve both stability and generalization."
        },
        {
            "title": "D More Cases",
            "content": "More Qualitative Comparison. Figure 8, 9, 10 and 11 present qualitative comparisons on different types of compositional prompts, including Position, Two-object, Color, and Attribute Binding. Compared to baseline methods such as DELL-E3, our approach produces images that more faithfully adhere to the spatial, numerical, and attribute-based constraints specified in the prompts. For instance, in Figure 7, our method accurately grounds relative positions (e.g., \"above\", \"left of\") by leveraging explicit reasoning, while DELL-E3 often fails to reflect such relations or misplaces objects entirely. Figure 8 highlights our advantage in handling prompts involving multiple objects, where baseline models tend to merge objects or hallucinate irrelevant content. Similarly, in Figure 9, our method is better at preserving specified colors for each object, whereas the baseline sometimes misbinds colors or applies them inconsistently. Finally, as shown in Figure 10, our approach improves attribute binding, ensuring that each attribute is applied to the correct object without confusion. These results demonstrate the effectiveness of integrating reasoning to improve alignment between visual outputs and complex prompt semantics. 17 Figure 8: Qualitative results on compositional prompts (Position). We compare DELL-E3, the intermediate reasoning process, and our final RePrompt outputs. While DELL-E3 often struggles with spatial relations (e.g., object positions), our reasoning-guided approach enables more accurate and faithful generations that better match the prompts compositional constraints. 18 Figure 9: Qualitative results on compositional prompts (Two-object). We show comparisons among DELL-E3, our reasoning process, and RePrompt outputs on prompts involving two objects. DELL-E3 often fails to generate both entities accurately, whereas our method uses explicit reasoning to guide the model in generating semantically correct and compositionally faithful images. Figure 10: Qualitative results on compositional prompts (Color). We present qualitative comparisons on prompts involving specific color attributes. While DELL-E3 tends to ignore or misinterpret color constraints, our approach leverages explicit reasoning to ensure accurate color grounding for each object, resulting in more faithful visual compositions. 20 Figure 11: Qualitative results on compositional prompts (Attribute binding). We show examples where prompts specify multiple attributes that must be correctly associated with the corresponding objects. Our method utilizes step-by-step reasoning to disambiguate attribute-object bindings, avoiding attribute swaps or omissions that are common in baseline outputs."
        }
    ],
    "affiliations": [
        "Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China, Xiamen University, 361005, P.R. China",
        "Microsoft"
    ]
}