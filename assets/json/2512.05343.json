{
    "paper_title": "SpaceControl: Introducing Test-Time Spatial Control to 3D Generative Modeling",
    "authors": [
        "Elisabetta Fedele",
        "Francis Engelmann",
        "Ian Huang",
        "Or Litany",
        "Marc Pollefeys",
        "Leonidas Guibas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains a key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SpaceControl, a training-free test-time method for explicit spatial control of 3D generation. Our approach accepts a wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. A controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SpaceControl outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/"
        },
        {
            "title": "Start",
            "content": "SPACECONTROL: INTRODUCING TEST-TIME SPATIAL CONTROL TO 3D GENERATIVE MODELING Elisabetta Fedele1,2, Francis Engelmann2, Marc Pollefeys1, Leonidas Guibas2 1ETH Zurich 2Stanford University 3Technion 4NVIDIA Ian Huang2, Or Litany3,4, 5 2 0 D 5 ] . [ 1 3 4 3 5 0 . 2 1 5 2 : r Figure 1: SPACECONTROL enables spatially controlled 3D asset generation from simple geometric primitives such as superquadrics (light blue) and other geometry types such as polygon meshes. Top: rapid asset generation. From quick 3D sketches and brief text prompts, we can generate high quality assets. Bottom: fine-grained editing, including adjusting chairs backrest and adding armrests (left) or precisely controlling sofas dimensions and pillow arrangements (right)."
        },
        {
            "title": "ABSTRACT",
            "content": "Generative methods for 3D assets have recently achieved remarkable progress, yet providing intuitive and precise control over the object geometry remains key challenge. Existing approaches predominantly rely on text or image prompts, which often fall short in geometric specificity: language can be ambiguous, and images are cumbersome to edit. In this work, we introduce SPACECONTROL, training-free test-time method for explicit spatial control of 3D generation. Our approach accepts wide range of geometric inputs, from coarse primitives to detailed meshes, and integrates seamlessly with modern pre-trained generative models without requiring any additional training. controllable parameter lets users trade off between geometric fidelity and output realism. Extensive quantitative evaluation and user studies demonstrate that SPACECONTROL outperforms both training-based and optimization-based baselines in geometric faithfulness while preserving high visual quality. Finally, we present an interactive user interface that enables online editing of superquadrics for direct conversion into textured 3D assets, facilitating practical deployment in creative workflows. Find our project page at https://spacecontrol3d.github.io/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generating 3D assets is fundamental step in building virtual worlds, useful for gaming, simulation, virtual reality applications, and digital design. Recently the field of 3D generation gained immense Equal contribution 1 traction, and we are now able to create assets of previously unseen quality (Xiang et al., 2025; Zhang et al., 2024; Vahdat et al., 2022; Gao et al., 2022; Wu et al., 2025; Siddiqui et al., 2024; Zhao et al., 2025; Chen et al., 2025b; Huang et al., 2025; Corsetti et al., 2025). persistent challenge, however, is controllability, i.e., how users can effectively steer generation to align with desired shapes and appearances. Current controllable 3D generation methods rely mainly on text or image conditioning. Text is accessible and flexible but inherently ambiguous and ill-suited for specifying precise geometry. Images provide stronger alignment with 3D structures but are cumbersome to edit and not intuitive for fine-grained adjustments. As result, neither modality enables artists or designers to directly manipulate the geometry of generated objects. more natural paradigm is to allow users to interact with the generative model in 3D space, starting from coarse or abstract geometry and refining toward detailed assets. Existing methods that introduce 3D geometric control fall into two categories: training-based and guidance-based. Training-based methods fine-tune existing generative models to support specific form of geometric input, e.g., LION (Vahdat et al., 2022) for voxel conditioning, and SpiceE (Sella et al., 2024) for primitive or mesh conditioning. These methods provide controllability but require retraining, which reduces the original models generalization capabilities. In contrast, guidance-based methods such as LatentNeRF (Metzer et al., 2023) and Coin3D (Dong et al., 2024) act solely at inference time without retraining, but usually involve substantial optimization overhead and constrain 3D structure only indirectly. Other works enrich existing 3D assets with geometric and appearance detail (Michel et al., 2022; Chen et al., 2023; Barda et al., 2025), yet they assume fine-grained input geometry, limiting usability in creative workflows where artists often begin with coarse sketches. In this work, we present SPACECONTROL, training-free method that injects explicit geometric control into modern framework for textor image-conditioned 3D generation, such as Trellis (Xiang et al., 2025) or SAM 3D (Chen et al., 2025a), by directly encoding user-specified geometry into its latent space and using it as explicit guidance. Our method requires no additional training and enables controllable generation from diverse forms of geometry, ranging from simple primitives to detailed meshes. We compare SPACECONTROL against both training-based (Sella et al., 2024) and guidancebased (Dong et al., 2024) approaches, as well as stronger training-based variant of Spice-E adapted to Trellis. Remarkably, despite requiring no fine-tuning, SPACECONTROL achieves superior geometric faithfulness while preserving visual realism. We further provide user interface that allows online editing of superquadrics and real-time generation of textured assets, supporting practical deployment in design workflows. In summary, our contributions are the following: We introduce training-free guidance method that conditions powerful pre-trained generative model (Trellis) on user-defined geometry via latent space intervention, enabling geometry-aware generation without the need for costly fine-tuning. We conduct extensive evaluations, including user study and quantitative analysis, showing that our method outperforms prior state-of-the-art methods for shape-conditioned 3D asset generation. We develop an interactive user interface that enables online editing of superquadrics and their real-time conversion into detailed, textured 3D assets, supporting practical deployment in creative workflows."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 3D GENERATIVE MODELS The field of 3D generation has experienced rapid growth during the past few years both in terms of output modalities and controllability. Similar to the first image diffusion models (Ramesh et al., 2021), early applications of diffusion models for 3D generation (Nichol et al., 2022) were conducting the diffusion process in the original input space and were limited in the generated output type. 2 More recent approaches (Vahdat et al., 2022; Jun & Nichol, 2023) started running the generation in more compact latent space, leading to substantial improvements both in terms of quality and efficiency. To achieve an even increased efficiency, (Zhang et al., 2024; Xiang et al., 2025) have started to disentangle the modeling of the structure from the appearance, leading to unprecedented high-quality generations. The separate modeling of geometry and appearance opens the door to explicit forms of spatially grounded conditioning, as done in our SPACECONTROL."
        },
        {
            "title": "2.2 CONTROLLABLE GENERATIVE MODELS",
            "content": "Given pretrained generative model, there are two main approaches to introduce new control modality: (1) methods which finetune part or the whole network to take new types of conditioning as input, and (2) training-free methods which condition the generation via inference-time guidance. In the last years, many approaches have been developed to control the generation of image generative models, enabling conditioning in several forms as strokes, depth maps, and human poses. In contrast, analogous mechanisms for 3D generation remain far less mature."
        },
        {
            "title": "CONTROLLING IMAGE GENERATIVE MODELS",
            "content": "A wide variety of methods have been proposed to introduce new control modalities to image generative models. Among works based on finetuning, we identify two main lines of research. On one side, there are works based on ControlNet (Zhang et al., 2023; Bhat et al., 2024) which add conditional control to section of the network by introducing trainable copy connected to the original via zero convolutions. The key idea is to learn to control the original network without throwing information from the original training. On the other side, there are approaches which add additional layers for additional control of the network (Garibi et al., 2025; Hertz et al., 2022). Among training-free methods (Von Rutte et al., 2024; Meng et al., 2022; Sajnani et al., 2025), one closely related to our work is SDEdit (Meng et al., 2022) which uses stroke paintings to condition the generation of SDE-based generative models for images, by leveraging the denoising process of SDE-based generative models. CONTROLLING 3D GENERATIVE MODELS Only limited works have explored spatially grounded control of 3D generative models. On one side, approaches as LatentNERF (Metzer et al., 2023), Fantasia3D (Chen et al., 2023), Instant3dit (Barda et al., 2025), and Phidias (Wang et al., 2025) apply the spatial control on image generative models by projecting the conditioning 3D model on multiple views and then leveraging test-time optimization to combine them in 3D representation. On the other side, Spice-E (Sella et al., 2024) directly apply the control in the 3D space, by finetuning Shap-E (Jun & Nichol, 2023) separately on chairs, tables and airplanes from ShapeNet (Chang et al., 2015). Both the approaches attempt explicit spatial control, but nonetheless fall short of introducing method thats as usable in unconstrained settings as introduced in their 2D counterparts. The former still requires long optimization times and use the geometric input to condition the generation of the 2D projections of the 3D objects, instead of directly conditioning in 3D. The latter needs class-specific fine-tuning which limits the applicability in unconstrained settings and does not allow to model the strength of the geometric control. By contrast, SPACECONTROL introduces test-time guidance directly within 3D generative models, providing framework that is efficient, accurate, and fast."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Before introducing our SPACECONTROL, we review the foundations on which it builds: rectified flow matching, the Trellis generative model, as well as superquadrics. 3.1 RECTIFIED FLOW MODELS Rectified flow models use linear interpolation forward (diffusion) process where for specific time step [0, 1], the latent zt can be expressed as zt = (1 t)z0 + tϵ, where ϵ (0, I) and z0 is clean sample from the target data distribution. The backward (denoising) process is represented by time dependent velocity field v(zt, t) = tzt. In practice, starting from noisy sample z1, we can obtain the denoised version z0 by discretizing the time interval [0, 1] into discrete steps, possibly 3 t t T a Noise up to t0 zc,0 zt0 Skip Flow 1 t0 0 Structure FM Output Flow 1 0 Appearance FM Input Structure Generation (Secs. 4.2.1) Appearance Generation (Sec. 4.2.2) Figure 2: Model Overview. Given an input conditioning which includes spatial control, text prompt and an image (optional), SPACECONTROL produces realistic 3D assets. First the different conditioning are encoded in latent space. Specifically, the spatial control is voxelized and encoded by Trellis encoder E, the text is encoded by CLIP encoder ECLIP , and the image (if present) is encoded by DINOv2 encoder EDIN O. The obtained latents z0,c are noised up to t0 to obtain zt0. From t0 to = 0, zt0 are denoised by the Structure Flow Model (FM), guided by the text prompt features. The clean latents z0 are then fed into the decoder D, which outputs the voxel grid x0. Then, the active voxels are augmented with point-wise noisy latent features, denoised by the Appearance Flow Model (FM), using either text or image conditioning. The clean latents can then be decoded into versatile output formats such as 3D gaussians (GS), radiance fields (RF), and meshes (M) via specific decoders DO = {DGS, DRF , DM }. not uniformly distributed, and recursively applying the equation zt(i+1) = zt(i) vθ (cid:0)zt(i), t(i)(cid:1)(cid:0)t(i) t(i + 1)(cid:1) , (1) where [1, 1], and the vector field vθ() is predicted for example by Diffusion Transformer (Peebles & Xie, 2023) as in Trellis (Xiang et al., 2025) or SAM 3D (Chen et al., 2025a). 3.1.1 STEPS SCHEDULE Time steps are initially defined as t(τ ) = 1 τ /T for τ [0, ], and then rescaled by factor λ: t(τ ) = λt(τ ) 1 + (λ 1)t(τ ) . (2) Since can be obtained from τ and vice versa, we will refer to either one interchangeably. 3.2 TRELLIS Trellis (Xiang et al., 2025) is recent 3D generative model which employs rectified flow models to generate 3D assets from either textual or image conditioning. Specifically, it consists of two separate steps of generations, where the first aims to generate the structure, while the second focus on the appearance. Here, we rely on Trellis, however the very recent SAM 3D follows the same approach. 3.2.1 STRUCTURE GENERATION In the first stage, noisy latent variable z1 R1616168 is sampled from (0, I) and denoised by rectified flow model iteratively applying Eq. 1 using either image or text conditioning. Specifically, text conditions are encoded via the CLIP (Radford et al., 2021) text encoder, while image conditions are encoded via DINOv2 (Oquab et al., 2024). The denoised latent z0 is then decoded by decoder into binary occupancy grid {0, 1}646464, which encodes the spatial structure of the 3D asset. Note that the pretrained decoder comes with corresponding pretrained encoder E. While is not used during Trellis inference, it is required for our guidance mechanism, as detailed later. 3.2.2 APPEARANCE GENERATION In the second stage, the active voxels are augmented with point-wise noisy latent features s1 RL8 sampled from (0, I), denoised by second flow model, using either text or image con4 Realistic Faithful Figure 3: Realism-faithfulness tradeoff. The hyperparameter τ0 allows smooth control over the strength of the control. In the left figure we show how variations of τ0 affects the generations quantitatively in terms of Chamfer distance to the spatial control (lower means more faithful) and of FID score (lower means more realistic). In the right figure we show it qualitatively, visualizing how higher values of τ0 lead to assets whose geometry looks even more similar to the control. For conciseness we only show the untextured geometry. ditioning. The clean latents s0 RL8 can then be decoded into versatile formats such as 3D gaussians (GS), radiance fields (RF), and meshes (M) via specific decoders DO = {DGS, DRF , DM }. 3.3 SUPERQUADRICS Superquadrics (Barr, 1981) provide compact parametric family of shapes capable of representing diverse geometries. canonical superquadric is defined by five parameters: scales (sx, sy, sz) and exponents (ϵ1, ϵ2). With parametric coordinates (η, ω) we can define their surface as: sxcos (η)ϵ1 cos (ω)ϵ2 sycos (η)ϵ1 sin (ω)ϵ2 szsin (η)ϵ1 s(η, ω) = . (3) Extending to world coordinates requires 6 additional pose parameters (3 translation, 3 rotation), giving 11 parameters in total. Their compactness makes them well-suited as spatial control primitives."
        },
        {
            "title": "4 METHOD",
            "content": "4.1 SETUP To introduce spatial control in 3D generation, the user provides geometric conditioning signal alongside text or image prompt. Our goal is to produce 3D assets that satisfy two key desiderata: Faithfulness: the generated asset should be aligned with the control geometry. Realism: the generated asset should retain the quality of the original model. 4.2 APPROACH Next, we introduce SPACECONTROL (see Fig. 2) and describe how it enables guided 3D asset generation by injecting spatial guidance into pretrained Trellis model. Because our control strategy differs between the first and second stages of generation, we outline the respective procedures in Secs. 4.2.1 and 4.2.2, respectively. 4.2.1 STRUCTURE GENERATION To control the first step of generation given an explicit control geometry we employ similar framework to SEdit (Meng et al., 2022), where instead of using strokes to guide the generation of 2D images, we use either coarse or detailed 3D geometry to guide the generation of 3D assets. Specifically, given user-specified 3D geometry, we voxelize it to obtain xc {0, 1}646464 and feed xc into the pretrained encoder to obtain zc,0 R1616168. Then given specific time step t0 [0, 1] we noise up the latents zc,0 to that specific step via the rectified flows forward equation: zt0 = t0z1 + (1 t0) zc,0 , (4) 5 where z1 (0, I). Given zt0, z0 can then obtained by iteratively applying Eq. 1 starting from t0 and employing the original Structure Flow Model. We note that this process does not require any need of architectural changes nor training. We guide the generation with additional textual prompt, which is helpful to disambiguate the semantics of the object. As in the standard setting, the denoised latent z0 is then decoded into final geometric structure x0 {0, 1}646464 by D."
        },
        {
            "title": "4.2.2 APPEARANCE GENERATION",
            "content": "Given the geometric structure generated in the first stage, we then employ either text or image conditioning to guide the generation of its appearance, by first expanding the active voxels with point-wise noisy latent features and then denoising them using the Appearance Flow Model. Notice that, even if the structure generation is always conditioned on text, image conditioning can still be used in to guide the appearance generation, allowing for finer control over the visual details (see Fig. 6a and Appendix)."
        },
        {
            "title": "4.3 CONTROLLING THE STRENGTH OF SPATIAL CONTROL",
            "content": "The strength of spatial control can be tuned through the parameter τ0. For lower values of τ0, the latent zt0 is initialized closer to the noise z1 than to the control signal zc,0, leading the model to perform more denoising steps. This favors samples that follow the data distribution of the original Trellis, producing outputs that are generally more realistic but less faithful to the spatial conditioning. In contrast, higher values of τ0 bias zt0 towards zc,0, effectively skipping earlier denoising steps and preserving more of the injected spatial structure, albeit sometimes at the expense of realism."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 COMPARING WITH STATE-OF-THE-ART METHODS Tasks. We evaluate SPACECONTROL under two types of spatial conditions: (1) coarse geometry and (2) detailed geometry, using simple geometric primitives and full object meshes, respectively. Boat Chicken Cow Elephant Radio Submarine Tree Whale t D 3 C - p - - P u Figure 4: Qualitative Comparison of Spatially Conditioned Generation. We show generations obtained conditioning our SPACECONTROL and baselines on text prompts and superquadrics from the Toys4K dataset. While other methods either fail to follow the conditioning (e.g., the antenna from the radio generated by Spice-E is wrongly placed) or to generate visually appealing 3D assets (e.g., the chicken generated by SPICE-E-T exhibits anatomically incorrect body part placements), SPACECONTROL exhibits good balance between realism and faithfulness. Table 1: Comparison with Baselines. The evaluation metrics are L2 Chamfer Distance (CD) and Frechet Inception Distance (FID). CD quantifies alignment with spatial control, while FID assesses realism. Results for SPACECONTROL are reported at τ0 = 6. CD scores are multiplied by 103. indicates methods fine-tuned on chair and table. Trellis (Xiang et al., 2025) (model: txt-DiT-XL) does not offer spatial guidance, and is shown for reference only. Method TRELLIS Chair CD CLIP-I FID P-FID CD CLIP-I FID P-FID CD CLIP-I FID P-FID Toys4K Table 117 0.33 217 78.60 14.7 0.31 40.82 19.7 0.30 132 49.40 Geometric Primitives 0.21 Coin3D Spice-E 0.29 SPICE-E-T 0.32 SPACECONTROL (Ours) 14.0 0. 54.4 65.9 39.1 102.0 18.5 231 233 66.52 7.66 223 53.51 5.92 81.3 0.98 221 Meshes 77.8 Coin3D 7.40 Spice-E (stylization) SPICE-E-T 23.3 SPACECONTROL (Ours) 4.89 0.04 0.30 0.32 0.29 182.5 14.6 293 224 81.21 6.37 222 90.99 22.7 244 72.47 0.66 0.25 0.29 0.31 0. 0.01 0.30 0.31 0.29 47.54 28.82 218 10.3 166 38.66 135 39.22 4.73 146 34.06 3.72 20.4 111.0 308 28.2 152 41.51 132 39.70 7.59 137 30.96 0.48 0.22 0.29 0.30 0.29 0.01 0.29 0.30 0.28 71.58 245 148 78.85 122 47.36 157 46. 178.2 224 132 58.01 116 46.76 130 42.33 Baselines. We compare SPACECONTROL to state-of-the-art baselines for the task of 3Dconditioned object generation. We compare to Spice-E (Sella et al., 2024), which fine-tunes ShapE (Jun & Nichol, 2023) to support cuboid primitives as spatial guidance for 3D object generation. Since Spice-E is based on the Shap-E model (Jun & Nichol, 2023), to allow fairer comparison we implement its correspondent for Trellis (Xiang et al., 2025), which we will refer to as SPICE-E-T. We provide more details on its implementation and training in the Appendix. Note that Spice-E provides separate checkpoint for shape stylization, which is used to evaluate the method on mesh conditioning, as it lead to better results. In addition, we also compare to Coin3D (Dong et al., 2024), which uses the shape-guidance to first generate single view of the desired 3D asset, then leverage Multi-View-Diffusion model to generate consistent multiple views, and finally extract the 3D representation using volumetric-based score distillation sampling. Datasets. To evaluate how different approaches handle geometric conditioning, we create dataset of objects which contains the original mesh, decomposition of it into geometric primitives, and textual description of the asset. We use the mesh to evaluate methods on mesh-conditioned generation and geometric primitives to evaluate on shape-conditioned generation. Moreover, to evaluate both generation and generalization capabilities, we use objects of two ShapeNet (Chang et al., 2015) categories (chairs and tables) that Spice-E was explicitly trained on together with objects from the Toys4K (Stojanov et al., 2021) dataset, unseen by all methods during training. We use SuperDec (Fedele et al., 2025) to obtain the decomposition of the 3D assets into superquadrics and Gemini on rendered views to obtain textual description of the assets from ShapeNet (Chang et al., 2015). For objects from Toys4k we use the textual description from Xiang et al. (2025). Metrics. Our experiments aim to evaluate both the faithfulness to the spatial and textual control and the realism of the generated assets. Faithfulness to the spatial control is quantified using the L2 Chamfer Distance (CD) between vertices sampled from the input superquadric primitives and the generated mesh decoded by DM . Faithfulness to the textual control is quantified with the CLIP similarity (CLIP-I) between the renderings of generated assets and the textual prompts. Realism is evaluated for texture via the Frechet Inception Distance (FID) (Heusel et al., 2017) on image renderings and for geometry, via the P-FID (Nichol et al., 2022), the point cloud analog for FID. To measure the FID on image rendering we measure the distance between the inception features extracted from the original image renderings of the datasets and the generated ones. To measure the P-FID of the generated meshes we measure the distance between the PointNet++ (Qi et al., 2017) features of the generated and original object meshes. Results. Quantitative results are reported in Table 1, while qualitative results are shown in Figure 4. Both Spice-E and SPICE-E-T perform well on chairs and tables but struggle to faithfully generate objects that they were not fine-tuned on (Toys4K). SPACECONTROL significantly outperforms the baselines in all experiments in terms of Chamfer Distance (CD) to the spatial control, while achieving comparable CLIP-I, FID, and P-FID scores. For completeness, we also report scores for the text-conditioned Trellis using the DiT-XL backbone, which is also the base model used in our 7 Table 2: Analysis of τ0. The evaluation metrics are L2 Chamfer Distance (CD) and Frechet Inception Distance (FID). CD quantifies alignment with spatial control, while FID assesses realism. CD scores are scaled by 103. We show scores for spatial control given as geometric primitives (P) and meshes (M). CD CLIP-I FID P-FID CD CLIP-I FID P-FID CD CLIP-I FID P-FID Toys4K Chair Table P M M τ0 0 117 75.4 0.33 0.29 217 254.9 78.6 79.4 14.7 30.6 0.31 0.29 129 133.7 40.8 39.9 19.7 49.21 0.30 0.28 132 137.5 49.40 49.3 2 110 65.5 0.33 0.29 216 256.9 79.1 82.7 14.1 30.0 0.31 0.29 131 136.7 41.2 41.5 18.5 43.51 0.30 0.28 132 134.7 51.97 41.5 4 56.8 32.4 0.32 0.29 222 252.8 84.1 83.9 7.3 13.9 0.31 0.29 137 141.1 34.1 31.9 6.33 2.68 0.30 0.28 135 133.5 51.79 45.8 6 14.0 4.89 0.32 0.29 221 244.9 81.3 72.5 0.98 0.66 0.30 0.29 146 136.6 34.0 31.0 3.72 0.48 0.29 0.28 157 131.0 46.28 42.3 8 9.04 1.57 0.29 0.29 257 241.3 94.0 77.0 0.27 0.28 0.30 0.28 156 134.3 37.1 29.2 3.29 0.19 0.29 0.28 175 127.3 50.16 43.2 10 8.85 1.84 0.27 0.29 268 209.3 101 74.9 0.22 0.26 0.30 0.28 160 134.0 36.5 30.1 3.26 0.19 0.29 0.29 181 125.9 50.74 42.6 P P M SPACECONTROL. Note that for the sake of simplicity in Tab. 1 we only report results of SPACECONTROL with τ0 = 6. However, τ0 can be chosen freely by the user, depending on the desired strength of conditioning. For completeness, we report results for different values of τ0 in Tab. 2. We can see that by increasing the value of τ0 and thus strength of the spatial conditioning, we obtain generations which align more closely to the input spatial control. User Study. To validate the numerical results, we conduct user study  (Fig. 5)  involving 52 volunteers, each one evaluating on average 20 randomly selected samples. Participants were asked to compare pairs of generated objects, voting which one was more faithful to the input control shape, which model looked more realistic, and which one they liked overall better (see appendix for more details). The study is performed on the same datasets discussed above, i.e.on ShapeNet (Chang et al., 2015) and Toys4k (Stojanov et al., 2021). We compare our SPACECONTROL to the Spice-E and Spice-ET baselines. We observe that our SPACECONTROL is always the preferred method both in terms of overall appearance and alignment to the input spatial control. 5.2 QUALITATIVE RESULTS Overall Faithful Realism 5% 10% 85% 23% 34% 42% 8% 5% 87% 23% 8% 13% 15% 69% 72% 47% 21% 32% Wins Ties Losses Spice-E Spice-E-T Spice-E Spice-E-T Spice-E Spice-E-T Figure 5: User Study Results. The bar plots present the proportion of favorable comparisons the achieved by our SPACECONTROL against baselines on overall appearance, faithfulness to spatial control, and realism, respectively. Besides Figures 1, 4, and 6, we provide additional qualitative results for object editing in the Appendix, visualizing outputs of different methods conditioned on both coarse and detailed input controls. In general, training-based methods struggle to generate objects in specific poses, whereas SPACECONTROL consistently produces plausible results. For example, other methods generate cow with two heads (Spice-E and Spice-E-T), an elephant with an eye on its back (Spice-E), or shapes that fail to strictly follow the spatial conditioning or exhibit low quality (Coin3D). 5.3 ANALYSIS EXPERIMENTS The Effect of the Control Parameter τ0. While existing methods for 3D spatial conditioning do not provide way to control its strength, our SPACECONTROL enables flexible interpolation between different levels of adherence. In this section, we evaluate how the parameter τ0 governs the trade-off between fidelity to the spatial control signal and the realism of the generated asset. Quantitative results are reported in Table 2, using the same metrics and datasets as in Table 1. We further present qualitative results in Fig. 3 and in the Appendix, showing how varying the conditioning strength produces different outcomes. Adjusting τ0 allows users to regulate this trade-off according to their preferences, balancing higher shape quality against stronger adherence to the spatial guidance. Additionally, the plot in Figure 3 (left) illustrates this trade-off on Toys4K, indicating that τ0 [4, 6] generally provides good compromise between spatial adherence and shape quality. 8 Trellis Coin3D Spice-E SPICE-E-T Oursτ0= floral chair floral chair n/a t h M a v (a) Image Conditioning. Given the two different spatial controls shown in thefirst row, we show objects generated by our SPACECONTROL without (second row) and with (third row) image conditioning. (b) Spatial alignment. We show how different methods align the generated 3D asset with the input condition. In the first row we show the input control, in the second the generated asset and in the third, we overlay the two. All the generations use the same prompt wooden chair.. Figure 6: Image conditioning and fine-grained alignment. We show analysis experiments on the role of image conditioning (left) and on fine-grained spatial alignment (right). The Role of Image Conditioning. SPACECONTROL supports multi-modal control for 3D asset generation by combining spatial guidance via superquadrics with natural language and optional image conditioning. While the model can synthesize assets using only superquadrics and textual prompts, images are particularly useful for maintaining visual consistency during object edits, as shown in Figure 6a and in the Appendix. As we only use image prompts in the Appearance Flow Model of Trellis, they primarily affect texture, with only minor influence on geometry. While this capability originates from the pre-trained Trellis, SPACECONTROL enables its practical use for crossmodal texture transfer, effectively performing style transfer from 2D images to generated 3D shapes. Spatial Alignment. We believe that key advantage of training-free approach that performs conditioning directly in 3D space is its ability to achieve fine-grained spatial control. In this section, we provide an example where the conditioning shapes are not aligned with axis-oriented rotations. As shown in Fig. 6b, our method is the only one that perfectly aligns with the input conditioning while preserving the quality of the generated mesh. Additional results are provided in the Appendix."
        },
        {
            "title": "6 DISCUSSION AND CONCLUSION",
            "content": "In summary, our approach introduces the first training-free method that by operating directly in the 3D space is able to spatially condition the generation of high quality assets. Through extensive evaluations and practical interface, we demonstrate both the effectiveness and usability of our method in real-world creative workflows. Limitations and Future Work. While SPACECONTROL enables flexible spatial control via tunable adherence parameter τ0, this parameter is currently selected manually. Although this supports user-driven control over the realismfaithfulness tradeoff, it complicates automated generation of diverse, high-quality assets without per-instance tuning. Additionally, our current formulation enforces uniform adherence level across the entire object. Future work could explore part-aware control, allowing users to specify which regions should closely follow the input structure and which can deviate more freely to support creative variation. Reproducibility Statement. Our approach builds on the open-source Trellis model (Xiang et al., 2025), and our experiments use open-source datasets, namely ShapeNet (Chang et al., 2015) and Toys4k (Stojanov et al., 2021). All experiments are fully reproducible, and upon acceptance we will release the code to support replication of our method and results. 9 Acknowledgments. Elisabetta Fedele acknowledges support from the ETH AI Center doctoral fellowship, the Swiss National Science Foundation (SNSF) Advanced Grant 216260 (Beyond Frozen Worlds: Capturing Functional 3D Digital Twins from the Real World), and an SNSF Mobility Grant. Francis Engelmann acknowledges support from an SNSF PostDoc mobility fellowship. Or Litany acknowledges support from the Israel Science Foundation (grant 624/25) and the Azrieli Foundation Early Career Faculty Fellowship. This research was also supported in part by an academic gift from Meta. The authors gratefully acknowledge this support."
        },
        {
            "title": "REFERENCES",
            "content": "Amir Barda, Matheus Gadelha, Vladimir Kim, Noam Aigerman, Amit Bermano, and Thibault Groueix. Instant3dit: Multiview inpainting for fast editing of 3d objects. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 3 Alan H. Barr. Superquadrics and Angle-Preserving Transformations. IEEE Computer Graphics and Applications, 1981. 5 Shariq Farooq Bhat, Niloy Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024. 3 Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. ShapeNet: An Information-rich 3D Model Repository. arXiv preprint arXiv:1512.03012, 2015. 3, 7, 8, Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. In International Conference on Computer Vision (ICCV), 2023. 2, 3 Xingyu Chen, Fu-Jen Chu, Pierre Gleize, Kevin Liang, Alexander Sax, Hao Tang, Weiyao Wang, Michelle Guo, Thibaut Hardin, Xiang Li, Aohan Lin, Jiawei Liu, Ziqi Ma, Anushka Sagar, Bowen Song, Xiaodong Wang, Jianing Yang, Bowen Zhang, Piotr Dollar, Georgia Gkioxari, Matt Feiszli, and Jitendra Malik. SAM 3D: 3Dfy Anything in Images. arXiv preprint arXiv:2511.16624, 2025a. 2, 4 Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3DTopia-XL: Scaling High-quality 3D Asset GenIn International Conference on Computer Vision and Pattern eration via Primitive Diffusion. Recognition (CVPR), 2025b. 2 Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. ABO: Dataset and Benchmarks for Real-world 3D Object Understanding. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 18 Jaime Corsetti, Francesco Giuliari, Davide Boscaini, Pedro Hermosilla, Andrea Pilzer, Guofeng Mei, Alexandros Delitzas, Francis Engelmann, and Fabio Poiesi. Language-guided 3d scene synthesis for fine-grained functionality understanding. arXiv preprint arXiv:2511.23230, 2025. 2 Wenqi Dong, Bangbang Yang, Lin Ma, Xiao Liu, Liyuan Cui, Hujun Bao, Yuewen Ma, and Zhaopeng Cui. Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning. In ACM SIGGRAPH 2024 Conference Papers, 2024. 2, Elisabetta Fedele, Boyang Sun, Leonidas Guibas, Marc Pollefeys, and Francis Engelmann. SuperDec: 3D Scene Decomposition with Superquadric Primitives. In International Conference on Computer Vision (ICCV), 2025. 7, 18 Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. GET3D: Generative Model of High Quality 3D Textured Shapes Learned from Images. In International Conference on Neural Information Processing Systems (NeurIPS), 2022. 2 10 Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. Tokenverse: Versatile multi-concept personalization in token modulation space. ACM Transactions On Graphics (TOG), 2025. 3 Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv, 2022. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by Two Time-scale Update Rule Converge to Local Nash Equilibrium. In International Conference on Neural Information Processing Systems (NeurIPS), 2017. 7 Rui Huang, Guangyao Zhai, Zuria Bauer, Marc Pollefeys, Federico Tombari, Leonidas Guibas, Gao Huang, and Francis Engelmann. Video Perception Models for 3D Scene Synthesis. International Conference on Neural Information Processing Systems (NeurIPS), 2025. 2 Heewoo Jun and Alex Nichol. Shap-E: Generating Conditional 3D Implicit Functions. arXiv preprint arXiv:2305.02463, 2023. 3, 7 Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In International Conference on Learning Representations (ICLR), 2022. 3, 5 Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-NeRF for Shape-guided Generation of 3D Shapes and Textures. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2Mesh: Text-driven In International Conference on Computer Vision and Pattern Neural Stylization for Meshes. Recognition (CVPR), 2022. 2 Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-E: System for Generating 3D Point Clouds from Complex Prompts. arXiv preprint arXiv:2212.08751, 2022. 2, 7 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning Robust Visual Features without Supervision. Transactions on Machine Learning Research Journal, 2024. 4 William Peebles and Saining Xie. Scalable Diffusion Models With Transformers. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. In International Conference on Neural Information Processing Systems (NeurIPS), 2017. 7 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models from Natural Language Supervision. In International Conference on Machine Learning (ICML), 2021. 4 Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-Shot Text-to-image Generation. In International Conference on Machine Learning (ICML), 2021. 2 Rahul Sajnani, Jeroen Vanbaar, Jie Min, Kapil Katyal, and Srinath Sridhar. Geodiffuser: Geometrybased image editing with diffusion models. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. 3 Etai Sella, Gal Fiebelman, Noam Atia, and Hadar Averbuch-Elor. Spic-E: Structural Priors in 3D Diffusion Models using Cross Entity Attention. ACM SIGGRAPH Conference Papers, 2024. 2, 3, 7 Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, et al. Meta 3D AssetGen: Text-to-mesh Generation with High-quality Geometry, Texture, and BPR Materials. In International Conference on Neural Information Processing Systems (NeurIPS), 2024. 2 Stefan Stojanov, Anh Thai, and James Rehg. Using Shape to Categorize: Low-shot Learning with an Explicit Shape Bias. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 7, 8, 9 Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis, et al. LION: Latent Point Diffusion Models for 3D Shape Generation. In International Conference on Neural Information Processing Systems (NeurIPS), 2022. 2, 3 Dimitri Von Rutte, Elisabetta Fedele, Jonathan Thomm, and Lukas Wolf. Fabric: Personalizing diffusion models with iterative feedback. In European Conference on Computer Vision (ECCV), 2024. 3 Zhenwei Wang, Tengfei Wang, Zexin He, Gerhard Petrus Hancke, Ziwei Liu, and Rynson WH Lau. Phidias: generative model for creating 3d content from text, image, and 3d conditions with reference-augmented diffusion. In International Conference on Learning Representations (ICLR), 2025. 3 Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, and Tat-Jen Cham. Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images. arXiv preprint arXiv:2503.13439, 2025. Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3D Latents for Scalable and Versatile 3D Generation. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 3, 4, 7, 9 Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. CLAY: Controllable Large-scale Generative Model for Creating Highquality 3D Assets. ACM Transactions on Graphics (TOG), 2024. 2, 3 Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding Conditional Control to Text-to-Image In International Conference on Computer Vision and Pattern Recognition Diffusion Models. (CVPR), 2023. 3 Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation. arXiv preprint arXiv:2501.12202, 2025."
        },
        {
            "title": "A ADDITIONAL RESULTS",
            "content": "A.1 FINE-GRAINED SPATIAL EDITING In this section we provide additional results which show how the generations from our SPACECONTROL are influenced by the change of the spatial control. We show results in pairs where the textual and/or image prompts are kept fixed. We notice that by providing additional image control, we are able to preserve the texture between different generations. t h y m G m l n s r o p P Birthday cake Daisy flower Satellite House Knife Clock Figure 7: Fine-grained spatial editing with superquadrics. Superquadrics offer fine-grained spatial control that is useful not only for generating wide variety of 3D assets, but also for editing them. They enable intuitive and localized modifications of 3D shapes, in more direct manner than textor image-only generative models in practicality. In addition to natural language prompts (top), SPACECONTROL supports image conditioned generation (bottom), enabling consistent visual appearance across edits. 13 A.2 COARSE AND FINE-GRAINED SPATIAL CONTROL WITH SUPERQUADRICS In this section, we provide additional results generated with different control strengths. Here the hyperparameter is chosen so that we were satisfied with the final result. Superquadrics prove to be an effective tool to provide both coarse and fine-grained control to the 3D generation. By combining the expressivity of superquadrics with the flexible control strength offered by our SPACECONTROL, users can condition the generation by either carefully designing geometric details or only drafting the spatial setting of the desired output. τ0 = 4 τ0 = τ0 = 5 τ0 = 6 τ0 = 4 t h y m G m l n s r o p P An airplane motorbike Staircase snowman duck τ0 = τ0 = 5 τ0 = 5 τ0 = 5 τ0 = 3 Greek column guitar Military helicopter Fighter jet Drumkit Figure 8: Coarse and fine-grained control with superquadrics. Superquadrics offer both finegrained spatial control when used to sculpt precise geometry (motorbike, staircase, helicopter) and coarse control, when only used to draft 3D sketch (duck, drumkit). 14 A.3 FINE-GRAINED ALIGNMENT WITH STATE-OF-THE-ART METHODS In Fig. 9 we show the results for the same experiment provided in the main paper, but with different control strengths. τ0 = 1 τ0 = 3 τ0 = 5 τ0 = 7 τ0 = τ0 = 11 t h d r Figure 9: Fine-grained alignment of SPACECONTROL with different τ0. In the first row we show the input control, in the second the generated asset and in the third, we overlay the two, to better visualize alignment. All the generations use the same spatial control and the same prompt wooden chair.. Furthermore, in Fig. 10 we show practical application when fine-grained spatial control can be particularly useful. With our method, user can provide sketch of the geometric primitives composing the scene and directly condition the generation on this input, without requiring any time-consuming post-processing to align the generated shapes. Input spatial control Output 3D assets Figure 10: SPACECONTROL for 3D scene generation. We show how SPACECONTROL can be used to generate objects of full scenes starting from coarse conditioning. On the left we show the superquadrics for the scene, where each object is represented with different color. On the right we show the assets generated with SPACECONTROL using the geometric primitives from the right as spatial condition. Note that each object is generated independently, by scaling the superquadrics to unit cube and giving them as spatial control to SPACECONTROL. Generated objects are then automatically placed, by undoing the transformation. 15 A.4 LOCAL CONTROL Explicit 3D geometric conditioning also paves the way for local semantic conditioning. While full exploration goes beyond the scope of this work, we implemented baseline approach which demonstrates that part-level semantic control can be achieved without any conceptual modification, providing solid foundation for future work. In our setting, shape conditioning already determines the geometry of the generated object . Therefore, we are interested in using local semantic conditioning to control the visual appearance of the final generation. We consider the case where the input geometric conditioning is given as superquadrics and the user defines both global semantic prompt which defines the global semantic of the object and some local semantic prompts, which attached to each superquadric and define their respective semantics. We use the geometry of the conditioning superquadrics together with the global prompt to generate the geometry of the object in the Structure FM. Once obtained the geometry of the object, the local semantic prompts are then used to condition the visual appearance generation, in the Appearance FM. In order to combine the local and the global conditioning, we modify the cross-attention layer of the Appearance FM DiTs so that instead of only cross-attending to global prompt, points also cross-attend to the corresponding local prompts. In practice, we first cross-attend separately to each conditioning prompt, and the feature of point is updated with the linear combination of the cross-attention performed with global (cglobal,i) and local (clocal,i) conditioning: zi 0.5 CA(z, cglobal,i) + 0.5 CA(z, clocal,i) , where clocal,i is the local semantic conditioning of the nearest superquadric to point i. qualitative result of this approach in Fig. 11, where we show how this approach can be used to generate white chair with red seat. Without local semantic conditioning. With local semantic conditioning. Figure 11: Local semantic control. From left to right we show: the input geometric control, the 3D asset generated by globally conditioning on white chair., the 3D asset generated by conditioning globally on white chair. and locally (on the superquadric highlighted in red) on read seat.."
        },
        {
            "title": "B INTERACTIVE USER INTERFACE",
            "content": "In Fig. 12 we visualize our interactive user interface. Starting from scratch or from template of superquadrics, users can freely edit superquadrics using their parameters, and add/delete them. Once given the conditioning, they can select control strength (higher control strength means that the generated shape looks more like the primitives) and text (and optionally image) conditioning. They can then toggle between the input primitives and meshes and proceed with new generations. We provide demo of the user interface in the supplementary video. Figure 12: Visualization of our interactive user interface. Users can control the generated geometry by changing the shape of the geometric primitives and deciding the strength of the conditioning. Other than spatial control, users can use text and, optionally, images."
        },
        {
            "title": "C USER STUDY",
            "content": "In Fig. 13, we show the web interface of our user study. From left to right, we show the given control shape, and two competing methods. The participants then choose which generated object is more faithful to the input control shape, which model looks more realistic, and which one they like best. Figure 13: User study interface. 17 SPICE-E-T We obtain our training-based baseline SPICE-E-T by adding an additional conditioning layer to the flow transformer blocks in the structure generator of textconditioned Trellis model (see Fig. 14) which perform cross attention on the shape conditioning. We encode the shape conditioning using the Trellis encoder E, and we perform the Cross-Attention in that feature space. We initialize the original layers with the weights from the text-conditioned Trellis and the newly added ones randomly. We then train the modified Structure Generator for 120.000 iterations with batch size of 4 on the ABO dataset (Collins et al., 2022), where the shape conditioning are obtained by running SuperDec (Fedele et al., 2025). During training, we use the same reconstruction loss of the original Trellis model. Figure 14: Comparison between the Flow Transformer from the original Trellis (left) and the one from SPICE-E-T (right), adapted to enable spatial control via superquadrics."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "NVIDIA",
        "Stanford University",
        "Technion"
    ]
}