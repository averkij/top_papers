{
    "paper_title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning",
    "authors": [
        "Jiaer Xia",
        "Yuhang Zang",
        "Peng Gao",
        "Yixuan Li",
        "Kaiyang Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning general-purpose reasoning capabilities has long been a challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to a VLM -- by prompting the model to produce a reasoning chain before providing an answer -- can lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to a caption-reason-answer output format: initially generating a detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks."
        },
        {
            "title": "Start",
            "content": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning Jiaer Xia1 Yuhang Zang2 Peng Gao2 Yixuan Li3 Kaiyang Zhou1(cid:66) 1 Hong Kong Baptist University 2 Shanghai AI Lab 3 University of Wisconsin-Madison 5 2 0 2 0 2 ] . [ 1 7 7 6 4 1 . 5 0 5 2 : r https://github.com/maifoundations/Visionary-R"
        },
        {
            "title": "Abstract",
            "content": "Learning general-purpose reasoning capabilities has long been challenging problem in AI. Recent research in large language models (LLMs), such as DeepSeek-R1, has shown that reinforcement learning techniques like GRPO can enable pre-trained LLMs to develop reasoning capabilities using simple question-answer pairs. In this paper, we aim to train visual language models (VLMs) to perform reasoning on image data through reinforcement learning and visual question-answer pairs, without any explicit chain-of-thought (CoT) supervision. Our findings indicate that simply applying reinforcement learning to VLMby prompting the model to produce reasoning chain before providing an answercan lead the model to develop shortcuts from easy questions, thereby reducing its ability to generalize across unseen data distributions. We argue that the key to mitigating shortcut learning is to encourage the model to interpret images prior to reasoning. Therefore, we train the model to adhere to caption-reason-answer output format: initially generating detailed caption for an image, followed by constructing an extensive reasoning chain. When trained on 273K CoT-free visual question-answer pairs and using only reinforcement learning, our model, named Visionary-R1, outperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on multiple visual reasoning benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Reasoning is essential for enabling AI to tackle complex problems and make informed decisions in real-world applications. However, training AI models to reason is extremely challengingprimarily due to the lack of large-scale human-annotated reasoning data [1, 2, 3]. Recent advances in large language models (LLMs), such as DeepSeek-R1 [4], have demonstrated the potential to induce reasoning capabilities in LLMs via reinforcement learning and using only question-answer pairs, without explicit step-by-step supervision. Meanwhile, the computer vision community has begun exploring RL approaches for visual language models (VLMs), using methods like GRPO [5] to extend reasoning to multimodal settings [6, 7, 8, 9]. While these efforts are promising, existing visual reasoning models often rely on complex multi-stage training pipelines that are both computationally expensive and time-consuming. Moreover, these models heavily rely on labeled chain-of-thought reasoning data distilled from proprietary models like GPT-4olimiting scalability and openness. In this paper, we aim to lower the development cost of training VLMs for visual reasoning by using only reinforcement learning and paired visual question-answer data, without relying on any chain-of-thought supervision. Inspired by DeepSeek-R1, we adapt GRPO to training VLMs using only question-answer pairs. Specifically, given an image and question, we prompt VLM to (cid:66)Corresponding author Preprint. Under review. Figure 1: Comparison between the GRPO model and Visionary-R1. Using the reason-answer output format, the GRPO model tends to generate shortcut responses for easy samples during training, which hinders the model from learning general-purpose reasoning capabilities and results in poor generalization performance. In contrast, with more comprehensive understanding of the image context, i.e., using the caption-reason-answer output format, Visionary-R1 consistently generates long, meaningful reasoning chains for both easy and hard samples. generate reasoning chain followed by an answer and optimize the model using combination of an accuracy reward (that evaluates the answer correctness) and format reward (that encourages the reason-answer output format). However, this seemingly straightforward setup leads to critical failure mode: the model develops shortcuts by producing short, uninformative reasoning chains. These shortcuts often suffice to answer easy training questions correctly, but the model fails to generalize to harder questions that require genuine visual understanding. As illustrated in Fig. 1, the model trained with GRPO performs well on simple training examples by exploiting shortcuts (top), but at test time, it produces incoherent reasoning and incorrect answers on unseen examples (bottom). To address the shortcut issue, we propose Visionary-R1, reinforcement learning framework that enforces visual understanding before reasoning. The key idea is to train the model in structured captionreasonanswer format, where it must first generate detailed caption of the image before reasoning and answering. The captioning step ensures that the model does not just rely on superficial cues or patterns but engages in deeper analysis of the image context, regardless of whether the question is easy or hardthis forces the model to adopt consistent problemsolving approach, thus mitigating potential shortcuts and consequently making the reasoning capabilities more generalizable across different data distributions. To ensure the caption is informative, we impose auxiliary supervision on the caption tokens by using reinforcement learning from 2 Figure 2: The longer the reasoning chain, the better the accuracy. AI feedback [10]. This caption reward is combined with standard accuracy and format rewards during policy optimization. The resulting model produces longer, more meaningful reasoning tokens than the model learned with GRPO alone (see Fig. 1), leading to better generalization performance on unseen data (see Fig. 2). To evaluate our approach, we compile comprehensive dataset that aggregates 11 popular questionanswer datasets, covering areas such as scene understanding, chart analysis, mathematical problemsolving, and document processing. In total, the training data consists of 272.6K CoT-free questionanswer pairs. After training, Visionary-R1 is evaluated on several challenging visual reasoning benchmarks including MathVista [11], MathVision [12], MMStar [13], and MMBench [14]. The results show that Visionary-R1 outperforms strong proprietary models, such as GPT-4o, Claude3.5Sonnet, and Gemini-1.5-Pro, as well as the latest competitors based on supervised pre-training and reinforcement fine-tuning. In summary, we make the following contributions in this paper: 1) We share an important finding that GRPO does not work directly with VLMs due to shortcut learning; 2) We address the shortcut learning problem with Visionary-R1, simple reinforcement learning-based model that interprets images before reasoning; 3) Through extensive experiments, we show that despite using only questionanswer pairs, Visionary-R1 beats strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on challenging visual reasoning benchmarks. Code and models will be publicly released to facilitate future research."
        },
        {
            "title": "2 Related Work",
            "content": "Supervised Learning for Visual Reasoning Learning LLMs/VLMs that can reason have gained increasing attention from both academia and industry due to their ability to generate human-like, step-by-step reasoning, which is advantageous for tackling complex problems and delivering more interpretable answers [15, 16]. Supervised fine-tuning (SFT) is the most straightforward method to enhance models reasoning capabilities, which relies on labeled data containing thinking processes. Since collecting human annotations is costly, existing work often resorts to using pre-trained model like OpenAIs GPT-4o to generate reasoning labels. For instance, LLaVA-CoT [17] utilizes GPT-4o to label 100K visual question-answer datasets with detailed chain-of-thought including summary, caption, and reasoning. However, the process of collecting CoT labels can be quite expensive, and the use of GPT-4o limits scalability while introducing significant performance upper bound. Similarly, MMCR [18] also creates 310k multi-turn reasoning dataset using GPT-4o. CoMCTS [19] introduces the Mulberry-260k dataset, which is specifically crafted to train tree-structure reasoning models. Compared to these models, our Visionary-R1 only uses simple question-answer pairs for training without any chain-of-thought supervision, yet it achieves stronger reasoning performance. Reinforcement Learning for Visual Reasoning Compared to SFT, reinforcement learning (RL) has recently been proved more effective in developing general-purpose reasoning capabilities as this paradigm has the potential to enable the model to explore reasoning in broader language space and develop its own thinking processes [20]. Insight-V [21] presents multi-agent system to select preference data from self-generated reasoning paths and optimizes the model based on preference learning algorithm. R1-VL [22] designs step-wise rewards to improve reasoning accuracy and validity but relies on labeled data for SFT. RL has also been applied in Vision-R1 [23] and R1-Onevision [24], but only 10K samples are used in these models for RL training while the main focus is on SFT (that uses more than 200K samples). Our Visionary-R1 departs from the popular SFT-followed-by-RL pipeline and adopts pure RL approach."
        },
        {
            "title": "3 Methodology",
            "content": "We propose Visionary-R1, reinforcement learning framework designed to improve the reasoning capabilities of VLMs, which can be trained using only visual question-answer pairs without any explicit CoT supervision. In what follows, we first highlight the shortcut issue that arises when applying RL to visual reasoning tasks (Section 3.1), then introduce our Visionary-R1 framework, which train the model to follow the caption-reason-answer output format, i.e., first generating an informative caption to understand the image context, followed by an extensive reasoning chain. 3 Figure 3: Overview of Visionary-R1. The primary training pipeline utilizes the GRPO method, which generates multiple reasoning paths for each question-answer pair. Additionally, an info tag is incorporated when calculating the format reward, and the policy models LLM part is used to answer questions based on the description between the info tags, serving as the caption rewards. All rewards are then aggregated to determine the final advantage of each path. 3.1 Motivation: The Shortcut Phenomenon in Visual Reasoning While the GRPO [5] algorithm has been shown effective in improving the reasoning capabilities of language models, we observe critical failure mode when transferring to visual reasoning tasks. This phenomenon manifests as shortcutGRPO often leads to degenerate behaviors where the model ignores the visual input and relies primarily on textual patterns from the question to generate an answer. As shown in Fig. 1, the model trained with GRPO can produce correct answers for simple questions during trainingyet this is achieved without grounding in the image. This shortcut behavior can be particularly problematic in visual reasoning tasks, where the correct answer often depends on subtle image features such as embedded text, numerical values, object relationships, or chart patterns. Without forcing the model to attend to these visual signals, reinforcement learning alone encourages reward hacking: the model learns to exploit training distribution artifacts instead of learning generalpurpose reasoning. To address this challenge, we propose simple but effective modification: force the model to explicitly interpret the image before it begins reasoning. We operationalize this through caption reward design (Section 3.2), which is then explicitly incorporated into the RL training objective (Section 3.3). 3.2 Visionary-R1: Grounding Reasoning via Captioning Caption-Reason-Answer Output Format We train the model to first generate captions before reasoning. This is operationalized via the caption-reason-answer output format: 4 1. Caption: generate detailed description of the image, capturing objects, numbers, text, spatial relations, and other salient visual features; 2. Reason: construct reasoning chain based on the captioned content; 3. Answer: provide the final answer to the question. Specifically, we prompt the model to generate detailed description, which is wrapped using <info></info> tag. The final format we request the model to follow is therefore <info>...</info> <think>...</think> <answer>...</answer> The output is evaluated using binary format reward rf {0, 1}, which checks whether the generated response adheres to this format. Caption Reward While the format enforces structure, it does not guarantee that the caption is sufficiently detailed to support reasoning. To address this issue, we introduce specialized caption reward rc {0, 1} based on reinforcement learning from AI feedback [10]. Specifically, we feed the generated caption into an LLM, and ask it to answer the question based solely on the caption. In implementation, we use the LLM component of the policy model. If the answer is correct, the caption is deemed informative and rewarded; otherwise, it is penalized. This encourages the model to produce useful, visually grounded descriptions. The final reward for sampled sequence is computed as: where ra is the accuracy reward and α is balancing weight controlling the contribution of the caption reward. Ri = ra + rf + αrc, (1) 3.3 Training Objective with Caption Reward Group Relative Policy Optimization, known as GRPO, was originally developed in DeepSeekMath [5] for text-only reasoning tasks, and later adopted in DeepSeek-R1 [4]. GRPO simplifies the reinforcement learning paradigm by getting rid of the critic model. This is done by generating group of responses for each sample and then computing the normalized reward within the group to determine an advantage value. To adapt it to visual reasoning, our method introduces two key differences. (1) First, as described in Section 3.2, we design new reward structure by adding caption reward that explicitly evaluates whether the model has interpreted the visual input, addressing the shortcut issue. (2) Second, we introduce cosine-annealed KL penalty to stabilize training and encourage longer, more meaningful outputsavoiding the limitations of static KL coefficient in multimodal settings. We now detail our training objective and implementation. For each training sample (i.e., question-image pair), we sample response Policy Optimization sequences {o1, o2, ..., on} from an old policy model πθold. Each output is scored using the combined reward Ri from Eq. 1. Then, an advantage value based on the rewards, = {R1, R2, ..., Rn}, is computed as Ai = Ri mean(R) std(R) , = 1, , n. (2) The updated policy πθ is trained using clipped surrogate objective (θ) = E[q (Q), {oi}n (cid:88) (cid:18) i=1 πθold(Oq)] 1 i=1 min (cid:18) πθ(oiq) πθold(oiq) Ai, clip (cid:18) πθ(oiq) πθold(oiq) (cid:19) (cid:19) (cid:19) , 1 ε, 1 + ε Ai βDKL (πθπref) , (3) where both ε and β are hyper-parameters. ε controls the clipping bound and limits the range of policy updates to avoid large changes that could destabilize training. β is the KL penalty coefficient that regularizes deviation from reference policy πref. 5 Cosine Annealing KL Coefficient The KL penalty is formulated as DKL [πθπref ] = πref (oi q) πθ (oi q) log πref (oi q) πθ (oi q) 1. (4) The KL divergence in Eq. 4 serves as penalty term to prevent the model from straying too far from the baseline policy model, thereby stabilizing the training. It is non-trivial to determine the balancing weight for this term: using large weight forces the model to stay within close neighborhood of the baseline model and therefore impedes the models ability to engage in more in-depth thinking and generating long, detailed reasoning; on the other hand, using small weight can lead to unstable training and potentially result in reward hacking [25]. To overcome this challenge, we propose dynamically annealing the KL penalty coefficient over time using cosine annealing, which uses large coefficient during the early, unstable training phase and gradually reduces the value to allow the model to produce longer outputs in later stages. Specifically, we replace β in Eq. 3 with ˆβ, which is calculated as ˆβ = β 2 (cid:18) (cid:18) 1 + cos π (cid:19)(cid:19)"
        },
        {
            "title": "Tcur\nTmax",
            "content": ", (5) where Tcur and Tmax represent the current and max training steps, respectively."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Training Data Unlike existing work that relies on curated data and reasoning labels, our approach allows the model to learn using CoT-free visual question-answer pairs. To ensure diversity, we aggregate 11 popular visual question-answer datasets by simply combining the training data without applying any preprocessing or filtering. The resulting training data consists of 272.6K visual questionanswer pairs and covers wide spectrum of visual formats, including general scenes, charts, tables, diagrams, math questions, documents, and 3D data. See Tab. 4 for details about the data composition. Benchmarks We evaluate our approach on four widely-used visual reasoning benchmarks that cover various visual formats and question types: MathVista (Testmini) [11], MathVision [12], MMStar [13], and MMBench (en) [14]. MathVista encompasses variety of reasoning types, including logical, algebraic, and scientific reasoning questions. MathVision focuses on mathematical visual reasoning tasks. MMstar aims to evaluate perception, math understanding, scientific and technology-related questions, and logical reasoning. MMBench is comprehensive evaluation suite concerned with visual and mathematical reasoning. Baseline Methods To justify the effectiveness of our designs, we implement two baselines: 1) SFT. The model is directly trained with the original question-answer data. 2) GRPO. The model is trained with GRPO. These models are trained using the same backbone and training data as our approach. We also compare our approach with state-of-the-art methods reported in the literature, including both proprietary (e.g., GPT-4o, Claude3.5) and open-source models (e.g., InternVL2.5, LLaMA3.2). Implementation Details We adopt Qwen2.5-VL-3B [26] as the base model. This pre-trained model has strong visual understanding capabilities but has not undergone post-training for reasoning. For the group reward computation, we generate 8 output sequences (i.e., = 8 in Eq. 3) and the sampling temperature is set to 0.9 following the common practice. All parameters are optimized with learning rate of 5 107. The caption rewards balancing weight α is set to 0.1. The KL coefficient β is set to 0.04. The model is trained for around 1,500 GPU hours on the 272.6K dataset with NVIDIA A800-80G GPUs. 4.2 Main Results The results are shown in Tab. 1. Comparing SFT with the base model, we observe that the performance of SFT is worse on three out of four datasets, with the biggest performance decline reaching 12% on MathVision. These results suggest that the model learned with question-answer pairs overfits the training data distribution. GRPO slightly outperforms the base model, achieving improvements of 6 Table 1: Comparison with state-of-the-arts on four challenging visual reasoning benchmarks. SFT and RL mean supervised fine-tuning and reinforcement learning, respectively. CoT means chain-ofthought, which is either self-generated or distilled from third-party models like GPT-4o. QA means that the model is learned with question-answer pairs only. Despite having only 3B parameters and using only QA data for training, Visionary-R1 beats strong commercial models like GPT-4o and Claude3.5-Sonnet. Note that indicates results borrowed from the Seeds report [27]."
        },
        {
            "title": "Size Strategy Data MathVista MathVision MMStar MMBench",
            "content": "Close-source models GPT-4o [28] GPT-o1 [29] Claude3.5-Sonnet [30] Claude3.7-Sonnet [31] Gemini-1.5-Pro [32] Gemini-2.5-Pro [33] Open-source models Qwen2.5-VL [26] InternVL2.5 [34] MiniCPM-V2.6 [35] LLaMA3.2 [36] Reasoning models Ovis [18] Mulberry [19] R1-Onevision [24] Insight-V [21] R1-VL [22] LLaVA-CoT [17] Our models Base Model SFT GRPO Visionary-R1 - - - - - - 3B 4B 8B 11B - - - - - - - - - - - - - - - - - - - - CoT SFT 4B 7B CoT SFT 7B SFT+RL CoT 7B SFT+RL CoT 7B SFT+RL CoT CoT SFT 11B 3B 3B 3B 3B - SFT RL RL - QA QA QA 63.8 71.8 67.7 74.5 63.9 82.7 62.3 60.5 60.6 51.5 66.6 63.1 64.1 59.9 63.5 54.8 61.5 54.6 61.8 69. 31.2 63.2 37.9 58.6 19.2 73.3 21.2 20.9 17.5 - - - 29.9 - 24.7 - 19.1 7.0 20.3 24.7 65.1 67.5 65.1 68.8 59.1 77.5 55.9 58.3 57.5 49. 59.5 61.3 - 61.5 60 57.6 52.4 61.9 54.3 66.5 84.3 83.8 82.6 82.0 73.9 90.1 79.1 81.1 81.5 65.8 79.3 - - 82.3 - 75 82.1 80.7 78.6 84. 0.3% on MathVista, 1.2% on MathVision, and 1.9% on MMStar. However, GRPO underperforms the base model by 1.5% on MMBench, which suggests that visual reasoning is difficult to learn from just question-answer pairs. By digging into the outputs, we observe that GRPO often leads to shortcuts in easy training samples while produces short, useless reasoning answers for unseen samples, as illustrated in Fig. 1. Compared to SFT and GRPO, Visionary-R1 demonstrates huge potential in learning general-purpose reasoning capabilities, evidenced by the improvements of 7.9% on MathVista, 5.6% on MathVision, 14.1% on MMStar, and 2% on MMBench, over the base model. Compared with reasoning models that rely on labeled reasoning data, Visionary-R1 still maintains clear advantages on most datasets, despite using only question-answer pairs. Notably, Visionary-R1 even surpasses strong commercial AI models, such as GPT-4o, Claude3.5-Sonnet, and Gemini-1.5-Pro, on MathVista, MMStar, and MMBench. These results strongly justify the effectiveness of learning to caption before reasoning. 4.3 Ablation Study and Analyses Effectiveness of Captioning and Caption Reward We conduct an ablation study to evaluate the effectiveness of each component in Visionary-R1. Specifically, we start from the GRPO model and incrementally add the caption output format and the caption reward rc. Instead of using the compiled 272.6K training data, we use individual datasets to save computation. Specifically, we perform two sets of experiments on different types of datasets (to ensure diversity): 1) training on ChartQA and testing on MathVista and MathVision, and 2) training on A-OKVQA and testing on MMStar and 7 Figure 4: Visualization of different model outputs. The caption output format enhances the reasoning while the caption reward further makes the reasoning more in-depth by improving the caption quality. Table 2: Ablation study on different components in Visionary-R1. Train: A-OKVQA Train: ChartQA Method MathVista MathVision MMStar MMBench Zero-shot GRPO GRPO+caption Visionary-R1 61.5 59.0 62.6 64. 19.1 18.2 20.9 22.7 52.4 54.2 60.4 62.9 82.1 82.6 85.5 87.6 MMBench. Tab. 2 shows the results, which clearly demonstrate the effectiveness of the caption output and the caption reward. Fig. 4 further illustrates the differences in the outputs of different models. KL Coefficient We experiment with different strategies for selecting the KL coefficient β. Specifically, we evaluate the following designs: 1) static values, 2) linear decay, and 3) cosine annealing (proposed in Eq. 5). For static values, we choose 0.04 and 0.008: the former is common practice while the latter is smaller value for testing the effect. The results are reported in Table 3. We find that using static value leads to the worst results while linear decay achieves significant improvementthis highlights the importance of using dynamic KL coefficient during training. Cosine annealing performs slightly better than linear decay. We also apply the cosine annealing strategy to GRPO but observe no performance gain, which suggests that this design mainly affects the captioning component in Visionary-R1. To better understand why the KL coefficient makes such huge impact, we dig into several key metrics logged during training, i.e., output length, the format reward, and the caption reward. The full training processes are shown in Fig. 5 (top). When setting the KL coefficient to 0.04, which has been widely adopted as standard practice in the literature, the output length rapidly climbs up and reaches an unreasonably high value at around 700 steps, and then falls back to the normal level at 100 tokens; meanwhile, both the format reward and caption reward decline drastically as the output length shoots up to an abnormal value, meaning that the model has collapsed in the middle of training. The model collapse is more clear in Fig. 5 (bottom): the model generates long but completely meaningless reasoning tokens. When using smaller value of 0.008, we encounter the reward hacking issue [37]: the model mistakenly generates short reasoning chain at the caption place (which is supposed to contain description about the image) while producing zero token in between <think></think>. This suggests that the model cheats in order to gain higher accuracy reward and as result the reasoning capabilities are not generalizable. The use of either linear decay or cosine annealing can effectively alleviate this issue. 8 Figure 5: Visualization of learning curves for different KL coefficients (top) and output examples (bottom). Table 3: Results of using different KL coefficients. Dynamic strategies (i.e., linear decay and cosine annealing) achieve significantly better results in Visionary-R1, with cosine annealing being the optimal choice. Method Strategy MathVista MathVision Visionary-R1 GRPO Static (0.04) Static (0.008) Linear Cosine Static (0.04) Cosine 60.9 60.7 63.4 64.6 59.0 59. 19.3 18.7 22.4 22.7 18.2 18."
        },
        {
            "title": "5 Conclusion, Limitations, and Future Work",
            "content": "This paper reveals the shortcut learning problem encountered when applying RL to VLMs. Unlike LLMs, VLMs are more difficult to train for reasoning without using annotated data. Visionary-R1, despite using CoT-free question-answer pairs, demonstrates strong performance on challenging visual reasoning benchmarks, surpassing strong commercial AI models that mostly likely benefit from largerscale, higher-quality training data. The results indicate that understanding image context through captioning is essential for enhancing reasoning for VLMs. Moreover, the results also highlight the importance of the KL coefficient, which should be dynamically tuned to stabilize RL training. We believe the finding of the cosine annealing strategy could be applied more broadly to other RL applications. In terms of limitations, the experiments are only based on 3B model due to tight budget on compute. We believe that the effectiveness of RL training can be significantly amplified by using larger models. Investigation on larger-scale models is left as future work."
        },
        {
            "title": "References",
            "content": "[1] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, Lets verify step by step, in The Twelfth International Conference on Learning Representations, 2023. [2] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, Deep reinforcement learning from human preferences, Advances in neural information processing systems, vol. 30, 2017. [3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., Training language models to follow instructions with human feedback, Advances in neural information processing systems, vol. 35, pp. 27 73027 744, 2022. [4] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [5] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [6] F. Meng, L. Du, Z. Liu, Z. Zhou, Q. Lu, D. Fu, T. Han, B. Shi, W. Wang, J. He et al., Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, arXiv preprint arXiv:2503.07365, 2025. [7] K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue, Video-r1: Reinforcing video reasoning in mllms, arXiv preprint arXiv:2503.21776, 2025. [8] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang, Visual-rft: Visual reinforcement fine-tuning, arXiv preprint arXiv:2503.01785, 2025. [9] H. Shen, P. Liu, J. Li, C. Fang, Y. Ma, J. Liao, Q. Shen, Z. Zhang, K. Zhao, Q. Zhang et al., Vlm-r1: stable and generalizable r1-style large vision-language model, arXiv preprint arXiv:2504.07615, 2025. [10] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon et al., Constitutional ai: Harmlessness from ai feedback, arXiv preprint arXiv:2212.08073, 2022. [11] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao, Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, arXiv preprint arXiv:2310.02255, 2023. [12] K. Wang, J. Pan, W. Shi, Z. Lu, H. Ren, A. Zhou, M. Zhan, and H. Li, Measuring multimodal mathematical reasoning with math-vision dataset, Advances in Neural Information Processing Systems, vol. 37, pp. 95 09595 169, 2024. [13] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin et al., Are we on the right way for evaluating large vision-language models? in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [14] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu et al., Mmbench: Is your multi-modal model an all-around player? in European conference on computer vision. Springer, 2024, pp. 216233. [15] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., Chain-of-thought prompting elicits reasoning in large language models, Advances in neural information processing systems, vol. 35, pp. 24 82424 837, 2022. [16] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, Large language models are zero-shot reasoners, Advances in neural information processing systems, vol. 35, pp. 22 19922 213, 2022. [17] G. Xu, P. Jin, L. Hao, Y. Song, L. Sun, and L. Yuan, Llava-cot: Let vision language models reason step-by-step, URL https://arxiv. org/abs/2411.10440, 2024. [18] D. Yan, Y. Li, Q.-G. Chen, W. Luo, P. Wang, H. Zhang, and C. Shen, Mmcr: Advancing visual language model in multimodal multi-turn contextual reasoning, arXiv preprint arXiv:2503.18533, 2025. [19] H. Yao, J. Huang, W. Wu, J. Zhang, Y. Wang, S. Liu, Y. Wang, Y. Song, H. Feng, L. Shen et al., Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search, arXiv preprint arXiv:2412.18319, 2024. [20] T. Chu, Y. Zhai, J. Yang, S. Tong, S. Xie, D. Schuurmans, Q. V. Le, S. Levine, and Y. Ma, Sft memorizes, rl generalizes: comparative study of foundation model post-training, arXiv preprint arXiv:2501.17161, 2025. [21] Y. Dong, Z. Liu, H.-L. Sun, J. Yang, W. Hu, Y. Rao, and Z. Liu, Insight-v: Exploring long-chain visual reasoning with multimodal large language models, arXiv preprint arXiv:2411.14432, 2024. [22] J. Zhang, J. Huang, H. Yao, S. Liu, X. Zhang, S. Lu, and D. Tao, R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization, arXiv preprint arXiv:2503.12937, 2025. [23] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin, Vision-r1: Incentivizing reasoning capability in multimodal large language models, arXiv preprint arXiv:2503.06749, 2025. [24] Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu et al., R1onevision: Advancing generalized multimodal reasoning through cross-modal formalization, arXiv preprint arXiv:2503.10615, 2025. [25] J. Skalse, N. Howe, D. Krasheninnikov, and D. Krueger, Defining and characterizing reward gaming, Advances in Neural Information Processing Systems, vol. 35, pp. 94609471, 2022. [26] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [27] D. Guo, F. Wu, F. Zhu, F. Leng, G. Shi, H. Chen, H. Fan, J. Wang, J. Jiang, J. Wang et al., Seed1. 5-vl technical report, arXiv preprint arXiv:2505.07062, 2025. [28] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [29] A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney et al., Openai o1 system card, arXiv preprint arXiv:2412.16720, 2024. [30] Anthropic, Claude 3.5 Sonnet, https://www.anthropic.com/news/claude-3-5-sonnet, 2024. [31] , Claude 3.7 sonnet, https://www.anthropic.com/claude/sonnet, 2025. [32] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang et al., Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv preprint arXiv:2403.05530, 2024. [33] Google, Gemini 2.5 pro, https://deepmind.google/technologies/gemini/pro/, 2025. [34] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu et al., Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. [35] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He et al., Minicpm-v: gpt-4v level mllm on your phone, arXiv preprint arXiv:2408.01800, 2024. [36] M. AI, Llama 3.2: Revolutionizing edge ai and vision with open, customizable models, https://ai.meta. com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/, 2024. [37] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano, Learning to summarize with human feedback, Advances in neural information processing systems, vol. 33, pp. 30083021, 2020. [38] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi, A-okvqa: benchmark for visual question answering using world knowledge, in European conference on computer vision. Springer, 2022, pp. 146162. [39] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, Towards vqa models that can read, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 83178326. [40] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque, Chartqa: benchmark for question answering about charts with visual and logical reasoning, arXiv preprint arXiv:2203.10244, 2022. [41] Y. Zhao, C. Zhao, L. Nan, Z. Qi, W. Zhang, X. Tang, B. Mi, and D. Radev, Robut: systematic study of table qa robustness against human-annotated adversarial perturbations, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 60646081. [42] J. Cao and J. Xiao, An augmented benchmark dataset for geometric question answering through dual parallel text encoding, in Proceedings of the 29th international conference on computational linguistics, 2022, pp. 15111520. [43] M. Mathew, D. Karatzas, and C. Jawahar, Docvqa: dataset for vqa on document images, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2021, pp. 22002209. [44] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14. Springer, 2016, pp. 235251. [45] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan, Learn to explain: Multimodal reasoning via thought chains for science question answering, Advances in Neural Information Processing Systems, vol. 35, pp. 25072521, 2022. 11 [46] A. D. Lindström and S. S. Abraham, Clevr-math: dataset for compositional language, visual and mathematical reasoning, arXiv preprint arXiv:2208.05358, 2022. [47] P. Lu, L. Qiu, J. Chen, T. Xia, Y. Zhao, W. Zhang, Z. Yu, X. Liang, and S.-C. Zhu, Iconqa: new benchmark for abstract diagram understanding and visual language reasoning, arXiv preprint arXiv:2110.13214, 2021. [48] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning, arXiv preprint arXiv:2209.14610, 2022."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Complete list of training data Tab. 4 shows the complete training data, which aggregates 11 popular question-answer datasets and covers wide range of visual formats and tasks, e.g., A-OKVQA [38] and TextVQA [39] for general scene understanding, ChartQA [40] and RoBUT SQA [41] for chart understanding, GeoQA+ [42] for mathematical problem-solving, and DocVQA [43] for document processing. Table 4: Composition of our training data. Dataset A-OKVQA [38] ChartQA [40] AI2D [44] ScienceQA [45] GeoQA+ [42] DocVQA [43] CLEVR-Math [46] Icon-QA [47] TabMWP [48] RoBUT SQA [41] TextVQA [39] Size 17.1K 28.3K 15.5K 6.2K 12.1K 39.5K 32.6K 29.9K 23.1K 34.1K 34.6K Total 272.6K Answer Type Visual Format Multi-choice Open-text+Num Multi-choice Multi-choice Multi-choice Open-text Num Multi-choice Open-text+Num Open-text+Num Multi-choice General Scene Chart Diagram Scene + Chart Math Document 3D Diagram Table Chart General Scene A.2 Policy Model Prompt To ensure the model interprets the image before engaging in the thought process, we include additional instructions in the system prompt to guide the policy model in generating the corresponding output. The complete model prompt can be seen from Fig. 6. Using this prompt, the model will insert the corresponding image description labeled as <info> before the thinking process, additional to the existing <think> and <answer>. A.3 Caption Reward Prompt Leveraging the language model within the policy model, we judge the level of detail by having the model answer questions based on the caption. sufficiently detailed description of the image in the caption is essential for providing the necessary information to answer the questions accurately. With this approach, we prompt the language model to respond to questions based on the caption. To prevent reward hackingwhere the model might include its thought process and answer in the information sectionwe incorporate an additional filtering command in the prompt to eliminate such interference. The complete caption reward prompt can be seen from Fig. 7. A.4 Visualization of the Visionary-R1 Output To illustrate the validity of Visionary-R1 across various visual formats, we visualized the output for each format individually. The results are presented in Fig. 8-14. 12 Figure 6: System prompt given to the policy model. Figure 7: System Prompt for the language model to answer the question based on the given caption. Figure 8: Visualization of Visionary-R1 Output in Document Format. Figure 9: Visualization of Visionary-R1 Output in General Scene Format. Figure 10: Visualization of Visionary-R1 Output in Table Format. Figure 11: Visualization of Visionary-R1 Output in 3D Format. 14 Figure 12: Visualization of Visionary-R1 Output in Chart Format. Figure 13: Visualization of Visionary-R1 Output in Math Format. The original input and output were both in Chinese, and we have translated them directly without any modifications. Figure 14: Visualization of Visionary-R1 Output in Diagram Format."
        }
    ],
    "affiliations": [
        "Hong Kong Baptist University",
        "Shanghai AI Lab",
        "University of Wisconsin-Madison"
    ]
}