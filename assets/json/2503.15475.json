{
    "paper_title": "Cube: A Roblox View of 3D Intelligence",
    "authors": [
        "Foundation AI Team",
        "Kiran Bhat",
        "Nishchaie Khanna",
        "Karun Channa",
        "Tinghui Zhou",
        "Yiheng Zhu",
        "Xiaoxia Sun",
        "Charles Shang",
        "Anirudh Sudarshan",
        "Maurice Chu",
        "Daiqing Li",
        "Kangle Deng",
        "Jean-Philippe Fauconnier",
        "Tijmen Verhulsdonck",
        "Maneesh Agrawala",
        "Kayvon Fatahalian",
        "Alexander Weiss",
        "Christian Reiser",
        "Ravi Kiran Chirravuri",
        "Ravali Kandur",
        "Alejandro Pelaez",
        "Akash Garg",
        "Michael Palleschi",
        "Jessica Wang",
        "Skylar Litz",
        "Leon Liu",
        "Anying Li",
        "David Harmon",
        "Derek Liu",
        "Liangjun Feng",
        "Denis Goupil",
        "Lukas Kuczynski",
        "Jihyun Yoon",
        "Naveen Marri",
        "Peiye Zhuang",
        "Yinan Zhang",
        "Brian Yin",
        "Haomiao Jiang",
        "Marcel van Workum",
        "Thomas Lane",
        "Bryce Erickson",
        "Salil Pathare",
        "Kyle Price",
        "Anupam Singh",
        "David Baszucki"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence."
        },
        {
            "title": "Start",
            "content": "Cube: Roblox View of 3D Intelligence Foundation AI team, Roblox"
        },
        {
            "title": "Abstract",
            "content": "Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such foundation model for 3D intelligence, model that can support developers in producing all aspects of Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such 3D foundation model and then present our first step towards building such model. We expect that 3D geometric shapes will be core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with discussion outlining our path to building fully unified foundation model for 3D intelligence. Our code and model weights can be found at: https://github.com/Roblox/cube. 5 2 0 2 9 1 ] . [ 1 5 7 4 5 1 . 3 0 5 2 : r Scene Generation. We are developing foundation model for 3D intelligence that will Figure 1 support applications like scene generation. This winter village scene was generated through multi-turn conversation with our prototype scene generation tool. 1See Appendix for the full author list. Figure 2 Overview. We present an important step towards the foundation model for 3D intelligence. Specifically, our report focuses on 3D shape tokenizationa technique for converting between shapes and discrete tokens. We also demonstrate how our tokenization scheme enables multiple applications including text-to-shape generation, shape-to-text generation, and text-to-scene generation. 1. Introduction Modern generative AI promises to give every human collaborative assistant for creating visual content. An assistant that can convert simple inputs, such as text prompts, into productionquality visual experiences. Today, these assistants typically rely on an underlying foundation AI model that is trained on vast amounts of broad, multi-modal data, and then fine-tuned to perform specific downstream assistive tasks (Bommasani et al., 2021). Recent foundation models that bridge the domains of text, image, audio and video have demonstrated remarkable reasoning and generation capabilities (Achiam et al., 2023; Team et al., 2024). At Roblox, our goal is to build foundation model for 3D intelligence. We envision this model as the base for variety of collaborative assistants that will aid developers in producing all aspects of Roblox experiences from creating individual 3D objects (e.g., make motorbike with wings), to full 3D scene layout (e.g., create futuristic cloud city), to rigged character avatars (e.g., generate an alien ninja capable of doing wall jumps) to scripts describing object behaviors, interactions and game logic (e.g., make the door open when player is near it and carrying gold key). We believe that building such foundation model for 3D reasoning and generation imposes three core design requirements. Learn jointly from sparse, multi-modal data. The amount of readily available 3D experience data suitable for training is much less than the data available for training text, image or video models. But 3D data is multi-modal (e.g., geometric meshes, CSG parts, layouts, textures, rigging, programmed scripts) and the modalities are strongly correlated with each other (e.g., an objects texture and placement in scene often strongly correlate with its shape). Thus, our model should jointly learn from all the modalities available in the training data. Handle unbounded input/output size via an autoregressive model. 3D experiences can be of vastly different sizes, such as micro-scene containing few static objects (e.g., room with dinner table and chairs) to multi-apartment building with interactive doors and elevators, to large-scale city with interacting vehicles and non-player characters. To handle inputs and outputs at each of these scales, our model should be autoregressive Figure 3 Shape Tokenization. Our pipeline encodes an input mesh into discrete tokens through several steps: (1) We sample points from the mesh surface and embed them using our Phased-Modulated Positional Encoding; (2) Perceiver-based transformer (Jaegle et al., 2021) encodes these points into continuous latent vectors, regularized with self-supervised loss; (3) We apply optimal-transport vector quantization (Zhang et al., 2024) to convert these vectors into discrete shape tokens; (4) These tokens can later be decoded into an occupancy field for mesh extraction. To improve training stability and the reconstruction quality, we also introduce Stochastic Gradient Shortcut layer that allows the decoder to utilize the continuous latent vectors directly during training. with large context length. Collaborate with humans and other AI systems via multi-modal inputs/outputs. Humans are familiar with data modalities such as natural language text, images, sketches and video and should be able to express design intents using any of them. Moreover, existing large language models (LLMs) such as GPT-4o (Achiam et al., 2023), which are trained with vast amounts of multi-modal data including text, programs, images, vector graphics, scene graphs, video, speech, audio, etc. can provide common sense knowledge (i.e., general priors) on variety of topics. Thus, for our model to seamlessly collaborate with humans as well as other LLMs, multi-modal data should be first-class input and output data types. In this report we present our first step towards this foundational model for 3D intelligence. Specifically, we focus on discrete tokenization of the 3D shape, recognizing that geometry will be central data type of our foundation model. We show how our tokenization scheme can be used to build several applications including text-to-shape generation, shape-to-text generation, and text-to-scene generation (Figures 1 and 2). We also demonstrate how these applications can collaborate with text-based LLM to perform scene analysis and reasoning tasks. Our code and inference model weights are available at: https://github.com/Roblox/cube. 2. Shape Tokenization We need representation of 3D geometry that is expressive enough to capture wide range of geometric properties faithfully, including smooth surfaces, sharp edges, high-frequency details, and can serve as both input and output tokens to multi-modal autoregressive sequence models. Faced with these needs, we start from an expressive, continuous shape representation such as 3DShape2VecSet (Zhang et al., 2023a), and adapt it into discrete tokens to enable native handling 3 () (a) Baseline Positional Encoding ùõæ (b) Phase-Modulated Positional Encoding ùõæ () Figure 4 Phase-Modulated Positional Encoding. Comparison of positional encoding methods , with corresponding dot-product similarity matrices. using 128 base frequencies for 1D inputs in enhances high-frequency details but exhibits periodic variations (a) Traditional positional encoding ùõæ 1 to 1. This causes dot-product similarities between across embedding dimensions as inputs vary from maintains clear encoded vectors to poorly reflect their spatial proximity. (b) Our proposed PMPE ùõæ () distinctions between spatially distant inputs, as reflected in the dot-product similarity of encoded vectors. ùõæ that preserves fine-grained details while ensuring Our final approach uses the combined encoding of ùõæ + similarity patterns align with global spatial proximity. 1, 1 [ () ] of input and output tokens across multiple modalities in manner similar to mixed-modal foundation models such as Chameleon (Team, 2024). As outlined in Figure 3, our high-level architecture follows an encoder-decoder design that encodes the input 3D mesh into latent representation capable of being decoded into an implicit occupancy field. One key distinction is that the continuous latents get discretized through an additional vector quantization process, which poses unique training challenges due to its non-differentiability nature. We propose two techniques, namely stochastic gradient shortcut and self-supervised latent space regularization, to address the VQ training challenges ( Section 2.2 and 2.3, respectively). Another architecture improvement is the use of phasemodulated positional encodings that improve the perceiver-based transformers ability to disambiguate spatially distinct points in cross-attention layers (Section 2.1). These architecture changes allow us to train shape tokenizer that faithfully captures wide range of shapes while yielding discrete tokens that are suitable as new modality for training token-based mixed-modal foundation models. 2.1. Phase-Modulated Positional Encoding To encode shape into compact latent representation, we first sample ùëÅùëù points from its surface 3 . Prior work (Li et al., 2024; Zhang et al., 2023a; Zhao et al., to create point cloud 2023, 2025) embeds via sinusoid positional encoding function ùõæ (Mildenhall et al., 2020) before processing it with transformer network: RùëÅùëù () where ùõæ ùëñ ùúîùëñ = 2 ( 2 / ùëù ) ùúã, ùúëùëñ = ùúã 2 ( = ùõæ ùúëùêø)] is applied separately to each of the three coordinate channels ùëù ùúë2) ùúë1) , sin ùúî1 ùëù ùúî2 ùëù , sin ùúîùêø ùëù ùëù ) sin + + + ( ( ( ( [ , , (1) in P, and ùë•, ùë¶, ùëß ] [ , for ùëñ = 1, The periodicity of ùõæ ùëñ mod 2 ) ùúîùëñ in space to identical maps points separated by multiples of 2ùúã / encodings in the ùëñùë°‚Ñé channel. The result is that spatially distant points can map to similar embedding vectors (Figure 4a) that are hard to distinguish after the dot-product in crossattention layers. The inability for the embedding to disambiguate spatially distant points, and , ùêø, with ùêø being the number of base frequencies. () 4 Figure 5 Latent Space Regularization with Self-supervised Loss. We regularize our latent space using self-supervised loss inspired by DINOv2 (Oquab et al., 2023). This loss is computed as the cross entropy between features extracted from the student and teacher encoders, where the teacher model is an Exponential Moving Average (EMA) of the student. Input queries to the student encoder are randomly masked for robustness. correspondingly, to disambiguate different shape surface features, results in reduced shape reconstruction quality. To address this problem, we need new technique to encode points such that it not only captures the multi-scale features as in the traditional position encoding but preserves the distinctiveness of spatially distant points with the dot-product attention. We take inspiration from phase modulation techniques (Haykin, 2008) and propose Phase-Modulated Position Encoding (PMPE). PMPE modulates phase offsets across all the sinusoidal functions, and uses the embedding function ùõæPM, defined as: ùõæ ùëù ) ( (cid:104) = sin ùúã 2 ( ùëù + ùúëùëñ = 2ùúã ùëù ) , sin = ùõæ ( ùúã 2 ( ùõæPM( ùúë1) (cid:18) ùëñ ùêø 1 ùëñ ùêø + ùõΩùêø ) ( + (cid:19) , ùëñ = 1, , ùêø, ùëù ) + ùëù ùõæ ( ùúë2) , ùëù ) , , sin ùúã 2 ( ùëù ùúëùêø) + (cid:105) , (2) ( ùëù ) where ùõæ rate of phase variation across channels. The term avoid resonance between ùõæ is the traditional encoding function, and ùõΩ is hyperparameter that controls the ùêø ) here is to vary the base frequency to ùõΩùêø ) ( ( 1 . ùëñ ùëù ) ( and ùõæ( ùëù ) ùëù ) In contrast to ùõæ , which uses exponentially increasing frequencies to capture multi-scale 2 for each channel but varies the phase offset ùúëùëñ via features, ùõæ( non-linear function of ùëñ. The non-linear phase modulation ensures the distinctiveness of spatially distant points when mapped to the embedding space, as shown in Figure 4b. uses the same frequency ùúã / ùëù ) ( Empirically, we find that PMPE yields significantly improved reconstruction fidelity, particularly for complex geometric details. PMPE also produces fewer artifacts such as disconnected components that appear in methods that use only traditional positional encoding. 2.2. Stochastic linear shortcut for gradient stabilization After encoding the input shape into continuous latent vector, we employ variant of VQVAE (van den Oord et al., 2017) (i.e., Optimal Transport VQ (Zhang et al., 2024)) to convert the latents into sequence of discrete tokens. The quantization layer in VQ-VAE can introduce 5 (a) Without self-supervised loss (b) With self-supervised loss Figure 6 Impact of Self-Supervised Loss. Cosine similarity between the latent vectors of 3D meshes for models trained (a) with and (b) without the proposed self-supervised loss term. (a) Without regularization, latent cosine similarity scores fail to correlate with geometric structure (e.g., the ice-cream in 4ùë°‚Ñé row has higher similarity to the car in 1ùë†ùë° row than the similar car in 2ùëõùëë row). (b) With the loss, latent space similarity aligns with ground-truth geometric relationships. training instabilities due to ill-defined gradients from the non-differentiable code embedding assignment (Huh et al., 2023). Prior works have attempted to use straight-through estimator (Bengio et al., 2013) or randomly replace fraction of the quantized embeddings with their continuous approximations during training (Takida et al., 2022; Zhang et al., 2023b). Inspired by the prior works, we make further adjustment by introducing an additional linear shortcut layer that stochastically skips the entire quantization bottleneck. With 50% probability, we project the continuous latents from the encoder with linear layer, and directly feed the projected latents to the decoder, bypassing the quantization layer entirely. Notice that this is different from the direct shortcut approach analyzed in the previous work (Fifty et al., 2024) that is equivalent to using an identity (instead of linear) layer in our formulation. Both Fifty et al. (2024) and our own experiments have found the direct shortcut approach to not perform well. Our intuition is that the additional linear layer in our formulation allows the shortcut pathway to learn slightly different set of weights but with well-defined gradients, and it allows the shortcut pathway to act as teacher network for the quantization pathway. As result, the quantization pathway is less prone to being stuck at local minima. Empirically, we observe that incorporating the linear shortcut layer leads to lower training and validation loss and makes training more stable under broader range of hyperparameters. 2.3. Learning geometrically clustered latents with self-supervised loss Following prior work in vision models (Caron et al., 2021; Grill et al., 2020; Oquab et al., 2023; Zhou et al., 2021), we employ self-supervised loss to regularize the latent space to be geometrically clustered and smooth, where similar shapes produce latent vectors that are close to each other. Figure 5 illustrates our encoders self-supervised learning pipeline. Following the architecture in DINOv2 (Oquab et al., 2023), we maintain an exponential moving average version of the encoder as the teacher model. The student encoder receives randomly masked Qualitative Analysis of Shape Reconstruction. Comparison between our method Figure 7 and Craftsman (Li et al., 2024) demonstrates that both of our model variants achieve superior reconstruction quality, preserving finer geometric details while producing fewer artifacts. input queries, while the teacher encoder has access to the full set of queries. Both encoders use an additional MLP head to produce prototype scores, and the self-supervised loss is the cross-entropy between these scores, weighted by hyperparameter ùúÜùëÜùëÜùêø that balances with the occupancy reconstruction loss. As shown in Figure 6, the ability to encode geometrically similar shapes into latent vectors with large cosine similarities naturally emerges from the additional self-supervised loss. We anticipate that this property will prove valuable for wide range of shape processing applications. 2.4. Results Architecture details. For training our shape tokenizer, we use similar transformer layers for both the encoder and the decoder, with each layer having 768 width and 12 heads. The encoder and decoder consist of 13 and 24 transformer layers respectively, with total number of 273M parameters. We use 512 latent code tokens and codebook size of 16, 384 with 32-dimensional embeddings. We set ùõΩ = 0.125 for PMPE and ùúÜùëÜùëÜùêø = 0.0005 for the self-supervised loss. For the VQ layer, we use recent variant called OptVQ (Zhang et al., 2024) that integrates an optimal transport method for index assignment. Training data. We train our shape tokenizer on roughly 1.5 million 3D object assets that utilize combination of licensed and public available datasets (e.g. Objaverse (Deitke et al., 2023)), as well as the free data assets that are available in Roblox Creator Store that are opted in for training. We center and rescale all assets to lie within normalized bounding box with each axis ranging from . For each asset within training batch, we sample 8, 192 points directly on its surface to facilitate input encoding. To compute the occupancy loss, we sample another 8, 192 points comprising two different sets, one from uniform sampling in the space to capture the overall geometric structure and the other from near-surface to capture high-frequency surface details. 1, 1 [ ] Model comparison. We compare our discrete shape tokenizer with continuous variant trained on the same 1.5 million object dataset, and with CraftsMan (Li et al., 2024) recent variant of 3DShape2VecSet trained on 170K object subset of Objaverse. Our continuous variant is trained using the same architecture as the discrete tokenizer, with the only difference that it skips the 7 CraftsMan (Li et al., 2024) Ours-VQ Ours-KL S-IoU V-IoU 68.8% 83.6% 91.7% 94.5% 94.8% 95.4% Table 1 Comparison of S-IoU and V-IoU results across different models. VQ process and uses KL-divergence to regularize the latent space (Kingma et al., 2013). We measure the shape reconstruction quality of different methods using surface-IoU (S-IoU) and volumetric IoU (V-IoU) on the Toys4K (Stojanov et al., 2021) dataset. We calculate S-IoU using points near the mesh surface and V-IoU using uniformly sampled points within the bounding volume. None of the three methods was trained on the Toys4K dataset. As shown in Table 1 and Figure 7, both our VQ-VAE (Ours-VQ) and the continuous variant (Ours-KL) outperform CraftsMan in both S-IoU and V-IoU metrics. Our continuous variant still outperforms its discrete counterpart, highlighting that there remains some loss of geometry fidelity through the vector quantization process. We aim to bridge this gap in future work. 3. Applications Shape tokenization is only one component of the unified foundation model we envision, but we have already found it to be an effective representation of geometric data. We have begun developing variety of applications using our shape tokens as the core geometric representation. These applications include text-to-shape generation, shape-to-text generation and text-to-scene generation. We briefly explain how each application takes advantage of our shape tokenization scheme and demonstrate how they might aid users in developing 3D experiences. 3.1. Text-to-shape generation Our text-to-shape application allows users to convert text description of shape into triangle mesh model that can serve as an element of 3D experience. Architecture. Our text-to-shape architecture is decoder-only transformer similar to GPT-2 (Radford et al., 2019). We tokenize the shape into discrete tokens and use the transformer to generate the shape tokens with text conditioning. Specifically, we encode input text prompts using pre-trained CLIP text encoder (Radford et al., 2021), and leverage dual-stream attention (Esser et al., 2024) to inject the text conditioning into the transformer model to output shape tokens in an autoregressive manner. We also incorporate Classifier-free Guidance (Ho and Salimans, 2022) into our GPT training and inference pipeline. During training, we randomly drop the text conditioning and replace it with an empty string 10% of the time following (Sun et al., 2024). Training data. Training our text-to-shape application requires paired examples of text prompts and their corresponding 3D shapes. For each of the 1.5 million assets used to train the shape tokenizer, we render multiple views and use GPT-4o (Achiam et al., 2023) to provide captions of various lengths. Mesh extraction. We use marching cubes (Lorensen and Cline, 1998) to extract the iso-surface from the occupancy field, along with an in-house mesh decimation algorithm built upon the quadric errors (Garland and Heckbert, 1997) for simplifying the mesh into desired number of faces. Finally, we apply post-processing step that removes small disconnected components (i.e. floater artifacts) from the output mesh. 8 Figure 8 meshes, capturing sharp edges, smooth surfaces and complex structures. Text-to-Shape Generation Result Gallery. Our model can generate diverse set of 3D 9 Figure 9 Shape-to-text examples. Shape-to-text captioning of example shapes from Toys4K dataset using short, medium and long captions. Words highlighted in blue indicate notable differences as captions increase in length. Short captions usually capture the shape category. More details about the shapes geometry and style are added as caption length increases. Results. We show representative set of text-to-shape generations in Figure 8. Our text-to-shape model is able to generate diverse set of 3D meshes, capturing sharp edges, smooth surfaces and complex structures. These meshes, as produced with our discrete shape tokens are approaching the visual quality to those produced using recent methods based on rectified flow transformers with continuous latents such as Trellis (Xiang et al., 2024), Hunyuan3D-2 (Zhao et al., 2025) and TripoSG (Li et al., 2025). While benefit of our approach is that it can easily support multi-modal tokens, full understanding of the relative strengths of the different approaches is an area of future work. 3.2. Shape-to-text generation Our shape-to-text application produces descriptive natural language caption for an input 3D shape. These text captions are designed to capture 3D shape information in the sense that they can be fed back to our text-to-shape application to produce corresponding 3D models. And, as we will show in Section 3.3, these text captions allow us to build AI tools that can seamlessly collaborate with existing LLMs to leverage their common sense knowledge and reasoning capabilities. Our architecture is inspired by recent work on vision-language models such as LLaVA (Liu et al., 2023). In order to perceive visual signals in vision-language models, pre-trained openset visual encoder, such as CLIP (Radford et al., 2021), is commonly used to encode the visual input into the input space of the language decoder. In our case, we encode the 3D shape input with our shape tokenizer and inject the shape tokens into pre-trained decoder-only transformer (Vaswani et al., 2017) with single two-layer MLP latent projection layer. For the pre-trained LLM backbone, we chose the language model from InternVL 2.5-2B (Chen et al., 2024), multimodal large language model pre-trained on large-scale image-text data. We choose 10 Figure 10 Shape cycle consistency. Our shape-to-text and text-to-shape models demonstrate the cycle consistency. Given shape, we caption it using shape-to-text model, then then regenerate the 3D shape using the text-to-shape model. We highlight some keywords in blue for the shape category and style. The process preserves the overall geometry and key characteristics of the original shape, although there can be some loss in fine-scale details. this backbone due to its outstanding performance on wide range of multimodal benchmarks. Our architecture design follows standard multimodal learning practice and it demonstrates that our shape tokenizer can be adapted and applied in standard multimodal setups. Training. Since the shape tokenizer is trained on the shape dataset only, the shape latent space can be different from that of the language transformer. Following the training recipe of LLaVA (Liu et al., 2023), we adopt two-stage training pipeline. In the first stage, only the latent projection layer is trained to align the shape features with the text features of the pre-trained transformer. In the second stage, we jointly finetune the latent projection layer and the weights of the language transformer to better align the shape features with the text features. We keep the shape tokenizer frozen during the entire training process. Similar to LLaVA, we train the model using the next-token prediction loss and only apply the loss to the text output sequence. Implementation details. We use the same dataset we developed for text-to-shape generation (Section 3.1). In the dataset, we have captions of various length. In order to control the output caption length in the shape-to-text generation, we append short text instruction after the input shape tokens to indicate the desired output caption length. Specifically, we use caption short:, caption medium:, and caption long: for caption lengths of less than 25 tokens, less than 75 tokens, and greater than 75 tokens respectively. Results. Figure 9 shows captions of various lengths generated by our shape-to-text model. When producing short captions, the model only gives category information about the input shape (e.g., cartoon dog). As the caption length increases, the model describes different parts of the object (e.g., rounded body, floppy ears, etc.) and styles (e.g., playful, cartoonish appearance, large, expressive eyes, etc.). Shape cycle consistency. Our goal is to capture notion of 3D shape in the natural language captions produced by our shape-to-text model. Figure 10 shows examples where we start from shape input, generate corresponding text captions using our shape-to-text model, and then regenerate the shape by treating the captions as inputs to our text-to-shape model. The results show that the captions produced by our shape-to-text model capture enough 3D information for our text-to-shape model to reproduce the overall 3D shape, albeit with some loss of highFigure 11 Scene graph with text-based object descriptions. For each scene object, the scene description includes the type of object (object_category) and text description of its shape (object_caption). These descriptions facilitate LLM understanding of the scene. To support the simple case of orienting objects on ground plane, our current implementation only supports axis rotations. frequency geometric details. We hypothesize that shape cycle-consistency emerges from the fact that the same shape tokens are used by the shape-to-text and text-to-shape models. 3.3. Text-to-Scene generation Our text-to-scene application combines the text-to-shape and shape-to-text models from the prior sections with an off-the-shelf LLM to provide users the ability to iteratively construct full scenes using only text descriptions. From simple prompts like Make campsite and also corrective instructions like Add another tent, our scene generation application creates collection of objects and lays them out (sets position, orientation and scale) in scene configuration. The system can also make suggestions to the user about what additional elements should be added to the scene (e.g., consider adding neon lights to 60s-style diner scene). Scene graph representation. Our shape-to-text model allows us to convert entire scene descriptions (the individual object shape descriptions and description of their placement in scene) into single text-based scene-graph that can be consumed and processed by LLMs. Our scene graph format is JSON-based, and represents scene as flat list of 3D objects (Figure 11). Each object includes text description of the objects shape (object_caption) and the type of object it is (object_category). These fields provide sufficiently detailed description of the 3D object for an LLM to be able to reason about some of its spatial and semantic properties. Objects also have scene layout information giving the position, orientation, and bounding box of the object in the scene. Given 3D scene (such as via GLTF file), we convert the scene to our textual scene graph by extracting all references to 3D models and replacing them with their bounding box and the appropriate text descriptions produced by our shape-to-text model (Section 3.2). Conversely, given scene graph containing only textual description of objects, we can convert all textual object descriptions into 3D geometry using text-to-shape (Section 3.1). Making scene. Given user-specified scene description, our text-to-scene application uses an LLM to generate scene descriptions in the format described above. Modern LLMs exhibit strong capabilities for reasoning about what objects are likely to be present given the input description, but they can struggle with more precise spatial reasoning needed to construct plausible scene layouts. To assist the LLM we generate database of prompt and JSON scene graph pairs (created by processing scenes using our shape-to-text model described above) and provide exemplars from this database to the LLM via in-context learning (Dong et al., 2022). Once the LLM has generated JSON scene graph, we generate scene geometry for each object 12 Figure 12 Scene generation examples. The layout, the rotation, position and scale of each object are produced by the LLM system, while the geometry and texture are generated by our text-to-shape and an in-house text-to-texture model built upon FlashTex (Deng et al., 2024). 13 3D scene suggestion assistant: (We remove all the scene related meta data, i.e. scene name, object category name from the artists, and only use the shape caption generated by our shape-to-text model to test the 3D scene Q&A) Please summarize the scene. User Assistant The scene features modern dining setting with tables, chairs, and various objects like plates, bottles, coins, and decorative items. Pendant lights hang above, and sleek, minimalist aesthetic dominates the space. There are also some irregularly shaped objects, possibly food items, enhancing the dining atmosphere. Id like to add condiments on top of the counter, where should place them? User Assistant The scene includes modern rectangular tables and counter-like surface (likely the trapezoidal prisms or modern tables). Consider these positions: 1.Near the center of the counter Ensures easy access for diners. 2.Close to plates or food items Keeps condiments within reach of those eating. 3.Evenly spaced along the counter Prevents crowding in one area. want to replace the stool near the counter, what other type of seat do you recommend? User Assistant For modern dining scene, here are some great alternatives to replace the stool near the counter: 1.High-Back Bar Chairs Sleek and cushioned for premium feel. 2.Bench Seating continuous, modern look that fits multiple players. 3.Swivel Chairs Adds dynamic, interactive element. What audio, background music would you recommend in this scene? User Assistant For your modern dining scene, recommend: 1. Lo-fi jazz with soft piano & muffled chatter for chic, relaxed vibe. 2. Light orchestral music with faint dish clinks for an elegant touch. 3. Acoustic guitar & caf√© ambiance for cozy feel. Table 2 Scene analysis and suggestion example. Given this diner scene, our scene suggestion assistant is able to summarize the scene, make suggestions for scene enhancements such as where to placement of condiments, what style of seating would be appropriate, and make background audio recommendations. using our text-to-shape model. Figure 12 shows renderings of scenes generated using our text-to-scene generator. The generated scenes are diverse and the layout placement is physicallyplausible (i.e., the sofa, chair and the office desk are placed in the correct location and the cars in the garage are facing in the consistent direction). The objects in the scene have consistent styles in both the geometry and texture (e.g., the pagoda and the traditional Japanese front gate match in the scale and style). The scenes depicted in the figure were generated by the LLM from starting scene description prompt, and then iteratively refined by the user through multi-turn conversations with our system. For example the user can issue new text instructions to the LLM, (Add tree to the scene) resulting in new scene description, or use 3D scene editor tool to manually adjust the position and orientation of objects after the initial placement, which can be used to occasionally correct the LLM placement errors. Scene analysis and suggestions. When creating 3D scene, it can be challenging to consider what objects populate it with. We envision human user working in tandem with our scene creation application to help them to quickly consider options for how to improve the scene. For example, Table 2 depicts 3D scene of 60s-style diner. The text description of scene provides sufficient information for the LLM to comprehend how the scene might be used, and to offer 14 suggestions on how to enhance it. For example, the system is able to suggest relevant places to put condiments on the counter top, recommend styles of seating that might be added, and even offer music suggestions. 4. Conclusion We have demonstrated the first step towards our goal of developing foundational model that is capable of 3D generation and reasoning. Specifically, we have shown how our shape tokenization scheme enables variety of generative 3D applications. Looking ahead, there are multiple challenges that must be addressed to achieve the complete vision of unified 3D foundational model capable of assisting human developers with building all aspects of Roblox experience. Here we outline some of the future steps towards this vision. Mixed generation of meshes and parts. Our text-to-shape application generates 3D geometric mesh from text prompt. But the Roblox platform also supports modeling 3D shapes via constructive solid geometry (CSG) operations on basic parts (e.g. cylinder, cube, sphere, etc.). These parts-based objects provide compact representation that can be rendered efficiently on variety of edge devices. They also provide distinct blocky visual style that is the hallmark of many Roblox experiences. We are exploring how to incorporate our shape tokens into an autoregressive GPT model that can generate parts-only CSG shapes or shapes that combine CSG and meshes. Character avatar generation. Avatars are the primary representation of players in Roblox experiences. For many players, their first creation experience involves designing or customizing their avatar. Avatars move and animate as players interact with scene, so AI-based avatar generation must produce riggable geometry: detailed head meshes, separate meshes for animatable features such as eyes and mouth, body mesh, and clothing meshes that are layered on the body. 4D behavior generation. Objects in Roblox experience can be rigged for animation and/or programmatically scripted to behave in different ways depending on how player interacts with them. For example, 3D car model might be rigged so that turning the steering wheel rotates the wheels. door might be scripted to slide open when player is near it. We refer to such rigged and/or scripted motions as 4D behavior, and our goal is to build AI-based 4D behavior generators. Our vision of unified foundational model for 3D intelligence aims to enable users of all backgrounds to easily create complete 3D experiences. Generative AI tools that can assist users at each step of the creation process requires such foundational model that can reason about 3D shape, object behaviors, player interactions and game logic. Our work so far has taken step towards the first of these reasoning goals. With the release of our open source model2 we hope to engage others in the research community to address these goals with us. 2https://github.com/Roblox/cube"
        },
        {
            "title": "References",
            "content": "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Y. Bengio, N. L√©onard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. M. Caron, H. Touvron, I. Misra, H. J√©gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In IEEE International Conference on Computer Vision (ICCV), 2021. Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, In IEEE A. Kembhavi, and A. Farhadi. Objaverse: universe of annotated 3d objects. Conference on Computer Vision and Pattern Recognition (CVPR), 2023. K. Deng, T. Omernick, A. Weiss, D. Ramanan, J.-Y. Zhu, T. Zhou, and M. Agrawala. Flashtex: Fast relightable mesh texturing with lightcontrolnet. In European Conference on Computer Vision (ECCV), 2024. Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu, Z. Wu, T. Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M√ºller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. C. Fifty, R. G. Junkins, D. Duan, A. Iger, J. W. Liu, E. Amid, S. Thrun, and C. R√©. Restructuring vector quantization with the rotation trick. arXiv preprint arXiv:2410.06424, 2024. M. Garland and P. S. Heckbert. Surface simplification using quadric error metrics. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, 1997. J.-B. Grill, F. Strub, F. Altch√©, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to selfsupervised learning. In Advances in Neural Information Processing Systems, 2020. S. Haykin. Communication systems. John Wiley & Sons, 2008. J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. M. Huh, B. Cheung, P. Agrawal, and P. Isola. Straightening out the straight-through estimator: Overcoming optimization challenges in vector quantized networks. In International Conference on Machine Learning (ICML), 2023. A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning (ICML), 2021. 16 D. P. Kingma, M. Welling, et al. Auto-encoding variational bayes, 2013. W. Li, J. Liu, H. Yan, R. Chen, Y. Liang, X. Chen, P. Tan, and X. Long. Craftsman3d: High-fidelity mesh generation with 3d native generation and interactive geometry refiner. arXiv preprint arXiv:2405.14979, 2024. Y. Li, Z.-X. Zou, Z. Liu, D. Wang, Y. Liang, Z. Yu, X. Liu, Y.-C. Guo, D. Liang, W. Ouyang, et al. Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. W. E. Lorensen and H. E. Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision (ECCV), 2020. M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. S. Stojanov, A. Thai, and J. M. Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Y. Takida, T. Shibuya, W. Liao, C.-H. Lai, J. Ohmura, T. Uesaka, N. Murata, S. Takahashi, T. Kumakura, and Y. Mitsufuji. Sq-vae: Variational bayes on discrete representation with self-annealed stochastic quantization. arXiv preprint arXiv:2205.07547, 2022. C. Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. A. van den Oord, O. Vinyals, and k. kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems, 2017. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, ≈Å. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. 17 J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. B. Zhang, J. Tang, M. Niessner, and P. Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. In ACM Transactions on Graphics (TOG), 2023a. B. Zhang, W. Zheng, J. Zhou, and J. Lu. Preventing local pitfalls in vector quantization via optimal transport. arXiv preprint arXiv:2412.15195, 2024. J. Zhang, F. Zhan, C. Theobalt, and S. Lu. Regularized vector quantization for tokenized image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023b. Z. Zhao, W. Liu, X. Chen, X. Zeng, R. Wang, P. Cheng, B. Fu, T. Chen, G. Yu, and S. Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Z. Zhao, Z. Lai, Q. Lin, Y. Zhao, H. Liu, S. Yang, Y. Feng, M. Yang, S. Zhang, X. Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021."
        },
        {
            "title": "Appendix",
            "content": "A. Contributions and Acknowledgments Team Leads Kiran Bhat (Overall) Nishchaie Khanna (Infrastructure) Karun Channa (Product) Tinghui Zhou (Modeling) Contributors Alexander Weiss Christian Reiser Ravi Kiran Chirravuri Ravali Kandur Alejandro Pelaez Akash Garg Michael Palleschi Jessica Wang Skylar Litz Leon Liu Anying Li David Harmon Derek Liu Liangjun Feng Denis Goupil Lukas Kuczynski Jihyun Yoon Naveen Marri Peiye Zhuang Yinan Zhang Brian Yin Haomiao Jiang Marcel van Workum Thomas Lane Bryce Erickson Salil Pathare Kyle Price Core Contributors Yiheng Zhu Xiaoxia Sun Charles Shang Anirudh Sudarshan Maurice Chu Daiqing Li Kangle Deng Jean-Philippe Fauconnier Tijmen Verhulsdonck Research Advisors Maneesh Agrawala Kayvon Fatahalian"
        },
        {
            "title": "Executive Advisors\nAnupam Singh\nDavid Baszucki",
            "content": ""
        }
    ],
    "affiliations": [
        "Foundation AI team, Roblox",
        "Roblox"
    ]
}