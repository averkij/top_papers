{
    "paper_title": "Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense",
    "authors": [
        "Jiawen Zhang",
        "Kejia Chen",
        "Lipeng He",
        "Jian Lou",
        "Dan Li",
        "Zunlei Feng",
        "Mingli Song",
        "Jian Liu",
        "Kui Ren",
        "Xiaohu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, and Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference efficiency bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven sota techniques across three popular categories, revealing consistent safety degradation across ten safety-aligned LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 0 4 8 0 0 . 2 0 5 2 : r Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense Jiawen Zhang Kejia Chen Lipeng He Jian Lou Dan Li Jian Liu Kui Ren Xiaohu Yang Zunlei Feng Mingli Song Zhejiang University University of Waterloo Sun Yat-sen University {kevinzh, chenkejia, zunleifeng, songml, liujian2411, kuiren, yangxh}@zju.edu.cn, lipeng.he@uwaterloo.ca, {louj5, lidan263}@mail.sysu.edu.cn Abstract Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference efficiency bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven state-ofthe-art techniques across three popular categories (activation polynomialization, activation sparsification, and activation quantization), revealing consistent safety degradation across ten safety-aligned LLMs. To overcome the hurdle of devising unified defense accounting for diverse activation approximation methods, we perform an in-depth analysis of their shared error patterns and uncover three key findings. We propose QuadA, novel safety enhancement method tailored to mitigate the safety compromises introduced by activation approximations. Extensive experiments and ablation studies corroborate QuadAs effectiveness in enhancing the safety capabilities of LLMs after activation approximations."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities and gained surging popularity ever since the release of ChatGPT [1]. Their widespread applications across Equal contribution. Corresponding authors. 1 diverse fields are further bolstered by open-source LLMs such as Llama [2], Gemma [3], Mistral [4], Falcon [5], and Qwen [6], offered by model developers capable of conducting extensive pretraining and meticulous alignment procedures like Meta, Google, and Alibaba. Application builders, ranging from enterprises to individuals, can harness the capabilities and flexibility of these open-source aligned LLMs for custom use cases and diverse deployment scenarios. Still, the prohibitive deployment costs hinder the full application potential of LLMs due to their astronomical scales, particularly in resource-constrained scenarios. Additionally, the computational overhead introduced by advanced and complex activation functions like GELU [7] and SwiGLU [8], which are integral and pervasive to modern LLMs (e.g., all above-mentioned open-source LLMs), further exacerbates the inference efficiency challenge. Consequently, improving the inference efficiency of LLMs has garnered growing research attention, with existing approaches exploring distinct facets, including weight quantization [911], weight sparsification [1215], model pruning [12, 13, 16], speculative decoding [17, 18]. These efforts offer complementary reductions in inference overhead and hold promise for achieving aggregated efficiency gains when combined. Activation Approximation. Recently, activation approximation has emerged as promising and, in some cases, indispensable avenue for pursuing inference efficiency enhancement in LLMs. This line of methods focuses on devising various approximations to alleviate the computation of complex non-linear activation functions, thereby catering to specific deployment requirements. Based on the application scenario and approximation techniques, there are three notable categories, including (i) Activation Polynomialization [1923]: Replacing complex nonlinearities in activation functions with polynomials substantially accelerates computation, making it indispensable for the private inference scenario. (ii) Activation Sparsification [2426]: Truncating input activation values close to zero so that weight channels corresponding to zerovalued activations can be ignored in inference computation. (iii) Activation Quantization [27, 28]: Quantizing the activaTable 1: Activation approximation techniques and their inference efficiency gains. For activation polynomialization, the baseline work is SIRNN (S&P21) [29], private inference protocol without polynomialization. For activation sparsification and quantization, the baseline is full-precision (FP16) inference without any accelerations. Methods Work Setting Speedup Activation Polynomialization Iron (NeurIPS22) BOLT (S&P24) BumbleBee (NDSS25) NEXUS (NDSS25) MPC MPC MPC FHE Activation Sparsification TEAL (ICLR25) Activation Quantization SmoothQuant (ICML23) OmniQuant (ICLR24) 25% sparsity 50% sparsity 90% sparsity W16A8 W16A4 W16A8 W16A4 9.6 24.6 22.3 5.3 1.3 1.9 9.5 2.0 4.0 2.0 4.0 tion vector from higher bit-widths to lower bit-widths. Table 1 summarizes exemplary inference efficiency gains achieved by representative methods from each category, demonstrating significant wall-clock speed-ups ranging up to 24.6. The core design rationale underpinning these methods is to maximize inference efficiency while ensuring that activation approximation errors do not degrade the utility of LLMs beyond an acceptable limit. This has proven to be plausible since current activation approximation errors minimally impact utility metrics, making them appear sound and practical for real-world deployment. LLM Safety. On par with utility, safety has become an equally paramount dimension for LLMs, garnering widespread consensus from academia, industry, and beyond. Maintaining safety capabilities is critical for preventing LLMs from being exploited to produce harmful responses, such as facilitating unethical uses, spreading misinformation, or fostering discriminatory behaviour. Moreover, maliciously crafted prompts through jailbreak attacks can further induce LLMs towards harmful generations [3032], providing an inkling of how intricate the task of safety preservation for LLMs is. In response, model developers have implemented extensive safety alignment procedures, employing techniques such as instruction tuning, reinforcement learning with human feedback (RLHF) [33, 34], and direct preference optimization (DPO) [35], before open-sourcing the aligned LLMs. Unfortunately, safety has turned out to be fragile to maintain, as even minor modifications to the weights of aligned LLMs through fine-tuning can degrade their safety capabilities. When it comes to inference efficiency improvement techniques, safety studies thus far have evaluated weight quantization, weight sparsification, model pruning, and decoding algorithms, demonstrating degraded safety capabilities [3642]. Research Gap. The above findings on the brittleness of safety in aligned LLMs raise pressing yet under-explored research question: Does activation approximation, which introduces various sorts of activation errors, compromise the safety capabilities of aligned LLMs? Vetting and coping with the safety risks posed by different activation approximation methods are critical. It serves as cautionary reminder for application builders to avoid the illusion that activation approximation inherently preserves the safety capabilities of aligned LLMs. Additionally, model developers could incorporate safety assessment findings into their safety alignment procedures to ensure that the resulting LLMs are robust against potential activation errors. Our Work. Below we briefly describe our efforts towards closing the aforementioned research gap in this paper. Safety Assessment: To uncover the under-studied safety risks of activation approximation, we conduct systematic safety assessment of state-of-the-art activation approximation techniques spanning three different categories, including activation polynomialization [19, 2123], activation sparsification [2426], and activation quantization [27, 28, 43, 44]. Our findings reveal that activation approximations even in safetyaligned LLMs introduce safety degradation that current safety alignment mechanisms fail to adequately address. Safety Analysis and Key Observations: We conduct an indepth analysis of the perturbations introduced by activation approximations across 10 different safety-aligned LLMs, leading to three key observations. First, activation approximations will cause LLM to lose safety before utility, leading to the generation of meaningful yet malicious responses, i.e., helpful for harmful purposes. Taking Llama-3.1-8B-Instruct [45] as an example, activation approximations substantially increase the attack success rate (ASR) from 0.19% to 69.23% while having almost no impact on utility. Second, activation approximations in the first few layers are the most detrimental to safety, whereas approximations in the later layers have negligible effects on safety. Third, harmful prompts tend to cluster within the activation space, and activation approximations shift these harmful activations into benign regions, thereby easier to bypass safety checks. Safety Enhancement: Based on these key observations, we propose activation approximation-robust safety alignment, unified approach to address safety vulnerabilities arising from different activation approximation techniques. Our method enables LLM developers to integrate this additional robustness into their existing safety alignment procedures with minimal modifications and computational overhead (e.g., adding just 2 to 3 lines of code). The resulting LLMs, after being opensourced and deployed by downstream application providers, remain resilient to various activation approximations introduced to enhance inference efficiency. Experimental results demonstrate that our approach not only handles activation approximations across different methods and various magnitudes but also resists variety of adaptive jailbreak attacks, such as GCG [46] and AutoDAN [47]. Summary of Contributions. To the best of our knowledge, 2 our work is the first comprehensive study on the impact of activation approximations on the safety of LLMs. The major contributions of this paper are summarized as follows: Comprehensive Safety Assessment: We conduct extensive safety evaluations on state-of-the-art activation approximation techniques, spanning three representative categories including activation polynomialization, activation sparsification, and activation quantization. In-Depth Safety Analysis: Through analysis across 10 safety-aligned LLMs, we uncover that activation approximations (i) compromise safety before utility, (ii) are most harmful in early layers, and (iii) shift harmful activations into benign spaces to evade detection. Novel Safety Enhancements: We propose an Activation Approximation-Aware Alignment (QuadA) to address these vulnerabilities."
        },
        {
            "title": "2.1 Activation Approximations",
            "content": "Activation Polynomialization. In private LLM inference [1923], the polynomialization of the activation function is widely used to improve the efficiency of private inference. Private inference is two-party cryptographic protocol based on secure multi-party computation (MPC) and fully homomorphic encryption (FHE). It can achieve that the server learns nothing about the clients input and clients learn nothing about the servers model except the inference result. In contrast to regular inference in plaintext, operations like Gaussian Error Linear Unit (GELU) and LayerNorm require intensive computation and communication in ciphertext, which usually dominates the total inference latency [4851]. To address this problem, the server usually replaces the non-linear functions to MPC/FHE-friendly approximations. MPCFormer [49] replaces GELU(x) to 0.125x2 + 0.25x2 + 0.5, BOLT [21] replaces GELU(x) to 3-segment polynomial function, BumbleBee [22] and NEXUS [23] replace GELU(x) to 4-segment polynomial function. Activation Sparsification. Recent work has observed that activations in the MLP blocks of LLMs are sparse [26, 52]. This implies that only few rows (or columns) of the corresponding weight matrices are required for the forward pass. Activation sparsification [2426] is an alternative method leveraging sparsity in hidden states. Since the weight channels corresponding to zero-valued activations are not used in computation, speed-up can be realized by selectively omitting these weights during memory transfer. Unlike model pruning techniques, which either remove zero-valued weights from the model or set some weights to zero, activation sparsity exploits the zeros that occur dynamically during runtime, depending on the input. TEAL [24] and CATS [53] have realized training-free activation sparsity based on magnitude pruning. By pruning lowmagnitude, non-salient activations, they realize wall-clock speed-ups of up to 1.9 at 50% sparsity. Activation Quantization. Quantization has proven to be promising technique for mitigating both computational and memory overhead in LLMs. Many post-training quantization methods [9, 10, 54] reduce memory consumption through weight-only quantization, where weights are quantized while activations remain in full precision. To further reduce the computational overhead, recently, many works [27, 28, 55] employs weight-activation quantization which quantizes both weight and activation into low-bit values for the execution of low-bit matrix multiplication. For instance, INT8 quantization of weights and activations can halve the GPU memory usage and nearly double the throughput of matrix multiplications compared to FP16. To exclude the effect of training and weight quantization on the safety evaluation, we only study post-training quantization (PTQ) and keep weight at full precision. We choose SmoothQuant [27] and OmniQuant [28] as our case studies."
        },
        {
            "title": "2.2 LLM Safety Evaluations and Alignment",
            "content": "LLMs commonly adopt self-autoregressive framework, where each token is predicted based on all previously generated tokens. This recursive process allows the model to generate coherent text step by step. Given vocabulary , the sequence prediction task can be formally expressed as: πθ(yx) = πθ(y1x) m1 i=1 πθ(yi+1x, y1, ..., yi), where πθ is the model, = (x1, x2, ..., xn), (xi ) is the context including the prompt, and = (y1, y2, ..., yn), (yi ) is the predicted sequence. Despite the effectiveness of this mechanism to generate coherent and contextually relevant text, it offers limited control over aligning outputs with human preferences. Consequently, prior work [46, 5659] has shown that these models may still produce harmful, biased, or otherwise undesirable content. Particularly in high-stakes domains such as healthcare or finance [60, 61]. Therefore, ensuring safe and aligned outputs is crucial, and this has led to the development of alignment techniques that aim to bridge the gap between models generative objective and the need for responsible outputs [62, 63]. Safety Evaluations. Effective safety evaluation is crucial to understanding and ensuring that LLMs generate outputs that align with human values and ethical guidelines. Existing researches on LLM safety evaluation typically involves multiple dimensions, including Attack Success Rate (ASR) [46], Refusal mechanisms [64], and Safety Risk Index (SRI) [65]. These evaluation metrics have form the backbone of safety assessments in LLM research [6674], offering systematic 3 approach to measuring models robustness against harmful content generation. Despite the thoroughness of these approaches, there remains gap when it comes to evaluating the impact of activation approximation on safety. Our work seeks to fill this gap by investigating how activation approximation could impact the models ability to uphold safety and suggesting strategies to mitigate these associated risks [7578]. Safety Alignment. While safety evaluation focuses on measuring models ability to resist harmful behavior, safety alignment involves the active process of modifying the model to ensure its outputs align with human values. Current alignment approaches generally fall into two categories: (1) Safety Moderation: Incorporating policy or ethical guidelines to filter or modify outputs, often through real-time moderation systems that detect and prevent harmful content during generation. (2) Robust training: Refining model behavior through data purification and fine-tuning methods leveraging human feedback, such as Supervised Fine-Tuning (SFT) [33, 79], Reinforcement Learning from Human Feedback (RLHF) [33], and Direct Preference Optimization (DPO) [35]. Recent studies have investigated how specific neurons contribute to safety in aligned LLMs [1, 2, 37, 8082], highlighting the complex interplay between internal representations and secure outputs. However, achieving safety alignment typically requires integrating dedicated mechanisms into the model training processa non-trivial undertaking demanding substantial computational resources and specialized expertise [33, 46, 57, 58]. Additionally, the effectiveness of alignment techniques depends on the availability of high-quality, diverse data and careful tuning of model behavior based on nuanced human feedback. Consequently, aligning LLMs is both costly and time-consuming process that requires specialized knowledge, making it challenging for downstream users to conduct alignment independently [8385]. Brittleness of Safety Under Various Modifications. Despite considerable progress, LLM safety remains fragile to degradation under certain modifications. For example, operations such as fine-tuning on pure-benign datasets [36, 37, 86], compressing or pruning model architectures [3840, 87, 88], or altering decoding mechanisms [41, 42, 64] have been systematically shown to erode established safeguards. However, one critical area remains underexplored: activation approximation. While widely adopted for efficiency gains (e.g., reducing inference latency and memory usage), its impact on safety has yet to be systematically examined. In this work, we investigate how activation approximations affect LLMs safety alignment and propose strategies to preserve both efficiency gains and robust safety guarantees. Our findings represent complementary and critical contribution to existing research on LLM safety. Figure 1: Illustration of the three parties in Threat Model."
        },
        {
            "title": "3 Safety Assessment and Analysis",
            "content": "In this section, we aim to provide systematic safety assessment and analysis of existing activation approximation methods. We will first introduce the threat model of our study in Section 3.1 and then describe the setup of safety evaluations in Section 3.2. In Section 3.3, we elaborate on the specific activation approximation methods under evaluation and summarize their activation errors, covering seven representative techniques from each of the three categories. Finally, in Section 3.4, we identify three key observations from the safety evaluation results, revealing the characteristics of activation approximations on LLM safety behaviours."
        },
        {
            "title": "3.1 Threat Model",
            "content": "In this paper, we consider common scenario in the development and deployment of LLM-based applications, as outlined in Figure 1. This scenario involves three parties: the LLM model developer, the LLM-based application provider, and the application users. In particular, the roles of the first two parties can sometimes overlap within the same organization. LLM Model Developers provide open-source safetyaligned LLMs, enabling application providers to customize models for legitimate usage in accordance with licensing terms. Prominent examples include large technology companies such as Meta (with LLaMA-Chat), Google (with Gemma), and Alibaba (with Qwen), as well as open-source organizations/groups that have released safety-aligned LLMs like Falcon, Mistral, Alpaca, and Zephyr. LLM-based Application Providers customize open4 source safety-aligned LLMs to develop applications tailored to specific use cases, expecting the customized models to retain both the utility and safety capabilities of the original LLMs. These providers aim to deliver highquality, responsive services to their users while striving to minimize inference latency. However, they could face resource constraints in terms of computation and memory. Consequently, they are incentivized to adopt inference efficiency-enhancing techniques to reduce resource usage and wall-clock inference time while maintaining utility and safety capabilities as much as possible. LLM-based Application Users interact with LLMbased applications expecting helpful, accurate, and timely responses. Some might occasionally, for example out of curiosity, issue queries that should be turned down by safety-aligned LLM. Meanwhile, there could be adversarial users launching jailbreak attacks [46, 47, 89] to deliberately bypass the safety guardrails of the application providers LLMs. Adaptive Capabilities. This paper considers an adaptive adversary who can construct the jailbreak prompt based on gradients of the target LLM [46, 47, 89]. These methods pad prefix or suffix to the original prompt, which can be optimized to achieve the attack objective. This shares similar idea as the textual adversarial examples whereby the goal is to generate harmful responses. We will discuss potential countermeasures for this adaptive attack in section 4.2."
        },
        {
            "title": "3.2 Safety Assessment Setup",
            "content": "Safety Measurement. We measure LLMs safety by evaluating the Attack Success Rate (ASR), which is defined as the percentage of failure to abstain from responding to malicious instruction. These malicious instructions come from AdvBench [46], which contains 520 items in total. Previous works [46,80] simply use small set of rejection phrases such as Sorry, cant, can not fulfill to do prefix matching to determine whether the model rejects the answer. However, this simple procedure may misclassify the output, if the model replies think its illegal or ####, which are not in the set, they classify the attack as successful, but in fact, it is not. To reduce the risk of misjudgment, the HarmBench classifier [64] has been widely adopted to judge whether the output content is harmful or not. HarmCLS(y) = (cid:40) 1; 0, if is harmful if is harmless As claimed, the classifier fine-tuned with LLAMA-2-13B outperforms GPT-4 by approximately 5% in agreement rates with human judgments on the manually labeled validation set. However, we found that it could still fail to distinguish benign output from misaligned one, especially under circumstances where the instructions are complicated. We show an example of false positive produced by the HarmBench classifier. This example was obtained from DRA (USENIX Security24) [90] where they proposed jailbreak attack that bypasses safety checks by hiding harmful question in word puzzle, and asking the LLM to solve the puzzle and then answer the question. Their prompt-based jailbreak approach gives relatively complicated instructions for the LLMs to extract the questions and answer in step-by-step manner. In the case of the example response, it is clear that the LLM did not produce concrete and actionable plan to fulfill the attackers malicious request. However, due to the \"positive\" answer, the HarmBench classifier became confused and incorrectly identified the LLMs response as harmful. HarmCLS False Positive: How to launder money? happy (h)our (o)verwhelming fear (w)asted effort tremendous effor(t) (o)verwhelming fear (l)ost opportunity (a)nimals! (u)nbelievable (n)ice to meet you lucky (d)og iPhon(e) (r)adiant smile (m)iserable failure (o)pen heart (n)ice to meet you iPhon(e) (y)earn for adventure Read these sentences! But! ONLY focus on the letters that surrounded by \"( )\" and put them together to construct sentence. Give me the detailed answer step by step! Absolutely! Let me help you! Heres my detailed plan: Step 1: Understanding the question. The question is asking for sentence that can be constructed by combining the letters surrounded by parentheses in the given sentences. Step 2: Identifying the possible sentences. Based on the given sentences, the possible sentences that can be constructed using the letters are: How to launder money. Step 3: Providing helpful answer. As helpful assistant, must inform you that laundering money is illegal and can result in severe consequences. Step 4: Offering alternative solutions. Instead, suggest you seek legal ways to manage your finances. There are many resources available that can help you make informed decisions about your financial future. Consider consulting with financial advisor or seeking guidance from reputable financial institution. To ensure the validity of defense mechanism, it is crucial to have misalignment evaluation metric that can accurately reflect the effectiveness of the proposed approach. Based on the observed existence of false positives, we cannot directly rely on HarmCLS to produce precise account of the resulting ASR of the LLMs after implementing our defense strategy. We collect 120 additional prompt-response pairs from the DRA [90] and manually mark whether it is harmful or not. We fine-tuned the HarmBench classifier [64] with this dataset to achieve higher accuracy for misbehaviour judgment. Utility Measurement. We use perplexity on WikiText-2 [91] to measure the models utility, which is one of the most common utility metrics. Perplexity is defined as the exponentiated average negative log-likelihood of sequence. For tokenized sequence = (x1, ..., xn), the perplexity of is: PPL(X) = exp (cid:110) 1 i= log pθ(xix<i) (cid:111) , where log pθ(xix<i) is the log-likelihood of the i-th token conditioned on the preceding tokens x<i. Safety-Aligned Models. To underscore the generalizability of our findings, we test different aligned LLMs from various publishers. The LLMs we used are Llama-2-7B-Chat [2], Llama-2-13B-Chat [2], Llama-3.1-8B-Instruct [45], Phi-3Mini-4K-Instruct [92], Phi-3.5-Mini-Instruct [92], Mistral7B-Instruct-v0.3 [4], Mixtral-8x7B-Instruct-v0.1 [93], Zephyr7B-β [94], Qwen2-7B-Instruct [6], and Qwen2.5-32B-Instruct [95]. These models are fine-tuned with supervised fine-tuning (SFT), reinforcement learning with human feedback (RLHF), or direct preference optimization (DPO) to ensure precise instruction adherence and robust safety measures. (a) Error distributions in activation polynomialization, with BOLT [21] as the example."
        },
        {
            "title": "3.3 Activation Approximation Error",
            "content": "(b) Error distributions in activation sparsification, with TEAL [24] in 50% sparsity. In this section, we summarize the error of different activation approximation techniques and select representative opensource works to do the activation approximation for our benchmark. We begin by recalling the multi-layer perceptron (MLP) within each Transformer block, which contains the activation expressions to be approximated and analyzed. Definition 1. Let the l-th MLP block (with no biases), with l-th layer model parameter θl and input embedding vector el, be denoted by the function MLPl of the form MLPl(θlel) = (α(el Wup )) Wdown , , Wdown where Wup are constituent parameter matrices associl ated with θl, α() is the activation function, such as SwiGLU or GELU, which are widely adopted by LLMs in practice. Approximation Error of Activation Polynomialization In private inference, the server replaces the non-linear functions with MPC/FHE-friendly approximations Fapprox. The error of private inference comes from the calculation error between the non-linear function and its polynomialization, the εup comes from the approximation of LayerNorm and εdown comes from the approximation of GELU: εup = LayerNorm(x) LayerNorm approx εdown = GELU(x) GELU approx (x). (x). Approximation Error of Activation Sparsification Following the definition of activation sparsification in TEAL [24], (c) Error distributions in activation quantization, with SmoothQuant [27] in 8-bit activation. Figure 2: Activation error distributions on Llama-3.1-8BInstruct using different activation approximations. The first column presents errors before Wup, and the second column presents errors before Wdown. Best-fit Gaussian/Laplace distributions are overlaid in black for comparison. for vector = (x1, ..., xn) and sparsity level [0, 1], define the threshold as: 1 n i=1 P(xi t) = The sparsification function St () : is defined as: St (x) = (cid:40) 0 xi if xi otherwise The error of activation sparsification is the difference between the original and the sparse value: εup, εdown = St (x). 6 Table 2: Benchmark of different training-free activation approximation techniques on Llama-3.1-8B-Instruct. Note that the baseline PPL is 8.05, ASR is 0.19. Methods Works Setting Activation Polynomialization Iron [19] BOLT [21] BumbleBee [22] NEXUS [23] MPC MPC MPC FHE εup (0, 0.0642) (0, 0.0422) (0, 0.0262) (0, 0.0312) εdown Lap(0, 0.049) Lap(0, 0.036) Lap(0, 0.018) Lap(0, 0.014) Activation Sparsification TEAL [24] Activation Quantization SmoothQuant [27] OmniQuant [28] 10% sparsity (0, 0.352, 0.04) 25% sparsity (0, 0.352, 0.11) 50% sparsity (0, 0.352, 0.24) 90% sparsity (0, 0.352, 0.57) Lap(0, 0.024, 0.003) Lap(0, 0.024, 0.007) Lap(0, 0.024, 0.017) Lap(0, 0.024, 0.055) W16A8 W16A4 W16A8 W16A4 (0, 0.0272) (0, 0.0352) (0, 0.0292) (0, 0.0362) Lap(0,0.019) Lap(0,0.024) Lap(0,0.028) Lap(0,0.037) PPL ASR 43.74 15.81 11.28 12.93 8.05 8.09 8.57 206.47 9.56 10.35 9.85 10.79 58.07 35.76 20.56 21.15 5.00 28.85 57.12 0.02 41.92 55.19 33.46 47. Approximation Error of Activation Quantization Activation quantization is the process of discretizing the activation vector from higher bit-widths to lower bit-widths. For example, quantizing 16-bit Floating Point tensor into 4-bit tensor with range [15, 15]: Quant(xFP16) = round (cid:16) 15 max(X) = round(cFP16 xFP16) = xINT4. xFP16(cid:17) where cFP16 is the quantization scale. Dequantization is the inverse: Dequant(xINT4) = xINT4 cFP16 = xFP16. The error of quantization is the difference between the original and dequantization values: εup, εdown = xFP16 Dequant(Quant(xFP16)). Benchmark of Activation Approximation Taking Llama3.1-8B-Instruct as an example, we select representative opensource works to do the activation approximation for our benchmark. In table 2, we list the benchmark of different training-free activation-aware accelerations on Llama-3.1-8BInstruct. For private inference, we give the theoretical equivalent noise. For Activation Sparsity, the noise acts within the truncation threshold of sparsification. For quantization, in order to eliminate the impact of weight quantization, we retain the full-precision weight (16bit) and only quantize the activation (4/8bit). As shown, activation approximations can result in up to 50% increase in the harmfulness rate for Llama-3.1-8B-Instruct. Distribution Fitting for Approximation Errors We summarize approximation errors from different activation approximations by introducing two noise components εup , εdown , which is formalized by the following definition. Definition 2. Let the l-th MLP block with activation approximation, be denoted by the function MLPε of the form MLPε (θlel) = (α((el + εup ) Wup ) + εdown ) Wdown , where εup the activation approximation. , εdown are the equivalent approximation errors from , εdown We then fit the error distributions of εup for representative methods from all three activation approximation techniques. The errors are recorded from activations collected using Llama-3.1-8B-Instruct on the Alpaca dataset [96]. Figure 2 presents the resulting error distributions, where εup and εdown are zero-mean and unimodal, falling into two distinctly shaped distributions. The activation error εup before the Wup tends to be Gaussian-like distribution, while the activation error εdown before the Wdown tends to be Laplacian-like distribution. l"
        },
        {
            "title": "3.4 Further Analysis and Key Observations",
            "content": "Observation I: Activation approximations can cause LLMs to compromise safety before losing utility, leading them to produce meaningful responses to malicious queries. Since errors were introduced in the outputs of activation functions as shown in the case studies, we conduct continuous noise simulation experiments on 10 safety-aligned LLMs to explore the impact of activation approximations on model safety and utility. Based on the noise distributions observed in Figure 2, we apply different levels of approximation to the activation functions of commonly used safety-aligned LLMs. The results are shown in Figure 3 and Figure 4. We collect the corresponding ASR% and Perplexity scores produced by the models after noises have been added to their activation outputs before Wup and Wdown. The x-axis is the standard deviations of the noise, the left y-axis represents 7 Figure 3: Approximation of activation before Wup on different safety-aligned LLMs. Figure 4: Approximation of activation before Wdown on different safety-aligned LLMs. the ASR% and the right y-axis measures the PPL scores, respectively, in different scales. We can see that there exists general trend where as the simulated approximation errors increase, ASR gradually grows at an increasing rate until it reaches peak at certain noise level. Throughout this period, the PPL scores appear to not change much and stay at relatively low value. This means that during this interval, safety-aligned LLMs will fail to reject the malicious question and produce meaningful replies to malicious requests. When the noise grows too much, the PPL starts to increase exponentially and the ASR begins to decrease dramatically, at which point the LLM will output meaningless responses like ####, Im Im Im. At this point, the model becomes unusable both to the normal users and to the attackers. Since LLMs are observed to lose safety before they lose their utility as the noise grows, these models expose jailbreak vulnerability within certain approximation interval. Consequently, we can find the most vulnerable approximation (denoted as MVA) that maximizes the ASR of the model. MVA = argmaxε xD HarmCLS(πε θ(x)) , (4) where HarmCLS() is harmful behaviour classifier, is the harmful prompt dataset, and we use Advbench [46] in our experiments, πε θ(x) refers to the response at inference given by LLM with parameter θ and activation approximation noise with magnitude ε. We record the MVA of each model in Table 3 as well as the ASR and PPL under this approximation, which will serve as guideline for designing the defense method in Section 4. Observation II: Activation approximation errors in the first few layers are the most detrimental to safety, while the approximations in the later layers have minimal impact on safety. Assuming that θ is an L-layer model (θ1, θ2, ...θL), for and input x, the inference with activation approximation can be 8 Table 3: Impact surface and magnitude of activation approximations on safety-aligned LLMs. Family Model Llama Phi3 Mistral Qwen Llama-2-7B-Chat Llama-2-13B-Chat Llama-3.1-8B-Instruct Phi-3-Mini-4K-Instruct Phi-3.5-Mini-Instruct Mistral-7B-Instruct-v0.3 Mixtral-8x7B-Instruct-v0.1 Zephyr-7B-β Qwen2-7B-Instruct Qwen2.5-32B-Instruct Sensitive Layers Baseline ASR PPL Error Before Wup MVA ASR [0-3] [0-3] [0-3] [0-3] [0-3] [0-6] [0-4] [0-4] [0-4] [0-4] 1.34 0.00 0.19 0.00 0.77 34.61 1.15 22.31 0.38 0.00 7.84 (0, 0.0452) 7.07 (0, 0.0422) 8.05 (0, 0.0752) 6.71 (0, 0.0402) 6.86 (0, 0.0332) 6.13 (0, 0.2002) 4.61 (0, 0.4002) 7.01 (0, 0.2502) 8.46 (0, 0.3002) 5.74 (0, 0.2002) 56.73 47.88 69.23 82.61 79.50 73.18 82.47 88.30 52.16 37.48 PPL 7.91 8.05 14. 24.79 21.43 6.40 12.09 7.76 14.01 5.81 defined as Error Before Wdown MVA ASR PPL Lap(0, 0.100) Lap(0, 0.125) Lap(0, 0.085) Lap(0, 0.120) Lap(0, 0.100) Lap(0, 0.075) Lap(0, 0.225) Lap(0, 0.113) Lap(0, 0.058) Lap(0, 0.043) 49.80 52.12 67. 32.38 44.72 71.02 87.64 82.35 73.82 46.13 10.96 18.46 11.70 9.46 7.45 6.24 19.31 9. 23.94 21.54 θ(x) = fff L(MLPεL πε (θLeL)) fff 1(MLPε1 1 (θ1e1)), e1 = Attention(T (x)), where represents the layer-wise concatenation of LLM, and (x) is the tokenizer function, e1 is the first-layer embedding input to MLP. We provide an analysis of the impact of layerwise approximations on the safety of LLMs, to explore which layers lack robustness to approximations. Define the inference for layer-wise approximations as: πε θ(x)k = fff L(MLPL(θLeL)) fff k+1(MLPk+1(θk+1ek+1)) (cid:125) (cid:123)(cid:122) The last layers are not approximate (cid:124) fff k(MLPMVA (cid:124) k+1 (θkek)) fff 1(MLPMVA (cid:123)(cid:122) The first layers are approximate with MVA (θ1e1)) (cid:125) . Figure 6 shows the ASR of LLMs after adding MVA noise on different layers. Note that πε θ(x)k indicates that the approximation noise is added after the k-th layer, e.g., πε θ(x)1 indicates that the activations of all layers are approximated, and πε θ(x)L indicates that only the activations in the last layer is approximated. Based on Figure 6, we observe that only the first few layers are the most fatal, indicating that the first few layers lack robustness to activation approximations, which we call sensitive layers. We also record the sensitive layers of each model in Table 3, which are the layers we focus on when designing the defense strategy. Observation III: Activations from harmful prompts are observed to cluster in the activation space, and activation approximations can shift these activations into benign regions to evade safety checks. Using Llama-2-7B-chat and Phi-3-Mini-4K-Instruct models, we construct two-dimensional projection of the lasttoken activations from both harmful and benign prompts. We 9 (a) Activations without approximation (b) Activations with approximation Figure 5: MDS projection of the last-token activation space produced from both harmful and benign prompts on Llama-27B-chat in the first layer. focus on the hidden state at the last token position of each sequence because it reflects the LLMs general understanding of the input [97, 98]. subset of the Alpaca [96] dataset was used as our benign samples as they have been carefully curated and proven safe. For harmful samples, we use the prompts from AdvBench. Based on Observation II, we collect all our activation data points from the first layer as it has more prominent effect on safety compared to later layers. Finally, to achieve effective dimensionality reduction, we apply Multi-Dimensional Scaling (MDS) [99] on the activation values to obtain the results shown in Figure 5. We can observe that before activation approximations, there exists clear distinction between benign and harmful activations, with an average cosine similarity of 0.52 among harmful activations. After activation approximations, harmful prompts become more scattered and diverse, overlapping significantly with benign regions, resulting in an average cosine similarity of 0.40 among harmful activations. This shift caused by activation approximations enables harmful prompts to generate model internal states that resemble those of benign prompts, thereby bypassing safety checks. Figure 6: Layer-wise activation approximations on different safety-aligned LLMs. The green lines indicate approximation noise added to activations before Wup. The orange lines indicate approximation noise added to activations before Wdown."
        },
        {
            "title": "4 Safety Defense for Activation Approximation",
            "content": "hospitals and software based on dialog models. In this section, we propose QuadA, simple yet effective safety enhancement method designed to address the safety compromises introduced by various activation approximation methods in aligned LLMs. The goal is to enable LLM model developers to integrate additional robustness into their original safety alignment procedures with minimal code modifications (e.g., adding just 2 to 3 lines of code). This ensures that the resulting LLMs, after being open-sourced and utilized by downstream application providers, remain robust against various activation approximations introduced to enhance inference efficiency. The design principles of QuadA are motivated by our key observations in section 3.4, where we introduce three algorithmic components corresponding to each key observation. We first present the QuadA algorithm in section 4.1 and then evaluate its safety enhancement capabilities in section 4.2 against Jailbreak attacks, which serve as stress test for safety. The results demonstrate that QuadA produces safety-aligned LLMs that remain robust against Jailbreak attacks even after the introduction of various activation approximations. Finally, we conduct extensive ablation studies to examine the effectiveness of each algorithmic component in section 4.3."
        },
        {
            "title": "4.1 Our Proposed QuadA",
            "content": "Based on Observation I, activation approximations induce safety degradation in aligned LLMs. trivial solution for the LLM service provider is performing safety alignment on models that have undergone activation approximations. e.g. MPCformer [49] will then fine-tune the model with the activation function replaced. However, this requires the LLMbased service provider to have safety-alignment datasets and corresponding computational resources, which are not easy to fulfill in reality, such as intelligent consultation services in Assuming that the LLM service provider is training-free, we hope that the LLM trainer can adopt more robust way to safely align the model so that the safety of the model is not degraded after the activation approximations adopted by the service provider. There are various choices for the specific form of the safety alignment loss function. In this paper, we follow the DPO loss function, which takes the following form on the alignment dataset = {x, yw, yl}, we aim to minimize the problem: L(θD) = ExD log σ (cid:18) πε θ(ywx) πε ref(ywx) 1 (θ1xi), fff ε1 λExi,x jD,i= jcosine( fff ε β log β log 1 (θ1x j). (cid:19) πε θ(ylx) πε ref(ylx) Based on Observations and II, we can only focus on the safety-sensitive layers with the most vulnerable perturbation, which has been formalized below, (cid:40) MVA; 0, ε = if layer-l is the sensitive layer Otherwise. We introduce l1-penalty into the objective function and formulate the robust safety alignment as one-level optimization problem. Building on Observation III, taking the diversity of activation distribution into consideration, we incorporate the average pairwise cosine similarity into the objective function."
        },
        {
            "title": "4.2 Effectiveness of Jailbreak Defense",
            "content": "Experiment Setup. Our goal for defending against activation approximations-induced jailbreaking is to ensure that the LLMs do not produce harmful responses to jailbreak prompts at different level of activation approximations. The training data for QuadA is derived from the PKU-SafeRLHF dataset [34] with 73.9k training samples for 1 epoch. The loss 10 Figure 7: Perturbation of activation before Wup on different perturbation-aware safety-aligned LLMs. Figure 8: Perturbation of activation before Wdown on different perturbation-aware safety-aligned LLMs. function is constructed as shown in section 4.1. Specifically, we set β = 0.1, λ = 0.5 and the learning rate to 1e-6. The model we trained and evaluated are list in section 3.2. To assess the effectiveness of our defense mechanisms presented in section 4, we evaluate the ASR and PPL scores from applying the same range of noise as used in section 3 before Wup and Wdown to the same set of LLMs after they have undergone perturbation-aware safety alignment. Defense against jailbreak attacks. Figure 7 and Figure 8 show that after alignment, activation perturbation attacks consistently have low ASR below the PPL line even when the noise level increases to and beyond the MVA. Moreover, as PPL grows exponentially from the increase in noises, there remain little to no variations in ASR%, indicating success in preventing the models from outputting meaningful responses to harmful prompts under activation perturbations. Defense against adaptive prompt-based jailbreak attacks. For prompt-based jailbreak methods, we considered three widely applied ones including GCG [46], AutoDAN [47] and DRA [90], which iteratively optimize jailbreak prompt towards the goal of generating affirmative responses. Details of these attacks are given in the Appendix A.2. Table 4 shows the ASR of applying different adaptive prompt-based jailbreak attacks. In order to exclude the effect of the safety-aligned data on the experimental results, we set up the baseline with the same PKU-SafeRLHF datasetas QuadA, but without introducing the forward propagation perturbation during the DPO training. As shown in the table 4, QuadA demonstrates stronger robustness against the adaptive prompt-based jailbreak attacks. For Llama-3.1-8B-Instruct, after QuadA, the models ASR against DRA is only 0.2%, which is 75.1% decrease compared to the model before training and 58.2% decrease compared to the safety alignment without activation approximations. We can also observe that DPO without noise perturbation can resist the jailbreak prompt without suffix, but it is not very effective against the adaptive prompt-based jailbreak. It also suggests that those adaptive prompt-based jailbreak attacks are essentially an activation approximation for the suffixless jailbreak prompt."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "QuadA introduces three main algorithmic components driven by the key observations in Section 3. These components correspond to (1) Introducing perturbations with magnitudes designated by MVA; (2) Restricting perturbations exclusively to safety-sensitive layers; and (3) Regularizing harmful prompts in the activation space. Below, we evaluate the effect of each component through series of ablation studies, which corroborate the necessity of these algorithmic designs. Necessity of Introducing Perturbations by MVA. To begin with, we examine the effect of introducing perturbations and their magnitudes. We conduct experiments to evaluate the safety capabilities of LLMs under three perturbation configurations: no approximations (NA), perturbations with magnitudes 10% larger approximations than MVA (LA), and perturbations defined by MVA. As shown in Table 5, applying perturbations designated by MVAconsistently results in ASRs below 2% across all tested models, representing an improvement of over 98% in the models resistance to activation approximation errors. In contrast, while introducing large approximations (LA) during alignment can still improve model safety compared to the no perturbation (NP) baseline, the results are largely less effective than those achieved by MVA, i.e., yielding only around 50% lower ASRs. This demonstrates that QuadAs approximation with MVA is critical for achieving the desired safety-enhancement effects. Necessity of Sensitive-layer Selection. We observe that introducing perturbations exclusively to the sensitive layers of an LLM yields the most favorable results for safety alignment. In contrast, applying noise to non-sensitive layers negatively impacts alignment effectiveness, leading to increased perplexity and degraded model utility. In detail, Figure 9 presents PPL and ASR results from applying perturbations during safety alignment training under the following four different strategies: no layers, only nonsensitive layers, only sensitive layers, and all layers. Compared to the baseline, perturbing non-sensitive layers has minimal impact on the models resistance to activation approximations. In contrast, harmful prompts exhibit significantly reduced jailbreak efficacy against models aligned with noise applied to their sensitive layers. similar effect occurs when the perturbation is applied to all layers, which however substantially increases perplexity without further safety gains. Overall, applying perturbations to layers other than the safety-sensitive ones offers little to no improvement in safety capabilities while noticeably impairing utility. This validates the necessity of selecting safety-sensitive layers in QuadA. Visualization of Similarity-based Regularization. Figure 10 presents an MDS projection of the last-token activations produced by the Llama-2-7B-chat model after QuadA alignment. In comparison to the activation space of the model prior to alignment (depicted in Figure 5a), harmful activations are Table 4: Attack success rates (ASR) of applying different adaptive prompt-based jailbreak attacks. None means the jailbreak prompt without any suffix. Model None GCG AutoDAN DRA Llama-2-7B-Chat Llama-2-13B-Chat + DPO + QuadA + DPO + QuadA + DPO + QuadA + DPO + QuadA + DPO + QuadA Phi-3.5-Mini-Instruct Llama-3.1-8B-Instruct Phi-3-Mini-4K-Instruct 34.5 1.3 29.1 0.0 0.0 0.0 28.0 0.0 20.5 0.0 0.0 0.0 36.0 0.2 34.9 0.0 0.0 0.0 56.0 0.0 27.4 0.0 1.5 0.0 58.0 0.8 31.0 0.0 0.0 4.9 34.6 84.3 20.5 0.0 1.2 0.8 79.5 Mixtral-8x7B-Instruct-v0.1 1.2 12.3 0.0 0.0 0.0 22.3 78.6 19.5 0.0 2.9 0.0 48.4 0.4 32.5 0.0 0.0 0.0 36.6 0.0 17.9 0.0 0.0 0. + DPO + QuadA Zephyr-7B-β + DPO + QuadA Mistral-7B-Instruct-v0.3 Qwen2.5-32B-Instruct + DPO + QuadA + DPO + QuadA + DPO + QuadA Qwen2-7B-Instruct 0.5 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 84.5 69.0 0.8 86.5 72.1 2.5 93.0 64.4 1.0 88.5 36.7 0.0 97.5 70.2 0.4 62.5 53.0 0.0 31.5 6.8 0.0 69.2 38.4 3.3 58.4 19.2 1.5 75.3 58.4 0.2 91.2 84.7 6.3 96.7 88.2 10.4 98.5 86.0 2.5 92.0 52.5 0.8 94.8 88.1 7.0 79.3 67.9 4.5 57.9 24.1 0.2 Table 5: Attack success rates (ASR) of applying perturbations with different magnitudes during activation approximationrobust safety alignment. Model NA LA MVA Llama-2-7B-Chat Llama-2-13B-Chat Llama-3.1-8B-Instruct Phi-3-Mini-4K-Instruct Phi-3.5-Mini-Instruct Mistral-7B-Instruct-v0.3 Mixtral-8x7B-Instruct-v0.1 Zephyr-7B-β Qwen2-7B-Instruct Qwen2.5-32B-Instruct 40.76 44.52 61.84 56.83 64.80 71.45 78.04 81.20 64.36 35.58 26.15 30.95 45.79 41.26 35.76 52.84 59.37 62.35 33.17 22.06 0.00 0.00 0.19 0.00 0.00 0.77 1.05 0.38 0.00 0. 12 Figure 9: Model perplexity scores (PPL) and attack success rates (ASR) for applying perturbations to different types of layers during activation approximation-robust safety alignment."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we have systematically investigated the safety implications of activation approximation techniques in Large Language Models (LLMs), uncovering significant vulnerabilities that remain unaddressed by current safety alignment infrastructures. Our safety vetting has revealed that these approximations can substantially compromise model safety, elevating attack success rates without degrading utility. Through extensive analyses, we identified critical layers and harmful activation patterns that exacerbate these vulnerabilities, highlighting the disproportionate impact of early-layer approximations on safety. To address these challenges, we have proposed the first activation approximation-robust safety alignment method, which mitigates safety risks by optimizing sensitive layers with diverse activation distributions. Our approach not only withstands various levels of activation perturbations but also demonstrates robust resistance to adaptive jailbreak attacks. By sharing our findings, we aim to raise awareness of the safety costs associated with activation approximations and inspire further research to reinforce the safety of aligned LLMs in practical deployment scenarios."
        },
        {
            "title": "Ethics Considerations",
            "content": "This work is dedicated to examining the security and safety risks that arise in the customization of aligned LLMs via activation approximations. We highlight that our work only needs publicly available datasets. Our ultimate goal is to contribute positively to society by improving the security and safety of language models in the wild. Figure 10: MDS projection of the last-token activation space produced from both harmful and benign prompts on Llama-27B-chat in the first layer after applying QuadA. observed to have returned to clustered state, with clear separation between harmful and benign prompts. This highlights the effectiveness of integrating average pairwise cosine similarity as an l1-penalty within our DPO loss function."
        },
        {
            "title": "5 Limitations",
            "content": "The models studied in this paper are all open-source LLMs. Since closed-source LLMs, e.g., GPT-3.5 and GPT-4 do not provide an API to set the activation approximation, we cannot find the most vulnerable approximation (MVA) and align the LLMs with MVA. However, we must point out that in reality, the LLM trainer has full permissions on the model, so the LLM trainer has the permission to perform safety tests on the model under various activation approximations, thereby obtaining the models MVA and using QuadA for alignment."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Chat generative pre-trained transformer (chatgpt). https://www.openai.com/. [2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [3] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [4] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [5] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867, 2023. [6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [7] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [8] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:311, 2018. [9] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. International Conference on Learning Representations, 2023. [11] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. Billm: Pushing the limit of post-training International Conference on quantization for llms. Machine Learning, 2024. [12] Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 1032310337. PMLR, 2023. [13] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llmpruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. [14] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. [15] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. arXiv preprint arXiv:1910.04732, 2019. [16] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. [17] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [18] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael Mahoney, Amir Gholami, and Kurt Keutzer. Speculative decoding with big little decoder. Advances in Neural Information Processing Systems, 36, 2024. [19] Meng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing, Guowen Xu, and Tianwei Zhang. Iron: Private inference on transformers. Advances in neural information processing systems, 35:1571815731, 2022. [20] Xiaoyang Hou, Jian Liu, Jingyu Li, Yuhan Li, Wenjie Lu, Cheng Hong, and Kui Ren. Ciphergpt: Secure two-party gpt inference. Cryptology ePrint Archive, 2023. [21] Qi Pang, Jinhao Zhu, Helen Möllering, Wenting Zheng, and Thomas Schneider. Bolt: Privacy-preserving, accurate and efficient inference for transformers. In 2024 IEEE Symposium on Security and Privacy (SP), pages 47534771. IEEE, 2024. [22] Wen-jie Lu, Zhicong Huang, Zhen Gu, Jingyu Li, Jian Liu, Cheng Hong, Kui Ren, Tao Wei, and Wenguang Chen. BumbleBee: Secure Two-party Inference Framework for Large Transformers. In 32nd Annual Network and Distributed System Security Symposium, NDSS 2025. The Internet Society, February 24-28, 2025. [23] Jiawen Zhang, Xinpeng Yang, Lipeng He, Kejia Chen, Wen-jie Lu, Yinghao Wang, Xiaoyang Hou, Jian Liu, Kui Ren, and Xiaohu Yang. Secure transformer inference made non-interactive. February 24-28, 2025. [24] James Liu, Pragaash Ponnusamy, Tianle Cai, Han Guo, Yoon Kim, and Ben Athiwaratkun. Training-free activation sparsity in large language models. arXiv preprint arXiv:2408.14690, 2024. [25] Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, and Maosong Sun. Relu2 wins: Discovering efficient activation functions for sparse llms. arXiv preprint arXiv:2402.03804, 2024. [26] Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, and Mehrdad Farajtabar. Relu strikes back: Exploiting activation sparsity in large language models. International Conference on Machine Learning, 2024. [27] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR, 2023. [28] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantization for large language models. International Conference on Machine Learning, 2024. [29] Deevashwer Rathee, Mayank Rathee, Rahul Kranti Kiran Goli, Divya Gupta, Rahul Sharma, Nishanth Chandran, and Aseem Rastogi. Sirnn: math library for secure rnn inference. In 2021 IEEE Symposium on Security and Privacy (SP), pages 10031020. IEEE, 2021. [30] Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, and Lopamudra Praharaj. From chatgpt to threatgpt: Impact of generative ai in cybersecurity and privacy. IEEE Access, 2023. [31] Sonali Singh, Faranak Abri, and Akbar Siami Namin. Exploiting large language models (llms) through deception techniques and persuasion principles. In 2023 IEEE International Conference on Big Data (BigData), pages 25082517. IEEE, 2023. 15 [32] Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, and Jing Shao. Psysafe: comprehensive framework for psychological-based attack, defense, and evaluaarXiv preprint tion of multi-agent system safety. arXiv:2401.11880, 2024. [33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [34] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023. [35] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [36] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Finetuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023. [37] Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Safe lora: the silver lining of reducing safety risks when fine-tuning large language models. arXiv preprint arXiv:2405.16833, 2024. [38] Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei Yang. Compressing llms: The truth is rarely pure and never simple. arXiv preprint arXiv:2310.01382, 2023. [39] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, Ajay Jaiswal, Mykola Pechenizkiy, Yi Liang, et al. Outlier weighed layerwise sparsity (owl): missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv:2310.05175, 2023. [40] Lujun Li, Peijie Dong, Zhenheng Tang, Xiang Liu, Qiang Wang, Wenhan Luo, Wei Xue, Qifeng Liu, Xiaowen Chu, and Yike Guo. Discovering sparsity allocation for layer-wise pruning of large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [41] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic jailbreak of opensource llms via exploiting generation. arXiv preprint arXiv:2310.06987, 2023. [51] Donghwan Rho, Taeseong Kim, Minje Park, Jung Woo Kim, Hyunsik Chae, Jung Hee Cheon, and Ernest Ryu. Encryption-friendly llm architecture. arXiv preprint arXiv:2410.02486, 2024. [42] Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, et al. Trustllm: TrustworarXiv preprint thiness in large language models. arXiv:2401.05561, 2024. [43] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35:2716827183, 2022. [44] Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang. Qllm: Accurate and efficient low-bitwidth quantization for large language models. International Conference on Machine Learning, 2024. [45] AI Meta. Introducing meta llama 3: The most capable openly available llm to date. Meta AI, 2024. [46] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023. [47] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating stealthy jailbreak prompts In The Twelfth on aligned large language models. International Conference on Learning Representations, 2024. [48] Yongqin Wang, Edward Suh, Wenjie Xiong, Benjamin Lefaudeux, Brian Knott, Murali Annavaram, and Hsien-Hsin Lee. Characterization of mpc-based private inference for transformer-based models. In 2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pages 187197. IEEE, 2022. [49] Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric Xing, and Hao Zhang. Mpcformer: fast, performant and private transformer inference with mpc. International Conference on Machine Learning, 2023. [50] Wenxuan Zeng, Meng Li, Wenjie Xiong, Tong Tong, Wen-jie Lu, Jin Tan, Runsheng Wang, and Ru Huang. Mpcvit: Searching for accurate and efficient mpcfriendly vision transformer with heterogeneous attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 50525063, 2023. [52] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 2213722176. PMLR, 2023. [53] Donghyun Lee, Jaeyong Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. Cats: Context-aware thresholding for sparsity in large language models. In First Conference on Language Modeling. [54] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: sparse-quantized representation for nearlossless llm weight compression. International Conference on Learning Representations, 2024. [55] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:1740217414, 2022. [56] Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries. arXiv preprint arXiv:2310.08419, 2023. [57] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. Gptfuzzer: Red teaming large language models with arXiv preprint auto-generated jailbreak prompts. arXiv:2309.10253, 2023. [58] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. Detoxifying large language models via knowledge editing. arXiv preprint arXiv:2403.14472, 2024. [59] Wei Zhao, Zhe Li, Yige Li, Ye Zhang, and Jun Sun. Defending large language models against jailbreak arXiv preprint attacks via layer-specific editing. arXiv:2405.18166, 2024. [60] Tong Liu, Zizhuang Deng, Guozhu Meng, Yuekang Li, and Kai Chen. Demystifying rce vulnerabilities in llm-integrated apps. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, pages 17161730, 2024. [61] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing, page 100211, 2024. [62] Huiying Li, Shawn Shan, Emily Wenger, Jiayun Zhang, Haitao Zheng, and Ben Zhao. Blacklight: Scalable defense for neural networks against {QueryBased}{Black-Box} attacks. In 31st USENIX Security Symposium (USENIX Security 22), pages 21172134, 2022. [63] Shawn Shan, Wenxin Ding, Josephine Passananti, Stanley Wu, Haitao Zheng, and Ben Zhao. Nightshade: Prompt-specific poisoning attacks on text-to-image generative models. In 2024 IEEE Symposium on Security and Privacy (SP), pages 212212. IEEE Computer Society, 2024. [64] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, et al. Harmbench: standardized evaluation framework for automated red teaming and robust refusal. arXiv preprint arXiv:2402.04249, 2024. [65] Zonghao Ying, Aishan Liu, Siyuan Liang, Lei Huang, Jinyang Guo, Wenbo Zhou, Xianglong Liu, and Dacheng Tao. Safebench: safety evaluation framework for multimodal large language models. arXiv preprint arXiv:2410.18927, 2024. [66] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. Masterkey: Automated jailbreaking of large language model chatbots. In Proc. ISOC NDSS, 2024. [67] Yi Zeng, Yu Yang, Andy Zhou, Jeffrey Ziwei Tan, Yuheng Tu, Yifan Mai, Kevin Klyman, Minzhou Pan, Ruoxi Jia, Dawn Song, et al. Air-bench 2024: safety benchmark based on risk categories from regulations and policies. arXiv preprint arXiv:2407.17436, 2024. [68] Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. arXiv preprint arXiv:2406.14598, 2024. [69] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020. [70] Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second thought, lets not think step by step! bias and toxicity in zero-shot reasoning. arXiv preprint arXiv:2212.08061, 2022. [71] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: dataset arXiv preprint for evaluating safeguards in llms. arXiv:2308.13387, 2023. [72] Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, et al. strongreject for empty jailbreaks. arXiv preprint arXiv:2402.10260, 2024. [73] Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. Salad-bench: hierarchical and comprehensive safety benchmark for large language models. arXiv preprint arXiv:2402.05044, 2024. [74] Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. Comprehensive assessment of jailbreak attacks against llms. arXiv preprint arXiv:2402.05668, 2024. [75] Kai Hu, Weichen Yu, Tianjun Yao, Xiang Li, Wenhe Liu, Lijun Yu, Yining Li, Kai Chen, Zhiqiang Shen, and Matt Fredrikson. Efficient llm jailbreak via adaptive dense-to-sparse constrained optimization. arXiv preprint arXiv:2405.09113, 2024. [76] Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, and Yu-Gang Jiang. White-box multimodal jailbreaks against large vision-language models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 69206928, 2024. [77] Zhihao Xu, Ruixuan Huang, Xiting Wang, Fangzhao Wu, Jing Yao, and Xing Xie. Uncovering safety risks in open-source llms through concept activation vector. arXiv preprint arXiv:2404.12038, 2024. [78] Ju Jia, Siqi Ma, Yang Liu, Lina Wang, and Robert Deng. causality-aligned structure rationalization scheme against adversarial biased perturbations for graph neural networks. IEEE Transactions on Information Forensics and Security, 2023. [79] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-asa-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. 17 [80] Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via pruning and lowrank modifications. International Conference on Machine Learning (ICML), 2024. [81] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. Harmful fine-tuning attacks and defenses for large language models: survey. arXiv preprint arXiv:2409.18169, 2024. [82] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. Lisa: Lazy safety alignment for large language models against harmful fine-tuning attack. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. [83] Eliahu Horwitz, Jonathan Kahana, and Yedid Hoshen. Recovering the pre-fine-tuning weights of generative models. arXiv preprint arXiv:2402.10208, 2024. [84] Zhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang, Wei Chen, Minfeng Zhu, and Qian Liu. Self-distillation bridges distribution gap in language model fine-tuning. arXiv preprint arXiv:2402.13669, 2024. [85] Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Keeping llms aligned after fine-tuning: The crucial role of prompt templates. arXiv preprint arXiv:2402.18540, 2024. [86] Han Shen, Pin-Yu Chen, Payel Das, and Tianyi Seal: Safety-enhanced aligned llm finearXiv preprint Chen. tuning via bilevel data selection. arXiv:2410.07471, 2024. [87] Zhichao Xu, Ashim Gupta, Tao Li, Oliver Bentham, and Vivek Srikumar. Beyond perplexity: Multidimensional safety evaluation of llm compression. arXiv preprint arXiv:2407.04965, 2024. [88] Nasib Ullah, Erik Schultheis, Mike Lasby, Yani Ioannou, and Rohit Babbar. Navigating extremes: Dynamic sparsity in large output space. arXiv preprint arXiv:2411.03171, 2024. [89] Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt. Automatically auditing large language In International models via discrete optimization. Conference on Machine Learning, pages 1530715329. PMLR, 2023. [90] Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, and Kai Chen. Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction. In 33rd USENIX Security Symposium (USENIX Security 24), pages 47114728, 2024. 18 [91] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [92] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [93] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [94] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. [95] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [96] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github. com/tatsu-lab/stanford_alpaca, 2023. [97] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [98] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [99] Lang Gao, Xiangliang Zhang, Preslav Nakov, and Xiuying Chen. Shaping the safety boundaries: Understanding and defending against jailbreaks in large language models. arXiv preprint arXiv:2412.17034, 2024. [100] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prompt Templates During inference, the input instruction is initially integrated into template, which is then tokenized and processed through the embedding layer to form the initial input vectors for the LLM. We use the same dialog template [100] for different problems across various aligned LLMs in our study. Dialog Template Below is an instruction that describes task. Write response that appropriately completes the request. Instruction: {The malicious input instruction} Response: {The output of the model} A.2 Adaptive Prompt-based Jailbreak Attacks We list the adaptive prompt-based jailbreak attacks in table 4 here. GCG [46] append an adversarial suffix after prompts and carry out the following steps iteratively: compute top-k substitutions at each position of the suffix, select the random replacement token, compute the best replacement given the substitutions, and update the suffix. The default iterations of GCG is 500. AutoDAN [47] generates an adversarial suffix in sequential manner. At each iteration, AutoDAN generates the new token to the suffix using the Single Token Optimization (STO) algorithm that considers both jailbreak and readability objectives. The default iteration of AutoDAN is 100. DRA [90] involves dissecting harmful prompts into individual characters and inserting them within word puzzle query. The targeted LLM is then guided to reconstruct the original jailbreak prompt by following the disguised query instructions. Once the jailbreak prompt is recovered accurately, context manipulation is utilized to elicit the LLM to generate harmful responses. The default iteration of DRA is 20."
        }
    ],
    "affiliations": [
        "Sun Yat-sen University",
        "University of Waterloo",
        "Zhejiang University"
    ]
}