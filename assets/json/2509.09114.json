{
    "paper_title": "Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation",
    "authors": [
        "Kelin Ren",
        "Chan-Yang Ju",
        "Dong-Ho Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal recommendation systems are increasingly becoming foundational technologies for e-commerce and content platforms, enabling personalized services by jointly modeling users' historical behaviors and the multimodal features of items (e.g., visual and textual). However, most existing methods rely on either static fusion strategies or graph-based local interaction modeling, facing two critical limitations: (1) insufficient ability to model fine-grained cross-modal associations, leading to suboptimal fusion quality; and (2) a lack of global distribution-level consistency, causing representational bias. To address these, we propose MambaRec, a novel framework that integrates local feature alignment and global distribution regularization via attention-guided learning. At its core, we introduce the Dilated Refinement Attention Module (DREAM), which uses multi-scale dilated convolutions with channel-wise and spatial attention to align fine-grained semantic patterns between visual and textual modalities. This module captures hierarchical relationships and context-aware associations, improving cross-modal semantic modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive loss functions to constrain global modality alignment, enhancing semantic consistency. This dual regularization reduces mode-specific deviations and boosts robustness. To improve scalability, MambaRec employs a dimensionality reduction strategy to lower the computational cost of high-dimensional multimodal features. Extensive experiments on real-world e-commerce datasets show that MambaRec outperforms existing methods in fusion quality, generalization, and efficiency. Our code has been made publicly available at https://github.com/rkl71/MambaRec."
        },
        {
            "title": "Start",
            "content": "Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation Chan-Yang Ju Department of Applied Artificial Intelligence Hanyang University Ansan, Republic of Korea karunogi@hanyang.ac.kr Kelin Ren Department of Computer Science and Engineering Hanyang University Ansan, Republic of Korea renkelin@hanyang.ac.kr Dong-Ho Lee Department of Applied Artificial Intelligence Hanyang University Ansan, Republic of Korea dhlee72@hanyang.ac.kr 5 2 0 2 1 1 ] . [ 1 4 1 1 9 0 . 9 0 5 2 : r Abstract Multimodal recommendation systems are increasingly becoming foundational technologies for e-commerce and content platforms, enabling personalized services by jointly modeling users historical behaviors and the multimodal features of items (e.g., visual and textual). However, most existing methods rely on either static fusion strategies or graph-based local interaction modeling, facing two critical limitations: (1) insufficient ability to model fine-grained crossmodal associations, leading to suboptimal fusion quality; and (2) lack of global distribution-level consistency, causing representational bias. To address these, we propose MambaRec, novel framework that integrates local feature alignment and global distribution regularization via attention-guided learning. At its core, we introduce the Dilated Refinement Attention Module (DREAM), which uses multi-scale dilated convolutions with channel-wise and spatial attention to align fine-grained semantic patterns between visual and textual modalities. This module captures hierarchical relationships and context-aware associations, improving cross-modal semantic modeling. Additionally, we apply Maximum Mean Discrepancy (MMD) and contrastive loss functions to constrain global modality alignment, enhancing semantic consistency. This dual regularization reduces mode-specific deviations and boosts robustness. To improve scalability, MambaRec employs dimensionality reduction strategy to lower the computational cost of high-dimensional multimodal features. Extensive experiments on real-world e-commerce datasets show that MambaRec outperforms existing methods in fusion quality, generalization, and efficiency. Our code has been made publicly available at https://github.com/rkl71/MambaRec. CCS Concepts Information systems Recommender systems. Keywords Multimodal Recommendation; Modality Alignment; Dimensionality Optimization Major in Bio Artificial Intelligence. Corresponding author. This work is licensed under Creative Commons Attribution 4.0 International License. CIKM 25, Seoul, Republic of Korea 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2040-6/2025/11 https://doi.org/10.1145/3746252.3761222 Figure 1: (a) illustrates the process in which existing methods perform simple processing of visual and textual data through aggregation modules, where the resulting feature representations (blue and green blocks) lack alignment. In contrast, (b) introduces Local Alignment and Global Alignment modules to establish direct, one-to-one correspondence between visual and textual features. ACM Reference Format: Kelin Ren, Chan-Yang Ju, and Dong-Ho Lee. 2025. Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation. In Proceedings of the 34th ACM International Conference on Information and Knowledge Management (CIKM 25), November 1014, 2025, Seoul, Republic of Korea. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3746252."
        },
        {
            "title": "1 Introduction\nWith the rapid development of information technology, recommen-\ndation systems have been widely used in various fields such as\ne-commerce and Short Video platforms, which can effectively help\nusers discover interesting content. Because user decisions are often\ninfluenced by the rich multimodal information (such as images\nand text) presented by items, effective fusion of these multimodal\nfeatures to accurately capture user preferences has become an im-\nportant research direction in the recommendation field. Numer-\nous studies have shown that multimodal recommendation systems\n(MRS) that integrate modal information generally perform better\nthan traditional recommendation methods that rely solely on his-\ntorical interaction data [9, 11, 17, 31, 32]. In particular, multimodal\ninformation can supplement sparse or missing behavioral data by\nproviding semantically rich content from auxiliary sources, thereby\nalleviating the cold start and data sparse issues inherent in many",
            "content": "CIKM 25, November 1014, 2025, Seoul, Republic of Korea Kelin Ren, Chan-Yang Ju, and Dong-Ho Lee real-world recommendation scenarios. The increasing popularity of user-generated content and high-dimensional multimedia data makes it not only desirable but also necessary to efficiently and effectively model cross-modal relationships for next-generation recommendation systems. In recent years, multimodal recommendation methods based on Graph Convolutional Networks (GCNs) have achieved good results, mainly because GCNs can effectively capture complex interactive information and semantic relationships between users and items [16, 26, 27]. The classic Multi-View Graph Convolutional Network (MGCN) [30] builds collaboration graph between users and items through feature mapping of different modes, thereby better reflecting the relationship between users and items [30]. Despite notable progress, existing multimodal recommendation methods still suffer from several limitations. In the feature extraction stage, many models rely on static linear projections or simplistic fusion techniques, which limit their ability to capture fine-grained semantic correspondence between visual and textual modalities. Models like Bootstrap Multimodal Matching (BM3) [35] and Self-supervised Learning for Multimedia Recommendation (SLMRec) [23] introduce contrastive objectives or local fusion, yet typically lack global constraints on modality-level representation distributions, leading to semantic misalignment. Moreover, graph-based approaches such as Local and Global Graph Learning for Multimodal Recommendation (LGMRec) [7] utilize modality-specific graphs to model preferences, but still struggle with the distribution shift across modalities [1] and the noise introduced by high-dimensional features [29]. These limitations hinder the effectiveness and generalization of multimodal recommendation systems in large-scale scenarios. To address these challenges, we propose Modality Alignment with Multi-scale Bilateral Attention (MAMBARec) for multimodal recommendations. The model is designed to achieve more efficient and accurate feature fusion and alignment. Specifically, this study introduces three key components to enhance multimodal feature alignment: In order to improve the accuracy of local feature alignment, we propose Dilated REfinement Attention Module (DREAM), which uses multi-scale dilated convolution to capture modal feature information at different scales, and combines channel and spatial attention mechanisms to adaptively highlight key features. The DREAM module can effectively expand the receptive field without increasing the computational burden, allowing the model to focus on fine-grained local interactions and broader context dependencies at the same time, which is critical to modeling fine preferences in heterogeneous modal spaces. Moreover, in order to effectively enforce the consistency of the global distribution of modal characteristics, we introduce the Maximum Mean Discrepancy (MMD) loss function [6] into the recommendation area. By using the Gaussian kernel function to explicitly restrict the consistency of modal feature distribution, the quality of feature fusion is significantly improved. Finally, in order to solve the memory consumption problem caused by high-dimensional characteristics, we designed novel memory optimization strategy. By introducing configurable dimension reduction factors and two-stage feature conversion mechanism, this strategy significantly reduces memory usage while maintaining high-quality feature representations. This ensures that MAMBARec remains scalable and computationally efficient, allowing it to be deployed in large-scale recommendation environments without sacrificing performance. Comprehensive experiments on three public datasets demonstrate the significant advantages of our proposed method. Our main contributions can be summarized as follows: We propose an innovative DREAM module that significantly enhances local feature alignment through multi-scale extended convolution and dual attention mechanisms. We also introduce the MMD loss function into the recommendation system to reduce modal discrepancies and enhance the models generalization, by enforcing consistency in the global distribution of modal features. Furthermore, we designed and implemented flexible and efficient memory optimization strategy to effectively reduce the memory requirements of high-dimensional modal features and improve the actual deployment capabilities of the model."
        },
        {
            "title": "2 Related Work\nTraditional recommendation systems primarily relied on collab-\norative filtering methods to learn latent user preferences from\nuser-item interaction data. For instance, the Bayesian Personal-\nized Ranking (BPR) model [21] optimizes item ranking using pair-\nwise ranking loss functions, enhancing recommendation accuracy.\nHowever, collaborative filtering faces challenges such as data spar-\nsity and cold-start problems, limiting its ability to capture diverse\nuser interests. To address these, some approaches incorporate multi-\nmodal features, providing richer contextual information. The Visual\nBayesian Personalized Ranking (VBPR) model [9] integrates visual\nfeatures extracted from pre-trained Convolutional Neural Networks\n(CNNs), mitigating cold-start issues and enhancing recommenda-\ntion performance. However, early multimodal approaches often\ntreat each modality independently, using simple linear combina-\ntions that fail to model complex cross-modal interactions, limiting\ntheir ability to capture fine-grained semantic alignments.",
            "content": "The advent of Graph Neural Networks (GNNs) has presented new opportunities for multimodal recommendation systems. LightGCN [10] introduces simplified graph convolution framework focused on neighborhood aggregation, which efficiently models high-order user-item relationships. The Multimodal Graph Convolutional Network (MMGCN) [27] further advances this by constructing interaction graphs for each modality and fusing their graph embeddings. While this fusion improves recommendation performance, it may amplify modality-specific noise. GRCN [26] addresses this by refining the graph through gating mechanisms to suppress noisy interactions. However, existing GCN-based models still struggle with accommodating heterogeneous semantic granularities across modalities, leading to information dilution and over-smoothing in dense graphs [14]. Recent work in multimodal recommendation has further explored self-supervised learning and fine-grained fusion mechanisms. SLMRec [23] introduces cross-modal self-supervised tasks and hierarchical contrastive learning framework to enhance the robustness of modal representations. BM3 [35] proposes self-supervised strategy that generates contrastive views through random feature Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation CIKM 25, November 1014, 2025, Seoul, Republic of Korea Figure 2: The overall architecture of the proposed MambaRec, comprising three key components. (i) The local alignment performs fine-grained matching between local features of images and text. (ii) The global alignment ensures consistency between different modalities at the level of global distribution. (iii) The dimensionality reduction mechanism was introduced before alignment to compress high-dimensional visual features and reduce memory overhead. dropping, reducing reliance on negative sampling while improving both efficiency and accuracy. FREEDOM [34] reduces memory consumption by freezing the item similarity graph and pruning user interaction graphs, aiming to improve training stability and resource efficiency. LGMRec [7] introduces localglobal modal graph learning strategy that models both modality-specific user interests and shared cross-modal preferences, in order to alleviate interference between modalities. Meanwhile, MGCN [30] leverages users historical behavior to guide the preprocessing and refinement of modal features and employs multi-view graph convolution to facilitate more fine-grained fusion of multimodal information. While these approaches represent important progress in multimodal recommendation, they often overlook semantic consistency across modalities and coordination of representation distributions. Few approaches explicitly integrate local feature alignment and distributional consistency in unified manner, which is crucial for achieving robust and generalized cross-modal fusion. In this study, we propose to jointly model local semantic correspondences and global distributional consistency from multimodal alignment perspective to achieve more consistent and efficient recommendation systems."
        },
        {
            "title": "3 Problem Formulation\nGiven a set of ğ‘€ users and ğ‘ items, the historical interactions\nbetween users and items are represented by a sparse binary matrix\nR âˆˆ {0, 1}ğ‘€ Ã—ğ‘ , where Rğ‘¢ğ‘– = 1 indicates that user ğ‘¢ has interacted\nwith item ğ‘– (e.g., by clicking or purchasing), and Rğ‘¢ğ‘– = 0 otherwise.\nTo capture collaborative signals, each user and item is assigned a ğ‘‘-\ndimensional identity embedding, denoted as pğ‘¢ and qğ‘– , respectively.\nIn addition, each item is associated with multimodal features. This\nwork focuses on two modalities, namely the visual mode (ğ‘£) and the",
            "content": "text mode (ğ‘¡), where the set of modalities is defined as = {ğ‘£, ğ‘¡ }. For each modality ğ‘š M, the raw feature of item ğ‘– is represented ğ‘– Rğ‘‘ğ‘š and is projected into unified embedding space using as xğ‘š learnable linear transformation ğ‘Šğ‘š Rğ‘‘ ğ‘‘ğ‘š , resulting in the modality embedding hğ‘š ğ‘– = ğ‘Šğ‘šxğ‘š . The predicted preference score ğ‘– between user ğ‘¢ and item ğ‘– is computed using scoring function Ë†ğ‘¦ğ‘¢ğ‘– = ğ‘“ (pğ‘¢, qğ‘–, {hğ‘š ğ‘– }ğ‘š M), where ğ‘“ can be simple inner product or deep model designed to fuse collaborative and multimodal information. The model is trained using observed interactions as supervision, and during inference, each user receives personalized recommendation list based on the predicted scores. Although this paper focuses on both visual and textual modalities, the proposed method is well generalizable. We aim to accurately predict the interaction probability Ë†ğ‘¦ğ‘¢ğ‘– between the user ğ‘¢ and the item ğ‘– for personalized recommendations."
        },
        {
            "title": "4.1 Local Feature Alignment\nPrior work [13] has highlighted the importance of fusing seman-\ntically aligned image and text features. Inspired by DeepLabV3\nwith Atrous Spatial Pyramid Pooling (ASPP) [2] and the Convolu-\ntional Block Attention Module (CBAM) [28], we design a Dilated\nREfinement Attention Module (DREAM) to achieve local feature\nalignment through multi-scale convolution and attention mecha-\nnisms. Let the input feature map be ğ‘‹ âˆˆ Rğ¶ Ã—ğ» Ã—ğ‘Š and the output",
            "content": "CIKM 25, November 1014, 2025, Seoul, Republic of Korea Kelin Ren, Chan-Yang Ju, and Dong-Ho Lee be ğ‘Œ Rğ¶ ğ» ğ‘Š . The DREAM module contains five parallel convolution operation branches to extract multi-scale features, which are then weighted and fused by simultaneously applying channel and spatial attention mechanisms to the fused feature map. 4.1.1 Multi-scale feature extraction. These five branches consist of 1 1 convolution branch, three dilated convolution branches with increasing dilation rates, and global average pooling branch. The first branch applies 1 1 convolution to extract fine-grained local features with minimal computational cost and without expanding the receptive field. This enables lightweight channel-wise transformation while preserving high-resolution spatial details. Branches 2 to 4 employ 33 dilated convolutions with dilation rates of 6, 12, and 18, respectively. These configurations enlarge the receptive field to capture mediumand long-range contextual information while maintaining the spatial resolution through appropriate padding. Branch 5 performs global average pooling over the entire spatial dimension, followed by 1 1 convolution to project the global representation into the desired number of channels. The resulting 1 1 feature map is then upsampled to ğ» ğ‘Š via bilinear interpolation to align with the spatial dimensions of the other branches and provide global semantic context. Let the output feature maps of the five branches be denoted as ğ¹1, ğ¹2, ğ¹3, ğ¹4 and ğ¹5 Rğ» ğ‘Š ğ¶ğ‘– . There are concatenated along the channel dimension to obtain the fused feature map: ğ¹ = Concatğ‘ (ğ¹1, ğ¹2, ğ¹3, ğ¹4, ğ¹5), (1) where Concatğ‘ denotes concatenation along the channel dimension. The resulting fused representation ğ¹ Rğ» ğ‘Š ğ¶ integrates both local detail and global context information, where ğ¶ = (cid:205)5 ğ¶ğ‘– . ğ‘–=1 4.1.2 Channel attention mechanism. In order to adaptively enhance important channels, DREAM introduced channel attention module to the fused feature map ğ¹ . Apply global average pooling to the feature map ğ¹ to obtain channel compression vector ğ‘§ Rğ¶ 11 (if there are ğ¶ channels after splicing). Then, enter ğ‘§ into the two fully connected layers in turn. The first fully connected layer reduces the channel dimension (using the ReLU activation function ğ›¿ () to ğ¶ /ğ‘Ÿ ), and the second fully connected layer restores the dimension to ğ¶ and outputs channel weight vector ğ‘€ğ‘ Rğ¶ 11. The above process can be expressed as: ğ‘€ğ‘ = ğœ (ğ‘Š2 (ğ›¿ (ğ‘Š1 (ğ‘§)))) , (2) where ğ‘Š1 and ğ‘Š2 are the weight matrices of the fully connected layer, and ğ›¿, ğœ denote the ReLU and Sigmoid activation functions respectively. The value range of each element of the obtained channel attention weight ğ‘€ğ‘ is [0, 1], which indicates the importance of the corresponding channel. Finally, the weight ğ‘€ğ‘ is multiplied back onto the fused feature map to achieve channel-by-channel feature recalibration: ğ¹ğ‘ = ğ¹ ğ‘€ğ‘, (3) where the symbol denotes element-wise multiplication for each channel. After channel attention is enhanced, the feature map ğ¹ğ‘ is enlarged on key channels to suppress unimportant channel information. Spatial attention mechanism. The DREAM module applies 4.1.3 spatial attention mechanisms to highlight key spatial locations. Specifically, the fused feature ğ¹ is pooled in the channel dimension to obtain two-dimensional feature map ğ‘ƒ R1ğ» ğ‘Š . Here we use an average per channel method, ğ‘ƒ (ğ‘–, ğ‘—) = 1 ğ¹ (ğ‘, ğ‘–, ğ‘—), and ğ¶ calculate the average response at each spatial location (ğ‘–, ğ‘—). Next, ğ‘ƒ is linearly transformed using convolution of 1 1, and then normalized through Sigmoid function to get spatial attention weight graph ğ‘€ğ‘  R1ğ» ğ‘Š : (cid:205)ğ¶ ğ‘= ğ‘€ğ‘  = ğœ (cid:0)ğ‘“ 11 (ğ‘ƒ)(cid:1) , (4) where ğ‘“ 11 denotes the 1 1 convolution operator, and ğœ maps the result to the [0, 1] interval. Each position (ğ‘–, ğ‘—) of ğ‘€ğ‘  corresponds to spatial weight, which indicates the importance of the position. This weight map is then expanded to match the number of channels in ğ¹ (copied to all channels at each position) and multiplied elementwise with the original fused feature map: ğ¹ğ‘  = ğ¹ ğ‘€ğ‘ , (5) this produces feature map ğ¹ğ‘  with enhanced spatial attention. After this operation, ğ¹ğ‘  is enhanced at important spatial locations, suppressing the influence of background or irrelevant areas. 4.1.4 Attention fusion. After obtaining the channel enhancement feature ğ¹ğ‘ and the spatial enhancement feature ğ¹ğ‘  , the DREAM module fuses the two features by performing an element-by-element maximization operation, thereby making full use of these two types of attention information. Specifically, we perform an element-byelement maximum operation on ğ¹ğ‘ and ğ¹ğ‘  (denoted as M) to merge them: ğ‘Œ = (ğ¹ğ‘, ğ¹ğ‘  ), (6) where (ğ‘, ğ‘) means selecting the larger of the two input tensors ğ‘ and ğ‘ element by element. For each position in each channel, the more responsive one of the channel attention branches or the spatial attention branches is adaptively selected as the output. After the DREAM module performs multi-scale feature refinement and attention-weighted alignment, more significant and accurate local feature information is included."
        },
        {
            "title": "4.2 Global Attribution Alignment\nThe global alignment module is used to achieve consistency in\nthe spatial representation distribution of image and text features,\nwith the Maximum Mean Discrepancy (MMD) loss [6] as the core,\nsupplemented by InfoNCE comparison loss [18] for further feature\nalignment. Specifically, assuming that the image modal feature set\nis ğ‘‰ = {ğ‘£ğ‘– }ğ‘\nğ‘–=1 and the text modal feature set is ğ‘‡ = {ğ‘¡ ğ‘— }ğ‘\nğ‘—=1, we use\nMMD to measure the difference between the two modal feature\ndistributions. MMD uses the embedding gap of the distribution\nmean in the reproducing kernel Hilbert space (RKHS) to measure\nthe inconsistency of the distribution. It is defined as follows,",
            "content": "MMD2 (ğ‘‰ ,ğ‘‡ ) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 ğ‘ ğ‘ ğ‘–=1 ğœ™ (ğ‘£ğ‘– ) 1 ğ‘ ğ‘ ğ‘—=1 ğœ™ (ğ‘¡ ğ‘— ) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) , (7) Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation CIKM 25, November 1014, 2025, Seoul, Republic of Korea where ğœ™ ()denotes the mapping function in the reproducing kernel Hilbert space (RKHS). This expression can be equivalently converted to kernel function form: MMD2 (ğ‘‰ ,ğ‘‡ ) = 1 ğ‘ 2 ğ‘ ğ‘ ğ‘˜ (ğ‘£ğ‘–, ğ‘£ğ‘– ) + 1 ğ‘ 2 ğ‘ ğ‘ ğ‘—=1 ğ‘— =1 ğ‘˜ (ğ‘¡ ğ‘—, ğ‘¡ ğ‘— ) ğ‘–=1 ğ‘–=1 ğ‘ ğ‘ ğ‘–=1 ğ‘—=1 2 ğ‘ 2 ğ‘˜ (ğ‘£ğ‘–, ğ‘¡ ğ‘— ) (8) where ğ‘˜ (, ) is the kernel function defined on the input space. The Gaussian kernel function we use in this work is defined as follows: (cid:18) (cid:19) ğ‘˜ (ğ‘£, ğ‘¡) = exp ğ‘£ ğ‘¡ 2 2ğœ , (9) where ğœ is kernel bandwidth hyperparameter, which is used to adjust the sensitivity of distance between features to similarity. By minimizing the above-mentioned MMD loss, the distance between image modality and text modality in the overall distribution can be effectively shortened, thereby achieving global semantic alignment. To further enhance the consistency of representations, we introduced InfoNCE comparative learning loss as supplement. It enhances the similarity between matching pairs of graphic features and suppresses the similarity between non-matching pairs through comparative learning, which is expressed as follows: LInfoNCE = 1 ğ‘ ğ‘ ğ‘–=1 log exp (sim(ğ‘£ğ‘–, ğ‘¡ğ‘– )/ğœ) ğ‘—=1 exp (cid:0)sim(ğ‘£ğ‘–, ğ‘¡ ğ‘— )/ğœ (cid:1) (cid:205)ğ‘ , (10) where sim(, ) is the dot product of the normalized feature vector, and ğœis the temperature hyperparameter. Ultimately, the overall loss of the global distribution alignment module consists of weighted combination of the two components: Lalign = ğœ†mmd LMMD + ğœ†cl LInfoNCE, (11) where ğœ†mmd and ğœ†cl denote the weight coefficients of these two losses, and their values are specified by the model configuration hyperparameter, respectively. With this joint optimization strategy, we achieved alignment and fusion of cross-modal semantic representations at the global distribution level."
        },
        {
            "title": "4.3 Dimensionality Optimization\nIn order to effectively solve the dimensional differences and redun-\ndancy problems between different modal (image and text) features,\nwe propose a dimensional optimization mechanism to improve the\nefficiency of multimodal feature fusion and the generalization abil-\nity of the model. Let the image modal feature matrix be ğ‘‰ âˆˆ Rğ‘ Ã—ğ·ğ‘‰ ,\nand the text modal feature matrix be ğ‘‡ âˆˆ Rğ‘ Ã—ğ·ğ‘‡ , where ğ‘ de-\nnotes the number of samples, and ğ·ğ‘‰ , ğ·ğ‘‡ denote the original fea-\nture dimensions of the image and text respectively. We introduce\ntwo linear dimensionality reduction matrices ğ‘Šğ‘‰ âˆˆ Rğ·ğ‘‰ Ã—ğ‘‘ and\nğ‘Šğ‘‡ âˆˆ Rğ·ğ‘‡ Ã—ğ‘‘ , which are used to project the two modal features\nrespectively:",
            "content": "ğ‘‰ = ğ‘‰ğ‘Šğ‘‰ , ğ‘‡ = ğ‘‡ğ‘Šğ‘‡ , (12) where ğ‘‰ ,ğ‘‡ Rğ‘ ğ‘‘ denote the unified feature representation after dimension reduction, and ğ‘‘ is the target embedding dimension, whose size is controlled by the dimension reduction coefficient ğ‘Ÿ , which is defined as follows: ğ‘‘ = (cid:22) min(ğ·ğ‘‰ , ğ·ğ‘‡ ) ğ‘Ÿ (cid:23) . (13) The dimensionality reduced modal features ğ‘‰ and ğ‘‡ will be further input into the DREAM module for structural enhancement and feature fine-grained enhancement. Overall, this dimension optimization mechanism can not only significantly reduce the dimension and computing overhead of the feature space, but also improve the robustness of the multimodal fusion representation."
        },
        {
            "title": "4.5 Prediction and Optimization\nIn this model, for any user ğ‘¢ and item ğ‘–, the model generates their\nğ‘– âˆˆ Rğ‘‘ respectively.\nfusion representation vectors ğ‘’âˆ—\nThe interaction scores of the two are calculated through the inner\nproduct of the vectors:",
            "content": "ğ‘¢ Rğ‘‘ and ğ‘’ Ë†ğ‘¦(ğ‘¢, ğ‘–) = (ğ‘’ ğ‘¢ )ğ‘’ ğ‘– , (14) Model training uses Bayesian Personalized Ranking (BPR) [21] as the basic optimization goal. This goal is used to characterize the users preference ranking relationship for positive and negative sample items. The loss function is defined as follows: LBPR = log ğœ ( Ë†ğ‘¦ğ‘¢ğ‘– Ë†ğ‘¦ğ‘¢ ğ‘— ), (15) (ğ‘¢,ğ‘–,ğ‘— ) where ğœ () denotes the Sigmoid function, and = {(ğ‘¢, ğ‘–, ğ‘—)} denotes the set of triples where user ğ‘¢ prefers positive samples ğ‘– over negative samples ğ‘—. To enhance the consistency of representation learning and crossmodal fusion, the model further introduces InfoNCE comparative learning loss Lcl and maximum mean discrepancy loss Lmmd as auxiliary optimization terms. Introduce â„“2 regularization to prevent overfitting the model. The final joint optimization goals are as follows: = LBPR + ğœ†cl Lcl + ğœ†mmd Lmmd + ğœ†reg Î˜2 2 where Î˜ denotes the set of all learnable parameters in the model, and ğœ†cl, ğœ†mmd, ğœ†reg are weight hyperparameters of the corresponding loss term. (16) ,"
        },
        {
            "title": "5 Experiments\nWe conduct a series of experiments to address the following re-\nsearch questions:",
            "content": "RQ1: How effective is the proposed MambaRec architecture compared to state-of-the-art general-purpose and multimodal recommendation models? CIKM 25, November 1014, 2025, Seoul, Republic of Korea Kelin Ren, Chan-Yang Ju, and Dong-Ho Lee RQ2: What are the contributions of key components and individual modalities within MambaRec to its overall performance? RQ3: How does the variation of hyperparameters affect the overall effectiveness of the proposed model? RQ4: Why does modality alignment lead to improved recommendation performance? Table 1: Statistics of the experimental datasets Dataset #User #Item #Interaction Density Baby Sports Clothing 19,445 35,598 39,387 7,050 18,357 23,033 160,792 296,337 278,677 0.117% 0.045% 0.031%"
        },
        {
            "title": "5.1 Experimental Settings\n5.1.1 Datasets. Following [7, 34], We use three representative sub-\nsets of Amazonâ€™s product review datasets 1 for experimental evalu-\nations: (a) Baby, (b) Sports and Outdoors, and (c) Clothing, Shoes\nand Jewelry. For ease of reference, we simply refer to these three\ncategories as Baby, Sports and Clothing. These three categories\ncover different consumption scenarios, have significant user behav-\nior differences and pattern characteristics diversity. The raw data\nare filtered based on the 5-core setting on both users and items,\nand previous works mostly use these processed subsets for multi-\nmodal recommendation. The processing results are shown in Table\n1. For the visual modality, we first use 4,096-dimensional visual\nfeatures extracted by a pre-trained VGG16 convolutional neural\nnetwork [22] followed by dimensionality reduction. For the textual\nmodality, we extract a 384-dimensional textual embedding by uti-\nlizing sentence-transformers [20] on the concatenation of the title,\ndescriptions, categories, and brand of each item.",
            "content": "5.1.2 Baselines. To evaluate the effectiveness of MambaRec model, we compare it with several state-of-the-art (SOTA) recommendation methods in two categories: i) General Models: BPR [21]: classic sorting optimization model based on matrix decomposition. LightGCN [10]: Simplifying the graph convolution structure to efficiently model high-order neighbor relationships. ii) Multimodal Models: MMGCN [27], GRCN [26]: Optimize information dissemination through specific modal graph and graph reconstruction respectively. MGCN [30]: Build multimodal collaboration graphs through feature fusion for recommendation. SLMRec [23], FREEDOM [34], LGMRec [7]: Capturing cross-modal semantic relationships based on self-supervised or comparative learning. 1Datasets are available at http://jmcauley.ucsd.edu/data/amazon/links.html 5.1.3 Evaluation Protocols. For fair comparison, we follow the same evaluation setup as in [7, 30, 34]. Specifically, we randomly split each users interaction history into training, validation, and test sets with ratio of 8:1:1 to ensure the robustness of the training process and the fairness of the evaluation. During the evaluation phase, we adopt two widely used metrics, Recall@K and NDCG@K, to assess the models ability to retrieve relevant items and its ranking quality in top-K recommendations. To ensure comparability across different models, we report the average performance across all users in the test set for = 10 and = 20. Implementation Details. We implemented the proposed model 5.1.4 and all comparison baseline methods based on the unified opensource framework MMRec [33] and evaluated on GeForce RTX4090 GPU card with 24 GB memory. To ensure the repeatability and fairness of experiments, all models were trained using the Adam optimizer [12] and adjusted with reference to the optimal hyperparameter configurations reported in each baseline paper. In addition, we uniformly set the embedding dimension of users and items to 64, and use the Xavier [5] initialization method to initialize model parameters. During the training phase, we set the batch size to 2048, the maximum number of training rounds to 1000, and triggered the early stop mechanism when Recall@20 on the validation set failed to improve for 20 consecutive times to avoid overfitting. The learning rate was set to 0.001, with decay factor of 0.96 every 50 epochs. For regularization, we used weight decay of 1 104 and contrastive learning loss (ğ‘ğ‘™_ğ‘™ğ‘œğ‘ ğ‘ ) of 0.01. Additionally, we incorporated Maximum Mean Discrepancy (MMD) [6] with varying weights [0.1, 0.15, 0.2] and kernel bandwidths [1.0, 1.5, 2.0]. reduction factor of 8 was applied during feature extraction to improve model efficiency. To further ensure the stability of the results, the same random seed was used in all implementations of the baseline model."
        },
        {
            "title": "5.2 Overall Performance (RQ1)\nTable. 2 shows the performance of the proposed MambaRec model\ncompared with other baseline models on three datasets. From the\ntable, we can clearly see the following results:",
            "content": "(1) MambaRec performs significantly better than general recommendation models and multimodal recommendation models. Specifically, the core advantage of our model lies in its multi-level modal alignment and fusion mechanism. At the local level, the model introduces dilated refinement attention module (DREAM), which enhances fine-grained representations of image and text modalities. At the global level, the model explicitly aligns image and text distributions using the Maximum Mean Discrepancy (MMD) constraint, which effectively alleviates cross-modal semantic bias. In addition, through channel attention and spatial attention mechanisms, our model achieves importance assessment and adaptive fusion of modal features, while the designed dimensional compression mechanism significantly reduces computing and memory overhead while maintaining representation capabilities. As result, MambaRec performed better than the existing baseline model on all three datasets. (2) Most benchmark models suffer from noise interference and insufficient extraction of effective information when processing modal features. Specifically, BPR [21] and LightGCN [10] Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation CIKM 25, November 1014, 2025, Seoul, Republic of Korea Table 2: The overall performance of MambaRec and other baseline models on three datasets. The best result shown in boldface and the next-best is underlined. The t-tests verified the significance of performance improvements with ğ‘-value < 0.01."
        },
        {
            "title": "Clothing",
            "content": "Recall@10 0.0382 Recall@20 0.0595 NDCG@10 0.0214 NDCG@20 0.0263 Recall@10 0.0417 Recall@20 0.0633 NDCG@10 0.0232 NDCG@20 0.0288 Recall@10 0.0200 Recall@20 0.0295 NDCG@10 0.0111 NDCG@20 0.0135 0.0453 0.0728 0.0246 0.0317 0.0542 0.0837 0.0300 0.0376 0.0338 0.0517 0.0185 0. 0.0425 0.0663 0.0223 0.0284 0.0561 0.0857 0.0307 0.0384 0.0281 0.0410 0.0157 0.0190 0.0424 0.0668 0.0223 0.0286 0.0386 0.0627 0.0204 0.0266 0.0224 0.0362 0.0118 0. 0.0534 0.0831 0.0288 0.0365 0.0607 0.0922 0.0325 0.0406 0.0428 0.0663 0.0227 0.0287 0.0545 0.0837 0.0296 0.0371 0.0676 0.1017 0.0374 0.0462 0.0461 0.0696 0.0249 0. 0.0627 0.0992 0.0330 0.0424 0.0717 0.1089 0.0385 0.0481 0.0629 0.0941 0.0341 0.0420 0.0620 0.0964 0.0339 0.0427 0.0729 0.1106 0.0397 0.0496 0.0641 0.0945 0.0347 0. 0.0644 0.1002 0.0349 0.0440 0.0720 0.1068 0.0390 0.0480 0.0555 0.0828 0.0302 0.0371 0.0660 0.1013 0.0363 0.0454 0.0763 0.1147 0.0416 0.0514 0.0673 0.0996 0.0367 0.0449 completely ignore multimodal content, which makes it difficult for them to cope with cold starts and sparse content-based scenarios, and while VBPR [9] introduces visual features, it lacks structural modeling and has limited representation capabilities. Although MMGCN [27], GRCN [26], and other graph neural network methods integrate graph structure and modal features, they use static or linear fusion strategies, which makes it difficult to effectively filter out noise information. Some methods attempt to alleviate the problems faced. SLMRec [23] enhances modal discrimination through self-supervision tasks, and FREEDOM [34]introduces graph structure freezing strategy to reduce error propagation. Problems of noise interference and insufficient information extraction still exist. In contrast, our proposed MambaRec model adaptively filters noise and enhances key modal features by introducing DREAM module that combines multi-scale convolution and dual attention mechanisms. Our method has achieved the best recommendation performance. (3) Modal alignment can indirectly alleviate the problems of noise interference between modes and insufficient information extraction. Although MGCN [30] introduces multimodal information into collaborative modeling, it is difficult to effectively suppress modal noise and extract fine-grained semantics due to the use of linear projection and static fusion strategies. In contrast, our method introduces local feature enhancement and global distribution alignment mechanisms. The DREAM module enhances critical information and dynamically suppresses redundant features, while MMD loss reduces cross-modal distribution differences and mitigate noise interference at the representation level. The two work together to demonstrate stronger anti-interference capabilities and optimal recommendation performance. Figure 3: Ablation studies on the proposed MambaRec"
        },
        {
            "title": "5.3 Ablations Studies (RQ2)\nTo verify the effectiveness of each module, we conducted ablation\nexperiments from two dimensions. On the one hand, we analyze\nthe impact of local and global modal alignment modules on rec-\nommendation performance, and on the other hand, we explore\nthe contribution of different modal input combinations to model\neffectiveness.",
            "content": "5.3.1 Effect of Alignment Modules. We divide the proposed model into three different variants: (i) MambaRec, (ii) MambaRec without CIKM 25, November 1014, 2025, Seoul, Republic of Korea Kelin Ren, Chan-Yang Ju, and Dong-Ho Lee Table 3: Performance Comparison on multi-modalities Datasets Modality R@ R@20 N@10 N@20 Baby Sports Clothing Text Visual Full Text Visual Full Text Visual Full 0.0561 0.0485 0.0660 0.0685 0.0575 0.0763 0.0601 0.0406 0. 0.0863 0.0761 0.1013 0.1024 0.0852 0.1147 0.0900 0.0627 0.0996 0.0307 0.0270 0.0363 0.0373 0.0310 0.0416 0.0327 0.0217 0. 0.0384 0.0341 0.0454 0.0460 0.0382 0.0514 0.0403 0.0273 0.0449 local feature alignment (w/o LA), (iii) MambaRec without global distribution alignment (w/o GA). The results are shown in figure. 3: The complete MambaRec model performed well on all evaluation metrics and datasets. In contrast, once the modal alignment module is removed, model performance drops significantly. The performance degradation is even more pronounced in the absence of local modal alignment module, indicating that the module is critical in characterizing modal details and capturing local associations. Although the global alignment module focuses on overall semantic alignment, its absence can also weaken the fusion effect. 5.3.2 Effect of Modalities. In order to further evaluate the impact of each modality, we conducted experiments under different input conditions: text input covers text modal content, visual input contains visual modal related information, and full input combines both text and visual modalities. As shown in Table 3, integrating information from text and visual modalities always leads to optimal performance. In contrast, if the model relies on only single modality, its recommendation effectiveness will be reduced to varying degrees. This phenomenon reflects the complementary characteristics of multimodal information in describing user preferences. Text modalities can better reflect the semantic attributes of items, while visual modalities intuitively present the appearance characteristics of items, and they have their own advantages in different usage scenarios."
        },
        {
            "title": "5.4 Sensitivity Analysis (RQ3)\n5.4.1 Effects of the weight of ğœ†ğ‘ğ‘™ . Figure. 4 shows the modelâ€™s per-\nformance on the Baby and Sports datasets, with the comparative\nlearning loss weight ğœ†ğ‘ğ‘™ ranging from [10âˆ’4, 1]. It can be observed\nthat when ğœ†ğ‘ğ‘™ increases from 10âˆ’4 to 0.01, both Recall@20 and\nNDCG@20 are steadily improving, and when ğœ†ğ‘ğ‘™ exceeds 0.01,\nmodel performance drops significantly. The experimental results\nshow that ğœ†ğ‘ğ‘™ = 0.01 is the optimal setting on both datasets. Too\nsmall the weight will limit the role of the contrast signal and cannot\neffectively improve the representation ability. However, too large\nthe weight will cause auxiliary tasks to dominate the training pro-\ncess and weaken the optimization effect of the recommended target.\nTherefore, moderate contrast loss helps enhance the consistency\nand discrimination of modal representations, thereby improving\nthe overall recommendation performance of the model.",
            "content": "Figure 4: Variation of MambaRec with ğœ†ğ‘ğ‘™ Figure 5: Variation of MambaRec with ğœ†ğ‘šğ‘šğ‘‘ 5.4.2 Effects of the weight of ğœ†ğ‘šğ‘šğ‘‘ . We compared Recall@20 performance on Baby and Sports datasets under different combinations of reduction factors and maximum mean discrepancy weight (ğœ†ğ‘šğ‘šğ‘‘ ), as shown in Figure. 5. The results show favorable performance region within the parameter space, with several combinations achieving comparable results. The combination of reduction factor 8 and ğœ†ğ‘šğ‘šğ‘‘ = 0.15 performs consistently well on both datasets, though other nearby parameter settings also demonstrate competitive performance. Moderate compression ratios can give better results than excessive compression or almost no compression because moderate compression can remove redundant parameters and noise without serious loss of characterization capabilities. ğœ†ğ‘šğ‘šğ‘‘ , which is the strength coefficient of modal alignment, also has significant impact on performance. Too small value may lead to insufficient alignment of different modal features and ineffective information fusion, while too large value may introduce irrelevant noise and reduce model discrimination capabilities. 5.4.3 Effects of Reduction Factor. To balance model performance and computing resource consumption, we examined three indicators under different compression factors for Baby and Sports datasets, namely, Recall@20, model parameter count as percentage Figure 6: Impact of Reduction Factor Modality Alignment with Multi-scale Bilateral Attention for Multimodal Recommendation CIKM 25, November 1014, 2025, Seoul, Republic of Korea of benchmark, and computation time as percentage of benchmark, as shown in Figure. 6. Experimental results demonstrate that as compression factor increases, model parameters and computation time exhibit continuous decline, while Recall@20 shows nonlinear fluctuation patterns, decreasing at moderate compression levels. Uncompressed models achieve superior performance but incur highest resource costs. Excessive compression significantly reduces parameters and time, but insufficient representation capability leads to notable performance degradation. At compression factor 8, Recall@20 reaches peak levels while reducing parameters by over 30% and significantly decreasing computation time. Model compression not only reduces resource burden, but also suppresses overfitting noise to certain extent, thereby improving generalization stability [3, 8]. It is important to choose reasonable compression factor in resource-constrained deployment scenarios [15]."
        },
        {
            "title": "5.5 Visualization Analysis (RQ4)\nTo verify the effectiveness of MambaRec in multimodal fusion,\nwe selected the classic multimodal model VBPR as a baseline for\ncomparison, used t-SNE [25] to reduce the dimensions of the fu-\nsion features to two dimensions, and visualized their distribution\nthrough Gaussian kernel density estimation, as shown in Figure. 7\nand Figure. 8. In these figures, the Baby dataset is represented in\nred and the Sports dataset in blue.",
            "content": "From the visualization results, the embedding distribution of the VBPR model shows significant feature clustering effects and multi-modal concentration patterns, which indicates that the model suffers from representation degradation problem [4, 19] when processing multimodal features, where different semantic categories of information concentrate in overlapping regions of the embedding space, making effective discrimination challenging. In contrast, MambaRec has more uniform embedding distribution, wide coverage and smooth kernel density estimation (KDE) [24] curve, indicating more discriminatory representation. In summary, the modal alignment mechanism not only fundamentally alleviates the semantic degradation and modal overlap problems that are easy to occur in traditional methods, but also significantly improves the overall performance of the multimodal recommendation system in terms of expression power and representation structure, which verifies the effectiveness and superiority of the MambaRec model."
        },
        {
            "title": "6 Conclusion\nIn this paper, we propose MambaRec, a novel multimodal recom-\nmendation framework that achieves local feature alignment via the\nDilated Refinement Attention Module (DREAM)â€”a mechanism that\nleverages multi-scale dilated convolutions combined with channel-\nwise and spatial attention to mitigate fusion challenges arising from\nmodality heterogeneity. To ensure global distribution-level align-\nment, MambaRec integrates Maximum Mean Discrepancy (MMD)\nloss and contrastive learning to enforce semantic consistency across\nmodalities. Moreover, a tailored dimensionality reduction scheme\nis introduced to substantially reduce memory consumption and\nenhance model scalability and deployment efficiency. Overall, Mam-\nbaRec offers an efficient, robust, and scalable solution for multi-\nmodal recommendation, with strong potential for extension to more\ncomplex modalities such as video and audio in future work.",
            "content": "Figure 7: Distribution of representations for Baby Datasets Figure 8: Distribution of representations for Sports Datasets Acknowledgments This work was partly supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS-2022-00155885, Artificial Intelligence Convergence Innovation Human Resources Development (Hanyang University ERICA)) and the MSIT (Ministry of Science and ICT), Korea, under the Convergence security core talent training business support program (IITP-2024-RS-2024-00423071) supervised by the IITP(Institute of Information & Communications Technology Planning & Evaluation) and the MSIT (Ministry of Science, ICT), Korea, under the National Program for Excellence in SW), supervised by the IITP (Institute of Information & communications Technology Planing & Evaluation) in 2025(2024-0-00058). CIKM 25, November 1014, 2025, Seoul, Republic of Korea Kelin Ren, Chan-Yang Ju, and Dong-Ho Lee GenAI Usage Disclosure This work has been moderately polished using the generative AI tool ChatGPT to improve the clarity and accuracy of the language. All research content, data analysis and academic opinions are independently conceived and completed by us. References [1] Jie Cai, Xin Wang, Haoyang Li, Ziwei Zhang, and Wenwu Zhu. 2024. Multimodal graph neural architecture search under distribution shifts. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 82278235. [2] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. 2017. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017). [3] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. 2017. survey of model arXiv preprint compression and acceleration for deep neural networks. arXiv:1710.09282 (2017). [4] Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019. Representation degeneration problem in training natural language generation models. arXiv preprint arXiv:1907.12009 (2019). [5] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 249256. [6] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard SchÃ¶lkopf, and Alexander Smola. 2012. kernel two-sample test. The Journal of Machine Learning Research 13, 1 (2012), 723773. [7] Zhiqiang Guo, Jianjun Li, Guohui Li, Chaoyang Wang, Si Shi, and Bin Ruan. 2024. LGMRec: local and global graph learning for multimodal recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 84548462. [8] Song Han, Huizi Mao, and William Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015). [9] Ruining He and Julian McAuley. 2016. VBPR: visual bayesian personalized ranking from implicit feedback. In Proceedings of the AAAI conference on artificial intelligence, Vol. 30. [10] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 639648. [11] Yungi Kim, Taeri Kim, Won-Yong Shin, and Sang-Wook Kim. 2024. MONET: Modality-embracing graph convolutional network and target-aware attention for multimedia recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining. 332340. [12] Diederik Kingma and Jimmy Ba. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [13] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems 34 (2021), 96949705. [14] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 32. [15] Defu Lian, Haoyu Wang, Zheng Liu, Jianxun Lian, Enhong Chen, and Xing Xie. 2020. Lightrec: memory and search-efficient recommender system. In Proceedings of The Web Conference 2020. 695705. [16] Kang Liu, Feng Xue, Dan Guo, Peijie Sun, Shengsheng Qian, and Richang Hong. 2023. Multimodal graph contrastive learning for multimedia-based recommendation. IEEE Transactions on Multimedia 25 (2023), 93439355. [17] Qidong Liu, Jiaxi Hu, Yutian Xiao, Xiangyu Zhao, Jingtong Gao, Wanyu Wang, Qing Li, and Jiliang Tang. 2024. Multimodal Recommender Systems: Survey. ACM Comput. Surv. 57, 2, Article 26 (Oct. 2024), 17 pages. doi:10.1145/3695461 [18] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [19] Ruihong Qiu, Zi Huang, Hongzhi Yin, and Zijian Wang. 2022. Contrastive learning for representation degeneration problem in sequential recommendation. In Proceedings of the fifteenth ACM international conference on web search and data mining. 813823. [20] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). [21] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012). [22] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014). [23] Zhulin Tao, Xiaohao Liu, Yewei Xia, Xiang Wang, Lifang Yang, Xianglin Huang, and Tat-Seng Chua. 2022. Self-supervised learning for multimedia recommendation. IEEE Transactions on Multimedia 25 (2022), 51075116. [24] George Terrell and David Scott. 1992. Variable kernel density estimation. The Annals of Statistics (1992), 12361265. [25] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning research 9, 11 (2008). [26] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. 2020. Graph-refined convolutional network for multimedia recommendation with implicit feedback. In Proceedings of the 28th ACM international conference on multimedia. 35413549. [27] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In Proceedings of the 27th ACM international conference on multimedia. 14371445. [28] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. Cbam: Convolutional block attention module. In Proceedings of the European conference on computer vision (ECCV). 319. [29] Shuo Yang, Zhaopan Xu, Kai Wang, Yang You, Hongxun Yao, Tongliang Liu, and Min Xu. 2023. Bicro: Noisy correspondence rectification for multi-modality data via bi-directional cross-modal similarity consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1988319892. [30] Penghang Yu, Zhiyi Tan, Guanming Lu, and Bing-Kun Bao. 2023. Multi-view graph convolutional network for multimedia recommendation. In Proceedings of the 31st ACM international conference on multimedia. 65766585. [31] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. 2021. Mining latent structures for multimedia recommendation. In Proceedings of the 29th ACM international conference on multimedia. 38723880. [32] Hongyu Zhou, Xin Zhou, Zhiwei Zeng, Lingzi Zhang, and Zhiqi Shen. 2023. comprehensive survey on multimodal recommender systems: Taxonomy, evaluation, and future directions. arXiv preprint arXiv:2302.04473 (2023). [33] Xin Zhou. 2023. Mmrec: Simplifying multimodal recommendation. In Proceedings of the 5th ACM International Conference on Multimedia in Asia Workshops. 12. [34] Xin Zhou and Zhiqi Shen. 2023. tale of two graphs: Freezing and denoising graph structures for multimodal recommendation. In Proceedings of the 31st ACM International Conference on Multimedia. 935943. [35] Xin Zhou, Hongyu Zhou, Yong Liu, Zhiwei Zeng, Chunyan Miao, Pengwei Wang, Yuan You, and Feijun Jiang. 2023. Bootstrap latent representations for multimodal recommendation. In Proceedings of the ACM web conference 2023. 845854."
        }
    ],
    "affiliations": [
        "Department of Applied Artificial Intelligence Hanyang University Ansan, Republic of Korea",
        "Department of Computer Science and Engineering Hanyang University Ansan, Republic of Korea"
    ]
}