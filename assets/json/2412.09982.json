{
    "paper_title": "SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video",
    "authors": [
        "Jongmin Park",
        "Minh-Quan Viet Bui",
        "Juan Luis Gonzalez Bello",
        "Jaeho Moon",
        "Jihyong Oh",
        "Munchurl Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, a COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is a novel Motion-Adaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with a small number of control points. For MAS, we introduce a Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present a joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGS's robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 2 8 9 9 0 . 2 1 4 2 : r SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video Jongmin Park1* Minh-Quan Viet Bui1* Juan Luis Gonzalez Bello1 Jaeho Moon1 Jihyong Oh2 Munchurl Kim1 2Chung-Ang University 1KAIST {jm.park, bvmquan, juanluisgb, jaeho.moon, mkimee}@kaist.ac.kr jihyongoh@cau.ac.kr https://kaist-viclab.github.io/splinegs-site/ Figure 1. Our SplineGS achieves state-of-the-art rendering quality with fast rendering speed for novel spatio-temporal view synthesis from monocular videos without relying on pre-computed camera parameters. (a) We use our predicted camera parameters for [21, 49] since COLMAP [38] is unable to provide reasonable camera parameters for most scenes in the DAVIS dataset [35]. (b) SplineGS achieves 1.1 dB higher PSNR and 8,000 faster rendering speed compared to the second-best method on the NVIDIA dataset [50]."
        },
        {
            "title": "Abstract",
            "content": "Synthesizing novel views from in-the-wild monocular videos is challenging due to scene dynamics and the lack of multi-view cues. To address this, we propose SplineGS, COLMAP-free dynamic 3D Gaussian Splatting (3DGS) framework for high-quality reconstruction and fast rendering from monocular videos. At its core is novel MotionAdaptive Spline (MAS) method, which represents continuous dynamic 3D Gaussian trajectories using cubic Hermite splines with small number of control points. For MAS, we introduce Motion-Adaptive Control points Pruning (MACP) method to model the deformation of each dynamic 3D Gaussian across varying motions, progressively pruning control points while maintaining dynamic modeling integrity. Additionally, we present joint optimization strategy for camera parameter estimation and 3D Gaussian attributes, leveraging photometric and geometric consistency. This eliminates the need for Structure-from-Motion preprocessing and enhances SplineGSs robustness in real-world conditions. Experiments show that SplineGS significantly outperforms state-of-the-art methods in novel view synthesis quality for dynamic scenes from monocular videos, achieving thousands times faster rendering speed. 1. Introduction Novel View Synthesis (NVS) is fundamental to 3D vision, supporting applications like virtual reality (VR), augmented reality (AR), and film production. NVS aims to generate images from any viewpoint in scene, requiring accurate reconstruction from multiple 2D images. NeRF [30] has ad- *Co-first authors (equal contribution). Co-corresponding authors. vanced the field of NVS by utilizing learned implicit functions to represent static scenes, though it requires considerable training and rendering time. Recently, 3D Gaussian Splatting (3DGS) [17] has revolutionized this process by replacing implicit volumetric rendering with differentiable rasterization of 3D Gaussians, enabling real-time rendering and providing more explicit scene representation. For NVS of dynamic scenes, prior works have extended the static scene representations in [17, 30] by incorporating deformation models in canonical space using implicit representations [11, 27, 31, 32, 49], grid-based models that decompose the 4D space-time domain into multiple 2D planes [4, 5, 9, 39, 46], and polynomial trajectories [21]. However, these methods face several challenges: implicit representations significantly increase computational overhead and reduce rendering speed [5, 46]; grid-based models struggle to fully capture the dynamic nature of scene structures, hindering their ability to model fine details accurately [21]; and although polynomial trajectories improve efficiency, their fixed degree restricts flexibility for representing complex motions. To the best of our knowledge, none of the dynamic 3DGS-based methods provide experimental evidence to reliably support their novel spatio-temporal view synthesis capabilities for rendering unseen intermediate time indices. Additionally, most existing methods for dynamic NVS rely heavily on external camera parameter estimation methods like COLMAP [38], which often produce imprecise results for challenging in-the-wild monocular videos. To address the aforementioned issues for modeling scene dynamics, we exploit spline-based model to dynamic 3D Gaussian trajectories, inspired by classic 3D-curve modeling in computer graphics [8]. Splines are widely used in geometric modeling for graphical applications, providing smooth and continuous representations of complex shapes with minimal number of control points [2, 7]. This efficiency makes them ideal for modeling intricate geometric structures while maintaining both flexibility and preciIn this paper, we propose SplineGS, framework sion. for high-quality dynamic scene reconstruction and real-time neural rendering from in-the-wild monocular videos without relying on external camera estimators like COLMAP [38]. SplineGS introduces Motion-Adaptive Spline (MAS), based on cubic Hermite splines [2, 7], to effectively represent dynamic 3D Gaussian deformations. Our MAS consists of piecewise cubic functions defined by control points that dictate each segments curvature and direction. Each control point is adjustable and optimized as learnable parameter, enabling faster and more precise modeling of complex motion trajectories. Additionally, to adaptively model the trajectory of each dynamic 3D Gaussian based on motion complexity during training, we introduce Motion-Adaptive Control points Pruning (MACP) method that adjusts the number of control points to improve rendering quality and efficiency. Furthermore, we incorporate camera parameter estimation method jointly optimized with 3D Gaussian attributes and MAS, leveraging photometric and geometric consistency, thus eliminating the need for external estimators. Experiments show that SplineGS significantly outperforms state-of-the-art (SOTA) dynamic NVS methods both qualitatively and quantitatively, offering faster rendering speed, as shown in Fig. 1. Our contributions are as follows: We propose novel Spline-based dynamic 3D Gaussian Splatting framework, SplineGS, which is (i) COLMAPfree, (ii) very fast and (iii) of high quality in reconstructing the dynamic scenes from in-the-wild monocular videos; novel Motion-Adaptive Spline (MAS) is introduced, which can accurately and effectively represent the continuous trajectory of each dynamic 3D Gaussian; Motion-Adaptive Control points Pruning (MACP) method is presented, which can efficiently adjust the number of control points for each spline function, optimizing both rendering quality and the efficiency of MAS. 2. Related Work Dynamic NeRF. Recent advancements in video view synthesis have extended the static NeRF model [30] to represent scene dynamics. These include dynamic NeRFs using scene flow-based frameworks [11, 22, 23], deformation estimation in canonical fields [1, 3, 14, 27, 31, 32, 36, 40, 43, 45, 47], and 4D grid-based spatio-temporal radiance fields [4, 5, 9, 39]. Techniques such as NSFF [22], DynNeRF [11], and DynIBaR [23] combine time-independent and time-dependent radiance fields to synthesize novel spatiotemporal perspectives from monocular videos. Despite these advancements, current dynamic NeRFs still fall short of recent dynamic 3D Gaussian Splatting (3DGS) techniques in rendering quality and efficiency. Dynamic 3DGS. The improvements in rendering quality and speed achieved by 3DGS [17] have inspired further studies [13, 21, 24, 28, 46, 49] to extend the static 3DGS framework to dynamic scenes by enabling the deformation of 3D Gaussian attributes. The pioneering work on dynamic 3DGS [28] introduces time-dependent offsets for the positions and rotations of dynamic 3D Gaussians via an MLP; however, this approach slows down the rendering. D3DGS [49] builds on this concept with an annealing smoothing training mechanism to improve temporal smoothness and rendering quality. 4DGS [46] replaces the MLP network with grid-based structure to boost efficiency, though this change requires quality trade-offs due to the resolution limits of grid-based methods. In contrast, SC-GS [13] combines an MLP deformation network with sparse spatial deformation, reducing the computational cost of MLP while maintaining quality. STGS [21] intro2 duces polynomial trajectories for motion modeling, improving speed and quality over implicit representations. CasualFVS [19] warps dynamic content from neighboring frames, and MoSca [20] proposes graph-based motion modeling approach to handle sparse control deformation. Very recently, concurrent work [18] proposes modeling dynamic 3D Gaussian trajectories using polynomial interpolation between fixed keyframes for NVS from multi-view videos with COLMAP [38] assistance. Unlike [18], our SplineGS adaptively optimizes the deformation of dynamic 3D Gaussians, accounting for varying motion degrees and types in in-the-wild monocular videos, without requiring preprocessed camera parameters (COLMAP-free approach). Neural Rendering without SfM Preprocessing. Accurate camera parameters, including extrinsics and intrinsics, are essential for neural rendering approaches to capture fine details [15, 25]. However, in real-world settings, camera parameters derived from Structure-from-Motion (SfM) algorithms such as COLMAP [38] often exhibit pixel-level inaccuracies [26, 37], compromising the structural details of rendered scenes [27]. To address this, several NeRF methods [25, 29, 33, 44] jointly optimize NeRF architectures and camera parameters. Recently, local-to-global training approach [10] is introduced to optimize both camera parameters and 3D Gaussians. However, these methods are limited to static scenes. For dynamic scene reconstruction, RoDynRF [27] and MoSca [20] use motion masks to gather multi-view cues from static regions, allowing robust rendering without pre-computed camera parameters. Our SplineGS is also COLMAP-free, significantly outperforming RoDynRF [27] and MoSca [20] in rendering quality and efficiency, enabled by our novel spline-based architecture. 3. Preliminary: 3D Gaussian Splatting 3DGS [17] represents the radiance field of scene using anisotropic 3D Gaussians, each of which is formulated as G(x) = exp((1/2)(x µ)Σ1(x µ)), (1) where R3 denotes 3D position, and µ R3 and Σ R33 represent the mean (center) and the covariance matrix of the 3D Gaussian, respectively. To ensure that Σ is positive semi-definite and contains physical meaning, it is decomposed into diagonal scaling matrix R33 of scale vector R3 and rotation matrix R33 as Σ = RSSR, where is parameterized by learnable unit-length quaternion R4. In addition, each 3D Gaussian is parameterized by an opacity σ and color R3. To render the color of each pixel, the color and the opacity of each 3D Gaussian are computed using Eq. 1, and the rendered color is computed by the alpha-blending of the ordered 3D Gaussians overlapping the pixel as = (cid:80) iN ciαi (cid:81)i1 j=1(1 αj), (2) where ci is the color of the ith 3D Gaussian and αi is density of the ith 3D Gaussian which is given by evaluating the 2D covariance Σ R22. Here, Σ is formulated as Σ = JW ΣW , where is the Jacobian of the affine approximation of the projective transformation and is viewing transformation matrix. Similar to prior works [20, 24], we extend 3DGS [17] = 1, 2, ..., nst} to union of static 3D Gaussians {Gst and dynamic 3D Gaussians {Gdy = 1, 2, ..., ndy} to represent static backgrounds and moving objects, respectively, in dynamic scenes. We maintain the same gradient-based } and dynamic {Gdy densification [17] for both static {Gst } 3D Gaussians. Following STGS [21], we use the splatted feature rendering to predict the final pixel colors. For static regions, we remove the time-encoded feature while preserving the diffuse and specular features. We model the mean µi of Gdy as time-dependent variable, defined by our novel deformation modeling method. We compute the time-dependent rotation qi and scale si by modeling them as learnable parameters of time. 4. Proposed Method: SplineGS 4.1. Overview of SplineGS We describe the overall architecture of SplineGS in Fig. 2. Given monocular video {Itt = 0, 1, ..., Nf 1} where Nf is the total number of frames, SplineGS is designed to synthesize high-quality novel spatio-temporal views with fast rendering speed, and to estimate the camera parameters, including the extrinsics [ ˆRt ˆTt] R34 for each time t, and the shared intrinsic ˆK R33. As shown in Fig. 2, to stabilize joint optimization of the 3D Gaussian attributes and camera parameter estimation, we adopt two-stage optimization process consisting of warm-up stage and main training stage for our SplineGS architecture. In the warm-up stage, we coarsely optimize the camera parameters [ ˆRt ˆTt] and ˆK using photometric and geometric consistency. In the main training stage, we initialize the 3D Gaussians based on the estimated camera poses and jointly optimize the 3D Gaussian attributes with the camera parameter estimation. Specifically, for each dynamic 3D Gaussian, we propose novel spline-based deformation modeling method, called Motion-Adaptive Spline (MAS), which utilizes cubic Hermite spline function [2, 7] to accurately model the continuous trajectory with small number of control points. For MAS, we introduce MotionAdaptive Control points Pruning (MACP) method that effectively adjusts the number of control points for each dynamic 3D Gaussian, taking into account the objects motion types and degrees. In the following sections, we detail the process of our MAS method in Sec. 4.2, followed by the camera parameter estimation process in Sec. 4.3. Finally, we describe the 3 Figure 2. Overview of SplineGS. Our SplineGS leverages spline-based functions to model the deformation of dynamic 3D Gaussians with novel Motion-Adaptive Spline (MAS) architecture. It is composed of sets of learnable control points based on cubic Hermite spline function [2, 7] to accurately model the trajectory of each dynamic 3D Gaussian and to achieve faster rendering speed. To avoid any preprocessing of camera parameters, i.e. COLMAP-free, we adopt two-stage optimization: warm-up and main training stages. overall optimization process in Sec. 4.4. 4.2. Motion-Adaptive Spline for 3D Gaussians To represent the continuous trajectory of each dynamic 3D Gaussian for moving objects over time, we propose MAS which modifies the mean parameter to set of learnable control points. For this, we use the cubic Hermite spline function [2, 7] to model the mean of each dynamic 3D Gaussian at time as µ(t) = S(t, P), (3) where = {pkpk R3}k[0,Nc1] is set of Nc learnable control points, serving as an additional attribute for each dynamic 3D Gaussian, and S(t, P) is formulated as (4) S(t, P) = (2t3 3t2 +(2t3 2t2 + 1)pts + (t3 + 3t2 ts = tn(Nc 1), r)pts+1 + (t3 + tr)mts r)mts+1, tn = t/(Nf 1), t2 tr = ts ts, where mk is the approximated tangent of the control point pk, formulated as mk = (pk+1 pk1)/2. Note that S(t, P) indicates the piecewise cubic function to represent the whole continuous trajectory of each dynamic 3D Gaussian, and Nc can be different from Nf . The optimal Nc can be estimated by Motion-Adaptive Control points Pruning (MACP) in the following section. Control Points Initialization. To stably optimize S(t, P), it is essential to initialize the control points with appropriate geometric considerations that ensure temporal consistency. For this, we leverage the long-range 2D tracking [16] φtr and the metric depth [34] priors. Let = {φtr R2}t[0,Nf 1] be 2D track, πK() be projection function from the camera space to the image space with the camera intrinsic K, and φtr be pixel coordinate corresponding to the 2D track at time t. We unproject each 2D track to the world space to compute the 3D track aided by the frames metric depth dt and camera extrinsic [ ˆRt ˆTt] at time t. The unprojection function Wt() from the image space to the world space is formulated as Wt(φtr ) = ˆR π1 ˆK (cid:0)φtr , dt(φtr )(cid:1) ˆR ˆTt. (5) It should be noted that we estimate [ ˆRt ˆTt] and ˆK from sequence of frames only, without using any given ground truth values, as mentioned in Sec. 4.3. Then, we initialize the per-Gaussian control points set P, by least-square (LS) approximation, such that the spline-described curve fits the initial curve by the tracker, given by (cid:80)Nf 1 t=0 (cid:13) (cid:13)Wt(φtr ) S(t, P)(cid:13) 2 2. (cid:13) min (6) Motion-Adaptive Control Points Pruning (MACP). An excessive number of control points may cause over-fitting and decrease the processing speed of our MAS module, leading to poorer rendering qualities with reduced rendering speed (see Table 3). Furthermore, as each scene exhibits various types and degrees of motion for moving objects, the number of control points required for each dynamic 3D Gaussian trajectory should be adaptively adjusted to accommodate the scene dynamics. To achieve this, we 4 propose the MACP method for MAS, which can generate sparser control points while ensuring no dynamic modeling degradation. MACP is designed on top of 3D Gaussian densification [17], but focuses on optimizing the number of control points in MAS. Our MACP computes new spline function S(t, P) after every 3D Gaussian densification step, R3}l[0,Nc2] is set of Nc 1 conwhere = {p trol points, which contains one-fewer control points than the current set of control points. We compute the LS approximation to find the reduced set of control points as (cid:13)S(t, P) S(t, P)(cid:13) (cid:13) 2 2. (cid:13) (cid:80)Nf 1 t=0 lp (7) min Then, the updated optimal set of control points is assigned with the values of if the error between S(t, P) and S(t, P) is smaller than threshold ϵ, as given by = (cid:40) P, if < ϵ P, otherwise , where 1 Nf = (cid:80)Nf 1 t= π ˆK( ˆRtS(t, P) + ˆTt) π ˆK( ˆRtS(t, P) + ˆTt)2 2. (8) By following MACP, each dynamic 3D Gaussian is allowed to have different number of control points. Therefore, the MACP can guide our MAS to have minimal number of control points when modeling simple motion, while having more control points for more complex motions (see Fig. 8). 4.3. Camera Parameter Estimation The traditional SfM methods, such as COLMAP [38], fail to reliably estimate camera parameters in dynamic scenes from in-the-wild monocular videos [27]. For this reason, we propose to estimate camera parameters for joint optimization with the 3D Gaussian attributes. For each frame at time t, we predict rotation matrix ˆRt R33 and translation vector ˆTt R3, representing the extrinsic parameters of monocular camera, using shallow MLP Fθ as [ ˆRt ˆTt] = Fθ(γ(t)), (9) where γ() is positional encoding [30]. We also predict the focal length ˆf of the camera intrinsic ˆK as learnable parameter that is shared across all frames in the monocular video. To accurately optimize the camera parameters, we enforce two types of consistencyphotometric and geometricfor the static background between random reference cameras [ ˆRtref ˆTtref ] and the target camera [ ˆRt ˆTt], encouraging alignment both visually and structurally. Photometric Consistency. Given the pre-computed metric depth [34], the camera intrinsics and extrinsics under optimization will converge as long as the projected reference frames color Itref(φttref ) is aligned to the target frames color It(φt). φttref is the reference pixel coordinate corresponding to the target frames pixel coordinate φt as ˆTt) + ˆTtref ). (cid:0)φt, dt(φt)(cid:1) ˆR φttref = π ˆK( ˆRtref ( ˆR (10) π1 ˆK We refer to such projection alignment as photometric consistency, which is encouraged by the loss Lpc given by Lpc = (cid:80) φt (cid:13)Mt,tref(φt) (It(φt) Itref (φttref))(cid:13) (cid:13) 2 2, (cid:13) (11) where is the Hadamard product [12], and Mt,tref is union motion mask that excludes dynamic objects in It and Itref for removing the inconsistencies due to dynamic regions, which is given by Mt,tref(φt) = Mt(φt)Mtref(φttref), (12) where Mt and Mtref are pre-computed motion masks [48] of It and Itref, respectively. Note that motion masks value is 0 for static regions, and 1 otherwise. Geometric Consistency. Along with the photometric consistency, we compute the geometric consistency of unprojected pixels in 3D space to make our optimization more stable. The geometric consistency loss is formulated as Lgc = (cid:80) φt (cid:13) (cid:13)Mt,tref(φt) (Wt(φt) Wtref (φttref))(cid:13) 2 2, (13) (cid:13) where Wt(φt) and Wtref (φttref) are the corresponding 3D locations of φt and φttref, respectively (see Eq. 5). 4.4. Optimization To stabilize joint training of the MAS and the camera parameter estimation, we adopt two-stage optimization process consisting of the warm-up stage and the main training stage. In the warm-up stage, we optimize only the camera parameters using the photometric and geometric consistency. The total loss in the warm-up stage is given by Lwarm total = λpcLpc + λgcLgc. (14) After the warm-up stage, we obtain the coarsely predicted camera intrinsic ˆK and the set of extrinsics [ ˆR ˆT ] for all frames, which are then used to initialize the set of control points for each dynamic 3D Gaussian, as described in In the main training stage, we jointly optimize Sec. 4.2. the static and dynamic 3D Gaussians along with the camera parameter estimation based on the total loss function as Lmain total = λrgbLrgb + λdLd + λMLM + λpcLpc + λd-pcLd-pc + λgcLgc, (15) where Lrgb and Ld are the L1 losses between the rendered frame and the GT frame, and between the rendered depth and the GT depth, respectively. Furthermore, in the main training stage, we compute an additional photometric consistency loss Ld-pc that utilizes the rendered depth ˆd of the 3D Gaussians instead of the metric depth [34] prior as ˆd allows the estimated 3D Gaussian geometry to guide the joint optimization of the camera parameter estimation and 3D Gaussian attributes. PSNR / LPIPS Method Jumping Skating Truck Umbrella Balloon1 Balloon2 Playground Average FPS DynNeRF (ICCV21) [11] MonoNeRF (ICCV23) [42] STGS (CVPR24) [21] SCGS (CVPR24) [13] D3DGS (CVPR24) [49] 4DGS (CVPR24) [46] RoDynRF (CVPR23) [27] Casual-FVS (ECCV24) [19] Ex4DGS (NeurIPS24) [18] MoSca (arXiv) [20] RoDynRF (CVPR23) [27] MoSca (arXiv) [20] SplineGS (Ours) 24.68 / 0.090 24.26 / 0.091 20.82 / 0.187 15.68 / 0.920 22.02 / 0.266 22.37 / 0.178 25.66 / 0.071 23.45 / 0.100 18.93 / 0.321 25.21 / 0.083 24.27 / 0.100 25.43 / 0.080 25.50 / 0.068 32.66 / 0.035 32.06 / 0.044 24.80 / 0.109 14.88 / 0.908 24.06 / 0.227 26.72 / 0.084 28.68 / 0.040 29.98 / 0.045 21.92 / 0.233 32.77 / 0.033 28.71 / 0.046 32.62 / 0.033 33.72 / 0.031 28.56 / 0.082 27.56 / 0.115 25.01 / 0.103 23.81 / 0.140 23.04 / 0.247 25.93 / 0.097 29.13 / 0.063 25.22 / 0.090 19.04 / 0.308 28.22 / 0. 28.85 / 0.066 28.29 / 0.086 28.66 / 0.056 23.26 / 0.137 23.62 / 0.180 21.88 / 0.195 21.84 / 0.160 22.67 / 0.192 22.36 / 0.178 24.26 / 0.089 23.24 / 0.096 19.03 / 0.340 24.41 / 0.092 23.25 / 0.104 24.40 / 0.091 25.61 / 0.071 22.36 / 0.104 21.89 / 0.129 20.36 / 0.196 20.17 / 0.179 21.22 / 0.202 21.89 / 0.153 22.37 / 0.103 23.76 / 0.079 14.69 / 0.503 23.26 / 0.092 21.81 / 0.122 23.27 / 0.091 24.43 / 0.068 27.06 / 0.049 27.36 / 0.052 23.12 / 0.124 21.07 / 0.149 25.86 / 0.118 24.85 / 0.081 26.19 / 0.054 24.15 / 0.081 16.29 / 0.457 28.90 / 0. 25.58 / 0.064 29.01 / 0.042 28.37 / 0.032 24.15 / 0.080 22.61 / 0.130 19.23 / 0.151 20.71 / 0.115 22.30 / 0.111 21.36 / 0.089 24.96 / 0.048 22.19 / 0.074 14.16 / 0.437 23.05 / 0.060 25.20 / 0.052 23.23 / 0.058 24.19 / 0.047 26.10 / 0.082 25.62 / 0.106 22.17 / 0.152 19.74 / 0.367 23.02 / 0.195 23.64 / 0.123 25.89 / 0.067 24.57 / 0.081 17.72 / 0.371 26.55 / 0.070 25.38 / 0.079 26.61 / 0.069 27.21 / 0.053 0.05 0.05 900 110 25 95 0.45 48 84 N/A 0.45 N/A 400 COLMAP COLMAP-Free Table 1. Novel view synthesis evaluation on the NVIDIA dataset. Red and Blue denote the best and second-best performances, respectively. N/A denotes that the rendering speed for MoSca [20] is unavailable, as the authors have not provided official code. For Casual-FVS [19], we directly use the results from their paper, as official code is also unavailable. Figure 3. Visual comparisons for novel view synthesis on the NVIDIA dataset. In addition to the camera parameter estimation and imagery reconstruction losses, we adopt binary dice loss [41] LM between the pre-computed motion mask Mt [48] and the rendered motion mask ˆMt that can be derived from the dynamic 3D Gaussians. The binary dice loss initially proposed in [41] for highly imbalanced segmentation of medical imagery helps encouraging better separation between our dynamic and static 3D Gaussians as described by LM = 1 2((cid:80) ((cid:80) φt Mt(φt) ˆMt(φt)) + ε φt Mt(φt) + ˆMt(φt)) + ε , (16) where ε is smooth term to avoid numerical issues. ˆMt(φt) is computed by the alpha-blending of the 3D Gaussians overlapping φt (similar to Eq. 2) as ˆMt(φt) = (cid:80) iN miαi (cid:81)i1 j=1(1 αj), (17) where mi = 0 if the ith 3D Gaussian is the static 3D Gaussian and mi = 1 otherwise. In conjunction, all terms in Lmain total guide our SplineGS to effectively and efficiently model dynamic 3D scenes from pure monocular videos, achieving more structural details and better temporal consistency than previous works [11, 13, 1821, 27, 42, 46, 49], without relying on camera parameters obtained from external estimators [38]. 5. Experiments Implementation Details. To develop our method, we build on top of the widely used open-source 3DGS codebase [17]. Our SplineGS architecture is trained over 1K iterations in the warm-up stage and 20K iterations in the main training stage. We optimize the number of control points with the proposed MACP every 100 iterations. For depth and 2D tracking estimation, we employ the pre-trained models from UniDepth [34] and CoTracker [16], respectively. The learnable camera extrinsics [ ˆRt ˆTt] are initialized by [I0], while the initial learnable focal length value ˆf is set to 500. Datasets. We evaluate both the quantitative and qualitative performance of novel view and time synthesis on the widely used NVIDIA dataset [50] which features challenging monocular videos. Additionally, we assess novel view synthesis performance on in-the-wild monocular videos from the DAVIS dataset [35] which contains an average of 70 frames per video sequence. 5.1. Comparison with State-of-the-Art Methods Novel View Synthesis. Table 1 presents quantitative comparison of NVS between our SplineGS and existing COLMAP-based [11, 13, 18, 19, 21, 42, 46, 49] and COLMAP-free [20, 27] methods on the NVIDIA dataset [50]. For this comparison, we follow the evaluation configuration in [27]. The results demonstrate that our SplineGS significantly outperforms SOTA methods in both the PSNR and LPIPS [51] metrics. Notably, SplineGS achieves 890 and 8,000 faster rendering speed compared to RoDynRF [27] and DynNeRF [11], respectively. Ex4DGS [18] and STGS [21], which are designed for multi-view settings, face Method PSNR LPIPS tOF DynNeRF (ICCV21) [11] 4DGS (CVPR24) [46] D3DGS (CVPR24) [49] STGS (CVPR24) [21] RoDynRF (CVPR23) [27] SplineGS (Ours) 23.36 17.07 19.63 15.72 21.58 25.92 0.219 0.459 0.343 0.474 0.221 0. 0.921 6.314 3.225 2.105 2.138 0.703 COLMAP COLMAP-Free Table 2. Novel view and time synthesis evaluation on the NVIDIA dataset. Figure 5. Visual comparisons for novel view and time synthesis on the NVIDIA dataset. 3DGS-based methods [21, 46, 49] yield even more significant degradation when predicting unseen time indices. In contrast, SplineGS, built upon 3DGS [17] but equipped with our novel spline-based deformation, provides SOTA novel view rendering for unseen intermediate time. Thanks to our MAS, SplineGS naturally and precisely captures the continuous trajectories of moving objects over time, enhancing the temporal consistency of rendered scenes that can be checked in tOF scores [6] of Table 2. To further analyze the SplineGSs ability to model continuous trajectories of dynamic 3D Gaussians, we visualize the projected 2D motion tracking of dynamic objects in pixel space, comparing it with D3DGS [49] and STGS [21], as shown in Fig. 6. We observe that D3DGS [49] and STGS [21] cannot provide reliable motion tracking for moving objects, underscoring their limitations in modeling continuous trajectories of dynamic 3D Gaussians. In contrast, SplineGS provides accurate motion tracking, demonstrating the effectiveness of our MAS for deforming dynamic 3D Gaussians. More results are provided in Suppl. Figure 6. Visual comparisons for motion tracking. We visualize 2D pixel tracks to analyze motions of dynamic 3D Gaussians. 5.2. Ablation Study Motion-Adaptive Spline (MAS). To demonstrate the effectiveness of MAS, we replace the MAS model with various deformation models, including an MLP, grid-based model, polynomial functions of third degree (denoted as Poly (3rd)) and tenth degree (denoted as Poly (10th)), and Figure 4. Visual comparisons for novel view synthesis on the DAVIS dataset. challenges with inconsistent geometry alignment over time when trained on monocular videos. Furthermore, the LPIPS score of our SplineGS is consistently superior to all the methods across all scenes. Fig. 3 shows the qualitative comparison between our SplineGS with the existing methods in [11, 21, 27, 49]. As highlighted by the red boxes, our method yields not only higher rendering quality but also dynamic objects more aligned and closer to the ground truth. In Fig. 4, we show our superior NVS results for in-the-wild monocular videos from the DAVIS dataset [35] compared to the existing methods in [21, 27, 49]. Compared to [27] that is also COLMAPfree, our SplineGS yields considerably more detailed novel views (red boxes) as shown in Fig. 4. For the other methods [21, 49], we observe that COLMAP [38] fails to recover camera parameters and initial point clouds on the DAVIS dataset [35], as also claimed in [27]. On the other hand, our COLMAP-free SplineGS reconstructs accurate camera parameters that are the ones actually used to train [21, 49], which are shown in Fig. 4 for comparison. More results on DAVIS [35] are provided in the Suppl. Novel View and Time Synthesis. To evaluate the capability of SplineGS to model continuous trajectories of moving objects in scene, we compare the novel view and time synthesis results of SplineGS with those of NeRFbased [11, 27] and 3DGS-based [21, 46, 49] methods. For this evaluation, we follow the dataset sampling strategy in [22], which samples 24 timestamps from the NVIDIA dataset [50]. In addition, to simulate larger motion, we exclude frames with odd time indices in the training sets. To ensure all test timestamps are not seen during training, and thus, to create more challenging novel view and time synthesis validation, we exclude frames with even time indices in the test sets. Table 2 and Fig. 5 show the quantitative and qualitative comparisons for this challenging experiment. We observe that the NeRF-based methods, RoDynRF [27] and DynNeRF [11], generate inconsistent artifacts and blurriness for unseen times. Furthermore, the Figure 7. Visual comparisons for MACP ablation study. Bezier curve [8]. For the MLP, grid-based model, and polynomial functions, we apply to them the structures similar to those in prior works, including D3DGS [49], 4DGS [46], and STGS [21], respectively. Additionally, we implement the Bezier curve [8], commonly used method for curve modeling in computer graphics. Table 3-(a) presents quantitative comparisons of each 3D Gaussian trajectory model, focusing on rendering quality (PSNR, LPIPS) and deformation latency per Gaussian, denoted as gdef. This latency reflects the computational time required to estimate the deformation of single dynamic 3D Gaussian. As shown in Table 3-(a), our MAS model achieves superior rendering quality compared to all other deformation models. Consistent with analyses in previous works [21, 46, 49], we observe that the MLP and grid-based architectures require substantial computational costs for rendering. Among these methods, Poly (3rd), as implemented in [21], demonstrates the best latency. However, fixed-degree polynomial functions have limited flexibility across varying motion complexities which adversely impacts rendering performance. To explore this further, we experiment with Poly (10th) to assess changes in modeling capability. This adjustment, however, leads to noisier optimization and reduced efficiency, as variables under high exponents in Poly (10th) lead to numerical instability. The Bezier curve [8] offers the second-best rendering quality, but its latency remains higher than our MAS due to its recursive nature of computation. Motion-Adaptive Control Points Pruning (MACP). To assess the effectiveness of our MACP technique for MAS, we compare our full model with MACP against other versions of our model with two fixed numbers of control points Nc = 4 and Nc = Nf . As shown in Table 3-(c) and Fig. 7, our SplineGS with MACP achieves good trade-off between rendering quality and gdef compared to the ablated models with fixed Nc. Using Nc = 4 for every dynamic 3D Gaussian limits the motion modeling capacity of MAS, resulting in significantly lower metrics and visible artifacts in the dynamic regions. Moreover, an excessive Nc = Nf decreases the rendering speed of our MAS module and still falls short of the quality achieved by our full model with MACP, potentially due to motion overfitting. Fig. 8 shows the distribution of Nc values after optimization with MACP across scenes of varying motion complexities. In Fig. 8-(a), we visualize Nc Heatmap that contains the pixel-wise averaged Nc values of the dynamic 3D Gaussians needed to (a) Motion-Adaptive Spline (b) Loss function PSNR LPIPS gdef (ns) PSNR LPIPS MLP Grid Poly (3rd) Poly (10th) Bezier Ours 23.51 25.48 25.14 24.38 27.19 27.21 0.125 0.090 0.111 0.120 0.060 0. 149.41 98.89 1.80 7.71 8.78 5.63 w/o Lpc w/o Lgc w/o Ld-pc w/o LM Ours 17.49 26.33 26.18 26.34 27. 0.853 0.067 0.066 0.088 0.053 (c) Motion-Adaptive Control points Pruning PSNR LPIPS gdef (ns) w/o MACP (Nc = 4) w/o MACP (Nc = Nf ) Ours 26.62 27.08 27.21 0.065 0.054 0. 5.34 6.11 5.63 Table 3. Ablation studies. We ablate our framework and report the average results on the NVIDIA dataset with the same setting as Novel View Synthesis experiment in Sec. 5.1. (a) Nc Heatmaps as Figure 8. Analysis of MACPs Efficacy. the averaged Nc values of dynamic 3D Gaussians and their corresponding rendered frames ˆIt for Balloon2 and Skating scenes. (b) Histograms of the number of control points (Nc) in percentages (%) of dynamic 3D Gaussians in two scenes. render the 2D pixels (red higher, blue lower number of control points). As shown, the simpler motions in these scenes, such as those of the human bodies, can be modeled by the dynamic 3D Gaussians with smaller averages of Nc values, whereas the objects with complex and extensive motions (e.g. balloon) require higher averaged Nc values. Fig. 8-(b) presents the corresponding histogram of Nc values for the scenes dynamic 3D Gaussians. For sequences with simple motion, such as Skating, the trajectories of most dynamic 3D Gaussians can be represented using minimal Nc, thanks to our MACP. While Balloon2 has more evenly distributed Nc due to more complex and diverse motion. Loss Functions. Table 3-(b) shows the effectiveness of each loss for our overall SplineGS architecture. As noted, no consistent camera parameters can be learned without Lpc, drastically impacting the rendering quality of the dynamic 3D Gaussians. Also, our Lgc, Ld-pc and LM can considerably impact the overall rendering quality. 8 6. Conclusion We present SplineGS, COLMAP-free dynamic 3DGS framework designed for novel spatio-temporal view synthesis from monocular videos. Leveraging our innovative for dynamic motion Motion-Adaptive Spline (MAS) modeling, SplineGS efficiently renders high-quality novel views from complex in-the-wild videos. The effectiveness of our approach is validated through extensive quantitative and qualitative comparisons, significantly outperforming the existing SOTA methods with very fast rendering speed."
        },
        {
            "title": "References",
            "content": "[2] Harold Ahlberg, [1] Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH Asia 2022 Conference Papers, 2022. 2 Edwin Norman Nilson, and Joseph Leonard Walsh. The Theory of Splines and Their Applications: Mathematics in Science and Engineering: Series of Monographs and Textbooks, Vol. 38. Elsevier, 2016. 2, 3, 4, 1 [3] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, and Zhixin Shu. Rignerf: Fully controllable neural 3d portraits. In CVPR, 2022. 2 [4] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew OToole, and Changil Kim. Hyperreel: High-fidelity 6-dof video with rayconditioned sampling. In CVPR, 2023. 2 [5] Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In CVPR, 2023. 2 [6] Mengyu Chu, You Xie, Jonas Mayer, Laura Leal-Taixe, and Nils Thuerey. Learning temporal coherence via selfsupervision for gan-based video generation. ACM Transactions on Graphics (TOG), 2020. 7 [7] De Boor. practical guide to splines. Springer-Verlag google schola, 1978. 2, 3, 4, 1 [8] Gerald Farin. Curves and surfaces for CAGD: practical guide. Elsevier, 2001. 2, [9] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023. 2 [10] Yang Fu, Sifei Liu, Amey Kulkarni, Jan Kautz, Alexei A. Efros, and Xiaolong Wang. Colmap-free 3d gaussian splatting. In CVPR, 2024. 3 [11] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In ICCV, 2021. 2, 6, 7, 1 [12] Roger Horn and Charles Johnson. Matrix analysis. Cambridge university press, 2012. 5 [13] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. CVPR, 2024. 2, [14] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neuman: Neural human radiance field from single video. In ECCV, 2022. 2 9 [15] Yifan Jiang, Peter Hedman, Ben Mildenhall, Dejia Xu, Jonathan Barron, Zhangyang Wang, and Tianfan Xue. Alignerf: High-fidelity neural radiance fields via alignmentaware training. In CVPR, 2023. 3 [16] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv, 2023. 4, 6, 2 [17] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. 2, 3, 5, 6, 7 [18] Junoh Lee, Chang-Yeon Won, Hyunjun Jung, Inhwan Bae, and Hae-Gon Jeon. Fully explicit dynamic gaussian splatting. In NeurIPS, 2024. 3, 6, 1 [19] Yao-Chih Lee, Zhoutong Zhang, Kevin Blackburn-Matzen, Simon Niklaus, Jianming Zhang, Jia-Bin Huang, and Feng Liu. Fast view synthesis of casual videos with soup-ofplanes. In ECCV, 2024. 3, [20] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. arXiv, 2024. 3, 6 [21] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In CVPR. 1, 2, 3, 6, 7, 8 [22] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021. 2, 7 [23] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In CVPR, 2023. 2, 3 [24] Yiqing Liang, Numair Khan, Zhengqin Li, Thu NguyenPhuoc, Douglas Lanman, James Tompkin, and Lei Xiao. Gaufre: Gaussian deformation fields for real-time dynamic novel view synthesis. arXiv, 2023. 2, 3 [25] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In ICCV, 2021. 3 [26] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-perfect structure-from-motion with featuremetric refinement. In ICCV, 2021. [27] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023. 2, 3, 5, 6, 7, 1 [28] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. 2 [29] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based neural radiance field without posed camera. In ICCV, 2021. 3, 2 [30] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1, 2, 5 [31] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo [46] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In CVPR. 2, 6, 7, 8, [47] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ramanan, Vedaldi Andrea, and Joo Hanbyul. Banmo: Building animatable 3d neural models from many casual videos. In CVPR, 2022. 2 [48] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos. arXiv, 2023. 5, 6 [49] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highfidelity monocular dynamic scene reconstruction. In CVPR, 2024. 1, 2, 6, 7, 8 [50] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera. In CVPR, 2020. 1, 6, 7, 3 [51] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021. [32] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven M. Seitz. Hypernerf: higherdimensional representation for topologically varying neural radiance fields. ACM Trans. Graph., 2021. 2 [33] Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T. Barron, and Ricardo Martin-Brualla. Camp: Camera preconditioning for neural radiance fields. ACM Trans. Graph., 2023. 3, 2 [34] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: In CVPR, Universal monocular metric depth estimation. 2024. 4, 5, 6 [35] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv, 2018. 1, 6, 7, 3 [36] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, 2021. [37] Vincent Raoult, Sarah Reid-Anderson, Andreas Ferri, and Jane Williamson. How reliable is structure from motion (sfm) over time and between observers? case study using coral reef bommies. Remote Sensing, 2017. 3 [38] Johannes Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In CVPR, 2016. 1, 2, 3, 5, 6, 7 [39] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering. In CVPR, 2023. 2 [40] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerfplayer: streamable dynamic scene representation with decomposed neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 2023. 2 [41] Carole H. Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and M. Jorge Cardoso. Generalised dice overlap as deep learning loss function for highly unbalanced segmenIn Deep Learning in Medical Image Analysis and tations. Multimodal Learning for Clinical Decision Support, 2017. 6 [42] Fengrui Tian, Shaoyi Du, and Yueqi Duan. MonoNeRF: Learning generalizable dynamic radiance field from monocular videos. In ICCV, 2023. 6 [43] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhofer, Christoph Lassner, and Christian Theobalt. Nonrigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from monocular video. In ICCV, 2021. 2 [44] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. Nerf-: Neural radiance fields without known camera parameters. CoRR, 2021. 3, [45] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. HumanNeRF: Free-viewpoint rendering of moving people from monocular video. In CVPR, 2022. 2 10 SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Demo Videos We provide an anonymous offline project page named SplineGS demo.html, showcasing extensive qualitative comparisons between our SplineGS and SOTA novel view synthesis methods [11, 18, 21, 27, 46, 49]. Please note that since the provided project page for this supplementary material is offline, and therefore, no modifications can be made after submission; it is offered solely for the convenience of visualization. The project page features various demo videos, including comparisons for (i) novel view synthesis on NVIDIA [50], (ii) novel view and time synthesis on NVIDIA [50], (iii) novel view synthesis on DAVIS [35], showcasing fixed views, spiral views, and zoomed-in/out views, (iv) dynamic 3D Gaussian trajectory visualization on DAVIS [35], and (v) visualization of toy example when we edit 3D positions of several control points. B. Additional Ablation Study for MotionAdaptive Control Points Pruning (MACP) As described in Eq. 8 of the main paper, we compute the error between S(t, P) and S(t, P) by projecting the 3D points of each cubic Hermite spline function [2, 7] over time into pixel space of all training cameras. This error is then used to update the new spline function. The 2D error measurement is particularly effective because it directly aligns with the image domain, where pixel-level accuracy is essential for precise spline function updates. To determine the updated spline function, we set the threshold value ϵ of the error in Eq. 8 to 1. To validate the rationale behind our setup, we conduct an ablation study for novel view synthesis on the NVIDIA dataset [50], examining different MACP settings, including the ablated models without MACP (w/o MACP (Nc = 4), w/o MACP (Nc = Nf ) in Table 3-(c)) and with MACP having variations in ϵ values. For the variations in ϵ values, we select 0.2, 1, 2, 3, and 5. Fig. 9 presents the average PSNR values and the average number of control points for dynamic 3D Gaussians after training across all scenes. As shown in Fig. 9, when ϵ is set to an excessively small value (ϵ = 0.2), our MAS architecture fails to prune control points effectively, resulting in reduced efficiency. Conversely, when ϵ is too large (ϵ = 5), the pruning becomes overly aggressive, resulting in an insufficient number of control points to accurately represent complex motion trajectories. This trade-off underscores the importance of selecting ϵ carefully to achieve balance between efficiency and representation quality. 1 Figure 9. Ablation study on MACP. We conduct an ablation study of our Motion-Adaptive Control points Pruning (MACP) method for novel view synthesis on the NVIDIA dataset [50] by adjusting the pruning error threshold ϵ. PSNR (dB) and # Ctrl. Pts. denote the average PSNR value and the average number of control points for dynamic 3D Gaussians after training, computed across all scenes, respectively. C. Memory Footprint Comparison To further highlight the efficiency of our SplineGS, we compared its memory footprint with other 3DGS-based methods [18, 21, 46, 49], as shown in Table 4. This comparison evaluates the average model storage requirements after optimization on the NVIDIA dataset [50]. The storage requirements of 3DGS-based methods depend on the number of 3D Gaussians, which is determined by their hyperparameters. For consistency, we use the same hyperparameter settings for the 3DGS-based methods [18, 21, 46, 49] as those specified in their original implementations. Ex4DGS [18] requires the largest memory footprint, attributed to its method of explicit keyframe dynamic 3D Gaussian fuIn contrast, our SplineGS, which achieves state-ofsion. the-art (SOTA) rendering quality as shown in Table 1, utilizes only about one-tenth of the memory footprint required by Ex4DGS [18], thanks to our efficient MAS representation and the MACP method. Method Memory footprint (MB) # Gaussian (K) 4DGS (CVPR24) [46] D3DGS (CVPR24) [49] Ex4DGS (NeurIPS24) [18] STGS (CVPR24) [21] SplineGS (Ours) 50 92 256 19 26 136 382 436 128 183 Table 4. Memory footprint comparison results. Memory footprint (MB) refers to the memory size of each trained model, while # Gaussian (K) represents the total number of 3D Gaussians after training. D. Dynamic 3D Gaussian Trajectory Visualization Please note that the term motion tracking in our main paper  (Fig. 6)  , also referred to as dynamic 3D Gaussian trajectory visualization in 2D space, differs from the term tracking used in 2D Tracking methods such as [16], which aim to find 2D pixel correspondences among given video frames. Our SplineGS leverages spline-based motion modeling to directly capture the deformation of each dynamic 3D Gaussian along the temporal axis, enabling the rendering of target novel views. For 2D visualization of the 3D motion of each dynamic 3D Gaussian, which is referred to as motion tracking in our main paper, we project its trajectory onto the 2D pixel space of the novel views. We compute rasterized 2D track = {φG R2}t[t1,t2] over the specified time interval [t1, t2] as the Gaussians trajectories visualization shown in Fig. 6 of the main paper. For this motion tracking rasterization, we compute the projected pixel coordinates at time for each 3D Gaussian using the camera pose [RT ] of the target novel view as π ˆK(RS(t, P) + ). Then, we compute φG by replacing the color ci in Eq. 2 with the projected pixel coordinate as φG = (cid:80) φG where αdy sian. iN π ˆK(RSi(t, P) + )αdy j=1(1 αdy ), (18) denotes the density of the ith dynamic 3D Gausi (cid:81)i1 Figure 10. Visual results of dynamic 3D Gaussian trajectory projected to novel views for our SplineGS. As shown in Fig. 6 of the main paper, D3DGS [49] fails to reconstruct dynamic regions. STGS [21] renders dynamic regions more effectively than D3DGS [49], but it still produces poor visualizations of 3D Gaussian trajectories. In the original STGS [21] paper, they propose the temporal opacity σi(t) as σi(t) = σs exp(sτ µτ 2), (19) further investigate the motion tracking results of STGS [21], we render novel views for STGS [21] after training by setting the opacity of each 3D Gaussian with (a) its original temporal opacity σi(t) and (b) the fixed value of timeindependent spatial opacity σs , as shown in Fig. 11. Figure 11. Visual results of novel view synthesis at specific time using the same STGS [21] models after optimization with (a) their original time-varying opacity and (b) timeindependent spatial opacity, respectively. Please note that we use their original time-varying opacity during training. We observe that when the opacity of each 3D Gaussian is set to time-independent value, the rendered novel view synthesis results show multiple instances of the same horse or parachute) appearing moving objects (e.g. simultaneously, as illustrated in Fig. 11-(b). This observation suggests that, to represent moving object across time, STGS [21] may adjust the opacities of different sets of 3D Gaussians through their temporal opacities σi(t), rather than deforming the spatial 3D positions of single set of 3D Gaussians along the temporal axis. While this approach can produce dynamic rendering results, it may not allow for the direct extraction of 3D Gaussian trajectories along the temporal axis. In contrast, our SplineGS with MAS directly models the motion trajectories of dynamic 3D Gaussians, enabling the extraction of more reasonable 3D trajectories, as shown in Fig. 10. E. Additional Details for Methodology Camera Intrinsic. To predict the shared camera intrinsics for our camera parameter estimation, we adopt pinhole camera model which is widely used in COLMAP-free novel view synthesis methods [27, 29, 33, 44] as fx 0 0 fy 0 , cx cy (20) where µτ factor and σs is the temporal center, sτ is the temporal scaling is the time-independent spatial opacity. To = where = 0 represents the skewness of the camera, while cx and cy denote the coordinates of the principal point in pixels. Without loss of generality, we assume that fx = fy = , indicating equal focal lengths in both directions, and set cx and cy to half the width and height of the video frame, respectively. Time-dependent Rotation and Scale. As described in Sec. 3 of the main paper, we model the rotation and scale of dynamic 3D Gaussians as time-dependent functions. For the rotation, we adopt polynomial function inspired by STGS [21], defined as qi(t) = q0 + (cid:80)nq k=1 qi,ktk, (21) where q0 is time-independent base quaternion of the ith dynamic 3D Gaussian and qi,k is an offset quaternion of the kth-order term of ith dynamic 3D Gaussian, both of which are learnable parameters. we set nq = 1. This ensures simple yet effective representation of time-dependent rotations [21]. For the scale, inspired by DynIBaR [23], we leverage the Discrete Cosine Transform (DCT) to capture the continuously varying scale of each dynamic 3D Gaussian. The scale function is expressed as Figure 12. Limitations of our SplineGS. When the training video frame contains blurriness, our model cannot effectively reconstruct sharp renderings due to the absence of deblurring method. G.2. Novel View and Time Synthesis on NVIDIA Figs. 16, 17, and 18 present additional visual comparisons for novel view and time synthesis on the NVIDIA dataset [50]. G.3. Novel View Synthesis on DAVIS Figs. 19 and 20 present additional visual comparisons for novel view synthesis on the DAVIS dataset [35]. si(t) = i + si(t), (cid:113) si(t) = 2/Nf (cid:80)K k=1 ζi,k cos (cid:16) π 2Nf (2t + 1)k (22) (cid:17) , where s0 is time-independent base scale vector of the ith dynamic 3D Gaussian and ζi,k R3 represents the kth coefficient of the ith dynamic 3D Gaussian, both of which are learnable parameters. Here, = 10 controls the number of frequency components used in the DCT, allowing flexible yet compact modeling of temporal scale variations. F. Limitation In-the-wild videos often exhibit significant and rapid camera and object movements, resulting in blurry input frames. This blurriness subsequently degrades the quality of the rendered novel views. As shown in Fig. 12, the methods solely designed for dynamic scene reconstruction may overfit to the blurry training frames. straightforward solution is to employ state-of-the-art 2D deblurring methods to enhance the quality of input frames. Additionally, in future research, we plan to integrate deblurring approach directly into the reconstruction pipeline. This integration could establish joint deblurring and rendering optimization framework, addressing low-quality issues and enhancing the final rendered outputs without requiring separate preprocessing. G. Additional Qualitative Results G.1. Novel View Synthesis on NVIDIA Figs. 13, 14, and 15 present additional visual comparisons for novel view synthesis on the NVIDIA dataset [50]. 3 Figure 13. Visual comparisons for novel view synthesis on the Jumping scene from the NVIDIA dataset. Figure 14. Visual comparisons for novel view synthesis on the Playground scene from the NVIDIA dataset. Figure 15. Visual comparisons for novel view synthesis on the Truck scene from the NVIDIA dataset. 4 Figure 16. Visual comparisons for novel view and time synthesis on the Balloon2 scene from the NVIDIA dataset. Figure 17. Visual comparisons for novel view and time synthesis on the Jumping scene from the NVIDIA dataset. Figure 18. Visual comparisons for novel view and time synthesis on the Umbrella scene from the NVIDIA dataset. 5 Figure 19. Visual comparisons for novel view synthesis on the Horsejump-high scene from the DAVIS dataset. Figure 20. Visual comparisons for novel view synthesis on the Paragliding-launch scene from the DAVIS dataset."
        }
    ],
    "affiliations": [
        "Chung-Ang University",
        "KAIST"
    ]
}