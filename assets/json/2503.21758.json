{
    "paper_title": "Lumina-Image 2.0: A Unified and Efficient Image Generative Framework",
    "authors": [
        "Qi Qin",
        "Le Zhuo",
        "Yi Xin",
        "Ruoyi Du",
        "Zhen Li",
        "Bin Fu",
        "Yiting Lu",
        "Jiakang Yuan",
        "Xinyue Li",
        "Dongyang Liu",
        "Xiangyang Zhu",
        "Manyuan Zhang",
        "Will Beddow",
        "Erwann Millon",
        "Victor Perez",
        "Wenhai Wang",
        "Conghui He",
        "Bo Zhang",
        "Xiaohong Liu",
        "Hongsheng Li",
        "Yu Qiao",
        "Chang Xu",
        "Peng Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 8 5 7 1 2 . 3 0 5 2 : r LUMINA-IMAGE 2.0: UNIFIED AND EFFICIENT IMAGE GENERATIVE FRAMEWORK Qi Qin2, Le Zhuo1, Yi Xin1,4, Ruoyi Du1, Zhen Li3, Bin Fu1, Yiting Lu1, Jiakang Yuan1, Xinyue Li1, Dongyang Liu1,3, Xiangyang Zhu1, Manyuan Zhang3, Will Beddow6, Erwann Millon6, Victor Perez6, Wenhai Wang1, Conghui He1, Bo Zhang1, Xiaohong Liu5, Hongsheng Li3, Yu Qiao1, Chang Xu2, Peng Gao1 1 Shanghai AI Laboratory, 2 The University of Sydney, 3 The Chinese University of Hong Kong, 4 Shanghai Innovation Institute, 5 Shanghai Jiao Tong University, 6 Krea AI"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification it adopts unified architecture (Unified Next-DiT) that treats text and image tokens as joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/ Alpha-VLLM/Lumina-Image-2.0."
        },
        {
            "title": "Introduction",
            "content": "Text-to-image (T2I) generative models have made significant strides over the past years. Notable open-source models [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] have shown significant improvements in both image fidelity and prompt adherence, thereby broadening the scope of their applicability in diverse downstream tasks [11, 12, 13, 14]. From these advancements, (1) the scalable text-conditional Diffusion Transformer (DiT) architectures, and (2) the large-scale, high-quality text-image datasets are witnessed as the most important factors for developing text-conditional image generative models. However, existing models still exhibit notable limitations in both aspects. First, many text-conditional Diffusion Transformers [15, 8, 16, 17, 18, 19] continue to rely on cross-attention mechanisms to inject textual information. This paradigm treats text embeddings as fixed external features, thus limiting the efficiency of multimodal fusion and may even introduce uni-directional bias when using text embedding extracted from causal large language models [20]. Moreover, extending these models to new tasks often requires specific architecture designs [11, 21]. Second, although recent efforts [2, 15, 22] have highlighted the importance of collecting high-quality image captions, the lack of dedicated captioning system tailored for T2I generation has resulted in inaccurate and insufficiently image captions for text-image paired training data. The limitations in both architecture and data quality constrain the expressiveness of text and visual representations, ultimately impairing the models ability to faithfully follow user instructions in generating high-quality images. Equal Contribution Corresponding Authors Project Leader Figure 1: High-quality samples from our Lumina-Image 2.0, showcasing its capabilities in ultra-realistic, text generation, artistic versatility, bilingual mastery, logical reasoning, and unified multi-image generation. Driven by the aforementioned challenges, we present Lumina-Image 2.0, unified and efficient T2I generative framework that comprises four key components: (1) Unified Next-DiT model for generating images that faithfully aligned with the text input, (2) Unified Captioner (UniCap) for producing high-quality text-image pairs, and series of specific designs for (3) efficient training and (4) efficient inference. Specifically, to address architectural limitations, our Unified Next-DiT model utilizes joint self-attention mechanism, enabling our model to process both textual and visual tokens in fully end-to-end manner, similar to decoder-only transformers in recent large language models [23, 24, 25, 26]. This design facilitates seamless multimodal interaction, allowing for the integration of additional multimodal tokens or specific prompt templates to extend the models capabilities without modifying the core architecture. In response to the scarcity of high-quality textual descriptions in paired text-image data, we introduce Unified Captioner (UniCap), unified captioning system specifically designed for T2I generation. UniCap excels at precisely understanding complex scenes, and generating comprehensive and coherent multilingual descriptions. Leveraging these capabilities, we employ UniCap to create multi-granularity, multi-dimensional textual descriptions that better align with the images. Furthermore, our experiments reveal that when combining the unified Next-DiT and UniCap for training, the text-to-image attention in transformer blocks dynamically adjusts its capacity based on the length of textual embeddings, behaving similarly to dynamic feed-forward network. This observation motivates us to further enhance model capacity and performance by increasing the richness of textual descriptions without introducing additional parameters. Furthermore, both training and inference efficiency are crucial for model development and deployment. To perform efficient training, Lumina-Image 2.0 employs multi-stage progressive training strategy with hierarchical high-quality data. The multi-domain system prompts and an auxiliary loss are further utilized to learn domain-specific knowledge and preserve low-frequency features, respectively. For efficient inference, we adopt several advanced sampling techniques and verify that the integration of CFG-Renormalization (CFG-Renorm) [27] and CFG-Truncation (CFG-Trunc) [28] can boost the sampling speed and maintain high sampling quality. Specifically, CFG-Renorm addresses the issue of over saturation at large classifier-free guidance (CFG) scales, while CFG-Trunc streamlines the inference process by eliminating redundant CFG computations, thereby enhancing both inference stability and speed. Additionally, we incorporate Flow-DPM-Solver [8] and TeaCache [29] to further optimize inference speed. We evaluate Lumina-Image 2.0 on publicly available benchmarks, including DPG [30], GenEval [31], and T2ICompBench [32]. Considering the limitation of current academic benchmarks in comprehensively evaluating T2I models, we report the ELO rankings of Lumina-Image 2.0 on several online T2I arenas, which is evaluated by human annotators. Our experimental results consistently demonstrate that Lumina-Image 2.0 achieves significant improvements over previous model (Lumina-Next [17]). We release the complete training details, code, and models to facilitate the full reproduction of Lumina-Image 2.0."
        },
        {
            "title": "2 Related Work",
            "content": "Recent advancements in text-to-image generation have been remarkable. Diffusion-based models have progressively transitioned from U-Net architectures [33] to Diffusion Transformers [34], as demonstrated by models such as PixArt [15, 35], FLUX [9], SD3 [36], Lumina [16, 17], and SANA [8]. These Diffusion Transformers exhibit superior scalability and are progressively evolving toward unified multimodal representation [37]. Regarding text encoders, early approaches [1] employed CLIP [38], while subsequent works [22, 36, 9] additionally adopted T5-XXL [39]. More recently, SANA [8], Lumina [17, 16] and our Lumina-Image 2.0 have incorporated Gemma [40] as the text encoder. Furthermore, the latest models leverage flow-based parameterizations [41, 42], which enhance both training and inference efficiency compared to conventional diffusion methods. In parallel, range of advanced autoregressive and hybrid text-to-image models have emerged [43, 3, 6, 44, 10, 7], achieving performance on par with their diffusion-based counterparts. However, the sampling speed of these autoregressive models remains significantly slower than that of diffusion-based approaches, posing critical challenge for their practical deployment. Meanwhile, the advancement of text-to-image models has been significantly shaped by the evolution of visionlanguage models (VLMs) [45, 46, 47, 48], where the quality of image captions plays critical role in both model performance [2, 36]. Currently, the most commonly employed image captioners in text-to-image research include LLaVA [49], CogVLM [50], ShareGPT-4 [46], and Qwen-VL [51, 52, 53], all of which are general-purpose visionlanguage models (VLMs). However, there is significant lack of research focused on developing captioner models specifically tailored for the text-to-image task, which may impede the further advancement of text-to-image models."
        },
        {
            "title": "3 Revisiting Lumina-Next",
            "content": "Model Architecture. Lumina-Next [17] introduces Next-DiT, scalable flow-based diffusion transformer, as its core architecture. Building upon the original diffusion transformer [34], Next-DiT employs sandwich normalization and query-key normalization [54] to enhance training stability, and leverages 2D Rotary Positional Encoding [55] to encode positional information of images. For text-to-image generation, Next-DiT utilizes zero-initialized gated cross-attention to inject text embeddings extracted by Gemma [40]. Training and Inference Strategy. Lumina-Next is trained on approximately 20M synthetic text-image pairs, with image captions generated using user prompts and VLM models. During training, Lumina-Next employs multi-stage 3 progressive training approach, similar to recent text-to-image models [15, 16]. This strategy involves sequential training at 256, 512, and 1024 resolutions, enabling the model to progressively capture both low-frequency and high-frequency information from images. During sampling, Lumina-Next introduces two time schedules tailored for flow models to minimize the ODE truncation errors, and it supports both first-order and higher-order solvers, such as Euler and Midpoint solvers. Naive Data Scaling. Inspired by the data scaling paradigm of large-scale models [56, 24, 57, 36], we believe that the performance gap in Lumina-Next primarily stems from insufficient training data. Therefore, we scaled Lumina-Nexts dataset from 20M to 200M samples. This expanded dataset encompasses diverse real and synthetic data processed by the same cleaning and annotation pipeline. We retain the same model architecture and training strategy in Lumina-Next. We observed that the model performance shows considerable improvement across various academic metrics compared to Lumina-Next. For example, on the DPG benchmark [30], the performance improved from 75.66 to 85.80 after data scaling. This demonstrates the effectiveness of Next-DiT as robust framework for scalable image generation."
        },
        {
            "title": "4 Lumina-Image 2.0",
            "content": "4.1 Framework Overview Lumina-Image 2.0 establishes unified and efficient framework by integrating Unified Next-DiT, Unified Captioner (UniCap), and set of efficient training and inference strategies. The overall pipeline is illustrated in Fig. 2, and we apply custom filtering pipeline [15, 58] to select high-quality training images. To improve text quality, our UniCap re-captions the training data to generate accurate and detailed textual descriptions at multiple levels of granularity. The resulting high-quality image-text pairs are organized into hierarchical training dataset, which is subsequently used to optimize the Unified Next-DiT model using our proposed training strategies. Finally, several inference strategies are further introduced to efficiently generate high-quality images. 4.2 Unified Next-DiT After revisiting the architecture of Next-DiT, we observe that zero-initialized gated cross-attention for integrating text embedding limits the capability of text-image alignment and also requires additional architecture modification when adapting to new tasks. Therefore, we propose Unified Next-DiT, unified text-to-image model that treats text and images as unified sequence to perform joint attention inspired by recent advances in unified multimodal learning [37]. Architecture of Unified Next-DiT. Next-DiT employs Gemma [40] as the text encoder, whose text embeddings exhibit unidirectional positional bias [20] caused by the causal self-attention in the large language model. During generation, the biased text embeddings are fixed and sparsely injected to the transformer block via zero-initialized gated cross-attention. Therefore, we remove all zero-initialized gated cross-attention in Next-DiT. Instead, we leverage unified single-stream block that fuses caption embeddings and noised latent by concatenating them and performing joint self-attention, which facilitates more effective textimage interaction and task expansion. As illustrated in Fig. 2, our single-stream blocks build upon the original DiT block with the addition of sandwich normalization and query-key normalization to ensure stable training. The Multimodal-RoPE [59] (mRoPE) is employed to jointly model textimage sequences in unified manner, which encodes the text length as well as the images height and width into three dimensions. Moreover, we further observe that textual and visual features at the input level exhibit considerable gap. To address this issue, we introduce text and image processors prior to the single-stream blocks. These processors with similar but lightweight single-stream blocks facilitate intra-modal information exchange and mitigate the gap between modalities. Since caption embeddings are fixed for all timesteps, the text processor does not incorporate timestep conditioning. Comparison with previous architectures. As illustrated in Fig. 3, we compare the architecture of Unified Next-DiT with mainstream Diffusion Transformers. PixArt [15] and Lumina-Next [17] employ an additional cross-attention block after self-attention to inject fixed text embeddings, whereas our model adopts single, unified attention module that jointly handles both text and noisy latent. Compared with the MMDiT architecture used in SD3 [36] and FLUX [9], the key difference is that MMDiT employs the double-stream blocks, allocating extensive and separate parameters for text and image sequences. In contrast, our method is designed from more unified perspective, utilizing single set of parameters to simultaneously model both the text and image sequences. Our model shares similarities with OmniGen [37], which introduces single-stream causal DiT architecture for unified image generation. In pursuit of unifying the transformer architecture with auto-regressive models, OmniGen removes adaptive Layer Normalization (adaLN) and applies causal self-attention initialized from large language model. However, adaLN is considered essential for Diffusion Transformers [34], and initializing from language model may introduce conflicts with the knowledge for image generation. 4 Figure 2: Overview of Lumina-Image 2.0, which consists of Unified Captioner and Unified Next-DiT. The Unified Captioner re-captions web-crawled and synthetic images to construct hierarchical text-image pairs, which are then used to optimize Unified Next-DiT with our efficient training strategy. 4.3 Unified Captioner Due to the crucial role of image captions in enhancing model performance [2], using out-of-box pre-trained Vision Language Models (VLMs) for image recaptioning has been standard practice in previous literatures [22, 36, 15, 35, 5 Figure 3: We compare the Diffusion Transformer architectures between our Unified Next-DiT, and PixArt [15], LuminaNext [17], Stable Diffusion 3 [36], OmniGen [37] and FLUX [9]. 16, 17]. However, these VLMs exhibit several limitations, including single-granularity descriptions, domain biases, and fixed low-resolution inputs, which result in suboptimal caption quality and noticeable gap from real-world user prompts. To address these limitations and construct high-quality text-image datasets, we develop Unified Captioner (UniCap), captioning system that unifies diverse visual inputs and provides multi-granularity, multi-perspective, and multi-lingual high-quality textual descriptions. In addition, we also introduce unified perspective to make the caption-driven model capacity more interpretable. Unifying Textual Description. To enable Lumina-Image 2.0 to handle diverse prompts ranging from multi-granularity, multi-perspective, and multilingual descriptions we train UniCap to deliver all types of descriptions to achieve unified image recaptioning. In particular, our approach comprises three key components: (1) For multi-granularity descriptions, we begin by carefully prompting GPT-4o [56] to generate highly detailed descriptions. Then we employ open-source large language models (LLMs) to simultaneously summarize detailed captions into medium, short, and tag-based descriptions for captioner training, which enables UniCap to deliver captions of multiple granularities while retaining essential information, as shown in Fig. 7 and Fig. 8. (2) For multi-perspective descriptions, we include image style descriptions, main object descriptions, all-object descriptions, object attribute descriptions, and spatial relationship descriptions, ensuring comprehensive coverage of visual elements, attributes, spatial structures, and stylistic nuances. (3) For multi-lingual descriptions, we utilize bilingual large language models to translate captions into Chinese, enabling UniCap to generate bilingual captions simultaneously. Surprisingly, although UniCap only captions all data in English and Chinese for Lumina-Image 2.0 training, the model benefits from Gemmas multilingual capabilities and emerges with the understanding of other languages, thereby expanding its accessibility to broader user base (see Fig. 6). Unifying Visual Understanding. Existing VLMs struggle with processing images from diverse domains and openworld scenarios, and are limited to low-resolution inputs, making it difficult to capture fine-grained details of images. To address this issue, we train UniCap with caption dataset that encompasses wide range of visual content, including natural images, web-crawled images, photographs, synthetic images, multi-image documents, infographics, OCR-related images, and multilingual content, ensuring comprehensive domain coverage and conceptual diversity. Besides, unlike LLaVA [45] and ShareGPT4V [46], which resize images of varying scales and aspect ratios to fixed low-resolution format, our UniCap processes images at their native scale in unified manner. This approach yields more accurate and detailed captions, significantly reduces hallucinations, and improves OCR recognition. This strategy has been widely adopted by recent VLMs, including SPHINX-X [48], InternVL [47], and XComposer [60]. Furthermore, inspired by the concept of specialized generalist intelligence (SGI) [61], where AI systems excel in specialized tasks while maintaining broad general abilities, we aim for Lumina-Image 2.0 to not only showcase powerful text-conditioned generation capabilities but also serve as unified interface for diverse visual generation tasks. To this end, we collect annotations from various visual tasks, such as depth maps, pose maps, canny maps, and sketches. We then concatenate them with paired images to form composite grids and leverage template captions (please refer to the last row of Fig. 1) to effectively describe the underlying logical process. This unified approach enables Lumina-Image 6 2.0 to handle advanced tasks beyond text-to-image generation, thereby laying the foundation for potential downstream applications. Figure 4: The training loss curve with respect to captions with different lengths. The Avg. Length represents the average character number. Unified Perspective on Caption-Driven Model Capacity. The importance of detailed image captions has been witnessed for scaling up diffusion models [2, 36]. During the training of Lumina-Image 2.0, we specifically observed that both the length and quality of image captions direct influence the models convergence speed. As shown in Fig. 4, we train the model using three versions of image captions: (1) short captions generated by Florence [62], (2) short but precise captions generated by our UniCap, and (3) long and detailed captions from UniCap. We observe that as captions become more precise and detailed, the models convergence speed significantly improved. During the inference phase, it is also commonly recognized by previous works [58, 22] that longer captions often lead to better generation results. These observations motivate us to rethink the role of caption embeddings in text-to-image generation. To further analyze the impact of image caption, in this paper, we provide an interpretable perspective on this phenomenon the attention operation between texts and images can be viewed as dynamic feed-forward network (FFN), where the choice of caption embeddings governs its effective knowledge integration and representational capacity. Generally, the FFN layer of transformer can be interpreted as key-value memory that encapsulates the general knowledge acquired by the model [63], which can even be manually constructed without the need for training [64]. It also has been shown that the FFN layer can be effectively substituted by self-attention with persistent memory [65]. Motivated by these findings, we further explore the relationship between the text-to-image attention and the FFN mechanism. Note that the term text-to-image attention encompasses both the independent cross-attention used in Next-DiT [17] and PixArt [35], as well as the image-text interaction component in models performing joint self-attention (e.g., our Unified Next-DiT). Given sequence of image tokens RLimgd and sequence of text tokens RLtextd. An ordinary image-text attention can be equivalently rewritten in the form of FFN as follows: Attn(X, ) = σ(cid:0)X W1(Y )(cid:1) W2(Y ), (1) where σ() denotes the Softmax function, and the two weight matrices are conditioned on the text embeddings : (cid:0)Y WK dk RdLtext, W1(Y ) = WQ (cid:1)T (2) W2(Y ) = WV RLtextd, (3) where WQ, WK, and WV are weight matrices for query, key, and value, respectively, and dk is the dimension of query / key. Notably, the hidden dimension between W1(Y ) and W2(Y ) changes dynamically with the context length as Ltext. Under this formulation, the text-to-image attention computation can be viewed as an FFN whose parameters are generated by hyper-network, with dynamic weights and dynamic hidden size. Specifically, the conditional information (i.e., the text) is encoded to form the dynamic weights, while the hidden size Ltext will adjust the capacity of this FFN-like module via its length. 7 Figure 5: Illustration of reformulating the Image-Text Attention as an FFN generated by hyper-network, with its weights and hidden dimensions dynamically determined by the input text token. From this perspective, we reach an interesting conclusion increasing caption length effectively serves as controllable means of scaling up model parameters. This insight suggests that the capacity of the model in both training and inference can be modulated simply by adjusting the length of the caption, which can lead to improved knowledge learning and overall performance. These findings are consistent with recent trends in existing work [66, 67] and highlight promising directions such as inference-time scaling [68]. 4.4 Efficient Training Lumina-Image 2.0 introduces an efficient training framework that integrates multi-stage progressive training, hierarchical high-quality dataset, multi-domain system prompts, and auxiliary loss. These strategies improve image quality and detail refinement while accelerating convergence. Multi-Stage Progressive Training. Unlike prior approaches [15, 33, 17, 37] that optimize generative models over three progressive resolution stages, we skip the intermediate 512 resolution stage and introduce an additional high-quality tuning phase. This results in three-stage progressive training pipeline: low-resolution phase (256 resolution), high-resolution phase (1024 resolution), and high-quality tuning phase (1024 resolution). In the low-resolution phase, Lumina-Image 2.0 focuses on learning global and low-frequency information, such as domain knowledge, object relationships, and structural patterns. The subsequent two phases first transfer this knowledge to higher resolutions and further enhance fine-grained visual details. Hierarchical High-quality Data. In contrast to Lumina-Next, which utilizes fixed dataset in all training stages, we construct hierarchical dataset by filtering images based on image quality criteria (e.g., aesthetic) at different stages. Specifically, we begin with dataset of 110M samples. For the low-resolution training stage, we select 100M samples. The remaining 10M samples, containing relatively higher-quality data, are then used in the high-resolution phase. From these, we further curate subset of the highest-quality 1M samples for the final fine-tuning stage. Multi-domain System Prompt. We collect training data from diverse domains, including high-aesthetic synthetic data, as well as photorealistic real-world data. However, there is substantial domain gap between these datasets, often resulting in slower convergence and difficulties in learning domain-specific knowledge. Motivated by ChatGPT [23], we propose distinct system prompts to differentiate between these domains, thereby reducing learning difficulty and accelerating convergence. Specifically, during our proposed three-stage progressive training phase, we use two types of system prompts (Template and Template B) that are directly prepended to the image prompt, as shown in Tab. 1. For the unified multi-image generation, we introduce an additional fine-tuning phase (see Sec. 5.1 for details) with the system prompt Template C. Auxiliary Loss. When training our model on high-resolution images, the model exhibits significant improvements in high-frequency details while some degradations in low-frequency structures. We introduce an auxiliary loss to address 8 Table 1: Prompt template for Lumina-Image 2.0. <Image Prompt> will be replaced with the user specific image description. <lower half> and <upper half> will be replaced with the specific spatial relationships. <depth map> will be replaced with the target image type. Template You are an assistant designed to generate high-quality images based on user prompts. <Prompt Start> <Image Prompt> Template Template You are an assistant designed to generate superior images with the superior degree of image-text alignment based on textual prompts or user prompts. <Prompt Start> <Image Prompt> Generate dual-panel image where the <lower half> displays <depth map>, while the <upper half> retains the original image for direct visual comparison. <Prompt Start> <Image Prompt> this issue, which computes the flow-matching objective [42] with latent features downsampled by factor of 4: Laux(θ) = Et,x,ϵ vθ (xt, t) ut2 , where [0, 1] denotes timestep, = AvgPool4(z) denotes the downsampled latent features using average pooling by factor of 4, ϵ (0, I) is random Gaussian noise, vθ() and ut = ϵ represent the predicted vector field and target vector field, respectively. This approach helps preserve low-frequency features while learning high-frequency details, enabling efficient knowledge integration and allowing direct fine-tuning at 1024 resolution. (4) 4.5 Efficient Inference To boost the sampling speed as much as possible while maintaining high sampling quality, Lumina-Image 2.0 makes deeper exploration on inference efficiency. CFG-Renormalization (CFG-Renorm). Classifier-free guidance (CFG) [69] is known for improving both visual quality and text-image alignment. During inference, at each timestep t, the predicted velocity vt is calculated as vt = vtu + w(vtc vtu ), where is the CFG scale, vtc and vtu represent the conditional and unconditional velocity, respectively. However, scaling by large may introduce extremely high activations in certain dimensions of vt, and these abnormal values can result in visual artifacts in the final generated samples. To address this, recent work introduces the CFG-Renorm method [27] to rescale the magnitude of the modified velocity of vt using that of the conditional velocity vtc. We find that this technique effectively improves the stability of CFG-guided generation without introducing additional computational costs. CFG-Truncation (CFG-Trunc). Recent research [28] indicates that text information is largely captured in the early generation stages. Therefore, evaluating vtc beyond the early timesteps may be redundant. The CFG-Trunc can be formulated as follows: (cid:26)vtu + w(vtc vtu ) vtu where α denotes predefined threshold. This modification can achieve over 20% acceleration in sampling speed, without visual degradation. α; < α. vt = (5) Flow-DPM-Solver (FDPM). Lumina-Next supports range of ODE solvers, such as Midpoint and Euler method. While these solvers ensure stability, they are relatively slow since they are not designed for flow models, requiring large number of function evaluations (NFE) for convergence. To improve this, we integrate FDPM [8, 70], which modify DPM-Solver++ [70] to flow models, into Lumina-Image 2.0. FDPM achieves convergence in just 14-20 NFEs, providing faster sampling solution. However, we find that FDPM sometimes suffers from poor stability in practice. Timestep Embedding Aware Cache (TeaCache). TeaCache [29] is designed to selectively cache informative intermediate results during the inference, thereby accelerating diffusion models. TeaCache has successfully accelerated various mainstream image and video generation models, including FLUX [9], HunyuanVideo [58], as well as LuminaNext. Building on its success, we integrate TeaCache into Lumina-Image 2.0. However, our experiments show that TeaCache also introduces visual quality degradations when combined with the above techniques. Discussion. The above four inference strategies are mutually compatible and can be applied in combination. Notably, Lumina-Image 2.0 is the first to demonstrate that CFG-Renorm and CFG-Trunc provide complementary benefits when 9 applied together. CFG-Renorm addresses the issue of over-saturation and visual artifacts when the CFG scale is large, while CFG-Trunc further alleviates this problem by eliminating redundant CFG calculations and achieving acceleration at the same time. The flexibility of the CFG scale can be significantly extended to wider range by combining these techniques. FDPM and TeaCache can also be integrated into the pipeline, but both of them present certain challenges. FDPM lacks sufficient stability and frequently produces suboptimal samples while TeaCache results in blurriness in the sampled images. For further details, refer to Sec. 5.4."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implement Details Training Dataset. Following the methods in [3, 4, 7, 36, 15, 37, 71], we constructed dataset combining both real and synthetic data, and performed data filtering based on the techniques outlined in [15, 22, 58], resulting in total 110M samples. This dataset is reorganized into three training phases, with 100M, 10M, and 1M samples for each training phase. As the dataset size decreased, the quality of the data progressively improved. Architecture and Training Setups. The architecture configurations of our Unified Next-DiT model, along with comparison to Lumina-Next [17], are summarized in Tab. 2. We employed 32 A100 GPUs across all three stages to optimize our Unified Next-DiT. The corresponding training configurations are detailed in Tab. 3. In addition, for multi-image generation task, we introduce an extra fine-tuning phase, where we consolidate different visual tasks into image grids and generate captions for these concatenated grids to form image-pair pairs. Besides, for the UniCap model, we finetune the Qwen2-VL-7B [59] based on the constructed caption dataset with multi-domain visual data and diverse textual descriptions. Table 2: Comparison of configuration between Lumina-Next and Lumina-Image 2.0. Model Params Patch Size Dimension Heads KV Heads Layers RMSNorm ϵ [72] Pos. Emb. Lumina-Next Lumina-Image 2.0 1.7B 2.6B 2 2 2304 2304 16 8 8 24 26 1e5 1e5 2D-RoPE M-RoPE Table 3: Training configuration across different stages. Stage Image Resolution #Images Training Steps (K) Batch Size Learning Rate GPU Days (A100) Optimizer Low Res. Stage High Res. Stage HQ Tuning Stage 256256 10241024 10241024 100M 10M 1M 144 40 15 1024 512 512 2 104 2 104 2 104 191 176 224 AdamW [73] 5.2 Quantitative Performance Main Results. We evaluate our model on three benchmarks: GenEval [31], DPG [30], and T2I-CompBench [32]. As shown in Tab. 4, Our model demonstrates strong performance across various metrics on the GenEval benchmark. In the Two Object, Counting, Color Attribute, and Overall metrics, we achieve the second-best performance compared to autoregressive and diffusion models. On the DPG benchmark, Lumina-Image 2.0 outperforms all compared models across three sub-metrics (Entity, Relation, and Attribute) as well as the Overall metric. Similarly, on T2I-CompBench, our model achieves the best results in both Color and Shape metrics. The significant advantage we achieve on the DPG benchmark is attributed to the detailed and accurate captions curated by our carefully designed captioning system. Our UniCap generates extremely long and detailed descriptions, which align with the characteristics of prompts contained in DPG, resulting in the strong performance across various metrics, especially in the Relation score. Compaison with ELO Scores. To better evaluate our model, we present evaluation results from three text-to-image arenas, with all ELO scores [76] based on ratings from human annotators. (1) We first perform tests on Artificial Analysis4. As shown in Tab. 5, Lumina-Image 2.0 achieves mid-tier results, outperforming almost all open-source models (e.g., SD3 [36] and Janus Pro [10]) and several closed-source systems (e.g., DALLE 3 [2]), but still lags behind the top closed-source models, such as FLUX Pro [9]. (2) To analyze the alignment and coherence abilities of our model, we also provide rankings from Rapidata5. As shown in Tab. 6, our model achieves comparable ranking. 4https://artificialanalysis.ai/text-to-image/arena?tab=Leaderboard 5https://www.rapidata.ai/leaderboard/image-models 10 Table 4: Performance comparison across different models on GenEval [31], DPG [30], and T2I-CompBench [32] benchmarks. \"\" or \"\" indicate lower or higher values are better. Bold indicates the best performance, while underlining denotes the second-best performance. Methods # Params AutoRegressive Models LlamaGen [4] Chameleon [5] HART [6] Show-o [3] Emu3 [7] Infinity [44] Janus-Pro-1B [10] Janus-Pro-7B [10] 0.8B 7B 732M 1.3B 8.0B 2B 1.5B 7B Diffusion Models LDM [1] SDv1.5 [1] Lumina-Next [17] SDv2.1 [1] PixArt-α [15] SDXL [33] SD3-medium [36] JanusFlow [74] Sana-0.6B [8] Sana-1.6B [8] DALL-E3 [2] OmniGen [37] Sana-1.5 [67] Lumina-Image 2.0 1.4B 0.9B 1.7B 0.9B 0.6B 2.6B 2B 1.3B 0.6B 1.6B - 3.8B 4.8B 2.6B GenEval DPG T2I-CompBench Two Obj. Counting Color Attri. Overall Entity Relation Attribute Overall Color Shape Texture 0.34 - - 0.52 0.81 0.85 0.82 0.89 0.29 - 0.49 0.51 0.50 0.74 0.74 0.59 0.76 0.77 0.87 0.86 0.85 0.87 0.21 - - 0.49 0.49 - 0.51 0.59 0.23 - 0.38 0.44 0.44 0.39 0.63 0.45 0.64 0.62 0.47 0.64 0.77 0.67 0.04 - - 0.28 0.45 0.57 0.56 0.66 0.05 - 0.15 0.50 0.07 0.23 0.36 0.42 0.39 0.47 0.45 0.55 0.54 0. 0.32 0.39 - 0.53 0.66 0.73 0.73 0.80 0.37 0.40 0.46 0.47 0.48 0.55 0.62 0.63 0.64 0.66 0.67 0.70 0.72 0.73 - - - - 87.17 - 88.63 88.90 - 74.23 83.78 - 79.32 82.43 91.01 87.31 89.50 91.50 89.61 - - 91.97 - - - - 90.61 90.76 88.98 89.32 - 73.49 89.78 - 82.57 86.76 80.70 89.79 90.10 91.90 90.58 - - 94. - - - - 86.33 - 88.17 89.40 - 75.39 82.67 - 78.60 80.91 88.83 87.39 89.30 88.90 88.39 - - 90.20 65.16 - 80.89 67.48 81.60 83.46 82.63 84.19 63.18 63.18 75.66 68.09 71.11 74.65 84.08 80.09 83.60 84.80 83.50 - 85.00 87.20 - - - - 0.7544 - - - - 0.3730 0.5088 0.5694 0.6886 0.6369 - - - - 0.8110 - - 0. - - - - 0.5706 - - - - 0.3646 0.3386 0.4495 0.5582 0.5408 - - - - 0.6750 - - 0.6028 - - - - 0.7164 - - - - 0.4219 0.4239 0.4982 0.7044 0.5637 - - - - 0.8070 - - 0.7417 Table 5: Comparison of ELO scores evaluated in text-to-image arena from Artificial Analysis 4 (as of February 23, 2025). Table 6: Comparison of ELO scores evaluated in text-to-image arena from Rapidata 5 (as of February 23, 2025). Methods Artificial Analysis Methods Rapidata Overall Traditional Art Fantasy & Mythical Anime Overall Alignment Coherence FLUX1.1 [pro] [9] FLUX1 [pro] [9] Lumina-Image 2.0 DALLE 3 [2] SD3 Medium [36] Janus Pro [10] 1107 982 970 945 748 983 1015 1008 990 828 1081 1051 1026 1026 784 1051 1037 977 929 766 FLUX1.1 [pro] [9] Imagen 3 [75] Lumina-Image 2.0 DALLE 3 [2] SD3 Medium [36] Janus Pro [10] 1018 969 952 952 734 1003 1031 1022 1022 932 1032 986 958 984 947 In particular, Lumina-Image 2.0 ranks second only to FLUX Pro in terms of prompt alignment, exceeding many other closed-source models such as Imagen 3 [75]. This further validates the effectiveness of our proposed Unified Next-DiT architecture and UniCap annotation system. Moreover, although Janus Pro[10] achieves state-of-the-art results on academic benchmarks, its scores were considerably lower than those of Lumina-Image 2.0 and FLUX Pro on user-driven leaderboards. This discrepancy highlights the inherent bias and limitations in current academic benchmarks. (3) Finally, results from AGI-Eval 6 in Tab. 7 demonstrate that Lumina-Image 2.0 significantly outperforms the previous Lumina-Next [17] as well as all other Chinese open-source models [77, 22]. In summary, we hope that this comprehensive evaluation and comparative analysis will provide the community with clearer understanding of Lumina-Image 2.0s capabilities and constraints, thereby guiding future improvements. We also believe that developing better human-aligned evaluation benchmarks is essential to accurately assess current models and advance generative modeling progress. 6https://ai-ceping.com/ 11 Figure 6: Visualization results of multilingual text-to-image generation by our Lumina-Image 2.0, covering five languages: Chinese, Japanese, English, Russian, and German. Table 7: Comparison of ELO scores evaluated in text-to-image arena from AGI-Eval 6 (as of February 23, 2025). Model FLUX1.1 [pro] [9] FLUX.1 [dev] [9] Lumina-Image 2.0 Kolors [77] HunyuanDiT [22] Lumina-Next [17] Score 0.4859 0.4712 0.4545 0.3924 0. 0.3229 5.3 Qualitative Performance Multi-lingual Generation. Compared to previous T2I models [15, 78] that use CLIP [38] and T5 [39] as text encoders, we employ Gemma2-2b [79] as the text encoder, enabling our model to understand multiple languages. It naturally exhibits zero-shot capability in languages such as German, Japanese, and Russian. As shown in Fig. 6, we present the generation results in five different languages. Captioning Everything With UniCap. We compare our proposed UniCap with existing captioners, such as ShareGPT4V [46] and Florence [62], from four dimensions: complex scenes, dense text, visual understanding, and spatial relationships. UniCap supports multilingual annotations, including both Chinese and English, and can generate captions of varying lengths to accommodate diverse user needs. As shown in Fig. 7 and Fig. 8, UniCap delivers highly detailed and accurate descriptions, significantly outperforming the other two methods. High-quality Image Generation. In Fig. 9, we present additional generation results of Lumina-Image 2.0. These results illustrate that our model is capable of producing high-quality images in various resolutions that are visually Figure 7: Comparison with ShareGPT4V [46] and Florence [62] in complex scenes and dense text for caption generation. The blue underline correspond to areas with more detailed and accurate descriptions, while red underline and red strikethrough represent the incorrect and insufficient descriptions respectively. realistic, highly aesthetic, and creatively expressive. Furthermore, extensive experiments with both Chinese and English prompts of different lengths demonstrate robust text-image alignment. 5.4 Ablation Study Ablation Study on Multi-Stage Training Strategy. During our three-stage progressive training, the models performance steadily improves as dataset size decreased and quality increased. As shown in Tab. 8 and Fig. 10, the performances continue to advance from the second to the third stage, evidenced by both quantitative improvements 13 Figure 8: Comparison with ShareGPT4V [46] and Florence [62] in visual understanding and spatial relationships. The blue underline correspond to areas with more detailed and accurate descriptions, while red underline and red strikethrough represent the incorrect and insufficient descriptions respectively. and loss curve trends. In the high-quality tuning stage, the model achieves substantial improvements within just 1K steps, e.g. from 85.7 to 86.6 on the DPG benchmark and from 0.67 to 0.71 on the GenEval. However, as high-quality tuning progressed, performance fluctuations are observed. Notably, at 11K steps in the third stage, the model continues improving on DPG benchmark, whereas the performance declines slightly on GenEval. Ablation Study on Efficient Inference Strategy. In Fig. 11, we evaluate the inference efficiency of Lumina-Image 2.0 under 1024-resolution setting using multiple inference strategies, including CFG-Renorm, CFG-Trunc, FDPM, and TeaCache. First, our results show that the proposed CFG-Renorm and CFG-Trunc fusion method (Sec. 4.5) not only saves sampling time but also has negligible impact on the quality of the sampled results. Second, integrating FDPM into our model can effectively reduce inference time. However, empirical evaluations indicate that FDPM suffers from poor stability, negatively affecting sample quality during the generation process. Third, while incorporating TeaCache further improves sampling speed, it significantly degrades image quality, often leading to blurriness. As result, in 14 Figure 9: High-quality image generation examples from Lumina-Image 2.0, showcasing its precise prompt-following ability and its capability to generate highly aesthetic and realistic images across different resolutions. 15 Table 8: Performance Comparison Across Stages on DPG [30] and GenEval [31] Benchmarks. Stage Steps (K) Low Res. Stage High Res. Stage HQ Tuning Stage HQ Tuning Stage HQ Tuning Stage 15 38 1 5 11 DPG 84.5 85.7 86.6 87.2 87. GenEval 0.63 0.67 0.71 0.73 0.72 Figure 10: Loss curves for the three training stages, showing steady performance increase in the DPG [30] and GenEval [31] benchmark. Figure 11: Ablation study on efficient inference strategy. The performances are measured on single A100 GPU with batch size 1. practical applications, we adopt Lumina-Image 2.0 with CFG-Renorm and CFG-Trunc as the final solution to balance efficiency and quality. Figure 12: Generation defects of Lumina-Image 2.0, categorized into overall structural errors, texture detail errors, and text errors."
        },
        {
            "title": "6 Limitation",
            "content": "Although we have followed previous works [8, 15, 44, 10, 37] to evaluate our method on benchmarks such as GenEval [31] and T2ICompBench [32], achieving comparable performance with state-of-the-art models, we argue that these academic benchmarks are not comprehensive and may sometimes fail to accurately assess image quality in alignment with human perception. To illustrate this point, Fig. 12 highlights several limitations of Lumina-Image 2.0. First, for complex and diverse structures (e.g., human bodies) and for rare concepts in the training data (e.g., handguns), our model struggles to consistently generate correct results. Second, when handling images with intricate textures, such as densely crowded scenes or tire spokes, our model frequently generates disordered details. Finally, our model still needs substantial improvements in accurately rendering long and complex text."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduces Lumina-Image 2.0, unified and efficient text-to-image generative framework that achieves strong performance in both image quality and prompt alignment. Specifically, Unified Next-DiT model is developed to generate high-quality images through the seamless integration of textual and visual information. Unified Captioner (UniCap) is proposed to produce detailed and accurate textual descriptions for constructing high-quality image-text training pairs. In addition, set of efficient training and inference strategies is developed to further optimize performance while reducing computational costs. Lumina-Image 2.0 achieves promising performance on public benchmarks, and provides transparent, reproducible text-to-image generative framework. We hope that our model will contribute to advancing the field of text-to-image generation."
        },
        {
            "title": "References",
            "content": "[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [3] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [4] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [5] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [6] Haotian Tang, Yecheng Wu, Shang Yang, Enze Xie, Junsong Chen, Junyu Chen, Zhuoyang Zhang, Han Cai, Yao Lu, and Song Han. Hart: Efficient visual generation with hybrid autoregressive transformer. arXiv preprint arXiv:2410.10812, 2024. [7] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [8] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. Proceedings of the International Conference on Learning Representations (ICLR), 2025. [9] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2023. [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling, 2025. [11] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 38363847, 2023. [12] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 86408650, 2024. [13] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 60076017, 2023. [14] Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Junlin Xie, Yu Qiao, Peng Gao, and Hongsheng Li. Pixwizard: Versatile image-to-image visual assistant with open-language instructions. arXiv preprint arXiv:2409.15278, 2024. [15] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Peng Gao, Le Zhuo, Chris Liu, , Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [17] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. [18] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [19] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. arXiv preprint arXiv:2502.04896, 2025. [20] Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. Exploring the role of large language models in prompt encoding for diffusion models. arXiv preprint arXiv:2406.11831, 2024. [21] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [22] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [23] OpenAI. Chatgpt: Optimizing language models for dialogue, 2022. [24] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [25] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 18 [26] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [27] Zongyu Lin, Wei Liu, Chen Chen, Jiasen Lu, Wenze Hu, Tsu-Jui Fu, Jesse Allardice, Zhengfeng Lai, Liangchen arXiv preprint Song, Bowen Zhang, et al. Stiv: Scalable text and image conditioned video generation. arXiv:2412.07730, 2024. [28] Mingyang Yi, Aoxue Li, Yi Xin, and Zhenguo Li. Towards understanding the working mechanism of text-to-image diffusion model. Advances in Neural Information Processing Systems (NeurIPS), 2024. [29] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. arXiv preprint arXiv:2411.19108, 2024. [30] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [31] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [32] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems (NeurIPS), 36:7872378747, 2023. [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), volume 4172, 2022. [35] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. Proceedings of the European Conference on Computer Vision (ECCV), 2024. [36] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning (ICML), 2024. [37] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [40] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [41] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [42] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [43] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. [44] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. [45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 19 [46] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pages 370387. Springer, 2024. [47] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [48] Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, et al. Sphinx-x: Scaling data and parameters for family of multi-modal large language models. arXiv preprint arXiv:2402.05935, 2024. [49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [50] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. [51] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization. Text Reading, and Beyond, 2, 2023. [52] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [54] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. [55] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: enhanced transformer with rotary position embedding. arxiv. arXiv preprint arXiv:2104.09864, 2021. [56] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [57] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [58] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [59] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [60] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, et al. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. [61] Kaiyan Zhang, Biqing Qi, and Bowen Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion. arXiv preprint arXiv:2407.08642, 2024. [62] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48184829, 2024. [63] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. 20 [64] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprint arXiv:2111.03930, 2021. [65] Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019. [66] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [67] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [68] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [69] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. Advances in Neural Information Processing Systems Workshops (NeurIPS Workshops), 2021. [70] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. [71] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. [72] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [73] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [74] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. [75] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. [76] Arpad Elo and Sam Sloan. The rating of chessplayers: Past and present. Arco Pub, 1978. [77] Kuaishou Technology. Kolors. https://github.com/Kwai-Kolors/Kolors, 2024. [78] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [79] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024."
        }
    ],
    "affiliations": [
        "Krea AI",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Sydney"
    ]
}