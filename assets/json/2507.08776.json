{
    "paper_title": "CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering",
    "authors": [
        "Zhengqing Wang",
        "Yuefan Wu",
        "Jiacheng Chen",
        "Fuyang Zhang",
        "Yasutaka Furukawa"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 7 7 8 0 . 7 0 5 2 : r CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering Zhengqing Wang1 Yuefan Wu1 Jiacheng Chen1 Fuyang Zhang1 Yasutaka Furukawa1,2 1Simon Fraser University 2Wayve"
        },
        {
            "title": "Abstract",
            "content": "This paper proposes neural rendering approach that represents scene as compressed light-field tokens (CLiFTs), retaining rich appearance and geometric information of scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent scene or render novel view with one trained network. Concretely, given set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects reduced set of rays as cluster centroids using the tokens. The multi-view condenser compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given target view and compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes novel view using compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed. Check demos and code on the project page: https://c-lift.github.io."
        },
        {
            "title": "Introduction",
            "content": "Global consumption of visual media is skyrocketing, driven by platforms like Instagram, YouTube, and TikTok. Billions of photos and videos are captured, shared, and streamed daily, placing enormous demands on storage and bandwidth. The success of these platforms owes much to advances in visual data compression, allowing high-resolution content to be delivered efficiently across range of devices and network conditions. From images to high-definition videos, modern compression algorithms have enabled rich visual experiences to become ubiquitous part of everyday life. Beyond passive viewing of recorded media, interactive novel view synthesis (NVS) is gaining momentum, allowing users to freely navigate virtual environments. Neural rendering techniques such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) are the driving forces. Recent research has explored efficiency of neural rendering pipelines, including compression of radiance fields [24, 35], sparse representations [17, 22], and adaptive quality rendering [9, 36] Along the line of interactive visual media, reconstruction-free novel view synthesis is emerging as promising direction. Models such as Large View Synthesis Models (LVSM) [12] and Scene Representation Transformers (SRT) [25] synthesize novel views directly, without defining bottleneck geometric and photometric representations by heuristics and reconstructing them. By avoiding explicit reconstruction, these methods would better handle scene dynamics and capture fine-grained visual details directly from the data. Compute-efficient representations and compute-adaptive rendering within these systems would unlock the full potential of NVS technology, driving new applications Preprint. Under review. across real estate (virtual property tours), entertainment (immersive media and games), online shopping (interactive product displays), and autonomous driving (simulation and model validation). Towards realizing this potential, this paper introduces new representation and rendering framework centered on the concept of Compressive Light-Field Tokens (CLiFT). CLiFT is compact set of light field rays with compressed learned embeddings. Concretely, given set of images with camera poses as input, multi-view encoder tokenizes the images with camera poses. latent-space K-means algorithm selects reduced set of rays as cluster centers. Intuitively, the resulting cluster centers preserve coverage due to geometric diversity among rays, while becoming denser in texture-rich regions. Multi-view condenser compresses the information of all the embeddings into the centroid tokens to construct CLiFTs At test time, given target camera pose and compute budget (i.e., the number of CLiFTs to use), the system collects the corresponding nearby tokens and synthesizes novel view using compute-adaptive renderer that is trained to handle variable number of tokens. We have evaluated the approach on RealEstate10K [37] and DL3DV [16] datasets, and compared with three state-of-the-art methods, LVSM [12] from the reconstruction-free approach, and MVSplat [3] and DepthSplat [34] from the reconstruction-based approach. Extensive quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed."
        },
        {
            "title": "2 Related Work",
            "content": "Light Field Imaging. Computational light field imaging techniques date back to the 1990s with notable work by Levoy et al. on Light Field Rendering [15] and Gortler et al. on Lumigraph [10]. Both introduced the idea of capturing dense array of rays from multiple viewpoints, enabling novel view generation without geometry reconstruction. Subsequent efforts, such as multi-camera arrays [32] and hand-held plenoptic cameras [23], brought these ideas into practical systems. For example, commercial light field cameras like the Lytro (circa 2011) demonstrated post-capture refocusing and viewpoint adjustment. Recent works combine classical light field concepts with neural rendering to enable efficient and accurate novel view synthesis with view-dependent effects [5, 26, 28, 29]. The idea of using light fields (or rays) as scene representation forms the core of our approach, where we combine this classical concept with modern neural rendering techniques. Compressive Sensing. In the mid-2000s, compressive sensing emerged as paradigm for capturing signals with far fewer samples than traditional Nyquist sampling theory would require, given the signal is sparse in some basis [1, 4]. In computational imaging, compressive sensing inspired novel camera designs that sample and project measurements into lower-dimensional spaces. Notable examples include the single-pixel camera [6] and coded aperture systems [20] that optically encode high-dimensional scene information into compressed sensor readings. Similar to compressive sensing, our approach selects which rays to store and how to represent them in compact form for successful view synthesis. key distinction is that our method learns to compress all the input rays into compact set of representative rays using neural networks, whereas compressive sensing relies on predefined heuristics to drop information and project to lower dimension. Reconstruction-Based Novel View Synthesis. Early image-based rendering (IBR) systems relied on reconstructed scene geometry for novel view generation. Photo Tourism is seminal example, using planar geometric proxy for renderingsimple yet producing compelling visual experiences [27]. Subsequent work extended this idea by more accurate depth maps to enhance rendering quality [8]. More recently, Neural Radiance Fields (NeRF) introduced continuous volumetric scene representation as 5D function that maps spatial location and viewing direction to color and density using an MLP [21]. 3D Gaussian Splatting (3DGS) eliminates neural networks in favor of explicit point-based primitives, representing the scene as millions of 3D Gaussians and rendering images by projecting (splatting) them with view-dependent shading [13]. These methods typically require per-scene optimization, lack generalization across scenes, and assume dense input coverage. To overcome the limitations, feed-forward Gaussian splatting methods [2, 3, 34] eliminate per-scene optimization by predicting Gaussian parameters in single forward pass. In parallel, image and video generative models have been incorporated to refine NVS outputs, enhancing realism and temporal consistency [7, 31]. Recent methods like Reconfusion [33] and NerfDiff [11] take step further by using generative refinement. 2 Figure 1: The training and the inference system overview. Top: The training consists of three steps: 1) Multi-view encoder, tokenizing the input images; 2) Latent K-means, selecting representative set of tokens; and 3) Neural condensation, compressing the information of all the tokens into the representative set to produce Compressed Light-Field Tokens (CLiFTs). Bottom: At inference time, given target view, we collect relevant set of rays with CLiFTs and render novel view. Reconstruction-Free Novel View Synthesis. Reconstruction-free novel view synthesis is gaining attention as an end-to-end solution. The Scene Representation Transformer (SRT) encodes set of input images into latent scene representationa collection of tokensand renders novel views from this latent in single feed-forward pass [25]. Another representative work is LVSM (Large View Synthesis Model), fully transformer-based pipeline that achieves state-of-the-art results from sparse inputs [12]. LVSM includes both an encoderdecoder transformer, which compresses input images into fixed-length latent code before decoding novel views, and decoder-only transformer that maps input views directly to output pixels. Multi-view generative models share similar characteristics, producing consistent images across views without reconstructing or generating explicit geometry [14, 18, 19, 30]. This paper advances the frontier of reconstruction-free NVS by introducing Compressive Light-Field Tokens: compact, variable-size scene representation paired with renderer that adaptively balances quality and resource usage on demand."
        },
        {
            "title": "3 Compressive Light-Field Tokens (CLiFT)",
            "content": "A light field is collection of half-rays, each associated with the radiance information. Compressive light-field tokens (CLiFTs) are collection of half-rays, each associated with latent vector to which neural encoder compresses the geometry and radiance information of scene (See Figure 1). Given target camera pose, traditional light field rendering synthesizes color for each target ray, typically by linear interpolation of nearby input rays. CLiFT rendering synthesizes an image (not necessarily per ray) by using nearby CLiFTs via neural renderer. The framework allows us to control data size, rendering quality, and rendering speed by two hyper parameters 1) Storage CLiFT count (Ns), the 3 number of tokens to represent scene; and 2) Render CLiFT count (Nr), the number of tokens to use in rendering frame. The section explains the problem, CLiFT construction, and CLiFT rendering. 3.1 Problem Definition This paper tackles the problem of taking Nc images with the camera poses, and constructing Ns CLiFTs as scene representation. Given target view, the output is its synthesized image, where Nr CLiFTs tokens are to be selected and used for rendering. The evaluation is based on standard image reconstruction metrics, PSNR, SSIM, and LPIPS. token (CLiFT) is dimensional embedding, associated with half-ray. Ray geometries are represented in global coordinate frame. 3.2 CLiFT Construction Construction of compressed light field tokens (CLiFTs) involves three key steps: encoding multi-view images and camera poses, selecting representative rays, and condensing information into the selected set. The section explains the steps. Multi-view Encoding. Given images with camera poses, we use standard Transformer encoder to extract our Light Field Tokens (LiFT), which capture both geometry and appearance information. Specifically, for each pixel in each image, we concatenate the 6D Plücker coordinates of the corresponding ray with its normalized 3D color vector. We then patchify non-overlapping 88 regions, resulting in 576 = (3 + 6)88 dimensional vectors. 1 Each vector is linearly projected to token of dimension = 768. This process converts Nc input images of resolution 256256 into 1024 Nc tokens. The Transformer encoder performs self-attention across all 1,024 Nc tokens, producing total of 4,096 LiFTs for scene of 4 input views. Specifically, our encoder has six self-attention blocks, each comprising Self-Attention (8 heads), Add & LayerNorm, Feedforward Network, and another Add & LayerNorm. Unlike the encoder-decoder architecture in LVSM, which uses learnable token to aggregate scene information, our method directly uses the outputs of Transformer encoder, which retains the geometry and appearance of specific region. Latent-space K-means for Ray Selection. Effective ray selection is crucial, as uniform ray selection across input images leads to two redundancies: 1) Appearance redundancy at texture homogeneous regions; and 2) Geometric redundancy at visual overlaps among different views. Our approach performs latent-space clustering to select representative rays that compactly represent an entire scene. K-means clustering algorithm finds the clusters from LiFT of all the images. The nearest neighbor sample from each center is retained as the cluster centroid. Centroid LiFT will be the storage CLiFTs, and is set to Ns (i.e, the number of storage CLiFTs). Neural Condensation. lightweight transformer condenses information from all LiFTs into set of centroid LiFTs, producing CLiFTs. Self-attention operates over the centroid LiFTs to enable information exchange across clusters. Within each cluster, cross-attention uses the centroid LiFT as the query and the remaining LiFTs as keys and values. To preserve the pretrained latent space, zero-initialized linear layer aggregates features back into the centroid LiFTs. Concretely, the condensation network consists of two transformer decoder blocks. Each block includes inter-cluster self-attention (8 heads), intra-cluster cross-attention (8 heads), and standard Add & LayerNorm operations. Let Tk RD denote the embedding of the k-th centroid LiFT, and {Tk,i i} represent the remaining LiFTs assigned to the k-th cluster. SA(), CA(, ), and FFN() denote the self-attention, cross-attention, and feed-forward network layers, respectively. Wz is zero-initialized linear projection operator Wz RDD. Each decoder block updates the centroid embeddings as follows, producing CLiFTs after the second block: { ˆTk} SA ({LN(T1), LN(T2), }) , ˆTk CA( ˆTk, {LN(Tk,1), LN(Tk,2), }), ˆTk FFN(LN( ˆTk)), k Tk Tk + Wz( ˆTk). (1) (2) (3) (4) 1As explained in 4, we use RealEstate10K [37] and DL3DV [16] datasets. Image resolutions are 256256 and 256448, respectively. The section uses RealEstate10K as an example to explain the feature dimensions. The patch size is the same for DL3DV, resulting in 1,792 Nc tokens instead. 4 3.3 CLiFT Rendering CLiFTs enable flexible and efficient rendering mechanism. Given target view and the render CLiFT count (i.e., the number of tokens to use), we collect set of relevant CLiFTs, which are then used by neural renderer to synthesize the view. Neural Renderer. The renderer is simple Transformer decoder where the target view serves as the query and the selected CLiFTs serve as keys and values. Specifically, we initialize 2D grid of Plücker coordinates at each pixel and patchify them. Each patch is represented as 384-dimensional vector derived from the 688 Plücker coordinates, which is projected to 768 dimensions via linear layer. The rendering network has six decoder blocks, each comprising Self-Attention (among query tokens), Add & LayerNorm, Cross-Attention (from CLiFTs to query tokens), Add & LayerNorm, Feedforward Network, and Add & LayerNorm. The number of heads is eight in SA and CA layers. The decoder output is passed through linear projection, followed by sigmoid activation to map it to the RGB space, and then an unpatchify operation to reconstruct the full-resolution image. We use combination of L2 loss and perceptual loss (with 0.5 weight) following LVSM [12], applied to the normalized RGB intensities. During training, we randomly vary the number of CLiFTs passed to the decoder, allowing it to learn how to handle different token counts and enabling dynamic trade-offs between rendering quality and computational cost. Token Selection. The token selection algorithm employs simple heuristic. Let Nr denote the number of CLiFTs used for rendering. To ensure spatial coverage of the target view, we divide the view into 1616 grid of patches and pad it with 4-patch margin on all sides, resulting in 2424 grid. For each patch, we cast ray through its center and retrieve the Nr/(24 24) closest CLiFTs from pool of Ns storage CLiFTs. The distance between patch and CLiFT is computed using heuristic based on their associated half-rays (ray origins and directions); details are provided in the supplementary material. Since CLiFTs may be selected multiple times, we greedily accumulate CLiFTs in order of their minimum distance to any patch ray until Nr unique tokens are selected."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Datasets. We use two scene datasets RealEstate10K [37] and DL3DV [16], following the recent literature [3, 12, 34]. 2 We preprocess videos and create training/testing video clips in exactly the same as PixelSplat [2] for RealEstate10K and DepthSplat [34] for DL3DV. The only difference is the number of images to use from each clip for training and testing. Concretely, LVSM [12], MVSplat [3], and DepthSplat [34] used 2 images for training and testing for RealEstate10K. MVSplat and DepthSplat used 2-6 images for training and testing for DL3DV. We use 4-6 images for training and 4-8 images for testing in both datasets to handle larger scenes. Evaluation Metrics. We evaluate the rendered image quality by PSNR, LPIPS, and SSIM. To assess computational efficiency, we report data size, rendering FPS, and rendering FLOPs, where FLOPs are theoretical numbers instead of measured ones. Training Details. We adopt two-stage training strategy. The first stage is to train the multi-view encoder where the neural renderer is directly connected to back-propagate gradients without the latent K-means or the condensation modules. In this case, all the tokens from the encoder are passed to the renderer through cross-attention. The second stage uses all the modules while freezing the multi-view encoder. Since K-means is not efficient for online sampling, we pre-compute the cluster assignments offline and use them in the second stage. For experiments on the 256256 RealEstate10K, the first and the second stages train for 90,000 steps with batch-size 64 and 50,000 steps with batch-size 80, respectively. For DL3DV (256448), we finetune the RealEstate10K-pretrained model for 100,000 steps with batch size of 24 in the first stage and 32 in the second stage. Both datasets use the same cosine learning rate scheduler with 2500-step warmup. The peak learning rate is 4 104 for RealEstate10K and 2 104 for DL3DV, with the learning rate of the renderer scaled by 0.1 in the second stage. We use four NVIDIA RTX A100 GPUs to train our model. Training takes approximately 3 days on RealEstate10K and 5 days on DL3DV. 2RealEstate10K is under Creative Commons Attribution 4.0 International License. DL3DV is under DL3DV10K Term of use and Creative Commons Attribution-NonCommercial 4.0 International License. 5 Figure 2: Main evaluation results on the RealEstate10K dataset (left) and the DL3DV dataset (right), comparing our approach with three baseline methods (LVSM-ED [12], DepthSplat [34], and MVSplat [3]). The x-axis is the data size of the scene representation. The y-axis is the rendering quality as PSNR. Our approach CLiFT is capable of changing the data size (i.e., the number of tokens) with one trained model. CLiFT achieves significant data size reduction with comparable rendering quality and the highest overall score, while providing trade-offs of data size, rendering quality, and rendering speed. Baselines. We compare with three state-of-the-art methods, one from the reconstruction-free approach and the other two from the reconstruction-based approach. LVSM [12] is state-of-the-art reconstruction-free method. While publicly available checkpoint exists for RealEstate10K, it corresponds to significantly larger model, featuring twice as many blocks in both the encoder and decoder, and was trained using 64 A100 GPUs with only 2 input views. To enable fair comparison, we trained the LVSM-ED system under our setting: using 6 blocks in both the encoder and decoder, and 4 input views. Given the substantial compute requirements of LVSM (i.e., 64 A100 GPUs as reported by the authors), we chose to compare on RealEstate10K. As the LVSM code was not available during our initial experiments, we first reproduced their system independently, and later refined our implementation based on the official code after its release. MVSplat [3] and DepthSplat [34] are state-of-the-art feedforward splatting-based novel view synthesis systems. They achieve strong performance on benchmarks like RealEstate10K and DL3DV by leveraging explicit 3D scene representations and depth-guided 3D Gaussian splatting for rendering. We use publicly available checkpoints for MVSplat and DepthSplat (i.e., 2-view case for RealEstate10K and 2, 4, or 6-view cases for DL3DV). 4.2 Main Results Figure 2 and Figure 4 show the main evaluation results on the RealEstate10K and DL3DV datasets. In the plots, the x-axis represents the data size of the scene representation: Storage CLiFTs for our method, decoder input tokens for LVSM, and splats for MVSplat and DepthSplat. The y-axis reports PSNR; due to space constraints, the LPIPS and SSIM plots are deferred to the Appendix, where they show consistent trends. None of the baselines support controlling the data size and require separately trained model for each data point. Many baseline points are missing from the plots because we either use publicly available checkpoints (for MVSplat and DepthSplat) or train new model (4.1). In contrast, our method trains single model per dataset and supports fine-grained control over both the storage data size and the render data size via the numbers of Storage (Ns) and Render (Nr) CLiFT tokens. The plots show that CLiFT achieves comparable PSNR with approximately 57 less data size than MVSplat and DepthSplat, and about 1.8 less than LVSM, highlighting the effectiveness of our compressed tokens and overall scene representation. CLiFT also attains the highest overall PSNR with significantly lower data usage. Qualitative results in Figure 4 support these findings: our method preserves sharp appearance details that are closer to the ground truth and maintains high visual fidelity even under strong compression, with only minor loss in high-frequency content. 6 Figure 3: Ablation studies on our individual components, in particular, latent K-means and neural condensation. The plots compare three variants of our system by dropping latent K-means and neural condensation one by one from the system, while varying the data size. Specifically, the x-axis is the size of the scene representation. The y-axis is rendering quality (PSNR, LPIPS, and SSIM), rendering speed (FPS), or rendering cost (FLOPs), measured on an NVIDIA RTX A6000 GPU. Table 1: We fix the number of storage CLiFTs to represent scene (Ns=4096), then vary the number of render CLiFTS (how many tokens to use for rendering) on-the-fly and measure rendering quality (PSNR), rendering speed (PSNR), and rendering cost (theoretical number as GFLOPs). Metrics PSNR GFLOPs FPS 4096 26.72 70.6 54.3 Render CLiFTs 3072 2048 512 26.71 (-0.01) 63.3 (-10%) 53.89 (-1%) 26.56 (-0.16) 56.1 (-21%) 66.44 (+22%) 25.75 (-0.97) 48.9 (-31%) 80.77 (+49%) 23.89 (-2.83) 45.23 (-36%) 90.15 (+66%) 4.3 Ablation Studies CLiFT Construction. To demonstrate the effectiveness of our individual components, Figure 3 compares three variants of our system: 1) The full system (blue); 2) The system without the condenser (green); and 3) The system without both the condenser and latent K-means (red), where tokens are selected randomly instead. We evaluate all variants on 92 randomly selected scenes to ensure consistent and fair comparisons, using an NVIDIA RTX A6000 GPU. Figure 3 shows how rendering quality (PSNR, LPIPS, SSIM), rendering speed (FPS), and rendering cost (FLOPs: theoretical number instead of measured) change with different compression rates. Specifically, we fix the number of CLiFT tokens for storage (Ns) and rendering (Nr) to be the same, and vary them together. When the compression rate is low (<2), all variants perform similarly since even randomly selected tokens provide reasonable coverage. However, at high compression rates, the gap between random selection and K-means selection becomes significant: random selection fails to capture representative tokens, while K-means selects tokens that better summarize the scene. Figure 5 visualizes the selected centroids and their associated cluster members. The clustering is performed jointly across multiple views, encouraging tokens from different images to form non-redundant clusters. Furthermore, clusters tend to grow larger in texture-homogeneous regions. CLiFT Rendering. CLiFT token is associated with specific ray, encoding localized geometry and appearance. This ray-based design enables intuitive and efficient token selection heuristics at inference time (3.3), allowing the renderer to control the trade-off between quality and speed on the fly. For example, in large scenes with multiple rooms, rendering view within one room can be done 7 Figure 4: Qualitative rendering results of the baselines and ours with different data size (i.e., the number of CLiFT tokens for ours). Top: Ours vs. LVSM [12] on RealEstate10K. Bottom: Ours vs. DepthSplat [34] on DL3DV. The PSNR value is recorded under each rendering. Figure 5: Visualization of the latent K-means clustering, where K=Ns=128. Each color represents cluster, and the yellow ring indicates the centroid token. Note that clustering is performed across multiple views, so single cluster can span multiple images. As result, some clusters may not have visible centroid in given image. Figure 6: Typical failure cases are (Left) Camera motions deviating too much from the training distributions in RealEstate10K; and (Right) Target views not covered by the input images in DL3DV. without using tokens from unrelated areas. In contrast, methods like LVSM represent the entire scene using fixed set of global tokens, which limits their ability to selectively render parts of the scene or adjust computation dynamically. We validate this ability on large scenes from the RealEstate10K dataset. Specifically, we construct large-scene test set by filtering for scenes with more than 200 frames, as higher frame counts typically indicate broader camera motion. We use 50 such scenes for evaluation, with 8 input views per scene to ensure sufficient coverage. As shown in Table 1, our method supports on-the-fly adjustment of rendering cost by varying the number of tokens used, achieving lower FLOPs and higher FPS when needed, or allocating more tokens for higher quality."
        },
        {
            "title": "5 Conclusions and Future Challenges",
            "content": "We introduced CLiFT, compressive light-field token representation paired with compute-adaptive transformer renderer. Experiments on RealEstate10K and DL3DV demonstrate that CLiFT achieves higher data reduction rate with comparable rendering quality and the highest overall rendering score, while capable of controlling the trade-offs between data size, rendering quality, and rendering speed. Our current system exhibits two typical failure modes. The first occurs when camera motions deviate significantly from the training distribution. As shown on the left of Figure 6, RealEstate10K training data primarily consists of smooth translations with minor rotations, making it difficult to generalize to more complex motions. The second failure mode arises in large scenes where target views are not adequately covered by the input views, resulting in blurry renderings. This is illustrated on the right of Figure 6 with examples from DL3DV. promising future direction is to extend our framework by incorporating generative priors to improve rendering quality in unseen or occluded areas. Broader Impacts. On the positive side, our method has potential applications in digital media content consumption, enabling more efficient and flexible rendering for immersive experiences. However, like generative models, it raises concerns around misuseparticularly in the creation of deep-fakes. We emphasize the importance of responsible deployment and clear content provenance."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Haian Jin for helpful discussions on reproducing LVSM and training on the DL3DV dataset. This research is partially supported by NSERC Discovery Grants, NSERC Alliance Grants, and John R. Evans Leaders Fund (JELF). We thank the Digital Research Alliance of Canada and BC DRI Group for providing computational resources."
        },
        {
            "title": "References",
            "content": "[1] Emmanuel Candès, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 2006. [2] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. [3] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. [4] David Donoho. Compressed sensing. IEEE Transactions on Information Theory, 2006. [5] Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views from wide-baseline stereo pairs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49704980, 2023. [6] Marco Duarte, Mark Davenport, Takhar, Jason Laska, Ting Sun, Keith Kelly, and Richard Baraniuk. Single-pixel imaging via compressive sampling. IEEE Signal Processing Magazine, 2008. [7] Lue Fan, Hao Zhang, Qitai Wang, Hongsheng Li, and Zhaoxiang Zhang. Freesim: Toward free-viewpoint camera simulation in driving scenes. arXiv preprint arXiv:2412.03566, 2024. [8] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE transactions on pattern analysis and machine intelligence, 2009. [9] Stephan Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien Valentin. Fastnerf: High-fidelity neural rendering at 200fps. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1434614355, 2021. [10] Steven Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael Cohen. The lumigraph. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 4354, 1996. [11] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In International Conference on Machine Learning, pages 1180811826. PMLR, 2023. [12] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.17242, 2024. [13] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. [14] Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95039513, 2024. 10 [15] Marc Levoy and Pat Hanrahan. Light field rendering. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 3142, 1996. [16] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 3d vision. Recognition, pages 2216022169, 2024. [17] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. Neural sparse voxel fields. Advances in Neural Information Processing Systems, 33:1565115663, 2020. [18] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [19] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99709980, 2024. [20] Kshitij Marwah, Gordon Wetzstein, Yosuke Bando, and Ramesh Raskar. Compressive light field photography using overcomplete dictionaries and optimized projections. In ACM Transactions on Graphics (TOG). ACM, 2013. [21] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. [22] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4): 115, 2022. [23] Ren Ng, Marc Levoy, Mathieu Brédif, Gene Duval, Mark Horowitz, and Pat Hanrahan. Light field photography with hand-held plenoptic camera. PhD thesis, Stanford university, 2005. [24] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1433514345, 2021. [25] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luˇcic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. [26] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:1931319325, 2021. [27] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In ACM Transactions on Graphics (TOG). ACM, 2006. [28] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Generalizable patchbased neural rendering. In European Conference on Computer Vision, pages 156174. Springer, 2022. [29] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and Ameesh Makadia. Light field neural In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern rendering. Recognition, pages 82698279, 2022. [30] Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan. Mvdiffusion++: dense high-resolution multi-view diffusion model for single or sparse-view 3d object reconstruction. In European Conference on Computer Vision. Springer, 2024. 11 [31] Qitai Wang, Lue Fan, Yuqi Wang, Yuntao Chen, and Zhaoxiang Zhang. Freevs: Generative view synthesis on free driving trajectory. arXiv preprint arXiv:2410.18079, 2024. [32] Bennett Wilburn, Neel Joshi, Vaibhav Vaish, Eino-Ville Talvala, Emilio Antunez, Adam Barth, Andrew Adams, Mark Horowitz, and Marc Levoy. High performance imaging using large camera arrays. In ACM Transactions on Graphics (TOG), 2005. [33] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2155121561, 2024. [34] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. arXiv preprint arXiv:2410.13862, 2024. [35] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 57525761, 2021. [36] Shiran Yuan and Hao Zhao. Slimmerf: Slimmable radiance fields. In 2024 International Conference on 3D Vision (3DV), pages 6474. IEEE, 2024. [37] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. The appendix provides:"
        },
        {
            "title": "Appendix",
            "content": "A: Additional experimental results in the same format as figures/tables in the main paper, including the full metric evaluations on the DL3DV dataset, more rendering samples, and more intermediate visualizations (i.e., clusteirng and token-selection results). B: Implementation details of the latent K-means algorithm (during CLiFT construction) and the token selection algorithm (during CLiFT rendering). Please check our project page at https://c-lift.github.io, which shows rendering results at different compression rates and compares with the baseline methods."
        },
        {
            "title": "A Additional Experiment Results",
            "content": "A.1 Additional quantitative results Figure 2 of the main paper reports PSNR as the primary rendering metric and shows that our method consistently outperforms baselines, achieving the highest overall PSNR. Figure 7 and Figure 8 show SSIM and LPIPS results for RealEstate10K and DL3DV. Our method consistently outperforms baselines under the same data size, while at very large data-size setting, DepthSplat [34] yields higher scores on DL3DV. Figure 7: Evaluation results (SSIM and LPIPS) on the RealEstate10K dataset, comparing our approach with three baseline methods (LVSM-ED [12], DepthSplat [34], and MVSplat [3]). The x-axis is the data size of the scene representation. Figure 8: Evaluation results (SSIM and LPIPS) on the DL3DV dataset, comparing our approach with two baseline methods (DepthSplat [34] and MVSplat [3]). The x-axis is the data size of the scene representation. 13 A.2 Additional rendered images Figure 4 of the main paper presents rendering results under varying data size constraints. We provide additional qualitative examples on RealEstate10K (see Figure 9 and Figure 10) and DL3DV (Figure 11 and Figure 12). These examples cover finer range of compression rates, extending up to 32, and highlight the robustness of our method across varying levels of compression."
        },
        {
            "title": "B Additional implementation details",
            "content": "B.1 Details of K-means algorithm During training, we precompute cluster assignments after the multi-view encoder training and before the condensation training, using faiss.Kmeans which supports GPU acceleration. At test time, we use sklearn.cluster.KMeans for better accuracy. B.2 Details of Token Selection Rendering specific region within large scene does not require all tokens. Using only the tokens essential to the target view improves FPS and reduces FLOPs, with minimal impact on PSNR. Algorithm 1 is our token selection algorithm. 14 Figure 9: Additional qualitative results on the RealEstate10K dataset. 15 Figure 10: Additional qualitative results on the RealEstate10K dataset. 16 Figure 11: Additional qualitative results on the DL3DV dataset. 17 Figure 12: Additional qualitative results on the DL3DV dataset. 18 Algorithm 1 Token Selection Algorithm θ: Ray-angle distance between patches δ: Camera center distance, δ = ot ok : Number of rays per patch (e.g., = 16 16) : number of condition images wangle = 1.0, wdist = 0.02, wmask = 0.03 momentum factor η = 0.5 1: Definitions: 2: 3: 4: mk: Last frame token mask 5: 6: 7: 8: 9: Input: 10: 11: 12: 13: 14: Output: Selected token indices 15: Downsample target and condition view rays into 16 16 patches 16: Expand each target patch to 24 24 region, including out-of-image rays, to incorporate Target view ray (ot, dt) with ot R3, dt RP 3; Condition view rays {(ok, dk)}N Last frame token mask {mk}N k=1 Total number of tokens to select k=1 with ok R3, dk RP 3; edge-adjacent context and avoid boundary under-coverage 17: Set number condition rays per target patch / / (24 24) 18: Initialize selected patches Ipatch 19: // Patch-wise selection 20: for each patch Pi in expanded target patches do 21: 22: 23: 24: 25: 26: for each condition patch k,j in condition view do for each condition view from 1 to do Compute ray-angle distance θ between rays in Pi and k,j Compute camera center distance between target and condition view δ = ot ok Retrieve last frame mask mk Compute frame distance: Di,j Apply momentum: Dk,j Store previous step objective: ˆDk,j = wangle θ + wdist δ + wmask mk + η ˆDk,j (1 η) Dk,j Dk,j 27: 28: 29: 30: 31: 32: end for end for Let Di = {Dk,j local Topn Ipatch Ipatch k, j} (cid:0)argsortasc(Di)(cid:1) local 33: end for 34: Compute unique set: Iunipatch unique(Ipatch) 35: Let Trest Iunipatch 36: // Global fallback selection 37: Initialize set of fallback distances: Dglobal 38: for each condition view from 1 to do for each condition patch in view do 39: 40: 41: 42: 43: end for 44: Iglobal Topn (argsortasc(Dglobal)) 45: return = Ipatch Iglobal Compute Dk,j = mini Dk,j Dglobal Dglobal { Dk,j} end for // best match to any target patch Figure 13: Additional visualization of the latent K-means algorithm for the RealEstate10k dataset. 20 Figure 14: Additional visualization of the latent K-means algorithm for the DL3DV dataset. 21 Figure 15: Additional visualization of the latent K-means algorithm with different values of K, which is 512, 1024, 2048, and 3072 from the top."
        }
    ],
    "affiliations": [
        "Simon Fraser University",
        "Wayve"
    ]
}