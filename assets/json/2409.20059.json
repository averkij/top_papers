{
    "paper_title": "Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis",
    "authors": [
        "Hippolyte Gisserot-Boukhlef",
        "Ricardo Rei",
        "Emmanuel Malherbe",
        "Céline Hudelot",
        "Pierre Colombo",
        "Nuno M. Guerreiro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Neural metrics for machine translation (MT) evaluation have become increasingly prominent due to their superior correlation with human judgments compared to traditional lexical metrics. Researchers have therefore utilized neural metrics through quality-informed decoding strategies, achieving better results than likelihood-based methods. With the rise of Large Language Models (LLMs), preference-based alignment techniques have gained attention for their potential to enhance translation quality by optimizing model weights directly on preferences induced by quality estimators. This study focuses on Contrastive Preference Optimization (CPO) and conducts extensive experiments to evaluate the impact of preference-based alignment on translation quality. Our findings indicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data with regard to the alignment metric, it may lead to instability across downstream evaluation metrics, particularly between neural and lexical ones. Additionally, we demonstrate that relying solely on the base model for generating candidate translations achieves performance comparable to using multiple external systems, while ensuring better consistency across downstream metrics."
        },
        {
            "title": "Start",
            "content": "Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis Hippolyte Gisserot-Boukhlef1,4 Ricardo Rei2 Emmanuel Malherbe1 Céline Hudelot4 1Artefact Research Center Pierre Colombo3,4 Nuno M. Guerreiro2,4,5,6 2Unbabel 3Equall 4 2 0 2 0 3 ] . [ 1 9 5 0 0 2 . 9 0 4 2 : r 4MICS, CentraleSupélec, Université Paris-Saclay 5Instituto de Telecomunicações 6Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit) hippolyte.gisserot-boukhlef@centralesupelec.fr"
        },
        {
            "title": "Abstract",
            "content": "Neural metrics for machine translation (MT) evaluation have become increasingly prominent due to their superior correlation with human judgments compared to traditional lexical metrics. Researchers have therefore utilized neural metrics through quality-informed decoding strategies, achieving better results than likelihood-based methods. With the rise of Large Language Models (LLMs), preferencebased alignment techniques have gained attention for their potential to enhance translation quality by optimizing model weights directly on preferences induced by quality estimators. This study focuses on Contrastive Preference Optimization (CPO) and conducts extensive experiments to evaluate the impact of preference-based alignment on translation quality. Our findings indicate that while CPO consistently outperforms Supervised Fine-Tuning (SFT) on high-quality data with regard to the alignment metric, it may lead to instability across downstream evaluation metrics, particularly between neural and lexical ones. Additionally, we demonstrate that relying solely on the base model for generating candidate translations achieves performance comparable to using multiple external systems, while ensuring better consistency across downstream metrics."
        },
        {
            "title": "Introduction",
            "content": "Neural metrics for machine translation evaluation that are trained to mimic human preferences, such as BLEURT (Sellam et al., 2020), COMET (Rei et al., 2020, 2022a), or Metric-X (Juraska et al., 2023), have become increasingly prevalent. These metrics offer greater accuracy and better reflect human judgments compared to traditional lexical metrics (Mathur et al., 2020; Kocmi et al., 2021; Freitag et al., 2022b; Kocmi et al., 2024) like 1All relevant preference datasets and aligned models, along with detailed evaluation metrics, are available at https://huggingface.co/collections/artefactory/ translation-alignment-analysis. BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005) or chrF (Popovic, 2015), which mainly consider lexical overlap with reference text. As such, researchers have attempted to leverage these improvements by integrating them directly into translation systems. One appealing strategy to incorporate quality information to improve downstream translation performance involves using decoding strategies such as N-Best reranking and Minimum Bayes Risk (MBR) decoding (Kumar and Byrne, 2002, 2004; Eikema and Aziz, 2020; Fernandes et al., 2022; Freitag et al., 2022a). These techniques rely on generating multiple candidates to maximize given quality metric at inference time, and research has shown that they consistently yield better results than likelihood-based decoding techniques (Eikema and Aziz, 2020; Koehn and Knowles, 2017; Ott et al., 2018). With the rise of decoder-only LLMs in MT, quality-informed fine-tuning techniques have gained significant attention. Unlike decodingbased methods that inject quality information at inference time, fine-tuning modifies model weights using training sets induced with quality information. These approaches include filtering parallel training data based on quality metric (Alves et al., 2024), distilling gains from more expensive qualityaware methods such as MBR (Finkelstein et al., 2024), or employing preference-based alignment techniques (Rafailov et al., 2024; Xu et al., 2024a), where the model learns preferences induced by quality metrics between candidate translations typically generated by multiple systems. In this work, we focus specifically on the latter. Alignment techniques represent paradigm shift from quality-aware inference time approaches, as they optimize the metric of interest indirectly. Understanding the impact of these approaches on translation quality is thus relevant problem. While some studies have examined qualityinformed decoding techniques and their influence on translation output (Amrhein and Sennrich, 2022), there is still gap in our understanding of how preference-based fine-tuning affects translation quality. In this work, we aim to bridge this gap by examining the properties of preference-based alignment techniques, with particular focus on Contrastive Preference Optimization (CPO) (Xu et al., 2024a), which has been used successfully to achieve very competitive translation performance. Our analysis seeks to describe the effects of preference-based fine-tuning on downstream performance, specifically regarding alignment effectiveness, the interactions between optimized and non-optimized metrics, and the impact of using multiple candidate translation systems for generating preference data. Through extensive experimentation, we find that: Preference-based alignment globally outperforms Supervised Fine-Tuning (SFT) on highquality data in terms of maximizing the alignment metric. However, preference-based alignment is highly sensitive to the choice of candidate systems used for generating preference data, affecting both the alignment metric and downstream metric consistency. Aligning model using its own translations achieves performance comparable to employing multiple external systems, while ensuring better metric consistency and allowing for improved control over the alignment process."
        },
        {
            "title": "2.1 Quality-Informed Translation",
            "content": "Along with human evaluation, lexical metrics like BLEU (Papineni et al., 2002), chrF (Popovic, 2015), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) have long been used for translation evaluation. However, human evaluation is costly, and lexical metrics have been shown to correlate poorly with human judgements. More recently, some neural metrics have emerged as preferred method to mimic human preferences without relying on expensive human evaluation. The intuitive approach involves training an encoder model on human-annotated sourcetranslation-reference triplets. Among the metrics most frequently mentioned in the literature are BLEURT (Yan et al., 2023), COMET (Rei et al., 2020), CometKiwi (Rei et al., 2022b), xCOMET (Guerreiro et al., 2023), and Metric-X (Juraska et al., 2023). They can be divided into two families: reference-based metrics, that include humanwritten gold reference as an input to the scoring model, and reference-free metrics, which only require access to the source sentence and the generated translation. These neural metrics have proven particularly effective at scoring translations and achieve much higher correlation with human judgments than their lexical counterparts (Mathur et al., 2020; Kocmi et al., 2021; Freitag et al., 2022b; Kocmi et al., 2024). These neural metrics have also been leveraged to improve translation models through decoding strategies. The approach involves sampling various candidate translations, scoring them according to given metric, and selecting the one with the highest score. This methodology is exemplified by MBR decoding in the reference-based setting and N-best reranking in the reference-free setting (Fernandes et al., 2022; Freitag et al., 2022a)."
        },
        {
            "title": "2.2 Quality-Based Fine-Tuning",
            "content": "With the recent rise of decoder-only LLMs applied to translation tasks (Zhu et al., 2023; Jiao et al., 2023; Hendy et al., 2023; Kocmi et al., 2023; Freitag et al., 2023; Xu et al., 2023; Alves et al., 2023; Xu et al., 2024a; Alves et al., 2024), and with automatic metrics increasingly reflecting human judgments (Sellam et al., 2020; Rei et al., 2020; Juraska et al., 2023), quality-based fine-tuning has gained considerable traction. This approach shifts the objective from selecting the best candidate translation according to metric at inference time to directly updating model weights through fine-tuning to produce the desired translations. straightforward approach is to perform SFT on high-quality translations, evaluated and then filtered with respect to metric of interest (Alves et al., 2024). Another attractive alternative is Preference Optimization (PO) (Simianer, 2018; Rafailov et al., 2024; Xu et al., 2024a; Yang et al., 2023; Xu et al., 2024b; Wu et al., 2024), which focuses on learning preferences between chosen and rejected translations rather than simply increasing the likelihood of high-quality sentences. popular PO method is Direct Preference Optimization (DPO) (Rafailov et al., 2024), which aims to maximize scaled likelihood gap between chosen and rejected option. More recently, CPO (Xu et al., 2024a) has emerged as promising alternative, incorporating an SFT term into the DPO loss, effectively combining the strengths of both methods. Moreover, by removing the reference policy from the learning objective, it improves training efficiency."
        },
        {
            "title": "3 Experimental Setup",
            "content": "Here, we detail our experimental setup, explaining how we built the preference data, and train and evaluate the models."
        },
        {
            "title": "3.1 Preference Data",
            "content": "Preference datasets. To build preference dataset, one needs candidate translations, an evaluation metric to score these translations, and method to select chosen and rejected hypotheses. We denote candidate dataset by = {(xi, Yi)}N i=1, where xi denotes the source sentence and Yi is set of candidate translations. One can then derive preference dataset, Dpref = {(xi, yr , yc Yi (chosen hypothesis) is translation Yi (rejected hypothesis) accordwhere yc preferred to yr ing to metric and given selection method. i=1, )}N Multi-system approach. In the multi-system scenario, we follow the setting outlined by Xu et al. (2024a). Candidate translations are generated using three different systems, namely ALMA-13BLoRA (the base model we aim to align, referred to as Base) (Xu et al., 2023), GPT-4 (OpenAI, 2023), and the human-written gold reference (referred to as Ref). Formally, for all data samples, (cid:110)"
        },
        {
            "title": "Y multi\ni",
            "content": "= yRef , yBase , yGP -4 (cid:111) . Then, for each sample, the three translations are evaluated with regard to m. The one with highest (resp. lowest) score is selected as the chosen (resp. rejected) hypothesis. Formally, (y) (y) yr = arg min yY multi yc = arg max yY multi Mono-system approach. In the mono-system setting, we solely rely on the base model for candidate generation. For each source sentence, = 50 candidates are top-p-sampled (p = 0.6) with temperature τ = 0.9,2 and are then ranked based on evaluation metric m. For all samples, this results in set of candidates"
        },
        {
            "title": "Y mono\ni",
            "content": "= (cid:8)y1 , , yK (cid:9), yK where y1 are sorted in increasing quality order, with no loss of generality. Preference pairs are then derived to ensure that yr yc holds for all samples. Further details on the construction of mono-system preference datasets are given in Section 5 and Appendix B.1. yBase Source dataset. We rely on the FLORES-200based (Team et al., 2022) dataset used in Xu et al. (2024a) as primary data source. It includes over 20000 translation pairs spanning six languages (English (en), Czech (cs), German (de), Icelandic (is), Russian (ru), and Chinese (zh)) and covering ten language directions, either into-English (xx-en) or out-of-English (en-xx). Alignment metrics. In line with Xu et al. (2024a), we rely on reference-free neural metrics, namely xCOMET-QE-XXL (Guerreiro et al., 2023) (referred to as xCOMET-QE), and the WMT23 version of CometKiwi-XXL (Rei et al., 2023) (denoted by CometKiwi), as well as on referencebased lexical metric, chrF (Popovic, 2015)."
        },
        {
            "title": "3.2 Training",
            "content": "Learning objective. We focus our diagnosis on CPO (Xu et al., 2024a), which combines preference term with likelihood term and achieves state-of-the-art performance in preference-based metric alignment for translation tasks. The empirical loss function is formally expressed as: (cid:88) (cid:20) (cid:18) log σ β log (cid:19)(cid:21) πθ (yc πθ (yr xi) xi) LCP ="
        },
        {
            "title": "1\nN",
            "content": "i=1 + LSF , (cid:80)N i=1 [log πθ (yc where LSF = 1 xi)] is the negative-log-likelihood loss applied to chosen translations, πθ is the model to fine-tune, σ is the sigmoid function and β is hyperparameter. In our experiments, CPO alignment is consistently compared to vanilla SFT on chosen translations.3 Training parameters. We replicate the exact same parameters as the ones outlined by Xu et al. (2024a). ALMA-13B-LoRA is LoRA fine-tuned 2These are the default parameters used in the ALMA pa3All our models are trained using the code implementation per (Xu et al., 2023, 2024a). provided by Xu et al. (2024a). xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "87.80 80.86 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "58.53 Neural xCOMET-QE CometKiwi 91.91 81."
        },
        {
            "title": "SFT\nCPO",
            "content": "89.13 89.95 89.26 89.82 87.61 78.51 Preferences induced with xCOMET-QE 81.49 81.89 59.82 59. 92.38 92.75 Preferences induced with CometKiwi 92.44 92.19 60.01 60.22 81.70 82."
        },
        {
            "title": "Preferences induced with chrF",
            "content": "80.82 75.62 56.97 45.32 92.20 88.89 81.67 83.60 81.93 83.64 81.70 80."
        },
        {
            "title": "Lexical\nchrF",
            "content": "49.49 50.28 47.69 50.49 48.11 50.30 42.50 Table 1: Comparison between SFT on preferred translations and CPO in the multi-system setting, using xCOMETQE, CometKiwi and chrF as alignment metrics. The same 3 metrics are reported for evaluation, separately for into-English (xx-en) and out-of-English (en-xx) translations on the WMT22 dataset. Green shades indicate metric improvements over the base model, while red shades indicate metric decreases. We represent with () scenarios where the preference metric matches the evaluation metric. Values in italic font denote statistically significant differences between SFTand CPO-based alignment at the 5% level, based on one-tailed paired Students t-tests. with rank 16 for one epoch, starting with learning rate of 104, using inverse square root decay and batch size of 128. The β parameter of the CPO objective function is set equal to 0.1, in line with the original DPO paper by Rafailov et al. (2024)."
        },
        {
            "title": "3.3 Evaluation",
            "content": "Inference setup. Following other works on LLMbased translation (Alves et al., 2024; Briakou et al., 2024), all generations at inference time are produced using greedy decoding, as it provides maximum computational efficiency while preserving high output quality.4 Evaluation datasets. We evaluate our approaches on the WMT22 test dataset, which consists of 17471 source-reference pairs and includes the same ten language pairs as the preference data. Evaluations on WMT23 test data are provided in Appendix A. Evaluation metrics. We use the same three metrics used to create the preference datasets: xCOMET-QE, CometKiwi, and chrF. Additional evaluation metrics are reported in Appendix A, specifically the reference-based version of MetricX-Large (referred to as Metric-X) (Juraska et al., 2023), and BLEU (Papineni et al., 2002). 4Inference is performed using the vLLM library (Kwon et al., 2023)."
        },
        {
            "title": "4 Multi-System Preference Fine-Tuning",
            "content": "We begin our analysis by focusing on the multisystem setting (Xu et al., 2024a), in which the chosen and rejected options are derived from pool of three candidate systems consisting of ALMA-13BLoRA (base model), GPT-4, and the gold reference."
        },
        {
            "title": "4.1 Top-Level Analysis",
            "content": "Neural-based alignment improves downstream performance. Table 1 shows that when aligning with neural metrics (xCOMET-QE or CometKiwi), both SFT on preferred translations and CPO consistently improve performance on the alignment metric across language pairs. We also observe that aligning on xCOMET-QE improves results on CometKiwi, and vice-versa. We hypothesize this may be the result of high correlation between different neural metrics, as they are typically trained on similar data. Overall, these results demonstrate that alignment-based techniques can achieve similar objectives to those of quality-aware decoding approaches like MBR, even though the target metric is only indirectly optimized. CPO induces adverse metric effects. In Table 1, we observe that when aligning with neural metrics, CPO yields significantly greater improvements on the alignment metric compared to SFT. The inclusion of the reject option seems to offer additional benefits over the traditional SFT objective xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "87.80 80.86 Preferences induced with xCOMET-QE 81.49 89.13 All systems 89.41 81.56 No Base 81.58 89.32 No Ref 81.15 88.44 No GPT-"
        },
        {
            "title": "Preferences induced with chrF\nAll systems\nNo Ref",
            "content": "87.61 89.21 80.82 81.49 Preferences induced with xCOMET-QE 81.89 89.95 All systems 81.73 89.59 No Base 81.86 89.91 No Ref 81.35 88.81 No GPT-"
        },
        {
            "title": "Preferences induced with chrF\nAll systems\nNo Ref",
            "content": "78.51 89.26 75.62 81.52 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "58.53 Neural xCOMET-QE CometKiwi 91.91 81."
        },
        {
            "title": "Lexical\nchrF",
            "content": "49."
        },
        {
            "title": "Optimization via SFT",
            "content": "59.82 60.26 60.08 58.86 56.97 60."
        },
        {
            "title": "Optimization via CPO",
            "content": "59.83 59.94 60.59 57.91 45.32 60.63 92.38 92.32 92.22 92.33 92.20 91.99 92.75 92.74 92.44 92.22 88.89 90. 81.67 81.65 81.33 81.74 81.70 80.96 83.60 83.13 81.97 83.16 80.99 79.37 50.28 50.52 50.05 50.06 50.30 50. 47.69 48.54 50.67 46.82 42.50 51.11 Table 2: Impact of the systems used for candidate generation on WMT22 performance in the multi-system setting after undergoing SFT and CPO optimization. Values in italic font denote statistically significant differences between all-systems-based alignment and alignment with one system removed, at the 5% significance level, based on one-tailed paired Students t-tests. Evaluation metrics and color codes are the same as in Table 1. in this context. However, aligning with CPO also introduces adverse effects between neural and lexical metrics for out-of-English translations. More specifically, and consistent with the findings of Xu et al. (2024a), aligning on neural metrics negatively impacts lexical metrics. Importantly, this is further evidence to support recommendations provided in (Kocmi et al., 2024): even though, in most cases, neural and lexical MT evaluation metrics should be positively correlated, we should employ caution when using the same metric for evaluation that was used during training/inference. Nevertheless and perhaps more interestingly, it turns out SFT does not produce such effects, raising the question of whether these contradictory evaluation dynamics seen with CPO stem from the learning objective itself or the mix of candidate systems used. Lexical alignment fails to improve downstream performance. Table 1 shows that preferencebased lexical alignment5 behaves differently com5When performing alignment using lexical metric like chrF, the chosen translation is by definition the gold reference as long as it is present in the pool of candidates. The translation with the lowest chrF score among the remaining systems pared to neural alignment. Specifically, SFT results are roughly stagnant, showing slight decrease in chrF for into-English translations and slight increase for out-of-English translations. In contrast, CPO results in steep drop across the metric board for both intoand out-of-English translations. Using the gold reference as the chosen system appears to impair downstream performance, especially when performing alignment using CPO. 4."
        },
        {
            "title": "Impact of the Candidate Systems",
            "content": "We now turn to investigating how much the success of alignment-based fine-tuning depends on the choice of the candidate systems. Unless otherwise specified, we use xCOMET-QE as the alignment metric and examine the performance impact of withdrawing systems from the candidate pool. We perform SFT and CPO on the newly created datasets. We report results in Table 2. The choice of the candidate systems impacts alignment performance. Table 2 shows that for both SFTand CPO-based methods, removing systems from the pool of candidates significantly afis then rejected. xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "87.80 80.86 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "58.53 Neural xCOMET-QE CometKiwi 91.91 81."
        },
        {
            "title": "SFT\nCPO",
            "content": "88.17 87.94 88.04 81.95 89.81 89.69 81.08 81.02 81.06 77.86 81.67 80."
        },
        {
            "title": "Chosen system set to Base",
            "content": "58.91 58.62 91.94 91."
        },
        {
            "title": "Chosen system set to Ref",
            "content": "57.73 48.75 92.35 86.97 Chosen system set to GPT-4 60.53 60.42 91.96 90.50 81.21 81. 81.94 80.01 80.83 78."
        },
        {
            "title": "Lexical\nchrF",
            "content": "49.49 49.35 48.56 50.12 39.81 50.73 50.22 Table 3: Impact of imposing the chosen system on WMT22 downstream performance in the multi-system setting. Values in italic font denote statistically significant differences between SFTand CPO-based alignment at the 5% significance level, based on one-tailed paired Students t-tests. Evaluation metrics and color codes are the same as in Table 1. fects performance on the alignment metric. This is particularly the case for out-of-English translation with CPO optimization. Notably, removing GPT-4 has the strongest negative impact on downstream xCOMET-QE. This is expected as it is the highestquality system among the system candidates (see Table 11 in Appendix B). Some candidate systems can be harmful to preference-based alignment. In Section 4.1, we observed CPO negatively impacts en-xx chrF when aligning on neural metrics, unlike SFT on preferred translations. Table 2 suggests this may stem from including gold references in the candidate system pool: removing them eliminates this adverse effect. We also noted in Section 4.1 that lexical alignment fails to improve downstream chrF, with sharp decreases with CPO. This issue is resolved by removing gold references. Overall, candidate system choice affects alignment effectiveness and downstream metric consistency, with CPO showing higher sensitivity to preference settings than SFT. is selected from the remaining systems (if one has lower xCOMET-QE than the chosen system); otherwise, the sample is discarded. CPO is not robust to the preference setting. In contrast to the observations made in Section 4.1, Table 3 shows that, under this setup, CPO fails to outperform SFT for both xx-en and en-xx translations. When systematically choosing base translations, CPO is unable to surpass the trivial SFT setting where the base model is fine-tuned on its own translations.6 Moreove, downstream CPO performance significantly declines when gold references are chosen, underperforming the non-aligned model across all metrics, even including the alignment metric. These results reinforce the claims made in Section 4.2 and indicate lack of robustness of CPO compared to SFT. In the following section (Section 5), we demonstrate that this instability observed with CPO can be mitigated by using more normalized preference setting, relying only on the base model for candidate generation. 4."
        },
        {
            "title": "Impact of the Chosen System",
            "content": "To complement findings from Section 4.2 and further characterize the sensitivity of preference-based alignment, we propose examining downstream performance when the chosen system is fixed to single system. We create three preference datasets based on xCOMET-QE, in which we either impose the base model, reference or GPT-4 as the chosen system. When applicable, the rejected translation"
        },
        {
            "title": "5 Mono-System Preference Fine-Tuning",
            "content": "So far, we have exclusively focused on multisystem alignment, which involves using external models for candidate generation and preference dataset building. Although this approach is common for metric alignment (Luong and Manning, 6As expected, performing SFT on models own greedy predictions has minimal impact on downstream performance. xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "87.80 80.86 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "58.53 Neural xCOMET-QE CometKiwi 91.91 81.17 Multi-system Mono-system Multi-system Mono-system Mono-system (opt.) 89.13 88.51 89.95 89.35 89."
        },
        {
            "title": "Optimization via SFT",
            "content": "81.49 81.29 59.82 59."
        },
        {
            "title": "Optimization via CPO",
            "content": "81.89 81.80 81.97 59.83 59.52 59.65 92.38 92.17 92.75 92.69 92.87 81.67 81.54 83.60 82.91 83."
        },
        {
            "title": "Lexical\nchrF",
            "content": "49.49 50.28 49.41 47.69 49.02 49.11 Table 4: Comparison between multiand mono-system fine-tuning on WMT22 test data. Alignment is performed on xCOMET-QE for both SFT and CPO. Mono-system (opt.) denotes the model fine-tuned on optimized monosystem preference data. Values in italic font denote statistically significant differences between multi-systemand mono-system-based alignment at the 5% significance level. Evaluation metrics and color codes are the same as in Table 1, based on one-tailed paired Students t-tests. 2015; Sennrich et al., 2016; Xu et al., 2024a), some works have shown that model can be aligned effectively using only its own outputs (Yang et al., 2023; Yuan et al., 2024; Dubey et al., 2024). In this section, we propose to take closer look at this strategy and identify its potential advantages and disadvantages compared to the multi-system approach. We use xCOMET-QE as the alignment metric. To ensure fair comparison, we first generate the mono-system dataset to approximately replicate the properties of the multi-system dataset regarding the alignment metric.7 Details on the construction of mono-system preference datasets are given in Section 3 and Appendix B.1."
        },
        {
            "title": "Alignment",
            "content": "Mono-system alignment improves downstream performance. Table 4 shows that performing SFT and CPO on mono-system dataset using xCOMET-QE for alignment results in improved downstream performance across all neural metrics compared to the base model, as observed in the multi-system scenario (Section 4.1). This finding highlights the effectiveness of alignment techniques even when using only the models own translations for candidate generation, without needing access to high-quality external systems. This is particularly relevant in practical scenarios in which such access may be limited or unavailable. 7The created mono-system dataset has an average rejected/- chosen xCOMET-QE of 87.8/97.3, compared to 87.9/97.2 for the multi-system dataset  (Table 11)  . CPO consistently outperforms SFT on neural metrics. Similar to when relying on multiple systems for candidate generation, we observe in Table 4 that CPO outperforms SFT regarding downstream performance on neural metrics. This finding reinforces the observation made in Section 4.1 and tends to confirm the superiority of the CPO objective over SFT on preferred translations in optimizing neural-based alignment performance. Mono-system alignment slightly underperforms multi-system alignment. Table 4 shows that while mono-system alignment increases downstream performance on neural metrics, the improvement levels are not as high as in the multi-system setting. Despite the monoand multi-system preference datasets being built with the same alignment metric properties, having translations from different distributions, particularly from GPT-4 (cf. Section 4.2 and Table 2), appears to add value for achieving optimized alignment effectiveness. Removing external systems almost eliminates the adverse metric effects observed with CPO. In Section 4.1, we showed that multi-system neural alignment using CPO greatly impacts lexical performance for out-of-English translations. Table 4 demonstrates that mono-system alignment almost completely mitigates these negative effects. While there is still slight decrease in en-xx chrF, it is much smaller compared to the multi-system scenario. This confirms the findings from Sections 4.2 and 4.3 that CPO is sensitive to the preference setting, but also shows that relying solely on candidate translations from the base model limits adverse efFigure 1: Impact of chosen and rejected option quality on downstream performance, using xCOMET-QE for alignment and evaluation. The chart is derived by linearly interpolating results from nine preference datasets (points to I), each with different average rejected and chosen qualities. Test performance on WMT22 (average across all language pairs) is reported in brackets. Example: point (avg. rejected xCOMET-QE: 75.4, avg. chosen: 98.2) achieves 90.9 xCOMET-QE on WMT22 test data. fects on downstream metric consistency. possible explanation is that candidate translations from the same system distribution tend to have similar properties, thereby reducing the likelihood of observing high lexical instability when performing alignment based on neural metric like xCOMET-QE. The mono-system approach offers better control over the alignment process. Specifically, monosystem alignment provides more fine-grained control over the respective qualities of the chosen and rejected options. This setting allows for tuning these qualities to maximize post-alignment performance, which is not possible when using limited number of external systems. This aspect is further explored in the following section (Section 5.2)."
        },
        {
            "title": "5.2 Optimizing the Preference Data",
            "content": "In this final experiment, we examine how the quality of chosen and rejected options affects downstream performance. We build nine preference datasets, each with varying average xCOMET-QE scores for chosen and rejected options. The hypotheses average qualities are categorized into three groups: High, Mid, and Low. As detailed in Section 3.1, the quality of the chosen (resp. rejected) option is always ensured to be above (resp. below) the quality of the base translation. The statistics of the created datasets are summarized in Appendix B.1  (Table 11)  . The respective qualities of the rejected and chosen options have significant impact on postCPO performance. Figure 1 highlights the need to closely monitor the qualities of chosen and rejected options to fully leverage the mono-system approach. Specifically, several properties of preference data were found to negatively impact postCPO performance: (i) chosen option of too low quality, (ii) an extremely low or high quality of the rejected option, and (iii) too wide gap between the qualities of the rejected and chosen options. Optimizing preference data yields competitive performance to multi-system setting. Figure 1 shows that for effective metric alignment with CPO, the rejected options quality should be moderate (neither too high nor too low), while the chosen options quality should be as high as possible. Specifically, optimal test performance was obtained with rejected options average around 90% ( = 10%) of the base models quality, and chosen options averaging around 105% ( = +5%). Under this scenario, we show that performance levels can match those in the multi-system setting while maintaining consistency with lexical scores  (Table 4)  . However, these results also highlight the complexity of achieving optimal preference-based alignment and get the most of the reject option."
        },
        {
            "title": "6 Conclusion",
            "content": "Our experiments revealed several key findings. Firstly, we showed that preference-based alignment, specifically using CPO, globally outperforms SFT on high-quality data in terms of improving neural evaluation metrics. However, we identified significant drawbacks when relying on multiple systems for preference data generation, revealing adverse effects between neural and lexical metrics, and highlighting lack of robustness in preferencebased alignment compared to the SFT approach. Finally, we showed that using candidate translations all originating from the same system distribution, specifically the base model, can be an effective strategy for gaining more control over preferencebased fine-tuning. This approach achieves performance comparable to using multiple external systems while ensuring better consistency across evaluation metrics. In nutshell, while preferencebased alignment techniques hold promise for improving MT quality, careful consideration must be given to the choice of candidate translations, the learning objective, and the potential trade-offs regarding downstream metric consistency."
        },
        {
            "title": "Limitations",
            "content": "In this work, we conducted extensive experiments to assess the impact of preference-based fine-tuning on downstream translation quality. For efficiency and practicality, we focused on the experimental setup detailed by Xu et al. (2024a), which utilizes three systems for candidate generation. Similarly, we used the same evaluation metrics and datasets. Future experiments could benefit from validating our findings using different model families, broader range of alignment and evaluation metrics, and additional translation datasets, for instance including other languages. Additionally, in the mono-system setting, we explored the impact of varying the qualities of chosen and rejected options and derived general insights on optimizing preference data. Further research could involve using different datasets, models, and alignment metrics to characterize more precisely the factors that influence downstream performance in this specific scenario. This approach could lead to deeper mathematical understanding of the elements that affect performance in preference-based fine-tuning, resulting in more robust and scalable optimization techniques. Finally, our evaluation relied on automatic metrics, both lexical and neural, with the latter closely approximating human judgments but still being unable to fully replace them. Given their imperfect correlation with human preferences, future work could benefit from additional human evaluation of outputs obtained via the approaches we studied to get an even deeper understanding of post-alignment downstream performance dynamics."
        },
        {
            "title": "Ethics Statement",
            "content": "Our work aims to investigate the mechanisms of model alignment to enhance transparency in the field of automatic translation. We believe this effort improves the interpretability of model outputs, which is beneficial for ethical considerations. Additionally, our analysis is distinctly multilingual, with an emphasis on low-resource languages, contributing to expanding the scope of MT. We have identified no potential negative societal impacts from our work."
        },
        {
            "title": "Acknowledgements",
            "content": "Training compute was provided by the Jean Zay supercomputer, operated by GENCI IDRIS, through compute grants 2023-AD011014668R1 and AD010614770, as well as by the Adastra supercomputer through projects c1615122, cad15031, and cad14770. Part of this work was also supported by the EUs Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the DECOLLAGE project (ERC-2022-CoG 101088763), and by the Portuguese Recovery and Resilience Plan through project C64500888200000055 (Center for Responsible AI)."
        },
        {
            "title": "References",
            "content": "Duarte Alves, Nuno Guerreiro, João Alves, José Pombal, Ricardo Rei, José GC de Souza, Pierre Colombo, and André FT Martins. 2023. Steering large language models for machine translation with finetuning and in-context learning. arXiv preprint arXiv:2310.13448. Duarte Alves, José Pombal, Nuno Guerreiro, Pedro Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, et al. 2024. Tower: An open multilingual large language model for translation-related tasks. arXiv preprint arXiv:2402.17733. Chantal Amrhein and Rico Sennrich. 2022. Identifying weaknesses in machine translation metrics through minimum bayes risk decoding: case study for comet. arXiv preprint arXiv:2202.05148. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Eleftheria Briakou, Jiaming Luo, Colin Cherry, and Markus Freitag. 2024. Translating step-by-step: Decomposing the translation process for improved translation quality of long-form texts. arXiv preprint arXiv:2409.06790. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vítor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. 2024. The llama 3 herd of models. Bryan Eikema and Wilker Aziz. 2020. Is MAP decoding all you need? the inadequacy of the mode in neural machine translation. In Proceedings of the 28th International Conference on Computational Linguistics, pages 45064520, Barcelona, Spain (Online). International Committee on Computational Linguistics. Patrick Fernandes, António Farinhas, Ricardo Rei, José G. C. de Souza, Perez Ogayo, Graham Neubig, and Andre Martins. 2022. Quality-aware decoding for neural machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 13961412, Seattle, United States. Association for Computational Linguistics. Mara Finkelstein, Subhajit Naskar, Mehdi Mirzazadeh, Apurva Shah, and Markus Freitag. 2024. Mbr and qe finetuning: Training-time distillation of the best and most expensive decoding methods. Markus Freitag, David Grangier, Qijun Tan, and Bowen Liang. 2022a. High quality rather than high model probability: Minimum Bayes risk decoding with neural metrics. Transactions of the Association for Computational Linguistics, 10:811825. Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, pages 578628, Singapore. Association for Computational Linguistics. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022b. Results of WMT22 metrics shared task: Stop using BLEU neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 4668, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Nuno Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT Martins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error detection. arXiv preprint arXiv:2310.10482. Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? comprehensive evaluation. arXiv preprint arXiv:2302.09210. Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is chatgpt good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745. Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. 2023. MetricX-23: The Google submission to the In Proceedings WMT 2023 metrics shared task. of the Eighth Conference on Machine Translation, pages 756767, Singapore. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic, and Mariya Shmatova. 2023. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Proceedings of the Eighth Conference on Machine Translation, pages 142, Singapore. Association for Computational Linguistics. Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478494, Online. Association for Computational Linguistics. Tom Kocmi, Vilém Zouhar, Christian Federmann, and Matt Post. 2024. Navigating the metrics maze: Reconciling score magnitudes and accuracies. arXiv preprint arXiv:2401.06760. Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 2839, Vancouver. Association for Computational Linguistics. Shankar Kumar and Bill Byrne. 2002. Minimum bayesrisk word alignments of bilingual texts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), pages 140147. Shankar Kumar and William Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004, pages 169176, Boston, Massachusetts, USA. Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Minh-Thang Luong and Christopher Manning. 2015. Stanford neural machine translation systems for spoken language domains. In Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign, pages 7679, Da Nang, Vietnam. Nitika Mathur, Timothy Baldwin, and Trevor Cohn. 2020. Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49844997, Online. Association for Computational Linguistics. OpenAI. 2023. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5). Myle Ott, Michael Auli, David Grangier, and MarcAurelio Ranzato. 2018. Analyzing uncertainty in neural machine translation. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 39563965. PMLR. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Maja Popovic. 2015. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the tenth workshop on statistical machine translation, pages 392395. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ricardo Rei, Nuno M. Guerreiro, JosÃ Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, José G. C. de Souza, and André Martins. 2023. Scaling up CometKiwi: Unbabel-IST 2023 submission for the quality estimation shared task. In Proceedings of the Eighth Conference on Machine Translation, pages 841848, Singapore. Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. pages 54285443, Toronto, Canada. Association for Computational Linguistics. Guangyu Yang, Jinghong Chen, Weizhe Lin, and Bill Byrne. 2023. Direct preference optimization for neural machine translation with minimum bayes risk decoding. arXiv preprint arXiv:2311.08380. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024. Self-rewarding language models. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023. Multilingual machine translation with large language models: Empirical results and analysis. arXiv preprint arXiv:2304.04675. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78817892, Online. Association for Computational Linguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8696, Berlin, Germany. Association for Computational Linguistics. Patrick Simianer. 2018. Preference Learning for Machine Translation. Ph.D. thesis. NLLB Team, Marta Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, et al. 2022. No language left behind: Scaling human-centered machine translation (2022). URL https://arxiv. org/abs/2207.04672. Qiyu Wu, Masaaki Nagata, Zhongtao Miao, and Yoshimasa Tsuruoka. 2024. Word alignment as prefarXiv preprint erence for machine translation. arXiv:2405.09223. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2023. paradigm shift in machine translation: Boosting translation performance of large language models. arXiv preprint arXiv:2309.11674. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024a. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417. Nuo Xu, Jun Zhao, Can Zu, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024b. Advancing translation preference modeling with rlhf: step towards costeffective solution. arXiv preprint arXiv:2402.11525. Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, and Mingxuan Wang. 2023. BLEURT has universal translations: An analysis of automatic metrics by minimum risk training. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),"
        },
        {
            "title": "A Additional Results",
            "content": "In this section, we present results on WMT23 test data. The findings in Tables 5, 6, 7 and 8 support the observations discussed in the main text for the WMT22 dataset. In Tables 9 and 10, we also provide additional insights, split by language pairs, and include extra metrics, specifically Metric-X and BLEU. xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "88.00 77.74 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "52.30 Neural xCOMET-QE CometKiwi 86.19 73.08 Preferences induced with xCOMET-QE 88.96 89. 78.46 78.95 53.30 53.47 87.07 88.09 73.99 76.75 89.03 89.58 Preferences induced with CometKiwi 87.11 87. 53.53 53.97 78.57 79.16 74.21 76."
        },
        {
            "title": "Preferences induced with chrF",
            "content": "87.91 81.79 77.62 72.38 51.20 41.46 86.95 83.21 73.96 74."
        },
        {
            "title": "Lexical\nchrF",
            "content": "47.31 48.38 44.29 48.45 44.48 48.14 37.96 Table 5: Comparison between SFT on preferred translations and CPO in the multi-system setting on WMT23 test data. Notations and formatting are the same as in Table 1. xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "88.00 77.74 Preferences induced with xCOMET-QE 78.46 88.96 All systems 78.53 89.07 No Base 78.47 89.05 No-Ref 78.02 88.29 No GPT-"
        },
        {
            "title": "Preferences induced with chrF\nAll systems\nNo Ref",
            "content": "87.91 88.89 77.62 78.47 Preferences induced with xCOMET-QE 89.77 78.95 All systems 78.54 89.52 No Base 79.26 89.57 No Ref 78.46 89.16 No GPT-"
        },
        {
            "title": "Preferences induced with chrF\nAll systems\nNo Ref",
            "content": "81.79 88.79 72.38 78.73 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "52.30 Neural xCOMET-QE CometKiwi 86.19 73."
        },
        {
            "title": "Lexical\nchrF",
            "content": "47."
        },
        {
            "title": "Optimization via SFT",
            "content": "53.30 53.57 53.39 52.62 51.20 53."
        },
        {
            "title": "Optimization via CPO",
            "content": "53.47 53.44 54.18 51.94 41.46 54.21 87.07 86.94 87.04 87.04 86.95 86.65 88.09 87.66 87.41 87.45 83.21 85. 73.99 73.70 73.60 74.08 73.96 73.02 76.75 75.84 74.46 76.62 74.76 71.82 48.38 48.52 48.65 48.03 48.14 49. 44.29 45.27 48.88 43.30 37.96 49.59 Table 6: Impact of candidate systems on WMT23 downstream performance in the multi-system setting. Notations and formatting are the same as in Table 2. xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "88.00 77.74 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "52.30 Neural xCOMET-QE CometKiwi 86.19 73."
        },
        {
            "title": "SFT\nCPO",
            "content": "88.07 88.05 88.33 84.06 77.93 77.95 77.92 74.22 52.52 52.24 86.52 86. 73.27 73."
        },
        {
            "title": "Chosen system set to Ref",
            "content": "51.75 44.53 87.29 81.01 74.57 73.55 Chosen system set to GPT-4 89.57 88.99 79.06 78. 54.08 53.95 86.70 85.14 73.18 71."
        },
        {
            "title": "Lexical\nchrF",
            "content": "47.31 47.52 46.54 47.86 34.64 49.23 48.68 Table 7: Impact of the chosen system on WMT23 downstream performance in the multi-system setting. Notations and formatting are the same as in Table 3. xx-en Neural xCOMET-QE CometKiwi"
        },
        {
            "title": "Base",
            "content": "88.00 77.74 en-xx"
        },
        {
            "title": "Lexical\nchrF",
            "content": "52.30 Neural xCOMET-QE CometKiwi 86.19 73.08 Multi-system Mono-system 88.96 88. 78.46 78.17 53.30 52."
        },
        {
            "title": "Optimization via SFT",
            "content": "Multi-system Mono-system Mono-system (opt.) 89.77 89.33 89.36 78.95 78.78 78.92 53.47 53.17 53."
        },
        {
            "title": "Optimization via CPO",
            "content": "87.07 86.75 88.09 87.94 88.50 73.99 73.87 76.75 76.01 76."
        },
        {
            "title": "Lexical\nchrF",
            "content": "47.31 48.38 47.43 44.29 46.65 46.48 Table 8: Comparison between multiand mono-system fine-tuning on WMT23 test data. Notations and formatting are the same as in Table 4. cs-en en-cs Neural xCOMET-QE CometKiwi Metric-X Lexical chrF BLEU Neural xCOMET-QE CometKiwi Metric-X Lexical chrF BLEU xCOMET-QE de-en Neural CometKiwi Metric-X Lexical chrF BLEU xCOMET-QE Neural CometKiwi Metric-X Lexical chrF BLEU en-de 83.42 82.57 2.00 65. 41.25 90.80 81.96 1.48 53.58 27. 93.33 83.48 2.03 55.24 29.02 96. 80.87 1.22 56.95 27.65 86.18 86.58 86.43 85.23 84.64 83.78 86.97 86. 83.63 86.15 83.17 83.01 83.20 83.03 82.87 82.83 82.69 83.31 82.85 82.91 1.98 1.99 1.97 2.02 2.03 2.04 2.00 1. 2.06 1.99 67.36 67.79 67.50 66.55 66.39 65.74 67.62 42.81 42.99 42.93 42.21 41.99 41.62 41.99 67.50 43.08 65.22 67. 40.77 43.23 91.49 91.61 91.28 91.53 90.77 91.36 91.14 91.57 91.42 91.29 82.69 82.63 82.10 82.78 81.69 82.65 81.33 83. 82.45 81.85 1.42 1.42 1.45 1.42 1.50 1.40 1.48 1.40 1.41 1.48 55.02 55.51 54.94 54.52 53.52 54.35 56.27 28.58 28.84 28.75 28.07 27.67 27.73 28. 55.29 28.69 54.76 55.83 28.11 29.24 93.88 93.88 93.88 93.41 93.55 93.35 94.22 94. 92.86 93.91 83.85 83.88 83.96 83.59 83.61 83.69 83.95 84.07 83.53 83.85 2.00 2.00 1.99 2.02 2.03 2.02 1.98 1. 2.03 2.00 56.32 56.62 56.63 55.41 55.82 54.67 57.02 29.86 30.08 30.06 29.10 29.48 28.48 30.10 56.40 29.79 54.26 56. 28.29 30.04 96.57 96.63 96.41 96.49 96.43 96.67 96.29 96.58 96.60 96.32 81.21 81.66 81.10 81.22 81.14 81.60 80.92 81. 81.63 80.74 1.19 1.19 1.20 1.20 1.22 1.19 1.21 1.19 1.18 1.22 57.62 57.83 57.49 57.38 56.97 57.32 58.05 28.22 28.15 28.08 28.14 27.80 27.99 28. 57.77 28.13 57.50 57.97 28.06 28.21 85.00 83. 1.99 66.38 41.81 91.04 82.23 1. 53.27 27.42 93.71 83.80 2.00 55. 29.27 96.46 81.16 1.21 57.08 27. 87.40 87.59 86.68 84.95 84.10 71.82 87.70 86.74 66.31 86.65 85.99 86.51 83.58 83.18 83.22 83.22 82.69 79.56 81.77 83. 76.41 82.99 83.66 83.84 1.94 1.95 1.96 1.99 2.05 2.22 2.07 1.95 2.53 1.98 1.93 1. 67.52 67.90 67.46 65.99 65.94 54.77 66.72 42.54 41.86 42.84 41.31 41.77 28.59 39.42 67.67 42.42 51.03 68.33 25.35 43. 67.10 67.08 42.09 41.73 90.86 91.60 90.94 90.45 90.42 79.03 89.73 90.21 83.47 89.83 91.40 91. 84.58 82.63 84.14 83.91 81.80 80.03 78.83 84.81 80.25 79.38 83.99 84.28 1.40 1.41 1.38 1.39 1.51 1.59 1.57 1. 1.72 1.59 1.39 1.37 is-en en-is Neural xCOMET-QE CometKiwi Metric-X Lexical chrF BLEU Neural xCOMET-QE CometKiwi Metric-X 50.58 56.16 51.48 49.32 52.29 40.19 56.21 23.44 28.97 24.91 22.04 26.71 14.25 27.28 51. 23.53 43.35 57.08 15.80 28.51 52.46 52.40 26.30 25.92 Lexical 94.22 94.28 93.97 93.65 93.41 89.61 94.17 94.20 87.27 93.94 94.01 94.26 chrF BLEU xCOMET-QE 84.10 84.07 84.06 83.88 83.48 81.47 83.06 84.22 79.30 83.67 84.14 84.32 1.94 1.98 1.96 1.96 2.04 2.08 2. 1.93 2.29 2.02 1.95 1.94 ru-en Neural CometKiwi Metric-X 56.21 56.98 56.32 54.95 55.53 47.32 57. 29.44 30.02 29.80 28.36 29.34 21.40 29.47 56.45 29.61 44.28 56.68 19.34 29.76 55.97 56. 29.18 29.26 Lexical 97.34 96.57 97.16 97.31 96.37 96.57 95.68 97.19 96.87 95.84 96.89 97. chrF BLEU xCOMET-QE 83.38 82.02 82.67 83.27 80.82 79.91 79.61 83.71 81.49 80. 82.29 82.78 1.12 1.18 1.12 1.12 1.23 1.12 1.27 1.13 1.14 1.27 1.15 1.12 55.95 57.86 56.53 55.30 56.17 48.62 57. 26.20 27.66 27.20 25.17 27.10 18.46 26.23 56.24 25.41 51.01 58.68 19.73 27.66 56.53 56. 27.25 27.09 en-ru Neural CometKiwi Metric-X Lexical chrF BLEU 76.22 85.36 1.89 59.72 35.34 89. 80.68 2.40 53.31 23.49 89.68 80. 1.82 62.72 35.29 92.77 82.62 2. 51.93 25.86 79.24 80.12 79.67 77.64 76.85 76.86 81.03 79.38 76.29 79.56 85.88 86.09 85.94 85.63 85.60 85.52 86. 86.03 85.36 85.96 1.82 1.84 1.83 1.87 1.87 1.87 1.84 1.81 1.89 1.83 62.05 62.77 62.52 60.33 59.94 59.88 63. 37.49 38.11 37.97 35.79 35.60 35.41 37.61 62.26 37.69 59.07 62.19 34.61 37.70 89.18 88.65 88.74 89.43 88.67 89.44 88. 89.06 88.99 88.34 80.63 80.19 80.10 81.05 80.35 81.24 79.01 80.95 80.46 79.65 2.38 2.45 2.47 2.34 2.42 2.30 2. 2.37 2.35 2.54 53.14 53.23 52.93 53.38 52.58 53.45 53.17 22.90 22.95 22.84 23.27 22.53 23.60 22.44 53.42 23. 53.09 53.22 23.20 22.88 90.59 90.85 90.82 90.04 89.87 89.39 91.11 90.78 88.63 90.79 81.35 81.40 81.50 81.15 80.99 80.92 81. 81.52 80.51 81.40 1.77 1.77 1.77 1.80 1.82 1.80 1.77 1.75 1.84 1.77 64.18 64.60 64.47 63.20 63.08 61.40 64. 36.97 37.20 37.24 36.19 36.01 34.43 37.39 64.31 37.09 60.40 64.56 33.44 37.30 93.18 93.02 92.90 93.13 92.87 93.29 92. 93.20 93.11 92.67 83.18 83.10 82.73 83.05 82.64 83.43 82.44 83.30 83.31 82.50 1.96 1.97 2.04 1.95 2.05 1.92 2. 1.96 1.93 2.06 53.02 53.26 52.57 52.82 52.13 52.74 53.53 26.79 26.91 26.29 26.65 26.22 26.54 26.96 53.22 26. 52.80 53.29 26.77 26.90 77.44 85.77 1.85 60. 35.69 89.11 80.46 2.38 52.95 23. 90.25 81.27 1.78 63.52 36.22 93. 83.07 2.00 52.13 26.40 80.73 81.03 79.90 77.75 75.89 62.78 81.02 80. 57.08 80.13 79.33 79.16 86.12 85.91 86.07 85.63 85.47 82.24 85.15 86.17 80.43 85.77 86.05 86. 1.81 1.84 1.80 1.84 1.93 2.09 1.92 1.79 2.40 1.85 1.80 1.78 63.01 63.29 62.63 60.46 59.27 51.44 62.85 38.10 37.90 37.81 35.73 35.04 25.64 36. 63.15 37.98 47.93 62.87 22.21 37.63 61.78 62.25 37.42 37. 89.28 89.15 89.67 88.70 88.83 76.02 86.51 88.15 81.59 86.51 89.75 89.81 82.70 80.18 82.02 82.37 80.38 77.65 76.73 82. 78.80 77.57 82.17 82.81 2.08 2.47 2.10 2.08 2.37 2.46 2.88 2.30 2.34 2.87 2.14 2. zh-en en-zh Neural xCOMET-QE CometKiwi Metric-X Lexical chrF BLEU Neural xCOMET-QE CometKiwi Metric-X 51.70 53.47 51.95 51.02 52.21 43.95 52.54 21.29 22.46 22.34 20.54 22.51 13.38 20.88 52.23 21.16 46.13 53. 14.70 22.12 52.71 52.79 22.69 22.60 Lexical 91.14 91.11 91.06 90.04 89.76 83.04 90.95 91. 79.00 90.89 90.78 90.96 chrF BLEU xCOMET-QE 81.57 81.61 81.57 81.07 81.01 77.41 80. 81.80 75.29 81.43 81.44 81.60 1.72 1.77 1.73 1.78 1.81 2.01 1.83 1.72 2.25 1. 1.75 1.72 xx-en Neural CometKiwi Metric-X 63.88 64.66 64.20 61.58 62.86 51.27 64.68 35.78 36.55 36.49 33.91 35.46 23.21 36.06 64. 36.14 47.38 64.57 20.28 36.80 63.50 63.69 35.72 35.57 Lexical 94.52 92.89 94.30 94.44 92.57 93.39 91.13 93.99 93.52 91.54 93.79 93.95 chrF BLEU xCOMET-QE 85.51 83.40 84.85 85.41 82.11 82.63 80.75 85.44 83.83 81.04 83.99 84.77 1.71 1.95 1.81 1.70 2.13 1.73 2. 1.77 1.82 2.18 1.87 1.84 50.60 53.27 51.28 49.69 51.33 42.24 53.43 23.97 26.27 25.44 23.09 25.64 15.76 25.58 51. 23.54 45.51 53.86 17.75 26.18 52.03 52.18 26.09 26.00 en-xx Neural CometKiwi Metric-X Lexical chrF BLEU 89.49 74. 4.10 51.25 21.69 89.10 79.48 2. 33.62 34.52 87.80 80.86 2.42 58. 31.78 91.91 81.17 1.83 49.49 28. 90.09 90.29 90.25 89.68 89.41 90.21 90.60 90.25 90.06 90.06 75.50 75.71 75.55 74.71 74.70 74.68 76.38 75.82 74.28 75. 3.98 3.96 3.94 4.11 4.14 4.05 3.87 3.94 4.10 4.01 51.82 52.30 51.98 51.11 51.38 49.70 52.71 21.72 21.97 21.80 21.42 21.65 20.34 22.06 52. 21.98 48.66 52.39 19.82 22.13 89.84 89.80 89.99 89.57 89.30 89.52 89.74 90.09 89.26 89. 80.12 79.92 79.98 80.24 79.79 80.41 79.50 80.45 80.01 79.38 2.23 2.27 2.24 2.23 2.26 2.23 2.30 2.22 2.26 2. 34.07 34.14 33.79 33.89 33.20 34.42 33.86 35.01 35.09 34.86 34.86 34.16 35.20 34.78 34.23 35.16 34.75 33.89 35.52 34. 89.13 89.41 89.32 88.44 88.17 88.04 89.81 89.26 87.61 89.21 81.49 81.56 81.58 81.15 81.08 81.06 81.67 81.70 80.82 81. 2.37 2.37 2.35 2.42 2.43 2.41 2.34 2.34 2.44 2.38 59.82 60.26 60.08 58.86 58.91 57.73 60.53 32.92 33.19 33.13 32.17 32.21 31.20 33.02 60. 33.06 56.97 60.17 30.56 33.23 92.38 92.32 92.22 92.33 91.94 92.35 91.96 92.44 92.20 91. 81.67 81.65 81.33 81.74 81.21 81.94 80.83 81.93 81.70 80.96 1.77 1.79 1.81 1.77 1.83 1.75 1.85 1.77 1.77 1. 50.28 50.52 50.05 50.06 49.35 50.12 50.73 28.91 29.00 28.77 28.76 28.26 28.73 28.89 50.49 29.03 50.30 50.57 28.91 29. 89.74 74.84 4.04 51.39 21.49 89. 80.24 2.24 33.45 34.26 88.51 81. 2.39 59.05 32.15 92.17 81.54 1. 49.41 28.37 91.03 90.50 90.80 91.27 89.59 90.71 89.77 91.03 89.56 89.43 90.82 91. 76.32 76.60 75.98 75.23 74.75 70.87 76.05 76.69 68.89 75.91 76.01 76.28 3.69 3.75 3.79 3.79 4.11 4.05 3.92 3. 4.50 3.97 3.77 3.68 51.65 52.96 51.96 49.49 51.34 41.49 53.10 21.09 21.86 21.67 19.91 21.65 14.40 21.12 52.68 21. 38.41 53.42 12.48 22.37 51.92 51.90 21.44 21.38 89.99 90.32 90.09 88.41 89.06 84.25 87.44 89. 85.29 88.20 90.14 90.44 81.38 80.72 81.40 80.44 79.83 78.61 77.07 81.27 79.47 77.93 81.75 82. 2.13 2.24 2.09 2.26 2.27 2.40 2.58 2.23 2.43 2.52 2.15 2.07 31.67 34.00 33.20 30.92 32.65 26.14 32.40 31.03 34.28 33.49 29.77 33.47 24.09 31. 31.90 30.95 28.37 33.71 26.33 33.97 33.22 33.32 33.80 33. 89.95 89.91 89.59 88.81 87.94 81.95 89.69 89.82 78.51 89.26 89.35 89.58 81.89 81.86 81.73 81.35 81.02 77.86 80.99 82. 75.62 81.52 81.80 81.97 2.27 2.31 2.30 2.32 2.44 2.53 2.40 2.24 2.83 2.38 2.29 2. 59.83 60.59 59.94 57.91 58.62 48.75 60.42 32.41 32.77 32.82 30.94 31.94 22.02 31.72 60.22 32.58 45.32 60.63 19.41 33. 59.52 59.65 32.26 32.16 92.75 92.44 92.74 92.22 91.75 86.97 90.50 92.19 88.89 90.83 92.69 92. 83.60 81.97 83.13 83.16 81.06 80.01 78.81 83.64 80.99 79.37 82.91 83.47 1.64 1.78 1.65 1.67 1.85 1.79 2.02 1. 1.84 2.00 1.69 1.66 47.69 50.67 48.54 46.82 48.56 39.81 50.22 25.63 28.55 27.17 24.53 27.60 17.62 27.00 48.11 25. 42.50 51.11 19.33 28.32 49.02 49.11 27.74 27.57 Base SFT Multi-system xCOMET-QE Vanilla No Base No Ref No GPT-4 Chosen = Base Chosen = Ref Chosen = GPT-4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla CPO Multi-system xCOMET-QE Vanilla No Ref No Base No GPT-4 Chosen = Base Chosen = Ref Chosen = GPT-4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla Optimized Base SFT Multi-system xCOMET-QE Vanilla No Base No Ref No GPT-4 Chosen = Base Chosen = Ref Chosen = GPT-4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla CPO Multi-system xCOMET-QE Vanilla No Ref No Base No GPT-4 Chosen = Base Chosen = Ref Chosen = GPT-4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla Optimized Base SFT Multi-system xCOMET-QE Vanilla No Base No Ref No GPT-4 Chosen = Base Chosen = Ref Chosen = GPT-4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla CPO Multi-system xCOMET-QE Vanilla No Ref No Base No GPT-4 Chosen = Base Chosen = Ref Chosen = GPT-4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla Optimized Table 9: Comprehensive downstream evaluation for the WMT22 dataset, reporting xCOMET-QE, CometKiwi, Metric-X, chrF, and BLEU scores for all models and language pairs. Learning objectives are indicated in bold font, candidate settings in italics, and alignment metrics are preceded by an arrow (). en-cs de-en en-de Neural xCOMET-QE CometKiwi Metric-X"
        },
        {
            "title": "Lexical",
            "content": "chrF"
        },
        {
            "title": "BLEU",
            "content": "Neural xCOMET-QE CometKiwi Metric-X"
        },
        {
            "title": "Lexical",
            "content": "chrF"
        },
        {
            "title": "BLEU",
            "content": "xCOMET-QE Neural CometKiwi Metric-X"
        },
        {
            "title": "Lexical",
            "content": "chrF"
        },
        {
            "title": "BLEU",
            "content": "85.90 73.23 1.91 52.57 27.45 84. 76.57 3.73 66.64 39.38 84.97 71. 3.04 61.69 32.78 87.19 86.89 87.44 87.42 86.89 87.68 86.89 87.19 87.15 87. 74.32 73.60 74.13 74.57 74.05 74.94 73.06 74.65 74.18 73.40 1.78 1.82 1.83 1.81 1.93 1.81 1.89 1.77 1.85 1. 54.31 54.46 54.43 53.84 52.71 53.56 55.36 28.71 28.83 29.20 28.62 27.72 28.18 29.20 54.27 28.71 53.91 54.96 28.57 29. 85.29 85.49 85.46 84.76 84.86 85.08 85.94 85.42 84.93 85.19 77.01 77.19 77.19 76.80 76.88 76.78 77.70 77.16 76.61 77. 3.59 3.58 3.64 3.74 3.71 3.62 3.52 3.58 3.69 3.66 67.82 68.17 67.90 67.05 67.28 66.26 68.55 40.50 40.88 40.46 39.47 39.59 38.70 41.07 68. 40.54 65.86 67.93 38.47 40.42 85.75 85.41 85.78 85.67 85.24 86.14 85.25 85.90 85.68 85. 72.56 72.40 72.71 72.60 72.09 72.93 72.07 72.93 72.47 72.56 2.91 2.88 2.91 2.90 2.99 2.86 2.92 2.83 2.90 2. 61.75 61.81 62.05 61.59 61.81 61.36 62.49 32.81 32.57 33.15 32.63 32.98 32.16 33.35 62.09 32.99 61.50 62.46 32.37 33. 86.89 74.78 1.86 52.57 27.60 85. 77.03 3.64 67.64 39.86 85.54 72. 3.03 61.82 33.10 86.53 87.48 86.45 86.27 86.71 72.09 85.67 85.70 77.49 85. 87.92 88.39 76.83 74.40 76.06 76.71 74.16 72.38 70.85 76.97 73.98 71.61 76.62 77.36 1.65 1.77 1.71 1.64 1.92 1.94 2. 1.73 2.02 2.09 1.76 1.71 49.84 55.51 50.44 49.10 51.74 37.30 55.59 23.64 28.99 25.26 23.01 26.79 12.61 27.58 50. 23.74 41.47 56.21 15.17 28.93 52.03 52.11 26.54 26.38 86.18 86.00 85.80 85.96 84.95 82.25 85. 85.76 79.30 85.13 85.72 86.00 77.55 77.80 77.06 77.19 76.91 74.74 77.18 77.66 72.47 77. 77.14 77.76 3.45 3.48 3.54 3.57 3.74 4.09 3.53 3.47 4.52 3.57 3.53 3.55 ru-en en-ru Neural xCOMET-QE CometKiwi Metric-X"
        },
        {
            "title": "Lexical",
            "content": "chrF"
        },
        {
            "title": "BLEU",
            "content": "Neural xCOMET-QE CometKiwi Metric-X 67.15 68.30 67.32 65.40 66.64 55.55 68.10 39.97 40.60 40.00 37.11 38.79 24.83 39.75 67.43 40.07 51.67 68. 21.14 40.49 67.22 67.05 39.30 38."
        },
        {
            "title": "Lexical",
            "content": "87.61 85.82 87.23 87.39 85.67 86.62 83.74 87.52 86.43 84.20 87.09 87.32 chrF"
        },
        {
            "title": "BLEU",
            "content": "xCOMET-QE 73.39 73.08 73.00 72.99 72.46 69.62 71.58 74.21 71.61 71.56 74.07 74.64 2.57 2.79 2.71 2.53 2.98 2.92 2. 2.57 2.65 3.12 2.70 2.57 58.32 62.73 59.44 57.36 61.20 49.04 63.11 27.66 32.81 29.86 26.45 32.37 17.37 32.22 59. 27.90 52.57 63.31 20.35 33.63 61.01 60.71 31.70 31.07 zh-en Neural CometKiwi Metric-X"
        },
        {
            "title": "Lexical",
            "content": "chrF"
        },
        {
            "title": "BLEU",
            "content": "86.22 80.04 2.56 55.59 28.45 89. 74.89 2.63 49.90 23.99 90.45 76. 3.68 45.44 18.79 87.66 87.96 87.79 86.62 86.39 86.03 88.30 87.70 85.27 87. 80.62 80.55 80.64 80.13 80.10 79.94 80.79 80.75 79.62 80.64 2.53 2.55 2.54 2.56 2.57 2.56 2.57 2.52 2.58 2. 56.62 56.80 56.64 55.99 55.72 55.01 57.15 29.52 29.50 29.34 29.03 28.79 28.26 29.47 56.77 29.68 54.54 56.67 28.04 29. 89.48 89.53 89.23 89.58 88.94 89.83 88.86 89.56 89.65 88.88 75.83 75.86 75.43 75.85 74.68 76.40 75.45 75.87 75.96 75. 2.50 2.52 2.58 2.47 2.68 2.45 2.60 2.48 2.48 2.61 50.74 50.72 50.88 50.36 50.17 50.35 51.26 24.80 24.66 24.82 24.36 24.24 24.38 24.72 50. 24.88 50.33 51.14 24.23 24.83 91.11 91.04 91.16 90.72 90.43 91.23 91.68 91.19 91.05 91. 76.98 77.15 76.92 76.52 76.33 76.47 77.92 77.07 76.15 76.99 3.61 3.57 3.58 3.70 3.72 3.63 3.46 3.55 3.71 3. 46.37 46.70 46.52 45.69 45.62 44.89 47.38 19.72 19.93 19.95 19.28 18.99 18.73 20.22 46.69 19.89 44.23 46.74 18.31 19. 86.94 80.33 2.56 55.86 28.64 89. 75.09 2.64 50.02 24.10 90.84 76. 3.64 45.89 18.97 88.43 88.50 87.98 87.00 86.11 77.69 88.19 88.34 74.05 87. 87.65 87.63 80.99 81.12 80.69 80.29 79.99 76.31 80.22 81.08 74.06 80.56 80.86 80.84 2.44 2.50 2.46 2.47 2.58 2.70 2. 2.44 3.00 2.55 2.46 2.45 56.88 57.26 57.06 55.39 55.40 46.94 56.56 29.63 29.55 29.77 28.24 28.16 20.32 27.95 57. 29.59 43.60 57.13 17.59 29.67 56.53 56.56 29.09 29.03 91.54 89.33 90.90 91.41 88.97 89.79 87. 91.08 90.38 87.72 90.28 90.64 79.24 76.05 77.72 79.47 75.01 76.43 73.65 79.22 77.64 74. 77.45 78.18 2.13 2.47 2.26 2.13 2.67 2.20 2.78 2.21 2.22 2.75 2.38 2.28 en-zh xx-en Neural xCOMET-QE CometKiwi Metric-X"
        },
        {
            "title": "Lexical",
            "content": "chrF"
        },
        {
            "title": "BLEU",
            "content": "Neural xCOMET-QE CometKiwi Metric-X 48.22 50.98 48.94 47.24 49.68 39.65 50.57 21.98 24.10 23.02 21.10 23.89 14.17 22.96 48.53 21.83 42.21 51. 15.79 23.90 49.85 49.94 23.92 23."
        },
        {
            "title": "Lexical",
            "content": "91.94 91.50 91.89 91.92 90.61 90.11 90.68 91.71 89.24 90.75 91.79 91.79 chrF"
        },
        {
            "title": "BLEU",
            "content": "xCOMET-QE 77.56 78.04 77.09 77.21 76.46 72.25 77.67 77.91 70.90 77.58 77.42 77.58 3.41 3.45 3.45 3.46 3.70 3.88 3. 3.39 4.22 3.61 3.44 3.39 46.69 47.58 46.42 45.18 45.48 39.36 47.74 19.65 20.12 19.67 18.62 18.88 14.29 19.60 47. 19.89 36.75 47.80 12.42 20.37 46.34 46.59 19.18 19.31 en-xx Neural CometKiwi Metric-X"
        },
        {
            "title": "Lexical",
            "content": "chrF"
        },
        {
            "title": "BLEU",
            "content": "83.92 71.42 2.56 35.58 35.94 88. 77.74 3.23 52.30 25.37 86.19 73. 2.42 47.31 29.43 84.89 84.80 84.78 84.50 84.06 84.66 84.75 84.90 84.40 84. 72.22 72.00 71.50 72.22 71.38 72.81 71.33 72.44 72.14 70.78 2.47 2.50 2.53 2.48 2.54 2.40 2.54 2.48 2.44 2. 36.50 36.80 37.05 36.26 35.85 36.06 37.51 37.00 37.47 37.73 36.77 36.39 36.28 38.34 36.43 36.92 36.59 37.42 36.93 38. 88.96 89.07 89.05 88.29 88.07 88.33 89.57 89.03 87.91 88.89 78.46 78.53 78.47 78.02 77.93 77.92 79.06 78.57 77.62 78. 3.17 3.16 3.17 3.24 3.25 3.19 3.11 3.13 3.25 3.19 53.30 53.57 53.39 52.62 52.52 51.75 54.08 26.38 26.52 26.41 25.84 25.62 25.18 26.67 53. 26.53 51.20 53.51 24.86 26.44 87.07 86.94 87.04 87.04 86.52 87.29 86.70 87.11 86.95 86. 73.99 73.70 73.60 74.08 73.27 74.57 73.18 74.21 73.96 73.02 2.30 2.33 2.36 2.31 2.44 2.27 2.39 2.29 2.31 2. 48.38 48.52 48.65 48.03 47.52 47.86 49.23 30.39 30.50 30.79 30.14 29.74 29.82 30.97 48.45 30.40 48.14 49.04 30.11 31. 84.52 72.02 2.53 35.82 36.16 88. 78.17 3.20 52.74 25.60 86.75 73. 2.40 47.43 29.60 86.32 85.84 85.75 84.69 84.64 79.65 82.88 84.89 80.91 83. 85.85 86.78 75.09 73.30 74.50 74.65 72.42 72.88 69.66 74.61 73.49 69.85 74.47 75.68 2.36 2.43 2.30 2.45 2.50 2.69 2. 2.52 2.69 2.81 2.33 2.25 31.02 36.43 32.63 29.78 34.26 23.12 36.00 29.15 36.32 31.93 27.30 34.58 19.34 35.56 30. 28.63 26.29 37.64 22.83 37.80 34.22 33.58 34.00 32.93 89.77 89.57 89.52 89.16 88.05 84.06 88. 89.58 81.79 88.79 89.33 89.36 78.95 79.26 78.54 78.46 77.95 74.22 78.64 79.16 72.38 78. 78.78 78.92 3.02 3.07 3.06 3.07 3.25 3.43 3.20 3.02 3.77 3.17 3.06 3.03 53.47 54.18 53.44 51.94 52.24 44.53 53. 26.32 26.59 26.40 24.91 25.22 18.10 25.59 53.97 26.43 41.46 54.21 15.65 26.74 53.17 53. 25.80 25.77 88.09 87.41 87.66 87.45 86.68 81.01 85.14 87.25 83.21 85.40 87.94 88.50 76.75 74.46 75.84 76.62 73.75 73.55 71. 76.71 74.76 71.82 76.01 76.87 2.09 2.27 2.14 2.11 2.41 2.33 2.58 2.19 2.34 2. 2.20 2.12 44.29 48.88 45.27 43.30 46.54 34.64 48.68 25.15 30.05 26.99 24.02 28.75 15.54 28.99 44.48 24.99 37.96 49. 18.13 30.49 46.65 46.48 28.45 27.99 Base SFT Multi-system xCOMET-QE Vanilla No Base No Ref No GPT4 Chosen = Base Chosen = Ref Chosen = GPT4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla CPO Multi-system xCOMET-QE Vanilla No Ref No Base No GPT4 Chosen = Base Chosen = Ref Chosen = GPT4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla Optimized Base SFT Multi-system xCOMET-QE Vanilla No Base No Ref No GPT4 Chosen = Base Chosen = Ref Chosen = GPT4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla CPO Multi-system xCOMET-QE Vanilla No Ref No Base No GPT4 Chosen = Base Chosen = Ref Chosen = GPT4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla Optimized Base SFT Multi-system xCOMET-QE Vanilla No Base No Ref No GPT4 Chosen = Base Chosen = Ref Chosen = GPT4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla CPO Multi-system xCOMET-QE Vanilla No Ref No Base No GPT4 Chosen = Base Chosen = Ref Chosen = GPT4 CometKiwi Vanilla chrF Vanilla No Ref Mono-system xCOMET-QE Vanilla Optimized Table 10: Comprehensive downstream evaluation for the WMT23 dataset. Metrics, notations and formatting are the same as in Table 9."
        },
        {
            "title": "B Additional Data Details",
            "content": "B.1 Building Preference Datasets in the Mono-System Setting Following the experimental setup detailed in the main text (Section 3), we here provide further details on the method used to construct mono-system preference datasets. As reminder, after generating the candidate translations for each source sentence, we have, for all 1 ,"
        },
        {
            "title": "Y mono\ni",
            "content": "= (cid:8)y1 , , yK (cid:9), yK where y1 evaluate yBase translations. We denote it by bi. Sorted in increasing quality order, we thereby have are assumed to be sorted in increasing metric score order. For each sample, we (the greedy-decoded translation) using metric and check its rank in the set of candidate ybi1 y1 yBase ybi yK . Finally, to determine the chosen and rejected hypotheses, we select two offset parameters or, oc N, such that the chosen and rejected options are respectively (cid:40) = ymin(K,bi+oc) yc = ymax(1,bior) yr . Intuitively, or and oc control the average quality of the chosen and rejected options in the resulting preference dataset and ensure that the chosen (resp. rejected) option always has higher (resp. lower) quality than the base translation. Table 11 presents the average quality properties for mono-system preference datasets, and compares them to the multi-system setting. Hyp. xCOMET-QE"
        },
        {
            "title": "Neural",
            "content": "Multi-system"
        },
        {
            "title": "Vanilla preference dataset",
            "content": "Mono-system Multi-system replica Chosen = Low / Rejected = Low Chosen = Low / Rejected = Mid Chosen = Low / Rejected = High Chosen = Mid / Rejected = Low Chosen = Mid / Rejected = Mid Chosen = Mid / Rejected = High Chosen = High / Rejected = Low Chosen = High / Rejected = Mid Chosen = High / Rejected = High Base GPT-4 Reference Rejected Chosen"
        },
        {
            "title": "Rejected\nChosen\nRejected\nChosen\nRejected\nChosen\nRejected\nChosen\nRejected\nChosen\nRejected\nChosen\nRejected\nChosen\nRejected\nChosen\nRejected\nChosen\nRejected\nChosen",
            "content": "93.09 94.58 91.84 87.86 97.24 87.80 97.29 75.36 93.60 84.54 93.60 92.15 93.60 75.36 95.77 84.54 95.77 92.15 95.77 75.36 98.16 84.54 98.16 92.15 98.16 87.13 88.32 86.72 84.15 89.81 83.04 89.20 75.46 87.04 81.02 87.04 85.54 87.04 75.46 88.40 81.02 88.40 85.54 88.40 75.46 89.84 81.02 89.84 85.54 89."
        },
        {
            "title": "Lexical\nchrF",
            "content": "58.33 60.93 100.00 78.48 75.95 55.69 57.18 52.95 57.14 54.93 57.14 55.86 57.14 52.95 57.43 54.93 57.43 55.86 57.43 52.95 57.56 54.93 57.56 55.86 57.56 Table 11: Average quality properties for xCOMET-QE-based mono-system preference datasets, compared to the multi-system setting. Multi-system replica is the mono-system dataset that matches the average chosen/rejected qualities of the multi-system preference data. Other mono-system datasets are represented by their relative average chosen/rejected qualities. B.2 Language Statistics Figure 2: Language statistics for preference datasets. The y-axis represents the number of samples, corresponding percentages are displayed above each bar. Figure 3: Language statistics for WMT22 and WMT23 test data. The y-axis represents the number of samples, corresponding percentages are displayed above each bar."
        }
    ],
    "affiliations": [
        "Artefact Research Center",
        "CentraleSupélec, Université Paris-Saclay",
        "Equall",
        "Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit)",
        "Instituto de Telecomunicações",
        "Unbabel"
    ]
}