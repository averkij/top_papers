{
    "paper_title": "One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation",
    "authors": [
        "Fabian Paischer",
        "Lukas Hauzenberger",
        "Thomas Schmied",
        "Benedikt Alkin",
        "Marc Peter Deisenroth",
        "Sepp Hochreiter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models (FMs) are pre-trained on large-scale datasets and then fine-tuned on a downstream task for a specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via a low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with a uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or a uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in a data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to a variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across a multitude of tasks per domain."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 0 7 1 7 0 . 0 1 4 2 : r One Initialization to Rule them All: Fine-tuning via Explained Variance Adaptation Fabian Paischer1* Benedikt Alkin1,3 Thomas Schmied1 Lukas Hauzenberger1* Sepp Hochreiter1,3 Marc Peter Deisenroth2 1 ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria 2 University College London 3 NXAI GmbH, Linz, Austria paischer@ml.jku.at"
        },
        {
            "title": "Abstract",
            "content": "Foundation models (FMs) are pre-trained on large-scale datasets and then finetuned on downstream task for specific application. The most successful and most commonly used fine-tuning method is to update the pre-trained weights via low-rank adaptation (LoRA). LoRA introduces new weight matrices that are usually initialized at random with uniform rank distribution across model weights. Recent works focus on weight-driven initialization or learning of adaptive ranks during training. Both approaches have only been investigated in isolation, resulting in slow convergence or uniform rank distribution, in turn leading to sub-optimal performance. We propose to enhance LoRA by initializing the new weights in data-driven manner by computing singular value decomposition on minibatches of activation vectors. Then, we initialize the LoRA matrices with the obtained right-singular vectors and re-distribute ranks among all weight matrices to explain the maximal amount of variance and continue the standard LoRA fine-tuning procedure. This results in our new method Explained Variance Adaptation (EVA). We apply EVA to variety of fine-tuning tasks ranging from language generation and understanding to image classification and reinforcement learning. EVA exhibits faster convergence than competitors and attains the highest average score across multitude of tasks per domain."
        },
        {
            "title": "Introduction",
            "content": "Foundation models (Bommasani et al., 2021, FMs) are usually trained on large-scale data and then fine-tuned towards particular downstream task. This training paradigm has led to significant advancements in the realm of language modeling (OpenAI, 2023; Touvron et al., 2023a; Reid et al., 2024), computer vision (Dehghani et al., 2023; Oquab et al., 2023), and reinforcement learning (Brohan et al., 2023; Zitkovich et al., 2023). With an increasing number of model parameters, the process of fine-tuning becomes prohibitively expensive. This results in the need for efficient alternatives to fine-tuning all parameters of the pre-trained model. Parameter-efficient fine-tuning (PEFT) approaches are commonly used as an effective alternative to full fine-tuning (FFT). PEFT methods modify the pre-trained model by introducing small number of new trainable parameters, while the pre-trained weights remain frozen. This leads to substantial reduction in computational cost, both in terms of time and space. particularly successful approach, LoRA (Hu et al., 2022), introduces new weights in the form of low-rank decomposition for each weight matrix in the pre-trained model. After training, the new weights can be readily merged into *Equal contribution Figure 1: Left: EVA performs singular value decomposition on activation vectors for the first few minibatches to obtain suitable initialization for the LoRA matrix A. Right: After initializing A, we allocate ranks to maximize the explained variance throughout the model and continue the standard LoRA fine-tuning procedure, where is kept frozen and only and are trained. the pre-trained weights without any additional inference latency. Recent research has explored two main avenues for enhancing LoRA: weight-driven initialization and adaptive rank allocation during training (see Table 1). While weight-driven initialization methods have shown promise, they typically rely on uniform rank distribution across pre-trained weights. Further, they are constrained to the information stored in the pre-trained weights. Finally, existing adaptive rank allocation techniques initialize low-rank matrices randomly. We propose new method based on LoRA that combines adaptive rank allocation with data-driven initialization by leveraging information from the downstream task at hand. Certain activation patterns of FMs have shown to be crucial for model performance (Sun et al., 2024). Therefore, we leverage minibatches of activations computed on downstream data to initialize LoRA weights. To this end, we propagate minibatches of the fine-tuning data through the model and compute the singular value decomposition (SVD) on activation vectors to obtain the right-singular vectors. We then sort the rightsingular vectors in descending order according to the variance they explain. Finally, we leverage the top-k components according to given rank budget for initializing LoRA. This results in an effective initialization, that (i) is data-driven by leveraging information from the downstream task, and (ii) allocates ranks to pre-trained weights to maximize the explained variance throughout the model. We call the resulting method EVA, which is short for Explained Variance Adaptation. Importantly, this procedure can be performed within the first few minibatches of LoRA fine-tuning without significant computational overhead. We demonstrate the benefits of EVA on an array of downstream tasks, namely language generation and understanding, image classification, and reinforcement learning (RL). EVA consistently improves average performance across multitude of tasks on each domain compared to LoRA and other recently proposed initialization or rank redistribution methods. For language generation, we fine-tune 7B-9B parameter language models on math and reasoning tasks, where EVA attains the highest average performance. Further, on set of language understanding tasks, EVA improves the average performance compared to competitors. On image classification we fine-tune pre-trained vision transformer (Dosovitskiy et al., 2021) on set of 19 diverse tasks. We find that EVA attains the highest average score and improves over LoRA and established extensions thereof, with most gains on in-domain data. For our RL experiments we conduct fine-tuning on continuous control tasks and find that EVA significantly exceeds performance of LoRA and even exceeds performance of full fine-tuning (FFT) when combined with DoRA (Liu et al., 2024a). Finally, we conduct ablation studies to demonstrate that the combination of direction and scale of EVA leads to the best performance."
        },
        {
            "title": "2 Related Work",
            "content": "LoRA (Hu et al., 2022) has sparked widespread interest in leveraging low-rank decompositions for fine-tuning due to its simplicity. Building on the success of LoRA, number of other variants have been proposed (Kopiczko et al., 2024; Zi et al., 2023; Babakniya et al., 2023; Dettmers et al., 2023; Li 2 Table 1: Comparison of EVA to existing initialization schemes for LoRA. Existing works either focus on weight-driven initialization or adaptive rank allocation. EVA combines data-driven initialization with adaptive rank allocation to enhance convergence and downstream performance."
        },
        {
            "title": "Method",
            "content": "LoRA (Hu et al., 2022) AdaLoRA (Zhang et al., 2023a) PiSSA (Meng et al., 2024) OLoRA (Büyükakyüz, 2024) EVA (Ours)"
        },
        {
            "title": "Initialization",
            "content": "Random Random Weight-driven Weight-driven Data-driven Adaptive ranks et al., 2023; Nikdan et al., 2024; Liu et al., 2024a; Zhang et al., 2023a; Hayou et al., 2024b; Chavan et al., 2023). The most similar variants to EVA are AdaLoRA (Zhang et al., 2023a) and PiSSA (Meng et al., 2024). AdaLoRA adaptively alters the number of ranks for LoRA matrices during fine-tuning. Other more recent approaches learn gates to switch ranks on or off during fine-tuning (Liu et al., 2024b; Meo et al., 2024). In contrast, the data-driven initialization allows EVA to redistribute ranks for each LoRA matrix prior to fine-tuning by leveraging small number of minibatches of data. PiSSA initializes LoRA matrix via the top singular vectors of the pre-trained weight matrices. Contrary, EVA initializes via the right-singular vectors of minibatches of activation vectors and is therefore data-driven. Since EVA mostly constitutes an effective initialization, it can be readily plugged into other LoRA variants such as DoRA (Liu et al., 2024a). Initialization of LoRA matrices Common initialization schemes for neural networks (He et al., 2015; Glorot & Bengio, 2010) were designed to stabilize training of deep neural networks based on activation functions and depth. In the context of PEFT, Hu et al. (2022) and Liu et al. (2022) explored data-driven initialization by either pre-training on different task first, or by unsupervised pre-training on the task at hand. Contrary, EVA does not require any gradient update steps, therefore it is much more efficient. Similarly, Nikdan et al. (2024) utilize warm-up stage in LoRA fine-tuning, where gradients with respect to LoRA weights are used to initialize sparse matrix for sparse adaptation (Sung et al., 2021) in combination with LoRA. Alternatively, Babakniya et al. (2023) initialize LoRA matrices using SVD on weight matrices obtained after few steps of full fine-tuning for federated learning with heterogeneous data. Meng et al. (2024) use the main directions of the pre-trained weights to initialize the LoRA matrices. In contrast, EVA takes data-driven approach to initialize the LoRA matrices, instead of relying on components of the pre-trained weights. Similar initialization schemes were proposed by Mishkin & Matas (2016); Krähenbühl et al. (2016) for training deep networks from scratch. Increasing efficiency of LoRA Several works have investigated how to further break down the complexity of LoRA for fine-tuning FMs. Kopiczko et al. (2024) decrease the memory complexity of LoRA by initializing both and at random and keeping them frozen while merely training newly-introduced scaling vectors. This way, only random seeds for initializing and need to be stored. Another fruitful avenue is quantization (Dettmers et al., 2022), which has been successfully combined with LoRA fine-tuning (Dettmers et al., 2023). Other LoRA variants (Nikdan et al., 2024; Valipour et al., 2023; Meng et al., 2024) also provide quantized versions. It has also been shown that proper initialization for quantization results in improved fine-tuning performance (Li et al., 2023)."
        },
        {
            "title": "3 Method",
            "content": "EVA aims at initializing LoRA weights in data-driven manner by leveraging data from the downstream task. Since EVAbuilds on low-rank decomposition of weight matrices as in LoRA (Hu et al., 2022), we first briefly explain LoRA in Section 3.1. In Section 3.2, we describe how we obtain an effective initialization for the low-rank decomposition of LoRA matrices via SVD on activation vectors. This enables an adaptive assignment of ranks across all layers to maximize the explained variance throughout the pre-trained model We explain this in more detail in Section 3.3. 3 Figure 2: Left: Training loss for fine-tuning Llama-3.1-8B on the MetaMathQA dataset. We compare EVA to recently proposed initialization methods OLoRA, PiSSA, and random initialization (LoRA). We show mean and standard deviation across three random seeds. Right: Mean and standard deviation of gradient norm for EVA, PiSSA, OLoRA and Random initialization of LoRA matrices. EVA exhibits significantly larger gradient norm leading to faster convergence. 3.1 Low-Rank Adaptation (LoRA) LoRA adds new trainable weights which are computed via an outer product of low-rank matrices (Hu et al., 2022). This is motivated by the low intrinsic dimensionality of language models (Aghajanyan et al., 2021) and relies on the assumption that the gradients during fine-tuning are also of low rank (Gur-Ari et al., 2018; Zhang et al., 2023b; Gauch et al., 2022). In the following, we explain LoRA in more detail. Let Rd1 be the input to pre-trained weight matrix Rkd. Then, LoRA introduces new weight matrices and as low-rank decomposition = + BAx , (1) where Rkr and Rrd. The rank is hyperparameter with k. During fine-tuning, remains frozen and only and are updated. Usually is initialized with zeros, such that fine-tuning starts from the pre-trained model. is usually initialized at random. Additionally, Hu et al. (2022) introduce hyperparamter α which is used to scale BAx by α . 3.2 Data-driven Initialization of Low-Rank Adaptation Our aim is to find an effective initialization for the low-rank matrix in data-driven manner to maximize performance on the downstream task. To this end, we perform SVD on batches of activation vectors Rbd to obtain the right-singular values, which constitute the directions that capture most of the variance. This procedure is done during the initial training stage where we propagate minibatches of data through the model and incrementally update the right-singular vectors. More formally, we collect batches of activations for pre-trained weight matrices {W 0, 1, ..., } that we choose to update fine-tune. Subsequently, we compute the SVD on each to obtain the right-singular vectors vi j,: and respective singular values σi as = (cid:88) j=1 ui :,jσi jvi j,:. (2) Importantly, we compute the SVD incrementally on each minibatch of fine-tuning data and update vi :r,: after each forward pass through the model. After each step we check whether vi :r,: has converged. To this end, we measure the column-wise cosine similarity between subsequent computations of v:r,: and determine convergence based on threshold τ . If the right-singular values have converged, i.e. cossim(vi,t1 1 r, we initialize Ai = vi :r,: and exclude the corresponding weight matrix from subsequent SVD computations. We continue this procedure until all vi :r,: have converged. j,: ) τ , vi,t j,: 4 The computation of SVD introduces computational overhead in the initial training stage. Since we do not require gradient computation or storing of optimizer states, there is no overhead in terms of memory. SVD has time complexity of O(min(b2d, bd2)) which can be reduced to O(k2b) for << by randomly choosing columns from as introduced in Halko et al. (2011). Let be the number of minibatches until all components are converged for weight matrices, then the time complexity is O(N k2b). In other words, the complexity scales linearly with the number of weight matrices and the number of minibatches. To speed up the computation of SVD, we provide an implementation that runs entirely on GPU. 3.3 Adaptive Rank Allocation The singular values obtained by SVD provide an estimate of the variance that is explained by their components. Leveraging this information, we can redistribute ranks across weight matrices of the pre-trained model such that the maximum amount of variance is explained. This can be done by allocating more ranks to layers that propagate more information, i.e., explain more variance. More formally, the variance explained by each component in vi j,: is given by their explained variance ratio ξi = σi2 (M 1)σi , (3) where 1 denotes the ℓ1 norm, σi is vector containing all singular values, and is the total number of samples used for the incremental SVD computation. Next, we sort the components vi j,: for each weight matrix in descending order according to their explained variance ratio ξi j. Finally, we assign ranks to pre-traiend weights until we reach certain rank budget. Additionally, we introduce hyperparameter ρ [1, ) which controls the uniformity of the rank distribution. ρ determines the number of ranks that we compute during SVD and increasing ρ allows for an increasingly heterogeneous rank distribution. That is, for each we compute rρ components initially meaning we obtain rρ components in total. For the redistribution we only use the top components according to their explained variance ratio ξi j. Thus, setting ρ = 1, results in uniform rank distribution as in LoRA, but initialized according to EVA. Therefore, ρ provides us with the means to change the rank distribution in controlled manner prior to fine-tuning at the initialization stage, as opposed to learning it throughout the training process as done in prior works (Zhang et al., 2023a; Valipour et al., 2023; Meo et al., 2024). In practice we found that the redistribution converges for values of ρ > 2 (see Appendix G). Finally, we initialize with zeros and perform the standard LoRA fine-tuning, as recommended in Hayou et al. (2024a). In Algorithm 1 we provide pseudocode for EVA. Algorithm 1 Fine-tuning via EVA Input: FM ψ(), ρ, rank r, dataset 1: while not all_converged(ψ) do 2: ψ(next(D)) 3: 4: 5: 6: 7: 8: end while 9: redistribute_ranks(ψ, ξ, Vnew) 10: lora_finetune(ψ, X) Vnew, ξ SVD(X, ρr) if isclose(Vold, vnew) then wrap_and_initialize(Wj, Vnew) end if Vold Vnew get activations"
        },
        {
            "title": "4 Experiments",
            "content": "First, we elaborate on implementation details of EVA in Section 4.1. Then, we show results for fine-tuning large language models (LLMs) on math and reasoning tasks in Section 4.2 and language understanding tasks in Section 4.3. Further we show results for image classification in Section 4.4 and decision making tasks in Section 4.5. Finally, in Section 4.6 we demonstrate that the computational overhead induced by EVA over LoRA is negligible and that incremental SVD converges and is invariant to batch order and batch size. 4.1 Implementation Details We follow the standard LoRA training procedure from Hu et al. (2022). Similar to Kalajdzievski (2023), we found LoRA training to be very sensitive to the scaling parameter α. Therefore, we 5 Table 2: Comparison of LoRA and DoRA to different initialization and rank re-distribution methods on NLG tasks. We report average performance across three seeds and respective standard deviation in Table 11. EVA+DoRA and EVA consistently attain the highest average performance across all tasks. Model Method BoolQ PIQA SIQA HellaSwag Winogrande ARC-e ARC-c OBQA Avg. Llama-2-7B Llama-3.1-8B Gemma-2-9B 67.2 LoRA 74.8 AdaLoRA 62.6 PiSSA 68.7 OLoRA 71.2 EVA 68.3 DoRA EVA+DoRA 73.5 LoRA 85.7 83.9 AdaLoRA 72.9 PiSSA 86.0 OLoRA 85.5 EVA 86.2 DoRA EVA+DoRA 85.8 LoRA 88.3 87.3 AdaLoRA 81.4 PiSSA 87.7 OLoRA 88.6 EVA DoRA 88.3 EVA+DoRA 88.6 83.9 82.2 84.8 84.8 85.2 85.1 85.3 90.3 89.5 87.3 90.4 90.8 90.8 90.8 92.9 91.8 90.0 92.5 93.0 92.6 93. 82.0 80.5 81.2 82.2 82.1 82.2 82.4 83.0 81.7 81.6 83.9 83.3 83.4 83.9 85.2 84.6 82.5 85.2 85.3 84.9 85.1 94.7 93.3 94.5 95.0 95.2 94.9 95.2 96.9 96.2 95.3 97.0 97.1 96.9 97.1 97.8 97.3 95.5 97.5 97.9 97.7 97. 84.0 79.4 84.8 85.0 84.5 84.3 84.8 88.4 86.3 87.8 88.6 88.6 88.6 89.2 92.3 91.3 89.0 92.5 92.8 92.2 92.5 87.8 86.1 87.8 88.1 88.9 88.7 88.9 94.2 93.7 91.7 94.5 94.7 94.3 94.4 97.2 97.0 93.6 96.6 97.5 97.1 97. 74.1 71.1 74.8 74.9 75.6 74.8 76.0 84.8 82.7 81.2 84.7 85.7 84.9 85.9 89.9 90.0 83.5 88.7 90.5 89.9 89.6 84.0 80.6 85.4 85.2 85.0 86.3 87.3 90.1 86.8 87.6 90.3 89.5 89.4 90.5 94.4 92.6 90.8 93.7 94.5 94.5 94. 82.2 81.0 82.0 82.9 83.4 83.1 84.2 89.2 87.6 85.7 89.4 89.4 89.3 89.7 92.2 91.5 88.3 91.8 92.5 92.1 92.4 set α = 1 since we found this to be the most stable setting and additionally tune the learning rate. We apply EVA to pre-trained weights only, i.e., we do not initialize newly introduced classifier heads. Following Zhang et al. (2023a), we apply LoRA to all pre-trained weight matrices except for the embedding layer. For EVA we always search over ρ {1, 2} to cover both uniform and non-uniform rank allocation and report the best score. All models we used are publicly available on the huggingface hub (Wolf et al., 2020). For the implementation of baselines we leverage the widely used PEFT library (Mangrulkar et al., 2022). Across experiments we highlight the highest scores in boldface and underline the second-highest. 4.2 Language Generation et al., We fine-tune three different LLMs, namely 2023b), Llama-2-7B (Touvron Llama-3.1-8B (Dubey et al., 2024), and Gemma-2-9B (Rivière et al., 2024) on common sense and math reasoning benchmarks. For common sense reasoning we follow Liu et al. (2024a) and amalgamate training set consisting of BoolQ (Christopher et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2020), ARC-e and ARC-c (Clark et al., 2018) and OpenBookQA (Mihaylov et al., 2018). We apply all methods listed in Table 1 to all three models and additionally add comparison to DoRA (Liu et al., 2024a) and EVA+DoRA, which combines EVA with DoRA. We train all methods with rank = 16 and learning rate of 5e 4 for three random seeds. Further details on the fine-tuning settings can be found in Appendix B. We present our results in Table 2. For Llama-2-7B and Llama-3.1-8B EVA+DoRA (ρ = 1) is the best performing method on average while also Table 3: Comparison of EVA to other initialization and adaptive rank methods on GSM8K and MATH datasets. We report mean and standard deviation across three random seeds. Model Method GSM8K MATH LoRA AdaLoRA PiSSA OLoRA EVA DoRA EVA+DoRA LoRA AdaLoRA PiSSA OLoRA EVA DoRA EVA+DoRA LoRA AdaLoRA PiSSA OLoRA EVA DoRA EVA+DoRA 59.7.8 56.9.4 61.1.3 60.7.5 61.9.5 59.8.5 62.5.8 78.3.6 76.9.2 78.8.2 78.0.1 78.8.3 77.9.1 79.1.5 83.4.9 83.5.5 79.8.5 82.2.2 83.6.8 82.5.6 82.9. 10.9.2 9.6.2 12.6.4 11.8.3 13.1.3 11.5.2 13.4.01 30.1.5 28.9.7 29.5.5 31.0.7 31.2.3 30.2.5 30.8.4 40.7.2 41.1.4 34.9.2 39.4.6 41.5.3 39.7.4 40.0.6 Llama-2-7B Llama-3.1-8B Gemma-2-9B 6 Table 4: Comparison of all methods for RoBERTaLarge (top) and DeBERTav3Base (bottom) on GLUE tasks. We report mean and standard deviation of Matthews correlation for CoLA, Pearson correlation for STS-B, matched accuracy for MNLI, and accuracy for remaining tasks. For CoLA, RTE, MRPC, and STS-B we average over five seeds and for the remaining tasks over three seeds. Method MNLI QNLI QQP SST2 CoLA MRPC RTE STS-B Avg 94.7 90.2 FFT 94.8.1 90.7.1 LoRA 94.8.2 AdaLoRA 90.5.1 90.1.1 94.7.0 PiSSA 90.9.1 95.0.1 OLoRA 95.0.2 90.8.1 EVA 94.6.1 89.5.1 DoRA 90.1 94.0 94.3.1 90.5.1 94.6 90.8 94.1.1 90.1.3 94.4.1 90.5.1 94.4.1 90.6.1 94.1.1 89.0.2 FFT LoRA AdaLoRA PiSSA OLoRA EVA DoRA 96.4 92.2 96.2.3 92.0.0 96.1.2 90.6.1 96.1.2 91.0.0 96.3.3 92.0.2 96.2.1 92.1.1 96.1.1 89.9.1 95.6 92.4 95.2.3 92.4.1 96.1 92.2 91.8.1 95.8.1 92.6.1 96.2.2 92.4.04 96.2.2 94.6.4 88.0.1 92.4 92.3.1 91.8.1 92.5.3 92.4.1 90.9 91.1.6 90.7.6 90.4.6 91.01.0 86.6 88.11.1 84.4.9 87.6.5 87.91. 88.93 68.0 89.29 69.1.5 88.39 68.2.7 88.89 68.71.3 89.32 69.01.5 69.51.4 91.4.8 88.81.2 92.6.1 89.55 69.3.8 88.90 69.2 72.01.3 71.5 72.71.7 72.01.0 72.51.3 70.3.5 92.4.1 88.28 91.6 89.64 91.7.1 89.46 91.8 89.19 91.6.2 92.0.2 89.80 92.0.2 89.91 88.44 91.8.1 88.41.2 83.8 88.9.5 88.1 86.51.2 89.1.9 89.4.7 87.8.7 91.0.6 89.5 91.4.7 90.7 90.9.6 91.6.7 91.8.6 91.9.6 exhibiting the best individual scores on most tasks. For Gemma-2-9B, EVA with adaptive ranks (ρ = 2) yields the highest performance. EVA as well as EVA+DoRA are consistently among the best performing methods on all individual tasks. This highlights the effectiveness of EVAs data-driven initialization and rank allocation. For the math fine-tuning experiments, we fine-tune all models on the MetaMathQA dataset (Yu et al., 2024) for one epoch with the same hyperparameters that we used for the common sense reasoning benchmarks and report the results in Table 3. We observe that EVA attains the highest performance on the GSM8K dataset for Gemma-2-9B using ρ = 2. For Llama-2-7B and Llama-3.1-8B the best performing method is EVA+DoRA using ρ = 1 closely followed by EVA. On MATH, EVA+DoRA performs best for Llama-2-7B with ρ = 1, while EVA attains the highest score for Llama-3.1-8B with ρ = 1 and Gemma-2-9B with ρ = 2. These results indicates that the performance of adaptive rank allocation depends on the selected model. We further analyze the resulting rank distributions for different values of ρ for Llama-2-7B and their effect on downstream performance in Appendix G. Finally, we provide additional results for Llama-2-7B on code fine-tuning tasks in Appendix B. 4.3 Language Understanding We train RoBERTaLarge (Liu et al., 2019) and DeBERTav3Base (He et al., 2023) on the GLUE benchmark (Wang et al., 2019). The GLUE benchmark comprises eight downstream tasks, such as natural language inference, or sentiment analysis. Additionally to learning rate, we also search over different ranks within maximal rank budget (r = 16). For further details about datasets, implementation, or hyperparameters, we refer the reader to Appendix C. We also add FFT as baseline, but neglect EVA+DoRA due to time constraints and report Matthews correlation for CoLA, Pearson correlation for STS-B, and accuracy for the remaining tasks in Table 4. For RoBERTaLarge, EVA attains the highest scores on QNLI, CoLA, MRPC, RTE, and STS-B, leading to the highest average score. Interestingly, DoRA usually only slightly improves over LoRA on low resource tasks (RTE, MRPC), while performing worse in high resource tasks (MNLI, QNLI, QQP, SST2). We also compare LoRA to EVA in Table 14 in Appendix for different rank budgets, where EVA consistently improves over LoRA. For DeBERTav3Base, EVA reaches the highest scores on SST2, RTE, and STS-B, again leading to the highest average score across all tasks. We visualize resulting rank distribution patterns of EVA for different GLUE tasks in Appendix C. More ranks are assigned to higher layers of the query, key, and value projections in the self-attention, while the remaining weights often receive lower number of ranks. This is consistent pattern for both, DeBERTav3Base and RoBERTaLarge. 4.4 Image Classification We investigate the efficacy of EVA on the VTAB-1K (Zhai et al., 2019) benchmark, which has been widely used to evaluate PEFT methods. VTAB-1K comprises 19 image classification tasks that are Table 5: Fine-tuning DINOv2-g/14 on the VTAB-1K benchmark. Best average performance is highlighted in boldface. We report average accuracy across five seeds. Natural Specialized Structured 0 0 1 i 1 0 1 t D 2 0 1 o t H 7 9 3 S y a S E 5 4 s h o e u - l t - l L s - I L - d O - d - A N E - N e e 73.1 89.7 78.4 99.7 92.2 89.5 55.5 74.8 95.0 88.2 70.5 93.6 64.2 63.6 68.8 92.0 64.3 50.2 56.8 76.8 FFT 85.9 92.2 82.2 99.7 94.5 64.1 63.6 88.8 97.0 92.6 76.6 97.7 65.3 62.1 83.6 90.6 63.0 37.1 52.3 78.4 LoRA AdaLoRA 85.4 92.5 81.4 99.7 95.2 90.5 62.2 87.1 96.4 91.2 76.6 94.4 64.4 60.3 83.7 85.4 61.0 32.9 46.0 78.2 85.5 93.6 82.3 99.7 94.6 92.8 62.3 87.1 96.6 91.9 76.3 95.0 66.3 63.2 84.9 90.5 60.1 36.3 48.6 79.4 PiSSA 85.5 93.0 82.1 99.7 95.1 78.3 62.1 86.7 96.3 91.9 76.8 94.3 66.0 62.4 71.3 89.0 60.9 34.3 49.5 77.6 OLoRA 85.6 93.9 82.2 99.7 95.9 93.2 63.6 86.8 96.6 92.3 76.1 96.1 65.1 61.1 83.3 91.4 61.6 35.0 55.0 79.7 EVA 85.9 92.7 82.1 99.7 95.2 34.4 61.4 88.6 96.8 92.4 76.8 97.6 65.4 62.7 84.4 43.2 63.1 37.8 52.6 74.4 DoRA EVA+DoRA 86.2 92.1 81.9 99.7 94.9 93.8 62.4 88.3 96.6 92.6 76.7 97.2 65.5 54.1 83.7 93.3 62.3 37.5 54.5 79.6 Table 6: Results for single task fine-tuning experiments on the Meta-World benchmark. We report mean success rates and standard error across three seeds for every task. l - u e h r - n g n - k - p p w - p l - h u - t e c - n g v 1.0.0 0.97.03 1.0.0 0.77.05 0.87.05 1.0.0 1.0.0 1.0.0 0.63.03 1.0.0 0.92 FFT 1.0.0 1.0.0 1.0.0 0.4.09 1.0.0 0.86 0.63.1 1.0.0 1.0.0 1.0.0 0.6.05 LoRA AdaLoRA 1.0.0 0.97.03 1.0.0 0.4.09 0.57.1 0.97.03 0.97.03 1.0.0 0.13.07 1.0.0 0.80 1.0.0 1.0.0 1.0.0 0.430.11 0.570.03 1.0.0 1.0.0 1.0.0 0.530.1 1.0.0 0.85 PiSSA 1.0.0 0.970.03 1.0.0 0.570.1 0.630.03 1.0.0 1.0.0 1.0.0 0.60.12 1.0.0 0.88 OLoRA 1.0.0 0.97.03 1.0.0 0.63.03 0.77.05 1.0.0 1.0.0 1.0.0 0.63.07 1.0.0 0.90 EVA 1.0.0 1.0.0 1.0.0 0.671.5 1.0.0 0.93 1.0.0 1.0.0 1.0.0 0.61.2 DoRA 1.0.0 1.0.0 1.0.0 0.63.03 1.0.0 0.94 EVA+DoRA 1.0.0 1.0.0 1.0.0 0.8.08 1.0.0 1.0.0 divided into natural images, specialized images (medical images and remote sensing), and structured images (e.g. orientation prediction, depth estimation or object counting). We fine-tune DINOv2-g/14 model (Oquab et al., 2023) that consists of around 1.1B parameters. For implementation details or hyperparameters we refer the reader to Appendix D. Our results are shown in Table 5 and we additionally report error bars in Table 17. EVA and EVA+DoRA attain the best and second-best average accuracy across all tasks, respectively. Interestingly, EVA mainly improves over competitors on the natural tasks, i.e. in-domain datasets. LoRA performs best on the specialized tasks and full fine-tuning (FFT) performs best on the structured task. However, both LoRA and FFT perform worse on the remaining tasks, leading to worse average score compared to EVA and EVA+DoRA. 4.5 Decision Making We follow the single task fine-tuning experiments in Schmied et al. (2023) and fine-tune Decision Transformer (Chen et al., 2021a, DT) on the Meta-World benchmark suite (Yu et al., 2020). MetaWorld consists of diverse set of 50 tasks for robotic manipulation, such as object manipulation, grasping, or pushing buttons. We split Meta-World according to Wolczyk et al. (2021) into 40 pre-training tasks (MT40) and 10 fine-tuning tasks (CW10). We pre-train 12 parameter DT on MT40 and fine-tune it on the CW10 holdout tasks. We report success rates and standard errors for each task of CW10 in Table 6. We observe that EVA significantly reduces that gap between LoRA and FFT. Furthermore, DoRA performs particularly well in this experiment and exceeds FFT performance. Finally, our EVA+DoRA even improves upon DoRA and attains the best average performance across 8 Figure 3: Left: Time in seconds until convergence of incremental SVD components for different batch sizes for Llama-2-7B on the MetaMathQA dataset. The dashed line indicates the total number of components. Right: Average cosine similarity between SVD components across 10 random seeds for permuting the batch order. The first 10 components remain mostly consistent across all permutations. While the remaining components vary, they strongly correlate with each other. all tasks. We report results for different rank budgets in Table 19, as well as implementation details and hyperparameters in Appendix E. 4.6 SVD Convergence Analysis The data-driven initialization of EVA relies on incremental SVD on minibatches of activations in the initial training stage. In Figure 3, left, we show that this process converges for Llama-2-7B on MetaMathQA for different minibatch sizes. Using minibatch size of 4 the computation for EVAs initialization lasts for approximately 80 seconds, which corresponds to around 90 minibatches. For batch size of 32 the computation of the SVD components takes around 500 seconds. In Figure 3, right, we additionally show, that the main components obtained via SVD mostly remain consistent across different batch orders for batch size of 4, again for Llama-2-7B on MetaMathQA. To this end, we plot cosine similarity between components obtained via incremental SVD after rank redistribution. These results indicate that these models exhibit certain activation patterns that remain consistent across different batch orders which lead to robust initialization for EVA. We also show that the components for different batch sizes converge to mostly the same final initialization in Appendix F. 4.7 Ablation Studies Finally, we conduct ablation studies on EVA to investigate important factors that contribute to its performance. Specifically, we investigate the impact of scale and directions. To this end, we use the VTAB-1K dataset because it comprises diverse set of tasks and allows for systematic investigation on in-domain data (natural), and out-of-distribution data (specialized and structured). We report results for our ablation studies in Table 7 and explain the different settings in the following paragraphs. Effect of scale. To investigate the effect of scale on the initialization, we add setting which uses whitening (EVA-whiten). Whitening scales the initialization by the reciprocal of their eigenvalues, which alters scale, but preserves directions. We found that whitening can significantly improve performance on structured (out-of-distribution) tasks even leading to slightly higher average score than EVA. This indicates that scale is especially important for structured data. However, EVA-whiten experiences slight performance drop on natural and specialized tasks. Effect of directions. To address the importance of the directions of the components, we randomly permute its rows (EVA-perm). This preserves scale while corrupting directions and ℓ2 norm of A. Additionally, we add setting where we randomly rotate (EVA-rot), which preserves ℓ2 norm, but alters directions. We find that altering directions leads to performance drop on the structured tasks, while changing ℓ2 norm leads to drop on the natural tasks. Both, EVA-perm and EVA-rot lead to worse average performance across all tasks compared to EVA. 9 Effect of rank redistribution. We conduct an experiment in which we randomly initialize after performing rank redistribution (LoRA-redist). This setting gives insights on the effect of the redistribution and whether its benefits are bound to EVA. The redistribution has positive effect on LoRA on the natural tasks, but negative effect on both structured and specialized tasks. This illustrates that rank redistribution is most beneficial in combination with EVAs initialization of A."
        },
        {
            "title": "5 Discussion and Limitations",
            "content": "Table 7: Group-wise averages for DINOv2-G/14 ablation studies on the VTAB-1K benchmark. initialization data-driven Alternative schemes. We also investigated alternative data driven initialization schemes. Such alternatives include, but are not limited to, Kernel-PCA (Schölkopf et al., 1997) or Linear Discriminant Analysis (Fisher, 1936, LDA). While KernelPCA can account for non-linearities in the data, it scales with the number of datapoints. In our setting we perform SVD on minibatches of sequences, therefore, the number of datapoints grows fast, making Kernel-PCA impractical. LDA projects the data onto subspace that maximizes linear separability between classes. Such an initialization scheme is particularly interesting for classification tasks like GLUE or VTAB-1K. However, we observed on the GLUE tasks that the LDA projection matrix never converges. Struct. All 78.4 69.0 79.4 68.2 79.8 69.1 79.6 68.2 79.5 68.3 79.7 68.6 Method LoRA LoRA-redist EVA-whiten EVA-rot EVA-perm EVA Spec. 88.8 88.0 87.5 88.0 87.8 87.9 Nat. 83.2 87.3 87.5 87.7 87.4 87. Additional latency of SVD. EVA leads to performance improvements over LoRA, but introduces additional latency in the beginning of training for computing the data-driven initialization. We demonstrated that this process converges quickly. In Appendix we also show that this process is mostly invariant to the batch size, meaning that smaller batch sizes may be used for the SVD computation. This results in an additional cost in the range of 100 seconds for Llama-2-7B model, which is negligible. Further, the SVD computation does not require backpropagation and storing of optimizer states. Hence, there is no overhead with respect to memory. What method performs well on which tasks? Throughout all of our experiments, we observed that EVA is the most stable method and consistently improves average scores across tasks for all domains compared to competitors. Interestingly, DoRA only outperformed LoRA on experiments with larger models and on RL tasks. Furthermore, FFT performed particularly well on out-of-distribution tasks in our image classification experiments, but often performs worse on in-domain or low resource tasks. Contrary, EVA consistently advances average performance on wide range of tasks, establishing its potential as state-of-the-art fine-tuning method. Reproducibility. We provide the source code to reproduce all our experiments (see Appendix for more details). Further, we integrate EVA into the widely used PEFT library (Mangrulkar et al., 2022)."
        },
        {
            "title": "6 Conclusion and Broader Impact",
            "content": "We propose novel method named Explained Variance Adaptation (EVA), extending the widely used LoRA with data-driven initialization and rank re-distribution. We initialize LoRA matrices in data-driven manner by performing SVD on minibatches of activation vectors. Further, we re-distribute ranks across weight matrices according to the amount of variance they explain. In this regard, we also introduce hyperparameter that allows for controlled investigation of different rank distributions. Thereby, in EVA we bind the benefits of adaptive rank allocation and data-driven initialization, resulting in one initialization to rule them all. We demonstrate performance gains of EVA over LoRA and initialization schemes thereof on variety of domains, ranging from language to vision and RL. EVA variants consistently reach the highest average performance on wide range of tasks across all domains. We believe that EVA sheds novel view on LoRA fine-tuning, where initialization of the newly introduced weights is guided by the downstream data and can have significant impact on future research on fine-tuning of foundation models. In the future, we aim at investigating the effect of incorporating gradient information in EVA and quantization, as well as alternative data-driven initialization schemes."
        },
        {
            "title": "Acknowledgments",
            "content": "We acknowledge EuroHPC Joint Undertaking for awarding us access to Vega at IZUM, Slovenia, Karolina at IT4Innovations, Czech Republic, MeluXina at LuxProvide, Luxembourg, Leonardo at CINECA, Italy, MareNostrum5 at BSC, Spain. The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State Upper Austria. We thank the projects Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), EPILEPSIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF-36235), AI4GreenHeatingGrids (FFG899943), INTEGRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE-01-01). We thank NXAI GmbH, Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund AG, GLS (Univ. Waterloo), Software Competence Center Hagenberg GmbH, Borealis AG, TÜV Austria, Frauscher Sensonic, TRUMPF and the NVIDIA Corporation. Fabian Paischer acknowledges travel support from ELISE (GA no 951847)"
        },
        {
            "title": "References",
            "content": "Aghajanyan, A., Gupta, S., and Zettlemoyer, L. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 73197328. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.568. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Babakniya, S., Elkordy, A. R., Ezzeldin, Y. H., Liu, Q., Song, K., El-Khamy, M., and Avestimehr, S. Slora: Federated parameter efficient fine-tuning of language models. CoRR, abs/2308.06522, 2023. doi: 10.48550/ARXIV.2308.06522. Beattie, C., Leibo, J. Z., Teplyashin, D., Ward, T., Wainwright, M., Küttler, H., Lefrancq, A., Green, S., Valdés, V., Sadik, A., Schrittwieser, J., Anderson, K., York, S., Cant, M., Cain, A., Bolton, A., Gaffney, S., King, H., Hassabis, D., Legg, S., and Petersen, S. Deepmind lab. CoRR, abs/1612.03801, 2016. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R. B., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N. D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M. S., Krishna, R., Kuditipudi, R., and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jackson, T., Jesmonth, S., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, K., Levine, S., Lu, Y., Malla, U., Manjunath, D., Mordatch, I., Nachum, O., Parada, C., Peralta, J., Perez, E., Pertsch, K., Quiambao, J., Rao, K., Ryoo, M. S., Salazar, G., Sanketi, P. R., Sayed, K., Singh, J., Sontakke, S., Stone, A., Tan, C., Tran, H. T., Vanhoucke, V., Vega, S., Vuong, Q., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B. RT-1: robotics transformer for real-world control at scale. In Bekris, K. E., Hauser, K., Herbert, S. L., and Yu, J. (eds.), Robotics: Science and Systems XIX, Daegu, Republic of Korea, July 10-14, 2023, 2023. doi: 10.15607/RSS.2023.XIX.025. 11 Büyükakyüz, K. Olora: Orthonormal low-rank adaptation of large language models. CoRR, abs/2406.01775, 2024. doi: 10.48550/ARXIV.2406.01775. Chavan, A., Liu, Z., Gupta, D. K., Xing, E. P., and Shen, Z. One-for-all: Generalized lora for parameter-efficient fine-tuning. CoRR, abs/2306.07967, 2023. doi: 10.48550/ARXIV.2306.07967. Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097, 2021a. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021b. Cheng, G., Han, J., and Lu, X. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 105(10):18651883, 2017. doi: 10.1109/JPROC.2017.2675998. Christopher, C., Kenton, L., Ming-Wei, C., Tom, K., Michael, C., and Kristina, T. Boolq: Exploring the surprising difficulty of natural yes/no questions. In NAACL, 2019. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, wild. Columbus, OH, USA, June 23-28, 2014, pp. 36063613. IEEE Computer Society, 2014. doi: 10.1109/CVPR.2014.461. Clark, K., Luong, M., Le, Q. V., and Manning, C. D. ELECTRA: pre-training text encoders as discriminators rather than generators. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems, 2021. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen, M., Arnab, A., Wang, X., Ruiz, C. R., Minderer, M., Puigcerver, J., Evci, U., Kumar, M., van Steenkiste, S., Elsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot, F., Bastings, J., Collier, M., Gritsenko, A. A., Birodkar, V., Vasconcelos, C. N., Tay, Y., Mensink, T., Kolesnikov, A., Pavetic, F., Tran, D., Kipf, T., Lucic, M., Zhai, X., Keysers, D., Harmsen, J. J., and Houlsby, N. Scaling vision transformers to 22 billion parameters. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 74807512. PMLR, 2023. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3031830332. Curran Associates, Inc., 2022. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 12 Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozière, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. Fei-Fei, L., Fergus, R., and Perona, P. One-shot learning of object categories. IEEE Trans. Pattern Anal. Mach. Intell., 28(4):594611, 2006. doi: 10.1109/TPAMI.2006.79. Fisher, R. A. The use of multiple measurements in taxonomic problems. Annals Eugenics, 7:179188, 1936. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. Gauch, M., Beck, M., Adler, T., Kotsur, D., Fiel, S., Eghbal-zadeh, H., Brandstetter, J., Kofler, J., Holzleitner, M., Zellinger, W., Klotz, D., Hochreiter, S., and Lehner, S. Few-shot learning by dimensionality reduction in gradient space. In Chandar, S., Pascanu, R., and Precup, D. (eds.), Conference on Lifelong Learning Agents, CoLLAs 2022, 22-24 August 2022, McGill University, Montréal, Québec, Canada, volume 199 of Proceedings of Machine Learning Research, pp. 10431064. PMLR, 2022. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The KITTI dataset. Int. J. Robotics Res., 32(11):12311237, 2013. doi: 10.1177/0278364913491297. Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Teh, Y. W. and Titterington, D. M. (eds.), Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceedings, pp. 249256. JMLR.org, 2010. Gur-Ari, G., Roberts, D. A., and Dyer, E. Gradient descent happens in tiny subspace. CoRR, abs/1812.04754, 2018. Halko, N., Martinsson, P., and Tropp, J. A. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217288, 2011. doi: 10.1137/090771806. Hayou, S., Ghosh, N., and Yu, B. The impact of initialization on lora finetuning dynamics, 2024a. Hayou, S., Ghosh, N., and Yu, B. Lora+: Efficient low rank adaptation of large models, 2024b. He, K., Zhang, X., Ren, S., and Sun, J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 10261034. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.123. 13 He, P., Gao, J., and Chen, W. Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):22172226, 2019. doi: 10.1109/JSTARS.2019.2918242. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Hu, Z., Wang, L., Lan, Y., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLMadapters: An adapter family for parameter-efficient fine-tuning of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 52545276, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.319. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., and Girshick, R. B. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 19881997. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.215. Kaggle and EyePacs. Kaggle diabetic retinopathy detection, July 2015. Kalajdzievski, D. rank stabilization scaling factor for fine-tuning with lora. CoRR, abs/2312.03732, 2023. doi: 10.48550/ARXIV.2312.03732. Kopiczko, D. J., Blankevoort, T., and Asano, Y. M. ELoRA: Efficient low-rank adaptation with random matrices. In The Twelfth International Conference on Learning Representations, 2024. Krähenbühl, P., Doersch, C., Donahue, J., and Darrell, T. Data-dependent initializations of convolutional neural networks. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. Krizhevsky, A. Learning multiple layers of features from tiny images. CoRR, pp. 3233, 2009. LeCun, Y., Huang, F. J., and Bottou, L. Learning methods for generic object recognition with invariance to pose and lighting. In 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2004), with CD-ROM, 27 June - 2 July 2004, Washington, DC, USA, pp. 97104. IEEE Computer Society, 2004. doi: 10.1109/CVPR.2004.144. Li, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen, W., and Zhao, T. Loftq: Lora-fine-tuningaware quantization for large language models. CoRR, abs/2310.08659, 2023. doi: 10.48550/ ARXIV.2310.08659. Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. Few-shot parameterefficient fine-tuning is better and cheaper than in-context learning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Liu, S., Wang, C., Yin, H., Molchanov, P., Wang, Y. F., Cheng, K., and Chen, M. Dora: Weightdecomposed low-rank adaptation. CoRR, abs/2402.09353, 2024a. doi: 10.48550/ARXIV.2402. 09353. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019. 14 Liu, Z., Lyn, J., Zhu, W., Tian, X., and Graham, Y. Alora: Allocating low-rank adaptation for finetuning large language models. In Duh, K., Gómez-Adorno, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 622641. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.NAACL-LONG.35. Loshchilov, I. and Hutter, F. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S., and Bossan, B. Peft: State-of-the-art parameter-efficient fine-tuning methods, 2022. Matthey, L., Higgins, I., Hassabis, D., and Lerchner, A. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. Meng, F., Wang, Z., and Zhang, M. Pissa: Principal singular values and singular vectors adaptation of large language models, 2024. Meo, C., Sycheva, K., Goyal, A., and Dauwels, J. Bayesian-lora: Lora based parameter efficient fine-tuning using optimal quantization levels and rank values trough differentiable bayesian gates. CoRR, abs/2406.13046, 2024. doi: 10.48550/ARXIV.2406.13046. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. Mishkin, D. and Matas, J. All you need is good init. In Bengio, Y. and LeCun, Y. (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. Nikdan, M., Tabesh, S., and Alistarh, D. Rosa: Accurate parameter-efficient fine-tuning via robust adaptation. CoRR, abs/2401.04679, 2024. doi: 10.48550/ARXIV.2401.04679. Nilsback, M. and Zisserman, A. Automated flower classification over large number of classes. In Sixth Indian Conference on Computer Vision, Graphics & Image Processing, ICVGIP 2008, Bhubaneswar, India, 16-19 December 2008, pp. 722729. IEEE Computer Society, 2008. doi: 10.1109/ICVGIP.2008.47. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P., Li, S., Misra, I., Rabbat, M. G., Sharma, V., Synnaeve, G., Xu, H., Jégou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learning robust visual features without supervision. CoRR, abs/2304.07193, 2023. doi: 10.48550/ARXIV.2304.07193. Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012, pp. 34983505. IEEE Computer Society, 2012. doi: 10.1109/CVPR.2012.6248092. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E. Z., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H. M., Larochelle, H., Beygelzimer, A., dAlché-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 80248035, 2019. 15 Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. CoRR, 2019. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T. P., Alayrac, J., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., Antonoglou, I., Anil, R., Borgeaud, S., Dai, A. M., Millican, K., Dyer, E., Glaese, M., Sottiaux, T., Lee, B., Viola, F., Reynolds, M., Xu, Y., Molloy, J., Chen, J., Isard, M., Barham, P., Hennigan, T., McIlroy, R., Johnson, M., Schalkwyk, J., Collins, E., Rutherford, E., Moreira, E., Ayoub, K., Goel, M., Meyer, C., Thornton, G., Yang, Z., Michalewski, H., Abbas, Z., Schucher, N., Anand, A., Ives, R., Keeling, J., Lenc, K., Haykal, S., Shakeri, S., Shyam, P., Chowdhery, A., Ring, R., Spencer, S., Sezener, E., and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. doi: 10.48550/ARXIV.2403.05530. Rivière, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., Ferret, J., Liu, P., Tafti, P., Friesen, A., Casbon, M., Ramos, S., Kumar, R., Lan, C. L., Jerome, S., Tsitsulin, A., Vieillard, N., Stanczyk, P., Girgin, S., Momchev, N., Hoffman, M., Thakoor, S., Grill, J., Neyshabur, B., Bachem, O., Walton, A., Severyn, A., Parrish, A., Ahmad, A., Hutchison, A., Abdagic, A., Carl, A., Shen, A., Brock, A., Coenen, A., Laforge, A., Paterson, A., Bastian, B., Piot, B., Wu, B., Royal, B., Chen, C., Kumar, C., Perry, C., Welty, C., Choquette-Choo, C. A., Sinopalnikov, D., Weinberger, D., Vijaykumar, D., Rogozinska, D., Herbison, D., Bandy, E., Wang, E., Noland, E., Moreira, E., Senter, E., Eltyshev, E., Visin, F., Rasskin, G., Wei, G., Cameron, G., Martins, G., Hashemi, H., Klimczak-Plucinska, H., Batra, H., Dhand, H., Nardini, I., Mein, J., Zhou, J., Svensson, J., Stanway, J., Chan, J., Zhou, J. P., Carrasqueira, J., Iljazi, J., Becker, J., Fernandez, J., van Amersfoort, J., Gordon, J., Lipschultz, J., Newlan, J., Ji, J., Mohamed, K., Badola, K., Black, K., Millican, K., McDonell, K., Nguyen, K., Sodhia, K., Greene, K., Sjösund, L. L., Usui, L., Sifre, L., Heuermann, L., Lago, L., and McNealus, L. Gemma 2: Improving open language models at practical size. CoRR, abs/2408.00118, 2024. doi: 10.48550/ARXIV.2408.00118. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 87328740. AAAI Press, 2020. doi: 10.1609/AAAI. V34I05.6399. Sap, M., Rashkin, H., Chen, D., Bras, R. L., and Choi, Y. Socialiqa: Commonsense reasoning about social interactions. CoRR, abs/1904.09728, 2019. Schmied, T., Hofmarcher, M., Paischer, F., Pascanu, R., and Hochreiter, S. Learning to modulate pre-trained models in RL. In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. Schölkopf, B., Smola, A., and Müller, K.-R. Kernel principal component analysis. In Gerstner, W., Germond, A., Hasler, M., and Nicoud, J.-D. (eds.), Artificial Neural Networks ICANN97, pp. 583588, Berlin, Heidelberg, 1997. Springer Berlin Heidelberg. ISBN 978-3-540-69620-9. Sun, M., Chen, X., Kolter, J. Z., and Liu, Z. Massive activations in large language models. In First Conference on Language Modeling, 2024. Sung, Y., Nair, V., and Raffel, C. Training neural networks with fixed sparse masks. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 2419324205, 2021. Todorov, E., Erez, T., and Tassa, Y. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 50265033. IEEE, 2012. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and 16 efficient foundation language models. CoRR, abs/2302.13971, 2023a. doi: 10.48550/ARXIV.2302. 13971. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. Valipour, M., Rezagholizadeh, M., Kobyzev, I., and Ghodsi, A. Dylora: Parameter-efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. In Vlachos, A. and Augenstein, I. (eds.), Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pp. 32663279. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EACL-MAIN.239. Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and Welling, M. Rotation equivariant cnns for digital pathology. In Frangi, A. F., Schnabel, J. A., Davatzikos, C., Alberola-López, C., and Fichtinger, G. (eds.), Medical Image Computing and Computer Assisted Intervention - MICCAI 2018 - 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II, volume 11071 of Lecture Notes in Computer Science, pp. 210218. Springer, 2018. doi: 10.1007/978-3-030-00934-2_24. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. Wołczyk, M., Zaj ac, M., Pascanu, R., Kucinski, Ł., and Miłos, P. Continual world: robotic benchmark for continual reinforcement learning. Advances in Neural Information Processing Systems, 34:2849628510, 2021. Wolczyk, M., Zajkac, M., Pascanu, R., Kucinski, L., and Milos, P. Continual world: robotic benchmark for continual reinforcement learning. Advances in Neural Information Processing Systems, 34:2849628510, 2021. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. SUN database: Large-scale scene recognition from abbey to zoo. In The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010, San Francisco, CA, USA, 13-18 June 2010, pp. 34853492. IEEE Computer Society, 2010. doi: 10.1109/CVPR.2010.5539970. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pp. 10941100. PMLR, 2020. 17 Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M., Djolonga, J., Pinto, A. S., Neumann, M., Dosovitskiy, A., Beyer, L., Bachem, O., Tschannen, M., Michalski, M., Bousquet, O., Gelly, S., and Houlsby, N. The visual task adaptation benchmark. CoRR, abs/1910.04867, 2019. Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a. Zhang, Z., Liu, B., and Shao, J. Fine-tuning happens in tiny subspaces: Exploring intrinsic taskspecific subspaces of pre-trained language models. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17011713, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.95. Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. Opencodeinterpreter: Integrating code generation with execution and refinement. https://arxiv.org/abs/2402.14658, 2024. Zi, B., Qi, X., Wang, L., Wang, J., Wong, K., and Zhang, L. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices. CoRR, abs/2309.02411, 2023. doi: 10.48550/ ARXIV.2309.02411. Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., Vuong, Q., Vanhoucke, V., Tran, H. T., Soricut, R., Singh, A., Singh, J., Sermanet, P., Sanketi, P. R., Salazar, G., Ryoo, M. S., Reymann, K., Rao, K., Pertsch, K., Mordatch, I., Michalewski, H., Lu, Y., Levine, S., Lee, L., Lee, T. E., Leal, I., Kuang, Y., Kalashnikov, D., Julian, R., Joshi, N. J., Irpan, A., Ichter, B., Hsu, J., Herzog, A., Hausman, K., Gopalakrishnan, K., Fu, C., Florence, P., Finn, C., Dubey, K. A., Driess, D., Ding, T., Choromanski, K. M., Chen, X., Chebotar, Y., Carbajal, J., Brown, N., Brohan, A., Arenas, M. G., and Han, K. RT-2: vision-language-action models transfer web knowledge to robotic control. In Tan, J., Toussaint, M., and Darvish, K. (eds.), Conference on Robot Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, volume 229 of Proceedings of Machine Learning Research, pp. 21652183. PMLR, 2023."
        },
        {
            "title": "Supplementary Material",
            "content": "Fabian Paischer1* Benedikt Alkin1,3 Thomas Schmied1 Lukas Hauzenberger1* Sepp Hochreiter1,3 Marc Peter Deisenroth2 1 ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria 2 University College London 3 NXAI GmbH, Linz, Austria paischer@ml.jku.at"
        },
        {
            "title": "Contents",
            "content": "A Reproducibility Statement Natural language generation B.1 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Natural language understanding C.1 Dataset Statistics . C.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Image Classification D.1 Dataset statistics . . D.2 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Decision Making E.1 Dataset statistics . . E.2 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Hyperparameter search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Additional results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . SVD convergence analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.1 Batch Size invariance . F.2 Excluding ignored tokens for SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . Rank re-distribution analysis 19 20 20 20 20 22 22 22 22 24 25 25 30 30 31 31 31 31 33 33"
        },
        {
            "title": "A Reproducibility Statement",
            "content": "We provide the source code to reproduce our experiments for language generation and understanding, as well as our RL experiments at https://github.com/ml-jku/EVA. For image classification we used custom implementations that are available at https://github.com/ BenediktAlkin/vtab1k-pytorch. We also integrated EVA into the widely used PEFT library (see https://github.com/sirluk/peft). minimal example for fine-tuning any model available on the huggingface hub can be found at https://github.com/sirluk/peft/blob/main/ examples/eva_finetuning/eva_finetuning.py."
        },
        {
            "title": "B Natural language generation",
            "content": "We follow the experiments conducted in Hu et al. (2023) and fine-tune Llama-2-7B, Llama-3.1-8B and Gemma-2-9B on 8 common sense reasoning tasks with qa style prompts. We keep the original prompt templates unchanged aside from two minor modifications: For BoolQ we prepend the the passage field before the question and for WinoGrande we add line \"Answer format: ...\" analogous to the other prompts. As done by Hu et al. (2023) as well as Liu et al. (2024a) we perform joint finetuning on all 8 tasks. We furthermore evaluate the pre-trained models mentioned above on the mathematical reasoning tasks GSM8K (Cobbe et al., 2021) and Math (Yu et al., 2024) after finetuning on MetaMathQA (Yu et al., 2024) as done in Meng et al. (2024). We keep the original prompt template for finetuning and evaluation. For all datasets we run finetuning for one epoch. B.1 Implementation details For finetuning our code base leverages peft implementations of adapter methods LoRA, AdaLoRA, PiSSA, OLoRA and DoRA. The initialization step for EVA is custom implementation but for finetuning we can reformulate EVA as LoRA adapter leveraging the rank_pattern argument of peft.LoraConfig. For evaluation we leverage scripts provided by the MetaMath github repository (Yu et al., 2024) for math reasoning tasks. For common sense reasoning we make use of the lm evaluation harness project (Gao et al., 2024) and define custom tasks using the finetuning prompts. For the SVD computation for joint finetuning on the common sense reasoning tasks we experiment with random and stratified sampling of examples from the 8 tasks and do not notice difference in performance. All training and evaluation runs for Llama-2-7B were done on 4 A100 GPUs. Runs for Llama-3.1-8B and Gemma-2-9B utilized two different nodes, one with 4 A100 GPUs and one with 4 H200 GPUs. B.2 Hyperparameter search The reported results on language generation tasks in Table 2 and Table 3 are the best setting based on grid search over different learning rates. We apply adapters to all linear layers including the language modelling head. Furthermore we set α = 1 for all our experiments. We use AdamW with weight decay and linear learning rate schedule with warm-up. We train for 1 epoch and use the final checkpoint for evaluation. All hyperparameters are summarized in Table 9 B.3 Additional results In addition to the results presented in Table 2 and Table 3 we also fine-tune Llama-2-7B on the Code-Feedback dataset Zheng et al. (2024) consisting of multi-turn conversations between user and AI Assistant. Due to limited computational resources and the long sequence lengths of the examples in this dataset we do not fine-tune Table 9: hyperparameters for finetuning on common sense reasoning and math reasoning Training Optimizer Weight Decay Lora Dropout Batch Size #Epoch LR Schedule Warmup ratio Label Smooth Learning Rate LoRA Dim LoRA α Batch Size SVD (EVA) τ cossim 0.99 Inference AdamW 0.0 0.0 32 1 Linear 0.03 0.0 5e-4 16 1 Beam Size Length Penalty repetition penalty 1.0 1.0 1.0 20 Table 8: Prompt templates with examples (red) used for finetuning on common sense and math reasoning tasks. Dataset BoolQ"
        },
        {
            "title": "SIQA",
            "content": "HellaSwag WinoGrande ARC-e & ARC-c OBQA Fine-tuning Data Template Passage: Drinking in public Drinking in public is most commonly accepted. After reading this passage, please answer the following question with true or false, question: can you drink on the street in china Answer format: true/false the correct answer is true Please choose the correct solution to the question: When boiling butter, when its ready, you can Solution1: Pour it onto plate Solution2: Pour it into jar Answer format: solution 1/solution2 the correct answer is solution2 Please choose the correct answer to the question: Carson relocated somewhere new. How would you describe Carson? Answer1: mobile Answer2: anxious Answer3: lonely Answer format: answer1/answer2/answer3 the correct answer is answer1 Please choose the correct ending to complete the given sentence: Playing drums: People are standing behind large drums. man Ending1: is playing bag pipe. Ending2: starts to play around the drums. Ending3: begins playing drum set. Ending4: begins playing the drums. Answer format: ending1/ending2/ending3/ending4 the correct answer is ending4 Please choose the correct answer to fill in the blank to complete the given sentence: Ian volunteered to eat Denniss menudo after already having bowl because _ despised eating intestine. Option1: Ian Option2: Dennis Answer format: option1/option2 the correct answer is option2 Please choose the correct answer to the question: Which factor will most likely cause person to develop fever? Answer1: leg muscle relaxing after exercise Answer2: bacterial population in the bloodstream Answer3: several viral particles on the skin Answer4: carbohydrates being digested in the stomach Answer format: answer1/answer2/answer3/answer4 the correct answer is answer2 Please choose the correct answer to the question: The sun is responsible for Answer1: puppies learning new tricks Answer2: children growing up and getting old Answer3: flowers wilting in vase Answer4: plants sprouting, blooming and wilting Answer format: answer1/answer2/answer3/answer4 the correct answer is answer4 MetaMathQA Below is an instruction that describes task. Write response that appropriately completes the request. ### Instruction: What is the value of the cosine of 90 degrees? ### Response: $boxed{0}$.The answer is: 0 21 Llama-3.1-8B and Gemma-2-9B or any DoRA variants. We evaluate the fine-tuned checkpoints on four coding benchmarks: MBPP Austin et al. (2021), HumanEval Chen et al. (2021b), MBPP+ and HumanEval+ Liu et al. (2023). The results are presented in Table 10. EVA shows the best performance on MBPP and MBPP+ while also exhibiting good performance on HumanEval and HumanEval+. On the latter two datasets, PiSSA is the best performing method. For finetuning we use maximum sequence length of 2028 with right-side truncation. For decoding we set the temperature to 0.2 and top_p to 0.7 Table 10: Comparison of EVA to other initialization and rank re-distribution schemes on code finetuning datasets. We report mean and standard deviation across three random seeds. Method MBPP HumanEval MBPP+ HumanEval+ 22.21.1 LoRA AdaLoRA 21.50.2 22.81.2 PiSSA 22.30.6 OLoRA 22.90.7 EVA 18.90.6 17.10.0 19.90.9 18.90.0 18.91.2 30.71.1 29.40.7 30.80.7 32.40.4 32.60.6 18.90.6 17.10.0 19.90.9 18.90.0 18.91. In Table 11 we report the standard deviation across three seeds from the results in Table 2. For Llama-3.1-8B and Gemma-2-9B EVA has the smallest average standard deviation across tasks. For Llama-2-7B the standard the variance of EVA is only slightly above average in comparison to other methods, mainly due to the high standard deviation on the BoolQ dataset."
        },
        {
            "title": "C Natural language understanding",
            "content": "C.1 Dataset Statistics The dataset statistics for each task in the GLUE benchmark (Wang et al., 2019) are shown in Table 12. Generally, GLUE contains four low-resource datasets (RTE, MRPC, STS-B, and CoLA) and four high resource datasets (SST-2, QNLI, QQP, MNLI). While CoLA and SST-2 rely on single sentence classification, STS-B evaluates for similarity and the remaining tasks are based on pairwise text classification. C.2 Implementation Details We base our implementation on the codebase of LoRA1. For these experiments, we initially precompute our initialization prior to the fine-tuning stage and store it as checkpoint. However, we also provide the possibility to directly compute the initialization during the fine-tuning stage, as done for our experiments on VTAB-1k and Meta-World. By default, we always offload the computation of the initial checkpoint to CPU to save VRAM. We ran all our experiments on nodes with four A100 GPUs and used PyTorchs data-distributed parallel functionality (Paszke et al., 2019). Runtimes ranges from as little as 10 minutes per run for smaller datasets (RTE, STS-B) to around 15 hours for the largest datasets (QQP, MNLI). C.3 Hyperparameter search For LoRA and EVA, we search over the number of ranks {2, 4, 6, 8} and different learning rates η {1e 3, 4e 4, 1e 4} for RoBERTaLarge and η {4e 3, 1e 3, 4e 4} for DeBERTav3Base. We report the best hyperparameter settings for both, RoBERTaLarge and DeBERTav3Base for LoRA and EVA in Table 13. For AdaLoRA, we search over the same ranks and always start initial ranks with + 4 that are then redistributed during training. Additionally, we search over the same learning rates as for the other LoRA variants. Further, we introduce hyperparameters that result in additional speed-up of our initialization, namely threshold τ that considers components as converged, and threshold δ that stops computation of the initialization when certain percentage of components 1https://github.com/microsoft/LoRA 22 Table 11: Standard deviation across three seeds on common sense reasoning tasks. Model Method BoolQ PIQA SIQA HellaSwag Winogrande ARC-e ARC-c OBQA Llama-2-7B Llama-3.1-8B Gemma-2-9B 1.498 0.252 0.233 LoRA 1.315 0.251 0.182 AdaLoRA 0.358 0.294 0.138 PiSSA 4.938 0.190 0.524 OLoRA 3.858 0.336 0.210 EVA DoRA 2.599 0.290 0.483 EVA+DoRA 5.281 0.273 0.293 0.472 0.194 0.419 LoRA 0.510 0.044 0.261 AdaLoRA 6.516 0.373 0.603 PiSSA 0.298 0.245 0.397 OLoRA 0.109 0.320 0.125 EVA DoRA 0.225 0.112 0.315 EVA+DoRA 0.225 0.168 0. 0.095 0.277 0.386 LoRA 0.088 0.353 0.217 AdaLoRA 2.761 0.286 0.214 PiSSA 0.066 0.451 0.501 OLoRA 0.275 0.136 0.111 EVA DoRA 0.189 0.420 0.301 EVA+DoRA 0.132 0.296 0.490 0.102 0.098 0.096 0.062 0.059 0.113 0.034 0.070 0.040 0.195 0.057 0.022 0.014 0.117 0.062 0.033 0.109 0.099 0.094 0.074 0.070 0.658 0.392 0.298 0.652 0.453 0.244 0.853 0.197 0.392 0.707 0.451 0.591 0.260 0. 0.324 0.098 0.621 0.501 0.260 0.419 0.037 0.072 0.362 0.386 0.339 0.221 0.215 0.110 0.052 0.201 0.325 0.173 0.110 0.119 0.105 0.072 0.209 0.447 0.267 0.119 0.091 0.150 0.489 0.106 0.494 0.672 0.358 0.489 0.494 0.563 0.804 0.245 0.329 0.241 0.698 0. 0.070 0.106 0.121 0.448 0.040 0.000 0.715 0.822 0.899 1.117 0.660 0.189 0.525 0.249 0.189 0.748 0.589 0.189 0.189 0.000 0.249 0.589 0.432 0.163 0.573 0.249 0.499 0.340 Table 12: GLUE benchmark suite statistics and evaluation metric for each corpus sorted by the number of examples in the training set. Corpus #Train #Dev #Test Metric RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI 2.5 3.7 7 8.5 67 108 364 393 276 408 1.5 1 872 5.7 40 20 Accuracy Accuracy Pearson correlation 3 1.7 1.4 1 Matthews correlation 1.8 5.7 391 20 Accuracy Accuracy Accuracy Accuracy have converged. By default, we set τ = 0.99 and δ = 1, i.e. we only stop when all components are converged, and they are almost exactly the same. These parameters provide additional leeway to speed up the initialization stage of EVA. We have explored the sensitivity of LoRA to different initialization schemes and found that, similar to other prominent initialization schemes (He et al., 2015; Glorot & Bengio, 2010), scale plays an important role along with directions. Originally, (Hu et al., 2022) propose to set α = 2r, however, we found that this parameter is quite sensitive as also shown in (Kalajdzievski, 2023). Similarly, different ranks lead to very different results on different downstream tasks. Therefore, we suggest to always search over more ranks and choose the best performing one if the required compute budget is available. We also experimented with different learning rates for the and matrices as proposed in (Hayou et al., 2024b), however, this did not result in consistent improvements. Instead, we found that learning rates for LoRA-style training can be surprisingly high (4e 3 for DeBERTav3Base), while for larger models the learning rate needs to be approximately magnitude smaller. simple recipe that worked consistently well, was setting α = 1, which results in similar scaling factor as in Kalajdzievski (2023), and searching over set of small learning rates for larger models and higher learning rates for smaller ones. For EVA, the only tunable hyperparameter is the rank budget, which we recommend to tune along with the fine-tuning learning rate. 23 Table 13: The best hyperparameters RoBERTaLargeand DeBERTav3Basethat were found via gridsearch for each task of the GLUE benchmark. Method Dataset MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B RoBERTaLarge LoRA RoBERTaLarge EVA DeBERTav3Base LoRA DeBERTav3Base EVA Optimizer Warmup Ratio LR Schedule Batch Size # Epochs LoRA rank Learning rate LoRA α Max Seq. Len. DDP GPUs Batch Size # Epochs LoRA rank Learning rate LoRA α Max Seq. Len. DDP GPUs Batch Size # Epochs LoRA rank Learning rate LoRA α Max Seq. Len. DDP GPUs Batch Size # Epochs LoRA rank Learning rate LoRA α Max Seq. Len. DDP GPUs 8 10 2 4e-4 8 10 2 4e-4 32 30 8 4e-4 32 30 8 4e16 10 8 1e-3 16 10 2 1e-3 32 60 4 1e-3 32 60 2 4e-4 8 20 8 4e-4 8 20 4 4e16 30 4 4e-3 16 30 4 4e-3 AdamW 0.06 Linear 8 10 8 1e-3 8 20 4 1e-3 16 20 2 1e8 10 2 4e-4 8 10 16 4e-4 8 20 8 1e-3 16 20 4 1e-3 8 10 4 1e-3 64 25 16 4e32 25 4 4e-3 32 80 4 4e-3 16 40 8 4e-3 64 25 16 4e-3 32 25 4 4e-3 32 80 2 4e16 40 2 4e-3 8 20 4 1e-3 1 512 4 8 20 2 1e-3 1 512 4 32 80 8 4e-3 1 512 4 32 80 8 4e-3 1 512 4 C.4 Additional results We report additional results for EVA compared to LoRA for different rank budgets in Table 14. We find that EVA consistently outperforms LoRA for different rank budgets. This demonstrates the effectiveness of EVA among different compute budgets. Further, we show additional rank redistributions for the CoLA, MRPC, RTE, and STSB tasks for different for = 2 (Figure 4), = 4 (Figure 5), = 8 (Figure 6), and = 16 (Figure 7) for both, RoBERTaLarge and DeBERTav3Base. The distributions for the different models show different patterns. For DeBERTav3Base the higher attention layers usually receive more ranks than lower ones. For CoLA, there is also high number of ranks in the very first layer. For RoBERTaLarge it seems to be the opposite, as the very first layers consistently receive more ranks compared to later layers. There is also notable difference across tasks for both models, which demonstrates the flexibility of EVA to allocate ranks dependent on the downstream task. Interestingly, for higher initial rank (r = 16), the redistribution for DeBERTav3Base puts more emphasis on fine-tuning the self-attention specific weight matrices. This is not true for RoBERTaLarge, as Wf 1 also receives plenty of ranks across all tasks. Overall, the rank redistribution incurs different fine-tuning paradigms depending on the task and the initial rank. Additionally, we show results for different rank redistributions that we obtain by using alternative measures for explained variance. Specifically, we compare EVA to using, (i), the raw eigenvalues (EVA-Raw), and (ii), normalizing by the maximum eigenvalue (EVA-Max). We report results for RoBERTaLarge on four of the GLUE tasks, namely CoLA, RTE, MRPC, and STS-B in Table 15. Our 24 Table 14: Comparison of LoRA to EVA using RoBERTaLarge on all tasks from GLUE for equal rank budgets. Mean and standard deviation of Matthews correlation for CoLA, pearson correlation for STS-B, and accuracy for remaining datasets on the development set across 5 seeds are shown. Method CoLA MRPC RTE STS-B MNLI QNLI QQP SSTAvg LoRAr=2 EVAr=2 LoRAr=4 EVAr=4 LoRAr=8 EVAr=8 LoRAr=16 EVAr=16 68.01.4 69.11.4 69.1.5 69.51.4 68.81.0 69.01.4 68.41.0 69.1. 90.9.8 90.8.5 90.7.7 91.4.8 91.1.6 91.1.4 90.5.5 91.2.8 88.11.1 88.2.7 86.9.2 88.81.3 87.10.7 88.4.6 88.0.5 88.0.5 92.3.1 92.5.1 92.3.1 92.6.1 92.2.2 92.6.3 92.3.1 92.6.2 91.9.1 90.8.1 90.6.1 90.7.0 90.6.2 90.6.1 90.6.1 90.7.0 94.8.3 94.9.1 94.7.2 94.9.1 94.8.1 94.9.1 94.8.1 95.0.2 90.6.1 91.9.1 92.0.0 91.8.0 91.8.0 92.1.1 91.9.1 91.8. 96.1.1 96.2.1 96.0.1 96.1.1 96.2.3 96.1.2 96.1.1 96.2.1 89.09 89.30 89.04 89.48 89.08 89.35 89.08 89.33 Table 15: Comparison of LoRA to EVA, EVA-Raw, and EVA-Max for RoBERTaLargeon the GLUE tasks CoLA, MRPC, RTE, and STS-B. We report mean and standard deviation of Matthews correlation for CoLA, pearson correlation for STS-B, matched accuracy for MNLI, and accuracy for remaining tasks across 5 seeds. Method CoLA MRPC RTE STS-B LoRA EVA EVA-Raw EVA-Max 69.1.5 69.51.4 69.41.1 69.10.5 91.10.6 91.40.8 91.00.9 91.20.5 88.11.1 88.81.2 88.20.3 88.41.2 92.30.1 92.60.1 92.50.2 92.50.2 Avg 85.2 85.6 85.3 85. results show that while EVA-Raw and EVA-Max slighthly improve upon LoRA, they perform worse on average than EVA."
        },
        {
            "title": "D Image Classification",
            "content": "D.1 Dataset statistics The VTAB-1K benchmark consists of 19 datasets, each containing subset of 1000 examples of their respective samples. We summarize the dataset statistics for each dataset in Table 16. While the original train sizes of the datasets vary drastically, the 1K subset provides equal datasets across tasks. The number of classes also varies from as little as two to almost 400. D.2 Implementation details We implemented custom pipeline to fine-tune DINOv2-L/14 on VTAB-1K that supports LoRA, DoRA and EVA. To train AdaLora, PiSSA and OLoRA, we integrate their implementation from the peft library (Mangrulkar et al., 2022) into our pipeline. This pipeline is designed to be highly parallelizable and to be executed on individual GPUs. single evaluation run of L/14 model (all 19 datasets with hyperparameter tuning and evaluation) takes roughly 160 A100 GPU-hours but can be easily parallelized. g/14 run takes roughly 140 H100 GPU-hours. single evaluation run consists of 1140 hyperparameter tuning runs (19 datasets * 5 learning rates * 4 ranks * 3 seeds) and 95 evaluation runs (19 datasets * 5 seeds). Details to hyperparameter tuning are described below. We use the original DINOv2 models (Oquab et al., 2023) and train classification head on top of the [CLS] token, where we initialize the classification head weights with normal distribution with σ = 2e-5 and bias with zeros. We train the classification head, LoRA matrices and biases. Images are resized to 224 224 resolution with bi-cubic interpolation and normalized with the per-channel mean and variance of ImageNet. We train all models in bfloat16 precision using the AdamW optimizer with weight decay of 0.05 for 30 epochs. We use cosine learning rate schedule with linear warm-up 25 Figure 4: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark (CoLA, MRPC, RTE, STSB) for DeBERTav3Base (left) and RoBERTaLarge (right) with initial rank = 2. 26 Figure 5: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark (CoLA, MRPC, RTE, STSB) for DeBERTav3Base (left) and RoBERTaLarge (right) with initial rank = 4. 27 Figure 6: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark (CoLA, MRPC, RTE, STSB) for DeBERTav3Base (left) and RoBERTaLarge (right) with initial rank = 8. 28 Figure 7: Rank distribution after initialization with EVA on four tasks of the GLUE benchmark (CoLA, MRPC, RTE, STSB) for DeBERTav3Base (left) and RoBERTaLarge (right) with initial rank = 16. 29 Table 16: Category, train size and classes of the VTAB-1K dataset. Dataset Caltech101 (Fei-Fei et al., 2006) CIFAR-100 (Krizhevsky, 2009) DTD (Cimpoi et al., 2014) Flowers102 (Nilsback & Zisserman, 2008) Pets (Parkhi et al., 2012) Sun397 (Xiao et al., 2010) SVHN (Netzer et al., 2011) Category Natural Natural Natural Natural Natural Natural Natural Specialized EuroSAT (Helber et al., 2019) Specialized Resisc45 (Cheng et al., 2017) Specialized Specialized Retinopathy (Kaggle & EyePacs, 2015) Structured Structured Structured Structured Structured Structured Structured Structured Clevr/count (Johnson et al., 2017) Clevr/distance (Johnson et al., 2017) dSprites/location (Matthey et al., 2017) dSprites/orientation (Matthey et al., 2017) SmallNORB/azimuth (LeCun et al., 2004) SmallNORB/elevation (LeCun et al., 2004) DMLab (Beattie et al., 2016) KITTI/distance (Geiger et al., 2013) Patch Camelyon (Veeling et al., 2018) Train size Classes 102 100 47 102 37 397 10 10 45 2 5 8 6 16 16 18 9 6 4 3060 50000 3760 2040 3680 87003 73257 21600 25200 294912 46032 70000 70000 663552 663552 36450 36450 88178 5711 Table 17: Standard deviations for the VTAB-1K results  (Table 5)  over 5 seeds. Natural Specialized Structured 0 0 1 i 1 0 1 t D 2 0 1 o t H 7 9 3 n e T r 5 4 s h o e u - l i - l b D D - I L - d O - d - A N E - N a A 1.5 1.1 1.6 0.0 0.4 1.2 0.9 14.9 0.4 0.6 2.7 1.7 0.9 1.2 23.6 0.5 0.4 1.6 1.9 3.0 FFT 0.2 0.4 0.2 0.0 0.3 36.4 0.1 0.5 0.3 0.1 0.4 0.2 0.3 0.5 1.2 0.4 0.4 0.7 0.4 2.3 LoRA AdaLoRA 0.0 0.2 0.4 0.0 0.1 0.4 0.1 0.3 0.3 0.2 0.3 0.3 0.2 0.3 0.8 0.8 0.3 0.3 0.4 0.3 0.2 0.4 0.3 0.0 0.2 0.5 0.2 0.7 0.2 0.1 0.4 0.3 0.4 0.2 0.7 0.3 0.5 0.4 0.5 0.3 PiSSA 0.3 0.3 0.4 0.0 0.3 29.4 0.1 0.3 0.1 0.2 0.2 0.5 0.1 0.3 24.6 0.3 0.4 0.3 0.8 3.1 OLoRA 0.2 0.5 0.2 0.0 0.1 0.3 0.1 0.3 0.2 0.3 0.4 0.5 0.3 0.6 0.6 0.5 0.5 0.2 0.5 0.3 EVA 0.1 0.2 0.5 0.0 0.2 29.7 0.4 0.7 0.1 0.2 0.4 0.4 0.3 0.3 0.6 36.2 0.5 0.3 0.3 3.8 DoRA EVA+DoRA 0.2 1.3 0.6 0.0 0.3 0.5 0.3 0.4 0.2 0.3 0.3 0.4 0.4 12.8 1.3 2.5 0.3 0.6 0.6 1. for the first 3 epochs. Batch size is set to 64 where we use gradient accumulation if the batchsize does not fit into GPU memory. Full fine-tuning uses layer-wise lr decay (Clark et al., 2020) of 0.75. D.3 Hyperparameter search We first fine-tune on the 800 train samples of VTAB-1K datasets to find the best learning rate for the task. We sweep over learning_rate {2.5e-3, 1e-3, 7.5e-4, 5e-4, 2.5e-4} and rank {2, 4, 8, 16} and average the accuracy on the 200 validation samples over 3 different seeds to choose the best learning rate and rank for each dataset. For evaluation, we train on the union of train and validation set using 5 different seeds and report the average accuracy on the test set. D.4 Additional results To complement our main results in Table 5, we report the respective standard deviations in Table 17."
        },
        {
            "title": "E Decision Making",
            "content": "E.1 Dataset statistics Meta-World (Yu et al., 2020) is an established benchmark in RL for multi-task continuous control. The benchmark consists of 50 challenging robotics tasks simulated using Sawyer robotic arm in the MuJoCo physics engine (Todorov et al., 2012). All 50 tasks in Meta-World share the same underlying robotic arm. Therefore, all tasks share common state (39-dimensional continuous vector) and action-space (6-dimensional). The reward functions in Meta-World are dense and based on the distance of the robotic arm to the goal location or objects. All episodes last for 200 environment interactions. For our experiments on Meta-World, we leverage the datasets released by Schmied et al. (2023). We follow Wołczyk et al. (2021) and Schmied et al. (2023), and split the 50 tasks into 40 pre-training tasks (MT40) and 10 fine-tuning tasks (CW10). The CW10 tasks are: hammer-v2, stick-pull-v2, stick-pull-v2, handle-press-side-v2, push-v2, shelf-place-v2, window-close-v2, and peg-unplug-side-v2. faucet-close-v2, push-wall-v2, push-back-v2, The datasets contain 2M transitions for every of the 50 tasks, amounting to 80M transitions (320M tokens) across all training tasks. The average success rate and rewards across all MT40 tasks are 84% and 1414.62, respectively. We list the statistics per task in Table 18. E.2 Implementation details We implemented our pipeline that supports training for Meta-World on top of the code-base provided by Schmied et al. (2023). Our custom implementation supports training LoRA, DoRA and EVA. Furthermore, we leverage the peft library (Mangrulkar et al., 2022) to train the remaining methods. For our experiments on Meta-World, we use GPT2-like network architecture (Radford et al., 2019) with 4 Transformer layers, 8 heads, and hidden dimension of 512 resulting in 16M parameters. We use context of 50 time steps, which amounts to sequence length of 200, as each timestep contains states, actions, rewards and RTGs. We embed states, actions, rewards and return-to-gos (RTGs) using separate linear embedding layers per modality, as proposed by Chen et al. (2021a). We train with batch size of 128 using constant learning rate of 1e4, 4000 linear warm-up steps followed by cosine decay to 1e6, using the AdamW optimizer (Loshchilov & Hutter, 2017). We employ gradient clipping of 0.25, weight decay of 0.01, and dropout rate of 0.2. Our DT implementation employs global position embedding. For every task, we set the target return to the maximum return achieved in the respective training datasets, as proposed by (Schmied et al., 2023). Furthermore, we employ mixed-precision (Micikevicius et al., 2017) and flash-attention (Dao, 2023) to speed-up training. We first pre-train DT on all MT40 tasks (80M transitions) for 1M updates via next-action prediction by minimizing the mean-squared error. The resulting pre-trained model attains an average success rate of 80% across all MT40 tasks. Then we fine-tune the DT on each of the CW10 down-stream tasks for 100K updates with the same set of hyperparameters as used for pre-training. We run all our experiments on public research cluster with 4xA100-40GB GPU nodes. single fine-tuning run with EVA for one task takes roughly 1 hour on one A100. E.3 Hyperparameter search In line with previous experiments, we tune the rank for LoRA, DoRA, AdaLora and EVA, rank {2, 4, 8, 16}. Further, we sweep over the same learning rates as for the GLUE tasks. E.4 Additional results In Table 19, we show the full comparison for all methods on CW10. EVA+DoRA consistently outperforms all competitors for the different rank budgets. 31 Table 18: Dataset statistics for all MT40 tasks from Schmied et al. (2023). Task A Success Rate Reward 0.0 0.9 0.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 0.6 0.8 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.9 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.1 1.0 1.0 1.0 1.0 1.0 1.0 0.4 1.0 1.0 1.0 1. 1206.9 1375.95 474.81 759.15 1299.24 1296.16 1430.44 1508.16 1499.17 1313.88 508.14 1674.29 1396.55 1535.4 1712.65 1544.32 1733.64 1845.92 1710.65 1727.98 1607.17 1854.79 1613.72 1581.75 1449.05 1545.19 1435.64 6.59 702.59 1766.24 1773.56 1663.35 1667.35 1858.99 1831.14 445.84 1470.71 1761.69 1458.35 1537.59 0.84 0.34 1414.62 439.39 assembly-v2 basketball-v2 bin-picking-v2 box-close-v2 button-press-topdown-v2 button-press-topdown-wall-v2 button-press-v2 button-press-wall-v2 coffee-button-v2 coffee-pull-v2 coffee-push-v2 dial-turn-v2 disassemble-v2 door-close-v2 door-lock-v2 door-open-v2 door-unlock-v2 drawer-close-v2 drawer-open-v2 faucet-open-v2 hand-insert-v2 handle-press-v2 handle-pull-side-v2 handle-pull-v2 lever-pull-v2 peg-insert-side-v2 pick-out-of-hole-v2 pick-place-v2 pick-place-wall-v2 plate-slide-back-side-v2 plate-slide-back-v2 plate-slide-side-v2 plate-slide-v2 reach-v2 reach-wall-v2 soccer-v2 stick-push-v2 sweep-into-v2 sweep-v2 window-open-v2 Average 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 39 - 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 - 32 Table 19: Rank-wise comparison for all methods on CW10. We fine-tune 12M DT on 10 tasks individually and report the mean success rates/rewards ( standard error) for every task. l - u r a 0.970.03 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 0.930.03 1.00.0 0.970.03 0.970.03 0.970.03 1.00.0 1.00.0 1.00.0 1.00.0 0.970.03 0.970.03 0.970.03 0.970.03 0.90.05 0.90.05 0.970.03 0.970.03 0.970.03 1.00.0 0.970.03 0.930.03 0.970.03 0.970.03 0.970.03 0.970.03 1.00.0 1.00.0 1.00.0 1.00.0 s - r - n 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 s - p - p 0.60.05 0.60.05 0.470.12 0.430.05 0.430.03 0.570.05 0.60.12 0.470.12 0.570.12 0.370.05 0.370.07 0.30.05 0.40.09 0.470.03 0.430.03 0.570.1 0.40.05 0.430.11 0.370.07 0.30.0 0.330.12 0.430.07 0.430.05 0.630.03 0.530.03 0.80.08 0.80.05 0.630.19 0.670.2 b - p p w - p l - h u - t 0.70.12 0.570.07 0.630.1 0.40.09 0.470.03 1.00.0 1.00.0 0.930.05 1.00.0 0.370.05 0.570.1 0.570.14 0.570.12 0.330.03 0.630.12 0.50.08 0.630.03 0.530.07 0.70.05 0.570.03 0.470.03 0.770.05 0.470.12 0.70.08 0.770.07 0.970.03 0.930.03 0.870.07 1.00.0 1.00.0 0.970.03 0.970.03 0.970.03 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 0.930.05 0.970.03 0.930.03 0.970.03 0.970.03 1.00.0 1.00.0 1.00.0 0.970.03 0.970.03 0.970.03 1.00.0 0.970.03 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 0.930.03 0.930.03 1.00.0 0.930.03 0.970.03 1.00.0 1.00.0 1.00.0 1.00.0 0.970.03 0.90.08 0.870.07 0.930.05 0.970.03 1.00.0 1.00.0 1.00.0 0.90.08 1.00.0 1.00.0 0.970.03 1.00.0 0.970.03 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 0.570.07 0.370.1 0.230.12 0.230.12 0.40.09 0.330.11 0.430.12 0.570.15 0.670.15 0.130.07 0.130.07 0.00.0 0.00.0 0.270.11 0.60.12 0.530.14 0.430.05 0.330.17 0.070.05 0.530.1 0.470.11 0.630.07 0.230.05 0.230.03 0.00.0 0.430.12 0.630.03 0.570.03 0.50.16 l - n 1.00.0 1.0.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 1.00.0 r 0.870.03 0.840.04 0.830.05 0.790.06 0.820.05 0.890.04 0.90.04 0.90.04 0.920.03 0.770.06 0.790.06 0.760.06 0.780.06 0.790.05 0.860.04 0.860.04 0.840.04 0.810.05 0.810.06 0.830.05 0.820.05 0.880.04 0.810.05 0.850.05 0.830.06 0.920.03 0.940.02 0.910.04 0.920.04 Method FFT LoRA DoRA AdaLoRA oLoRA PiSSa EVA EVA + DoRA Rank - 2 4 8 16 2 4 8 16 2 4 8 16 2 4 8 2 4 8 16 2 4 8 16 2 4"
        },
        {
            "title": "F SVD convergence analysis",
            "content": "F.1 Batch Size invariance We conduct an analysis on the convergence of the components obtained via SVD. Specifically, we investigate the difference in components according to cosine similarity across different batch sizes. Previously we have seen that the components obtained across different batch orderings are heavily correlated. In Figure 8 we visualize the cosine similarities between the SVD components for different batch sizes, namely 4, 8, 16, and 32 for Llama-2-7B on the MetaMathQA dataset. We observe that the components correlate strongly and remain mostly invariant to the batch size. This indicates that smaller batch sizes may be used for obtaining the initialization which results in less computational overhead. In the case of Llama-2-7B on MetaMathQA, this means that we can use batch size of 4 since it induces computational overhead of around 100 seconds. Afterwards we can continue the fine-tuning process with larger batch size. F.2 Excluding ignored tokens for SVD For some datasets we notice that masking out tokens for the SVD computation which are ignored for the loss calculation during finetuning can be advantageous. This can however result in significant reduction of the effective batch size for SVD if the number of completion tokens is small. An example where this is the case in our experiments are the common sense reasoning tasks which have long prompts but completion tokens are only one word per sample. This setting can lead to cases were SVD does not converge for lower batch sizes. We therefore do not mask out the prompt tokens for the common sense reasoning tasks, but apply masking for the math and coding tasks. For the multi-turn coding tasks  (Table 10)  , we found masking to significantly improve the performance. In this setup we mask out all tokens except for the assistant tokens. 33 Figure 8: Average cosine similarity between components obtained via SVD on minibatches of activation vectors across different batch sizes. The components strongly correlate indicating that the SVD computation is mostly invariant to the batch size and returns mostly the same components. Rank re-distribution analysis To illuminate the rank re-distribution process, we visualize the resulting ranks for each weight matrix after SVD for Llama-2-7B on the MetaMathQA dataset for different values of ρ. Setting ρ = 1 results in uniform rank distribution as in standard LoRA. However, setting ρ > 1 alters the number of ranks per weight matrix. In Figure 9 we visualize the number of ranks assigned to each weight matrix for different values of ρ > 1 and in Figure 10 we visualize the corresponding deltas. Both visualizations clearly illustrate that the most change occurs for values of ρ < 1.5. Setting ρ to higher values results in less and less change. Interestingly, some ranks still change when going from ρ = 2.5 to ρ = 3. Finally, we conduct hyperparameter search in which we search over different values of ρ {1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2, 2.5, 3}. We report the results in Figure 11. We find that for Llama-2-7B on MetaMathQA uniform distribution performs favorably. The second-best performance is shared by ρ = 1.5 and ρ = 2. Therefore, we always search for ρ = 1 and ρ = 2 for all our remaining experiments when we apply EVA and select the best performing one. 34 Figure 9: The resulting rank allocation per weight matrix in each layer for Llama-2-7B on the MetaMathQA dataset with different values of ρ. The first row represents uniform distribution where each weight matrix receives the same rank = 16. The most change occurs for ρ < 1.5. The re-distribution converges for larger values of ρ. Figure 10: Deltas between rank distributions per weight matrix in each layer for Llama-2-7B on the MetaMathQA dataset with different values of ρ. The first row represents uniform distribution where each weight matrix receives the same rank = 16. The most change occurs in the range ρ [1, 1.5]. Larger values of ρ do not induce additional significant changes to the rank distribution. Figure 11: Accuracy for different values of ρ when fine-tuning Llama-2-7B on the MetaMathQA dataset."
        }
    ],
    "affiliations": [
        "ELLIS Unit, LIT AI Lab, Institute for Machine Learning, JKU Linz, Austria",
        "NXAI GmbH, Linz, Austria",
        "University College London"
    ]
}