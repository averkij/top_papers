{
    "paper_title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
    "authors": [
        "Yijun Liang",
        "Ming Li",
        "Chenrui Fan",
        "Ziyue Li",
        "Dang Nguyen",
        "Kwesi Cobbina",
        "Shweta Bhardwaj",
        "Jiuhai Chen",
        "Fuxiao Liu",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 1 5 0 1 . 4 0 5 2 : r COLORBENCH: Can VLMs See and Understand the Colorful World? Comprehensive Benchmark for Color Perception, Reasoning, and Robustness Yijun Liang*, Ming Li*, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, Tianyi Zhou University of Maryland, College Park {yliang17,minglii,tianyi}@umd.edu Project: https://github.com/tianyi-lab/ColorBench"
        },
        {
            "title": "Abstract",
            "content": "Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces COLORBENCH, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating suite of diverse test scenarios, with grounding in real applications, COLORBENCH evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on COLORBENCH, while the language model plays more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on COLORBENCH but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our COLORBENCH can serve as foundational tool for advancing the study of human-level color understanding of multimodal AI. 1. Introduction Color is widely recognized as fundamental component of human visual perception [11, 33], playing critical role and providing critical clues in object detection, scene interpretation, contextual understanding, planning, etc., across *These authors contributed equally to this work. Figure 1. Evaluation of VLMs on COLORBENCH. The plot reports accuracy of 8 representative VLMs on 11 tasks across three categories (Perception, Reasoning, and Robustness). critical application scenarios such as scientific discovery, medical care, remote sensing, shopping, visualization, artwork interpretation, etc. For instance, [19] leverages spectral color signatures to distinguish vegetation, health, and water bodies in satellite imagery, and [1] utilizes sediment color patterns to detect marine ecosystems. These applications underscore how color-driven features play an important role in real-world scenarios. Moreover, colors can convey affective or semantic information beyond simply recognizing and naming colors since colors are highly correlated to other attributes or concepts and thus can provide key information to various downstream tasks that do not even directly ask about colors [18, 36, 43]. As modern vision-language models (VLMs) [12, 40, 46] continue to be deployed to increasingly diverse scenarios, it is essential to examine whether and how these models can understand and leverage color information as in human perception and reasoning, and their ability to interpret illusions, handle ambiguous cues, and maintain reliable performance despite variations in colors. However, existing benchmarks for VLMs mainly focus on tasks that may not heavily depend on color understanding or 1 Figure 2. Test samples from COLORBENCH. COLORBENCH evaluates VLMs across three core capabilities: Perception, Reasoning and Robustness. The benchmark comprises 11 tasks designed to assess fine-grained color understanding abilities and the effect of color on other reasoning skills, including counting, proportion calculation, and robustness estimation. With over 1,400 instance, COLORBENCH covers wide range of real-world application scenarios, including painting analysis, test kit readings, shopping, satellite/wildlife image analysis, etc. require color-centric reasoning, thereby overlooking nuanced color-related factors [25, 28]. Hence, there is lack of benchmarks that systematically assess how well VLMs understand color when it serves as the main or distinguishing feature of scene and key information to task. Moreover, robustness to variations in color, such as recoloring and shifting hues, has also been largely neglected in the LLM era [6, 8, 20]. Consequently, it remains unclear whether VLMs can perceive and reason about color with human-like proficiency and to what extent their performance deteriorates under significant color perturbations. This shortfall underscores the need for dedicated benchmark that comprehensively probes various facets of color comprehension in VLMs. To bridge this gap, we propose novel benchmark, COLORBENCH, that aims at comprehensively evaluating VLMs on three core capabilities of color understanding: Color Perception, Color Reasoning, and Color Robustness. Color Perception examines VLMs fundamental capability to correctly detect and interpret colors from inputs. Color Reasoning refers to the reasoning skills to draw further conclusions based on the understanding of colors from input and prior knowledge, in which colors act as crucial clue to formulate accurate judgments. Color Robustness assesses how consistently VLMs perform when an images colors are altered, ensuring they maintain accurate predictions across different color variants of an image. Under these three core dimensions, 11 fine-grained tasks assessing different aspects of color understanding capabilities are formulated as shown in Figure 2, which not only shows test examples in COLORBENCH but also presents potential real-world applications. By focusing on these facets, COLORBENCH offers granular view of VLMs capabilities in color understanding, aiming to illuminate both their strengths and shortcomings. We evaluate 32 widely used VLMs in our benchmark, ranging from open-source to proprietary models, from relatively small models (0.5B) to larger models (78B), and obtain some unrevealed observations. Figure 1 shows the results of 8 representative VLMs. Main Contribution. We introduce COLORBENCH, the first dedicated benchmark for assessing the color percep2 tion, reasoning, and robustness of VLMs. We develop an evaluation suite for 11 color-centric tasks, covering diverse application scenarios and practical challenges. Moreover, we report fine-grained empirical evaluation of 32 stateof-the-art VLMs, which exposes their limitations in color understanding and offers novel insights for future research. Our key findings are highlighted in the following: 1. The scaling law still holds for color understanding but is much weaker and mainly depends on the language model parts. The correlation between the performance and the vision encoders size is not significant due to the limited choices in current VLMs. 2. The absolute performances of different VLMs are relatively low, and the gaps between different models (opensource vs. proprietary, small vs. large) are not large, indicating the challenges of COLORBENCH and the negligence of color understanding in existing VLMs. 3. Despite the weaknesses of VLMs on color understanding, adding reasoning steps can still improve their performance on COLORBENCH tasks, even for color robustness, which has not been investigated by the community. 4. Color clues are indeed leveraged more or less by VLMs in most of the tasks in COLORBENCH. However, in color illusion and mimicry tasks, colors might mislead VLMs to give wrong answers, and converting colorful images into grayscale can improve the accuracy. 2. COLORBENCH Construction We present COLORBENCH, the first benchmark explicitly designed to comprehensively evaluate the color understanding capabilities of VLMs across three key dimensions: Color Perception, Color Reasoning, and Color Robustness. This benchmark consists of 1, 448 instances and 5, 814 image-text questions spanning 11 diverse tasks. For the Color Perception and Color Reasoning categories, each instance contains an image, question, and multiple-choice (3 to 6) options, with only one correct answer. For Color Robustness, each instance consists of 10 multiple-choice image-text questions including seed image and 9 edited images with color changes. 2.1. Taxonomy Motivated by the existing evaluation criteria from prior benchmarks and real-world application scenarios, we categorize the color understanding capability into 3 core dimensions and 11 detailed axes, as shown in Figure 2. The detailed question templates and sample cases are shown in Appendix B. 2.1.1. Color Perception This core dimension refers to the fundamental capability to correctly detect and interpret colors from inputs. We assess this capability through 3 key aspects: i) Color Recognition, ii) Color Extraction, and iii) Object Recognition. Figure 3. Statistics of 3 categories and 11 tasks in COLORBENCH. Color Recognition includes questions that either ask for the color of given object or determine whether specific color is present in the image. Color Extraction requires the model to extract the value of color code (e.g., RGB, HSV, or HEX) for given single color image. This task measures the ability to perform fine-grained color retrieval from visual input. Object Recognition evaluates the models capability to identify objects that match specified color described in the text input. These two tasks require VLMs to be able to detect and interpret the color in either the image or text input. 2.1.2. Color Reasoning This dimension refers to the reasoning skills to draw further conclusions based on the understanding of colors from input and prior knowledge, in which colors act as crucial clue to formulate accurate judgments. This category encapsulates 7 key aspects: i) Color Proportion, ii) Color Comparison, iii) Color Counting, iv) Object Counting, v) Color Illusion, vi) Color Mimicry and vii) Color Blindness. Color Proportion tests the models capability to estimate the relative area occupied by specific color. Questions in this task require both color perception and proportion calculation capabilities. Color Comparison requires the model to be able to distinguish among multiple colors in the image, assessing its sensitivity to hue, saturation, and brightness differences in visual input. Color Counting focuses on identifying the number of unique colors in the image, evaluating the models perception and differentiation of distinct color variations, and counting ability. Object Counting extends this challenge by requiring the model to count objects that match specific color pattern. This task requires an integration ability of object recognition and color perception. Color Illusion questions query VLMs to compare colors in potential illusionary environments. This task evaluates the models ability to account for color-induced optical illusions. Color Mimicry challenges the model to detect objects camouflaged within their surroundings, where color serves as misleading factor, requiring advanced pattern recognition 3 is firstly utilized to obtain the color histogram of the image. Questions and options are then manually designed based on these color statistics. For tasks including Color Extraction, Color Blindness, and Color Illusion, testing images are generated by corresponding code programs to ensure the controllability of the questions and answers. The detailed data sources are shown in Appendix A. After the initial data is collected, additional filtering processes are conducted in human-machine interactive process. We first conduct inference on variety of VLMs and discard those samples that are less challenging based on (i) model prediction correctness, (ii) model prediction confidence scores, and (iii) human evaluation. For synthesized data, similar processes are conducted but with additional code (for generation) and image assessment. The above process is conducted in three rounds before the final benchmark instances are settled. This refinement process ensures COLORBENCH rigorous and informative benchmark for assessing color-related understanding. For the Color Robustness, we evaluate the consistency of VLMs when faced with instances that differ only in the color of the visual input. To systematically assess this effect, we define 3 recoloring strategies that determine which part of the image is altered: i) Target Segment, ii) Largest Segment, and iii) Entire Image. As mentioned in Table 1, Target Segment strategy recolors only the segment containing the object referenced in the question. This strategy ensures that the modification directly affects the models perception of task-relevant content. Largest Segment strategy alters the color of the largest segment that is irrelevant to the question, testing whether models are distracted by dominant but unrelated visual changes. In contrast, Entire Image strategy applies global color shift to evaluate the models sensitivity to overall color variations. As summarized in Table 1, the first two strategies introduce localized modifications, while the third assesses robustness to broader image-wide color changes. By incorporating both task-relevant and irrelevant edits, our benchmark provides comprehensive evaluation of VLMs ability to handle color perturbations across different contexts. While generating color variations, we derive seed images from CV-Bench [41], publicly available benchmark. For each seed image, as shown in Figure 4, we first employ Grounded Segmentation Model (GAM) [37] to extract segments and their corresponding labels. We then apply the predefined recoloring strategies to determine the editing region. Once the editing region is determined, we modify the corresponding region by adjusting the Hue value in the HSV color space. Specifically, we shift the Hue by 90, 180, and 270. These three values ensure that the color manipulations cover significant perceptual differences across the color spectrum. These variations allow us to examine whether VLMs rely excessively on color information and whether they mainFigure 4. Generation Pipeline for Color Robustness. For each seed image, we apply 3 recoloring strategies (Entire Image, Target Segment, Largest Segment) to generate edited images. For each strategy, we change the color of the recoloring region via shifting the Hue values by 90, 180, or 270in HSV color space. and contextual reasoning. These two tasks both assess the models ability to make correct predictions under the misleading of color-related information in visual input. Color Blindness, inspired by Ishihara tests, assesses the models ability to recognize numbers or text embedded in color patterns, testing its understanding of shape-color relationships. These 7 tasks comprehensively assess the models capacity for logical reasoning, spatial awareness, and adaptive interpretation of color-based visual cues. 2.1.3. Color Robustness Color Robustness assesses how consistently VLMs perform and whether they can consistently deliver accurate predictions under color variants of given image. It involves measuring the stability of VLMs responses when confronted with the same text input and series of recolored images. To ensure that color does not influence the predictions, we select questions and corresponding answers that are independent of color attributes. Under these conditions, robust model should produce unchanged predictions regardless of recoloring manipulation. Any variation in the models responses is then used to quantify its susceptibility to color changes, providing direct measure of robustness. 2.2. Data Curation For most of the tasks in the category of Color Perception and Color Reasoning, we rely on human experts to manually collect images from multiple online benchmarks and websites. For the Color Proportion task, to ensure the correctness of the ground truth, an extra color extraction tool 4 Strategy Editing Region Purpose Entire Image Whole image Assesses the models robustness to global color shifts Target Segment Segment containing the object referenced in the question Evaluates the models sensitivity to task-relevant color changes Largest Segment The largest segment that is irrelevant to the question Tests whether changes in dominant but unrelated regions affect model predictions Table 1. Recoloring strategies. tain consistency in their predictions despite substantial color shifts. This process produces nine variations per seed image, covering different strategies and degrees of color change to enable comprehensive robustness assessment. To ensure interpretability, human experts filter out unnatural or negligible modifications, resulting in final selection of 493 seed images for robustness evaluation. 2.3. Evaluation Metrics For Perception and Reasoning, we use accuracy as the evaluation metric, as all tasks follow multiple-choice format. Accuracy is computed per task and per category, representing the proportion of correctly answered questions. For Robustness, we evaluate models ability to maintain consistent accurate predictions under color variations. As described in Section 2.2, each seed image Is is transformed into recolored versions using predefined recoloring strategies, while keeping the original question unchanged. model is considered robust on seed image Is and corresponding question if and only if it provides correct prediction for Is and maintains correct on all recolored versions. To quantify robustness, we define the instancelevel robustness metric R(Is, q) {0, 1} and model-level robustness metric RobustM [0, 1]. Instance-level Robustness. Let the recolored images be I1, , In and the generation output of model for image Ii and question is M(Ii, q). Define c(M(Ii, q)) as the model correctness: c(M(Ii, q)) = 1 if model result M(Ii, q) is correct, otherwise 0. The instance-level robustness metric R(Is, q) for seed image Is and question is defined as: (cid:40) R(Is, q) = 1 if c(M(Ii, q)) = c(M(Is, q)) = 1, [n] 0 otherwise (1) Overall Robustness. Let be the set of seed images. We define model robustness to be: RobustM = (cid:80) IsS R(Is) , RobustM [0, 1] (2) RobustM represents the proportion of seed images on which the model maintains correctness across all color variations. model is more robust when RobustM is higher. 3. Experimental 3.1. Implementation Details To further advance our understanding of VLMs capabilities in visual reasoning, we conduct an extensive evaluation of 32 vision-language models (VLMs) spanning range of large language model (LLM) sizes and architectures. Our evaluation includes state-of-the-art models such as GPT-4o[34], Gemini-2-flash[7], LLaVA-OV[24], LLaVA-NEXT [30], Cambrian[41], InternVL2[5], InternVL2.5[5], Qwen2.5VL[2], and Eagle[40]. This selection covers diverse set of architectures, including both proprietary and open-source models, enabling comprehensive assessment of their reasoning capabilities under different computational constraints. To ensure fair comparison, we standardize our experimental setup across models. Open-source models with fewer than 70B parameters are evaluated using single NVIDIA A100 80GB GPU, while larger models require four NVIDIA A100 80GB GPUs to accommodate their increased memory and computational demands. 3.2. Main Results Table 2 presents the performances of wide range of VLMs on our COLORBENCH. In general, according to the overall accuracy, larger models tend to perform better than small models, and the two proprietary models, GPT-4o and Gemini-2-flash, perform the best. Color Perception. In Color Recognition (CRecog), most models perform well (above 60%), indicating that this task is relatively basic for color perception. Gemini-2 without CoT obtains the highest performance, and GPT-4o does not stick to the top. In Color Extraction (CExtra), to our surprise, the two powerful proprietary models without CoT prompting only reach the middle-tier performances, indicating the potential limitation on the color perception of their vision encoders. Similar to the Color Existence task, almost all the models perform well in Object Recognition (ORecog), and the 2 proprietary models do not reach the top. This is probably due to the strong alignment between this task and the common training recipe, which includes abundant general object detection images. In Color Proportion (CProp), even Color Reasoning. the best model, Gemini-2 with CoT, can only reach 55.0% of the accuracy, which is almost only slightly better than random guessing, showcasing the supreme difficulty of this task. In Color Comparison (CComp), larger models perform better in this task, and the proprietary models with CoT reach the top performance unsurprisingly. Surprisingly, in Color Counting (CCount), all models show extremely poor performances. The highest performance comes from Gemini2 with CoT, exceeding the second place by more than 10 percent, although its performance is also unsatisfactory at only 45.1%. In Object Counting (OCount), surpassing the Color Perception Color Reasoning & Color Robustness CRecog CExtract ORecog CProp CComp CCount OCount CIllu CMimic CBlind Overall CRobust LLaVA-OV-0.5B InternVL2-1B InternVL2.5-1B InternVL2-2B InternVL2.5-2B Qwen2.5-VL-3B Cambrian-3B LLaVA-Next-v-7B LLaVA-Next-m-7B Eagle-X5-7B LLaVA-OV-7B Qwen2.5-VL-7B Cambrian-8B InternVL2-8B Eagle-X4-8B InternVL2.5-8B LLaVA-Next-13B Cambrian-13B Eagle-X4-13B InternVL2-26B InternVL2.5-26B Eagle-X5-34B Cambrian-34b LLaVA-Next-34b InternVL2.5-38B InternVL2-40B LLaVA-Next-72B LLaVA-OV-72B InternVL2-76B InternVL2.5-78B GPT-4o Gemini-2-flash GPT-4o (CoT) Gemini-2-flash (CoT) 26.3 35.5 55.3 60.5 69.7 72.4 67.1 29.0 21.1 52.6 71.1 76.3 72.4 72.4 71.1 77.6 56.6 67.1 73.7 72.4 72.4 79.0 75.0 69.7 71.1 72.4 72.4 73.7 72.4 75.0 73.7 80.3 76.3 79. 44.8 34.4 36.5 36.5 28.1 38.5 31.3 38.5 18.8 47.9 53.1 49.0 28.1 50.0 47.9 47.9 31.3 34.4 43.8 52.1 45.8 27.1 57.3 46.9 60.4 52.1 54.2 63.5 42.7 58.3 29.2 31.3 36.5 42. 46.8 59.7 61.0 66.2 71.4 74.0 66.2 57.1 63.6 67.5 81.8 84.4 72.7 77.9 68.8 83.1 71.4 74.0 76.6 87.0 89.6 80.5 77.9 76.6 89.6 83.1 79.2 83.1 85.7 81.8 84.4 83.1 85.7 83. 30.0 23.8 42.5 40.0 33.8 43.8 47.5 21.3 27.5 41.3 52.5 47.5 48.8 42.5 45.0 50.0 27.5 46.3 43.8 52.5 45.0 48.8 50.0 43.8 53.8 51.3 41.3 52.5 45.0 43.8 51.3 46.3 51.3 55. VLMs: < 7B 23.8 41.6 45.5 38.6 48.5 48.5 50.5 22.6 19.6 22.6 19.6 25.5 22.6 25.5 VLMs: 7B 8B 34.7 42.6 42.6 53.5 52.5 54.5 48.5 50.5 62.4 23.5 16.7 20.6 19.6 19.6 31.4 20.6 26.5 25. VLMs: 10B 30B 41.6 47.5 47.5 56.4 63.4 27.5 32.4 23.5 20.6 22.6 VLMs: 30B 70B 48.5 46.5 56.4 63.4 61.4 23.5 22.6 28.4 29.4 19. VLMs: > 70B 49.5 69.3 62.4 68.3 24.5 27.5 27.5 27.5 VLMs: Proprietary 64.4 74.3 73.3 76.2 28.4 33.3 27.5 45. 21.4 22.3 25.2 29.1 30.1 25.2 29.1 25.2 34.0 35.0 26.2 34.0 33.0 35.9 37.9 33.0 28.2 35.0 38.8 35.0 35.0 35.9 32.0 41.8 40.8 35.9 35.9 50.5 35.0 36.9 30.1 36.9 35.9 41. 38.7 34.4 43.0 26.9 32.3 43.0 44.1 38.7 41.9 44.1 48.4 44.1 41.9 38.7 40.9 34.4 29.0 38.7 34.4 34.4 32.3 37.6 37.6 36.6 34.4 34.4 33.3 36.6 31.2 34.4 54.8 43.0 46.2 43. 58.6 38.6 41.4 52.9 55.7 45.7 61.4 41.4 47.1 48.6 48.6 55.7 57.1 50.0 48.6 52.9 45.7 55.7 57.1 55.7 62.9 60.0 64.3 61.4 61.4 58.6 48.6 55.7 50.0 61.4 70.0 74.3 74.3 74. 26.8 33.1 28.0 21.0 19.8 24.2 22.3 17.8 29.9 22.9 23.6 28.7 17.2 23.6 27.4 19.8 25.5 24.8 26.1 27.4 29.3 25.5 24.2 29.9 26.8 21.0 34.4 31.9 23.6 28.7 56.7 53.0 65.6 54. 32.6 33.6 38.3 36.4 38.5 41.1 41.5 31.2 33.4 40.0 44.7 46.2 42.3 43.1 44.1 45.2 36.4 42.8 43.7 46.3 46.8 43.4 45.3 46.6 50.0 45.6 45.2 51.9 44.6 48.8 52.8 53.9 56.2 57. 38.7 39.4 52.3 54.2 59.8 63.7 59.0 52.1 55.2 48.5 74.0 74.4 64.9 65.5 63.7 69.8 53.3 64.7 66.3 74.0 83.0 67.1 67.7 65.9 84.6 78.7 66.5 79.5 65.7 84.2 46.2 70.7 69.9 73. Table 2. Performance of 32 VLMs (grouped by size) on COLORBENCH. Within each group, models are ranked based on the overall accuracy on color perception and reasoning tasks (P & Overall). The best performance in each group is highlighted in bold. higher value represents better robustness towards color altering. The only 3 models that exceed 80% are InternVL2.526B, InternVL2.5-38B and InternVL2.5-72B, which utilize relatively larger vision encoders, InternViT-6B, compared with others (mostly only 300-400M). In the meantime, GPT4o has really low robustness (46.2%) to colors, indicating its vulnerable sensitivity to color changes, while Gemini-2 shows promising robustness (70.7%) towards colors. Moreover, another surprising observation is that even though only the colors are changed and all the original queries are kept, utilizing more reasoning steps can consistently improve robustness for GPT-4o (+23.7%) and Gemini-2 (+2.9%). 2 proprietary models, LLaVA-OV-72B reaches the top and becomes the only model that exceeds 50% of the accuracy. Similar to the findings from the Object Recognition task, this might be caused by the extremely adequate object detection tasks in open-sourced training recipes. In Color Illusion (CIllu), the accuracies of most models lie in the range of 30% to 50%, and GPT-4o without CoT is the only one that exceeds 50% of the accuracy. In Color Mimicry (CMimic), the 2 proprietary models reach the top, while more reasoning steps do not benefit lot. In Color Blindness (CBlind), most of the models present accuracies under 30%. Considering the extremely practical usage of this scenario, we think the current community should pay more attention to this. Moreover, we also observe that, surprisingly, more reasoning steps benefit VLMs in the color blindness test, although it seems like pure color perception task. Color Robustness. In Color Robustness (CRobust), Color Perception Color Reasoning & Color Robustness CRecog CExtract ORecog CProp CComp CCount OCount CIllu CMimic CBlind Overall L+V 0.5657 (*) 0.5255 (*) 0.7107 (*) 0.5125 (*) 0.6358 (*) 0.4316 (*) 0.7566 (*) -0.3460 0.4832 (*) 0.2460 0.7619 (*) 0.5724 (*) 0.3955 (*) 0.4937 (*) 0.2856 0.6769 (*) 0.5465 (*) 0.4696 (*) 0.6242 (*) 0.6118 (*) 0.5295 (*) 0.4408 (*) 0.2089 0.7611 (*) 0.3608 -0.3697 (*) -0.0127 0.4559 (*) 0.6024 (*) 0.2824 -0.0679 0.7436 (*) 0.5271 (*) CRobust 0.7226 (*) 0.7026 (*) 0.5320 (*) Table 3. Spearmans rank correlation between VLM performance and different model parts sizes on each task. denotes the language model parts size and represents the vision encoder parts size. We use (*) to mark correlations with p-values 0.05. It shows that the scaling law still holds for color understanding but it is much weaker. Figure 5. The heatmaps related to performances and VLM sizes. Deeper color represents higher performance of P&R Overall Accuracy or Robustness. Each line represents model family with the sizes growing from small to large. This visualization clearly shows the correlation between performances and model sizes, larger model leads to higher performance. 3.3. Further Findings Finding 1. The scaling law still holds for color understanding but is much weaker and mainly depends on the language model parts. The correlation between the performance and the vision encoders size is not significant due to the limited choices in current VLMs. To quantitatively analyze the correlation between VLM performances on color understanding tasks and their sizes, Spearmans rank correlation is calculated between VLM performances and (i) overall model sizes (L + V), (ii) language model sizes (L), and (iii) vision encoder sizes (V). The correlation values and signs are presented in Table 3; star is notated when the p-value of the correlation is lower than 0.05. It is observed that between the performances and language model sizes, most of the tasks have correlation greater than 0.5 and p-value smaller than 0.05, except for Color Illusion and Color Blindness due to their special characteristics. Since the correlation between overall model sizes (L + V) and P&R Overall (0.7619), and Robustness (0.7390), we conclude that the color understanding, including Color Perception, Color Reasoning, and Color Robustness, still follows the scaling law of model sizes. Figure 5 presents the correlations between performances and model sizes in each model family. This visualization clearly shows the correlation between performances and model sizes; larger model leads to higher performance within each model family. However, between the performances and vision encoder sizes, most of the tasks either have correlation lower than 0.5 or p-value greater than 0.05, which is not sufficient (a) VLM Size & Model Performance (b) Vision Size & Model Performance Figure 6. The scatter plots related to performances and model sizes. Each plot illustrates the relationship between the log-scaled size of parameter numbers and the performance across all models. (a) Number of parameters for the entire VLM; (b) Number of parameters for the vision encoder. There is significant correlation between performance and VLM sizes. However, since the choices of vision encoder size are concentrated and limited, it is hard to conclude the relationship between performance and vision sizes. to conclude with evident positive correlation. Despite these findings, we try to avoid conveying the message that there is no positive correlation between performances and vision encoder sizes. We think it is because of the negligence of the current community to focus on the scaling laws of vision encoders. The vision encoders used in the current mainstream VLMs are constrained in very small set: (i) most of the VLMs only use one type of vision encoders for the whole family, except for the InternVL2 and InternVL2.5 series; (ii) most of VLMs use the vision encoder with the size of 300 - 400M. These challenges make it hard to evaluate 7 Color & Overall Color Robustness Model Size Model Best Model Best <7B 7B8B 10B30B 30B50B >70B Cambrian-3B Qwen2.5-VL-7B InternVL2.5-26B 46.8 InternVL2.5-38B 50.0 51.9 Llava-ov-72B 63.7 41.5 Qwen2.5-VL-3B 74.4 46.2 Qwen2.5-VL-7B nternVL2.5-26B 83.0 InternVL2.5-38B 84.6 InternVL2.5-78B 84.2 Proprietary Gemini-2 Proprietary Gemini-2 (CoT) 53.9 Gemini-2 57.8 Gemini-2 (CoT) 70.7 73.6 Table 4. The best model within each group and its performances (on P&R accuracy and Robustness). The absolute performances of different VLMs on COLORBENCH are relatively low, and the performance gaps between models are not large. Figure 7. The percentage of change in accuracy (y-axis) by converting colorful images to grayscale in each COLORBENCH task (x-axis). Each violin plot visualizes the distribution over all 32 VLMs. Higher (lower) percentage indicates that VLMs rely more (less) on color clues for the task. Positive (negative) percentage indicates degradation (improvement) on grayscale images. Color clues are indeed more or less leveraged by VLMs in most tasks but they might mislead VLMs (illusion & mimicry). the scaling laws of vision encoders. Figure 6b presents the correslations between model performances and vision encoders sizes for all models. Since the sizes of vision encoders are limited, it is hard to conclude the relationship between vision sizes and performance. Further visualizations are presented in Appendix D.2. Finding 2. The absolute performances of different VLMs are relatively low, and the gaps between different models (open-source vs. proprietary, small vs. large) are not large, indicating the challenges of COLORBENCH and negligence of color understanding in existing VLMs. As shown in Table 4, we separate all the VLMs into several groups based on their sizes and present the best accuracy and the model name within each group. We can see that even the powerful proprietary models, GPT-4o and Gemini-2, can only reach an overall color perception and reasoning (P & Overall) accuracy of 53.9%, only +2.0% better than the best open-sourced model. Moreover, the best model from 8 group 1 has the accuracy of 41.5% (cambrian-3B), which is only 10.4% lower than the best open-sourced model. As for the robustness, the powerful propitiatory models even show weaker robustness than the 7B model. Considering the lack of existing benchmarks specifically evaluating VLMs color understanding capabilities, we conclude that this area is long-neglected by the community, and the open-sourced community is still on the same page with the proprietary model providers. Finding 3. Despite the weaknesses of VLMs on color understanding, adding reasoning steps can still improve their performance on COLORBENCH tasks, even for color robustness, which has not been investigated by the community. The impact of using CoT prompting is shown in Table 5, in which we can see CoT improves the average P&R Overall accuracy across both models by +3.65%, indicating that reasoning benefits these color-related tasks. Within the category of Color Perception, the improvements from CoT on Color Recognition and Object Recognition are quite limited as these tasks heavily rely on the vision encoder. Figure 52 and 53 in Appendix illustrate that adding reasoning steps does not take effect since the initial visual perception and color identification are incorrect in the slow thinking process. However, to our surprise, we find that the Color Extraction task benefits extremely from more reasoning steps, although it seems only related to the vision encoder. After thorough investigation, we observe that most of the current VLMs are not capable of directly extracting color values, so they need to use more reasoning steps to reach reasonable answers. Within the category of Color Reasoning, CoT benefits most of the tasks. However, in the Color Illusion task, CoT harms the model performance. After manual investigation, we observe that more reasoning steps might cause VLMs to focus more on the misleading environments rather than directly compare the assigned colors, as shown in Figure 54. Another observation occurs in the Color Blindness task. Unlike other reasoning-related tasks, humans can read color blindness test image with simple glimpse without any slow thinking. This fascinating misalignment between humans and VLMs intrigues us to further investigation. We find that VLMs recognize these digits in button-up pattern: they need to first infer that the dots in the image can form digit before they really recognize these dots as digits. In addition, the consistent improvement of CoT on Color Robustness is also an un-revealed phenomenon. In our setting, only the colors of the image are altered, and the questions are strictly the same as the original. Thus, under this circumstance, color is the only variant, which is supposed to be more related to the capability of the vision encoder. However, counterintuitively, as shown in our experiments, more reasoning steps make the VLMs more robust to the color Color Perception Color Reasoning & Color Robustness CRecog CExtract ORecog CProp CComp CCount OCount CIllu CMimic CBlind Overall CRobust GPT-4o Gemini-2 +2.6 -1.3 Average +0.65 +7.3 +11.4 +9. +1.3 0.0 0.0 +8.7 +0.65 +4.35 +8.9 +1.9 +5. -0.9 +11.8 +5.45 +5.8 +4.9 +5.35 -8.6 0.0 -4. +4.3 0.0 +2.15 +8.9 +1.1 +5.0 +3.4 +3.9 +3. +23.7 +2.9 +13.3 Table 5. Adding reasoning steps can improve VLMs performance on COLORBENCH. The change of accuracy brought by Chain of Thought (CoT) prompting on all tasks for GPT-4o and Gemini-2-flash. The last row presents the average improvement across both models. changes, which is probably caused by the higher confidence of correct answers after reasoning. significant correlation can be concluded. Finding 4. Color clues are indeed leveraged more or less by VLMs in most of the tasks in COLORBENCH. However, in color illusion and mimicry tasks, colors might mislead VLMs to wrong answers, and converting colorful images to grayscale can improve the accuracy. In order to examine whether VLMs really leverage color clues to handle tasks in COLORBENCH, experiments are conducted by converting all the original colorful images in the Color Perception and Reasoning categories into grayscale ones, without changing the questions. Under this circumstance, the accuracies are expected to decrease dramatically as all our questions are related to colors. For quantitative analysis, we calculate the accuracy changing ratio as (Accori Accgray)/Accori for each VLM on each task. This value directly represents how the original accuracy changes with gray-scale transformation. The positive value represents that the VLM has higher accuracy on the original colored images, indicating that it needs color clues to solve the task. Higher positive values represent higher significance of the color clues. On the contrary, if the value is negative, it means that the VLM can reach better accuracy after the gray-scale transformation, indicating that it does not need color clues for the task, and colors might even mislead VLMs judgment. Lower negative values represent severe harm the color can have on the task. The accuracy changing ratio distribution across all VLMs and tasks are presented in Figure 7 as the violin plot. As shown in the figure, for most of the tasks, the ratios of VLMs are above 0, indicating that VLMs indeed leverage color clues to correctly solve the tasks; removing the color directly harms the original accuracies dramatically. However, when it comes to Color Illusion and Color Mimicry, the majority of the changing ratios are beneath 0, which means that VLMs can get better accuracies when all the color information is removed. This phenomenon is reasonable as the colors on both of these two tasks are more likely serving as the misleading factors. In the meantime, for the Color Counting and Color Blindness tasks, almost half the accuracies increase and half decrease, indicating that the color clues might not be so significant in this task, thus, some of the models can find other ways to get the answer. We also investigate the correlation between accuracy changing ratios and model sizes, while no 4. Related works 4.1. VLM Benchmarks With the rapid advancements in Vision-Language Models (VLMs) [9], numerous benchmarks have emerged to systematically evaluate VLM capabilities across diverse dimensions [28]. These benchmarks generally fall into two categories: text-centric and vision-centric evaluations, each designed to assess distinct multimodal competencies. Textcentric benchmarks primarily measure commonsense knowledge, reasoning, and complex problem-solving capabilities, exemplified by tasks in MMMU [45] and NaturalBench [23]. Conversely, vision-centric benchmarks focus on visual perception and reasoning (MMBench [31] and MME [10]), and robustness to visual perturbations (Grit [14] and Visual Robustness [17]). Furthermore, several benchmarks have extended their scope to evaluate specialized visual tasks, such as spatial relationship comprehension (SEEDBench [22] and MM-Vet [44]), chart and map understanding (MMSTAR [4] and MuirBench [42]), visual grounding (Flickr30k [35] and TRIG [26]) and the detection and understanding of visual hallucinations (POPE [27] and HallusionBench [13]). However, despite the extensive scope covered by existing VLM benchmarks, none currently provide an integrated evaluation that simultaneously assesses visual perception, reasoning, and robustness within unified framework. Moreover, although certain benchmarks [10, 31] have incorporated color-related questions, these have typically addressed basic color perception and recognition, neglecting deeper assessments of reasoning and robustness associated with color understanding. 4.2. Color Evaluation Color understanding is increasingly recognized as crucial aspect of Vision-Language Models ability to perceive and interpret visual content. Limited studies have explored how color information influences model performance on specific tasks. Some studies [47, 48] explore the understanding of color by replacing color-related words in textual inputs to evaluate the models ability to handle color-specific information. More recent research [16, 21] focus on assessing fine-grained color discrimination by asking models to distinguish subtle color differences in 9 visual inputs. Samin et al. [38] introduced color-related foils to test VLMs capacity to cognize about basic colors like red, white, and green, particularly in contexts requiring attention to subtle cues. Additionally, Burapacheep et al. [3] developed benchmark dataset to evaluate and enhance compositional color comprehension in VLMs, emphasizing tasks where understanding minimal color relationships is essential. IllusionVQA [39] evaluates model perception of color illusions in photorealistic scenes. While these works have addressed isolated aspects of color understanding, none has provided holistic assessment framework. In contrast to these previous works, our study establishes the first comprehensive and specialized benchmark for evaluating the color-related abilities of VLMs, offering quantitative, automated approach to further this area of research. 5. Conclusion, limitation and Future Works In this paper, we introduce COLORBENCH, the first benchmark designed to comprehensively evaluate the color understanding capabilities of VLMs, including Perception, Reasoning, and Robustness. After evaluating 32 widely used VLMs on our benchmark, several undiscovered observations are revealed by us. These observations emphasize the need for more sophisticated model architectures that integrate deeper color reasoning capabilities. We plan to expand to more diverse tasks that involve complex interplays of color and texture, shape, or spatial relationships. Additionally, investigating the impact of different visual encoders and language models could further elucidate the pathways through which VLMs process color information."
        },
        {
            "title": "References",
            "content": "[1] Basit Alawode, Iyyakutti Iyappan Ganapathi, Sajid Javed, Naoufel Werghi, Mohammed Bennamoun, and Arif Mahmood. Aquaticclip: vision-language foundation model for underwater scene analysis. arXiv preprint arXiv:2502.01785, 2025. 1 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report, 2025. 5 [3] Jirayu Burapacheep, Ishan Gaur, Agam Bhatia, and Tristan Thrush. Colorswap: color and word order dataset for multimodal evaluation. arXiv preprint arXiv:2402.04492, 2024. 10 [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large visionlanguage models? arXiv preprint arXiv:2403.20330, 2024. 9 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 5 [6] Kanjar De and Marius Pedersen. Impact of colour on roIn Proceedings of the bustness of deep neural networks. IEEE/CVF international conference on computer vision, pages 2130, 2021. 2 [7] Google DeepMind. Gemini 2.0 flash, 2025. 5 [8] Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 48294837, 2016. 2 [9] Hao Fei, Yuan Yao, Zhuosheng Zhang, Fuxiao Liu, Ao Zhang, and Tat-Seng Chua. From multimodal llm to human-level ai: Modality, instruction, reasoning, efficiency and beyond. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024): Tutorial Summaries, pages 18, 2024. 9 [10] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. 9 [11] Karl R. Gegenfurtner and Jochem Rieger. Sensory and cognitive contributions of color to the recognition of natural scenes. Current Biology, 10(13):805808, 2000. 1 [12] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha. Exploring the frontier of vision-language models: survey of current methodologies and future directions. arXiv preprint arXiv:2404.07214, 2024. [13] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 9 [14] Tanmay Gupta, Ryan Marten, Aniruddha Kembhavi, and Derek Hoiem. Grit: General robust image task benchmark. arXiv preprint arXiv:2204.13653, 2022. 9 [15] Shuai He, Anlong Ming, Li Yaqi, Sun Jinyuan, Zheng ShunTian, and Ma Huadong. Thinking image color aesthetics assessment: Models, datasets and benchmarks. ICCV, 2023. 14 [16] Nam Hyeon-Woo, Moon Ye-Bin, Wonseok Choi, Lee Hyun, and Tae-Hyun Oh. Vlms eye examination: Instruct and inspect visual competency of vision language models. arXiv preprint arXiv:2409.14759, 2024. 9 [17] Md Farhan Ishmam, Ishmam Tashdeed, Talukder Asir Saadat, Md Hamjajul Ashmafee, Abu Raihan Mostofa Kamal, and Md Azam Hossain. Visual robustness benchmark for visual question answering (vqa). arXiv preprint arXiv:2407.03386, 2024. 9 [18] Ali Jahanian, Shaiyan Keshvari, SVN Vishwanathan, and Jan Allebach. Colorsmessengers of concepts: Visual design mining for learning color semantics. ACM Transactions on Computer-Human Interaction (TOCHI), 24(1):139, 2017. [19] Johannes Jakubik, Benedikt Blumenstiel, and Clive Tinashe Marimo. Ms-clip: Multi-spectral vision language learning for earth observation. In American Geophysical Union Fall Meeting, 2024. 1 [20] Jayendra Kantipudi, Shiv Ram Dubey, and Soumendu Chakraborty. Color channel perturbation attacks for fooling convolutional neural networks and defense against such attacks. IEEE Transactions on Artificial Intelligence, 1(2): 181191, 2020. 2 [21] Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, et al. Vhelm: holistic evaluation of vision language models. arXiv preprint arXiv:2410.07112, 2024. 9 [22] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 9 [23] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. arXiv preprint arXiv:2410.14669, 2024. 9 [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer, 2024. [25] Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, 11 Wankou Yang, Yabiao Wang, and Chengjie Wang. survey on benchmarks of multimodal large language models, 2024. 2 [26] Ming Li, Ruiyi Zhang, Jian Chen, Jiuxiang Gu, Yufan Zhou, Franck Dernoncourt, Wanrong Zhu, Tianyi Zhou, and Tong Sun. Towards visual text grounding of multimodal large language model, 2025. 9 [27] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. [28] Zongxia Li, Xiyang Wu, Hongyang Du, Huy Nghiem, and Guangyao Shi. Benchmark evaluations, applications, and challenges of large vision language models: survey. arXiv preprint arXiv:2501.02189, 2025. 2, 9 [29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 14 [30] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 5 [31] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European conference on computer all-around player? vision, pages 216233. Springer, 2024. 9 [32] Lingjun Mao, Zineng Tang, and Alane Suhr. Evaluating model perception of color illusions in photorealistic scenes. arXiv preprint arXiv:2412.06184, 2024. 14 [33] Daniela Mapelli and Marlene Behrmann. The role of color in object recognition: Evidence from visual agnosia. Neurocase, 3(4):237247, 1997. [34] OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and etc. Gpt-4o system card, 2024. 5 [35] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649, 2015. 9 [36] Ragini Rathore, Zachary Leggon, Laurent Lessard, and Karen Schloss. Estimating color-concept associations from IEEE Transactions on Visualization and image statistics. Computer Graphics, 26(1):12261235, 2019. 1 [37] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 4 [38] Ahnaf Mozib Samin, Firoz Ahmed, and Md Mushtaq Shahriyar Rafee. Colorfoil: Investigating color blindness in large vision and language models. arXiv preprint arXiv:2405.11685, 2024. 10 [39] Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, and Rifat Shahriyar. Illusionvqa: challenging optical illusion dataset for vision language models. arXiv preprint arXiv:2403.15952, 2024. 10, 14 [40] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring the design space for multimodal llms with mixture of encoders. arXiv preprint arXiv:2408.15998, 2024. 1, 5 [41] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 4, 5, 14 [42] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 9 [43] Hanna-Sophia Widhoelzl and Ece Takmaz. Decoding emotions in abstract art: Cognitive plausibility of clip in arXiv preprint recognizing color-emotion associations. arXiv:2405.06319, 2024. 1 [44] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 9 [45] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [46] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [47] Le Zhang, Rabiul Awal, and Aishwarya Agrawal. Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic fine-grained understanding. arXiv preprint arXiv:2306.08832, 2023. 9 [48] Tiancheng Zhao, Tianqi Zhang, Mingwei Zhu, Haozhan Shen, Kyusong Lee, Xiaopeng Lu, and Jianwei Yin. Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations. arXiv preprint arXiv:2207.00221, 2022. 9 [49] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. 14 [50] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019."
        },
        {
            "title": "Table of Contents for Appendix",
            "content": "A. Data Sources B. COLORBENCH Categories and Questions C. Evaluation Prompts 14 14 D. More Visualizations 19 D.1. VLM Size & Model Performance for Each Task 19 D.2. Vision Size & Model Performance for Each . . . . . D.3. Performance for Each Model Family on Each . . . . . Task . Task . 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Samples Cases . . E.1. Effect of CoT . . E.2. Effect of Grayscale . . . E.3. Easy Cases . . E.4. Difficult Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 23 28 30 32 A. Data Sources We conduct COLORBENCH from multiple sources, including website sources, publicly available benchmarks, and generated images. The detailed sources are included in Table 6. Category Data Source Synthetic Data CRecognition Website, ICAA17K [15] CRecognition Website, ICAA17K [15] CExtraction CProportion Website, Synthetic Data CComparison Website CCounting COunting CMimicry CBlindness CRobust Website, Synthetic Data Website, ADA20K [49, 50], COCO2017 [29] Website, IllusionVQA[39], RCID[32] Synthetic Data CV-Bench[41] Table 6. Data sources for each task. B. COLORBENCH Categories and Questions Table 7 provides detailed description of each task, alongside representative figures and sample questions that effectively demonstrate the specific capabilities being tested. Cases are provided for each task in Figure 8 to 18. 14 t r g o R"
        },
        {
            "title": "Task",
            "content": "#"
        },
        {
            "title": "Color Recognition",
            "content": "76 Figure"
        },
        {
            "title": "Color Extraction",
            "content": "96 Figure"
        },
        {
            "title": "Object Recognition",
            "content": "77 Figure"
        },
        {
            "title": "Sample Questions",
            "content": "Ask for the color of specific object or determine if particular color is present in the image. What is the color of object in this image? What color does not exist in this image? Extract the color code value (e.g., RGB, HSV, or HEX) from single color in the image. What is the HSV value of the given color in the image? What is the RGB value of the given color in the image? Identify objects in the image that match specified color noted in the text input. What object has color of pink in this image?"
        },
        {
            "title": "Color Proportion",
            "content": "80 Figure 11 Estimate the relative area occupied by specified color in the image. What is the dominant color in this image? What is the closest to the proportion of the red color in the image?"
        },
        {
            "title": "Color Comparison",
            "content": "101 Figure 12 Distinguish among multiple colors present in the image to assess overall tones and shades. Which photo is warmer in overall color? Which object has darker color in the image?"
        },
        {
            "title": "Color Counting",
            "content": "102 Figure 13 Identify the number of unique colors present in the image. How many different colors are in this image?"
        },
        {
            "title": "Object Counting",
            "content": ""
        },
        {
            "title": "Color Illusion",
            "content": ""
        },
        {
            "title": "Color Mimicry",
            "content": ""
        },
        {
            "title": "Color Blindness",
            "content": "157 Figure 14 Count the number of objects of specified color present in the image. Figure 15 Assess and compare colors in potential illusionary settings within the image. Figure 16 Detect objects that are camouflaged within their surroundings, where color is key deceptive element. Figure 17 Recognize numbers or text that are embedded in color patterns, often used in tests for color vision. How many objects with green color are in this image? Do two objects have the same color? How many animals are in this image? What is the number in the center of the image? Table 7. Task and question definition in COLORBENCH. 15 Figure 8. Cases for Color Recognition Task. Figure 9. Cases for Color Extraction Task. Figure 10. Cases for Object Recognition Task. Figure 11. Cases for Color Proportion Task. 16 Figure 12. Cases for Color Comparison Task. Figure 13. Cases for Color Counting Task. Figure 14. Cases for Object Counting Task. Figure 15. Cases for Color Illusion Task. Figure 16. Cases for Color Mimicry Task. 17 Figure 17. Cases for Color Blindness Task. Figure 18. Cases for Color Robustness Task. 18 C. Evaluation Prompts Instruction Prompt Youll be given an image, an instruction and some options. You have to select the correct one. Do not explain your reasoning. Answer with only the letter that corresponds to the correct option. Do not repeat the entire answer. CoT Instruction Prompt Youll be given an image, an instruction and some options. You have to select the correct one. Think step by step before answering. Then conclude with the letter that corresponds to the correct option. Make sure the option letter is in the parentheses like (X). Do not include ( or ) in the response except for the answer. Figure 23. The heatmap for Color Comparison. Figure 24. The heatmap for Color Counting. D. More Visualizations D.1. VLM Size & Model Performance for Each Task Figure 25. The heatmap for Object Counting. Figure 26. The heatmap for Color Illusion. Figure 19 to 28 present detailed correlations between the log-scaled sizes of VLM parameters and the performance metrics for each task of Perception and Reasoning Categories. Deeper color represents higher accuracy. Each line represents model family with the sizes growing from small to large. This visualization clearly shows the correlation between performances and model sizes, larger model leads to higher performance. Figure 19. The heatmap for Color Recognition. Figure 20. The heatmap for Color Extraction. Figure 21. The heatmap for Object Recognition. Figure 22. The heatmap for Color Proportion. 19 Figure 27. The heatmap for Color Mimicry. Figure 28. The heatmap for Color Blindness. D.2. Vision Size & Model Performance for Each"
        },
        {
            "title": "Task",
            "content": "Figure 29 to 33 show detailed correlations between the logscaled sizes of vision encoders and the performance metrics for each task of Perception and Reasoning Categories. Colors represent different model families. Models that have the same vision encoder sizes but with different LLM sizes are plotted as different points. Given that the majority of Vision-Language Models (VLMs) utilize singular type of vision encoder, and that the sizes of these encoders generally range between 300-400M, it becomes challenging to assess the scaling effects within vision encoders. Figure 29. The scatter plot for Color Recognition and Color Extraction. Figure 32. The scatter plot for Object Counting and Color Illusion. Figure 30. The scatter plot for Object Recognition and Color Proportion. Figure 33. The scatter plot for Color Mimicry and Color Blindness. D.3. Performance for Each Model Family on Each"
        },
        {
            "title": "Task",
            "content": "Figures 34 to 40 illustrate task performance across different models within the same model families. In general, models with more parameters tend to perform better on the majority of tasks. Figure 31. The scatter plot for Color Comparison and Color Counting. 20 Figure 34. Performance of LLaVA-OV models on all tasks. Figure 37. Performance of Eagle models on all tasks. Figure 35. Performance of LLaVA-NEXT models on all tasks. Figure 38. Performance of InternVL2 models on all tasks. Figure 36. Performance of Cambrian models on all tasks. Figure 39. Performance of InternVL2.5 models on all tasks. 21 Figure 40. Performance of Qwen2.5 models on all tasks. E. Samples Cases E.1. Effect of CoT In this section, we present cases that the answers are influenced by adding reasoning steps for each task. For most of the tasks in COLORBENCH, adding reasoning steps can significantly improve the model performances. The samples cases of Perception and Reasoning categories are shown in Figure 41 to Figure 50. Case for Robustness category is shown in Figure 51. Figure 43. Case with CoT for Object Recognition task. Figure 41. Case with CoT for Color Recognition task. Figure 42. Case with CoT for Color Extraction task. Option backgrounds correspond to their color codes. Figure 44. Case with CoT for Color Proportion task. 23 Figure 45. Case with CoT for Color Comparison task. Figure 47. Case with CoT for Object Counting task. Figure 46. Case with CoT for Color Counting task. Figure 48. Case with CoT for Color Illusion task. 24 Figure 49. Case with CoT for Color Mimicry task. Figure 50. Case with CoT for Color Blindness task. 25 Figure 51. Case with CoT for Color Robustness task. However, for Color Recognition and Object Recognition tasks, the improvement of involving slow thinking is limited, as these two tasks heavily rely on the accurate cognition of the vision encoder. The sample cases are shown in Figure 52 and 53. For Color Illusion task, adding reasoning steps causes the model to focus more on the misleading environment and the relationship between the environment and the foreground objects. This thinking negatively influences the model performance. sample case is shown by Figure 54. Figure 52. Case that CoT results in an incorrect answer for Color Recognition task. Figure 54. Case that CoT results in an incorrect answer for Color Illusion task. Figure 53. Case that CoT results in an incorrect answer for Object Recognition task. 27 E.2. Effect of Grayscale For most of the tasks in COLORBENCH, colors are critical clues for VLMs to generate the answers. We highlight these cases in Figure 55 to 62. However, for Color Illusion and Color Mimicry tasks, color clues might mislead VLMs to wrong answers, as shown in Figure 63 and 64. Figure 55. Color clues play as critical role for Color Recognition task. Figure 58. Color clues play as critical role for Color Proportion task. Figure 59. Color clues play as critical role for Color Comparison task. Figure 56. Color clues play as critical role for Color Extraction task. Option backgrounds correspond to their color codes. Figure 57. Color clues play as critical role for Object Recognition task. Figure 60. Color clues play as critical role for Color Counting task. 28 Figure 61. Color clues play as critical role for Object Counting task. Figure 64. Color clues negatively affect VLMs prediction for Color Mimicry task. Figure 62. Color clues play as critical role for Color Blindness task. Figure 63. Color clues negatively affect VLMs prediction for Color Illusion task. 29 E.3. Easy Cases We present samples cases that majority of VLMs reach the correct answers. Figure 65. Color Recognition case that majority of VLMs provide correct results. Figure 69. Color Comparison case that majority of VLMs provide correct results. Figure 66. Color Extraction case that majority of VLMs provide correct results. Option backgrounds correspond to their color codes. Figure 70. Object Counting case that majority of VLMs provide correct results. Figure 67. Object Recognition case that majority of VLMs provide correct results. Figure 68. Color Proportion case that majority of VLMs provide correct results. Figure 71. Color Mimicry case that majority of VLMs provide correct results. Figure 72. Color Robustness case that majority of VLMs provide unchanged results over color variations in images. 31 E.4. Difficult Cases We present samples cases that majority of VLMs reach the incorrect answers. Figure 73. Color Recognition case that majority of VLMs provide incorrect results. Figure 77. Color Comparison case that majority of VLMs provide incorrect results. Figure 74. Color Extraction case that majority of VLMs provide incorrect results. Option backgrounds correspond to their color codes. Figure 78. Color Counting case that majority of VLMs provide incorrect results. Figure 75. Object Recognition case that majority of VLMs provide incorrect results. Figure 79. Object Counting case that majority of VLMs provide incorrect results. Figure 76. Color Proportion case that majority of VLMs provide incorrect results. Figure 80. Color Illusion case that majority of VLMs provide incorrect results. 32 Figure 81. Color Mimicry case that majority of VLMs provide incorrect results. Figure 82. Color Blindness case that majority of VLMs provide incorrect results. Figure 83. Color Robustness case that majority of VLMs change the answers over color variations in images."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}