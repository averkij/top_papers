{
    "paper_title": "Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models",
    "authors": [
        "Doohyuk Jang",
        "Yoonjeon Kim",
        "Chanjae Park",
        "Hyun Ryu",
        "Eunho Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit a problematic reliance on familiar reasoning patterns, a phenomenon we term \\textit{reasoning rigidity}. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, a behavior largely unexplored in prior work, we introduce a expert-curated diagnostic set, \\dataset{}. Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 2 2 7 1 . 5 0 5 2 : r Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models Doohyuk Jang1 Yoonjeon Kim1 Chanjae Park1 Hyun Ryu1 Eunho Yang1,2 1 KAIST 2 AITRICS {jadohu , yoonkim313, chanjae.park, ryuhyun1905}@kaist.ac.kr eunhoy@gmail.com (cid:135) (cid:140)"
        },
        {
            "title": "Abstract",
            "content": "Large language models have demonstrated remarkable proficiency in long and complex reasoning tasks. However, they frequently exhibit problematic reliance on familiar reasoning patterns, phenomenon we term reasoning rigidity. Despite explicit instructions from users, these models often override clearly stated conditions and default to habitual reasoning trajectories, leading to incorrect conclusions. This behavior presents significant challenges, particularly in domains such as mathematics and logic puzzle, where precise adherence to specified constraints is critical. To systematically investigate reasoning rigidity, behavior largely unexplored in prior work, we introduce expert-curated diagnostic set, ReasoningTrap. Our dataset includes specially modified variants of existing mathematical benchmarks, namely AIME and MATH500, as well as well-known puzzles deliberately redesigned to require deviation from familiar reasoning strategies. Using this dataset, we identify recurring contamination patterns that occur when models default to ingrained reasoning. Specifically, we categorize this contamination into three distinctive modes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention, each causing models to ignore or distort provided instructions. We publicly release our diagnostic set to facilitate future research on mitigating reasoning rigidity in language models."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Team et al., 2023; Chowdhery et al., 2023) have demonstrated remarkable proficiency in various challenging tasks, including mathematical reasoning (Cobbe et al., 2021; Hendrycks et al.), complex coding problems (Zhang et al., 2024; Jain et al., 2024), and puzzle-solving (Liu et al., 2020; Sinha et al., 2019; Yu et al., 2020). Recently, reasoning models (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025; Team, 2025c; Claude, 2024; Google DeepMind, 2025a) utilizing extended chain-of-thought prompting with increased test-time compute have attracted Equal contribution. Corresponding author. Preprint. Under review. Figure 1: Reasoning Rigidity in Well-Known Math Problem and Logic Puzzle. When solving subtly modified version of well-known math problems (AIME) and famous logic puzzles (Fibonacci Rabbit and Tower of Hanoi), advanced reasoning models such as Qwen3-32B and OpenAI o3 default to familiar reasoning template leading to incorrect conclusions. significant attention due to their capability to solve intricate reasoning problems. However, problematic behavior, reasoning rigidity, has emerged in models specifically trained for long chain of thought reasoning. Crucially, unlike hallucination, where models fabricate factually incorrect content, or prompt brittleness, where minor changes in prompt form lead to unstable outputs, reasoning rigidity reflects cognitive bias: even when the conditions are fully understood, the model will override them in favor of familiar solution templates. This distinction highlights reasoning rigidity as unique failure mode that cannot be addressed merely by improving factual grounding or prompt robustness. Alarmingly, this reasoning rigidity manifests itself by causing models to override explicit user instructions. As illustrated in Figure 1(a), despite the clear instruction specifying that is positive real number, advanced reasoning models capable of solving complex mathematical problems incorrectly assume must be complex number with modulus 4. Similar issues also appear in puzzle contexts; for instance, the explicitly stated condition permanently infertile is arbitrarily altered by the model into temporarily infertile, thus converting the problem into familiar Fibonacci sequence scenario. Additionally, direct instructions explicitly stating this is not Tower of Hanoi problem are mistakenly interpreted by the model as typo, causing it to default to the familiar Tower of Hanoi reasoning. These examples collectively illustrate how LLMs systematically disregard explicit instructions when such directives conflict with their ingrained reasoning patterns. This rigidity poses challenges across domains where following user-stated constraints is crucial, such as mathematics and logic puzzles that come with multiple conditions that must be fulfilled. Through the models reasoning rigidity that unconsciously edits or ignores essential user given conditions, the models entire reasoning path can become contaminated by ingrained reasoning patterns, ultimately leading to erroneous conclusions or suboptimal solutions. This behavior is highly alarming, but yet to be analyzed to the best of our knowledge. Therefore, there is need for the evaluation dataset that tackles the reasoning model ability to faithfully follow the user instruction, overcoming its innate rigidity to ingrained reasoning patterns introducing contamination to reasoning path. To systematically evaluate this phenomenon and analyze the ingrained patterns of reasoning models, we introduce ReasoningTrap, diagnostic dataset comprising mathematical problems and puzzles intentionally designed to closely resemble well-known challenges but modified through carefully introduced variations. ReasoningTrap assesses not only the abil2 ity of large language models to detect and incorporate these constraints but also investigates whether these models persistently default to familiar reasoning paths. This diagnostic set thus provides novel insights into both the capabilities and limitations of contemporary deep reasoning models. Our analysis of ReasoningTrap yields several important findings: i) contamination begins in the intermediate steps of the reasoning process, and ii) such contamination manifests in identifiable, recurring patterns in the models outputs. Based on these observations, we propose an automated problem restatement algorithm aimed at mitigating reasoning rigidity. Specifically, we categorize these recurrent reasoning patterns that prevent faithful adherence to explicit conditions into three distinct classes: (i) Interpretation Overload, (ii) Input Distrust, and (iii) Partial Instruction Attention. Our contributions can be summarized as follows: We identify and highlight notable behavior of reasoning models deviating from the given condition due to rigidity in reasoning patterns. We introduce ReasoningTrap, carefully constructed diagnostic set that enables rigorous evaluation and understanding of reasoning rigidity across diverse reasoning scenarios. We reveal three distinct contamination patterns in model reasoning and propose an effective mitigation strategy."
        },
        {
            "title": "2 Related Works",
            "content": "Large Reasoning Models The rapid advancement of LLMs has led to increasing efforts to apply them to complex problem-solving tasks such as mathematics(Touvron et al., 2023; Azerbayev et al., 2023; Imani et al., 2023). In this context, Chain-of-Thought (CoT) prompting (Wei et al., 2022) elicits the LLM model ability to verbalize internal reasoning process. Recently, by explicitly training to generate significantly longer chains of thought with extensive test-time computation before producing final answers, reasoning models with long CoT ability has gained tremendous attention (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025; Team, 2025c). These reasoning models achieve state-of-the-art performance on challenging tasks such as AIME and Codeforces, surpassing previous frontier LLMs and garnering widespread attention. The recently released Qwen3 (Team, 2025b) introduces unified fusion architecture that supports both reasoning and non-reasoning modes, allowing users to explicitly choose whether to use long CoT or not. Instruction Following of Reasoning Models The performance drop of reasoning models when provided with multiple in-context examples or long-winded instruction is well-known phenomenon (Guo et al., 2025; Jaech et al., 2024). Such phenomenon states that reasoning models are less capable of following user-provided examples. Our work investigates the phenomenon that reasoning models are capable of following instructions from the user, but sticks to the familiar reasoning pattern thus conform less to the given instruction. Rigidity in Reasoning Models Several works have pointed out the possibility that LLM models show rigid pattern in reasoning in specific subfields, medical domain (Kim et al., 2025) and educational domain (Araya, 2025). Our work is the first to systematically analyze the reasoning rigidity in larger domain including mathematics and puzzles. Closely related to our work, are several previous studies that explore rigidity in large language models (LLMs). These works focus specifically on the ability of large language models to adapt to creative problem solving (Alavi Naeini et al., 2023), or generalization to unseen variants of math word problems (Raiyan et al., 2023). Our work specifically examines the underlying model-driven rigidity of reasoning models, and identifying deliberate overrides of atypical user instructions rather than mere inability to solve tasks creatively or generalizing. Underlying Reason for Rigidity Some research has explored why such rigidity arises in LLMs, pointing to biases embedded within training data or optimization methods. Yue et al. (2025) noted that RL-trained reasoning models excel at exploitation, achieving higher accuracy efficiently, yet paradoxically showing narrower knowledge coverage compared to Figure 2: Dataset Construction Pipeline The dataset construction pipeline of ConditionedMath consists of two steps. Step1: Create new questions with unusual conditions that are (1) valid, (2) meaningfully different from the original, and (3) solvable without ambiguity. Two modified versions of card-guessing problem are shown. While Modif 1 introduces small tweak that preserves validity and solvability, Modif 2 includes an invalid condition (multiplying card count by 3), rendering the problem unsolvable. (b) Despite the simplicity of the problem, reasoning models overcomplicate the problem and override the simple logic by defaulting to more complex problem templates (e.g., assuming two-card setup). non-reasoning models. Moore et al. (2024) attributed this behavior to biases inherent in training datasets. While these studies identify potential training-induced biases, our research specifically uncovers and characterizes an active cognitive bias, describing an explicit tendency of reasoning models to prioritize conventional reasoning traces over user-provided instructions, especially when the latter seem atypical or unconventional."
        },
        {
            "title": "3 ReasoningTrap: Reasoning Rigidity Diagnostic Set",
            "content": "In this section, we introduce ReasoningTrap, well-curated diagnostic set specifically designed to reveal reasoning rigidity in language models. Reasoning rigidity occurs when models, despite fully comprehending given conditions, choose to ignore or mistrust explicit instructions, defaulting instead to their preferred, yet incorrect reasoning pathways. To systematically investigate this phenomenon, we curated two specialized datasets: ConditionedMath (Section 3.1), consisting of challenging mathematical problems augmented with novel constraints, and PuzzleTrivial (Section 3.2), comprising puzzle questions subtly modified version from original logic puzzles. Dataset Structure The ReasoningTrap dataset consists of pairs of original questionreasoning-answer tuples (qorig, rorig, aorig) and modified counterparts (qmod, rmod, amod). The modified solutions and answers diverge from the original counterparts to facilitate the assessment if the reasoning correctly follows the instructions stated in the modified question, not the original one. In Table 1, our benchmark comprises 164 items in total: 84 drawn from the mathematical domain and 80 from puzzles. Every question in ConditionedMath is conceptually distinct, non-overlapping, and has been rigorously verified by human annotators. Meanwhile, PuzzleTrivial spans ten unique puzzle concepts. Therefore, the dataset can be readily expanded into much larger collection of questionanswer pairs, which we leave for future work. 4 Table 1: Diagnostic Dataset Configuration ConditionedMath PuzzleTrivial AIME (22-24) MATH500 (lv.5) # Questions Original Size 34 90 50 130 80 N/A"
        },
        {
            "title": "3.1 ConditionedMath: Popular Math Benchmark with Additional Conditions",
            "content": "We construct the ConditionedMath dataset by adapting questions from historical AIME 20222024 (AIME) and MATH500 Level 5 (Hendrycks et al.) datasets. The construction followed two-stage process as illustrated in Figure 2. (1) original question modification, and (2) rigorous filtering based on predefined validation criteria3. For generating novel conditions, we provided three in-context examples pairing original problems alongside known solutions to language model, prompting it to propose five distinct, constraints that meaningfully alter the problems reasoning trajectory and eventually leading to different answer. These modified questions were further validated on three critical criteria: (a) mathematical validity of the modified conditions to ensure that no internal contradictions exist, (b) divergence of the resulting solution from the original problems solution, and (c) existence of solution. The final criterion is to facilitate the assessment on whether the model continues to employ its previously learned reasoning paths or effectively generates new reasoning trajectory as dictated by the modified conditions. Following automated verification and filtering using the o4-mini model, human annotator with mathematical expertise further reviewed each question-solution pair for compliance with these constraints. Specifically, for the AIME dataset, 90 original question-answer pairs were expanded into five variants each (totaling 450), which, after filtering for validity, resulted in final set of 34 questions. Similarly, 130 Level-5 questions from the MATH500 dataset were expanded into 650 variants, which were subsequently filtered down to 50 validated problems."
        },
        {
            "title": "3.2 PuzzleTrivial: Puzzles with Subtle Modifications to Trivial Solutions",
            "content": "Building upon insights from Williams and Huckle (2024); Vellum AI (2025), we developed the PuzzleTrivial dataset, designed to assess models susceptibility to familiar but unnecessarily complex reasoning approaches. Classic puzzle questions were subtly altered by modifying premises or omitting specific constraints, thereby drastically simplifying the logical reasoning required. In some cases, these alterations introduced multiple plausible answers. To eliminate resulting ambiguity, clarifying instructions, such as find the simplest valid solution, were explicitly included. Additionally, select puzzles require only straightforward, commonsense reasoning. For instance, the Fibonacci Rabbit illustrated in Figure 1(b) conditions on permanently infertile rabbit pair. While the non-reasoning model correctly concludes no reproduction occurs, yielding constant population, the reasoning model dismisses the literal meaning as trivial and instead interprets the initial state as temporarily infertile, reverting to the familiar Fibonacci growth structure. This demonstrates the models tendency to override explicit conditions in favor of familiar reasoning templates."
        },
        {
            "title": "4 Contamination Ratio and Early Detection Algorithm",
            "content": "To systematically measure reasoning model contamination from familiar reasoning pattern, we propose the Contamination Ratio, representing the proportion of contaminated reasoning from the familiar patterns (Section 4.1). To generalize our findings to arbitrary problems that we do not have ground truth label for familiar patterns, we introduce an algorithm capable of detecting contamination, thus enabling broader applicability to novel problems (Section 4.2). 3We use OpenAI gpt-4o-mini for stage 1 and o4-mini for stage 2, since stage 2 requires more powerful language model as verifier. 5 (a) (b) Figure 3: Patterns Associated with Contamination Ratio (a) Relationship between contamination ratio and p-pass@1 reveals that contamination in the reasoning path does not affect the final output up to certain point (approximately 40%), while contamination over this point drastically reduces the p-pass@1 score, indicating that the model is trapped into wrongful reasoning path and arrived at incorrect output. (b) Observing the contamination ratio between specific interval of reasoning steps, wrong output reasoning exhibits progressively worsening contamination as the reasoning step length increases."
        },
        {
            "title": "4.1 Contamination Ratio in Synthetic Dataset",
            "content": "Upon the construction of ReasoningTrap, we observe that highly advanced reasoning models frequently show contamination from familiar reasoning patterns. Given the modified questions that completely differ from the original problems (AIME, MATH500, Logic Puzzles), reasoning models try to reason starting from the original question, but the reasoning trajectory gradually gets contaminated by familiar, but irrelevant solution trajectory that is closer to the reasoning pattern for the original question. Note that the modified questions are designed to require completely different solution trajectories from the originals. To quantify the ratio of contamination from familiar yet wrong reasoning, we devise novel evaluation metric called contamination ratio. More specifically, the reasoning outputs generated by the model are segmented into individual paragraphs and encoded into textual representations4. The reasoning outputs are denoted as = [r1, r2, . . . , rp], where represents the number of paragraphs. For each paragraph ri R, we measure the cosine similarity between ri and two reference reasoning texts: the original reasoning rorig and the modified reasoning rmod. The contamination ratio is defined as the proportion of reasoning steps for which the cosine similarity between ri and rorig is higher than that between ri and rmod. Formally, this metric is expressed as: Scontam = 1 i= 1 cs(i) orig > cs(i) mod , (1) where the cosine similarity is computed as cs(i) orig = (ri)rorig rirorig and cs(i) mod = (ri)rmod rirmod . Evaluation of Reasoning Rigidity To reliably observe reasoning rigidity, models tendency to default to familiar and template-based reasoning paths even when they contradict explicit problem constraints, we must disentangle two sources of failure. The first failure comes from misunderstanding the problem setup and the second comes from misapplying reasoning despite understanding it. To this end, we first verify if the model correctly interprets the given conditions. Once this is ensured, we evaluate whether its reasoning remains aligned with those conditions or instead diverges toward heuristics observed during training. 4Paragraphs are split using double line breaks, each indicating reasoning block, and encoded using OpenAIs text-embedding-small model. 6 Figure 4: Reasoning Pattern Analysis and Corresponding Prompt Hinting. To capture this distinction, we propose new metric called p-pass@k, which is modified pass@k metric with additional consideration on how much the model perceives constraints in its reasoning process. Unlike conventional pass@k, which focus solely on answer correctness, p-pass@k evaluates whether the reasoning path of model well perceives the problems conditions. This enables more precise diagnosis of reasoning failures, revealing when the models deviation stems from rigidity rather than misunderstanding the given conditions. More formally, p-pass@1 = PN k=1 0, (cid:3) pk 1(cid:2)ˆak = PN i=1 pi , if PN i=1 pi > 0, if PN i=1 pi = 0 (2) where is the number of samples, ˆak is the models predicted answer, is the ground truth answer, and pk {0, 1} is perception indicator where pk = 1 if the model correctly understands the given conditions, and pk = 0 otherwise. In order to determine rather the model reasoning trajectory appears to perceive the user instruction, we employ an LLM to judge rather the conditions in the question and ground truth solution is reflected in the reasoning process, even when only the subset of the reasoning includes the groundings. From the observation that the question perception is readily finished in the early phase of reasoning process, we input the first 15 paragraphs of reasoning to compare with the ground truth solution and question. This benefits the accurate measurement of perception since overly lengthy reasoning process make gpt-based evaluation prone to misjudging that the original solution is not included. The full judgment prompt is provided in the Appendix B. Using these two metrics, we observe two consistent patterns across reasoning models. As shown in Figure 3(a)(c), the accuracy (p-pass@1) appears largely unaffected by contamination ratios below approximately 40%. In Figure 3(d), we record the average contamination ratio across specific intervals of the reasoning steps. Interestingly, base models without long chain-of-thought (CoT) capabilities do not show consistent pattern of contamination dominating the reasoning process. In contrast, more advanced reasoning models tend to exhibit increasingly severe contamination as the reasoning process becomes longer and more elaborate."
        },
        {
            "title": "4.2 Signals for Contamination in Realistic Situation",
            "content": "In realistic use cases where only the question is given, it is impossible to automatically detect if the generated reasoning is being contaminated by unwanted but familiar patterns. Therefore, we devise simple yet effective method to detect such suspicious pattern from the patterns when contamination happens. The provided taxonomy of reasoning contamination, illustrate in Figure 4, is applicable for robust mitigation strategies. 7 Table 2: Comparison of Base vs. Reasoning Models on ConditionedMath. AIME MATH500 Model Name Type p-pass@1 pass@1 p-score p-pass@1 pass@1 p-score Qwen2.5-32B-Instruct + QwQ-32B Qwen3-32B No think + Qwen3-32B Think Qwen3-235B No think + Qwen3-235B Think DeepSeek + DeepSeek R1 GPT-4o ChatGPT-4o + o3-mini + o4-mini Gemini2.5 Flash No think + Gemini2.5 Flash Think Claude 3.7 Sonnet No think Base Reason Base Reason Base Reason Base Reason Base Base Reason Reason Base Reason Base + Claude 3.7 Sonnet Think Reason 59.127.81 49.216.79 45.146.97 33.256.58 46.177.36 24.105.32 54.727.92 49.098.21 55.137.22 38.267.12 36.906.88 31.256.59 58.647.17 50.457.52 57.807.86 57.008.00 45.777.22 42.466.63 43.387.03 29.606.32 42.657.29 20.775.07 45.597.65 39.717.76 47.067.06 33.826.99 22.795.72 19.125.49 52.217.17 46.127.33 50.747.65 46.727.63 75.555.01 81.804.27 90.812.66 76.844.91 86.403.08 81.624.12 77.945.46 80.885. 82.353.54 84.564.35 61.766.35 58.826.75 84.014.77 89.812.52 80.154.94 72.996.01 55.956.02 47.645.94 50.515.52 34.605.60 55.495.66 27.494.65 60.676.34 48.636.44 46.335.32 42.946.26 49.636.14 39.065.76 56.615.63 56.416.36 41.525.79 40.385.75 40.885.74 34.755.74 47.135.30 30.635.59 53.505.62 23.254.63 47.006.05 38.006.40 35.504.89 38.003.26 38.005.81 26.505.17 49.805.59 47.956.27 36.005.49 32.005.58 70.374.39 71.374.59 85.882.90 75.503.74 84.252.72 79.133.39 75.004.57 73.005. 69.873.93 81.503.26 67.505.40 64.005.06 83.533.41 82.513.47 85.502.95 78.004.44 Table 3: Comparison of Base vs. Reasoning Models on PuzzleTrivial. Model Name Type p-pass@1 pass@ p-score Qwen2.5-32B-Instruct + QwQ-32B Qwen3-32B No think + Qwen3-32B Think Qwen3-235B No think + Qwen3-235B Think DeepSeek V3 + DeepSeek R1 GPT-4o ChatGPT-4o + o3-mini + o4-mini Gemini2.5 Flash No think + Gemini2.5 Flash Think Claude 3.7 Sonnet No think Base Reason Base Reason Base Reason Base Reason Base Base Reason Reason Base Reason Base + Claude 3.7 Sonnet Think Reason 40.903.98 39.124.40 74.303.33 38.283.47 74.163.43 38.494.04 66.213.83 51.734.33 64.074.60 63.633.74 59.254.93 56.384.84 70.094.21 69.444.32 79.973.85 65.884.63 30.233.51 38.364.38 67.663.53 37.193.40 64.533.72 37.974.05 53.983.82 50.554.33 48.384.53 58.593.63 39.224.49 29.534.18 65.944.27 65.634.34 73.284.03 52.814. 72.973.01 97.660.48 84.212.07 96.330.64 86.172.66 97.420.56 80.003.45 97.270.97 75.233.63 89.142.18 62.503.66 39.774.25 94.061.79 94.061.95 89.302.05 79.693.50 Interpretation Overload The model starts to reject the given question conditions by reinterpreting the question into multiple ways rather than accepting straightforward interpretation. It is also observed that the model tends to drift between different semantic interpretations mid-reasoning, causing inconsistent or contradictory conclusions. Input Distrust Reasoning models have unique patterns assuming the presence of typos, translation mistake, or input errors. This leads to the dismissal of the conditions stated in the question and make the reasoning process overly complicated even in the straightforward cases. Partial Instruction Attention The models focus selectively on portion of provided instructions, typically to the latter or more salient part."
        },
        {
            "title": "5 Experiments",
            "content": "Experimental Details The experiments are conducted on three variants from our diagnostic set ReasoningTrap, which consists of ConditionedMath (AIME, MATH500), and PuzzleTrivial. In Table 2 and Table 3, we report the p-pass@1 scores across various models, including Qwen2.5-32B-Instruct (Yang et al., 2024), QwQ-32B (Team, 2025c), Qwen332B (Team, 2025b), Qwen3-235B, DeepSeek V3 (671B) (DeepSeek-AI, 2024), DeepSeek R1 (671B) (DeepSeek-AI, 2025), and proprietary models ChatGPT-4o, GPT-4o, o3-mini, o4mini (OpenAI, 2024), Google gemini2.5-flash (Google DeepMind, 2025b) and Claude 3.7 8 Table 4: Budget Forcing and Prompt Hinting on ReasoningTrap. (a) ConditionedMath AIME vs. Original AIME ConditionedMath AIME Original AIME p-pass@1 pass@1 p-score p-pass@1 pass@1 p-score Qwen3-32B 33.256.58 29.606.32 76.844.91 75.426.88 72.796.95 86.763. Budget Force Prompt Hint + low + medium + high + Hint 1 + Hint 2 + Hint 3 53.667.63 44.076.94 39.826.97 45.308.08 38.247.42 41.237. 51.477.46 39.716.69 36.036.94 42.658.14 37.507.48 36.037.17 90.442.80 86.763.38 83.094.44 86.034.11 75.004.95 83.824.71 31.095.98 52.218.00 57.607.31 81.366.59 76.276.17 74.196. 28.685.98 50.007.76 57.357.35 75.746.55 73.536.15 69.856.82 87.503.38 83.094.57 91.912.29 86.763.98 86.763.54 91.183.79 (b) ConditionedMath MATH500 vs. Original MATH500 ConditionedMath MATH Original MATH500 p-pass@1 pass@1 p-score p-pass@1 pass@ p-score Qwen3-32B 34.605.60 30.635.59 75.503.74 87.984.70 85.504. 91.502.21 Budget Force Prompt Hint + low + medium + high + Hint 1 + Hint 2 + Hint 3 51.326.35 43.755.99 40.796. 46.886.51 42.116.37 37.756.00 42.005.91 36.005.90 34.005.92 40.506.46 37.006.20 32.005.85 76.004.46 80.003.91 76.003.97 80.004.16 76.004.10 75.504.31 68.685.51 80.335.28 82.515. 88.764.18 88.464.64 90.064.16 68.005.39 76.505.32 81.005.13 85.504.41 85.004.63 87.004.24 91.002.34 91.502.33 91.501.97 89.002.49 91.002.55 90.502.56 sonnet (Claude, 2025). These models are grouped into seven pairs, each consisting of base model and its corresponding reasoning-aligned variant trained for long-form reasoning. The experiments are conducted with Chain-of-Thought prompting, by wrapping the given question with Please reason step by step, and put your final answer within boxed{}.nn{Question}. Sampling was performed 16 times per question for the main experiments in Table 2 and Table 3, and 4 times per question for the other experiments. Evaluation Details For math problems, correctness was determined via exact matching after cleaning step that removes unwanted parts such as measurement units. For puzzle problems, where answers are often in free-form sentences, an LLM was used to assess the correctness by comparing the models output against the ground truth answer."
        },
        {
            "title": "5.1 Observations on Various Reasoning Models",
            "content": "In most configurations, the reasoning models under-perform compared to their base model counterparts, contrary to expectations, given the typical capability gap favoring larger or instruction-tuned models. On both ConditionedMath and PuzzleTrivial, base models achieve significantly higher p-pass@1 scores. This suggests that, once the model correctly interprets the question, base models tend to adhere more rigorously to the original instruction and are more likely to reach the correct answer."
        },
        {
            "title": "5.2 Ablation Study",
            "content": "Budget Forcing Following budget forcing from Team (2025b), we append the prompt Considering the limited time by the user, have to give the solution based on the thinking directly now.</think> to the generated response and continue output generation once the predefined token budget is reached. This enforce model to directly generate answer without furthre thinking. We apply low and medium token budget for each dataset and observe the g-pass@1 score. For MATH500, we use 2000, 4000, 6000 as low, medium, high budget and for AIME, we apply 2000, 6000, 10000 as low, medium, high budget, each. As shown in Table 4, even though low token budget is beneficial for our diagnostic set, it harms the performance on the original datasets. Based on this result, we confirm that strict budget forcing has inherent problems. 9 Prompt Hinting While we have carefully filtered out nonsensical or contradictory conditions that render problems unsolvable, there remains possibility that the model might attribute unusual patterns to errors made by the user. Although such behavior is not inherently incorrect, it could undermine the intended solution process. To mitigate this, we introduced an additional prompt to the models response, explicitly stating that the problem contains no typographical errors and that the model must adhere to the instructions provided in the prompt. We conducted experiments on the ConditionedMath dataset, testing three variants of the additional prompt hints based on the 3 major pattern observed in Figure 4. Despite providing this additional condition to focus on the given instructions, we observe that the model still continues to display similar behavior of reasoning rigidity. Specifically, it persists to relying on familiar reasoning patterns, without adapting to the new conditions introduced by the prompts. As result, even though some of the prompt shows better performance on given dataset, some prompt harm the performance on original dataset."
        },
        {
            "title": "Limitation",
            "content": "This study identifies clear limitation in RL-based reasoning models, reasoning rigidity, but does not provide fundamental analysis of which specific components of the reinforcement learning framework are responsible for this phenomenon. Since reasoning rigidity is significantly more pronounced in reasoning models compared to non-reasoning models, investigating its underlying causes remains critical direction for future work. Another important caveat is that our diagnostic set focuses exclusively on mathematics and puzzle-solving tasks, which may introduce domain bias. It therefore remains unclear whether similar rigidity arises in other application areas where the nature of correct reasoning differs substantially. Extending our evaluation to these domains will be necessary to assess the generality of reasoning rigidity and to tailor domain-specific mitigation strategies."
        },
        {
            "title": "Conclusion",
            "content": "To the best of our knowledge, this work is the first to highlight the surprising rigidity exhibited by advanced reasoning models during multi-step reasoning. Despite their strong capability to comprehend both user-provided conditions and problem details, these models often failnot due to lack of understanding, but because they default to ingrained reasoning patterns over faithfully following user instructions. To investigate this phenomenon, we construct high-quality, curated diagnostic dataset and propose tailored metric designed to capture both reasoning rigidity and contamination from familiar solution trajectories."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Will Arnold for his constructive feedback and assistance on experimental design on reasoning models."
        },
        {
            "title": "References",
            "content": "AIME. AIME 2024. https://artofproblemsolving.com/wiki/index.php/2024_AIME_I?srsltid= AfmBOoqfUhmDQZd1-etOmNCjXpUgzyI46O4aZZ8hjLFPLSGMw_35PqJJ. Accessed: 2025-05. 5 Saeid Alavi Naeini, Raeid Saqur, Mozhgan Saeidi, John Giorgi, and Babak Taati. Large language models are fixated by red herrings: Exploring creative problem solving and einstellung effect using the only connect wall dataset. Advances in Neural Information Processing Systems, 36: 56315652, 2023. 3 Roberto Araya. Do chains-of-thoughts of large language models suffer from hallucinations, cognitive biases, or phobias in bayesian reasoning? arXiv preprint arXiv:2503.15268, 2025. 3 Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 1 Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. 24 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. 1 Claude. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2025-05. 1 Claude. Claude 3.7 Sonnet. https://www.anthropic.com/news/claude-3-7-sonnet, 2025. Accessed: 2025-05. 9 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 1 Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. 24 DeepSeek-AI. Deepseek-v3 technical report, 2024. 8 DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Hugging Face. Open r1: fully open reproduction of deepseek-r1, 2025. 23 Google DeepMind. Gemini 2.5 Pro. https://deepmind.google/technologies/gemini/pro/, 2025a. Accessed: 2025-05. 1 Google DeepMind. Gemini 2.5 flash: Faster, https://blog.google/ technology/google-deepmind/google-gemini-updates-io-2025/, 2025b. Blog post, accessed 22 May 2025. lower-cost reasoning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, 3 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). 1, 5 Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. 23, 24 11 Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023. 3 Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 1, 3 Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 1 Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, MoonarXiv preprint tae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv:2504.16828, 2025. 23 Jonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu, Ahmed Alaa, and Danilo Bernardo. Limitations of large language models in clinical problem-solving arising from inflexible reasoning. arXiv preprint arXiv:2502.04381, 2025. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. 1 Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. 23 Kyle Moore, Jesse Roberts, Thao Pham, and Douglas Fisher. Reasoning beyond bias: study on counterfactual prompting and chain of thought reasoning. arXiv preprint arXiv:2408.08651, 2024. 4 OpenAI. Gpt-4o system card. https://openai.com/index/gpt-4o-system-card/, 2024. Accessed 22 May 2025. 8 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1 Syed Rifat Raiyan, Md Nafis Faiyaz, Shah Md Jawad Kabir, Mohsinul Kabir, Hasan Mahmud, and Md Kamrul Hasan. Math word problem solving by generating linguistic variants of problem statements. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 362378, 2023. 3 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 24 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 24 Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, and Chuang Gan. Satori: Reinforcement learning with chainof-action-thought enhances llm reasoning via autoregressive search, 2025. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William Hamilton. Clutrr: diagnostic benchmark for inductive reasoning from text. arXiv preprint arXiv:1908.06177, 2019. 1 Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 1, 3 NovaSky Team. Sky-t1: Fully open-source reasoning model with o1-preview performance in 450 budget. https://novasky-ai.github.io/posts/sky-t1, 2025a. Accessed: 2025-05-23. 23 Qwen Team. Qwen3, 2025b. 3, 8, 12 Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025c. 1, 3, 8 RUCAIBox STILL Team. Still-3-1.5b-preview: Enhancing slow thinking abilities of small models through reinforcement learning. 2025d. 23 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 3 Vellum AI. Reasoning models are indecisive parrots, 2025. Accessed: 2025-05-11. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 3 Sean Williams and James Huckle. Easy problems that llms get wrong. arXiv preprint arXiv:2405.19616, 2024. 5 An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 8, Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: reading comprehension dataset requiring logical reasoning. arXiv preprint arXiv:2002.04326, 2020. 1 Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. 3 Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. arXiv preprint arXiv:2412.00154, 2024. 1 Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. Absolute zero: Reinforced self-play reasoning with zero data, 2025."
        },
        {
            "title": "A Dataset Construction Details",
            "content": "As shown in Figure 2, ConditionedMath construction pipeline consists of two stages. We provide the detailed prompt provided to gpt-4o-mini and o3-mini in the construction phase."
        },
        {
            "title": "User",
            "content": "[Instruction]: Given the original question, generate 5 different modified questions that are completely unusual conditions, each producing different solution process and different answer from the original. Please double check to make sure newly generated modified question has following properties: should be valid question. should be different from the original question. But, mere change of constant or variable is not allowed. should be solvable without error. [Output Format] modifications: modified reason: ... (in LaTeX) modified question: ... (in LaTeX) modified reason: ... (in LaTeX) modified question: ... (in LaTeX) ... (total 5 entries) [Example 1]: 1. original question: Get largest integer smaller than ( 7 + 5) 2. original solution: Expand ( 5)6 via the binomial theorem, compute each term exactly, then subtract 1 to find the greatest integer less than the sum. 7 + 3. modification reason: Rounding each square root term down before exponentiation transforms all inner terms into integers, making the final calculation trivial. 4. modified question: Get largest integer smaller than ( 5)6. Added constraint: Square root terms are rounded down to the nearest integer before exponentiation. Do not use calculator. 7 + [Example 2]: 14 1. original question: Determine w2 + x2 + y2 + z2 if x2 22 1 x2 42 1 x2 62 1 x2 82 1 + + + + y2 22 32 + y2 42 32 + y2 62 32 + y2 82 32 + z2 22 52 + z2 42 52 + z2 62 52 + z2 82 52 + w2 22 72 = 1 42 72 = 1 w2 62 72 = 1 w2 82 72 = 1 2. original solution: Solve the 44 linear system in variables x2, y2, z2, w2 by expressing it in matrix form and inverting or using elimination to find each squared term, then sum them. 3. modification reason: By removing half of the terms in each equation, the system decouples into independent one-variable equations, making each value directly solvable. 4. modification question: Determine w2 + x2 + y2 + z2 if x2 22 1 x2 42 1 x2 62 1 x2 82 1 + + + + y2 22 32 + y2 42 32 + y2 62 32 + y2 82 32 + z2 22 52 + z2 42 52 + z2 62 52 + z2 82 52 + w2 22 72 = 1 42 72 = 1 w2 62 72 = 1 w2 82 72 = 1 Before solving problem, remove last two terms in left hand side of first two equations and remove first two terms in left hand side of last two equations. After removing terms, solve problem and determine value. [Example 3]: 1. original question: regular 12-gon is inscribed in circle of radius 12. The sum of the lengths of all sides and diagonals of the 12-gon can be written 6, where a, b, and are positive integers. in the form + Find + + + d. 3 + 2 + 2. original solution: Compute each chord length using 2R sin(πk/12) for = 1, 2, . . . , 6, sum like terms to express in the prescribed form, then add coefficients. 3. modification reason: Replacing the 12-gon with 3-gon (triangle) reduces the number of chords to 3, making the sum of side lengths immediate. 4. modified question: regular 12-gon is inscribed in circle of radius 12. The sum of the lengths of all sides and diagonals of the 12-gon can be written in 6, where a, b, and are positive integers. Find the form + + + + d. Before solving problem, change regular 12-gon into regular triangle, and solve changed problem. 3 + 2 + [Input]: original question: Zou and Chou are practicing their 100-meter sprints by running 6 races against each other. Zou wins the first race, and after that, the probability that one of them wins race is 2 3 if they won the previous race but only 1 3 if they lost the previous race. The probability that Zou will win exactly 5 of the 6 races is , where and are relatively prime positive integers. Find + n. original solution: 15 1. The probability that Zou loses race is 1 3 , and the probability that Zou wins the next race is 1 3 . For each of the three other races, the probability that Zou wins is 2 3 . 2. The probability that Zou loses ace is 1 races, the probability that Zou wins is 2 3 . (cid:1)1 (cid:0) 2 243 = 16 243 = 48 81 . 3 3. 4 (cid:0) 1 4. The answer is 16 + 81 = 97. 243 + (cid:1)2 (cid:0) 2 3 + (cid:0) 1 = 32 (cid:1)3 (cid:1)4 3 3 . For each of the four other gpt-4o-mini modified question: Zou and Chou run single 100-meter sprint. Zou wins the first race. The probability that person wins race given they won the previous one is 2 3 . What is the probability that Zou wins exactly 1 of the 1 race? 3 , and if they lost the previous one is 1 modified solution: 1. Since there is only one race and it is given that Zou wins the first race, he wins exactly one out of one. 2. Therefore the probability is 1. modified answer: 1 User Original question: Zou and Chou are practicing their 100-meter sprints by running 6 races against each other. Zou wins the first race, and after that, the probability that one of them wins race is 2 3 if they won the previous race but only 1 3 if they lost the previous race. The probability that Zou will win exactly 5 of the 6 races is , where and are relatively prime positive integers. Find + n. Modified question: Zou and Chou run single 100-meter sprint. Zou wins the first race. The probability that person wins race given they won the previous one is 2 3 . What is the probability that Zou wins exactly 1 of the 1 race? 3 , and if they lost the previous one is 1 [Solution]: Solve the modified question and provide the solution and the answer. 1. concise chain-of-thought solution (your step-by-step reasoning) 2. final ground-truth answer [Analysis]: Analyze the modified question with concise yes/no and one-sentence reasoning for each: 1. Mathematically valid and consistent? 2. Different solution path or different answer from the original? 3. Solvable without error? Respond only with JSON object in this form, without additional text: ... { } \"solution\": \"...\", \"answer\": \"...\", \"valid\": \"...\", \"different\": \"...\", \"solvable\": \"...\" 16 o3-mini [Return]: { } \"solution\": \"1. Since there is only one race and it is given that Zou wins the first race, he wins exactly one out of one. 2. Therefore the probability is 1.\", \"answer\": \"1\", \"valid\": \"Yes. The modified question is internally consistent and well-defined.\", \"different\": \"Yes. The modification reduces the scenario to single race, changing both the solution path and the answer.\", \"solvable\": \"Yes. The result follows directly from the given conditions.\""
        },
        {
            "title": "B Perception Score Evaluation Prompt",
            "content": "To evaluate if the models reasoning is properly grounded on the provided user instruction, we measure whether the model perceives the instruction or not by comparing with the given question and ground truth solution. For this purpose, gpt-4o-mini is prompted with simple instruction to determine whether subset of model reasoning is similar with the ground truth. From the observation that most of the perception process is handled in the beginning of the reasoning phase, we split first 15 paragraphs of reasoning from model, then compare with the ground truth reasoning and question."
        },
        {
            "title": "User",
            "content": "[Evaluation Criteria]: Evaluate whether part of the model output is similar to the ground truth solution. The ground truth solution is provided as list of reasoning steps. Even if the model output is not exactly the same as the ground truth, it should be considered correct if subset of the model output contains reasoning steps that are similar to any of the ground truth steps. The question is ... The ground truth solution is ... The model output is ... [Output Format]: Answer in true or false. gpt-4o-mini true or false"
        },
        {
            "title": "C Dataset Samples",
            "content": "We present several examples of ConditionedMath and PuzzleTrivial in Figure 5, Figure 6, and Figure 7. (a) MATH500 example 1 (b) MATH500 example 2 (c) MATH500 example 3 Figure 5: ConditionedMath (MATH500) sample problems (a) AIME example 1 (b) AIME example 2 (c) AIME example 3 Figure 6: ConditionedMath (AIME) sample problems 20 (a) PuzzleTrivial example (b) PuzzleTrivial example 2 (c) PuzzleTrivial example 3 (d) PuzzleTrivial example 4 Figure 7: PuzzleTrivial sample problems"
        },
        {
            "title": "D Discussions",
            "content": "D.1 Relationship Between Output Token Length and Accuracy Using the reasoning effort parameter of o4-mini, we demonstrate that just using small amount of tokens for reasoning do not lead to performance gain in our dataset, ReasoningTrap. Although o4mini underperforms compared to the base model, increasing its reasoning effort consistently yields better results. This proves that our curated diagnostic set require complex reasoning in most cases, and simply choosing short reasoning leads to performance drop. Table 5: Reasoning effort and Performance on ReasoningTrap p-pass@1, pass@1, and perception score on ConditionedMath. (a) ConditionedMath (AIME)"
        },
        {
            "title": "Reasoning Effort",
            "content": "p-pass@1 pass@1 p-score o4-mini + low + medium + high 31.256.59 41.987.10 36.906. 19.125.49 25.006.06 22.795.91 58.826.75 59.566.84 61.766.78 (b) ConditionedMath (MATH500)"
        },
        {
            "title": "Reasoning Effort",
            "content": "p-pass@1 pass@1 p-score o4-mini + low + medium + high 39.065.76 51.806.32 53.476. 26.505.17 37.506.28 38.506.11 64.005.06 69.505.55 72.005.42 22 D.2 Model Size and Accuracy We compare non-distilled reasoning models by comparing reasoning models that are directly trained from Qwen2.5 1B, 3B, 7B, and 14B (Yang et al., 2024). Since Qwen3 0.7B, 1.7B, 3B, 8B models are distilled models from the largest dense reasoning model Qwen3-32B, this is out of scope for our experimental purpose. We evaluate DeepScaleR 1.5B (Luo et al., 2025), STILL-3-1.5B-preview (Team, 2025d), OpenR1-Qwen-7B (Face, 2025), ThinkPRM-14B (Khalifa et al., 2025), Sky-T132B-Preview (Team, 2025a), OpenReasoner-Zero-32B (Hu et al., 2025). We use instruction-tuned model for evaluating base models performance. On ConditionedMath AIME and MATH500, the base model Qwen2.5 Instruct outperforms its counterparts that have been fine-tuned for extended mathematical reasoning. Except for the smallest variant, Qwen2.5 Instruct 1.5B, the base model achieves the highest p-pass@1 score among all evaluated models. Interestingly, although the fine-tuned reasoning models consistently record higher perception scoresreflecting stronger understanding of each questions conditions and the derivation of optimal solutionstheir final accuracy suffers as result of reasoning rigidity. Table 6: Model Size and Performance p-pass@1, pass@1, and perception score on ConditionedMath. (a) ConditionedMath (AIME) Base + Reasoning Model p-pass@1 pass@1 p-score Qwen2.5-1.5B + DeepScaleR 1.5B + STILL-3-1.5B-preview 39.945.65 38.296.24 41.535.80 24.634.04 33.826.18 37.505.43 56.624.89 81.624.34 81.434.23 Qwen2.5-7B + OpenR1-Qwen7B Qwen2.5-14B + ThinkPRM-14B 62.968.10 49.537.33 51.477.53 47.066.57 79.414.89 78.685. 58.437.58 33.335.92 48.537.24 29.045.88 79.604.38 82.174.22 Qwen2.5-32B + SkyT1-32B-Preview + OpenReasoner-Zero-32B 59.127.81 56.576.71 53.276. 45.777.22 52.216.49 48.906.37 75.555.01 86.763.14 81.434.23 (b) ConditionedMath (MATH500) Base + Reasoning Model p-pass@1 pass@ p-score Qwen2.5-1.5B + DeepScaleR 1.5B + STILL-3-1.5B-preview 39.845.27 41.045.44 35.215.11 20.253.72 33.385.40 30.755.03 48.004.85 79.503.74 75.623. Qwen2.5-7B + OpenR1-Qwen7B Qwen2.5-14B + ThinkPRM-14B 55.566.14 45.816.22 38.005.94 39.506. 67.505.68 77.504.12 61.505.65 37.445.22 44.125.54 30.384.97 70.124.46 76.123.29 Qwen2.5-32B + SkyT1-32B-Preview + OpenReasoner-Zero-32B 55.956.02 54.805.67 45.816.22 40.885.74 44.625.52 39.506.02 70.384.39 76.883.67 77.504.12 23 D.3 RL Training Objective and Accuracy Reasoning models are trained from base large language models by various strategies, including GRPO (Shao et al., 2024), PPO (Schulman et al., 2017), or even zero-data regime (Zhao et al., 2025). Open-Reasoner-Zero (Hu et al., 2025) is fine-tuned from the Qwen2.5-7B-Instruct model using proximal policy optimization (PPO) with simple binary reward for answer correctness. Satori-7B (Shen et al., 2025) explicitly trains its base model to decide when to reflect on previous actions and to incorporate an external process reward. Absolute Zero Reasoner (Zhao et al., 2025) introduces novel reward scheme in which the LLM serves both as task proposer and task solver, with outputs verifiable in code. RM-R1 (Chen et al., 2025) structures its reward to improve alignment with human preferences during intermediate reasoning steps. Eurus-PRIME (Cui et al., 2025) employs an iterative training regimen combining policy model that generates rollouts and an implicit process-reward model that verifies them. ThinkPRM is fine-tuned from the R1-distilled Qwen14B base model (Qwen2.5-14B-Instruct) using the generative PRM objective, which evaluates the stepby-step correctness of the reasoning process. Among all variants of reinforcement-learning objectives, the base models Qwen2.5-7B and Qwen2.514B achieved outstanding performance p-pass@1 in most cases. This suggests that current RL regimes may exacerbate the reasoning rigidity inherent in these models. Hence, further exploration of reinforcement-learning algorithms that are robust to reasoning rigidity is essential for the development of faithful and credible reasoning systems. Table 7: Performance Comparison on Reasoning Models Trained with Different RL Strategies. (a) ConditionedMath (AIME) Base + RL Objective p-pass@ pass@1 p-score Qwen2.5-7B + Open-Reasoner-Zero + Satori-7B 62.968.10 47.497.42 53.333.03 51.477.53 43.016.92 4.923. 79.414.89 84.384.17 5.683.84 + Absolute Zero Reasoner 49.867.11 33.466.14 63.425.52 + RM-R 54.836.94 44.266.61 76.105.08 + Eurus-PRIME 59.168.24 40.447. 61.216.64 Qwen2.5-14B + Absolute Zero Reasoner + ThinkPRM 58.437.58 50.737.27 33.335.92 48.537.24 34.386.63 29.045.88 79.604.38 63.054.46 82.174. (b) ConditionedMath (MATH500) Base + RL Objective p-pass@1 pass@1 p-score Qwen2.5-7B + Open-Reasoner-Zero + Satori-7B 55.566.14 50.936.16 49.506.15 38.005.94 40.506.06 37.255.96 67.505.68 74.124.39 75.004.59 + Absolute Zero Reasoner 37.285. 22.624.10 56.004.62 + RM-R1 36.814.39 26.503.89 68.253. + Eurus-PRIME 57.296.52 42.386.20 72.004.71 Qwen2.5-14B + Absolute Zero Reasoner + ThinkPRM 61.505.65 44.255.34 37.445.22 44.125.54 26.254.42 30.384.97 70.124.46 57.634.53 76.123."
        }
    ],
    "affiliations": [
        "AITRICS",
        "KAIST"
    ]
}