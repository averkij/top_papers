{
    "paper_title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization",
    "authors": [
        "Hyung Gyu Rho"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of $\\beta$-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative $\\beta$ values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 2 4 3 5 0 . 0 1 5 2 : r MARGIN ADAPTIVE DPO: LEVERAGING REWARD MODEL FOR GRANULAR CONTROL IN PREFERENCE OPTIMIZATION"
        },
        {
            "title": "A PREPRINT",
            "content": "Hyung Gyu Rho sirano1004@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Direct Preference Optimization (DPO) has emerged as simple and effective method for aligning large language models. However, its reliance on fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of β-DPO suffers from its own limitations: its batch-level adaptation applies single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative β values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), method that provides stable, data-preserving, and instance-level solution. MADPO employs practical two-step approach: it first trains reward model to estimate preference margins and then uses these margins to apply continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide comprehensive theoretical analysis, proving that MADPO has well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3% on High Quality data and +10.5% on Low Quality data over the next-best method. Our results establish MADPO as more robust and principled approach to preference alignment. The code for our experiments is available online at https://github.com/sirano1004/ Margin-Apative-Direct-Preference-Optimization. Keywords Direct Preference Optimization Preference Alignment Adaptive Regularization"
        },
        {
            "title": "Introduction",
            "content": "Aligning Large Language Models (LLMs) with human preferences has become cornerstone of modern AI, enabling models that are more helpful and harmless [Bai et al., 2022], follow complex instructions [Ouyang et al., 2022], and excel at sophisticated tasks [Ziegler et al., 2019]. The dominant paradigm for this is learning from preference data, which has given rise to range of powerful techniques. While early successes were driven by multi-stage methods like Reinforcement Learning from Human Feedback (RLHF), the field has recently seen the emergence of more direct and stable approaches, most notably Direct Preference Optimization (DPO) [Rafailov et al., 2023]. While DPO offers more direct approach to preference alignment, its effectiveness is constrained by critical factor: the joint influence of the temperature parameter, β, and the quality of the preference data. Seminal work in this area by Wu et al. [2024a] demonstrated that the optimal choice of β is highly contingent on the reward margin of given pair. Their analysis revealed that easy pairs with large margin benefit from high, conservative β to prevent overfitting, whereas hard pairs with subtle margin require low, aggressive β to ensure the learning signal is captured. The vanilla DPO framework, with its single fixed β applied to all samples, is fundamentally unable to reconcile these competing Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization requirements. This inherent tension has motivated recent work on adaptive regularization strategies, which aim to tailor the learning objective to the difficulty of each preference pair. The challenge of adaptive regularization has led to several innovations that improve upon vanilla DPO. Identity Preference Optimization (IPO) [Azar et al., 2024], for instance, effectively mitigates the general overfitting issue by replacing the loss function with squared-error objective. While not explicitly designed to resolve the tension between highand low-margin data, its uniform target margin partially addresses the problem by regularizing easy pairs, though at the risk of being overly conservative on more informative examples. The most direct attempt to solve this is β-DPO [Wu et al., 2024a], which introduces adaptive, batch-level strategies. However, while demonstrating improved results, its mechanisms introduce significant new challenges. Its β-batch adaptation, for instance, is potentially unstableproducing divergent negative β for difficult dataand applies single, compromised temperature to mixedmargin batches. Furthermore, its β-guided filtering approach can be data-inefficient, as it potentially discards very high and low margin samples that may still contain useful learning signals. These issues of potential instability, coarse granularity, and data inefficiency highlight the need for solution that is not only instance-level and data-preserving, but also inherently stable. In this paper, to address these challenges, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), method that precisely controls the DPO objective through practical two-step process. First, we train standard reward model to learn how strongly one response is preferred over another for each training example. Our approach then leverages this reward model to guide the DPO policy, which works by learning to match the preferences captured by the reward model. MADPO strategically modifies the strength of the preference signal from the reward model before showing it to the policy. For hard and informative pairs, it amplifies the signal to make the preference seem stronger, forcing the policy to learn more aggressively, achieving the same effect as low β. Conversely, for easy and uninformative examples where the preference is already obvious, it dampens the signal to make the preference seem weaker, which provides stabilizing, per-sample regularization, achieving the same effect as high β. This strategic modification of the preference signal allows for granular, instance-level control, making the alignment process more robust and data-efficient. Our theoretical analysis validates the design of MADPO. We demonstrate that its instance-level weighting scheme successfully regularizes the learning objective for easy preference pairs while amplifying the signal for difficult ones, all while maintaining well-behaved and stable optimization landscape with bounded gradients. Crucially, we also prove that this granular control is not brittle; our analysis provides formal guarantee that the practical two-step algorithm is robust to the estimation errors inherent in reward modeling. These theoretical results establish MADPO as principled and reliable method, claim we validate with empirical experiment. We validate our theoretical claims with series of experiments on controlled sentiment generation task using the IMDB dataset. Our results demonstrate that MADPO consistently and significantly outperforms strong baselines, including DPO, IPO, and β-DPO, across all data quality tiers: High, Medium, and Low. Notably, our method shows strong robustness to degrading data quality, with performance gains over the next-best baseline, β-DPO, ranging from significant +10.5% on the challenging Low Quality dataset to +33.3% on the High Quality dataset. Further analysis provides deeper insight into our methods mechanics: detailed ablation study reveals that this robust performance is primarily driven by the amplification mechanism, while sensitivity analysis demonstrates that the methods hyperparameters are well-behaved and exhibit clear, predictable trends. These empirical findings confirm the claims of our theoretical analysis and establish MADPO as more reliable and effective method for preference alignment. The remainder of this paper is structured as follows. Section 2 reviews the necessary background on DPO and related methods. Section 3 introduces our proposed method, MADPO, detailing its adaptive weight function and practical twostep training algorithm. Section 4 provides theoretical analysis of MADPO, establishing its stability and robustness to reward estimation errors. Section 5 presents our empirical results, demonstrating MADPOs superior performance against strong baselines, particularly on datasets of varying quality. Finally, Section 6 concludes the paper and discusses future work. All detailed proofs for our theoretical claims are provided in the Appendix."
        },
        {
            "title": "2 Preliminaries",
            "content": "The goal of preference alignment is to fine-tune language model policy πθ, parameterized by θ, using dataset of human preferences = {(x, yw, yl)}N i=1. For each prompt x, yw is the response preferred over the response yl. The alignment process is typically framed by modeling the probability of these preferences. 2.1 Reinforcement Learning from Human Feedback (RLHF) The RLHF paradigm [Ouyang et al., 2022] aligns policy in two main stages: reward modeling and policy optimization. 2 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization 1. Reward Modeling. This stage aims to learn reward model rϕ(x, y), parameterized by ϕ, that reflects human preferences. The probability that yw is preferred to yl is modeled using the Bradley-Terry-Luce (BTL) framework [Bradley and Terry, 1952, Luce et al., 1959]: (yw ylx; ϕ) = σ(hϕ (x, yw, yl)) = σ(rϕ (x, yw) rϕ (x, yl)), (1) where σ() is the logistic function. The optimal reward model parameters, ϕ, are found by maximizing the likelihood of the preference dataset, which corresponds to minimizing the following negative log-likelihood loss: LRM(rϕ) = E(x,yw,yl)D [log σ(hϕ(x, yw, yl))] . (2) In the second stage, the policy πθ is then fine-tuned using the trained reward model ˆϕ(x, y), 2. Policy Optimization. where ˆϕ is the empirical estimate of the optimal parameters ϕ. The policy is optimized to maximize the expected reward while being regularized by Kullback-Leibler (KL) divergence penalty against reference policy πref: max θ ExDp,yπθ(yx)[r ˆϕ(x, y)] βDKL(πθ(yx)πref(yx)). Here, β is hyperparameter that controls the strength of the KL regularization, and Dp represents the dataset of prompts. 2.2 Direct Preference Optimization (DPO) DPO [Rafailov et al., 2023] is an alternative that bypasses the explicit reward modeling and reinforcement learning stages. The key insight is that the optimal solution to the KL-regularized objective has closed-form solution that connects the optimal policy πθ to the optimal reward function rϕ : rϕ (x, y) = β log πθ (yx) πref(yx) + β log Z(x), (3) where Z(x) is the partition function that normalizes the distribution. By substituting this mapping (Eq. 3) into the BTL preference model, the likelihood can be expressed directly in terms of the policy πθ. This allows for end-to-end optimization of the policy by minimizing single negative log-likelihood loss: LDPO(πθ; πref) = E(x,yw,yl)D [log σ (βhθ(x, yw, yl))] πθ(ywx) πref(ywx) = E(x,yw,yl)D β log log σ (cid:18) (cid:20) β log πθ(ylx) πref(ylx) (cid:19)(cid:21) . For notational clarity, we define the following reward margin functions: The explicit reward margin from reward model rϕ: The implicit reward margin from policy πθ: hϕ(x, yw, yl) = rϕ(x, yw) rϕ(x, yl). hθ(x, yw, yl) = log πθ(ywx) πref(ywx) log πθ(ylx) πref(ylx) . 2.3 Limitations of Vanilla DPO The relationship between the true reward function and the optimal policy is well-defined. From Eq. 3, we can see that the explicit reward margin is proportional to the implicit reward margin: hϕ (x, yw, yl) = βhθ (x, yw, yl). Here, the inverse temperature β acts as global hyperparameter. smaller β encourages larger difference in the policys log-ratios for given explicit reward margin, promoting more aggressive, confident updates. Conversely, larger β encourages more conservative updates. However, recent work has revealed that using single, static β as global hyperparameter is significant limitation of the vanilla DPO framework, often leading to overfitting. As argued by Azar et al. [2024], this issue is particularly acute on finite datasets. If all annotators in sample unanimously prefer one response, the empirical explicit reward margin 3 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization becomes infinite. To match this, the DPO objective will push the learned log-policy difference, hˆθ, to be arbitrarily large, causing the model to become overconfident and overfit to the winning response. Complementing this finding, Wu et al. [2024a] suggest that single β is insufficient for handling the diverse quality of preference data. They find that easy pairs with large explicit reward margin are best handled with high β (a more conservative update) to prevent overfitting. In contrast, hard pairs with small, subtle margin require low β (a more aggressive update) to effectively learn the preference signal. This tension reveals the need for more dynamic, instance-aware approach to regularization, which motivated the development of subsequent methods like IPO and β-DPO. 2.4 Identity Preference Optimization (IPO) Identity Preference Optimization (IPO) [Azar et al., 2024] addresses the overfitting issue by replacing the log-likelihood objective with squared-error loss, LIPO(πθ, πref ) = E(x,yw,yl)D (cid:34)(cid:18) hθ(x, yw, yl) (cid:19)2(cid:35) . 1 2β The mechanism of IPO can be understood by analyzing the optimality condition of its loss function. The squared-error loss is minimized for any given sample when the implicit reward margin satisfies βhθ (x, yw, yl) = 1/2. Unlike DPO, which attempts to match the true explicit reward margin hϕ , IPO effectively sets single, uniform target margin of 1/2 for every preference pair in the dataset. This has dual effect: for hard pairs where the true explicit margin is small (hϕ < 1/2), the higher target margin amplifies the learning signal. Conversely, for easy pairs where the true explicit margin is large (hϕ > 1/2), the low target margin aggressively dampens the signal, which is the source of the regularization. 2.5 β-DPO To address the limitations of fixed temperature, β-DPO [Wu et al., 2024a] introduces adaptive strategies to modulate the learning process. It proposes two primary mechanisms: Batch-Level β Adaptation (β-batch). This approach adapts the temperature β for each training batch using linear function of the batchs average implicit reward margin, hθ. The batch-specific temperature, βbatch, is set as: βbatch = β(1 + (hθ h0)), where hθ = 1 batch (cid:88) hθ(x, yw, yl). (x,yw,yl)batch Here, β is base temperature, is scaling factor between zero and one, and h0 is predetermined threshold. This allows batches with higher average implicit margins to be trained with higher, more conservative βbatch. β guided filtering. This approach modifies the training data by stochastically filtering each batch. For each sample (x, yw, yl) in batch, score is computed using the probability density function of Normal distribution, ( ; h0, σ2), evaluated at the policys current implicit margin, hθ(x, yw, yl). The score is highest for samples whose margin is close to the mean h0. new, smaller batch is then formed by performing weighted random sampling without replacement from batch, where the probability of selecting sample is proportional to its score. This method dynamically focuses training on examples of target difficulty level, effectively down-sampling both overly easy and potentially noisy pairs."
        },
        {
            "title": "3 Margin Adaptive Direct Preference Optimization (MADPO)",
            "content": "In this section, we introduce Margin Adaptive Direct Preference Optimization (MADPO), method that enhances the DPO objective by adaptively re-weighting each training sample. The core idea is to modulate the loss based on the explicit reward margin, hϕ, to amplify the learning signal from informative low-margin pairs while dampening it for easy, high-margin pairs to prevent overconfidence. This provides more granular and flexible approach to regularization than IPO and β-DPO. We begin by detailing the central component of our method: continuous, margin adaptive weight function. We then define the full MADPO loss function and describe the practical two-step algorithm for its optimization. 4 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization 3.1 Margin Adaptive Weight The central component of our MADPO method is the weight function which adaptively modulates the learning objective for each training sample based on its preference margin. This subsection provides detailed exposition of this functions design, its hyperparameters, and the reasoning behind its piecewise structure. To simplify the notation, where the context is unambiguous, any function will be denoted by its symbol alone, suppressing the explicit dependence on its arguments for conciseness. The core of our method is coefficient function, : [cmin, cmax], which maps the explicit reward margin hϕ to modulating scalar. This function is designed to be greater than 1 for low margins and less than 1 for high margins. It is defined as: c(hϕ) = cmin + 1 + (cid:16) cmax1 1cmin (cid:17) exp (λ(hϕ τ )) , cmax cmin Using this margin-dependent coefficient, we define piecewise weighting function, w(hϕ), which selectively modifies the likelihood ratio. (cid:40) σ(c(hϕ)hϕ) σ(hϕ) w(hϕ) = 1 if hϕ > τ if hϕ τ (4) The threshold τ > 0 provides concrete definition for what constitutes high-margin or easy preference pair. This value can be chosen based on practitioner judgment or derived from the data. Specifically, any pair is classified as high-margin if its absolute explicit reward margin satisfies hϕ τ , and as low-margin if its explicit margin satisfies hϕ < τ . The parameter cmax acts as the amplifier for low-margin pairs. As the explicit margin hϕ approaches zero, the coefficient approaches cmax. This forces the model to learn more aggressively from the most informative and subtle preferences. higher value provides stronger signal boost, but risks overfitting to noise in these difficult examples. By definition, this parameter must be greater than one to ensure amplification. The parameter cmin acts as the dampener for high-margin pairs, setting floor on the regularization. As the margin grows, the learned target gap is scaled down by factor approaching cmin. value near 0 instructs the model to almost entirely ignore obvious preferences, preventing overconfidence. value closer to 1 instructs the model to still learn from them, but with less intensity. By definition, this parameter must be between zero and one, cmin [0, 1), to ensure damping. The parameter λ controls the sharpness of the transition around the threshold τ . large λ creates steep, switch-like change, treating samples on either side of τ very differently. small λ creates much more gradual and smooth transition from amplification to dampening. While these intuitions provide strong starting points for setting the parameters, their optimal values are typically dataset-dependent. Therefore, the most rigorous approach is to perform hyperparameter search, using method like cross-validation on held-out set of preference data to find the combination that yields the best empirical performance. The piecewise nature of the weight function is crucial design choice for ensuring training stability. While the re-weighting mechanism works as intended for most pairs, it can lead to undesirable behavior at the extremes of the margin distribution without this piecewise control. We analyze the behavior of the core ratio σ(c(hϕ ) hϕ )/σ(hϕ ), evaluated at the optimal reward model parameter ϕ, in two distinct cases: For large positive margins (hϕ τ ): In this region, where cmin, the weight function converges to small, stable, positive value. This correctly applies consistent penalty to all easy pairs, achieving the desired regularization effect. For large negative margins (hϕ τ ): Without the piecewise cutoff, the weight function would explode. As hϕ , the weight can be approximated by w(hϕ ) e(cmin1)hϕ . Since cmin < 1 and hϕ is negative, the exponent is positive and grows linearly with hϕ . This growth in the weight would assign massive, potentially infinite loss to these samples, leading to severe gradient instability. The piecewise definition elegantly solves this problem. By setting w(hϕ ) = 1 for all hϕ τ , we cap this potential explosion. This ensures that samples with very large negative marginswhich may be mislabeled or adversarialare handled by the vanilla, stable DPO loss instead of causing the training to diverge. This design allows us to achieve our desired penalization for high positive margins without sacrificing the stability of the overall training process. The impact of this weighting on the final loss function is discussed in the following sections. 5 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization 3.2 Loss Function and Optimization Having defined the margin adaptive weight, w(hϕ), we now formally incorporate it into our final loss function. We then detail the practical, two-step procedure used to train policy with this new objective. The MADPO Loss Function. The MADPO loss for single preference pair is the vanilla DPO log-likelihood, re-weighted by our margin-dependent weight function: L(θ, ϕ; x, yw, yl) = w(hϕ(x, yw, yl)) log σ(βhθ(x, yw, yl)). (5) This loss depends on both the policy parameters, θ, (through hθ) and the reward model parameters, ϕ, (through hϕ). To optimize this effectively, we employ two-step approach. Step 1: Reward Model Estimation. First, we obtain high-quality estimate of the preference margins. This is achieved by training standard reward model, rϕ, on the preference dataset to find the estimated parameters, ˆϕ. This step is identical to the reward modeling stage of traditional RLHF: ˆϕ = argmin ϕ LRM(rϕ). Step 2: Margin Adaptive Policy Optimization. Second, we treat the estimated reward parameters ˆϕ as fixed, ground-truth source of preference margins. These parameters are plugged into our MADPO loss function, which now becomes an objective solely for the policy. We then find the final policy parameters, ˆθ, by minimizing the expectation of this loss over the dataset: ˆθ = argmin L(θ, ˆϕ) = argmin θ θ E(x,yw,yl)D (cid:104) (cid:105) L(θ, ˆϕ; x, yw, yl) . This two-step process provides stable and practical method for training policy that is explicitly aware of the nuance and difficulty of the preference data it learns from."
        },
        {
            "title": "4 Theoretical Analysis",
            "content": "In this section, we provide the theoretical justification for the MADPO algorithm. Our analysis proceeds in three parts. First, we establish that our method achieves its primary goal: in an idealized oracle setting, MADPO encourages the policy to be confident on informative, low-margin pairs while achieving the desired regularization for high-margin pairs. Second, we prove formal performance guarantee for our practical two-step algorithm, showing that the loss function is Lipschitz continuous and therefore robust to errors from the reward estimation stage. Finally, we demonstrate that the gradient and Hessian of the MADPO loss are scaled versions of their vanilla DPO counterparts, which shows that our method provides controllable training stability while retaining the well-behaved optimization landscape of DPO. 4.1 Oracle Characterization of Margin-Adaptive Regularization In this section, we formally characterize the behavior of the MADPO loss function, showing how it achieves the dual goals of amplifying the learning signal for informative low-margin pairs and regularizing the objective for high-margin pairs. To isolate this mechanism, we conduct our analysis in an oracle setting, assuming access to the true optimal reward parameters, ϕ. We begin with our first proposition, which characterizes how MADPO aggressively learns from low-margin pairs by optimizing the policy towards an amplified preference target. Proposition 4.1. Under the BTL model with an optimal reward model rϕ , the optimal policy parameter θ that minimizes the MADPO loss L(θ, ϕ; x, yw, yl) satisfies the following for any preference pair (x, yw, yl) Dlow: βhθ (x, yw, yl) = c(hϕ (x, yw, yl)) hϕ (x, yw, yl), where the low-margin subset Dlow is defined as: Dlow = {(x, yw, yl) hϕ (x, yw, yl) < τ }. This proposition formalizes the amplification mechanism of MADPO for low-margin data. For any sample in the subset Dlow, the coefficient c(hϕ ) is greater than one by construction. Consequently, the optimality condition reveals that the policy is not optimized to simply match the true explicit reward margin, hϕ , but rather an amplified version of 6 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization it, c(hϕ ) hϕ . This encourages the policy to learn more aggressively from these subtle and informative examples, increasing the separation in its log-ratios to greater degree than what the true reward margin alone would suggest. This is the core mechanism by which MADPO boosts the learning signal for hard, informative pairs. Having characterized the amplification mechanism for low-margin data, we now turn our attention to the complementary case: how MADPO achieves regularization for high-margin pairs. Proposition 4.2. Under the BTL model with an optimal reward model rϕ , the optimal implicit reward βhθ is monotonically increasing with respect to the coefficient c(hϕ ) for any preference pair (x, yw, yl) Dhigh. Formally: where the high-margin subset Dhigh is defined as: Dhigh = {(x, yw, yl) hϕ (x, yw, yl) τ }. (βhθ ) > 0, This proposition provides the formal justification for the regularization mechanism of MADPO on high-margin data. It establishes that the learned implicit reward margin, βhθ , is directly and monotonically controlled by the coefficient c. For any preference pair in the high-margin subset Dhigh, our method sets c(hϕ ) to value less than one. The monotonic relationship guarantees that this shrinks the optimization target, βhθ , relative to the true explicit reward margin, hϕ . This acts as powerful regularization tool, dampening the learning signal for easy pairs and preventing the policy from becoming overconfident or overfitting to these less informative examples. Our theoretical results establish that MADPO offers more granular, per-pair control over the learned preference margin compared to the global mechanisms of IPO and β-DPO. Propositions 4.1 and 4.2 formalize this: for each preference pair, the optimal policy learns to match dynamically scaled target, c(hϕ )hϕ , and this target margin can be monotonically controlled by the coefficient c. This provides sharp, per-example comparative-statics guarantee. In contrast, IPO regularizes globally (a uniform mechanism that does not adapt to each pairs margin), and β-DPO adapts batch-level temperature β shared across all samples in batch. Neither provides the sample-specific, per-pair control formalized by Propositions 4.1 and 4.2. 4.2 Lipschitz Continuity and Robustness to Reward Estimation Error Having analyzed our method in an ideal oracle setting, we now establish its robustness to the reward estimation errors that occur in practice. To do so, we prove that the MADPO loss function is Lipschitz continuous with respect to the reward model parameters, culminating in formal performance guarantee that bounds the impact of these errors. Our analysis in this section follows the theoretical framework established by Chowdhury et al. [2024]. We adopt the following assumptions from their work to analyze the stability of our method. Assumption 4.3. We assume the following constraints on the reward models parameter space, Φ: The parameter space is defined as Φ = {ϕ Rδ (cid:80)δ i=1 ϕi = 0}. For any parameter vector ϕ Φ, there exists constant > 0 such that its Euclidean norm is bounded: ϕ B. This assumption places two standard constraints on the reward models parameter space. The first, the zero-mean condition ((cid:80) ϕi = 0), is necessary for identification. Because the preference probability in the BTL model depends only on the difference in rewards, rϕ(x, yw) rϕ(x, yl), the underlying reward function is only unique up to an arbitrary constant shift. This constraint resolves the inherent shift ambiguity, ensuring the identification of reward function. The second condition, boundedness (ϕ B), is standard regularity assumption required in most theoretical analyses to ensure the parameter space is well-behaved, forming necessary prerequisite for the performance guarantees that follow. Assumption 4.4. The reward function rϕ(x, y) is assumed to be well-behaved with respect to its parameters. Specifically, there exist constants α0, α1, α2 > 0 such that for any ϕ Φ and any sample (x, y), the function, its gradient, and its Hessian are uniformly bounded: rϕ(x, y) α0, ϕrϕ(x, y) α1, 2 ϕrϕ(x, y) α2I. This assumption imposes standard smoothness and boundedness conditions on the reward function, rϕ. These conditions are crucial as they ensure that the explicit reward margin function, hϕ(x, yw, yl) = rϕ(x, yw) rϕ(x, yl), is also wellbehaved. Specifically, they imply that hϕ is bounded and Lipschitz continuous with respect to its parameters ϕ, and 7 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization that its gradient is also Lipschitz continuous. Such regularity assumptions are common prerequisite for establishing performance guarantees in the analysis of policy optimization algorithms and are consistent with the theoretical frameworks used in related work [Agarwal et al., 2021, Chowdhury et al., 2024]. Assumption 4.5. There exists constant Lθ > 0 such that for any policy parameters θ in the parameter space Θ and for any sample (x, yw, yl), the absolute value of the log-likelihood term is bounded: log σ(βhθ(x, yw, yl)) Lθ. This is standard technical assumption that is well-justified. It is mild condition, as it is fundamentally constraint that the policy, πθ, cannot assign probability of exactly 0 or 1 to any response. Furthermore, this condition is not arbitrary; it can be derived by imposing boundedness constraints on the policy function, πθ(yx), that are directly analogous to those we placed on the reward model in Assumption 4.4. This approach is consistent with similar assumptions made in prior theoretical analyses of preference-based learning, including the framework established by Chowdhury et al. [2024]. The following theorem provides high-probability bound that connects the estimation error of the reward model to the stability of our final loss function. This error is measured in data-dependent semi-norm, ˆΣϕ , which is induced by the empirical covariance of the reward gradients. Let the gradient vectors zi be defined with respect to the reward model parameters ϕ: zi = ϕhϕ(xi, yw,i, yl,i). Then, the empirical covariance matrix ˆΣϕ is given by: Theorem 4.6. Under Assumptions 4.3, 4.4 and 4.5, let ρ (0, 1] and κ > 0. Then, with probability of at least 1 ρ, the following bound holds for the MADPO loss function, for all (x, yw, yl) ˆΣϕ = 1 N (cid:88) i=1 ziz . (cid:12) (cid:12) (cid:12) ˆϕ ϕ ˆΣϕ +κI (cid:12)L(θ, ϕ; x, yw, yl) L(θ, ˆϕ; x, yw, yl) (cid:12) (cid:12) (cid:114) δ + log(1/ρ) γ + (cid:114) κ + α2 γ + α1α2B, where is the Lipschitz constant, and are absolute constants, and γ is constant dependent on the bound of the reward function: γ = 1 2 + e4α0 + e4α0 . This result certifies the plug-in stability of our practical, two-stage MADPO pipeline. It guarantees that the training objective remains stable even when using an estimated reward model, ˆϕ, instead of the optimal, ϕ. Concretely, Theorem 4.6 shows that for any given preference pair, the gap between the oracle loss L(θ, ϕ; x, yw, yl) and the plug-in loss L(θ, ˆϕ; x, yw, yl) is controlled linearly by the reward-parameter error. This guarantee is per-sample and uniform over the dataset, which is stronger claim than statement about the average-case error. This means every mini-batch, curriculum subset, or the full empirical objective inherits the same deviation control. The bound is also operational, as it reveals the key levers that ensure MADPO is robust in practice. The two terms in the bound expose what makes the plug-in procedure reliable: Data Quality and Quantity: The first term decays at the familiar O((cid:112)(δ + log(1/ρ))/N ) rate with more data. It also improves with more informative data that leads to well-conditioned covariance matrix ˆΣϕ. Regularization and Boundedness: The second term shows that gentle regularization κ stabilizes learning in directions where data is sparse. Furthermore, the constant γ, derived from the bounded-reward assumption, prevents the logistic loss from saturating, ensuring that small errors in ˆϕ do not cause disproportionately large swings in the objective. Finally, this theorem complements our earlier results. Propositions 4.1 and 4.2 characterize what MADPO learns at the pair level (amplifying low margins, shrinking high ones); Theorem 4.6 guarantees how reliably that mechanism survives the reality of an estimated reward model. In short, the pairwise control that defines MADPO is not brittle. Under the stated regularity conditions, the two-stage procedure is uniformly robust to reward estimation error, making the method dependable for the training regimes used in practice. 8 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization 4.3 Smoothness & Curvature: MADPO vs. DPO In this subsection, we compare the smoothness and curvature of the MADPO loss to the vanilla DPO loss. We show that the gradient and Hessian of our objective are simply re-scaled version of the DPO derivatives, which confirms that our method has stable and well-behaved optimization landscape. Proposition 4.7. Let L(θ, ϕ; x, yw, yl) be the MADPO loss function with bounded, θ independent weight 0 < w(hϕ) wmax. Then, for any sample (x, yw, yl) D, the first and second derivatives of the loss with respect to the implicit reward margin, hθ, satisfy the following bounds: 1. Bounded Gradient: 2. Bounded Hessian: (cid:12) (cid:12) (cid:12) (cid:12) hθ (cid:12) (cid:12) (cid:12) (cid:12) wmaxβ. (cid:12) (cid:12) (cid:12) (cid:12) 2L h2 θ (cid:12) (cid:12) (cid:12) (cid:12) wmaxβ2 4 . This proposition reveals that MADPO is principled modification of the vanilla DPO framework. It shows that the scalar gradient and Hessian of the MADPO loss are simply the vanilla DPO derivatives multiplied by our bounded weight, w(hϕ). This direct scaling is crucial because it ensures MADPO preserves the benign optimization geometry of the original DPO objective, which has intrinsically capped sensitivity and curvature. For practitioners, this translates to predictable gradients that are compatible with standard control techniques like learning rate tuning or gradient clipping. Crucially, because these derivatives are bounded on per-sample basis, it guarantees that the instance-level amplification and regularization effects established in our prior propositions are applied in stable and reliable manner."
        },
        {
            "title": "5 Experiment",
            "content": "In this section, we present the empirical evaluation of our proposed method, MADPO. We first detail our experimental setup, which is designed to test the performance and robustness of our algorithm against strong baselines on datasets of varying quality. We then present and thoroughly analyze the quantitative and qualitative results of these experiments. 5.1 Experiment Setup The goal of our experiments is to evaluate how effectively different preference alignment algorithms can teach base language model specific stylistic trait: to consistently generate positive-sentiment responses. To test this in controlled manner, we design synthetic environment inspired by the methodology of Chowdhury et al. [2024], which allows us to compare our method against standard baselines across datasets of varying difficulty. All models were trained using LoRA [Hu et al., 2022]. Models and Datasets. Our setup uses combination of base language model for fine-tuning, powerful existing model to serve as ground-truth reward oracle, and standard text dataset for prompts and content. Base Language Model: We use google/gemma-3-270M [Team, 2025] as the base model for all fine-tuning tasks. Ground-Truth Reward Model: To simulate human preferences with known reward function, we use cardiffnlp/twitter-roberta-base-sentiment-latest [Loureiro et al., 2022], strong sentiment analysis model, as our oracle. Text Corpus: All prompts and responses are derived from the stanfordnlp/imdb dataset [Maas et al., 2011]. Training Procedure Our full experimental pipeline consists of four sequential stages: supervised fine-tuning of base model, generation of synthetic preference data, training reward model on this data, and finally, fine-tuning policy using preference alignment algorithm. 1. Supervised Fine-Tuning (SFT): We first create base policy with strong stylistic prior. To do this, we fine-tune the model on the final 12,000 positive-sentiment reviews from the IMDB training set. The model is trained to generate positive-sentiment text when prompted with the beginning of review. This SFT model serves as the starting point for all subsequent policy fine-tuning. Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization 2. Synthetic Data Generation: The SFT model is then used to generate preference dataset of 12,000 pairs. We prompt the SFT model with the first 12,000 negative-sentiment reviews from the IMDB training dataset, forcing it to generate positive text in negative context. These generations are then used to construct three distinct preference datasets of varying quality (High, Medium, and Low). The precise methodology for this process is detailed in the following paragraph. 3. Reward Model Training: For each of the three quality-tiered datasets, we train separate reward model. The reward model is trained on the first 10,000 preference pairs of its corresponding dataset. The final 2,000 pairs are held out as validation set to implement early stopping, ensuring the reward model does not overfit to the training data. 4. Policy Fine-Tuning: In the final stage, we take the SFT model from Step 1 and fine-tune it using one of the preference alignment algorithms (DPO, IPO, β-DPO, and our proposed MADPO). Each policy is trained for two epochs on the same 10,000-pair training set that was used to train its corresponding reward model for that quality tier. Data Generation and Quality Tiers. To create challenging and diverse preference dataset, we generate our preference data by prompting this optimistic SFT model with the 12,000 negative-sentiment reviews. For each negative prompt, the SFT model generates two distinct positive-leaning responses. From these, we construct three dataset versions with varying quality: High Quality: Both responses in each pair, (y1, y2), are generations from the SFT model. Medium Quality: For the first 6,000 pairs, one response is high-scoring generation from the SFT model and the other is real, negative review from the IMDB dataset. The remaining 6,000 pairs are high-quality (SFT vs. SFT). Low Quality: For all 12,000 pairs, one response is high-scoring SFT generation and the other is real, negative review. For each generated pair, we score both responses using the ground-truth RoBERTa model. Specifically, we extract the probability of the positive label, p, and apply the linear transformation (p) = 6(p 0.5) to map it to reward value in the range [3, 3]. This range was chosen to be wide enough to generate meaningful distribution of reward margins, yet narrow enough that the BTL model in our choice simulation remains impactful, ensuring the resulting preferences are probabilistic rather than deterministic. We then simulate human choice by adding independent and identically distributed Gumbel noise to the reward score of each of the two responses. The response with the highest resulting noisy score is then selected as the winner for that preference pair, process which is consistent with the BTL preference model [McFadden, 2001]. To ensure controlled comparison, all three 12,000-pair datasets are randomly shuffled using the same fixed permutation, guaranteeing that the training and evaluation splits contain the identical set of prompts across each quality tier. From each shuffled dataset, the first 10,000 pairs are used for training, and the prompts from the remaining 2,000 are reserved for evaluation. Baselines and Hyperparameters. We compare MADPO against several standard baselines: DPO, IPO, and β-DPO (applying both β-batch and β guided filtering). For all methods, we use fixed temperature of β = 0.1. We perform targeted hyperparameter search for both β-DPO and our proposed method, MADPO. For β-DPO, we tune the β-batch scaling factor {0.4, 0.6, 0.8} and the β-filtering proportion {0.4, 0.6, 0.8}. Instead of full Cartesian product, we conduct search over 5 configurations for each dataset quality: we first fix = 0.6 while varying p, and then fix = 0.8 while varying m. For the filtering mechanism, the target margin is set to h0 = 0 with an initial standard deviation of one and momentum coefficient of 0.9. For our method, MADPO, we perform similar targeted search over 6 configurations. The steepness is fixed at λ = 1 and the minimum coefficient is set as the reciprocal of the maximum, cmin = 1/cmax. We search over the threshold τ {2, 4, 7, 10} and the maximum coefficient cmax {2, 3, 4}. Specifically, we first fix τ = 7 while varying cmax, and then fix cmax = 2 while varying τ . Evaluation. For each method and on each of the three quality-tiered datasets, we fine-tune separate policy. We evaluate the performance of each final policy by using it to generate responses for the 2,000 held-out evaluation prompts. The primary metric for comparison is the mean ground-truth reward of these generated responses, as scored by our RoBERTa oracle. higher mean reward indicates better-aligned policy. 5.2 Experiment Result In this subsection, we present series of experiments designed to empirically validate the effectiveness and robustness of our proposed method. We compare MADPO against strong preference alignment baselines on synthetic datasets of 10 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization varying quality to specifically test its performance in challenging, noisy settings. The section is organized as follows: first, we present the main quantitative results comparing all methods across the three data quality tiers. Second, we provide detailed sensitivity analysis on MADPOs key hyperparameters. Third, we conduct an ablation study to disentangle the contributions of our methods core components. (b) Performance Visualization (a) Main Results Table Method High Medium Low"
        },
        {
            "title": "IPO",
            "content": "β-DPO MADPO 1.62 (0.03) 0.35 (0.05) 1.67 (0.03) 2.23 (0.02) 1.71 (0.03) 0.31 (0.05) 1.84 (0.03) 2.23 (0.02) 1.48 (0.04) 0.10 (0.05) 1.76 (0.03) 1.95 (0.03) Figure 1: Main experimental results. (a) Table of mean rewards (standard error) for all methods across three data quality tiers. (b) Bar chart visualizing the mean rewards, clearly showing MADPOs superior performance and robustness compared to baselines. For β-DPO and MADPO, we report the performance of the best hyperparameter configuration found for each individual tier. Main Result. The main experimental results, visualized in Figure 1, clearly demonstrate the superior performance and robustness of our proposed method, MADPO. Our method achieves the highest mean reward across all three data quality tiers, substantially outperforming all baselines. The annotations on the chart highlight the significant margin of victory over the next-best method, β-DPO, with performance gains of +33.3% on High Quality, +20.8% on Medium Quality, and +10.5% on Low Quality data. Notably, MADPOs absolute performance is also remarkably stable, showing no degradation between the High and Medium quality settings and maintaining strong lead on the most challenging Low Quality dataset. The performance of the baseline models validates our experimental design. The vanilla DPO models score is volatile and drops on the Low Quality set, confirming the difficulty of this tier. While β-DPO consistently improves upon DPO, it is clearly outperformed by MADPOs instance-level approach and shows sensitivity to the data mixture, with its peak performance occurring on the Medium Quality dataset. IPO performs poorly in all scenarios, indicating that its uniform regularization strategy is ill-suited for this task. Figure 2: Sensitivity analysis for MADPOs key hyperparameters across the three data quality tiers. (Left) Performance as function of the margin threshold, τ . Higher values are generally better, though performance plateaus on High Quality data. (Right) Performance as function of the amplification intensity, c, where we set cmax = and cmin = 1/c. Performance consistently improves with higher intensity across all tiers. 11 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization Sensitivity Analysis. To understand the behavior of MADPO with respect to its key hyperparameters, we conduct sensitivity analysis on the margin threshold, τ , and the amplification intensity, c. The results, shown in Figure 2, reveal several key insights into the methods mechanics. The analysis for the threshold τ (Figure 2a) reveals nuanced interaction between the threshold and the underlying data quality, highlighting the importance of both of MADPOs components. For the High Quality dataset, performance peaks at an optimal value (τ = 4) before plateauing. This indicates that setting the threshold too high on clean data is suboptimal; it misclassifies easy-to-learn pairs as hard, applying unnecessary amplification where regularization would be more beneficial. This result underscores the value of the regularization component when data quality is high. Conversely, for the Low and Medium quality datasets, performance consistently improves with larger τ . This suggests that when the data is noisy, higher threshold is advantageous as it ensures the truly informative, low-margin pairs are amplified. The large gradients from these amplified pairs then dominate the training update, providing an implicit regularization effect that prevents the model from overfitting to the uninformative, high-margin pairs present in lower-quality data. Therefore, the optimal setting for τ is dependent on the expected data quality, with noisier datasets benefiting from more aggressive and wide-ranging amplification strategy. The sensitivity to the amplification intensity (Figure 2b), where we set cmax = and cmin = 1/c, shows more uniform trend. Across all three data quality tiers, higher intensity leads to stronger performance. This robustly demonstrates the effectiveness of our core mechanism: aggressively amplifying the learning signal for informative pairs while simultaneously and strongly dampening it for uninformative ones is beneficial strategy regardless of the overall data quality. Overall, this analysis confirms that MADPO is not overly sensitive to its hyperparameter choices and that clear trends can guide practitioners toward an optimal configuration. (a) Ablation Study with = 2, τ = 7 (b) Ablation Study with = 4, τ = 7 Figure 3: Ablation study of MADPOs amplification and regularization components. The study compares the full MADPO model against vanilla DPO and two ablated versions: Amp-Only, which only amplifies low-margin pairs (by setting w(h ˆϕ) = 1 for ˆϕ τ ), and Reg-Only, which only regularizes high-margin pairs (by setting w(h ˆϕ) = 1 for ˆϕ < τ ). The comparison is shown under two hyperparameter settings: (Left) moderate amplification intensity (c = 2, τ = 7), and (Right) high amplification intensity (c = 4, τ = 7). Ablation Study The results of our ablation study, presented in Figure 3, reveal that the amplification mechanism is the primary driver of MADPOs performance gains. To isolate the two core components of our method, we analyze two ablated models. The first is an Amplification-Only version, where the weight is set to one for high-margin pairs (h ˆϕ τ ) to disable the regularization component. This model consistently achieves the highest or near-highest performance across all settings, suggesting that aggressively learning from informative, low-margin pairs is the most critical factor for success. One interpretation is that strong amplification also serves as form of implicit regularization; by forcing the optimization to prioritize hard examples, the gradients from easy examples have less influence on the overall update, which may prevent overfitting. The second model is Regularization-Only version, where the weight is set to one for low-margin pairs (h ˆϕ < τ ) to disable amplification. The results for this model show that when tuned properly, explicitly dampening high-margin pairs provides consistent improvement over the vanilla DPO baseline across all data quality tiers. This confirms that the regularization component is beneficial mechanism in its own right, even if its impact is secondary to that of amplification. Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization While the Amplification-Only model performs exceptionally well, this does not render the explicit regularization component meaningless. The full MADPO model, which includes both mechanisms, represents more complete and theoretically robust solution designed for general-purpose application. The regularization component provides crucial safeguard against overfitting on easy examples, problem which may be more or less severe depending on the specific dataset and base model. Our results show that the full models performance is highly competitive with the Amplification-Only version, indicating that the regularization term offers this theoretical robustness without significant performance trade-off in practice."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we addressed key limitation in the vanilla Direct Preference Optimization (DPO) framework: its reliance on single, fixed temperature parameter that struggles with preference data of varying quality. We introduced Margin-Adaptive Direct Preference Optimization (MADPO), method that applies an instance-level, adaptive weight to the DPO loss. Our theoretical analysis proved that MADPO is principled modification that maintains stable optimization landscape and is robust to the errors inherent in practical two-step training pipelines. Our empirical results on sentiment generation task confirmed these findings, demonstrating that MADPO consistently outperforms strong baselines, particularly on challenging, low-quality datasets. Our analysis revealed that this success is primarily driven by MADPOs ability to amplify the learning signal for informative, low-margin examples. We acknowledge two primary limitations in our current study. First, our experiments were conducted on 270Mparameter language model. While this allowed for controlled and thorough analysis of the methods mechanics, further research is needed to validate whether our findings generalize to larger, state-of-the-art models. Second, our work relies on synthetic preference dataset generated from an oracle reward model. This provided clean, controlled environment for analysis, but the dynamics of training on real-world, human-annotated preference datawhich can be inherently noisier and more inconsistentremain an important area for future investigation. 13 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization"
        },
        {
            "title": "References",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. β-dpo: Direct preference optimization with dynamic β. Advances in Neural Information Processing Systems, 37: 129944129966, 2024a. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pages 44474455. PMLR, 2024. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Duncan Luce et al. Individual choice behavior, volume 4. Wiley New York, 1959. Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. arXiv preprint arXiv:2403.00409, 2024. Alekh Agarwal, Sham Kakade, Jason Lee, and Gaurav Mahajan. On the theory of policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):176, 2021. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Gemma Team. Gemma 3. 2025. URL https://arxiv.org/abs/2503.19786. Daniel Loureiro, Francesco Barbieri, Leonardo Neves, Luis Espinosa Anke, and Jose Camacho-collados. TimeLMs: In Proceedings of the 60th Annual Meeting of the Association for Diachronic language models from Twitter. Computational Linguistics: System Demonstrations, pages 251260, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.acl-demo.25. URL https://aclanthology.org/2022. acl-demo.25. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015. Daniel McFadden. Economic choices. American economic review, 91(3):351378, 2001. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. Junkang Wu, Xue Wang, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. α-dpo: Adaptive reward margin is what direct preference optimization needs. arXiv preprint arXiv:2410.10148, 2024b. Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, and Min Zhang. Omni-dpo: dual-perspective paradigm for dynamic preference learning of llms. arXiv preprint arXiv:2506.10054, 2025. Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. Wpo: Enhancing rlhf with weighted preference optimization. arXiv preprint arXiv:2406.11827, 2024. 14 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization"
        },
        {
            "title": "A Acknowledgements",
            "content": "We used OpenAI ChatGPT, Google Gemini, and xAI Grok for literature search support, wording/clarity edits, mathematical cross-checks, and code review/debugging. The authors verified all outputs and are solely responsible for the papers content. The experiments presented in this paper were made possible by the computational resources provided by Google Colab and the Kaggle platform."
        },
        {
            "title": "B Proofs",
            "content": "Proof of Proposition 4.1. For mathematical representation in this proof, we adopt the notation (x, y, y, d) D, where y, are responses and {0, 1} is binary preference indicator such that = 1 indicates (i.e., = yw, = yl) and = 0 indicates (i.e., = yw, = yl). The introduction of allows us to flexibly model preference directions using the BTL model, where EdPϕ [dx, y, y] = σ(hϕ ). We now analyze the expected loss for any triplet (x, y, y, d) Dlow. The analysis begins by taking the expectation of the sample-level loss over the optimal preference distribution Pϕ . This expectation simplifies to new cross-entropy objective. The derivation proceeds as follows: L(πθ, ϕ; x, y, y) = EdPϕ [d w(hϕ ) log σ(βhθ) + (1 d) w(hϕ ) log σ(βhθ)x, y, y] = [σ(hϕ )w(hϕ ) log σ(βhθ) + σ(hϕ )w(hϕ ) log σ(βhθ)] = [σ(c(hϕ ) hϕ ) log σ(βhθ) + σ(c(hϕ ) hϕ ) log σ(βhθ)] . The second equality holds by BTL where EdPϕ [dx, y, y] = σ(hϕ ). The last equality is satisfied by the construction of w(h). This final form is the cross-entropy loss between the policys distribution, Pθ, and margin-aware target distribution, ϕ , with logits defined by c(hϕ ) hϕ . Since the total loss, L(θ, ϕ), is the expectation of these non-negative, instance-level cross-entropy losses over the dataset, the global minimum is achieved when the loss for each instance is minimized simultaneously. This occurs if and only if the distributions are identical for each instance, which requires their logits to be equal. Therefore, the optimal solution satisfies: βhθ = c(hϕ ) hϕ . Proof of Proposition 4.2. As we did for the proof of Proposition 4.1, we adopt the notation (x, y, y, d) D. Without loss of generality, we assume hϕ hϕ (x, y, y) = rϕ (x, y) rϕ (x, y) > 0 by swapping and if necessary. So the weighting function is w(hϕ ) = σ(c(hϕ ) hϕ ) σ(hϕ ) and w(hϕ ) = 1 We derive the expected loss for triplet (x, y, y, d) Dhigh with EdPϕ [dx, y, y] = σ(hϕ ) from BTL model: L(πθ, ϕ; x, y, y) = [σ(c(hϕ ) hϕ ) log σ(βhθ) + σ(hϕ ) log σ(βhθ)] . (6) Now, we take derivatives of Eq. (6) with respect to βhθ, which results in L(πθ, ϕ; x, y, y) βhθ = [σ(c(hϕ ) hϕ )σ(βhθ) σ(hϕ )σ(βhθ)] . At the optimum θ, we have (c, βhθ ) L(πθ, ϕ; x, y, y) βhθ (cid:12) (cid:12) (cid:12) (cid:12)θ=θ = [σ(c(hϕ ) hϕ )σ(βhθ ) σ(hϕ )σ(βhθ )] = 0. We denote c(hϕ ) for notation simplicity. By the implicit function theorem, βhθ partial derivatives: = F/c F/βhθ . Compute the c = [σ(c hϕ )σ(βhθ ) σ(hϕ )σ(βhθ )] = hϕ σ(c hϕ )σ(c hϕ )σ(βhθ ). βhθ = σ(c hϕ ) [σ(βhθ )σ(βhθ )] σ(hϕ ) [σ(βhθ )σ(βhθ )] = σ(βhθ )σ(βhθ ) [σ(c hϕ ) + σ(hϕ )] . 15 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization Thus: βhθ = hϕ σ(c hϕ )σ(c hϕ )σ(βhθ ) σ(βhθ )σ(βhθ ) [σ(c hϕ ) + σ(hϕ )] = hϕ σ(c hϕ )σ(c hϕ )σ(βhθ ) σ(βhθ )σ(βhθ ) [σ(c hϕ ) + σ(hϕ )] . Since hϕ > 0, and all sigmoid terms are strictly positive, the numerator and denominator are positive, so βhθ Thus, (βhθ ) > 0, proving the proposition. > 0. Lemma 1. Let w(h) be the weight function defined in Eq. 4. The function is differentiable almost everywhere, and its derivative, w(h), is uniformly bounded. That is, there exists constant Lw > 0 such that for all where the derivative is defined: w(h) Lw. Proof. For the purpose of this analysis, we can simplify the expression by fixing the hyperparameters to representative values. Hyperparameters specific value does not affect the following stability analysis, so for simplicity we let cmin = 0, along with cmax = 2 and λ = 1. By construction, the weight function w(h) is continuous everywhere and differentiable almost everywhere. The only non differentiable points occur at = 0 due to the absolute value in the coefficient function and at = τ due to the piecewise definition. Case 1: For (0, ). Let k(h) = c(h) h. The weight function and its derivative are: w(h) = σ(k(h)) σ(h) , w(h) = σ(k(h))k(h)σ(h) σ(k(h))σ(h) σ2(h) The sigmoid function σ() and its derivative σ() are universally bounded (by 1 and 1/4, respectively). For > 0, the denominator σ2(h) is bounded away from zero. Therefore, to show that w(h) is bounded, we only need to show that k(h) is bounded. By the product rule, k(h) = c(h) + c(h)h. By definition, c(h) is bounded between [cmin, cmax]. The term c(h)h involves the derivative of the coefficient function, which contains an exponential decay term that dominates the linear growth of h. As , c(h)h 0. Since k(h) is continuous on (0, ) and has finite limits at its boundaries (h 0+ and ), it is bounded on this interval. Thus, w(h) is also bounded for > 0. Case 2: For (τ, 0). On this bounded open interval, the derivative w(h) is continuous function. As established in our analysis of the boundaries, the one-sided limits of w(h) as τ + and as 0 are both finite. function that is continuous on bounded open interval and has finite limits at its endpoints is necessarily bounded. Thus, w(h) is bounded on this interval. Case 3: For < τ . In this region, w(h) = 1. Therefore, its derivative is w(h) = 0, which is trivially bounded. Since the derivative w(h) is bounded on all three regions that cover its domain, we conclude that it is uniformly bounded. Lemma 2. The weight function defined in Eq. 4 is Lw-Lipschitz continuous. That is, for any h, R, the following inequality holds: w(h) w(h) Lwh h. (7) Proof. The proof relies on Lemma 1, which establishes that the derivative of the weight function is bounded almost everywhere, i.e., w(t) Lw. function with bounded derivative (a.e.) is absolutely continuous, which is the required condition to apply the Fundamental Theorem of Calculus for Lebesgue Integrals. For any two points a, with < b, the theorem states: w(b) w(a) = (cid:90) a w(t)dt. 16 (8) Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization We can now take the absolute value of both sides and apply the bound from our lemma: w(b) w(a) = w(t)dt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:90) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:90) w(t)dt (Triangle inequality for integrals) (cid:90) a"
        },
        {
            "title": "Lwdt",
            "content": "(By Lemma 1, w(t) Lw) = Lw(b a). Since this holds for any < b, we can generalize to w(h) w(h) Lwh for any h, R. This is the definition of Lw-Lipschitz continuity. Proof of Theorem 4.6. The proof proceeds in two main parts. First, we establish that the margin function hϕ is Lipschitz continuous with respect to its parameters ϕ. Second, we leverage this result to show that the full loss function, L(θ, ϕ; x, yw, yl), is also Lipschitz continuous, which allows us to invoke the final result from prior work. Part 1: Lipschitz Continuity of the Margin Function (hϕ). We begin with the definition of the margin function: hϕ(x, yw, yl) = rϕ(x, yw) rϕ(x, yl). By the Mean Value Theorem, there exists ϕ on the line segment between ϕ and ˆϕ such that: ˆϕ(x, yw, yl) hϕ (x, yw, yl) = (cid:0)ϕr ϕ(x, yw) ϕr ϕ(x, yl)(cid:1) ( ˆϕ ϕ). Taking the absolute value and applying the generalized Cauchy-Schwarz inequality with the semi-norm Σϕ+κI gives: ˆϕ hϕ (cid:13) (cid:13)ϕr ϕ(x, yw) ϕr ϕ(x, yl)(cid:13) (cid:13)(Σϕ+κI)1 ˆϕ ϕΣϕ+κI . Under Assumption 4.4, the gradient of the reward function is bounded. This implies that the term involving the gradients is also bounded by some constant, which we will call Lϕ. Thus, the margin function is Lϕ-Lipschitz continuous with respect to ϕ: ˆϕ(x, yw, yl) hϕ (x, yw, yl) Lϕ ˆϕ ϕΣϕ+κI . Part 2: Lipschitz Continuity of the Full Loss Function (L). Now we analyze the full L(θ, ϕ; x, yw, yl) = w(hϕ(x, yw, yl)) log σ(βhθ(x, yw, yl)). loss function, (cid:12) (cid:12) (cid:12)L(θ, ϕ; x, yw, yl) L(θ, ˆϕ; x, yw, yl) (cid:12) (cid:12) (cid:12) = log σ(βhθ(x, yw, yl) Lθ (cid:12) (cid:12) (cid:12)w(hϕ (x, yw, yl)) w(h ˆϕ(x, yw, yl)) (cid:12) (cid:12) (cid:12)w(hϕ (x, yw, yl)) w(h ˆϕ(x, yw, yl)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Lθ Lw hϕ (x, yw, yl) ˆϕ(x, yw, yl) Lθ Lw Lϕ ˆϕ ϕΣϕ+κI . The second inequality holds by Assumption 4.5. The third inequality holds by Lemma 2. The last inequality holds by Part 1 of this proof. By defining the final Lipschitz constant as = LθLwLϕ, we have shown that our loss function is L-Lipschitz continuous: (cid:12) (cid:12) (cid:12)L(θ, ϕ; x, yw, yl) L(θ, ˆϕ; x, yw, yl) (cid:12) ˆϕ ϕΣϕ+κI . (cid:12) (cid:12) With this result established, the final statistical bound on the estimation error follows directly by invoking the general framework for two-step estimators, such as Theorem 4.2 in Chowdhury et al. [2024]. This completes the proof. Proof of Proposition 4.7. For readability in this proof, we suppress the explicit dependence on the sample (x, yw, yl) and parameters (θ, ϕ) for functions like L, hθ, and hϕ, unless required for clarity. The proof for both claims relies on the fact that the weight function, w(hϕ), is uniformly bounded. From its construction in Eq. 4, there exists constant wmax such that w(hϕ) wmax for all inputs. 17 Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization 1. Bounded Gradient: The first derivative of the loss is given by: hθ = w(hϕ) βσ(βhθ). This expression is product of the weight function, constant β, and the sigmoid function, all of which are bounded. Therefore, their product is uniformly bounded. 2. Bounded Hessian: The second derivative of the loss is given by: 2L h2 θ = w(hϕ) β2σ(βhθ)σ(βhθ). This is product of the bounded weight function, constant β2, and the derivative of the sigmoid function, σ() = σ()σ(), which is famously bounded by 1/4. Therefore, the second derivative is also uniformly bounded."
        },
        {
            "title": "C Related Work",
            "content": "Recent research in preference alignment has sought to address the limitations of DPOs fixed temperature, which can lead to overfitting on easy, high-margin pairs. Prominent approaches such as IPO [Azar et al., 2024] and β-DPO [Wu et al., 2024a] tackle this problem with mechanisms that are applied at coarse granularity. IPO proposes uniform target margin for all samples, while β-DPOs strategies operate at the batch level. While an improvement, the batch-level mechanisms of β-DPO have several notable drawbacks. First, its adaptation is coarse approximation of the instance-level ideal. single training batch can easily contain mix of highand low-margin pairs, yet the β-batch method applies single, compromised temperature to all of them. Second, the linear adaptation rule, βbatch = β0(1 + (hθ h0)), can be unstable; for difficult batches where the average margin hθ is negative, the resulting temperature βbatch can also become negative, leading to divergent objective that actively learns to prefer the dispreferred response. Finally, the batch-dependent temperature complicates hyperparameter selection, as the absence of fixed β makes it difficult to perform reliable cross-validation to find the optimal tuning parameters. Our work, MADPO, is distinct in that it provides fully instance-level and data-preserving solution that avoids these issues. By applying continuous, adaptive weight to each training sample based on its unique reward margin, our method can granularly control the learning signal. This allows it to be aggressive on hard pairs and conservative on easy ones within the same batch, providing more flexible and stable approach to preference alignment. Beyond the methods discussed above, other recent works have extended the DPO framework in various directions. For instance, methods like SimPO [Meng et al., 2024] and α-DPO [Wu et al., 2024b] focus on simplifying the objective by removing the need for an explicit reference policy. Another line of work has explored reweighting preference data. Omni-DPO [Peng et al., 2025] dynamically weights pairs based on both their inherent quality and the models current learning state, while WPO [Zhou et al., 2024] reweights off-policy preference data to more closely resemble the on-policy distribution. While these methods also use reweighting scheme, their motivation is distinct from that of MADPO. Whereas these approaches weight data based on the policys dynamic state or distributional properties, MADPOs weighting is based on static, external signal of sample difficulty derived from the reward margin, hϕ. Our goal is not to correct for distributional shift, but to granularly control the regularization strength for each individual sample based on its intrinsic difficulty."
        }
    ],
    "affiliations": []
}