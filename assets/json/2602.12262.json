{
    "paper_title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
    "authors": [
        "Tunyu Zhang",
        "Xinxi Zhang",
        "Ligong Han",
        "Haizhou Shi",
        "Xiaoxiao He",
        "Zhuowei Li",
        "Hao Wang",
        "Kai Xu",
        "Akash Srivastava",
        "Hao Wang",
        "Vladimir Pavlovic",
        "Dimitris N. Metaxas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model's own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D."
        },
        {
            "title": "Start",
            "content": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Tunyu Zhang * 1 Xinxi Zhang * 1 Ligong Han 2 3 Haizhou Shi 1 Xiaoxiao He 1 Zhuowei Li 4 Hao Wang 2 3 Kai Xu 2 3 Akash Srivastava 2 3 Hao Wang 1 Vladimir Pavlovic 1 Dimitris N. Metaxas 1 6 2 0 2 2 1 ] . [ 1 2 6 2 2 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to substantial degradation in generation quality. To alleviate this, we propose trajectory self-distillation framework that improves few-step decoding by distilling the models own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on highprobability teacher modes. Across benchmarks, our approach consistently outperforms strong fewstep baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D. 1. Introduction Inference-time efficiency is central challenge in large language modeling, especially for real-time and computeconstrained applications (Zhen et al., 2025; Miao et al., 2025; Alizadeh et al., 2024). Many practical deployments, such as real-time decision-making and on-device AI, require consistently low-latency responses. Diffusion large language models (DLLMs) (Labs et al., 2025; Song et al., 2025; Nie et al., 2025; Cheng et al., 2025; Ye et al., 2025) *Equal contribution Rutgers University 1Department of Computer SciInnovation ence, 3MIT-IBM Watson AI Lab 4Amazon. Correspondence to: Ligong Han <ligong.han@rutgers.edu>, Dimitris N. Metaxas <dnm@cs.rutgers.edu>. Tunyu Zhang <ty.zhang@rutgers.edu>, 2Red Hat AI Preprint. February 13, 2026. 1 offer promising direction by enabling parallel token generation. However, existing DLLMs rely on long decoding chains consisting of many diffusion steps (Sahoo et al., 2024; Schiff et al.; Nie et al., 2025; Ye et al., 2025), which significantly limits their efficiency gains. When decoding is made more aggressive by reducing the number of steps, these models struggle to accurately generate multiple tokens simultaneously (Cheng et al., 2025). Recent work (Yoo et al., 2025; Chen et al., 2025; Xu et al., 2024a; Qian et al., 2026; Kim et al., 2025; Zhang et al., 2025b) has explored fine-tuning DLLMs to support few-step decoding, aiming to further reduce inference latency beyond standard diffusion sampling. However, few-step decoding is fundamentally challenged by the common mean-field (tokenfactorized) parameterization used in masked diffusion models (Xu et al., 2024a; Yoo et al., 2025; Zhang et al., 2025b), which induces factorization (approximation) error that typically grows as the number of denoising steps decreases. Moreover, training and inference often operate on different intermediate-state distributions: training commonly uses random masking, while inference frequently relies on nonrandom heuristics (e.g., confidence-based schedules) that change the masked states that the model encounters (Wang et al., 2025b; Qian et al., 2026). Together, this motivates the following important research question: Can we reduce mean-field approximation error under tight step budgets for DLLMs? We answer this question affirmatively by introducing two complementary training ingredients. (i) Trajectory supervision. First, we introduce trajectory self-distillation, which trains the student on the teachers own rollout trajectories. This directly addresses traintest mismatch in intermediate states: training often relies on random masking, while inference typically uses non-random heuristics (e.g., confidence-based schedules) that induce different masking patterns and introduce dependencies among simultaneously decoded tokens. By distilling on rollout trajecotires produced by the teacher under the target decoding procedure, the student is supervised on-policy and can use its limited step budget more effectively, matching the Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization trajectory distribution encountered at inference time rather than optimizing only endpoint transitions as previous selfdistillation frameworks (Yoo et al., 2025; Chen et al., 2025). (ii) Better training objective for few-step decoding. To strengthen learning under aggressive step compression, we incorporate Direct Discriminative Optimization (DDO) (Zheng et al., 2025) into the distillation objective. With only few decoding steps, the denoising posterior becomes highly multimodal (Xiao et al., 2021; Xu et al., 2024b), and trajectory matching with forward-KL objective tends to be mode-covering, yielding over-smoothed predictions and weak alignment with teacher trajectories. DDO mitigates this issue via GAN-inspired likelihood-ratio loss that can be interpreted as reverse-KL style, mode-seeking distillation: by contrasting the fine-tuned model against its initialization, it emphasizes high-probability teacher modes rather than uniformly matching likelihood across all continuations. Moreover, for masked diffusion, this likelihood-ratio objective can be evaluated directly from the models predicted probabilities, avoiding Monte Carlo ELBO estimation (and its associated gradient variance). Finally, we introduce path-consistency regularizer that reweights token-level losses based on decoding order, mitigating error propagation from early mispredictions in blockwise few-step decoding. We refer to the resulting approach as Self-Trajectory Distillation via DDO (T3D). T3D provides simple yet effective solution for few-step diffusion language modeling. By distilling directly from teacher-generated trajectories, T3D eliminates distribution shift and stabilizes training in the few-step regime, without requiring any additional ground-truth supervision. Our theoretical analysis shows that trajectory-level distillation reduces conditional dependencies in the reverse process, leading to lower factorization error. Extensive experiments on standard reasoning and code-generation benchmarks demonstrate that T3D consistently outperforms existing few-step DLLM methods under tight decoding budgets, substantially narrowing the gap to full-step diffusion decoding. 2. Preliminaries This section introduces our notation, reviews Masked Diffusion Language Models (MDLMs), and Direct Discriminative Optimization (DDO). Notations. Throughout this paper, scalars are denoted by lowercase or uppercase letters (e.g., t, ), vectors by lowercase boldface letters (e.g., x), and random vectors by lowercase boldface letters with roman font (e.g., x). The symbol denotes joint distribution. We use KL() for the KullbackLeibler divergence and D() to denote either the forward or reverse KullbackLeibler divergence. 2.1. Masked Diffusion Models Masked Diffusion Language Models (MDLMs) (Sahoo et al., 2024; Shi et al., 2024) are diffusion-based generative models for discrete sequences. Let pdata denote the data distribution. data sample x0 pdata is length-L token sequence x0 = (x1 0, . . . , xL 0 denotes discrete token from finite vocabulary augmented with special mask token m. 0 ), where xi The forward (noising) diffusion process is defined over continuous time [0, 1] and corrupts sequence by independently masking tokens. The corruption distribution factorizes across token positions: q(xt x0) = (cid:89) i=1 q(xi xi 0), (1) xi where the token-wise kernel q(xi 0) is governed by monotonically decreasing noise schedule αt [0, 1]: at time t, xi 0 with probability αt and replaced by the mask token with probability 1 αt. We choose αt = 1 following previous works (Nie et al., 2025; Sahoo et al., 2024). is preserved as xi The reverse (denoising) process is modeled by neural network pθ that also factorizes over tokens: pθ(xs xt) = (cid:16) pθ x(i) xt (cid:17) . (cid:89) i= (2) As shown in (Sahoo et al., 2024; Shi et al., 2024), maximizing the evidence lower bound (ELBO) for MDLMs admits remarkably simple form. Concretely, the optimization objective reduces to masked-token cross-entropy objective: L(θ) = Extq(x0) (cid:2)log pθ(x0 xt)(cid:3) . (3) 2.2. Direct Discriminative Optimization (DDO) Direct Discriminative Optimization (DDO) (Zheng et al., 2025) is GAN-inspired objective for likelihood-based generative models. Unlike standard GANs (Goodfellow et al., 2020), which introduce an additional discriminator network, DDO implicitly parameterizes the discriminator using likelihood ratios. Consider pretrained model pθref that supplies fake samples. To distinguish real data pdata from reference samples pθref , the optimal discriminator is: d(x) = pdata(x) pdata(x)+pθref (x) = σ (cid:16) log pdata(x) pθref (x) (cid:17) , where σ() denotes the sigmoid function. DDO replaces the unknown pdata by parameterizing discriminator through learnable likelihood-based model pθ: dθ(x) := σ (cid:16) log pθ(x) pθref (x) (cid:17) . 2 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Substituting this implicit discriminator into the GAN discriminator loss yields the DDO objective: min θ L(θ) = Expdata (cid:104) log σ Expθref (cid:104) log (cid:16) (cid:17)(cid:105) (cid:16) log pθ(x) pθref (x) (cid:16) 1 σ log pθ(x) pθref (x) (cid:17)(cid:17)(cid:105) . (4) With unlimited model capacity, Zheng et al. (2025) show that the global minimizer of the DDO objective above satisfies θ = pdata. 3. Methods In this section, we first formulate and introduce trajectory distillation in Sec. 3.1. We then present how Direct Discriminative Optimization (DDO) is adapted to trajectory distillation in Sec. 3.2, followed by the path consistency regularization described in Sec. 3.3. Finally, we summarize the complete training procedures as T3D in Sec. 3.4. 3.1. Trajectory Self-Distillation Motivated by the insight that intermediate denoising states encode strong signals of the teachers decoding behavior (Song & Dhariwal, 2023; Hu et al., 2025a; Wang et al., 2025b; Qian et al., 2026), we propose trajectory selfdistillation, which trains few-step student directly on teacher rollout trajectories. Given pretrained teacher model pϕ, we want to train few-step student model pθ initialized from pϕ without using any ground truth data. Instead of matching only the marginal distribution pϕ(x0), our approach leverages pairs of clean and intermediate states (x0, xt) sampled along the teachers trajectory pϕ(x0:T ). The most direct approach is using forward KL divergence objective, which leads to the following loss: Ltraj(θ) = Epϕ(xt)Ex0pϕ(xt)[ log pθ(x0 xt)] . (5) Ltraj(θ) differs from the original diffusion loss L(θ) (Eqn. 3) only in how training pairs are constructed: it draws (x0, xt) from the same teacher-generated trajectory. Under mild assumptions, we show that minimizing Eqn. 5 is optimal for on-policy objective matching; see Proposition 4.3 for details. Moreover, in Sec. 4.3 we show that trajectory self-distillation enables few-step generation by reducing the factorization error in the reverse process. This perspective allows the method to be interpreted as variant of Discrete Rectified Flow (Yoo et al., 2025). 3.2. Trajectory Distillation via DDO When the student is restricted to small number of decoding steps, the posterior p(x0 xt) becomes highly multimodal, as multiple future completions remain plausible under coarse intermediate states (Xiao et al., 2021). In this regime, forward-KLstyle Ltraj(θ) in Eqn. 5 suffers from mode covering, leading to over-smoothed predictions and suboptimal alignment with teacher-generated trajectories. To address this issue, we adopt Direct Discriminative Optimization (DDO) (Zheng et al., 2025) for trajectory distillation. The DDO objective can be interpreted as reverseKLstyle optimization that encourages mode-seeking behavior, allowing the student to focus on high-probability teacher modes rather than uniformly covering all possible continuations. Formally, we define the trajectory-level DDO objective as: Ltraj-DDO(θ) = Extpϕ(xt) (cid:2) l(θ) (cid:3), (6) where the per-step DDO loss is: l(θ) = log σ (cid:16) log 1 σ (cid:16) Ex0pϕ(xt) (cid:104) (cid:105)(cid:17) log pθ(x0xt) pθref (x0xt) (cid:104) log pθ(x0xt) pθref (x0xt) (cid:16) Ex0pθref (xt) (cid:105)(cid:17)(cid:17) , (7) where pθref is reference model that provides fake samples and is initialized from pθ. The first term encourages the student to assign higher likelihood than the reference model to teacher-generated samples, while the second term penalizes overestimation of samples from the reference model. In Sec. 4.4, we provide theoretical analysis of the DDO objective, which reveals the implicit optimization target of DDO in trajectory distillation. 3.3. Path Consistency Regularization We introduce lightweight path-consistency regularization to stabilize few-step decoding by emphasizing earlydecoded tokens, where mistakes are most likely to cascade under aggressive step compression. By placing stronger supervision on early decoded tokens, we reduce error propagation and improve overall decoding stability. 0, . . . , xL Specifically, given sampled x0 = (x1 0 ) pϕ(x0) and fixed decoding step budget B, we define π [B]L to be the step index of each token, where πi is the step at which token xi is decoded in the decoding trajectory. Then we define token-level weighted path-consistency regularization loss as: Lpath(θ) = Epϕ(xt)Ex0pϕ(xt) (cid:34) (cid:88) wi (cid:0)log pθ(xi 0 x(i) )(cid:1) (cid:35) , (8) Here wi 0 is step-dependent weighting coefficient. In 3 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization this work, we use: wi = Bπi+1 . (9) This choice assigns larger weights to tokens decoded earlier in the trajectory: if token is decoded at an earlier step, then πi is smaller, which increases πi + 1 and hence wi. 3.4. T3D: Trajectory Self-Distillation via DDO Putting all the components together, we present selfdistillation algorithm for enabling few-step generation in diffusion language models. We first construct trajectory dataset by sampling from the pretrained teacher model pϕ, decoding one token per step to get the best performance. The student is then trained on the collected trajectories using DDO, combined with path consistency regularization. We term our method: LT3D(θ) = Ltraj-DDO(θ) + λ Lpath(θ). (10) Here, λ controls the strength of the path consistency regularization. An overview of the method is shown in Fig. 1, and the full training algorithm is provided in Appendix A. 4. Theoretical Analysis In this section, we present the theoretical analysis of our method. First, we formulate the self-distillation setting in Sec. 4.1. After that, in Sec. 4.2, we show that under mild assumptions, Trajectory Self-Distillation is optimal for on-policy posterior matching. Next, in Sec. 4.3, we provide theoretical analysis showing that Trajectory SelfDistillation enables few-step generation by reducing the factorization error. Finally, in Sec. 4.4 we analyze how the upper bound of the DDO objective characterizes its discriminative optimization behavior. 4.1. Self-Distillation Setting Let pϕ denote pretrained diffusion language model. The target teacher distribution pϕ(x0) is defined as the marginal distribution over clean sequences induced by the teachers generative process. Starting from fully masked sequence xT , the teacher generates x0 pϕ() by discretizing the diffusion process into [T ] := {0, 1, 2, . . . , } steps. We choose = (the sequence length) and decode exactly one token per step, which we treat as full-step sampling procedure; this yields high-quality teacher-generated samples and provides clean target distribution for the following trajectory distribution: pϕ(x0:T ) = p(xT ) (cid:89) t= pϕ(xt1 xt). (11) 4 The marginal distribution of the noisy states xt is obtained by integrating out the intermediate states: (cid:90) pϕ(xt) = pϕ(xt:T ) dxt+1:T , [T 1]. (12) For any [T ], we define joint distribution over (x0, xt) by combining this marginal with the forward corruption process q(xt x0) defined in Eqn. 1: Jϕ(x0, xt) = pϕ(x0) q(xt x0). (13) Now consider student model pθ, initialized from the teacher pϕ. We define the students induced joint distribution over (x0, xt) as: Jθ(x0, xt) = pθ(xt) pθ(x0 xt). (14) Self-distillation aims to bridge this gap by aligning the student model with the teacher through minimizing divergence between their joint distributions. Concretely, we optimize either the KL or reverse KL divergence between the teacher and student joint distributions over intermediate diffusion states: θ = arg min θ Et (cid:2)D(cid:0)Jϕ(x0, xt) (cid:13) (cid:13) Jθ(x0, xt)(cid:1)(cid:3) . (15) When choosing the forward KL as the objective, the resulting self-distillation loss reduces to form closely resembling the standard training objective of masked diffusion models: Ldistill(θ) = Epϕ(x0) Extq(xtx0)[log pθ(x0 xt)] . (16) 4.2. Optimality of Trajectory Self-Distillation We study the optimality properties of trajectory selfdistillation by comparing the teacherand student-induced joint distributions over (x0, xt). Our analysis focuses on their reverse conditionals pϕ(x0 xt) and pθ(x0 xt), which characterize how each model maps an intermediate state back to clean sequence. Definition 4.1 (Trajectory Joint Distribution). Let pϕ be pretrained diffusion language model with diffusion steps. For any [T ], we define the trajectory joint distribution between the clean sequence x0 and an intermediate state xt as: Tra ϕ (x0, xt) := pϕ(xt) pϕ(x0 xt) (17) where pϕ(xt) denotes the marginal distribution of xt and pϕ(x0 xt) denotes the posterior distribution of the clean sequence given the intermediate state. Assumption 4.2 (Margin Matching). Given the teacher trajectory joint distribution Tra ϕ (x0, xt) and the student joint distribution Jθ(x0, xt), we assume that the teacher and student share the same marginal distribution over intermediate states xt, that is, pϕ(xt) = pθ(xt), [T ]. (18) Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization The Assumption 4.2 allows us to compare the teacher and student reverse conditionals under common reference distribution over xt. It is reasonable on-policy assumption in our distillation setting for two reasons. First, the student model is initialized from the teacher. Second, trajectory distillation is performed exclusively on trajectories generated by the teacher, which limits marginal distribution drift during optimization. Proposition 4.3 (Optimality of Tra for on-policy posterior matching). For teacher model pϕ and student model pθ, we define the on-policy risk for distillation: [T ], ϕ Rt(θ) := Extpϕ(xt) (cid:104) D(cid:0)pϕ( xt) pθ( xt)(cid:1)(cid:105) . (19) Given the target joint distribution Jϕ and Tra defined in Eqn. 14 and Definition 4.1, as well as joint distribution of student model Jθ in Eqn. 14, let ϕ θ Tra arg min θ θ arg min θ D(J Tra ϕ (x0, xt) Jθ(x0, xt)), D(Jϕ(x0, xt) Jθ(x0, xt)). Under the Assumption 4.2, there is Rt(θ Tra) Rt(θ). (20) Proposition 4.3 shows that trajectory distillation yields solution that is optimal for the on-policy conditional matching objective (see the proof in Appendix B). Intuitively, trajectory self-distillation aligns the student with the teacher under the same intermediate-state distribution encountered at inference time, thereby avoiding the factorization mismatch induced by marginal-only training. 4.3. Trajectory-Distillation as Discrete Rectified Flow. Rectified Discrete Flow (ReDi). Rectified Discrete Flow (ReDi) (Yoo et al., 2025) provides theoretical framework for enabling few-step generation in discrete diffusion models by reducing the factorization error induced by independent token-wise decoding. The core quantity underlying ReDi is the conditional total correlation (TC) defined in Definition 4.4, which measures the discrepancy between the true joint conditional distribution and its fully factorized approximation. Definition 4.4 (Conditional Total Correlation (TC)). Let J(xs, xt) = p(xt) p(xs xt) be joint distribution over two intermediate states in diffusion process, with < t. The Conditional Total Correlation (TC) of xs given xt is defined as CJ (xs xt) := Ext (cid:34) (cid:16) p(xs xt) (cid:13) (cid:13) KL (cid:89) i=1 p(xi xt) (cid:17) (cid:35) . (21) 5 smaller conditional TC implies that the joint conditional distribution can be better approximated by parallel, tokenwise conditionals (Xu et al., 2024a; Yoo et al., 2025), which is necessary condition for accurate few-step generation. ReDi iteratively constructs and refines joint distribution over (x0, xT ) by alternating between rectification and refitting. Under certainty assumptions, each ReDi iteration monotonically decreases the conditional TC. Theorem 4.5 (Trajectory distribution induces lower conditional dependence). Let pϕ denote pretrained teacher model and pθ student model. Consider the trajectory joint distribution Tra defined in Definition 4.1, and the modelinduced joint distribution Jθ defined in Eqn. 14. We use Tra to denote this optimal student distribution. Then under θ certain assumptions the following inequalities hold: ϕ (cid:104) Et (cid:104) Et CJ Tra θ (x0 xt) + CJ Tra θ CJ Tra ϕ (x0 xt) + CJ Tra ϕ (cid:105) (xt xT ) (cid:105) (xt xT ) , (22) where C( ) denotes the Conditional Total Correlation defined in Definition 4.4. Please see the proof in Appendix B. Trajectory Distillation induces lower Conditional Total Correlation. While ReDi operates on the joint distribution (x0, xT ), our proposed T3D generalizes this rectification process to intermediate states along the diffusion trajectory. As shown in Theorem 4.5, trajectory distillation further reduces Conditional Total Correlation, yielding stronger inductive bias toward factorized decoding. This provides theoretical explanation for why trajectory-level supervision enables more accurate few-step generation. 4.4. Analysis of DDO We observe that the DDO objective admits the following upper bound: LTraj-DDO(θ) Tra ϕ EJθref [log σ()] (cid:2)log(cid:0)1 σ()(cid:1)(cid:3) , where = log pθ(x0xt) pθref (x0xt) . (23) (24) This upper bound reveals that DDO performs discriminative comparison between the student and the reference trajectory distributions by optimizing the log-likelihood ratio . Consequently, the objective emphasizes regions where the student assigns lower conditional likelihood than the reference model, focusing optimization on trajectory states that require the most correction. Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Table 1. Few-step performance comparison across baselines on SDAR-1.7B-Chat and SDAR-4B-Chat (Cheng et al., 2025). All trainable baselines are trained with decoding block size of 8. Few-step performance is evaluated using tokens-per-step (TokPS): for example, Block Size= 4 and TokPS= 2 means decoding uses blocks of 4 tokens while generating 2 tokens per diffusion step, resulting in 4/2 = 2 diffusion steps per block. SD means Self-Distillation methods. Improvement denotes the percentage change relative to the original model; (green) indicates improvement and (red) indicates degradation. Boldface denotes the best performance among self-distillation methods. The Original Model and SFT are reported for reference only and are excluded from best-result comparisons. TokPS Method SD Block Size = 4 Block Size = 8 Average Improvement (%) MATH500 GSM8K MBPP HumanEval MATH500 GSM8K MBPP HumanEval 2 4 2 4 Original Model SFT ReDi dParallel Naive TD T3D (Ours) Original Model SFT ReDi dParallel Naive TD T3D (Ours) Original Model SFT ReDi dParallel Naive TD T3D (Ours) Original Model SFT ReDi dParallel Naive TD T3D (Ours) - - - - 39.40 43.00 40.60 43.40 43.00 47.00 5.00 22.40 15.00 22.80 24.20 25. 54.40 54.60 41.00 52.60 50.80 60.00 13.80 39.00 25.40 34.20 39.00 47.80 63.00 61.79 63.99 68.23 66.19 70. 13.34 36.62 32.45 45.26 46.02 42.91 78.77 54.60 73.62 76.57 72.78 83.85 41.09 48.14 53.30 45.94 57.92 69. 30.40 30.00 13.20 22.20 21.20 27.20 10.60 6.20 3.40 10.20 6.00 9.40 34.20 26.80 20.00 23.80 21.40 38. 14.00 9.00 5.00 13.20 10.40 22.60 SDAR-1.7B-Chat 32.93 34.76 16.46 24.39 19.51 30.49 12.20 5. 5.49 12.20 15.24 15.24 33.60 36.80 36.40 45.20 39.00 47.80 4.80 20.00 12.80 25.40 26.60 24.40 SDAR-4B-Chat 49.39 37.20 21.95 39.63 24.39 51.83 18.29 15.85 7.32 20.73 17.07 23.78 49.60 54.44 23.60 51.20 47.00 61. 16.80 40.20 20.20 40.80 36.20 44.80 55.88 62.55 62.17 67.70 63.99 68.84 12.74 39.65 29.72 42.91 39.73 37. 72.33 77.41 71.87 75.97 68.01 81.96 41.02 55.42 47.84 53.83 50.80 63.99 27.80 27.20 12.80 23.20 17.20 26. 10.20 4.40 4.00 10.40 9.00 9.20 33.40 25.60 19.20 18.20 19.60 37.00 10.00 8.80 6.80 9.60 12.40 21. 37.20 37.80 13.41 26.83 18.29 25.61 10.37 7.93 4.88 11.59 9.76 14.02 46.95 29.88 23.17 28.66 22.56 56. 16.46 11.59 6.71 20.12 9.00 23.17 40.03 41.74 32.38 40.14 36.05 43.06 9.91 17.84 13.47 22.60 22.07 22. 52.38 46.76 36.80 45.83 40.82 58.89 21.43 28.50 21.57 29.80 28.92 39.66 - 4.28 19.11 0.29 9.94 7. - 80.05 35.95 128.09 122.78 124.79 - 10.73 29.74 12.51 22.07 12.43 - 32.98 0.65 39.05 34.95 85. 5. Experiments In this section, we present experimental results demonstrating that our method, T3D, effectively improves few-step generation performance in diffusion language models. In addition, we conduct series of ablation studies to complement our main results. Please refer to Appendix for further details. 5.1. Experimental Settings Baselines. We first compare against ReDi (Yoo et al., 2025), which learns directly from teacher-generated samples and can be viewed as standard self-distillation method corresponding to Eqn. 16. We also include dParallel (Chen et al., 2025), distillation approach that maximizes the transition probability from fully masked sequences to clean sequences generated by the teacher model. We also take the Naive Trajectory Self-Distillation (Navie TD) defined in Eqn. 5 as baseline to verify the role of DDO. Finally, we report results from Supervised Fine-Tuning (SFT) on real data as reference point to contextualize the performance of self-distillation methods. Training Dataset. For self-distillation methods, we collect model-generated responses on the MATH training set (Hendrycks et al., 2021) for mathematical reasoning tasks and the PrimeIntellect dataset (Jaghouar et al., 2024) for code generation tasks. For the SFT baseline, we use dataset derived from the Bespoke-Stratos-17k benchmark (Labs et al., 2025). Following prior work (Wang et al., 2025a), we adopt two open-source collections that are pre-filtered to maximum sequence length of 600 tokens. Benchmark Datasets. We conduct experiments on four widely used benchmarks covering mathematical reasoning and code generation: GSM8K (Cobbe et al., 2021) (gradeschool arithmetic word problems), MATH500 (Lightman et al., 2023) (challenging high-school and competition-level mathematics problems), MBPP (Austin et al., 2021), and HumanEval (Chen, 2021). These benchmarks require multi-step reasoning or iterative refinement to produce correct solutions, making them particularly sensitive to decoding quality under step compression. As such, they provide suitable testbed for evaluating the effectiveness and robustness of few-step generation in diffusion language models. Models. All experiments are conducted on the SDAR fam6 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization ily of models (Cheng et al., 2025). SDAR is block-based diffusion language model that performs diffusion-based generation within each block and autoregressive generation across blocks, resulting in semi-autoregressive decoding process. Compared to full diffusion models, prior studies have shown that SDAR-style block diffusion achieves better trade-off between generation quality and inference efficiency (Cheng et al., 2025; Arriola et al., 2025). We use SDAR-1.7B-Chat and SDAR-4B-Chat as our base models. Implementations. In our experiments, we construct the training datasets by prompting the teacher model to answer questions from the corresponding training sets. To get data with better quality, we use low-confidence remasking strategy (Nie et al., 2025; Wang et al., 2025b) with static decoding, setting block size to 4 and steps per block to 4. To recover the exact generation trajectories, we record the decoding order of tokens in the final clean sequences, following procedure similar to prior work (Wang et al., 2025b). Given trajectory, intermediate states xt are obtained by masking tokens according to the recorded decoding order. We also mix some random tokens into the input to improve the training robustness that is following the previous work (Zhu et al., 2025). For all training settings, we perform full-parameter fine-tuning. All experiments are conducted using 8 NVIDIA A100 (40GB) GPUs. For more details, please refer to Appendix C. Table 2. Preserving diffusion performance under full decoding. We revert few-step distilled models to full diffusion decoding using static decoding (one token per step) without additional training. Results are reported under block size 4 and 4 steps per block, showing that T3D preserves diffusion performance. Bold numbers denote the best result among self-distillation methods. Method MATH500 GSM8K MBPP HumanEval SDAR-1.7B-Chat Original Model SFT ReDi dParallel Naive TD T3D (Ours) 59.40 52. 47.00 0.40 49.80 56.80 80.59 73.09 73.77 0.23 72.40 78.01 SDAR-4B-Chat Original Model SFT ReDi dParallel Naive TD T3D (Ours) 68.00 60.20 50.40 13.20 57.40 70.00 89.84 86.05 82.03 2.88 82.11 89.31 45.20 44.20 27.60 34.60 35.20 41. 58.60 50.20 34.00 34.00 37.60 54.20 59.76 60.37 31.10 43.29 32.93 57.32 71.95 69.51 37.80 48.17 43.90 73. T3D is more resilient to extreme few-step generation and better preserves generation quality when the decoding budget is severely constrained. 5.3. Preserving Diffusion Performance under Full 5.2. Improving Performance of Few-Step Decoding by Decoding Self-Distillation Settings. In this experiment, we evaluate the ability of diffusion language models to perform few-step decoding under high Tokens Per Step (TokPS) settings. We consider four configurations by varying both the block size and the decoding budget. Specifically, we use block sizes of 4 and 8, and for each block size we evaluate TokPS values of 2 and 4. For block size 4, TokPS = 2 and 4 correspond to decoding each block in two and one steps, respectively; for block size 8, TokPS = 2 and 4 correspond to decoding each block in four and two steps, respectively. Together, these settings enforce increasingly aggressive few-step generation and allow us to systematically assess model robustness under tight decoding budgets. Results. Table 1 compares different self-distillation methods under aggressive few-step decoding across four TokPS settings and two model scales. Overall, T3D consistently achieves the best or near-best performance among all self-distillation baselines, demonstrating strong robustness under tight decoding budgets. For both SDAR-1.7B-Chat and SDAR-4B-Chat, T3D maintains stable performance as TokPS increases, while competing methods degrade substantially under more aggressive few-step settings (TokPS = 4). These results indicate that Settings. In this experiment, we investigate whether fewstep distillation leads to diffusion property forgetting, i.e., whether model optimized for compressed decoding degrades when reverted to the original full diffusion process. To evaluate this, we take models distilled for few-step generation and directly restore them to full diffusion decoding using static decoding strategy, decoding one token per step without any additional training. Results. Table 2 reports the results. Across both SDAR1.7B-Chat and SDAR-4B-Chat, our methods preserve strong In particular, T3D performance under full decoding. achieves performance nearly identical to the original pretrained model on all benchmarks, and in some cases slightly outperforms it. In contrast, prior baselines such as ReDi and dParallel exhibit substantial performance degradation. Discussions. These results indicate that trajectory selfdistillation does not overfit to few-step decoding, but instead preserves the models fine-grained denoising capability. Overall, our approach enables few-step generation without sacrificing full diffusion performance. 5.4. Experiments on Dynamic Decoding Settings. Dynamic decoding (Wu et al., 2025b; Yang et al., 2025) is an adaptive inference strategy that determines the 7 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Table 3. Dynamic decoding results with block size 4, 4 steps per block, confidence threshold 0.9, and temperature 0.1. We report throughput (TPS), per-sample latency (Latency), average decoding steps and sequence length (Avg Steps & Avg Len), and accuracy (Acc). Bold numbers indicate the best performance among baseline methods. All experiments are done using SDAR4B-Chat model. Method MATH Original ReDi dParallel Naive TD T3D (Ours) GSM8K Original ReDi dParallel Naive TD T3D (Ours) MBPP Original ReDi dParallel Naive TD T3D (Ours) HumanEval Original ReDi dParallel Naive TD T3D (Ours) TPS Latency Avg Steps Avg Len Acc 657.72 715.71 692.08 693.85 791. 580.60 636.58 805.02 696.99 843.05 262.66 298.83 215.65 314.99 313.18 175.48 163.77 130.34 216.39 222. 1.10 1.04 0.95 0.97 0.66 0.43 0.49 0.39 0.47 0.37 0.36 0.21 0.63 0.31 0. 0.73 0.47 0.48 0.29 0.26 196.19 198.24 170.22 177.99 137.95 721.90 39. 757.05 653.98 678.55 525.50 27.00 45.80 44.00 49.40 71.12 84.63 83.23 89.78 83.03 27.25 17.11 36.03 26.43 16. 36.56 21.23 17.41 17.15 16.21 249.52 61.56 311.99 310.58 330.82 312.48 93. 62.57 135.16 98.80 61.62 54.89 67.02 62.40 72.40 23.40 10.00 8.40 9.80 23.60 127.54 33. 76.75 62.19 62.10 58.10 10.00 23.78 23.17 29.27 number of tokens decoded at each step based on confidence threshold. Although our methods are trained to improve performance under tight static step budgets and are primarily evaluated with static decoding for fair comparison, we additionally report results under dynamic decoding due to its practical relevance. All dynamic decoding experiments use block size 4, 4 steps per block, and fixed threshold of 0.9. Metrics. We report throughput, latency, average decoding steps and output length to measure efficiency, along with accuracy to assess model quality. Results. Table 3 summarizes the results under dynamic decoding. Although our models are trained primarily for static decoding, trajectory-based self-distillation remains effective in this adaptive inference setting. Across all benchmarks, T3D consistently achieves strong performance, improving accuracy while reducing the number of decoding steps and latency. Notably, T3D attains the best or near-best accuracy on all datasets and simultaneously achieves higher throughput than other baselines. These results indicate that T3D are compatible with dynamic decoding and generalize beyond static decoding regimes used during training. 6. Related Work Few-step Diffusion Despite their remarkable success, diffusion-based generative models (Yang et al., 2023) incur substantial computational cost due to iterative sampling, motivating extensive efforts to reduce the number of sampling steps. Consistency Models (Song et al., 2023; Song & Dhariwal, 2023) accelerate generation by enforcing consistency across time, while flow-map-based methods (Geng et al., 2025; Boffi et al., 2024) reduce sampling by directly modeling state-to-state displacements. In practice, their distillation-based variants are often more stable and achieve stronger performance, largely because they exploit teacher trajectories as supervision: Consistency Distillation (Song et al., 2023) matches teacher intermediate states, CMT (Hu et al., 2025a) bootstraps training with teacher rollouts, and Re-MeanFlow (Zhang et al., 2025a) leverages teacher-rectified trajectories for efficient one-step modeling. In discrete spaces, recent work (Yoo et al., 2025; Chen et al., 2025; Deschenaux & Gulcehre, 2024) also distills discrete diffusion by borrowing insights from continuous diffusion, but does not fully utilize supervision available throughout the generated denoising trajectory. Efficiency for Diffusion Language Models. In practice, the efficiency of Diffusion Language Models (DLMs) is often limited by the requirement of many refinement steps and the lack of effective KV caching (Li et al., 2025), resulting in high inference latency. Recent work has therefore explored improving DLM efficiency from several complementary perspectives. Dynamic Decoding (Wu et al., 2025b) proposes training-free strategy that accelerates inference by adaptively selecting the number of tokens decoded at each step. Building on this idea, dParallel (Chen et al., 2025) further improves decoding efficiency by explicitly training the model to increase the maximum number of tokens that can be decoded per step, thereby enabling more aggressive dynamic decoding. Other approaches investigate how to adapt KV caching mechanisms to DLMs despite their bidirectional attention (Hu et al., 2025b; Ma et al., 2025; Liu et al., 2025). Specifically, dKV-Cache (Ma et al., 2025) and FastDLLM (Wu et al., 2025b) address the challenges caused by bidirectional dependencies using delayed and conditioningbased caching strategies. Block-Diffusion (Arriola et al., 2025; Cheng et al., 2025; Wu et al., 2025a) further exploit block-level generation to enable block-wise KV caching and reduce inference cost. Finally, Discrete Diffusion Forcing (D2F) (Wang et al., 2025a) reformulates discrete diffusion language models with block-wise autoregressive generation, achieving significantly faster inference. 7. Conclusion In this work, we introduce T3D, trajectory self-distillation framework that trains few-step student by distilling from Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization rollout trajectories generated by the same pretrained model under the target decoding procedure. To better handle the highly multimodal denoising posteriors under tight step budgets, we replace mode-covering forward-KL trajectory matching with mode-seeking Direct Discriminative Optimization (DDO) objective, and introduce lightweight path-consistency reweighting to mitigate error propagation from early decoding decisions. Across reasoning and codegeneration benchmarks, our approach improves few-step accuracy at lower latency and generalizes to dynamic decoding, substantially narrowing the gap to full-step diffusiontaking concrete step toward practical few-step diffusion language models."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Alizadeh, K., Mirzadeh, S. I., Belenko, D., Khatamifard, S., Cho, M., Del Mundo, C. C., Rastegari, M., and Farajtabar, M. Llm in flash: Efficient large language model inference with limited memory. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1256212584, 2024. Arriola, M., Gokaslan, A., Chiu, J. T., Yang, Z., Qi, Z., Han, J., Sahoo, S. S., and Kuleshov, V. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Boffi, N. M., Albergo, M. S., and Vanden-Eijnden, E. Flow map matching. arXiv preprint arXiv:2406.07507, 2, 2024. Chen, M. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Deschenaux, J. and Gulcehre, C. Beyond autoregression: Fast llms via self-distillation through time. arXiv preprint arXiv:2410.21035, 2024. Geng, Z., Deng, M., Bai, X., Kolter, J. Z., and He, K. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hu, Z., Lai, C.-H., Mitsufuji, Y., and Ermon, S. Cmt: Midtraining for efficient learning of consistency, mean flow, and flow map models. arXiv preprint arXiv:2509.24526, 2025a. Hu, Z., Meng, J., Akhauri, Y., Abdelfattah, M. S., Seo, J.-s., Zhang, Z., and Gupta, U. Accelerating diffusion language model inference via efficient kv caching and guided diffusion. arXiv preprint arXiv:2505.21467, 2025b. Jaghouar, S., Ong, J. M., Basra, M., Obeid, F., Straube, J., Keiblinger, M., Bakouch, E., Atkins, L., Panahi, M., Goddard, C., et al. Intellect-1 technical report. arXiv preprint arXiv:2412.01152, 2024. Kim, M., Xu, C., Hooper, C., Singh, H., Athiwaratkun, B., Zhang, C., Keutzer, K., and Gholami, A. Cdlm: Consistency diffusion language models for faster sampling. arXiv preprint arXiv:2511.19269, 2025. Labs, I., Khanna, S., Kharbanda, S., Li, S., Varma, H., Wang, E., Birnbaum, S., Luo, Z., Miraoui, Y., Palrecha, A., et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Chen, Z., Fang, G., Ma, X., Yu, R., and Wang, X. dparallel: Learnable parallel decoding for dllms. arXiv preprint arXiv:2509.26488, 2025. Li, T., Chen, M., Guo, B., and Shen, Z. survey on diffusion language models. arXiv preprint arXiv:2508.10875, 2025. Cheng, S., Bian, Y., Liu, D., Zhang, L., Yao, Q., Tian, Z., Wang, W., Guo, Q., Chen, K., Qi, B., et al. Sdar: synergistic diffusion-autoregression paradigm for scalable sequence generation. arXiv preprint arXiv:2510.06303, 2025. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. 9 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Liu, Z., Yang, Y., Zhang, Y., Chen, J., Zou, C., Wei, Q., Wang, S., and Zhang, L. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. Wang, Y., Yang, L., Li, B., Tian, Y., Shen, K., and Wang, M. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025b. Ma, X., Yu, R., Fang, G., and Wang, X. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. Wolfer, G. and Watanabe, S. Geometric aspects of dataprocessing of markov chains. Transactions of Mathematics and Its Applications, 8(1):tnae001, 2024. Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Jin, H., Chen, T., and Jia, Z. Towards efficient generative large language model serving: survey from algorithms to systems. ACM Computing Surveys, 58(1):137, 2025. Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Qian, Y.-Y., Su, J., Hu, L., Zhang, P., Deng, Z., Zhao, P., and Zhang, H. d3llm: Ultra-fast diffusion llm using pseudotrajectory distillation. arXiv preprint arXiv:2601.07568, 2026. Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136 130184, 2024. Schiff, Y., Sahoo, S. S., Phung, H., Wang, G., Boshar, S., Dalla-torre, H., de Almeida, B. P., Rush, A. M., PIERROT, T., and Kuleshov, V. Simple guidance mechanisms for discrete diffusion models. In The Thirteenth International Conference on Learning Representations. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. Song, Y. and Dhariwal, P. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. 2023. Song, Y., Zhang, Z., Luo, C., Gao, P., Xia, F., Luo, H., Li, Z., Yang, Y., Yu, H., Qu, X., et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Wang, X., Xu, C., Jin, Y., Jin, J., Zhang, H., and Deng, Z. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192, 2025a. Wu, C., Zhang, H., Xue, S., Diao, S., Fu, Y., Liu, Z., Molchanov, P., Luo, P., Han, S., and Xie, E. Fastdllm v2: Efficient block-diffusion llm. arXiv preprint arXiv:2509.26328, 2025a. Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., and Xie, E. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025b. Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. Xu, M., Geffner, T., Kreis, K., Nie, W., Xu, Y., Leskovec, J., Ermon, S., and Vahdat, A. Energy-based diffusion language models for text generation. arXiv preprint arXiv:2410.21357, 2024a. Xu, Y., Gong, M., Xie, S., Wei, W., Grundmann, M., Batmanghelich, K., and Hou, T. Semi-implicit denoising diffusion models (siddms). Advances in neural information processing systems, 36:17383, 2024b. Yang, H., Hu, R., Sun, Z., Zhou, R., Cai, Y., and Wang, Y. Wavefrontdiffusion: Dynamic decoding schedule for improved reasoning. arXiv preprint arXiv:2511.19473, 2025. Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B., and Yang, M.-H. Diffusion models: comprehensive survey of methods and applications. ACM computing surveys, 56(4):139, 2023. Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Yoo, J., Kim, W., and Hong, S. Redi: Rectified discrete flow. arXiv preprint arXiv:2507.15897, 2025. Zhang, X., Tan, S., Nguyen, Q., Dao, Q., Han, L., He, X., Zhang, T., Mrdovic, A., and Metaxas, D. Flow straighter and faster: Efficient one-step generative modeling via meanflow on rectified trajectories. arXiv preprint arXiv:2511.23342, 2025a. Zhang, Y., Schwing, A., and Zhao, Z. Variational masked arXiv preprint arXiv:2510.23606, diffusion models. 2025b. 10 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Zhen, R., Li, J., Ji, Y., Yang, Z., Liu, T., Xia, Q., Duan, X., Wang, Z., Huai, B., and Zhang, M. Taming the titans: survey of efficient llm inference serving. arXiv preprint arXiv:2504.19720, 2025. Zheng, K., Chen, Y., Chen, H., He, G., Liu, M.-Y., Zhu, J., and Zhang, Q. Direct discriminative optimization: Your likelihood-based visual generative model is secretly gan discriminator. arXiv preprint arXiv:2503.01103, 2025. Zhu, Y., Wang, X., Lathuili`ere, S., and Kalogeiton, V. Di [m] o: Distilling masked diffusion models into one-step generator. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1860618618, 2025. 11 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Appendix This appendix provides additional technical details that support the main paper. In Appendix A, we summarize our method and present the full algorithm for clarity. In Appendix B, we provide detailed proofs for the theoretical analysis. In Appendix C, we report implementation details used in our experiments, including: Implementation of the random token mixture training (Appendix C.1), Multi-round reference model update (Appendix C.2), Prompts used for data collection during experiments (Appendix C.3), The accelerated inference engine used for rollouts (Appendix C.4). Finally, in Appendix D, we present additional ablation studies, including: The choice of regularization coefficient λ (Appendix D.1), Analysis of preserving full-step diffusion properties (Appendix D.2), Additional ablations on few-step generation (Appendix D.3). A. Algorithm In this section, we describe the training algorithm of T3D. Algorithm 1 provides the pseudocode of the full training procedure, while Fig. 1 presents high-level overview of the method for better conceptual understanding. Algorithm 1 T3D Training Require: Teacher model pϕ Require: Student model pθ (initialized from teacher) Require: Path regularization weight λ 1: Sample trajectory pairs (x0, xt) pϕ 2: repeat 3: 4: 5: 6: Set reference model pθref StopGrad(pθ) Compute trajectory DDO loss LtrajDDO Compute path consistency loss Lpath Update student model using = LtrajDDO + λLpath 7: until convergence output pθ B. Proof of Theoretical Analysis In this section, we provide detailed proofs for the theoretical results presented in the main paper. Our analysis focuses on understanding the behavior of trajectory-level self-distillation under few-step decoding and its effect on the factorization properties of the reverse diffusion process. Assumption B.1 (Margin Matching). Given the teacher trajectory joint distribution Tra ϕ (x0, xt) and the student joint distribution Jθ(x0, xt), we assume that the teacher and student share the same marginal distribution over intermediate states xt, that is, pϕ(xt) = pθ(xt), [T ]. (25) Lemma B.2 (KL decomposition for joint distributions). For any two joint distributions (x, y) = (y)P (x y) and Q(x, y) = Q(y)Q(x y), the KullbackLeibler divergence admits the decomposition KL(P (x, y)Q(x, y)) = KL(P (y)Q(y)) + EyP (y) (cid:2)KL(cid:0)P (x y)Q(x y)(cid:1)(cid:3) . (26) 12 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Figure 1. Overview of T3D for enabling few-step diffusion decoding. (a) Teacher full trajectory. pretrained diffusion language model generates sequences through full diffusion trajectory from the fully masked state xT to the clean sequence x0 using many refinement steps, producing intermediate states that reflect the inference-time decoding distribution. (b) Trajectory-level objective learning. T3D trains few-step student model via trajectory self-distillation on teacher rollout trajectories. We incorporate Direct Discriminative Optimization (DDO) to perform mode-seeking trajectory matching between the student and reference model under an on-policy discriminative objective. (c) Few-step decoding. After training, the student can decode using significantly fewer diffusion steps (e.g., 12 steps per block) while preserving generation quality, substantially narrowing the gap with full-step decoding and enabling efficient parallel token generation. Proposition B.3 (Optimality of Tra pθ, we define the on-policy risk for distillation at step t: ϕ for on-policy posterior matching). For given teacher model pϕ and student model Rt(θ) := Extpϕ(xt) (cid:104) D(cid:0)pϕ( xt) pθ( xt)(cid:1)(cid:105) , [T ] (27) Given the target joint distribution Jϕ and Tra model Jθ in Eqn. 14, let ϕ defined in Eqn. 14 and Definition 4.1, as well as joint distribution of student θ Tra arg min θ θ arg min θ D(J Tra ϕ (x0, xt) Jθ(x0, xt)), D(Jϕ(x0, xt) Jθ(x0, xt)). Under the Assumption 4.2, there is Rt(θ Tra) Rt(θ). (28) Proof. Forward KL case. We first consider the case where is the forward KullbackLeibler divergence. Under Assumption 4.2, the teacher and student share the same marginal distribution over intermediate states, i.e., pθ(xt) = pϕ(xt) for all [T ]. By applying Lemma B.2 to the joint distributions, for any fixed we can decompose the KL divergence as KL(J Tra ϕ (x0, xt) Jθ(x0, xt)) = Extpϕ(xt) (cid:104) KL(cid:0)pϕ(x0 xt) pθ(x0 xt)(cid:1)(cid:105) , KL(Jϕ(x0, xt) Jθ(x0, xt)) = KL(cid:0)pA(xt) pθ(xt)(cid:1) + ExtpA(xt) KL(cid:0)pϕ(x0 xt) pθ(x0 xt)(cid:1)(cid:105) (cid:104) , (29) (30) where pA(xt) = (cid:82) pϕ(x0)q(xt x0) dx0 denotes the marginal distribution induced by the forward noising process. Eqn. 29 shows that minimizing KL(J Tra Extpϕ(xt)[KL(pϕ( xt) pθ( xt))]. Hence, θ choice from the perspective of R(θ), the inequality follows immediately: ϕ Jθ) is exactly equivalent to minimizing the on-policy distillation risk R(θ) = Tra is global minimizer of R(θ). Since θ is an arbitrary parameter Rt(θ Tra) Rt(θ), (31) which completes the proof for the forward KL case. 13 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Reverse KL case. Now we prove the claim also holds when is the reverse KL divergence, i.e., D(P Q) = KL(QP ). Accordingly, define the on-policy reverse-KL risk"
        },
        {
            "title": "RrKL\nt",
            "content": "(θ) := Extpϕ(xt) (cid:104) KL(cid:0)pθ( xt) pϕ( xt)(cid:1)(cid:105) . (32) Under Assumption 4.2, we have pθ(xt) = pϕ(xt) for all t. Applying Lemma B.2 to the reverse direction, for any fixed we obtain KL(Jθ Tra ϕ ) = KL(cid:0)pθ(xt) pϕ(xt)(cid:1) + Extpθ(xt) (cid:104) KL(cid:0)pθ(x0 xt) pϕ(x0 xt)(cid:1)(cid:105) (cid:104) KL(cid:0)pθ(x0 xt) pϕ(x0 xt)(cid:1)(cid:105) = Extpϕ(xt) = RrKL (θ), (33) (34) where the second equality uses pθ(xt) = pϕ(xt). Similarly, recall that Jϕ(x0, xt) = pA(xt) pϕ(x0 xt) with pA(xt) = (cid:82) pϕ(x0)q(xt x0) dx0. Again by Lemma B.2, KL(Jθ Jϕ) = KL(cid:0)pθ(xt) pA(xt)(cid:1) + Extpθ(xt) (θ), = KL(cid:0)pθ(xt) pA(xt)(cid:1) + RrKL KL(cid:0)pθ(x0 xt) pϕ(x0 xt)(cid:1)(cid:105) (cid:104) (35) where the last equality again uses pθ(xt) = pϕ(xt). Therefore, minimizing KL(JθJ Tra hand, minimizing KL(JθJϕ) is minimizing RrKL This implies ϕ ) is exactly minimizing RrKL (θ). On the other (θ) plus the constant KL(pϕ(xt)pA(xt)), which does not depend on θ. Tra is an optimizer of RrKL (θ), hence θ Definition B.4 (Conditional Total Correlation (TC)). Let J(xs, xt) = p(xt) p(xs xt) be joint distribution over two intermediate states in diffusion process, with < t. The Conditional Total Correlation (TC) of xs given xt is defined as RrKL (θ Tra) RrKL (θ). (36) CJ (xs xt) := Ext (cid:34) (cid:16) KL p(xs xt) (cid:13) (cid:13) (cid:89) i= (cid:17) p(xi xt) (cid:35) . (37) This quantity measures the degree of conditional dependence among the components of xs given xt, by comparing the true conditional joint distribution to its fully factorized approximation. Assumption B.5 (Ideal Optimality of TD and T3D). We assume that both trajectory distillation methods considered in this work reach their ideal optima under sufficient model capacity and perfect optimization. Specifically, we assume that [T ], pθ Tra = arg min pθ (cid:16) KL pϕ(x0 xt) (cid:13) (cid:13) pθ(x0 xt) (cid:17) , (38) Tra is the optimal result of T3D. This assumption abstracts away optimization and approximation errors and allows where pθ us to focus on the effect of trajectory factorization and sampling constraints in few-step generation. Assumption B.6. Let be the family of T-step decoding processes. We assume that [T ], pθ(x0xt) lies within the log-convex hull of . Lemma B.7 (Pythagorean Inequality for KL Divergence (Wolfer & Watanabe, 2024)). Let be log-convex set. If = arg minqQ KL(pq) and Q, then KL(pr) KL(pq) + KL(qr). Theorem B.8 (Trajectory distribution induces lower conditional dependence). Let pϕ denote pretrained teacher model and pθ student model. Consider the trajectory joint distribution Tra defined in Definition 4.1, and the model-induced joint distribution Jθ defined in Eqn. 14. Let ϕ θ Tra arg min θ KL(J Tra ϕ (x0, xt) Jθ(x0, xt)), 14 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization and we use Tra θ to denote this optimal student distribution. Then the following inequalities hold: Et (cid:104) CJ Tra θ (x0 xt) + CJ Tra θ (cid:105) (xt xT ) Et (cid:104) CJ Tra ϕ (x0 xt) + CJ Tra ϕ (xt xT ) (cid:105) , (39) where C( ) denotes the Conditional Total Correlation defined in Definition B.4. Proof."
        },
        {
            "title": "T CJ Tra",
            "content": "ϕ (x0, xt) = Extpϕ(xt) KL (cid:34) (cid:34) Extpϕ(xt) KL (cid:34) Extpϕ(xt) KL (cid:34) = Extpϕ(xt) KL (cid:16) (cid:16) (cid:16) (cid:16) pϕ(x0 xt) (cid:13) (cid:13) (cid:35) (cid:17) pϕ(xi 0 xt) (cid:89) i= pϕ(x0 xt) (cid:13) (cid:13) pθ Tra (x0 xt) (cid:17) (cid:16) + KL pθ Tra (xi 0 xt) (cid:13) (cid:13) (cid:89) i=1 (cid:35) pϕ(xi 0 xt) (cid:17) pθ Tra (xi 0 xt) (cid:13) (cid:13) pθ Tra (xi 0 xt) (cid:13) (cid:13) (cid:35) pϕ(xi 0 xt) (cid:17) (cid:17) (xi 0 xt) (cid:17) (xi 0 xt) pθ Tra pθ Tra (cid:89) i=1 (cid:89) i=1 (cid:89) i=1 (xi 0 xt) (cid:13) (cid:13) pϕ(xi 0 xt) (cid:88) (cid:16) KL + pθ Tra (cid:35) (40) (41) (42) (cid:35) (cid:17) (43) (44) (45) (cid:34) Extpϕ(xt) KL (cid:16) pθ Tra (xi 0 xt) (cid:13) (cid:13) = CJ Tra θ (x0, xt) The first inequality follows from Assumption B.5, which assumes that optimizing trajectory self-distillation is equivalent to minimizing the expected KL divergence. The result then follows by applying Lemma B.7 and Assumption B.6. Because the CJ Tra (xt, xT ) is not optimized in our method, there is CJ Tra (xt, xT ), we have (xt, xT ) = CJ Tra θ ϕ ϕ [T ], CJ Tra θ (x0 xt) + CJ Tra θ (xt xT ) CJ Tra ϕ (x0, xt) + CJ Tra ϕ (xt, xT ). (46) Since the above inequality holds for any fixed t, taking expectation over preserves the inequality. Therefore, we obtain Et (cid:104) CJ Tra θ (x0 xt) + CJ Tra θ (xt xT ) (cid:105) Et (cid:104) CJ Tra ϕ (x0 xt) + CJ Tra ϕ (cid:105) (xt xT ) . (47) C. Implementation Details. In this section, we provide implementation details of our method and experimental setup. C.1. Mixture of Random Tokens As described in Sec. 5.1, we replace some mask tokens with random tokens sampled from the vocabulary uniformly. This design is inspired by recent work on one-step discrete generative modeling for images (Zhu et al., 2025), where mixing mask tokens with uniformly sampled tokens is shown to improve training stability and robustness. Formally, let = (x1, . . . , xL) denote token sequence of length L, and let denote the vocabulary. For each position i, we introduce binary replacement indicator ri Bernoulli(prand), where prand is the probability of replacing mask token with random token. Let (cid:101)xi denote the corrupted token at position i. For positions originally masked, the corruption process is defined as (cid:40) (cid:101)xi = ui, [MASK], if ri = 1, if ri = 0, 15 (48) Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization where ui Uniform(V) is sampled independently from the vocabulary. Equivalently, the corruption distribution for masked positions can be written as q((cid:101)xi masked) = prand Uniform(V) + (1 prand) δ[MASK], (49) where δ[MASK] denotes point mass at the mask token. For positions that are not masked, we keep the original token unchanged, i.e., (cid:101)xi = xi. In our experiments, we set prand = 0.1 during the training and keep it to 0 during the inference. C.2. Multi-Round and Self-Play Update In our loss function Eqn. ??, we introduce reference model pθref, which is initialized from the student model pθ. Following the setup of prior work (Zheng et al., 2025), we adopt multi-round refinement strategy for training. Formally, this process can be written as: Round : pθ n1 (cid:124) (cid:123)(cid:122) (cid:125) Reference (cid:18) σ (cid:19) β log pθn pθ n1 (cid:124) (cid:123)(cid:122) Discriminator (cid:125) Round + 1 : pθ (cid:124)(cid:123)(cid:122)(cid:125) Reference where θ n1 denotes the best-performing student model obtained in round n. In each round, the reference model serves as fixed generator. In our experiments, we update the reference model every 10 global steps, which corresponds to one round in our training schedule. C.3. Prompts In this section, we present the prompts used in our experiments. These prompts are used to query the model and generate responses, which are then collected as trajectories for training. Prompt For Math Reasoning [User]: {problem}. Please reason step by step, and put your final answer within boxed{}. You are precise math problem solver. Solve the given math problem step by step. [Assistant]: Prompt For Code Generation [User]: This is the problem: {problem}. Place your code within single Python code block python. Do not include more than one code block. [Assistant]: C.4. Accelerated Inference For all SDAR-series experiments, rollouts are performed using JetEngine 1, vLLM-style inference framework tailored for diffusion language models. JetEngine is lightweight yet high-performance inference engine designed for SDAR models and other block-wise diffusion decoding architectures. It supports both dense and MoE models, as well as Tensor Parallel distributed inference, and achieves significant speedups compared to naive inference implementations. D. Ablation Study In this section, we present ablation studies for our proposed T3D. In Appendix D.1, we analyze the effect of the regularization coefficient λ. In Appendix D.2, we examine how different components of our method contribute to preserving the full diffusion decoding behavior. Finally, in Appendix D.3, we present ablations under few-step generation settings to evaluate the contribution of each component to the overall performance of our method. 1https://github.com/Labman42/JetEngine 16 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Table 4. Ablation study on the effect of the regularization weight λ under different decoding configurations. We report the model performance across varying Tokens Per Step (TokPS), block sizes, and decoding steps. All experiments are done using MATH500 dataset."
        },
        {
            "title": "TokPS Block Size Decoding Steps",
            "content": "λ = 0.05 λ = 0.2 λ = 0.5 1 1 2 4 4 8 4 8 8 4 8 8 4 8 4 1 2 67.80 62.60 57.20 47.00 40.20 7.20 69.00 64.80 58.60 47.20 45.20 7.60 69.20 65.40 56.20 46.00 42.00 6.20 Table 5. Ablations of self-distillation objectives under full-step diffusion decoding (block size=4, decoding steps=4), evaluated on MATH500. BS means Block size and DS means Decoding Step per Block. Decoding Models MATH500 ACC BS=4, DS=4 SDAR-4B-Chat SDAR-4B-Chat + SFT SDAR-4B-Chat + ReDi SDAR-4B-Chat + Naive TD SDAR-4B-Chat + Naive TD + Lpath SDAR-4B-Chat + Naive TD + Random Tokens SDAR-4B-Chat + Naive TD + Lpath + Random Tokens SDAR-4B-Chat + DDO SDAR-4B-Chat + DDO + random SDAR-4B-Chat + DDO + Lpath + Random Tokens (Ours) 68.00 60.20 50.40 22.00 58.00 57.40 58.60 12.00 65.40 69.00 D.1. The Effectiveness of λ in Training Objective We conduct an ablation study on the regularization weight λ in Eqn. 10. We run these experiments using the SDAR-4BChat model and evaluate it on MATH500 benchmark. Table 4 reports performance under different decoding configurations with varying Tokens Per Step (TokPS), block sizes, and decoding steps. Results. Overall, moderate regularization consistently yields the best or near-best performance across most settings. In particular, λ = 0.2 achieves the strongest results in the majority of configurations, especially under more aggressive few-step decoding regimes (e.g., higher TokPS). In contrast, smaller regularization weight (λ = 0.05) is often insufficient to stabilize training, while overly strong regularization (λ = 0.5) can lead to degraded performance in several settings. Based on these observations, we fix λ = 0.2 for all experiments reported in the main results. D.2. Preserving Full-Step Diffusion Properties In this section, we investigate how different components of the training objective affect models ability to preserve the original full-step diffusion decoding behavior. Specifically, after training with different variants of the few-step training objective, we directly revert the resulting models to full diffusion decoding without any additional fine-tuning, and evaluate whether the diffusion properties of the teacher model are retained. Experimental Settings. We evaluate models under full diffusion decoding using block size 4 and 4 steps per block, corresponding to the original full-step diffusion process. We compare different primary distillation objectives (Naive TD or DDO), combined with different regularization strategies (random initialization or Lpath). All experiments are conducted using SDAR-4B-Chat and evaluated on the MATH500 benchmark. Results and Analysis. Table 5 reports ablation results under full-step diffusion decoding. Although all models are trained using few-step distillation, evaluation is performed by reverting to the original diffusion process. We observe that naively applying trajectory distillation significantly degrades diffusion behavior. Both Naive TD and DDO without additional regularization lead to severe performance collapse (22.00 and 12.00 ACC, respectively), indicating strong diffusion property forgetting. Introducing random initialization, as described in Sec. 5.1, substantially mitigates this issue. For both Naive TD and DDO, 17 Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization Table 6. Ablation study on few-step generation under block size (BS) 8 and 4 decoding steps (DS) per block. Decoding Models MATH500 ACC BS=8, DS=4 SDAR-4B-Chat SDAR-4B-Chat + SFT SDAR-4B-Chat + ReDi SDAR-4B-Chat + Naive TD SDAR-4B-Chat + Naive TD + Lpath SDAR-4B-Chat + Naive TD + random SDAR-4B-Chat + Naive TD + Lpath + random SDAR-4B-Chat + DDO SDAR-4B-Chat + DDO + random SDAR-4B-Chat + DDO + Lpath SDAR-4B-Chat + DDO + Lpath + random (Ours) 49.60 54.44 23.60 52.60 49.40 46.60 47.20 52.22 59.60 60.60 58.60 Table 7. Ablation study on few-step generation under block size (BS) 8 and 2 decoding steps (DS) per block. Decoding Models MATH500 ACC BS=8, DS=2 SDAR-4B-Chat SDAR-4B-Chat + SFT SDAR-4B-Chat + ReDi SDAR-4B-Chat + Naive TD SDAR-4B-Chat + Naive TD + Lpath SDAR-4B-Chat + Naive TD + random SDAR-4B-Chat + Naive TD + Lpath + random SDAR-4B-Chat + DDO SDAR-4B-Chat + DDO + random SDAR-4B-Chat + DDO + Lpath SDAR-4B-Chat + DDO + Lpath + random (Ours) 16.80 40.20 20.20 38.80 37.20 28.80 35.00 36.40 43.20 41.00 45.00 random initialization alone recovers large portion of the original performance, suggesting that it helps prevent the student model from overfitting to degenerate few-step trajectories. Furthermore, incorporating the proposed path consistency loss Lpath consistently improves robustness. When combined with random initialization, Lpath restoresand in some cases surpassesthe original full-step diffusion performance, with DDO + Lpath + random achieving the best result (69.00 ACC). Overall, these results demonstrate that both random initialization and path-level regularization are critical for preserving the intrinsic diffusion properties of the model under aggressive few-step distillation. D.3. Ablation Study on Few-Step Generation In this section, we investigate how different components of the distillation objective affect few-step generation performance. In Sec. 5, we showed that T3D effectively improves model performance under few-step generation. Here, we decompose our method and systematically compare individual components against representative baselines to evaluate their contributions to few-step generation. Experimental Settings. We consider decoding with block size 8 under two regimes: moderate setting with 4 steps per block and more aggressive setting with 2 steps per block. We compare representative baselines including SFT and ReDi, as well as naive trajectory distillation (Naive TD) and our proposed DDO objective. For both Naive TD and DDO, we further ablate the effects of random initialization and the proposed path consistency loss Lpath. All experiments are conducted on the SDAR-4B-Chat model and evaluated on the MATH500 benchmark. Results and Analysis. Table 6 and Table 7 summarize the results under block size 8 with 4 and 2 decoding steps, respectively. We first observe that few-step generation exhibits behavior that is qualitatively different from full-step diffusion decoding. Naive trajectory distillation already improves over the pretrained baseline in several settings, but its performance is sensitive to both initialization and regularization. In particular, random initialization consistently degrades few-step generation performance for Naive TD but improves Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization performance for DDO, highlighting its importance for stabilizing mode-seeking optimization such as DDO. similar phenomenon has been observed in prior work (Zhu et al., 2025). The effect of the path consistency loss Lpath is more regime-dependent. Under moderate compression (block size = 8, decoding steps = 4), incorporating Lpath further improves DDO and yields the best overall performance. Under more aggressive compression (block size = 8, decoding step = 2), path regularization mainly improves robustness when combined with random initialization, suggesting complementary interaction between the two components. Notably, DDO consistently outperforms Naive TD when equipped with appropriate initialization and regularization. Across both decoding regimes, the strongest results are achieved by combining discriminative trajectory optimization, random initialization, and path consistency regularization, highlighting the importance of all three components for effective few-step generation."
        }
    ],
    "affiliations": [
        "Amazon",
        "Department of Computer Science",
        "MIT-IBM Watson AI Lab",
        "Red Hat AI",
        "Rutgers University"
    ]
}