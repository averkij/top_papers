{
    "paper_title": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models",
    "authors": [
        "Sunghee Jung",
        "Donghun Lee",
        "Shinbok Lee",
        "Gaeun Seo",
        "Daniel Lee",
        "Byeongil Ko",
        "Junrae Cho",
        "Kihyun Kim",
        "Eunggyun Kim",
        "Myeongcheol Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, a novel method that enhances TA-LLM's dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as a Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce a specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4o's performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling."
        },
        {
            "title": "Start",
            "content": "DiaTool-DPO: Multi-Turn Direct Preference Optimization for Tool-Augmented Large Language Models Sunghee Jung, Donghun Lee, Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Junrae Cho, Kihyun Kim, Eunggyun Kim, and Myeongcheol Shin Kakao Corp. Seongnam-si, Gyeonggi-do, South Korea kong.2024@kakaocorp.com"
        },
        {
            "title": "Abstract",
            "content": "Tool-Augmented Larage Language Models (TA-LLMs) have shown promise in real-world applications, but face challenges in handling incomplete queries and out-of-scope requests. While existing approaches rely mainly on Supervised Fine-Tuning with expert trajectories, we propose DiaTool-DPO, novel method that enhances TA-LLMs dialogue capabilities through Direct Preference Optimization. We model TA-LLM interactions as Markov Decision Process with 5 distinct dialogue states and categorize user queries into 3 types based on their state transition trajectories. We automatically construct paired trajectory datasets of correct and incorrect dialogue flows and introduce specialized objective loss for dialogue control. Our comprehensive evaluation demonstrates that DiaTool-DPO approaches GPT-4os performance (94.8% in information gathering, 91% in tool call rejection) with substantial improvements over baseline (44% and 9.6% respectively) while maintaining core functionality. Our approach opens new possibilities for developing TA-LLMs that can handle diverse real-world scenarios without requiring additional expert demonstrations or human labeling."
        },
        {
            "title": "Introduction",
            "content": "The conversational capabilities of Tool-Augmented Large Language Models (TA-LLMs) are crucial. Specifically, they must be able to control conversation flow by determining whether to 1) ask followup questions for additional information or 2) make tool call or 3) reject tool calls when no suitable tools(functions, used interchangeably) are available. In particular, failing to generate follow-up questions or reject tool calls leads to the risk of invoking tool calls with hallucinated information (Huang et al., 2024; Yang et al., 2024). While early benchmarks focused on successful tool calls through proper tool selection and argument extraction (Zhang et al., 2024; Ye et al., 2024; Farn and Shin, 2023; Yan et al., 2024), recent benchmarks increasingly evaluate the ability to engage in user conversations (Huang et al., 2024; Yang et al., 2024; Lee et al., 2024). However, despite this growing emphasis in benchmarks, research addressing these challenges remains scarce. Current TA-LLMs predominantly employ Supervised Fine-Tuning (SFT) techniques to learn tool-calling capabilities from expert trajectories (Qin et al., 2024; Patil et al., 2024; Tang et al., 2023). While research applying techniques beyond SFT for training TA-LLMs is limited, many other areas of LLM research successfully utilize reinforcement learning to capture subtle human preferences (Kaufmann et al., 2023; Nguyen et al., 2024). Notably, many works solved agent tasks such as WebShop, ALFWorld, and ScienceWorld with reinforcement learning techniques due to their explicitly defined rewards in the datasets. (Yao et al., 2022; Shridhar et al., 2021; Wang et al., 2022; Chen et al., 2024; Qiao et al., 2024; Song et al., 2024; Shi et al., 2024) Here, we formulate TA-LLM task as Markov Decision Process(MDP) and adapt reinforcement learning techniques from agent tasks to enhance TA-LLMs conversational abilities. We define five internal states for TA-LLMs and classify user queries into three types based on state transition trajectories (Bellman, 1957; Howard, 1960; Puterman, 1994; Sutton and Barto, 1998). We automatically generate rejected trajectories by pairing user queries with mismatched conversation trajectories. The resulting dataset consists of paired trajectories (τc, τr), where τc represents the chosen (preferred) trajectory and τr denotes the rejected (non-preferred) trajectory for each training instance. While our primary experiments were conducted in Korean, we demonstrate that our approach is language-agnostic and can be applied to English as well. This dataset is termed the DiaTool-DPO (Dialogue Tool DPO) dataset, with its corresponding training algorithm named DiaTool-DPO. To com5 2 0 2 2 ] . [ 1 2 8 8 2 0 . 4 0 5 2 : r prehensively evaluate the impact of this dataset and algorithm, we assess not only slot-filling and tool call rejection capabilities but also all other general competencies across various base LLM models. In this paper, we leverage Direct Preference Optimzation(DPO) techniques to improve conversational abilities by controlling TA-LLMs dialogue flow (Rafailov et al., 2023a). Specifically, the contributions of this paper are as follows: Dataset Construction: We construct the DiaTool-DPO (Dialogue Tool DPO) dataset that enables TA-LLMs to learn which conversation flow to choose in specific situations. Novel Alignment Objective: We propose specialized alignment objective for TA-LLMs that controls conversation flow through the contrast between chosen and rejected trajectories. Evaluation: We comprehensively assessed how the proposed algorithm not only improves the models ability to judge conversation flow, but also affects its ability to make accurate tool calls and generate precise tool call completion messages."
        },
        {
            "title": "2 Related Works",
            "content": "Benchmarks for TA-LLMs Early benchmarks (T-Eval (Zhang et al., 2024), ToolEyes (Ye et al., 2024), BFCL (Yan et al., 2024)) focused on tool call evaluation. Recent ones emphasize conversational abilities, including tool awareness (Huang et al., 2024), query disambiguation (Yang et al., 2024), and multi-turn dialogue (BFCL v3 (Yan et al., 2024), API-Bank (Li et al., 2023), ToolSandbox (Lu et al., 2024)). FunctionChat-Bench (Lee et al., 2024) provides comprehensive evaluation framework for tool-related capabilities. Reinforcement Learning for Agent Tasks Recent work has explored reinforcement learning for improving LLM-based agents. Chen et al. (2024) addressed disambiguation using quasi-online DPO with user simulation (Rafailov et al., 2023b). Qiao et al. (2024) enhanced tool usage through rankingbased learning that considers response-ground truth distances. Song et al. (2024) combined SFT with offline exploration, creating DPO datasets from expert and failure trajectories. Shi et al. (2024) introduced Direct Multi-turn Preference Optimization (DMPO) for language agents,using state-action occupancy measure constraints and trajectory length normalization to improve multi-turn alignment learning."
        },
        {
            "title": "3.1 Definition of TA-LLM Internal States",
            "content": "As shown in Figure 1, we defined five states for TALLMs. The traversed states and their trajectories vary depending on the query type, as not all states are necessarily visited in every interaction. The system falls into one of the following states in its tool execution lifecycle: Initial State state with no history. If the users request cannot be supported by any tool in the available tool list, the system returns tool call rejection message and remains in this state. Tool Selected without Complete Slots state where the dialogue history has provided information about which tool to select, but not all required parameters for tool invocation. Through slot-filling QA interactions, the system can transition to the next stage. If multiple required fields need to be determined, this stage may be repeated. Tool Selected with Complete Slots state where the dialogue history has provided both the tool selection and all necessary argument values. The system can transition to this state either directly from the Initial State or through the Tool Selected without Complete Slots state. Wait for Tool Response state where the tool call has been executed and the system is awaiting execution results from the tool. Complete state reached after receiving the tools execution results, prior to generating completion message containing the tool execution outcome for the user."
        },
        {
            "title": "3.2 Definition of TA-LLM Query Types",
            "content": "We classified queries into three types based on their state transition trajectories: Type 1: Queries that contain all necessary information for tool calls in the initial user query, enabling immediate tool calls without slot-filling. 1http://www.flaticon.com Figure 1: Visualization of five internal states of TA-LLMs and state trajectories for three different query types. User queries are shown in green message bubbles, while other conversational turns are displayed in blue. OOT (Out-of-Tools) queries represent requests for functionality not available in the teal-colored \"Tools\" list. Slot-Filling QAs denote conversational turns aimed at gathering required fields for tool execution. Tool calls represent messages where the assistant invokes tool, tool responses show the returned execution results, and completion messages demonstrate the assistants final response using the tool output. For optimal visualization of the state transitions and message types, we recommend viewing this figure in color. Icons from Flaticon1are used in this diagram. Type 2: Queries that lack argument information for tool calls in the initial user query, requiring slot-filling before making tool calls. Type 3: Queries where the requested functionality is not available in the TA-LLMs capability list (referring to the tools shown in Figure 1), necessitating rejection of the tool call."
        },
        {
            "title": "3.3 Formal Definitions",
            "content": "A Markov Decision Process (MDP) is formally defined as 5-tuple (S, A, P, R, γ), where is finite set of states, represents the set of possible actions, : [0, 1] is the state transition probability function, : denotes the reward function, and γ [0, 1] is the discount factor. In our setting, πref is reference model obtained from SFT and πθ is the model under training with alignment loss. \"turn\" refers to one exchange between the user and assistant, and for each query type, there are chosen and rejected trajectories. The chosen trajectory is represented as τ = {sc }, and the rejected trajectory is represented as τ = {sr , ar represent Tr the users utterances at dialogue turn in the chosen 1, ..., sr Tr 1, ..., sc Tc , ac Tc 0, ar 1, ar 1, ac 0, ac }. sc 0, sr 0, sc , sr , ar 0 == sr or rejected trajectory. This also includes the tool response from executing call. ac represent the TA-LLMs responses at dialogue turn in the chosen or rejected trajectory. sc 0 contains two components: 1) the users initial request corresponding to Type 1, 2, or 3, and 2) the list of tools available to the TA-LLM. sc , where = 0 contain either 1) user responses to slot-filling questions or 2) tool responses. ac is one of: slot-filling question, tool call, or completion message. Tc, Tr refer to the number of turns in chosen trajectory and rejected trajectory, respectively. From now on, we will call these the chosen trajectory length and rejected trajectory length. Tc = Tr holds true in our setting. , ar , sr"
        },
        {
            "title": "3.4 Evaluation",
            "content": "Our primary focus in this paper is the conversational ability of TA-LLMs, specifically concerning slot-filling and relevance aspects. However, we also evaluate metrics such as call and completion to ensure that enhancing these abilities does not compromise other fundamental performance indicators. We adopted the open-source FunctionChat-Bench (Lee et al., 2024) because this benchmark best suits Query Chosen traj. Type 1 1345 12345 1(2*N)345 Type Type 3 11 Rejected traj. 12345 1(2*N)345 1(2*M)345 11 1345 1345 1(2*M)345 11 134 1(2*N)34 Learning lesson Count Included Prevent redundant slot-filling Prevent redundant slot-filling Prevent redundant slot-filling Tool call accept Prevent slot hallucination Prevent slot hallucination Prevent slot hallucination Tool call accept 2,089 Easy 562 Hard 2,530 Hard 2,090/562 Easy, Hard 2,089 Easy 562 Hard 2,530 Hard 2,089/562 Easy, Hard Tool call reject Tool call reject 567 Hard 562 Hard Table 1: Training data composition and corresponding learning objectives. Each query type is defined in Section 3.2. In the trajectory notation, (2 ) denotes that state 2 is visited consecutive times (e.g., 2 2 2 for = 3), where represents the number of total unknown required fields. denotes the size of subset of total unknown required fields (N > > 1). In the Count column, when both difficulty levels are included, values are presented as \"number of Easy examples/number of Hard examples\" our evaluation requirements. Details of evaluated metrics are available at Appendix A. Difficulty Trajectory Slot Relevance Easy Hard All Chosen Rejected Chosen Rejected Chosen Rejected 3.05 2.00 4.83 3. 4.11 2.80 N/A N/A 1.00 2.33 1.00 2.33 Table 2: Average number of turns for chosen and rejected trajectories across different difficulty levels. Note that relevance metrics are only applicable for Hard and All difficulty samples."
        },
        {
            "title": "4.1 Seed Trajectory Construction",
            "content": "We will create the DiaTool-DPO dataset by pairing chosen trajectories with rejected trajectories as shown in Table 1. Before this, we first generate seed trajectory dataset (chosen trajectory dataset) that matches query Types 1, 2, and 3. Using the glaive-function-calling-v2 dataset (glaiveai, 2023), commonly known as glaive 2.0, we created the seed trajectory dataset. While each data point in the glaive 2.0 dataset corresponds to either Type 1 or Type 3, we need to construct matching chosen and rejected pairs as in Table 1. Therefore, we sample Type 1 dialogues and augment them to create corresponding Type 2 and Type 3 trajectories. We first sampled portion of Type 1 queries from the glaive 2.0 based on difficulty levels. These queries were then modified into incomplete queries requiring slot-filling through ChatGPT (OpenAI, 2023) prompting, and augmented with subsequent slotfilling conversations to create matching Type 2 trajectories (See Appendix for the detailed prompt). We then generated corresponding Type 3 data by removing the appropriate tool from the tools list in both Type 1 and Type 2 samples. We constructed the DiaTool-DPO dataset by matching pairs from these Type 1, Type 2, and Type 3 trajectory triplets. The dataset composition follows three distinct approaches based on difficulty levels, which we detail in Section 4.3."
        },
        {
            "title": "4.2 Dataset Structure",
            "content": "The DPO dataset consists of three fields: prompt, chosen, and rejected. The prompt is the users query, while chosen and rejected are the desirable/undesirable answers to that query. The DiaTool-DPO dataset is similar to the DPO dataset, but its distinctive feature is that it lacks prompt field. The absence of prompt is because while DPO deals with single-turn situations where chosen and rejected share the user query and only differ in answers, DiaTool-DPO handles multi-turn situations where user turns also differ between chosen and rejected at every turn (except for the initial query). Therefore, instead of sharing prompt, corresponding user turns are included in chosen and rejected trajectories. However, during training, user turns are masked to exclude them from the loss calculation."
        },
        {
            "title": "Difficulty Levels",
            "content": "We stratified our dataset by difficulty, creating two subsets: Easy and Hard. The aggregation of these subsets constitutes our complete dataset, denoted as All throughout this paper. All experimental evaluations were conducted using the All dataset unless explicitly specified otherwise. Specifically, Easy comprises 8,357 samples and Hard contains 8,437 samples, collectively forming the All dataset with total of 16,794 instances. Easy In the Easy dataset, slot-filling questions occur at most once per dialogue. As shown in Table 2, chosen trajectories with slot-filling average 3 turns, while rejected trajectories average 2 turns, consisting only of tool call and tool completion without the necessary slot-filling QA. Notably, as shown in Tables 1 and 2, the Easy dataset excludes Type 3. However, as demonstrated in Type 1 and Type 2 entries of Table 1, we incorporated 11 as rejected trajectories, enabling indirect learning about tool call rejection through tool call accept Hard The Hard dataset extends the Easy dataset by introducing complex slot-filling scenarios and tool call rejection cases. It features Type 1 trajectories with three or more required fields and Type 2 trajectories where users provide partial information incrementally. For Type 3, we created scenarios requiring tool call rejection by modifying the available tool list. In Table 2, while chosen trajectories in relevance data average 1.0 turns due to immediate rejection, rejected trajectories average 2.3 turns. In summary, the Hard dataset requires more sophisticated handling of slot-filling interactions and demonstrates higher average turn counts compared to the Easy dataset."
        },
        {
            "title": "5 Objective Loss",
            "content": "Lalign = E(s0,τ c,τ r)D log σ (cid:104) Tc1 (cid:88) t=0 β ϕ(t, Tc) ψ(Tc) log πθ(ac πref (ac sc ) sc ) Tr1 (cid:88) t= β ϕ(t, Tr) ψ(Tr) log πθ(ar πref (ar sr ) sr ) (cid:105) ρ where, ψ(T ) = 1 (cid:88) t=0 1 γT 1 γT , ϕ(t, ) = 1 γT 1 γT ρ = arbitrary margin Equation 1 presents the proposed objective loss for learning DiaTool-DPO. Notations are as defined in Section 3.3. DMPO As mentioned in Section 2, Shi et al. (2024) proposed DMPO to address agent tasks such as WebShop, ScienceWorld, and ALFWorld (Yao et al., 2022; Wang et al., 2022; Shridhar et al., 2021). They suggested integrating turn-length normalization term ϕ in DPO loss to cancel out the partition function from the Bradley-Terry equation under the condition where Tc = Tr Total Turn-Length Normalization Building upon DMPO, we modified the objective loss to better suit TA-LLMs rather than agents. The key difference between our tasks and agent tasks lies in the consistent disparity in turn counts between chosen and rejected trajectories. To be specific, in agent tasks, either chosen or rejected trajectories can have more turns. However, in DiaTool-DPO data for slot-filling, rejected trajectories always have fewer turns due to partial or complete omission of slot-filling QAs, making chosen trajectories consistently longer. Conversely, in DiaTool-DPO data for relevance, chosen trajectories complete with tool call rejection in one turn, while rejected trajectories continue dialogue for tool calls, making rejected trajectories consistently longer. As result of such turn-length imbalance, during slot-filling training, theres an inherent bias towards larger chosen rewards, while during relevance training, theres bias towards larger rejected rewards. Such weight bias based on turn count is undesirable, and its impact becomes more significant when the total number of turns is small. Due to this bias, slot-filling is deemed as an easy task by the model, resulting in less effective learning, while relevance learning occurs more actively as the gap between chosen and rejected rewards is smaller. Based on this observation, we propose ψ, the sum of weights attributed to turn counts, as normalization term, where ϕ ψ becomes the weight for rewards at each turn. Reward Gap Margin Subtraction Since we hypothesized that relevance learning occurs more actively due to the structural bias causing smaller differences between chosen and rejected rewards, we can leverage this by subtracting margin ρ from the reward gap. This approach is expected to pro- (1) mote more active learning in both slot-filling and relevance tasks. The effectiveness of subtracting margin has been previously demonstrated in the SimPO (Meng et al., 2024)."
        },
        {
            "title": "6.1 Experimenetal Setup",
            "content": "Unless otherwise specified, we used LLaMA3-8BInstruct as the baseline model (Van Der Maaten et al., 2024). All models undergo two sequential training phases prior to DiaTool-DPO: continual pretraining(CPT) and SFT. Details of CPT, SFT and training setup are available at Appendix and D. The DiaTool-DPO was trained on 95% of the dataset, with the remaining 5% reserved for validation. To rigorously evaluate the models generalization performance, we conducted testing on the independent FunctionChat-Bench dataset, which remained completely isolated from both training and validation processes (Lee et al., 2024)."
        },
        {
            "title": "6.2 Ablation Study",
            "content": "Table 3 demonstrates the impact of each DiaToolDPO component on various performance metrics. All scores are normalized to maximum of 1.0. Since DiaTool-DPO datasets contain expert trajectories in the form of chosen trajectories, we needed to validate whether performance improvements stem from the DiaTool-DPO itself rather than mere increased exposure to expert trajectories. To this end, we conducted an \"SFT w/ preferred responses\" experiment, which applies further SFT to the \"SFT-only\" model using chosen trajectories extracted from the DiaTool-DPO dataset. Results show improvements in slot and relevance scores but significant 45% drop in call performance. We hypothesize that while SFT exposes the model to glaive 2.0-augmented expert trajectories with slotfilling QA, it fails to contrastively learn contextual differences between performing slot-filling QAs and making tool calls. \"DiaTool-DPO w/o {ϕ, ψ and ρ}\" showed similar results to \"SFT w/ preferred responses\" but notably avoided the drops in call and completion metrics. This preservation of performance can be attributed to {Type 1 - Type 2} and {Type 1 - Type 3} trajectories in Table 1, preventing inappropriate slot-filling or LLM dialogue in place of call situations. The comparison between these two approaches demonstrates that DPO-based methodology effectively learns context-dependent dialogue flow control. Subsequent experiments examined the contribution of individual components within the DiaToolDPO. Contrary to the findings of DMPO (Shi et al., 2024), comparing \"DiaTool-DPO w/o {ϕ, ψ and ρ}\" and \"DiaTool-DPO w/o {ψ,ρ}\" revealed minimal impact of reward scaling on evaluation metrics. We attribute this discrepancy to different evaluation approaches between agent tasks and TA-LLM tasks : Agent tasks accumulate errors through trajectories, making initial turns more important than the later turns, while most TA-LLM benchmarks, including FunctionChat-Bench, employ teacherforcing and evaluate each turn independently and thus mitigating error accumulation. Adding normalization ψ improved slot scores while maintaining relevance scores. As shown in Table 2, slot-filling training data consistently features longer chosen trajectory turns compared to rejected ones. Without total turn-length normalization, this creates artificially inflated reward gaps, potentially hampering effective parameter updates. Conversely, relevance-learning trajectory pairs have longer rejected trajectory turns, explaining the lack of performance gain from the normalization. The introduction of reward gap margin ρ, observed when comparing \"DiaTool-DPO w/o {ψ,ρ}\", and \"DiaTool-DPO w/o ψ\", resulted in 4.5% increase in relevance performance while maintaining slot scores. This aligns with previous research (Meng et al., 2024) showing benefits of reward gap margin subtraction. The different impact on slot versus relevance metrics likely stems from the dominance of aforementioned turn-length bias in slot tasks. Our complete DiaTool-DPO approach achieved 44% and 9.6% improvements over the SFT-only baseline. The final model reaches 94.8% of the slot performance of GPT-4o. It also achieves 123.5% of the relevance score of GPT-4o-mini and 91.3% of the relevance score of GPT-4o. Notably, call and completion metrics remained comparatively stable throughout ablation studies, confirming that DiaTool-DPO implementation does not compromise tool call capabilities or completion message generation."
        },
        {
            "title": "Performance",
            "content": "Table 4 compares performance across evaluation metrics for different levels of data difficulty. While Description Call Completion Slot Relevance Micro Avg. Macro Avg. SFT-only SFT w/ preferred responses DiaTool-DPO w/o {ϕ, ψ, ρ} DiaTool-DPO w/o {ψ ρ} DiaTool-DPO w/o ρ DiaTool-DPO w/o ψ DiaTool-DPO (Ours) GPT-4o-mini-2024-07-18 GPT-4o-2024-08-06 0.843 0.457 0.857 0.857 0.843 0.886 0.857 0.929 0.914 0.957 0.900 0.943 0.943 0.929 0.957 0.929 0.971 0. 0.639 0.806 0.806 0.778 0.833 0.778 0.917 0.972 0.972 0.826 0.913 0.870 0.870 0.870 0.913 0.913 0.739 1.000 0.844 0.725 0.879 0.874 0.874 0.894 0.905 0.920 0. 0.816 0.769 0.869 0.862 0.869 0.883 0.904 0.903 0.953 Table 3: Ablation study results of DiaTool-DPO comparing different model variants. SFT-only represents the model before DPO training, and SFT w/ preferred responses indicates training with only chosen responses from the DPO dataset. We systematically remove key components (ϕ: reward scaling, ψ: total turn-length normalization, ρ: reward gap threshold) from our full model to analyze their individual contributions. GPT-4 models are included as reference points. Dataset Difficulty Call Completion Slot relevance Micro Avg. Macro Avg. Baseline Easy Hard All 0.843 0.871 0.871 0. 0.957 0.943 0.957 0.929 0.639 0.778 0.778 0.917 0.826 0.913 0.913 0. 0.844 0.850 0.840 0.905 0.816 0.876 0.880 0.904 Table 4: Model performance across different training dataset configurations (Baseline, Easy-only, Hard-only, and All). Evaluation metrics include tool call accuracy, completion rate, slot-filling accuracy, and relevance scores, all reported on scale of 0 to 1. DiaTool-DPO training on both Easy and Hard datasets showed significant improvements over the baseline (SFT-only) model in call, slot, micro average, and macro average metrics, the completion scores remained largely unchanged. This stability in completion scores is reasonable given that this metric primarily evaluates tool call response paraphrasing ability, which is not primary focus of our dialogue enhancement objectives. Training separately on Easy and Hard datasets yielded comparable results without clear superiority of either dataset. However, the All dataset, which combines both difficulty levels, demonstrated marked improvements in slot, micro average, and macro average metrics compared to individual training on either Easy or Hard datasets, while maintaining consistent relevance scores. This suggests that slot-filling ability benefits from exposure to spectrum of difficulty levels rather than exclusively training on either simple or complex examples. The consistency in relevance scores across Easy, Hard, and All datasets can be attributed to indirect the models understanding of learning transfer: when to accept tool calls (learned from Type 1 and Type 2 in Easy data) appears to contribute to its ability to identify situations requiring tool call rejection. This finding suggests that the TA-LLM develops an integrated understanding of tool call acceptance and rejection criteria. Nevertheless, the generalizability of this observed phenomenon, wherein training for tool call acceptance inadvertently affects tool call rejection patterns across different TA-LLM architectures, remains to be investigated in Section 6.4."
        },
        {
            "title": "Different Base Models",
            "content": "While some alignment techniques omit SFT (Hong et al., 2024), it typically precedes alignment training in most of the cases (Stiennon et al., 2020; Xu et al., 2024; Ethayarajh et al., 2024; Ahmadian et al., 2024). As shown in Table 5, we conducted experiments to determine whether SFT is essential before applying DiaTool-DPO. Since open-source models cannot effectively process Korean without SFT, making evaluation with FunctionChatBench challenging, we conducted these experiments on proprietary 8B and 3.1B models trained from scratch with Korean language capabilities. 8B-Sized Model. Without preceding SFT, DiaTool-DPO-only training showed significant performance degradation across all metrics except for slot-filling. This suggests that SFT must Model Method Call Completion Slot relevance Micro Avg. Macro Avg. Prop.-8B Prop.-3.1B LLaMA-3-8B 0.314 DiaTool-DPO-only 0.900 SFT-only SFT + DiaTool-DPO 0. 0.357 DiaTool-DPO-only 0.771 SFT-only SFT + DiaTool-DPO 0.743 0.029 DiaTool-DPO-only SFT-only 0.843 SFT + DiaTool-DPO 0.857 0.700 0.916 0.929 0.551 0.817 0.871 0.449 0.957 0.929 0.833 0.694 0. 0.528 0.750 0.833 0.056 0.639 0.917 0.609 0.913 0.826 0.391 0.826 0.826 0.261 0.826 0.913 0.575 0.870 0. 0.455 0.790 0.765 0.205 0.844 0.905 0.614 0.856 0.868 0.457 0.791 0.818 0.199 0.816 0.904 Table 5: Performance comparison across different base models and training methods. Results show the impact of DiaTool-DPO training on Proprietary-8B-Instruct, Proprietary-3.1B-Instruct, and Meta-LLaMA-8B-Instruct models. For each model, we compare DiaTool-DPO-only, SFT-only, and SFT followed by DiaTool-DPO. precede DiaTool-DPO to establish basic TA-LLM tool-calling capabilities. The 8B-sized model with SFT + DiaTool-DPO showed 17% improvement in slot performance compared to the SFT-only model, but exhibited 10% decrease in relevance In our ablation study in Section 6.2, scores. we observed consistent relevance scores across Easy, Hard, and All datasets, despite Easy data lacking Type 3 data specifically designed for tool call rejection. This led us to question whether indirect learning of tool call rejection from tool call acceptance data was universal across models. The decreased relevance performance of SFT + DiaTool-DPO compared to SFT-only models answers this question, indicating that the ability to indirectly learn tool call rejection is not inherent to all models. As shown in Table 1, Type 3 (tool call rejection) data comprises mere 14% compared to Type 1 (tool call) or Type 2 (slot-filling) data. Without indirect learning from tool call acceptance data in Type 1 and Type 2, learning tool call rejection becomes challenging, likely resulting in catastrophic forgetting of relevance capabilities compared to the SFT-only model. 3.1B-Sized Model. The 3.1B-sized model also demonstrated consistently poor performance across all metrics when trained with DiaTool-DPO without preceding SFT, underscoring the necessity of SFT. However, unlike the 8B-sized model, the 3.1B model showed significant improvements across all performance metrics, including relevance, when comparing SFT-only versus SFT followed by DiaTool-DPO. This suggests that while the 3.1B model may not have fully grasped tool call rejection contexts during SFT, it successfully learned these concepts through Type 3 data in DiaToolDPO training, demonstrating different learning pattern from its 8B counterpart."
        },
        {
            "title": "6.5 Hyperparameter Analysis",
            "content": "Effects of hyperparameters β, γ and ρ are analyzed in Appendix E."
        },
        {
            "title": "6.6 Extensibility to Other Languages",
            "content": "While this study was primarily conducted in Korean and evaluated using Korean benchmarks, we created an English dataset corresponding to the Easy subset to verify that our approach is applicable to English as well. Although we couldnt perform quantitative evaluation due to the lack of exactly matching English benchmarks, we have included qualitative examples. These examples are appended in Appendix F. Also, detailed analysis in Appendix illustrates how DiaTool-DPO effectively prevents three distinct types of errors commonly observed in SFT-Only baselines."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced DiaTool-DPO, novel method for enhancing TA-LLMs conversational capabilities through DPO. Our approach effectively addresses key challenges in dialogue management by formulating the task as Markov Decision Process with five distinct states and suggesting noble objective loss that is specified for TA-LLMs. Comprehensive evaluation shows that DiaTool-DPO significantly improves both slot-filling and tool call rejection capabilities while maintaining core functionalities, achieving 94.8% of GPT-4s slot-filling performance and 91% of its tool call rejection accuracy. Our analysis demonstrates the benefits of exposure to varied difficulty levels and provides insights into indirect learning transfer between tool call acceptance and tool call reject across different model architectures."
        },
        {
            "title": "Limitations",
            "content": "While we utilized FunctionChat-Bench (Lee et al., 2024) as it appropriately evaluates slot-filling abilities, relevance, and tool call capabilities, its limitation to Korean language prevented quantitative evaluation in other languages. However, we demonstrated qualitative extensibility to English through Section 6.6. For future work, we plan to extend our experiments to more suitable benchmarks in other languages as they become available. To clearly define our problem scope, we did not address scenarios involving multiple simultaneous tool calls such as planning, parallel tool calls, or sequential tool calls. However, our approach can potentially be applied to any task that requires tuning TA-LLMs according to subtle user preferences. Thus, we leave applying this algorithm to more complex problems, including planning and canonical representation (Lu et al., 2024), for future work. For another limitation, the experimental figures in this paper were obtained through single run."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. 2024. Back to basics: Revisiting REINFORCE-style optimization for learning from human feedback in LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1224812267, Bangkok, Thailand. Association for Computational Linguistics. Richard Bellman. 1957. markovian decision process. Journal of Mathematics and Mechanics, 6(5):679 684. Maximillian Chen, Ruoxi Sun, Sercan O. Arık, and Tomas Pfister. 2024. Learning to clarify: Multiturn conversations with action-based contrastive selftraining. arXiv preprint arXiv:2406.00222. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization. In Proceedings of the 41st International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR. Nicholas Farn and Richard Shin. 2023. Tooltalk: Benchmarking tool-augmented llms in conversational ai. arXiv preprint arXiv:2311.10775. glaiveai. 2023. glaive-function-calling-v2. https://huggingface.co/datasets/glaiveai/ glaive-function-calling-v2. Accessed: 2024-11-17. Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Ronald Howard. 1960. Dynamic Programming and Markov Processes. The M.I.T. Press, Cambridge, MA. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. 2024. MetaTool benchmark for large language models: Deciding whether to use tools and which to use. In Proceedings of the 2024 International Conference on Learning Representations, Vienna, Austria. ICLR. To appear in ICLR 2024. Timo Kaufmann, Sebastian Kauschke, and Volker Roth. 2023. survey of reinforcement learning from human feedback. arXiv preprint arXiv:2312.14925. Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Sunghee Jung, and Shin Myeongcheol. 2024. Functionchat-bench: Comprehensive evaluation of language models generative capabilities in korean tool-use dialogs. arXiv preprint arXiv:2411.14054. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-bank: comprehensive benchmark for tool-augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 31023116, Singapore. Association for Computational Linguistics. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, et al. 2024. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. arXiv preprint arXiv:2406.18518. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming Pang. 2024. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682. MeetKai. 2024. Functionary. https://github.com/ MeetKai/functionary. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with In Advances in Neural Inreference-free reward. formation Processing Systems. Thong Nguyen, Luu Anh Nguyen, Trung Nguyen, and Khoat Than Nguyen. 2024. Reinforcement learning from answer reranking feedback for retrievalIn Proceedings of augmented answer generation. INTERSPEECH 2024. OpenAI. 2023. ChatGPT. https://chat.openai. com/. Large language model. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2024. Gorilla: Large language model connected with massive apis. In Advances in Neural Information Processing Systems. Martin Puterman. 1994. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, New York. Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. 2024. Making language models better tool learners with execution feedback. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35503568, Mexico City, Mexico. Association for Computational Linguistics. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Toolllm: Facilitating large language models to master 16000+ In International Conference on real-world apis. Learning Representations. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023a. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. 2023b. Direct preference optimization: Your lanIn Adguage model is secretly reward model. vances in Neural Information Processing Systems, volume 36, New Orleans, Louisiana, USA. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI Conference on Artificial Intelligence. Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, and Fuli Feng. 2024. Direct multi-turn preference optimization for language agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, St. Julians, Malta. Association for Computational Linguistics. To appear. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations. Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 75847600, Bangkok, Thailand. Association for Computational Linguistics. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. LearnIn Ading to summarize from human feedback. vances in Neural Information Processing Systems, volume 33, pages 30083021. Richard Sutton and Andrew Barto. 1998. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA. Qiaoyu Tang, Ziliang Chen, Binyuan Li, Zhiyi Huang, Yue Feng, Chaojun Xiao, Xiaozhi Chen, Jifan Liu, Dongyan Zhao, and Rui Yan. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301. Laurens Van Der Maaten et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. SCIENCEWORLD: Is your agent smarter than 5th grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1127911298, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Haoran Xu, Tianxing Zhang, Yunmo Xu, Cheng Xu, Jiatao Gu, and Caiming Xiong. 2024. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 5520455224. PMLR. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley function calling leaderboard. https://gorilla.cs.berkeley. edu/leaderboard.html. Accessed: November 11, 2024. Seungbin Yang, ChaeHun Park, Taehee Kim, and Jaegul Choo. 2024. Can tool-augmented large language models be aware of incomplete conditions? arXiv preprint arXiv:2406.12307. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. In Advances in Neural Information Processing Systems, volume 35, pages 2393723954, New Orleans, Louisiana, USA. Neural Information Processing Systems Foundation. Junjie Ye, Xuanyu Gao, Yiwen Feng, Yicheng Xu, Yiming Huang, Nan Xu, Dongyan Zhao, and Rui Jiang. 2024. ToolEyes: Fine-grained evaluation for tool learning capabilities of large language models in realworld scenarios. arXiv preprint arXiv:2401.00741. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Tao Shen, Zhuosheng Zhang, Rui Wang, Wei Bi, Shuming Shi, and Dongyan Zhao. 2024. T-eval: Evaluating the tool utilization capability of large language models. In Proceedings of the 2024 Annual Conference of the Association for Computational Linguistics, pages 9164 9184, Bangkok, Thailand. Association for Computational Linguistics. To leverage widely available open-source data, we utilize English datasets for CPT, while the SFT phase uses Korean-translated datasets. The details of dataset usage for each phase are available in Table 12. schema_guided_dstc8 refers to SchemaGuided Dialogue dataset which include interactions with services and APIs spanning 20 domains, such as banking, events, media, calendar, travel, and weather (Rastogi et al., 2020). xlam-functioncalling-60k refers to the work of Liu et al. (2024) which generates this dataset through automated data generation pipline named APIGen."
        },
        {
            "title": "D Training Setup",
            "content": "For evaluation, opensource FunctionChat-Bench (Lee et al., 2024) was adopted. They evaluate accuracy for following aspects of tool calls. Call: This metric evaluates whether the correct tool was selected and called with accurate arguments Completion: This metric assesses the ability to convert tool responses into appropriate textbased answers, termed completion messages, that address the users initial query Slot: This metric evaluates whether appropriate questions were asked when the user query lacked necessary arguments for tool calls Relevance: This metric assesses whether the system can properly decline requests when the required tool for answering the user query is not available in the tools list Prompt for Type 2 Trajectory Augmentation from Type 1 Trajectory Table 6, 7 and 8 show GPT prompts for generating Type 2 trajectory by augmenting Type 1 trajectory to create Easy subset of DiaTool-TPO. Table 9, 10 and 11 show GPT prompts for generating Type 2 trajectory by augmenting Type 1 trajectory to create Hard subset of DiaTool-TPO. gpt-4-turbo2024-04-09 was used for data augmentation."
        },
        {
            "title": "C Details of CPT and SFT",
            "content": "During the CPT phase, the language model is trained using next-token prediction loss on an opensource tool call dataset. During SFT, user turns are masked, and the loss is backpropagated only through the assistant turns next-token predictions. All experiments were conducted on 8 NVIDIA A100-80GB GPUs, with training completed in approximately 2.5 hours. For training, we used the AdamW optimizer (β1 = 0.9, β2 = 0.999, ϵ = 1e 8, weight decay = 0.0) with learning rate of 1e-7 and mixed-precision training with bfloat16 to optimize memory usage and computational efficiency. We employed total batch size of 8 distributed across the GPUs. The sequences were configured with maximum total length of 8,192 tokens, allowing prompts up to 4,096 tokens. The model was trained for single epoch using linear learning rate scheduler with 150 warmup steps. Unless otherwise specified, we maintained consistent hyperparameters across experiments with β = 0.5, γ = 0.5, and ρ = 2.0. All other hyperparameters followed the default settings of the DPOConfig in the TRL library (von Werra et al., 2020). Also, for the tool conversation template, we used functionarys v3.llama3 (MeetKai, 2024). We created the DiaTool-DPO dataset based on the glaive-2.0 dataset (glaiveai, 2023), which is licensed under Apache 2.0. Additionally, we developed the DiaTool-DPO source code based on opensource TRL (von Werra et al., 2020) and Functionary (MeetKai, 2024), which are licensed under Apache 2.0 and MIT licenses respectively. Our research has properly utilized these licenses within their permitted scope."
        },
        {
            "title": "E Hyperparameter Analysis",
            "content": "Impact of β Value As depicted in Figure 2 (a), experimental results demonstrate that increasing the β parameter leads to consistent improvements in call performance, with corresponding gains in both micro and macro averages. Conversely, models trained with lower β values exhibit overfitting In the first dialogue turn of source sample, the user provides all the information to extract the required field of corresponding function. Thus, in the second dialogue turn of source sample, the assistant had all the argument information to call the calculate_tip function in tool_calls. If the user had uttered the utterance in the first turn such that there was missing information about any argument in the required fields, the conversation scenario would have been the same as the target sample. In other words, couple of more dialogue turns must have been made between the user and assistant to clarify the missing arguments to make the tool call with the required arguments filled. Note that only required fields in the function definition needs to be asked. Other arguments are optional and assistant must not ask about optional arguments. For the following input sample, 1. Modify the user utterance in the first dialogue turn so that there is not enough information about one or more arguments among the required fields. 2. Generate following assistant question and user answer utterances accordingly. 3. Your answer MUST be in strict JSON format. 4. As you can see from the example target sample, generate only messages part, not, tools part. 5. The generated conversation MUST be in Korean 6. Do not change any detail of the conversation if not mentioned otherwise ### Source sample { \"messages\": [ \"role\": \"user\", \"content\": \"Hi, need to calculate my BMI. weigh 70 kg and my height is 1.75 m.\" { }, { \"role\": \"assistant\", \"content\": \"Sure, can help you with that. Let's calculate your BMI.\", \"tool_calls\": [ { \"id\": null, \"type\": \"function\", \"function\": { \"name\": \"calculate_bmi\", \"arguments\": \"{\"weight\": 70, \"height\": 1.75}\" } } ] } Table 6: The prompt for Type 1 to Type 2 data augmentation in DiaTool-DPO Easy subset (Part 1). \"role\": \"tool\", \"content\": \"{\"bmi\": 22.86}\", \"tool_call_id\": null, \"name\": \"calculate_bmi\" { }, { \"role\": \"assistant\", \"content\": \"Your Body Mass Index (BMI) is 22.86. This is considered healthy weight for your height.\" } ], \"tools\": [ { \"type\": \"function\", \"function\": { \"name\": \"calculate_bmi\", \"description\": \"Calculate the Body Mass Index (BMI)\", \"parameters\": { \"type\": \"object\", \"required\": [\"weight\", \"height\"], \"properties\": [ { \"type\": \"number\", \"description\": \"The weight in kilograms\" }, { \"type\": \"number\", \"description\": \"The height in meters\" } ] } } } ] } Table 7: The prompt for Type 1 to Type 2 data augmentation in DiaTool-DPO Easy subset (Part 2). ### Target sample { \"messages\": [ \"role\": \"user\", \"content\": \"Hi, need to calculate my BMI. My height is 1.75 m.\" \"role\": \"assistant\", \"content\": \"How much do you weigh?\" \"role\": \"user\", \"content\": \"I weigh 70 kg.\" { }, { }, { }, { \"role\": \"assistant\", \"content\": \"Sure, can help you with that. Let's calculate your BMI.\", \"tool_calls\": [ { \"id\": null, \"type\": \"function\", \"function\": { \"name\": \"calculate_bmi\", \"arguments\": \"{\"weight\": 70, \"height\": 1.75}\" } } ] }, { \"role\": \"tool\", \"content\": \"{\"bmi\": 22.86}\", \"tool_call_id\": null, \"name\": \"calculate_bmi\" }, { } ] \"role\": \"assistant\", \"content\": \"Your Body Mass Index (BMI) is 22.86. This is considered healthy weight for your height.\" } ### Source sample Table 8: The prompt for Type 1 to Type 2 data augmentation in DiaTool-DPO Easy subset (Part 3). You are an AI tasked with transforming conversations between tool-augmented LLM assistant and user that lack slot-filling conversations (Source) into conversations that include slot-filling conversations (Target). To understand what slot-filling conversation is, you need to know the following rule for tool-augmented LLMs: \"If the user hasnt mentioned any of the required fields for corresponding function in the first turn of conversation, the assistant must ask questions to determine all required field values before calling the function. It is prohibited to call the function by arbitrarily filling in required fields or calling the function with empty required fields without discovering all required fields.\" slot-filling question is question that the assistant asks the user to determine all required fields for function in tool-augmented LLM. The users answer to this question is called slot-filling answer. And together, slot-filling questions and answers are called slot-filling conversation. When we say slot-filling conversation is unnecessary in the Source, it specifically means that in the first turn of the Source conversation, the user provided all required fields for the function, allowing the assistant to immediately call the appropriate function in the second turn. You must generate Target conversations, so please thoroughly understand all the following rules. Generate your response in strict JSON format. Rules 1. In the first conversation turn, the user makes request to the assistant without including any information about required fields. 2. In the second turn, since there are 3 required field arguments but the users first turn contained 0 arguments, the assistant asks questions about 3-0=3 arguments in the order they appear in the required fields. 3. In the third turn, the user randomly answers only 1 out of the 3 questions asked by the assistant. 4. In the fourth turn, the assistant asks about the remaining 2 required fields. 5. In the fifth turn, the user randomly answers only 1 out of the 2 questions asked by the assistant. 6. In the sixth turn, the assistant asks about the remaining 1 question. 7. In the seventh turn, the user answers this final question. #### Example # Source { \"messages\": [ \"role\": \"user\", \"content\": \"Hi, please translate this French sentence into English. \"Je suis vraiment heureux de te rencontrer\"\" { }, { \"role\": \"assistant\", \"content\": \"Translation begins.\", \"tool_calls\": [ { \"id\": null, \"type\": \"function\", \"function\": { \"name\": \"translate_text\", \"arguments\": \"{\"text\": \"Je suis vraiment heureux de te rencontrer\", \"source_language\": \"French\", \"target_language\": \"English\"}\" } } ] }, Table 9: The prompt for Type 1 to Type 2 data augmentation in DiaTool-DPO Hard subset (Part 1). { }, { \"role\": \"tool\", \"content\": \"{\"translated_text\": \"I'm really happy to meet you\"}\", \"tool_call_id\": null, \"name\": \"translate_text\" \"role\": \"assistant\", \"content\": \"\"Je suis vraiment heureux de te rencontrer\" translates into \"I'm really happy to meet you\" in English\", \"tool_calls\": null } ], \"tools\": [ { \"type\": \"function\", \"function\": { \"name\": \"translate_text\", \"description\": \"Text translation from one language to another.\", \"parameters\": { \"type\": \"object\", \"required\": [ \"text\", \"source_language\", \"target_language\" ], \"properties\": { \"text\": { \"type\": \"string\", \"description\": \"Text to translate\" }, \"source_language\": { \"type\": \"string\", \"description\": \"Source langauge of the text\" }, \"target_language\": { \"type\": \"string\", \"description\": \"Target language to translate into\" } } } } } ] } Table 10: The prompt for Type 1 to Type 2 data augmentation in DiaTool-DPO Hard subset (Part 2). # Target { \"messages\": [ { }, { }, { }, { }, { }, { }, { }, { \"role\": \"user\", \"content\": \"Hi, have something to translate.\" \"role\": \"assistant\", \"content\": \"Please tell me the sentence you want to translate, the source language, and the target language.\" \"role\": \"user\", \"content\": \"Translate this into English\" \"role\": \"assistant\", \"content\": \"Please tell me the sentence you want to translate and the source language.\" \"role\": \"user\", \"content\": \"The sentence is \"Je suis vraiment heureux de te rencontrer\".\" \"role\": \"assistant\", \"content\": \"Please tell me the source language.\" \"role\": \"user\", \"content\": \"The source language is French.\" \"role\": \"assistant\", \"content\": \"Let's start translating.\", \"tool_calls\": [ { \"id\": null, \"type\": \"function\", \"function\": { \"name\": \"translate_text\", \"arguments\": \"{\"text\": \"Je suis vraiment heureux de te rencontrer\", \"source_language\": \"French\", \"target_language\": \"English\"}\" } } ] }, { \"role\": \"tool\", \"content\": \"{\"translated_text\": \"I'm really happy to meet you\"}\", \"tool_call_id\": null, \"name\": \"translate_text\" }, { \"role\": \"assistant\", \"content\": \"\"Je suis vraiment heureux de te rencontrer\" translates into \"I'm really happy to meet you\" in English\", \"tool_calls\": null } ] } # Source Table 11: The prompt for Type 1 to Type 2 data augmentation in DiaTool-DPO Hard subset (Part 3). Figure 2: Effects of hyperparameters on model performance metrics. (a) Impact of DPO regularization parameter (β) ranging from 0.1 to 0.5. (b) Impact of reward scaling factor (γ) from 0.1 to 0.9. (c) Impact of reward gap margin (ρ) from 0 to 5. All experiments measure six different performance metrics: call accuracy, completion accuracy, slot accuracy, relevance accuracy, and micro/macro-averaged scores. Stage Dataset Name Train Set Size Language"
        },
        {
            "title": "F Extensibility to Other Languages",
            "content": "Our approach is not limited to Korean. We qualitatively validate its extensibility to English, with examples shown in Figures 3, 4, and 5. Through qualitative analysis, we demonstrate distinct error patterns in how the SFT-Only model processes missing required fields. In Figure 3, we observe that the model fails to recognize missing required field and simply omits it without generating slot-filling question. more concerning behavior is illustrated in Figure 4, where the model hallucinates the missing required field amount with an arbitrary value of 1,000. This case is particularly concerning as incorrect tool calls can be made without user awareness. Figure 5 reveals another problematic pattern: the model fabricates comments about missing values instead of requesting them. In contrast, our SFT + DiaTool-DPO approach consistently generates appropriate slot-filling questions for missing required fields across all these scenarios, demonstrating robust handling of incomplete information. CPT SFT glaive2.0 xlam-function-calling-60k schema_guided_dstc8 glaive2.0 schema_guided_dstc8 110K 60K 16K 47K 16K English English English Korean Korean Table 12: Training datasets used in each stage. English datasets are employed for CPT while Korean datasets are used for SFT. Dataset sizes are reported in thousands (K) of examples. to slot-filling and relevance tasks while showing catastrophic forgetting of tool call capabilities previously acquired during supervised fine-tuning. Impact of γ Value Variations in the γ parameter did not yield statistically significant differences in performance metrics. As previously discussed in Section 6.2, we hypothesize that this phenomenon occurs because the evaluation methodology of FunctionChat-Bench is based on teacherforcing, which prevents error propagation from previous turns, thereby making it insensitive to variations in the γ value. Impact of ρ Value We observed negative correlation between margin size and call performance. Since margin serves as reward gap threshold, larger values intensify the DiaTool-DPO training signal. This phenomenon mirrors the behavior observed with small β values, suggesting that excessive margins lead to catastrophic forgetting of fundamental tool call capabilities acquired during supervised fine-tuning. Figure 3: Comparison of responses between SFT-Only and SFT + DiaTool-DPO models. The Messages shows the users initial query, and Tools presents the tool specification required to resolve the users query. For brevity, we omit the remaining 4-6 candidate tools in Tools. While the tool specification includes min and max as required fields, the user only specifies the min value in the query. The SFT-Only model proceeds to call the tool using only the min value, whereas SFT + DiaTool-DPO model engages in slot-filling by asking for the missing max value. Figure 4: Comparison of responses between SFT-Only and SFT + DiaTool-DPO models. The Messages shows the users initial query, and Tools presents the tool specification required to resolve the users query. For brevity, we omit the remaining 4-6 candidate tools in Tools. While the tool specification lists amount, from_currency, and to_currency as required fields, the users query lacks the amount information. The SFT-Only model hallucinates value of 1,000 for the amount field, whereas the SFT + DiaTool-DPO model engages in slot-filling by asking question to determine the missing amount value. Figure 5: Comparison of responses between SFT-Only and SFT + DiaTool-DPO models. The Messages shows the users initial query, and Tools presents the tool specification required to resolve the users query. For brevity, we omit the remaining 4-6 candidate tools in Tools. While the tool specification lists bill_amount and tip_percentage as required fields, the users query does not specify the tip_percentage. The SFT-Only model generates comment assuming 10% tip percentage, whereas the SFT + DiaTool-DPO model generates slot-filling question to determine the tip percentage value."
        }
    ],
    "affiliations": [
        "Kakao Corp. Seongnam-si, Gyeonggi-do, South Korea"
    ]
}