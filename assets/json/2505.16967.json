{
    "paper_title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval",
    "authors": [
        "Nandan Thakur",
        "Crystina Zhang",
        "Xueguang Ma",
        "Jimmy Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training robust retrieval and reranker models typically relies on large-scale retrieval datasets; for example, the BGE collection contains 1.6 million query-passage pairs sourced from various data sources. However, we find that certain datasets can negatively impact model effectiveness -- pruning 8 out of 15 datasets from the BGE collection reduces the training set size by 2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a deeper examination of training data quality, with a particular focus on \"false negatives\", where relevant passages are incorrectly labeled as irrelevant. We propose a simple, cost-effective approach using cascading LLM prompts to identify and relabel hard negatives. Experimental results show that relabeling false negatives with true positives improves both E5 (base) and Qwen2.5-7B retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the cascading design is further supported by human annotation results, where we find judgment by GPT-4o shows much higher agreement with humans than GPT-4o-mini."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 7 6 9 6 1 . 5 0 5 2 : r Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval Nandan Thakur* Crystina Zhang* Xueguang Ma"
        },
        {
            "title": "Jimmy Lin",
            "content": "David R. Cheriton School of Computer Science, University of Waterloo, Canada Code: https://github.com/castorini/rlhn Dataset: https://huggingface.co/rlhn"
        },
        {
            "title": "Abstract",
            "content": "Training robust retrieval and reranker models typically relies on large-scale retrieval datasets; for example, the BGE collection contains 1.6 million query-passage pairs sourced from various data sources. However, we find that certain datasets can negatively impact model effectiveness pruning 8 out of 15 datasets from the BGE collection reduces the training set size by 2.35 and increases nDCG@10 on BEIR by 1.0 point. This motivates deeper examination of training data quality, with particular focus on false negatives, where relevant passages are incorrectly labeled as irrelevant. We propose simple, cost-effective approach using cascading LLM prompts to identify and relabel hard negatives. Experimental results show that relabeling false negatives with true positives improves both E5 (base) and Qwen2.5-7B retrieval models by 0.71.4 nDCG@10 on BEIR and by 1.71.8 nDCG@10 on zero-shot AIRBENCH evaluation. Similar gains are observed for rerankers fine-tuned on the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the cascading design is further supported by human annotation results, where we find judgment by GPT-4o shows much higher agreement with humans than GPT-4o mini."
        },
        {
            "title": "Introduction",
            "content": "Modern-day retrievers and rerankers are datahungry, relying on large and high-quality training datasets to accurately retrieve or rerank across challenging domains (Thakur et al., 2021; Muennighoff et al., 2023; Yang et al., 2024b; SU et al., 2025). training dataset for information retrieval (IR) typically has multiple instances consisting of training query, labeled positive passages, and set of mined hard negative passages. Sampling hard negatives has been consistently used in retrieval models to improve downstream retrieval accuracy (Karpukhin *Both authors equally contributed in this work. 1 et al., 2020; Xiong et al., 2021; Qu et al., 2021; Moreira et al., 2024, inter alia). More recently, state-of-the-art (SoTA) retrieval models are observed to fine-tune on enormous training dataset sizes. While the general notion is that more training data is better (Chen et al., 2024a; Li et al., 2024; Muennighoff et al., 2025), we show the contrary, fine-tuning on select few datasets is rather crucial. For example, removing ELI5 surprisingly improves nDCG@10 on 7 out of 14 of the BEIR datasets (Thakur et al., 2021) and the average nDCG@10 by 0.6 points. similar observation is also made on other training datasets: by pruning 8 out of the 15 datasets in the BGE training collection (Li et al., 2024),1 we improve E5 (base) fine-tuned retrieval model by 1.0 point nDCG@10 on BEIR (as shown in Figure 4). The above observation reveals that there is nonnegligible amount of false or mislabeled data mixed in the current enormous training dataset, which not only adds unnecessary training cost but also hurts the training process. In this work, we propose eliminating the issue from the perspective of false negatives, which are typically introduced by misclassifying unlabeled candidates as negative training examples. This is especially severe for sparsely-annotated retrieval datasets, such as MS MARCO (Nguyen et al., 2016) or NQ (Kwiatkowski et al., 2019). Figure 3 shows an example of false negative present in HOTPOTQA (Yang et al., 2018). Previously, it has been observed that fine-tuning on datasets with false negatives affects the models generalizability (Qu et al., 2021; Moreira et al., 2024; Tao et al., 2024). To avoid false negatives, existing approaches adopted knowledge distillation or hard negative filtering: Qu et al. (2021) distill knowledge from cross-encoder. Moreira et al. (2024) filter poten1The pruned dataset contains only 42.5% pairs of the original dataset, making it 2.35 smaller. Figure 1: Example of training instance (query, ground truth positives, and unlabeled hard negatives) with detected false negatives taken from HOTPOTQA. The false negative passage (Splash Works) is mislabeled as it is relevant in answering the users query. The relevant parts of the text are highlighted in blue. tial false negatives based on relevance score to the query. However, they dont directly tackle the false negative data samples. The former alleviates the issue based on the assumption that the cross-encoder is more robust to false negatives than retrieval models, yet does not remove the false negatives themselves. As we will show in the results section, albeit smaller, inferior training data also negatively affect cross-encoders. The latter alleviates the issue based on the assumption that the relevance scores of false negatives are systematically higher than 95% of the positive scores, which does not consider variance at the level of data samples. In our work, we propose RLHN (ReLabeling Hard Negatives), utilizing cost-effective framework with LLM cascading (Chen et al., 2023) to identify and accurately relabel false negatives (at data sample level) in seven pruned training datasets from the BGE training collection (Li et al., 2024). In the first stage, we use GPT-4o-mini, costeffective LLM, to identify false negatives in all training instances. Next, the detected false negatives are relabeled with more reliable judge, GPT4o. We observe maximum of 56% of training pairs in MS MARCO can contain false negatives, to minimum of about 3% in SCIDOCSRR. The framework is better illustrated in Figure 2. With false negative documents detected, we modify the training datasets in three different ways: (i) remove: discarding the whole training instance, (ii) HN remove: removing only the false hard negatives, and (iii) RLHN: relabeling the false hard negatives as ground truth. Our results show consistently that RLHN setting achieves the highest nDCG@10 scores on BEIR (Thakur et al., 2021) and AIR-BENCH (Yang et al., 2024b), amongst their counterparts with both retrievers: E5 (base) and Qwen2.5-7B and reranker with Qwen2.5-3B. To better understand the behavior of LLM judgment, we compare LLM judgment with human assessors on 670 randomly sampled queryhard negative pairs. We observe the Cohens Kappa (Îº) score of GPT-4o is 10 points higher than GPT-4omini, which echoes their effectiveness in improving training data quality. Lastly, we provide qualitative analysis examining different categories of false negatives identified in training datasets. To the best of our knowledge, we are the first to report that carelessly adopting enormous training data may negatively affect the retriever and reranker model training, and propose solution by focusing on the dataidentifying and relabeling the false hard negatives."
        },
        {
            "title": "2 Related Work",
            "content": "Sparsely-annotated datasets. Popular IR training datasets, such as MS MARCO (Nguyen et al., 2016), were shallow pooled and sparsely judged by human assessors (Mackenzie et al., 2021; Arabzadeh et al., 2022). The assessor observed few passages from baseline retrieval system, picked those relevant to the query, and labeled them as ground-truth. On the other hand, non-relevant judged passages (i.e., passages seen but preferred lower than the ground truth) were not provided. Therefore, an assumption is made in fine-tuning where remaining passages (in passage corpus) are negatives, and few mined passages similar to the query are labeled as hard negatives. In this work, we avoid relabeling false positives, as these labels are trustworthy, provided by human assessor, who can have different preference than the LLM itself. 2 Figure 2: Flowchart for RLHN (ReLabeling Hard Negatives): (1) Provide the query, ground-truth or positive passages, and hard negative passages from training instance as input, (2) Prompt cost-effective LLM judge (e.g., GPT-4o-mini) and evaluate whether any hard negative is misclassified, (3) If yes, repeat the prompt with an accurate LLM judge (e.g., GPT-4o) (4) Output the relabeled hard negative passages (which are found relevant) and either remove them or relabel them as ground-truth passages in our experiments. LLM-based data curation. Hiring human assessors for judgments is expensive and timeconsuming, and produces limited training pairs, e.g., 1K pairs in LIMA (Zhou et al., 2023). Alternatively, LLMs as judges have been recently explored for dataset curation in tasks, such as instruction fine-tuning (Chen et al., 2024b; Chen and Mueller, 2024), or even code-generation (Jain et al., 2024). False negatives. Qu et al. (2021) first noted the issue of false negatives in retrieval, where certain hard negative passages should have been classified as positives. However, instead of curating the training datasets, RocketQA (Qu et al., 2021) fine-tuned models by distilling knowledge from the cross-encoder label. Similarly, Moreira et al. (2024) examined various filtering methods for negative sampling by avoiding very hard negatives. Lee et al. (2024, 2025b) use an LLM such as Gemini to relabel positive passages and identify better hard negatives, but they do not provide further qualitative analysis on its effectiveness or publicly share their information or training datasets."
        },
        {
            "title": "3 The RLHN Methodology",
            "content": "In this section, we discuss the LLM judge cascading framework, training dataset modifications, and dataset postprocessing and statistics."
        },
        {
            "title": "3.1 LLM Judge Cascading Framework",
            "content": "We adopt simple and cost-effective approach of LLM judge cascading framework (shown in Figure 2) inspired by Chen et al. (2023) to identify false hard negatives in training datasets at large scale. The framework involves two major steps: 1. Cost-effective judge (GPT-4o-mini): We prompt GPT-4o-mini (OpenAI, 2024), costeffective LLM in the first stage to scan and identify potential pairs with false negatives across all training pairs. 2. Accurate judge (GPT-4o): Next, we prompt GPT-4o (OpenAI, 2024), more reliable but expensive judge2 to evaluate the potential pairs containing false negatives identified by GPT-4omini and re-evaluate them using GPT-4o."
        },
        {
            "title": "3.2 Training Dataset Modification",
            "content": "Upon successful completion of identifying the false negatives in our training datasets, we compare three operations on the identified false negatives, modifying the training datasets: Remove: Discard the complete training instance due to the low quality, even if it contains at least one false negative.3 HN Remove: Discard only the detected false negatives from the hard negative subset, keeping the training instance with the remaining hard negatives. RLHN: Relabel only the detected false negatives, by adding them to the ground truth subset, keeping the training instance with the remaining hard negatives."
        },
        {
            "title": "3.3 Dataset Postprocessing & Statistics",
            "content": "In Table 1, we show the training dataset statistics observed in the BGE training collection. MS MARCO contains the highest amount of training pairs, followed by HOTPOTQA. All datasets contain training pairs with 13 ground-truth passages and 1325 hard negatives (except NQ with 98 hard negatives). False negatives. From Table 1, we see majority of detected false negatives occur in MS MARCO (91.6% of all detected pairs). maximum of up to 56% of all training pairs in MS MARCO contain false negatives, to minimum of about 3% 2GPT-4o-mini and GPT-4o pricing (as of May 15th, 2025) is 0.6$ and 5.0$ for 1M input tokens and 2.4$ and 20.0$ for 1M output tokens, respectively. 3We lose out on the training pair in the remove technique."
        },
        {
            "title": "Dataset",
            "content": "#Train Avg. Avg."
        },
        {
            "title": "RLHN",
            "content": "Pairs GT/Q HN/Q Stage 1 Stage 2 MS MARCO 485,823 84,516 HOTPOTQA 58,568 NQ 29,096 FEVER 12,655 SCIDOCSRR 5,500 FIQA-2018 4,065 ARGUANA 1.1 2.0 1.0 1.3 1.6 2.6 1.0 25.0 20.0 98.5 20.0 19.7 15.0 13.6 391,965 11,268 32,184 7,764 2,068 3,632 326,301 4,756 19,199 3,577 351 1,833 0 Table 1: BGE training dataset statistics (Chen et al., 2024a). Avg. GT/Q denotes the average ground truth passages per query, and Avg. HN/Q denotes the average hard negative passages per query. RLHN Stages 1 & 2 show training pairs with at least one false hard negative. in SCIDOCSRR.4 From Figure 3, we observe that in 58% of all detected false negative pairs, only single false positive was detected, and 19% with two false negatives, and less than 1% with eight or more false negatives. If we detect any training pair with detected false negatives over certain threshold (k = 7 in our experiments), we excluded the pair completely in RLHN, as the query is likely to be ambiguous, that might not be useful training instance (e.g., what color is amber urine?)."
        },
        {
            "title": "4 Experimental Setting",
            "content": "BGE training data. We utilize the original BGE training dataset5 (Li et al., 2024), comprehensive collection with training datasets for retrieval (e.g., NQ, MS MARCO), clustering (e.g., TwentyNewsgroups), and classification (e.g., AmazonReviews) tasks. Many of these training datasets are used in fine-tuning of popular retriever models such as E5-Mistral (Wang et al., 2024), GRITLM (Muennighoff et al., 2024), Linq (Choi et al., 2024), LLM2Vec (BehnamGhader et al., 2024), CDE (Morris and Rush, 2025), or NV-Embed (Lee et al., 2025a). Our work focuses on the retrieval task, therefore, we remove all training datasets from clustering and classification tasks, resulting in 15 datasets focused on the retrieval task, comprising total of 1.6M training pairs, originally released with the MIT license. LLM judges. In our work, we use GPT-4omini (version 2024-07-18) and GPT-4o (version 2024-11-20) as the judge using the Azure OpenAI service in the batch setting. We follow tem4We avoid relabeling ARGUANA due to its inherent complex task, which doesnt measure directly for argument similarity, but rather counter arguments given an argument. Therefore, we keep the original dataset in fine-tuning without relabeling. 5BGE dataset: huggingface.co/datasets/cfli/bge-full-data Figure 3: The distribution of training pairs (with at least one false negative) across false hard negatives detected. 58% of the training pairs detected contain single false negative, 19% with two false negatives, and so on. perature setting of 0.1 and use chain-of-thought prompt setting (Wei et al., 2022). The prompt first evaluates the relevance between every hard negative passage and the question, and compares them with the ground truth to identify potential false negatives. We can prompt up to 25 hard negative passages per query as shown in Figure 6. Evaluation benchmarks. We evaluate the retrieval and reranker accuracy of the models finetuned on datasets with false negatives either removed or relabeled with RLHN on the BEIR benchmark (Thakur et al., 2021) and AIR-BENCH (Yang et al., 2024b). Both benchmarks evaluate retrieval accuracy in nDCG@10. BEIR contains humanconstructed datasets, and AIR-BENCH contains datasets automatically generated by LLMs without human intervention. In BEIR, we drop Quora and CQADupstack and evaluate on the remaining 16 datasets. In AIR-BENCH (version 24.05), we evaluate five specific domains in English-only: Arxiv, Finance, Healthcare, Law, and News. Backbone models. We use the E5 (base) unsupervised6 (Wang et al., 2022b, 2024), BERT-based encoder, due to its high accuracy on BEIR (preliminary results in Appendix A), the inclusion of pre-training stage, and lower training complexity. E5 (base) contains 110M parameters, 12 layers, and 768 embedding dimension with mean pooling. Also, we use LLM-based decoder model with Qwen2.5-7B model7 (Yang et al., 2024a) with 7.61B parameters, 28 layers, and 3584 embedding dimension with the [EOS] token pooling as the retrieval models. In addition, we use Qwen2.5-3B model (Yang et al., 2024a)8 for the reranker. 6intfloat/e5-base-unsupervised on HuggingFace. 7Qwen/Qwen2.5-7B on HuggingFace. 8Qwen/Qwen2.5-3B on HuggingFace."
        },
        {
            "title": "No Filtering",
            "content": "Cascading Stage 1: GPT-4o-mini Cascading Stage 2: GPT-4o-mini + GPT-4o No Filtering Cascading Stage"
        },
        {
            "title": "Backbone",
            "content": "E5 (base) E5 (base) E5 (base) E5 (base) E5 (base) E5 (base) E5 (base) Qwen2.5-7B Qwen2.5-7B Qwen2.5-7B TREC-COVID NFCorpus NQ HotpotQA FiQA-2018 ArguAna TouchÃ©-2020 DBPedia SCIDOCS FEVER Climate-FEVER SciFact TREC-NEWS Robust04 Signal-1M (RT) BioASQ Avg. 16 (All) Avg. 7 (OOD) 0.783 0.378 0.595 0.737 0.439 0.701 0.256 0.438 0.242 0.878 0.391 0.735 0.465 0.442 0.275 0.378 0.508 0.425 0.786 0.378 0.593 0.737 0.443 0.702 0.255 0.439 0.243 0.875 0.388 0.741 0.470 0.448 0.279 0. 0.510 0.428 0.793 0.380 0.592 0.736 0.440 0.706 0.271 0.437 0.243 0.876 0.385 0.731 0.466 0.452 0.275 0.385 0.511 0.432 0.798 0.381 0.602 0.739 0.444 0.700 0.268 0.442 0.244 0.877 0.391 0.733 0.473 0.471 0.275 0.392 0.514 0.437 0.794 0.380 0.573 0.741 0.441 0.700 0.218 0.433 0.245 0.881 0.382 0.744 0.464 0.447 0.274 0. 0.506 0.423 0.785 0.382 0.598 0.736 0.445 0.700 0.265 0.441 0.243 0.876 0.384 0.735 0.473 0.458 0.270 0.384 0.511 0.431 0.809 0.390 0.591 0.735 0.448 0.692 0.266 0.447 0.242 0.871 0.367 0.740 0.484 0.497 0.274 0.394 0.515 0.445 0.797 0.389 0.597 0.704 0.453 0.554 0.221 0.443 0.245 0.863 0.370 0.755 0.494 0.501 0.275 0. 0.504 0.441 0.771 0.389 0.602 0.702 0.461 0.550 0.211 0.456 0.243 0.857 0.373 0.755 0.480 0.501 0.268 0.412 0.502 0.433 0.815 0.391 0.623 0.729 0.465 0.560 0.230 0.472 0.252 0.872 0.360 0.767 0.487 0.540 0.280 0.438 0.518 0.454 Table 2: Retrieval results measuring nDCG@10 on 16 datasets in the BEIR benchmark by fine-tuning retrieval models on variants of the BGE training dataset after relabeling false negatives. The seven unseen (or out-of-domain) datasets during fine-tuning are highlighted with and their average scores are provided in Avg. 7."
        },
        {
            "title": "5.1 Preliminary Results: Dataset Pruning",
            "content": "False datapoint can hurt the training of retriever models. We assess the individual dataset contribution by evaluating several model variants by leaving one dataset out and fine-tuning the rest. As we fine-tune many models, i.e., one for each removed dataset, we limit these experiments to E5 (base). Summarized results are shown in Figure 4 (detailed results can be found in Table 10), demonstrating that training datasets (highlighted in red) can hurt the model retrieval accuracy, such as ELI5, removing which improves the nDCG@10 on BEIR (0.519 0.525). Also, it shows that certain datasets (highlighted in green) are crucial for model accuracy. Based on findings in Figure 4 and selecting necessary datasets for individual task-based performances in BEIR, we prune the original 16 retrieval datasets in the BGE collection and select seven datasets (highlighted as ), reducing the training dataset size from 1.6M to 680K training pairs in our experiments. The average nDCG@10 score of E5 (base) improves from 0.519 0.529 on 14 datasets on average in BEIR, by fine-tuning on almost 2.35 smaller dataset (1.6M 680K)."
        },
        {
            "title": "5.2 Main Results: Relabeling False Negatives",
            "content": "This section shows the results of the fine-tuned models on the variants of the training dataset described in Section 3.1 and 3.2, keeping the rest of the model training parameters unchanged. Figure 4: Dataset pruning by leaving one dataset out during fine-tuning E5 (base) on the BGE-training collection; [ALL] denotes fine-tuning on all datasets with 1.6M training pairs; [7 Pruned] denotes fine-tuning on 680K training pairs with seven remaining datasets (or 42.5% pairs) after dataset pruning. Fine-tuning details. All our E5 (base) models are fine-tuned using 4L40S GPUs, with learning rate of 2e-5, sequence length of 350 tokens for both the query and passage for 45 epochs. We append query: and passage: prefix during fine-tuning each model. Similarly, our Qwen2.57B and Qwen2.5-3B models are fine-tuned using maximum of two H200 GPUs. All our retrieval training datasets are converted into the Tevatron format, and fine-tuning is conducted via the Tevatron repository9 (Gao et al., 2023; Ma et al., 2025). 9Tevatron repository: https://github.com/texttron/tevatron"
        },
        {
            "title": "Backbone",
            "content": "Technique Arxiv Finance Health. Law News Avg. 5 0.117 0.455 0.368 E5 (base) E5 (base) E5 (base) E5 (base) E5 (base) E5 (base) E5 (base) Qwen2.5-7B 0.401 0."
        },
        {
            "title": "0.521\nCascading Stage 1: GPT-4o-mini\n0.526\n0.522\n0.522",
            "content": "0.346 0.344 RLHN 0.362 Cascading Stage 2: GPT-4o-mini + GPT-4o Remove HN Remove 0.407 0.406 0.421 0.341 0.346 RLHN 0.356 0.403 0.411 0.440 0.514 0.525 0.521 0.118 0.118 0. 0.125 0.124 0.138 Default Cascading Stage 2: GPT-4o-mini + GPT-4o 0.391 0.479 0.325 0. 0.452 0.459 0.465 0.438 0.464 0.476 0.370 0.370 0.379 0.364 0.374 0.386 0.430 0. Qwen2.5-7B HN Remove Qwen2.5-7B"
        },
        {
            "title": "0.335\nRLHN 0.330",
            "content": "0.384 0.418 0.487 0.494 0.111 0.133 0.423 0.450 0.348 0.365 Table 3: Retrieval results measuring nDCG@10 on five specialized domains in AIR-BENCH dev (version 24.05) by fine-tuning E5 (base) and Qwen2.5-7B on variants of the BGE training dataset with RLHN. BEIR benchmark. Results in Table 2 show that in both E5 (base) and Qwen2.5-7B, the RLHN technique achieves the best overall average nDCG@10 of 0.515 and 0.518 on 16 datasets on BEIR, outperforming models trained in the default setting and remove techniques. The relabeled data in RLHN improves model generalization, with improvements strongly visible in seven out-of-domain (OOD) datasets in BEIR. Stage 1 (RLHN) outperforms the default setting by 2.0 points and Stage 2 (RLHN) by 3.2 points in nDCG@10. Overall, relabeling false negatives improves the data quality, which is quite visible in model generalization across out-ofdomain settings in BEIR. AIR-BENCH. In addition to BEIR, AIR-BENCH provides zero-shot setting to evaluate on challenging domains, such as Law. Table 3 shows the average nDCG@10 on five specialized domains. We also confirm that the improvements in model generalization are similar to what we observed in BEIR. Stage 1 (RLHN) setting improves the default setting by 1.1 point nDCG@10, and Stage 2 (RLHN) further improves by 2.1 points nDCG@10. Overall, without changing the model or training parameters, mitigating false negatives in training datasets with RLHN helps the model generalize better on specialized domains in AIR-BENCH. Reranker results. Training data with improved quality also benefits the cross-encoder rerankers. Table 4 shows the result comparison on the BEIR benchmark, where we rerank the top-100 results from the fine-tuned E5 (base) in the default setting. We observe that training rerankers with data fixed on RLHN Stages 1 and 2 progressively increases nDCG@10 on BEIR datasets by 0.5 points and 0.8 points. This improvement is most prominent on the 7 OOD datasets, consistent with the above"
        },
        {
            "title": "BEIR Dataset",
            "content": "TREC-COVID NFCorpus NQ HotpotQA FiQA-2018 ArguAna TouchÃ©-2020 DBPedia SCIDOCS FEVER Climate-FEVER SciFact TREC-NEWS Robust04 Signal-1M BioASQ Avg. 16 (All) Avg. 7 (OOD) No Filtering Cascading Stage 1 Cascading Stage"
        },
        {
            "title": "RLHN",
            "content": "0.836 0.401 0.730 0.863 0.517 0.740 0.275 0.532 0.278 0.941 0.457 0.786 0.507 0.531 0.292 0.510 0.575 0.479 0.861 0.414 0.739 0.861 0.521 0.730 0.308 0.536 0.273 0.939 0.468 0.793 0.513 0.548 0.276 0.505 0.580 0.489 0.862 0.415 0.736 0.861 0.519 0.763 0.313 0.538 0.270 0.936 0.430 0.794 0.527 0.589 0.274 0.500 0.583 0. Table 4: Reranker results measuring nDCG@10 on 16 datasets in BEIR by fine-tuning reranker models (based on Qwen2.5-3B) on variants of the BGE training datasets after relabeling false negatives. Stage 1 and 2 refers to GPT-4o-mini and GPT-4o-mini + GPT-4o. observation on retrievers: the data correction on the two stages improves the averaged OOD results by 1.0 and 1.8 points, respectively. We notice that the scale of the improvement on cross-encoders is not as large as on retrievers, which may indicate that cross-encoder rerankers are comparatively robust to false negative data than retrievers. However, albeit small, cross-encoders still benefit from training data of higher quality, especially for generalizing to unseen domains."
        },
        {
            "title": "6 Analysis",
            "content": "Robustness of RLHN across varying training data subsets. As training datasets can be large, effectively relabeling using the LLM cascading pipeline to label all training pairs can be prohibitive. Therefore, from Figure 5, we demonstrate that RLHN remains robust and shows similar accuracy gains, even for smaller randomly sampled subsets of the training dataset. To test this, we conducted four random subsets (100K, 250K, 400K, and 680K) of the training datasets, with each dataset distribution shown in Table 8. Overall, we have two main findings here: (i) E5 (base) model fine-tuned on RHLN Stages 1 and 2 training data with false hard negatives relabeled as positives outperforms the default setting consistently, and (ii) the higher slope in nDCG@10 demonstrates the continual improvement across zero-shot domains with increasing amounts of training data, especially as observed in AIR-BENCH. Figure 5: nDCG@10 scores on BEIR (Avg. 16 and Avg. 7) and AIR-BENCH (Avg. 5) by fine-tuning E5 (base) on subset of the 100K, 250K, 400K, and 680K training pairs using the RLHN technique for both stages. All individual dataset scores for both BEIR and AIR-BENCH are provided in Figure 7 and Figure 8. Datasets FEVER (3,521) FIQA-2018 (1,829) HOTPOTQA (4,720) SCIDOCSRR (350) SoTA Reranker Judge mAP@10 P@L(GT) mAP@10 P@L(GT) mAP@10 P@L(GT) mAP@10 P@L(GT) BAAI/bge-reranker-v2-gemma mxbai/rerank-large-v2 mxbai/rerank-base-v2 Cohere (rerank-v3.5) Alibaba-NLP/gte-reranker-modernbert-base cross-encoder/ms-marco-MiniLM-L12-v2 0.839 0.496 0.570 0.811 0.688 0. 0.777 0.365 0.455 0.740 0.602 0.656 0.632 0.658 0.598 0.572 0.545 0.517 0.492 0.525 0.464 0.437 0.408 0.387 0.742 0.737 0.671 0.694 0.658 0.587 0.638 0.634 0.565 0.588 0.560 0.479 0.926 0.680 0.612 0.838 0.843 0. 0.875 0.524 0.462 0.743 0.754 0.755 Table 5: Reranker as the judge as baseline to identify RLHN false negatives in each training dataset (written along with the count of training pairs). mAP@10 calculates the average precision of false negatives (labeled as positives) in the top-10 reranked results. P@L(GT) calculates the precision of false negatives present in top-k reranked results, where varies in each query, measuring the count of false negatives detected using RLHN. Reranker distillation is competitive, but lacks in detecting false negatives. reranker or crossencoder is used in knowledge distillation to finetune retriever model as an alternative to the traditional contrastive or InfoNCE loss function (HofstÃ¤tter et al., 2020; Qu et al., 2021; Wang et al., 2022a). This technique bypasses the original relevance judgments, relying on knowledge within the reranker. Instead of relying on RLHN, we evaluate how well existing rerankers detect false negatives in training datasets. To achieve this, we rerank the hard negatives present in each training instance, and compute two metrics: (i) mAP@10, measuring the average precision of false negatives in the top-10 results, and (ii) P@L(GT), measuring the precision of false negatives present in the top-k results, where is the count of false negatives. Table 5 shows results by six reranker judges from various sources on four datasets. We observe bge-reranker-v2-gemma judge achieves the highest scores amongst its counterparts, to identify false negatives labeled by RLHN (except FiQA-2018). However, on datasets such as FIQA-2018 and HOTPOTQA, rerankers can only identify 52.563.8% false negatives, indicating that existing rerankers are competitive, but still require improvement. We suspect that rerankers are fine-tuned on these existing training datasets containing false negatives, which hurt their model accuracy."
        },
        {
            "title": "Metric",
            "content": "GPT-4o-mini GPT-4o Cohens Kappa (Îº) 0.320 0.390 Table 6: Cohens Îº score of GPT-4o-mini and GPT-4o judgments with human judgments on 670 queryhard negative pairs."
        },
        {
            "title": "7 Human Validation",
            "content": "We conducted validation study with three human assessors conducting data annotation in Label Studio10. Assessors were briefed on the relevance task, and then manually evaluated 670 queryhard negative pairs in total, where the hard negatives were randomly selected from the RLHN set containing at least one false negative. During the assessment, all annotators worked independently and were not exposed to the LLM prediction. An example of the data annotation interface is provided in Figure 9. Table 6 reports the Cohens Kappa (Îº) between each LLMs predictions and the human labels. The Îº scores are consistent with others reporting similar human-LLM agreement (Arabzadeh and Clarke, 2025). GPT-4o shows substantially higher agreement level with human annotators than GPT-4omini. This confirms the above empirical results, where relabeling with GPT-4o shows consistent gains over GPT-4o-mini on the training effectiveness of retrieval and reranker models. 10Label Studio: github.com/HumanSignal/label-studio"
        },
        {
            "title": "Query",
            "content": "(Q1) Which is food magazine, Latin Mass Magazine or Saveur?"
        },
        {
            "title": "Ground Truth or Positive Passages",
            "content": "False Negatives (Detected by RLHN)"
        },
        {
            "title": "False Hard Negative",
            "content": "Latin Mass Magazine: Journal of Catholic Culture, commonly referred to as Latin Mass Magazine, is an American Catholic magazine published quarterly, with traditionalist Catholic viewpoint. [ ... ] Saveur: Saveur is gourmet, food, wine, and travel magazine that specializes in essays about various world cuisines. Its slogan\"Savor World of Authentic Cuisine\"signals the publications focus on enduring culinary traditions [ ... ] Food & Wine: Food & Wine is monthly magazine published by Time Inc. It was founded in 1978 by Ariane and Michael Batterberry. It features recipes, cooking tips, travel information, restaurant reviews, chefs, wine pairings and seasonal holiday content [ ... ] Cocina (magazine): is Colombianbased monthly magazine published by Publicaciones Semana S.A.. It features recipes, cooking tips, culinary tourism information, restaurant reviews, chefs, wine pairings and seasonal holiday content [ ... ] (Q2) What year was the premier professional ice hockey league in the world established? 201617 Minnesota Wild season: The 201617 Minnesota Wild season was the 17th season for the National Hockey League franchise that was established on June 25, 1997. National Hockey League: The National Hockey League (NHL; French: \"Ligue nationale de hockeyLNH\" ) is professional ice hockey league currently comprising 31 teams [ ... ] (Q3) name meaning yin and yang Yin and yang: In Chinese philosophy, yin and yang (also, yin-yang or yin yang) describes how apparently opposite or contrary forces are actually complementary, interconnected, and interdependent in the natural world, and how they give rise to each other as they interrelate to one another. (Q4) Charles, Prince of Wales is patron of numerous other organizations. Charles, Prince of Wales: Charles, Prince of Wales (Charles Philip Arthur George; born 14 November 1948) is the eldest child and heir apparent of Queen Elizabeth II [ ... ] Charless interests encompass range of humanitarian and social issues: he founded The Princes Trust in 1976, sponsors The Princes Charities, and is patron of numerous other charitable and arts organisations. [ ... ] History of the National Hockey League (191742): History of the National Hockey League (191742) The National Hockey League (NHL) was founded in 1917 following the demise of its predecessor league, the National Hockey Association (NHA). [ ... ] Yin and yang: Yin and Yang are ancient Chinese philosophical terms, with the Yin Yang Theory being fundamental part of Feng Shui. It is Chinese theory on the perspective of continuous change and balance. [ ... ] Yin Yang Symbols and Their Meanings: In nutshell, Chinese yin yang symbols represent perfect balance. great deal of Chinese philosophy stems from the concept of yin and yang - opposites interacting [ ... ] Julia Cleverdon Dame: Julia Charity Cleverdon (born 19 April 1950) is British charity worker who served for 16 years as Chief Executive of Business in the Community, one of the Princes Charities of Charles, Prince of Wales. The Princes Trust: The Princes Trust is charity in the United Kingdom founded in 1976 by Charles, Prince of Wales, and Frederick John Pervin to help young people. [ ... ] Table 7: Qualitative analysis showcasing the different varieties of false negatives detected by RLHN. The first two questions are taken from HOTPOTQA, the third from MS MARCO, and the last from FEVER. The text supporting the query is highlighted in green, partially supporting in orange, and not supporting with red."
        },
        {
            "title": "8 Qualitative Analysis on False Negatives",
            "content": "We qualitatively analyze the labeling accuracy of our LLM cascading framework by manually spotchecking few training instances. As shown in Table 7, we observe different variety of false negatives, which can be one of the following scenarios: 1. Detected false negatives are incorrect or not relevant. GPT-4o can detect false negative, not relevant to the query. E.g., (Q1) query asks which is food magazine, within Latin Mass or Saveur, however, the false negatives identify different food magazines such as Food & Wine or Cochina, which are both incorrect. 2. The ground truth may be incorrectly labeled. In limited number of queries, we observe that the ground truth passage can have conflicting information with the false negative, and can be incorrectly labeled. E.g., the correct answer to the (Q2) query asking about the professional ice hockey establishment is 1917, present within the false negative, and 1997, mentioned in the ground truth, is incorrect. 3. The query may be too generic or ambiguous. In substantial amount of training pairs in MS MARCO, we observe that the training query is rather ambiguous, leading to many false negatives being detected. E.g., all passages are relevant to the (Q3) query, including both the ground truth and false negatives, define yin and yang correctly in different interpretation. 4. False negatives detected can be partially correct. Not all detected false negatives are relevant to the query. E.g., one of the false negatives is partially relevant to the query (Q4) about Charles or the Prince of Wales organizations."
        },
        {
            "title": "9 Conclusion",
            "content": "In this work, we emphasize the importance of clean training datasets. First, we showed that certain datasets can negatively impact model effectiveness when fine-tuned across huge collection with many training pairs. Dataset pruning removes 57.5% (8 datasets out of 15) and improves the model accuracy on BEIR by even 1.0 point and making the dataset 2.35 smaller. Next, after pruning, we observed the issue of false hard negatives in the remaining training datasets, where passages in the hard negative list are misclassified and are relevant to the query. We presented RLHN, an effective cascading LLM approach for relabeling hard negatives as ground truth or positives. Using RLHN, retrievers and rerankers were finetuned, and they consistently improved in model generalization, without any other modifications in the training settings on both BEIR and zero-shot AIR-Bench evaluations. We have released our modified training datasets and code. We will continue cleaning other popular training datasets to promote its use by the community and highlight the critical role of data quality in fine-tuning retrieval and reranker models."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada. Additional funding is provided by Microsoft via the Accelerating Foundation Models Research program."
        },
        {
            "title": "Limitations",
            "content": "Even though we propose an effective technique to identify and relabel false hard negatives with RLHN, no technique is perfect and has its limitations. Making those explicit is critical point in understanding the RLHN results and improvements, and for future work, to propose even better detection techniques. 1. False positives in training datasets. Detecting and relabeling false positives in training datasets is an important avenue of potential research. However, we avoid checking for false positives, as these labels are trustworthy, provided by human assessor, who can have different preference than the LLM itself. False positives might occur in dataset due to human errors in existing datasets, but we suspect both the importance and frequency of detected false positives to be much lower than false negatives. 2. Cleaning extremely large training datasets. The maximum training dataset size that we covered in our work contained 1M training pairs. This is reasonable dataset size to apply RLHN within strict compute budget. Cleaning extremely large training datasets (for example, containing between 110M training pairs) is not feasible, as it may require very high computation budget, with detection using GPT-4o. In the future, we wish to experiment with open-sourced LLMs, such as Qwen-3 (Yang et al., 2025), as an alternative in our LLM cascading pipeline, allowing relabeling of extremely large training datasets. 3. Multilingual and long-context document retrieval datasets. majority of the training datasets included in the BGE training collection have average document lengths up to few hundred words, roughly equivalent to few paragraphs. Applying RLHN to clean long-context document retrieval datasets, such as MLDR (Chen et al., 2024a) and multilingual training datasets, such as MIRACL (Zhang et al., 2023), would be highly relevant in the future. 4. Multi-vector retrieval models. popular suite of retrieval models includes multi-vector models, such as ColBERT (Khattab and Zaharia, 2020; Santhanam et al., 2022), representing queries and documents by multiple contextualized token-level embeddings. In our work, we limited our experiments to dense retrievers and rerankers. We keep RLHN with an extension to multi-vector models as future work, using training repository such as PyLate (Chaffin and Sourty, 2024)."
        },
        {
            "title": "References",
            "content": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel MartÃ­n BlÃ¡zquez, Guilherme Penedo, Lewis Tunstall, AndrÃ©s Marafioti, Hynek KydlÃ­cek, AgustÃ­n Piqueres LajarÃ­n, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, ClÃ©mentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, and 3 others. 2025. SmolLM2: When Smol goes big - datacentric training of small language model. CoRR, abs/2502.02737. Negar Arabzadeh and Charles LA Clarke. 2025. Human-AI comparative analysis of prompt sensitivity in LLM-based relevance judgment. arXiv preprint arXiv:2504.12408. Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2022. Shallow pooling for sparse labels. Inf. Retr. J., 25(4):365385. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. LLM2vec: Large language models are secretly powerful text encoders. In First Conference on Language Modeling. Antoine Chaffin and RaphaÃ«l Sourty. 2024. Pylate: Flexible training and retrieval for late interaction models. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024a. M3embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfIn Findings of the Assoknowledge distillation. ciation for Computational Linguistics: ACL 2024, pages 23182335, Bangkok, Thailand. Association for Computational Linguistics. Jiuhai Chen and Jonas Mueller. 2024. Automated data curation for robust language model fine-tuning. CoRR, abs/2403.12776. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2024b. AlpaGasus: Training better alpaca with fewer data. In The Twelfth International Conference on Learning Representations. 9 Lingjiao Chen, Matei Zaharia, and James Zou. 2023. FrugalGPT: How to use large language models while reducing cost and improving performance. CoRR, abs/2305.05176. Daniel Salz, Michael Boratko, Jay Han, Blair Chen, Shuo Huang, Vikram Rao, Paul Suganthan, and 28 others. 2025b. Gemini embedding: Generalizable embeddings from gemini. CoRR, abs/2503.07891. Chanyeol Choi, Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung Cho, and Jy-yong Sohn. 2024. Linq-Embed-Mistral technical report. CoRR, abs/2412.03223. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Tevatron: An efficient and flexible toolkit for neural retrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 23, page 31203124, New York, NY, USA. Association for Computing Machinery. Sebastian HofstÃ¤tter, Sophia Althammer, Michael SchrÃ¶der, Mete Sertkan, and Allan Hanbury. 2020. Improving efficient neural ranking models with cross-architecture knowledge distillation. CoRR, abs/2010.02666. Naman Jain, Tianjun Zhang, Wei-Lin Chiang, Joseph E. Gonzalez, Koushik Sen, and Ion Stoica. 2024. LLMassisted code cleaning for training accurate code generators. In The Twelfth International Conference on Learning Representations. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 20, page 3948, New York, NY, USA. Association for Computing Machinery. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452 466. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2025a. NV-Embed: Improved techniques for training LLMs as generalist embedding models. In The Thirteenth International Conference on Learning Representations. Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gustavo HernÃ¡ndez Ãbrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, Xiaoqi Ren, Shanfeng Zhang, Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo HernÃ¡ndez Ãbrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, MingWei Chang, and Iftekhar Naim. 2024. Gecko: Versatile text embeddings distilled from large language models. CoRR, abs/2403.20327. Chaofan Li, Minghao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, and Zheng Liu. 2024. Making text embedders few-shot learners. CoRR, abs/2409.15700. Xueguang Ma, Luyu Gao, Shengyao Zhuang, Jiaqi Samantha Zhan, Jamie Callan, and Jimmy Lin. 2025. Tevatron 2.0: Unified document retrieval toolkit across scale, language, and modality. Preprint, arXiv:2505.02466. Joel Mackenzie, Matthias Petri, and Alistair Moffat. 2021. sensitivity analysis of the MSMARCO passage collection. CoRR, abs/2112.03396. Gabriel Moreira, Radek Osmulski, Mengyao Xu, Ronay Ak, Benedikt Schifferer, and Even Oldridge. 2024. NV-Retriever: Improving text embedding models with effective hard-negative mining. CoRR, abs/2407.15831. John Xavier Morris and Alexander Rush. 2025. ConIn The Thirteenth textual document embeddings. International Conference on Learning Representations. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative representational instruction tuning. CoRR, abs/2402.09906. Niklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2025. Generative representational inIn The Thirteenth International struction tuning. Conference on Learning Representations. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. MTEB: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 20142037, Dubrovnik, Croatia. Association for Computational Linguistics. Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: human generated machine reading comprehension dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located 10 with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org. OpenAI. 2024. Hello GPT-4o. Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 58355847, Online. Association for Computational Linguistics. Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. ColBERTv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 3715 3734. Association for Computational Linguistics. Hongjin SU, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Liu Haisu, Quan Shi, Zachary Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. In The Thirteenth International Conference on Learning Representations. Chongyang Tao, Tao Shen, Shen Gao, Junshuo Zhang, Zhen Li, Zhengwei Tao, and Shuai Ma. 2024. LLMs are also effective embedding models: An in-depth overview. CoRR, abs/2412.12591. Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL: Generative pseudo labeling for unsupervised domain adaptation of dense retrieval. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 23452360, Seattle, United States. Association for Computational Linguistics. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022b. Text embeddings by weakly-supervised contrastive pre-training. CoRR, abs/2212.03533. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1189711916. Association for Computational Linguistics. Benjamin Warner, Antoine Chaffin, Benjamin ClaviÃ©, Orion Weller, Oskar HallstrÃ¶m, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. CoRR, abs/2412.13663. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-Thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text reIn International Conference on Learning trieval. Representations. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024a. Qwen2.5 technical report. CoRR, abs/2412.15115. Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, and Jingren Zhou. 2024b. AIR-bench: Benchmarking large audio-language models via generative comprehension. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19791998, Bangkok, Thailand. Association for Computational Linguistics. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP). 11 Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. MIRACL: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:11141131. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: Less is more for alignment. In Advances in Neural Information Processing Systems, volume 36, pages 5500655021. Curran Associates, Inc."
        },
        {
            "title": "A Pretrained or Backbone Choice",
            "content": "We experimented with several pretrained or base model choices. In particular, we focused on finetuning recently introduced encoder models such as ModernBERT (Warner et al., 2024) to decoderbased large language models such as Qwen-2.5 (less than <500M parameters). We fine-tune each backbone on the whole BGE retrieval training subset (15 datasets & 1.6M training pairs) for up to 3 training epochs with different hyperparameters to fit the training with 4xA6000 GPUs. We plot the model configurations and training settings in Table 9. Validation results. From Table 9, we observe that encoder models pre-trained such as E5-base or E5large achieve the highest nDCG@10 scores on four BEIR datasets. These outperform recent backbones such as ModernBERT-base (Warner et al., 2024) or even smaller-sized LLMs such as Qwen-2.5 (0.5B). This anecdotally confirms that the unsupervised pre-training stage in E5 pretrained models is useful and necessary for achieving competitive nDCG@10 score on BEIR. Since fine-tuning E5 (large) is around 2 slower than fine-tuning E5 (base), we run our main experiments on E5 (base) due to computational budget constraints."
        },
        {
            "title": "Dataset",
            "content": "100K 250K 400K 680K"
        },
        {
            "title": "MS MARCO\nHOTPOTQA\nNQ\nFEVER\nSCIDOCSRR\nFIQA\nARGUANA",
            "content": "49,571 10,250 6110 8017 12,654 5500 4065 145,000 30,000 30,000 28,755 12,654 5,500 4,065 210,000 84,516 58,568 28,755 12,654 5,500 4,065 485823 84516 58,568 28,755 12,654 5,500 4,"
        },
        {
            "title": "Total Pairs",
            "content": "96,167 255,974 404,058 679,881 Table 8: Training pair distribution across seven datasets for four configurations: 100K, 250K, 400K, and 680K. Leaving-one-dataset-out We provide detailed scores for leaving-one-datasetout (Figure 4) in Table 10, where we fine-tune E5-base retriever models on: Part (a): no datasets; Part (b): all 15 datasets; Part (c): all 15 datasets but one left-out dataset; Part (d): 7 datasets that cause the most significant effectiveness drop after being removed;"
        },
        {
            "title": "Backbone",
            "content": "#Params #Layers Hidden Size"
        },
        {
            "title": "Pool",
            "content": "LR"
        },
        {
            "title": "Batch Size",
            "content": "Epoch Time Taken COVID NFC."
        },
        {
            "title": "FiQA SciFact",
            "content": "E5-large (unsup.) (Wang et al., 2022b) ModernBERT-base (Warner et al., 2024) E5-base (unsup.) (Wang et al., 2022b) E5-small (unsup.) (Wang et al., 2022b) Qwen-2.5-0.5B (Yang et al., 2024a) SmolLM2-360M (Allal et al., 2025) SmolLM2-135M (Allal et al., 2025) 330M 149M 110M 33M 500M 360M 135M 24 22 12 12 24 32 1024 768 768 384 896 960 576 mean mean mean mean last last last 1e 5 2e 4 2e 5 3e 5 1e 5 1e 5 1e 128 8 4 256 8 4 256 8 4 256 8 4 96 8 4 96 8 4 128 8 4 3 3 3 3 3 3 3 36 hours 12 hours 18 hours 13 hours 36 hours 33 hours 24 hours 0.712 0.560 0.731 0.667 0.503 0.670 0.668 0.383 0.279 0.381 0.349 0.356 0.336 0.327 0.475 0.440 0.444 0.420 0.417 0.355 0. 0.747 0.602 0.728 0.698 0.692 0.635 0.608 Table 9: Model configuration, training settings, and retrieval results (nDCG@10) for backbone models fine-tuned on the BGE-training dataset (1.6M training pairs) and evaluated on four datasets from the BEIR benchmark. The models are sorted according to parameter size; The best score is highlighted as bold, the second best is underlined. COVID denotes the TREC-COVID dataset and NFC. denotes the NFCorpus dataset. a i r D C T - 0 0.610 1.60M 0.731 1.27M 0.772 1.57M 0.748 1.51M 0.724 1.23M 0.742 1.58M 0.720 1.54M 0.729 1.60M 0.729 1.51M 0.709 1.59M 0.736 1.59M 0.728 1.11M 0.699 1.54M 0.745 1.54M 0.759 1.59M 0.733 1.60M 0.718 r N 0.358 0.381 0.378 0.379 0.381 0.380 0.379 0.380 0.380 0.379 0.381 0.380 0.377 0.381 0.382 0.378 0.379 o H 0.524 0.726 0.728 0.725 0.642 0.726 0.726 0.730 0.726 0.723 0.728 0.727 0.730 0.728 0.727 0.727 0.727 8 1 0 2 - F 0.401 0.444 0.424 0.446 0.449 0.445 0.444 0.450 0.445 0.445 0.448 0.428 0.440 0.451 0.451 0.447 0.446 u 0.422 0.652 0.652 0.647 0.652 0.656 0.650 0.647 0.652 0.654 0.434 0.658 0.650 0.659 0.653 0.662 0. 0.390 0.595 0.593 0.598 0.600 0.586 0.593 0.595 0.594 0.598 0.598 0.596 0.551 0.553 0.599 0.595 0.596 0 2 0 2 - Ã© o 0.169 0.181 0.213 0.175 0.178 0.175 0.174 0.174 0.177 0.181 0.174 0.174 0.162 0.178 0.185 0.178 0. d D 0.354 0.437 0.434 0.434 0.425 0.435 0.436 0.440 0.437 0.437 0.434 0.436 0.407 0.435 0.436 0.436 0.437 D 0.211 0.233 0.235 0.234 0.232 0.235 0.235 0.234 0.233 0.234 0.234 0.235 0.237 0.234 0.234 0.201 0. V 0.634 0.871 0.868 0.787 0.863 0.866 0.870 0.870 0.870 0.872 0.871 0.871 0.869 0.867 0.867 0.868 0.867 E - m C 0.154 0.370 0.377 0.240 0.358 0.347 0.368 0.382 0.368 0.376 0.378 0.370 0.338 0.369 0.371 0.374 0.369 N T - 0.441 0.434 0.469 0.423 0.441 0.458 0.431 0.443 0.436 0.439 0.445 0.433 0.431 0.435 0.436 0.434 0. a S 0.737 0.728 0.734 0.749 0.725 0.742 0.729 0.731 0.728 0.729 0.731 0.729 0.733 0.728 0.729 0.740 0.729 4 0 b 0.416 0.477 0.478 0.483 0.489 0.490 0.487 0.481 0.477 0.481 0.486 0.477 0.484 0.472 0.481 0.475 0. 4 1 . 0.416 0.519 0.525 0.505 0.511 0.520 0.517 0.520 0.518 0.518 0.506 0.517 0.508 0.517 0.522 0.518 0.517 680K 0. 0.376 0.593 0.728 0.421 0.664 0. 0.440 0.204 0.875 0.397 0.748 0. 0.464 0.529 ? a p - - u - - 5 5 7 5 2 3 3 3 3 2 10 4 1 4 4 5 - o I - - 7 6 4 6 7 7 1 5 8 3 3 5 6 5 1 9 Setting (a) Pre-trained (Only) (b) (ALL) Training Pairs w/o ELI5 w/o FEVER w/o HotpotQA w/o MS MARCO Document w/o Stack Overflow (Dup.) w/o Trivia QA w/o NLI (c) w/o SQuAD w/o ArguAna w/o FIQA-2018 w/o MS MARCO Passage w/o NQ w/o Quora w/o SCIDOCSRR w/o STS 7 Datasets Pruned () (d) Table 10: Retrieval results measuring nDCG@10 on 14 datasets in the BEIR benchmark by fine-tuning E5 (base) by leaving out one training dataset at time and fine-tuning the rest. Improved denotes E5 (base) with nDCG@10 better than +1 point, Reduced with nDCG@10 worse than 1 point, and No Change within the 1 point range, compared to part (b) E5 (base) fine-tuned on ALL Training Pairs. Each row in part (c) is fine-tuned on all but one left-out dataset. Part (c) is fine-tuned on the 7 selected datasets. 14 SYSTEM: Given (1) search question, (2) relevant ground-truth document, (3) and set of unrelated documents that may appear in any systems response to that question. Your task is to evaluate whether any of the unrelated documents are relevant compared to the ground-truth document in answering the question. document is only considered relevant to the question if it provides sufficient information in answering the question. ## Input You will receive: 1. question: The question that the to-be-judged documents will be evaluated on. 2. ground_truth: pre-validated document judged as most relevant to the question. This document can answer the question and should be used as guide for your analysis. 3. documents: set of unrelated documents which may not be relevant in answering the question. You will first read the question and carefully analyze each unrelated documents provided to you. Read every question and unrelated document carefully as you would when proofreading. ## Criteria Use the following criteria to judge the relevance of each document: - Relevant: document is considered relevant to the question if it provides sufficient information in answering the question, containing all necessary parts highlighted in the ground truth. - Not Relevant: The document does not answer the question and does not provide information in entailing parts present in the ground truth. ## Output Follow these detailed steps and output your reasoning for each step wrapped for each respective XML tag below: 1. You should think and provide your reasoning under <thinking> [ . . . ] </thinking> on why and how if an unrelated document is relevant following the criteria above. 2. Next, for all unrelated documents which are found to be relevant, compare them against the ground truth (<ground_truth>) document in answering the question under <preference> [ . . . ] </preference> tokens. 3. Finally, output the list of documents which are (1) relevant and (2) prefer better or equal under the XML tag (<better>) or worse (<worse>) than the ground truth (<ground_truth>) document for answering the question in <verdict> [ . . . ] </verdict>. Output [ ] if none of the documents are found to be relevant. Follow strictly the format below: <thinking> Evaluate the reasoning individually for all unrelated documents to answer the question Doc (1): output the reasoning here Doc (2): output the reasoning here . . . </thinking> <preference> Compare the ground truth and every relevant document individually to answer the question Doc (1): compare the relevance of Doc (1) with the <ground_truth> document here, which is more preferred? . . . </preference> <verdict> <better> Preferred over or equally as ground truth: [Doc (2) . . . ] </better>, <worse> Relevant but not preferred over ground truth: [Doc (1) . . . ] </worse> </verdict> <question> {question} </question> <ground_truth> {ground_truth} </ground_truth> <documents> {documents} </documents> Figure 6: Prompt used in RLHN with GPT-4o-mini and GPT-4o for relabeling hard negatives for all BGE training datasets. Certain texts above in the prompt are bolded and tab-aligned to assist with reading. 15 Figure 7: nDCG@10 scores on all 16 BEIR datasets by fine-tuning E5 (base) retrieval model on subset of the 100K, 250K, 400K, and 680K training pairs using the RLHN technique on both stages: Stages 1 and 2. Figure 8: nDCG@10 scores on all 5 AIR-BENCH datasets by fine-tuning E5 (base) retrieval model on subset of the 100K, 250K, 400K, and 680K training pairs using the RLHN technique on both stages: Stages 1 and 2. 16 Figure 9: screenshot of the human validation study conducted via Label Studio. First, the human assessor reads the query and the relevant passages. Next, the assessor reads sequence of hard negative passages one by one and evaluates the relevancy with the question, marking their decision in the checkbox as either (1) relevant or (2) non-relevant."
        }
    ],
    "affiliations": [
        "David R. Cheriton School of Computer Science, University of Waterloo, Canada"
    ]
}