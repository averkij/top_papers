{
    "paper_title": "Language Self-Play For Data-Free Training",
    "authors": [
        "Jakub Grudzien Kuba",
        "Mengting Gu",
        "Qi Ma",
        "Yuandong Tian",
        "Vijai Mohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant high-quality training data, and reinforcement learning. Yet this progress faces a fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose a reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages a game-theoretic framework of self-play, where a model's capabilities are cast as performance in a competitive game and stronger policies emerge by having the model play against itself - a process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 4 1 4 7 0 . 9 0 5 2 : r Language Self-Play For Data-Free Training Jakub Grudzien Kuba1,2, Mengting Gu1, Qi Ma1, Yuandong Tian1, Vijai Mohan 1Meta Superintelligence Labs, 2UC Berkeley Large language models (LLMs) have advanced rapidly in recent years, driven by scale, abundant highquality training data, and reinforcement learning. Yet this progress faces fundamental bottleneck: the need for ever more data from which models can continue to learn. In this work, we propose reinforcement learning approach that removes this dependency by enabling models to improve without additional data. Our method leverages game-theoretic framework of self-play, where models capabilities are cast as performance in competitive game and stronger policies emerge by having the model play against itselfa process we call Language Self-Play (LSP). Experiments with Llama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained models can not only enhance their performance on challenging tasks through self-play alone, but can also do so more effectively than data-driven baselines. Date: September 2, 2025 Correspondence: Kuba at iamkuba@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) trained on massive datasets began mastering plethora of instruction following and reasoning tasks at levels of expert humans (Achiam et al., 2023; Rafailov et al., 2023; Team et al., 2023; Touvron et al., 2023; Shao et al., 2024; Guo et al., 2025). While in the initial stage of training, known as pre-training, the trained model absorbs vast amounts of information, post-training techniques, such as reinforcement learning (RL), enable the model to develop preferable behaviors and expertise in specialized tasks (Sutton et al., 1998; Schulman et al., 2017; Christiano et al., 2017). The RL paradigm is very different from the popular predictive or generative learning paradigms (Shalev-Shwartz and Ben-David, 2014). While these try to either predict label (Krizhevsky et al., 2017) or to reconstruct the data itself (Ho et al., 2020), RL does not set clear target for the model. Instead, the model, by taking actions in response to presented scenarios, operates in an environment that sends the model feedback known as reward. RL algorithms configure the agents behavior to maximize the reward. Thus, human preference-based rewards enable aligning LLMs with human preferences and values (Christiano et al., 2017; Ouyang et al., 2022; Achiam et al., 2023), and task rewards help LLMs improve in specific tasks (Lambert et al., 2024; Shao et al., 2024). Nevertheless, while offering potential of gaining super-human breadth of skills (Silver and Sutton, 2025), RL does share the weakness of all machine learning paradigms, which is that of reliance on data. Although dispensing with concrete targets to predict, RL methods do rely on the availability of task examples which take form of prompts in the LLM context, and thus face the same bottleneck (Villalobos et al., 2022; Jones, 2024). To circumvent this issue, the LLM community turned its attention to training from synthetic data (Patel et al., 2024; Setlur et al., 2024) and to utilizing the available data more efficiently through means of meta-learning (Zweiger et al., 2025; Calian et al., 2025). In this paper, we take different approach and, by formulating streming of the data as actions taken by an RL agent, we introduce training technique that dispenses with training from data entirely. This work introduces an algorithm whose consecutive iterations improve both the LLM and the distribution of examples that it learns from. To that end, we define competitive game in which one of the players learns to generate increasingly more challenging queries and the other one learns to respond to them. By utilizing self-play (Silver et al., 2017; Bai and Jin, 2020; OpenAI et al., 2021; McAleer et al., 2022), the algorithm uses only one LLM to induce this process, without need for an adversarial expert, thus making this training autonomous. We experiment with this technique, dubbed Language Self-Play (LSP), applied to 1 Figure 1 Language Self-Play agent operates under two modes: Challenger and Solver. Challenger generates instructions that Solver follows. While Solver learns to improve its responses to the prompts, Challenger learns to make them more difficult. Both modes are instantiated by one model and thus enable perpetual training on increasingly higher-quality self-generated data. Llama-3.2-3B-Instruct (Dubey et al., 2024; Meta, 2024) in instruction-following tasks from AlpacaEval (Li et al., 2023). The results of our experiments demonstrate that such training delivers models with just-as-strong or stronger performance than that of LLMs that do rely on the abundance of training data."
        },
        {
            "title": "2 Language Self-Play",
            "content": "In this section, we propose our solution to the problem of dependency on training data that bottlenecks LLM training. Our approach stems from the following observation: supplying learning model, as it progresses, with new, increasingly challenging data would become possible if the dataset itself was learning agent. Thus, in addition to the trained LLM, one could model streaming increasingly challenging instructions as actions of another LLM. For clarity, we will refer to that model as Challenger, and denote it as πCh, while the model following the instructions is referred to as Solver, denoted as πSol. The interaction between these agents consists of query generation step by Challenger, πCh(q), and the query-answering step by Solver, πSol(aq). Since Solver tries to maximize the task reward R(q, a), which can be either verification-based or preference-based, Challenger can guide its behavior to generate increasingly challenging queries by aiming to minimize the reward. Thus, the agents find themselves playing the following minimax game min πCh max πSol EqπCh,aπSol [R(q, a)]. (1) As discussed before, playing and learning through this game would enable Solver to improve even in the absence of training data. At the first glance, however, one may presume that representing πCh would require an additional model. That would put us in adversarial training which, in addition to requiring extra memory for the adversary, is notoriously unstable (Salimans et al., 2016; Mescheder et al., 2017). Fortunately, competitive games in which the competing players share the action space are solved effectively by self-play (Silver et al., 2 2017; Berner et al., 2019). Since both of our players are language models, they operate in the space of tokens, enabling us to adopt the self-play setting as well and use single model πθ to instantiate the two players. Thus, we represent Challenger by prompting our model with special challenger prompt <cp> (see Box 1)1. As such, we play the game with Challenger modeled as πCh θ(q) = πθ(q< cp >) and Solver modeled by πSol θ(aq) = πθ(aq). Box 1: Challenger Prompt <ChallengerPrompt> Background Language tasks take form of tuple (input, output). Given an input, the language agent produces an output that addresses the request posed by the input. For example, in question answering task, the input is question, and the output is an answer to the question. In essay title generation, the input is an essay, and the output is title that captures the essence of that essay. Request Generate an input for the language task {task} that will be passed to another language agent. That is, your response to this prompt MUST be valid input for the language task. The input can be easy, intermediate, hard, or simply noisy. It should stress-test the agent or push it think outside of the box. Note that in this run you will generate ONLY single example. Details Below, between tags [Start Generation] and [End Generation], you will find template that you MUST follow while generating the input - you will copy the text of the template (WITHOUT the tags) and fill out the templates placeholders marked by curly braces {} with your own generated content. For example, if the placeholder is {ice cream flavor}, you should generate name of an ice cream flavor. [Start Generation] {template} [End Generation] Final Remarks Do NOT generate response (an output), hints, or leak information together with the input you generate. The input must NOT request actions that cannot be performed by language agent. Stop generating immediately once your response fulfills the template. Your response must NOT include [Start Generation] and [End Generation] tags. Follow the template strictly unless both the task and template are None or empty (when you should generate an input according to your best judgment). Now, as response to this prompt, generate single input for the language task {task}, following the template. </ChallengerPrompt> See Box 2 for examples of prompts generated by Challenger and Box 4 (Appendix A) for Solvers responses. To turn the game into an efficient RL process, we found it natural to invoke the group-relative trick from GRPO (Shao et al., 2024). Specifically, at each iteration, we let Challenger generate queries q1, . . . , qN . Then, for each query qi, Solver generates answers a1 ), respectively. Then, calculating the group value as which receive rewards R(qi, a1 ), . . . , R(qi, aG , . . . , aG (qi) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 R(qi, aj ) (2) allows us to both obtain baseline to compute the relative advantage of each response to query qi, ASol(qi, aj ) = R(qi, aj )V (qi), as well as to derive notion of query difficulty which Challenger wants to maximize. Specifically, by rewarding Challenger with (qi), we encourage it to generate queries that probe Solver in areas where its performance is lacking. Thus, building upon this reward and defining baseline = 1 i=1 (qi), we derive the Challengers advantage function as (cid:80)N 1The prompt can be customized for specific task and template. ACh(qi) = (qi) (3) 3 and perform an RL update for both playes with these advantage values, as well as with KL-divergence regularization (Ouyang et al., 2022; Achiam et al., 2023; Shao et al., 2024). Hence, for sample of interactions {qi, {aj i=1, the loss functions for Solver and Challenger are j=1}N }G Solver LSol(θ) = 1 (cid:88) (cid:88) i=1 j=1 πSol πSol θ(aj θ(aj qi) qi) ASol(qi, aj ) β log θ(aj πSol πRef(aj qi) qi) Challenger LCh(θ) = 1 N (cid:88) i=1 πCh πCh θ(qi) θ(qi) ACh(qi) β log θ(qi) πCh πRef(qi) , (4) (5) respectively, where denotes the stop-gradientdetach operation (Foerster et al., 2018b). These losses are added together and differentiated with respect to θ, upon which gradient step is taken. It is worth noting that the KL-divergence regularization plays pivotal role here. While, traditionally, it ensures that the fine-tuned model does not simply deviate much from the reference model πRef, here, it also prevents Challenger from mindlessly generating adversarial sequences that do not have any semantic meaning. We refer to this approach as Language Self-Play Zero (LSP-Zero), where Zero stands for zero-sum, and evaluate it on AlpacaEval benchmark in Section 4. Box 2: Challenger-Generated Prompts 500 iterations. What is the typical difference in approach between Montessori classroom and traditional classroom setup when it comes to student-led project-based learning? 1000 iterations. Follow these steps to build functional submarine that can dive to depth of 100 meters underwater while using only scuba tank that holds 12 liters of air. The submarine must be able to withstand water pressure at that depth without collapsing, and it should surface automatically after 20 minutes. 1500 iterations. Follow the instructions to create new programming language that is combination of Python and Haskell, with unique syntax and semantics, and can compile and run on 64-bit x86 processor, using only the standard library and without any external dependencies, in under 1000 characters. The above self-play set-up may seem to naturally induce indefinite training that results in perpetual selfimprovement of the LLM. However, in some of our experiments, we did find that, eventually, the play would degenerate into an adversarial nonsense. For example, common pattern we observed while working with OpenAssistants reward-model-deberta-v3-large-v2 (OpenAssistant, 2025) was reward-hacking done by Solver through responding to most queries in Python, even if that clearly was not helpful. Thus, to guide the play towards high-quality interaction, we found it very helpful to add quality self-reward RQ(qi, aj ) that we generate with the reference model by appropriately prompting it (Yuan et al., 2024). See Box 3 for our exact prompt2. We add the quality score to Solvers reward, R(qi, aj ), as well as its average, VQ(qi) = 1 ), to Challengers reward, (qi) + VQ(qi), before computing advantage and loss functions. Once calculated, the self-reward is added to both players rewards, making the game no longer zero-sum. With it in place, we found that the self-play training could be conducted, effectively, indefinitely. We summarize the entire algorithm that we call Language Self-Play (LSP) as pseudocode in Algorithm 1. ) + RQ(qi, aj j=1 RQ(qi, aj (cid:80)G"
        },
        {
            "title": "3 Related Work",
            "content": "While for the majority of its history, deep reinforcement learning (RL) has been believed to be useful tool for strategic games (Mnih et al., 2013; Silver et al., 2017; Foerster et al., 2018a; Berner et al., 2019; Samvelyan et al., 2019) and robotics (Abbeel and Ng, 2004; Schulman et al., 2015, 2017; Kalashnikov et al., 2018; Wu et al., 2023), the breakthroughs of large language models (LLMs) have shown that it is powerful tool for 2The prompt gets formatted with the actual instruction and response. 4 Algorithm 1 Language Self-Play Require: Pre-trained model πθ, reward function R(q, a), Challenger coefficient αCh 1: Initialize reference model πRef = πθ 2: for each epoch = 1 to do 3: Generate queries qi πCh 4: Generate answers to each query, aj 5: θ(q), for = 1, . . . , πSol θ(aqi), for = 1, . . . , & = 1, . . . , Compute reward R, self-reward RQ, advantage ASol & ACh, and KL-divergence functions on the playouts {qi, {aj Calculate the total loss LSelf-Play = LSol + αChLCh Update parameters: θ = θ ηθLSelf-Play j=1}N }G i=1. 6: 7: 8: end for 9: return Trained language model πθ model alignment and enhancement (Christiano et al., 2017; Achiam et al., 2023; Bubeck et al., 2023; Rafailov et al., 2023; Team et al., 2023; Shao et al., 2024; Guo et al., 2025). However, instead of access to simulator that can put the agent at any environment statea common setting in games and roboticsLLMs learn to respond to prompts that predominantly come from human users (Achiam et al., 2023; Team et al., 2023; Guo et al., 2025). Thus, the intellectual abilities of the models are bottlenecked by the intellectual complexity of human-provided queries as well as their limited quantity (Villalobos et al., 2022; Silver and Sutton, 2025). Box 3: Self-Reward Prompt Review the user-assistant interaction (users instruction and the corresponding response) and score it using the additive 7-point integer scoring system described below. The base score is 0. Points are accumulated based on the satisfaction of each binary criterion (+1 if the criterion is met and 0 otherwise): 1. +1 iff the users task is clearly identifiable from the instruction. 2. +1 iff the instruction is clear, specific, and well-structured. 3. +1 iff it is clear that this user will be able to understand the response. 4. +1 iff the response addresses significant portion of the users question but is not necessarily fully complete. 5. +1 iff the response usefully and comprehensively answers the core elements of the question. 6. +1 iff the response is clearly written, concise, well-organized, and helpful. 7. +1 iff this user will most likely like the form and the style of the response. <Instruction >{instruction} </Instruction> <Response >{response} </Response> After examining the users instruction and the response: - Briefly justify your scores, using up to 100 words in total. Remember the score for each criterion. - Write down the calculation adding up all individual points between <Calculation> and </Calculation> tags (e.g. <Calculation>1+0+1+0+1+1+0=4</Calculation>). The result is the total score. MAKE SURE THE CALCULATION IS CORRECT! - Conclude with the total score value, from 0 to 7, between <Score> and </Score> tags (e.g. <Score>4</Score>). THE CORRECT FORMAT IS CRUCIAL! To tackle these issues, the LLM community has been developing methods of training with synthetic data, either through filtered bootstrapping (Huang et al., 2022; Wang et al., 2022; Setlur et al., 2024) or even meta-learned data augmentation (Zweiger et al., 2025; Calian et al., 2025). Works that are most related to this paper view these techniques through game-theoretic lenses. In particulear, Wu et al. (2024) view preference maximization, which is the ultimate goal of alignment, as competitive game. While they solve it with self-play, the problem they solvelearning responses to provided prompts that maximize preferenceis substantially different from our goal of streaming the whole learning process with self-play. More closely, 5 Cheng et al. (2024) introduced an LLM formulation of Adversarial Taboo game, and showed that solving it with self-play leads to improved reasoning abilities of the model on various tasks. However, the method requires prior playouts of Adversarial Taboo from upper-shelf models, such as GPT-4, which are then used for game-specific supervized fine-tuning phase. Our algorithm does not require introducing specialized language game. Instead, we show that running perpetually-improving training process can be viewed as competitive game, and that solving it does not require prior specialized SFT phases. Furthermore, recently, Zweiger et al. (2025) showed empirical benefits of learnable self-adaptation step that edits the data fed to the LLM. While the procedure is done autonomously, like our self-play, it still assumes access to training data that edits can be applied to. In contrast, the only time we expose our model to data is during evaluation. Lastly, we consider the family of self-referential algorithms (Schmidhuber, 2007) related to our work. Traditionally, such algorithms governed their own updates by either changing their weights (Irie et al., 2022; Kirsch and Schmidhuber, 2022) or system prompts (Fernando et al., 2023) according to self-invented rules. Additionally, recent work of Yuan et al. (2024) introduced form of self-reference by generating self-rewards that the model itself maximizes. While our Language Self-Play is not mere instantiation of any of these, it is self-referential in sense that it learns from self-generated data while simultaneously improving its data generation ability. Furthermore, while we use self-rewards in our method, they serve as regularizer in fundamentally competitive game that we solve with self-play."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we conduct two experiments with AlpacaEval benchmark (Li et al., 2023) and Llama-3.23B-Instruct (Dubey et al., 2024; Meta, 2024) as the base model. First, we compare data-free LSP and, as ablation for the self-rewarding regularization, LSP-Zero to model that was trained with RL from Alpaca data (Taori et al., 2023). The goal of that experiment is to analyze how much of performance of data-based training can be recovered with self-play alone in scenario when RL data is fully missing. In that setting, all Figure 2 Comparison of win-rates of models trained with RL (GRPO, backed by data, yellow bars), LSP-Zero & LSP (no data, red and blue bars, respectively) against the base model (Llama-3.2-3B-Instruct) on AlpacaEval benchmark. All algorithms improve upon the base model on the overall benchmark (right-most bars). The overall win rates are, GRPO 40.9%, LSP-Zero 40.1%, LSP 40.6%. The solid gray line is the win-rate of the base model against itself (it wins, ties, and looses against itself with equal probability). models are initialized with the base model (Llama-3.2-3B-Instruct). Then, to study the effectiveness self-play as the next stage in training that follows data-based RL, we evaluate model that was initialized from data-based RL model and further calibrated with self-play. Thus, in that experiment, we initialize our model 6 with the model we obtained from data-based RL training in the first experiment. All algorithms are evaluated with model sampling at τ = 0.01 temperature. In order to deliver interpretable results, as baselines, we compared our algorithms to From base model. the base model itself (Llama-3.2-3B-Instruct, Dubey et al. (2024); Meta (2024)), as well as to one that we fine-tuned with traditional RL for LLMs that we instantiate with Group-Relative Policy Optimization (Shao et al., 2024, GRPO), whose implementation we obtained from HuggingFaces TRL library (von Werra et al., 2020), on Alpaca training data (Taori et al., 2023). For all algorithms (GRPO, LSP-Zero, and LSP), as reward model, we used Skywork-Reward-V2-Llama-3.2-3B (Liu et al., 2025)3. For each these algorithms, we calculate their win-rates against Llama-3.2-3B-Instruct on AlpacaEval (with GPT-4o as judge), including results on each individual dataset, which we report in Figure 2. The results show that LSP-Zero and LSP effectively improve upon the base model, overall, in spite of not having used any training data, and achieve similar overall result to GRPO. The experiment also reveals greater effectiveness of LSP than that of LSP-Zero which we thus consider the main method proposed by this paper. It is also worth noting that in some tasks, such as Vicunaa dataset that specializes in conversational, open-ended instructionsLSP-Zero and LSP ended up performing significantly better than the base model and GRPO. This could be expected given that prompts generated by the challenger have such character (see Box 2). From RL model. Now, we initialize our model with the one trained with data-based RL (GRPO) in the previous experiment and train it with LSP-Zero and LSP. Then, we calculate its win-rates against Llama-3.23B-Instruct and compare them to the RL model we started from. Th results from Figure 3 show significant improvement of LSP (from 40.9% to 43.1%) of overall win-rate (against Llama-3.2-3B-Instruct) after further self-play training. Similarly to the previous experiment, the biggest gains can be observed in Vicuna where our algorithms improve GRPOs win-rate from 28.7% to 36.3% (LSP-Zero) and 46.3% (LSP). However, in this Figure 3 Comparison of win-rates of the model trained with LSP-Zero and LSP (no data, red and blue bars) against the model it was initialized from - the model trained with GRPO (backed by data, yellow bars) on AlpacaEval benchmark. LSP performs better than GRPO overall and significantly better in Vicuna. The specific win rates are, GRPO 40.9%, LSP-Zero 40.0%, and LSP 43.1%. The solid gray line is the win-rate of the base model against itself. stage, LSP-Zero has negative net effect on the overall model performance further justifying the self-rewarding regularization. Also, admittedly, LSP hinders the performance on some tasks, such as Koala, which is known to consist of chatbot user-type queries. This is likely consequence of our algorithm that biases its training 3We note that in much of our algorithm development we utilized OpenAssistants reward-model-deberta-v3-large-v2 but we found that, for all methods, the improved evaluation reward did not translate into improved AlpacaEval evaluation scores, while Skywork-Reward-V2 turned out to be very reliable. 7 queries to structured, orderly style (see Boxes 2 & 3). Thus, making self-play robust by generating more diverse queries without damaging the model quality is an important avenue for future work."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we have offered framework of perpetual improvement of language model and the self-generated data that it learns from, as well as designed practical algorithmLanguage Self-Play (LSP)that enacts the framework given pre-trained LLM. Our experiments confirm that LSP-Zero and LSP algorithms can improve pre-trained LLMs with no access to training data, especially on conversational tasks. While our experiments were conducted with preferential reward models, our algorithms can be just as well, if not easier, applied to problems with verifiable rewards. In the case when verifiable ground-truth rewards are not readily available, the upper bound of the LSP models performance is related to the judgement quality of the utilized reward model, as well as bounded by the human knowledge about the physical world. We believe, however, that such self-play framework has great potential to expand that knowledge once AI becomes embodied and capable of collecting its own empirical data."
        },
        {
            "title": "References",
            "content": "Pieter Abbeel and Andrew Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the twenty-first international conference on Machine learning, page 1, 2004. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Yu Bai and Chi Jin. Provable self-play algorithms for competitive reinforcement learning. In International conference on machine learning, pages 551560. PMLR, 2020. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Dan Calian, Gregory Farquhar, Iurii Kemaev, Luisa Zintgraf, Matteo Hessel, Jeremy Shar, Junhyuk Oh, András György, Tom Schaul, Jeffrey Dean, et al. Datarater: Meta-learned dataset curation. arXiv preprint arXiv:2505.17895, 2025. Pengyu Cheng, Yong Dai, Tianhao Hu, Han Xu, Zhisong Zhang, Lei Han, Nan Du, and Xiaolong Li. Self-playing adversarial language game enhances llm reasoning. Advances in Neural Information Processing Systems, 37: 126515126543, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023. Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018a. Jakob Foerster, Gregory Farquhar, Maruan Al-Shedivat, Tim Rocktäschel, Eric Xing, and Shimon Whiteson. Dice: The infinitely differentiable monte carlo estimator. In International Conference on Machine Learning, pages 15291538. PMLR, 2018b. 8 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022. Kazuki Irie, Imanol Schlag, Róbert Csordás, and Jürgen Schmidhuber. modern self-referential weight matrix that learns to modify itself. In International Conference on Machine Learning, pages 96609677. PMLR, 2022. Nicola Jones. The ai revolution is running out of data. what can researchers do? Nature, 636(8042):290292, 2024. Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on robot learning, pages 651673. PMLR, 2018. Louis Kirsch and Jürgen Schmidhuber. Self-referential meta learning. In First Conference on Automated Machine Learning (Late-Breaking Workshop), 2022. Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):8490, 2017. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/ tatsu-lab/alpaca_eval, 5 2023. Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, et al. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352, 2025. Stephen McAleer, John Banister Lanier, Kevin Wang, Pierre Baldi, Roy Fox, and Tuomas Sandholm. Self-play psro: Toward optimal populations in two-player zero-sum games. arXiv preprint arXiv:2207.06541, 2022. Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. Advances in neural information processing systems, 30, 2017. Meta. Llama-3.2-3b-instruct, 2024. https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. OpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya, Vineet Kosaraju, Peter Welinder, Ruben DSa, Arthur Petron, Henrique Pinto, et al. Asymmetric self-play for automatic goal discovery in robotic manipulation. arXiv preprint arXiv:2101.04882, 2021. OpenAssistant. reward-model-deberta-v3-large-v2. https://huggingface.co/OpenAssistant/ reward-model-deberta-v3-large-v2, 2025. Accessed: 2025-08-13. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Ajay Patel, Colin Raffel, and Chris Callison-Burch. Datadreamer: tool for synthetic data generation and reproducible llm workflows. arXiv preprint arXiv:2402.10379, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 9 Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019. Jürgen Schmidhuber. Gödel machines: Fully self-referential optimal universal self-improvers. In Artificial general intelligence, pages 199226. Springer, 2007. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 18891897. PMLR, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. Advances in Neural Information Processing Systems, 37:4300043031, 2024. Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. David Silver and Richard Sutton. Welcome to the era of experience. 2025. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550 (7676):354359, 2017. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. arXiv preprint arXiv:2211.04325, 2022. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/ huggingface/trl, 2020. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: World models for physical robot learning. In Conference on robot learning, pages 22262240. PMLR, 2023. Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Selfrewarding language models. arXiv preprint arXiv:2401.10020, 3, 2024. Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting language models. arXiv preprint arXiv:2506.10943, 2025."
        },
        {
            "title": "A Example Plays",
            "content": "Box 4: Challenger-Generated Prompts and Solver-Generated Responses 500 iterations. What is the typical difference in approach between Montessori classroom and traditional classroom setup when it comes to student-led project-based learning? In Montessori classroom, student-led project-based learning is core approach to education, whereas in traditional classrooms, its often teacher-led. Here are the main differences: Montessori Approach: 1. Self-Directed Learning: Montessori students are encouraged to take ownership of their learning process, making choices about topics, methods, and pace. 2. Project-Based Learning: Students work on self-chosen projects that align with their interests and learning objectives, promoting in-depth exploration and mastery. 3. Collaboration: Students work in small groups, promoting peer-to-peer learning, discussion, and mutual respect. 4. Flexible Time: Students have the freedom to work at their own pace, with minimal supervision, allowing them to take charge of their time management. 5. Emphasis on Process over Product: The focus is on the learning process, not just the end product, encouraging students to reflect on their progress and adjust their approach as needed. Traditional Classroom Approach: 1. Teacher-Led Instruction: The teacher plans and delivers the lesson, setting the agenda, pace, and content for the students. 2. Project-Based Learning with Teacher Guidance: Teachers assign projects, provide guidance, and facilitate discussions, but students may have limited autonomy in their choices. 3. Structured Time: Students follow set schedule, with teachers monitoring their progress and providing direct instruction when needed. 4. Emphasis on Assessment: The focus is on evaluating student performance and understanding through standardized tests, quizzes, and assessments. 5. Linear Approach: The learning process is typically structured and linear, with students progressing through set curriculum. Key differences between the two approaches: * Autonomy: Montessori students have more control over their learning, while traditional classrooms have more structured environment. * Flexibility: Montessori students can adjust their pace and approach, whereas traditional classrooms have more rigid structure. * Emphasis: Montessori emphasizes the process, while traditional classrooms focus on the end product. While both approaches have their strengths and weaknesses, Montessoris student-led approach is designed to foster independence, creativity, and love of learning, whereas traditional classrooms prioritize teacher-led instruction and standardized evaluation. 11 1000 iterations. Follow these steps to build functional submarine that can dive to depth of 100 meters underwater while using only scuba tank that holds 12 liters of air. The submarine must be able to withstand water pressure at that depth without collapsing, and it should surface automatically after 20 minutes. Building functional submarine that can dive to 100 meters while using 12-liter scuba tank is highly challenging, if not impossible, with current technology. However, can provide hypothetical design that could potentially meet your requirements. Please note that this design is purely theoretical, and actual construction and testing would require extensive expertise and resources. Design Considerations 1. Pressure Hull: The submarines pressure hull must be strong enough to withstand water pressure at 100 meters. The pressure at this depth is approximately 1000 times greater than at sea level. Well use cylindrical design with diameter of 1 meter and length of 5 meters. 2. Material: Well use high-strength steel for the pressure hull, such as HY-100 or similar materials. 3. Scuba Tank: Well use the 12-liter scuba tank as the primary air supply. 4. Power Source: Well use electric motors powered by rechargeable batteries. 5. Control Systems: Well use simple control system with buttons and display to control the descent, ascent, and surface. 6. Life Support System: Well include basic life support system, but it wont be sufficient for an extended dive. Hypothetical Design: Heres simplified design: Pressure Hull: * Material: High-strength steel (1 meter diameter 5 meters long) * Thickness: 6 mm * Weight: approximately 200 kg Scuba Tank: * 12 liters of compressed air * Tank pressure: 200 bar (approx.) * Valve system to regulate air flow Power Source: * 2 rechargeable lithium-ion batteries (12V, 10 Ah) * Electric motors (2 DC, 10 kW each) * Power electronics and control system Control Systems: * Simple control panel with buttons for descent, ascent, and surface * Display showing depth, pressure, and battery level Life Support System: * Basic oxygen generator (not sufficient for extended dives) * Carbon dioxide scrubber (not sufficient for extended dives) Autonomous Ascent: * The submarine will use pressure hulls buoyancy to ascend when the air supply is depleted or when the time limit is reached. Calculations: * Weight of the pressure hull: 200 kg * Volume of scuba tank: 12 liters * Depth to reach: 100 meters 12 1500 iterations. Follow the instructions to create new programming language that is combination of Python and Haskell, with unique syntax and semantics, and can compile and run on 64-bit x86 processor, using only the standard library and without any external dependencies, in under 1000 characters. must point out that creating full-fledged programming language in under 1000 characters is extremely challenging, if not impossible. However, can provide minimal example of language that combines elements of Python and Haskell, along with basic compiler, within the given constraints. Language: \"Pyhaskell\" (yes, its real name now) Syntax: Inspired by Pythons concise syntax and Haskells type inference. pyhaskell type Val = Int String None expr = Expr expr = Lit Val of Val App Expr Expr Var Str exprLit = Lit Val of Val exprApp = App Expr Expr exprVar = Var Str app = App exprLit exprLit Compiler: pyhaskell import re def compile(expr): if isinstance(expr, Var): return fvar expr elif isinstance(expr, exprLit): return flit exprLit elif isinstance(expr, exprApp): return fapp exprApp else: raise ValueError(Invalid expression) Example usage: pyhaskell result = app var (app lit 1) print(compile(result)) # Output: app lit 1 This implementation is extremely simplified and lacks error handling, type checking, and optimization. However, it demonstrates the basic structure of Pyhaskell and its compatibility with Pythons standard library. Please note that creating functional programming language in few hundred characters is not feasible task, and this example is more of proof of concept than fully functional language."
        }
    ],
    "affiliations": [
        "Meta Superintelligence Labs",
        "UC Berkeley"
    ]
}