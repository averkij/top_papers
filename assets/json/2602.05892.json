{
    "paper_title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
    "authors": [
        "Han Li",
        "Letian Zhu",
        "Bohan Zhang",
        "Rili Feng",
        "Jiaming Wang",
        "Yue Pan",
        "Earl T. Barr",
        "Sarro Federica",
        "Zhaoyang Chu",
        "He Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 1 ] . [ 2 2 9 8 5 0 . 2 0 6 2 : r CONTEXTBENCH: Benchmark for Context Retrieval in Coding Agents Han Li1 Letian Zhu1 Bohan Zhang1 Rili Feng1 Jiaming Wang1 Yue Pan2 Earl T. Barr2 Federica Sarro2 Zhaoyang Chu2 He Ye2 1Nanjing University 2University College London https://contextbench.github.io/"
        },
        {
            "title": "Abstract",
            "content": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce CONTEXTBENCH, process-oriented evaluation of context retrieval in coding agents. CONTEXTBENCH consists of 1,136 issueresolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using CONTEXTBENCH, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (The Bitter Lesson of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. CONTEXTBENCH augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks."
        },
        {
            "title": "Introduction",
            "content": "Large Language Model (LLM) coding agents, exemplified by SWE-agent [26], OpenHands [23], and Agentless [24] have recently demonstrated strong proficiency in real-world software engineering tasks, including bug fixing [12, 18], feature implementation [20, 8], and interaction with terminal-based execution environments [15]. These agents are autonomous systems capable of multi-step reasoning and tool use, enabling them to solve complex, repository-scale tasks in real-world development scenarios. Limitations of Existing Benchmarks. Advancements in this area are typically assessed on benchmarks such as SWE-bench [12] and SWE-bench Pro [9], which curate tasks from realworld GitHub issues and require agents to submit patches validated by test suites. However, these benchmarks prioritize end-to-end task success rates (e.g., Pass@k) and ignore how coding agents arrive at their solutions. In particular, they overlook the evaluation of context retrieved from large codebases, which underpins the agents reasoning. Without verifying intermediate contexts, high success rates may result from trial-and-error or overfitting to specific test cases, yielding coding agents that are unreliable in practical scenarios. We therefore argue for context-based evaluation system that opens the black box of software tasks and addresses the key question: How do LLM agents retrieve and use critical code context when resolving software engineering tasks? These authors contributed equally as co-second authors. Corresponding authors. Email: {he.ye, zhaoyang.chu.25}@ucl.ac.uk Preprint. (a) Coding Agent Comparison (b) LLM Comparison Figure 1: Radar plots comparing context retrieval performance on CONTEXTBENCH. (a) Coding agent comparison and (b) LLM comparison, both evaluated at file-, block-, and line-level granularity in terms of precision, recall, and F1. Across both LLMs and agents, higher recall is consistently favored over precision. CONTEXTBENCH: Beyond End-to-End Benchmarking. To address this question, we develop semi-automated pipeline for curating Gold Contexts, defined as standard sets of repository artifacts that expert developers identify as necessary to resolve given issue. We begin by sampling high-quality tasks from four widely used benchmarks, i.e., SWE-bench Verified [12], Multi-SWEbench [29], SWE-PolyBench PB500 [20], and SWE-bench Pro [9]. After deduplication via rulebased matching and embedding-based similarity detection, we select challenging tasks based on three difficulty metrics, i.e., agent solvability; edit scope; and edit dispersion based on gold patches. For each selected task, human experts iteratively annotate and refine the required context in human-in-the-loop process, ensuring compact and sufficient gold contexts for issue resolution. Building on this pipeline, we introduce CONTEXTBENCH, an extensive benchmark designed to evaluate the context retrieval ability of coding agents in software engineering tasks, enabling fine-grained analysis of agent behavior beyond conventional end-to-end issue resolution metrics. CONTEXTBENCH comprises 1,136 issue resolution tasks from 66 code repositories across 8 programming languages. These tasks are annotated with 522,115 lines of human-verified gold contexts covering 23,116 classes and functions within 4,548 files. To facilitate efficient evaluation, we also identify Lite subset of 500 tasks based on task difficulties. We further introduce an automated evaluation framework with systematic suite of metrics to assess how effectively LLM agents retrieve code contexts during problem solving. These metrics evaluate code context recall and precision at different stages of task resolving, complementing the final success rate. Specifically, we instrument the agents trajectory to record all code regions it inspects. We then utilize the tree-sitter3 tool to parse the corresponding repository and map both gold and agent contexts onto shared coordinate system that captures their file paths, AST blocks, and line ranges. Based on this alignment, we compute recall, precision, and F1 using interval overlap at three granularities: file, block, and line levels. Figure 1 summarizes the results for four LLMs and five coding agents on CONTEXTBENCH, highlighting systematic differences in retrieval behavior across models and agents. Take-Aways. We benchmark four state-of-the-art LLMs, including GPT-5 [22], Claude Sonnet 4.5 [1], Gemini 2.5 Pro [7], and Devstral 2 [17], as well as five widely used coding agents, including 3https://tree-sitter.github.io/tree-sitter/ 2 mini-SWE-agent, SWE-agent [26], OpenHands [23], Agentless [24], and Prometheus [6]. Our analysis reveals several key findings: ❶ Sophisticated scaffolding does not necessarily lead to better context retrieval performance. Across five coding agents, more complex retrieval scaffolds do not consistently outperform simple baseline, as shown in Figure 1a, suggesting potential overengineering and echoing The Bitter Lesson of AI research. ❷ CONTEXTBENCH is challenging for state-of-the-art LLMs. State-of-the-art LLMs struggle to retrieve effective code contexts, often covering relevant information while introducing substantial noise that undermines retrieval precision, as shown in Figure 1b. ❸ Recall is favored over precision in LLM context retrieval. All evaluated LLMs retrieve broad context to maximize coverage, introducing substantial noise and yielding limited precision and F1 gains. ❹ Balanced retrieval improves performance and cost efficiency. Models that balance retrieval frequency and context granularity achieve higher Pass@1 at lower cost, while aggressive retrieval mainly increases token consumption. ❺ Significant gaps exist between retrieved and utilized context. Agents often inspect gold-relevant code but fail to retain or use it in final patch generation, highlighting consolidation as key bottleneck. Contributions. To summarize, our work provides three key contributions as follows: We construct CONTEXTBENCH, containing 1,136 full instances for evaluating context retrieval in coding agents, each of which provides expert-annotated, verified gold context for assessing intermediate states of issue resolution. CONTEXTBENCH moves beyond final success-rate evaluation by enabling dynamic, processoriented assessment across issue resolution stages through tracking and comparing agentretrieved and gold contexts, providing an important intermediate signal for rewarding agent behavior. We evaluate four frontier LLMs and five coding agents on context retrieval recall and precision, yielding new insights into how LLMs and coding agents retrieve code context during problem solving."
        },
        {
            "title": "Coding Agents",
            "content": "As illustrated in Figure 2, we present CONTEXTBENCH, benchmark for evaluating the agents context retrieval ability in software engineering tasks, which is curated through three key steps: (1) Task Deduplication. We pool tasks from multiple issue resolution benchmarks and eliminate exact and near-duplicate tasks through both rule-based and embedding-based detection. (2) Task Selection. We select challenging tasks using three difficulty metrics, i.e., agent solvability, the scope and dispersion of edits in the ground-truth patch, prioritizing tasks that are complex or remain unsolved by existing agents. (3) Expert Annotation. For each selected task, expert developers iteratively annotate gold contexts in human-in-the-loop process by tracing essential code dependencies from the ground-truth patches. We then validate each annotated context by evaluating whether an LLM (e.g., GPT-5), conditioned solely on the context, can generate patch that passes the official test suite or whether the context achieves high inter-annotation consistency. Based on this pipeline, we collect 1,136 task instances with verified gold contexts across 66 repositories and 8 programming languages. 2.1 Step 1: Task Deduplication We curate CONTEXTBENCH by systematically sampling tasks from four widely used issue resolution benchmarks, i.e., SWE-bench Verified [12] (500 tasks), Multi-SWE-bench [29] (1632 tasks), SWEPolyBench PB500 [20] (500 tasks), and SWE-bench Pro [9] (1865 tasks), totaling 4,497 tasks. These benchmarks offer high-quality task sources curated from real-world GitHub repositories and issues, with strong reproducibility guarantees and multilingual coverage across Python, Java, JavaScript, TypeScript, Go, Rust, C, and C++. The source benchmarks originate from GitHub and may contain overlapping tasks across repositories. To ensure uniqueness, we apply rule-based matching on task metadata (e.g., repository names and issue identifiers) to eliminate exact duplicates, yielding 3,981 tasks. We further compute embeddingbased semantic similarity between issue descriptions and remove near-duplicates above predefined threshold (e.g., 0.9). Finally, we manually inspect borderline cases flagged by the threshold to avoid false removals, resulting in 3,100 unique tasks. This deduplication step ensures that each task 3 Figure 2: An overview of the CONTEXTBENCH construction pipeline. CONTEXTBENCH is curated through three key steps: (1) Task Deduplication removes exact and near-duplicate tasks from multiple issue resolution benchmarks using rule-based and embedding-based detection. (2) Task Selection identifies challenging tasks based on agent solvability and the scope and dispersion of edits in ground-truth patches. (3) Expert Annotation employs human-in-the-loop procedure in which expert developers trace code dependencies to construct gold contexts, which are validated through LLM-based patch generation and inter-annotator agreement checks. in CONTEXTBENCH corresponds to distinct real-world issue, minimizing bias from redundant evaluations. More implementation details are provided in Section A. 2.2 Step 2: Task Selection To ensure that CONTEXTBENCH evaluates challenging and non-trivial context retrieval scenarios, we rank candidate tasks using three complementary difficulty metrics, i.e., agent solvability, the scope and dispersion of edits in the ground-truth patch. Agent Solvability. We automatically crawl results from the public leaderboards of the source benchmarks and use them as coarse proxy for task difficulty. For each task, we record the number of agents that successfully resolve it and prioritize tasks that remain unsolved or have been solved by only few agents. Edit Scope. For each issue task, we analyze its ground-truth patch and record the number of modified files. We prioritize tasks with larger edit scopes, as they typically require more extensive context retrieval and involve more complex reasoning patterns. Edit Dispersion. We measure the distribution of code edits across files and directories in the repository. To quantify this, we compute the average structural distance between edited regions in the repository tree parsed by Tree-Sitter. We prioritize tasks with highly dispersed edits that span multiple modules or distant files, since these cases involve broader contextual dependencies over scattered code regions. Based on these difficulty metrics, we select 1,500 candidate tasks. We then manually review them to remove cases that appear challenging by metrics but are semantically trivial, such as large-scale variable renaming across multiple files or bulk formatting changes that do not affect program logic, resulting in final set of 1,136 tasks. More implementation details are provided in Appendix B. 2.3 Step 3: Expert Annotation The annotation is conducted collaboratively by six authors of this paper and group of engaged expert developers over four months. To ensure quality and consistency, we adopt human-in-theloop procedure where annotators iteratively construct and refine gold contexts for 1,136 selected tasks, validating them through LLM-based patch generation and inter-annotator agreement checks. All annotators have over three years of software development experience working with large-scale codebases. To ensure the annotators can proficiently mark the data, we provide detailed guidelines 3https://github.com/langchain-ai/langchain/pull/4009 4 Figure 3: An illustrative example of the human-verified gold context. This example is derived from the issue task associated with Pull Request #4009 in the langchain-ai/langchain repository. The blue, red, and green arrows represent the manually annotated contextual dependencies associated with variable, function, and class invocations, respectively. and examples that illustrate how to trace code dependencies, identify relevant artifacts, and record annotations in standardized format. We also specify clear criteria and task requirements to guide annotators throughout the process. Please refer to the details of our annotation process in Section F. Context Annotation. The annotation process begins with the ground-truth patch for each issue task. Annotators follow the provided guidelines to trace essential code dependencies and semantically related artifacts originating from the modified regions. For each edited location, they inspect its function and class invocations, inheritance relations, and related control-flow and data-flow paths. Annotators also review the surrounding code within the same file or module to identify components that are semantically relevant to the modification. Through this tracing process, annotators are instructed to ensure that the recorded contexts are sufficient for issue resolution while remaining as compact as possible by excluding redundant or irrelevant code. Context Verification. To validate whether each annotated context is sufficient to resolve its corresponding issue, we utilize state-of-the-art LLM (e.g., GPT-5) as an evaluator. For each task, the model is conditioned solely on the annotated context and prompted to generate multiple candidate patches (e.g., 5 attempts). We then consider an annotated context sufficient if at least one generated patch successfully passes the official test suite, including both fail-to-pass and pass-to-pass test cases. Detailed rationale and protocol are provided in Section G. Context Refinement. When an annotated context passes verification, we assign it to different annotator, distinct from the one who performed the initial annotation, to conduct compactness checking. This secondary annotator reviews the verified context to identify and remove any code segment that is redundant or irrelevant to issue resolution. Subsequently, the two annotators jointly review the updated context, resolve any disagreement, and finalize consensus version of the gold context. If an annotated context fails verification, we conduct two additional rounds of annotation and verification, each handled by different annotator. In each round, the new annotator identifies the context by independently tracing code dependencies and then validates it through the LLM-based verification procedure described earlier. This iterative process accounts for the possibility that some tasks are inherently difficult for LLMs to solve, not necessarily due to incomplete context annotation. After the two additional rounds, all three annotators jointly inspect the resulting contexts, reconcile differences, and refine them into final context version. Consequently, the annotation process produces gold context represented by compact set of code regions that are essential for resolving the issue and generating the corresponding patch, as illustrated in Figure 3. This collaborative refinement ensures that each gold context in CONTEXTBENCH is both semantically sufficient for issue resolution and as concise as possible. 2.4 CONTEXTBENCH Statistics Figure 4 illustrates the data filtering pipeline, showing how tasks are processed through deduplication, selection, and expert annotation. This pipeline produces CONTEXTBENCH, repository-level benchmark featuring human-verified gold contexts. As summarized in Table 1, CONTEXTBENCH comprises 66 repositories and 1,136 issue tasks across eight programming languages, with context 5 Table 1: Statistics of CONTEXTBENCH. repository-level benchmark spanning 8 programming languages and introducing human-verified gold contexts to expose intermediate context retrieval signals missing from final task resolution rate evaluation. Language #Repo #Task Context Statistics #Block #File #Line Python Java JavaScript TypeScript Go Rust C++ Total 20 6 9 8 7 9 3 4 66 512 57 153 119 104 63 68 60 1,136 1,520 262 819 537 679 272 250 209 4, 6,714 3,030 3,949 1,106 3,000 1,842 1,591 1,884 115,122 49,057 87,907 40,621 71,596 50,402 62,300 45,110 23,116 522,115 Figure 4: The data flow during the CONTEXTBENCH construction pipeline. We construct CONTEXTBENCH step by step, starting from task deduplication and selection to gold context annotation. annotations provided at the file, block, and line levels, encompassing 4,548 files, 23,116 blocks, and 522,115 lines of code. These gold contexts enable systematic evaluation of intermediate context retrieval beyond final task resolution rates. To facilitate efficient evaluation, we further construct Lite subset of 500 tasks, selected based on coding agent solvability and the scope and dispersion of edits in the ground-truth patch. 2.5 CONTEXTBENCH Evaluation Protocol To evaluate the context retrieval ability of coding agents, we instrument their execution to trace the code regions they inspect during issue resolution and subsequently compare their retrieved contexts with human-verified gold contexts. Agent Context Tracing. For each issue task, we instruct the agent to perform the full resolution process while automatically recording every accessed code segment, including the corresponding file path, line range, and content. We then apply regular expressions to extract these regions and standardize them into structured <PATCH CONTEXT> blocks, which serve as intermediate checkpoints representing the agents context snapshots throughout its execution trajectory. Moreover, we require the coding agent to explicitly declare, before submitting its final patch, the code context it considers essential for generating the patch. This final declared context may not completely overlap with the intermediate checkpoint contexts, as it reflects the agents own judgment of what information is most relevant to the patch. More implementation details are provided in Section C. Agent Context Evaluation. We then parse the entire repository with the tree-sitter tool to construct unified structural coordinate system, onto which both the gold and agent contexts are mapped for comparison. The two context sets are aligned across three granularities: (i) the file level, by matching file paths; (ii) the block level, by aligning Abstract Syntax Tree (AST) nodes at definition level, including classes, functions, interfaces, and trait declarations (detailed in Section E); and (iii) the line level, by comparing line ranges using their byte offsets within files. This structured alignment enables fine-grained comparison between the gold and agent contexts. Based on this alignment, we evaluate recall, precision, F1, and process-level context dynamics (e.g., efficiency, redundancy, and usage) at the file, block, and line levels, with detailed definitions provided in Section H. 6 Table 2: Performance of different coding agents on context retrieval. We evaluate five coding agents, including basic baseline and four state-of-the-art scaffolds, on CONTEXTBENCH Lite using an advanced LLM (i.e., GPT-5). We report recall, precision, and F1 at the file, block, and line levels, along with the Pass@1 issue resolution rate. The results suggest that more sophisticated scaffolding does not necessarily lead to better context retrieval performance, indicating potential over-engineering in current agent designs, which echoes The Bitter Lesson of AI research. Coding Agent File-Level Block-Level Line-Level Pass@1 Recall Precision F1 Recall Precision F1 Recall Precision F1 mini-SWE-Agent 0.682 0.609 Agentless 0.726 SWE-agent 0.733 OpenHands 0.717 Prometheus 0.709 0.352 0.537 0.400 0.336 0.634 0.645 0.390 0.328 0.544 0.625 0.463 0.505 0.403 0.646 0.369 0.344 0.312 0.283 0.258 0.375 0.606 0.242 0.461 0.285 0.476 0.190 0.472 0.285 0.584 0.301 0.318 0.228 0.203 0. 0.312 0.376 0.208 0.130 0.231 0.472 0.452 0.490 0.490 0."
        },
        {
            "title": "3 Can Coding Agents Retrieve Effective Contexts to Resolve Issues?",
            "content": "To evaluate coding agents capability in retrieving effective contexts for software engineering tasks, we investigate the following Research Questions (RQs): RQ1: Benchmarking Coding Agents. How effectively do coding agents retrieve contexts from large-scale repositories when resolving issues using state-of-the-art LLM (i.e., GPT-5)? RQ2: Benchmarking Large Language Models. How effectively do LLMs retrieve code contexts for issue resolution when equipped with standard coding agent framework (i.e., mini-SWEagent)? RQ3: Analysis of Context Retrieval Patterns. What characteristic patterns emerge when LLM agents retrieve code contexts? RQ4: Analysis of Context Retrieval Dynamics. What dynamics emerge in LLM agents context retrieval processes during task execution? RQ5: Analysis of Gold Context Robustness. How robust are the human-verified gold contexts? 3.1 RQ1: Benchmarking Coding Agents Table 2 benchmarks five coding agents that are open-sourced, actively maintained, and representative of major paradigms in context retrieval. Specifically, we assess standard baseline that retrieves context through simple bash commands (i.e., min-SWE-agent [26]) and four state-of-the-art scaffolds, including embedding-based semantic retrieval (i.e., Agentless [24]), file-system navigation via specialized interfaces (i.e., SWE-agent [26], OpenHands [23]), and graph-based repository retrieval (i.e., Prometheus [6]). All agents are evaluated with an advanced LLM backbone (i.e., GPT-5 [22]). Detailed experiment settings are listed in Section C.2. The results reveal distinct context retrieval behaviors across coding agents, driven by differences in their tool interfaces and control policies. Some agents (e.g., Prometheus) favor broad exploration and retrieve larger contexts, achieving higher recall but lower precision, whereas others adopt more conservative strategies. Yet, despite differences in retrieval strategies, their overall context metrics often fall below those of the baseline mini-SWE-agent. This finding suggests that current agent frameworks may be over-engineered for context retrieval. In practice, simple iterative search and inspection through basic shell commands can already recover substantial portion of the necessary context, whereas more complex orchestration introduces additional overhead and may even degrade retrieval performance. Answer to RQ1: Sophisticated agent scaffolding does not necessarily improve context retrieval performance, revealing potential over-engineering in current designs, which echoes The Bitter Lesson of AI research. 7 Table 3: Performance of various LLMs on code context retrieval. We assess four state-of-the-art LLMs on CONTEXTBENCH Lite with standard coding agent scaffolding, reporting recall, precision, and F1 at the file, block, and line levels, along with the Pass@1 issue resolution rate. The results reveal that they still struggle with effective context retrieval, overlooked by prior end-to-end benchmarking. LLM File-Level Block-Level Line-Level Pass@1 Recall Precision F1 Recall Precision F1 Recall Precision F1 GPT-5 0.682 Claude Sonnet 4.5 0.720 0.587 Gemini 2.5 Pro 0.660 Devstral 2 0.709 0.665 0.752 0.693 0.634 0.645 0.624 0.631 0.600 0.393 0.615 0. 0.369 0.449 0.632 0.576 0.375 0.606 0.420 0.588 0.403 0.313 0.422 0.404 0.301 0.374 0.529 0.485 0.312 0.344 0.311 0.332 0.472 0.530 0.364 0.402 3.2 RQ2: Benchmarking Large Language Models Table 3 benchmarks four state-of-the-art LLMs, including GPT-5 [22], Claude Sonnet 4.5 [1], Gemini 2.5 Pro [7], and Devstral 2 [17], in retrieving relevant code contexts from large-scale codebases when resolving issues using the standard coding agent scaffold (i.e., mini-SWE-agent [26]). Detailed experiment settings are listed in Section C.2. The results suggest that state-of-the-art LLMs still face challenges in retrieving effective context during issue resolution. For instance, their block-level F1 scores fall below 0.45, while line-level F1 scores remain below 0.35. Moreover, higher recall does not necessarily indicate better context retrieval performance, as clear trade-off exists between recall and precision. Some LLMs tend to expand retrieved contexts aggressively to achieve higher recall, while introducing excessive irrelevant content that results in lower precision. For example, GPT-5 achieves higher recall at both the block and line levels but sacrifices precision, leading to lower overall F1 and, consequently, reduced issue resolution performance compared to Claude Sonnet 4.5. We also observe positive correlation between context recall and downstream task success. LLM agents that cover larger portion of gold contexts tend to achieve higher Pass@1 scores. This finding reinforces the central premise of CONTEXTBENCH: context retrieval quality is not merely diagnostic auxiliary metric but meaningful predictor of end-to-end performance. However, the observed recall-precision trade-off suggests that gains from broader retrieval must be balanced with more accurate evidence selection; otherwise, excessive context can introduce noise and impair the agents reasoning and issue resolution performance. Answer to RQ2: State-of-the-art LLMs struggle with effective code context retrieval, often covering relevant information at the cost of introducing substantial noise and sacrificing retrieval precision. 3.3 RQ3: Analysis of Context Retrieval Patterns Table 4 compares the context retrieval patterns across LLM agents on CONTEXTBENCH, measured by the number of retrieval steps per instance, lines retrieved per step, total lines retrieved per instance, and cost per instance. We can observe that LLMs adopt different context retrieval strategies, trading off retrieval steps against retrieval granularity. GPT-5 retrieves context in the fewest rounds (5.87), with substantially more lines per step (119.29 lines of code on average), whereas Devstral 2 performs the most retrieval rounds (22.16) while retrieving much smaller contexts per step (11.98 lines of code, approximately 10% of GPT-5). Notably, Claude Sonnet 4.5 adopts more balanced strategy, combining moderate retrieval rounds with moderate context size. Recall in Table 3, this balanced behavior aligns with its strong performance, achieving the best line-level F1 score and Pass@1 resolution rate. The last column of Table 4 reports the average cost per instance for context retrieval. Devstral 2 is the most expensive due to more retrieval rounds that produce costly output tokens. This suggests that reducing the number of queries is an effective way to lower LLM cost. 8 Table 4: Context retrieval patterns of different LLM agents. LLMs vary in how they balance retrieval rounds and context granularity, and those that maintain more balanced strategy tend to achieve higher line-level retrieval accuracy and stronger end-to-end issue resolution performance. LLM Avg. Steps Per Instance Avg. Lines Per Step Avg. Cost ($) Per Instance GPT-5 Claude Sonnet 4.5 Gemini 2.5 Pro Devstral 2 5.87 14.38 7.57 22.16 119.29 29.74 26.29 11.98 0.45 0.76 0.38 0. Answer to RQ3: LLMs differ in how they balance retrieval rounds and context granularity, and models with more balanced strategies achieve better line-level retrieval quality and end-to-end resolution performance. 3.4 RQ4: Analysis of Context Retrieval Dynamics Beyond evaluating the final aggregated contexts submitted before patch generation, we further analyze how coding agents retrieve context step by step during execution, using the metrics detailed in Section H. As shown in Table 5, LLM agents exhibit different context retrieval dynamics. Among it achieves the highest them, Claude Sonnet 4.5 demonstrates particularly unique behavior: efficiency, capturing relevant gold contexts earlier than other models, but at the cost of substantial redundancy. This behavior may stem from its exploration strategy, in which the model repeatedly revisits previously accessed files to maintain coverage of potentially relevant code regions, leading to redundant retrieval of overlapping content. We also observe consistent context usage drop across all evaluated agents, indicating that considerable portion of the retrieved context during execution is ultimately unused in the final patch generation. Consequently, many gold contexts successfully retrieved at intermediate steps are not incorporated into the final reasoning process, leading to issue resolution failures despite retrieval success. Notably, Gemini 2.5 Pro and Devstral 2 exhibit the most severe context loss, which likely explains their relatively low issue resolution rates. Answer to RQ4: LLM agents exhibit diverse context retrieval dynamics, revealing inherent trade-offs between efficiency and redundancy. Moreover, the significant drop in context usage hinders effective issue resolution. 3.5 RQ5: Analysis of Gold Context Robustness Our gold contexts are human-annotated based on gold patch. However, in practice, single issue may admit multiple semantically equivalent but syntactically different patches, raising the question of whether the collected gold context is reliable under solution multiplicity. To answer this question, we analyze how gold contexts vary across multiple distinct test-passing patches for the same issue. Specifically, we conduct case study on 82 tasks from CONTEXTBENCH, each with two semantically equivalent patches, for which we derive corresponding patch-conditioned gold contexts using the same context tracing procedure. We quantify consistency using the Jaccard similarity between each pair of contexts, detailed in Section H. For each task, we report the average similarity across all context pairs. Across all evaluated instances, the average Jaccard similarity is 0.9518 (average distance 0.0482), indicating that our gold contexts are highly consistent across alternative valid patches. Answer to RQ5: The high consistency of gold contexts across semantically equivalent patches supports their reliability for context retrieval evaluation. 9 Table 5: Context retrieval dynamics of different LLM agents. LLM agents show diverse retrieval dynamics, exposing trade-offs between efficiency and redundancy. In addition, the significant loss of context usage impedes effective issue resolution. LLM Efficiency Redundancy Usage Drop GPT-5 Claude Sonnet 4.5 Gemini 2.5 Pro Devstral 2 0.591 0.658 0.529 0.616 0.487 0.708 0.558 0.672 0.179 0.196 0.431 0."
        },
        {
            "title": "4 Related Work",
            "content": "Repository-Level Code Evaluation Benchmarks. Repository-level benchmarks ground evaluation in real-world codebases and executable test suites, making them the de facto standard for assessing end-to-end issue resolution. SWE-bench and its variants (Lite, Verified, multilingual extensions [30], long-horizon tasks [10], live environments [31], goal-oriented settings [27], and freelance scenarios [16]) evaluate whether system can produce test-passing patch under realistic repository settings [26, 12], while complementary benchmarks such as RepoBench [14] and SWE-PolyBench [20] emphasize cross-file completion or structure-aware analysis. Despite their realism, these benchmarks are predominantly outcome-driven: they largely measure final task success, offering limited visibility into whether agents succeed by identifying the right information or by compensating through extensive exploration [6]. Our work complements outcome-only evaluation by (i) introducing verified gold contexts and (ii) enabling dynamic, process-level evaluation of context retrieval beyond final outcomes. Coding Agents. LLM-based coding agents address repository-level issue resolution through either pipeline-based or agentic designs. Pipeline-based systems decompose tasks into fixed stages with controlled context selection [24, 32, 21, 11, 28], agentic systems adopt open-ended interaction loops in which agents dynamically search, inspect, and modify codebases via tool calls [26, 23, 25, 4, 6]. Despite architectural diversity, context retrieval remains shared bottleneck. Existing benchmarks primarily evaluate end-to-end success, making it difficult to compare how agents retrieve and manage code context. CONTEXTBENCH fills this gap by providing retrieval-centric benchmark that directly evaluates context acquisition behavior, independent of final patch correctness. Trajectory Analysis and Failure Diagnosis. Recent work analyzes agent trajectories to diagnose failure modes via manual inspection, error taxonomies, or large-scale statistics over execution logs [33, 13, 5, 2, 3, 19]. Such analyses have improved understanding of where agents fail (e.g., reasoning deadlocks, tool misuse, runtime exceptions), and some studies report that context-related errors are frequent in repository-level settings. However, trajectory analysis is often labor-intensive, difficult to standardize across frameworks, and typically provides post hoc explanations rather than instance-level supervision. CONTEXTBENCH addresses this gap by offering scalable, standardized evaluation layer: verified gold contexts and automatic metrics that quantify retrieval effectiveness without requiring manual reading of long trajectories, while still enabling connections between intermediate retrieval behavior and end-to-end outcomes."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced CONTEXTBENCH, benchmark for process-level evaluation of context retrieval in LLM-based coding agents. By augmenting existing issue-resolution benchmarks with humanannotated gold contexts and retrieval-centric metrics, CONTEXTBENCH enables analysis beyond end-to-end success rates. Our evaluation across four frontier LLMs and five coding agents reveals limited benefits from complex agent scaffolding, consistent recall-over-precision retrieval behavior, and notable gaps between retrieved and utilized context. These results highlight the need for processoriented evaluation and suggest intermediate context signals as promising direction for improving LLM-based software engineering systems. This work introduces new process-level evaluation paradigm for LLM-based coding agents, enabling the community to move beyond coarse-grained end-to-end metrics toward deeper understanding of agentic reasoning and context utilization. By providing verified gold contexts and automated 10 retrieval metrics, CONTEXTBENCH facilitates transparent, reproducible, and fine-grained assessment of how coding agents interact with large-scale codebases. We expect this benchmark to support the development of more reliable and interpretable software engineering agents, improving their robustness in real-world development workflows. In the broader context, this work contributes to responsible AI research by promoting rigorous evaluation practices, reducing overfitting to leaderboard-style benchmarks, and fostering more trustworthy integration of AI systems in software engineering and related domains."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Introducing claude sonnet 4.5. https://www.anthropic.com/news/ claude-sonnet-4-5, 2025. Accessed: 2026-01-31. [2] Islem Bouzenia and Michael Pradel. Understanding software engineering agents: study of thought-action-result trajectories. arXiv preprint arXiv:2506.18824, 2025. [3] Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multiagent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. [4] Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, and Qianxiang Wang. Swe-exp: Experience-driven software issue resolution. arXiv preprint arXiv:2507.23361, 2025. [5] Zhi Chen, Wei Ma, and Lingxiao Jiang. Beyond final code: process-oriented error analysis of software development agents in real-world github scenarios. arXiv preprint arXiv:2503.12374, 2025. [6] Zimin Chen, Yue Pan, Siyu Lu, Jiayi Xu, Claire Le Goues, Martin Monperrus, and He Ye. Prometheus: Unified knowledge graphs for issue resolution in multilingual codebases. arXiv preprint arXiv:2507.19942, 2025. [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [8] Le Deng, Zhonghao Jiang, Jialun Cao, Michael Pradel, and Zhongxin Liu. Nocodebench: benchmark for evaluating natural language-driven feature addition. arXiv preprint arXiv:2507.18130, 2025. [9] Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, et al. Swe-bench pro: Can ai agents solve long-horizon software engineering tasks? arXiv preprint arXiv:2509.16941, 2025. [10] Xiang Deng, Jeff Da, Edwin Pan, Yannis Yiming He, Charles Ide, Kanak Garg, Niklas Lauffer, Andrew Park, Nitin Pasari, Chetan Rane, Karmini Sampath, Maya Krishnan, Srivatsa Kundurthy, Sean Hendryx, Zifan Wang, Vijay Bharadwaj, Jeff Holm, Raja Aluri, Chen Bo Calvin Zhang, Noah Jacobson, Bing Liu, and Brad Kenstler. Swe-bench pro: Can ai agents solve long-horizon software engineering tasks?, 2025. [11] Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, et al. Trae agent: An llm-based agent for software engineering with test-time scaling. arXiv preprint arXiv:2507.23370, 2025. [12] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world GitHub issues? In International Conference on Learning Representations, 2024. [13] Simiao Liu, Fang Liu, Liehao Li, Xin Tan, Yinghao Zhu, Xiaoli Lian, and Li Zhang. An empirical study on failures in automated issue solving. arXiv preprint arXiv:2509.13941, 2025. [14] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023. [15] Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang, Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis, Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian, Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun, Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu, Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan, Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman, Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbjorn Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang, Alex Dimakis, Andy Konwinski, and Ludwig Schmidt. Terminal-bench: Benchmarking agents on hard, realistic tasks in command line interfaces. arXiv preprint arXiv:2601.11868, 2026. [16] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering?, 2025. [17] Mistral AI. Introducing: Devstral 2 and mistral vibe cli. https://mistral.ai/news/ devstral-2-vibe-cli, 2025. Accessed: 2026-01-31. [18] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with SWE-gym. In Forty-second International Conference on Machine Learning, 2025. [19] Divya Pathak, Harshit Kumar, Anuska Roy, Felix George, Mudit Verma, and Pratibha Moogi. Detecting silent failures in multi-agentic ai trajectories. arXiv preprint arXiv:2511.04032, 2025. [20] Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buchholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, and Laurent Callot. Swe-polybench: multi-language benchmark for repository level evaluation of coding agents. arXiv preprint arXiv:2504.08703, 2025. [21] Haifeng Ruan, Yuntong Zhang, and Abhik Roychoudhury. Specrover: Code intent extraction via llms. arXiv preprint arXiv:2408.02232, 2024. [22] Aaditya Singh, Adam Fry, Adam Perelman, Adam Tart, Adi Ganesh, Ahmed El-Kishky, Aidan McLaughlin, Aiden Low, AJ Ostrow, Akhila Ananthram, et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. [23] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, and et al. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. [24] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [25] Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, and Lingming Zhang. Live-swe-agent: Can software engineering agents self-evolve on the fly? arXiv preprint arXiv:2511.13646, 2025. [26] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 12 [27] John Yang, Kilian Lieret, Joyce Yang, Carlos E. Jimenez, Ofir Press, Ludwig Schmidt, and Diyi Yang. Codeclash: Benchmarking goal-oriented software engineering, 2025. [28] Xu Yang, Jiayuan Zhou, Michael Pacheco, Wenhan Zhu, Pengfei He, Shaowei Wang, Kui Liu, and Ruiqi Pan. Lingxi: Repository-level issue resolution framework enhanced by procedural knowledge guided scaling. arXiv preprint arXiv:2510.11838, 2025. [29] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. Multi-SWE-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [30] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, Siyao Liu, Yongsheng Xiao, Liangqiang Chen, Yuyu Zhang, Jing Su, Tianyu Liu, Rui Long, Kai Shen, and Liang Xiang. Multi-swe-bench: multilingual benchmark for issue resolving, 2025. [31] Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, Elsie Nallipogu, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, and Dongmei Zhang. Swe-bench goes live!, 2025. [32] Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2024, page 15921604, New York, NY, USA, 2024. Association for Computing Machinery. [33] Kunlun Zhu, Zijia Liu, Bingxuan Li, Muxin Tian, Yingxuan Yang, Jiaxun Zhang, Pengrui Han, Qipeng Xie, Fuyang Cui, Weijia Zhang, et al. Where llm agents fail and how they can learn from failures. arXiv preprint arXiv:2509.25370, 2025."
        },
        {
            "title": "A Details on Data Filtering",
            "content": "This subsection provides detailed description of the data filtering pipeline used to construct the final evaluation task set. The filtering process is designed to progressively remove redundancy, lowinformative tasks, and annotation artifacts, while preserving task diversity and evaluation difficulty. Initial task pool. We begin with an initial pool of approximately 4.4k source tasks aggregated from existing software engineering benchmarks, including SWE-bench Verified, SWE-PolyBench, and other related curated datasets. Although these tasks originate from real-world issue and pull request scenarios, our work does not directly crawl raw repositories; instead, it consolidates and refines tasks from established benchmarks. At this stage, the task set intentionally serves as broad superset and contains substantial redundancy and variability in task characteristics. ID-based deduplication. As first step, we apply an ID-level deduplication procedure that removes tasks with identical or invalid issue/PR identifiers. This step eliminates exact duplicates introduced during dataset aggregation and ensures that each remaining task corresponds to unique identifier. After this filtering, 3.9k tasks remain. Semantic deduplication. Next, we perform semantic-level deduplication using an embeddingbased filtering strategy. Each task is encoded into semantic vector representation using pretrained embedding model, and pairwise cosine similarity is computed between task embeddings. Tasks whose cosine similarity exceeds predefined threshold of 0.90 are considered semantically redundant and are filtered out, even if their identifiers differ. This step removes near-duplicate tasks that differ only in superficial wording while preserving semantically distinct problem instances. After semantic deduplication, the task set is reduced to 3.1k tasks."
        },
        {
            "title": "B Details on Agent Solvability",
            "content": "Difficulty-based filtering. The third stage applies difficulty-based filtering and accounts for the largest reduction in task count. Specifically, we exclude tasks that fall into one of the following categories: (i) high solvability, where the task is trivially solvable by most models and provides limited evaluative value; (ii) low dispersion, where model performance exhibits minimal variance across different capability levels; and (iii) limited scope, where the task can be resolved using only minimal and easily obtainable context. Concretely, tasks classified as limited scope typically involve fewer than four distinct code hunks in the annotated gold context provided to the agent, and therefore do not sufficiently exercise context retrieval or multi-step reasoning capabilities. This step is critical for ensuring that the benchmark meaningfully differentiates model behaviors. After difficulty-based filtering, 1.5k tasks remain. Annotation filtering. Finally, we apply manual annotation filter to ensure annotation quality and consistency. Tasks are retained only if they are associated with high-quality gold context annotations that are complete, precise, and suitable for structured analysis. This manual review step removes residual noise and annotation artifacts that are difficult to detect automatically. The final benchmark consists of 1.2k tasks."
        },
        {
            "title": "C Implementation Details on Agent Context Tracing",
            "content": "We describe our context extraction mechanism and the adaptations applied across multiple agents. For clarity, we divide this section into two parts: A1.1 focuses on model variations within the mini-SWE-agent; A1.2 focuses on cross-agent differences for the same model under varying prompt designs and context extraction protocols. C.1 Model Variations within mini-SWE-Agent C.1.1 Context Extraction Architecture In this subsection, we analyze how different models behave under the mini-SWE-agent context extraction protocol. We extend the default agent architecture with ContextAwareAgent class to enable structured and standardized context collection. To enforce strict structure and machine-readability, prompts are augmented with predefined templates, and the <PATCH CONTEXT> block is extracted using verification function with regular expressions. This setup ensures consistency, reproducibility, and automated verification within mini-SWE-agent workflows. C.1.2 Two-Stage Submission Protocol The submission workflow follows two-stage protocol. In the first stage, when the agent detects either MINI SWE AGENT FINAL OUTPUT or COMPLETE TASK AND SUBMIT FINAL OUTPUT, it does not finalize the task immediately. Instead, it raises ContextRequested exception, which triggers structured context request. In the second stage, the agent validates the returned context format and proceeds with final submission only after successful verification. C.1.3 Format Validation Mechanism To enforce strict structure and machine-readability, the prompt is augmented with predefined templates that explicitly constrain the output format. Format validation is performed by dedicated verification function, and regular expression is used to extract the standardized <PATCH CONTEXT> block for downstream processing. This design enforces strict structural constraints on contextual information, ensuring consistency, reproducibility, and compatibility with automated verification and downstream processing pipelines in agent-based software engineering workflows. C.1.4 Cross-Model Evaluation Results After establishing the context extraction framework within mini-SWE-agent, we evaluated its effectiveness across four distinct models: Claude Sonnet 4.5, GPT-5, Gemini 2.5 Pro, and DevStral2. Under this standardized extraction protocol, all four models demonstrated robust adherence to the framework, consistently producing accurate and machine-verifiable <PATCH CONTEXT> information. These results indicate that the context extraction mechanism effectively constrains model outputs, ensuring that the returned context is both structured and semantically relevant for downstream task submission. During the evaluation, we observed that DevStral2 occasionally exhibited unexpected behaviors that deviated from the standardized output protocol. These anomalies are discussed in detail in Section K, where we provide the corresponding raw interaction traces and highlight the specific issues encountered. 15 Context Control Templates in mini-SWE-Agent (context aware.yaml) context request template: <context request> Before finalizing your submission, please provide the CODE CONTEXT you used to generate the patch. Requirements List ONLY the specific code sections from source files that you examined to create your patch. Format (strictly follow) Each entry must include: <absolute file path> <start line>-<end line> - File: - Lines: Example File: /testbed/src/core/handler.ext Lines: 34-56 File: Lines: File: Lines: /testbed/lib/utils.ext 128-145 /testbed/src/parser.ext 67-89 DO NOT Include - Explanations, reasoning, or commentary - Error messages or debugging output - Test code or test results - Documentation or comments - Any non-code content Please format your response as: THOUGHT: Providing code context used for patch generation. <PATCH CONTEXT> File: Lines: <absolute file path> <start line>-<end line> <absolute file path> <start line>-<end line> File: Lines: </PATCH CONTEXT> bash echo COMPLETE TASK AND SUBMIT FINAL OUTPUT && (git status >/dev/null 2>&1 && git add -A && git diff --cached) (cd */. diff --cached) (echo \"Error: No git repository found\") 2>/dev/null && git add -A && git **CRITICAL:** Provide ONLY file paths and line ranges (no code snippets, no explanations) within <PATCH CONTEXT> tags. </context request> context confirmation template: <context received> Context received and recorded ({{context length}} characters). Now proceeding with final submission. </context received>"
        },
        {
            "title": "Experiment Setting",
            "content": "3600 environment: timeout: 180 pull timeout: env: PAGER: cat MANPAGER: cat LESS: -R PIP PROGRESS BAR: off TQDM DISABLE: 1 environment class: model: model name: model kwargs: drop params: temperature: true 0.0 docker \"anthropic/claude-sonnet-4-5-20250929\" C.2 Cross-Agent Comparison for the Same Model C.2.1 Context Extraction Protocol Extension To evaluate the impact of agent architecture and prompt design on context extraction, we extended the standardized context enforcement protocol, originally developed for mini-SWE-agent, to multiple other agents, including SWE-Agent, Openhands, and Prometheus. Each of these agents adopts context extraction framework built upon the mini-SWE-agent as the foundational design, with agent-specific adaptations to accommodate differences in workflow and output formatting. In all systems, an explicit pre-submission constraint was introduced, requiring the agent to output the precise code context prior to executing the final submission call. This ensures that all inspected source files and line ranges are consistently logged, enabling reproducibility, auditing, and automated verification of agent behavior. Moreover, prompt templates and verification routines were adjusted for each agent to maintain uniform, machine-readable <PATCH CONTEXT> structure. These adaptations facilitate controlled comparison of context extraction effectiveness across heterogeneous agent frameworks, and provide foundation for analyzing the influence of agent-specific design choices on model performance. C.2.2 SWE-Agent Implementation Pre-Submission Context Enforcement Specification in SWE-Agent Required Format: In the next response, the agent must: 1. Output <PATCH CONTEXT> block listing all source files and line ranges examined. 2. Invoke the submit function. Format for <PATCH CONTEXT> <PATCH_CONTEXT> File: /testbed/path/to/file1.ext Lines: 10-50 File: /testbed/path/to/file2.ext Lines: 100-150 </PATCH_CONTEXT> 17 Rules List only source files that were viewed or edited (exclude test files). Include the exact line ranges examined. Do not include code snippets, explanations, or commentary. This step is mandatory and cannot be skipped. config.yaml model: 0.0 0.0 gpt-5 name: api base: api key: per instance cost limit: total cost limit: temperature: 1.0 top p: null retry: retries: min wait: max wait: completion kwargs: timeout: tools: execution timeout: env variables: PAGER: cat MANPAGER: cat LESS: -R GIT PAGER: cat 10 5 60 600 120 C.2.3 Openhands Implementation Context Extraction Protocol in Openhands (system prompt context aware.j2) Explore-context marking protocol. When exploring source code context (e.g., reading files or printing line ranges), the agent is required to emit machine-parseable explore-context block before the corresponding command. Explore Context Format < EXPLORE_CONTEXT > File : / absolute / path / to / file . ext Lines : < start > - < end > </ EXPLORE_CONTEXT > The explore-context block must contain one or more file-range entries with absolute paths and positive line indices. It is only included when the command prints source code content to stdout and must not be used for metadata-only commands (e.g., ls, grep, git status). 18 Patch Context Submission Requirement Before finalizing the submission, the agent must provide the code context used for patch generation. Format (strictly enforced): THOUGHT : Providing code context used for patch generation . < PATCH_CONTEXT > File : < absolute_file_path > Lines : < start_line > - < end_line > File : < absolute_file_path > Lines : < start_line > - < end_line > </ PATCH_CONTEXT > Only absolute file paths and line ranges are permitted inside the <PATCH CONTEXT> block. Explanations, comments, debugging output, test code, or any non-code content are strictly disallowed. Unlike mini-SWE-agent and SWE-Agent, Openhands employs different prompt format. Nevertheless, this variation in prompt design does not preclude the implementation of an equivalent context extraction mechanism. By adapting the extraction protocol to accommodate the agent-specific prompt structure, we ensure that the resulting <PATCH CONTEXT> outputs remain consistent, structured, and machine-verifiable, thereby preserving the integrity and comparability of the context extraction process across heterogeneous agents. C.2.4 Agentless Enhancements Bug Fixes and Compatibility. Additionally, we implement several extensions and bug fixes for Agentless to ensure compatibility with the SWE-bench evaluation harness. We adapt to SWE-bench harness API changes by fixing import paths due to the test spec module restructuring. We also unify Docker environment configurations by removing hardcoded docker specs and aligning with build env images() to ensure consistent env image key generation. Additionally, we remove the deprecated pip option no-use-pep517 which has been removed in pip 23.0. Multi-Language Support. To support cross-language evaluation, we optimize the multi-language adaptation capability of Agentless, enabling better support for multi-language benchmarks. Improvement 1: Dynamic File Extension Adaptation Strategy: Automatically detect the repositorys primary programming language by analyzing file extension frequency, then dynamically inject the most common extension into prompt examples. Detection Method: Traverse repository structure to count file extensions Select extension with highest occurrence (e.g., .py, .java, .js, .go) Replace hard-coded examples with detected extension 19 Before: Hard-coded Python Extensions For example: file1.py file2.py ### Examples: full_path1/file1.py line: 10 class: MyClass1 full_path2/file2.py function: MyClass2.my_method After: Repository-Adaptive Extensions For example: file1.<detected_extension> file2.<detected_extension> ### Examples: full_path1/file1.<detected_extension> line: 10 class: MyClass1 full_path2/file2.<detected_extension> function: MyClass2.my_method Impact: Eliminates language-specific bias in fault localization prompts, enabling cross-language applicability without manual configuration. Improvement 2: Language-Agnostic DIFF Examples Strategy: Replace language-specific DIFF examples with universal Hello World modification patterns that adapt to the detected programming language. Before: Python-Specific DIFF Example Here is an example: python ### mathweb/flask/app.py <<<<<<< SEARCH from flask import Flask ======= import math from flask import Flask >>>>>>> REPLACE 20 After: Python Hello World Example Here is an example: python ### example/hello.py <<<<<<< SEARCH print(\"Hello World\") ======= print(\"Hello\") print(\"World\") >>>>>>> REPLACE After: Java Hello World Example Here is an example: java ### example/Hello.java <<<<<<< SEARCH System.out.println(\"Hello World\"); ======= System.out.println(\"Hello\"); System.out.println(\"World\"); >>>>>>> REPLACE After: JavaScript Hello World Example Here is an example: javascript ### example/hello.js <<<<<<< SEARCH console.log(\"Hello World\"); ======= console.log(\"Hello\"); console.log(\"World\"); >>>>>>> REPLACE After: Go Hello World Example Here is an example: go ### example/hello.go <<<<<<< SEARCH fmt.Println(\"Hello World\") ======= fmt.Println(\"Hello\") fmt.Println(\"World\") >>>>>>> REPLACE Adaptation Logic: Detect repository language from file extensions Select corresponding Hello World DIFF template Inject into repair prompts at runtime 21 Impact: Provides language-appropriate syntax examples without domain-specific dependencies (e.g., Flask), improving LLM understanding across diverse codebases. C.2.5 Framework Generalization Results After applying the standardized context extraction framework to all five agentsmini-SWE, SWEAgent, Openhands, agentless, and Prometheuswe observed that each system was able to accurately produce the corresponding <PATCH CONTEXT> outputs. This demonstrates that the framework is robust and generalizable across heterogeneous agent architectures, effectively constraining model outputs to structured, machine-verifiable format. The consistent extraction of patch context across agents facilitates reproducibility and auditing, and provides reliable basis for downstream evaluation of model performance in automated software engineering tasks. Moreover, these results indicate that the underlying principles of context enforcement can be adapted to accommodate differences in prompt design, agent workflow, and system architecture without compromising the fidelity or completeness of the extracted context. Overall, the successful deployment of the framework across multiple agents underscores its utility as foundational tool for systematic evaluation and comparison of agent-based patch generation systems."
        },
        {
            "title": "D Gold Context Definition and Scope",
            "content": "Gold Context as Compact Sufficient Reference. In CONTEXTBENCH, we define the gold context as human-annotated set of code regions that is verified to be sufficient for resolving the issue, while being as compact as possible under our annotation guideline (the minimal context principle). We do not claim global minimality: enforcing globally minimal sufficient set is generally intractable in repository-scale codebases due to the combinatorial space of dependency chains and code spans. Instead, our refinement stage removes clearly redundant regions while preserving sufficiency under an executable feasibility check (Section G). Accordingly, precision/F1 should be interpreted with respect to this compact-and-verified reference, rather than as strict penalty for retrieving any additional context that may also be helpful. 22 Block-Level (AST) Alignment Details At (AST) Alignment. the block level, we do not Block-Level treat arbitrary AST Instead, we standardize blocks as definition-level symbols (e.g., nodes as evaluation units. function/method/class/interface/trait definitions) to ensure cross-language consistency. Concretely, we parse each repository file with tree-sitter and extract language-specific set of node types that correspond to top-level or member definitions  (Table 6)  . Each extracted node is mapped to canonical block span by its file path and (start line, end line) range (converted from tree-sitter byte offsets). This design ensures that block refers to comparable semantic units across languages (definitions that encapsulate reusable logic or APIs), rather than low-level syntax nodes whose granularity varies substantially across parsers. Table 6: Tree-sitter node types used for block-level (definition-level) evaluation across languages. Blocks are standardized as definition-level symbols to improve cross-language comparability. Language Python JavaScript TypeScript TSX Java Go Rust C++ C# PHP Ruby Swift Kotlin Scala Node types (block units) method definition, method definition, method definition, class declaration, class declaration, class declaration, class declaration, function definition, class definition, async function definition function declaration, arrow function function declaration, interface declaration function declaration, interface declaration method declaration, constructor declaration function declaration, method declaration, type declaration function item, impl item, struct item, trait item function definition, struct specifier function definition, class specifier, struct specifier method declaration, class declaration, interface declaration function definition, method declaration, class declaration method, class, module function declaration, class declaration, protocol declaration function declaration, class declaration function definition, class definition, trait definition interface declaration,"
        },
        {
            "title": "F Details on Annotation",
            "content": "To obtain precise problem-specific contextual annotations, we developed dedicated annotation frontend that supports multiple annotators working in parallel with randomized task assignment. The interface provides lightweight IDE-like functionalities, including navigation via function and variable references, as well as object-oriented class definition lookup, enabling annotators to efficiently explore repository-level codebases. Figure 5 presents the annotation frontend UI. The code annotation view (left) allows annotators to select minimal problem-specific code spans with lightweight IDE-style navigation. The side panel (right) displays structured task metadata, including issue descriptions, reference patches, and real-time annotation statistics, enabling annotators to maintain awareness of the overall task scope. (a) Code annotation view. (b) Sidebar with issue description and statistics. Figure 5: Annotation frontend UI. Annotators select minimal problem-specific code context while monitoring task-level metadata in the side panel. For given annotation task, annotators are blinded to each others annotations to prevent information leakage and anchoring bias. We refer readers to the main paper for the full annotation and verification pipeline.This multi-round blind annotation protocol ensures both correctness and robustness of the resulting gold context dataset. Our annotator team consists of highly trained software engineering experts with advanced research experience in large-scale codebases. All annotators underwent unified training process before participating in the annotation tasks. During training, we enforced standardized annotation guideline, requiring annotators to provide precise and minimal contextual information. Specifically, annotators were instructed to follow minimal context principle, selecting only the code segments that were strictly necessary for resolving the issue, and avoiding redundant or irrelevant code regions. The annotation time per issue ranged from approximately 20 minutes to 1.5 hours, depending on the complexity of the repository and the issue scope. On average, producing high-quality annotation for 24 single issue in repository-scale project requires approximately 40 minutes of effort by an expert annotator. For repositories with exceptionally large or complex codebases, we imposed an upper bound on annotation time to control annotation cost and prevent diminishing returns from excessively long inspection sessions. We periodically audited annotations to ensure guideline compliance and consistency across annotators."
        },
        {
            "title": "G Details on Context Verification",
            "content": "This subsection provides additional details on the context verification procedure used in our analysis. The goal of this process is to determine whether an annotated context contains sufficient information to resolve its corresponding issue, rather than to evaluate or compare the repair capabilities of specific language models. Verification Setup. For each task, we condition strong state-of-the-art language model (e.g., GPT-5) solely on the annotated context, without access to any external files, repositories, or retrieval mechanisms. The model is prompted to generate multiple independent candidate patches (five attempts in our experiments), each following the same prompt template. All generated patches are evaluated using the official test suite associated with the task, including both fail-to-pass and pass-to-pass test cases. Sufficiency Criterion. An annotated context is considered sufficient if at least one of the generated patches passes the entire test suite. This criterion is intentionally defined in terms of existence rather than consistency: we do not require the model to reliably solve the task across all attempts. Instead, we only check whether valid solution is possible given the provided context. This aligns with our objective of verifying context sufficiency, rather than measuring average model performance. Rationale. Using strong language model in this setting serves as conservative lower-bound feasibility check. If even capable agent fails to produce correct fix when restricted to the annotated context, it is unlikely that the context alone contains all information required to resolve the issue. Conversely, successful patch indicates that the annotated context is, in principle, sufficient to support correct solution. Sampling and Stochasticity. We employ multiple independent generations to mitigate the effects of stochastic decoding and occasional generation failures. This design reduces false negatives caused by randomness, while avoiding over-reliance on single generation outcome. Importantly, correctness is determined exclusively by the test suite, independent of the models internal confidence or reasoning traces. Overall, this verification procedure should be interpreted as feasibility-oriented validation of annotated contexts, rather than benchmark of language model repair performance."
        },
        {
            "title": "H Details on Evaluation Metrics",
            "content": "We evaluate coding agents by quantifying how effectively they retrieve relevant context compared to the human-verified gold contexts. Formally, given an issue task , let denote the gold context, denote the final context retrieved by the agent, and represent the intermediate context snapshot collected at the i-th checkpoint during the agents execution trajectory. All G, A, and are represented as sets of code elements at specified granularity (e.g., file, AST block, or line). Context Recall. This metric quantifies the proportion of the gold context required for issue resolution that is successfully retrieved by the agent, formulated as: Recall(C A, G) = G G . (1) higher recall indicates that the coding agent is more effective at covering the essential context, whereas lower score suggests that critical information is missing from its retrieved context. Context Precision. This metric quantifies the proportion of the agents retrieved context that overlaps with the gold context, defined as: Precision(C A, G) = G A . (2) higher precision indicates that the coding agent retrieves relevant and concise context, whereas lower score suggests over-retrieval with redundant or irrelevant information that may introduce noise and hinder reasoning. Context F1. Context recall and precision capture two complementary aspects of an agents context retrieval behavior. High recall with low precision indicates over-retrieval, where the agent includes excessive irrelevant context, while high precision with low recall indicates under-retrieval, where essential information is missed. To provide balanced evaluation, we compute the F1 score as the harmonic mean of precision and recall, defined as: F1 = 2 Precision Recall Precision + Recall . (3) Context Retrieval Efficiency. Beyond the final retrieved context A, we evaluate retrieval as process along the trajectory. For each run, we log every file-observation action and construct }T sequence of intermediate context snapshots {C is the set of code elements (at chosen granularity) inspected at step t. We define the cumulative explored context up to step as t=1, where A(t) = (cid:91) i=1 . (4) We then track the cumulative gold coverage curve Coverage(A(t), G) over = 1, . . . , and summarize how early an agent reaches high gold coverage using the normalized area under this curve: AUC-Cov = 1 (cid:88) t= Recall(A(t), G). (5) higher AUC-Cov indicates that the agent retrieves critical gold evidence earlier during execution (i.e., reaches high coverage with fewer observation steps), whereas lower value suggests that the agent either misses gold evidence or only discovers it late after many iterations. Context Retrieval Redundancy. While AUC-Cov captures how quickly coverage increases, it does not measure whether an agent repeatedly inspects the same regions. We therefore quantify redundancy by measuring how much of each newly retrieved snapshot overlaps with what has already been observed. Formally, for 2, we define the per-step redundancy ratio as the fraction of elements in that have appeared in previous steps: Redunt = (cid:12) (cid:12) (cid:12)C (cid:16)(cid:83)t1 i=1 (cid:12) (cid:12) (cid:12)C (cid:12) (cid:17)(cid:12) (cid:12) (cid:12) . (6) We report the overall redundancy as the average across steps: Redun ="
        },
        {
            "title": "1\nT − 1",
            "content": "T (cid:88) t=2 Redunt. (7) higher redundancy indicates that an agent spends more retrieval budget revisiting previously seen context (i.e., looping or re-reading), whereas lower redundancy indicates more novel context acquisition per step. Evidence Drop (Retrieval = Use). High explored coverage does not necessarily translate to effective patching, because agents may observe gold-relevant regions during exploration but fail to retain them in the final aggregated patch context A. To quantify this gap, we define the explored-gold set as the gold elements that have been observed at least once during the trajectory: Gseen = (cid:33)"
        },
        {
            "title": "C A\nt",
            "content": "G. (cid:32) (cid:91) t="
        },
        {
            "title": "We then measure how much of this observed gold evidence is kept in the final patch context by",
            "content": "Keep = (cid:12)C G(cid:12) (cid:12) (cid:12) Gseen , and define evidence drop as its complement: Drop = 1 Keep = 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)C G(cid:12) (cid:12) (cid:16)(cid:83)T t=1 (cid:17) . (cid:12) (cid:12) (cid:12) (8) (9) (10) lower Drop (higher Keep) indicates that the agent successfully consolidates and preserves goldrelevant evidence it has already discovered, whereas higher Drop indicates that gold evidence is frequently observed but discarded during consolidation, highlighting failures in localization and evidence selection rather than pure recall-oriented retrieval. Gold Context Robustness. Our gold contexts are human-annotated based on gold patch. However, in practice, single issue may admit multiple semantically equivalent but syntactically different patches, raising the question of whether the collected gold context is reliable under solution multiplicity. To answer this question, we analyze how gold contexts vary across multiple distinct testpassing patches for the same issue. We conduct case study on 82 instances from CONTEXTBENCH. For each instance, we collect two or three semantically equivalent patches and derive patchconditioned gold context for each patch using the same patch-driven context tracing procedure. Let = {G(1), . . . , G(K)} denote the set of gold contexts for given instance. We quantify consistency using the Jaccard similarity (Equation 11) between each pair of contexts. For each instance, we report the average similarity across all context pairs. Jaccard(G(i), G(j)) = G(i) G(j) G(i) G(j) . (11) Note that all reported metrics are averaged over evaluated tasks. Each metric can be computed at three granularities: file, block, and line levels. By evaluating context retrieval across multiple metrics and granularities, CONTEXTBENCH provides more fine-grained understanding of agent behavior, beyond conventional end-to-end evaluation metrics that focus solely on final task success rates."
        },
        {
            "title": "I Details on Empirical Study",
            "content": "This subsection provides implementation-level details of our empirical analysis. To analyze agent behavior beyond final patch correctness, we instrument the evaluation pipeline to record intermediate artifacts, including tool-call logs, retrieved file and element contexts, and step-by-step edit trajectories. Based on these records, we perform manual inspection to trace how each agent localizes relevant files, retrieves semantic information, and arrives at its final fix. All annotation and analysis are conducted with the support of the annotation frontend UI introduced earlier, which enables structured inspection and labeling of the recorded artifacts. The following case studies illustrate this analysis procedure for different agents, highlighting how the recorded artifacts are used to diagnose failure points at various stages of the debugging process. I.1 Case 1: Prometheus Agent Incomplete Class Semantics Retrieval Task: Fix issue psf requests-1921 where setting session headers to None sends literal None instead of omitting the header. Diagnostic Findings: 1. Partial Class Retrieval: Agent logs show successful retrieval of CaseInsensitiveDicts or getitem methods (lines 71-78), but no retrieval of its init setitem and update methods. 2. Semantic Mismatch: The generated patch passed generator expression to CaseInsensitiveDict (line 54 of requests/sessions.py): merged_setting = dict_class (( , ) for , in ... if is not None ) Analysis of the missing generator. init reveals it requires mapping with .items(), not 3. Impact: 26 test failures due to dropped headers, broken authentication, and cookie persistence failures. Gold Patch Comparison: The correct fix adds single filtering line after merging, preserving proper initialization: merged_setting = dict (( , ) for (k , ) in merged_setting . items () if is not None ) Root Cause: Element-level slicing insufficiencythe agent retrieved operational methods but missed constructor semantics, leading to API contract violation. I.2 Case 2: Agentless Failure at File Localization Stage Task: Fix for django django-11630. Diagnostic Findings: 1. Stage 1 Failure (File Localization): The agent retrieved 10 files, including 5 from django/db/models/* (e.g., base.py, options.py, fields/) and 5 framework configuration files. Critically, django/core/checks/model checks.pythe source of error E028 based on gold patch analysiswas never retrieved. 2. Cascading Failure: With incorrect files localized in Stage 1, subsequent element localization (Stage 2) focused on ModelBase, Options classes, and edit localization (Stage 3) modified options.pys db table calculation. The gold patch, by contrast, modifies logic in model checks.py to conditionally emit warnings when database routers are configured. 3. Information-Architecture Gap: The issue mentions db table collision (symptom-level) but not the validation framework (implementation-level). File tree shows core/checks/ exists but lacks semantic annotations. Without backward-tracing from error code E028 to its source, the agent inferred the problem resided in model definition layers. Root Cause: File localization failure propagated through all stages. Operating on surface-level keywords from the issue (models, db table), the agent lacked mechanisms to: (i) trace error codes to implementation sources, (ii) recognize Djangos architectural separation of validation from model logic. This initial misstep rendered all downstream localization (element and edit) ineffective, as they operated on an incorrect file set. I.3 Case 3: OpenHands Agent Cross-Context Exploration Gap Task: Fix for django django-11138 regarding ignored TIME ZONE values in multi-backend configurations. Diagnostic Findings: 1. Keyword-Centric Search Bias: The agent relied heavily on grep for string matching. Since this specific SQL function is unique to MySQL, the tools output anchored the agents search space to the MySQL implementation, effectively shielding the relevant SQLite and Oracle modules from the localization process. 2. Structural Observation-Action Gap: While ls -R tool calls explicitly listed the sqlite3/ and oracle/ directories, the agents heuristic failed to trigger follow-up audit calls (e.g., view or targeted grep) for these parallel modules. This represents failure to translate structural awareness into diagnostic action. 3. Cross-Backend Reproduction Paradox: Faced with environment constraints for MySQL, the agent used bash tool calls to successfully run reproduction script in SQLite environment. Although the tool-verified evidence confirmed the bugs presence in SQLite, the localization logic failed to propagate this feedback, resulting in reproduce in SQLite, fix in MySQL logical disconnect. 4. Canonical Pattern Neglect: Despite retrieving the base operations class via the agent bypassed the frameworks canonical connection.timezone name view, property. Instead, it utilized tool calls to inject non-standard dictionary-access logic (settings dict.get(TIME ZONE)), failing to align with Djangos architectural standards. Root Cause: Search-Induced Context Tunneling. The agents localization strategy is highly sensitive to initial tool-call results but lacks semantic generalization. High-relevance hits in one module (MySQL) acted as distractor, suppressing the horizontal exploration of parallel modules (SQLite/Oracle). This reliance on keyword-driven slicing leads to systemic fix incompleteness in modular architectures where logic is distributed across similar but syntactically distinct files. 30 Cross-Language Radar Distributions for File/Block/Line Metrics (a) File Precision (b) Block Precision (c) Line Precision (d) File Recall (e) Block Recall (f) Line Recall (g) File (h) Block F1 (i) Line F1 Figure 6: Radar distributions of localization quality across eight programming languages. Columns correspond to granularity (File / Block / Line), and rows correspond to metric (Precision / Recall / F1). Each radar axis is one language; values are computed on the per-language split under the same evaluation protocol as the main paper. 31 Potential Data Contamination and Protocol Misalignment in Devstral2 During our evaluation with the mini-SWE-agent framework, we observed anomalous behaviors from DevStral2 that raise concerns about potential data contamination and protocol misalignment. Specifically, after reading the issue description, DevStral2 frequently produced long, structured solution plan in single response, outlining the complete debugging and patching procedure without waiting for any feedback from the agent framework. The model often hallucinated intermediate system states, behaving as if it had already received environment responses. When the framework subsequently returned invalid-output feedback, DevStral2 tended to directly trigger the final submit action without performing corrective iterations. One plausible explanation is training data contamination, where DevStral2 may have been exposed to SWE-bench instances, patches, or benchmark-derived artifacts during pretraining or fine-tuning. Such leakage can induce benchmark-specific memorization, resulting in unrealistically confident and fully structured solution traces. Another contributing factor may be mismatch between the models training on interactive coding protocols and the simplified execution semantics of mini-SWE-agent, leading to premature assumptions about system states and response formats. These behaviors can significantly distort benchmark evaluation. First, memorization of benchmark instances can artificially inflate reported performance, violating the independence assumption between training and test data. Second, hallucinated system states undermine the validity of agent trajectories, making it difficult to distinguish genuine reasoning from spurious pattern completion. Finally, premature submission without iterative correction disrupts the intended evaluation protocol, reducing the reliability of automated correctness assessment. We emphasize that such artifacts highlight the importance of strict data hygiene and protocol-aligned training for code-generation models evaluated on public software engineering benchmarks. Future benchmark evaluations should incorporate contamination detection and protocol compliance checks to ensure fair and reproducible comparisons. DevStral2 Raw JSON Response Trace Listing 1: DevStral2 Raw JSON Response Trace \" extra \" : { \" response \" : { \" id \" : \" 7 2 4 5 4 0 8 8 4 2 4 3 3 ab 3 5 2 3 0 8 4 3 9 4 \" , \" created \" : 1 7 6 8 6 4 2 6 7 5 , \" model \" : \" devstral - 2 5 1 2 \" , \" object \" : \" chat . completion \" , \" syst em _f inge rprin \" : null , \" choices \" : [ { \" finish_reason \" : \" tool_calls \" , \" index \" : 0 , \" message \" : { \" content \" : \"I ll start by exploring the Django codebase ... < EXPLORE_CONTEXT > File : / testbed / django / core / management / base . py nLines : 1 -1 0 0 </ EXPLORE_CONTEXT > bash nfind / testbed - type - name \" base . py \" grep \" management \" head -5 ... ... Perfect ! found the issue . Now let me create test script to reproduce the problem ... bash ncat << EOF > / testbed / es _ ut al ly _ ex lu e . py nimport os nimport sys 32 import django nfrom django . core . management import call_command from django . core . management . base import BaseCommand , CommandError ... Let me run the core management tests : bash ncd / testbed && python -m pytest tests / core_management / -v 2 >& 1 head -1 0 0 Let me check what tests exist : bash nfind / testbed / tests - type - name \"*. py \" xargs grep -l \" ua _e us _g p \" head -5 n Let me check if there test file for the management module : bash nfind / testbed / tests - type - name \"* management *. py \" head -1 0 ... Now let me verify the fix one more time and check if there are any syntax errors ... Great ! The tests pass . Let me create final summary of the changes ... The fix is minimal , handles edge cases , and doesn break existing functionality .\" , \" role \" : \" assistant \" , \" tool_calls \" : null , \" function_call \" : null , \" v r _s if _ l \" : null } , \" v r _s if _ l \" : { } } ] , \" usage \" : { \" comple tion_t okens \" : 1 3 9 8 9 , \" prompt_tokens \" : 1 8 3 6 , \" total_tokens \" : 1 5 8 2 5 , \" p i _ e _ a il \" : null , \" pr om t_ to en _ det ails \" : null } } } { \" role \" : \" user \" , \" content \" : \" Please always provide EXACTLY ONE action in triple backticks , found 1 0 7 actions . nPlease format your action in triple backticks as shown in < response_example >. < response_example > nHere are some thoughts about why you want to perform the action . bash < action > </ response_example > nIf you have completed your assignment , please consult the first message about how to nsubmit your solution ( you will not be able to continue working on this task after that ) .\" , \" timestamp \" : 1 7 6 8 6 4 2 8 7 7 . 7 6 1 8 9 0 6 } , \" extra \" : { \" response \" : { \" id \" : \" 4 8 6 7 9 cc 0 2 4 4 7 3 5 9 1 0 1 8 ca 7 9 0 9 \" , \" created \" : 1 7 6 8 6 4 2 8 7 8 , \" model \" : \" devstral - 2 5 1 2 \" , \" object \" : \" chat . completion \" , \" sy st em _fingerprint \" : null , \" choices \" : [ { \" finish_reason \" : \" stop \" , 33 \" index \" : 0 , \" message \" : { \" content \" : \" THOUGHT : have successfully implemented the fix for the issue where call_command fails when an argument of required mutually exclusive group is passed in kwargs . The fix has been applied to / testbed / django / core / management / __init__ . py and thoroughly tested . Now need to submit the solution . bash necho P E _ K _ _ M _ A _ P && ( git status >/ dev / null 2 >& 1 && git add -A && git diff -- cached ) ( cd */. 2 >/ dev / null && git add -A && git diff -- cached ) ( echo \" Error : No git repository found \") \" , \" role \" : \" assistant \" , \" tool_calls \" : null , \" function_call \" : null , \" v r _s if _ fi s \" : null } , \" v r _ sp fi _ fi s \" : { } } ] , \" usage \" : { \" completion_tokens \" : 1 3 6 , \" prompt_tokens \" : 1 5 9 2 5 , \" total_tokens \" : 1 6 0 6 1 , \" p i _ e _ a il \" : null , \" pr om t_ ok ens _det ils \" : null } } }"
        }
    ],
    "affiliations": [
        "Nanjing University",
        "University College London"
    ]
}