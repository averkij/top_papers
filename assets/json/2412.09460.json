{
    "paper_title": "The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective",
    "authors": [
        "Javier de la Rosa",
        "Vladislav Mikhailov",
        "Lemei Zhang",
        "Freddy Wetjen",
        "David Samuel",
        "Peng Liu",
        "Rolv-Arild Braaten",
        "Petter Mæhlum",
        "Magnus Breder Birkenes",
        "Andrey Kutuzov",
        "Tita Enstad",
        "Svein Arne Brygfjeld",
        "Jon Atle Gulla",
        "Stephan Oepen",
        "Erik Velldal",
        "Wilfred Østgulen",
        "Liljia Øvrelid",
        "Aslak Sira Myhre"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The use of copyrighted materials in training generative language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of copyrighted materials on the performance of large language models (LLMs) for Norwegian. We found that both books and newspapers contribute positively when the models are evaluated on a diverse set of Norwegian benchmarks, while fiction works possibly lead to decreased performance. Our experiments could inform the creation of a compensation scheme for authors whose works contribute to AI development."
        },
        {
            "title": "Start",
            "content": "The Impact of Copyrighted Material on Large Language Models:"
        },
        {
            "title": "A Norwegian Perspective",
            "content": "Javier de la Rosa1 Vladislav Mikhailov2 Lemei Zhang3 Freddy Wetjen1 David Samuel2 Peng Liu3 Rolv-Arild Braaten1 Petter Mæhlum2 Magnus Breder Birkenes1 Andrey Kutuzov2 Tita Enstad1 Svein Arne Brygfjeld1 Jon Atle Gulla3 Stephan Oepen2 Erik Velldal2 Wilfred Østgulen1 Liljia Øvrelid2 Aslak Sira Myhre1 1National Library of Norway 2University of Oslo 3Norwegian University of Science and Technology versae@nb.no"
        },
        {
            "title": "Abstract",
            "content": "The use of copyrighted materials in training generative language models raises critical legal and ethical questions. This paper presents framework for and the results of empirically assessing the impact of copyrighted materials on the performance of large language models (LLMs) for Norwegian. We found that both books and newspapers contribute positively when the models are evaluated on diverse set of Norwegian benchmarks, while fiction works possibly lead to decreased performance. Our experiments could inform the creation of compensation scheme for authors whose works contribute to AI development."
        },
        {
            "title": "Introduction",
            "content": "Generative language models have fundamentally reshaped the landscape of natural language processing (NLP), enabling the development of systems that can generate and interact with human language at an unprecedented level. This includes Norwegian, for which several large language models (LLMs) have been trained and published in the recent years using different architectures and licensing choices (Kummervold et al., 2021; Kutuzov et al., 2021; Samuel et al., 2023; Liu et al., 2024). However, the vast quantities of data required for training these models often include copyrighted materials, presenting novel challenges related to intellectual property rights and compensation. These challenges have led to numerous lawsuits across jurisdictions, fundamentally questioning the legitimacy of training models on copyrighted data without explicit permissions from content creators (Panettieri, 2024; Madigan, 2024; Weisenberger et al., 2024). The first wave of lawsuits emerged shortly after the public release of advanced generative AI models. Content creators, including authors, visual artists, and musicians, began to express concerns about the unauthorized use of their work in training datasets. Multiple class-action lawsuits were filed in the United States, accusing prominent AI companies such as OpenAI and Meta Platforms of infringing on copyright laws by using copyrighted materials without obtaining explicit permissions. The authors argued that the unauthorized use of their works without any form of compensation or recognition undermines their intellectual property rights and jeopardizes their ability to earn living from their creative endeavors. In Europe, coalition of news publishers has taken legal action against Google and Meta Platforms, arguing that the use of journalistic content in training models without fair remuneration constitutes breach of copyright and undermines the sustainability of high-quality journalism. Likewise, Norwegian rightsholder organizations representing publishing houses across the country, contacted the government in late 2023 expressing their concerns over the use of their material in training generative language models and demanding some sort of compensation were their contents to be used in the training of generative language models. In this context, this paper describes first at4 2 0 2 2 ] . [ 1 0 6 4 9 0 . 2 1 4 2 : r tempt at empirically evaluating the impact of copyrighted content in the training of LLMs for Norwegian. We introduce set of carefully curated datasets that are later used in the training of foundational, domain-tuned, and instruction-tuned models. We establish the proper training conditions to be able to compare models trained on the different datasets. newly created benchmarking suite is used to evaluate the performance of each individual model and make the comparison meaningful. As collaborative effort among several institutions, we hope the results of our investigations set the basis to guide policymaking and proper compensation schemes for authors and rightsholders in Norway."
        },
        {
            "title": "2 Methodology",
            "content": "The methodology involves comprehensive analysis that spans several stages. Initially, diverse corpus of primarily Norwegian language data is assembled, incorporating both copyrighted and noncopyrighted materials, plus materials commonly found on the Internet. This corpus serves as the foundation for training various LLMs, each with different configurations and access levels to copyrighted content. By comparing the performance of these models across range of linguistic and natural language processing tasks, such as text generation, translation, summarization, questionanswering, sentiment analysis and more, we seek to quantify the specific contributions of copyrighted materials to the overall model quality. To ensure robustness and reliability, the evaluation framework focuses on generation capabilities, natural language understanding, and linguisticallyinspired metrics. Quantitative measures include traditional NLP metrics like accuracy, F1, BLEU, and ROUGE, which provide assessments of model accuracy and fluency. Linguistic analysis, on the other hand, involves assessing the coherence, language variability, and contextual relevance of the generated outputs. This dual approach allows for nuanced understanding of how copyrighted materials impact the performance and utility of LLMs."
        },
        {
            "title": "3 Data Collection",
            "content": "At the core of our research sits the need for guidelines and well-designed compensation mechanisms in Norway. Hence, we had to abide by Norwegian law (Åndsverkloven) for the definition of which works are protected under copyright in the country and what we are permitted to do with said works (which might differ from other jurisdictions). In line with provisions that allow research on language technology and data mining, and with the consent of the Norwegian rightsholder organizations, this study primarily relied on material legally deposited at the National Library of Norway. Materials not deposited were therefore omitted. As the institution has digitized almost all books and around 85% of the newspapers ever published in the country (Nasjonalbiblioteket, 2024), we believe this set of works is representative enough to draw valid conclusions from its study. We collaborated with the National Library of Norway and the rightsholder organizations to gain access to protected materials for the building of our datasets. Others sources were also added via specific agreements with third party organizations such as the Norwegian Broadcasting Corporation (NRK), the TV channel TV2, and the newspaper conglomerates Amedia and Schibsted. Publicly available text collections like Wikipedia, datasets from the HPLT (de Gibert et al., 2024) and CulturaX (Nguyen et al., 2024) projects, and code in different programming languages from Lozhkov et al. (2024) were also added with the objective of setting up realistic training scenario where using Internet crawled sources is commonplace (see Figure 1). This mixture of data allowed us to evaluate the impact of high-quality copyrighted material versus non-copyrighted alternatives. The models trained on the copyrighted materials will not be made publicly available for further use and only serve the purpose of this study."
        },
        {
            "title": "3.1 Core Datasets",
            "content": "We followed the recipe of the Norwegian Colossal Corpus (NCC) by Kummervold et al. (2022), adapting and updating it with new up-to-date contents, re-OCRing some materials, enriching their metadata, and ensuring uniform format and functionality across datasets. The preparation involved cleaning, deduplication, metadata tagging, and language balancing to maintain consistent representation of Norwegian, preventing other languages from overshadowing it. The corpus was divided into two main datasets: base dataset excluding copyrighted materials1, and an extended dataset that included all collected texts, thus including all of base (see Table 1). 1Except for newspapers that fall under the Language Technology Use (Språkteknologiformål), as they were already included in other datasets such as NCC. Figure 1: Treemap with the number of words contributed by each source in the extended dataset. Dataset Documents Words Subset base extended 60,182,586 125,285,547 40,122,626,81 82,149,281,266 Table 1: Number of documents and words in each of the core datasets. Words refer to whitespaceseparated sub-strings. We decided to include texts from other Scandinavian (Swedish, Danish, and Icelandic) and English sources to boost the performance of the resulting language models. To ensure that languages other than Norwegian, and primarily coming via Internet crawling, were balanced, we adapted the perplexitybased sampling strategy from De la Rosa et al. (2022) to maintain high quality in the selected data. Instead of sampling fixed number of documents, parameters for Gaussian curve were calculated from 500,000-1M random documents per source, utilizing Wikipedia-based Kneser-Ney language models from Wenzek et al. (2019) and Conneau et al. (2020)2. We also modified the perplexity calculation to account for normalized text. These parameters then guided dataset sub-sampling to target ratios per language, reducing foreign language content while maintaining quality. Moreover, we also built three custom perplexity models for specific Norwegian domains that proved too divergent from Wikipedia: books, newspapers, and government documents. These perplexity models were used to score each document in the datasets. Based on their perplexity scores, the documents were further divided into three segments corresponding to their quartile distribution. Documents with scores below the first quartile were clas2Built with KenLM (Heafield, 2011). books newspapers books + newspapers fiction books nonfiction books nonfiction books + newspapers original books original books + newspapers translated books Documents Words 492,281 46,764,024 47,256,305 117,319 359,979 42,083,532 392,887 47,156,911 96,258 18,122,699,498 9,001,803,515 26,078,915,554 5,287,109,366 12,384,323,012 20,340,539, 13,352,261,605 22,354,065,120 4,695,814,506 Table 2: Number of documents and words in each copyrighted subset of the extended dataset. sified as good, those between Q1 and the third quartile as medium, and those above Q3 were considered bad. To ensure robust training, the documents in each segment were randomized. It is also important to notice than in order to maintain the language distributions for foreign languages with respect to the amount of Norwegian texts, the total number of documents in foreign languages in the extended dataset is consequently higher and slightly different (due to the sampling strategies) than that of base; we keep the same ratios."
        },
        {
            "title": "3.2 Domain Specific Subsets",
            "content": "The copyrighted materials present in the extended dataset were further divided into groups. These subsets were carefully designed to test the effect of specific features of the copyrighted content. As illustrated in Table 2, we created subsets for books vs newspapers, fiction vs factual content, and original Norwegian vs translations. 3.3 Instruction-tuning Datasets To align the model more closely with human objectives and assess whether instruction tuning with limited high-quality data can enhance the performance of our pre-trained models across various tasks, we built upon prior work and collected nearly 5,000 instructions annotated by research assistants formatted as (instruction, input, output) triplets. In this format, instruction refers to the directive provided by humans for the model, input is an optional field containing task-related information, and output denotes the desired response that follows the given instruction."
        },
        {
            "title": "4 Model Training",
            "content": "The training phase involved multiple models, each based on the Mistral architecture (Jiang et al., 2023). The training was conducted in the following stages. 1. To measure the overall impact of copyrighted material and its effect in realistic scenarios, we conducted pre-training on the base and extended datasets, both from scratch and warmstarted: using the existing weights of the pretrained model Mistral 7B v0.13. These four core models were trained on the same amount of data (64,000 steps of 4 million sub-word tokens each) using identical sets of hyperparameters. 2. To further isolate the effect of different ablations of the copyrighted materials, we continuously fine tuned the model trained on base from scratch for an extra 10,000 steps on each of the 9 domain specific subsets. 3. The core models were also fine tuned on the instruction data for 4 iterations to evaluate their performance on downstream tasks. Overall, we trained 17 models (7 billion parameters each) using total of 270,000 GPU-hours. Model training specifications are shown in Table 3. The infrastructure for training included the LUMI supercomputer, Idunn cluster, and Google TPUs through the Tensor Research Cloud program.4 Besides, we trained two tokenizers with the base and 3https://huggingface.co/mistralai/ Mistral-7B-v0.1 4To assess the deviation introduced by differences in training infrastructures and platforms across the participating institutions, each team trained control model with 1.5B parameters based on the Llama 2 architecture. The training setups were identical, utilizing the base dataset. After comparing the validation loss curves from each team, we found that the curves were almost the same, with deviation of less than 0.05 in terms of the final convergence validation loss. extended datasets separately, both with the same vocabulary size of 32, 768. After an initial test of the fertility5 of the tokenizers, we found the difference between them was only 0.0013. Therefore, we decided to use the same tokenizer trained with the base dataset for all the models."
        },
        {
            "title": "5 Evaluation Framework",
            "content": "Evaluating generative LLMs is far from solved problem, in particular for Norwegian, where there were few existing resources at the outset of our work. We compiled set of 28 common NLP tasks, encompassing range of different metrics to assess the performance of each of the models. These tasks can be grouped into 9 higher level skills: 1. Sentiment Analysis, here defined as binary polarity classification on both the sentenceand document-level, based on the existing NoReC datasets of professional reviews (Velldal et al., 2018; Øvrelid et al., 2020). 2. Fairness & Truthfulness. Fairness refers to the absence of bias in the predictions and outputs of model. Evaluating fairness ensures that the model does not favor or discriminate against particular groups based on attributes like race, gender, or ethnicity. This skill was evaluated using newly-created dataset,6 which covers wide range of bias types, including race, religion, gender, geography, occupation, age etc. Truthfulness involves the accuracy and reliability of the information produced by the model, ensuring it generates factual and verifiable content. This skill was evaluated using the newly-created NorTruthfulQA dataset which adapts the English TruthfulQA dataset (Lin et al., 2021) to Norwegian and assesses whether model is truthful in selecting and generating answers to questions that involve common human misconceptions.7 3. Reading Comprehension, which measures the ability of model to understand and interpret text. It involves answering questions 5Fertility expresses the fragmentation rate of tokenizer and is #tokens/#words in one corpus. 6https://huggingface.co/datasets/ mimir-project/mimir-bias 7https://huggingface.co/datasets/ltg/ nortruthfulqa_mc and https://huggingface. co/datasets/ltg/nortruthfulqa_gen Initialization GPU/hours Accelerator Model base extended base (warm) extended (warm) Core Models From scratch From scratch Mistral 7B v0.1 Mistral 7B v0.1 Domain Tuned Models base + fiction books base + nonfiction books base + nonfiction books + newspapers base + newspapers base + books base + books + newspapers base + original books + newspapers base + original books base + translated books base base base base base base base base base base instruct extended instruct base (warm) instruct extended (warm) instruct Instruction Fine Tuned Models base extended base (warm) extended (warm) 50K AMD MI250X 50K AMD MI250X 13.8K NVIDIA H100 55.6K AMD MI250X 7.5K AMD MI250X 7.5K AMD MI250X 7.5K AMD MI250X 4.8K Google TPUv4 4.8K Google TPUv4 4.8K Google TPUv4 9.1K AMD MI250X 9.1K AMD MI250X 9.1K AMD MI250X 14.2 NVIDIA H100 14.2 NVIDIA H100 14.2 NVIDIA H100 14.2 NVIDIA Table 3: Model training specifications, where Model represents the model identifier and the data used for training, Initialization represents the base model used for training, GPU/hours indicates the total GPU hours required for model training, and Accelerator represents the type of accelerator used. about given passage, summarizing content, or explaining the meaning of specific phrases or sentences. This skill estimates how well the model grasps the context and details in the text. It was evaluated using the existing extractive question-answering NorQUAD dataset (Ivanova et al., 2023). 4. World Knowledge, which assesses the extent of factual information language model has about the world. This includes historical events, geographical data, scientific facts, cultural knowledge, and more. The model should correctly answer questions or provide information based on real-world knowledge. This skill was evaluated using the newlycreated NorOpenBookQA8 dataset, which adapts the English OpenBookQA dataset (Mihaylov et al., 2018) to Norwegian and the NRK-Quiz-QA9 dataset, consisting of collection of online quizzes on number of different topics. 5. Commonsense Reasoning, which involves the ability of model to make logical inferences based on everyday knowledge and understanding of the world. The model should reason about situations that require practical, everyday knowledge that people take for granted. This skill was evaluated using the newly-created NorCommonsenseQA dataset10 of multiple-choice commonsense question answer-pairs which adapts the corresponding English CommonsenseQA dataset (Talmor et al., 2019) to Norwegian. 6. Norwegian Language evaluation focuses on the ability of model to understand and generate text in Norwegian, specifically its grammar, structure, and sentence construction. This skill is important for assessing how well the model handles Norwegian and their specific syntactic rules. It was evaluated using the existing NCB (Farsethås and Tjøstheim, 2024) and ASK-GEC (Jentoft, 2023) datasets, and the newly-created NorIdiom11 dataset. 7. Summarization, which measures the ability of model to condense longer pieces of text into shorter, coherent summaries that capture the main points. This skill is crucial for applications where users need quick understanding of large volumes of information, such as news articles or research papers. It was evalu8https://huggingface.co/datasets/ltg/ 10https://huggingface.co/datasets/ltg/ noropenbookqa norcommonsenseqa 9https://huggingface.co/datasets/ltg/ 11https://huggingface.co/datasets/ nrk mimir-project/noridiom ated using the MediaFutures datase.12 8. Translation, which assesses how accurately language model can convert text from one language to another while preserving the meaning, tone, and context. It was evaluated using the existing Tatoeba dataset (Tiedemann, 2020). 9. Variation and Readability, which consists of measuring the lexical diversity of model by looking at the amount of redundancy in the text it produces and at the readability of these texts measured by average sentence length and the proportion of long words. As such, this skill evaluation did not require any specific benchmarking datasets. [A complete description of the evaluation framework is available in forthcoming publication which is currently under review.] All the newly created datasets will be made publicly available upon paper acceptance. We follow the standard in-context learning evaluation design for pretrained decoder-only language models (e.g., Brown et al., 2020; Touvron et al., 2023), which includes zeroshot and few-shot evaluation. In this paper, for the sake of simplicity, we selected the most common metrics per task and aggregated scores using simple cumulative sum per higher-level skill. In order to aggregate results into an overall score, with the caveats of aggregating metrics of different nature, scores were extracted for the best available {0, 1, 4, 16}-shot configuration for each task and the best score for each of the prompts (we formulated five different prompts for each task, but the evaluation is based on the best performing prompt for each of them). Metrics were normalized to exhibit the same higher-is-better behavior in range of 0 to 100."
        },
        {
            "title": "6 Results",
            "content": "The evaluation of the trained models demonstrated that incorporating copyrighted materials provided measurable boost in performance across range of NLP tasks. To illustrate the overall performance differences, Figure 2 shows the total scores across all skills, averaged by task for each model. Nonaggregated scores for all tasks and models are availModel SA FT RC WK RC NL VR extended base extended (warm) base (warm) 3 4 2 1 2 3 3 3 4 1 2 3 4 2 1 2 3 1 1 2 4 1 3 1 3 2 2 3 4 1 2 3 1 4 Table 4: Results for ranking the core models on all tasks by skill via (i) finding the best kshot configuration for each task and (ii) aggregating metric-wise rankings. SA=Sentiment Analysis. FT=Fairness & Truthfulness. RC=Reading NL=Norwegian Language. Comprehension. WK=World Knowledge. CR=Commonsense Reasoning. S=Summarization. T=Translation. VR=Variation & Readability. Lower is better. able at the Mímir repository.13 6.1 Core Models As shown in Table 4 and Figure 3, the performance analysis of core models across various tasks reveals distinct strengths for different configurations. The base (warm-started) configuration consistently excels in Sentiment Analysis, World Knowledge, and Norwegian Language. In contrast, the extended (warm-started) configuration leads in Fairness & Truthfulness, Reading Comprehension, Commonsense Reasoning, Translation, and Variation & Readability, indicating its robust performance for language-intensive tasks. The base configuration generally lags behind others, scoring the lowest across multiple tasks. Meanwhile, the extended configuration performs well, particularly in Summarization. Furthermore, it indicates that we could leverage the existing metadata available at the National Library to tailor subsets of the copyrighted material and build models that excel at specific tasks. However, the difference between the base and extended warm-started models is very small. Further testing is required to assess whether this difference is still statistically significant. While warm-started models generally outperformed models trained from scratch, there was reduced sensitivity to the presence of copyrighted materials. This suggests that the pre-existing weights, which were primarily trained on English data, diminished the impact of adding high-quality Norwegian copyrighted texts (see more on that in 7). 12https://huggingface.co/datasets/ 13https://github.com/mimir-project/ SamiaT/NorSumm mimir-evaluation-suite extended base base + books base + newspapers base + books + newspapers base + fiction books base + nonfiction books base + nonfiction books + newspapers base + original books base + original books + newspapers base + translated books extended (warm) base (warm) Sentiment Analysis World Knowledge Summarization Fairness & Truthfulness Commonsense Reasoning Norwegian Language Translation Variation & Readability Reading Comprehension 441.83 413.98 427.30 440. 435.64 408.20 427.40 440.97 427.87 436.81 409.54 482.98 480.28 Figure 2: Total summed scores across all skills averaged by task for each model. Best scores among from-scratch models underlined, best overall from-scratch in bold. Dashed line at the base score. 413.98 441.83 480. 482.98 448.01 base extended base (warm) extended (warm) Mistral 7B v0.1 Sentiment Analysis Fairness & Truthfulness Reading Comprehension World Knowledge Commonsense Reasoning Norwegian Language Summarization Translation Variation & Readability Figure 3: Cumulative score of the core pre-trained models under different regimes. Including original Mistral 7B v0.1 for reference. base extended base + nonfiction books + newspapers base + newspapers base + original books + newspapers 6.73% 6.52% 6.37% 5.51% base + nonfiction books base + books 3.24% 3.22% base + fiction books -1.40% Figure 4: Average percentage gains over the performance of the base model. Negative results indicate decrease in performance over base, positive results gain."
        },
        {
            "title": "6.2 Domain-tuned Models",
            "content": "To further explore the specific effects of different types of training data, we analyzed the gains in performance by focusing on different sub-corpora, such as newspapers, books, and mixed datasets. Figure 4 provides an overview of the average percentage gains for models trained on various data configurations compared to the base model. It shows that the extended model exhibits the highest average gain at 6.73%, indicating substantial overall improvement. The addition of nonfiction books and newspapers follows with 6.52% gain, and the addition of only newspapers shows 6.37% improvement. Other configurations, such as adding original books and newspapers or nonfiction books, also demonstrate positive gains of 5.51% and 3.22%, respectively. Conversely, the addition of fiction books is the only one to show negative performance, with decrease of 1.40%. Interestingly, when decomposed by skill, the addition of fiction books makes the model excel at generating more diverse texts. 6.3 Instruction-tuned Models Lastly, as shown in Figure 5, when the core models are further fine-tuned on data to follow instructions, the gains across models are all consistent, showing that the core advantage lies in the pre-training data, while further training on instructions gives consistent boost in performance. Moreover, instruction tuning also seems to reduce the gap between the base and extended configurations, suggesting that copyrighted material might become less relevant as supervised finetuning datasets increase in size in the post-training phases of LLMs. Interestingly, adding Norwegian instruction data on top of the base model seems enough to improve over the performance of Mistral 7B v0.1."
        },
        {
            "title": "7 Discussion",
            "content": "Our findings underline the value of copyrighted materials in improving the performance of generative language models, particularly for specialized NLP tasks in Norwegian. The inclusion of curated, highquality texts provides substantial advantage in terms of language richness, coherence, and contextspecific understanding. At the same time, these advantages are much less pronounced for the models warm-started with the weights pre-trained on other languages (mostly English). We see two possible reasons for this: 1. The amount of training data matters more than its quality or licensing status. Warm-started models are effectively trained on more data than the from-scratch models, and at some point adding even more data brings diminishing returns (with given model size). 2. Copyrighted Norwegian data is indeed beneficial for LLMs, but the original models used for warm-starting were presumably already pre-trained on parts of this data. The exact composition of the training sets of models like Mistral is not known; it is quite probable that they may contain copyrighted content. Thus, continuous pre-training on (partially) the same content did not bring the expected benefits to the warm-started extended models."
        },
        {
            "title": "7.1 Ethical and Legal Considerations",
            "content": "The use of copyrighted materials in model training raises significant ethical and legal questions. The observed improvements in model quality must be balanced against the rights of content creators, who have not consented to the use of their work. This highlights the need for clear guidelines and fair compensation mechanisms that recognize the value of copyrighted materials in LLM development. 7."
        },
        {
            "title": "Implications for Policy",
            "content": "The empirical evidence gathered in our research is crucial for informing copyright policy in the digbase base instruct extended extended instruct base (warm) base (warm) instruct extended (warm) extended (warm) instruct Mistral 7B v0.1 413.98 445. 441.83 458.36 480.28 503.36 482.98 503. 448.01 Sentiment Analysis Fairness & Truthfulness Reading Comprehension World Knowledge Commonsense Reasoning Norwegian Language Summarization Translation Variation & Readability Figure 5: Total scores (sum) of all averaged scores per skill for the core models and their instruct versions, with original Mistral 7B v0.1 for reference. Dashed line at the base score. Best scores in bold, second best underlined. ital age. Policymakers can use these findings to establish frameworks that ensure creators are adequately compensated, balancing the needs of LLM innovation with the rights of authors and publishers. This is particularly relevant in light of ongoing lawsuits against major AI companies, underscoring the necessity of legal clarity."
        },
        {
            "title": "8 Conclusion",
            "content": "Our study represents pioneering effort to quantify the impact of copyrighted materials on LLMs for Norwegian. Our results indicate that high-quality copyrighted content significantly enhances model performance, especially for complex NLP tasks. However, these benefits bring forth ethical and legal challenges that must be addressed to ensure sustainable and fair approach to LLM development. By providing empirical evidence, we aim to contribute to the ongoing discourse on the role of copyright in AI and inform future policies that support both innovation and the rights of content creators."
        },
        {
            "title": "9 Future Work",
            "content": "Future work should focus on testing models at various scales and different pre-trained open weights to better understand how dataset composition affects performance. By experimenting with models of different sizes, we could identify any scaling thresholds where the impact of copyrighted material varies significantly. Additionally, the observed effects of fiction on model performance highlight the need to 1) examine how different types of fictionsuch as fantasy or historical fictionimpact tasks like Sentiment Analysis and Commonsense Reasoning, and 2) design new and adequate benchmarks for evaluating the contribution of fiction in Norwegian LLMs for tasks such as creative writing, plot understanding, or descriptive language use. This targeted investigation could clarify the role of fiction in model training and help refine data curation strategies. Lastly, exploring genre-specific influences more deeply, including essays, technical writing, and narrative nonfiction, may reveal distinct benefits or biases tied to each genre. Analyzing these nuances, even in diachronic manner, will guide balanced genre representation in datasets and support the development of better performing models."
        },
        {
            "title": "10 Distribution",
            "content": "The base dataset and models were intended to be freely distributed, as all materials included were granted redistribution permissions under different agreements. After we communicated the results of our investigations to the different partners, some rightholders demanded reinterpretation of the agreements in the light of the results and this new era of generative AI. This will most likely prohibit us from sharing publicly the exact models trained within the Mímir project, but we will publish Norwegian language model trained on version of the base dataset with this material excluded."
        },
        {
            "title": "References",
            "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Javier De la Rosa, Eduardo González Ponferrada, Paulo Villegas, Pablo González de Prado Salas, Manu Romero, and María Grandury. 2022. BERTIN: Efficient pre-training of Spanish language model using perplexity sampling. Procesamiento de Lenguaje Natural, 68:1323. Hans Christian Farsethås and Joakim Tjøstheim. 2024. Norwegian comma benchmark. https: //huggingface.co/datasets/hcfa/ncb. Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema RamírezSánchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, and Jörg Tiedemann. 2024. new massive multilingual dataset for high-performance language technologies. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 11161128, Torino, Italia. ELRA and ICCL. Kenneth Heafield. 2011. Kenlm: Faster and smaller language model queries. In Proceedings of the sixth workshop on statistical machine translation, pages 187197. Sardana Ivanova, Fredrik Andreassen, Matias Jentoft, Sondre Wold, and Lilja Øvrelid. 2023. NorQuAD: Norwegian question answering dataset. In Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 159168, Tórshavn, Faroe Islands. University of Tartu Library. Matias Jentoft. 2023. Grammatical error correction with byte-level language models. Masters thesis, University of Oslo. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Per Kummervold, Freddy Wetjen, and Javier de la Rosa. 2022. The Norwegian colossal corpus: text corpus for training large Norwegian language models. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 38523860, Marseille, France. European Language Resources Association. Per Kummervold, Javier De la Rosa, Freddy Wetjen, and Svein Arne Brygfjeld. 2021. Operationalizing national digital library: The case for Norwegian transformer model. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 2029, Reykjavik, Iceland (Online). Linköping University Electronic Press, Sweden. Andrey Kutuzov, Jeremy Barnes, Erik Velldal, Lilja Øvrelid, and Stephan Oepen. 2021. Large-scale contextualised language modelling for Norwegian. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 3040, Reykjavik, Iceland (Online). Linköping University Electronic Press, Sweden. Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. Peng Liu, Lemei Zhang, Terje Farup, Even W. Lauvrak, Jon Espen Ingvaldsen, Simen Eide, Jon Atle Gulla, and Zhirong Yang. 2024. Nlebench+norglm: comprehensive empirical analysis and benchmark dataset for generative language models in norwegian. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, WenDing Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2024. Starcoder 2 and the stack v2: The next generation. Kevin Madigan. 2024. Mid-year review: Ai lawsuit developments in 2024. Accessed: 2024-10-07. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium. Association for Computational Linguistics. Nasjonalbiblioteket. 2024. Årsrapportar. Accessed: 2024-10-26. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2024. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 4226 4237, Torino, Italia. ELRA and ICCL. Lilja Øvrelid, Petter Mæhlum, Jeremy Barnes, and Erik Velldal. 2020. fine-grained sentiment dataset for Norwegian. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5025 5033, Marseille, France. European Language Resources Association. Joe Panettieri. 2024. Generative ai lawsuits timeline: Legal cases vs. openai, microsoft, anthropic, nvidia, intel and more. Accessed: 2024-10-07. David Samuel, Andrey Kutuzov, Samia Touileb, Erik Velldal, Lilja Øvrelid, Egil Rønningstad, Elina Sigdel, and Anna Palatkina. 2023. NorBench benchmark In Proceedings for Norwegian language models. of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 618633, Tórshavn, Faroe Islands. University of Tartu Library. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Jörg Tiedemann. 2020. The tatoeba translation challenge realistic data sets for low resource and multilingual MT. In Proceedings of the Fifth Conference on Machine Translation, pages 11741182, Online. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. Erik Velldal, Lilja Øvrelid, Eivind Alexander Bergem, Cathrine Stadsnes, Samia Touileb, and Fredrik Jørgensen. 2018. NoReC: The Norwegian review corpus. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Theresa M. Weisenberger, Diana C. Milton, Harrison A. Enright, and Jiwon Kim. 2024. Case tracker: Artificial intelligence, copyrights, and class actions. Accessed: 2024-10-07. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. 2019. CCNet: Extracting high quality monolingual datasets from web crawl data. In International Conference on Language Resources and Evaluation."
        }
    ],
    "affiliations": [
        "National Library of Norway",
        "Norwegian University of Science and Technology",
        "University of Oslo"
    ]
}