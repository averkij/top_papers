{
    "paper_title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
    "authors": [
        "Linke Ouyang",
        "Yuan Qu",
        "Hongbin Zhou",
        "Jiawei Zhu",
        "Rui Zhang",
        "Qunshu Lin",
        "Bin Wang",
        "Zhiyuan Zhao",
        "Man Jiang",
        "Xiaomeng Zhao",
        "Jin Shi",
        "Fan Wu",
        "Pei Chu",
        "Minghao Liu",
        "Zhenxiang Li",
        "Chao Xu",
        "Bo Zhang",
        "Botian Shi",
        "Zhongying Tu",
        "Conghui He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, a novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes a meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides a flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes a robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies. The codes and dataset is available in https://github.com/opendatalab/OmniDocBench."
        },
        {
            "title": "Start",
            "content": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations 4 2 0 2 0 1 ] . [ 1 6 2 6 7 0 . 2 1 4 2 : r Linke Ouyang1 Yuan Qu1 Hongbin Zhou1 Jiawei Zhu1 Rui Zhang1 Qunshu Lin2 Bin Wang1 Zhiyuan Zhao1 Man Jiang1 Xiaomeng Zhao1 Jin Shi1 Fan Wu1 Pei Chu1 Minghao Liu3 Zhenxiang Li1 Chao Xu1 Bo Zhang1 Botian Shi1 Zhongying Tu1 Conghui He1 1Shanghai AI Laboratory 2Abaka AI 32077AI"
        },
        {
            "title": "Abstract",
            "content": "Document content extraction is crucial in computer vision, especially for meeting the high-quality data needs of large language models (LLMs) and retrieval-augmented generation (RAG) technologies. However, current document parsing methods suffer from significant limitations in terms of diversity and comprehensive evaluation. To address these challenges, we introduce OmniDocBench, novel multi-source benchmark designed to advance automated document content extraction. OmniDocBench includes meticulously curated and annotated high-quality evaluation dataset comprising nine diverse document types, such as academic papers, textbooks, slides, among others. Our benchmark provides flexible and comprehensive evaluation framework with 19 layout category labels and 14 attribute labels, enabling multi-level assessments across entire datasets, individual modules, or specific data types. Using OmniDocBench, we perform an exhaustive comparative analysis of existing modular pipelines and multimodal end-to-end methods, highlighting their limitations in handling document diversity and ensuring fair evaluation. OmniDocBench establishes robust, diverse, and fair evaluation standard for the document content extraction field, offering crucial insights for future advancements and fostering the development of document parsing technologies. The codes and dataset is available in https://github. com/opendatalab/OmniDocBench. 1. Introduction Document parsing is foundational task in computer vision, focused on accurately extracting content from documents [18, 36, 39, 41, 45]. High-quality document content The authors contributed equally. Project lead. Corresponding author (heconghui@pjlab.org.cn). extraction typically involves the integration of multiple algorithmic modules. Layout detection algorithms identify different content areas on page, OCR technology converts images of text regions into text, while formula and table recognition models identify specific regions and transform them into corresponding source code. These modules and reading order algorithms form comprehensive process of converting documents into machine-readable formats. With large models increasingly requiring high-quality data, the importance of document content extraction has become more pronounced. Although vast amounts of data are available online for training, knowledge-rich document data is relatively scarce. Documents such as academic papers and technical reports contain rich structured information that can significantly enhance the knowledge depth of large models. Moreover, the development of retrieval-augmented generation (RAG) [10, 21] technology relies on extracting accurate information from documents to improve the quality and relevance of generated content. Consequently, research in document content extraction has intensified, leading to series of pipeline-based high-quality document extraction algorithms [36] and the emergence of end-to-end multimodal large model solutions [3, 5, 6, 27, 39, 40, 42]. These methods have significantly improved document content parsing quality, providing robust support for the needs of large models and RAG technology. In analyzing current module-based pipeline and multimodal end-to-end methods, we identified several limitations. For instance, methods like Marker and MinerU, which are mainstream pipeline methods, primarily evaluate individual modules on academic paper data, lacking document diversity and comprehensive evaluation results. Although MinerU considers the generalization of diverse data, it only demonstrates this through single model and visualization results, lacking overall end-to-end evaluation. Multimodal large model methods [3, 5, 27, 39, 40], while easier to use than pipeline methods, lack performance validation on diverse documents, and some evaluation metrics are in1 Figure 1. OmniDocBench Data Diversity. It contains 9 PDF page types, along with Layout Annotations and Recognition Annotations. Furthermore, there are 5 Page Attributes, 3 Text Attributes, and 6 Table Attributes. adequate. Additionally, these methods often use data similar to their training set distribution for comparison, resulting in unfair evaluations. Overall, current document content extraction faces the following challenges: contrast, this paper proposes document content extraction benchmark, OmniDocBench, characterized by diverse types, detailed annotations, and reasonable evaluation (Figure 1). The specific contributions are as follows: Limited document types. Current evaluations mostly focus on single type of academic paper, while real-world scenarios include textbooks, exam papers, financial reports, newspapers, magazines, and other document types. Pipeline-based methods typically evaluate specific algorithmic modules, such as OCR, layout detection, or formula recognition, while the overall quality of parsing results requires comprehensive metrics. Monotonous evaluation dimensions. Inadequate evaluation metrics. Multimodal large model approaches attempt to evaluate document parsing quality across multiple dimensions, such as dividing document content into text, formulas, tables, etc. However, these models commonly employ evaluation metrics such as BLEU scores or Edit distances, which fail to accurately and fairly assess parsing effectiveness when dealing with markup languages like LaTeX or HTML that allow diverse syntactic expressions. Building diverse, comprehensive, and accurate evaluation system poses significant challenges, requiring diverse and high-quality data annotation and reasonable evaluation metrics. While READOC extends the evaluation scope to include GitHub README files based on Nougat, there remains substantial gap in real-world diversity, and the evaluation dimensions lack consideration of attributes. In High-quality, diverse evaluation set: Through automated annotation, manual verification, and expert review, we construct comprehensive, detailed, high-quality OmniDocBench evaluation set, encompassing nine types of diverse document pages, textbooks, exam questions, and research reports. including papers, Flexible and comprehensive evaluation dimension support: The OmniDocBench validation set covers 19 layout category labels and 14 attribute labels. To facilitate user evaluation from an overall, single module, or different data types, we provide end-to-end evaluation, single algorithm module evaluation, and attribute-based evaluation, covering various evaluation needs. Comprehensive evaluation of mainstream methods: Based on OmniDocBench, we conduct comprehensive evaluation of current mainstream modular pipeline and end-to-end large model methods, providing fairer assessment of existing methods and summarizing the shortcomings of current document parsing methods, thereby guiding further development in document parsing. 2. Related Work 2.1. Traditional Document Content Extraction Document Content extraction remains challenging task, and there is yet to emerge unified benchmark tailored for 2 Benchmark Single-Module Eval Benchmark Robust Reading [19] PubLayNet [43], DocBank [24], DocLayNet [31], M6Doc [7] PubTabNet [47],TableX [9], TableBank [23] Im2Latex-100K [8],UniMER-Test [34] End-to-end Eval Benchmarks Nougat [5] Fox [27] GOT OCR 2.0 [39] READoc [26] OmniDocBench Document Categories Annotaion Type BBox Text Table Order Single-Module Eval End-to-End Eval Formula OCR DLA TR MFR OCR ROD TR MFR 1 1, 1, 5, 6 1, 1, 1 1 1 2 2 9 (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) Table 1. comparison between OmniDocBench and existing DCE benchmarks. OCR: Optical Character Recognition; DLA: Document Layout Analysis; MFR: Math Formula Recognition; TR: Table Recognition; ROD: Reading Order Detection real-world scenarios. Traditional algorithms typically employ multiple expert modules to handle different extraction subtasks, such as document layout detection [11, 16, 32, 46], optical character recognition (OCR) [14, 22, 28, 33, 37], formula recognition [4, 25, 34, 44], and table recognition [15, 17, 22]. While expert models of these subtasks are advancing rapidly, recent work such as Mineru [36] attempts to concatenate multiple expert modules into pipeline and provides high-precision open-source solution for document content extraction. READOC [26] also unifies heterogeneous evaluation methods from the perspective of Document Structure Extraction, breaking down texts, images, formulas, tables, and other dimensions for evaluation, thus offering solution-oriented towards real-world scenarios for DSE tasks. However, due to the complexity of Document data sources and the intricacies of PDF document information, previous efforts still fall short in terms of data diversity, failing to cover the categories users encounter in practical applications. Similarly, there is an issue with the explainability of document parsing. 2.2. VLM-based Document Content Extraction The emergence of Vision-Language Models (VLMs) [1, 6, 12, 38] has revolutionized the field of document content extraction. These models leverage multi-modality capability to achieve remarkable performance in document understanding tasks. Document extraction tools powered by VLMs excel at comprehending both visual layouts and textual content, effectively handling complex document structures while capturing rich contextual information. Representative works such as Nougat [5], Vary [40], Fox [27], and GOT [39], along with recent advances [13, 29], demonstrate significant progress in automated document parsing and comprehension. Despite these advances, the field lacks standardized and unified benchmark for evaluating VLMbased document extraction task. This absence has hindered objective assessment of PDF document processing capabilities and impeded fair comparison across different approaches. To address this limitation, we present OmniDocBench, comprehensive end-to-end benchmark designed specifically for evaluating VLM-based document parsing in real-world scenarios. 2.3. Benchmark for Document Content Extraction An end-to-end benchmark for PDFs can intuitively reflect the effectiveness of PDF extraction tools, which is crucial for their iteration and selection. However, current benchmarks predominantly focus on module-level evaluations; we have listed related benchmarks in Table 1. Additionally, while there are existing end-to-end benchmarks, they lack detailed annotation rules and suffer from insufficient diversity, as well as unreasonable metrics for formula and table evaluations. For example, READOC [26] covers only two types of sourcesarXiv and GitHuband uses EDS [20] and TEDS [47] to compute metrics for formulas and tables, which may lead to inaccuracies CDM [35]. Therefore, there is need for more finely annotated, diverse, and reasonably evaluated end-to-end benchmark. 3. OmniDocBench Dataset Constructing diverse and comprehensive document parsing benchmark with precise annotations is formidable challenge. As illustrated in Figure 2, we have designed systematic and professional annotation framework for OmniDocBench, encompassing data acquisition, intelligent pre-annotation, and manual refinement. This ensures that OmniDocBench possesses the following key attributes: Page Diversity. We sourced document pages from variety of origins to ensure wide range of document types. Comprehensive Annotation. We meticulously annotributes, and 2 formula attributes. (3) Reading Order Annotations: Annotating the reading sequence of detected boxes. (4) Affiliation Annotations: For images, tables, formulas, and code blocks, we annotate captions and titles to distinguish them from main text. Similarly, for cross-page paragraphs, we annotate affiliation relationships. Content Recognition Annotations: Based on the format of the content area, we conduct the following three types of area annotations: (1) Text Annotations: Pure text annotations for titles, text paragraphs, and other plain text content. (2) Formula Annotations: LaTeX format annotations for inline formulas, display formulas, and subscripts. (3) Table Annotations: Providing both HTML and LaTeX annotations for table data. 3.2.2. Annotation Process For these annotation tasks on diverse pages, we design standardized process to ensure quality and efficiency, comprising intelligent pre-annotation, annotator correction, and expert quality inspection. Intelligent Pre-Annotation. Manually annotating entire documents is time-consuming and costly. To enhance efficiency, we employ state-of-the-art detection and recognition models for pre-annotation of layout detection and content recognition. Specifically, we use fine-tuned LayoutLMv3 [16] for layout detection annotations and PaddleOCR [22], UniMERNet [34], and GPT-4o [2] for text, formula, and table annotations, respectively. Annotator Correction. After layout detection phase, annotators refine the detection boxes and enhance annotations with reading order and affiliation details. Each character is verified to ensure accuracy in content recognition. For complex annotations of tables and formulas, requiring LaTeX and HTML formats, annotators use tools like Tables Generator 2 and latexlive 3 for verification and correction. Expert Quality Inspection. Despite thorough annotator corrections, the complexity of formulas and tables may result in residual issues. To address these, we use CDMs rendering techniques to identify unrenderable elements. These are then reviewed and corrected by three researchers to ensure accuracy and fidelity in the final annotations. 3.3. Dataset Statistics Page Diversity. OmniDocBench comprises total of 981 PDF pages across 9 distinct types. Each page is annotated with global attributes, including text language, column layout type, and indicators for blurred scans, watermarks, and colored backgrounds. Annotation Diversity: OmniDocBench contains over 10,0000 annotations for page detection and recognition: (1) 2https://www.tablesgenerator.com/ 3https://www.latexlive.com/ Figure 2. Overview of the OmniDocBench dataset construction. including bounding tated all elements on the pages, boxes, specific content, and various potential attributes. Annotation Accuracy. By integrating semi-automated annotation processes, annotator corrections, and expert quality checks, we ensure the reliability of all annotations. The following sections detail the data acquisition process, the annotation methodology, and statistical analysis of the final annotated dataset. 3.1. Data Acquisition During the data acquisition phase, we sourced document pages from diverse origins and used clustering algorithms to initially select visually diverse pages, followed by manual annotation of page attributes to finalize the OmniDocBench pages. Specifically, we collected 200,000 initial PDF documents from Common Crawl, Google, Baidu search engines, and internal data. Subsequently, we extracted visual features from these document pages using ResNet-50 and performed clustering using Faiss 1, sampling 6,000 visually diverse pages from 10 cluster centers. Finally, annotators provided page-level attribute annotations, including page type, layout type, and language type, and further balanced the selection to 981 samples for the final dataset. The OmniDocBench dataset includes pages with 9 types of pages, multiple layout categories, and various attribute annotations, covering wide range of real-world scenarios. 3.2. Data Annotation To ensure the comprehensiveness of OmniDocBenchs annotations, we conducted detailed annotations for layout detection and content recognition. 3.2.1. Annotation Types Layout Detection Annotations: Unlike typical layout detection tasks, OmniDocBench includes four comprehensive types of annotations: (1) Layout Bounding Box Annotations: Locating information for 19 types of regions such as titles, text paragraphs, tables, and images. (2) Layout Attribute Annotations: Detailed attribute annotations for detected boxes, including 3 text box attributes, 6 table at1https://github.com/facebookresearch/faiss 4 bles, display formulas, markdown tables (which are then converted into HTML format), and code blocks. Pure Text Extraction: After extracting special components, the remaining content is considered pure text. Paragraphs are separated by double line breaks, allowing them to participate in subsequent matching processes, thus aligning with reading order annotation units in the GTs. If no double line break exists, single line breaks are used for paragraph separation. Additionally, previously extracted code blocks are merged into the text category for processing. Inline Formula Format Converting: We standardized inline formulas within paragraphs to Unicode format. This was necessary because different models produce inconsistent outputs for inline formulas. For formulas originally written in Unicode, it is hard to extract them using regular expressions. Therefore, to ensure fair comparison, we do not extract inline formulas for separate evaluation. Instead, we include them in their Unicode format alongside the text paragraphs for evaluation. Reading Order Extraction: Upon completion of the extraction, the start and end positions of the extracted content in the original markdown are recorded for subsequent reading order calculation. 4.2. Matching Algorithm To avoid the impact of paragraph splitting on the final results, we proposed method, Adjacency Search Match, that merges and splits paragraphs in both GTs and Preds to achieve the best possible match. The specific strategy involves: i) Calculate metrix of Normalized Edit Distance between GTs and Preds. If the similarity between Pred and GT exceeds specific threshold, they are considered successful match. ii) For the rest, we apply fuzzy matching to determine whether one string is subset of another string. If so, we further apply the truncation and merging algorithm which would try to merge adjacent paragraph. This process would continue to merge more paragraph until the Normalized Edit Distance starts to decrease. After this process, the best match will be found for GTs and Preds. 4.3. Metric Calculation Ignore Handling: We implement an ignore logic for certain components in PDF page content, meaning they participate in matching but are excluded from metric calculations. This is mainly because of inconsistent output standards among models, which should not affect the validation results. For fairness, we ignore: (1) Headers, footers, page numbers, and page footnotes, which are handled inconsistently by different models. (2) Captions for figures, tables, and footnotes often have uncertain placement, complicating reading order. Additionally, some models embed table captions in HTML or LaTeX tables, while others treat them as plain text. Figure 3. OmniDocBench Evaluation Pipeline. More than 20,000 block-level annotations across 15 categories, including over 9,000 text paragraphs, 989 image boxes, 428 table boxes, and so on. All document components except headers, footers, and page notes are labeled with reading order information, totaling over 16,000 annotations. (2) The dataset also includes more than 80,000 span-level annotations across four categories, with 4,000 inter-line formulas and footnote markers represented in LaTeX format, while the remaining annotations are in text format. Annotation Attribute Diversity: (1) Text Attributes: All block-level annotations, except for tables and images, include text attribute tags. In addition to standard Chinese and English text, there are over 2,000 blocks with complex backgrounds and 146 with rotated text. (2) Table Attributes: Besides standard Chinese and English tables, there are 142 with complex backgrounds, 81 containing formulas, 150 with merged cells, and 7 vertical tables. 4. OmniDocBench Evaluation Methodology To provide fair and comprehensive evaluation for various models, we proposed an end-to-end evaluation pipeline consisting of several modules, including extraction, matching algorithm, and metric calculation, as shown in Figure 3. It ensures that OmniDocBench automatically performs unified evaluation on end-to-end DCE tasks, thereby producing reliable and effective evaluation results. 4.1. Extraction Preprocessing: The model-generated markdown text should be preprocessed, which includes removing images, eliminating markdown tags at the beginning of the document, and standardizing the number of repeated characters. Special Component Extraction: Extraction is primarily carried out using regular expression matching. To ensure that the extraction of content does not interfere with each other, it is necessary to follow specific order. The extraction sequence is as follows: LaTeX tables, HTML ta5 Method Type Methods TextEdit FormulaEdit FormulaCDM TableTEDS TableEdit Read OrderEdit OverallEdit Pipeline Tools Expert VLMs General VLMs EN ZH EN ZH EN MinerU Marker Mathpix GOT-OCR Nougat GPT4o Qwen2-VL InternVL2 0.058 0.141 0. 0.187 0.365 0.144 0.252 0.353 0.211 0.303 0.358 0.315 0.998 0.409 0.251 0.290 0.278 0.667 0. 0.360 0.488 0.425 0.468 0.543 0.577 0.868 0.454 0.528 0.941 0.606 0.572 0.701 66.9 18.4 71. 81.8 17.4 76.4 54.9 69.8 ZH 49.5 12.7 72.7 51.4 16.9 48.2 60.9 49. EN ZH EN ZH EN ZH EN ZH 79.4 54.0 77.9 53.5 40.3 72.8 59.9 63.8 62.7 45.8 68. 48.0 0.0 63.7 66.8 61.1 0.305 0.718 0.322 0.521 0.622 0.363 0.591 0.616 0.461 0.763 0. 0.594 1.000 0.474 0.587 0.638 0.079 0.138 0.105 0.141 0.382 0.128 0.255 0.317 0.288 0.306 0. 0.28 0.954 0.251 0.223 0.228 0.180 0.416 0.209 0.302 0.464 0.265 0.392 0.457 0.384 0.560 0. 0.429 0.973 0.435 0.408 0.464 Table 2. Comprehensive evaluation of document parsing algorithms on OmniDocBench: performance metrics for text, formula, table, and reading order extraction, with overall scores derived from ground truth comparisons. Model Type Models Book Slides Financial Report Textbook Pipeline Tools Expert VLMs General VLMs MinerU Marker Mathpix GOT-OCR Nougat GPT4o Qwen2-VL InternVL2 0.044 0.188 0.131 0.105 0.734 0.157 0.094 0. 0.124 0.327 0.168 0.222 0.958 0.163 0.08 0.098 0.033 0.087 0.202 0.067 1.000 0.348 0.145 0. 0.102 0.292 0.199 0.132 0.820 0.187 0.148 0.184 Exam Paper 0.159 0.423 0.278 0.204 0. 0.281 0.219 0.247 Magazine Academic Papers Notes Newspaper Average 0.072 0.134 0.138 0.198 0. 0.173 0.065 0.150 0.025 0.102 0.091 0.179 0.214 0.146 0.315 0.419 0.984 0.470 0.631 0.388 0. 0.607 0.298 0.226 0.148 0.270 0.648 0.771 0.871 0.751 0.79 0.903 0.188 0.255 0.276 0.252 0. 0.313 0.239 0.289 Table 3. End-to-end text recognition performance on OmniDocBench: evaluation using edit distance across 9 PDF page types."
        },
        {
            "title": "MinerU\nMarker\nMathpix",
            "content": "0.15 0.286 0.294 0.151 0.436 0.290 0.107 0.290 0.182 0.136 0.337 0.255 0.0004 0.0049 0."
        },
        {
            "title": "MinerU\nMarker\nMathpix",
            "content": "0.311 0.231 0.189 0.101 0.251 0.175 0.117 0.309 0.225 0.376 0.378 0.413 0.226 0.292 0.250 0.0143 0.0033 0."
        },
        {
            "title": "Expert VLMs",
            "content": "GOT-OCR Nougat 0.175 0.934 0.190 0.915 0.186 0.873 0.184 0.907 0.0000 0. GOT-OCR Nougat 0.163 0.852 0.145 0.601 0.257 0.662 0.468 0.873 0.258 0. 0.0165 0."
        },
        {
            "title": "General VLMs",
            "content": "GPT4o 0.263 Qwen2-VL 0.101 0.120 InternVL2 0.195 0.157 0.197 0.184 0.114 0.155 0.214 0.124 0.157 0.0012 0.0006 0.0010 GPT4o 0.109 Qwen2-VL 0.098 0.082 InternVL 0.204 0.248 0.312 0.254 0.517 0.682 0.426 0.429 0.444 0.248 0.323 0.380 0.0132 0.0263 0.0472 Table 4. End-to-end text recognition on OmniDocBench: evaluation under various page attributes using the edit distance metric. Columns represent: Fuzzy (Fuzzy scan), Water (Watermark), Color (Colorful background). Metric: Different calculation methods are used for various document components: (1) Pure Text: We calculate Normalized Edit Distance, averaging these metrics at the sample level to obtain the final scores. (2) Tables: All tables are converted to HTML format before calculating the TEDS metric and Normalized Edit Distance. (3) Formulas: Formulas are currently evaluated using the CDM [35], Normalized Edit Distance, and BLEU. We did not convert interline formulas into Unicode because Unicode cannot represent certain complex formulas, such as matrices. (4) Reading Order: Reading order use the Normalized Edit Distance as Table 5. End-to-end reading order evaluation on OmniDocBench: results across different column layout types using Normalized Edit Distance. metric. It only involves text components, where tables, images, and ignored components do not participate in the final reading order calculation. 5. Benchmarks 5.1. Component-specific Evaluation Results The OmniDocBench dataset features comprehensive and precise annotations, allowing for fair and rigorous comparison of various document content extraction algorithms in real-world scenarios. Based on the distinct characteristics of these algorithms, we categorize document content"
        },
        {
            "title": "43.44\nDiT-L\n42.12\nLayoutLMv3\nDOCX-Chain\n30.86\nDocLayout-YOLO 43.71",
            "content": "13.72 13.63 11.71 48.71 45.85 43.22 39.62 72.83 15.45 21.00 19.23 42."
        },
        {
            "title": "Exam\nPaper",
            "content": "3.40 5.48 10.67 35."
        },
        {
            "title": "Notes Newspaper Average mAP",
            "content": "29.23 31.81 23.00 51.44 66.13 64.66 41.60 66.84 0.21 0.80 1.80 9.54 23.65 30.84 16.96 57.54 26.90 28.84 21.27 48.71 Table 6. Component-level layout detection evaluation on OmniDocBench layout subset: mAP results by PDF page type. Model Type Model Language EN ZH Mixed Table Frame Type Three Full Omission Zero Merge Cell(+/-) Formula(+/-) Colorful(+/-) Rotate(+/-) Special Situation OCR-based Models Expert VLMs General VLMs PaddleOCR RapidTable StructEqTable GOT-OCR 76.8 80.0 72.0 72.2 Qwen2-VL-7B 70.2 70.9 InternVL2-8B 71.8 83. 72.6 75.5 70.7 71.5 80.1 91.2 81.7 85.4 82.4 77.4 67.9 83. 68.8 73.1 70.2 69.5 74.3 79.7 64.3 72.7 62.8 69.2 81.1 83. 80.7 78.2 74.5 74.8 74.5 78.4 85.0 75.7 80.3 75.8 70.6/75.2 77.1/85. 65.1/76.8 65.0/80.2 60.8/76.5 58.7/78.4 71.3/74.1 76.7/83.9 69.4/73.5 64.3/77.3 63.8/72.6 62.4/73.6 72.7/74.0 77.6/84. 66.8/75.7 70.8/76.9 71.4/70.8 68.2/73.1 23.3/74.6 25.2/83.7 44.1/73.3 8.5/76.3 20.0/72.1 20.4/72.6 Overall 73.6 82.5 72.7 74.9 71.0 71.5 Table 7. Component-level Table Recognition evaluation on OmniDocBench table subset. (+/-) means with/without special situation. extraction methods into three main classes: Pipeline Tools. These methods integrate layout detection and various content recognition tasks (such as OCR, table recognition, and formula recognition) into document parsing pipeline for content extraction. Prominent examples include MinerU [36], Marker [30], and Mathpix 4. Expert VLMs. These are large multimodal models specifically trained for document parsing tasks. Representative models include GOT-OCR2.0 [39] and Nougat [5]. General VLMs. These are general-purpose large multimodal models inherently capable of document parsing. Leading models in this category include GPT-4o [2], Qwen2-VL [38], and InternVL2 [6]. 5.2. End-to-End Evaluation Results Utilizing the OmniDocBench dataset and our evaluation framework, we conducted end-to-end assessments of mainstream document parsing methods, evaluating their performance from input PDF images to the resultant document parsing outputs. Overall Evaluation Results. As illustrated in Table 2, pipeline tools specifically designed for document parsing, demonstrate superior performance across the board. MinerU and Mathpix achieved the best results for English and Chinese pages, respectively. In contrast, even the best general-purpose Vision Language Models (VLMs), GPT4o, exhibits performance gap compared to these specialized models, especially in Chinese. This trend is evident across sub-tasks like text recognition, formula recognition, and table recognition, where methods tailored for document parsing consistently outperform others. This advantage is largely due to the fine-tuning of these models on large datasets specific to document parsing tasks. 4https://mathpix.com/ Performance Across Diverse Page Types. To gain deeper insights into model performance on diverse document types, we evaluated text recognition tasks across different page types. As shown in Table 3, an intriguing finding emerged: For commonly used data, such as academic papers and financial reports, pipeline tools perform well. However, for more specialized data like slides and handwritten notes, general VLMs demonstrate stronger generalization. The reason is clear: Pipeline tools and expert VLMs are relatively more constrained by the range of training data, whereas general VLMs having been trained on wide variety of samples, maintained excellent recognition performance even in traditionally challenging long-tail scenarios, underscoring the value of VLMs. Performance on Pages with Specific Attributes. For documents in OmniDocBench with attributes such as fuzzy scans, watermarks, and colorful backgrounds, our evaluation results are presented in Table 4. In these scenarios, the VLMs InternVL2 and Qwen2-VL exhibit the strongest resistance to interference, achieving the best accuracy and robustness. MinerU also performs commendably. Performance on Different Column Layout Types. OmniDocBench annotates page attributes such as column layout type, which is crucial for analyzing model performance in reading order. As depicted in Table 5, all models experience noticeable decline in reading order accuracy when dealing with complex layouts. MinerU and Mathpix excels in reading order across various column layouts, demonstrating robust performance across different page types. From these end-to-end evaluations, it is evident that pipeline tools like MinerU and Mathpix, specifically designed for document parsing, achieve the best overall performance. However, in terms of versatility and scalability, VLMs offer distinct advantage over pipeline tools. Fine-"
        },
        {
            "title": "Language\nZH",
            "content": "EN"
        },
        {
            "title": "Single Multi Normal",
            "content": "Rotate90 Rotate270 Horizontal"
        },
        {
            "title": "Vision Language\nModels",
            "content": "PaddleOCR 0.071 Tesseract OCR 0.179 0.057 Surya 0.041 GOT-OCR 0.033 Mathpix Qwen2-VL InternVL2 GPT4o 0.072 0.074 0.020 0.055 0.553 0.123 0.112 0.240 0.274 0.155 0.224 0.118 0.553 0.164 0.135 0. 0.286 0.242 0.125 0.060 0.453 0.093 0.092 0.185 0.234 0.113 0.167 0.038 0.463 0.186 0.052 0.121 0.155 0.352 0.140 0.085 0.394 0.235 0.155 0. 0.148 0.269 0.220 0.060 0.448 0.104 0.091 0.180 0.223 0.132 0.168 0.015 0.369 0.634 0.562 0.038 0.273 0.610 0.115 0.285 0.979 0.767 0.966 0. 0.721 0.907 0.718 0.021 0.982 0.255 0.097 0.638 0.067 0.595 0.132 Table 8. Component-level evaluation on OmniDocBench OCR subset: results grouped by text attributes using the edit distance metric."
        },
        {
            "title": "Models",
            "content": "CDM ExpRate@CDM BLEU Norm Edit GOT-OCR Mathpix Pix2Tex UniMERNet-B GPT4o InternVL2 Qwen2-VL 74.1 86.6 73.9 85.0 86.8 67.4 83.8 28.0 2.8 39.5 60. 65.5 54.5 55.4 55.07 66.56 46.00 60.84 45.17 47.63 53.71 0.290 0.322 0.337 0.238 0.282 0.308 0.285 Table 9. Component-level formula recognition evaluation on OmniDocBench formula subset. tuning general large model like Qwen2-VL with specialized data could yield models even more adept at document parsing, indicating promising direction for future research in multimodal approaches. The OmniDocBench dataset provides comprehensive annotations for document parsing, including layout detection, text boxes and content, formula boxes and content, and table boxes and content. These detailed annotations enable the evaluation of current state-of-the-art (SOTA) methods across various document types, allowing us to analyze their performance in diverse scenarios. Additionally, these results can be used to assemble enhanced pipeline tools for document parsing tasks. 5.3. Single Algorithm Evaluation Results Layout Detection Results. Layout detection is the first step in document parsing using pipeline tools. robust layout detection algorithm should perform well across variety of document types. Table 6 presents an evaluation of leading layout detection models. The DocLayout-YOLO method, which is pre-trained on diverse synthetic document data, significantly outperforms other approaches. This superiority is key factor in MinerUs integration of DocLayoutYOLO, contributing to its outstanding overall performance. The table also reveals that, aside from DocLayout-YOLO, other methods perform well on books and academic literature but are less effective on other document types, primarily due to lack of pre-training on diverse documents. Table Recognition Results. Table recognition results evaluated by Tree-Edit-Distance-based Similarity (TEDS) metric are presented in Table 7. We evaluate table recognition models across three dimensions on our OmniDocBench table subset: language diversity, table frame types, and special situations. Among all models, OCR-based models demonstrate superior overall performance, with RapidTable achieving the highest scores in language diversity and maintaining stable performance across different frame types. Expert VLMs show competitive results in specific scenarios, with StructEqTable [48] excelling in no frame tables and showing better rotation robustness. General VLMs (Qwen2-VL-7B and InternVL2-8B) exhibit relatively lower but consistent performance, suggesting that while generalpurpose VLMs have made progress in table understanding, they still lag behind specialized solutions. Text Recognition Results. In the traditional OCR task, Table 8 shows that PaddleOCR leads the field, surpassing other models significantly, with GOT also performing relatively well. Selecting these two methods for the OCR module is prudent choice. Formula Recognition Results. For formula recognition, the CDM metric provides clear comparison in Table 9. GPT-4o, Mathpix, and UniMERNet achieve results of 86.8%, 86.6%, and 85.0%, respectively. Notably, GPT-4o excels with recall rate of 65.5% under strict conditions requiring perfect character accuracy. Although Mathpix shows high character-level precision, it occasionally omits punctuation, such as commas, leading to lower overall correctness rate. Nonetheless, all three models are strong candidates for formula recognition tasks. 6. Conclusion This paper addresses the lack of diverse and realistic benchmarks in document parsing research by introducing OmniDocBench, dataset featuring variety of page types with comprehensive annotations, along with flexible and reliable evaluation framework. OmniDocBench enables systematic and fair assessments of document parsing methods, providing crucial insights for advancing the field. Its task-specific and attribute-level evaluations facilitate targeted model optimization, promoting more robust and effective parsing solutions."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. 3 [2] Open AI. Hello gpt 4o, 2024. Accessed July 24, 2024. 4, 7 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv:2308.12966, 2024. 1 [4] Lukas Blecher. pix2tex - latex ocr. https://github. com/lukas-blecher/LaTeX-OCR, 2022. Accessed: 2024-2-29. 3 [5] Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents. arXiv:2308.13418, 2024. 1, 3, 7 [6] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 1, 3, 7 [7] Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, and Lianwen Jin. M6doc: large-scale multi-format, multi-type, multilayout, multi-language, multi-annotation category dataset In Proceedings of for modern document layout analysis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1513815147, 2023. 3 [8] Yuntian Deng, Anssi Kanervisto, Jeffrey Ling, and AlexanImage-to-markup generation with coarse-toIn International Conference on Machine der Rush. fine attention. Learning, pages 980989. PMLR, 2017. 3 [9] Harsh Desai, Pratik Kayal, and Mayank Singh. Tablex: benchmark dataset for structure and content information exIn Document Analysis and traction from scientific tables. RecognitionICDAR 2021: 16th International Conference, pages 554569, 2021. 3 [10] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv:2312.10997, 2023. 1 [11] Jiuxiang Gu, Jason Kuen, Vlad Morariu, Handong Zhao, Rajiv Jain, Nikolaos Barmpalios, Ani Nenkova, and Tong Sun. Unidoc: Unified pretraining framework for document understanding. Advances in Neural Information Processing Systems, 34:3950, 2021. 3 [12] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, et al. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895, 2024. 3 [13] Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl2: High-resolution compressing for ocrarXiv preprint free multi-page document understanding. arXiv:2409.03420, 2024. 3 [14] Mingxin Huang, Yuliang Liu, Zhenghao Peng, Chongyu Liu, Dahua Lin, Shenggao Zhu, Nicholas Yuan, Kai Ding, and Lianwen Jin. Swintextspotter: Scene text spotting via better synergy between text detection and text recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45934603, 2022. 3 [15] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling usarXiv preprint ing contextual embeddings. arxiv 2020. arXiv:2012.06678, 2012. 3 [16] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified text and image masking, 2022. 3, 4 [17] Yongshuai Huang, Ning Lu, Dapeng Chen, Yibo Li, Zecheng Xie, Shenggao Zhu, Liangcai Gao, and Wei Peng. Improving table structure recognition with visual-alignment sequential coordinate modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1113411143, 2023. 3 [18] Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, and Minjoon Seo. Spatial dependency parsing for semi-structured document information extraction. In Findings of the Association for Computational Linguistics: ACLIJCNLP, pages 330343. Association for Computational Linguistics (ACL), 2021. [19] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, Faisal Shafait, Seiichi Uchida, and Ernest Valveny. Icdar 2015 competition on robust reading. In 2015 13th International Conference on Document Analysis and Recognition, pages 11561160, 2015. 3 [20] Vladimir Levenshtein et al. Binary codes capable of In Doklady correcting deletions, insertions, and reversals. Physics, pages 707710. Soviet Union, 1966. 3 [21] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. 1 [22] Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du, Lingfeng Zhu, Baohua Lai, Xiaoguang Hu, Dianhai Yu, and Yanjun Ma. Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system, 2022. 3, 4 [23] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. Tablebank: Table benchmark for image-based table detection and recognition. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 19181925, 2020. [24] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: benchmark dataset for document layout analysis. arXiv:2006.01038, 2020. 3 9 [25] Zhe Li, Lianwen Jin, Songxuan Lai, and Yecheng Zhu. Improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. In 2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 175180. IEEE, 2020. 3 [26] Zichao Li, Aizier Abulaiti, Yaojie Lu, Xuanang Chen, Jia Zheng, Hongyu Lin, Xianpei Han, and Le Sun. Readoc: unified benchmark for realistic document structured extraction. arXiv:2409.05137, 2024. 3 [27] Chenglong Liu, Haoran Wei, Jinyue Chen, Lingyu Kong, Zheng Ge, Zining Zhu, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Focus anywhere for fine-grained arXiv:2405.14295, multi-page document understanding. 2024. 1, 3 [28] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei Wang. Abcnet: Real-time scene text spotting with adaptive bezier-curve network. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 98099818, 2020. [29] Tengchao Lv, Yupan Huang, Jingye Chen, Yuzhong Zhao, Yilin Jia, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha Zhang, and Furu Wei. Kosmos-2.5: multimodal literate model, 2024. 3 [30] Vik Paruchuri. Marker, 2024. 7 [31] Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. Doclaynet: large humanannotated dataset for document-layout segmentation. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pages 37433751, 2022. 3 [32] Subhojeet Pramanik, Shashank Mujumdar, and Hima Patel. Towards multi-modal, multi-task learning based pretraining framework for document representation learning. arXiv preprint arXiv:2009.14457, 2020. 3 [33] Ray Smith, Daria Antonova, and Dar-Shyang Lee. Adapting the tesseract open source ocr engine for multilingual ocr. In Proceedings of the International Workshop on Multilingual OCR, 2009. 3 [34] Bin Wang, Zhuangcheng Gu, Guang Liang, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. Unimernet: universal network for real-world mathematical expression recognition, 2024. 3, 4 [35] Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, and Conghui He. Cdm: reliable metric for fair and accurate formula recognition evaluation. arXiv:2409.03643, 2024. 3, 6 [36] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. Mineru: An open-source solution for precise document content extraction. arXiv:2409.18839, 2024. 1, 3, [37] Pengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaoqiang Zhang, Pengyuan Lyu, Junyu Han, Jingtuo Liu, Errui Ding, and Guangming Shi. Pgnet: Real-time arbitrarilyshaped text spotting with point gathering network. In Pro10 ceedings of the AAAI Conference on Artificial Intelligence, pages 27822790, 2021. 3 [38] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 7 [39] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv:2409.01704, 2024. 1, 3, 7 [40] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large In European Conference on Comvision-language model. puter Vision, pages 408424. Springer, 2025. 1, 3 [41] Renqiu Xia, Song Mao, Xiangchao Yan, Hongbin Zhou, Bo Zhang, Haoyang Peng, Jiahao Pi, Daocheng Fu, Wenjie Wu, Hancheng Ye, et al. Docgenome: An open largescale scientific document benchmark for training and testarXiv preprint ing multi-modal large language models. arXiv:2406.11633, 2024. [42] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024. 1 [43] Zhong Xu, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In 2019 International conference on document analysis and recognition, pages 10151022, 2019. 3 [44] Jianshu Zhang, Jun Du, and Lirong Dai. Multi-scale attention with dense encoder for handwritten mathematical In 2018 24th international conexpression recognition. ference on pattern recognition (ICPR), pages 22452250. IEEE, 2018. 3 [45] Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang, Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Wentao Zhang, and Conghui He. Document parsing unveiled: Techniques, challenges, and prospects for structured information extraction. arXiv preprint arXiv:2410.21169, 2024. 1 [46] Zhiyuan Zhao, Hengrui Kang, Bin Wang, and Conghui He. Doclayout-yolo: Enhancing document layout analysis through diverse synthetic data and global-to-local adaptive perception, 2024. 3 [47] Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation. In European conference on computer vision, pages 564580, 2020. 3 [48] Hongbin Zhou, Xiangchao Yan, and Bo Zhang. Structeqtable-deploy: high-efficiency open-source toolkit for table-to-latex transformation. https://github. com / UniModal4Reasoning / StructEqTable - Deploy, 2024. 8 OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations"
        },
        {
            "title": "Supplementary Material",
            "content": "I. More End-to-End Evaluation Results Table S1 presents the evaluation results of End2End Tables grouped by Table Attributes. As it shows, most of the models perform better in English Tables rather than Chinese ones. Most models perform relatively poorly with Full Frame and No Frame tables. The accuracy of most models is affected by special conditions. Merged cells and formulas mainly test the breadth of data the model can recognize, while colored backgrounds and table rotation test their robustness. The results show that table rotation significantly impacts the accuracy of all models. Pipeline Tools perform well on more challenging tables, but colored backgrounds can affect recognition accuracy. Several Vision Language Models (VLMs) tend to perform worse on tables with merged cells, but colored backgrounds do not significantly impact table recognition accuracy. Table S2 shows the evaluation results of End2End Text blocks grouped by Text Attributes. Almost all models have lower recognition accuracy in Chinese compared to English. Some models, such as MinerU and Marker, experience further decrease in accuracy when recognizing mixed Chinese and English content. Complex background colors significantly affect the recognition accuracy of pipeline tools, but they have little impact on VLMs. II. Dataset Statistics and Visualization OmniDocBench contains 981 pages, including 9 types of PDF pages, 4 types of layouts, and 3 types of languages. Some pages also include special conditions, such as watermarks. Table S3 and Figure S1 show the number of pages with each page attribute. Figures S3 to S6 are examples of PDF pages with different PDF types, Layout Types, and Special Issues. Table S6 and Figure S2 show all annotation categories included in OmniDocBench. All of them are annotated by bounding boxes. There are 15 types of block-level annotations and 4 types of span-level annotations, with span-level annotations nested within the block-level ones. In addition, there are 3 types of annotations marked as page interference information (No.20-22), whose bounding boxes are used to mask the specific regions of the PDF pages to avoid affecting the evaluation results. The recognition annotations are also provided for each annotation category except for Figures. Formulas is written in LaTeX format and Table is annotated in both HTML and LaTeX formats. Others are annotated in plain text. Furthermore, the Text Attributes are also annotated for each block-level category that contains text. There are 3 types of Text Attributes that might influent OCR accuracy: Language, Text Background Color, and Text Rotation. Table S5 shows the statistics of annotations with specific text attributes. There are 23,010 block-level annotations are labeled with text attributes. Tables are also annotated with Table Attributes. There are 6 types of Table Attributes that might influent the Table Recognition accuracy: Language, Table Frame Type, Merge Cell, Colorful Background, Contain Formula, and Rotation. Table S5 shows the numbers of annotations with specific table attributes. Figures S7 and S8 are the examples of Tables with different Frames and Special Issues. III. Model Results Visualization Figures S9 to S17 show the examples of Good model outputs and Bad model outputs of Document Parsing among different PDF types. As it shown, different models exhibit varying performance across different PDF types. For example, MinerU detects all handwritten notes as figures, resulting in very low recognition accuracy in Notes. Marker and InternVL2 experience missed detections, leading to lower scores. InternVL2 and Qwen2-VL, in specific PDF types (such as slides or financial reports), tend to merge multicolumn text. Figures S18 to S20 show the examples of Good model outputs and Bad model outputs under special issues of the PDF pages. It shows that Marker tends to generate typos when the PDF pages are fuzzy scanned or with watermarks, while GOT-OCR fails to recognize content on pages with colored backgrounds. MinerU performs well under special situations, while Mathpix occasionally generates typos. Figures S21 to S24 show examples of Good model outputs and Bad model outputs for PDF pages with different layouts. MinerU has low reading order score for single-column layouts primarily because most notes are single-column, and MinerU performs poorly in recognizing Notes, leading to low reading order score accordingly. InternVL2 scores high in Single-Column layouts but scores poorly on Double-Column and Three-Column layouts. It is mainly due to frequent missed content recognition and errors in reading order judgment in multi-column layouts pages. MinerUs reading order and recognition accuracy decrease with complex layouts, primarily because it incorrectly merges multiple columns during recognition. Figures S27 and S28 show the models recognition abilIn text recognition with ity under special issues of text."
        },
        {
            "title": "Language",
            "content": "EN"
        },
        {
            "title": "Full Omission",
            "content": "Zero Merge Cell(+/-) Formula(+/-) Colorful(+/-) Rotate(+/-)"
        },
        {
            "title": "MinerU\nMarker\nMathpix",
            "content": "GOT-OCR Nougat GPT4o Qwen2-VL InterVL2 75.7 52.5 76.1 51.9 36.5 71.8 57.4 61.5 59.9 43.0 64. 47.0 0.4 58.8 62.9 59.3 79.6 44.2 71.9 49.4 0.0 57.9 72.7 65.9 60.0 41.8 68. 46.2 6.3 63.3 70.7 59.7 72.8 55.3 79.3 49.3 3.6 69.5 64.1 66.5 70.1 47.1 67. 51.6 22.2 61.9 48.3 58.7 60.4 52.4 25.8 47.2 0.0 31.8 57.6 56.2 64.1/66.0 43.8/47.0 71.2/66. 46.5/49.7 15.1/9.1 57.5/65.5 49.4/68.2 49.6/65.9 66.7/65.0 42.9/46.6 69.8/67.6 46.4/49.1 21.2/8.9 61.6/62.9 48.5/64.7 54.4/61.6 59.8/68.1 44.3/46.7 60.5/71. 40.2/52.7 2.8/15.3 62.0/63.0 63.5/60.7 59.4/60.6 2.9/66.4 6.3/46.6 20.7/68.8 0.0/49.4 0.0/11.4 14.5/63.5 41.6/61.9 7.3/61.1 Table S1. End-to-End Table TEDS Result grouped by Table Attributes Model Type Model Pipeline Tools Expert Vision Models Vision Language Models MinerU Marker Mathpix GOT-OCR Nougat GPT4o Qwen2-VL InternVL2 Language ZH Text background Mixed White Single Multi 0.206 0.389 0.774 0.763 0.991 0.647 0.575 0.606 0.742 0.499 0.538 0.266 0.983 0.322 0.310 0. 0.163 0.339 0.675 0.669 0.874 0.536 0.537 0.589 0.147 0.389 0.554 0.595 0.935 0.423 0.400 0. 0.513 0.497 0.570 0.440 0.972 0.406 0.233 0.221 EN 0.123 0.267 0.173 0.251 0. 0.170 0.337 0.418 Table S2. End-to-End Text Normalized Edit Distance results grouped by Text Attributes. Mixed represents mixture of Chinese and English, Single and Multi represent single color and multi color. complex background colors, Marker may produce errors or miss content, whereas Qwen2-VL still performs well. Most models fail to recognize text when it is rotated 270 degrees. Some vision language models generate hallucinated information based on the content they can recognize. Figures S29 to S32 show the examples of good and bad model results for tables with different attributes. For threeline tables, RapidTable demonstrates good performance with accurate structure recognition, while PaddleOCR shows limitations by missing the last column in its outputs. Interestingly, in tables without frames, PaddleOCR performs well with accurate table predictions, while Qwen2VL-7B exhibits errors in the last two columns. This indicates that the presence or absence of table frames can significantly impact different models performance in different ways. Rotated tables prove to be particularly challenging, with most models, including GOT-OCR, failing to recognize the table structure. However, StructEqTable shows promising results by correctly identifying most of the table content, though with few detail errors. For tables containing formula, Qwen2-VL-7B shows more accurate table structure recognition compared to InternVL2-8B. IV. Model Settings For pipeline tools such as MinerU, Marker, and Mathpix, default settings are used for evaluation. Specifically, MinerU with Version 0.9.35 is employed. For Marker, Version 0.2.176 is evaluated. For Nougat, we utilize its 0.1.0base model (350M). For GOT-OCR, we employ its format OCR mode to output structured data. For general VLMs, we used the GPT4o, Qwen2-VL-72B, and InternVL2-Llama376B by setting the do sample=False to ensure the reproducibility."
        },
        {
            "title": "Special Issues",
            "content": "Attribute Name Book PPT2PDF Research Report Colorful Textbook Exam Paper Magazine Academic Literature Notes Newspaper Single Column Double Column Three Column One&More Mixed Complex Layout English Simplified Chinese Mixed Fuzzy Scan Watermark Colorful Background Count 104 133 81 96 114 97 129 116 111 477 126 45 120 213 290 612 79 28 65 246 Table S3. The Page Attributes Statistics of OmniDocBench. 5https : / / github . com / opendatalab / MinerU / releases/tag/magic_pdf-0.9.3-released 6https : / / github . com / VikParuchuri / marker / releases/tag/v0.2."
        },
        {
            "title": "Text Rotate",
            "content": "English Simplified Chinese EN&CH Mixed White Single-Colored Multi-Colored Normal Rotate90 Rotate270 Horizontal 5857 16073 1080 19465 1116 2429 22865 14"
        },
        {
            "title": "Special Issues",
            "content": "English Simplified Chinese EN&CH Mixed"
        },
        {
            "title": "Merge Cell\nColorful Background\nContain Formula\nRotate",
            "content": "128 285 15 205 62 147 14 150 142 81 7 Table S4. Text Attributes Statistics of OmniDocBench. Table S5. Table Attributes Statistics of OmniDocBench. Title Text Block Figure Figure Caption Figure Footnotes Table Table Caption Table Footnotes Header Footer Page Number Page Footnote Code Block Code Block Caption Reference Text Span Equation Inline Equation Ignore Footnote Mark Other Abandoned Categories No. Category Name 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Masked Text Block 22 Organic Chemical Formula Explaination Include main titles, chapter titles, etc. Text paragraphs, which are usually separated by double line breaks in Markdown. Including images, visual charts, etc. Typically starts with Figure followed by number, or just descriptive language below the figure. Descriptive language, apart from the figure caption, usually starts with an asterisk (*). Content organized in table form usually includes borders or clear table structure. Typically starts with Table followed by number, or just descriptive language above the Table. Descriptive language, apart from the table caption, usually starts with an asterisk (*). Information located at the top of PDF page or in the sidebar, separate from the main content, typically includes chapter names and other details. Information located at the bottom of PDF page, separate from the main content, typically includes the publishers name and other details. It is usually represented by numbers, which may be located at the top, in the sidebar, or at the bottom of the page. It provides further explanation of the footnotes marked within the page content. For example, information about the authors affiliations. In Markdown, code block is typically defined using triple backticks (). Descriptive language above the Code Block. Typically found only in academic literature. Span-Level text box, which is the plain text content can be directly written in Markdown format. Formulas that need to be represented using LaTeX format and embedded within the text. Some formulas that can be displayed correctly without using LaTeX formatting, such as 15 kg. Typically embedded within the text as superscripts or subscripts, and their numbering usually corresponds to page footnotes. (Masked) Some uncategorizable, irrelevant page information, such as small icons, etc. (Masked) Some difficult-to-recognize information that disrupts text flow, such as pinyin annotations above Chinese characters. (Masked) Organic chemistry formulas, which are difficult to write using Markdown and are easily recognized as Figures. Total 2972 15979 989 651 133 428 299 132 1271 541 669 92 13 / 260 73143 4009 3685 357 538 34 24 Table S6. Annotation Explanations and Statistics. Figure S1. The Data Proportion of Pages for each Attribute in OmniDocBench. 13 Figure S2. The Visualization of vary Annotations in OmniDocBench. 14 Figure S3. The Examples of Academic Papers, Books, Textbooks, Notes, and Magazines in OmniDocBench. 15 Figure S4. The Examples of Finacial Reports, Newspapers, Example Papers, and Slides in OmniDocBench. 16 Figure S5. The Examples of PDF pages with different Layout Types in OmniDocBench. Figure S6. The Examples of PDF pages under Special Issues in OmniDocBench. 17 Figure S7. The Examples of Tables with different Frame in OmniDocBench. Figure S8. The Examples of Tables under Special Issues in OmniDocBench. 18 Figure S9. The Good Model Result and Bad Model Result for Academic Papers. Figure S10. The Good Model Result and Bad Model Result for Books. 19 Figure S11. The Good Model Result and Bad Model Result for Exam Papers. Figure S12. The Good Model Result and Bad Model Result for Magazines. 20 Figure S13. The Good Model Result and Bad Model Result for Newspaper. Figure S14. The Good Model Result and Bad Model Result for Handwriting Notes. 21 Figure S15. The Good Model Result and Bad Model Result for Financial Reports. Figure S16. The Good Model Result and Bad Model Result for Slides. 22 Figure S17. The Good Model Result and Bad Model Result for Textbooks. Figure S18. The Good Model Result and Bad Model Result for Fuzzy Scan Pages. 23 Figure S19. The Good Model Result and Bad Model Result for Pages with Watermark. Figure S20. The Good Model Result and Bad Model Result for Colorful Background Pages. 24 Figure S21. The Good Model Result and Bad Model Result for Single Column Pages. Figure S22. The Good Model Result and Bad Model Result for Double Column Pages. 25 Figure S23. The Good Model Result and Bad Model Result for Three Column Pages. Figure S24. The Good Model Result and Bad Model Result for Complex Layout Pages. 26 Figure S25. The Good Model Result and Bad Model Result for Text Language in Chinese. Figure S26. The Good Model Result and Bad Model Result for Text Language in English. 27 Figure S27. The Good Model Result and Bad Model Result for Text with Colorful Background. Figure S28. The Bad Model Result for Text with Rotation. 28 Figure S29. The Good Model Result and Bad Model Result for Three Line Frame Table. Figure S30. The Good Model Result and Bad Model Result for No Frame Table. 29 Figure S31. The Good Model Result and Bad Model Result for Rotated Table. Figure S32. The Good Model Result and Bad Model Result for Table with Formula."
        }
    ],
    "affiliations": [
        "Abaka AI",
        "Shanghai AI Laboratory"
    ]
}