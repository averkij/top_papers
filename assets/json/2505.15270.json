{
    "paper_title": "Scaling Diffusion Transformers Efficiently via $μ$P",
    "authors": [
        "Chenyu Zheng",
        "Xinyu Zhang",
        "Rongzhen Wang",
        "Wei Huang",
        "Zhi Tian",
        "Weilin Huang",
        "Jun Zhu",
        "Chongxuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization ($\\mu$P) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether $\\mu$P of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard $\\mu$P to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that $\\mu$P of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-$\\alpha$, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing $\\mu$P methodologies. Leveraging this result, we systematically demonstrate that DiT-$\\mu$P enjoys robust HP transferability. Notably, DiT-XL-2-$\\mu$P with transferred learning rate achieves 2.9 times faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of $\\mu$P on text-to-image generation by scaling PixArt-$\\alpha$ from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under $\\mu$P outperform their respective baselines while requiring small tuning cost, only 5.5% of one training run for PixArt-$\\alpha$ and 3% of consumption by human experts for MMDiT-18B. These results establish $\\mu$P as a principled and efficient framework for scaling diffusion Transformers."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 7 2 5 1 . 5 0 5 2 : r Scaling Diffusion Transformers Efficiently via µP Chenyu Zheng1,2,3, Xinyu Zhang4, Rongzhen Wang1,2,3, Wei Huang5, Zhi Tian4, Weilin Huang4, Jun Zhu6, Chongxuan Li1,2,3 1Gaoling School of AI, Renmin University of China, 2Beijing Key Laboratory of Research on Large Models and Intelligent Governance, 3Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE, 4ByteDance Seed, 5RIKEN AIP, 6Tsinghua University Work done during an internship at ByteDance Seed, Correspondence to Chongxuan Li."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformers have emerged as the foundation for vision generative models, but their scalability is limited by the high cost of hyperparameter (HP) tuning at large scales. Recently, Maximal Update Parametrization (µP) was proposed for vanilla Transformers, which enables stable HP transfer from small to large language models, and dramatically reduces tuning costs. However, it remains unclear whether µP of vanilla Transformers extends to diffusion Transformers, which differ architecturally and objectively. In this work, we generalize standard µP to diffusion Transformers and validate its effectiveness through large-scale experiments. First, we rigorously prove that µP of mainstream diffusion Transformers, including DiT, U-ViT, PixArt-α, and MMDiT, aligns with that of the vanilla Transformer, enabling the direct application of existing µP methodologies. Leveraging this result, we systematically demonstrate that DiT-µP enjoys robust HP transferability. Notably, DiT-XL-2-µP with transferred learning rate achieves 2.9 faster convergence than the original DiT-XL-2. Finally, we validate the effectiveness of µP on text-to-image generation by scaling PixArt-α from 0.04B to 0.61B and MMDiT from 0.18B to 18B. In both cases, models under µP outperform their respective baselines while requiring small tuning costonly 5.5% of one training run for PixArt-α and 3% of consumption by human experts for MMDiT-18B. These results establish µP as principled and efficient framework for scaling diffusion Transformers. Date: May 21, 2025 Project Page: https://github.com/ML-GSAI/Scaling-Diffusion-Transformers-muP"
        },
        {
            "title": "Introduction",
            "content": "Owing to its generality and scalability, diffusion Transformers [1, 48] have become the backbone of modern vision generation models, with widespread applications in various tasks such as image generation [3, 15, 19, 32, 50] and video generation [2, 6, 56, 70]. As datasets grow and task complexity increases, further scaling of diffusion Transformers has become inevitable and is attracting increasing attention [16, 28, 37, 38, 68]. However, as model sizes reach billions of parameters, hyperparameter (HP) tuning becomes prohibitively expensive, often hindering the model from achieving its full potential. This underscores the urgent need for principled approach to efficiently identify the optimal HPs when scaling diffusion Transformers. Maximal Update Parametrization (µP) [63, 65, 66] was recently proposed as promising solution to the HP selection problem for large-scale vanilla Transformer [55]. It stabilizes optimal HPs across different model 1 (a) Samples produced by the MMDiT-µP-18B. (b) Efficiency of µP search. Figure 1 Visualization results and efficiency of HP search under µP. (a) Samples generated by the MMDiT-µP-18B model exhibit strong fidelity and precision in aligning with the provided textual descriptions. (b) HP search for large diffusion Transformers is efficient under µP, requiring only 5.5% FLOPs of single training run for PixArt-α and just 3% FLOPs of the human experts for MMDiT-18B. widths, enabling direct transfer of HPs searched from small models to large models (a.k.a., µTransfer) and significantly reducing tuning costs at scale. Due to its strong transferability, µP has been applied to the pretraining of large language models (LLMs) [12, 27, 39, 40, 66, 71]. Unfortunately, diffusion Transformers [1, 8, 15, 48] differ fundamentally from vanilla Transformers [55]. First, their architectures incorporate additional components to integrate text information and diffusion timesteps for final vision generation. Second, they operate under distinct generative framework based on iterative denoising, in contrast to the autoregressive paradigms typically used in vanilla Transformers (e.g., LLMs). As result, existing µP theory and its associated HP transfer properties may not directly apply to diffusion Transformers [54]. This paper systematically investigates this issue, as detailed below. First, we extend the standard µP theory from vanilla Transformers to diffusion Transformers in Section 3.1. Using the Tensor Programs technique [60, 63, 65], we rigorously prove that the µP formulation of mainstream diffusion Transformers, including DiT [48], U-ViT [1], PixArt-α [8], and MMDiT from Stable Diffusion 3 [15], matches that of vanilla Transformers (Theorem 3.1). This compatibility enables us to directly apply existing practical methodologies developed for standard µP to diffusion Transformers, as described in Section 3.2. Based on the rigorous µP result of diffusion Transformers, we then conduct systematic study of DiT-µP on the image generation task using the ImageNet dataset [11], presented in Section 4. We first verify the stable HP transferability of DiT-µP across widths, batch sizes, and training steps. We then µTransfer the optimal learning rate searched from small models to DiT-XL-2-µP. Notably, DiT-XL-2-µP trained with the transferred learning rate achieves 2.9 faster convergence than the original DiT-XL-2 [48], suggesting that µP offers an efficient principle for scaling diffusion Transformers. Finally, we further validate the efficiency of µP on large-scale text-to-image generation tasks. In Section 5.1, we apply the µTransfer algorithm to PixArt-α [8], scaling the model from 0.04B to 0.61B. In Section 5.2, Table 1 µP for (diffusion) Transformers with Adam/AdamW optimizer. We use purple text to highlight the differences between µP and standard parameterization (SP) in practice (e.g., Kaiming initialization [24]), and gray text to indicate the SP settings. Formal definitions of the weight type are provided in Appendix B. Input weights Hidden weights Output weights aW bW cW 0 0 0 0 1/2 1 (0) 1 (0) 0 (1/2) we apply it to MMDiT [15], scaling from 0.18B to 18B. In both cases, diffusion Transformers under µP consistently outperform their respective baselines with small HP tuning cost on proxy tasks. For PixArt-α, tuning consumes only 5.5% of the FLOPs required for full pretraining run, while for MMDiT, tuning uses just 3% of the FLOPs typically consumed by human experts. These real-world experiments further confirm the scalability and reliability of µP. We begin by establishing the necessary background for diffusion Transformers and µP. Detailed discussion of additional related work is placed in Appendix A."
        },
        {
            "title": "2.1 Diffusion Transformers",
            "content": "Due to its superior scalability and compatibility, diffusion Transformers [1, 15, 48] has replaced U-Net [49] as the backbone for advanced diffusion models [3, 15, 19, 32, 50]. DiT [48] introduces Transformers with adaptive layer normalization (adaLN) blocks for class-to-image generation and demonstrates strong scalability with respect to network complexity. U-ViT [1] achieves comparable performance by applying ViT [14] with long skip connections between shallow and deep layers. PixArt-α [8] extends DiT for text-conditional image generation by incorporating cross-attention for text features and more efficient shared adaLN-single block. MMDiT [15] further extends DiT by introducing two separate parameter sets for image and text modalities, along with joint attention mechanism to facilitate multimodal interaction. As diffusion Transformers continue to scale, increasing attention is being paid to principled approaches for scientific scaling [28, 37, 38, 68]."
        },
        {
            "title": "2.2 Maximal Update Parametrization\nIn this section, we provide a practical overview of µP, with a focus on the widely used AdamW optimizer [44].\nThe comprehensive review of the theoretical foundations of µP is in Appendix B.",
            "content": "µP identifies unified parameterization that applies to common architectures expressive in Neor Program [65] (e.g., vanilla Transformer [55]), offering strong guidance for practical HP transfer across model widths, batch sizes, and training steps. Under µP, HPs can be tuned on small proxy task (e.g., 0.04B parameters and 6B tokens in [66]) and directly transferred to large-scale pretraining task (e.g., 6.7B parameters and 300B tokens in [66]), significantly reducing tuning costs at scale. As result, µP has been widely adopted in the pretraining of LLMs [12, 22, 27, 39, 40, 66, 71]. Concretely, µP is implemented by analytically adjusting HPs with model width, typically involving the weight multiplier, initialization variance, and learning rate (a.k.a., abc-parameterization). Formally, let denote the network width, we set each weight as = ϕW naW (cid:102)W , where the trainable component (cid:102)W is initialized as n2bW ), and its learning rate is ηW ncW . Henceforth, we call the width-independent parts (cid:102)Wij (0, σ2 (ϕW , σW , ηW ) as base HPs. As summarized in Table 1, µP identifies values of aW , bW , and cW that enable models of different widths to share the (approximately) same optimal base HPs ϕ . µP adjustion for other HPs is placed in Appendix B.4. , and η , σ 3 (a) Implementation of DiT-µP. (b) µTransfer. Figure 2 overview of applying µP to diffusion Transformers. (a) We illustrate the implementation of µP for DiT as an example. The abc-parameterization of each weight is adjusted based on its type and visualized using different colors. Modules that differ from the vanilla Transformer are also highlighted. (b) We µTransfer the optimal base HPs searched from multiple trials on small models to pretrain the target large models."
        },
        {
            "title": "3 Scaling Diffusion Transformers by µP",
            "content": "In this section, we extend the principles of µP to scale diffusion Transformers. In Section 3.1, we prove that despite fundamental differences from vanilla Transformer, mainstream diffusion Transformersincluding DiT [48], U-ViT [1], PixArt-α [8], and MMDiT [15]adhere to the standard µP formulation summarized in Table 1. Then, in Section 3.2, we introduce the practical methodology for applying µP to diffusion Transformers, as illustrated in Figure 2. 3.1 µP of Diffusion Transformers As mentioned in Section 2, the existing µP results [65, 66] in Table 1 apply only to architectures expressible as Neor program. Therefore, it is crucial to determine whether the diffusion Transformers can be represented in this framework. The following theorem establishes that several prominent diffusion Transformers [1, 8, 15, 48] are compatible with the existing µP results. Theorem 3.1 (µP of diffusion Transformers, proof in Appendix C). The forward passes of mainstream diffusion Transformers (DiT [48], U-ViT [1], Pixart-α [8], and MMDiT [15]) can be represented within the Neor Program. Therefore, their µP matches the standard µP presented in Table 1. The proof of Theorem 3.1 relies on rewriting the forward pass of diffusion Transformers using the three operators defined in the Neor Program [65]. Given that the representability of standard Transformer [55] within the Neor Program has been established [60], we focus on demonstrating that the novel modules specific to diffusion Transformers can also be expressed in this formalism. These modules primarily serve to integrate text and diffusion timestep information for vision generation; examples include the adaLN blocks of DiT [48] in Figure 2a. Theorem 3.1 ensures that existing practical methodologies developed for standard µP apply directly to DiT, U-ViT, PixArt-α, and MMDiT. Moreover, our analysis technique naturally extends to other variants of diffusion Transformers [7, 18, 45, 46, 53]. To the best of our knowledge, mainstream diffusion Transformer variants in use can be expressed within the Neor Program."
        },
        {
            "title": "3.2 Practical Methodology of µP in Diffusion Transformers\nGiven the rigorous µP result for diffusion Transformers established in Theorem 3.1, this section introduces\nhow to apply µP to diffusion Transformers in practice.",
            "content": "3.2.1 Implementation of µP The width of multi-head attention layer is determined by the product of the head dimension and the number of attention heads. Thus, there are two degrees of freedom when scaling the width of diffusion Transformers. Following theoretical justification [5] and standard practice [21, 27, 40, 66], we fix the head dimension and scale the number of heads in this work. Given scaling the number of heads, we implement the µP of diffusion Transformers following the standard procedure in [66].1 Specifically, we replace the vanilla width in the abc-parameterization (see Section 2) with the width ratio n/nbase when standard parameterization (SP) and µP differ in Table 1, where nbase is fixed base width. This implementation remains consistent with Table 1 since nbase is constant. For example, for hidden weight matrix with width and base HPs ϕW , σ2 , η (maybe searched from proxy model), we set = ϕW (cid:102)W with initial (cid:102)W (0, σ2 /n), as in SP. In contrast, its learning rate is ηnbase/n rather than η/n, where µP differs from SP. Finally, we also follow the suggestion from [66] to initialize the output weights by zero (i.e., σ2 out = 0). 3.2.2 Base Hyperparameter Transferability of µP Prior work has shown that µP provides strong guidance for base HP transferability in image classification and language modeling tasks [4, 12, 23, 27, 54, 57, 66]. Building on the rigorous µP form for diffusion Transformers in Theorem 3.1, we further validate its HP transferability in the context of visual generation. We summarize the methodology for verifying base HP transferability across widths, batch sizes, and training steps in Algorithm 1, 2, and 3 in Appendix D, respectively. We describe how to verify HP transfer across widths below; those for batch size and step are similar. For simplicity, we describe how to evaluate the transferability of the base learning rate η across widths; similar as the sets of widths procedures apply to other HPs, such as the multiplier ϕ. We define {ni}P and base learning rates used in the evaluation. Given fixed batch size and training iterations of , we train diffusion Transformers, each parameterized by µP with base width nbase, true width ni, and base learning rate ηj. Finally, given evaluation metrics, if models at different widths (approximately) share the same optimal base learning rate, we conclude that learning rate transferability holds for diffusion Transformers. In Section 4, we verify that diffusion Transformers under the µP exhibit robust transferability of base HPs. and {ηj}R j=1 i=1 3.2.3 µTransfer from Proxy Task to Target Task Once base HP transferability is validated for diffusion Transformers, we can directly apply the µTransfer algorithm [66] (Algorithm 4 in Appendix D) to zero-shot transfer base HPs from proxy task to target task. Specifically, both the proxy model with width nproxy and the target model with width ntarget are parameterized by µP using the same base width nbase. We first search for an optimal combination of base HPs using various proxy models trained with small batch size Bproxy and limited training steps Tproxy. These optimal base HPs are then used to train the large target model with larger batch size Btarget and longer training iteration Ttarget. Experimental results in Section 5 show the superior performance of the µTransfer in real-world vision generation."
        },
        {
            "title": "4 Systematic Investigation for DiT-µP on ImageNet",
            "content": "In this section, we first empirically verify the base HP transferability of DiT [48] under µP. We then µTransfer the optimal learning rate to train DiT-XL-2-µP, which achieves 2.9 faster convergence. 1To scaling head dimension, Yang et al. [66] additionally change the calculation of attention logit from qk/ to qk/d, where query and key have dimension d. We do not use this modification [5]. (a) HP transfer across widths. (b) HP transfer across batch sizes. (c) HP transfer across iterations. Figure 3 DiT-µP enjoys base HP transferability. Unless otherwise specified, we use model width of 288, batch size of 256, and 200K training iterations. Missing data points indicate training instability, where the loss explodes. Under µP, the base learning rate can be transferred across model widths, batch sizes, and steps."
        },
        {
            "title": "4.1 Basic Experimental Settings\nTo ensure a fair comparison between DiT-µP and the original DiT [48], we adopt the default configurations\nfrom [48] and describe the basic setup in detail below.",
            "content": "Dataset. We train DiT and DiT-µP on the ImageNet training set [11], which contains 1.28M images across 1,000 classes. All images are resized to resolution of 256 256 during training, following standard practice in generative modeling benchmarks [1, 46, 48]. Architecture of DiT-µP. The architecture of DiT-µP models is identical to that of DiT-XL-2, except for the width. We fix the attention head dimension at 72 (as in DiT-XL-2) and vary the number of heads. The base width nbase in the µP setup corresponds to 288, which uses 4 attention heads. Training. We train DiT and DiT-µP using the AdamW [44]. Following the original DiT setup [48], we do not apply any learning rate schedule or weight decay, and constant learning rates are used in all experiments. The original DiT-XL-2 is trained with learning rate 104 and batch size of 256. Evaluation metrics. To comprehensively evaluate generation performance, we report FID [25], sFID [47], Inception Score [51], precision, and recall [31] on 50K generated samples. In the main text, we present the FID results, while the remaining metrics are provided in Appendix E.2."
        },
        {
            "title": "4.2 Base Hyperparameter Transferability of DiT-µP\nIn this section, we evaluate the HP transferability of DiT-µP across different widths, batch sizes, and\ntraining steps. We focus primarily on the base learning rate, as it has the most significant impact on\nperformance [12, 27, 40, 66]. Results for other HP (weight multiplier) are provided in Appendix E.2. We sweep\nthe base learning rate over the set {2−13, 2−12, 2−11, 2−10, 2−9} across various widths, batch sizes, and training\nsteps. FID-50K results are shown in Figure 3, and comprehensive results for other metrics are presented in\nTables 7, 8, and 9 in Appendix E.2.",
            "content": "As presented in Figure 3, the optimal base learning rate 210 generally transfers across scaling dimensions when some minimum width (e.g. 144), batch size (e.g., 256), and training steps (e.g., 150K) are satisfied, which verifies the base HP transferability of DiT-µP. Interestingly, we observe that neural networks with µP tend to favor large learning rate close to the maximum stable value (e.g., see Figure 3a). This aligns with empirical findings reported for standard neural networks trained for multiple epochs [9, 34, 35, 41, 58]. It suggests that the optimization landscape of µP shares certain similarities with that of SP, offering direction for future theoretical investigation."
        },
        {
            "title": "4.3 Scaling Performance of DiT-µP\nWe µTransfer the optimal base learning rate of 2−10 to train DiT-XL-2-µP with a width of 1152. The batch\nsize is set to 256, following [48]. We evaluate DiT-µP every 400K steps without using classifier-free guidance.",
            "content": "6 Training continues until DiT-µP surpasses the best performance reported by the original DiT at final 7M steps [48]. To enable detailed comparison throughout the training, we also reproduce the original DiT training using its official codebase. Complete evaluation results throughout training are provided in Table 12 in Appendix E.2. As shown in Figure 4, DiT-XL-2-µP with the transferred base learning rate performs effectively, achieving 2.9 faster convergence (2.4M steps) compared to the original DiT (7M steps). These results suggest that µP offers simple and promising approach to improve the pretraining of large-scale diffusion Transformers. In the following sections, we further validate this claim through experiments on real-world text-to-image generation tasks."
        },
        {
            "title": "5 Large-Scale Text-to-Image Generation",
            "content": "In this section, we verify the efficiency of µTransfer algorithm on real-world text-to-image generation tasks. Diffusion Transformers under µP outperform the baselines while requiring small tuning cost."
        },
        {
            "title": "5.1 Scaling PixArt-α-µP on SA-1B\nIn this section, we perform µTransfer experiments on the PixArt-\nα [8], scaling from 0.04B to 0.61B parameters. Using the same\npretraining setup, PixArt-α-µP with the transferred learning rate outperforms the original PixArt-α, while\nincurring only 5.5% FLOPs of one full pretraining run.",
            "content": "Figure 4 µP accerlates the training of diffusion Transformers. Considering FID-50K, DiT-XL-2-µP with transferred learning rate achieves 2.9 faster convergence than the original DiT-XL-2 and slightly better result. 5.1.1 Experimental Settings To ensure the fairest possible comparison between PixArt-α-µP and the original PixArt-α [8], we mainly adopt the original setup [8] and summarize the key components below. Dataset. We use the SAM/SA-1B dataset [30], which contains 11M high-quality images curated for segmentation tasks with diverse object-rich scenes. For text captions, we use the SAM-LLaVA annotations released in [8]. All images are resized to resolution of 256 256 during training. Architecture of PixArt-α-µP models. The target PixArt-α-µP model adopts the same architecture as PixArt-α (0.61B parameters). The proxy model also follows the same architecture, differing only in width. To construct the proxy PixArt-α-µP model, we fix the attention head dimension at 72 (as in PixArt-α) and reduce the number of heads from 16 to 4 (0.04B parameters). In the µP framework, the base width nbase (see Section 3.2.1) is set to the proxy width of 288. Training. PixArt-α is implemented using the official codebase and original configuration. We train the original PixArt-α and the target PixArt-α-µP model for 30 epochs with batch size of 176 32 (approximately 59K steps).2 The small proxy PixArt-α-µP models are trained for 5 epochs with batch size of 176 8 (approximately 39K steps). Notably, the model width, batch size, and training steps are all smaller than those used in the target pretraining setting. Hyperparameter search. In this section, we focus solely on the base learning rate. We search over the candidate set {213, 212, 211, 210, 29} as in Section 4, resulting in five proxy training trials. Ratio of tuning cost to pretraining cost. We consider the FLOPs as the metric for the computational cost, then the ratio of tuning cost to pretraining cost can be estimated as ratio = RSproxyEproxy StargetEtarget = RSproxyBproxyTproxy StargetBtargetTtarget , (1) 2We confirmed with the authors that this setup is reasonable; longer training may result in overfitting. 7 Table 2 Results of base learning rate search on PixArt-α-µP proxy tasks. 0.04B proxy models with different learning rates are trained for 5 epochs on the SAM dataset. Overall, the base learning rate 210 is the optimal. log2(lr) Training loss GenEval FID-30K (MS-COCO) FID-30K (MJHQ) -9 -10 -11 -12 -13 NaN 0.1493 0.1494 0.1504 0.1536 NaN 0.083 0.078 0.030 0.051 NaN 47.46 49.24 66.77 60.28 NaN 47.71 46.31 63.36 60.93 Table 3 Comprehensive comparison between PixArt-α-µP and PixArt-α. Both models are trained on the SAM dataset for 30 epochs. PixArt-α-µP (0.61B), using base learning rate transferred from the optimal 0.04B proxy model, consistently outperforms the original baseline throughout the training process. Epoch Method GenEval MJHQ MS-COCO FID-30K CLIP Score FID-30K CLIP Score 10 30 PixArt-α [8] PixArt-α-µP (Ours) PixArt-α [8] PixArt-α-µP (Ours) PixArt-α [8] PixArt-α-µP (Ours) 0.19 0.20 0.20 0.23 0.15 0. 38.36 33.35 35.68 33.42 42.71 29.96 25.78 26.25 26. 26.83 26.25 27.13 34.58 29.68 30.13 29.05 37.61 25. 28.12 28.87 28.81 29.53 28.91 29.58 where is the number of HP search trials, is the number of parameters, is the training epochs, is the batch size and is the training iteration. The ratio 5.5% here, as detailed in Appendix E.3. Evaluation metrics. We evaluate text-to-image generation performance following standard practice [1, 8, 15, 59], including FID, CLIP Score, and GenEval [20]. Both FID and CLIP Score are computed on the aesthetic MJHQ-30K [36] and real MS-COCO-30K [42] datasets. MJHQ-30K contains 30K images generated by Midjourney, while MS-COCO-30K is randomly sampled subset of the MS-COCO [42] dataset. GenEval evaluates text-image alignment using 533 test prompts. 5.1.2 Experimental Results of PixArt-α-µP We begin by conducting base learning rate search using the PixArt-α-µP proxy models. The evaluation results for different base learning rates are summarized in Table 2. Since overfitting is not observed in this setting, we include training loss as an additional evaluation metric. Overall, the base learning rate of 210 yields the best performance. Interestingly, this optimal learning rate matches that of DiT-µP (see Figure 3a). We hypothesize that this consistency arises from the architectural similarity between the two models and the fact that both the ImageNet and SAM datasets consist of real-world images. This observation suggests that optimal base HPs may exhibit some degree of transferability across different datasets and architectures. We then apply µTransfer by transferring the searched optimal base learning rate of 210 to train the target PixArt-α. comparison between PixArt-α and PixArt-α-µP throughout training is provided in Table 3 (with the complete results shown in Table 14 in Appendix E.3). The results demonstrate that PixArt-α-µP consistently outperforms PixArt-α across all evaluation metrics during training, supporting µP as an efficient and robust approach for scaling diffusion Transformers. In the following section, we further extend this principle to large-scale applications."
        },
        {
            "title": "5.2 Scaling MMDiT to 18B\nIn this section, we validate the efficiency of µP in the large-scale setup. We scale up the MMDiT [15]\narchitecture from 0.18B to 18B. Under the same pretraining setup, MMDiT-µP-18B with the transferred base",
            "content": "8 (a) Base learning rate. (b) Base gradient clip. (c) Base REPA weight. (d) Base Warm-up steps. Figure 5 Results of base HP search on proxy MMDiT-µP tasks. We train 0.18B MMDiT-µP proxy models with 80 different base HPs settings. The optimal base HPs are transferred to the training of 18B target model. HPs outperforms the MMDiT-18B tuned by human algorithmic experts."
        },
        {
            "title": "5.2.1 Experimental Settings",
            "content": "Dataset. We train models on an internally constructed dataset comprising 820M high-quality image-text pairs. All images are resized to resolution of 256 256 during training. Baseline MMDiT-18B. The width and depth of MMDiT-18B are 5,120 and 16, respectively. The training objective combines flow matching loss [43, 46] and representation alignment (REPA) loss [69]. The model is optimized by AdamW [44], with batch size of 4,096 and 200K training iterations. The learning rate schedule is constant with warm-up duration. The HPs were tuned by algorithmic experts, requiring roughly 5 times the cost of full pretraining, as detailed in Appendix E.4. Architecture of MMDiT-µP models. The target MMDiT-µP-18B model shares the same architecture as MMDiT-18B. The proxy model also follows this architecture, differing only in width by reducing the number of attention heads. It contains 0.18B parameters (1% of the target model) with width of 512. In the µP setup, the base width nbase is set to 1,920 (see Section 3.2.1). Training of MMDiT-µP models. The training procedure for the target MMDiT-µP-18B is identical to that of the baseline MMDiT-18B models, except for the selected HPs. The proxy models are trained for 30K steps with batch size of 4,096. We conduct 80 searches over four base HPs. Concretely, we uniformly sample base learning rate from 2.5 105 to 2.5 103, gradient clipping from 0.01 to 100, weight of REPA loss from 0.1 to 1, and warm-up iteration from 1 to 20K. To further verify that 30K iterations are enough for the proxy task, we conducted five proxy training runs (100K steps each) using the searched optimal HPs and different learning rates, as detailed in Appendix E.4. As derived in Appendix E.4 based on Equation (1), the total tuning FLOPs under µP amounts to 14.5% of one full pretraining cost, and thus only 3% of the human-tuned cost. We think the batch size and iterations used during the base HP search could be further reduced to lower the tuning cost. However, due to limited resources, we are unable to explore additional setups in this work. Evaluation metrics. We use training loss to select base HPs on proxy tasks, as the dataset is passed through only once, and overfitting does not occur. To assess the final text-to-image generation, we created an internal benchmark comprising 150 prompts to comprehensively evaluate text-image alignment. Each prompt includes an average of five binary alignment checks (yes or no), covering wide range of concepts such as nouns, verbs, adjectives (e.g., size, color, style), and relational terms (e.g., spatial relationships, size comparisons). Ten human experts conducted the evaluation, with each prompt generating three images, resulting in total of 22,500 alignment tests. The final score is computed as the average correctness across all test points. The details can be found in Appendix E.4. 5.2.2 Analyzing the Results of the Random Search The visualization of the results of base HP search is shown in Figure 5. First, the base learning rate has the most significant impact on training loss, with the optimal value identified as 2.5 104. Interestingly, unlike 9 Table 4 Results of human evaluation for textimage alignment. The alignment accuracy (acc.) is computed as the average over 22,500 human alignment tests. MMDiT-µP-18B achieves superior results with only 3% of the manual tuning cost. Method Alignment acc. MMDiT-18B MMDiT-µP-18B (Ours) 0.703 0.715 Figure 6 MMDiT-µP-18B achieves consistently lower training loss than baseline after 50K steps. the DiT and PixArt setups, the optimal learning rate here is not close to the maximum stable value. This highlights key difference between multi-epoch training in traditional deep learning and the single-epoch training in large model pretraining [58]. Second, the optimal gradient clipping value is 1, which deviates from the common practice of using small value (e.g., 0.1) to stabilize pretraining. Intuitively, µP favors larger clipping value, as aggressive clipping can undermine the maximal update property central to µP. Third, the optimal weight for the REPA loss is determined to be around 0.5, consistent with the experience from existing works [69]. Finally, the warm-up iteration has negligible impact on the training loss, so we adopt the default value of 1K. 5.2.3 µTransfer Results of MMDiT-µP The comparisons of training losses and human evaluations are shown in Figure 6 and Table 4, respectively. In addition, the complete comparison of training losses and detailed visualizations are provided in Figure 8 and Figure 10 in Appendix E.4. As result, MMDiT-µP-18B outperforms the baseline MMDiT-18B in both cases, achieving this with only 3% FLOPs of the standard manual tuning cost. These results demonstrate that µP is reliable principle for scaling diffusion Transformers. As models grow larger and standard HP tuning becomes prohibitively expensive, µP offers scientifically grounded framework to unlock the full potential of large models."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this paper, we extend µP from standard Transformers to diffusion Transformers. By proving that mainstream diffusion Transformers share the same µP form as vanilla Transformers, we enable direct application of existing µP practice and verify the reliable base HP transfer from small to large diffusion Transformers. This leads to practical performance gains on DiT-XL-2, PixArt-α, and MMDiT-18B, while requiring small fraction of the usual tuning effort (e.g., 3% for MMDiT-18B). Our results establish µP as principled and efficient scaling strategy for diffusion Transformers. Broader Impacts and Limitations. Our work has the potential to accelerate progress in generative modeling applications using diffusion Transformers, including text-to-image and video generation. However, improvements in scaling diffusion Transformers could also facilitate the creation of deepfakes for disinformation. Regarding the limitations of this work, although we demonstrate the efficiency of µP in large-scale applications, we do not identify the optimal proxy task size that balances HP tuning cost and target model performance, due to limited computational resources."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Enze Xie for helpful discussion on the experimental setup of PixArt-α."
        },
        {
            "title": "References",
            "content": "[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, pages 2266922679, 2023. 10 [2] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [4] Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Y. Prince, Björn Deiseroth, Andrés Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, and Douglas Orr. u-µp: The unit-scaled maximal update parametrization. CoRR, abs/2407.17465, 2024. [5] Blake Bordelon, Hamza Tahir Chaudhry, and Cengiz Pehlevan. Infinite limits of multi-head transformer dynamics. In NeurIPS, 2024. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1:8, 2024. [7] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-Σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, volume 15090, pages 7491, 2024. [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [9] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In ICLR, 2021. [10] Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z. Kaplan, and Enrico Shippole. Scalable high-resolution pixel-space image synthesis with hourglass diffusion transformers. In ICML, 2024. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. [12] Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster. CoRR, abs/2304.03208, 2023. [13] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, pages 87808794, 2021. [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [16] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to 16 billion parameters. CoRR, abs/2407.11633, 2024. [17] Peng Gao, Le Zhuo, Dongyang Liu, Ruoyi Du, Xu Luo, Longtian Qiu, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. [18] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In ICCV, pages 2310723116, 2023. [19] Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. [20] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. In NeurIPS, 2023. 11 [21] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [22] Ishaan Gulrajani and Tatsunori B. Hashimoto. Likelihood-based diffusion language models. In NeurIPS, 2023. [23] Moritz Haas, Jin Xu, Volkan Cevher, and Leena Chennuru Vankadara. µp2: Effective sharpness aware minimization requires layerwise perturbation scaling. In NeurIPS, 2024. [24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, pages 10261034, 2015. [25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Advances in Neural Information Processing Systems, pages 66266637, 2017. [26] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):17351780, 1997. [27] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. CoRR, abs/2404.06395, 2024. [28] Zhongzhan Huang, Pan Zhou, Shuicheng Yan, and Liang Lin. Scalelong: Towards more stable training of diffusion model via scaling network long skip connection. In NeurIPS, 2023. [29] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, ICLR, 2015. [30] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloé Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick. Segment anything. In ICCV, pages 39924003, 2023. [31] Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In Advances in Neural Information Processing Systems, pages 39293938, 2019. [32] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [33] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proc. IEEE, 86(11):22782324, 1998. [34] Jaehoon Lee, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. In NeurIPS, 2020. [35] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. CoRR, abs/2003.02218, 2020. [36] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. CoRR, abs/2402.17245, 2024. [37] Hao Li, Shamit Lal, Zhiheng Li, Yusheng Xie, Ying Wang, Yang Zou, Orchid Majumder, R. Manmatha, Zhuowen Tu, Stefano Ermon, Stefano Soatto, and Ashwin Swaminathan. Efficient scaling of diffusion transformers for text-to-image generation. CoRR, abs/2412.12391, 2024. [38] Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R. Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, and Stefano Soatto. On the scalability of diffusion-based text-to-image generation. In CVPR, pages 94009409, 2024. [39] Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen Qin, Zheng Zhang, Aixin Sun, and Yequan Wang. FLM-101B: an open LLM and how to train it with $100k budget. CoRR, abs/2309.03852, 2023. [40] Xiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Chao Wang, Xinzhang Liu, Zihan Wang, Yu Zhao, Xin Wang, Yuyao Huang, Shuangyong Song, Yongxiang Li, Zheng Zhang, Bo Zhao, Aixin Sun, Yequan Wang, Zhongjiang He, Zhongyuan Wang, Xuelong Li, and Tiejun Huang. Tele-flm technical report. CoRR, abs/2404.16645, 2024. 12 [41] Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large learning rate in training neural networks. In NeurIPS, pages 1166911680, 2019. [42] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, volume 8693, pages 740755, 2014. [43] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. [44] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR. OpenReview.net, 2019. [45] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. In ICML, 2024. [46] Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, volume 15135, pages 2340, 2024. [47] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W. Battaglia. Generating images with sparse representations. In ICML, volume 139, pages 79587968. PMLR, 2021. [48] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41724182. IEEE, 2023. [49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, volume 9351, pages 234241, 2015. [50] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. [51] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pages 22262234, 2016. [52] Kyle Sargent, Kyle Hsu, Justin Johnson, Li Fei-Fei, and Jiajun Wu. Flow to the mode: Mode-seeking diffusion autoencoders for state-of-the-art image tokenization. CoRR, abs/2503.11056, 2025. [53] Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers. In NeurIPS, 2024. [54] Leena Chennuru Vankadara, Jin Xu, Moritz Haas, and Volkan Cevher. On feature learning in structured state space models. In NeurIPS, 2024. [55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 59986008, 2017. [56] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [57] Mitchell Wortsman, Peter J. Liu, Lechao Xiao, Katie E. Everett, Alexander A. Alemi, Ben Adlam, John D. Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-scale proxies for large-scale transformer training instabilities. In ICLR, 2024. [58] Lechao Xiao. Rethinking conventional wisdom in machine learning: From generalization to scaling. CoRR, abs/2409.15156, 2024. [59] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: efficient high-resolution image synthesis with linear diffusion transformers. CoRR, abs/2410.10629, 2024. [60] Greg Yang. Wide feedforward or recurrent neural networks of any architecture are gaussian processes. In NeurIPS, pages 99479960, 2019. 13 [61] Greg Yang. Tensor programs II: neural tangent kernel for any architecture. CoRR, abs/2006.14548, 2020. [62] Greg Yang. Tensor programs III: neural matrix laws. CoRR, abs/2009.10685, 2020. [63] Greg Yang and Edward J. Hu. Tensor programs IV: feature learning in infinite-width neural networks. In Marina Meila and Tong Zhang, editors, ICML, volume 139, pages 1172711737. PMLR, 2021. [64] Greg Yang and Etai Littwin. Tensor programs iib: Architectural universality of neural tangent kernel training dynamics. In ICML, volume 139, pages 1176211772, 2021. [65] Greg Yang and Etai Littwin. Tensor programs ivb: Adaptive optimization in the infinite-width limit. CoRR, abs/2308.01814, 2023. [66] Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs V: tuning large neural networks via zero-shot hyperparameter transfer. CoRR, abs/2203.03466, 2022. [67] Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs VI: feature learning in infinite depth neural networks. In ICLR, 2024. [68] Yuanyang Yin, Yaqi Zhao, Mingwu Zheng, Ke Lin, Jiarong Ou, Rui Chen, Victor Shea-Jay Huang, Jiahao Wang, Xin Tao, Pengfei Wan, Di Zhang, Baoqun Yin, Wentao Zhang, and Kun Gai. Towards precise scaling laws for video diffusion transformers. CoRR, abs/2411.17470, 2024. [69] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. CoRR, abs/2410.06940, 2024. [70] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [71] Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, et al. Yulan: An open-source large language model. arXiv preprint arXiv:2406.19853, 2024."
        },
        {
            "title": "Contents of Appendix",
            "content": "A Additional Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Scaling Diffusion Transformers A.2 Applications of µP in AIGC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Theoretical Background of µP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 abc-Parameterization Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Neor Program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 µP of Any Representable Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Extensions to Other HPs Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Proof of DiT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Proof of PixArt-α . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Proof of U-ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Proof of MMDiT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Details for Section 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Experimental Details and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1 Assets and Licenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Additional Details of DiT Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.1 Evaluation Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.2 Additional Results of Base HP Transferability . . . . . . . . . . . . . . . . . . . . . . . E.2.3 Additional Results of DiT-XL-2 Pretraining . . . . . . . . . . . . . . . . . . . . . . . . E.2.4 Computational Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Additional Details of MMDiT Experiments E.3 Additional Details of PixArt-α Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.1 FLOPs ratio of HP Search on Proxy Models . . . . . . . . . . . . . . . . . . . . . . . . E.3.2 Additional Results of Base Learning Rate Search . . . . . . . . . . . . . . . . . . . . . E.3.3 Additional Results of PixArt-α Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4.1 HPs Tuned by Human Experts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4.2 Additional Details of Base HP Search . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4.3 FLOPs ratio of HP Search on Proxy Models . . . . . . . . . . . . . . . . . . . . . . . . E.4.4 Additional Results of Training Loss Comparison . . . . . . . . . . . . . . . . . . . . . E.4.5 Additional Details of Human Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . E.4.6 Additional Results of Visualization Comparison . . . . . . . . . . . . . . . . . . . . . . 16 16 16 16 16 17 18 18 18 22 23 24 24 24 24 24 25 25 25 32 32 32 33 33 33 33 33 34"
        },
        {
            "title": "Appendix A Additional Related Work",
            "content": "A.1 Scaling Diffusion Transformers HourglassDiT [10] introduces hierarchical architecture with downsampling and upsampling in DiT, reducing computational complexity for high-resolution image generation. SD3 [15] scales MMDiT to 8B parameters by increasing the backbone depth. Large-DiT [17] incorporates LLaMAs text embeddings and scales the DiT backbone, demonstrating that scaling to 7B parameters improves convergence speed. DiT-MoE [16] further scales diffusion Transformers to 16.5B parameters using mixture-of-experts architecture. Li et al. [37] empirically investigate the scaling behavior of various diffusion Transformers, finding that U-ViT [1] scales more efficiently than cross-attention-based DiT variants. Yin et al. [68] systematically analyze scaling laws for video diffusion Transformers, enabling the prediction of optimal hyperparameters (HPs) for any model size and compute budget. We note that all these works adopt standard parameterization (SP), which prevents the transfer of optimal HPs across different model sizes, and thus suffer from heavy tuning costs at scale. A.2 Applications of µP in AIGC Recently, µP has been successfully applied to the pretraining of large language models (LLMs) [12, 22, 27, 39, 40, 66, 71], reducing HP tuning costs and stabilizing training. For diffusion Transformers, some works have also employed µP. For example, Gulrajani and Hashimoto [22] searches for the base learning rate under µP and transfer it to 1B-parameter diffusion language model, while Sargent et al. [52] use µP to tune the HPs of Transformer-based diffusion autoencoder, achieving state-of-the-art image tokenization across multiple compression rates. However, these studies differ fundamentally from ours. First, while they assume the HP transferability property of original µP (even though it may not hold in their setups), our work provides rigorous theoretical guarantees for the µP of diffusion Transformers and systematically verifies its HP transferability. Second, we validate the scalability of µP at up to 18B parameters, significantly larger than the models used in prior work. Appendix Theoretical Background of µP In this section, we review the theoretical foundations of µP. For simplicity, we mainly focus on the Adam/AdamW optimizer widely used in practice. Besides, the ϵ in the optimizer is assumed to be zero, which is very small in practice (e.g., 108). B.1 abc-Parameterization Setup For completeness, we restate the abc-parameterization from Section 2, which underpins typical µP theory. The abc-parameterization specifies the multiplier, initial variance, and learning rate for each weight [63, 66]. Specifically, let denote the network width, we parameterize each weight as = ϕW naW (cid:102)W , where the trainable component (cid:102)W is initialized as (cid:102)Wij (0, σ2 n2bW ). The learning rate for (cid:102)W is parameterized as ηW ncW . The width-independent quantities ϕW , σW , and ηW are referred to as base HPs and can be transferred across different widths. B.2 Neor Program µP is originally proposed by Tensor Programs [6065, 67], which is theoretical series mainly developed to express and analyze the infinite-width limits of neural networks both at initialization and during training. The latest and most general version among them is the Neor Program [65], which is formed by inductively generating scalars and vectors, starting from initial matrices/vectors/scalars (typically the initial weights of the neural network) and applying defined operations, including vector averaging (Avg), matrix multiplication (MatMul), and nonlinear outer product transformations (OuterNonlin). We introduce these operators, where is the model width tending to be infinite. 16 Table 5 µP for (diffusion) Transformers with Adam/AdamW optimizer. We use purple text to highlight the differences between µP and standard parameterization (SP) in practice (e.g., Kaiming initialization [24]), and gray text to indicate the SP settings. We do not contain the scalar weights in the main text because they do not exist in DiT. Input weights Hidden weights Output weights Scalar weights aW bW cW 0 0 0 0 1/2 1 (0) 1 (0) 0 (1/2) 0 0 0 0 Avg We can choose existing vector Rn and append to the program scalar 1 (cid:88) α=1 xα R. MatMul We can choose matrix Rnn and vector Rn existing in the program, and append to the program vector Rn or Rn. OuterNonlin For any integer k, 0, we can aggregate existing vectors and existing scalars to Rnk and Rl, respectively. With an integer 0 and pseudo-Lipschitz function ψ : Rk(r+1)+l (e.g., SiLU, GeLU), we append to the program vector Rn, yα = 1 nr (cid:88) β1,...,βr=1 ψ(Xα:; Xβ1:; ...; Xβr:; c), where the + 1 is called the order of ψ. While the above operators do not directly instruct to transform scalars into scalars, previous work [65] has proved that this can be done by combining the above operators. This can be formed as follows. Lemma B.1 (Scalars-to-scalars transformation is representable by the Neor Program, Lemma 2.6.2 in [65]). If ψ : Rl is pseudo-Lipschitz function and are the collected scalars in Neor program, then ψ(c) can be introduced as new scalar in the program. B.3 µP of Any Representable Architecture The seminal work of Yang and Littwin [65] systematically studies the dynamics of common neural architectures trained with adaptive optimizers (e.g., Adam [29], AdamW [44]), where the forward pass of neural network can be expressed as Neor program (e.g., MLPs, CNNs [33], RNNs [26], standard Transformers [55]). In this setting, Yang and Littwin [65] prove the existence of unique, optimal scaling ruleMaximal Update Parametrization (µP)under which features in each layer evolve independently of width and at maximal strength. Otherwise, feature evolution in some layers will either explode or vanish in the infinite-width limit, rendering large-scale pretraining ineffective. Concretely, µP is implemented by analytically adjusting HPs with model width, typically involving the weight multiplier, initialization variance, and learning rate considered in abc-parameterization. The µP formulation of any architecture whose forward pass can be expressed as Neor program follows the same scaling rules presented in Table 5 (equivalent to Table 1 in the main text). Specifically, it adjusts the abc-parameterization of each weight according to its type, where the weights are categorized into four types: input weights, hidden weights, output weights, and scalar weights. The definitions are as follows: input weights satisfy din = Θ(1) and dout = Θ(n); hidden weights satisfy din = Θ(n) and dout = Θ(n); output weights satisfy din = Θ(n) and dout = Θ(1); and scalar weights satisfy din = Θ(1) and dout = Θ(1), where n, din, dout denotes the model width, fan-in dimension and fan-out dimension, respectively. The µP result is formally summarized in the following lemma. 17 Lemma B.2 (µP of representable architecture, Definition 2.9.12 in [65]). If the forward pass of an architecture can be represented by Neor program, its µP formulation follows the scaling rules in Table 5. B.4 Extensions to Other HPs The µP principle can be extended to other HPs, such as common optimization-related HPs (e.g., warm-up iteration). We refer the reader to [65, 66] for comprehensive discussion. We mainly focus on the HPs that occur in the main text. Gradient clip: The clip value should be held constant with respect to width. Warm-up iteration: The warm-up iteration should be held constant with respect to width. Weights of different losses (e.g., REPA loss, noise schedule): The weights of different losses should be held constant with respect to width. Weight decay: For coupled weight decay (e.g., Adam [29]), the weight decay value can not be transferred. For decoupled weight decay (e.g., AdamW [44]), the weight decay value should be held constant with respect to width. Appendix Proof of Theorem 3. Elementary notations. We use lightface (e.g., a, A), lowercase boldface (e.g., a), and uppercase boldface letters (e.g., A) to denote scalars, vectors, and matrices, respectively. For vector a, we denote its i-th element as ai. For matrix A, we use Ak:, A:k and Aij to denote its k-th row, k-th column and (i, j)-th element, respectively. We define [n] = {1, 2, . . . , n}. For the readers convenience, we restate Theorem 3.1 as follows. Theorem C.1. The forward passes of mainstream diffusion Transformers (DiT [48], U-ViT [1], Pixart-α [8], and MMDiT [15]) can be represented within the Neor Program. Therefore, their µP matches the standard µP presented in Table 5. Following the standard practice in the literature [23, 63, 65, 67] to simplify the derivation, we adopt the following assumption in all of our proofs. The general case is trivial adaptation [23, 60, 65]. Assumption C.1. We assume that constant dimensions (fixed during scaling the width of neural networks) are 1, which includes data dimension, label dimension, patch size, frequency embedding size of time embedding, dimension of attention head, and so on. We also assume the width of different layers is the same and do not consider bias parameters here. C.1 Proof of DiT Proof. By Lemma B.2, we can prove the theorem by proving the forward pass of DiT can be represented by the Neor Program. The DiT architecture includes an embedder for the input latent R, an embedder for the diffusion timestep R, an embedder for the label R, Transformer blocks with adaLN, and final layer with adaLN. In the following, we use the Neor Program to represent the forward computation of these modules in sequence. Embedder for the input latent x. The embedder for the is one-layer CNN, which is known to be representable by the Neor Program (e.g., see Program 7 in [60] for general CNNs with pooling). We write the program for clarity here. In our case, the one-layer CNNs parameters can be denoted by wCNN Rn, and the operation can be implemented by where ψ(wCNN α ; x) = wCNN α x. xembed α := ψ(wCNN α ; x), (OuterNonlin) 18 Positional embedding. Given number of patches (1 in our simplified case), the positional embedding is initialized with the sine-cosine version and fixed during the training. We use the xpos to denote the positional embedding, and adding it to the xembed can be written as xembed α := ψ([xembed, xpos]α:), (OuterNonlin) where ψ([xembed, xpos]α:) = xembed the input latent without confusion. α + xpos α . Here, we reuse the notation of xembed of the final embedding for Embedder for the diffusion timestep t. The embedder for the diffusion timestep is frequency embedding (one dimension here) followed by two-layer MLP. First, for the frequency embedding, we can represent it as tfreq := ψ(t). (Lemma B.1) Second, the MLP is known to be representable by the Neor Program (e.g., Program 1 in [60]). We write it here for completeness. We use w(1) Rn and (2) Rnn to denote the weights in the first layer and the second layer of the MLP, respectively. We can derive α ; tfreq), α := ψ1(w(1) t(1) α := ψ2(t(1) h(1) α ), tembed := (2)h(1), (OuterNonlin) (OuterNonlin) (MatMul) α ; tfreq) = w(1) where ψ1(w(1) Embedder for the label y. Denoting the weights of embedding as wembed Rn, then its computation can be written directly as α tfreq and ψ2(t(1) α ) = SiLU(t(1) α ). where ψ(wembed α ; y) = wembed α y. yembed α := ψ(wembed α ; y), (OuterNonlin) Merging the embeddings of and y. In DiT, we sum the embeddings of and as the condition for the following calculation, which can be written as cα := ψ([tembed, yembed]α:), . (OuterNonlin) where ψ([tembed, yembed]α:) = tembed Transformer block with adaLN. First, the adaLN block maps the condition to the gate, shift, and scale values (θattn, βattn, γattn, θmlp, βmlp, γmlp) for attention layer and MLP layer in the Transformer block. We denote the parameters of the adaLN block by (W θattn, βattn, γattn, θmlp, βmlp, γmlp), each of them is in Rnn. Then the forward pass of adaLN block can be written as + yembed α α (cid:101)cα := ψ(cα), θattn := θattn (cid:101)c, βattn := βattn (cid:101)c, γattn := γattn (cid:101)c, θmlp := θmlp (cid:101)c, βmlp := βmlp (cid:101)c, γmlp := γmlp (cid:101)c, (OuterNonlin) (MatMul) (MatMul) (MatMul) (MatMul) (MatMul) (MatMul) where ψ is the SiLU activation. Now, we denote the input feature of the Transformer block as Rn. layer norm without learnable parameters first normalizes it, which is also known to be representable by the Neor Program (e.g., Program 9 in [60]). We rewrite it in our simplified case. E[x] := 1 (cid:88) α=1 xα, (Avg) α := ψ1(xα; E[x]), x2 1 V[x] := (cid:88) x2 α, α= xnorm α := ψ2(xα; [E[x], V[x]]), (OuterNonlin) (Avg) (OuterNonlin) where ψ1(xα; E[x]) = (xα E[x])2, ψ2(xα; [E[x], V[x]]) = (xα E[x])/(cid:112)V[x] + ϵ. After that, the normalized feature interacts with the condition information via the output of adaLN (βattn, γattn), which can be represented as follows. (cid:101)xnorm α := ψ([xnorm, βattn, γattn]α:), (OuterNonlin) where ψ([xnorm, βattn, γattn]α:) = γattn Now, the processed feature (cid:101)xnorm is put to the multi-head attention layer, which is known to be representable by the Neor Program (e.g., see Program 10 in [60]). We write it for completeness here. We denote K, Q, Rnn the attention layers key, query, and value matrix, each row representing head with dimension 1. The forward pass can be written as follows. α xnorm α + βattn α . := := := xattn α (cid:101)xnorm, (cid:101)xnorm, (cid:101)xnorm, := ψ([k, q, v]α:), (MatMul) (MatMul) (MatMul) (OuterNonlin) where ψ([k, q, v]α:) = kαqαvα. Before discussing the MLP layer, we still have residual connection and interaction from the gate value θattn, which can be represented by hα := ψ([xattn, θattn, x]α:), (OuterNonlin) where ψ([xattn, θattn, x]α:) = xα + θattn The remaining part is very similar to the above, which includes the layer norm, interaction with the outputs of adaLN, the MLP layer, and the residual connection. We write them in sequence for clarity. α xattn α . For the layer norm, we can write (cid:88) α=1 hα, E[h] := 1 h2 α := ψ1(hα; E[h]), 1 V[h] := (cid:88) h2 α, α= hnorm α := ψ2(hα; [E[h], V[h]]), (Avg) (OuterNonlin) (Avg) (OuterNonlin) where ψ1(hα; E[h]) = (hα E[h])2, ψ2(hα; [E[h], V[h]]) = (hα E[h])/(cid:112)V[h] + ϵ. For the interaction with scale and shift value from adaLN, we have (cid:101)hnorm α := ψ([hnorm, βmlp, γmlp]α:), (OuterNonlin) where ψ([hnorm, βmlp, γmlp]α:) := γmlp α hnorm α + βmlp α . 20 For the two-layer MLP in the Transformer block, we use (1), (2) Rnn to denote the weights in its first layer and the second layer, respectively. We have α := (1)(cid:101)hnorm, h(1) (cid:101)h(1) α := ψ(h(1) hmlp := (2)(cid:101)h(1), α ), (MatMul) (OuterNonlin) (MatMul) where the ψ is the GeLU activation function. For the final residual connection, it can be represented by hTblock α := ψ([hmlp, θmlp, h]α:), (OuterNonlin) where ψ([hmlp, θmlp, h]α:) = hα + θmlp Final layer with adaLN. The final layer includes the adaLN, layer norm, and linear projection. We discuss them in sequence. α hmlp α . The adaLN block has parameters (W βfinal, γfinal) and receive the condition c. Its forward pass can be written as follows. (cid:101)cα := ψ(cα), βfinal := βfinal γfinal := γfinal (cid:101)c, (cid:101)c, (OuterNonlin) (MatMul) (MatMul) where ψ is the SiLU activation. The layer norm layer normalizes the output of the transformer block hTblock, which is the same as the above layer norm. We omit the derivation here for simplicity. We denote the output of the layer norm by znorm Rn and the parameter of the linear projection by wfinal, then the remains calculation can be derived as α (cid:101)znorm (cid:101)zfinal α := ψ1([znorm, βfinal, γfinal]α:), := ψ2([wfinal, (cid:101)znorm]α:), zfinal = 1 (cid:88) α=1 (cid:101)zfinal α , (OuterNonlin) (OuterNonlin) (Avg) where ψ1([znorm, βfinal, γfinal]α:) := γfinal α (cid:101)znorm. Therefore, the forward pass from the inputs (x, y, t) to output zfinal can be represented by the Neor Program, which means that The µP of DiT is the same as the standard µP in Table 5. The proof is completed. and ψ2([wfinal, (cid:101)znorm]α:) = wfinal + βfinal α znorm α α C.2 Proof of PixArt-α Proof. Since most parts of the PixArt-α are the same as those in DiT, we only discuss the different modules from DiT for simplicity. These modules are mainly induced from the condition process. Embedder for the diffusion timestep t. The embedder for the diffusion timestep not only outputs tembed but also gate, shift, and scale values based on tembed (named adaLN-single in [8]), which are shared by the following Transformer block. We denote the additional parameters by (W θtattn, βtattn, γtattn, θtmlp, βtmlp, γtmlp), each of them is in Rnn. Then its forward pass can be written as ), (cid:101)tα := ψ(tembed α θtattn := θtattn (cid:101)t, βtattn := βtattn (cid:101)t, (OuterNonlin) (MatMul) (MatMul) 21 γtattn := γtattn (cid:101)t, θtmlp := θtmlp (cid:101)t, βtmlp := βtmlp (cid:101)t, γtmlp := γtmlp (cid:101)t, (MatMul) (MatMul) (MatMul) (MatMul) where ψ is the SiLU activation. Transformer block with gate-shift-scale table. PixArt-α replaces the adaLN block in DiT block with the gate-shift-scale table, which parameters are denoted by θxattn, βxattn, γxattn, θxmlp, βxmlp, γxmlp Rn. The gate-shift-scale table interacts with the shared embeddings of in the following way. θattn α βattn α γattn α θmlp α βmlp α γmlp α := ψ([θxattn, θtattn]α:), := ψ([βxattn, βtattn]α:), := ψ([γxattn, γtattn]α:), := ψ([θxmlp, θtmlp]α:), := ψ([βxmlp, βtmlp]α:), := ψ([γxmlp, γtmlp]α:), (OuterNonlin) (OuterNonlin) (OuterNonlin) (OuterNonlin) (OuterNonlin) (OuterNonlin) where ψ(a, b) = + b. The above outputted gate, shift, and scale values are used in the following attention layer and MLP layer, just like DiT. One more difference between the Transformer block of PixArt-α and DiT is the existence of the additional cross-attention layer, which integrates the output of the self-attention layer and the text embedding yembed. We denote its parameters by cK, cQ, cV , proj Rnn It can be written as follows. := cKyembed, := cQh, := cV yembed, hv α := ψ([k, q, v]α:), hcross := projhv, (MatMul) (MatMul) (MatMul) (OuterNonlin) (MatMul) where ψ([k, q, v]α:) = kαqαvα. The following pre-LN MLP with gate-shift-scale table is trivial extension of the pre-LN MLP layer in the DiT block, like what we did above, so we omit them here. Final layer with shift-scale table. The only difference is that we replace the adaLN block with shift-scale table, just like what we did for the attention layer, so we omit the derivation here. C.3 Proof of U-ViT Proof. We only discuss the modules that do not occur in DiT and PixArt-α. We find that the only new operator in U-ViT is the long skip connection, which concatenates the current main branch hm Rn and the long skip branch hs Rn, and then performs linear projection to reduce the dimension. We denote the parameters of the linear layer by = [W m, s], where m, Rnn, then the long skip connection can be represented by (cid:101)hm = mhm, (cid:101)hs = shs, hα = ψ([(cid:101)hm, (cid:101)hs]α:), (MatMul) (MatMul) (OuterNonlin) where ψ([(cid:101)hm, (cid:101)hs]α:) = (cid:101)hm attention layer, MLP layer, residual connection) have been studied in DiT, we finish the proof. . Because other operators in U-ViT (CNN embeddings/decoder, layer norm, α + (cid:101)hs α 22 C.4 Proof of MMDiT Proof. For simplicity, we only discuss the modules that do not occur in the above models. The new module that emerges in the MMDiT is the joint attention with the QK-Norm for the latent feature and text condition c. For the latent feature, we denote the key/query/value parameters by Kx, Qx, Rnn, the parameters of QK-norm by γKx, γQx Rn, and the parameters of linear projection by Ox Rnn. We can define the similar parameters for the text condition as Kc, Qc, c, γKc, γQc, Oc. The forward pass of the joint attention layer can be written as follows. kx := Kxx, qx := Qxx, vx := xx, kc := Kcc, qc := Qcc, vc := cc, kxnorm α qxnorm α kcnorm α qcnorm α (cid:101)vx := ψ2([qxnorm, kxnorm, vx, kcnorm, vc]α:), (cid:101)vc := ψ2([qcnorm, kxnorm, vx, kcnorm, vc]α:), vxout := Ox vcout := Oc := ψ1([kx, γKx]α:), := ψ1([qx, γQx]α:), := ψ1([kc, γKc]α:), := ψ1([qc, γQc]α:), (cid:101)vx, (cid:101)vc, (MatMul) (MatMul) (MatMul) (MatMul) (MatMul) (MatMul) (OuterNonlin) (OuterNonlin) (OuterNonlin) (OuterNonlin) (OuterNonlin) (OuterNonlin) (MatMul) (MatMul) where ψ1(a, b) = ab/a and ψ2(a, b, c, d, e) = abc + ade. The proof is completed. 23 Appendix Additional Details for Section 3 In this section, we summarize the methodology for verifying base HP transferability across widths, batch sizes, and training steps in Algorithm 1, 2, and 3 respectively. We also present the µTransfer algorithm [66] in Algorithm 4. Algorithm 1 Validate the base HP Transferability of µP across widths 1: Input: set of widths {ni}P , set of base HP to validate {λj}R j= i=1 training iteration , base width nbase for = 1 to do 2: Output: whether model with µP enjoy base HP transferability across widths 3: for = 1 to do 4: 5: 6: 7: 8: λij µP(λj, nbase, ni) θi µP(θ, nbase, ni) Mij train ni-width model with b.s. B, iter. , HPs λij, θi sij evaluate score for Mij , other fixed base HPs θ, batch size B, actual HP for ni-width model actual HP for ni-width model end for arg maxλj ({sij}R λ 9: 10: 11: end for 12: return whether λ j=1) are approximately same Obtain the optimal base HP for ni-width model Algorithm 2 Validate the base HP Transferability of µP across batch sizes 1: Input: set of batch sizes {Bi}P , set of base HP to validate {λj}R j=1 i=1 training iteration , base width nbase for = 1 to do 2: Output: whether model with µP enjoy base HP transferability across batch sizes 3: for = 1 to do 4: 5: 6: λij µP(λj, nbase, n) θi µP(θ, nbase, n) Mij train n-width model with b.s. Bi, iter. , HPs λij, θi sij evaluate score for Mij 7: 8: 9: 10: 11: end for 12: return whether λ end for arg maxλj ({sij}R λ j=1) are approximately same , other fixed base HPs θ, width n, actual HP for n-width model actual HP for n-width model Obtain the optimal base HP for n-width model"
        },
        {
            "title": "Appendix E Additional Experimental Details and Results",
            "content": "In this section, we present the additional experimental details and results which are neglected by the main text. E.1 Assets and Licenses All used assets (datasets and codes) and their licenses are listed in Table 6. E.2 Additional Details of DiT Experiments E.2.1 Evaluation Implementation Following the original DiT paper [48], we use the codebase of ADM [13] to obtain the results of FID, sFID, IS, precision, and recall. 24 Algorithm 3 Validate the base HP Transferability of µP across training steps 1: Input: set of training steps {Ti}P size B, width n, base width nbase , set of base HP to validate {λj}R j=1 i=1 for = 1 to do 2: Output: whether model with µP enjoy base HP transferability across training steps 3: for = 1 to do 4: 5: 6: 7: λij µP(λj, nbase, n) θi µP(θ, nbase, n) Mij train n-width model with b.s. B, iter. Ti, HPs λij, θi sij evaluate score for Mij , other fixed base HPs θ, batch actual HP for n-width model actual HP for n-width model Obtain the optimal base HP for n-width model end for arg maxλj ({sij}R λ 8: 9: 10: 11: end for 12: return whether λ j=1) are approximately same Algorithm 4 µTransfer (Algorithm 1 in [66]) 1: Input: base/proxy/target width nbase/nproxy/ntarget, proxy/target batch size Bproxy/Btarget, proxy/target training steps Tproxy/Ttarget, set of base HPs to search {θi}R j= sj evaluate score for Mj θproxy,j µP(θj, nbase, nproxy) 2: Output: Trained target model Mtarget 3: for = 1 to do 4: 5: Mj train nproxy-width model with batch size Bproxy, training steps Tproxy, HPs θproxy,j 6: 7: end for 8: θ arg maxθj ({sj}R 9: θtarget µP(θ, nbase, ntarget) 10: Mtarget train ntarget-width model with batch size Btarget, training steps Ttarget, HPs θtarget 11: return Mtarget j=1) actual HPs of target model actual HPs of proxy model E.2.2 Additional Results of Base HP Transferability Base learning rate. Comprehensive results of base learning rate transferability across widths, batch sizes, and training steps are presented in Tables 7, 8, and 9, respectively. Base output multiplier. Likewise that for base learning rate, we also sweep the base multiplier of output weight over the set {26, 24, 22, 20, 22, 24, 26} across various widths and training steps. FID-50K results are shown in Figure 7, and comprehensive results for other metrics are presented in Tables 10 and 11, respectively. As presented in Figure 7, the optimal base output multiplier is approximately transferable across different widths and training iterations. E.2.3 Additional Results of DiT-XL-2 Pretraining Complete benchmark results of DiT-XL-2 and DiT-XL-2-µP throughout the training are provided in Table 12. DiT-XL-2-µP, using base learning rate transferred from small proxy models, consistently outperforms the original DiT-XL-2 throughout the training process. E.2.4 Computational Cost It takes 104 (138) A100-80GB hours to train the DiT-µP with width of 288, batch size of 256, and train iteration of 200K steps. The computational cost of other DiT-µP proxy models can be inferred based on this situation. It takes around 224 (327) A100-80GB days to reproduce the pretraining of DiT-XL-2-µP. 25 Table 6 The used assets and licenses. URL Citation License https://www.image-net.org/ https://ai.meta.com/datasets/segment-anything/ http://images.cocodataset.org/zips/val2014.zip https://huggingface.co/datasets/playgroundai/MJHQ-30K https://github.com/djghosh13/geneval https://github.com/facebookresearch/DiT https://github.com/openai/guided-diffusion https://github.com/PixArt-alpha/PixArt-alpha https://github.com/microsoft/mup [11] [30] [42] [36] [20] [48] [13] [8] [66] non-commercial Apache-2.0 Commons Attribution 4.0 - MIT Link MIT Apache-2.0 MIT (a) HP transfer across widths. (b) HP transfer across iterations. Figure 7 DiT-µP enjoys base HP transferability. Unless otherwise specified, we use model width of 288, batch size of 256, and 200K training iterations. Missing data points indicate training instability, where the loss explodes. Under µP, the base output multiplier can be transferred across model widths and steps. 26 Table 7 DiT-µP enjoys base learning rate transferability across widths. We use batch size of 256 and 200K training iterations. NaN data points indicate training instability, where the loss explodes. Under µP, the base learning rate can be transferred across model widths. Width log2(lr) FID-50K sFID-50K Inception Score Precision Recall 144 144 144 144 144 288 288 288 288 288 576 576 576 576 576 -9 -10 -11 -12 -13 -9 -10 -11 -12 -13 -9 -10 -11 -12 - NaN 88.89 91.88 93.61 102.99 NaN 61.65 63.85 65.99 79.17 NaN 43.73 45.00 50.30 66. NaN 16.19 16.79 17.94 21.26 NaN 9.06 10.32 10.79 13.04 NaN 6.65 7.22 7.87 9. NaN 14.24 13.73 13.41 11.93 NaN 20.60 20.59 19.73 15.92 NaN 28.82 28.61 25.67 18. NaN 0.30 0.29 0.28 0.25 NaN 0.41 0.40 0.38 0.33 NaN 0.51 0.50 0.47 0. NaN 0.437 0.431 0.406 0.366 NaN 0.563 0.561 0.544 0.503 NaN 0.611 0.606 0.602 0. Table 8 DiT-µP enjoys base learning rate transferability across batch sizes. We use width of 288 and 200K training iterations. NaN data points indicate training instability, where the loss explodes. We bold the best result and underline the second-best result. Under µP, the base learning rate can be (approximately) transferred across batch sizes. Batch size log2(lr) FID-50K sFID-50K Inception Score Precision Recall 128 128 128 128 256 256 256 256 256 512 512 512 512 512 -9 -10 -11 -12 -13 -9 -10 -11 -12 -13 -9 -10 -11 -12 -13 NaN 74. 73.47 76.98 88.79 NaN 61.65 63.85 65.99 79.17 53.90 53.72 54.36 57.60 72.09 NaN 10.32 10.33 12.12 14.63 NaN 9.06 10.32 10.79 13.04 8.52 9.27 9.33 10.13 12.15 27 NaN 16. 16.83 16.50 14.10 NaN 20.60 20.59 19.73 15.92 24.18 24.66 24.96 23.37 17.81 NaN 0. 0.366 0.343 0.293 NaN 0.407 0.398 0.383 0.327 0.452 0.448 0.442 0.420 0.356 NaN 0.513 0.523 0.498 0. NaN 0.563 0.561 0.544 0.503 0.585 0.592 0.587 0.577 0.537 Table 9 DiT-µP enjoys base learning rate transferability across training steps. We use width of 288 and batch size of 256. NaN data points indicate training instability, where the loss explodes. Under µP, the base learning rate can be transferred across training steps. Iteration log2(lr) FID-50K sFID-50K Inception Score Precision Recall 100K 100K 100K 100K 100K 150K 150K 150K 150K 150K 200K 200K 200K 200K 200K -9 -10 -11 -12 -13 -9 -10 -11 -12 -13 -9 -10 -11 -12 -13 74.40 75.35 78.57 84.74 102.79 70.44 67.48 69.61 72.72 88. NaN 61.65 63.85 65.99 79.17 11.37 10.67 11.69 12.97 18.37 10.07 9.45 10.51 11.33 14. NaN 9.06 10.32 10.79 13.04 16.75 16.61 16.17 14.78 11.74 18.03 18.83 18.45 17.52 14.34 NaN 20.60 20.59 19.73 15.92 0.363 0.363 0.338 0.315 0.243 0.396 0.387 0.380 0.357 0.296 NaN 0.407 0.398 0.383 0. 0.504 0.514 0.495 0.474 0.363 0.534 0.553 0.533 0.515 0.455 NaN 0.563 0.561 0.544 0. 28 Table 10 DiT-µP enjoys base output multiplier transferability across widths. We use batch size of 256 and 200K training iterations. NaN data points indicate training instability, where the loss explodes. Under µP, the base output multiplier can be (approximately) transferred across model widths. Width log2(ϕout) FID-50K sFID-50K Inception Score Precision Recall 144 144 144 144 144 144 144 288 288 288 288 288 288 288 576 576 576 576 576 576 576 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 168.81 119.38 104.26 88.89 89.01 88.68 100.03 134.82 63.93 63.65 61.65 64.64 59.85 NaN 167.14 44.82 42.75 43.73 41.93 46.87 52.93 62.70 27.05 15.65 16.19 16.83 15.71 19.33 41.52 8.98 9.15 9.06 9.44 9.39 NaN 69.32 6.66 6.68 6.65 6.68 6.74 7. 6.30 10.77 12.17 14.24 13.97 14.19 12.48 9.44 20.31 20.23 20.60 19.30 20.95 NaN 5.04 29.47 30.73 28.82 29.45 26.78 24.07 0.157 0.185 0.262 0.297 0.294 0.303 0.262 0.184 0.395 0.401 0.407 0.398 0.415 NaN 0.130 0.510 0.526 0.509 0.537 0.501 0. 0.061 0.300 0.419 0.437 0.433 0.441 0.386 0.186 0.552 0.563 0.563 0.555 0.566 NaN 0.051 0.592 0.600 0.611 0.600 0.594 0.584 29 Table 11 DiT-µP enjoys base output multiplier transferability across training steps. We use width of 288 and batch size of 256. NaN data points indicate training instability, where the loss explodes. Under µP, the base output multiplier can be (approximately) transferred across training steps. Iteration log2(ϕout) FID-50K sFID-50K Inception Score Precision Recall 0.016 0.508 0.523 0.514 0.529 0.514 NaN 194.59 75.87 72.09 75.35 75.43 72.26 NaN 4.38 16.64 17.15 16.61 16.57 17.08 NaN 0.084 0.350 0.352 0.363 0.361 0.378 NaN 87.88 10.28 9.43 10.67 9.87 9.47 NaN 100K 100K 100K 100K 100K 100K 100K -6 -4 -2 0 2 4 150K 150K 150K 150K 150K 150K 150K 200K 200K 200K 200K 200K 200K 200K -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 150.86 68.94 68.24 67.48 68.67 67.32 NaN 134.82 63.93 63.65 61.65 64.64 59.85 NaN 48.06 9.26 9.77 9.45 9.29 9.08 NaN 41.52 8.98 9.15 9.06 9.44 9.39 NaN 8.17 18.57 18.78 18.83 17.96 18.17 NaN 9.44 20.31 20.23 20.60 19.30 20.95 NaN 0.163 0.377 0.386 0.387 0.383 0.397 NaN 0.184 0.395 0.401 0.407 0.398 0.415 NaN 0.143 0.541 0.535 0.553 0.545 0.539 NaN 0.186 0.552 0.563 0.563 0.555 0.566 NaN 30 Table 12 Benchmark results of DiT-XL-2 and DiT-XL-2-µP without classifier-free guidance. Both models are trained on the ImageNet 256256 dataset. Results with * are reported in the original paper [48], and others are reproduced by us. DiT-XL-2-µP, using base learning rate transferred from small proxy models, consistently outperforms the original baseline throughout the training process. Iteration Method FID-50K sFID-50K Inception Score Precision Recall 0.8M 0.1M 0.4M DiT-XL-2 [48] DiT-XL-2-µP DiT-XL-2 [48] DiT-XL-2-µP DiT-XL-2 [48] DiT-XL-2-µP DiT-XL-2 [48] DiT-XL-2-µP DiT-XL-2 [48] DiT-XL-2-µP DiT-XL-2 [48] DiT-XL-2-µP 2.4M DiT-XL-2 [48] 1.2M 1.6M 2M 7M DiT-XL-2 [48] 2.4M DiT-XL-2-µP 26.61 28.54 65.02 67.74 85. 89.46 95.13 99.89 100.88 107.58 105.86 111.23 108.23 - 112. 0.49 0.54 0.63 0.66 0.66 0.68 0.66 0.68 0.67 0.69 0. 0.69 0.67 - 0.68 0.595 0.575 0.635 0.606 0.638 0. 0.644 0.638 0.651 0.649 0.657 0.651 0.665 - 0.653 48. 44.15 20.38 18.63 14.73 12.91 12.78 11.25 11.79 10.16 10.97 9.72 10. 9.62* 9.44 7.33 6.72 6.37 5.49 6.37 5.48 6. 5.56 6.49 5.59 6.45 5.62 6.59 - 5.66 E.3 Additional Details of PixArt-α Experiments E.3.1 FLOPs ratio of HP Search on Proxy Models Based on Equation (1) in the main text, we can derive that ratio ="
        },
        {
            "title": "RSproxyEproxy\nStargetEtarget",
            "content": "= 5 0.04B 5 0.61B 30 5.5%. Therefore, the HP tuning requires only 5.5% FLOPs of single training run for PixArt-α. E.3.2 Additional Results of Base Learning Rate Search We present the detailed GenEval results [20] of different base learning rates in Table 13. Overall, the base learning rate 210 achieves the best GenEval performance. Table 13 GenEval results of base learning rate search on PixArt-α-µP proxy models. 0.04B models with different learning rates are trained for 5 epochs on the SAM dataset. Overall, the base learning rate 210 achieves the best GenEval performance. log2(lr) Overall Single Two Counting Colors Position Color Attribution -9 -10 -11 -12 -13 0.083 0.078 0.030 0.051 30.63 29.38 12.81 20. 4.04 5.05 0 3.54 2.5 1.25 0.31 0.31 12.23 9.57 4.79 5.59 0.5 1.5 0.25 0. 0 0.25 0 0 E.3.3 Additional Results of PixArt-α Pretraining We present the complete comparison between PixArt-α and PixArt-α-µP throughout training in Table 14. The results demonstrate that PixArt-α-µP consistently outperforms PixArt-α across all evaluation metrics during training. Table 14 Benchmark comparison between PixArt-α-µP and PixArt-α. Both models are trained on the SAM dataset for 30 epochs. PixArt-α-µP with transferred base learning rate from the optimal 0.04B proxy model consistently outperforms the original baseline throughout the training process. Epoch Method GenEval MJHQ MS-COCO FID-30K CLIP Score FID-30K CLIP Score 10 16 20 26 30 PixArt-α [8] PixArt-α-µP (Ours) PixArt-α [8] PixArt-α-µP (Ours) PixArt-α [8] PixArt-α-µP (Ours) PixArt-α [8] PixArt-α-µP PixArt-α [8] PixArt-α-µP (Ours) PixArt-α [8] PixArt-α-µP (Ours) 0.14 0.17 0.19 0.20 0.22 0.23 0.20 0.23 0.18 0.28 0. 0.26 43.19 34.24 38.36 33.35 36.19 32.28 35.68 33.42 38. 32.34 42.71 29.96 32 25.16 25.99 25.78 26.25 26. 26.67 26.54 26.83 26.44 27.17 26.25 27.13 42.25 32.62 34. 29.68 33.71 28.03 30.13 29.05 34.98 27.59 37.61 25.84 27. 28.16 28.12 28.87 28.35 29.45 28.81 29.53 29.17 29.83 28.91 29. E.4 Additional Details of MMDiT Experiments E.4.1 HPs Tuned by Human Experts Algorithmic experts take roughly 5 times the cost of full pretraining to tune HPs based on their experience. The best HPs are learning rate of 2E-4, gradient clip of 0.1, REPA loss weight of 0.5 and warm-up iteration of 1K. E.4.2 Additional Details of Base HP Search Based on the optimal base HPs searched from 80 trials trained for 30K steps, we further evaluate the HP transferability across iterations. We conducted an additional 5 proxy training runs (100K steps each) using the searched optimal HPs and five different base learning rates, as detailed in Table 15. Table 15 Iteration of 30K is enough for proxy task. The base learning rate of 2.5E-4 is optimal when proxy models are trained for 100K steps, consistent with the results observed at 30K steps. Base learning rate 1E-4 2E-4 2.5E-4 4E-4 8ETraining loss 0.185028 0.184505 0.184423 0.184436 0. E.4.3 FLOPs ratio of HP Search on Proxy Models Based on Equation (1) in the main text, we can derive the ratio of tuning cost to single pretraining cost as ratio = RSproxyEproxy StargetEtarget = 80 0.18B 30K + 5 0.18B 100K 18B 200K = 14.5%. Besides, as algorithmic experts take roughly 5 times the cost of full pretraining, the ratio of µP tuning cost to standard human tuning cost is approximately 3%. E.4.4 Additional Results of Training Loss Comparison We present the complete comparison between the training loss of MMDiT-18B and MMDiT-µP-18B in Figure 8. MMDiT-µP-18B achieves consistently lower training loss than the baseline after 15K steps, and the advantage is gradually expanding. Figure 8 Comparision between training loss. We present the training loss of MMDiT-µP-18B minus that of MMDiT-18B. The training loss gap less than 0 means that MMDiT-µP-18B is better. MMDiT-µP-18B achieves consistently lower training loss than the baseline after 15K steps, and the advantage is gradually expanding. E.4.5 Additional Details of Human Evaluation In this section, we provide the details about the user study in Section 5.2. To evaluate the text-image alignment, we created an internal benchmark comprising 150 prompts, where each prompt includes an average of five binary alignment checks (yes or no), covering wide range of concepts such as nouns, verbs, adjectives (e.g., size, color, style), and relational terms (e.g., spatial relationships, size comparisons). Ten human experts conducted the evaluation, with each prompt generating three images, resulting in total of 22,500 alignment tests. The final score is computed as the average correctness across all test points. We present two examples in Figure 9. Figure 9 Examples of text-image alignment test evaluated by human. Ten human experts conducted the evaluation, with each prompt containing an average of five test points and generating three images, resulting in 22,500 alignment test points. The final score is computed as the average correctness across all test points. E.4.6 Additional Results of Visualization Comparison In this section, we provide visual comparisons between MMDiT-µP-18B and MMDiT-18B baseline in Figure 10. Figure 10 Visual comparison between MMDiT-µP-18B and MMDiT-18B baseline. The configurations for generating images (e.g., random seed, cfg value) of two models are the same. MMDiT-µP-18B shows better text-image alignment than the baseline model."
        }
    ],
    "affiliations": [
        "Beijing Key Laboratory of Research on Large Models and Intelligent Governance",
        "ByteDance Seed",
        "Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE",
        "Gaoling School of AI, Renmin University of China",
        "RIKEN AIP",
        "Tsinghua University"
    ]
}