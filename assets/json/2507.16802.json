{
    "paper_title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning",
    "authors": [
        "Yanjun Zheng",
        "Xiyang Du",
        "Longfei Liao",
        "Xiaoke Zhao",
        "Zhaowen Zhou",
        "Jingze Song",
        "Bo Zhang",
        "Jiawei Liu",
        "Xiang Qi",
        "Zhe Li",
        "Zhiqiang Zhang",
        "Wei Wang",
        "Peng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 3 2 0 8 6 1 . 7 0 5 2 : r Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning Yanjun Zheng, Xiyang Du, Longfei Liao Xiaoke Zhao, Zhaowen Zhou, Jingze Song, Bo Zhang, Jiawei Liu Xiang Qi, Zhe Li, Zhiqiang Zhang, Wei Wang, Peng Zhang"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates high-quality, systematic financial task label system with comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multiagent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova. *Equal contribution. Corresponding Author. {zhengyanjun.zyj, duxiyang.dxy, liaolongfei.llf}@antgroup.com 1. Introduction Figure 1: Agentar-Fin-R1-32B performance on financial benchmarks. Results on general reasoning benchmark (MATH-500, GPQA-diamond) are detailed in following sections. Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, with recent advances in reasoning-optimized models such as OpenAIs o1 series [18], QwQ [20], DeepSeek-R1 [11],Seed-Thinking-v1.5[22] and Qwen3 [29] achieving significant breakthroughs in mathematics, programming, and logical inference. However, the direct deployment of these general-purpose models in financial applications reveals critical limitations: insufficient domain-specific financial knowledge integration, leading to poor performance on finance-related tasks; susceptibility to hallucinations that violate the stringent safety and compliance requirements essential in financial environments. Existing financial LLMs can be broadly categorized into two types. Non-reasoning financial models such as Baichuan [31], DISC-FinLLM[4], XuanYuan [7], and PIXIU [26] incorporate domain-specific financial knowledge but lack sophisticated analytical and reasoning capabilities required for complex financial decision-making scenarios involving multi-step analysis, risk assessment, and strategic planning. Reasoning-enhanced financial models including XuanYuanFinX1-Preview [8], Fino1 [19], Fin-R1 [16], and Dianjin-R1 [33] attempt to integrate advanced reasoning mechanisms but still exhibit significant limitations: insufficient reasoning capabilities for handling complex financial scenarios that require deep analytical thinking; lack of scenariospecific reasoning adaptation, failing to align reasoning processes with the unique demands of financial contexts such as market dynamics, regulatory compliance constraints, and risk tolerance considerations. We refer to some current consensus in academia and industry(Liu et al. [16],Wang et al. [25],Dong et al. [5],Fatouros et al. [9],Li et al. [14],Tong et al. [24],Xie et al. [27],Zhang et al. [31]), and here identify three fundamental requirements for effective financial AI systems that distinguish them from general-purpose applications: 1. Adaptive Knowledge Integration: Efficient acquisition and assimilation of evolving domain knowledge, including regulatory updates and emerging financial instruments. 2. Verifiable Reasoning: Transparent, auditable reasoning processes essential for stakeholder confidence in high-stakes decisions. 3. Compliance Adherence: Robust protection of sensitive data while meeting stringent 2 regulatory standards. Based on the aforementioned limitations, we introduce Agentar-Fin-R1, family of reasoningoptimized financial LLMs that systematically addresses key challenges in the financial domain through three core innovations: Professional Label-Guided Framework: We construct fine-grained financial task label system that decomposes the financial domain into precisely defined categories, serving as an active guidance framework throughout the entire development pipeline. This label system not only guides data processing and training workflows but also enables systematic task-oriented optimization, ensuring comprehensive coverage of financial reasoning scenarios and providing professional support for model training. Multi-Dimensional Trustworthiness Assurance: Our framework ensures trustworthiness through three levels: (i) source trustworthiness via rigorous knowledge engineering of authenticated financial data; (ii) synthesis trustworthiness through verifiable multi-agent collaborative frameworks that guarantee data quality; and (iii) governance trustworthiness via comprehensive data processing including deduplication, toxicity removal, and preference-based filtering. Efficient Training Optimization: We achieve scalable and efficient development through multiple dimensions: (i) data efficiency via weighted training frameworks that deeply exploit data potential, enhanced by label-guided synthesis and intelligent selection to improve data utilization; (ii) training efficiency through two-stage training strategy that further enhances model capabilities; and (iii) attribution efficiency via comprehensive attribution system that enables rapid bottleneck identification and targeted improvements, providing scientific guidance for continuous model evolution. To assess real-world deployment capabilities, we introduce Finova (Financial Nova: Operational, Verifiable, Agent), comprehensive benchmark encompassing three critical dimensions: Agent Capabilities: Autonomous task execution including intent detection, slot recognition, tool planning, and expression generation. Complex Reasoning: Multi-step analytical tasks combining financial mathematics, code understanding, and domain-specific inference. Safety and Compliance: Security risk mitigation and regulatory adherence assessment. Our primary contributions are: label-guided methodology for developing trustworthy and efficient financial LLMs that systematically addresses data fragmentation, reasoning transparency, and scenario generalization challenges. Agentar-Fin-R1 model series (8B and 32B parameters) achieving state-of-the-art performance on financial benchmarks while maintaining general reasoning capabilities, demonstrating the effectiveness of our training methodology. Finova evaluation benchmark providing standardized assessment of financial LLM capabilities across critical application dimensions, enabling systematic comparison and development guidance for the research community. Our experimental results demonstrate that Agentar-Fin-R1 achieves superior performance across financial benchmarks (Fineva, FinEval, FinanceIQ and Finova) while maintaining competitive results on general reasoning tasks (MATH-500, GPQA-diamond), validating the effectiveness of domain-specialized optimization without catastrophic forgetting. 3 Figure 2: Overview of the Agentar-Fin-R1 development pipeline. 2. Data 2.1. Overview In developing financial large language models (LLMs), data quality and integrity serve as the cornerstone of model performance. While data volume remains important, ensuring trustworthiness and real-world representativeness for financial applications is absolutely critical. Our methodology is built upon sophisticated Label System that systematically structures the data synthesis process, guaranteeing that each data point is verifiable, task-specific, and precisely aligned with domain requirements. This comprehensive system facilitates transparent and efficient data generation, enabling the creation of high-quality, task-relevant datasets that maintain rigorous logical consistency and establish solid foundation for training robust financial models. 2.2. Label System The Label System serves as the cornerstone of our data construction pipeline, meticulously designed to capture the inherent complexity and heterogeneity of financial tasks. It systematically categorizes tasks along two fundamental dimensions: Scene Dimension: Encompasses diverse real-world financial scenarios, such as Banking, Securities, Insurance, Trusts, and Mutual Funds. Each scenario embodies distinct application context with specialized requirements, enabling the model to adapt to the operational nuances and domain-specific characteristics of each financial sector. Task Type Dimension: Defines the specific types of tasks to be performed, such as Named Entity Recognition (NER), Intent Classification, Slot Filling, Entity Disambiguation, and Consultation-style Question Answering. These task types specify the exact operations the model should execute on input data, providing clear directives for what actions to take 4 during instruction learning and response generation phases. We formally define the label system as set of composite labels, where each label 𝑙𝑖 is represented as tuple comprising content category and task attribute: 𝑙𝑖 = (𝑐𝑖, 𝑎𝑖) (1) Here, 𝑐𝑖 represents scene category (e.g., Banking, Insurance), and 𝑎𝑖 represents task attribute (e.g., Entity Disambiguation, Slot Filling). This structured formulation facilitates fine-grained task decomposition and enables targeted data generation that is precisely aligned with downstream application requirements. It is crucial to recognize that the Scene and Task Attribute dimensions exhibit non-orthogonal characteristics: not all task attributes are equally applicable across every scene. This phenomenon results in sparsely populated cross-product space, which more authentically reflects the natural distribution and practical constraints of real-world financial tasks. 2.3. Data Construction Our data construction methodology is meticulously designed to ensure the quality, diversity, and fidelity of synthesized data. It integrates rigorous knowledge engineering, sophisticated multi-agent synthesis mechanisms, and rigorous multi-stage verification processes. The resulting dataset comprehensively captures the multifaceted nature of financial tasks and is optimally suited for training large-scale financial language models capable of robust generalization, domain-specific adaptation, and high-stakes reasoning under complex financial scenarios. 2.3.1. Source: Trusted Sources and Knowledge Engineering We construct reliable data by sourcing from authoritative financial institutions and regulatory bodies, while applying sophisticated knowledge engineering techniques to ensure data integrity and domain relevance. The sourced data undergoes comprehensive knowledge engineering to guarantee authenticity and domain alignment through systematic multi-stage preprocessing pipeline: 1. Data Extraction: Processing raw financial data using state-of-the-art NLP techniques, including Named Entity Recognition (NER), dependency parsing, and Part-of-Speech (POS) tagging, to systematically extract meaningful financial entities, relationships, and semantic structures. 2. Data Normalization: Standardizing heterogeneous data formats and reconstructing data structures to achieve uniform financial data representation, thereby enhancing downstream semantic understanding and cross-domain compatibility. 3. Data Detoxification: Systematically removing non-compliant, contaminated, and potentially harmful content from the dataset to ensure data quality, regulatory compliance, and ethical standards adherence. 4. Knowledge Refinement: Applying advanced processing and quality enhancement techniques to generate high-fidelity refined knowledge repository that meets stringent financial domain requirements. The final refined knowledge repository 𝐾 comprises high-quality structured financial knowledge units that have undergone rigorous validation: 5 𝐾 = 𝑘1, 𝑘2, . . . , 𝑘𝑛 where each knowledge unit 𝑘𝑖 has undergone comprehensive verification and refinement processes to ensure accuracy, relevance, and domain-specific validity. 2.3.2. Synthesis: Trusted Multi-Agent Generation of Reasoning Triplets To construct trustworthy, diverse, and reasoning-enhanced instruction dataset tailored for financial tasks, we design dual-track data synthesis pipeline that combines domain-grounded generation with self-evolving instruction refinement. The final dataset comprises high-quality (query, thinking, answer) triplets that are semantically aligned with the financial domain and optimized for verifiable reasoning. Figure 3: Overview of the dual-track data synthesis pipeline, incorporating both task-oriented knowledge-guided generation and self-evolution mechanisms for generating verifiable reasoning triplets. Track I: Task-Oriented Knowledge-Guided Generation This track leverages curated financial knowledge base and domain-specific task label system to drive the generation of high-quality, verifiable (query, thinking, answer) triplets. Task Label Matching Mechanism We define structured label system of financial tasks as = {𝑡1, 𝑡2, . . . , 𝑡𝑚}, where each label 𝑡 𝑗 represents distinct task category (e.g., fraud detection, portfolio analysis, regulatory compliance). For each task label 𝑡 𝑗, dedicated generation agent 𝐴𝑡 𝑗 is instantiated. Knowledge-Guided Generation Process Given task label 𝑡 𝑗 and domain-specific knowledge snippet 𝑘𝑖 K𝑡 𝑗 K, the agent 𝐴𝑡 𝑗 generates reasoning triplet: 6 (𝑞𝑖, 𝑡𝑖, 𝑎𝑖) = 𝐴𝑡 𝑗 (𝑘𝑖, 𝑡 𝑗; 𝜃𝐴𝑡 𝑗 ), (2) where 𝑞𝑖 is the query, 𝑡𝑖 is the intermediate reasoning process (\"thinking\"), and 𝑎𝑖 is the final answer. The generation process respects financial logic and compliance constraints through controlled decoding and templated prompting. The resulting dataset is: task = {(𝑞𝑖, 𝑡𝑖, 𝑎𝑖)}𝑁 𝐷 task 𝑖=1 . (3) Track II: Self-Evolution of Instructions with Reasoning Supervision To promote diversity and complexity, this track evolves existing prompts into more sophisticated reasoning tasks using feedback-driven self-evolution agent. Seed Initialization and Evolution Mechanism Starting from an initial set 𝐼0, which intask, the self-evolution agent 𝐴evo cludes either manually curated queries or samples from 𝐷 generates enhanced instructions by incorporating feedback signals: 𝐼𝑘+1 = 𝐴evo(𝐼𝑘, R𝑘; 𝜃evo), (4) where R𝑘 includes diversity metrics, task novelty scores, and answerability filters. The process continues until convergence criterion (e.g., saturation in novelty or quality) or maximum iteration count 𝐾max is reached. Evolution Strategies The instruction refinement process is guided by three core strategies: Progressive Reasoning Complexity: Injecting step-by-step chains of thought to increase cognitive depth and analytical rigor. Structural Diversity: Applying prompt mutations and recombinations to expand coverage across financial domains and reasoning types. Fitness-Based Filtering: Retaining only samples that demonstrate factual soundness, logical coherence, and linguistic fluency. Each refined instruction is used to produce reasoning triplet using the base model: evolution = {(𝑞 𝑗, 𝑡 𝑗, 𝑎 𝑗)}𝑁evo 𝐷 𝑗=1 . (5) Final Trusted Reasoning Dataset The final corpus is constructed as the union of task-guided and self-evolved reasoning triplets: synthesis = 𝐷 𝐷 task 𝐷 evolution. (6) This dual-track framework ensures that the generated data is not only domain-specific and diverse but also trustworthy and verifiable, facilitating the training of financial LLMs with robust reasoning and compliance capabilities. 7 2.3.3. Verification and Checking: Rigorous Multi-Modal Validation We implement comprehensive multi-tier validation framework to ensure data quality, accuracy, and reliability across all generated instances, establishing robust foundation for trustworthy financial AI systems. Multi-Model Ensemble Verification Consistency Validation We deploy multiple independent models {𝑀1, 𝑀2, . . . , 𝑀𝑝} to generate responses for identical queries, assessing data accuracy through comprehensive answer consistency analysis: consistency(𝑞𝑖) = 1 𝑝( 𝑝 1) 𝑝 𝑗=1 𝑘 𝑗 sim(𝑀 𝑗 (𝑞𝑖), 𝑀𝑘 (𝑞𝑖)) (7) where sim(, ) represents semantic similarity function that incorporates both lexical overlap and contextual embedding similarity measures to capture nuanced agreement patterns. Reasoning Validation An independent third-party model validates the logical correctness of answers through prompt-based approach, analyzing queries and reasoning processes to determine answer validity: reasoning_valid(𝑞𝑖, 𝑎𝑖) = 𝑀 verify(query(𝑞𝑖), thinking(𝑞𝑖)) 𝑎𝑖 (8) Human Annotation and Quality Control We perform stratified random sampling of the generated data to ensure representative coverage of task types, complexity levels, and domain subcategories. The sampling ratio is carefully calibrated to balance annotation cost with statistical significance and coverage requirements. Experienced financial domain experts will conduct comprehensive multidimensional assessment of sampled data instances. Rating Model Training and Application Training Data Construction We construct comprehensive training dataset for the rating model by strategically combining multi-model ensemble verification results with expert human annotation data: 𝐷rating = 𝐷 ensemble 𝐷 human (9) This hybrid approach leverages both automated consistency checks and expert human judgment to create robust and reliable training signals for quality assessment. Rating Model Architecture We train specialized rating model 𝑅𝑀 to perform comprehensive final quality assessment: score(𝑑𝑖) = 𝑅𝑀 (𝑑𝑖; 𝜃RM) (10) Data Governance and Cleansing Our data governance framework implements rigorous cleansing procedures to ensure dataset integrity: 1. Deduplication: Employing advanced semantic hashing and similarity computation techniques to identify and remove duplicate instances while preserving meaningful variations and edge cases. 2. Detoxification: Systematically identifying and filtering potentially harmful, biased, or inappropriate content that could produce negative downstream effects or ethical concerns. 3. Decontamination: Identifying and removing training data instances that overlap with evaluation benchmarks to prevent data leakage and ensure fair, unbiased model assessment. Final Dataset Definition The final dataset, having undergone complete verification and cleansing procedures, is formally defined as: final = {𝑑𝑖 𝐷 𝐷 synthesis verify(𝑑𝑖) clean(𝑑𝑖) score(𝑑𝑖) > 𝜏} (11) where verify(𝑑𝑖) indicates successful multi-modal verification, clean(𝑑𝑖) represents successful data cleansing, and 𝜏 is the quality threshold determined through rigorous empirical validation and domain expert consensus. This comprehensive data construction pipeline ensures that the final training dataset maintains exceptional quality, diversity, and task relevance, providing robust and trustworthy foundation for training large language models in the financial domain. 3. Training 3.1. Weighted Training Framework Training financial large language models (LLMs) involves addressing the inherent heterogeneity and complexity of financial tasks, which exhibit varying levels of difficulty and domain-specific requirements. Traditional training methods treat all training samples uniformly, without accounting for the fact that some tasks are significantly more challenging than others. Consequently, models may overfit to simpler, more frequently encountered tasks while underperforming on complex tasks that are crucial for real-world financial decision-making and risk assessment. To address this fundamental limitation, we propose weighted training framework that dynamically adjusts the importance of each task based on the empirically measured difficulty of the corresponding instances. This framework employs sophisticated domain-specific tagging system that categorizes tasks by their semantic labels and complexity characteristics. Prior to training, for each task label, representative subset of 𝑛 samples is selected through stratified sampling, and the current model generates 𝑘 diverse responses for each sample. The pass@k score [3, 30] is then computed for these responses, which quantitatively reflects the models ability to produce correct answers within the top 𝑘 generated responses. Additionally, 𝑚 reference models from different architectural families and training paradigms are employed to generate their own response sets, and their respective pass@k values are computed for comparative analysis. The pass@k values from both the current model and the 𝑚 reference models are systematically used to assess task difficulty and relative model performance. Tasks with lower pass@k scores for the current model are identified as more challenging and receive proportionally higher training weights. Furthermore, when there exists significant performance gap between the current model and reference models, the weight for that specific task is increased to reflect the models relative weakness and prioritize improvement in that domain. The computed difficulty weights are then assigned to the corresponding task labels, and during the training process, tasks with higher weights receive enhanced attention through modified loss functions. This approach ensures that the model focuses more computational resources on tasks it struggles with, thereby improving performance on complex financial tasks while maintaining learning efficiency for simpler tasks. Difficulty-Aware Weight Estimation The difficulty-aware weight for each task label is computed based on comprehensive analysis of pass@k values from both the current model and reference models. Let = {(𝑥𝑖, 𝑦𝑖, 𝑡𝑖)} represent the tagged dataset, where 𝑥𝑖 is the input data, 𝑦𝑖 is the target output, and 𝑡𝑖 is the task label. For each task label 𝑡, representative subset of 𝑛 samples is selected through stratified sampling to ensure comprehensive coverage across different subtask variations and complexity levels. To ensure stable training dynamics and prevent oscillatory behavior caused by abrupt weight shifts between training epochs, we employ sophisticated exponential smoothing mechanism for task difficulty weights. Specifically, for each task label 𝑡, the final weight is updated according to: 𝑤(final) 𝑡 = 𝜌 𝑤(prev) 𝑡 + (1 𝜌) 𝑤(raw) 𝑡 (14) where 𝜌 [0, 1] is smoothing coefficient that controls the inertia of the update process, is the newly denotes the previous smoothed weight from the preceding epoch, and 𝑤(raw) 𝑡 𝑤(prev) 𝑡 estimated raw difficulty weight. By progressively integrating new difficulty estimates while maintaining historical context, this mechanism effectively mitigates training instability and sharp fluctuations in learning dynamics. To ensure that no task category is completely neglected during training, we apply lowerbound clipping mechanism such that each final weight satisfies: 𝑤(final) 𝑡 𝛾 (15) where 𝛾 > 0 is carefully tuned base weight that preserves minimal attention on all task types, including relatively straightforward ones. The weights are subsequently normalized across all tasks to maintain consistent global training scale and prevent loss magnitude drift. The difficulty-aware weight 𝑤𝑡 for each task label 𝑡 is computed as principled combination of three key components: (1) the inverse of the current models pass@k score to prioritize 10 Algorithm 1: Difficulty-Aware Weight Estimation for Task Labels Input: = {(𝑥𝑖, 𝑦𝑖, 𝑡𝑖)}: tagged dataset, where 𝑥𝑖 is the input, 𝑦𝑖 the target, and 𝑡𝑖 the task label; {𝑀 𝑗}𝑚 𝑗=1: reference models from diverse architectural families; 𝛼, 𝛽, 𝛾: weighting hyperparameters for difficulty components; 𝜌: exponential smoothing coefficient; 𝑛, 𝑘: sampling and generation hyperparameters Output: Final normalized difficulty weights 𝑤𝑡 for each task label foreach task label 𝑡 do Draw 𝑛 instances {(𝑥ℓ, 𝑦ℓ)}𝑛 Generate 𝑘 diverse responses with current model and compute pass@kcurrent for 𝑗 1 to 𝑚 do ℓ=1 for task 𝑡 via stratified sampling; (𝑡); Generate 𝑘 responses with reference model 𝑀 𝑗 and compute pass@k 𝑗 (𝑡); Compute average reference performance: pass@kref (𝑡) = Compute raw difficulty weight: 1 𝑚 𝑚 𝑗=1 pass@k 𝑗 (𝑡); 𝑤(raw) 𝑡 = 𝛼(cid:0)1 pass@kcurrent (𝑡)(cid:1) + 𝛽 max(cid:0)0, pass@kref (𝑡) pass@kcurrent (𝑡)(cid:1) + 𝛾 (12) if task 𝑡 encountered in previous epochs then Apply exponential smoothing: 𝑤(final) 𝑡 = 𝜌 𝑤(prev) 𝑡 + (1 𝜌) 𝑤(raw) 𝑡 ; else Initialize: 𝑤(final) 𝑡 = 𝑤(raw) 𝑡 ; Store 𝑤(final) 𝑡 as 𝑤(prev) 𝑡 for subsequent epochs; Apply normalization for stable scaling: 𝑤𝑡 = return { 𝑤𝑡}𝑡 𝑡 𝑤(final) (cid:205)𝑡 𝑤(final) 𝑡 (13) challenging tasks, (2) an additional penalty term when the current model significantly underperforms compared to reference models, and (3) base weight to ensure comprehensive task coverage. The exponential smoothing mechanism prevents dramatic weight oscillations between training iterations, promoting stable and consistent convergence behavior. Enhanced Loss Functions with Weighted Training Once the task difficulty weights 𝑤𝑡 have been computed and properly normalized, they are systematically incorporated into the training process through modified loss functions. For Supervised Fine-Tuning (SFT), the standard crossentropy loss function is enhanced by weighting the log-likelihood loss for each training sample according to its task difficulty: LSFT = 1 𝑁 𝑁 𝑖=1 𝑤𝑡𝑖 log 𝑃𝜃( 𝑦𝑖𝑥𝑖) 11 (16) where 𝑁 is the total number of training samples, and 𝑤𝑡𝑖 is the normalized difficulty weight for the task label associated with sample 𝑖. This formulation ensures that the overall loss magnitude remains comparable to standard training procedures while systematically emphasizing difficult tasks. For Reinforcement Learning (RL) training phases, we modify the preference-based objective function to incorporate difficulty weights in theoretically principled manner. The computational overhead for difficulty estimation is 𝑂(𝑚 𝑛 𝑘) per task label, where 𝑚 is the number of reference models, 𝑛 is the number of sampled instances per task, and 𝑘 is the number of responses generated per instance. This estimation procedure is performed periodically (e.g., once per epoch or at specified intervals) rather than at every training step, ensuring that it does not significantly impact overall training efficiency or computational scalability. This enhanced weighted training framework, incorporating empirically-driven task difficulty assessment through pass@k scores with theoretical stability guarantees and practical implementation considerations, is seamlessly integrated into the training pipeline to improve model generalization and performance on complex financial tasks while maintaining robust and stable learning dynamics throughout the training process. 3.2. Two-Stage Training Pipeline We propose two-stage training strategy to systematically optimize financial large language models (LLMs) for domain-specific applications. This approach addresses the challenge of balancing comprehensive financial knowledge acquisition with performance optimization on challenging tasks. Our strategy consists of two sequential stages: 1. Stage 1: Financial Knowledge and Capability Injection Comprehensive domain knowledge and capability acquisition through supervised fine-tuning on diverse financial tasks. 2. Stage 2: Challenge Task Enhancement Performance optimization on challenging tasks using GRPO and targeted fine-tuning. Stage 1: Financial Knowledge and Capability Injection The first stage employs supervised fine-tuning (SFT) leveraging high-quality financial reasoning data synthesized through our approach described in Section 2, augmented with extensive general reasoning datasets. We implement the weighted training framework from the previous section, which strategically prioritizes challenging samples to accelerate convergence on complex problems. This stage substantially enhances the models comprehensive capabilities across the financial domain, establishing robust foundation that integrates both specialized domain knowledge and general reasoning proficiency. Stage 2: Challenge Task Enhancement The second stage is specifically designed to further strengthen the models performance when confronting difficult and challenging problems. We employ sophisticated hybrid approach combining: GRPO: Optimizes decision-making capabilities in complex financial scenarios with multiobjective considerations and intricate reward structures 12 Targeted SFT: Systematically addresses specific performance gaps and weaknesses identified through comprehensive Stage 1 evaluation Tasks demanding sophisticated reasoning capabilities (e.g., multi-step financial forecasting, comprehensive risk assessment, dynamic portfolio optimization) are prioritized in this stage. When GRPO encounters convergence challenges on specific task categories, we strategically apply targeted SFT using carefully curated high-quality examples to ensure robust and consistent performance across all challenging scenarios. Training Efficiency and Scalability This two-stage approach delivers significant practical advantages for real-world deployment: Efficient initialization: Stage 1 provides strong foundational model, dramatically reducing fine-tuning requirements for domain adaptation Flexible modular optimization: Stage 2 can be selectively applied to specific task categories based on business priorities and requirements Cost-effective scalability: The pipeline enables efficient adaptation to emerging financial domains and use cases without requiring complete model retraining 3.3. Attribution Loop The Attribution Loop is post-training mechanism that refines the model by tracing errors to specific financial scenarios and tasks, enabling targeted data sampling and model enhancement through dynamic resource allocation. Pass@1 Attribution Framework The Attribution Loop employs the aforementioned twodimensional labeling framework to categorize prediction errors. For given label 𝑡, the pass@1 accuracy is defined as: Pass@1(𝑡) = (cid:205)𝑖:ℓ𝑖=𝑡 I[ ˆ𝑦𝑖 = 𝑦𝑖] {𝑖 : ℓ𝑖 = 𝑡} (17) Dynamic Attribution Loop The Dynamic Attribution Loop optimizes data allocation and model training by adaptively prioritizing tasks based on their performance gaps, learning efficiency, and available resources. The goal is to minimize training cost while ensuring target performance across all tasks, while also addressing the diminishing returns of overfitting. Task Prioritization: Compute performance gap for each task: Δ𝑡 = max(0, 𝑃target 𝑝𝑡) (18) Estimate learning efficiency based on the ratio of performance improvement to data added in the current iteration: 𝑒𝑡 = (cid:16) 0, Δ 𝑝(𝑘) 𝑡 (cid:17) max Δ𝑑 (𝑘) 𝑡 + 𝜀 13 (19) where Δ 𝑝(𝑘) Δ𝑑 (𝑘) 𝑡 𝑡 is the performance improvement of task 𝑡 in the current iteration, and is the amount of data allocated to task 𝑡 in the same iteration. The priority score 𝜋𝑡 for each task is computed as: 𝜋𝑡 = Δ𝑡 𝑒𝑡 exp(𝜆𝑑𝑡) (20) This ensures that tasks with larger performance gaps and higher learning efficiency are prioritized, with decay based on the amount of allocated data 𝑑𝑡. Data Allocation: Compute iteration budget: 𝐵𝑘 = 𝐵0 𝛽𝑘1 Allocate data based on priority scores: Δ𝑑𝑡 = 𝜋𝑡 (cid:205)𝑡 𝜋𝑡 𝐵𝑘 Update total data allocation for each task: 𝑑𝑡 𝑑𝑡 + Δ𝑑𝑡 Data Reversion: (21) (22) (23) When performance regression occurs, revert to the previous version of the data for the affected task: 𝑡 𝑑 (𝑘1) 𝑑 (𝑘) 𝑡 (24) If performance continues to degrade for multiple iterations, trigger synthetic data generation by making substantial modifications to the original synthetic data to improve task performance. Data Synthesis Feedback: Provide feedback to the data synthesis pipeline to generate additional synthetic data for underperforming tasks, ensuring that generated data reflects the distribution and complexity of the task. Model Training and Evaluation: Train the model with allocated data and evaluate task performance. Continue iterations until: * All tasks meet or exceed target performance 𝑃target, or * Total data allocation exceeds 𝐵max, with performance saturation criteria. * Marginal performance improvements fall below 𝜀 eff, reducing further resource allocation. * Convergence is assessed across the entire system, considering inter-task performance interactions and global improvements. Ptarget Selection Set the target performance 𝑃target to the current state-of-the-art (SOTA) level for the specific task plus fixed increment, such as 5 or 10, to ensure the model achieves high performance level on this particular subtask. 14 4. Experiments 4.1. Benchmark In this section, we describe the datasets used to evaluate the performance of our proposed financial large language model (LLM), with focus on real-world applicability and domainspecific capabilities. We introduce novel dataset, Finova, which is designed to assess the true deployment capabilities of financial LLMs. Additionally, we evaluate our model on several established financial benchmarks and general reasoning tasks to ensure that it maintains strong performance across wide range of tasks. Figure 4: comprehensive overview diagram of the Finova benchmark, consisting of three components: Agent Capabilities, Complex Reasoning, and Safety and Compliance. The primary dataset used for evaluating our model is Finova, comprehensive financial benchmark specifically designed to assess the real-world deployment capabilities of financial LLMs. The dataset is structured around three critical domains to ensure that the model meets the diverse needs of financial applications: Agent Capabilities, Complex Reasoning, and Safety and Compliance. These categories collectively reflect the essential skills needed for effective deployment in real-world financial scenarios. Agent Capabilities: This section evaluates tasks essential for intelligent agents in financial settings. It focuses on key stages of financial agent interaction, abstracted into four core competency dimensions for the industry. The tasks are designed from actual business needs but are standardized to evaluate the general capabilities required by any financial agent, regardless of specific business logic. The following tasks are included: Financial Intent Detection: Evaluates the agents ability to understand user intentions in financial scenarios, such as investment consulting, product inquiries, risk assessment, and portfolio management. This task serves as critical component in financial agent systems, enabling accurate identification of user needs and subsequent routing decisions to ensure that user requests are properly directed to appropriate processing modules. 15 Financial Slot Recognition: Evaluates the agents ability to recognize and structure financial terms, such as specific insurance products (e.g., universal life insurance) or stock market terminology (e.g., STAR Market). This task forms the foundational capability of financial text understanding, encompassing tasks like report analysis, customer service dialogues, and product recommendations. The entity types are designed to cover mainstream financial sectors (insurance, mutual funds) but are extendable to emerging fields like bonds and derivatives. Financial Tool Planning: Assesses the agents ability to interpret user needs and recommend suitable financial tools, such as portfolio analysis, market comparisons, or performance evaluations. This task reflects common financial interaction modes (querying, comparing, filtering, analyzing) and evaluates the agents ability to match the right tool to the users intent, execute it, and process the results. This represents key competency for any tool-enhanced financial agent. Financial Expression Generation: Evaluates the agents capacity to generate responses that strictly adhere to the context and authoritative data sources, while resisting information hallucination. This ability is critical for financial decision-making agents, which must generate accurate, reliable statements based on real-world financial data, ensuring that the model can be deployed in high-stakes domains such as finance, healthcare, and law. Complex Reasoning: This section evaluates tasks that demand integrated, multi-step reasoning and inference, capturing the multifaceted complexities inherent in financial decision-making. It combines elements from financial mathematics, code understanding, and sophisticated reasoning into unified framework, requiring models to handle problems such as asset valuation, portfolio optimization, and risk analysis while simultaneously interpreting, generating, or refining financial code for algorithmic trading, financial software, and automated systems. This fusion emphasizes how quantitative tools and computational methods intertwine with deep logical deductionsfor instance, analyzing intricate relationships between financial variables, forecasting outcomes based on historical data, or navigating complex scenarios that necessitate domain expertise and layered inferences to derive actionable insights. By synthesizing mathematical rigor with code-driven execution and high-level reasoning, the task mirrors realworld financial challenges where model-based calculations and algorithmic implementations are inseparable from contextual interpretation and strategic decision-making. Safety and Compliance: This section addresses Safety and Compliance, critical domain designed to comprehensively assess the models ability to navigate security risks while adhering to the financial industrys legal and ethical standards. It fuses the technical imperatives of security protection with the legal mandates of regulatory compliance. The evaluation requires the model to not only identify and mitigate security threatssuch as malicious inputs, data leakage, and system abuseto ensure system integrity and data confidentiality, but also to simultaneously demonstrate deep understanding of and adherence to diverse financial regulatory frameworks. These include anti-money laundering regulations, data privacy protection, investor protection rules, and risk disclosure standards. Through this assessment, we verify the models capacity to form robust line of defense for both safety and compliance, thereby safeguarding system stability, data security, and user rights in complex financial scenarios. The dataset includes real-world queries accumulated from actual business environments, ensuring that the model is tested on high-value, realistic scenarios that reflect the complexities and requirements of production financial systems. In addition to Finova, we also evaluate our model on several widely-used financial benchmarks to gauge its performance on more traditional tasks. These benchmarks include: 16 Category Task Number of Samples Agent Capabilities Financial Intent Detection Financial Slot Recognition Financial Tool Planning Financial Expression Subtotal Complex Reasoning Subtotal Safety and Compliance Subtotal 150 360 258 100 868 282 200 Table 1: Finova Dataset: Comprehensive Task Distribution Total 1350 Fineva[1]: financial benchmark encompassing 33 sub-dimensions across five core capability categories: financial cognition, financial knowledge, financial logic, content generation, and security & compliance. FinEval[12]: benchmark focused on evaluating financial question-answering models, covering variety of financial topics and scenarios. FinanceIQ[6]: dataset designed to assess models ability to answer financial questions based on real-world financial knowledge. While the primary focus of our model is financial applications, we also seek to evaluate whether the specialized training for financial tasks impacts its general reasoning capabilities. To ensure that our model maintains robust reasoning abilities across domains, we perform tests on two widely-used general reasoning benchmarks: MATH[13]: benchmark designed to assess models ability to solve mathematical problems that require multi-step reasoning. We used the MATH-500 subset for evaluation. GPQA[21]: general-purpose question-answering benchmark that tests models ability to comprehend and reason through diverse, non-financial tasks. We used the GPQAdiamond subset for evaluation. By evaluating on these general reasoning tasks, we ensure that our model remains wellrounded and does not overfit to financial tasks at the cost of its general problem-solving abilities. 4.2. Training detail We train Agentar-Fin-R1-8B and Agentar-Fin-R1-32B based on Qwen3-8B-Instruct and Qwen332B-Instruct respectively. The training process consists of two stages: initial SFT, and then GRPO and SFT refinement. For the 8B model, we use 16 NVIDIA A100 GPUs, while the 32B model is trained on 64 A100 GPUs. All training uses bf16 precision with sequence length of 16K and appropriate gradient accumulation steps to ensure training stability. Beyond the synthetically generated data derived from our data synthesis framework detailed in Section 2, our dataset incorporates financial reasoning data from our proprietary Agentar-DeepFinance-300K[32], general-purpose training corpora[23], as well as datasets sourced from Llama-Nemotron[2] and openthoughts[10]. 17 4.3. Baseline We conduct comprehensive comparisons across four distinct model categories: General models without explicit reasoning: GPT-4o[17](Version 2024-08-06), Qwen2.514B-Instruct[28], Qwen2.5-72B-Instruct[28], and DeepSeek-V3[15](Version 2025-03-24). General models with reasoning capabilities: GPT-o1[18](Version 2024-12-17), Qwen38B[29], Qwen3-32B[29], Qwen-QwQ-32B[20], and DeepSeek-R1[11](Version 2025-05-28). Financial-specialized models without explicit reasoning: Xuanyuan3-70B-Chat[7]. Financial-specialized models with reasoning abilities: Qwen-Fin-R1-7B[16], QwenDianjin-R1-7B[33], Qwen-Dianjin-R1-32B[33] and Xuanyuan-FinX1-Preview[8]. 4.4. Main Results Model Params Financial Financial General General Overall Fineva FinEval FinanceIQ Finova Avg. MATH GPQA Avg. Avg. Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct DeepSeek-V3 GPT-4o Qwen3-8B Qwen3-32B Qwen-QwQ-32B DeepSeek-R1 GPT-o1 14B 72B 671B - 8B 32B 32B 671B - 84.41 87.14 87.69 85.82 84.36 88.94 89.63 88.04 88.28 General Models (No Reasoning) 71.60 76.64 77.99 74.26 68.82 74.03 73.93 72.18 37.95 48.22 54.29 45. General Models (With Reasoning) 76.27 80.50 82.69 84.93 81.32 73.06 78.03 81.58 83.98 78.72 54.45 59.46 61.70 61.28 60.46 Financial Models (No Reasoning) 65.70 71.51 73.48 69. 72.04 76.73 78.90 79.56 77.20 79.40 82.60 88.40 78.80 93.80 95.40 93.60 95.20 94.80 35.35 43.43 52.53 51.01 59.60 63.13 61.62 72.39 76.77 57.38 63.02 70.47 64. 76.70 79.27 77.61 83.80 85.79 62.92 68.68 72.47 67.88 73.59 77.58 78.47 80.97 80.06 Xuanyuan3-70B-Chat 70B 79. 62.60 64.66 37.26 61.00 43.80 28. 36.04 52.68 Financial Models (With Reasoning) Qwen-Fin-R1-7B Qwen-Dianjin-R1-7B Qwen-Dianjin-R1-32B XuanYuan-FinX1-Preview Agentar-Fin-R1-8B Agentar-Fin-R1-32B 7B 7B 32B 70B 8B 32B 79.18 84.30 88.47 81.85 91.52 92. 65.80 74.90 80.41 69.03 85.09 87.70 61.6 72.89 81.49 70.02 84.24 86.79 38.37 41.89 56.02 50.74 63.56 69.93 61.24 68.50 76.60 67.91 81.10 84.20 72.80 74.60 84.40 71.80 93.40 93.80 28.28 41.92 58.59 42.42 60.10 68. 50.54 58.26 71.50 57.11 76.75 80.99 57.67 65.08 74.90 64.31 79.65 83.13 Table 2: Performance comparison in accuracy across financial benchmarks (Fineva, FinEval, FinanceIQ, Finova) and general reasoning tasks (MATH: MATH-500, GPQA: GPQA-diamond). Scores in bold indicate the best results. Results include individual benchmark performance and averaged scores for financial tasks (Financial Avg.), general reasoning (General Avg.), and overall performance (Overall Avg.). Agentar-Fin-R1-32B achieves state-of-the-art performance across all financial benchmarks, as well as competitive results on general reasoning. The comprehensive evaluation results presented in Table 2 demonstrate the superior performance of our Agentar-Fin-R1 models across both financial and general domains. Our AgentarFin-R1-32B model establishes new state-of-the-art benchmark with an average score of 83.13, substantially surpassing all competing baselines. Particularly noteworthy is the consistent dominance of our models across the entire spectrum of financial evaluation benchmarks: AgentarFin-R1-32B achieves optimal performance on Fineva (92.38), FinEval (87.70), FinanceIQ (86.79), and Finova (69.93), while the more parameter-efficient Agentar-Fin-R1-8B variant maintains highly competitive performance despite its reduced computational footprint. critical observation is that our domain-specialized models exhibit remarkable capability preservation in general-purpose tasks, with Agentar-Fin-R1-32B attaining 93.80 score on Figure 5: Performance comparison between Agentar-Fin-R1 and Qwen3 models (8B and 32B variants) on financial benchmarks (Fineva, FinEval, FinanceIQ, Finova) and general reasoning benchmarks (MATH: MATH-500, GPQA: GPQA-diamond). Our proposed models show improvements across both domain-specific and general reasoning tasks. MATH-500 and 68.18 on GPQA-diamondperformance levels that match or exceed those of general-purpose reasoning models with comparable parameter counts. Notably, we observed substantial improvements on GPQA-diamond compared to the base model. This empirical evidence validates our hypothesis that targeted domain optimization can be achieved without compromising general cognitive capabilities, and in some cases, may even strengthen them. Our comparative analysis reveals two fundamental insights regarding model architecture and specialization strategies. First, reasoning-augmented architectures consistently demonstrate superior performance over their non-reasoning counterparts across cognitively demanding tasks, as evidenced by the systematic performance gains observed when comparing the Qwen2.5 series against the Qwen3 series. Second, domain-specialized models exhibit marked advantages over general-purpose alternatives, particularly manifest in the performance differential between our Agentar-Fin-R1 models and general reasoning models such as Qwen3 and Qwen-QwQ. These findings provide compelling evidence for the efficacy of integrating domain-specific expertise with advanced reasoning mechanisms in addressing complex financial challenges. Table 3 presents comprehensive performance analysis based on the Finova evaluation framework, benchmark specifically designed to assess the practical application capabilities of financial large language models. The Agentar-Fin-R1 models exhibit clear and overwhelming advantage across the board, particularly in key areas of financial AI that are critical for real-world deployment. Agentar-Fin-R1-32B stands out with the highest overall score of 69.93, outperforming even larger-scale general-purpose models such as DeepSeek-R1 (671B parameters, 61.28) and GPT-o1 (60.46). This strong performance underscores the significant benefits of domain specialization for financial tasks, where general-purpose models fall short. In the dimension of agent capabilities, our models demonstrate remarkable superiority in all evaluated agent capabilities. Notably, in the financial expression generation task, Agentar-FinR1-32B achieves an outstanding score of 69.00, significantly surpassing all competing models. 19 Model Params Intent Slotting Tool Expression Complex Reasoning Safety & Compliance Avg. Agent Capabilities Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct DeepSeek-V3 GPT-4o Qwen3-8B Qwen3-32B Qwen-QwQ-32B DeepSeek-R1 GPT-o1 14B 72B 671B - 8B 32B 32B 671B - 19.33 56.67 54.00 42.00 58.00 60.67 60.00 60.00 54.67 General Models (No Reasoning) 70.75 71.11 63.31 68.15 20.16 25.97 46.90 25.97 20.00 25.00 31.00 21.00 General Models (With Reasoning) 70.83 72.20 77.88 73.70 80.36 40.31 41.09 39.53 50.00 51.55 32.00 44.00 57.00 48.00 40.00 Financial Models (No Reasoning) Xuanyuan3-70B-Chat 70B 25.33 61.71 24.42 31.00 Financial Models (With Reasoning) Qwen-Fin-R1-7B Qwen-Dianjin-R1-7B Qwen-Dianjin-R1-32B XuanYuan-FinX1-Preview Agentar-Fin-R1-8B Agentar-Fin-R1-32B 7B 7B 32B 70B 8B 32B 28.14 25.33 54.67 49.33 64.00 66.67 49.90 55.72 69.74 69.37 74.19 86.73 20.93 22.48 31.78 29.07 48.06 53.87 27.00 28.00 47.00 48.00 63.00 69.00 40.74 41.57 52.05 35. 48.57 54.29 50.78 54.96 53.19 15.60 28.22 39.28 51.44 36.14 51.63 56.33 57.00 69.00 78.50 79.00 77.00 84.50 85.00 81.00 83.00 65. 76.00 80.50 81.50 72.50 80.50 87.00 37.95 48.22 54.29 45.20 54.45 59.46 61.70 61.28 60.46 37.26 38.37 41.89 56.02 50.74 63.56 69.93 Table 3: Performance comparison on Finova. Agent Capabilities: Intent(Financial Intent Detection), Slotting(Financial Slot Recognition), Tool(Financial Tool Planning), Expression(Financial Expression Generation). Complex Reasoning: Combined score of Financial Mathematics and Code Understanding and Financial Complex Reasoning. Safety & Compliance: Combined score of Safety and Compliance. Scores in bold indicate the best results. This task evaluates the models ability to integrate complex information, contextually relevant expressions in financial contexts. This is particularly important as it correlates directly with hallucination suppression, critical requirement in practical applications. The ability of the Agentar-Fin-R1 models to generate precise, coherent and reliable financial expressions indicates their exceptional accuracy and reliability, making them highly suitable for real-world financial decision-making tasks. In the realm of complex reasoning, which combines financial mathematics, code understanding, and intricate financial problem solving, Agentar-Fin-R1-32B leads the way with score of 56.33. This positions our model as the best performing model in handling sophisticated financial reasoning tasks, outperforming both general-purpose models and other financial-specific models. The ability to efficiently process and solve complex reasoning tasks is vital for applications such as financial analysis, forecasting, and decision support, areas where Agentar-Fin-R1 excels due to its combination of financial expertise and advanced reasoning capabilities. Equally significant is our models performance in safety and compliance tasks, where Agentar-Fin-R1-32B achieves the highest score of 87.00, far exceeding the performance of all other models. Financial systems are subject to stringent regulatory standards, and Agentar-FinR1 demonstrates an exceptional capacity to comply with these regulations while maintaining safety in its operations. The Agentar-Fin-R1-8B model also excels in this domain with score of 80.50, showcasing its trustworthiness when handling sensitive financial data. These results validate the application of our model in regulated financial environments, ensuring both safety and compliance, which are paramount in any financial AI system. 20 4.5. Ablation Study 4.5.1. Ablation Study on Label System and Weighted Training Framework To evaluate the effectiveness of our proposed label-guided weighted training framework, we conducted an ablation study under constrained data regimes. The primary goal is to demonstrate that, by leveraging structured task label system and difficulty-aware sample weighting, the model can achieve comparable or even superior performance with significantly fewer training samples. We compare the following training configurations across different data budget constraints, all of which employ the SFT training method. Ours (Label + Weighting): The full framework using the task label system for stratified sampling and difficulty-aware weighting for each instance, trained using the standard supervised fine-tuning (SFT) method. The weighting strategy is deeply integrated with the label system. We evaluate this approach under three data budget settings: 10%, 30%, and 50% of the full dataset. Label-Only Stratified Sampling: Data are sampled according to the label distribution, but all samples are treated with equal weight during training, using the SFT method. This setting isolates the effect of the label system without weighting. We use 50% of the full dataset for this baseline. Unlike random sampling, which does not consider label distribution, this approach ensures balanced representation of the task labels within the sampled data. Random Sampling: Training samples are randomly selected from the full data pool (50% of the full size), without any use of the label system or weighting, and trained with the SFT method. This is the simplest sampling method, where no structure is imposed on the sample selection. Full Data (Vanilla SFT): Standard supervised fine-tuning (SFT) on the entire training dataset (300k data samples) without label guidance or instance weighting. This method serves as comparison point. All configurations are evaluated on the same downstream tasks, using the benchmark datasets: Fineva, FinEval, FinanceIQ, Finova, MATH-500, and GPQA-diamond. We report task-specific accuracy and overall average performance. The model used for the experiment is Qwen3-8B. Key Findings and Analysis As shown in Table 4, our proposed method demonstrates consistent improvements across different data budget constraints: Data Efficiency: Even with only 10% of the training data (30k samples), our approach achieves competitive performance (76.68 average), highlighting the efficiency of the label-guided weighted training framework. The performance progressively improves as we increase the data budget: 10% 30% 50% (76.68 77.35 78.12). Our method consistently outperforms both label-only stratified sampling and random sampling baselines across all evaluation datasets. Component Contribution Analysis: 21 Training Strategy Financial General Average Random Sampling (50% data) Label-Only Stratified Sampling (50% data) Ours (10% data) Ours (30% data) Ours (50% data) Full Data (Vanilla SFT) Base Model (Qwen3-8B) Fineva FinEval FinanceIQ Finova MATH GPQA All Datasets 86.43 88.61 88.14 89.34 89.97 89.23 84. 79.23 82.98 81.94 83.46 84.24 83.89 76.27 76.72 78.43 77.22 78.13 79. 78.69 73.06 58.73 61.32 61.01 61.28 62.92 61.63 54. 92.20 92.00 93.20 91.80 92.60 91.80 93.80 58.59 57.07 58.59 60.10 60. 58.08 59.60 75.32 76.74 76.68 77.35 78.12 77.22 73. Table 4: Performance comparison across different training strategies under constrained and full data budgets. We evaluate our method with 10%, 30%, and 50% of the training data to demonstrate its effectiveness across various data budget constraints. Datasets are grouped into Financial and General. Note that MATH refers to MATH-500 and GPQA refers to GPQA-diamond. Bold indicates the best performance among all configurations. Label System Impact: Label-only stratified sampling (76.74) outperforms random sampling (75.32) by 1.42, demonstrating the value of structured sampling. Weighting Mechanism Impact: Our full framework (78.12) further improves upon labelonly sampling by 1.38, validating the effectiveness of difficulty-aware weighting. Combined Effect: The synergy between label system and weighting mechanism provides total improvement of 2.80 over random sampling. Efficiency vs. Full Data Comparison: Our method with 50% data achieves superior performance compared to the full-data vanilla baseline, demonstrating remarkable data efficiency Even our 30% data configuration maintains competitive results, indicating that training efficiency is significantly improved by our framework The performance gap is especially pronounced on challenging datasets, suggesting that the weighting mechanism effectively prioritizes harder, more informative samples Discussion These results validate several key aspects of our framework: 1. Label System Effectiveness: Structured task labeling provides meaningful guidance for sample selection, leading to more balanced and representative training data 2. Weighting Strategy Value: Difficulty-aware sample weighting amplifies the learning signal from challenging examples, improving model robustness 3. Data Efficiency: The combination of label-guided sampling and weighting achieves superior performance with significantly reduced data requirements 4. Scalability: The framework maintains effectiveness across various data budget constraints, from extremely limited (10%) to moderately constrained (50%) scenarios In conclusion, our label-guided weighted training framework demonstrates that wellstructured label system, when combined with difficulty-aware weighting, provides an efficient and effective training signal that can match or exceed full-data performance while using only half the training samples. 22 4.5.2. Ablation Study on Two-Stage Training Pipeline To evaluate the effectiveness of our proposed two-stage training strategy, we design two complementary ablation experiments. The first experiment validates the performance improvements of two-stage training compared to single-stage training; the second experiment evaluates the advantages of our method in rapid adaptation to downstream tasks. We compare the following three training configurations to validate the contribution of each training stage: Base Model (Qwen3-8B): The original foundation model without any domain-specific training, serving as the performance lower bound baseline. Single-Stage (SFT Only): Using only the first-stage supervised fine-tuning for financial knowledge injection, representing the standard domain adaptation approach. Ours (Two-Stage): The complete two-stage pipeline, where the first stage performs SFT financial knowledge injection, and the second stage combines GRPO and SFT for reasoning enhancement and knowledge refinement. All configurations are evaluated on the same benchmark datasets: Fineva, FinEval, FinanceIQ, Finova, MATH-500, and GPQA-diamond. Training Strategy Financial General Average Fineva FinEval FinanceIQ Finova MATH GPQA All Datasets Single-Stage (SFT Only) Ours (Two-Stage) 90.21 91.52 Base Model (Qwen3-8B) 84.36 84.19 85.09 76.27 82.32 84. 73.06 62.87 63.56 54.45 94.20 93.40 93.80 58.59 59. 59.60 78.73 79.57 73.59 Table 5: Performance comparison across different training strategies. Datasets are grouped into Financial and General categories. Note that MATH refers to MATH-500 and GPQA refers to GPQA-diamond. Bold indicates the best performance among all training configurations. Key Findings: The first-stage SFT brings significant improvement The second-stage GRPO+SFT further advances the performance upper bound. The improvement is most pronounced on financial specialized tasks validating the effectiveness of domain-specific training Discussion These experiments collectively validate the effectiveness of our two-stage training strategy: 1. Stage-wise improvement: Two-stage training brings significant performance gains compared to single-stage training 2. Component synergy: The combination of GRPO and SFT in the second stage produces optimal results 3. Efficiency gains: Achieves dual optimization of performance and efficiency while maintaining high performance 23 These results demonstrate that our two-stage training strategy achieves dual optimization of performance and efficiency through progressive knowledge injection and reasoning enhancement. 5. Conclusion In this work, we introduce Agentar-Fin-R1, family of specialized, efficient, and reasoningenhanced financial large language models that systematically addresses fundamental challenges in domain-specific language model development. Our comprehensive approach demonstrates three principal contributions validated through rigorous experimental analysis. Domain Expertise: Agentar-Fin-R1 achieves state-of-the-art performance across comprehensive financial benchmarks, establishing new performance standards that consistently surpass existing approaches. The specialization manifests most prominently in three critical dimensions: (1) Agent Capabilities demonstrating sophisticated multi-step reasoning, tool integration, and complex task decomposition in financial workflows; (2) Hallucination Mitigation maintaining factual precision and epistemic reliability in high-stakes financial decision-making contexts; and (3) Safety and Regulatory Compliance ensuring adherence to stringent regulatory frameworks and ethical guidelines. Our data construction methodology leverages authenticated, high-fidelity sources through principled synthesis framework that preserves data provenance and maintains quality assurance throughout the pipeline. Training Efficiency: We propose novel weighted training framework incorporating difficulty-aware sample estimation and two-stage optimization strategies that systematically maximizes data utilization efficiency while achieving superior convergence properties. This methodology enables targeted optimization across heterogeneous financial task distributions without incurring the computational penalties typically associated with domain specialization, thereby establishing new paradigm for efficient domain adaptation. Our sophisticated financial label system further enhances training efficiency by enabling strategic sample selection and curriculum learning approaches that optimize resource allocation. Advanced Reasoning: Our models demonstrate robust reasoning capabilities that extend beyond specialized financial tasks, maintaining competitive performance on general-domain reasoning benchmarks while achieving domain expertise. This dual competency validates our approachs ability to circumvent catastrophic forgetting while successfully acquiring specialized knowledge representations. We present Finova, meticulously designed evaluation suite specifically engineered to comprehensively assess real-world deployment capabilities. This research establishes foundational principles for developing trustworthy, efficient, and capable domain-specific language models with broad implications for specialized AI applications. The methodological contributions presented herein extend beyond financial domains to other mission-critical applications where specialization, computational efficiency, and reasoning fidelity are paramount. Future research directions include real-time adaptation mechanisms for dynamic environments and cross-domain generalization of our proposed frameworks."
        },
        {
            "title": "References",
            "content": "[1] Alipay Team. Financial Evaluation Dataset, 2023. URL https://github.com/alipay/ financial_evaluation_dataset. Accessed: 2024-03-18. [2] Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, et al. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505.00949. [3] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. [4] Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, and Zhongyu Wei. Disc-finllm: chinese financial large language model based on multiple experts fine-tuning, 2023. URL https: //arxiv.org/abs/2310.15205. [5] Zihan Dong, Xinyu Fan, and Zhiyuan Peng. Fnspid: comprehensive financial news dataset in time series. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 49184927, 2024. [6] Duxiaoman DI Team. FinanceIQ, 2023. URL https://github.com/Duxiaoman-DI/Xu anYuan/tree/main/FinanceIQ. Accessed: 2024-03-18. [7] Duxiaoman DI Team. XuanYuan370b-chat, 2024. URL https://github.com/Duxiaom an-DI/XuanYuan. Accessed: 2024-03-18. [8] Duxiaoman DI Team. XuanYuan-finx1-preview, 2024. URL https://github.com/Dux iaoman-DI/XuanYuan. Accessed: 2024-03-18. [9] Georgios Fatouros, Konstantinos Metaxas, John Soldatos, and Dimosthenis Kyriazis. Can large language models beat wall street? unveiling the potential of ai in stock selection. arXiv preprint arXiv:2401.03737, 2024. [10] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, et al. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. [12] Xin Guo, Haotian Xia, Zhaowei Liu, Hanyang Cao, Zhi Yang, Zhiqiang Liu, Sizhe Wang, Jinyi Niu, Chuqi Wang, Yanhui Wang, Xiaolong Liang, Xiaoming Huang, Bing Zhu, Zhongyu Wei, Yun Chen, Weining Shen, and Liwen Zhang. Fineval: chinese financial domain knowledge evaluation benchmark for large language models, 2024. URL https://arxiv.org/abs/2308.09975. [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Proceedings of NeurIPS, 2021. [14] Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun Huang, and Wei Lin. Alphafin: Benchmarking financial analysis with retrieval-augmented stock-chain framework. arXiv preprint arXiv:2403.12582, 2024. [15] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, et al. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. URL https://arxiv.org/abs/2412.19437. [16] Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, and Liwen Zhang. Fin-r1: large language model for financial reasoning through reinforcement learning. CoRR, abs/2503.16252, 2025. URL https://arxiv.org/abs/2503.16252. [17] OpenAI. Gpt-4o technical report. https://openai.com/research/gpt-4o, 2024. [18] OpenAI. Learning to reason with llms. https://openai.com/index/learning-to-r eason-with-llms/, 2024. [19] Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Han Yi, Jimin Huang, Qianqian Xie, and Jianyun Nie. Fino1: On the transferability of reasoning enhanced llms to finance. CoRR, abs/2502.08127, 2025. URL https://arxiv.org/abs/2502.08127. [20] Qwen. QwQ: Reflect Deeply on the Boundaries of the Unknown. https://github.com /QwenLM/QwQ, 2024. [21] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level googleproof q&a benchmark. In Proceedings of COLM, 2024. [22] ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv e-prints, pages arXiv2504, 2025. [23] Ling Team, Bin Hu, Cai Chen, Deng Zhao, Ding Liu, Dingnan Jin, Feng Zhu, Hao Dai, Hongzhi Luan, Jia Guo, Jiaming Liu, Jiewei Wu, Jun Mei, Jun Zhou, Junbo Zhao, Junwu Xiong, Kaihong Zhang, Kuan Xu, Lei Liang, Liang Jiang, Liangcheng Fu, Longfei Zheng, Qiang Gao, Qing Cui, Quan Wan, Shaomian Zheng, Shuaicheng Li, Tongkai Yang, Wang Ren, Xiaodong Yan, Xiaopei Wan, Xiaoyun Feng, Xin Zhao, Xinxing Yang, Xinyu Kong, Xuemin Yang, Yang Li, Yingting Wu, Yongkang Liu, Zhankai Xu, Zhenduo Zhang, Zhenglei Zhou, Zhenyu Huang, Zhiqiang Zhang, Zihao Wang, and Zujie Wen. Ring-lite: Scalable reasoning via c3po-stabilized reinforcement learning for llms, 2025. URL https://arxiv. org/abs/2506.14731. [24] Hanshuang Tong, Jun Li, Ning Wu, Ming Gong, Dongmei Zhang, and Qi Zhang. Ploutos: Towards interpretable stock movement prediction with financial large language model. arXiv preprint arXiv:2403.00782, 2024. [25] Saizhuo Wang, Hang Yuan, Lionel Ni, and Jian Guo. Quantagent: Seeking holy grail in trading by self-improving large language model. arXiv preprint arXiv:2402.03755, 2024. [26] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro LopezLira, and Jimin Huang. Pixiu: large language model, instruction data and evaluation benchmark for finance, 2023. URL https://arxiv.org/abs/2306.05443. 26 [27] Qianqian Xie, Jimin Huang, Dong Li, Zhengyu Chen, Ruoyu Xiang, Mengxi Xiao, Yangyang Yu, Vijayasai Somasundaram, Kailai Yang, Chenhan Yuan, et al. Finnlp-agentscen-2024 shared task: Financial challenges in large language models-finllms. In Proceedings of the Eighth Financial Technology and Natural Language Processing and the 1st Agent AI for Scenario Planning, pages 119126, 2024. [28] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, et al. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. URL https://arxiv.org/abs/2412.15115. [29] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, et al. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [30] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. [31] Hanyu Zhang, Boyu Qiu, Yuhao Feng, Shuqi Li, Qian Ma, Xiyuan Zhang, Qiang Ju, Dong Yan, and Jian Xie. Baichuan4-finance technical report, 2025. URL https://arxiv.org/ abs/2412.15270. [32] Xiaoke Zhao, Zhaowen Zhou, Lin Chen, Lihong Wang, Zhiyi Huang, Kaiyuan Zheng, Yanjun Zheng, Xiyang Du, Longfei Liao, Jiawei Liu, Xiang Qi, Bo Zhang, Peng Zhang, Zhe Li, and Wei Wang. Agentar-deepfinance-300k: large-scale financial dataset via systematic chain-of-thought synthesis optimization, 2025. URL https://arxiv.org/abs/2507.1 2901. [33] Jie Zhu, Qian Chen, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, and Chi Zhang. Dianjinr1: Evaluating and enhancing financial reasoning in large language models, 2025. URL https://arxiv.org/abs/2504.15716."
        },
        {
            "title": "Appendix",
            "content": "Figure 6: An example for Financial Intent Detection of Finova. Figure 7: An example for Financial Slot Recognition of Finova. 28 Figure 8: An example for Financial Tool Planning of Finova.. Figure 9: An example for Financial Expression Generation of Finova."
        }
    ],
    "affiliations": [
        "@antgroup.com"
    ]
}