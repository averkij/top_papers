{
    "paper_title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning",
    "authors": [
        "Liang Chen",
        "Hongcheng Gao",
        "Tianyu Liu",
        "Zhiqi Huang",
        "Flood Sung",
        "Xinyu Zhou",
        "Yuxin Wu",
        "Baobao Chang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This ``knowing-doing'' gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, a curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates a perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at https://github.com/chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents."
        },
        {
            "title": "Start",
            "content": "G1: BOOTSTRAPPING PERCEPTION AND REASONING ABILITIES OF VISION-LANGUAGE MODEL VIA REINFORCEMENT LEARNING Liang Chen1,3 Hongcheng Gao2,3 Tianyu Liu1 Zhiqi Huang3 Flood Sung3 Xinyu Zhou3 Yuxin Wu3 Baobao Chang1 1Peking University 2UCAS 3Moonshot AI May 23,"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but struggle to translate this prowess into effective decision-making within interactive, visually rich environments like games. This knowing-doing gap significantly limits their potential as autonomous agents, as leading VLMs often performing badly in simple games. To address this, we introduce VLM-Gym, curated reinforcement learning (RL) environment featuring diverse visual games with unified interfaces and adjustable, compositional difficulty, specifically designed for scalable multi-game parallel training. Leveraging VLM-Gym, we train G0 models using pure RL-driven self-evolution, which demonstrate emergent perception and reasoning patterns. To further mitigate challenges arising from game diversity, we develop G1 models. G1 incorporates perception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models consistently surpass their teacher across all games and outperform leading proprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals an intriguing finding: perception and reasoning abilities mutually bootstrap each other throughout the RL training process. Source code including VLM-Gym and RL training are released at chenllliang/G1 to foster future research in advancing VLMs as capable interactive agents. 5 2 0 2 9 1 ] . [ 1 6 2 4 3 1 . 5 0 5 2 : r Figure 1: Comparison of different models on games from VLM-Gym. Equal contribution. Work done during internship at Moonshot AI. G"
        },
        {
            "title": "Introduction",
            "content": "Interactive environments, particularly visually rich games ranging from classic Atari [17] platforms and board games like Go (famously tackled by AlphaGo [27]) to recently popular complex simulations like Pokémon [1], have been serving as popular testbeds for evaluating and advancing the general decision making capabilities of Artificial Intelligence (AI) agents, spanning visual perception, reasoning, and action. While Vision-Language Models (VLMs) [1, 2, 5, 19, 30, 32], powered by advances in Large Language Models (LLMs), have demonstrated remarkable success in tasks like image captioning, visual question answering, and mathematical reasoning, translating this prowess to effective decision-making within interactive environments such as embodied robot [6] and visual games remains significant hurdle, limiting the boarder applications of VLMs in developing truly autonomous agents capable of reasoning and acting effectively based on rich visual information in the real world. In fact, VLM performance in such settings often falls considerably short of human capabilities [21, 35, 38]. Our findings also underscore this challenge: even leading VLMs such as OpenAI-o1 [18] and Qwen2.5VL-72B [2] can struggle profoundly with straightforward games like Shisen-Sho and 2048, sometimes even achieving scores lower than random choice. This significant performance gap raises our interest in its potential causes and in methods for improving performance. Perception and reasoning are two core abilities of VLMs. However, solving visual games requires not only accurate visual perception and reasoning about the current state and game rules, but also the crucial ability to translate that understanding into effective action, bridging the knowing-doing gap. While manually curating multimodal chain-ofthought data is one approach for training game-playing agents [6, 41], this method faces scalability limitations. We therefore consider Reinforcement Learning (RL) with both historical and current relevance. RL is well-established technique in game AI training (e.g., CNN-based Atari players [17]). It is also gaining significant recent attention through methods such as Reinforcement Learning with Verifiable Rewards (RLVR), recognized as key for enhancing LLM reasoning capabilities, as highlighted by DeepSeek-R1 [8]. While games naturally provide verifiable reward such as scores, an effective and scalable training framework for RLVR of VLMs in interactive games is still lacking, and the potential benefits for perception and reasoning also remain unclear. To address these challenges, we first introduce VLM-Gym: curated suite of RL environments featuring multiple visual games (2048, Shisen-Sho, Shisen-Sho-Cifar10, Swap) with unified observation/action spaces and adjustable, compositional difficulty levels. Crucially, VLM-Gym supports the scalable multi-game parallel training and parallel action sampling required by advanced LLM-RL algorithms like GRPO [24]. With VLM-Gym, we start with training weak VLM gamer (Qwen2.5-VL-7B) with pure RL through self-evolution during playing games, resulting in our G0 models. During training, we find that G0 naturally emerged with effective perception (e.g., localization patterns ) and reasoning patterns. These emergent behaviors led to substantially boosted performance in various games (e.g., improving the score from 1.9 to 12.8 in Shisen-Sho), already surpassing powerful multimodal models such as OpenAI-o1 and Qwen2.5VL-72B. However, G0 models still face challenges due to the diversity of games. These challenges include perception prior gap, inaccurate reward credit assignment, and sparse reward problems. To address these issues and further enhance overall game performance, we introduce the G1 models. These models incorporate perception-enhanced cold start and knowledge distillation from teacher model prior to reinforcement learning training. The resulting G1 models outperform their teacher on all games and significantly surpass the state-ofthe-art Claude-3.7-Sonnet-Thinking model, as detailed in Figure 1. We conducted systematic analysis of G0 and G1 by disentangling their perception and reasoning accuracies, and found that these two abilities bootstrapped each other during the RL training process. We release source code at chenllliang/G1 to facilitate future research."
        },
        {
            "title": "2 VLM-Gym: A Scalable Interactive Environment for VLMs",
            "content": "VLM-Gym is curated environment designed for scalable Reinforcement Learning and algorithm design of VLMs within interactive games. As shown in Figure 2, it incorporates three key features which are missing in current RL environments for VLMs: Scalable Environments: VLM-Gym supports parallel execution across numerous game states simultaneously, as well as across multiple distinct games. This capability facilitates efficient large-batch training and enables research into multi-task reinforcement learning for VLMs. Parallel Actions: Many recently proposed RL algorithms for reasoning models (e.g., GRPO, RLOO, Reinforce++) require sampling numerous outputs from given state to estimate advantages. This capability is often absent in standard Gym-style environments, which typically advance the state based on single action and lack built-in mechanisms to efficiently evaluate multiple hypothetical actions from the same observation. VLM-Gym overcomes this limitation by enabling the parallel sampling of multiple actions for any given observation and subsequently 2 G1 Figure 2: Key features of VLM-Gym. We illustrate them using the Shisen-Sho game as an example. computing the reward associated with each action. This provides crucial support for advanced RL algorithms specifically tailored for foundation models. Compositional Difficulties: Environments in VLM-Gym feature adjustable difficulties across multiple dimensions (e.g., perceptual complexity, reasoning depth), specifically tailored for each game. These dimensions can often be combined, allowing for fine-grained control over task difficulty and facilitating future studies on the generalization capabilities of VLMs within RL settings. 2.1 Game Environment Description 2048 is single-player sliding puzzle where players merge identical numbered tiles on 44 grid to reach 2048. 2048 Players slide tiles in four directions, planning merges carefully to avoid filling the board. Reward2048(a, s) = (cid:26)1 1 if leads to tile merged in otherwise (1) Shisen-Sho Shisen-Sho is tile-matching game using shape tiles. Players match identical tiles with path of no more than three straight lines. RewardShisen-Sho(a, s) = (cid:26)1 1 if leads to tile matched in otherwise (2) Shisen-Sho-Cifar10 Shisen-Sho-Cifar-10 is an advanced variant of the traditional Shisen-Sho game that incorporates more visually complex tiles, which used for exploring theimprovement of perception. Instead of traditional symbols or patterns, this version uses CIFAR10 [14] images as tiles, creating more perceptually challenging experience. The reward function is same as Shisen-Sho in Eq. 2. Swap Swap is typically tile-matching puzzle game where players swap adjacent tiles to create matches of three or more identical tiles in row or column. When matches are formed, the matched tiles disappear, and new tiles fall from the top to fill the empty spaces. Rewardswap(a, s) = (cid:26)1 1 if leads to matched tile disappeared in otherwise (3) G1 2.2 Game Prompt and Action Format"
        },
        {
            "title": "Game Prompt Template",
            "content": "Game Rule Description {game rule} Output Format Description First describe the board in <perception></perception>. Then output your thinking process in <think></think> and final action in <answer></answer>. the action format is represented by choice {up, down, lef t, right} indicatFor the game 2048, ing the slide direction. In both Shisen-Sho and Shisen-Sho-Hard-Perception, actions are formatted as <answer>(x1, y1), (x2, y2)</answer>, specifying the coordinates of two tiles to be matched. Similarly, for Swap, the action format <answer>(x1, y1), (x2, y2)</answer> identifies the coordinates of two tiles that need to be swapped. More details about the prompt for each game can be found in Appendix C.3."
        },
        {
            "title": "3 Reinforcement Learning with VLM-Gym",
            "content": "In this section, we elaborate the training details of our G0 and G1 models on VLM-Gym, including the RL training objectives (Sec. 3.1), cold start finetuning (Sec. 3.2) and the evaluation details (Sec. 3.4). 3.1 RL Training Objective Rewards Setting In the RL process, we define three distinct rewards to evaluate each model output response: 1. Game Reward (GR): The primary reward associated with the action taken in the game environment. This reward reflects the success or progress achieved by the model in completing game-specific objectives, such as solving puzzles, navigating obstacles, or achieving high scores. The details of game reward are explained in Section 2. 2. Format Reward (FR): An auxiliary 0-1 reward that evaluates whether the models output adheres to the required format: <perception>...</perception><think>...</think><answer>...</answer>. This ensures that the model produces structured and interpretable outputs. 3. Perception Reward (PR): An auxiliary 0-1 reward that evaluates the accuracy of the models perception by comparing its outputs to the environment-provided ground truth. PR is 1 only when the model follows the predefined output perception structure and also produces fully correct results; otherwise, the reward is 0. To compute the final reward, we combine these three components using weighted sum, where α (default 1) and β (default 0) are hyperparameters that control the relative importance of Format Reward and Perception Reward, respectively: Final Reward = GR + α FR + β PR. (4) Reinforcement Learning Algorithm We adopt Group Relative Policy Optimization [24](GRPO) as the primary reinforcement learning algorithm, following the approach of DeepSeek-R1 [8]. GRPO optimizes the policy model πθ by maximizing the following objective: JGRP O(πθ) = E[{qs}B s=1 pQ, {oi}G (cid:40) (cid:34) i=1 πθold (qs)] (cid:32) 1 (cid:88) (cid:88) 1 oi (cid:88) 1 oi s=1 i=1 t=1 πs,i,t θ πs,i,t θold πs,i,t θ πs,i,t θold min ˆAs,i,t, clip , 1 ϵ, 1 + ϵ ˆAs,i,t βDKL [πθπref ] , (cid:33) (cid:35) (cid:41) (5) where qs denotes the s-th observation in batch B, consisting of game screenshot and prompt, which together serve as input to the old VLM πθold . The model then generates group of outputs, denoted as {oi}G i=1. The advantage of each model output is normalized within the rewards of its output group to have mean of 0 and standard deviation of 1. ˆAs,i,t = ˆAGRP s,i,t = R(qs, oi) mean({R(qs, o1), . . . , R(qs, oG)}) std({R(qs, o1), . . . , R(qs, oG)}) (6) 4 G1 Exploring Diverse and Dependent Game States The nature of game states in our reinforcement learning approach differs significantly from independent tasks, such as answering distinct math questions. Instead, game states evolve according to Markov process, where future states depend on the past states. This is evident in games like 2048, which starts with minimal tiles, or Shisen-Sho, where the initial board is full. Solely using the base VLM as policy would limit the exploration of different game states due to the limit of prior knowledge. Inspired by the ϵ-greedy method widely adopted in RL and to promote broader exploration and ensure the training policy model encounters diverse game states, we employ random policy as baseline. This involves executing predetermined number (2048: 100 steps, Shisen-Sho: 250 Steps, Shisen-Sho-Cifar10: 250 Steps, Swap: 250 Steps) of random steps for each game. While random policy serves this purpose, exploration could be further enhanced by implementing more sophisticated searching algorithm or stronger model, which we reserve for future investigation. 3.2 Knowledge Distillation with Ground-truth Perception Our initial experiments revealed that our base model struggled to accurately recognize visual elements in game observations, such as the numbers in 2048 and the shapes in Shisen-Sho. We were also interested in exploring how supervision from stronger model could influence the base models reinforcement learning process. Therefore, leveraging the ground truth perception information provided by our VLM-Gym environment, we prompted more advanced model, Claude-3.7-Sonnet-Thinking, with this data to obtain responses including thinking and action information (within <think>...</think> and <answer>...</answer> ). We then used the resulting response concatenated with the groundtruth perception information to finetune the base model prior to reinforcement learning, leading to our G1-series model. The template prompt is shown below and we also give concrete example in Figure 9 from the Appendix. Prompt for Knowledge Distillation Game Rule Description {game rule} Output Format Description will give you the board description in <perception></perception>. Then output your thinking process in <think></think> and final action in <answer></answer>. <perception>{ground-truth perception}</perception> 3.3 Implementation Details Usage of Open Source Resources For all training experiments, encompassing both reinforcement learning and supervised fine-tuning, we utilized Qwen2.5-VL-7B [2] as our base model. Our VLM-Gym environment adheres to the interface standards established by Gymnasium [33]. The implementation of our RL system leverages the open-source EasyR1 [42] framework, which is built upon the VeRL [26] architecture. The SFT codebase is based on the LLamaFactory [43] library. Training Details Each game was trained independently. The observation screenshot has resolution of 640840 pixels (width height). The G0-series models were trained using RL. The G1-series models underwent SFT prior to RL training. For RL, constant learning rate of 1.0e-6 with 0.02 weight decay was used, with batch size of 128 parallel games and group size of 5 for 500 training steps per game. The clip range ϵ of GRPO is set to 0.2 and the KL efficient β is set to 0.01. For SFT, 1,000 observations were collected per game using the same random policy as in RL training, and 1,000 corresponding distilled model responses were generated. SFT was performed with learning rate of 2e-5 for 1 epoch for each game. 3.4 Evaluations During evaluation, we benchmarked different models in multi-turn setting, adhering to the original games scoring rules. We report the cumulative game scores over multiple steps, averaged across several independent runs. For 2048, the score of each step is calculated as the sum of the values of merged tiles. For Shisen-Sho, Shisen-Sho-Cifar10, and Swap, the score is equivalent to the game reward. We compare our G0 and G1 models with state-of-the-art open source VLM 5 G1 Table 1: Average cumulative game scores over multiple evaluation runs. 100 steps 10 means we report the average scores of 10 random runs, each has 100 game steps."
        },
        {
            "title": "Models",
            "content": "Random Qwen2.5VL-7B Qwen2.5VL-72B GPT4o o1 Claude-3.7-Sonnet-Thinking G0-7B G1-7B 2048 100 steps 10 Shisen-Sho 36 steps 10 Shisen-Sho-Cifar10 36 steps 10 Swap 1 step 100 701 246 243 620 539 892 759 0.4 1.9 2.6 1.5 3.9 15.3 12.8 17.5 0.4 0.4 0.8 4.7 5.4 8.7 8.0 14.1 0.01 0.02 0.06 0.01 0.04 0.43 0.05 0.78 Qwen2.5-VL-72B [2] and close source models such as OpenAI-o1 [18], GPT4o [20], Claude-3.7-Sonnet-Thinking [1] through API calling."
        },
        {
            "title": "4 Bootstrapping Perception and Reasoning Abilities via RL",
            "content": "We provide comprehensive comparison of game performance, including the G0, G1, and other baseline models, in Table 1. Notably, the G1-7B model achieves the highest performance across all games, while the G0-7B model also outperforms strong models such as o1, GPT4o and Qwen2.5VL-72B. In this section, we present the main experimental results for the G0 (Sec. 4.1) and G1 (Sec. 4.2) models, along with our key insights into reinforcement learning for vision-language models. 4.1 G0: From Zero to Game Masters without Supervision Figure 3: Average game reward curves of different games for G0 models during RL process. Reward Dynamics across Games Figure 3 illustrates that G0 models exhibit distinct average game reward dynamics across various games. Given the unified reward space of {-1, 1}, these comparisons are fair. We explore each games RL process and summarize the key reasons for their differences: Perception Prior Gap, Inaccurate Reward Credit and Sparse Reward. Shisen-Sho: Over 400 steps, the games reward rises efficiently from -1 to 0.8, markedly boosting evaluation performance to 12.8a substantial improvement for the 7B model compared to its base model (1.9) and even the 72B version (2.6). Surprisingly, we found that the base VLM learned the optimal perception patterns and reasoning G1 paradigms as shown in Figure 4 in the RL process similar to the aha moment of Deepseek-R1, introduced in the next paragraph. Shisen-Sho-Cifar10: The G0 model can continue improving during the RL process, but at slower rate than the base Shisen-Sho version. Although the game shares the same rules, its significantly higher perceptual difficultywhich we term the Perception Prior Gapslows down the learning process without affecting the reasoning challenge. It showcases that perception ability of base model could be bottleneck for VLM RL. In evaluation, G0 (8.0) still significantly outperforms its baseline (0.4). 2048: Based on the reward curve, 2048 gains little from the G0 RL process. Analyzing the RL rollouts, we identify the issue as the Inaccurate Reward Credit problem. With only four moves up, down, left, right, even random strategy serves as strong baseline. As shown in Table 1, in 2048 the random baseline outperforms Qwen2.5VL-72B, GPT4o, and the o1 model. The policy model can produce entirely incorrect perception and reasoning responses while still receiving positive reward, which biases the learning process. To validate our assumption, we reviewed G0 2048s rollouts before and after RL and found that after RL the model crashed, completely ignored the screenshot image, and took random actions as shown in Figure 10 from Appendix. The case shows that before RL training, the model produced incorrect perception and reasoning outputs, yet still received positive game rewards, which encouraged the policy model to adopt these flawed behaviors. Swap: The Swap game also gains little from the G0 RL process. However it is due to different reason, Sparse Reward, where the policy model can hardly gain positive reward from the environments as the game is too hard for the base model to get positive reward. In summary, visual game reinforcement learning poses unique perception and reasoning challenges due to the diversity of games. We are interested in how these abilities evolve during RL and aim to develop general methods for effective learning process. Bootstrapping of Perception and Reasoning Abilities During Exploration The performance of the G0 model in the games Shisen-Sho and Shisen-Sho-Cifar10 is particularly noteworthy. In these instances, the base model surpasses other models without the need for external supervision, attaining remarkable results solely through reinforcement learning from its own experience. This leads to significant performance improvements. We are intrigued by what aspects of the G0 models reinforcement learning process make it more successful in these two games compared to others. As shown in Figure 4, we analyze the rollouts of the G0 model during RL. Our findings reveal that the policy model successfully acquired optimal perception and reasoning patterns for the game. Prior to RL, the perception output consisted primarily of vague board descriptions without precise coordinate information for shapes. However, after 400 steps of RL, the policy developed two distinctive patterns: localization pattern in perception, which systematically identifies shapes with their exact coordinates (e.g., (0, 0): Yellow square), and an enumeration pattern in reasoning, which methodically analyzes the game state row by row. As the localization pattern has clear textual structures, we use regex to parse and calculate the average number of localization patterns in each model output in each RL step and plot the trend for different games in Figure 5. Our analysis reveals two key findings: 1) distinct increase in localization patterns appears exclusively in Shisen-Sho and Shisen-Sho-Cifar10 variants (notably, 2048 was excluded from plotting due to the complete absence of such patterns throughout all steps), and 2) The proliferation of localization patterns consistently precedes improvements in game rewards, suggesting this adaptation serves as precursor to enhanced performance. The experiment results expose that the perception and reasoning abilities are actually bootstrapping each other during the reinforcement learning process in the games. Reasoning patterns cannot develop without localization patterns, as they rely on sufficient perception information. Similarly, optimal perception patterns are incentivized only through correct reasoning patterns that lead to actions yielding rewards. large action space that prevents incorrect perception and reasoning process from gaining reward is also critical in the RL process. RL also helps fill the Knowing-Doing gap [21] of the base and finetuned model, make it effectively utilize the prior knowledge in practice. 4.2 G1: Reinforcement Learning with Perception-Enhanced Cold Start Previous experiments on the G0-series model have shown that reinforcement learning can bring significant improvements to certain gaming scenarios, but not all of them due to lack of prior knowledge in perception or reasoning. This raises the natural question: Can we enhance the RL process for all games by introducing some cold-start SFT data? By leveraging programmable environments, we can easily obtain ground-truth perceptions for different game states, as illustrated in Figure 9 in the Appendix. We then use these perception-enhanced prompts to query the teacher model and distill the data to fine-tune the base model, as explained in Section 3.2. 7 G1 Figure 4: The explored perception and reasoning patterns during G0 RL training in Shisen-Sho Game. Figure 5: Localization patterns count during G0 RL training for different games. After cold start SFT, we ran the RL experiments with the same configuration as G0 models. The evaluation result of G1 model is listed in Table 1. The RL training curves of G1 and comparisions to G0 are shown in Figure 6 and Figure 7, respectively. Notably, G1 surpasses all the baseline models on all games including the teacher model Claude-3.7-Sonnet-Thinking. Quantify Improvement of Perception and Reasoning Ability Inspired by previous work which differentiates perception and reasoning abilities of VLMs during evaluation such as PCA-Bench [6] and MMEvalPro [12], we introduce two additional metrics beyond the original reward: Perception Accuracy and Reasoning Accuracy, as plotted in Figure 6. Perception Accuracy, defined as Pacc = I(pmodel = pgt), measures whether the output within <perception></perception> tags matches the ground-truth perception information pgt. Note that we can only track Pacc in the G1-series model as the base models are finetuned to output perception information following the same format as ground-truth, which makes accurate comparison possible. Reasoning Accuracy, formulated as: Racc = I(r > 0 Pacc = 1), measures whether the policy obtains positive game reward when Perception Accuracy is 1, representing the models ability to reason correctly given accurate perception. 8 G1 Figure 6: Training curves of G1. Figure 7: Comparisons of game reward between G0 and G1 across different games during RL. G1 Training Dynamics Across Games As depicted in Figure 7, the G1-series models demonstrate more efficient RL process across all games, notable improvement over the G0 model which failed to converge in 2048 and Swap. Regarding perception, accuracy remains consistently high for 2048, Shisen-Sho, and Swap. This sustained accuracy is due to the perception-enhanced cold start and the low perceptual complexity of these environments, characterized by simple colors and shapes. In contrast, Shisen-Sho-Cifar10 presents difficult perception task. For this game, we observe that perception accuracy improves alongside game reward during RL, highlighting the co-evolution of perception and reasoning abilities throughout the learning process. SFT+RL Jointly Helps the Base Model Outperform the Supervisor Across all games, G1 RL training significantly improves game reward compared to the base model after the cold start SFT. This SFT process is crucial, as it improves the 7B base model to exceed the teacher model and addresses the Inaccurate Reward Credit and Sparse Reward issues observed in G0 experiments that led to inefficient RL training by providing prior knowledge. The training curve of Swap clearly demonstrates the significance of the RL process; the policy after SFT yielded an average game reward of merely -0.8, whereas the subsequent RL phase dramatically increased it to around +0.6. 4.3 Discussions Including Perception Reward as Process Reward Leveraging the availability of ground-truth perception from the Shisen-Sho game environment, we can incorporate perception accuracy as an auxiliary reward into the G1 RL process. To ensure the base model outputs perception data in the ground-truth format without unduly improving its broader perception or reasoning abilities, we applied SFT exclusively to its visual encoder. This was achieved using only 50 distilled examples, which provided ground-truth perception alone, devoid of reasoning or action content, to limit the scope of SFTs impact. We then ran comparative RL experiments, with and without the perception reward, to assess its effect. Figure 8 illustrates that while the perception reward significantly accelerates improvement in perception Figure 8: RL training curves exploring Perception Accuracy as process reward. 9 G1 accuracy, it does not impact the overall game reward. This may be because the model initially prioritizes its capacity on the process reward (perception). In Shisen-Sho, for instance, accurately recognizing all tiles is necessary for the full perception reward, yet complete recognition isnt always required to perform correct actions and achieve game rewards. Nevertheless, policy trained without an explicit perception reward still demonstrates an increase in its perceptual abilities during learning, trend also observed in the Shisen-Sho-Cifar10 experiments (Figure 6). This experiment further indicates that the perception and reasoning abilities of Vision-Language Model (VLM) can co-evolve using only final, accurate, and verifiable reward, suggesting more generalizable training methodology. Another interesting direction is how to set verifiable process reward for VLMs in RL process that benefits the outcome. When and why does Supervision work in G1? Observing the learning curves of G0 and G1 models (Figure 7), it is evident that the performance gap between the two architectures fluctuated depending on the game. Supervision exerted its most pronounced effect on games inherently difficult for reinforcement learning, exemplified by 2048 and Swap. This positive impact can be attributed to several factors: the provision of additional perceptual cues, the stabilization of perception accuracy throughout the training phase (in contrast to G0-2048 as shown in Figure 10), and the incorporation of more pertinent prior knowledge regarding the games mechanics, which collectively facilitated more efficient training trajectory. This improved efficiency and stability also suggest that dedicated cold-start process to stabilize the RL process may become less critical, or potentially redundant, if the base model possesses sufficient intrinsic strength, concept potentially mirrored in the Shisen-Sho series experiments that G0 and G1 finally reach the same game reward during RL training. 4.4 Limitations and Future Work VLM-Gym currently comprises specific set of visual games (2048, Shisen-Sho, Shisen-Sho-Cifar10, Swap), which, while offering varied perceptual and reasoning challenges, predominantly feature relatively straightforward rule sets. significant avenue for future work involves expanding this suite to include games with more complex mechanics, deeper strategic requirements, and diverse genres. This expansion would more rigorously test the generalization capabilities of VLM agents. Furthermore, while existed experiments reveal how perception and reasoning abilities of VLMs co-evolve in RL process, current training still encountered challenges such as sparse rewards, particularly in games like Swap. Future research could focus on developing effective RL strategies or reward-shaping mechanisms to effectively train VLM agents in scenarios with extended multi-turn interactions where feedback is infrequent, thereby better addressing the complexities of long-horizon decision-making and sparse reward tasks."
        },
        {
            "title": "5 Related Work",
            "content": "Vision-Language Models in Games Pioneering AI achievements like Deep Blue [3] and AlphaGo [28] conquered structured games. More recently, Large Language Models (LLMs) have expanded into diverse game genres (e.g., [22, 34, 37]); however, their text-centric nature limits their application in visually rich environments. Vision-Language Models (VLMs), by integrating visual perception with language understanding, are inherently better equipped for these multimodal games, demonstrating utility in complex interaction, planning, and combat scenarios (e.g., [7, 15, 29]). Benchmarking approaches for these agents have advanced from text-only evaluations [39] and text-converted game states [9] to sophisticated frameworks leveraging direct visual and textual inputs [21, 36]. While efforts like [41] have explored enhancing VLM game performance via Reinforcement Learning (RL), they often relied on text-based games (e.g., 24-points) lacking customizable difficulty. Similarly, [23] examined LLM failures in text games and RL-based solutions, but this research also remained within text-only confines. This highlights critical gaps: the need for scalable gaming environments tailored to advancing VLM capabilities through RL and insight on how different abilities of VLM agent evolves during RL experiences. VLM-Gym, introduced herein, provides platform for RL research with VLMs in visually-complex, interactive gaming scenarios. Our experiments on G0 and G1 series models offer insights into the learning dynamics of VLMs in reward-driven environments. Reinforcement Learning for Enhancing Vision-Language Models Recently, reinforcement learning has been widely used in improving reasoning capabilities in both text [8, 10, 31] and visual domains [25, 31, 32] for complex problem-solving. In the visual domain, for instance, R1-V [4] first applied GRPO to object-counting tasks, enabling 3B parameter model to outperform 72B counterpart. Similarly, VisualThinker-R1-Zero [44] demonstrated that applying R1 methodology to base VLMs yields more substantial improvements than with their SFT variants. This observation was further validated by MMEureka [16], which utilized RLOO on both instruction-tuned and base VLMs. Supporting such advanced reasoning, Vision-R1 [13] and R1-OneVision [40] developed multimodal Chain-of-Thought (CoT) datasets by transforming visual information into textual formats. Beyond these applications in static visual tasks, researchers have extended RL to dynamic gaming environments. VLM Q-Learning [11], for example, introduces 10 G1 actor-critic architectures featuring advantage-filtered supervised fine-tuning; this allows models to learn effectively from suboptimal interactions while iteratively refining their decision-making policies. Despite recent progress, most existing work examines perception and reasoning separately in RL, leaving it unclear how these core VLM abilities might mutually improve. Our G0 and G1 models offer initial evidence that such bootstrapping pattern indeed occurs during the RL process."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we tackled the challenge of translating VLMs general capabilities to effective decision-making in interactive visual environments, where \"knowing-doing\" gap often leads to suboptimal performance. To this end, we introduced VLM-Gym, scalable multi-game RL training suite for VLMs, and developed two model series: G0, which demonstrated emergent perception and reasoning via pure RL to surpass strong baselines, and G1, which incorporated perception-enhanced cold start and knowledge distillation to achieve state-of-the-art performance, outperforming even Claude-3.7-Sonnet-Thinking. Our systematic analysis revealed crucial synergistic bootstrapping between perception and reasoning during RL training, which demonstrate large space of improving VLMs abilities through. We believe VLM-Gym and the RL training framework could also serve as useful resources for the community for future research in advancing VLMs as capable interactive agents and multimodal RL development."
        },
        {
            "title": "References",
            "content": "[1] Claude 3.7 sonnet system card. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Murray Campbell, Joseph Hoane Jr, and Feng-hsiung Hsu. Deep blue. Artificial intelligence, 134(1-2):5783, 2002. [4] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [5] Liang Chen, Zekun Wang, Shuhuai Ren, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge Zhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen, Junyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Minjia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, and Baobao Chang. Next token prediction towards multimodal intelligence: comprehensive survey, 2024. [6] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain, 2024. [7] Peng Chen, Pi Bu, Jun Song, Yuan Gao, and Bo Zheng. Can vlms play action role-playing games? take black myth wukong as study case. arXiv preprint arXiv:2409.12889, 2024. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. G1 [9] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. arXiv preprint arXiv:2402.12348, 2024. [10] Hongcheng Gao, Yue Liu, Yufei He, Longxu Dou, Chao Du, Zhijie Deng, Bryan Hooi, Min Lin, and Tianyu Pang. Flowreasoner: Reinforcing query-level meta-agents. arXiv preprint arXiv:2504.15257, 2025. [11] Jake Grigsby, Yuke Zhu, Michael Ryoo, and Juan Carlos Niebles. Vlm q-learning: Aligning vision-language models for interactive decision-making. arXiv preprint arXiv:2505.03181, 2025. [12] Jinsheng Huang, Liang Chen, Taian Guo, Fu Zeng, Yusheng Zhao, Bohan Wu, Ye Yuan, Haozhe Zhao, Zhihui Guo, Yichi Zhang, Jingyang Yuan, Wei Ju, Luchen Liu, Tianyu Liu, Baobao Chang, and Ming Zhang. Mmevalpro: Calibrating multimodal benchmarks towards trustworthy and efficient evaluation, 2025. [13] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [14] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [15] Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, and Yitao Liang. Jarvis-vla: Post-training large-scale vision language models to play visual games with keyboards and mouse. arXiv preprint arXiv:2503.16365, 2025. [16] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. [17] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013. [18] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. [19] OpenAI. Gpt-4v(ision) system card. 2023. [20] OpenAI. hello-gpt-4o, 2024. [21] Davide Paglieri, Bartłomiej Cupiał, Sam Coward, Ulyana Piterbarg, Maciej Wołczyk, Akbir Khan, Eduardo Pignatelli, Łukasz Kucinski, Lerrel Pinto, Rob Fergus, Jakob Nicolaus Foerster, Jack Parker-Holder, and Tim Rocktäschel. Benchmarking agentic llm and vlm reasoning on games. arXiv preprint arXiv:2411.13543, 2024. [22] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. G1 [23] Thomas Schmied, Jörg Bornschein, Jordi Grau-Moya, Markus Wulfmeier, and Razvan Pascanu. Llms are greedy agents: Effects of rl fine-tuning on decision-making abilities, 2025. [24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [25] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [26] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [27] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, L. Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529:484489, 2016. [28] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. [29] Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. [30] Gemini Team, Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry, Lepikhin, Timothy Lillicrap, Jean baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, Luke Vilnis, Oscar Chang, Nobuyuki Morioka, George Tucker, Ce Zheng, Oliver Woodman, Nithya Attaluri, Tomas Kocisky, Evgenii Eltyshev, Xi Chen, Timothy Chung, Vittorio Selo, Siddhartha Brahma, Petko Georgiev, Ambrose Slone, Zhenkai Zhu, James Lottes, Siyuan Qiao, Ben Caine, Sebastian Riedel, Alex Tomala, Martin Chadwick, Juliette Love, Peter Choy, Sid Mittal, Neil Houlsby, Yunhao Tang, Matthew Lamm, Libin Bai, Qiao Zhang, Luheng He, Yong Cheng, Peter Humphreys, Yujia Li, Sergey Brin, Albin Cassirer, Yingjie Miao, Lukas Zilka, Taylor Tobin, Kelvin Xu, Lev Proleev, Daniel Sohn, Alberto Magni, Lisa Anne Hendricks, Isabel Gao, Santiago Ontanon, Oskar Bunyan, Nathan Byrd, Abhanshu Sharma, Biao Zhang, Mario Pinto, Rishika Sinha, Harsh Mehta, Dawei Jia, Sergi Caelles, Albert Webson, Alex Morris, Becca Roelofs, Yifan Ding, Robin Strudel, Xuehan Xiong, Marvin Ritter, Mostafa Dehghani, Rahma Chaabouni, Abhijit Karmarkar, Guangda Lai, Fabian Mentzer, Bibo Xu, YaGuang Li, Yujing Zhang, Tom Le Paine, Alex Goldin, Behnam Neyshabur, Kate Baumli, Anselm Levskaya, Michael Laskin, Wenhao Jia, Jack W. Rae, Kefan Xiao, Antoine He, Skye Giordano, Lakshman Yagati, Jean-Baptiste Lespiau, Paul Natsev, Sanjay Ganapathy, Fangyu Liu, Danilo Martins, Nanxin Chen, Yunhan Xu, Megan Barnes, Rhys May, Arpi Vezer, Junhyuk Oh, Ken Franko, Sophie Bridgers, Ruizhe Zhao, Boxi Wu, Basil Mustafa, Sean Sechrist, Emilio Parisotto, Thanumalayan Sankaranarayana Pillai, Chris Larkin, Chenjie Gu, Christina Sorokin, Maxim Krikun, Alexey Guseynov, Jessica Landon, Romina Datta, Alexander Pritzel, Phoebe Thacker, Fan Yang, Kevin Hui, Anja Hauth, Chih-Kuan Yeh, David Barker, Justin Mao-Jones, Sophia Austin, Hannah Sheahan, Parker Schuh, James Svensson, Rohan Jain, Vinay Ramasesh, Anton Briukhov, Da-Woon Chung, Tamara von Glehn, Christina Butterfield, Priya Jhakra, Matthew Wiethoff, Justin Frye, Jordan Grimstad, Beer Changpinyo, Charline Le Lan, Anna Bortsova, Yonghui Wu, Paul Voigtlaender, Tara Sainath, Shane Gu, Charlotte Smith, Will Hawkins, Kris Cao, James Besley, Srivatsan Srinivasan, Mark Omernick, Colin Gaffney, Gabriela Surita, Ryan Burnell, Bogdan Damoc, Junwhan Ahn, Andrew Brock, Mantas Pajarskas, Anastasia Petrushkina, Seb Noury, Lorenzo Blanco, Kevin Swersky, Arun Ahuja, Thi Avrahami, Vedant Misra, Raoul de Liedekerke, Mariko Iinuma, Alex Polozov, Sarah York, George van den Driessche, Paul Michel, Justin Chiu, Rory Blevins, Zach Gleicher, Adrià Recasens, Alban Rrustemi, Elena Gribovskaya, Aurko Roy, Wiktor Gworek, Sébastien M. R. Arnold, Lisa Lee, James Lee-Thorp, Marcello Maggioni, Enrique Piqueras, Kartikeya Badola, Sharad Vikram, Lucas Gonzalez, Anirudh Baddepudi, Evan Senter, Jacob Devlin, James Qin, Michael Azzam, Maja Trebacz, Martin Polacek, Kashyap Krishnakumar, Shuo yiin Chang, Matthew Tung, Ivo Penchev, Rishabh Joshi, Kate Olszewska, Carrie Muir, Mateo Wirth, Ale Jakse Hartman, Josh Newlan, Sheleem Kashem, Vijay Bolina, Elahe Dabir, Joost van Amersfoort, Zafarali Ahmed, James Cobon-Kerr, Aishwarya Kamath, Arnar Mar Hrafnkelsson, Le Hou, Ian Mackinnon, Alexandre Frechette, Eric Noland, Xiance Si, Emanuel Taropa, Dong Li, Phil Crone, Anmol Gulati, Sébastien Cevey, Jonas Adler, Ada Ma, David Silver, Simon Tokumine, Richard Powell, Stephan Lee, Kiran Vodrahalli, Samer Hassan, Diana Mincu, Antoine Yang, Nir Levine, Jenny Brennan, Mingqiu Wang, Sarah Hodkinson, Jeffrey Zhao, Josh Lipschultz, Aedan Pope, Michael B. Chang, Cheng Li, Laurent El Shafey, Michela Paganini, Sholto Douglas, Bernd Bohnet, Fabio Pardo, Seth Odoom, Mihaela Rosca, Cicero Nogueira dos Santos, Kedar Soparkar, Arthur Guez, Tom Hudson, Steven Hansen, Chulayuth Asawaroengchai, Ravi Addanki, Tianhe Yu, Wojciech Stokowiec, Mina Khan, Justin Gilmer, Jaehoon Lee, Carrie Grimes Bostock, Keran Rong, Jonathan Caton, Pedram Pejman, Filip Pavetic, Geoff Brown, Vivek Sharma, Mario Luˇcic, Rajkumar Samuel, Josip Djolonga, Amol Mandhane, Lars Lowe Sjösund, Elena Buchatskaya, Elspeth White, Natalie Clay, Jiepu Jiang, Hyeontaek Lim, Ross Hemsley, Zeyncep Cankara, Jane Labanowski, Nicola De Cao, David Steiner, Sayed Hadi Hashemi, Jacob Austin, Anita Gergely, Tim Blyth, Joe Stanton, Kaushik Shivakumar, Aditya Siddhant, Anders Andreassen, Carlos Araya, Nikhil Sethi, Rakesh Shivanna, Steven Hand, Ankur Bapna, Ali Khodaei, Antoine 13 G1 Miech, Garrett Tanzer, Andy Swing, Shantanu Thakoor, Lora Aroyo, Zhufeng Pan, Zachary Nado, Jakub Sygnowski, Stephanie Winkler, Dian Yu, Mohammad Saleh, Loren Maggiore, Yamini Bansal, Xavier Garcia, Mehran Kazemi, Piyush Patil, Ishita Dasgupta, Iain Barr, Minh Giang, Thais Kagohara, Ivo Danihelka, Amit Marathe, Vladimir Feinberg, Mohamed Elhawaty, Nimesh Ghelani, Dan Horgan, Helen Miller, Lexi Walker, Richard Tanburn, Mukarram Tariq, Disha Shrivastava, Fei Xia, Qingze Wang, Chung-Cheng Chiu, Zoe Ashwood, Khuslen Baatarsukh, Sina Samangooei, Raphaël Lopez Kaufman, Fred Alcober, Axel Stjerngren, Paul Komarek, Katerina Tsihlas, Anudhyan Boral, Ramona Comanescu, Jeremy Chen, Ruibo Liu, Chris Welty, Dawn Bloxwich, Charlie Chen, Yanhua Sun, Fangxiaoyu Feng, Matthew Mauger, Xerxes Dotiwalla, Vincent Hellendoorn, Michael Sharman, Ivy Zheng, Krishna Haridasan, Gabe Barth-Maron, Craig Swanson, Dominika Rogozinska, Alek Andreev, Paul Kishan Rubenstein, Ruoxin Sang, Dan Hurt, Gamaleldin Elsayed, Renshen Wang, Dave Lacey, Anastasija Ilic, Yao Zhao, Adam Iwanicki, Alejandro Lince, Alexander Chen, Christina Lyu, Carl Lebsack, Jordan Griffith, Meenu Gaba, Paramjit Sandhu, Phil Chen, Anna Koop, Ravi Rajwar, Soheil Hassas Yeganeh, Solomon Chang, Rui Zhu, Soroush Radpour, Elnaz Davoodi, Ving Ian Lei, Yang Xu, Daniel Toyama, Constant Segal, Martin Wicke, Hanzhao Lin, Anna Bulanova, Adrià Puigdomènech Badia, Nemanja Rakicevic, Pablo Sprechmann, Angelos Filos, Shaobo Hou, Víctor Campos, Nora Kassner, Devendra Sachan, Meire Fortunato, Chimezie Iwuanyanwu, Vitaly Nikolaev, Balaji Lakshminarayanan, Sadegh Jazayeri, Mani Varadarajan, Chetan Tekur, Doug Fritz, Misha Khalman, David Reitter, Kingshuk Dasgupta, Shourya Sarcar, Tina Ornduff, Javier Snaider, Fantine Huot, Johnson Jia, Rupert Kemp, Nejc Trdin, Anitha Vijayakumar, Lucy Kim, Christof Angermueller, Li Lao, Tianqi Liu, Haibin Zhang, David Engel, Somer Greene, Anaïs White, Jessica Austin, Lilly Taylor, Shereen Ashraf, Dangyi Liu, Maria Georgaki, Irene Cai, Yana Kulizhskaya, Sonam Goenka, Brennan Saeta, Ying Xu, Christian Frank, Dario de Cesare, Brona Robenek, Harry Richardson, Mahmoud Alnahlawi, Christopher Yew, Priya Ponnapalli, Marco Tagliasacchi, Alex Korchemniy, Yelin Kim, Dinghua Li, Bill Rosgen, Kyle Levin, Jeremy Wiesner, Praseem Banzal, Praveen Srinivasan, Hongkun Yu, Çaglar Ünlü, David Reid, Zora Tung, Daniel Finchelstein, Ravin Kumar, Andre Elisseeff, Jin Huang, Ming Zhang, Ricardo Aguilar, Mai Giménez, Jiawei Xia, Olivier Dousse, Willi Gierke, Damion Yates, Komal Jalan, Lu Li, Eri Latorre-Chimoto, Duc Dung Nguyen, Ken Durden, Praveen Kallakuri, Yaxin Liu, Matthew Johnson, Tomy Tsai, Alice Talbert, Jasmine Liu, Alexander Neitz, Chen Elkind, Marco Selvi, Mimi Jasarevic, Livio Baldini Soares, Albert Cui, Pidong Wang, Alek Wenjiao Wang, Xinyu Ye, Krystal Kallarackal, Lucia Loher, Hoi Lam, Josef Broder, Dan Holtmann-Rice, Nina Martin, Bramandia Ramadhana, Mrinal Shukla, Sujoy Basu, Abhi Mohan, Nick Fernando, Noah Fiedel, Kim Paterson, Hui Li, Ankush Garg, Jane Park, DongHyun Choi, Diane Wu, Sankalp Singh, Zhishuai Zhang, Amir Globerson, Lily Yu, John Carpenter, Félix de Chaumont Quitry, Carey Radebaugh, Chu-Cheng Lin, Alex Tudor, Prakash Shroff, Drew Garmon, Dayou Du, Neera Vats, Han Lu, Shariq Iqbal, Alex Yakubovich, Nilesh Tripuraneni, James Manyika, Haroon Qureshi, Nan Hua, Christel Ngani, Maria Abi Raad, Hannah Forbes, Jeff Stanway, Mukund Sundararajan, Victor Ungureanu, Colton Bishop, Yunjie Li, Balaji Venkatraman, Bo Li, Chloe Thornton, Salvatore Scellato, Nishesh Gupta, Yicheng Wang, Ian Tenney, Xihui Wu, Ashish Shenoy, Gabriel Carvajal, Diana Gage Wright, Ben Bariach, Zhuyun Xiao, Peter Hawkins, Sid Dalmia, Clement Farabet, Pedro Valenzuela, Quan Yuan, Ananth Agarwal, Mia Chen, Wooyeol Kim, Brice Hulse, Nandita Dukkipati, Adam Paszke, Andrew Bolt, Kiam Choo, Jennifer Beattie, Jennifer Prendki, Harsha Vashisht, Rebeca Santamaria-Fernandez, Luis C. Cobo, Jarek Wilkiewicz, David Madras, Ali Elqursh, Grant Uy, Kevin Ramirez, Matt Harvey, Tyler Liechty, Heiga Zen, Jeff Seibert, Clara Huiyi Hu, Andrey Khorlin, Maigo Le, Asaf Aharoni, Megan Li, Lily Wang, Sandeep Kumar, Norman Casagrande, Jay Hoover, Dalia El Badawy, David Soergel, Denis Vnukov, Matt Miecnikowski, Jiri Simsa, Praveen Kumar, Thibault Sellam, Daniel Vlasic, Samira Daruki, Nir Shabat, John Zhang, Guolong Su, Jiageng Zhang, Jeremiah Liu, Yi Sun, Evan Palmer, Alireza Ghaffarkhah, Xi Xiong, Victor Cotruta, Michael Fink, Lucas Dixon, Ashwin Sreevatsa, Adrian Goedeckemeyer, Alek Dimitriev, Mohsen Jafari, Remi Crocker, Nicholas FitzGerald, Aviral Kumar, Sanjay Ghemawat, Ivan Philips, Frederick Liu, Yannie Liang, Rachel Sterneck, Alena Repina, Marcus Wu, Laura Knight, Marin Georgiev, Hyo Lee, Harry Askham, Abhishek Chakladar, Annie Louis, Carl Crous, Hardie Cate, Dessie Petrova, Michael Quinn, Denese Owusu-Afriyie, Achintya Singhal, Nan Wei, Solomon Kim, Damien Vincent, Milad Nasr, Christopher A. Choquette-Choo, Reiko Tojo, Shawn Lu, Diego de Las Casas, Yuchung Cheng, Tolga Bolukbasi, Katherine Lee, Saaber Fatehi, Rajagopal Ananthanarayanan, Miteyan Patel, Charbel Kaed, Jing Li, Shreyas Rammohan Belle, Zhe Chen, Jaclyn Konzelmann, Siim Põder, Roopal Garg, Vinod Koverkathu, Adam Brown, Chris Dyer, Rosanne Liu, Azade Nova, Jun Xu, Alanna Walton, Alicia Parrish, Mark Epstein, Sara McCarthy, Slav Petrov, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. [31] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [32] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-VL technical report, 2025. [33] Mark Towers, Ariel Kwiatkowski, Jordan Terry, John U. Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulão, Andreas Kallinteris, Markus Krimmel, Arjun KG, Rodrigo Perez-Vicente, Andrea Pierré, Sander Schulhoff, Jun Jet Tai, Hannah Tan, and Omar G. Younis. Gymnasium: standard interface for reinforcement learning environments, 2024. G1 [34] Chen Feng Tsai, Xiaochen Zhou, Sierra Liu, Jing Li, Mo Yu, and Hongyuan Mei. Can large language models play text games well? current state-of-the-art and open questions. arXiv preprint arXiv:2304.02868, 2023. [35] Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players?, 2025. [36] Xinyu Wang, Bohan Zhuang, and Qi Wu. Are large vision language models good game players? arXiv preprint arXiv:2503.02358, 2025. [37] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023. [38] Nicholas R. Waytowich, Devin White, MD Sunbeam, and Vinicius G. Goecks. Atari-gpt: Benchmarking multimodal large language models as low-level policies in atari games, 2024. [39] Yue Wu, Xuan Tang, Tom Mitchell, and Yuanzhi Li. Smartplay: benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557, 2023. [40] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, and Wei Chen. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [41] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [42] Yaowei Zheng, Shenzhi Wang Junting Lu, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [43] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. [44] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros \"aha moment\" in visual reasoning on 2b non-sft model, 2025. 15 Perception-Enhanced Cold Start Data Construction G1 Figure 9: The process of constructing perception-enhanced cold start data via knowledge distillation and programmable environments. G0 2048 Case Studies G1 Figure 10: G0 2048 case studies. The case shows that before RL training, the model produced incorrect perception and reasoning outputs, yet still received positive rewards, which encouraged the policy to adopt these flawed behaviors."
        },
        {
            "title": "C Details of Games",
            "content": "G1 (a) Correct Action (b) Wrong Action Figure 11: Rules of 2048. C.1 Rule In the 2048 game, the player swipes the entire 44 grid up, down, left, or right. All tiles slide as far as possible in the chosen direction, and any two tiles of equal value that collide merge immediately into single tile whose value is their sum. After swipe that produces at least one mergeas in Fig. 11a, where two 2 tiles combine into 4 and new tile spawnsthe move is accepted and fresh 2 (or occasionally 4) appears in random empty cell. By contrast, swipe that produces no mergesas shown in Fig. 11b, where downward swipe only shifts tiles without combining themcounts as failed move: original 2 tile remains 2 and new 2 tile is added. Model Play The model takes 44 grid (representing the current 2048 game board) as input. After evaluating all possible moves, it outputs single integer representing the chosen swipe direction: 0 for Up, 1 for Right, 2 for Down, or 3 for Left. Game Prompt Here is the game prompt of 2048 for models: Game Prompt for 2048 Game Rule Description You are now playing the 2048 game. 2048 is sliding tile puzzle game where you combine numbered tiles to create tile with the value 2048. Only Tiles with the SAME number merge when they collide. After each move, new tile (2 or 4) appears randomly on the board. The game ends when there are no more valid moves. Available actions: - (0): Up (slide all tiles upward) - (1): Right (slide all tiles to the right) - (2): Down (slide all tiles downward) - (3): Left (slide all tiles to the left) What action should you take to achieve the highest score and reach the 2048 tile? Output Format Description First describe the board in <perception></perception>. Then output your thinking process in <think></think> and final action in <answer></answer>. C.2 Shisen-Sho and Shisen-Sho-Cifar10 Rule In Shisen-Sho, the player clears an 88 grid by removing tiles in matching pairs: pair may be removed only if the two tiles are identical in both shape and color and if there exists connecting path between them that runs orthogonally (up, down, left, right), makes at most two 90 turns, and passes through no other tiles (see Fig. 13a). For instance, two green circles can be cleared when linked around an empty corridor. If the shapes or colors differ, or if 18 G1 (a) Correct Action (b) Wrong Action Figure 12: Rules of Shisen-Sho. (a) Correct Action (b) Wrong Action Figure 13: Rules of Shisen-Sho-CIFAR10. no unobstructed path with 2 turns exists (Fig. 13b), the move fails and the board remains unchanged. As shown in Fig. 13, the same rules apply in the Shisen-Sho-CIFAR10 variant, except that each tile displays CIFAR-10 image: valid match must have the same image class (e.g. cat, airplane). Model Play The model receives an 88 grid as input, where cells contain colored shapes or CIFAR-10 images. It outputs the coordinates of matching pair(same shape with same color, or same CIFAR class) that can be connected by an orthogonal path with at most two 90 turns and no occupied cells in between, in the format \"(row1,col1) (row2,col2)\". Game Prompt Here is the game prompt of Shisen-Sho and Shisen-Sho-Cifar10 for models: Game Prompt for Shisen-Sho Game Rule Description You are playing Shisen-sho puzzle game. The objective is to match pairs of identical tiles by connecting them with path that has at most 2 turns and doesnt cross any other tiles. The tiles are distinguished by their color and shape: - Color include: Red, Green, Blue, Yellow, Magenta, Cyan, etc. - Shapes include: circle, square, triangle, diamond, cross, star, etc. Please analyze the game board and identify two matching tiles that can be connected according to these rules. Return your answer as follows: 1. First coordinate: (row1, col1) 2. Second coordinate: (row2, col2) Where row and col are 0-indexed numbers such as (0, 1), starting from the top-left of the board. Output Format Description First describe the board in <perception></perception>. Then output your thinking process in <think></think> and final action in <answer>(row1, col1) (row2, col2)</answer>. G1 Game Prompt for Shisen-Sho-Cifar10 Game Rule Description You are playing Shisen-sho puzzle game that uses CIFAR-10 images. Each tile on the board corresponds to one of the CIFAR-10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The objective is to find pair of tiles that belong to the same class and can be connected with path that does not cross any other tiles and makes at most two turns. Please analyze the game board and identify two matching tiles that can be connected according to these rules. Return your answer as follows: 1. First coordinate: (row1, col1) 2. Second coordinate: (row2, col2) Where row and col are 0-indexed numbers such as (0, 1), starting from the top-left of the board. Output Format Description First describe the board in <perception></perception>. Then output your thinking process in <think></think> and final action in <answer>(row1, col1) (row2, col2)</answer>. C.3 Swap Rule The player may select any two shapes that share side (i.e. are orthogonally adjacent) and swap thembut only if doing so immediately creates at least one straight line (horizontal or vertical) of three or more identical shapes. For example, in Fig. 14a, swapping the red circle with its adjacent blue circle produces horizontal row of three blue circles, so the move succeeds. By contrast, in Fig. 14b, swapping the yellow circle with the adjacent yellow triangle does not form any three-in-a-row; this swap therefore fails, and the two shapes revert to their original positions. Model Play The model receives an mn board filled with colored shapes as input. It outputs the coordinates of the first valid adjacent tile swap that creates match-3 or longer, in the format \"(row1,col1) (row2,col2)\". Game Prompt Here is the game prompt of Swap for models: Game Prompt for Swap Game Rule Description You are playing Swap Game where you need to swap adjacent tiles to create matches of 3 or more identical tiles. - Tiles are identified by color (Red, Green, Blue, Yellow) and shape (circle, square, triangle) - You can only swap adjacent tiles (not diagonal) - valid move must create at least one match of 3 or more identical tiles - After matches are removed, tiles above will fall down and new tiles will appear at the top - If no valid moves are available, the board will automatically be shuffled - The game ends when you run out of moves Please analyze the game board and identify two adjacent tiles to swap that will create match. Return your answer as follows: 1. First coordinate: (row1, col1) 2. Second coordinate: (row2, col2) Where row and col are 0-indexed numbers starting from the top-left of the board. Output Format Description First describe the board in <perception></perception>. Then output your thinking process in <think></think> and final action in <answer>(row1, col1) (row2, col2)</answer>. G1 (a) Correct Action (b) Wrong Action Figure 14: Rules of Swap."
        }
    ],
    "affiliations": [
        "Moonshot AI",
        "Peking University",
        "UCAS"
    ]
}