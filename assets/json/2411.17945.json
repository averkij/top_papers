{
    "paper_title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
    "authors": [
        "Sankalp Sinha",
        "Mohammad Sadil Khan",
        "Muhammad Usama",
        "Shino Sam",
        "Didier Stricker",
        "Sk Aziz Ali",
        "Muhammad Zeshan Afzal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating high-fidelity 3D content from text prompts remains a significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL-40M+, an extensive dataset with 40 million text annotations for over 8.9 million 3D assets aggregated from seven major 3D datasets. Our contribution is a novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi-level descriptions, ranging from detailed (150-200 words) to concise semantic tags (10-20 words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domain-specific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, a two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use a pretrained image-to-3D network to generate 3D textured meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of 72.41% by GPT-4 and 73.40% by human evaluators."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 5 4 9 7 1 . 1 1 4 2 : r MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation Sankalp Sinha1 Mohammad Sadil Khan1 Muhammad Usama1 Shino Sam1 Didier Stricker1 Sk Aziz Ali2 Muhammad Zeshan Afzal1 1,2DFKI 1RPTU Kaiserslautern-Landau 1MindGarage 2BITS Pilani, Hyderabad sankalp.sinha@dfki.de mohammad.khan@dfki.de Figure 1. Left: Examples of MARVEL annotations created using our proposed pipeline, which produces high-quality, domain-specific and multi-level text descriptions for 3D assets (Sec 3.1). Right: Qualitative results from MARVEL-FX3D, our two-stage text-to-3D pipeline, which can generate textured mesh from text within 15s (Sec 3.3). Please zoom in for details."
        },
        {
            "title": "Abstract",
            "content": "Generating high-fidelity 3D content from text prompts remains significant challenge in computer vision due to the limited size, diversity, and annotation depth of the existing datasets. To address this, we introduce MARVEL40M+, an extensive dataset with 40 million text annotations for over 8.9 million 3D assets aggregated from seven major 3D datasets. Our contribution is novel multi-stage annotation pipeline that integrates open-source pretrained multi-view VLMs and LLMs to automatically produce multi- *Equally contributing first authors. Corresponding Author. level descriptions, ranging from detailed (150-200 words) to concise semantic tags (10-20 words). This structure supports both fine-grained 3D reconstruction and rapid prototyping. Furthermore, we incorporate human metadata from source datasets into our annotation pipeline to add domainspecific information in our annotation and reduce VLM hallucinations. Additionally, we develop MARVEL-FX3D, two-stage text-to-3D pipeline. We fine-tune Stable Diffusion with our annotations and use pretrained image-to-3D network to generate 3D textured meshes within 15s. Extensive evaluations show that MARVEL-40M+ significantly outperforms existing datasets in annotation quality and linguistic diversity, achieving win rates of 72.41% by GPT-4 and 1 73.40% by human evaluators. 1. Introduction Text-to-3D (TT3D) content generation has emerged as pivotal area in computer graphics, vision, and AI, enabling the creation of complex 3D objects from textual prompts [32, 38, 62] by understanding the shape, material properties [71, 89], and complex visual elaborations [36, 77, 90]. This technology holds significant potential for various industries, including gaming, augmented reality (AR), virtual reality (VR), and film production [32, 38]. Recent advancements in text-to-image (TTI) synthesis [3, 21, 67, 69] have achieved remarkable realism and precise control over visual effects [19, 65, 67]. However, extending these capabilities to high-fidelity TT3D generation remains significant challenge [22, 32, 38, 90]. This is due to the intricate nature of modeling 3D shapes [35, 36, 44, 76, 89], textures [43, 44, 77], colors [71, 89] and spatial relationships [22, 90] from text descriptions, challenge further amplified by the scarcity of high-quality 3D captions. Current TT3D datasets like CAP3D [53], 3D-Topia [28], CLAY [89] and Kabra et al [34] attempt to bridge this gap through automated annotations but often fall short due to their reliance on single-view VLMs [11, 39, 45, 46] or GPT4 [60] for caption generation. This approach frequently results in contradictory or inconsistent captions [34, 54]. Moreover, the captions lack the necessary details for finegrained 3D reconstruction. Additionally, their dependence on proprietary models like GPT-4 [60] introduces significant scalability and cost constraints. Manual annotation is also impractical for large-scale datasets like Objaverse [18] and Objaverse-XL [17]. These datasets contain diverse range of 3D modelsfrom characters and biological elements to historical artifacts and complex ambiguous structuresrequiring domain-specific expertise for accurate annotation (See Figure 1 - Left). Beyond being timeconsuming and expensive, CAP3D [53] has shown that human-generated captions may not necessarily surpass automated methods in quality. To address the previously mentioned challenges, we introduce MARVEL(Multi-Level Visual ELAboRation), an automated and scalable 3D captioning pipeline. Our approach combines state-of-the-art multi-view VLM InternVL2 [13, 15] and Qwen 2.5 LLM [85] to generate high-quality captions for over 8.9 million 3D models across seven datasets [10, 1618, 20, 73, 74, 80]. To ensure domain specific information into our captions and reduce VLM hallucinations, we integrate human metadata from source datasets into our pipeline. Following [12, 92], we identify five key aspects for fine-grained 3D reconstruction: object names and components, shape and geometry, texture and materials, colors, and contextual environments. Our pipeline progressively compresses these aspects to generate five levels of annotations, ranging from comprehensive descriptions (200 words) for fine-grained 3D reconstruction to concise tags (10 words) for quick modeling, resulting in 40+ million annotations. Our pipeline addresses three fundamental challenges in 3D captioning - detail, accuracy, and scalability. Through comprehensive experimental analysis, we show that MARVEL-40M+ has superior annotation quality, information density, and linguistic diversity compared to other methods [28, 34, 53]. To showcase the application of our dataset, we introduce MARVEL-FX3D (Fast eXecution for 3D), two-stage In pipeline designed for high-fidelity TT3D generation. the first stage, we fine-tune Stable Diffusion (SD) 3.5 [3] with our annotations to improve its capability to produce images for suitable 3D reconstruction. In the second stage, we leverage the pre-trained Stable Fast 3D (SF3D) [7] for rapid image-to-3D conversion. This enables the creation of textured meshes from texts within 15s . Our approach is inspired by multi-stage TT3D pipelines [41, 58, 71], promising direction [32, 38, 41] that addresses the limitation of existing Score Distillation Sampling (SDS)[62]-based methods like janus problem [43, 44, 62, 77], oversaturation [43], and lengthy per-prompt optimization [43, 44, 62, 76, 77, 91]. Our experiments demonstrate that MARVEL-FX3D outperforms current state-of-the-art TT3D methods in terms of prompt fidelity and overall preference. Our contributions can be summarized as follows: 1. We present MARVEL, an automated, scalable annotation pipeline for generating high-quality 3D captions. To the best of our knowledge, MARVEL-40M+ is the largest 3D caption dataset to date. 2. We propose multi-level annotation structure that spans from detailed descriptions for fine-grained 3D reconstruction to concise tags for quick modeling. 3. We incorporate human metadata from source datasets into our pipeline to inject domain-specific information in the text descriptions and reduce VLM hallucinations. 4. As downstream task, we introduce MARVEL-FX3D, two-stage framework for high-fidelity TT3D generation. 5. Thorough experiments demonstrate that MARVEL40M+ achieves state-of-the-art performances in linguistic diversity, image-text alignment, caption accuracy, and high-fidelity TT3D generation. 2. Related Work 3D-Text Data: 3D datasets such as ShapeNet [10], Objaverse [17, 18], and Omniobject3D [80] have played crucial role in advancing 3D understanding tasks such as single [29, 48, 49, 84] or multi-view [70, 86] 3D reconstruction, multi-view consistent image generation [27, 50, 87], and 3D object synthesis [31, 89]. However, they often lack 2 Cap3D [53] 3DTopia [28] Kabra [34] MARVEL ShapeNet 51,209 Pix3D OmniObject3D Toys4K GSO 735 4,000 5,878 1,030 ABO 6,400 [52] 7, Objaverse Objaverse-XL Total 3D Objects Total Captions 785,150 361,357 763,827 798,759 1,013,182 361,357 763,827 44,506,005 1,013,182 361,357 763,827 8,901,201 221,632 8,031,637 Table 1. Overview of datasets [10, 1618, 20, 73, 74, 80] annotated using our MARVEL pipeline. MARVEL provides the most extensive 3D asset annotations to date, encompassing over 8.9M 3D objects and 40M captions. meaningful language descriptions, with available metadata being either noisy or inadequate [47, 53]. This language-3D gap has been major bottleneck in developing high-fidelity TT3D models [28, 34, 53]. Recent works like CAP3D [53] addresses this by proposing an automated pipeline. It starts with BLIP [40] for single-view captioning of 3D assets followed by refinement using CLIP [64] and caption aggregation by GPT-4 [60]. Subsequent works, Kabra et al.[34] introduced ScoreAgg and PaLI-X[11] to improve caption accuracy, while 3D-Topia [28] explored an alternative path with LLaVA [45, 46] and GPT-3.5. CLAY [89] took more direct approach, leveraging GPT-4 [60] for multi-view caption generation. Yet, all these approaches face inherent trade-offs. Single-view VLM approaches [28, 34, 53, 54] often produce incomplete or inaccurate annotations [34, 54] for 3D models, while GPT-4-based methods [28, 53, 60, 89] struggle with scalability and cost[4]. Our work presents novel solution to these challenges through three key innovations. First, we leverage open-source multi-view VLM InternVL2 [13, 15] and Qwen 2.5 LLM [85], achieving GPT-4 [60] comparable performance [2, 13, 15, 85] without its scalability and cost constraints. Second, instead of discarding human metadata from source datasets as done in previous works [28, 34, 53, 89], we recognize its value as domain-specific prior knowledge. We incorporate filtered versions of this metadata into our pipeline to inject relevant context and reduce VLM hallucinations. Finally, we introduce hierarchical annotation framework with five distinct levels, ranging from detailed descriptions to abstract tags. This multi-level approach represents significant departure from existing methods [28, 34, 53, 89], which typically provide only single-level annotations. Text-to-3D: Current TT3D methodologies can be broadly categorized into two main approaches. One prominent direction is based on the seminal work of DreamFusion [62], which introduced Score Distillation Sampling (SDS) to learn NeRF [57] representation by leveraging information from pretrained TTI models [5, 67, 69]. Subsequent studies have advanced this framework by improving training stability [43, 77],increasing output diversity [43, 77, 91] and geometry extraction [14, 44, 76, 88]. However, SDSbased methods face two key challenges: geometric inconsistencies known as the Janus problem [32, 37, 38] and slow optimization times. This issue has been partially addressed using amortization efforts [51, 82]. The second group of methods consists of multistage pipelines [24, 33, 41, 42, 58, 71]. The goal is to generate single or multiview images from TTI model [3, 5, 21, 67, 69], followed by view reconstruction into various 3D representations [7, 24, 29, 41, 57, 70]. These methods often fine-tune the TTI [3, 5, 21, 67, 69] models on TT3D datasets [34, 53] to align the output image with reconstruction needs. PointE [58] fine-tunes GLIDE [59] for TTI synthesis and uses point diffusion transformer for 3D point cloud generation. Instant3D [41] fine-tunes SD [67] to produce 2 2 grid of multi-view images and uses LRM [29] for 3D Gaussian reconstruction. AssetGen [71] extends LRM towards high-quality 3D meshes with detailed textures and PBR materials. Our dataset, MARVEL-40M+, is uniquely positioned to advance this domain by providing comprehensive, high-quality, and domain-specific text annotations that bridge the gap between TTI generation and image-to-3D reconstruction. By fine-tuning on MARVEL-40M+, we develop MARVEL-FX3D, which demonstrates better performance for high-fidelity TT3D generation compared to existing state-of-the-art methods. 3. Methodology 3.1. Multi-Stage Annotation Pipeline We now present our proposed MARVEL annotation pipeline, shown in Figure 2 (left). Our goal is to generate detailed and domain-specific captions, for both finegrained and abstract 3D modeling cases. Through carefully designed five-stage process, MARVEL produces hierarchy of information-rich and domain-specific annotations. These annotations range from detailed descriptions of object names, shapes, textures, and contextual relationships to concise summaries. Starting with multi-view rendering, our pipeline processes each asset through sequential stages of human metadata refinement, dense description generation via InternVL2 [15], multi-level elaboration using Qwen 2.5 [85], and ethical filtering. Below, we detail each component of our pipeline. Multi-View Rendering: We first generate 4 multi-view images of resolution 512 512 for each 3D model using Blender [1]. We rotate the camera around the object with azimuth angle, θ = { πi 2 }i=4 i=1 and fixed elevation angle, ϕ = 60. The camera distance is set to 1.5. The four images correspond to the front, back, left, and right sides of the 3D model. Unlike existing 3D captioning pipelines [28, 53], we focus solely on these standard viewpoints. This method 3 Figure 2. Left: MARVEL annotation pipeline for 3D assets. Our pipeline starts with human metadata [17, 18] and rendered multi-view images to create detailed visual descriptions using InternVL-2 [13]. These contain object names, shapes, textures, colors, and environments. Qwen2 [85] then processes these descriptions into five hierarchical levels, progressively compressing different aspects of the 3D assets. Right: Our Text-to-3D pipeline finetunes SD 3.5 [3, 21] with this dataset and uses pretrained SF3D [7] to generate textured mesh in 15s . aligns with recent studies [68, 79], which demonstrate that VLMs perform better on images from these viewpoints. texture) and semantic properties (e.g. Human Metadata Filtering: High-quality 3D annotation requires capturing both visual characteristics (e.g. shape, color, domainspecific nomenclature and object identification). This dual focus ensures that descriptions are not only visually precise but also contextually relevant within specific domains. significant challenge in this process is the tendency of pretrained VLMs [40] to hallucinate when dealing with complex datasets, such as Objaverse [17, 18], due to the inherent 2D-3D domain gap [34]. To address this, we use the user-generated metadata from source datasets, which provides valuable domain-specific names and descriptions that can guide VLMs [13, 15] toward generating more precise and informative annotations. However, this metadata often contains noise, including personalized or sensitive information [17, 18], which can compromise annotation quality. To mitigate this, we use Mistral-Nemo-Instruct-2407 [72] to filter the metadata, removing random, redundant, and sensitive content to ensure that only information relevant to 3D attributes is passed to the annotation pipeline. It is worth noting that our pipeline functions independently of human metadata, with it serving purely as an optional enhancement to add domain-specific terminology in the captions. Dense Description Generation: InternVL2 [13, 15] processes the 4 rendered multi-view images along with our metadata-augmented prompt to genIn this stage, erate dense description of the 3D models. This description contains several key requirements for fine-grained 3D (1) structural decomposition with model reconstruction: object identification and relative positions, (2) geometric properties, analyzing shape characteristics, symmetry axes, and proportional relationships, (3) surface characteristics, addressing texture and material properties and tactile qualities such as roughness and reflectivity; (4) chromatic analysis, mapping colors across primary objects and subcomponents, including patterns and transitions (5) environmental context, capturing spatial relationships and its interaction with other elements. To efficiently scale this process for large-scale annotation, we select InternVL2-40B [13, 15] for its balance of speed, accuracy, and prompt adherence. Recent studies show that InternVL2-40B [13, 15] performs comparably [2, 13, 15] to GPT-4o [60] with significantly lower annotation cost*. Multi-Level Visual Elaboration: This stage focuses on generating multi-level visual elaborations using Qwen2.572B [85] by compressing different aspects of 3D reconstructions at varying levels of detail. This hierarchical approach allows for flexible and adaptive 3D modeling outputs optimized for different use cases, such as scenarios where only key detailslike object name and colorsare specified, but texture is excluded or where simplified semantic tags is necessary for rapid prototyping. While direct prompting method will be to specify which aspects *InternVL2-40B [13, 15] ranked third on the Huggingface Open-VLM Leaderboard [2] during our project. to compress, we found that this strategy often constrains the models ability to create rich and meaningful captions, aligning with findings from recent studies [75, 78]. To overcome these challenges, we develop hierarchical prompting strategy that specifies the essential content for each level of elaboration, balancing detail and brevity. Below, we describe each level: 1. Comprehensive Description (Level 1): detailed description covering all aspects of the 3D model, including precise geometric specifications, materials, spatial relationships, and structural details, in 150-200 words. 2. Moderately Descriptive (Level 2): Description of the models primary structures, components, and key geometric features. This level focuses on the overall shape and main features of the model in 100-150 words. 3. Functional-Semantic Description (Level 3): Basic description about the models functional aspects, general form, and primary characteristics in 50-100 words. 4. Summary (Level 4): brief description of the object, highlighting its basic form, purpose, and most notable features, similar to existing datasets in 30 words. 5. Concise tags (Level 5): list of distinct concepts of the 3D model for rapid 3D modeling in 10-20 words. An example of multi-level visual elaboration is illustrated in Figure 2 (left), where Qwen 2.5 [85] compresses texture information progressively from Level 1 to Level 4. By Level 5, the output shifts to concise format, highlighted with colored words representing key semantic tags and core attributes. To assess the effectiveness of this hierarchical compression, we conduct an ablation study in Section 4.3 B, measuring how well semantic information is retained across all the levels. Ethical Filtering: Given the diverse metadata sources in our annotation pipeline, there is risk of ethically problematic content being included in the multi-level descriptions. To mitigate this, we use the Qwen 2.5-14B [85] model with targeted prompt for ethical filtering. This prompt removes meaningless or offensive words, personal names (unless they are famous or contextually relevant), and overly specific identifiers. Importantly, it retains well-known terms, scientific and cultural references, preserving valuable context. This filtering step maintains annotation quality, prevents the leakage of sensitive or inappropriate information, and upholds the integrity of the dataset. For details, please refer to the supplementary material. 3.2. Caption Generation Datasets: We aggregate 8.9M 3D assets from seven diverse sources [10, 1618, 20, 73, 74, 80]. For human metadata injection, we use the name, tags and description from Objaverse 1.0 [18] (Sketchfab) and metadata from Objaverse-XL [18] (thingiverse and github). Samples from Objaverse-XL [17] containing the file extension .ply were excluded from the dataset. For the rest of the six datasets [10, 16, 20, 73, 74, 80], we use the class categories as metadata. Any samples that do not have renderable multi-views or lack textual information post-annotation are removed from the dataset. The final details of the dataset are provided in Table 1, with additional preparation information available in the supplementary materials. Implementation Details: Our MARVEL annotation pipeline is optimized for large-scale processing, achieving throughput of 24, 000 samples per day. For human metadata filtering, we run the Mistral-Nemo-Instruct-2407 [72] on single NVIDIA RTX 4090 GPU. Both InternVL240B [13, 15] for dense description generation and the Qwen2.5-72B [85] with 8-bit quantization for multi-level visual elaboration, runs on single NVIDIA H100 GPU. For the final ethical filtering stage, we run Qwen 2.514B [85] on single NVIDIA RTX A6000 GPU. For complete details including hyperparameter details, please refer to the supplementary material. 3.3. MARVEL-FX3D Architecture In this section, we present MARVEL-FX3D, two-stage pipeline that demonstrates the practicality of the MARVEL40M+ dataset for TT3D synthesis. By leveraging our datasets comprehensive text descriptions and diverse 3D assets [18], MARVEL-FX3D generates high-quality textured 3D meshes from text descriptions that can specify multiple objects, scenes, geometric properties, colors, and textures. The pipeline consists of (1) TTI generation using fine-tuned Stable Diffusion [21], followed by (2) singleview 3D reconstruction with pretrained view reconstruction model [7]. This entire process generates high quality 3D assets in 15s , as illustrated in Figure 2 (right). Fine-Tuning TTI Model: The objective of this stage is to generate high-quality, diverse images from text prompts that can be effectively converted into 3D textured meshes using pretrained image-to-3D methods [7, 83]. primary challenge in multi-stage TT3D pipelines [41, 42, 55, 58, 71] is the inherent 2D-3D domain gap, where reconstructing accurate and geometrically consistent 3D models from 2D images is hindered by the ambiguity between background and foreground information [42, 55]. To address this, some methods have fine-tuned TTI [5, 67] models on TT3D datasets [28, 34, 53, 89]. Following this approach, we finetune Stable Diffusion 3.5 [3, 21] using the LORA [30] strategy to bridge this domain gap and generate images similar to the training distribution of the image-to-3D methods [7]. In the second stage, Image-to-3D Generation: the background is removed from the generated image using DIS [63]. The refined image is then processed by pretrained SF3D [7] to generate high-quality textured mesh within 5s. Further details can be found in the supplementary. 5 Figure 3. Qualitative Annotation Comparison: From top to bottom Cap3D [53], 3DTopia [28], Kabra [34], MARVEL (Level-4) annotations and GPT-4 [60] evaluation. MARVEL consistently provides the most comprehensive and precise annotations, capturing intricate details such as object names, color, structure, and specific attributes. Red is for wrong captions. 4. Experiment The experiment section is divided into two parts. In Sec.4.1, we evaluate the quality of our annotations in comparison to the baseline datasets [28, 34, 53]. While Sec. 4.2 presents the performance evaluation of MARVEL-FX3D against current state-of-the-art methods [33, 43, 62, 91]. Both experiments are conducted on Objaverse [18] dataset. 4.1. Annotation Evaluation Experimental Setup and Metrics: We assess annotation quality through (1) Linguistic Assessment, (2) Image-Text Alignment, and (3) Caption Accuracy. The linguistic assessment evaluates annotation richness and diversity using the Measure of Textual Lexical Diversity (MTLD) [56] and N-gram analysis [8]. The MTLD metric calculates the average segment length at which the typetoken ratio (TTR) drops below threshold (typically 0.72), with higher MTLD scores indicating more diverse annotations. We randomly select 50K annotations for analysis. Image-text alignment is measured using both GPT-4 [60] and human evaluators who review four multi-view images of each 3D model and select the best-matching caption. 5, 000 samples are evaluated using GPT-4 and 1, 000 samples by five human reviewers with each assigned 200 samples. Level 4 annotations from MARVEL-40M+ are used for fair comparison due to their similar average length to baseline datasets [28, 34, 53] as shown in Table 2. Caption accuracy is separately assessed, where GPT-4 and human reviewers evaluate whether all the 3D attributes mentioned in the captions accurately correspond to the 3D models using four multi-view images. For MARVEL40M+, Level 1 annotations are used, which are detailed and form the foundation for subsequent levels. GPT-4 evaluates 1, 000 samples, while human reviewers assess 250 samples due to the evaluations time demands. More details are provided in the supplementary section. Dataset Cap3D [53] 3D-Topia [28] Kabra [34] MARVEL (Level 4) Average Length 16 29 5 MTLD [56] (@50K) 39.71 41.43 25.85 47.43 Unigram (@50K) 15,189 10,329 3,862 27,659 Bi-Gram (@50K) 123,071 95,856 19,753 239,052 GPT4 (@5K) 14.55 10.80 2.24 72.41 Human (@1K) 9.50 14.00 3.10 73.40 Table 2. Quantitative comparison of annotation quality across datasets. MARVEL surpasses existing datasets [28, 34, 53] in all metrics, showcasing superior linguistic diversity, vocabulary coverage, and significantly higher ratings from GPT-4 and humans. Linguistic Assessment: Table 2 (left) shows the MTLD [56] score and N-Gram analysis [8]. MARVEL demonstrates notable improvement, achieving an MTLD score approximately 83% higher than Kabra [34] 19% higher than Cap3D [28] and 14% higher than 3D-Topia[53] signifying richer caption diversity. In addition, MARVEL shows significantly higher unigram vocabulary size, surpassing Kabra [34], Cap3D [53] and 3D-Topia [28] by factors of approximately 7.1, 1.8 and 2.6 respectively. The trend extends to bigram analysis and average word length as well, showing MARVELs superior linguistic diversity and information density. Figure 3 also illustrates that MARVELs annotations contain more unique words, particularly focusing on object names, colors, textures, and attributes. Image-Text Alignment: As shown in Table 2, MARVEL achieves notably higher ratings in image-text alignment, with win rates of 72.41% from GPT-4 [81] and 73.40% from human evaluators, outperforming prior methods. This reflects MARVELs superior alignment of captions with 3D models. Figure 3 highlights this through examples, showing that MARVELs detailed descriptions capture nuances, such as the flowing robes and book-holding posture of historical statue, even at Level 4. In contrast, baselines [28, 34, 53] produce simpler, more generic descriptions. MARVELs integration of filtered humanannotated metadata further enhances the identification of complex, domain-specific entities like Monument of Dante 6 90 : 5 : 5 ratio. Fine-tuning is conducted in half-precision for 5 epochs with batch size of 8, using single NVIDIA H100 GPU, with LoRA [30] rank and alpha set to 4. Further details are provided in the supplementary section. Baselines: To evaluate MARVEL-FX3Ds performance in high-fidelity TT3D generation, we compare it with Shap-E [33], Dreamfusion [62], Luciddreamer [43], and HIFA [91]. We use the official implementations and pretrained models for Shap-E and Luciddreamer, training the latter for 3k steps. Dreamfusion and HIFA are trained using the open-source threestudio [23] implementation, with 10k and 24k steps, respectively, under default settings. Due to the slower optimization of Dreamfusion, HIFA, and Luciddreamer, we limit comparisons to 50 randomly selected samples from the Objaverse [18] test set. Instant3D [41] and Assetgen [71] are excluded due to unavailable code. User Study: We conducted human evaluation to assess the geometric consistency, visual quality, prompt fidelity, and overall preference of reconstructed 3D assets. Geometric consistency measures realism and physical plausibility, identifying issues like the janus problem. Prompt fidelity evaluates alignment with input text, while visual quality considers aesthetic elements such as colors and textures. Five users were presented with the text prompt and videos of the rendered 3D assets generated by the baseline methods and MARVEL-FX3D. The users scored each asset separately from 1 to 10 based on these criteria, and the final scores were averaged across all users. Shap-E [33] DreamFusion [62] HiFA [91] Lucid-Dreamer [43] MARVEL-FX3D Time 5s 30m >1h 45m 15s Geometric Consistency 3.31 0.71 4.88 0.47 6.59 0.57 7.25 0.60 7.20 0.91 Visual Quality 2.25 0.43 3.74 0.80 6.42 0.26 6.47 1.24 6.58 0.86 Prompt Fidelity 2.65 0.51 4.22 0.79 6.88 0.46 6.62 1.37 7.71 0.68 Overall 2.41 0.50 4.09 0.81 6.44 0.35 6.59 0.86 6.94 0. Table 4. Quantitative evaluation focusing on time and human evaluation criteria: geometric consistency, visual quality, prompt fidelity, and overall preference. Results: Table 4 presents the quantitative comparison of MARVEL-FX3D against the baselines [33, 43, 62, 91]. MARVEL-FX3D shows notable improvements, achieving the highest prompt fidelity (7.71) and overall preference (6.94), indicating strong alignment with input descriptions and balanced performance across criteria. It also tops visual quality with score of 6.58, slightly ahead of LucidDreamer [43] (6.47), which marginally exceeds MARVELFX3D in geometric consistency (7.25 vs. 7.20) due to occasional flat outputs from SF3D [7]. Despite this, MARVELFX3Ds processing time is significantly faster, completing in just 15s compared to Lucid-Dreamers 45 minutes, HiFAs over 1 hour, and DreamFusions 30 minutes. ShapE [33], while the quickest at 5 seconds, shows considerably lower performance across all metrics. Figure 4 includes Figure 4. Visual results of high fidelity TT3D generation. Left to right, the reconstructed 3D assets from Shap-E [33], DreamFusion [62], Lucid-Dreamer [43], HIFA [91] and MARVEL-FX3D. Alighieri, Two Face, and Diablo. Caption Accuracy: Table 3 shows the caption accuracy results, where MARVEL (Level 1) achieves the highest scores84.70% in GPT-4 evaluation and 82.80% in human evaluationdemonstrating superior consistency compared to other methods. Baseline datasets with shorter captions, like Kabra [34] and others [28, 53], tend to capture objects semantically (e.g., statute,a man in tuxedo) but lack detailed descriptions. Although longer captions increase the risk of errors, MARVEL (Level 1) maintains high accuracy with an average length of 170 words (34 that of Kabra [34], 10 that of Cap3D [53], and 5 that of 3DTopia [28]), effectively balancing detail and correctness. As shown in Figure 3, MARVEL captures both domain-specific names and intricate features, exemplified by captions like The Monument of Dante Alighieri... flowing robes, holding book. It stands on decorated stone plinth and is made of smooth, polished marble... Method Average Length Cap3D [53] 3D-Topia [28] Kabra [34] MARVEL (Level 1) 16 29 5 170 Correct GPT4 Evaluation (@1k) 76.00 54.60 83.40 84.70 Human Evaluation (@250) 72.80 44.80 78.20 82.80 Table 3. Comparison of caption accuracy using GPT-4V [60] and humans, highlighting MARVELs (Level 1) superior consistency desite significantly higher caption length. 4.2. Text-to-3D Generation Implementation Details: We fine-tune SD 3.5 [3, 21] using the Objaverse [18] dataset, which includes 798, 759 3D assets, split into training, validation, and test sets in 7 Source Level Target Level Level 1 Level 2 Level 3 Level 4 Level 2 Level 3 Level 4 Level Semantic Similarity 0.91 0.92 0.88 0.72 Compression Ratio 0.30 0.27 0.47 0.22 Table 5. Ablation study results showing SCS across MARVEL40M levels, illustrating strong semantic retention through Levels 1-4 and reduced detail at Level 5. 5. Limitation Our analysis reveals some limitations in MARVEL annotation pipeline and MARVEL-FX3D. First, the underlying VLMs and LLMs exhibit inherent weaknesses in numerical precision [9, 61] and directional understanding [26] in complex scenes with multiple objects and occlusion. Second, InternVL-2 struggles with very thin objects, often misidentifying their side-views as different objects entirely. Finally, without metadata support, the caption accuracy becomes generic for complex 3D structures, particularly in scenes with fragmented geometries like architectural interiors. Additionally, MARVEL-FX3D sometimes generates flat 3D objects due to depth ambiguity in the input image. In supplementary section, some visual examples are provided. Despite these challenges, it is important to note that the strengths of our proposed pipeline and architecture remain significant, as they are both model-agnostic and adaptable to future enhancements. 6. Conclusion In this work, we introduced MARVEL-40M+, the largest 3D captioning dataset to date, comprising over 40 million high-quality text annotations for 8.9 million 3D assets across seven major 3D datasets. Our primary contributions include scalable, multi-stage annotation pipeline that combines open-source pretrained multi-view VLMs and LLMs with filtered human metadata to reduce hallucinations and introduce domain-specific information. Our pipeline produces five levels of annotations for diverse 3D modeling needs, from detailed reconstruction descriptions to rapid prototyping tags. Additionally, we introduce MARVELFX3D, two-stage architecture that leverages fine-tuned Stable Diffusion on our dataset and pretrained Stable Fast 3D to generate high-quality, textured 3D meshes in just 15 seconds. Through extensive experimentation, we demonstrated both MARVEL-40M+s superior annotation quality and linguistic depth, and MARVEL-FX3Ds state-of-the-art performance in high fidelity text-to-3D generation. We believe that MARVEL-40M+ will serve as foundational resource for future advancements in text-to-3D content creation, inspiring further research to address the current limitations and expand the datasets applications. Figure 5. MARVEL uses human-generated metadata from source datasets to create detailed, accurate captions (e.g., names of the lunar craters, detection of human footprints) and reduce hallucinations. Without metadata, VLMs like GPT-4 [60] and InternVL2 [15] generate vague or speculative descriptions. some qualitative examples. 4.3. Ablation Study A. Effect of Human Metadata on Annotation Quality: Human-generated metadata is vital in the MARVEL annotation pipeline, enriching text captions with domain-specific details. While quantitative analysis would require specialized expertise, we provide qualitative evidence of its impact. As shown in Figure 5, MARVEL accurately identifies specific details, such as three human footprints on rocky surface, which both InternVL-2 [13, 15] and GPT-4 [60] miss, producing only generic descriptions. Similarly, MARVEL annotations include detailed identifiers like specific lunar craters, which are absent in outputs from InternVL-2 and GPT-4. This highlights how integrating human metadata enhances the context in annotations. Additional examples spanning other domains (e.g. biological elements, historical sites) are detailed in the supplementary material. B. Inter-Level Semantic Retention Evaluation: This ablation study measures how well semantic information is retained across MARVEL-40M+ annotation levels as they progress from detailed descriptions to concise tokens. To evaluate this, we report the semantic similarity (cosine similarity of embeddings) between levels using sentence-BERT [66] and the compression ratio (word count ratio) [6]. Results in Table 5 show strong semantic retention from Levels 1-4, demonstrating effective compression while preserving meaning. However, the shift to Level 5 results in lower similarity, reflecting the transition to list of concepts at the expense of cohesive descriptions. 8 7. Acknowledgement"
        },
        {
            "title": "Horizon",
            "content": "agreement parts in Europe 101135724 supported Framework by the under (LUMINOUS)."
        },
        {
            "title": "References",
            "content": "[1] Blender - 3d modelling and rendering software. https: //www.blender.org. 3 [2] Openvlm leaderboard - hugging face space. https:// huggingface.co/spaces/opencompass/open_ vlm_leaderboard. 3, 4 [3] Stable diffusion 3.5 large - huggingface. https : / / huggingface . co / stabilityai / stable - diffusion-3.5-large. 2, 3, 4, 5, 7, 14 [4] Mohammed Aldeen, Joshua Luo, Ashley Lian, Venus Zheng, Allen Hong, Preethika Yetukuri, and Long Cheng. Chatgpt vs. human annotators: comprehensive analysis of chatgpt for text annotation. In 2023 International Conference on Machine Learning and Applications (ICMLA), pages 602609. IEEE, 2023. [5] DeepFloyd Lab at StabilityAI. DeepFloyd IF: novel stateof-the-art open-source text-to-image model with high degree of photorealism and language understanding. https: //www.deepfloyd.ai/deepfloyd-if, 2023. Retrieved on 2023-11-08. 3, 5 [6] Timothy Bell, Ian Witten, and John Cleary. Modeling for text compression. ACM Computing Surveys (CSUR), 21 (4):557591, 1989. 8 [7] Mark Boss, Zixuan Huang, Aaryaman Vasishta, and Varun Jampani. Sf3d: Stable fast 3d mesh reconstruction with uvunwrapping and illumination disentanglement, 2024. 2, 3, 4, 5, 7 [8] Peter Brown, Stephen Della Pietra, Vincent Della Pietra, Jennifer Lai, and Robert Mercer. An estimate of an upper bound for the entropy of english. Computational Linguistics, 18(1):3140, 1992. 6 [9] Declan Campbell, Sunayana Rane, Tyler Giallanza, Nicol`o De Sabbata, Kia Ghods, Amogh Joshi, Alexander Ku, Steven M. Frankland, Thomas L. Griffiths, Jonathan D. Cohen, and Taylor W. Webb. Understanding the limits of vision language models through the lens of the binding problem, 2024. 8 [10] Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, L. Yi, and Fisher Yu. Shapenet: An information-rich 3d model repository. ArXiv, abs/1512.03012, 2015. 2, 3, 5, 14, [11] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin 9 Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Palix: On scaling up multilingual vision and language model, 2023. 2, 3 [12] Yixin Chen, Junfeng Ni, Nan Jiang, Yaowei Zhang, Yixin Zhu, and Siyuan Huang. Single-view 3d scene reconstruction with high-fidelity shape and texture. In 2024 International Conference on 3D Vision (3DV), pages 14561467. IEEE, 2024. 2 [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2, 3, 4, 5, 8, 14, 15 [14] Zilong Chen, Feng Wang, Yikai Wang, and Huaping Liu. Text-to-3d using gaussian splatting, 2024. 3 [15] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 2, 3, 4, 5, 8, 14 [16] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 3, 5, 14, 25 [17] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-XL: In Thirty-seventh ConferA universe of 10m+ 3d objects. ence on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 2, 4, 5, 13 [18] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 2, 3, 4, 5, 6, 7, 13, 14, 21 [19] Ankit Dhiman, Manan Shah, Rishubh Parihar, Yash Bhalgat, Lokesh Boregowda, and Venkatesh Babu. Reflecting reality: Enabling diffusion models to produce faithful mirror reflections. arXiv preprint arXiv:2409.14677, 2024. 2 [20] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items, 2022. 2, 3, 5, 13, 14, [21] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. 2, 3, 4, 5, 7, 14 [22] Chongjian Ge, Chenfeng Xu, Yuanfeng Ji, Chensheng Peng, Masayoshi Tomizuka, Ping Luo, Mingyu Ding, Varun Jampani, and Wei Zhan. Compgs: Unleashing 2d compositionality for compositional text-to-3d via dynamically optimizing 3d gaussians, 2024. 2 [23] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, ZiXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github.com/threestudioproject/ threestudio, 2023. 7 [24] Junlin Han, Jianyuan Wang, Andrea Vedaldi, Philip Torr, and Filippos Kokkinos. Flex3d: Feed-forward 3d generation with flexible reconstruction model and input view curation. arXiv preprint arXiv:2410.00890, 2024. 3 [25] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. [26] Nils Hoehing, Ellen Rushe, and Anthony Ventresque. Whats left cant be right the remaining positional incompetence of contrastive vision-language models, 2023. 8 [27] Lukas Hollein, Aljaˇz Boˇziˇc, Norman Muller, David Novotny, Hung-Yu Tseng, Christian Richardt, Michael Zollhofer, and Matthias Nießner. Viewdiff: 3d-consistent image generation with text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2 [28] Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Shuai Yang, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu. 3dtopia: Large text-to-3d generation model with hybrid diffusion priors, 2024. 2, 3, 5, 6, 7, 14, 16, 17, 18, 19, 20 [29] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. 2, 3 [30] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 5, 7 [31] Ka-Hei Hui, Aditya Sanghi, Arianna Rampini, Kamal Rahimi Malekshan, Zhengzhe Liu, Hooman Shayani, and Chi-Wing Fu. Make-a-shape: ten-million-scale 3D shape model. In Proceedings of the 41st International Conference on Machine Learning, pages 2066020681. PMLR, 2024. 2 [32] Chenhan Jiang. survey on text-to-3d contents generation in the wild. arXiv preprint arXiv:2405.09431, 2024. 2, 3 [33] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions, 2023. 3, 6, 7, 28, 29 [34] Rishabh Kabra, Loic Matthey, Alexander Lerchner, and Niloy J. Mitra. Leveraging vlm-based pipelines to annotate 3d objects. In Proceedings of the 41st International Conference on Machine Learning. PMLR, 2024. 2, 3, 4, 5, 6, 7, 14, 16, 17, 18, 19, 20 [35] Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, and Djamila Aouada. Cad-signet: Cad language inference from point clouds using layer-wise sketch instance guided attention. In In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 47134722, 2024. 2 [36] Mohammad Sadil Khan, Sankalp Sinha, Talha Uddin Sheikh, Didier Stricker, Sk Aziz Ali, and Muhammad Zeshan Afzal. Text2cad: Generating sequential cad models from beginner-to-expert level text prompts. In Advances in Neural Information Processing Systems, 2024. 2 [37] Min-Seop Kwak, Donghoon Ahn, Ines Hyeonsu Kim, JinHwa Kim, and Seungryong Kim. Geometry-aware score distillation via 3d consistent noising and gradient consistency modeling, 2024. [38] Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, and Choong Seon Hong. Generative ai meets 3d: survey on arXiv preprint arXiv:2305.06131, text-to-3d in aigc era. 2023. 2, 3 [39] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. 2 [40] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 3, 4 [41] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model, 2023. 2, 3, 5, 7 [42] Xinyang Li, Zhangyu Lai, Linning Xu, Jianfei Guo, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Dual3d: Efficient and consistent text-to-3d generation with dual-mode multi-view latent diffusion, 2024. 3, 5 [43] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards highfidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 65176526, 2024. 2, 3, 6, 7, 28, [44] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution In IEEE Conference on Comtext-to-3d content creation. puter Vision and Pattern Recognition (CVPR), 2023. 2, 3 [45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2, 3 [46] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 3 [47] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao 10 Su. Openshape: Scaling up 3d shape representation towards open-world understanding. Advances in neural information processing systems, 36, 2024. 3 [48] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36, 2024. [49] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. 2 [50] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 2 [51] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, MingYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object synthesis. The International Conference on Computer Vision (ICCV), 2023. 3 [52] Tiange Luo, Honglak Lee, and Justin Johnson. Neural shape compiler: unified framework for transforming between text, point cloud, and program. 2022. 3 [53] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. In Advances in Neural Information Processing Systems, pages 7530775337. Curran Associates, Inc., 2023. 2, 3, 5, 6, 7, 14, 16, 17, 18, 19, 20 [54] Tiange Luo, Justin Johnson, and Honglak Lee. View selection for 3d captioning via diffusion ranking. arXiv preprint arXiv:2404.07984, 2024. 2, [55] Yiwei Ma, Yijun Fan, Jiayi Ji, Haowei Wang, Xiaoshuai Sun, Guannan Jiang, Annan Shu, and Rongrong Ji. X-dreamer: Creating high-quality 3d content by bridging the domain gap between text-to-2d and text-to-3d generation, 2024. 5 [56] Philip M. McCarthy and Scott Jarvis. Mtld, vocd-d, and hdd: validation study of sophisticated approaches to lexical diversity assessment. Behavior Research Methods, 42:381 392, 2010. 6 [57] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Computer Vision ECCV 2020, 2020. 3 [58] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. 2, 3, 5 [59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2022a eprint=2112.10741, archivePrefix=arXiv, primaryClass=cs.CV, url=https://arxiv.org/abs/2112.10741,. 3 [60] Josh OpenAI, Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3, 4, 6, 7, 8, 14, 15 [61] Letitia Parcalabescu, Albert Gatt, Anette Frank, and Iacer Calixto. Seeing past words: Testing the cross-modal capabilities of pretrained v&l models on counting tasks, 2021. 8 [62] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. 2, 3, 6, 7, 28, 29 [63] Xuebin Qin, Hang Dai, Xiaobin Hu, Deng-Ping Fan, Ling Shao, and Luc Van Gool. Highly accurate dichotomous image segmentation. In ECCV, 2022. [64] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 3 [65] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 8 [66] Reimers. [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 5 [68] Shouwei Ruan, Yinpeng Dong, Hanqing Liu, Yao Huang, Hang Su, and Xingxing Wei. Omniview-tuning: Boosting viewpoint invariance of vision-language pre-training models, 2024. 4 [69] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2, [70] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2024. 2, 3 [71] Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, and David Novotny. Meta 3d assetgen: Text-to-mesh generation with high-quality geometry, texture, and pbr materials. arXiv, 2024. 2, 3, 5, 7 [72] Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Llm pruning and distillation in practice: The minitron approach, 2024. 4, 5 [73] Stefan Stojanov, Anh Thai, and James M. Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. 2021. 2, 3, 5, 13, 14 [74] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua Tenenbaum, and William Freeman. Pix3d: Dataset and methods for Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. 2, 3, 4, 5 [86] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency for multiIn Proceedings of the IEEE/CVF view images diffusion. Conference on Computer Vision and Pattern Recognition, pages 70797088, 2024. 2 [87] Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang, Hengshuang Zhao, Tong He, and Xihui Liu. Dreamcomposer: Controllable 3d object generation via multi-view conditions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81118120, 2024. 2 [88] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models, 2024. 3 [89] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 2, 3, 5 [90] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong, Jinlin He, Zhiwei Lin, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Gala3d: Towards text-to-3d complex scene generation via layout-guided generative gaussian splatting. arXiv preprint arXiv:2402.07207, 2024. [91] Junzhe Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance, 2023. 2, 3, 6, 7, 28, 29 [92] Peiye Zhuang, Songfang Han, Chaoyang Wang, Aliaksandr Siarohin, Jiaxu Zou, Michael Vasilkovsky, Vladislav Shakhrai, Sergey Korolev, Sergey Tulyakov, and HsinYing Lee. Gtr: Improving large 3d reconstruction models through geometry and texture refinement. arXiv preprint arXiv:2406.05649, 2024. 2 In IEEE Conference on single-image 3d shape modeling. Computer Vision and Pattern Recognition (CVPR), 2018. 2, 3, 5, 13, 14, 27 [75] Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? study on the impact of format restrictions on performance of large language models. arXiv preprint arXiv:2408.02442, 2024. 5 [76] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. Textmesh: Generation of realistic 3d meshes from text prompts. In International conference on 3D vision (3DV), 2024. 2, 3 [77] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. 2, [78] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse SpencerSmith, and Douglas Schmidt. prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023. 5 [79] Sangmin Woo, Jaehyuk Jang, Donguk Kim, Yubin Choi, and Changick Kim. Ritual: Random image transformations as universal anti-hallucination lever in lvlms, 2024. 4 [80] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Liang Pan Jiawei Ren, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, Dahua Lin, and Ziwei Liu. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and In IEEE/CVF Conference on Computer Vision generation. and Pattern Recognition (CVPR), 2023. 2, 3, 5, 13, 14, 22 [81] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt4v(ision) is human-aligned evaluator for text-to-3d generation, 2024. 6 [82] Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, and Xiaohui Zeng. Latte3d: Large-scale amortized text-to-enhanced3d synthesis. The 18th European Conference on Computer Vision (ECCV), 2024. 3 [83] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [84] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and Honglak Lee. Perspective transformer nets: Learning singleview 3d object reconstruction without 3d supervision. Advances in neural information processing systems, 29, 2016. 2 [85] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei 12 MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 6. An example use case of MARVEL-FX3D, demonstrating how multiple prompts can be combined to create detailed and complex 3D scene, with each prompt contributing specific elements such as characters, structures, and environmental details. Please zoom in for details. This supplementary material provides additional details and results to support the main paper. Section 8 outlines the captioning process, including dataset preparation and implementation specifics. Sections 9 and 10 delve deeper into MARVEL annotations and MARVEL-FX3D results, offering more examples, discussions, and insights into their applications and limitations. 8. Additional Details on Captioning Process 8.1. Dataset Preparation Objaverse: Objaverse [18] contains 798, 759 3D assets, with metadata (e.g., name, tags, description) available for 93% samples after filtering. From ObjaverseXL [17], we rendered 8, 031, 637 assets, of which 3.7M included metadata. After filtering, around 3M samples are retained as valid metadata. ShapeNet: For the ShapeNet dataset, which contains 51,209 samples, we use the ShapeNet taxonomy as its metadata (e.g., airplane, bowl, cap, clock, etc.). Pix3D: For the Pix3D[74] dataset, which contains 735 samples, we use the associated category tag as its metadata (e.g., bed, table, desk, chair, etc.). OmniObject3D: The Omni-Object-3D[80] dataset, which contains 5,878 samples, we use the folder names (e.g., bed, table, desk, chair, etc.) as our metadata. Toys4K: For the Toys4K[73] dataset, which contains 4,000 samples, we use the folder names (e.g., car, airplane, train, robot, etc.) as our metadata. GSO: The GSO (Google Scanned Objects)[20] dataset, which contains 1,030 samples, we use the folder names (e.g., lamp, sofa, vase, refrigerator, etc.) as our metadata. http://pix3d.csail.mit.edu/ https://omniobject3d.github.io/ https://github.com/rehg-lab/lowshot-shapebias/ tree/main/toys4k https://objaverse.allenai.org/objaverse-1.0 https://goo.gle/scanned-objects 13 ABO: The ABO (Amazon Berkeley Objects)**[16] dataset, which contains 7,953 samples, provides metadata through listings information. Since these listings are multilingual, we first use the nllb-200 model to translate the listings to English. The translated English listings are then used as our metadata. 8.2. Implementation Details For human metadata filtering, we use the Mistral-NemoInstruct-2407 model with temperature of 0.3 and top-p value of 0.95. For dense description generation, we employ InternVL2-40B, configured with temperature of 0.70, top-p value of 0.95, and repetition penalty of 1.10, with multinomial sampling enabled. For multi-level visual elaboration, we utilize Qwen2.5-72B with 8-bit quantization, temperature of 0.70, top-p value of 0.80, and repetition penalty of 1.05. Finally, the Qwen2.5-14B model, used for the ethical filtering stage, is configured with temperature of 0 and top-p value of 0.90. For human evaluations in our paper, we developed Gradio app to compare our captions with those from baseline datasets, including Cap3D, 3DTopia, and Kabra, as well as to evaluate FX3D results against text-to-3D baselines. The evaluations were conducted by panel of five human experts. 9. Additional details on MARVEL annotations and our proposed MARVEL-40M+ dataset. For consistency, we used only Level 4 annotations, as their length closely matches that of the baselines. To improve clarity, we further categorized examples into distinct domains. Figure 9 showcases 3D models of automotive designs (e.g., cars, planes) and CAD models. Figure 10 features iconic characters from anime, movies, and video games. Figure 11 illustrates biological elements such as animals, plants, and molecules. Figure 12 includes diverse items ranging from everyday objects, essentials, food to luxury items. Figures 13 and 14 depict historical artifacts (e.g., statues, memorials) and various scenes (e.g. digital elevation maps, realistic and animated scenes) respectively. As illustrated in the figures, MARVEL annotations offer more precise and domain-specific descriptions, leveraging accurate nomenclature and contextual terminology, surpassing the quality of the baseline datasets. 9.3. More Multi-Level Examples We present additional qualitative results showcasing our multi-level annotations across all seven datasets [10, 16, 18, 20, 73, 74, 80], with two examples per dataset - Objaverse (Figure 15), OmniObject3D (Figure 16), ShapeNet (Figure 17), Toys4k (Figure 18), ABO (Figure 19) and GSO (Figure 20). 9.1. More Results on Effects of Human Metadata 9.4. Failure Cases Figure 7 showcases examples where human-provided metadata from source datasets reduce VLM hallucination and enhances annotations with domain-specific information. To generate captions using InternVL2 [13, 15] and GPT-4 [60], we input the same multi-view images used for MARVEL annotations, instructing them to produce concise descriptions that include names, shapes, textures, colors, and contextual environments. Examples 1, 2, and 3 demonstrate how the inclusion of simple metadata (e.g. La Cava Window, Mount St. Helens) significantly reduces VLM hallucination, resulting in more accurate captions. Example 4 illustrates how metadata can support the generation of highly domain-specific information (e.g. alpha-helices and beta sheets, N-terminus, middle, and C-terminus). Figure 8 presents examples of the failure cases discussed in Section 5 of the main paper, illustrating the challenges associated with using pretrained VLMs to generate dense descriptions of 3D models. 10. Additional results of MARVEL-FX3D 10.1. More Implementation Details As discussed in the main paper, MARVEL-FX3D is twostage pipeline. In the first stage, Stable Diffusion 3.5 [3, 21] is fine-tuned. During each epoch, one annotation is sampled from five levels and paired with randomly selected multiview image for MSE loss calculation. During inference, CFG [25] is set to 7.5, and 30 steps are used to balance speed and output diversity. 9.2. More 3D Captioning Results 10.2. More Text-to-3D Results We provide more qualitative comparisons of annotations, highlighting differences between the baseline [28, 34, 53] **https://amazonberkeleyobjects.s3.amazonaws. com/index.html https : / / huggingface . co / facebook / nllb - 200 - distilled-600M Figures 22 and 23 showcase visual results of TT3D generation on unseen prompts. Using GPT-4 [60], we generated 10 random prompts focused on shape and scene descriptions. As demonstrated, MARVEL-FX3D produces higherfidelity 3D models from text prompts compared to the baseline methods. 14 Figure 7. Effect of including human metadata, highlighting improvements in descriptive accuracy and contextual relevance compared to outputs generated without metadata, even when using state-of-the-art models like GPT-4 [60] and InternVL2 [13]. Metadata inclusion helps reduce hallucinations and enhances domain-specific understanding. Figure 8. Failure cases of the MARVEL annotation pipeline. From left to right, the examples illustrate errors such as counting mistakes, object misidentification, and challenges with ambiguous views. 10.3. Discussion on Application of MARVEL The MARVEL-40M+ dataset, with its scale and diversity, serves as powerful resource for text-to-3D tasks such as reconstruction, multi-view consistency, and compositional scene generation. notable real-world use case, illustrated in Figure 6, demonstrates how MARVEL-FX3D which is trained on MARVEL dataset enables rapid prototyping of diverse 3D objects from complex, fine-grained or simple text prompts. This capability facilitates the creation of intricate scenes, making it particularly valuable for applications in gaming, AR, and VR. 15 Figure 9. Qualitative comparison of 3D annotations across baselines [28, 34, 53] and the proposed MARVEL-40M+ for automotive (cars, planes, etc) and CAD models. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green. 16 Figure 10. Qualitative comparison of 3D annotations across baselines [28, 34, 53] and the proposed MARVEL-40M+ for popular anime, movie, and cartoon characters. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green. 17 Figure 11. Qualitative comparison of 3D annotations across baselines [28, 34, 53] and the proposed MARVEL-40M+ for biological objects, including animals, plants, and molecular models. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green. 18 Figure 12. Qualitative comparison of 3D annotations across baselines [28, 34, 53] and the proposed MARVEL-40M+ for diverse items including daily objects, essentials. MARVEL-40M+ provides more accurate and domain-specific annotations, compared to the baselines. Incorrect captions are highlighted in red, while important captions are highlighted in green. 19 Figure 13. Qualitative comparison of 3D annotations across baselines [28, 34, 53] and the proposed MARVEL-40M+ for historical elements including statues, places, memorials, etc. Incorrect captions are highlighted in red, while important captions are highlighted in green. Figure 14. Qualitative comparison of 3D annotations across baselines [28, 34, 53] and the proposed MARVEL-40M+ for diverse scenes including digital elevation maps, places, realistic or animated scenes. Incorrect captions are in red, while important captions are in green. 20 Figure 15. Multi-level annotation examples of MARVEL for the Objaverse [18] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions. 21 Figure 16. Multi-level annotation examples of MARVEL for the Omni-Object [80] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions. Figure 17. Multi-level annotation examples of MARVEL for the ShapeNet [10] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions. 23 Figure 18. Multi-level annotation examples of MARVEL for the Toys4K dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions. 24 Figure 19. Multi-level annotation examples of MARVEL for the ABO (Amazon Berkeley Objects) [16] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions. Figure 20. Multi-level annotation examples of MARVEL for the GSO (Google Scanned Objects) [20] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions. 26 Figure 21. Multi-level annotation examples of MARVEL for the Pix3D [74] dataset. Words corresponding to Object and Components are highlighted in violet, Shape and Geometry in green, Texture and Materials in orange, Colors in blue, and Contextual Environment in purple. From top to bottom, we go from level-5 (Concise Tags) captions to level-1 (Comprehensive Description) captions. 27 Figure 22. Qualitative Results for high fidelity TT3D generation on unseen prompts. From left to right, 3D models generated using ShapE [33], DreamFusion [62], LucidDreamer [43], HiFA [91] and MARVEL-FX3D (ours). Figure 23. Qualitative Results for high fidelity TT3D generation on unseen prompts. From left to right, 3D models generated using ShapE [33], DreamFusion [62], LucidDreamer [43], HiFA [91] and MARVEL-FX3D (ours)."
        }
    ],
    "affiliations": [
        "BITS Pilani, Hyderabad",
        "DFKI",
        "MindGarage",
        "RPTU Kaiserslautern-Landau"
    ]
}