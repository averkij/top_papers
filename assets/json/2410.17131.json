{
    "paper_title": "Aligning Large Language Models via Self-Steering Optimization",
    "authors": [
        "Hao Xiang",
        "Bowen Yu",
        "Hongyu Lin",
        "Keming Lu",
        "Yaojie Lu",
        "Xianpei Han",
        "Le Sun",
        "Jingren Zhou",
        "Junyang Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization ($SSO$), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. $SSO$ maintains the accuracy of signals by ensuring a consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy model's learning capacity. $SSO$ can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of $SSO$ with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, $SSO$ leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by $SSO$ significantly enhanced the performance of the reward model on Rewardbench. Our work presents a scalable approach to preference optimization, paving the way for more efficient and effective automated alignment."
        },
        {
            "title": "Start",
            "content": "Under review as conference paper at ICLR 2025 ALIGNING LARGE LANGUAGE MODELS VIA SELFSTEERING OPTIMIZATION Hao Xiang1,3, Bowen Yu2, Hongyu Lin1, Keming Lu2, Yaojie Lu1, Xianpei Han1, Le Sun1, Jingren Zhou2, Junyang Lin2 1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 2Alibaba Group 3University of Chinese Academy of Sciences, Beijing, China {xianghao2022, hongyu, luyaojie, xianpei, sunle}@iscas.ac.cn {yubowen.ybw, lukeming.lkm, jingren.zhou, junyang.ljy}@alibaba-inc.com ABSTRACT Automated alignment develops alignment systems with minimal human intervention. The key to automated alignment lies in providing learnable and accurate preference signals for preference learning without human annotation. In this paper, we introduce Self-Steering Optimization (SSO), an algorithm that autonomously generates high-quality preference signals based on predefined principles during iterative training, eliminating the need for manual annotation. SSO maintains the accuracy of signals by ensuring consistent gap between chosen and rejected responses while keeping them both on-policy to suit the current policy models learning capacity. SSO can benefit the online and offline training of the policy model, as well as enhance the training of reward models. We validate the effectiveness of SSO with two foundation models, Qwen2 and Llama3.1, indicating that it provides accurate, on-policy preference signals throughout iterative training. Without any manual annotation or external models, SSO leads to significant performance improvements across six subjective or objective benchmarks. Besides, the preference data generated by SSO significantly enhanced the performance of the reward model on Rewardbench. Our work presents scalable approach to preference optimization, paving the way for more efficient and effective automated alignment. https://github.com/icip-cas/SSO"
        },
        {
            "title": "INTRODUCTION",
            "content": "4 2 0 2 2 2 ] . [ 1 1 3 1 7 1 . 0 1 4 2 : r (a) Online Training on Llama3.18B. (Iteration 3) (b) Offline Training on Llama3.18B. (c) RM Training on Llama3.1-8BInstruct. Figure 1: Results of SSO in Online, Offline, and RM Training. Detailed results will be presented in Section 3.2. In these figures, SF indicates Llama3.1-8B-SFT, which we trained from Llama3.18B. Instruct indicates Llama3.1-8B-Instruct. Skywork is the dataset leading to the SOTA reward model for RewardBench. 1 Under review as conference paper at ICLR 2025 Figure 2: The philosophical motivation of our methods. Greater overlap on the x-axis (performance) between the generated distributions (red and blue) and the original distribution (orange) indicates stronger on-policy behavior. Previous automated methods extract chosen and rejected distributions through different methods, which may be less learnable for the policy model and hard to distinguish after iterative training. Our approach (SSO) optimizes models to generate near-on-policy signals where there remains gap between chosen and rejected distributions, which benefits the automated alignment process. The field of Natural Language Processing has undergone revolutionary advancements driven by Large Language Models (LLMs). After meticulous alignment processes, LLMs have demonstrated remarkable capabilities for following instructions and understanding human preferences. This leads to the development of widely acclaimed products like ChatGPT (OpenAI, 2023), which captured significant public attention. However, aligning LLMs with human preferences is not trivial. Despite the existence of preference optimization algorithms such as Proximal Policy Optimization (PPO) (Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2023), an ideal alignment training process necessitates robust explicit or implicit reward model. This model must effectively differentiate between chosen and rejected responses and guide it to optimizing toward the preferred responses. Unfortunately, the reward model depends on large amount of high-quality annotated preference data and continuous updates of labeled response pairs to prevent reward hacking, which is resource-intensive and requires meticulous attention. Besides, the limited capabilities of human annotators cause the inherent limitations of annotated data, making it challenging to achieve superalignment (Burns et al., 2023). Consequently, recent researchers have shifted their focus towards automated alignment, intending to develop scalable, high-quality alignment systems with minimal human intervention. The cornerstone of this approach is the pursuit of scalable alignment signals that are capable of replacing human-annotated preference signals effectively. Current popular strategies include: (1) Employing the policy model to discriminate chosen and rejected responses (Yuan et al., 2024). However, hampered by the models inherent limitations, this judging capability is constrained and challenging to improve, often resulting in reward hacking and inaccurate reward signals (Wu et al., 2024). (2) Directly generating chosen and rejected responses based on predefined principles, rules, or requests (Yang et al., 2024b; Bai et al., 2022b; Franken et al., 2024; Kumar et al., 2024). However, as illustrated in Figure 2, incorporating additional inputs or processes may lead to off-policy and unsuitable outputs, blurring the accuracy of preference signals and ultimately diminishing the effectiveness of the optimization. We then recognized the need for novel approach to generate accurate, learnable, and on-policy preference signals to address these limitations and advance automated alignment. 2 Under review as conference paper at ICLR 2025 In this paper, we introduce Self-Steering Optimization (SSO), pioneering method that continuously generates automated, accurate, and learnable preference signals for the policy model. The design philosophies of Self-Steering Optimization emphasize that the chosen and rejected responses, along with their associated signals, should primarily be on-policy, in other words, able to extract directly from the policy model to suit the policy models learning capacity. Besides, the accuracy of the synthetic signals should progressively increase or at least maintain high level as the model undergoes training. To implement these philosophies, SSO first prompts the policy model with the original query and set of contrastive principles for responses. We then optimize the model based on three key objectives: a) Steer the model towards the direction of the chosen responses, which are collected by prompting the policy model with queries and good principles. b) Ensure responses are approximately on-policy, allowing the model to sample them even without additional principles. c) Maintain consistent gap between the chosen and rejected responses. To summarize, as the policy model strengthens, it should become increasingly adept at generating accurate and near-on-policy response pairs based on different principles, thereby enabling further optimization of the model. We demonstrate the effectiveness of Self-Steering Optimization on Qwen2 (Yang et al., 2024a) and Llama3.1 (Llama Team, 2024) backbones. Our experiments reveal SSOs ability to generate accurate and learnable automated signals throughout training. As result, continuous improvements are observed across wide range of objective benchmarks such as GPQA (Rein et al., 2023), MATH (Hendrycks et al., 2021), MMLU Pro (Wang et al., 2024b), and GSM8K (Cobbe et al., 2021), as well as subjective evaluation sets like MT-Bench (Zheng et al., 2024b) and AlpacaEval 2.0 (Dubois et al., 2024). Remarkably, these improvements are achieved without any human annotation or external models. SSO even outperforms baselines with annotated data (Cui et al., 2024), underscoring its potential as scalable and efficient approach. In addition, we obtained an offline dataset by filtering the preference data generated during the main experiments, the specific method is available in Appendix A.1.4. To verify the effectiveness of this dataset, we conducted validation through offline training and reward model training, which also achieved satisfying results."
        },
        {
            "title": "2 SELF-STEERING OPTIMIZATION",
            "content": "In this section, we explain the motivation and design of Self-Steering Optimization. SSO follows modified principle-based automated alignment paradigm (Yang et al., 2024b; Franken et al., 2024) and new optimization strategy to generate learnable and accurate signals. Figure 3: Our approach consists of two iterative steps: 1) Constructing contrastive prompts and sampling responses. Given query, the policy model first identifies the most relevant features and principles to the query. We then construct pair of contrastive prompts based on these principles and sample corresponding responses. These responses are then used to form three preference pairs for alignment. 2) Training the model with weighted objective incorporating three distinct losses. 3 Under review as conference paper at ICLR"
        },
        {
            "title": "2.1 PREVIOUS METHODS",
            "content": "While some works focus on the self-reward or self-correct method, attempting to improve the models judgment or correcting capabilities during alignment (Wu et al., 2024; Yuan et al., 2024; Ye & Ng, 2024; Wang et al., 2024a; Kumar et al., 2024), we focus on principle-based automated alignment (PBAA) (Yang et al., 2024b; Franken et al., 2024). This simpler paradigm generates accurate preference data as the contrastive principles possess distinctly opposite attributes (e.g., harmful vs. harmless). Besides, compared to self-reward and self-correct, it samples fewer responses, leading to lower cost. However, previous principle-based methods suffer from several limitations. Firstly, during iterative training, it is gradually harder to generate chosen and rejected responses with enough quality gaps. This results in lower signal accuracy, diminishing benefit, and even collapse of alignment (Lee et al., 2024b; Yu et al., 2024), particularly pronounced in small models. Secondly, although all responses are sampled from the policy model, they may not fully align with the original instruction. Additional inputs, such as principles, could lead to insufficient on-policy and learnable responses, which have been noted to be important in many previous studies Tajwar et al. (2024). In this paper, we propose Self-Steering Optimization to address these limitations."
        },
        {
            "title": "2.2 SELF-STEERING OPTIMIZATION",
            "content": "As mentioned in the last section, Self-Steering Optimization aims to enhance the learnability and accuracy of the generated preference data. Given principles p+ and combined with the original instruction for chosen response y+ and rejected response y, we propose SSO as: LSSO = W(x, y+, y) (cid:124) (cid:125) (cid:123)(cid:122) weight function for learnable and on-policy signal θ G(x, p+, p, y+, y) (cid:124) (cid:125) (cid:123)(cid:122) self-steering loss for accurate signal + Lbase(x, y+, y) (cid:125) (cid:123)(cid:122) base loss for optimizing model (cid:124) (1) where is the self-steering loss that controls the quality gap between y+ and y, θ is parameter controls the weight of G. is the base loss (we used the IPO loss), optimizing the model toward the chosen responses. Inspired by WPO (Zhou et al., 2024), we control the on-policy behavior through weight function W. It is important to note that while WPO aims to approximate on-policy effects by re-weighting existing data, our goal is to directly generate near-on-policy data. Therefore, unlike WPO, we did not detach W."
        },
        {
            "title": "2.3 DESIGN OF SELF-STEERING LOSS G",
            "content": "As mentioned in formula 1, we add for accurate signals. Therefore, given three responses sampled from the policy model: the original response yo for x, y+ for x+, and for x, SSO have the following expectations: Expectation 1: yo, y+, and should all possess high quality under their corresponding instructions (i.e., x, x+, and x). natural approach is to construct the loss by using x+ and as instructions, with their corresponding responses as positive responses: = Lbase(x+, y+, y) + Lbase(x, y, y+) (2) However, this design introduces backdoor problem: with carefully crafted prompts, it becomes easy to manipulate LLMs to unpredictable results such as poison text. Expectation 2: should try to approximate yo while still satisfying x. This goal is crucial, as we want to prevent the model from using as backdoor. Therefore, we consider adjusting Lbase(x, y, y+) by using yo as the positive response. Therefore, the final form of is: = Lbase(x+, y+, y) + Lbase(x, yo, y+) (3) 4 Under review as conference paper at ICLR"
        },
        {
            "title": "2.4 DESIGN OF WEIGHT FUNCTION W",
            "content": "We also designed for learnable signals. Instead of more complex functions, we apply simple format that utilizes the average log probabilities of y+ and y, denoted as πθ(yx): πθ(yx) = logπθ(yx) larger π indicating better on-policy behaviors. We then set as: W(x, y+, y) = Sigmoid (cid:0) (cid:0)α πθ(y+x) + (1 α)πθ(yx)(cid:1)(cid:1) Here, α is hyperparameter. Unless specified, we set it to 0.66. (4) (5)"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In this section, we first introduce the experimental setup in section 3.1. Then, we present the main results in section 3.2, which includes the results on the sft and aligned models."
        },
        {
            "title": "3.1 EXPERIMENTAL SETUP",
            "content": "Base Models We primarily conducted experiments on Qwen2-7B (Yang et al., 2024a) and Llama3.1-8B (Llama Team, 2024). We trained Llama3.1-8B and Qwen2-7B on UltraChat (Ding et al., 2023) for three epochs. Qwen2-7B-instruct and Llama3.1-8B-instruct are the official aligned versions of Qwen2 and Llama3.1. Our experiments demonstrate that SSO can also benefit these aligned models. Besides, we also used stronger SFT model of Llama3.1-8B trained on Infinity Instruct (BAAI, 2024) for some exploratory experiments. 1 Datasets For datasets, apart from applying UltraChat to train SFT models, most of our experiments are based on UltraFeedback (Cui et al., 2024). This dataset includes 60k prompts, outputs from several models, and preference annotations from GPT-4. We split the dataset into three portions with size ratio of 1:1:1 and only used the queries of each portion per iteration, with all responses sampled from the policy model. Training Setting We chose IPO (Azar et al., 2023) as the basic loss in most experiments and used batch size of 128 to prevent overfitting. We applied simple hyperparameter search to determine the learning rate and β parameter in IPO. We fine-tuned Qwen2-7B and Llama3.1-8B with learning rate of 2E-5. For alignment training, the learning rate was 5E-7, and β was 0.2. The α in the function was 0.66, and the weight of the function was 0.1 as default. We employed generation parameters of top-p=0.8, temperature=0.7, and max new tokens=2048 for sampling responses. The training scripts were based on LlamaFactory(Zheng et al., 2024c). Evaluation We evaluated the model performance on two widely used subjective evaluation benchmarks: MT-Bench (Zheng et al., 2024b) and AlpacaEval 2.0 (Dubois et al., 2024). MT-Bench comprises 80 questions with answers scored by GPT-4. AlpacaEval 2.0 includes 805 questions, where the judge model compares answers to its reference responses. Notably, we employ the more advanced GPT-4o as the judging model and GPT-4 as the baseline in AlpacaEval for lower cost. Additionally, we evaluated models on series of objective benchmarks: MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), MMLU Pro (Wang et al., 2024b) and GPQA (Rein et al., 2023). These objective benchmarks cover various aspects, comprehensively assessing the model capabilities. Data Generation We generated preference data based on principle-based automated alignment (PBAA) (Yang et al., 2024b; Franken et al., 2024) paradigm. This simple paradigm assumes that responses with varying quality can be extracted from LLMs through contrastive prompts. These methods manually construct set of principles and build contrastive prompts for contrastive response pairs used as the training data. We modified this paradigm for better iterative training. Specifically, we defined seven preference features: Safety, Logicality, Concise, etc. To ensure these principles are relevant to the query, we first determined the most crucial features to reply to the query and then randomly selected one of these features and corresponding principles to construct prompts. Subsequently, we utilized these prompts to instruct the policy model for responses and construct preference data. The used principles and templates are provided in Appendix A.3.1 and A.3.2. 1You can also find additional experiments conducted on Llama3-8B in Appendix A.1. 5 Under review as conference paper at ICLR"
        },
        {
            "title": "3.2.1 HOW SSO PERFORMS IN ITERATIVE ONLINE TRAINING",
            "content": "Table 1: Results on Llama3.1-8B-SFT and Qwen2-7B-SFT. We conduct experiments with Ultrafeedback, modified PBAA (principle-based automated alignment), and SSO. In this table, AE2 represents AlpacaEval 2.0 Length Control Win Rate. MT represents MT-Bench. Iter Len AE2 MT GPQA MMLU Pro MATH GSM8K Len AE2 MT GPQA MMLU Pro MATH GSM8K Llama3.1-SFT Qwen2-SFT 967 6.4 6.69 32.3 37.6 20. 62.9 841 12.1 7.42 33.8 42.5 44.7 78.7 Iter1 935 9.9 6.75 34.8 Iter2 1025 10.9 7.12 36.9 Iter3 1185 10.5 7.31 31. Iter1 1465 12.3 6.98 26.8 Iter2 2628 14.9 7.09 25.8 Iter3 9160 2.6 6.46 26.8 Iter1 1146 10.2 7.07 30.8 Iter2 1466 12.5 7.37 32.3 Iter3 2274 15.0 6.96 33.8 UltraFeedback + IPO 20.2 20.4 20.6 63.8 63.9 62.5 917 12.2 7.38 32.8 942 12.4 7.48 31.8 1014 13.7 7.60 31. Modified PBAA (IPO Based) 20.2 20.5 14.7 20.4 21.7 20.6 64.2 63.5 61.8 1011 12.5 7.52 31.3 1183 14.5 7.62 33.3 1402 16.9 7.71 33.3 SSO (IPO Based) 64.0 63.0 60.4 929 12.9 7.25 29.3 1025 15.0 7.47 31.8 1120 17.3 7.75 33.8 38.0 38.2 38.4 37.4 36.8 36.5 37.6 38.1 37.5 42.6 42.1 42. 42.3 42.4 41.8 42.7 42.0 41.9 45.5 45.8 45.4 45.3 46.0 46.3 45.7 45.6 46.4 79.6 79.0 78. 79.2 79.4 79.6 78.7 78.3 79.8 Results on SFT Models This part compares the performance of SSO against modified principlebased alignment on SFT models. Table 1 demonstrates that SSO achieved outstanding results on MT-Bench and AlpacaEval 2.0. Compared to the SFT model, SSO showed an average improvement of nearly 8% on AlpacaEval 2.0 and 0.5 points on MT-Bench. In contrast, while the baseline initially showed improvements, they failed to sustain this progress. SSO also showed benefits on objective benchmarks, especially in mathematical reasoning tasks. These benefits may attributed to the Logicality or Helpful preference features. Although there were no significant benefits for MMLU Pro, it aligned with expectations, as limited data is unlikely to enhance knowledge capabilities. We also compared SSO with annotated data. Models trained with original UltraFeedback and IPO showed less improvement on AlpacaEval 2.0 and MT-Bench than those trained with synthetic data. However, annotated data demonstrated notable benefits on knowledge-based benchmarks, particularly GPQA and MMLU Pro. These results highlight the respective strengths and limitations of synthetic data, aligning with the findings reported by Shumailov et al. (2024). Results on Aligned Models We also applied SSO on aligned models, with results shown in Table 2. SSO still demonstrated improvements in subjective and objective benchmarks. Detailed results of every iteration can be found in Table 8 at Appendix A.1.1. Although it showed less benefit than results on SFT models, considering that these models have already undergone complex alignment processes, SSOs improvement remains encouraging. Notably, combining Table 1, we found that SFT models optimized with SSO already show performance approaching Instruct models on some benchmarks. This encourages us to use more powerful SFT models to achieve performance close to or even surpassing Instruct models. These experimental results will be detailed in section 4. Table 2: Results on Llama3.1-8B-Instruct and Qwen2-7B-Instruct. Method AE2 MT MMLU Pro MATH Llama3.1-Instruct Instruct UltraFeedback BAA SSO 32.8 39.3 27.2 39.2 8.34 8.00 8.28 8.48 Qwen2-instruct Instruct UltraFeedback BAA SSO 33.2 19.3 30.7 36.2 8.37 7.79 8.41 8. 42.9 46.1 46.8 47.4 44.4 43.8 44.2 44.5 40.9 42.8 42.3 43.7 50.4 30.6 32.4 50.4 6 Under review as conference paper at ICLR Table 3: Results on Llama3.1 trained with synthetic offline data. Model Training Data Len AE2 MT GPQA SFT Instruct Ultrafeedback SSO Ultrafeedback SSO 1283 1319 2105 2446 11.5 18.0 41.2 41. 7.23 7.36 8.13 8.58 32.3 32.8 32.8 36.1 MMLU Pro 38.5 35. 46.1 48.6 MATH GSM8K 20.1 20.6 42.8 43.3 61.2 62.9 82.9 84."
        },
        {
            "title": "Training Data",
            "content": "Table 4: Our Reward Models Chat Hard"
        },
        {
            "title": "Safety Reason",
            "content": "Skywork Skywork + Synthetic Skywork + UltraFeedback 90.8 91.7 90.9 93.6 93.3 95.8 85.5 86.2 80.0 90.1 92.6 92.3 94.1 94.9 95. Reward Model We also tried to train reward model based on our offline dataset. Unlike offline training, we maintained every response pair instead of choosing one for each query. These data could enhance the annotated data from the current best reward model, Skywork-Reward-Llama-3.18B Liu & Zeng (2024). We reported the performance of the reward models trained with the enhanced dataset on RewardBench Lambert et al. (2024). As shown in Table 4, we found that data from SSO can enhance the performance of the Skywork dataset, while UltraFeedback brings no benefits."
        },
        {
            "title": "4 DISCUSSION",
            "content": "Table 5: Results on Qwen2-7BInstruct under different ablations (Iteration 3). Method AE2 MT"
        },
        {
            "title": "1786\nInstruct\nSSO\n2789\nw/o W 4512\nw/o G\n2799\nw/o W, G\n4458",
            "content": "33.24 36.18 36.07 36.03 30.70 8.37 8.47 8.35 8.40 8.41 Ablation Study In this part, we conducted an ablation study on SSO. Results are shown in Table 5, and detailed results can be found in Table 12 in Appendix A.2. We observed that removing either the function or the function would lead to significant performance decrease, demonstrating the importance of SSOs each component. Furthermore, it is notable that SSO with only or still produced some benefit, indicating that both the function and function can independently contribute to the alignment process. Quality of synthetic data It is generally believed that lower noise in the preferences data will lead to better alignment process (Lee et al., 2024a; Gao et al., 2024). question is whether SSO effectively maintains the quality of generated preference data. To assess this, we used GPT4o to judge the accuracy of the synthetic preference data. We took Llama3.1-SFT as an example. Specifically, given query x, we asked GPT-4o to determine if y+ had higher quality than y. To mitigate selection bias (Zheng et al., 2024a), we swapped the positions of y+ and for two rounds of judgment. Figure 4(a) shows that SSO maintained higher-quality synthetic data, while 7 Under review as conference paper at ICLR IPO caused gradually decreased accuracy. Moreover, given policy model π, instruction x, and response pair (y+, y), we tested the average probability eπθ(yx) (Formula 4) of the synthetic data. Figure 4(b) shows the eπθ(yx) for three iterations, where bigger values indicate better on-policy performance. SSO generated better near-on-policy data than baselines. (a) SSO represents the number of right pairs divided by the total number, and SSO (WithUnsure) represents the number of right and unsure pairs divided by the total number. (b) Compared to IPO, SSO significantly raises the π(y+x) and π(yx). Figure 4: Quality analysis of synthetic data for Llama3.1-SFT training. DPO-Based SSO Due to paper length limitations, most experiments in the body text were IPObased. However, our method can be extended to other losses. Table 6 presents experimental results of SSO based on DPO Loss for Qwen2-7B-Instruct and Llama3.1-8B-Instruct. Detailed results are shown in Appendix A.1.2. Table 6: Results with DPO-Based SSO. Model Instruct Model Modified PBAA(DPO Based) Iter3 SSO(DPO Based) Iter3 Len AE2 MT Len AE2 MT Qwen2 Llama3,1 1786 3653 2611 33. 32.9 37.2 8.37 8.27 8.46 2947 2745 32.8 40.0 41.4 8. 8.39 8.57 Results on Stronger SFT Model Additionally, we applied SSO on stronger SFT model of Llama3.1-8B trained on Infinity Instruct (BAAI, 2024). The results, shown in Table 7, indicate that the model outperformed the Llama-3.1-8B-Instruct on some benchmarks. Table 7: Results on Infinity-Instruct-7M-Gen-Llama3.1-8B Model Len AE2 MT GPQA Llama3.1-Instruct Infinity-Llama3.1-SFT Infinity-Llama3.1-SSO Iter3 2146 1758 32.8 37.5 50.0 8.34 7.49 8. 27.3 24.7 37.4 MMLU Pro 42.9 40. 42.9 MATH GSM8K 40.9 33.4 35.8 80. 76.6 80.7 Other implementation of We further explored the effectiveness of other implementations of 5. We optimized the policy model to maximize the average probability of generating yo with x+ and x. We called this function : = Sigmoid (cid:0) (cid:0)α πθ(yox+) + (1 α)πθ(yox)(cid:1)(cid:1) (6) We then optimized Llama3.1-instruct with the SSO constructed with . Results are shown in Figure 4. 8 Under review as conference paper at ICLR (a) Results on AlpacaEval 2.0. Figure 5: Results of Different Optimization Loss on Llama3.1-Instruct. (b) Results on MT Bench."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Preference Alignment with Human Preference Researchers have proposed various algorithms to align large language models (LLMs) with human preference. These algorithms can broadly be categorized into reward model-based approaches and direct preference optimization methods, with RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2023) as representative examples. Ziegler et al. (2020); Ouyang et al. (2022); Bai et al. (2022a) train reward model based on annotated human preference data and employ reinforcement learning algorithms such as PPO (Schulman et al., 2017) to align LLMs. However, these algorithms require numerous preference labels and online sampling during the training process. To further reduce costs, direct preference optimization (DPO), sequence likelihood calibration (SLiC) (Zhao et al., 2023), and identity preference optimization (IPO) (Azar et al., 2023) simplify the RLHF objective by directly increasing the margin between chosen and rejected responses. Additionally, Kahneman-Tversky optimization (KTO) (Ethayarajh et al., 2024) utilizes human feedback in binary format, avoiding dependency on pairwise preference data. Our methodology primarily depends on direct preference optimization techniques. While we employ IPO as the foundational loss for our model, we demonstrate in Appendix A.1 the versatility of our approach, emphasizing its adaptability and broad applicability across diverse objective functions. Automated alignment Previous alignment studies rely on manually annotated preference data and algorithms like RLHF and DPO to conduct model alignment. However, annotating preference data requires expensive and high-quality human effort, limiting the development of related methods. Moreover, with the rapid advancement of LLMs, their capabilities have gradually approached or even surpassed human levels, making it challenging for humans to produce meaningful supervise data for LLMs (Burns et al., 2023). Recently, numerous studies have found that data generated by LLMs can reach the quality of ordinary manual annotations (Zheng et al., 2024b). These findings increased the attention of automated alignment (Yuan et al., 2024; Chen et al., 2024). Automated alignment aims to minimize human intervention, addressing the prohibitively expensive cost of human annotation. Current methods can be divided into four types based on the source of alignment signals (Cao et al., 2024): 1) Inductive Bias, which automatically guides the model to generate preference signals to align itself by introducing appropriate assumptions and constraints (Huang et al., 2023; Bai et al., 2022b; Yang et al., 2024b; Yuan et al., 2024; Chen et al., 2024). 2) Behavioral Imitation, which achieves automatic alignment by imitating the behavior of another already-aligned model (Peng et al., 2023; Tunstall et al., 2023; Burns et al., 2023). 3) Model Feedback, which optimizes the policy model through feedback from other models (Lee et al., 2023; Hosseini et al., 2024). 4) Environmental Feedback, which aligns models by obtaining alignment signals or feedback through environmental interaction (Liu et al., 2023; Qiao et al., 2024). Our approach falls under the Inductive Bias. The most related works are RLCD (Yang et al., 2024b) and SAIM (Franken et al., 2024). However, they do not guarantee learnable, on-policy, and accurate synthetic signals during iterative training."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we proposed novel approach called SSO (Self-Steering Optimization) to enhance model alignment by iteratively optimizing the learnability and accuracy of generated preference data. SSO achieved self-optimization through an additional self-steering loss controlling the accuracy of the preference data, as well as weight function that regulates the data to be learnable and 9 Under review as conference paper at ICLR 2025 on-policy. These mechanisms relieve the gradual quality decline of generated signals in automated alignment. Our approach demonstrated effectiveness through subjective and objective benchmarks, including AlpacaEval, MT-Bench, GPQA, GSM8K, etc. Notably, our method significantly improves Llama-3.1 and Qwen2 without additional human feedback, surpassing the baselines. We further verified the effectiveness of SSO on offline training and RM training, demonstrating the prospects and effectiveness of SSO in these areas. Verified by wide and deep experiments, SSO substantially enhanced the quality of synthetic preference data and effectively benefited model alignment. Our work underscores the importance of learnable and accurate signals in automated alignment, suggesting the feasibility of aligning models without human annotations."
        },
        {
            "title": "7 LIMITATIONS",
            "content": "Despite SSO performing well across multiple benchmarks, we must acknowledge that there are still some limitations. Firstly, the design of the and functions is too simplistic. The function is not specially designed but directly uses existing loss. While SSO can work with broader range of base losses, it may also incur unnecessary computational costs, such as redundant KL Loss calculations, leading to SSOs relatively high overhead in model optimization. Similarly, the function directly uses average generation probability, but as reported in some works Zhou et al. (2024), employing more complex weight functions could yield better results. Secondly,SSO is based on principlebased automated alignment. This may slightly limit its application scenarios. However, considering the increasing research on automated alignment, we believe that studies like SSO will have considerable usage."
        },
        {
            "title": "8 FUTURE WORK",
            "content": "In previous experiments, all the principles we used were manually defined. We are now considering fully automated SSO, where the policy model generates both the features and principles. Preliminary experiments show that generated principles can improve data diversity and alignment benefits. Additionally, we are also considering designing new and functions. As mentioned in the last section, the SSO loss we used is quite simple from the design perspective. We believe that better designs could bring more alignment benefits. Lastly, SSO can be applied beyond principle-based automated alignment. We are considering extending SSO to other automated alignment paradigms, which we believe is feasible."
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. general theoretical paradigm to understand learning from human preferences, 2023. URL https://arxiv.org/abs/2310.12036. BAAI. Infinity instruct. arXiv preprint arXiv:2406.XXXX, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback, 2022a. URL https://arxiv.org/abs/2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. ArXiv preprint, abs/2212.08073, 2022b. URL https: //arxiv.org/abs/2212.08073. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision, 2023. URL https://arxiv.org/abs/2312.09390. Under review as conference paper at ICLR 2025 Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, and Bowen Yu. Towards scalable automated alignment of llms: survey, 2024. URL https://arxiv.org/abs/2406.01252. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. ArXiv preprint, abs/2401.01335, 2024. URL https://arxiv.org/abs/2401.01335. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2024. URL https://openreview.net/forum?id=pNkOx3IVWI. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 30293051, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.183. URL https://aclanthology.org/2023.emnlp-main.183. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf, 2024. URL https://arxiv.org/abs/2405.07863. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators, 2024. URL https://arxiv.org/ abs/2404.04475. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024. URL https://arxiv.org/abs/ 2402.01306. Jan-Philipp Franken, Eric Zelikman, Rafael Rafailov, Kanishk Gandhi, Tobias Gerstenberg, and Noah D. Goodman. Self-supervised alignment with mutual information: Learning to follow principles without preference labels, 2024. URL https://arxiv.org/abs/2404.14313. Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment performance of generative language models, 2024. URL https://arxiv.org/abs/2404. 09824. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners, 2024. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id= uuUQraD4XX. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via reinforcement learning, 2024. URL https://arxiv.org/abs/2409.12917. Under review as conference paper at ICLR 2025 Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. https://huggingface. co/spaces/allenai/reward-bench, 2024. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human feedback with AI feedback, 2024a. URL https://openreview.net/ forum?id=AAxIs3D2ZZ. Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, and Youngjae Yu. Aligning large language models by on-policy self-judgment, 2024b. URL https://arxiv. org/abs/2402.11253. Chris Yuhao Liu and Liang Zeng. Skywork reward model series. https://huggingface.co/ Skywork, September 2024. URL https://huggingface.co/Skywork. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language models on simulated social interactions, 2023. AI @ Meta.(A detailed author list can be found in llama3 report) Llama Team. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. OpenAI. Introducing chatgpt, 2023. URL https://openai.com/index/chatgpt/. Accessed: 2023-10-01. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4, 2023. Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. Making language models better tool learners with execution feedback, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization: Your Language Model is Secretly Reward Model, 2023. URL https://arxiv.org/abs/2305.18290. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of LLMs should leverage suboptimal, on-policy data. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=bWNPx6t0sF. Under review as conference paper at ICLR 2025 Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023. Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. Self-taught evaluators, 2024a. URL https://arxiv.org/abs/2408.02666. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024b. URL https://arxiv.org/abs/2406.01574. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge, 2024. URL https://arxiv.org/abs/2407.19594. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024a. URL https://arxiv.org/abs/2407.10671. Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. RLCD: Reinforcement learning from contrastive distillation for LM alignment. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum? id=v3XXtxWKi6. Hai Ye and Hwee Tou Ng. Self-judge: Selective instruction following with alignment selfevaluation, 2024. URL https://arxiv.org/abs/2409.00935. Runsheng Yu, Yong Wang, Xiaoqi Jiao, Youzhi Zhang, and James T. Kwok. Direct alignment of language models via quality-aware self-refinement, 2024. URL https://arxiv.org/abs/ 2405.21040. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. ArXiv preprint, abs/2401.10020, 2024. URL https: //arxiv.org/abs/2401.10020. Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, and Peter Liu. Calibrating sequence likelihood improves conditional language generation. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=0qSOodKmJaN. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum?id=shr9PXz7T0. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Judging llm-as-a-judge with mt-bench Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2024b. URL https://proceedings.neurips.cc/paper_files/paper/2023/ hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_ Benchmarks.html. 13 Under review as conference paper at ICLR Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models, 2024c. URL https://arxiv.org/abs/2403.13372. Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. Wpo: Enhancing rlhf with weighted preference optimization, 2024. URL https://arxiv.org/abs/2406.11827. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2020. URL https://arxiv.org/abs/1909.08593. 14 Under review as conference paper at ICLR"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ADDITIONAL RESULTS This section includes the results that are not shown in the body text. A.1.1 DETAILED RESULTS OF INSTRUCT MODELS Here are the detailed results of the Instruct models. Table 8: Results on Llama3.1-8B-Instruct and Qwen2-7B-Instruct. Iter Len AE2 MT GPQA MMLU Pro MATH GSM8K Len AE2 MT GPQA MMLU Pro MATH GSM8K Llama3.1-Instruct Qwen2-Instruct 2146 32.8 8.34 27.3 42.9 40.9 80.8 1786 33.2 8.37 25.8 44. 50.4 80.4 Iter1 2204 35.0 8.19 33.3 Iter2 2211 37.2 8.10 36.9 Iter3 2177 39.3 8.00 31.3 Iter1 2292 40.2 8.31 31.3 Iter2 2588 37.8 8.38 31.8 Iter3 2936 27.2 8.28 30.8 Iter1 2220 39.0 8.37 32.8 Iter2 2416 40.7 8.45 35.4 Iter3 2670 39.2 8.48 32.3 UltraFeedBack+IPO 41.9 42.8 42.8 82.2 82.0 82.9 1955 35.6 8.17 28.8 1976 31.0 8.23 26.3 1999 19.3 7.79 25.3 Modified PBAA(IPO Based) 42.5 41.6 42.3 42.3 43.3 43. 83.4 79.6 73.4 2252 34.6 8.41 29.8 3034 32.0 8.38 30.3 4458 30.7 8.41 30.3 SSO(IPO Based) 82.6 83.5 81.9 2062 34.9 8.42 30.3 2390 35.1 8.46 29.8 2789 36.2 8.47 27.3 44.1 45.1 46. 45.7 47.1 46.8 45.7 47.3 47.4 44.5 44.3 43.8 44.8 44.3 44.2 44.2 44.7 44.5 46.8 38.9 30. 49.7 43.3 32.4 50.0 51.6 50.4 76.9 73.8 71.1 77.1 73.5 70.4 79.8 77.6 77.0 A.1.2 SSO BASED ON OTHER DPO LOSSES To illustrate the broad applicability of our method, we conducted experiments on SSO based on vanilla DPO Loss. The training parameters are the same as the main experiments, with only the Base Loss of SSO modified. As presented in Table 9, the observed gains demonstrate SSOs scalability, suggesting that SSO can integrate with other DPO Losses, fully leveraging existing studies. We plan to explore SSOs applicability in future work across wider range of DPO losses. Table 9: Results with DPO Loss, SSO here is based on DPO Loss instead of IPO Loss. AE2LW represent AlpacaEval2 Length-Control Win Rate, AE2W represent AlpacaEval2 Win Rate AE2 WR AE2 LWR AE2 LWR AE2 WR"
        },
        {
            "title": "Len",
            "content": "MT MT"
        },
        {
            "title": "Model",
            "content": "Qwen2 Llama3,"
        },
        {
            "title": "Instruct",
            "content": "DPO-Iter1 DPO-Iter2 DPO-Iter3 SSODP O-Iter1 SSODP O-Iter2 SSODP O-Iter3 1786 2245 2877 3653 2125 2301 2611 33. 33.5 35.1 32.9 33.8 38.1 37.2 29.0 36.5 42.9 44.6 34.9 41.6 43.4 8. 8.31 8.35 8.27 8.35 8.17 8.46 2146 2373 2693 2947 2405 2584 2745 32. 37.7 38.2 40.0 35.1 37.5 41.4 35.2 42.4 45.6 49.3 40.3 44.4 43.2 8. 8.42 8.54 8.39 8.38 8.40 8.57 15 Under review as conference paper at ICLR 2025 A.1.3 RESULTS ON LLAMA3-8B This part shows our results on Llama3-8B using the same training parameters as the body text. We did not include them in the body text due to length limitations. Instead of training our SFT model, we reuse the open-source model from Online-RLHF (Dong et al., 2024). The model is trained from Llama-3-8B on mixture of diverse open-source high-quality data for 1 epoch. We havent analyzed its training data, so this part of the results may differ from other parts. Table 10: Results on Llama3-8B-SFT (Dong et al., 2024) and Llama3-8B-Instruct."
        },
        {
            "title": "Len",
            "content": "AE2 LWR AE2 WR Llama3-SFT MT"
        },
        {
            "title": "Len",
            "content": "AE2 LWR AE2 WR MT Llama3-Instruct 1126 13. 7.8 7.23 1965 33.6 33.1 7. Iter1 Iter2 Iter3 1704 1859 1932 Iter1 Iter2 Iter3 1647 2900 6170 Iter1 Iter2 Iter3 1345 1647 24.8 33.8 33.2 29.4 30.8 15.2 24.2 29.8 32.7 UltraFeedBack+IPO 21.2 30.9 33.1 8.02 8.07 7. 1963 1935 1904 35.5 37.2 37.5 Modified PBAA(IPO Based) 23.2 34.3 21.1 15.8 24.3 34.5 7.82 8.02 7. 2070 2598 3379 SSO(IPO Based) 7.75 7.82 8.05 2004 2306 2760 37.4 35.5 25.6 36.6 37.6 33. 21.2 30.9 33.1 39.2 44.7 38.6 36.3 42.2 43.7 7.84 7.90 7.95 8.01 8.25 8.10 7.92 8.24 8. A.1.4 DATA SELECTION"
        },
        {
            "title": "Len",
            "content": "Table 11: Results on Filtered dataset MMLU Pro AE2 MT GPQA MATH GSM8K"
        },
        {
            "title": "SFT",
            "content": "967 6."
        },
        {
            "title": "Ultrafeedback",
            "content": "1283 11."
        },
        {
            "title": "SSO",
            "content": "1319 18."
        },
        {
            "title": "SSO",
            "content": "2146 2105 2446 32.8 41.2 41. Llama3.1-SFT 32.3 32.3 32.8 37.6 38. 35.5 Llama3.1-Instruct 27.3 32.8 36.1 42. 46.1 48.6 6.69 7.23 7.36 8. 8.13 8.58 20.6 20.1 20.6 40. 42.8 43.3 62.9 61.2 62.9 80. 82.9 84.5 The iterative alignment process produced thousands of preference data. We filtered these intermediate results and selected over 50k high-quality data points. Specifically, our filtering process consisted of three steps: 1. Building pre-filtered set: We selected all data from iterations 1 and 2 synthesized by all models and methods. For iteration 3, considering that methods other than SSO often have lower accuracy, we only chose data produced by the SSO method. After removing duplicates, we obtained nearly 300k data points. We then removed data where the length 16 Under review as conference paper at ICLR difference between chosen and rejected responses exceeded 3000 characters, resulting in about 226k partial pairs. 2. LLM-as-judge: Based on the pre-filtered set, we conducted round of judging using Llama3.1-8B-Instruct and Qwen2-Instruct as judges. The evaluation template was the same in A.3.2. For each pair, if any judge thought the quality of the rejected response was higher than the chosen one, it was removed. This procedure left us with 110k partial pairs. 3. Length filtering: Finally, we performed round of length filtering to ensure the average lengths of chosen and rejected responses were close. We balanced the number of pairs where chosen responses were longer than rejected ones with those where chosen responses were shorter and reserved one pair for each query, resulting in filtered dataset. It is worth noting that, unlike ultrafeedback, our responses have more significant length differences. Therefore, although we brought the average lengths of chosen and rejected responses closer, this simple length control still carries risk of verbosity. A.2 DETAIL ABLATION Here are the detailed results of the ablation study. We train Qwen2-7B-Instruct and Llama3.1-8BInstruct under different ablations. Table 12: Results on Qwen2-7B-Instruct and Llama3.1-8B-Instruct under different ablations. AE2 MT AE2 MT Method Len Len Model Qwen2-7B-Instruct Llama3.1-8B-Instruct SSO w/o w/o w/o W, Iter1 Iter2 Iter Iter1 Iter2 Iter3 Iter1 Iter2 Iter3 Iter1 Iter2 Iter3 2062 2390 2789 2244 3001 4512 2042 2409 2252 3034 4458 34.92 35.12 36.18 35.12 33.43 36.07 35.38 36.07 36.03 34.55 32.02 30.70 8.42 8.46 8. 8.28 8.36 8.35 8.29 8.21 8.40 8.41 8.38 8.41 2220 2416 2670 2297 2592 2805 2226 2433 2292 2588 2936 39.02 40.73 39.57 39.30 37.35 30.44 39.59 40.13 34.25 40.22 37.75 27.24 8.37 8.45 8. 8.31 8.43 8.35 8.30 8.27 8.54 8.31 8.38 8.28 A.3 PROMPT TEMPLATES This section introduces the prompts and templates we used to generate training signals. A.3.1 PRINCIPLES This part shows the principles we use. Table 13: The principles we use. Each feature has good principle, bad principle, and pair of adjectives to indicate these principles. Feature Name Engagement Principles adjective: [Engaging, Dull] Under review as conference paper at ICLR 2025 Use friendly and conversational tone that Incorporate interactive Create responses that are designed to Good Principle: captivate the users attention and encourage active engagement. This involves personalizing the content to align with the users interests, preferences, and prior interactions. invites the user to participate in dialogue rather than simply receiving information. elements such as questions, prompts for feedback, or suggestions for further exploration. The goal is to foster sense of connection and make the experience enjoyable and fulfilling for the user. Bad Principle: impersonal, and fail to engage the user in any meaningful way. preferences, opting instead for generic content that does not resonate on personal level. detached tone that discourages conversation and makes the interaction feel transactional. elements, leaving the response static and uninviting. The overall effect should be one of disinterest and detachment, reducing the likelihood of the user feeling connected or motivated to continue the interaction. This involves ignoring the users interests and Produce responses that are monotonous, Avoid any interactive Use formal or Accuracy Literariness Commit to delivering responses that are adjective: [Accurate, Inaccurate] Good Principle: meticulously accurate and grounded in verified facts. This involves conducting thorough research to ensure the information provided is current, correct, and sourced from reputable and credible authorities. Double-check all facts, figures, and statements to eliminate errors Cite sources when necessary and misinterpretations. to substantiate claims and allow users to verify the information independently. Accuracy is paramount, as it builds trust and ensures that the user receives reliable and trustworthy guidance. Bad Principle: inaccuracies, outdated information, or unverified facts. This involves presenting information without proper research or verification, relying on assumptions, conjecture, or unreliable sources. Errors, misinterpretations, and factual discrepancies should be common, undermining the credibility and reliability of the response. references, leaving the user with no means to validate the information. Inaccuracy can lead to misinformation, which can have serious consequences for the users decisions and actions. Avoid citing sources or providing Provide responses that contain Craft responses that showcase refined adjective: [Literary, Boring] Good Principle: command of language and incorporate literary techniques to make the content more captivating and enjoyable. Utilize rich vocabulary, varied sentence structures, and employ literary devices such as metaphors, analogies, and allusions to enrich the narrative. should demonstrate an appreciation for linguistic artistry while still maintaining clarity and relevance to the users query. accessibility, ensuring that the literary elements enhance the message without overwhelming the reader. Strive for balance between eloquence and The response 18 Under review as conference paper at ICLR 2025 Bad Principle: Compose responses that lack literary finesse, using plain or crude language that detracts from the overall quality of the content. Avoid using any literary devices or stylistic elements that could elevate the text, opting instead for simplistic or repetitive phrasing. The response should feel unpolished and lacking in aesthetic appeal, potentially making it less engaging for the user. compelling narrative by failing to utilize the richness of language, resulting in response that is functional but devoid of literary merit. Disregard the opportunity to create more Helpfulness Comprehensiveness Consider the The goal is to empower the user with Focus on delivering responses that are adjective: [Helpful, Unhelpful] Good Principle: genuinely helpful and cater to the users specific needs. This involves actively listening to the users concerns, understanding their context, and providing tailored advice that directly addresses their situation. Offer practical solutions, step-by-step guidance, and actionable tips that the user can apply immediately. users capabilities, resources, and constraints when formulating advice. knowledge and tools that facilitate problem-solving or decision-making, enhancing their ability to take positive action. Bad Principle: irrelevant, or unhelpful, failing to address the users actual needs. context and circumstances presented by the user, offering generic advice that does not offer real solutions. should be impractical, difficult to apply, or completely unrelated to the users situation. actionable steps or guidance that could assist the user in resolving issues or making decisions. The response should leave the user feeling unsupported and unsure of how to proceed, undermining their confidence and ability to take effective action. This involves ignoring the specific Provide responses that are vague, Avoid providing any Advice Strive to deliver responses that are adjective: [Comprehensive, Incomplete] Good Principle: comprehensive, covering all pertinent aspects of the This involves conducting topic or question at hand. thorough research to gather wide range of information, including different perspectives, nuances, and details that contribute to holistic understanding. the response addresses all aspects of the users query, Include providing complete and detailed explanation. supplementary material, such as links, references, or examples, to enrich the response and offer additional resources for further exploration. approach demonstrates expertise and dedication to providing the user with well-rounded and informative answer. comprehensive Ensure that 19 Under review as conference paper at ICLR 2025 This involves Offer responses that are superficial, Bad Principle: lacking depth, or omit key details, resulting in an incomplete understanding of the topic. skimming over important aspects of the question, ignoring alternative viewpoints, or failing to delve into the nuances of the subject matter. information, leaving out critical details that would contribute to more thorough understanding. incorporating supplementary material or additional resources that could help the user gain broader perspective. feeling unsatisfied, uninformed, and unable to fully grasp the complexities of the topic. An incomplete response may leave the user Provide minimal Avoid Logicality Safety Construct responses that adhere to strict adjective: [Logical, Illogical] Good Principle: logical principles, ensuring each point flows seamlessly from the last, forming coherent and well-reasoned argument. Start with clear premise or statement of the issue, then methodically develop the argument by presenting supporting evidence, examples, and reasoning. Each segment of the response should logically progress to the next, building towards sound conclusion. Check for consistency and coherence throughout the response, eliminating any contradictions or illogical leaps. goal is to present rational and persuasive case that respects the users intelligence and fosters deeper understanding of the topic. Bad Principle: structure or logical flow, leading to confusion and weakened argument. premises, then jump from point to point without establishing clear connections or providing supporting evidence. Allow the response to meander, losing focus and failing to build coherent argument. with haphazard or unrelated statement, leaving the user uncertain about the intended message. Disregard the importance of logical progression, resulting in muddled and unconvincing response that undermines the users ability to understand and evaluate the information presented. Begin with ambiguous or contradictory Create responses that lack clear Conclude The Promote Encourage the user to seek professional help or adjective: [Safe, Unsafe] Good Principle: Design responses that prioritize the safety and well-being of the user at all times. This involves avoiding any content that could cause harm, offense, or distress, such as graphic descriptions, triggering topics, or insensitive language. responsible behavior by providing information on safety measures, precautions, and guidelines related to the topic. support when dealing with sensitive issues. the response creates safe and supportive environment where the user feels comfortable and respected. Bad Principle: safety and well-being of the user, potentially causing This involves including harm, offense, or distress. graphic or disturbing content, insensitive language, or triggering topics without warning. safety measures, precautions, or guidelines, leaving the user vulnerable to potential risks. irresponsible behavior by downplaying the seriousness of certain situations or providing misleading information. The response should create an unsafe environment where the user may feel uncomfortable, threatened, or disrespected. Generate responses that disregard the Avoid discussing Ensure that Encourage 20 Under review as conference paper at ICLR 2025 A.3.2 OTHER TEMPLATES Table 14: The template we use to allocate features to query. You are an excellent teacher who guides AI assistants in better replying to user queries. and your task is to identify the most crucial two features to reply to the query. Safety, Logicality, Comprehensiveness, Helpfulness, Literariness, Accuracy, Engagement. Each feature will be one of the following: Specifically, you will receive query, Each part of the response should The response should avoid causing harm, Covers all relevant aspects of the topic Ensures responses follow clear and logical Prioritizes the physical, emotional, and psychological - Safety: well-being of the user. offense, or distress and should promote responsible behavior. - Logicality: sequence from start to finish. build logically on the previous, culminating in well-reasoned conclusion. - Comprehensiveness: or question, providing broad and detailed understanding. response should encompass various perspectives, nuances, and details that contribute to full picture of the subject matter. - Helpfulness: addresses the users needs and aids in problem-solving or decision-making. context and circumstances presented by the user. - Literariness: use of literary language, literary devices, and stylistic elements that make the text more engaging and aesthetically pleasing. - Accuracy: up-to-date, and based on reliable sources. avoid errors, misinterpretations, and outdated facts. - Engagement: interaction. and use conversational tone to foster sense of connection and dialogue. Responses should be tailored to the users interests Enhances the quality of the response through the The response should be tailored to the specific Captivates the users attention and encourages Ensures the information provided is correct, Provides practical, actionable advice that The response should The You should identify several core features relevant to the query. Output the identified features in the following format: feature1,feature2,... Each feature should be separated by comma without any additional spaces. ### Query You will be given definition of task first, then an example. Follow the example to solve new instance of the task.nYou are given math word problem. You are supposed only to use subtraction on the numbers embedded in the text to answer the following question and then only report the final numerical Sam is 6 feet tall, and Alex is 5 feet answer.nnContext: tall nn Question: By how many feet is Sam taller than Alex? nSolution: 1nnNew input: Context: gave equal numbers of crackers to his three friends and still had 17 crackers left nn Question: give to his friends?nSolution: ### Output Logicality,Accuracy Matthew had 24 crackers. If Matthew Sams height - Alexs height = 6-5 = How many crackers did Matthew 1nWhy? ### Query What are some cuss words in English? ### Output Safety,Engagement 21 Under review as conference paper at ICLR ### Query How can use JavaScript to correct sentence in Grammarly data format while ensuring that the corrected sentence maintains the same overall meaning and context as the original sentence? nFor example, given the sentence \"He got that job because of his credentials.\", want to create JavaScript function that not only corrects the spelling error in the word \"because\", but also ensures that the sentence still accurately reflects the intended meaning of the original sentence. ### Output Accuracy,Logicality How can achieve this? ### Query {query} ### Output Table 15: The template we use to evaluate signal accuracy. <im start>system You are highly efficient assistant, who evaluates and selects the best large language model (LLMs) based on the quality of their responses to given instruction. This process will be used to create leaderboard reflecting the most accurate and human-preferred answers. <im end> <im start>user require leaderboard for various large language models. Ill provide you with prompts given to these models and their Your task is to assess these responses, corresponding outputs. and select the model that produces the best output from human perspective. ## Instruction {{ \"instruction\": }} ## Model Outputs \"{prompt}\", Here are the unordered outputs from the models. associated with specific model, identified by unique model identifier. Each output is \"{resp1}\" {{ {{ \"model identifier\": \"output\": }}, {{ \"model identifier\": \"output\": }} }} \"{resp2}\" ## Task \"m\", \"M\", Under review as conference paper at ICLR 2025 Evaluate the models based on the quality and relevance of their outputs, and select the model that generated the best output. Answer by providing the model identifier of the best model. will use your output as the name of the best model, so make sure your output only contains one of the following model identifiers and nothing else (no quotes, no spaces, no new lines, ...): M. We or ## Best Model Identifier <im end>"
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences, Beijing, China"
    ]
}