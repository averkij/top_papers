{
    "paper_title": "Mobile Video Diffusion",
    "authors": [
        "Haitam Ben Yahia",
        "Denis Korzhenkov",
        "Ioannis Lelekas",
        "Amir Ghodrati",
        "Amirhossein Habibian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video diffusion models have achieved impressive realism and controllability but are limited by high computational demands, restricting their use on mobile devices. This paper introduces the first mobile-optimized video diffusion model. Starting from a spatio-temporal UNet from Stable Video Diffusion (SVD), we reduce memory and computational cost by reducing the frame resolution, incorporating multi-scale temporal representations, and introducing two novel pruning schema to reduce the number of channels and temporal blocks. Furthermore, we employ adversarial finetuning to reduce the denoising to a single step. Our model, coined as MobileVD, is 523x more efficient (1817.2 vs. 4.34 TFLOPs) with a slight quality drop (FVD 149 vs. 171), generating latents for a 14x512x256 px clip in 1.7 seconds on a Xiaomi-14 Pro. Our results are available at https://qualcomm-ai-research.github.io/mobile-video-diffusion/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 3 8 5 7 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Mobile Video Diffusion",
            "content": "Haitam Ben Yahia* Denis Korzhenkov*"
        },
        {
            "title": "Amirhossein Habibian",
            "content": "Qualcomm AI Research {hyahia, dkorzhen, ilelekas, ghodrati, ahabibia}@qti.qualcomm.com"
        },
        {
            "title": "Abstract",
            "content": "Video diffusion models have achieved impressive realism and controllability but are limited by high computational demands, restricting their use on mobile devices. This paper introduces the first mobile-optimized video diffusion model. Starting from spatio-temporal UNet from Stable Video Diffusion (SVD), we reduce memory and computational cost by reducing the frame resolution, incorporating multi-scale temporal representations, and introducing two novel pruning schema to reduce the number of channels and temporal blocks. Furthermore, we employ adversarial finetuning to reduce the denoising to single step. Our model, coined as MobileVD, is 523 more efficient (1817.2 vs. 4.34 TFLOPs) with slight quality drop (FVD 149 vs. 171), generating latents for 14 512 256 px clip in 1.7 seconds on Xiaomi-14 Pro. Our results are available at https://qualcomm-ai-research.github.io/ mobile-video-diffusion/ 1. Introduction Video diffusion models are making significant progress in terms of realism, controllability, resolution, and duration of the generated videos. Starting from zero-shot video models [12, 25, 29, 35], which deploy pretrained image diffusion models to generate consistent frames, e.g., through cross-frame attention, modern video diffusion models rely on spatio-temporal denoising architectures, i.e., 3D UNets [2, 3, 18] or 3D DiTs [32, 67, 73]. This involves inflating image denoising models by adding temporal transformers and convolutions to denoise multiple frames simultaneously. Despite their impressive generation qualities, spatiotemporal denoising architectures demand high memory and computational power, which limits their usage to clouds with *Equal contribution Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. Snapdragon and Qualcomm branded products are products of Qualcomm Technologies, Inc. and/or its subsidiaries. Figure 1. Quality-efficiency trade-off. Our MobileVD accelerate SVD by 523 (in FLOPs) with slight decrease in the generation qualities (in FVD) reaching to better quality vs. efficiency tradeoff than alternatives. high-end GPUs. This hinders the wide adoption of video generation technology for many applications that require generating content locally on mobile devices. Prior work on accelerating video diffusion models has mostly focused on reducing the number of sampling steps [62, 71]. By extending the consistency models [58] and adversarial distillation [53] to video diffusion models, they managed to reduce the number of denoising steps from 25 to only 4 [62] and 1 step [71], which tremendously accelerates video generation. However, step distillation alone does not reduce the memory usage of the model, which is the key setback in deploying video diffusion models on mobile devices. This paper is the first attempt to build video diffusion models for mobile. Starting from the spatio-temporal UNet from Stable Video Diffusion (SVD) [2, 3], as representative for variety of video diffusion models, we conduct series of optimizations to reduce the size of activations to build mobile-friendly UNet: driven by the lower resolution needs for user-generated content on phones, we opt for using smaller latent space sufficient for generating 512 256 px frames. Instead of preserving the number of frames through1 out the denoising UNet, we introduce additional temporal downand up-sampling operations to extend the multi-scale representation both in space and time, which reduces the memory and computational cost with minimal loss in quality. Moreover, we discuss how naive visual conditioning through cross-attention leads to significant computational overhead that can be avoided without damaging visual quality. We further accelerate the mobile-friendly UNet by reducing its parameters using novel channel compression schema, coined channel funneling, and novel technique to prune the temporal transformers and temporal residual blocks from the UNet. Finally, following Zhang et al. [71], we reduce the number of denoising steps to single step using adversarial finetuning. This results in the first mobile video diffusion model called MobileVD, which is 523 more efficient (1817.2 vs. 4.34 TFLOPs) at slightly worse quality in terms of FVD (149 vs. 171) as reported in Fig. 1. MobileVD generates the latents for 14 512 256 px clip in 1.7 seconds on Xiaomi 14-Pro smartphone which uses Qualcomm Snapdragon 8 Gen 3 Mobile Platform. 2. Related work Video generation. Fueled by advancements in generative image modeling using diffusion models, there has been notable progress in the development of generative video models [2, 3, 15, 16, 23, 24, 31, 46, 51, 74]. These video models generally evolve from image models by incorporating additional temporal layers atop spatial blocks or by transforming existing 2D blocks into 3D blocks to effectively capture motion dynamics within videos. Although these advancements have paved the way for the generation of high-resolution videos, the significant computational demands make them impractical for use on low-end devices. In this work, we address this by optimizing representative of video generative model, SVD [2], to make it accessible to broader range of consumer-graded devices. Diffusion optimization. The problem of making diffusion models efficient naturally consists of the following two parts: (i) reducing the number of denoising steps and (ii) decreasing the latency and memory footprint of each of those steps. Reducing number of steps is achieved by using higher-order solvers [39, 40, 68], distilling steps to reduced set using progressive step distillation [33, 42, 52], straightening the ODE trajectories using Rectified Flows [34, 36, 75], mapping noise directly to data with consistency models [38, 57, 58], and using adversarial training [53, 54, 63, 71]. To decrease computational cost of each step, research has been done in weight quantization [21, 47, 55] and pruning [9, 33] as well as architectural optimization of the denoiser [10, 19, 30, 65]. In this work, following [71] we reduce number of steps to one using adversarial training and optimize the UNet denoiser using multiple novel techniques. On-device generation. On-device generation has attracted interest due to its ability to address privacy concerns associated with cloud-based approaches. There have been advancements in running text-to-image generation on mobile devices and NPUs [6, 8, 9, 33, 72]. While there has been progress in the video domain with fast zero-shot video editing models [26, 64, 70], there have been no attempts to implement spatiotemporal video generative models on device due to their high computational and memory requirements, which are challenging to meet within on-device constraints. In this work, we propose the first on-device model based on SVD [2] image-to-video model. 3. Mobile Video Diffusion In this section, we propose series of optimizations to obtain fast and lightweight version of an off-the-shelf image-tovideo diffusion model [2], suitable for on-device deployment. 3.1. Preliminaries Base model. We adopt Stable Video Diffusion (SVD) [2] as the base model for optimization. The SVD released checkpoint1 is an image-to-video model that by default generates 14 frames at the resolution 1024 576 with 25 sampling steps. To generate video from an image, the input image is first mapped into latent code of resolution 128 72 using Variational Auto-Encoder (VAE). Then it is duplicated 14 times and concatenated with noise latent of spatiotemporal resolution 14 128 72. The combined latent is then denoised by conditional UNet through an iterative process. Additionally, the input image is encoded with CLIP image embedding for use in cross-attention layers [49]. The UNet denoiser consists of four downsampling blocks, one middle block and four upsampling blocks. To handle the dynamics of video sequences, the model employs temporal blocks after spatial blocks. Notably, upand downsampling is conducted across spatial axes only, and temporal resolution of the latent is kept constant to 14 throughout the UNet. The UNet denoiser as-is is too resource-intensive for on-device use, requiring 45.43 TFLOPs and 376 ms per denoising step on high-end A100 GPU. Using FLOPs and latency as proxy metrics, we propose series of optimizations to make the model suitable for on-device deployment. Adversarial finetuning. In addition to the high cost of the UNet, the iterative sampling process in video diffusion models further slows them down. For example, with the conventional 25-step sampling it takes 11 seconds to generate video on high-end A100 GPU. To reduce the cost, we follow SF-V framework [71] and use adversarial finetuning, enabling our models to generate videos in single forward 1https : / / huggingface . co / stabilityai / stable - video-diffusion-img2vid/tree/9cf024d Figure 2. Effect of optimized cross-attention for mobile device. We show the number of cycles of the top-4 operations on mobile hardware for an input resolution of 128 128. Note that removing the no-op similarity map computation in cross-attention layers reduces cycles on softmax operations by roughly 80%. Figure 3. Channel funnels. We show an example of channel funnels applied to couple of layers within the model. At training time, funnels serve as adaptors reducing model width. At inference, they are merged with corresponding weight matrices without loss of quality. pass. Namely, we initialize the discriminator feature extractor with an encoder part of the denoising UNet and do not train it. After each block of this backbone, the extracted feature maps are passed to the two projection discriminator heads, one spatial and one temporal [44]. The heads also use the frame index and the CLIP embedding of the conditional image as input. At each training step, we apply the pseudo-Huber (Charbonnier) [7] and non-saturating adversarial loss [17] to the generator output to update its weights. To regularize the discriminator, the R1 penalty is used [43]. For further details please refer to the original SF-V work. 3.2. Mobile-friendly UNet Our first modifications to SVD architecture regard optimizations along the latent and feature resolution that affect both GPU and mobile latency. Then we highlight some lossless optimizations that have big effect on mobile latency. These are the first optimizations that allow us to run the UNet on device. Low resolution finetuning. To satisfy the memory constraints of mobile devices, we decrease the resolution of denoising UNet input to 64 32 by resizing the conditioning image to 512 256. While the released SVD checkpoint supports multiple resolutions, we found out that the original model demonstrates deteriorated quality for our target spatial resolution as reported in Tab. 1. Therefore, we finetuned the diffusion denoiser at our target spatial size. With this optimization, computational cost and GPU latency is reduced to 8.60 TFLOPS and 82 ms respectively, see Tab. 2. Temporal multiscaling. To further reduce computational burden, one might lower the input resolution more heavily. However, this significantly degrades visual quality. Instead, we can additionally downscale the feature maps by factor of 2 along either spatial or temporal axis after the first downsampling block of the UNet. To maintain the same output (cid:17) (cid:16) QK / shape, this is accompanied by the corresponding nearestneighbor upscaling operation before the last upsampling block. We refer to these two additions as multiscaling. In terms of computational cost, spatial multiscaling results in 51% reduction in FLOPs and 33% in GPU latency, while temporal multiscaling reduces FLOPs and GPU latency by 34% and 22%, respectively. For our final deployed model, we use temporal multiscaling as it offers better trade-off between quality and efficiency, as reported in Sec. 4.3.1. Optimizing cross-attention. In SVD, each cross-attention layer integrates information from the conditioning image into the generation process. The attention scores are computed similarly to self-attention layers, Attn(Q, K, ) = , but the key and value pair (K, ) softmax comes from the context tokens. However, the context in the cross-attention blocks always consists of single token, namely, the CLIP embedding vector of the conditional image. Consequently, each query token attends to only single key token. Therefore, computation of similarity map QK and softmax becomes no-op, and query and key projection matrices can be removed without any difference in results. While this loss-less optimization reduces GPU latency only by 7%, we found that it significantly impacts on-device behavior. In detail, at target resolution of 512 256, the model runs out of memory (OOM) on the device without the described modification of cross-attention. And at smaller resolution of 128 128 this optimization reduces mobile latency by 32%. The gain is attributed to the time-consuming nature of softmax operation on device, as shown in Fig. 2. 3.3. Channel funnels Channel size, which refers to the width of neural network layers, is crucial for scaling models. Increasing channel size generally enhances model capacity but also increases the number of parameters and computational cost. Research 3 has focused on compressing the number of channels by either discarding less informative channels through channel pruning [13, 45] or representing weight matrices as low-rank products of matrices using truncated singular decomposition [69]. However, these could have sub-optimal tradeoffs in quality and efficiency when deployed on device. While low-rank decomposition is relatively straightforward to implement, it only reduces the number of parameters and computational complexity if the rank is reduced by more than half for feed forward layers. Additionally, not all layers of neural network types benefit equally from low-rank factorization. Moreover, this method does not reduce the size of output feature maps, which can cause significant memory overhead on mobile devices. In this part, we propose channel funnels, straightforward method to reduce the number of channels at inference time, with negligible quality degradation. Intuitively, channel funnel is placed between two affine layers, reducing the intermediate channel dimensionality to save computation. Consider two consecutive linear layers = W2σ(W1x) and the non-linear function σ in between, where W1 Rcinnercin, W2 Rcoutcinner . We introduce two funnel matrices, F1 Rccinner and F2 Rcinnerc , where < cinner, and rewrite our network as = W2F2σ(F1W1x). The -weights, having fewer channels, can be merged during inference with their associated -weights, resulting weight matrix with smaller inner dimension c, see Fig. 3. We refer to the reducing factor of the inner rank of the layers, i.e. c/cinner, as the funnel-factor. Initialization. We propose to use coupled singular initialization (CSI) for funnel matrices F1 and F2 that improves the model results, as demonstrated below. In this method we ignore the non-linearity and consider the effective weight matrix W2F2F1W1 which in practice has rank of c. For that reason, we aim to use such an initialization which mimics the best c-rank approximation of the original effective matrix. As Eckart-Young-Mirsky theorem implies, this can be achieved by means of truncated singular decomposition [11]. Let W2W1 = ΣV be the singular vector decomposition, to be its truncated c-rank version. Then it and UcΣcV and F1 = Σ1/2 suffices to set F2 = 1 to obtain W2F2F1W1 UcΣcV , where means the Moore-Penrose pseudoinverse. 2 UcΣ1/2 T Training. We apply channel funnels in attention layers where query and key embedding Wq and Wk are used to compute the similarity map of XWq (XWk)T . With funnel matrices Fq and Fk of size cinner c, we modify the aforementioned bilinear map as XWqFq (XWkFk)T = XWqFqF T . Similarly, funnels are applied to the pair of value and output projection matrices of selfattention layer. In our ablations we also show the impact of channel funnel on convolutions in residual blocks. Unless specified otherwise, we use fun-factor of 50%. 4 (a) Temporal blocks in the original architecture of SVD. (b) zero-one gate multiplier is sampled to each temporal block during training. Figure 4. Learned pruning of temporal blocks. (a) Each temporal block in the base SVD model is implemented as residual block w.r.t. its input xs. The output of temporal layers rt is summed with the input xs, and after that once again averaged with xs with learnable weight α. By reordering the terms, we derive the effective update rule αxs + (1 α) xt = xs + (1 α) rt. (b) During training, we introduce scalar gate ˆz {0, 1} to the residual update rule of each block. We learn importance values {qi}i of temporal blocks which are transformed to inclusion probabilities {pi}i at each training step. Zero-one gate multipliers are sampled according to those probabilities. To enable end-to-end training, we use straight-through estimator trick. At inference, only blocks with highest importance values are used. 3.4. Temporal block pruning Motivation. The original UNet in the SVD model does not contain 3D convolutions or full spatiotemporal attention layers. Instead, to model motion dynamics, SVD incorporates temporal blocks after each spatial block. The output of such group is linear combination of the spatial and temporal block outputs, xs and xt respectively, αxs + (1 α) xt, where α is weight scalar that emphasizes spatial features when higher and temporal features when lower, see Fig. 4a. While this approach leverages image model priors when extending the model to videos, it adds computational cost. Moreover, not all of these blocks are equally important for maintaining quality. Here, we propose learnable pruning technique to remove less important temporal blocks while minimizing quality degradation. To this end, for each temporal block we define an importance value qi, 0 qi 1 where = 1, . . . , and is number of temporal blocks. The values {qi}i are trained to identify the blocks that are the most crucial for model performance. At inference time, Figure 5. Comparison with recent models. We provide the 1st, 6th, 10th and 14th frames from the videos generated with different models2. For AnimateLCM [62] and SF-V [71] we downsampled the released high-resolution videos from Zhang et al. [71]. For SVD [2] and our MobileVD model, videos were generated at their native resolution, 1024 576 and 512 256 respectively. only blocks with the highest importance qi are kept where is the budget chosen in advance. In our experiments we found it possible to remove as many as 70% of all temporal blocks which leads to 14% reduction in FLOPS as compared to the model with optimizations from Sec. 3.2 applied. Training. At each training iteration, we sample randomly blocks which participate in the computational graph. To this end, we define the indicator variable zi {0, 1} where zi = 1 if i-th block is sampled for participation, and zi = 0 otherwise. The sum of all participants should equal to the budget value i.e. (cid:80) zi = n. Note that [(cid:80) pi = n, where pi is the ini clusion probability of the i-th block. We relate i-th importance value qi to the inclusion probability of pi using the [zi] = (cid:80) zi] = (cid:80) constrained optimization problem min c,{pi}i (cid:88) (pi cqi)2, s.t. (cid:88) pi = n, 0 pi 1, 0. We obtain closed-form solution of the above optimization with Lagrange multipliers which is differentiable w.r.t. qi. In simple words, we find the proper set of inclusion probabilities pi cqi, with importance values qi and proportionality coefficient c. After obtaining {pi}i at each training iteration, we sample blocks without replacement using Brewers sampling [4, 60]. As such sampling is non-differentiable, we employ straight-through estimators (STE) [1] to enable 2Conditioning images are under MIT license 2024 Fu-Yun Wang. https://github.com/G-U-N/AnimateLCM/blob/9a5a314/ LICENSE.txt Model NFE FVD TFLOPs Resolution 1024 576 50 SVD AnimateLCM 8 UFOGen 1 LADD 1 SF-V 1 Resolution 512 256 SVD MobileVD (ours) 50 1 149 281 1917 1894 181 476 45.43 45.43 45.43 45.43 45.43 8.60 4.34 Latency (ms) GPU Phone 376 376 376 376 376 82 OOM OOM OOM OOM OOM OOM 1780 Table 1. Comparison with recent models. FLOPs and latency are provided for single function evaluation with batch size of 1. For rows marked with asterisk FVD measurements were taken from Zhang et al. [71], while performance metrics are based on our measurements for UNet used by SVD. For consistency with these results, FVD for SVD and our MobileVD model was measured on UCF-101 dataset at 7 frames per second. end-to-end training. Namely, we define gate ˆzi as STE of the probability pi, i.e. ˆzi = pi + stop gradient(zi pi). The output of temporal block is multiplied by this gate which serves as an analog of Dropout layer [22], as Fig. 4b shows. For practical aspects of training, please refer to the Appendix B.4. 4. Experiments In this section, we describe our experimental setup, followed by qualitative and quantitative evaluation of our model. Finally, we present ablations to justify our choices for the final model deployed on the device. 4.1. Implementation details Dataset. For our experiments, we use collected video dataset. We follow the data curation pipeline from OpenSora [73, V.1.1], selecting videos with motion scores between 4 and 40, and aesthetic scores of at least 4.2. This results in curated dataset of approximately 31k video files for finetuning. Micro-conditioning. UNet used by SVD, has two conditioning parameters called FPS and Motion bucket id. To obtain videos with different FPS, we chose each k-th frame from the video with randomly sampled k, 1 4, and adjusted the native FPS value of the video accordingly. The notion of motion bucket has not been properly documented at time of the model release. While it is connected with the speed of motion in the video, as described in the original SVD paper, the motion estimator has not been open-sourced. For that reason, we implemented our own definition of the motion bucket using simple heuristic. For the sampled chunk of 14 frames, we converted them to gray color, spatially downsampled to the resolution of 14 128 64, and reshaped to the matrix of size 14 (128 64). After that, we computed the singular values of that matrix. Note that for completely static video this matrix has rank of 1, and therefore the only non-zero singular value. And the less similar frames are, the more singular components are needed to faithfully reconstruct the full video. Based on that observation, we re-defined the motion bucket as the area under the normalized cumulative sum of singular values. Training. For training, we begin with the original SVD weights, apply all the optimizations (excluding temporal block pruning), and train the resulting UNet with standard diffusion loss for 100k iterations on 4 A100 GPUs with total batch size of 8. This UNet serves as the initialization for adversarial finetuning, where good initialization is crucial for fast convergence. We found that training for 5k iterations on 2 GPUs suffices for the second stage. We implement temporal block pruning in the second stage as we observed that excessive pruning in the first stage hinders model performance. In this case, we train the second stage for 10k steps. Check Appendix for more details. Metrics. We used DeepSpeed library [50, v0.14.2] to measure the number of FLOPs. For GPU latency, NVIDIA A100 SXMTM 80GB GPU was used. To measure GPU latency, UNet model was compiled using the PyTorch [48, v2.0.1] compiler with default settings. Phone latencies are measured on Xiaomi-14 Pro that uses Snapdragon 8 Gen. 3 Mobile Platform with Qualcomm HexagonTM processor. All performance metrics were measured for single UNet evaluation with batch size of 1. For video quality metric, we follow existing works [2, 71] by using Frechet Video Distance (FVD) [61] with I3D feature extractor [28]. We use the first frame of UCF-101 dataset [59] as the conditioning image, generating 14-frame clips at the models native resolution. Unless stated otherwise, we set the FPS, an SVD micro-condition, to 25, matching the UCF-101 frame rate [2]. For motion bucket, for our models we used the median value at the specified frame rate across UCF-101 data. For SVD the default bucket of 127 was used. 4.2. Results MobileVD. In Tab. 2 we compare MobileVD to our base model. As the results indicate, each optimization reduces speed of inference on mobile phone. Decreased spatial resolution and optimized cross-attention together unlock ondevice execution with latency of 3.6 seconds. Temporal downsampling layers in UNet make inference 29% faster. Additionally, temporal blocks pruning reduces phone latency by 13%, and channel funneling further decreases it by 9%. Empirically, we found that difference of up to 20 FVD units does not significantly affect visual quality and typically falls within the standard deviation when using different random seeds. Based on that, we see that our optimizations have minimal impact on FVD. SOTA comparison. In Tab. 1 and Fig. 1 we compare to the"
        },
        {
            "title": "NFE",
            "content": "FVD TFLOPs"
        },
        {
            "title": "7 FPS",
            "content": "Latency (ms)"
        },
        {
            "title": "Phone",
            "content": "SVD (resolution 1024 576) SVD (resolution 512 256) + low-resolution finetuning + optimized cross-attention + adversarial finetuning + temporal multiscaling + temporal block pruning + channel funneling 50 50 50 50 1 1 1 1 140 366 194 194 133 139 127 149 149 476 196 196 168 156 150 171 45.43 8.60 8.60 8.24 8.24 5.42 4.64 4.34 376 82 82 76 76 59 47 OOM OOM OOM 3630 3630 2590 2100 1780 Table 2. Effect of our optimizations. We successfully deployed the image-to-video model to mobile device without significantly sacrificing the visual quality. FLOPs and latency are provided for single function evaluation with batch size of 1. We call the model in the bottom row Mobile Video Diffusion, or MobileVD. Spatial multiscaling Temporal multiscaling FVD TFLOPs 133 138 145 163 8.24 5.42 4.35 3.39 Latency (ms) GPU Phone 76 59 51 48 3630 2590 2280 Table 3. Effect of additional multiscaling layers in UNet. We observe that both temporal and spatial multiscaling has good impact on mobile latency without compromising much on FVD, while combining the two increases FVD by noticeable amount. recent works that similarly aim for accelerating SVD imageto-video model, namely, AnimateLCM [62], LADD [53], SF-V [71], and UFOGen [66]. We observe that MobileVD leads to better FVD with significantly less computation. Qualitative results. Following previous works [62, 71], we show qualitative results with commonly used set of conditioning images. Sampled frames from the generated videos are presented in Fig. 5. For this visualization, we generated videos at 7 FPS and with spatial resolution of 512 256 using our MobileVD. Please refer to our webpage for the full videos. We observe that in general our method produces videos with sharp details and consistent motion. 4.3. Ablations In this section, we evaluate our design choices through ablation experiments. Unless otherwise specified, we use the SVD checkpoint with low-resolution input, optimized crossattention, and adversarial finetuning as the reference model, cf . Tab. 2. 4.3.1. Resolution impact in UNet In Tab. 3 we compare different latent multiscaling optimizations proposed in Sec. 3.2. Specifically, we investigate the impact of inserting spatial or temporal multiscaling layers after the first UNet block in terms of FLOPs, latency, and FVD. Spatial multiscaling offers better FLOPs and latency than temporal multiscaling and it increases FVD by 12 units compared to 5 for temporal downsampling. While we typically do not see video degradation with this increase in FVD, we do see clear degradation in video quality when using spatial instead of temporal multiscaling. We hypothesize that this is because the model already enjoys multiple stages of spatial downsampling, while temporal downsampling was originally absent. Based on these results, we have opted for temporal downsampling for our mobile-deployed model. We hold similar conclusions for combining the two multiscaling approaches with spatiotemporal multiscaling. 4.3.2. Funnel finetuning Fun-factor and funnel initialization. Reducing the width of affine layers in the model is form of lossy compression, and overly aggressive fun-factor values will hurt the model performance. In Tab. 4, we observe the impact of the fun-factor. Reducing the fun-factor to 0.25 results in performance loss of 22 FVD units compared to fun-factor of 1 (i.e., no compression). To avoid performance degradation from stacking multiple optimizations described in Sec. 3, we set the fun-factor to 0.5 for the deployed model. Additionally, the results highlight that the proposed coupled singular initialization (CSI) is essential for optimal funnel behavior, whereas the standard He initialization [20] is suboptimal. Funnel merging and low-rank layers. We compare channel funnels with multiple baselines in terms of FLOPs, on-device latency and FVD. All baselines are applied on the same attention layers, unless specified otherwise. The first baseline uses channel funnels but merges funnel and weight matrices during training, hence mimicking behavior at inference time as shown in Fig. 3. We report in Tab. 5 that keeping funnel and weight matrices separate at training performs equally well. The second baseline is applying funnels to convolutions in ResNet blocks instead of attention layers. While we obtain favourable FVD and even greater gain in Initialization Fun-factor FVD Coupled singular init. (CSI) Coupled singular init. (CSI) Coupled singular init. (CSI) Coupled singular init. (CSI) He init. [20] 0.25 0.50 0.75 1.00 0.50 155 132 145 133 Table 4. Effect of funnel initialization and fun-factor. Initialization funnels with CSI is crucial to getting good FVD as He initialization [20] obtains roughly 200 FVD units more. Additionally, we see that reducing the fun-factor beyond 0.5 starts to affect the performance. Width reduction method FVD TFLOPs Latency (ms) Original UNet + Funnels + Funnels (merge before finetune) + Funnels (convolutions) + Truncated singular decomposition + Truncated singular decomposition - 0.5 0.5 0.5 0.5 0. 133 132 138 139 142 130 8.6 8.0 8.0 7.2 8.6 8.0 3630 2870 2870 3400 3482 3345 Table 5. Comparison of model width reduction methods. We compare the proposed channel funneling (in grey) with finetuned low-rank approximation of individual attention layers with truncated singular decomposition. We additionally compare to Funnels applied to convolutions instead of attention. The reduction rate (referred to as fun-factor in case of funnels) is highlighted with r. TFLOPs (7.2 vs 8.6), it does not translate to the latency reduction we see with funnels on attention (3.40 vs 2.87 seconds). We hypothesize that the attention layers play greater role in reducing latency on device than convolutions for this model. The last baselines employ the standard technique of truncated singular decomposition of individual layers [69]. This decomposition breaks down weight matrix of linear layer Rcoutcin into low-rank product of two matrices W1 Rrccin and W2 Rcoutrc where is the rank reduction factor, < 1, and = min (cin, cout). Note that reduction in the number of parameters and FLOPs is achieved only if < 0.5, while the size of the feature map after these two matrices remain intact in this approach. While truncated decomposition after finetuning performs well in terms of FVD for both = 0.25 and = 0.5, it is slower on device compared to channel funneling (3.35 and 3.48 vs. 2.87 seconds respectively). This difference is attributed to memory transfer overhead from introducing additional layers as well as not decreasing the original feature size, emphasizing the benefit of funnels. 4.3.3. Temporal blocks pruning As mentioned in Sec. 3.4, in our experiments we found it possible to reduce up to 70% of all temporal blocks in the UNet. Notably, even with such high pruning rate the quality is comparable to the original model: it achieves FVD of 127 and requires 14% less FLOPs, while the original model has FVD of 139. However, pruning even further seems far from straightforward. FVD degrades to the values above 200 with Blocks pruned (%) FVD TFLOPs Latency GPU (ms) Our method 90 80 70 L1 regularization 70 53 201 245 207 165 4.06 4.35 4.64 4.67 5.17 42 44 47 48 52 Table 6. Impact of temporal blocks pruning. Our pruning outperforms the L1 regularization which does not have explicit control over the number of removed blocks. We use the checkpoint, optimized up to the temporal block pruning stage, as the starting point. pruning rate of 80%, and visual quality drops drastically. Additionally, we compare our method with another pruning baseline. As described in Sec. 3.4, the output of spatiotemporal block in SVD is linear combination of the spatial and temporal blocks xs and xt, respectively, αxs + (1 α)xt, where α is weight scalar. The baseline aims to minimize the influence of temporal blocks, by minimizing 1 α. This is done by adding an additional loss term during finetuning: = λ (cid:80) (1 αi). After training, blocks with the highest weight scalar αi are pruned. However, this method lacks explicit control over the desired pruning rate, as only weight hyperparameter λ can be adjusted. While effective for small pruning rates, this approach did not allow us to remove as many blocks as our method, achieving only 7% reduction in FLOPs with acceptable FVD level, see Tab. 6. 5. Conclusion This paper introduced the first mobile-optimized video diffusion model, addressing the high computational demands that have limited their use on mobile devices. By optimizing the spatio-temporal UNet from Stable Video Diffusion and employing novel pruning techniques, we significantly reduced memory and computational requirements. Our model, MobileVD, achieves substantial efficiency improvements with minimal quality loss, making video diffusion technology feasible for mobile platforms. Limitations. Despite the impressive acceleration achieved as steppingstone towards video generation on phones, the output is currently limited to 14 frames at low resolution of 256 512 pixels. The next step involves leveraging more efficient autoencoders to achieve higher spatial and temporal compression rates, enabling the generation of larger and longer videos at the same diffusion latent generation cost. Potential negative impacts. Our work is step towards making video generation technology accessible to broader audience, allowing users to create content on their phones with fewer access controls and regulations compared to cloudbased solutions."
        },
        {
            "title": "References",
            "content": "[1] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. 5, 15 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets, 2023. 1, 2, 5, 6, 12 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 1, 2 [4] Kenneth E. W. Brewer. Simple Procedure for Sampling πpswor1. Australian Journal of Statistics, 17(3):166172, 1975. 5 [5] Tim Brooks, Janne Hellsten, Miika Aittala, Ting chun Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei Efros, and Tero Karras. Generating Long Videos of Dynamic Scenes. In NeurIPS, 2022. 12 [6] Thibault Castells, Hyoung-Kyu Song, Tairen Piao, Shinkook Choi, Bo-Kyeong Kim, Hanyoung Yim, Changgwun Lee, Jae Gon Kim, and Tae-Ho Kim. Edgefusion: On-device text-to-image generation, 2024. [7] P. Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Barlaud. Two deterministic half-quadratic regularization algorithms for computed imaging . In ICIP, 1994. 3 [8] Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, and Matthias Grundmann. Speed is all you need: On-device acceleration of large diffusion models via gpu-aware optimizations. In CVPR, 2023. 2 [9] Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-Joon Kim, and Hyungjun Kim. Squeezing large-scale diffusion models for mobile, 2023. 2 [10] Tim Dockhorn, Robin Rombach, Andreas Blatmann, and Yaoliang Yu. Distilling the knowledge in diffusion models. In CVPRW, 2023. 2 [11] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211218, 1936. 4 [12] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In ECCV, 2023. [13] Gongfan Fang, Xinyin Tan Ma, Michael Bi Mi, and Xinchao Wang. Isomorphic Pruning for Vision Models. In ECCV, 2024. 4 [14] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, Jun-Yan Zhu, and Jia-Bin Huang. On the Content Bias in Frechet Video Distance. In CVPR, 2024. 12 [15] Genmo. Mochi-1, 2024. 2 [16] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing textto-video generation by explicit image conditioning, 2023. 2 [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 3 [18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In ICLR, 2023. 1 [19] Amirhossein Habibian, Amir Ghodrati, Noor Fathima, Guillaume Sautiere, Risheek Garrepalli, Fatih Porikli, and Jens Petersen. Clockwork diffusion: Efficient generation with model-step distillation. In CVPR, 2024. 2 [20] Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. 2015. 7, [21] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models. In NeurIPS, 2024. 2 [22] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors, 2012. 6 [23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models, 2022. 2 [24] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling, 2024. 2 [25] Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki Asano, and Amirhossein Habibian. Objectcentric diffusion for efficient video editing. In ECCV, 2024. 1 [26] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In CVPR, 2024. [27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020. 12 [28] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017. 6 [29] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In CVPR, 2023. 1 [30] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: lightweight, fast, and cheap version of stable diffusion, 2023. 2 9 [31] Kuaishou. Kling, 2024. 2 [32] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 1 [33] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. In NeurIPS, 2023. [34] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport, 2022. 2 [35] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, 2024. 1 [36] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow, 2022. 2 [37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [38] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models, 2024. 2 [39] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 2022. 2 [40] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2022. 2 [41] Ge Ya Luo, Gian Favero, Zhi Hao Luo, Alexia JolicoeurMartineau, and Christopher Pal. Beyond FVD: Enhanced Evaluation Metrics for Video Generation Quality, 2024. 12 [42] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In CVPR, 2023. 2 [43] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which Training Methods for GANs do actually Converge? In ICML, 2018. 3 [44] Takeru Miyato and Masanori Koyama. cGANs with Projection Discriminator. In ICLR, 2018. 3 [45] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Bhuminand Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact Language Models via Pruning and Knowledge Distillation. In NeurIPS, 2024. 4 [46] OpenAI. Sora, 2024. 2 [47] Nilesh Prasad Pandey, Marios Fournarakis, Chirag Patel, and Markus Nagel. Softmax bias correction for quantized generative models. In CVPR, 2023. 2 [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In NeurIPS, 2019. 6 [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In ICML, 2021. [50] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In SIGKDD, 2020. 6 [51] Runway. Introducing gen-3 alpha, 2024. 2 [52] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022. 2 [53] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion distillation, 2024. 1, 2, 7 [54] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In ECCV, 2024. 2 [55] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In CVPR, 2023. [56] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. StyleGAN-V: Continuous Video Generator With the Price, Image Quality and Perks of StyleGAN2. In CVPR, 2022. 12 [57] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models, 2023. 2 [58] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. 1, 2 [59] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild, 2012. [60] Yves Tille. Sampling algorithms. Springer, New York, 2006. 5 [61] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. FVD: new metric for video generation, 2019. 6 [62] Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song, Yu Liu, and Hongsheng Li. AnimateLCM: Accelerating the animation of personalized diffusion models and adapters with decoupled consistency learning, 2024. 1, 5, 7 [63] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion, 2022. 2 [64] Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, and Peter Vajda. Fairy: Fast parallelized instruction-guided video-to-video synthesis. In CVPR, 2024. [65] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers, 2024. 2 [66] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. In CVPR, 2024. 7 [67] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer, 2024. 1 10 [68] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator, 2022. [69] X. Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks for classification and detection. PAML, 2015. 4, 8 [70] Youyuan Zhang, Xuan Ju, and James Clark. FastVideoEdit: Leveraging consistency models for efficient text-to-video editing, 2024. 2 [71] Zhixing Zhang, Yanyu Li, Yushu Wu, Yanwu Xu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov, and Jian Ren. SF-V: Single Forward Video Generation Model. In NeurIPS, 2024. 1, 2, 5, 6, 7, 12 [72] Yang Zhao, Yanwu Xu, Zhisheng Xiao, and Tingbo Hou. Mobilediffusion: Subsecond text-to-image generation on mobile devices, 2023. 2 [73] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 1, 6 [74] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models, 2022. [75] Yuanzhi Zhu, Xingchao Liu, and Qiang Liu. Slimflow: Training smaller one-step diffusion models with rectified flow. In ECCV, 2025."
        },
        {
            "title": "Supplementary Material",
            "content": "Model NFE FVD TFLOPs Resolution 1024 576 SVD AnimateLCM UFOGen LADD SF-V MobileVD-HD (ours) Resolution 512 256 SVD MobileVD (ours) 50 8 1 1 1 1 50 1 149 281 1917 1894 181 476 171 45.43 45.43 45.43 45.43 45.43 23.63 8.60 4.34 Latency (ms) GPU Phone 376 376 376 376 376 82 45 OOM OOM OOM OOM OOM OOM OOM 1780 Table 7. Comparison with recent models. The set of optimizations proposed in our paper, can also be applied to high-resolution generation. FLOPs and latency are provided for single function evaluation with batch size of 1. For rows marked with asterisk FVD measurements were taken from Zhang et al. [71], while performance metrics are based on our measurements for UNet used by SVD. For consistency with these results, FVD for SVD and our models was measured on UCF-101 dataset at 7 frames per second. A. Additional results A.1. High-resolution generation In addition to our main effort to port the SVD model on mobile phone, we tested if our set of optimizations can be applied to high-resolution model. For this purpose, we trained model called MobileVD-HD which is capable of generating 14-frame videos with spatial size of 1024 576 px. Architecture hyperparameters used to finetune MobileVDHD, i.e. temporal multiscaling factor, fun-factor and number of pruned temporal blocks, were the same as for our lowresolution MobileVD. We report the results of that model in Tab. 7. As shown, it achieves visual quality on par with SF-V while decreasing its GPU latency by 40%. A.2. Visual quality metrics Although FVD is widely used metric for visual quality in the community, its ability to assess the faithfulness of motion as opposed to the appearance of individual frames is sometimes argued [5, 14, 56]. For that reason, we also evaluate the quality of different models using recently proposed JEDi (JEPA Embedding Distance) metric [41, v.0.1.3]. JEDi reportedly demonstrates much better correlation with the human preference than FVD. Since Luo et al. [41] have not recommended guidelines for comparison between generative models using their metric, we opted for the setup similar to FVD computation [2, 71]. In detail, we used the same set of clips from UCF-101, and similarly downsampled the videos to the resolution closest to 320 240, preserving the original aspect ratio, with the central crop afterwards. In Tab. 8 we provide the extended quantitative results. We observe that in general the new metric is better aligned with architecture optimizations that we apply. B. Additional details B.1. Training For diffusion training, we used AdamW optimizer [37] with learning rate of 1 106 and weight decay 1 103, while other hyperparameters were default. During adversarial finetuning, we also used AdamW. For generator the learning rate was equal to 1.25 106, and for discriminator we set it 10 times higher. For logits of importance values, used for pruning of temporal blocks, learning rate was equal to 1 103. Momentum weights for both optimizers we set as follows β1 = 0.5, β2 = 0.999. For generator, the weights for adversarial and pseudo-Huber loss were equal to 1 and 0.1 respectively. For discriminator, weight of R1 penalty was 1 106, and we applied it once per 5 iterations, as recommended in [27]. For high-resolution training, the first (diffusion) stage lasted for 10k iterations with total batch size of 4. The second (adversarial) stage comprised 30k iterations with batch size of 2. The learning rates for adversarial finetuning were twice as lower as for low-resolution case, except for the learning rate of importance values. R1 penalty was applied at each step. Other training aspects were the same both for MobileVD and MobileVD-HD. B.2. Decoding latents As our approach stems from SVD, the presented MobileVD is also latent model. Therefore, each generated latent code must be decoded to raw RGB pixels. For all the experiments reported in the main text, we used the native autoencoder weights released alongside the SVD model itself. The decoding was conducted independently for each frame. Notably, this model is relatively slow on device: it takes 91.5 ms per frame, resulting in 1,280 ms for decoding the full generated video. This timing is comparable with the latency of MobileVD (1,780 ms). As an alternative, we used the decoder from TAESD autoencoder3. It is significantly faster on smartphone: 6.4 ms per frame, or 90 ms for the full video. At the same time, the difference in quality metrics is negligible, see Tab. 9. 3https://huggingface.co/madebyollin/taesd/tree/ 614f768 Model NFE Resolution 1024 576 SVD MobileVD-HD Resolution 512 256 SVD + low-resolution finetuning + optimized cross-attention + adversarial finetuning + temporal multiscaling + temporal block pruning + channel funneling 50 1 50 50 50 1 1 1 1 FVD JEDi 25 FPS 7 FPS 25 FPS 7 FPS TFLOPs Latency (ms) GPU Phone 140 126 366 194 194 133 139 127 149 149 184 476 196 196 168 156 150 0.61 0.96 1.05 0.71 0.71 0.66 0.83 0.97 1.07 0.59 1.75 1.14 0.65 0.65 0.71 0.81 1.32 1.21 45.43 23.63 376 OOM OOM 8.60 8.60 8.24 8.24 5.42 4.64 4.34 82 82 76 76 59 47 45 OOM OOM 3630 3630 2590 2100 1780 Table 8. Effect of our optimizations. We successfully deployed the image-to-video model to mobile device without significantly sacrificing the visual quality. FLOPs and latency are provided for single function evaluation with batch size of 1. We call the model in the bottom row Mobile Video Diffusion, or MobileVD. The model trained with the same hyperparameters but intended for high-resolution generations is referred to as MobileVD-HD. Decoder FVD JEDi 25 FPS 7 FPS 25 FPS 7 FPS Latency (ms) Original decoder TAESD decoder 149 149 171 179 1.07 1.05 1.21 1. 1280 90 Table 9. Impact of latent decoder. While being significantly faster on device, decoder from TAESD has little to no impact on visual quality as measured by FVD and JEDi. B.3. Channel funnels be computed as Yp,j = (cid:88) Wq,j, Xp+q , (1) where , denotes inner product. Simply speaking, this means that convolution can be treated as linear layer with weight matrix of shape cout (kh kw cin) applied to each flattened input patch. At the same time, another way of reshaping the kernel is also possible. Consider tensor of shape kh kw cout hw defined as Attention layers. Consider query and key projection matrices in self-attention similarity map computation, XWq (XWk)T with layer input having shape of Lcin, and Wq and Wk of cin cinner. With funnel matrices Fq and Fk of size cinner c, we modify the aforementioned bilinear map as XWqFq (XWkFk)T = XWqFqF T . We apply our coupled singular initialization (CSI) by setting Fq = . For value and output projections matrices the initialization is applied in the way discussed in the main text. and Fk = UcΣ1/2 k VcΣ1/2 c Convolutional layers. Applying the same initialization to pair of convolutional layers is not straightforward. Weight tensors of 2D convolutional layers are 4-dimensional, and therefore computation of the effective weight tensor is not obvious. However, we make use of the following observation. Consider input tensor with shape cin. We refer to its 2-dimensional pixel coordinate as p, and therefore Xp is vector with cin entries. Let be convolutional kernel with size kh kw cout cin, and we refer to its spatial 2-dimensional coordinate as q, while = 0 is center of convolutional filter. For j-th output channel, Wq,j is also vector with cin entries. The layer output Rhwcout can 13 Eq,j,p = Wq,j, Xp . the convolution kernel In other words, reshaped as (kh kw cout) cin is multiplied by features of each input pixel. Then Eq. (1) can be rewritten as (2) Yp,j = (cid:88) Eq,j,p+q = (cid:88) Eq,j, δp+q , (3) where δ is 4-dimensional identity tensor, i.e. δu,v = 1 if = and 0 otherwise. Having said that, sequence of two convolutions can be presented as (i) flattening the input patches; (ii) matrix multiplication by the first kernel reshaped according to Eq. (1); followed by (iii) matrix multiplication by the second kernel reshaped as in Eq. (2); and with (iv) independent of kernels operation described by Eq. (3) afterwards. Therefore, coupled singular initialization can be applied to the product of matrices used in steps (ii) and (iii). We follow this approach and introduce funnels to the pairs of convolutions within the same ResNet block of denoising UNet. B.4. Pruning of temporal adaptors Practical considerations. In the main text we described that we transform the update rule of the temporal block as follows xs + ˆz (1 α) rt, where xs and rt are outputs of the spatial and temporal layers respectively, α is the learnable weight, and ˆz is zero-one gate multiplier. Note that if the temporal block is pruned, i.e. ˆzi = 0, then the gradient of the loss function w.r.t. the temporal blocks learnable parameters equals zero. This affects the gradient momentum buffers used by optimizers. For that reason, we do no update the momentum of the temporal blocks parameters in case it has been pruned by all the devices at current iteration of multi-GPU training. In the network, we parametrize the importance values qi with the sigmoid function with fixed temperature value of 0.1. In the same way weight coefficients αi were reparametrized. For faster convergence, each value qi is initialized with the weight of the corresponding temporal block, i.e. 1 αi. We also found necessary to set the learning rate for the logits of importance values qi significantly higher than for the other parameters of the denoising UNet. Constrained optimization. As discussed in the main text, we relate the importance values of temporal blocks {qi}N i=1 to their inclusion probabilities for sampling without replacement {pi}N i=1 by solving the following constrained optimization problem: min c,{pi}i s.t. (cid:88) (cid:88) (pi cqi)2 , pi = n, 0 pi 1. To solve it, we employ the common method of Lagrange multipliers assuming that all q-values are strictly positive. W.l.o.g. we consider the case of sorted values {qi}i, i.e. q1 q2 qN > 0. In detail, we define Lagrangian L(c, {pi}i , λ, β, {γi}i , {δi}i) = λ (cid:88) (cid:16)(cid:88) (pi cqi)2 (cid:17) pi + β γi (pi) (cid:88) (cid:88) δi (pi 1) + + inequalities = 2λ (pi cqi) + β γi + δi = 0 i, (6) pi c (cid:88) = 2λ (cid:88) (cqi pi) qi = 0, pi = n, γipi = 0 i, δi (pi 1) = 0 i, γi, δi 0 i, (cid:88) λ2 + β2 + (cid:0)γ + δ2 (cid:1) > 0. (7) (8) (9) (10) (11) (12) Case λ = 0. In this case γi δi = β = const. If β > 0, then γi > δi 0 γi > 0 pi = 0, which leads to contradiction. Cases β < 0 and β = 0 also trivially lead to contradictions. Case λ = 1. First, we derive that (cid:80) thus piqi, and = (cid:80) q2 = (cid:80) (cid:80) piqi q2 pi = 0, then (cid:80) qi > 0. = 0. (13) (14) Since L pi (cid:88) qi pi (cid:88) = 2 qi (pi cqi) + β (cid:88) qi (cid:88) qi (γi δi) (15) (4) (cid:88) qi (cid:88) qi (γi δi) = β = 0, and therefore, β = (cid:80) qi (γi δi) qi (cid:80) (16) (17) (18) . Lemma B.1. For all indices it holds true that γi = 0. Proof. Proof is given by contradiction. Let us assume that γk > 0 pk = 0 δk = 0 2cqk + β γk = 0 β = 2cqk + γk > 0. Then for any index such that > and, consequently, qj qk, the following equality holds true: (5) 2 (pj cqj) + β γj + δj = 2 (pj cqj) + 2cqk + γk γj + δj = 2pj + 2c (qk qj) + (γk γj) + δj = 0. (19) (20) (21) (22) and aim to solve the following system of equalities and 14 All the terms in the last sum are known to be non-negative except for γkγj. Therefore, γk γj γj > 0 pj = 0. We define index as the largest index for which γs = 0, i.e. γ1 = = γs = 0, γs+1 > 0. Note that > n, since otherwise the equality (cid:80) pj = 0 for > s. Now we can rewrite Eq. (18) as follows pi = cannot be satisfied. Also, For t, 2 (pi cqi) + β = 0 pi = cqi β 2 . Therefore, (cid:88) qiδi + (cid:88) qiγi i: is i: i>s (cid:88) pi = (cid:88) pi + (cid:88) pi = 1 + i: i<t i: it (cid:18) (cid:88) i: it cqi (cid:19) β 2 (23) = n. (33) Similarly, (cid:88) q2 = (cid:88) piqi = (cid:88) qi + (cid:88) i: i<t i: it (cid:18) qi cqi (cid:19) β 2 (34) For any given = 2, . . . , two last equations allow us to compute the values of and β. (cid:80) i: it qi (N + 1) (cid:80) i: i<t q2 (cid:80) i: it qi β 2 + 1 = (cid:80) i: i<t qi , (35) where is the total number of important values. The solution exists and is unique for each since, obviously, det (cid:80) i: it qi (N + 1) (cid:80) i: i<t q2 (cid:80) i: it qi > 0. In practice, we solve this matrix equation for each 2 n, test if the solution satisfies all the constraints, and after that select the solution that delivers the minimum value of our objective function. At least one proper solution always exists, since for = + 1 inclusion probabilities are equal to 1 for largest importance values and equal to 0 for all the rest indices. The solution of the system is differentiable w.r.t. all the qi, leading to differentiable probabilities pi. However, as mentioned earlier, we use only pi computed with these equations for t, while for < we set pi = 1. Therefore, we employ straight-through estimator for these indices [1]. β = 1 (cid:80) qi = 1 (cid:80) qi = 1 (cid:80) qi (cid:88) qiδi + (cid:88) i: is i: i>s qi (β 2cqi) (24) (cid:88) i: is qiδi 2c q2 + qi β (cid:80) i: i>s (cid:80) qi . (cid:88) i: i>s (25) After moving the last term from RHS to LHS, we obtain qi β (cid:80) i: is (cid:80) qi = 1 (cid:80) qi (cid:88) i: is qiδi 2c (cid:88) i: i>s q2 . (26) Note that LHS is obviously strictly positive, while RHS is non-positive. Since γi = 0, we rewrite Eq. (18), β = (cid:80) (cid:80) qiδi qi . (27) If δi = 0, then it is also true that β = 0, leading to pi = cqi. This is the case when inclusion probabilities are exactly proportional to the importance values. However, this is possible if and only if the maximum value q1 is not too large in comparison with other values, since otherwise p1 > 1. In this last case δk > 0 pk = 1 2 (1 cqk) + β + δk = 0 β = 2 (cqk 1) δk. For any index such that < we have 2 (pj cqj) + β + δj = 2 (pj cqj) + 2 (cqk 1) δk + δj = 2pj + 2c (qk qj) + (δj δk) 2 = 0. (28) (29) (30) (31) By regrouping the terms, we obtain 2pj + δj = 2c (qj qk) + δk + 2 > 2, (32) and since pj 1, this means that δj > 0 pj = 1. Therefore, if for some index it turns out that δk > 0, then for all smaller indices j, δj > 0, and consequently pj = 1. Again, let us define the index as the least index with zero δ coefficient, δt1 > 0, δt = δt+1 = = 0. Note that + 1, since more that inclusion probabilities cannot be equal to 1."
        }
    ],
    "affiliations": [
        "Qualcomm AI Research"
    ]
}