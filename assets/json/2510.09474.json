{
    "paper_title": "Multimodal Policy Internalization for Conversational Agents",
    "authors": [
        "Zhenhailong Wang",
        "Jiateng Liu",
        "Amin Fazel",
        "Ritesh Sarkhel",
        "Xing Fan",
        "Xiang Li",
        "Chenlei Guo",
        "Heng Ji",
        "Ruhi Sarikaya"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: https://mikewangwzhl.github.io/TriMPI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 7 4 9 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "MULTIMODAL POLICY INTERNALIZATION FOR CONVERSATIONAL AGENTS Zhenhailong Wang1, Jiateng Liu1, Amin Fazel2, Ritesh Sarkhel2, Xing Fan2, Xiang Li2, Chenlei Guo2, Heng Ji2, Ruhi Sarikaya2 1University of Illinois Urbana-Champaign, 2Amazon wangz3@illinois.edu, jihj@amazon.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Modern conversational agents such as ChatGPT and Alexa+ have become indispensable in everyday life. To handle diverse business requirements and enable agentic capabilities, these LLM-based systems often rely on predefined policies, which specify instructions such as model metadata, response styles, and tool-using rules. These policies, typically implemented as in-context prompts, are becoming increasingly complex and lengthy, posing challenges for models in faithfully following them. Moreover, they impose large fixed computational cost regardless of the input query. As multimodal conversational agents emerge, complex policies that govern multimodal tasks and even involve visual instructions are becoming increasingly necessary, yet they have been rarely studied in previous work. In particular, prior work on prompt compression has focused solely on reducing the length of task templates and demonstrations, which require limited reasoning compared to policies. Meanwhile, related work on policy alignment has been limited to internalizing text-only safety instructions. To bridge this gap, we introduce Multimodal Policy Internalization (MPI), new task that aims to internalize reasoning-intensive multimodal policies into the parameters of large multimodal model, enabling stronger policy-following behavior without requiring the policy to be included in-context during inference. MPI presents unique challenges from both data and algorithmic perspectives. We construct two new datasets that cover complex decision-making and tool-using tasks across both synthetic and real-world visual inputs. We investigate diverse internalization strategies and propose novel three-stage training framework, TriMPI, which enables stronger guidance from the original policy during internalization. Specifically, we first introduce continual pretraining stage before supervised finetuning, which directly injects policy knowledge into the model. We then propose PolicyRollout, simple yet effective extension to GRPO-style RL algorithms, which enables more grounded exploration by augmenting the rollout space with policy-aware responses. We show significant improvements of TriMPI over strong baselines in end-to-end performance, generalization capability, and robustness to catastrophic forgetting. As the first work on multimodal policy internalization, we aim to build strong foundation for future research by providing datasets, training recipes, and comprehensive evaluations. Code and data will be made publicly available for research purposes. Project page: https://mikewangwzhl.github.io/TriMPI."
        },
        {
            "title": "INTRODUCTION",
            "content": "Conversational agents such as ChatGPT, Claude, and Alexa+ (OpenAI, 2025; Anthropic, 2025; Alexa AI, 2025) have become integral to daily life. To manage diverse business rules and enable agentic functionality, these LLM-based systems often rely on predefined policies, which are structured instructions that specify model metadata, response styles, tool-usage rules, and more. These policies, typically provided as in-context prompt prefixes, are becoming increasingly long and complex (estimated to range from 1K to 50K tokens*), imposing substantial fixed computational cost regardless of the query size. In contrast, typical user queries usually range from 50 to 200 tokens (Clark *Exact numbers are not disclosed due to the proprietary nature of system prompts."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Motivation of the proposed Multimodal Policy Internalization task. The goal is to enhance the policy-following abilities of large multimodal model without requiring the policy to be provided in-context during inference, thereby improving both performance and efficiency. et al., 2025), leading to 20 to 250 higher input token cost from the policy prompts compared with the actual user queries. Moreover, as these policies expand and become more reasoning-intensive (e.g., requiring the model to follow rules distributed across different sections), models often struggle to adhere to them consistently (Yao et al., 2024; Qian et al., 2024). natural research question arises: can we internalize the knowledge of policies into the model parameters while also improving the models policy-following abilities? Previous research on prompt compression (Li et al., 2024) has shown initial success in reducing token usage through hard prompting (Li et al., 2023; Jiang et al., 2023), soft prompting (Mu et al., 2023; Ge et al., 2023), or progressive fine-tuning (Zou et al., 2024). However, these approaches primarily focus on compressing templates and demonstration examples, which require minimal reasoning. More recent work (Guan et al., 2024) introduced deliberative alignment, which aims to internalize knowledge of more complex safety specifications and emphasizes improving policyfollowing performance beyond token reduction. Nonetheless, it remains limited to issues of model trustworthiness and has been explored only in text-only models. With the growing trend of multimodal conversational agents, policies are increasingly tied to multimodal tasks and may even include visual instructions such as demo images. Yet, no prior work has explored how to learn and internalize complex policies in multimodal models. To address this gap, we propose new task, Multimodal Policy Internalization (MPI), which aims to train multimodal models that can generate policy-compliant responses without requiring the policy to be included in-context. Compared with prior work, the MPI task introduces several unique challenges: (1) The target policies focus on reasoning-intensive multimodal tasks, such as decision-making and tool usage for conversational agents. (2) There is lack of existing datasets containing multimodal question-answer pairs that adhere to predefined policies. (3) There is lack of established training paradigms for internalizing multimodal policies. For example, some text-only knowledge injection methods, such as continual pretraining (Ovadia et al., 2025; Maini et al., 2024), may not be directly applicable to MPI due to the presence of visual tokens. In this work, we contribute from both data and algorithmic perspectives to advance research in multimodal policy internalization. We first construct two new datasets, ClevrPolicy and GTAPolicy, to support training and evaluation across different multimodal policy types. Specifically, ClevrPolicy focuses on internalizing complex decision-making policies that require multi-hop reasoning. Built upon the synthesized CLEVR dataset (Johnson et al., 2017), ClevrPolicy provides flexible control over policy complexity and dataset size, enabling in-depth investigation of the effectiveness of different algorithms. Curated from the GTA dataset (Wang et al., 2024a), GTAPolicy targets multimodal tool-usage instructions with real-world images and user queries. GTAPolicy emphasizes low-data regime, where only limited question-answering data is available to demonstrate the expected behavior. From the method perspective, key challenge lies in how to align the response with the policy without assuming its existence during inference. Prior methods (Zou et al., 2024; Guan et al., 2024) focus solely on learning policy-following behavior, leaving it unclear how to better leverage the policy context itself as guidance during training. To address this, we introduce novel three-stage training framework, TriMPI, which incorporates two key ideas: (1) warming up the model through continual pretraining directly on the policy context; and (2) new RL algorithm, PolicyRollout, which enables additional conditioning on the policy context without introducing gap between training and inference. Specifically, TriMPI consists of visually-masked continual pretraining (VM-CPT) stage, chain-of-thought supervised finetuning (CoT-SFT) stage, and reinforcement"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: ClevrPolicy dataset. Left: Illustration of policy generation, where decision tree is first generated and converted into natural language instructions (see Appendix C.1 for details on the decision node ontology, and Figures 14, 15 for full policy examples). Right: Example input-output pair corresponding to the policy. The policy is available only during training and not during inference. learning (RL) stage with PolicyRollout. The VM-CPT stage enables language modeling directly on the multimodal policy, thereby explicitly injecting the entirety of the policy knowledge into the model and facilitating reasoning in later stages. The RL stage is essential for internalizing reasoningintensive policies, as it enables the model to learn from broader range of policy-related responses through trail-and-error rather than memorization. PolicyRollout further enhances RL exploration by augmenting the rollout space with policy-grounded responses, serving as simple yet effective extension to GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025). We demonstrate that TriMPI yields significant improvements in MPI, approaching 70.7% and 79.4% absolute gains over CoT SFT baseline and in-context setting, respectively. Beyond end-task performance, we also demonstrate enhanced generalization capability to policy updates and robustness against catastrophic forgetting. Further analysis shows that the improvements of TriMPI are consistent across different policy complexities and model sizes, with more pronounced gains observed on complex policies. To summarize, our main contributions are threefold: (1) new task that targets the internalization of complex policies in the multimodal domain; (2) two new datasets that support both analytical and real-world training and evaluation; and (3) new training algorithm that introduce an effective training paradigm with customized RL algorithm for policy internalization."
        },
        {
            "title": "2 PROBLEM FORMULATION",
            "content": "We formally define the proposed task, Multimodal Policy Internalization (MPI), as illustrated in Figure 1. Consider multimodal conversational agentic task, where, given an input text query and visual inputs I, the expected output is textual response A, which can be either free-form natural language or structured code for tool calling. Additionally, the response behavior should follow the instructions predefined in policy context . Note that the policy may include both textual PT and visual PI components. Let Mθ denote multimodal model parameterized by θ. The following illustrates the expected inputs and outputs before and after internalization: = Mθ(Q, I, ) Policy Internalization θ = Mθ(Q, I) (1) The goal is to embed the knowledge of the policy into the model parameters θ, enabling better policy-compliant generation without requiring in-context during inference. MPI shares the same high-level motivation as deliberative alignment (Guan et al., 2024; Zhang et al., 2025), where we emphasize improving models alignment with the policy beyond compressing the prompt. We do not consider training additional special embeddings as in soft prompting, since they are inherently tied to specific tasks (Li & Liang, 2021; Patel et al., 2025) and therefore limit the models ability to maintain general reasoning capabilities and robustness (Fan et al., 2025; Bailey et al., 2023)."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: GTAPolicy dataset. Left: illustration of the policy, consisting of two major parts, tool description and tool calling rules (see Figure 16 for the full policy). Right: input and output example corresponding to the policy. The visual input can contain multiple images. Table 1: Zero-shot in-context performance on the ClevrPolicy and GTAPolicy benchmarks. The metrics are reported as accuracy percentages (%) and are detailed in Appendix C.3. We observe strong correlation between the number of layers (N) and policy complexity. Introducing multimodal demonstrations into the policy (in ClevrPolicy-M) further increases the difficulty. Model ClevrPolicy-T ClevrPolicy-M GTAPolicy N=2 N=4 N=6 N=2 N=4 N=6 Tool Acc Arg Score Overall 36.60 10.85 4.80 33.05 8.05 4.55 Qwen2.5-VL-3B Qwen2.5-VL-7B 72.00 32.20 13.15 51.20 13.90 5.65 Claude-3.7-Sonnet 97.65 92.60 85.35 95.00 82.75 77.63 98.10 96.70 90.10 93.40 78.55 77.76 Claude-4-Sonnet 18.87 23.58 42.45 60.38 15.44 19.44 37.13 51.53 17.15 21.51 39.79 55."
        },
        {
            "title": "3 DATASET CREATION",
            "content": "3.1 CLEVRPOLICY DATASET We first create ClevrPolicy, new dataset focused on reasoning-intensive, visually dependent decisionmaking. ClevrPolicy is built upon images and scene graphs from the Clevr dataset (Johnson et al., 2017), enabling fine-grained control over policy complexity and supporting comprehensive evaluation of different MPI algorithms. Here, policy complexity refers to how difficult it is to follow the instructions in the policy when it is provided in-context. To construct the policies, we generate binary decision trees in which decision nodes specify visual conditions and response nodes define actions, with tree depth determining complexity. Each decision tree is then converted into structured natural language instruction as the final policy. We provide two variants of ClevrPolicy: ClevrPolicy-T, where policies are purely text-based, and ClevrPolicy-M, where policies include image demonstrations as part of the decision node conditions. Figure 2 illustrates the policy creation process and shows an input-output example. Further details are provided in Appendix C.1. 3.2 GTAPOLICY DATASET We further introduce GTAPolicy, which focuses on complex tool-using policies with real-world images and queries. We particularly consider low-data regime, reflecting common practical challenge in real-world settings where only limited QA pairs are available for training. The policy in GTAPolicy is constructed from the GTA dataset (Wang et al., 2024a), containing tool descriptions for 13 tools and 24 tool-calling rules with versioning and user-conditional mechanisms to simulate real-world business constraints. The policy can be automatically expanded when additional tools are provided. Training data reformulates multi-turn interactions into single-turn tool-calling tasks, where each instance includes visual inputs, user profile, query, an interaction history, and an instruction prompt, with the expected output being JSON-formatted tool call specifying tool name and arguments. Figure 3 illustrates the policy and an input-output example in GTAPolicy. Further details on the GTAPolicy dataset are provided in Appendix C.2. The statistics and evaluation metrics for both datasets are presented in Table 6 and Appendix C.3, respectively."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Overview of different training algorithms for multimodal policy internalization. The solid purple outlines indicate the parts where the next-token prediction loss is computed. On the right, we illustrate the proposed three-stage training strategy, TriMPI, which enables direct policy knowledge injection through the VM-CPT stage and policy-grounded reinforcement learning through PolicyRollout. The PolicyRollout algorithm is detailed in 4.3 and illustrated in Figure 5. 3.3 ZERO-SHOT IN-CONTEXT RESULTS ON CLEVRPOLICY AND GTAPOLICY We conduct an in-context evaluation on the proposed two benchmarks using off-the-shelf models, where the policy is directly inserted into the inference prompt. This evaluation provides useful reference for assessing the complexity of the policies. The in-context performance also provides guidance on which methods to rely on for generating chain of thought (CoT) data for MPI. As shown in Table 1, on ClevrPolicy, performance decreases as the layer number increases. Even the strongest model, such as Claude-4 (Anthropic, 2025), begins to struggle at N=6 particularly on ClevrPolicy-M. GTAPolicy also poses significant challenges for all models, with the best tool accuracy only at 60%."
        },
        {
            "title": "4 MULTIMODAL POLICY INTERNALIZATION (MPI) ALGORITHMS",
            "content": "4.1 BASELINES Inspired by previous work on prompt compression (Zou et al., 2024) and deliberative alignment (Guan et al., 2024) in text domains, we consider the following baseline methods for MPI. Direct SFT. We directly train on the non-CoT data using supervised finetuning (SFT), aiming to learn mapping from the input to the answer without performing intermediate reasoning. CoT SFT. We first obtain chain-of-thought (CoT) data in which the model explicitly reasons over the rules in the policy before producing the final answer, and then perform SFT on this CoT data. To generate CoT data for ClevrPolicy and GTAPolicy, we adopt different approaches depending on task complexity and data availability. Further details are provided in Appendix D. 4.2 TRIMPI: THREE-STAGE TRAINING STRATEGY FOR MPI We observe key limitation in the baseline methods: they focus solely on learning the expected behavior without direct access to the original policy. This limitation becomes more evident for complex policies, where directly learning output patterns is increasingly difficult. This raises the following question: can we better augment the learning process with the policy, without introducing gap between training and inference? Empirically, we find that simply inserting the policy into the prompt during training but removing it during inference results in near-random performance. To address this challenge, we propose TriMPI, three-stage training framework that (1) warms up the model via continual pretraining on the original policy, and (2) introduces PolicyRollout, new RL algorithm enables more policy-aware explorations (detailed in 4.3). TriMPI consists of three stages: (1) Visually-Masked Continual Pretraining (VM-CPT); (2) Supervised Finetuning with Chain-of-thought (CoT SFT); (3) Reinforcement learning (RL). The CoT SFT stage is identical to the baseline described in 4.1. We present the details on the VM-CPT and RL stage next. An illustration of the baselines and TriMPI stages is presented in Figure 4."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Illustration of the PolicyRollout algorithm (applied to GRPO as an example). During the rollout phase, we additionally construct set of input instances with the policy included in-context. These policy-aware responses are added to the rollout space as if they were generated from the original inputs without the policy in-context. The advantage and policy gradient are then computed on the combined rollouts, indicated by the thick red outlines. PolicyRollout enables more policyaware exploration without introducing gap between training and inference, leading to significant improvements in MPI, especially on complex policies. VM-CPT Stage. This stage aims to inject policy knowledge directly into the model parameters before performing SFT. Specifically, we construct new variant of the CoT dataset D, which contains input-output sequences = (PT , PI , I, Q, C, A) defined as the concatenation of tokens from the policy = (PT , PI ), the visual inputs I, the text query Q, the CoT reasoning C, and the final answer A. We then compute the next-token prediction loss over all tokens except the visual tokens. L(θ) = ExD (cid:34) 1 t=1 mt (cid:80)T (cid:88) t=1 mt log pθ(xt x<t) , mt = 1[xt / PI I]. (2) (cid:35) The visual masking mt enables us to adopt CPT (Ovadia et al., 2025; Maini et al., 2024) in this multimodal domain, where continuous visual tokens may appear in both the input and the policy PI , and it has shown empirical success despite its simplicity. RL Stage. As the target policy becomes more complex and reasoning-intensive, SFT baselines face the challenge of insufficient coverage of policy-related behaviors, particularly in low-data regimes. Inspired by recent advancements in reinforcement learning with verifiable rewards (RLVR), which can effectively learn from negative samples and exploration, we investigate its effectiveness for the MPI task. We first adopt GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025), following standard response format and reward design. Specifically, we ask the model to generate thinking block enclosed within <think></think> and an answer block enclosed within boxed{}, on which we compute both format reward and an accuracy reward (detailed in Appendix C.3). From our initial experiments, we observe that although GRPO and DAPO yield substantial empirical gains, exploration in the RL stage remains insufficiently grounded in the policy. This limitation becomes more pronounced with complex policies, where ungrounded exploration rarely produces positive rewards. Simply adding the policy in-context during training is ineffective, since the model lacks access to the policy at inference time, introducing misalignment between training and inference. In the next section, we present our solution through modified rollout phase. 4.3 POLICYROLLOUT (PORO): POLICY-AWARE REINFORCEMENT LEARNING To further improve the effectiveness of the RL stage for MPI, we introduce PolicyRollout (PoRo), simple yet effective extension to GRPO-style algorithms that augments the rollout space with policy-aware responses. Specifically, during the rollout stage, for each sampled instance, we construct variant by inserting the policy in-context. We then allow the current policy model to generate an additional set of responses conditioned on the query Q, the image I, and the policy"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Main results and ablations on multimodal policy internalization performance. By default, we use Qwen2.5-VL-7B as the base model. PoRo refers to PolicyRollout. The metrics are reported as percentages (%) and are detailed in Appendix C.3. We observe significant improvements of TriMPI over in-context and SFT baselines. Comprehensive ablations demonstrate the importance of each stage and the effectiveness of PolicyRollout. The RL steps indicate the actual update steps (for the three datasets, respectively, separated by ). Early stopping (marked with *) may occur in DAPO (Yu et al., 2025) runs due to its dynamic sampling strategy. Notably, on DAPO, TriMPI achieves competitive or stronger performance while using fewer steps. Method In-Context Direct SFT CoT SFT Stages CPT SFT RL RL Steps ClevrPolicy-T ClevrPolicy-M Acc (N=6) Acc (N=6) GTAPolicy Tool Acc Arg Score Overall Zero-Shot with Policy In-Context 13.15 5.65 23.58 19.44 21.51 SFT Baselines for MPI 15.15 17.80 TriMPI Ablation without RL Stage 14.55 14.30 44.34 57. 37.16 51.45 40.75 54.50 22.75 27.05 69. 61.13 65.47 VM-CPT + CoT SFT CoT SFT + GRPO CoT SFT + DAPO TriMPI Ablation without VM-CPT Stage 10410450 1048550 47.05 67.60 64.50 74.40 TriMPI without PolicyRollout (PoRo) TriMPI w/ GRPO TriMPI w/ DAPO 10410450 707050 55.90 65.85 TriMPI w/ PoRo-GRPO TriMPI w/ PoRo-DAPO 10410450 907550 65.85 77.80 TriMPI with PolicyRollout (PoRo) 80.80 81.45 84.70 85.00 76.42 76.42 83.96 79.25 85.85 80.19 68.02 68. 72.22 72.43 74.70 70.85 76.28 71.82 79.33 75.05 81.06 76.01 . These policy-aware responses are concatenated with the no-policy responses to form the rollout space, after which we proceed with group-based advantage estimation. An illustration is provided in Figure 5, and the PolicyRollout objective (applied to GRPO as an example) is written as follows: JPoRo-GRPO(θ) = [{oi}G i=1πθold (OQ,I), {oj }2G j=Gπθold (OQ,I,P )] 1 2G 2G (cid:88) (cid:110) i=1 (cid:111) (cid:104) min ri(θ) ˆAi, clip (ri(θ), 1 ϵl, 1 + ϵh) ˆAi (cid:105) βDKL [πθπref ] , ri(θ) = πθ(oiQ, I) πθold (oiQ, I) (3) The blue part highlights the main modification compared with the original GRPO objective. Note that the policy gradient is applied only to the no-policy path (conditioning solely on and I), thereby ensuring that the training and inference remain aligned."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We conduct comprehensive experiments on multimodal policy internalization, evaluating MPI task performance (5.1), generalization capability (5.2), policy knowledge injection (5.3), and robustness to catastrophic forgetting (5.4). By default, we use Qwen2.5-VL-7B as the base model. We tune all model parameters in the VM-CPT and RL stages, and apply LoRA (Hu et al., 2022) finetuning in the SFT stage. For ClevrPolicy, we use the most complex policies (N = 6) unless otherwise specified. Additional implementation details are provided in Appendix B. 5.1 MAIN RESULTS MPI Task Performance. In Table 2, we present evaluation results on ClevrPolicy and GTAPolicy in terms of task performance. Our best-performing model achieves up to 70.7% and 79.4% absolute gains in accuracy over the CoT SFT baseline and the in-context setting, respectively. We also find that the relative superiority of GRPO and DAPO varies across datasets: DAPO performs better on ClevrPolicy, while GRPO excels on GTAPolicy. We hypothesize that this difference arises because DAPO makes bolder updates due to the removal of the reference KL. This leads to faster learning on ClevrPolicy, which contains more abundant and diverse data, but results in overfitting on GTAPolicy,"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Left: Policy Override results. We show that TriMPI consistently outperforms strong baselines in generalizing to updated policies, demonstrating favorable real-world usage where model behavior can be governed by both internalized policies and in-context instructions. Right: Policy Referral results. We use Claude-4 to rank the consistency between the models intermediate thoughts and the original policy on scale of 0-10. higher score indicates better embedded policy knowledge. Method In-Context Direct SFT CoT SFT CoT SFT + GRPO CoT SFT + DAPO TriMPI w/ PoRo-GRPO TriMPI w/ PoRo-DAPO ClevrPolicy Override (N=6) ClevrPolicy-T ClevrPolicy-M Tool Acc Arg Score Overall ClevrPolicy-T ClevrPolicy-M GTAPolicy Policy Referral (0-10 Scores) GTAPolicy Override 13.15 5.40 16.40 30.80 41.60 48.70 59.40 6. 5.30 25.20 34.00 37.60 82.70 85.15 20.75 40.57 42.45 50.94 58.49 66.98 63.21 18. 33.42 34.98 41.60 50.12 59.03 54.66 19.43 36.99 38.71 46.27 53.31 63.00 58.94 - - 3.54 5.04 5.70 5.60 6.26 - - 3.44 6.74 6.74 8.72 8.35 - - 7.68 8.93 8.73 9.45 9.13 which has very limited data. We further present qualitative example in Figure 7, showing that TriMPI achieves better alignment with the policy in both intermediate thoughts and final answers. Ablation Analysis. In Table 2, we additionally provide comprehensive ablation study on the key components proposed in TriMPI, including the RL stage, the VM-CPT stage, and the PolicyRollout algorithm. The findings are as follows: (1) The RL stage contributes most of the improvements compared with SFT baselines, highlighting the importance of learning from experience for complex, reasoning-intensive policies. We also observe unique benefit of the RL stage in better leveraging non-CoT data, which is typically more abundant than CoT data, as shown in Table 9. (2) the VM-CPT stage brings further improvements to both the SFT and RL stages, with the effect being more pronounced in the RL stage due to more grounded exploration; and (3) PolicyRollout yields additional gains over the original GRPO and DAPO algorithms. Efficiency Analysis. We report efficiency metrics in Figure 6, including the number of prompt tokens and the prefill inference time (i.e., the first forward pass on the input prompt) before and after internalization. All metrics are computed on Qwen2.5-VL-7B. With the policy removed from the prompt, we observe reductions of up to 93.9% in prompt tokens and 85.7% in prefill inference time. Figure 6: Efficiency metrics before and after MPI. Impact of Policy Complexity and Model Size. We further investigate the impact of policy complexity and model size on the effectiveness of different MPI algorithms. As shown in Table 7, we conduct additional experiments on (1) = 4 policies in ClevrPolicy and (2) 3B models. Our findings are twofold: (1) TriMPI yields more pronounced gains over the baselines on complex policies, while the performance gap is smaller on simpler policies (e.g., = 4). This highlights the importance of our method for handling increasingly complex policies in real-world settings. (2) TriMPI consistently outperforms the baselines on 3B models, demonstrating the generality of the method. 5.2 POLICY OVERRIDE: EVALUATING GENERALIZATION TO POLICY UPDATES In real-world scenarios, policies may be frequently updated or partially overridden by new rules. Ideally, the internalized model should generalize to these updated policies when they are provided in-context. To evaluate this capability, we introduce new evaluation setting, Policy Override, where unseen policy content is specified in-context during inference with internalized models. As illustrated in Figure 10, for ClevrPolicy we retain the policy name but modify the conditions, while for GTAPolicy we alter the tool-calling rules, resulting in different choice of tool version. Table 3 (left) shows that TriMPI consistently outperforms all baselines and ablated settings, demonstrating stronger generalization beyond merely fitting to specific policy."
        },
        {
            "title": "5.3 POLICY REFERRAL: EVALUATING POLICY KNOWLEDGE INJECTION",
            "content": "Beyond end-task performance, which measures how well the model generates policy-compliant responses, we further investigate how well the model embeds the policy knowledge itself. To this end, we consider new evaluation setting, Policy Referral, where we leverage LLM-as-a-judge to examine the intermediate reasoning process of the models responses. The goal is to assess whether the referral to the original policy is accurate and to assign score between 0 and 1. Figure 8 illustrates this evaluation setting, and Table 3 (right) presents the results, reporting the average score over subset of 100 test responses from each dataset. We find that TriMPI achieves more accurate policy referral, indicating that it not only learns end-task behavior but also internalizes the underlying policy."
        },
        {
            "title": "5.4 EVALUATING ROBUSTNESS TO CATASTROPHIC FORGETTING",
            "content": "Similar to related work in continual learning (Yu et al., 2024) and knowledge injection (Song et al., 2025), major challenge in MPI is avoiding catastrophic forgetting, ensuring that policy-related performance improves while general non-policy abilities are preserved. To assess this, we adopt two widely used general reasoning benchmarks, MMMU-Pro (Yue et al., 2024) (multimodal) and MMLUPro (Wang et al., 2024b) (textual), to evaluate the models robustness to catastrophic forgetting after internalization. The results are presented in Table 8. We observe that the baselines exhibit significant performance degradation after MPI on GTAPolicy, while maintaining strong performance after MPI on ClevrPolicy. This discrepancy arises because the small dataset size of GTAPolicy makes it more prone to overfitting. In contrast, TriMPI consistently preserves strong general reasoning abilities across all settings, achieving the best overall performance."
        },
        {
            "title": "6 RELATED WORK",
            "content": "6.1 PROMPT COMPRESSION AND DELIBERATIVE ALIGNMENT line of related research (Li et al., 2024) focuses on compressing long prompts into more compact forms that can still effectively guide large language models. Early efforts explored both hard prompts (Li et al., 2023; Jiang et al., 2023; Chuang et al., 2024), where discrete tokens are carefully pruned, and soft prompts (Zhao et al., 2023; Wingate et al., 2022; Mu et al., 2023; Ge et al., 2023), where continuous embeddings are learned to replace verbose instructions or demonstrations. More recent work, such as PromptIntern (Zou et al., 2024), adopts progressive fine-tuning approach that internalizes prompts into the model without introducing additional parameters. Another line of related work is on personalized multimodal models (Nguyen et al., 2024; 2025), for which we provide detailed discussion in Appendix K. While promising, these methods focus on prompts limited to task templates and demonstrations that demand little reasoning. Moreover, the introduction of special embeddings confines the model to specific task, reducing its ability to handle general queries. Deliberative Alignment (Guan et al., 2024; Zhang et al., 2025) extends this idea to more general alignment setting without sacrificing the models overall capabilities. Its goal is to embed the knowledge of safety specification into the model parameters while emphasizing reasoning over the internalized knowledge. Our proposed MPI task follows this high-level motivation but further extends the scope to multimodal models and considers diverse conversational agent tasks beyond safety. 6.2 IMPROVING GRPO FROM THE ROLLOUT PERSPECTIVE We also draw inspiration from recent multimodal reasoning work that investigates improving GRPOrelated RLVR algorithms from the rollout perspective. Methods such as NoisyRollout (Liu et al., 2025) and R1-ShareVL (Yao et al., 2025) demonstrate the benefits of diversifying the rollout space with responses generated from altered instances using semantically consistent augmentations, such as applying moderate Gaussian noise to the visual inputs. Our proposed PolicyRollout algorithm follows the same high-level idea of augmenting the rollout space, but instead of introducing noise, it provides the policy in-context to enable more grounded exploration. PolicyRollout illustrates the potential of more general approach to incorporating additional guidance in the RL stage, while remaining aligned with the original optimization task."
        },
        {
            "title": "7 CONCLUSION AND LIMITATIONS",
            "content": "In this work, we propose new task, Multimodal Policy Internalization, which aims to address the emerging challenge of maintaining in-context efficiency while following complex policies in multimodal conversational agents. We introduce two new benchmarks spanning analytical and realworld settings, along with highly effective training paradigm, TriMPI. The remaining limitations are: (1) scaling up the datasets with more diverse real-world images and tasks; (2) developing more sophisticated continual pretraining strategies beyond simply masking visual tokens; and (3) designing training strategies for internalizing mixtures of tasks with very different response formats."
        },
        {
            "title": "8 REPRODUCIBILITY STATEMENT",
            "content": "We include an anonymous source code archive in the supplementary material, containing training and evaluation instructions for reproducing the results in this paper. Details of dataset creation and evaluation metrics are provided in and D. Full policy examples are shown in Figures 14, 15, and 16. Implementation details of the training procedure are provided in B. Prompts used for the LLM-as-a-judge Policy Referral evaluation are presented in Figure 9."
        },
        {
            "title": "9 ETHICS STATEMENT",
            "content": "This work focuses on fundamental research aimed at advancing the understanding of how multimodal models can internalize complex policies more effectively. All experiments are conducted on publicly available datasets, and no human subjects or private user data are involved. The GTAPolicy dataset introduced in this work contains fully synthesized user profiles and does not include or rely on any real user data. Code and data from this work will be made publicly available for research purposes in the near future."
        },
        {
            "title": "REFERENCES",
            "content": "Yuval Alaluf, Elad Richardson, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. In European Conference on Computer Vision, pp. 7391. Springer, 2024. 21 Alexa AI. Amazon alexa. https://developer.amazon.com/en-US/alexa, 2025. Voicebased intelligent assistant by Amazon. 1 Anthropic. Claude 4, 2025. URL https://www.anthropic.com/news/claude-4. 1, 5, 18, 20 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Luke Bailey, Gustaf Ahdritz, Anat Kleiman, Siddharth Swaroop, Finale Doshi-Velez, and Weiwei Pan. Soft prompting might be bug, not feature. 2023. 3 Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu. Learning to compress prompt in natural language formats. arXiv preprint arXiv:2402.18700, 2024. 9 Brian Clark, Srividhya Pallay, and Prerna Mishra. Demystifying amazon bedrock pricing https://aws.amazon.com/blogs/machine-learning/ for chatbot assistant. demystifying-amazon-bedrock-pricing-for-a-chatbot-assistant/ ?utm_source=chatgpt.com, 2025. Sinan Fan, Liang Xie, Chen Shen, Ge Teng, Xiaosong Yuan, Xiaofeng Zhang, Chenxi Huang, Wenxiao Wang, Xiaofei He, and Jieping Ye. Improving complex reasoning with dynamic prompt corruption: soft prompt optimization approach. arXiv preprint arXiv:2503.13208, 2025. 3 Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022."
        },
        {
            "title": "Preprint",
            "content": "Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. arXiv preprint arXiv:2307.06945, 2023. 2, 9 Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. 2, 3, 5, 9 Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations (ICLR), 2022. URL https://openreview. net/forum?id=nZeVKeeFYf9. 7 Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023. 2, 9 Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 19881997. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.215. URL https://doi.org/10.1109/CVPR.2017.215. 2, 4 Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 3 Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models. arXiv preprint arXiv:2310.06201, 2023. 2, 9 Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language models: survey. arXiv preprint arXiv:2410.12388, 2024. 2, 9 Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: recipe for compute and data-efficient language modeling. arXiv preprint arXiv:2401.16380, 2024. 2, 6 Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 1932719352. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf. 2, 9 Thao Nguyen, Haotian Liu, Yuheng Li, Mu Cai, Utkarsh Ojha, and Yong Jae Lee. Yollava: Your personalized language and vision assistant. Advances in Neural Information Processing Systems, 37:4091340951, 2024. 9, 21 Thao Nguyen, Krishna Kumar Singh, Jing Shi, Trung Bui, Yong Jae Lee, and Yuheng Li. Yochameleon: Personalized vision and language generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1443814448, 2025. 9, 21 OpenAI. Introducing gpt-5, 2025. URL https://openai.com/index/ introducing-gpt-5/. 1 Oded Ovadia, Meni Brief, Rachel Lemberg, and Eitam Sheetrit. Knowledge-instruct: Effective continual pre-training from limited data using instructions. arXiv preprint arXiv:2504.05571, 2025. 2, Oam Patel, Jason Wang, Nikhil Shivakumar Nayak, Suraj Srinivas, and Himabindu Lakkaraju. Towards interpretable soft prompts. arXiv preprint arXiv:2504.02144, 2025."
        },
        {
            "title": "Preprint",
            "content": "Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. 2 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2250022510, 2023. 21 Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3, 6 Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, and Xiuying Chen. Injecting domain-specific knowledge into large language models: comprehensive survey. arXiv preprint arXiv:2502.10708, 2025. 9 Jize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: benchmark for general tool agents. Advances in Neural Information Processing Systems, 37: 7574975790, 2024a. 2, 4, 16 Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37: 9526695290, 2024b. 9, David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162, 2022. 9 Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, and Jiaxing Huang. R1-sharevl: Incentivizing reasoning capability of multimodal large language models via share-grpo, 2025. 9 Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. tau-bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. 2 Dianzhi Yu, Xinni Zhang, Yankai Chen, Aiwei Liu, Yifei Zhang, Philip Yu, and Irwin King. Recent advances of multimodal continual learning: comprehensive survey. arXiv preprint arXiv:2410.05352, 2024. 9 Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 3, 6, 7 Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. 9, Haoran Zhang, Yafu Li, Xuyang Hu, Dongrui Liu, Zhilin Wang, Bo Li, and Yu Cheng. Reasoning over boundaries: Enhancing specification alignment via test-time delibration. arXiv preprint arXiv:2509.14760, 2025. 3, 9 Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. 17 Wenbo Zhao, Arpit Gupta, Tagyoung Chung, and Jing Huang. SPC: Soft prompt construction for cross domain generalization. In Burcu Can, Maximilian Mozes, Samuel Cahyawijaya, Naomi Saphra, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Chen Zhao, Isabelle Augenstein, Anna Rogers, Kyunghyun Cho, Edward Grefenstette, and Lena Voita (eds.), Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023), pp. 118130, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.repl4nlp-1.10. URL https://aclanthology.org/2023.repl4nlp-1.10/."
        },
        {
            "title": "Preprint",
            "content": "Jiaru Zou, Mengyu Zhou, Tao Li, Shi Han, and Dongmei Zhang. Promptintern: Saving inference costs by internalizing recurrent prompt during large language model fine-tuning. arXiv preprint arXiv:2407.02211, 2024. 2, 5,"
        },
        {
            "title": "APPENDIX",
            "content": "A Use of Large Language Models Implementation Details Dataset Creation Details C.1 ClevrPolicy . C.2 GTAPolicy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Evaluation Metrics and RLVR Rewards . . . . . . . . . . . . . . . . . . . . . . . C.4 Full Policy Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details on CoT SFT D.1 CoT Data Generation . D.2 CoT SFT Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Results on Varying Policy Complexity and Model Size Qualitative Analysis Results on Robustness to Catastrophic Forgetting Illustration of the Policy Referral Evaluation Illustration of the Policy Override Evaluation RL Effectively Leverages Non-CoT Data Additional Related Work 15 15 15 16 17 17 18 18 19 19 20 20 21"
        },
        {
            "title": "Preprint",
            "content": "Table 4: MPI training hyperparameters for different stages. EP denotes the number of epochs, LR is the learning rate, and BS is the batch size, expressed as per-device batch size number of devices. For the RL stage, BS refers to the update batch size. Additional configuration details, such as the rollout batch size, are provided in Appendix B. Table 5 shows the actually RL steps taken in different training settings. Table 6 shows the exact dataset sizes. Stage Dataset Data Type Data Size LoRA LoRA Rank EP LR BS VM-CPT VM-CPT ClevrPolicy Policy + CoT GTAPolicy Policy + CoT Direct SFT ClevrPolicy Direct SFT GTAPolicy Non-CoT Non-CoT CoT SFT CoT SFT RL RL ClevrPolicy GTAPolicy ClevrPolicy GTAPolicy CoT CoT Non-CoT Non-CoT 2.5K 0.5K 20K 0.5K 2.5K 0.5K 20K 0.5K - - 8 8 8 8 - - 5 5 2 10 5 2 50 1e-5 168 88 1e-5 1e-4 164 84 1e-4 1e-4 164 84 1e-4 1e-6 1e-6 48"
        },
        {
            "title": "A USE OF LARGE LANGUAGE MODELS",
            "content": "Large language models, such as ChatGPT and Claude, are used solely for grammar checking during the writing process and not for research ideation. Additionally, LLM-based coding assistants, such as Copilot, are employed to aid in code implementation."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "In-context Experiment Details. The exact model versions used in Table 1 are as follows. For Qwen2.5-VL, we use the model checkpoints from Huggingface (3B, 7B). For Claude models, we use claude-3-7-sonnet-20250219 and claude-sonnet-4-20250514 hosted on Amazon Bedrock. Multimodal Policy Internalization Training Details. Table 4 summarizes the training hyperparameters for different stages. For the RL stages, we set the clipping ratio to ϵl = 0.2, ϵh = 0.3 for GRPO and ϵl = 0.2, ϵh = 0.28 for DAPO. The rollout batch size is 384 for ClevrPolicy and 256 for GTAPolicy. The reference KL coefficient is set to β = 0.01 for GRPO. The maximum number of retries for dynamic sampling is set to 20 for DAPO. The actual number of RL steps taken in each run is reported in Table 5. We use 4 NVIDIA H100 80GB GPUs for SFT stages and 8 H100 for CPT and RL stages."
        },
        {
            "title": "C DATASET CREATION DETAILS",
            "content": "C.1 CLEVRPOLICY C.1.1 POLICY GENERATION IN CLEVRPOLICY As described in 3.1, we first generate set of binary decision trees to construct the policies. We define two types of nodes: decision nodes and response nodes. decision node checks whether the current input image satisfies sampled condition (for example, the existence of cyan object), while response node corresponds to unique outcome. Each decision node (non-leaf node) samples an attribute type and an attribute value from the following ontology: Shape: cube, sphere, cylinder Color: gray, red, blue, green, brown, purple, cyan, yellow Size:small, large https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct https://docs.anthropic.com/en/api/claude-on-amazon-bedrock"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Detailed statistics of the number of RL steps under different settings. Numbers annotated with (*) denote early stopping, which may occur in DAPO runs due to dynamic sampling. Methods ClevrPolicy-T (N=6) ClevrPolicy-M (N=6) RL Steps ClevrPolicy-T (N=4) ClevrPolicy-M (N=4) GTAPolicy Base Model: Qwen2.5-VL-7B CoT SFT + GRPO CoT SFT + DAPO TriMPI w/ PoRo-GRPO TriMPI w/ PoRo-DAPO 104 104 104 90* 104 85* 104 75* 104 25* 104 15* CoT SFT + GRPO CoT SFT + DAPO TriMPI w/ PoRo-GRPO TriMPI w/ PoRo-DAPO Base Model: Qwen2.5-VL-3B - - - - 104 104 104 95* 104 104 104 100* 104 45* 104 20* - - - - 50 50 50 50 - - - - Material:rubber, metal If child decision node has parent node and the edge to the parent node is true, the child node is restricted from sampling the same attribute type. For example, if the parent node checks for cyan objects and the edge is true, the child node will not check the color attribute again, to avoid logical conflicts. This results in 4, 16, and 55 unique target responses for policies with = 2, = 4, and = 6, respectively. Given sampled decision tree, we then convert it into textual instruction containing three main components: general instruction, unique policy name, and sections of conditions governing the response behavior. This policy challenges the model to perform multi-hop reasoning across different sections in order to faithfully follow the rules. We further introduce two variants of ClevrPolicy, namely, ClevrPolicy-T and ClevrPolicy-M, depending on whether the policy itself contains image content. As shown in Figure 2 (left), in ClevrPolicy-M the attribute value in decision node may be demonstrated by small image crop instead of natural language. This introduces an additional level of difficulty for policy following. C.1.2 TRAINING DATA CREATION IN CLEVRPOLICY We generate mixture of 10 unique policies for each of the three levels of complexity indicated by the layer number 2, 4, 6. For each policy, we then sample 2K images as training data. The groundtruth answers can be automatically obtained by verifying the scene graph against the corresponding decision tree. This results in 20K non-CoT QA pairs for each training task in ClevrPolicy-T and ClevrPolicy-M. Similarly, we sample another 2K unseen instances for testing. Importantly, the underlying decision trees are not accessible to the model during subsequent MPI training. An input-output example is illustrated in Figure 2 (right). Evaluation is performed based on exact string matching, and results are reported in terms of accuracy. C.2 GTAPOLICY C.2.1 POLICY CREATION IN GTAPOLICY We manually construct complex policy based on the GTA dataset (Wang et al., 2024a), which was originally proposed as multimodal tool using benchmark. As illustrated in Figure 3 (left), the GTAPolicy policy consists of two main components: tool descriptions and tool calling rules. The tool descriptions contain the metadata for all 13 types of tools, including the tool name, description, and arguments. To ensure that model performance reflects its ability to follow the internalized policy rather than relying on prior knowledge, we further introduce versioning mechanism and user-conditional tool-calling rules. Specifically, tool such as OCR may have multiple versions, e.g., OCR v1 and OCR v2. total of 24 tool-calling rules specify which particular version to use depending on the profile attributes of the input user. This design simulates real-world business"
        },
        {
            "title": "Preprint",
            "content": "rules in which certain user properties must be considered during decision-making and tool calling, such as whether the user is premium member. C.2.2 TRAINING DATA CREATION IN GTAPOLICY We first reformulate the multi-turn data from the GTA dataset into single-turn tool calling task to avoid overcomplicating the MPI setting with intermediate tool calling errors. As illustrated in Figure 3 (right), each input instance consists of five parts: visual inputs (potentially multiple images), user profile, user query, an interaction history containing previous tool calls and returned values, and general instruction prompt. The expected output is tool call formatted in JSON that specifies the tool name (with version) and the corresponding arguments. The final dataset contains 451 instances for training and 106 instances for testing. To evaluate the tool calls, we use exact match on the tool name, and apply different evaluation metrics to the argument values depending on their type (e.g., IoU for bounding boxes, text similarity for free form text queries). Detailed evaluation metrics are provided in Appendix C.3. C.3 EVALUATION METRICS AND RLVR REWARDS Evaluation Metrics for ClevrPolicy. We use accuracy based on the exact match between the predicted and ground truth outcomes, for example, Case 0. Accordingly, the following accuracy reward is used for GRPO and DAPO on ClevrPolicy: RAcc-ClevrPolicy = 1[Exact Match(y, ˆy)], (4) where and ˆy denote the parsed response and the ground truth, respectively. Evaluation Metrics for GTAPolicy. For the tool name, we use accuracy based on exact match. For the arguments, we identify four categories of argument types and apply different metrics accordingly. The full list of tools and argument definitions is shown in Figure 16. We provide the argument names and their corresponding evaluation metrics as follows: Exact match: position, color, image, k, top1 Text similarity: attribute, text, query, command, annotation, keywords, instruction Python Eval: expression IoU: bbox Exact match is used for arguments that require precise string matching, for example, the image name when running tool. The text similarity metric evaluates arguments in free-form text format, such as search queries or object names. We use BertScore (Zhang et al., 2019) to compute the semantic similarity between two text sequences. For expressions involving numerical operations, we use the Python eval function to compute the final outcome and compare it with the ground truth. For bounding box arguments, we use Intersection over Union (IoU) to compute the overlap between the predicted and ground truth coordinates. All scores are normalized to the range [0, 1]. The final argument score is computed as the average across all argument fields. We then compute an overall score for the entire tool call as the average of the tool accuracy and the argument score. Accordingly, the following accuracy reward is used for GRPO and DAPO on GTAPolicy: RAcc-GTAPolicy = 0.5 Tool Acc + 0.5 Argument Score (5) C.4 FULL POLICY EXAMPLES Examples of the full policy for ClevrPolicy-T, ClevrPolicy-M and GTAPolicy are presented in Figures 14, 15 and 16, respectively."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Detailed dataset statistics. Dataset CoT Generator CoT Strategy Filtering Train Size CoT Non-CoT Test Size ClevrPolicy-T (N=6) Qwen2.5-VL-7B Forward CoT ClevrPolicy-T (N=4) Qwen2.5-VL-7B Forward CoT Forward CoT ClevrPolicy-M (N=6) Claude-4-Sonnet ClevrPolicy-M (N=4) Claude-4-Sonnet Forward CoT Claude-4-Sonnet Reverse CoT GTAPolicy Yes Yes Yes Yes No 2526 2526 2472 2282 451 20000 20000 20000 20000 451 2000 2000 2000 2000 106 Table 7: Additional results on varying task complexity and model size. We show that TriMPI consistently outperforms strong baselines across different complexities and model sizes. Notably, the performance gain is more pronounced on complex policies. Varying Policy Complexity Varying Model Size Method ClevrPolicy-T N=6 N=4 ClevrPolicy-M ClevrPolicy-T (N=6) ClevrPolicy-M (N=6) N=4 N=6 3B 7B 3B 7B In-Context 32.20 13. 13.90 5.65 Direct SFT CoT SFT CoT SFT + GRPO CoT SFT + DAPO 25.65 49.10 98.25 97.15 TriMPI w/ PoRo-GRPO 99.15 TriMPI w/ PoRo-DAPO 97.30 15.15 17.80 47.05 67. 65.85 77.80 37.85 55.35 87.10 88.75 99.05 99.40 14.55 14.30 64.50 74.40 84.70 85.00 4. 13.40 19.00 31.55 48.10 46.65 68.75 13.15 15.15 17.80 47.05 67.60 65.85 77.80 4. 12.90 9.25 22.40 33.25 78.80 82.35 5.65 14.55 14.30 64.50 74.40 84.70 85."
        },
        {
            "title": "D DETAILS ON COT SFT",
            "content": "D.1 COT DATA GENERATION Given the practical challenges of accessing APIs for stronger models such as GPT and Claude, key principle we follow is to minimize reliance on API models whenever possible. Specifically, on ClevrPolicy, we use forward CoT with answer filtering to generate CoT data, where generator model is asked to solve the task with intermediate reasoning steps given the policy and the input query. The generated instances are then filtered based on answer correctness. For GTAPolicy, due to the small dataset size, we adopt reverse CoT, where the generator model is provided with both the inputs and the ground truth answer and asked to generate the intermediate rationales. Reverse CoT is suitable in such settings where filtering is difficult or the data samples are scarce. We select the generator model based on the zero-shot in-context performance shown in Table 1. For ClevrPolicy-T, we use the base model itself, Qwen-2.5VL-7B (Bai et al., 2025), as the generator. For more challenging settings, i.e., ClevrPolicy-M and GTAPolicy, we use Claude-4(Anthropic, 2025). In the case of ClevrPolicy-M with Claude-4, we subsample the input non-CoT data to 3K in order to match the CoT dataset size of ClevrPolicy-T. After filtering, we obtain approximately 2.5K CoT instances for each ClevrPolicy setting and 451 instances for GTAPolicy. The full statistics of the CoT and non-CoT data can be found in Table 6. Figures 11, 12 and 13 provide examples of the CoT data for all three datasets. D.2 COT SFT OBJECTIVE Given the generated CoT annotation for each question-answer pair (Q, A), the training objective for the CoT SFT stage can be written as follows: LSFT = E(Q,O)D (cid:34) (cid:88) t=1 (cid:35) log pθ(ot Q, o<t) , = [C; A] (6) where is concatenation of and A."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Qualitative example comparing different MPI algorithms. On the left, we show the inputs and the ground-truth reasoning trajectory annotated with the original policy sections. On the right, the CoT SFT model makes an error in correctly recalling the policy condition, and the CoT SFT + GRPO model makes an incorrect decision at the fifth condition, both leading to an incorrect final outcome. In contrast, the proposed TriMPI correctly recalls all policy conditions and performs reasoning consistent with the input image."
        },
        {
            "title": "SIZE",
            "content": "Table 7 illustrates the additional results on varying task complexity and model size. TriMPI is especially valuable for handling complex policies, showing larger gains over baselines in these settings while maintaining consistent improvements even on simpler cases (e.g., = 4). Moreover, it generalizes well across model scales, consistently outperforming baselines on 3B models as well."
        },
        {
            "title": "F QUALITATIVE ANALYSIS",
            "content": "Figure 7 shows qualitative example from the ClevrPolicy dataset, comparing the responses of different MPI algorithms. The results demonstrate that the proposed TriMPI algorithm successfully refers to the original policy and produces policy-compliant answer, whereas the SFT and SFT+RL baselines suffer from incorrect policy referral or reasoning errors."
        },
        {
            "title": "Preprint",
            "content": "Table 8: Robustness to catastrophic forgetting. We evaluate the internalized models on MMMUpro (Yue et al., 2024) and MMLU-pro (Wang et al., 2024b) benchmarks. The results show that TriMPI consistently maintains strong general capabilities across all settings, while most baselines exhibit significant drop on the smaller dataset, i.e., GTAPolicy. Method MMMU-Pro (Acc) MMLU-Pro (Acc) ClevrPolicy-T ClevrPolicy-M GTAPolicy ClevrPolicy-T ClevrPolicy-M GTAPolicy AVG Before MPI Training Qwen2.5-VL-7B 31.91 31.91 31.91 31. 31.73 31.73 31.82 Direct SFT CoT SFT CoT SFT + GRPO CoT SFT + DAPO TriMPI w/ PoRo-GRPO TriMPI w/ PoRo-DAPO 34.10 31.85 32.60 34. 31.45 31.56 After MPI Training 33.24 32.95 32.08 34.16 33.29 32.66 27.86 10.17 22.95 18.84 30.12 30. 39.25 37.67 37.63 38.41 38.57 37.68 37.01 38.70 40.13 39.79 39.34 39.49 28.91 10.43 22.66 20.86 35.77 35. 33.40 26.96 31.34 31.04 34.76 34.59 Figure 8: Illustration of the Policy Referral evaluation setup. We take the responses from the internalized model and ask strong LLM (Claude-4 (Anthropic, 2025)) to score the consistency between any policy referral in the response and the original policy. Policy referral is designed to evaluate the quality of the embedded policy knowledge beyond end-task performance."
        },
        {
            "title": "G RESULTS ON ROBUSTNESS TO CATASTROPHIC FORGETTING",
            "content": "Table 8 presents results on general multimodal (MMMU-pro (Yue et al., 2024)) and textual (MMLUpro (Wang et al., 2024b)) reasoning benchmarks for models after MPI training. This evaluation examines whether the model preserves general reasoning capabilities while internalizing policycompliant behavior."
        },
        {
            "title": "H ILLUSTRATION OF THE POLICY REFERRAL EVALUATION",
            "content": "Figure 8 illustrates the Policy Referral evaluation setting, where we leverage strong LLM, Claude4 (Anthropic, 2025), to score how accurate does the response cited to the original policy. The exact prompt we used for prompting the LLM is presented in Figure 9. This evaluation aims to test whether the model has actually absorbed the policy knowledge beyond merely mimicking the expected behavior. I"
        },
        {
            "title": "ILLUSTRATION OF THE POLICY OVERRIDE EVALUATION",
            "content": "Figure 10 illustrates the Policy Override evaluation setting, where we place the updated policy in-context during evaluation and expect the model to follow the overridden policy. This evaluation"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Prompt to Claude-4 for the Policy Referral evaluation. Figure 10: Illustration of the Policy Override evaluation setup. For ClevrPolicy, we provide randomly sampled new policy in-context during inference as updated policy content while keeping its original unique name. For GTAPolicy, we modify the tool-calling rules, resulting in different model version. Policy Override evaluates the models capability to generalize to updated or modified policies beyond overfitting. This property is particularly important in real-world scenarios, where policies are constantly changing and model behavior must be governed by both the internalized policy and the in-context instructions. aims to test the models ability to generalize in policy following, beyond memorizing particular policy. RL EFFECTIVELY LEVERAGES NON-COT DATA During CoT data generation, we observe that in real-world settings, non-CoT data is often much more abundant than CoT data. For example, in ClevrPolicy, after filtering, we obtain 2.5K CoT examples versus 20K non-CoT examples. unique benefit of the RL stage (with GRPO and DAPO) is that it does not require CoT annotations. To investigate the extent to which the RL stage can benefit from non-CoT data relative to CoT data, we conduct the following experiment. We run RL algorithms using only CoT data and additionally include an SFT baseline, where the model is first trained on CoT data and then further trained on non-CoT data. The results are shown in Table 9. We observe that the RL stage can more effectively leverage the abundant non-CoT data compared to SFT-only approaches."
        },
        {
            "title": "K ADDITIONAL RELATED WORK",
            "content": "Another related line of work from the multimodal community is personalization through textual inversion (Gal et al., 2022). This spans from early studies on personalized image generation (Gal et al., 2022; Ruiz et al., 2023) to more recent approaches on personalized multimodal models (Nguyen et al., 2024; Alaluf et al., 2024; Nguyen et al., 2025). The central idea is to learn lightweight embeddings that represent new visual concepts, enabling models to generate personalized responses. However, the"
        },
        {
            "title": "Preprint",
            "content": "Table 9: Results of the RL stage with different data source. RL algorithms such as GRPO can better leverage the more abundant non-CoT data compared to SFT. Method SFT SFT SFT + GRPO SFT + GRPO TriMPI w/ GRPO TriMPI w/ GRPO Data Dataset Size ClevrPolicy-T ClevrPolicy-M Subset (CoT) All (Non-CoT) Subset (CoT) All (Non-CoT) Acc (N=6) Acc (N=6) 2.5K 2.5K 2.5K 2.5K 2.5K 2.5K 20K 20K 20K 20K 20K 20K 17.80 22. 34.10 47.05 39.45 55.90 14.30 14.15 54.00 64.50 74.55 80.80 Figure 11: CoT data example (ClevrPolicy-T N=6). supported concepts are typically limited to one or few personal photos, which is far less complex than the policies required for multimodal conversational agents."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: CoT data example (ClevrPolicy-M N=6)."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: CoT data example (GTAPolicy)."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Full policy example (ClevrPolicy-T N=6)."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Full policy example (ClevrPolicy-M N=6)."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Full policy example (GTAPolicy)."
        }
    ],
    "affiliations": [
        "Amazon",
        "University of Illinois Urbana-Champaign"
    ]
}