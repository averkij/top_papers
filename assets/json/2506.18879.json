{
    "paper_title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
    "authors": [
        "Junyan Li",
        "Yang Zhang",
        "Muhammad Yusuf Hassan",
        "Talha Chafekar",
        "Tianle Cai",
        "Zhile Ren",
        "Pengsheng Guo",
        "Foroozan Karimzadeh",
        "Colorado Reed",
        "Chong Wang",
        "Chuang Gan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 7 8 8 1 . 6 0 5 2 : r CommVQ: Commutative Vector Quantization for KV Cache Compression Junyan Li 1 Yang Zhang 2 Muhammad Yusuf Hassan 1 Talha Chafekar 1 Tianle Cai 3 Zhile Ren 4 Pengsheng Guo 4 Binazir Karimzadeh 4 Colorado Reed 4 Chong Wang 4 Chuang Gan 1 Abstract Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing LLaMA-3.1 8B model to run with 128K context length on single RTX 4090 GPU. The source code is available at: https://github. com/UMass-Embodied-AGI/CommVQ. 1. Introduction We are witnessing growing trend in increasing the context length of large language models (LLMs). For instance, the latest LLaMA 3.1 models (Dubey et al., 2024) support up to 128K context length, and recent research (Ding et al., 1University of Massachusetts Amherst 2Massachusetts Institute of Technology 3Princeton University 4Apple Inc. Correspondence to: Junyan Li <junyanli@umass.edu>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 2024; Jin et al., 2024) has managed to extend this even further, with some models achieving context lengths of over 1 million. Supporting longer contexts enables LLMs to process richer inputs and generate more tokens, improving their capacity for more complex tasks and reasoning (Wei et al., 2022). However, this increased context length presents significant challenge on GPU memory usage. The causal attention mechanism used in LLMs relies on Key-Value (KV) cache to speed up inference. This cache stores the keys and values of all previous tokens, eliminating the need to recompute them when generating the next token. As context lengths increase, the size of the KV cache grows proportionally, eventually becoming the primary bottleneck for memory usage often far exceeding the memory required for the model itself. For instance, LLaMA 3.1 8B model requires 16 GB of memory to store the model weight in FP16 precision. If the context length is set to its maximum of 128K with batch size of 2, the KV cache alone would require 88 GB of memory. This makes it impossible to run inference on single GPU without KV cache offloading, even for H100-80GB. Efforts to reduce KV cache size are ongoing (Shi et al., 2024; Yuan et al., 2024), with KV cache quantization (Liu et al., 2024b; Hooper et al., 2024) being key approach. Quantization lowers the memory footprint by reducing the bit width used to represent each FP16 scalar in the KV cache. For instance, INT4 quantization cuts memory usage by factor of four compared to FP16. However, this comes at cost: aggressive quantization, such as 2-bit or even 1-bit quantization, results in significant information loss, severely degrading model performance. To address these challenges, we propose Commutative Vector Quantization (CommVQ), novel method for efficient and accurate KV cache quantization tailored to longcontext LLMs. Unlike existing quantization techniques that treat each scalar in the KV cache independently, CommVQ performs quantization at the vector level. Specifically, we treat the key/value vector for each token as single unit rather than processing scalars individually. To achieve this, All experiments were conducted by Junyan Li at UMass Amherst. 1 CommVQ: Commutative Vector Quantization for KV Cache Compression we leverage additive quantization (Babenko & Lempitsky, 2014), variant of vector quantization, to encode each vector into low-bitwidth representation utilizing learned codebook, minimizing quantization error while significantly reducing memory usage. Zhang et al., 2023) aim to reduce KV cache size by evicting less important tokens, storing only the keys and values for the most relevant tokens. These approaches are orthogonal to our method and could potentially be combined with it to achieve even higher compression rates. More importantly, to integrate additive quantization into the self-attention mechanism in computationally efficient manner, the codebook is innovatively designed to be commutative with the Rotary Position Embedding (RoPE) matrix (Su et al., 2024). This allows drastic reduction in the computational overhead of KV decoding, where intermediate results can be pre-computed against each code in the codebook, and are then efficiently reused in computing the lengthy key-query products. By combining these innovations, CommVQ achieves superior trade-off between memory savings and accuracy. Extensive evaluation on two long-context benchmarks, LongBench (Bai et al., 2023) and InfiniteBench (Zhang et al., 2024b), as well as GSM8K (Cobbe et al., 2021), benchmark designed for complex reasoning, shows that compared to state-of-the-art KV cache quantization baselines, we achieve nearly lossless KV cache compression with 2-bit quantization, outperforming other methods. Furthermore, we achieve 1-bit quantization with significantly better accuracy than existing baselines, demonstrating the effectiveness of our method in pushing the compression limits of KV cache. We summarize our contributions as follows: Unlike prior work that quantizes each scalar individually in the KV cache, we quantize each vector as whole in the KV cache into low bit-width representation using learned codebook. By leveraging the commutative property of the RoPE matrix and the characteristics of self-attention, we refine our codebook to be RoPE-commutative. This refinement enables us to reformulate the self-attention computation to incorporate the decoding process more efficiently. Additionally, we provide Triton implementation of our method to demonstrate real memory savings. Extensive experiments demonstrate the superiority of our method, particularly in ultra-low-bitwidth KV cache quantization scenarios (e.g., 1-bit quantization). This opens up the possibility of serving long-context LLMs under limited GPU memory constraints. 2. Related Works KV Cache Compression. Several prior works have addressed KV cache compression, which generally fall into two categories: token eviction and quantization. Methods focused on token eviction (Xiao et al., 2024; Liu et al., 2024a; Another important approach to KV cache compression is quantization (Liu et al., 2024b; Hooper et al., 2024; Zhang et al., 2024a; Kumar, 2024), which reduces the bit width of the KV cache, thereby lowering its overall storage requirements. While prior works have successfully demonstrated 4-bit and 2-bit quantization for KV caches, few have explored the feasibility of achieving 1-bit quantization for long-context LLMs. Our method addresses this gap by introducing novel vector based KV cache quantization technique that enables 1-bit quantization with minimal accuracy loss. Furthermore, we offer new perspective on how to reformulate self-attention mechanisms and our quantization method for much more efficient integration. Vector Quantization. Vector quantization (Gray, 1984) is widely studied technique in signal processing and machine learning that represents high-dimensional data using smaller set of representative vectors, known as codebooks. Variants of VQ, such as product quantization (PQ) (Jegou et al., 2010) and additive quantization (AQ) (Babenko & Lempitsky, 2014), have been introduced to improve its efficiency and capacity. In machine learning, vector quantization has been successfully applied to areas such as generative modeling (Van Den Oord et al., 2017; Esser et al., 2021). However, its potential extension in the context of KV cache compression, remains largely unexplored. recent study, VQLLM (Kumar, 2024), introduced residual vector quantization (RVQ) for compressing the KV cache. However, their achieved compression rate is relatively modest, and their basic decodethen-self-attention process introduces significant computational overhead, limiting its practicality for serving longcontext LLMs. There remains lack of in-depth research on how to optimize vector quantization and better integrate it with self-attention for both more efficient and effective KV cache compression. 3. Preliminaries 3.1. Self-Attention and KV Cache Self-attention is the fundamental building block of LLMs. It takes query (Q), key (K), and value (V) matrices as inputs and produces an output (O) with the same shape as Q. The causal attention mask used in self-attention allows us to cache the key and value matrices, significantly speeding up token generation. During LLM inference, the process is divided into two stages: CommVQ: Commutative Vector Quantization for KV Cache Compression Prefilling Stage. At this stage, given the input prompt, the attention output is computed while simultaneously generating the KV cache. Given the hidden states of the prompt, RN d, where is the number of tokens, and is the hidden size, the Q, K, and matrices, as well as the self-attention output, are computed as follows: = XWQ, = XWK, Self-Attn = Softmax = XWV (cid:18) QK (cid:19) (1) Here, WQ, WK, and WV Rdd are the projection matrices for the query, key, and value, respectively. The computed and matrices are then cached for use in the subsequent decoding stage. Decoding Stage. During this stage, the KV cache is reused for self-attention computations. Given the current input hidden state R1d, the KV cache is updated as follows: Concat(K, xWK), Concat(V, xWV ) (2) The updated KV cache is then used to compute the selfattention output, following the same equation as in Eqn. 1: = xWQ Self-Attn = Softmax (cid:18) QK (cid:19) (3) The prefilling stage occurs once to process the input tokens and generate the first output token. This is followed by multiple iterations of the decoding stage, which produces all subsequent output tokens. 3.2. Rotary Position Embedding Positional encoding is added to the query (Q) and key (K) matrices to encode token position information during selfattention computation. Recent open-source LLMs, such as LLaMA (Dubey et al., 2024), Mistral (Jiang et al., 2023), and Qwen (Yang et al., 2024), commonly utilize Rotary Position Embedding (RoPE) (Su et al., 2024) for this purpose: qm qmRm, km kmRm (4) where qm, km R1d represent the query and key vector for the mth token, respectively, while Rm Rdd denotes the RoPE matrix applied to the mth token. RoPE matrix is sparse matrix with nonzero values only in its 2 2 diagonal blocks. Therefore, the application of RoPE can be reformulated by first dividing the km vector (and similarly qm; here, we use km as an example) into multiple 2-dimensional sub-vectors: where each 2-dimensional sub-vector ki = (kix, kiy) consists of two scalars within the km vector. The corresponding R22, 2 2 diagonal sub-matrix of Rm, denoted as Ri is then applied to each sub-vector ki (cid:18) cos mθi sin mθi cos mθi (cid:19) (6) Ri = sin mθi mRi ki ki where θi = 100002(i1)/d. The 2 2 diagonal sub-matrix Ri is, in fact, rotation matrix, which satisfies the following property: Property 1 (Commutativity): Let R22 defined as = (cid:19) (cid:18) y The following commutativity holds: Ri mC = CRi (7) (8) This property shows that and Ri are commutative under matrix multiplication. We leverage this key property to optimize our method in Sec. 4.2. 4. Method The KV cache is dominant factor in long-context LLM inference scenarios, as storing and loading the large KV cache becomes significant memory and latency bottleneck. Therefore, compressing the KV cache, even at the cost of additional encoding and decoding processes, is advantageous. Motivated by vector quantization, classical quantization technique from signal processing, and particularly inspired by its recent variant, additive quantization (Babenko & Lempitsky, 2014), we adopt similar approach to quantize each vector in the KV cache into compressed representation using learned encoder and codebook, as described in Sec. 4.1. To efficiently integrate vector quantization into self-attention while minimizing the computational overhead introduced by the additional decoding process, we innovatively reformulate the self-attention computation by designing RoPEcommutative codebook and reordering the matrix multiplications involved in self-attention. This refinement enables large portion of computation reuse and significantly reduces the computational cost of our method, as detailed in Sec. 4.2. 4.1. Learning Additive Quantization for KV Cache km = [k1x, k1y, ..., k(d/2)x, k(d/2)y] m, ..., kd/2 ] = [k1 (5) Additive quantization (Babenko & Lempitsky, 2014) aims to represent given d-dimensional vector as the element-wise 3 CommVQ: Commutative Vector Quantization for KV Cache Compression sequent computations in the self-attention mechanism. The encoder and codebook are optimized via gradient descent to minimize the MSE loss between the original tensor ti and its decoded counterpart ˆti. KV Cache Reduction Analysis. The size of the original FP16 KV cache for each layer can be calculated as 2 16 bits, where is the batch size, is the number of tokens and is the hidden size. After quantization, the size of the quantized KV cache for each layer becomes Nc 2 1 bits. Therefore, the reduction rate RR can be expressed as 1 Nc 16d . Subsequently, the average bit width used to represent each scalar in the KV cache is: Avg. bit = 16 16 RR ="
        },
        {
            "title": "Nc\nd",
            "content": "(10) For instance, the hidden size for the key and value for LLaMA-3.1-8B-Instruct model is 1024. If we set Nc to 1024, the reduction rate will be 15 16 , which is equivalent to 1-bit quantization. We will use Avg. bit as the metric to measure the compression rate in our experiment section. lower Avg. bit indicates higher compression rate and more reduced GPU memory usage compared to FP16. Computation Complexity. The computational overhead arises from both encoding and decoding the KV cache. Since the KV cache only needs to be encoded once, the primary source of computational overhead comes from decoding the full KV cache each time it is used during the generation of the next token. The computation for self-attention with KV cache decoding during inference step can be summarized as: qRt s0CKR0 s1CKR1 ... sN 1CKRN 1 SV CV Self-Attn = Softmax (cid:16) qK (cid:17) (11) (12) (13) (14) where R1d is the current query, is the number of tokens generated so far, SV {0, 1}N Nc are the quantized value, si {0, 1}Nc is the quantized vector for token-ith key, CK, CV RNcd are the codebook for the key and value, respectively, and Ri denote the RoPE matrix for ith token. The computational complexity can be expressed as: ((2d + 1)N + 2dNcN ) (15) where is the hidden dimension, is the number of tokens, and Nc is the number of rows in the codebook. The first 4 Figure 1. An illustration of vector quantization for KV cache compression. Each vector in the KV cache, corresponding to token, is first encoded into low-bitwidth representation, significantly reducing storage size. This process is applied to all vectors in the KV cache individually. When needed, the compressed representations are loaded and decoded back to the original KV cache using codebook. summation of several vectors from learned codebook C. For simplicity, we adopt per-token quantization scheme for both the key and value matrices, meaning that the ddimensional key and value vectors are quantized individually for each token. Inspired by additive quantization, our method incorporates learned encoder to encode the given d-dimensional vector into binary sequence, consisting of 0s and 1s, of length Nc, and codebook RNcd to decode the original vector back using simple matrix multiplication, as illustrated in Figure 1. Encoding. Given key or value vector for the ith token, ti Rd, we use an encoder to encode this vector into quantized vector si {0, 1}Nc, namely si = E(ti). In our implementation, the encoder consists of simple linear layer followed by an activation function, and then another linear layer for output. Gumbel-softmax (Jang et al., 2016) is used to make the encoder end-to-end differentiable. The encoding step takes place after the generation of the KV cache, and the quantized vector si for each token is concatenated and stored together to form the quantized KV cache S. Decoding. When loading the KV cache, the quantized KV cache is retrieved. For each tokens si, we perform simple matrix multiplication with the codebook to reconstruct the decoded tensor ˆti: ˆti = siC (9) The decoded key and value can then participate in the subCommVQ: Commutative Vector Quantization for KV Cache Compression term in Eqn. 15 corresponds to the computation for Eqn. 14, which represents vanilla self-attention. The second term, 2dNcN , accounts for the KV cache decoding process for the key and value, where each contributes dNcN . Comparing the first and second terms in Eqn. 15, we see that the computational overhead from the additional vector quantization decoding process is Nc times higher than that of vanilla self-attention. This increase arises from the need to first sum large number of rows in the codebook to reconstruct the decoded vector, after which the decoded vector can participate in the self-attention computation. The overhead is particularly significant because Nc is typically large (e.g., 1024 for 1-bit quantization in LLaMA-3.1 8B model). In the next section, we introduce our novel redesign, which leverages commutative codebooks to more efficiently integrate vector quantization with self-attention. This method will significantly reduce the overall computational overhead. 4.2. Commutative Codebook for Efficiency To simplify notation, we focus on the calculation within the constant: α = qK , where the softmax and ignore the output α is an -dimensional vector. Each scalar entry αi in α is computed as: αi = qRt(siCKRi)T = (qRt)RT T KsT (16) where Rt denotes the RoPE matrix for the query, Ri denotes the RoPE matrix for the ith key, and si is the quantized vector for the ith key. Notice that qRt and CK remain unchanged as varies. If Ri were independent of as well, then the computation of (qRt)RT would only need to be performed once across different i, significantly reducing the computational cost. Unfortunately, since Ri varies with i, such reduction is not possible. However, if we could design the codebook CK to be commutative with Ri, then the right-hand-side of Eqn. 16 could be rewritten as (qRt)C sT , which makes the bulk of computation, (qRt)C K, independent of and thus reusable, leading to substantial computational savings. KRT Next, we discuss how to define our new commutative codebook CK as well as how to learn it. Designing Commutative Codebook. As explained in Sec. 3.2, since the RoPE matrix is block-diagonal, we can break the problem into 2-dimensional subspaces and design the commutative codebook within each block. Formally, as defined in Eqns. 5 and 6, kj represent the j-th 2-dimensional sub-vector/sub-matrix of ki and Ri, respectively. Extending Property 1, we can obtain design for sub-space codebook. Specifically, let Cj be set of codebooks for subspace of and Rj 5 the key vector, i.e., = {C j0 Cj , j1 , . . . , j(Nc 1) } (17) where Nc is the number of quantization levels, and each jl is 2 2 matrix that satisfies the form defined in Eqn. 7, and thus has the commutative property Rj KRj . The quantized vector for kj is 2dimensional vector taking the values from {0, . . . , Nc 1}. The decoded key is represented as = jl , denoted as sj , jl Nc 1 (cid:88) [sj = l]C jl ˆkj = (18) l=0 where [sj = l] is 2-dimensional boolean indicator vector, with each dimension equal to 1 if the corresponding dimension of sj is equal to l, and 0 otherwise. Notice that the decoding scheme in Eqn. 18 is more complicated than previously discussed (Eqn. 9). This is because previously we used the same codebook for all the dimensions, but now the codebook for different dimensions is different. Please refer to Appendix A.1 for more explanations. With the new decoding scheme, the benefit of commutativity remains. To see this, notice that Eqn. 16 should now be rewritten as (cid:88) (qjRj )(ˆkj Rj )T (cid:88) (cid:88) αi = = = = (qjRj )(cid:0) (cid:88) [sj = l]C jl KRj l (cid:1)T (19) (qjRj )RjT jlT [sj = l]T j,l (cid:88) (qjRj )C jlT RjT [sj = l]T j,l The first equality decompose the inner products into those of the sub-vectors; the last equality applies the commutativity property, making (qjRj reusable across i. )C jlT Learning the Codebook. The codebook is learned by minimizing the reconstruction error: min i(Cj ,sj ) ˆkj kj 2, s.t. Eqn. 18 (20) (cid:88) holding Cj This is canonical clustering objective and can be efficiently solved via an EM-like algorithm, where the E-step minimizes over sj fixed, and the M-step minimizes over Cj fixed, as described in Algorithm 1. More details can be found in Appendix A.2, including the actual update formula and the techniques used to stabilize the optimization process. holding sj CommVQ: Commutative Vector Quantization for KV Cache Compression Algorithm 1 EM Algorithm for Learning RoPE Commutative Codebook. 1: Input: calibration set RN 2 2: Parameters: codebook Cj 3: Goal: Optimize Cj to minimize the clustering error over calibration set K. converges do 4: while Cj 5: Step: Fix Cj K, update the assignment such that each is assigned to its nearest clustering center. 6: Step: Fix S, update Cj such that the MSE loss between each and its assigned clustering center is minimized. 7: end while Notice that sj requires 2 log2(Nc) bits, which limits its ability to achieve high compression rate relative to kj . To improve compression, we group consecutive sub-vectors into one group and share the quantized value within the = = sg1 group, i.e., s0 . This allows the entire vector to be represented using significantly fewer bits. = s1 To further improve the quantization accuracy, we iteratively apply the clustering algorithm on the quantization error tensors with new codebook for each time, repeating the process until the error is sufficiently minimized. Specifically, we run the clustering algorithm times, resulting in codebooks Cj for subspace j. is hyperparameter that balances quantization accuracy and compression rate. This iterative refinement is conceptually similar to the residual approach in residual vector quantization (Barnes et al., 1996), where subsequent iterations focus on reducing the remaining error. As result, total of instances of are required to quantize 2g FP16 scalars, and the total number of bits needed to quantize the full d-dimensional vector is 2R log2(Nc) 2g . Accordingly, the average quantization bit can be computed as: Avg. bit = log2(Nc) (21) For example, to achieve 1-bit quantization, we set Nc = 64, = 11 and = 64. We provide an ablation study on how to choose Nc, and in Appendix A.4. Reduced Computational Complexity. For value quantization, we retain our original method but reorder the matrix multiplication process. Specifically, we first multiply the softmaxed attention weights by SV , followed by multiplying the result by CV , as illustrated below: Self-Attn = Softmax(A)SV CV (22) This simple reordering of matrix multiplication reduces the computational complexity from O(dNcN + dN ) in Eqns. 13,14 to O(NcN + dNc) in Eqn. 22, which is nearly times lower, assuming and Nc are of similar scale. By integrating these adjustments into the original selfattention mechanism, the optimized computation complexity is now: O((Rd + Nc + 1)N + d(Nc + RNc)) (23) This optimization effectively reduces the computational cost compared to the unoptimized decode-then-self-attention calculation (Eqn. 15). Previously, the complexity was Nc times higher than that of the original self-attention; now, it is approximately R+1 times higher. Since is relatively small hyperparameter (e.g., = 11 for 1-bit quantization), the overhead remains minimal. 2 5. Experiments 5.1. Settings Models. We evaluate CommVQ using the latest LLaMA3.1-8B-Instruct model (Dubey et al., 2024), which supports context length of up to 128K tokens. subset of the FineWeb-Edu dataset (Lozhkov et al., 2024) is used to learn the encoder and codebooks. We present evaluation results for two quantization levels: 2-bit and 1-bit quantization (see Appendix A.4 for codebook configuration). To demonstrate the generalizability of our method, we also conduct additional experiments on the LLaMA-2-8B (Touvron et al., 2023) and Mistral-8B (Jiang et al., 2023) models. Baselines. We compare our method to three recent KV cache quantization techniques: KIVI (Liu et al., 2024b), KVQuant (Hooper et al., 2024), and VQLLM (Kumar, 2024). KIVI employs asymmetric quantization, KVQuant uses non-uniform quantization, and VQLLM applies residual vector quantization. For fairness, we reproduced their results using their official open-source implementations on the same models. We denote quantization versions as <method>-<n>, where <n> represents bits per scalar in KV cache. For VQLLM, we set = 256, = 8 for 2-bit and = 256, = 4 for 1-bit quantization. Tasks. To evaluate the effectiveness of our method for longcontext LLMs, we test it alongside the baselines on two long-context benchmarks: LongBench (Bai et al., 2023) and InfiniteBench (Zhang et al., 2024b). Additionally, to assess the models ability to perform complex reasoning, we evaluate it on GSM8K (Cobbe et al., 2021). Apart from the task score, we also report the average quantization bit, denoted as Avg. bit, for each method to quantify the actual KV cache size reduction. lower Avg. bit indicates less storage required for the KV cache, leading to greater memory savings. For baseline methods, we follow the calculations provided in their respective papers to determine the Avg. bit. CommVQ: Commutative Vector Quantization for KV Cache Compression Method Avg. bit () Qasper QMSum MultiNews TREC TriviaQA SAMSum LCC RepoBench-P Average () FP16 Baseline KIVI-2 KVQuant-2 VQLLM-2 CommVQ-2 KIVI-1 KVQuant-1 VQLLM-1 CommVQ-1 16 3.00 2.33 2.00 2. 2.00 1.33 1.00 1.03 25.19 22.71 41.86 32.39 24.67 4.99 1.01 11.92 18.86 23.31 24.33 22.37 25.20 24. 9.57 8.71 17.91 23.02 26.82 27.29 25.76 26.22 26.48 9.20 6.06 13.12 24.34 72.50 72.50 69.00 69.95 72. 38.75 1.00 47.98 69.00 91.65 92.06 89.00 92.01 91.92 25.07 1.50 63.34 91.61 43.49 43.26 42.09 41.03 43. 11.93 6.64 23.72 41.83 52.47 51.32 36.22 40.58 53.02 17.67 11.01 18.92 48.78 49.01 47.53 36.51 36.19 46. 16.40 11.09 22.44 42.08 48.05 47.62 45.35 45.45 47.98 16.70 5.88 27.42 44.94 Table 1. LongBench evaluation for the LLaMA-3.1-8B-Instruct model. Method Avg. bit () R.PK R.Num R.KV En.Sum En.QA En.MC En.Dia Code.D Math.F Average () FP16 Baseline KIVI-2 KVQuant-2 VQLLM-2 CommVQ-2 KIVI-1 KVQuant-1 VQLLM-1 CommVQ-1 16 3.00 2.33 2.00 2. 2.00 1.33 1.00 1.03 100.00 100.00 98.81 100.00 100.00 1.86 0.00 82.37 99.15 99.49 97.80 88.81 97.96 93. 0.00 0.00 14.75 62.37 55.20 0.60 0.00 0.00 12.20 0.00 0.00 0.00 0.00 26.74 25.41 25.02 18.27 24. 12.44 17.83 10.46 19.34 14.28 13.90 7.77 8.09 14.57 3.24 1.36 2.69 11.30 66.81 66.81 35.81 44.54 67. 55.46 0.44 25.33 65.50 20.00 22.50 8.00 9.50 18.00 4.50 2.00 1.50 18.00 22.08 23.35 25.63 21.83 22. 23.86 0.25 21.83 22.08 33.43 33.71 10.29 29.71 33.14 34.00 1.14 7.43 33.14 48.67 42.68 33.34 36.66 42. 15.04 2.56 18.48 36.76 Table 2. InfiniteBench evaluation for the LLaMA-3.1-8B-Instruct model. 5.2. Long Context Benchmarks Evaluation sequence length is set to 128K. LongBench Evaluation. LongBench (Bai et al., 2023) is benchmark for evaluating models on long-context tasks like multi-doc QA, summarization, and code completion. Following KIVI (Liu et al., 2024b), we assess performance on the same eight tasks across four subgroups and report both individual and average scores. The maximum sequence length is set to 128K. Experiment results are presented in Table 1. Our 2-bit quantization model achieves lossless accuracy on most tasks, maintaining almost the same average score as FP16 model while outperforming other baselines as well as offering greater memory savings. Compared to KIVI, our method provides 33% more memory savings while achieving higher average score. Among baselines with similar average quantization bit, our approach performs significantly better, with scores 2.63% higher than KVQuant and 2.53% higher than VQLLM. For 1-bit quantization, our method substantially outperforms other baselines, achieving 17.52% higher average score than VQLLM while maintaining comparable memory savings. This minimizes accuracy degradation compared to the FP16 model. InfiniteBench Evaluation. InfiniteBench (Zhang et al., 2024b) evaluates models on ultra-long contexts, mimicking real-world scenarios with near-infinite input lengths. Its tasks include multiple types of retrieval, QA, summarization, In our experiment, the maximum and code debugging. Experiment results are shown in Table 2. Our method continues to excel in ultra-long context scenarios, with even greater superiority in 1-bit quantization. For challenging tasks that require accurate information, such as retrieval (R.PK, R.Num, and R.KV), other methods fail to produce accurate results, whereas ours retains some capacity even at low quantization levels. This is due to our codebook design, which effectively preserves information in the KV cache, as confirmed by our methods lower quantization error (measured in MSE) shown in Table 11 in Appendix A.5. Needle-in-a-Haystack Evaluation. We further evaluate our method using LLaMA-3.1 8B model on the Needle-ina-Haystack (NIAH) benchmark, which specifically targets retrieval capabilities in long-context settings by requiring the model to identify small piece of information embedded within large amount of irrelevant text. This test serves as strong indicator of how well KV cache and attention mechanisms are preserved under quantization. Experimental results in Figure 2 demonstrate that our 2-bit quantization model successfully retains full retrieval capability, matching the performance of the FP16 baseline. This confirms that our approach introduces negligible degradation even under aggressive compression. More notably, our 1-bit CommVQ variant achieves stronger retrieval accuracy than KIVIs 1-bit counterpart, highlighting the effectiveness of our codebook design in preserving critical attention 7 CommVQ: Commutative Vector Quantization for KV Cache Compression Figure 2. Needle-in-a-Haystack test using the LLaMA-3.1 8B model. We present the test result for FP16 baseline (top-left), KIVI-1 (bottom-left), CommVQ-2 (top-right) and CommVQ-1 (bottom-right). Our methods 2-bit version matches FP16 performance, while the 1-bit CommVQ variant outperforms KIVI, demonstrating strong retrieval fidelity under extreme compression. Method Avg. bit () GSM8K () Model Avg. bit () LongBench () FP16 Baseline KIVI-2 VQLLM-2 CommVQ-2 KIVI-1 VQLLM-1 CommVQ-1 16 3.00 2.00 2.00 2.00 1.00 1.03 76. 73.69 52.69 76.04 2.20 1.67 66.57 Table 3. GSM8K benchmarks evaluation for the LLaMA-3.1-8BInstruct model. Exact match accuracy is used as the metric. signals. These findings reinforce that CommVQ maintains high fidelity in information-dense retrieval tasks, even under extreme compression ratios. 5.3. GSM8K Evaluation To demonstrate the effectiveness of our approach on challenging and complex tasks, we conduct experiments on GSM8K (Cobbe et al., 2021), rigorous benchmark comprising high-quality, linguistically diverse math problems meticulously crafted by human experts. These problems require intricate multi-step reasoning and involve complex arithmetic operations, making GSM8K an ideal benchmark for evaluating complex reasoning capabilities. As shown in Table 3, our 2-bit quantization model outperforms other baselines, maintaining significantly higher accuracy, 2.35% above KIVI and 23.35% above VQ-LLM, while exhibiting 8 Llama-2-7B Mistral-7B FP16 KIVI CommVQ FP16 KIVI CommVQ 16 3.00 2. 16 3.00 2.00 48.43 47.14 47.27 53.40 52.78 53.04 Table 4. Performance comparison of full precision (FP16), KIVI, and CommVQ applied to two additional LLMs for model ablation. only minimal accuracy degradation of 0.23% compared to FP16. Furthermore, while our baselines struggle to generate accurate results under 1-bit quantization, our method continues to demonstrate strong reasoning capabilities even under this extreme compression. 5.4. Model Ablation We also apply our method to two additional long-context LLMs: LLaMA-2-7B (the 32K context length version from Together.ai) and Mistral-7B-v0.3 (which natively supports 32K context length). We evaluate both on LongBench, comparing their average scores to the FP16 baseline and KIVI. The experimental results are summarized in Table 4. For both the LLaMA-2 and Mistral models, our method consistently preserves the FP16 baselines average score while achieving better compression rateaccuracy trade-off than KIVI. These results highlight the broad applicability and effectiveness of our approach. CommVQ: Commutative Vector Quantization for KV Cache Compression Latency (ms) 8K 32K 128K Naive Impl. Optimized Impl. Speedup 2.4 0.4 6.0 9.2 1.1 8.4 36.6 3. 9.6 Table 5. Latency comparison between the naive implementation and the optimized implementation utilizing the commutative codebook. Latency per layer per token is measured for context lengths of 8K, 32K, and 128K, reported in milliseconds (ms). Method FineWeb-Edu GSM-8K Repobench-p KV Retrieval FP16 CommVQ-2 PPL Diff 10.17 11.54 +1.37 5.67 6.14 +0.47 2.20 2.78 +0. 31.93 32.72 +0.79 Table 6. Perplexity (PPL) comparison between FP16 baseline and CommVQ-2 across different domains. 5.5. Robustness Analysis Under Domain Shift Our codebook and encoder are trained on the FineWebEdu (Lozhkov et al., 2024) pre-training dataset. To evaluate their robustness when the testing domain differs from the training domain, we conducted an analysis using the LLaMA-3.1 8B model. Specifically, we compared the perplexity of CommVQ-2 against the FP16 baseline across four distinct datasets: FineWeb-Edu, general text dataset, also the training set; GSM-8K (Cobbe et al., 2021), math dataset; Repobench-p (Bai et al., 2023), code retrieval and completion dataset; and KVRetrieval in InfiniteBench (Zhang et al., 2024b), synthetic UUID keyvalue retrieval dataset. The first dataset represents in-domain evaluation, while the last three represent evaluations with domain shifts, i.e., the codebooks and encoder are trained on general text and tested on math, code, and synthetic UUID data. The results are summarized in Table 6. We find no significant increase in perplexity (PPL) due to domain shifts when compared to in-domain evaluations. This suggests that our method performs consistently well across domains that differ from the calibration data, including synthetic UUID data, which is unlikely to appear in the calibration set. Overall, we conclude that our method is robust and generalizable under domain shifts. (a) Memory usage vs. context length (batch size = 1). (b) Memory usage vs. batch size (context length = 32K). Figure 3. Per-token decoding memory usage of CommVQ (1-bit) compared to FP16 model. Experiments are conduct on LLaMA3.1-8B-Instruct model. validating the commutative codebooks effectiveness in reducing computational overhead. We also implement Triton kernels to achieve real memory savings. Figure 3 highlights the real per-token decoding memory savings using the LLaMA-3.1-8B-Instruct model, measured on an H100-80GB GPU. 120K context length requires 60GB in FP16, while our method reduces it to 20GB, enabling inference on single consumer GPU such as RTX 4090. For 32K context length, FP16 runs into OOM at batch size of 8, but our method scales up to 128. This improves long-context and large-batch serving, benefiting lot of applications such as long document QA. We also provide additional analysis of codebook size in Appendix A.3, demonstrating that its size is negligible, especially compared to the large KV cache for long contexts. 6. Conclusion 5.6. Efficiency Results In Table 5, we present latency comparison to demonstrate the impact of our methods reduced computation (denoted as the optimized implementation) compared to the naive implementation, which does not utilize the commutative codebook to reduce the computation. For context lengths of 8K, 32K, and 128K, the optimized implementation consistently achieves speedups over the naive implementation, We introduce CommVQ, novel KV cache quantization approach for long-context LLMs. By leveraging vector quantization and RoPE-commutative codebook, CommVQ significantly reduces KV cache size while maintaining high computational efficiency. Evaluations on longcontext benchmarks show that CommVQ outperforms existing KV cache quantization methods, enabling more memoryefficient and scalable long-context LLM inference. 9 CommVQ: Commutative Vector Quantization for KV Cache Compression"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Babenko, A. and Lempitsky, V. Additive quantization for extreme vector compression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 931938, 2014. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Barnes, C. F., Rizvi, S. A., and Nasrabadi, N. M. Advances in residual vector quantization: review. IEEE transactions on image processing, 5(2):226262, 1996. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J., Yang, F., and Yang, M. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Gray, R. Vector quantization. IEEE Assp Magazine, 1(2): 429, 1984. Hooper, C., Kim, S., Mohammadzadeh, H., Mahoney, M. W., Shao, Y. S., Keutzer, K., and Gholami, A. Kvquant: Towards 10 million context length llm inarXiv preprint ference with kv cache quantization. arXiv:2401.18079, 2024. Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. Jegou, H., Douze, M., and Schmid, C. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117128, 2010. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.- Y., Chen, H., and Hu, X. Llm maybe longlm: Selfextend llm context window without tuning. arXiv preprint arXiv:2401.01325, 2024. Kumar, A. Residual vector quantization for kv cache compression in large language model. arXiv preprint arXiv:2410.15704, 2024. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis, A., and Shrivastava, A. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36, 2024a. Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V., Chen, B., and Hu, X. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024b. Lozhkov, A., Ben Allal, L., von Werra, L., and Wolf, T. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/ datasets/HuggingFaceFW/fineweb-edu. Shi, L., Zhang, H., Yao, Y., Li, Z., and Zhao, H. Keep the cost down: review on methods to optimize llms kv-cache consumption. arXiv preprint arXiv:2407.18003, 2024. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 10 CommVQ: Commutative Vector Quantization for KV Cache Compression Xiao, G., Tang, J., Zuo, J., Guo, J., Yang, S., Tang, H., Fu, Y., and Han, S. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yuan, J., Liu, H., Zhong, S., Chuang, Y.-N., Li, S., Wang, G., Le, D., Jin, H., Chaudhary, V., Xu, Z., et al. Kv cache compression, but what must we give in return? comprehensive benchmark of long context capable approaches. arXiv preprint arXiv:2407.01527, 2024. Zhang, T., Yi, J., Xu, Z., and Shrivastava, A. Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization. arXiv preprint arXiv:2405.03917, 2024a. Zhang, X., Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M., Han, X., Thai, Z., Wang, S., Liu, Z., et al. Bench: Extending long context evaluation beyond 100k tokens. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1526215277, 2024b. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. 11 CommVQ: Commutative Vector Quantization for KV Cache Compression A. Appendix A.1. Explanation of Encoding and Decoding Using Commutative Codebooks In this section, we provide an additional explanation of how the 2-dimensional sub-vector kj quantized vector sj and the codebook Cj K, as defined in Eqn. 18. is represented using the We first formulate the quantization process as clustering problem, where the cluster centers are defined as follows: ca,b = (cid:20)1 (cid:21) 0 Cj K[a] + (cid:20)0 (cid:21) 1 Cj K[b] (24) clustering centers. We can then quantize kj K[b] represent the ath and bth 2 2 sub-codebooks in Cj forms total into its nearest clustering center, and use = {a, b} as the quantized K. Consequently, the codebook Cj K[a] and Cj Here, Cj of 2 representation to represent kj . When decoding, we use the assigned clustering center to approximate kj , so ˆkj = ca,b (cid:21) (cid:20)1 0 ="
        },
        {
            "title": "C j",
            "content": "K[a] + (cid:21) (cid:20)"
        },
        {
            "title": "C j",
            "content": "K[b] (25) And Eqn. 25 is doing exactly the same thing as Eqn. 18. A.2. EM Algorithm Implementation Details As mentioned in Sec 4.2, we apply EM-like algorithm to learn the codebook. In this section we provide the algorithm detail. We use subset of FineWeb-Edu (Lozhkov et al., 2024) as the calibration set and optimize the codebook over this calibration set. The Step is straightforward as it simply assign each vector in to its nearest clustering center. For Step, we derive closed form formula to update Cj given the current assignment S. Recall the definition of Cj K: , . . . , j(Nc 1) , j1 = {C j0 Cj (cid:18) xl yl jl yl xl = (cid:19) } We first define some useful terms: i=0 ϕ = [xi, yi]Nc 1 = [mij]Nc 1,Nc 1 = diag(Nij, Nij)Nc 1,Nc 1 i,j=0 i,j=0 (26) (27) (28) where ϕ is 2Nc-dimensional vector that represent the codebook Cj and we are going to optimize it. is 2 cdimensional vector that represents the mean for the data points assigned to each cluster center. is 2Nc 2Nc matrix that has the total number of data points assigned to each cluster center in its diagonal. Next, we define an auxiliary constant matrix {1, 0, 1}(2Nc 2)(2Nc ), where its entries are given by: T2(xNc +y),2x = 1, T2(xNc +y),2y+1 = 1 T2(xNc +B)+1,2y = 1, T2(xNc +y)+1,2x+1 = 1 for all x, {0, 1, . . . , Nc 1}, and all other elements of are zero. We can then rewrite Eqn. 18 into the matrix form: (T ϕ m)T S(T ϕ m) min ϕ 12 (29) (30) (31) (32) CommVQ: Commutative Vector Quantization for KV Cache Compression"
        },
        {
            "title": "The closed form solution is given by",
            "content": "ϕ = (T ST )1T Sm We iteratively update ϕ until it converges. During our experiments, we found that the learning process was not stable and it would frequently fail to optimize since the number of the clustering centers is large in our case (e.g., there are 4096 clustering centers forNc = 64) and the clustering centers needs to satisfy Eqn. 24. In order to stabilize the optimization process, we employ two techniques. The first technique is soft clustering center assignment, where instead of hard-assigning data point to its nearest clustering center, we distribute it among all clustering centers with different weights. The weight assigned to each center depends on its proximity to that data point, with the closest center receiving the highest weight. To be specific, let be the number of data points in the calibration set K, and Ncc be the number of clustering centers, weight matrix RN Ncc is calculated as (33) Wij = eDij eDik (cid:80) (34) where RN Ncc is the L2 distance matrix measuring the L2 distance between each data point and each clustering center. We then use instead of the hard assignment to calculate and in Eqn. 28. Furthermore, we empirically observe that in the early iterations, the distribution of needs to be smoother to prevent dead clustering centers. In contrast, in later iterations, sharper distribution of helps achieve better convergence. Therefore, we introduce temperature annealing in Eqn. 35 to regulate the distribution: Wij = Dij e Dik (cid:80) (35) where is the temperature parameter, which exponentially decays over the iterations. A.3. Codebook Size Analysis Our method uses vector quantization with codebook to compress the KV cache, which requires additional GPU memory to store the codebook. The codebook configuration is shown in Table 8 and is stored in FP16. The total codebook size is calculated as: Value Codebook Size (MB) = Nc Key Codebook Size (MB) = 2 2 Nc 2 2 (36) (37) where is the hidden size of the LLM. For LLaMA-3.1-8B-Instruct model, = 1024. Nc is the number of rows in the value codebook. For the key codebook, Nc means the number of quantization level, means the number of the residual quantization. We analyze the codebook size for both 2-bit and 1-bit quantization for LLaMA-3.1-8B-Instruct model, as shown in Table 7. For comparison, KV cache with 128K context length requires 256 MB each for values and keys. Notably, the codebook size remains constant regardless of the number of tokens, making its memory overhead negligible for long-context LLM inference. A.4. Ablation Study on Commutative Codebook Configuration We utilize specially designed commutative codebook to quantize key cache as described in Sec. 4.2. There are three hyper-parameters that control the level of compression: Nc, the number of quantization levels, R, the number of the residual quantization, and g, the number of sub-vector in group that share the same quantization vector s. Revisit that the formula to compute the average quantization bit is: Avg. bit = log2(Nc) (38) All experiments in this section are conducted using LLaMA-3.1-8B-Instruct model and the quantization error, measured in MSE, is calculated using the key cache for the first layer of the LLaMA model on subset of the FineWeb-Edu (Lozhkov et al., 2024) dataset. 13 CommVQ: Commutative Vector Quantization for KV Cache Compression Avg. Bit 1 bit 2 bit"
        },
        {
            "title": "2.00 MB 4.00 MB\n2.75 MB 5.25 MB",
            "content": "Table 7. Analysis on Codebook Size. We calculate the codebook size based on LLaMA-3.1-8B-Instruct model. 1 bit 2 bit 1024 11 64 2048 21 64 Nc Nc Table 8. Codebook Configuration. Nc is used for value codebook, and R, Nc is used for key codebook. In our first ablation study, we examine the effect of when we make = 1 and keep the average quantization bit to be the same, as shown in Table 9. We also vary for different to keep the average quantization bit to be 1, as shown in Table 10. Nc MSE 8 16 32 64 2 4 16 64 1 1 1 0.2699 0.2011 0.1265 0.0906 Nc Avg. bit MSE 8 16 32 64 2 4 16 64 8 8 8 1 bit 1 bit 1 bit 1 bit 0.0798 0.0790 0.0254 0.0095 Table 9. Ablation study on with = 1, while maintaining consistent Avg. bit. Table 10. Ablation study on and while keeping the Avg. bit to be 1. From both Table 9 and 10 we can conclude that when keeping the average quantization bit the same, larger will result in lower quantization error, though in the cost of an increased number of Nc which will induce an increase in the computation complexity as stated in Eqn. 23. Comparing the same row in Table 9 and 10 that has the same and Nc, we can further conclude that larger will bring lower quantization error when keeping and Nc unchanged. Figure 4. Ablation study on while keeping = 64 and Nc = 64. Next, we keep = 64 and Nc = 64, and vary to get the quantization error for different compression rate. The result is shown in Figure 4. The value for 1-bit quantization and 2-bit quantization is labeled in the figure by blue dot. As we can see, increasing will continuously decrease the quantization error, in the cost of higher average quantization bit. In conclusion, larger and larger will lead to better quantization accuracy, but also lead to higher computation and lower compression rate. As result, to achieve good trade-off between quantization accuracy, computation cost and 14 CommVQ: Commutative Vector Quantization for KV Cache Compression compression rate, we set = 64, Nc = 64 for all our main experiments, and = 11 for 1-bit quantization and = 21 for 2-bit quantization respectively. A.5. Quantization Error Comparison We conduct experiments to compare the quantization error of our method, measure in MSE between the original KV cache and the decoded KV cache, with our baseline method. To be specific, we use the value cache from the first layer of LLaMA-3.1-8B-Instruct model for MSE calculation. Asymmetric quantization serves as the baseline for comparison. From Table 11, we observe that our additive quantization-based method outperforms asymmetric quantization, particularly when the quantization bit is low, such as in the case of 1-bit quantization. Method Avg. bit MSE Asymmetric quant. CommVQ Asymmetric quant. CommVQ 2 bit 1 bit 0.00030 0.00014 0.00380 0.00027 Table 11. Comparison of MSE between CommVQ and asymmetric quantization used in KIVI (Liu et al., 2024b), calculated using the cached value matrix from the first layer of LLaMA-3.1-8B-Instruct. The MSE is evaluated on small subset of the FineWeb-Edu dataset (Lozhkov et al., 2024)."
        }
    ],
    "affiliations": [
        "Apple Inc.",
        "Massachusetts Institute of Technology",
        "Princeton University",
        "University of Massachusetts Amherst"
    ]
}