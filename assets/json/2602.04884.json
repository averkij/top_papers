{
    "paper_title": "Reinforced Attention Learning",
    "authors": [
        "Bangzheng Li",
        "Jianmo Ni",
        "Chen Qu",
        "Ian Miao",
        "Liu Yang",
        "Xingyu Fu",
        "Muhao Chen",
        "Derek Zhiyuan Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance. We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 4 8 8 4 0 . 2 0 6 2 : r 2026-2-"
        },
        {
            "title": "Reinforced Attention Learning",
            "content": "Bangzheng Li*1, Jianmo Ni4, Chen Qu3, Ian Miao3, Liu Yang3, Xingyu Fu2, Muhao Chen1 and Derek Zhiyuan Cheng4 1UC Davis, 2Princeton University, 3Google, 4Google DeepMind Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance. We propose Reinforced Attention Learning (RAL), policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as principled and general alternative for multimodal post-training. Figure 1 Reinforced Attention Learning formulates internal attention distributions as policy. Unlike traditional methods that optimize next-token probabilities (what to generate), our approach prioritizes the selective allocation of information (where to focus). By optimizing for the advantage, the model explores high-reward attention policy that effectively isolates salient information from dense contexts. 1. Introduction Large Language Models (LLMs) have achieved remarkable proficiency in complex domains such as mathematics and programming. Beyond massive unsupervised pre-training [4, 28], post-training has emerged as critical technology for eliciting long-form Chain-of-Thought (CoT) reasoning. Current paradigms predominantly employ Reinforcement Learning (RL) to optimize the models policy, utilizing rewards derived from learned models or programmatic verifiers to favor high-utility token sequences. Fundamentally, these policy gradient methods refine the next-token distribution to maximize expected rewards. Recent research has further highlighted robust correlation between reasoning length and task accuracya phenomenon central to the concept of test-time scaling. To extend these gains to multimodal LLMs (MLLMs), recent studies have attempted to incorporate thinking processes into Visual Question Answering (VQA) tasks. Under this paradigm, models are incentivized to generate exhaustive textual descriptions of visual inputs as CoT prior to providing * Work done as student researcher at Google. Reinforced Attention Learning final answer. However, our results reveal that for core perception taskssuch as image or video question answeringextensive textual reasoning provides only marginal gains and may even degrade the models fundamental perceptual capabilities. We attribute this limitation to the insufficiency of next-token prediction as the fundamental policy objective in MLLM post-training. In typical MLLM architectures, visual inputs are encoded as tokens and projected into the textual embedding space to serve as context for generation. Accurately answering fine-grained questions requires the model to precisely identify and attend to task-relevant visual information. This process is governed by the Transformers attention mechanism, which must learn to assign high weights to salient multimodal tokens. Standard RLHF, however, optimizes for the result (the token) rather than the process (the internal information allocation). Inspired by this observation, we reformulate the post-training policy for MLLMs to operate directly on the attention distribution during generation. This yields Reinforced Attention Learning (RAL), an algorithm designed to optimize the model toward high-utility attention trajectories. Unlike traditional methods, RAL treats the attention pattern itself as the policy: when response receives high reward, the algorithm encourages the underlying attention distribution by minimizing the divergence between the current and reference policies. Conversely, for low-reward responses, the model is penalized by increasing the divergence from those sub-optimal patterns. By shifting the optimization target from token likelihood to attention-based allocation, RAL fine-tunes MLLMs more directly for multimodal alignment. Our results indicate that RAL consistently outperforms Group Relative Policy Optimization (GRPO) across video and image benchmarks, particularly on perceptionintensive tasks. The efficacy of optimizing attention distributions naturally extends to On-Policy Distillation. While traditional distillation focuses on token-level probability alignment, we propose dual-distillation approach that transfers knowledge via both token and attention distribution alignment. Our experiments indicate that the inclusion of attention distillation provides significant additional performance gains. In summary, this paper introduces novel post-training paradigm for MLLMs. Our contributions are as follows: Reinforced Attention Learning: We propose RAL, policy-gradient method that shifts the optimization objective from next-token prediction to attention-distribution alignment, enabling direct reinforcement of visual grounding rather than indirect supervision through textual outputs. On-Policy Attention Distillation: We further extend this framework to an On-Policy Distillation setting, which substantially improves student models ability to inherit fine-grained perceptual and grounding behaviors from teacher. Empirical Validation: Extensive experiments demonstrate that RAL consistently improves upon GRPO across diverse visual question answering benchmarks that require fine-grained visual understanding and perception. 2. Related Works 2.1. Post training LLMs through Reinforcement Learning Post-training is now the standard for aligning Large Language Models (LLMs) with human intent [22]. The traditional RLHF pipeline involves Supervised Fine-Tuning (SFT), training Reward Model (RM) to mimic human preferences, and optimizing the policy via Reinforcement Learning (RL) [6]. While early RLHF methods significantly improved model safety and helpfulness [3], they relied heavily on Proximal Policy Optimization (PPO) [24]. However, PPOs actor-critic framework is memory-intensive due to the auxiliary critic model. Group Relative Policy Optimization (GRPO) [25] addresses this by replacing the critic with group-averaged reward estimates. This shift reduces computational overhead while maintaining high performance, particularly in verifiable domains like reasoning and code [11], leading to the domain of RL with Verifiable Rewards (RLVR). Extending post-training to multimodal LLMs (MLLMs) introduces challenges beyond text-only alignment, including visual hallucination and robust cross-modal grounding [18, 19, 39]. Recent 2 Reinforced Attention Learning methods adapt RLHF, RLVR, or Direct Preference Optimization (DPO) to improve visual grounding and reduce hallucinations [2, 15, 40]. persistent issue is modality bias, where the model over-relies on linguistic priors or, conversely, overfits to superficial visual cues [5]. To address this, recent work designs reward functions and training signals that discourage text-only shortcuts, penalize spurious visual correlations, and promote faithful, evidence-based responses [33, 35]. Our approach targets the same goal by leveraging fundamental information-selection mechanism: attention. Since cross-modal reasoning depends on identifying salient evidence in both modalities, directly shaping attention weights provides principled way to control the cross-modal reasoning policy in our method, rather than relying solely on text-token-level policies. 2.2. Distilling knowledge and beyond from teacher to student models Knowledge Distillation (KD) transfers knowledge from high-capacity teacher to student by matching softened output distributions rather than hard labels [13]. By providing richer supervisory signals, KD has been widely adopted for model compression, domain adaptation, and efficient deployment [23, 38]. In the context of large language models, distillation has been extended beyond output logits to intermediate representations, attention maps, and hidden states, enabling improved preservation of structural and reasoning behaviors [16, 27, 31]. More recent work has explored on-policy distillation [1], in which the student generates responses under its own policy and receives supervision from teacher evaluations along these trajectories. Compared to offline KD on static datasets, on-policy distillation mitigates exposure bias and better aligns the students generation distribution with deployment-time behavior. This paradigm is closely related to RLbased post-training, yet retains the stability and efficiency of supervised learning objectives. In this work, we investigate knowledge distillation and attention distillation as alternative mechanisms for learning effective attention distributions. Incorporating attention-level supervision within an on-policy distillation framework provides principled means of regularizing the students internal information allocation while maintaining policy alignment. This motivates our experimental study of on-policy attention distillation as complementary approach to purely reward-driven optimization. 3. Reinforced Attention Learning Traditional reinforcement learning (RL) for Large Language Models (LLMs) typically optimizes policy by maximizing the expected return over the output token distribution. Modern off-policy algorithms like PPO and GRPO utilize surrogate objective based on importance sampling: LRL = 洧댶洧노 (cid:20) 洧랢洧랚(洧녩洧노 洧멇롐) 洧랢old(洧녩洧노 洧멇롐) (cid:21) 洧냢洧노 (1) where 洧냢洧노 denotes the advantage estimate. This formulation explicitly optimizes the divergence between the current and the behavioral policys token distributions. While effective for maximizing immediate rewards, this token-level optimization often precipitates diversity collapse. In such cases, the model overfits to specific high-reward surface forms, thereby weakening its generalization across diverse reasoning patterns and leading to reward hacking of the linguistic structure rather than the underlying logic. To mitigate this, we propose shifting the optimization target from the external output distribution to the internal attention distributions. By supervising how the model allocates its computational focus over contextual information, we provide robust form of structural regularization. We frame the models aggregate attention over an input prompt as an information-gathering policy. 3.1. Aggregated causal Attention Distribution Policy We posit that the models internal attention mechanism constitutes an alternative, latent policy space. By reinforcing these internal distributions, we guide the models reasoning processspecifically, how it integrates both the original context and its own generated rationalewithout strictly constraining the output tokens to narrow, brittle distribution. Reinforced Attention Learning Let the total sequence be 洧녡 = (洧논1, . . . , 洧논洧녢 ), where 洧논1, . . . , 洧논洧녞 represents the prompt and 洧논洧녞+1, . . . , 洧논洧녢 represents the generated response. Let 洧띺洧노,洧녰 represent the attention weight from the generated token at position 洧노 to any preceding token at position 洧녰 (洧녰 < 洧노), extracted from the final layer and averaged across all heads. For each generated token 洧노 [洧녞 + 1, 洧녢], we define the causal Attention Distribution Policy 洧녷洧노 洧랚 as the distribution over all preceding positions: 洧녷洧노 洧랚(洧녰) = 洧띺洧노,洧녰 (cid:205)洧노1 洧녱=1 洧띺洧노, 洧녱 , 洧녰 [1, 洧노 1] (2) This formulation captures how the model attends to its own emerging reasoning in addition to the initial instructions and visual input. 3.2. Advantage-Weighted Attention Divergence To ensure the model retains an effective information-gathering policy, we derive per-token loss function. Drawing inspiration from the importance sampling ratios in PPO/GRPO, this objective encourages attention patterns that correlate with high rewards. The objective is defined as the expected divergence of the current attention policy and the old attention policy: 洧쮸ttnRL = 洧댶洧노 (cid:2)洧냢洧노 洧냥( 洧녷洧노 洧랚 洧녷洧노 old)(cid:3) (3) where 洧냢洧노 is the sequence-level advantage and 洧냥() is symmetric, bounded divergence measure, such as Jensen-Shannon Divergence (JSD). Using JSD ensures training stability and behaves according to the sign of the advantage: If 洧냢洧노 > 0, minimizing 洧쮸ttnRL pulls the current policy 洧녷洧랚 toward the successful strategy 洧녷old. If 洧냢洧노 < 0, the objective pushes 洧녷洧랚 away from the suboptimal strategy. This per-token granularity prevents the vanishing gradient effect often encountered when averaging attention over long sequences, ensuring that even late-stage tokens in long response are supervised by the internal attention objective. 3.3. Combined Optimization Objective The final training objective integrates the standard token-level policy gradient with our internal attention regularizer: Ltotal = LRL + 洧랝attn洧쮸ttnRL where 洧랝attn is hyperparameter balancing explicit output maximization with internal attention-level exploration. This dual-objective approach ensures the model remains linguistically flexible while maintaining structured and reward-aligned reasoning process. (4) 3.4. Gradient Derivation The gradient of LAttnRL with respect to the attention logits 洧뉧롐,洧녰 is derived via the chain rule. Let 洧냫洧노 = JSD( 洧녷洧노 洧랚 洧녷洧노 old). Gradient w.r.t. Distribution. The partial derivative of JSD with respect to the current distribution 洧녷洧노 洧랚 is: 洧녷洧노 洧랚 洧냫洧노 = (cid:32) 1 2 ln (cid:33) 2洧녷洧노 洧랚 洧랚 + 洧녷洧노 洧녷洧노 old Gradient w.r.t. Logits. Using the softmax Jacobian 洧녷 洧녱 洧뉧롐 洧뉧롐,洧녰 is: = 洧녷洧녰 (洧洧녰 洧녱 洧녷 洧녱), the gradient for specific logit (cid:32) 洧뉧롐,洧녰 洧냫洧노 = 洧녷洧노 洧랚(洧녰) 洧녷洧노 洧랚 (洧녰) 洧냫洧노 (cid:33) 洧녱 洧녷洧노 洧랚( 洧녱)洧녷洧노 洧랚 ( 洧녱) 洧냫洧노 4 Reinforced Attention Learning Total Parameter Update. The final gradient for the attention loss is the advantage-weighted accumulation across the sequence: (cid:34) 洧랚LAttnRL = 洧댶洧랦 洧냢洧랦 洧녢 洧노1 (cid:35) (cid:0)洧뉧롐,洧녰 洧냫洧노(cid:1) 洧랚洧뉧롐,洧녰 洧노=洧녞+1 When 洧냢洧랦 > 0, the update minimizes divergence from successful patterns. When 洧냢洧랦 < 0, it pushes the model to explore alternative attention fragments, penalizing the specific reasoning path that led to low reward. 洧녰=1 3.5. On-Policy Attention Distillation Beyond reward-driven optimization, the attention policy framework can be extended to an on-policy distillation setting where the student model 洧랢洧랚 aims to inherit the attention distribution, as the structural reasoning patterns, from teacher model 洧랢洧랯. In this regime, we provide dense supervision by minimizing the divergence between the students and teachers internal attention distributions over the students own generated trajectories. Teacher-Student Alignment. For each token 洧노 in trajectory 洧랦 sampled from the student policy 洧랢洧랚, we define the distillation loss as the sum of divergences between the student attention policy 洧녷洧노 and 洧랚 the teacher attention policy 洧녷洧노 洧랯 : LAttnDistill = 洧댶洧랦 洧랢洧랚 (cid:35) JSD( 洧녷洧노 洧랚 洧녷洧노 洧랯) (cid:34) 洧녢 洧노=洧녞+1 (5) Crucially, this objective does not involve an advantage term 洧냢洧노. The goal is pure structural imitation, ensuring that for any generated token, the student utilizes the same contextual evidence as the teacher. This provides denser gradient signal than token-level KL divergence alone. Unified Distillation Objective. The final objective for on-policy distillation combines the RL objective, the generalized knowledge distillation (GKD) on output logits, and our proposed attention alignment: Ltotal = LRL + 洧랞LGKD + 洧쬬ttnLAttnDistill (6) where LGKD typically represents the reverse Kullback-Leibler divergence between the output distributions 洧랢洧랚(洧녩洧노 洧멇롐) and 洧랢洧랯(洧녩洧노 洧멇롐). 洧랞 controls the strength of the distillation and 洧쬬ttn balances the strength of the attention distillation. Gradient Flow. The gradient of LAttnDistill with respect to student parameters 洧랚 is derived similarly to 3.4, but is purely aligning toward the teachers distribution: 洧랚LAttnDistill = 洧댶洧랦洧랢洧랚 (cid:34) 洧녢 洧노1 洧노=洧녞+1 洧녰= (cid:35) (cid:0)洧뉧롐,洧녰 洧냫洧노(cid:1) 洧랚洧뉧롐,洧녰 (7) where 洧냫洧노 = JSD( 洧녷洧노 洧랯). By sampling trajectories from the student (洧랦 洧랢洧랚), the model learns to maintain teacher-like attention patterns even when navigating states it would not encounter under the teachers original distribution, thereby mitigating exposure bias in internal representations. 洧랚 洧녷洧노 4. Experiments In this section, we empirically evaluate RAL for post-training MLLMs. Our experiments investigate both the standard RAL objective and the on-policy attention distillation variant, implemented on top of the Group Relative Policy Optimization (GRPO) framework. 5 Reinforced Attention Learning Figure 2 Sample data of the SFT and RL training stages. The SFT stage adapts the model to think-and-answer paradigm, while the RL stage employs reward function to verify the format and correctness of the rollout responses. 4.1. Experimental Setup Model Configurations We adopt Qwen-2.5-VL-7B as our foundation MLLM. For the on-policy attention distillation experiments, we utilize Qwen-2.5-VL-32B as the teacher model. In all experimental settings, the visual encoder and multimodal projector are kept frozen, with gradients updated only for the language model backbone. Training Pipeline Our training pipeline utilizes the Video-R1 [7] dataset and consists of two primary stages, implemented using the veRL infrastructure [26]: Supervised Fine-Tuning (SFT): To align the model with our target reasoning schema, we perform SFT using the Video-R1-COT-165k subset. This dataset provides 165k instances of video-question pairs paired with structured Chain-of-Thought reasoning. Following the format in Figure 2, the thinking process and final answer are enclosed in <think> and <answer> tags, respectively. The computational overhead for this stage is roughly 10 hours on cluster of 8 NVIDIA H100s. Reinforcement Learning (RL): We subsample the first 51.2k instances from the Video-R1260k dataset and perform training for single epoch. For each input, the policy generates 洧냨 = 8 rollouts to facilitate advantage estimation. This RL phase requires approximately 120 hours of compute on cluster of 8 NVIDIA H100 GPU cluster. 4.2. Implementation Details Visual Processing For the visual encoder, we set the maximum resolution of images to 51202828 pixels and the minimum to 128 28 28 pixels. Video frames are sampled at 1 frame per second (fps), with the total number of frames capped at 128. Each individual frame is constrained to maximum of 128 28 28 pixels. Hyperparameters. We extract the attention weights of the last Transformer layer of the LLM by patching its attention implementation with eager attention. The attention weights are averaged across all attention heads. For RL training, we maintain fixed sampling temperature of 洧랦 = 0.9 and KL coefficient of 洧띻 = 0.04. The learning rate is set to 5 106 for SFT and 1 105 for RL. Detailed hyperparameter ranges for RAL are provided in Table 1. 6 Reinforced Attention Learning Parameter Value Rollout batch size KL Coefficient (洧띻) Sampling Temperature (洧랦) 1 洧랝 洧녩洧노洧노洧녵 洧쮫롐뀛롐뫯롐뫯롐 8 0.04 {0.5, 1, 5} {0.05, 0.5, 1} Table 1 Hyperparameters for RAL experiments. Baselines. We primarily compare our method against the Group Relative Policy Optimization (GRPO) algorithm, which eliminates the need for separate value model by computing the relative advantage of each response within group of 洧냨 sampled outputs for the same query. For given response 洧녰, the advantage 틙洧냢洧녰 is calculated as: 틙洧냢洧녰 = (cid:205)洧냨 洧洧녰 1 洧녱=1 洧 洧녱 洧냨 std(洧1, . . . , 洧洧냨) (8) where 洧洧녰 represents the total reward for response 洧녰. Our reward system is entirely rule-based, comprising two components: Accuracy Reward (洧洧녩洧녫洧녫): We extract the content within the <answer>...</answer> tags and compare it against the ground truth. The model receives reward of 1.0 for an exact match and 0.0 otherwise. Format Reward (洧 洧녭 洧녴洧노): regular expression-based verifier checks if the response strictly adheres to the template: <think>...</think><answer>...</answer>. reward of 1.0 is granted for perfect formatting and 0.0 otherwise. The final reward 洧洧녰 is weighted combination: 洧洧녰 = 0.9 洧洧녩洧녫洧녫 + 0.1 洧 洧녭 洧녴洧노. These settings remain consistent across all experiments, including RAL, on-policy attention distillation, the baseline GRPO and the GRPO-based on-policy knowledge distillation. We also include Video-R1-7B for comparison, which was introduced alongside the dataset. This model is trained using Temporal-GRPO, proposed variant of the standard GRPO framework that incorporates temporal coherence reward. Specifically, this method perturbs the chronological ordering of video frames and provides an auxiliary reward when the model demonstrates higher accuracy on correctly sequenced inputs compared to shuffled ones. Since Video-R1-7B utilizes the same Qwen-2.5-VL-7B backbone as our other configurations, it facilitates strictly controlled and fair comparison across our evaluation suite. 4.3. Evaluation Benchmarks We evaluate RAL across broad spectrum of vision-centric benchmarks to assess spatial, temporal, and reasoning-intensive capabilities. Image QA Tasks. For static image understanding, we utilize:(1) 洧녤 Bench for fine-grained visual search and spatial reasoning;(2) MMMU Pro [37] for multi-step expert-level knowledge;(3) MME [8] for basic perception and high-level cognition;(4) MuirBench [30] for robustness across diverse image types;(5) ChartQA [21] for complex data extraction;(6) VizWiz [12] for real-world visual grounding;(7) Blink [10] for foundational perception tasks; and (8) CVBench [29] for benchmarking core computer vision capabilities. Video QA Tasks. To evaluate temporal reasoning and long-context integration, we adopt:(1) LongVideoBench [34] for long-range referring reasoning;(2) NExT-QA [36] for causal and temporal action explanation;(3) Video-MME [9] for comprehensive multi-domain evaluation;(4) Video-MMMU [14] for expert-level knowledge acquisition;(5) LVBench [32] for extreme long-form comprehension;(6) MVBench [17] for multi-task temporal perception; and(7) TempCompass [20] for sensitivity to motion and temporal order. 4.4. Main Results Table 3 and Table 2 present comprehensive comparison of RAL against Group Relative Policy Optimization (GRPO) and standard on-policy distillation across diverse suite of image and video 7 Reinforced Attention Learning Method LongVideoBench NExTQA VideoMME VideoMMMU LVBench MVBench TempCompass Qwen-2.5-VL-7B Video-R1 GRPO RAL RAL-zero Kowledge Distillation + Attention Distillation 57. 56.5 57.9 60.1 58.8 59.7 59.7 73.7 61.6 Reinforcement Learning 62.5 62.0 63.4 65.1 On-Policy Distillation 61.3 63.9 65.3 70.7 74.1 76.2 70.9 75. 47.6 44.7 49.7 48.6 49.2 47.3 48.5 40.5 63.4 43.5 43.9 44.2 45. 43.7 44.7 62.0 64.0 65.5 62.7 65.5 65.5 69.5 68.0 68.3 70.0 68.5 69.2 70. Table 2 Performance comparison across long-video benchmarks. We evaluate RAL and its variants against baselines on tasks requiring extended visual context. Results for RAL and Attention Distillation are highlighted in blue. RAL-zero (discussed in Section 4.5) denotes the variant where explicit thinking process are removed, isolating the impact of the attention-based policy gradient. These results demonstrate that optimizing internal attention distributions provides robust, complementary signal to token-based reinforcement learning, consistently enhancing multimodal understanding. Method Vstar MMMUpro MME MuirBench ChartQA VizWiz Blink CVBench Qwen-2.5-VL-7B 70.7 36. Video-R1 GRPO RAL RAL-zero On-policy Distillation + Attention Distillation 66.5 68.6 73.3 72.3 68.1 72.3 36.0 36.8 37.8 38.4 37.8 38. 2309.3 2266.2 2258.7 2352.8 2306.2 44.9 Reinforcement Learning 40.9 43.9 47.4 43.4 On-Policy Distillation 39.9 43.4 2344.7 2345.1 84.0 71. 56.3 77.6 84.2 81.7 86.4 82.7 81.7 83.6 63.5 67.9 71.7 71.9 71.2 72. 47.4 54.9 57.2 55.4 56.3 57.2 72.3 78.1 79.0 78.4 78.7 78.4 Table 3 Performance comparison across image VQA benchmarks. VQA benchmarks. Overall, RAL consistently surpasses both the GRPO baseline and the base Qwen-2.5VL-7B model. These results suggest that optimizing internal attention distributions provides more stable and effective learning signal for multimodal reasoning than token-level policy gradients alone. Image VQA. Under the reinforcement learning paradigm, RAL outperforms GRPO across all eight image benchmarks. We observe significant improvements on 洧녤 (+5.8), MME (+94.1), ChartQA (+2.8), and VizWiz (+3.8). These gains indicate that attention-level supervision strengthens visual grounding and compositional reasoning, particularly for perception-intensive and document-based tasks. Crucially, RAL not only mitigates the performance degradation often introduced by GRPO relative to the base model (e.g., on 洧녤 and VizWiz) but also exceeds the original Qwen-2.5-VL-7B performance across all metrics. This suggests that our approach fosters genuine generalization rather than narrow reward-model overfitting. In the on-policy distillation regime, the integration of attention distillation yields superior results over standard distillation on 7 out of 8 benchmarks, with marked increases on 洧녤 (+3.6) and MuirBench (+1.8). These findings demonstrate that supervising where teacher model attends provides crucial, complementary signal that simple output imitation lacks. Notably, the strong performance on 洧녤 , which specifically probes fine-grained object attributes and spatial relationships, underscores RALs efficacy in resolving complex scene geometries. Video VQA. On long-video benchmarks  (Table 2)  , RAL outperforms GRPO on 6 out of 7 datasets. The most pronounced improvements occur on LongVideoBench (+2.2), NExTQA (+3.4), and MVBench (+1.5), all of which demand robust temporal understanding and multi-hop reasoning. While GRPO maintains marginal lead on VideoMMMU, RAL remains highly competitive, suggesting that attentionlevel optimization does not compromise domain-specific factual accuracy. 8 Reinforced Attention Learning Figure 3 RAL improves GRPO along the increasing video frames or image resolution. Similarly, under on-policy distillation, attention-enhanced alignment improves performance on every benchmark except for ties on LongVideoBench and MVBench. The substantial gains on NExTQA (+4.4) and VideoMME (+2.6) suggest that attention alignment is particularly potent for long-context temporal reasoning and precise event localization. Implications. Across both modalities and training regimes, attention-centric learning provides consistent and well-distributed performance increments. In contrast to GRPO, which can exhibit benchmark-specific trade-offs or degrade base model capabilities, RAL delivers uniform gains. This supports our hypothesis that supervising internal information allocation offers more stable and generalizable training signal than pure token-level gradients. Furthermore, the success of attention distillation confirms that attention distributions serve as transferable and semantically rich representation of reasoning behavior. In summary, these results validate attention-based policy optimization as robust complement for, conventional RL and distillation in MLLM post-training, especially for perception-heavy and long-horizon video understanding. 4.5. Ablation Studies RAL yields consistent improvements across varying visual resolutions and frame rates. To investigate the robustness of these gains relative to visual information density, we analyze the performance of RAL versus GRPO across different video sampling lengths and image resolutions. We utilize LongVideoBench and 洧녤 as our evaluation suite: the former requires extracting sparse, salient information from extremely long video sequences, while the latter features high-resolution images of complex scenes that demand fine-grained reasoning. Together, these benchmarks facilitate diagnostic analysis of cross-modal reasoning capabilities across diverse temporal and spatial densities. We evaluate performance on LongVideoBench using maximum frame counts of 32, 64, and 128. For 洧녤 , we vary the maximum token budget per image (512, 1024, and 2048 tokens). Here, unit image patch is defined as 28 28 square; thus, 1024 tokens correspond approximately to an 896 896 image. As illustrated in Figure 3, RAL consistently outperforms GRPO across all temporal scales, demonstrating the efficacy of attention-based policies in locating salient cues within dense temporal contexts. On 洧녤 , the performance margin widens as image resolution increases, rising from +1.6 gain at 512 tokens to significant +6.3 gain at 2048 tokens. This trend indicates that the advantages of RAL become increasingly pronounced as visual information becomes more granular, suggesting superior scalability for high-fidelity multimodal understanding. Is an Explicit Thinking Process Necessary for VQA? To isolate the impact of attention-level supervision, we investigate whether RAL improves cross-modal reasoning even in the absence of an explicit thinking process. We introduce RAL-zero, variant where the thinking process is completely removed from both the SFT and RL stages. In this configuration, the model is trained to generate 9 Reinforced Attention Learning the final answer directly. By excluding high-reward text tokens (rationales), the training signal is dominated by the attention policy, allowing us to evaluate its intrinsic contribution to visual understanding. We maintain all hyper-parameters and data volumes from the main experiments but modify the data format and reward functions. During SFT, the <think> blocks (see Figure 2) are stripped, forcing the model to produce only the final response. Correspondingly, the RL stage employs format reward that penalizes any output different from the <answer>...</answer> structure. As shown in Table 2, RAL-zero surpasses the base model on 5 out of 7 long-video benchmarks and outperforms the full GRPO baseline on 5 benchmarks. Notably, RAL-zero achieves state-of-the-art performance on NExTQA (temporal reasoning), VideoMME (comprehensive video understanding), and LVBench (long-video event understanding). These results demonstrate that the attention policy space is significantly underexplored; RAL effectively unlocks visual understanding capabilities by optimizing the distribution of internal attention weights. On image-based benchmarks  (Table 3)  , RAL-zero improves upon the base model in 4 out of 8 cases, achieving the highest scores among RL-based methods on MMMU-Pro and VizWiz. Given that MMMU-Pro tests complex visual knowledge and VizWiz focuses on fine-grained object recognition, these gains suggest that optimizing attention distributions serves as powerful bridge between modalities. This confirms that policy gradient methods applied directly to attention mechanisms can induce superior cross-modal reasoning, even without explicit verbalized logic. 5. Conclusion We introduced Reinforced Attention Learning, MLLM post-training paradigm that shifts optimization from text token distribution to internal attention distributions. By treating attention as policy, RAL directly reinforces visual grounding and perceptual focus, addressing fundamental limitation of outcome-based RL methods that neglect the underlying cross-modal reasoning process. Our experiments across diverse image and long-video benchmarks demonstrate that RAL consistently outperforms the base Qwen-2.5-VL-7B and GRPO baselines. Notably, our approach provides more stable and uniform gains than token-level RL, which can occasionally degrade base model performance. These results validate our hypothesis that supervising internal information allocation yields more reliable and generalizable training signal than next-token gradients alone. Furthermore, we showed that this attention-centric perspective extends naturally to on-policy distillation, where transferring where to focus provides complementary and semantically rich signal that surpasses simple output imitation. Ultimately, this work establishes attention distributions as first-class optimization target for multimodal alignment. By reinforcing internal computation pathways, RAL offers principled, processaware alternative to standard RLHF. We believe this perspective paves the way for future research into fine-grained credit assignment and the optimization of other internal structures, such as MoE routing or cross-modal fusion, to foster more robust and grounded multimodal intelligence. 10 Reinforced Attention Learning References [1] R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R. Garea, M. Geist, and O. Bachem. Onpolicy distillation of language models: Learning from self-generated mistakes. In The twelfth international conference on learning representations, 2024. [2] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report, 2025. URL https: //arxiv.org/abs/2502.13923. [3] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt, M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume, S. R. Bowman, Z. Hatfield-Dodds, B. Mann, D. Amodei, N. Joseph, S. McCandlish, T. Brown, and J. Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022. URL https://arxiv.org/abs/2212.08073. [4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. [5] R. Cai, B. Li, X. Wen, M. Chen, and Z. Zhao. Diagnosing and mitigating modality interference in multimodal large language models. arXiv preprint arXiv:2505.19616, 2025. [6] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [7] K. Feng, K. Gong, B. Li, Z. Guo, Y. Wang, T. Peng, B. Wang, and X. Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [8] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. In NeurIPS Datasets and Benchmarks Track, 2025. [9] C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In CVPR, 2025. [10] X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024. [11] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. [13] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. 11 Reinforced Attention Learning [14] K. Hu, P. Wu, F. Pu, W. Xiao, Y. Zhang, X. Yue, B. Li, and Z. Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. 2025. URL https://arxiv. org/abs/2501.13826. [15] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. URL https://arxiv.org/ abs/2503.06749. [16] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling BERT for natural language understanding. In T. Cohn, Y. He, and Y. Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 41634174, Online, Nov. 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.372. URL https://aclanthology.org/2020.findings-emnlp.372/. [17] K. Li, Y. Wang, Y. He, Y. Li, Y. Wang, Y. Liu, Z. Wang, J. Xu, G. Chen, P. Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2219522206, 2024. [18] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [19] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In NeurIPS, 2023. [20] Y. Liu, S. Li, Y. Liu, Y. Wang, S. Ren, L. Li, S. Chen, X. Sun, and L. Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv: 2403.00476, 2024. [21] A. Masry, D. Long, J. Q. Tan, S. Joty, and E. Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.f indings-acl.177. URL https: //aclanthology.org/2022.findings-acl.177. [22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [23] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep nets, 2015. URL https://arxiv.org/abs/1412.6550. [24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [25] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [26] G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. [27] S. Sun, Y. Cheng, Z. Gan, and J. Liu. Patient knowledge distillation for bert model compression, 2019. URL https://arxiv.org/abs/1908.09355. [28] G. Team. Gemini: family of highly capable multimodal models, 2025. URL https://arxiv. org/abs/2312.11805. [29] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, A. Wang, R. Fergus, Y. LeCun, and S. Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024. 12 Reinforced Attention Learning [30] F. Wang, X. Fu, J. Y. Huang, Z. Li, Q. Liu, X. Liu, M. D. Ma, N. Xu, W. Zhou, K. Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. [31] W. Wang, F. Wei, L. Dong, H. Bao, N. Yang, and M. Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:57765788, 2020. [32] W. Wang, Z. He, W. Hong, Y. Cheng, X. Zhang, J. Qi, S. Huang, B. Xu, Y. Dong, M. Ding, and J. Tang. Lvbench: An extreme long video understanding benchmark, 2024. [33] Z. Wang, X. Guo, S. Stoica, H. Xu, H. Wang, H. Ha, X. Chen, Y. Chen, M. Yan, F. Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025. [34] H. Wu, D. Li, B. Chen, and J. Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. URL https://arxiv.org/abs/2407.15754. [35] J. Xia, Y. Zang, P. Gao, S. Li, and K. Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning. arXiv preprint arXiv:2505.14677, 2025. [36] J. Xiao, X. Shang, A. Yao, and T.-S. Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 97779786, June 2021. [37] X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, Y. Su, W. Chen, and G. Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. [38] S. Zagoruyko and N. Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928, 2016. [39] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [40] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, H. Tian, Y. Duan, W. Su, J. Shao, Z. Gao, E. Cui, X. Wang, Y. Cao, Y. Liu, X. Wei, H. Zhang, H. Wang, W. Xu, H. Li, J. Wang, N. Deng, S. Li, Y. He, T. Jiang, J. Luo, Y. Wang, C. He, B. Shi, X. Zhang, W. Shao, J. He, Y. Xiong, W. Qu, P. Sun, P. Jiao, H. Lv, L. Wu, K. Zhang, H. Deng, J. Ge, K. Chen, L. Wang, M. Dou, L. Lu, X. Zhu, T. Lu, D. Lin, Y. Qiao, J. Dai, and W. Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479."
        }
    ],
    "affiliations": [
        "Google",
        "Google DeepMind",
        "Princeton University",
        "UC Davis"
    ]
}