{
    "paper_title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis",
    "authors": [
        "Shubhashis Roy Dipta",
        "Tz-Ying Wu",
        "Subarna Tripathi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video captions offer concise snapshots of actors, objects, and actions within a video, serving as valuable assets for applications such as question answering and event localization. However, acquiring human annotations for video captions is costly or even impractical, especially when dealing with diverse video domains. Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild. To address these limitations, we propose a reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, a novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train a multimodal model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos. Overall, VC-Inspector offers a scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains."
        },
        {
            "title": "Start",
            "content": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis Shubhashis Roy Dipta 1* Tz-Ying Wu 2 1University of Maryland, Baltimore County Subarna Tripathi 2 2Intel Labs 5 2 0 S 0 2 ] . [ 1 8 3 5 6 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Video captions offer concise snapshots of actors, objects, and actions within video, serving as valuable assets for applications such as question answering and event localization. However, acquiring human annotations for video captions is costly or even impractical, especially when dealing with diverse video domains. Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild. To address these limitations, we propose reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train multimodal model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos. Overall, VC-Inspector offers scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains. 1. Introduction Video captioning plays pivotal role in bridging visual content with natural language, offering concise descriptions of salient entities, actions, and interactions within video. These captions are instrumental in enabling wide range of downstream applications, including video-based question answering [27], event localization [12, 28], and con- *These authors contributed equally to this work. Figure 1. Existing reference-free metrics like EMScore [31] often fail to detect factual inaccuracies and lack consistent scoring scale. VC-Inspector addresses these limitations by providing factually grounded, interpretable evaluations with explanations. tent retrieval [8]. To ensure the quality and reliability of captions, robust evaluation metrics are vital for advancing the field of video-language research. However, current captioning models, predominantly trained on supervised datasets [14, 41], rely heavily on reference-based evaluation protocols [3, 19, 26, 33, 39] that compare generated captions against human-written ground truth. While effective in controlled settings, these metrics face significant limitations in real-world scenarios. First, generating such references is prohibitively labor-intensive and costly, often becoming unavailable or infeasible to obtain when scaling across diverse video domains with varying visual and contextual characteristics. Second, these metrics demonstrate limited ability to understand the semantic meanings of the captions and often fail to recognize valid alternatives. The challenges of assessing open-domain or in-the-wild videos necessitate the development of evaluation protocols 1 that do not rely on reference captions. However, this remains underexplored within the video research community. Current reference-free metrics [30, 31] typically measure the visual-language semantic alignment using pretrained multimodal embeddings [29], with the comprehension of caption content restricted by the context length of the underlying text encoder. In addition, these metrics frequently overlook factual inaccuracies in the candidate captions, as shown in Figure 1. Furthermore, they are primarily imagebased solutions, making them suboptimal for video content. To address these limitations, we propose novel reference-free evaluation framework, VC-Inspector, that eliminates the dependency on human-annotated captions. Our approach emphasizes factual grounding, ensuring that the evaluation of caption quality is based on the alignment between the caption and the actual video content, rather than comparison to predefined reference. We build VC-Inspector atop lightweight large multimodal model (LMM), equipping the model with the ability to judge the quality of video captions based on factual accuracy, through instruction tuning. However, while supervised video caption datasets are already scarce, captions of imperfect quality are even harder to obtain. We tackle this challenge by introducing novel data generation pipeline powered by large language model (LLM), creating pseudo captions of varying quality by altering the factual elements (i.e., objects and actions) in the ground truth captions from ActivityNet [14]. This assembles ActivityNet-FG-It, comprising 44K data points for instruction tuning. Figure 2 illustrates the data generation and training pipeline of VC-Inspector. Unlike previous metrics that offer only single quality score, our approach also requires the model to explain the rationale behind its scoring, making the evaluator more interpretable. We start with evaluating VC-Inspector on the ActivityNet-FG-Eval and YouCook2-FG-Eval datasets, where the labels are synthetically generated with the same data generation pipeline as the instruction tuning dataset. Results demonstrate that the quality estimates are consistent across datasets, and VC-Inspector is versatile for different visual content and caption lengths. We further evaluate the correlation between the quality scores given by VC-Inspector and human judgments using the VATEX-Eval dataset [31], standard benchmark for video captioning metric evaluation. The proposed evaluator stands out compared to the previous reference-free metrics, even surpassing most of the reference-based metrics. Finally, we extend the evaluation to image caption datasets, Flickr8K-Expert and Flickr8K-CF [10], showing the generalization of the proposed metric. Overall, this work makes the following contributions: We present one training dataset and two evaluation datasets consisting of ground truth captions, synthetic candidate captions, and pseudo quality scores, generated using novel data generation pipeline. We propose VC-Inspector, an open-source referencefree evaluator featuring factual grounding, with text context length of 32K tokens and support for long videos. Our evaluator outperforms existing reference-free metrics for video captions, achieving higher correlations with human evaluations and generalizing across visual domains. 2. Related Works Traditional text-based evaluation can be extended to video captioning by discarding the video and treating the reference caption as ground truth. Alternatively, image-caption metrics can be adapted by sampling images from the video and aggregating image-caption scores. We review these metrics and their limitations below: Text-only metrics based on references. Several rulebased metrics have been developed to evaluate generated texts, including widely used methods such as BLEU [26], METEOR [3], ROUGE [19], and CIDEr [33]. While these metrics perform reasonably well in controlled settings, they primarily focus on syntactic structure and fail to capture the semantic meaning of the generated text. To address this limitation, the SPICE series [1, 18] attempts to embed semantic understanding by parsing both reference and candidate captions into objects, attributes, and their relationships. However, these metrics still struggle to account for semantically similar or identical words expressed differently. This persistent challenge has motivated the development of embedding-based evaluation methods, such as BERTScore [40] and its extended version [37], which consider the intrinsic variance between multiple ground truth captions. More recently, CLAIR [5] explores an LLMas-a-Judge approach for image caption evaluation, demonstrating stronger correlation with human judgments than the above metrics. However, these metrics all rely exclusively on reference captions for evaluation and do not incorporate the visual input that the captions are meant to describe. Image-augmented metrics. To address the limitation of text-only metrics, various approaches have been explored that cross-reference between visual input and captions. VIFIDEL [24] computes semantic similarity by matching object names extracted from the image with those mentioned in the candidate caption. However, objects are discrete representations limited to fixed categories and cannot capture the motion dynamics that are particularly informative for videos. Visual-language models (VLMs), by contrast, provide continuous representations that offer richer semantic alignment between visual and textual modalities. ViLBERTScore [16] extends BERTScore by leveraging the pretrained ViLBERT [23] models, but still requires compariGround Truth Candidate Caption Rouge-L SPICE Soft-SPICE The man is feeding cat on the sofa in the living room The man is feeding lion on the sofa in the living room The girl is dancing in the room The girl is sleeping in the room 92.31 75.00 78.57 85.71 66. 88.19 Table 1. Text-based metrics rely on ground truth for evaluation and often fail to detect factual errors in objects/actions (bolded). However, it is unclear whether this image-based prompting generalizes to longer or more dynamic videos. Moreover, the dependence on proprietary models such as GPT-4o limits its scalability and practicality for widespread use. In contrast, VC-Inspector finetunes lightweight, opensource LMM to produce both evaluation scores and reasoning, offering more interpretable and scalable solution. 3. Video Caption Quality Estimation In this section, we provide concise overview of existing metrics used for evaluating video captions, identify their shortcomings, and outline our objectives. 3.1. Overview Video caption quality estimation is task to quantitatively measure the correctness of caption ˆX = M(V ) given video , where is video captioning model. Prior metrics typically rely on the ground truth caption as reference to compare with the caption to evaluate ˆX, referred to as reference-based metrics. This can be based on n-gram matching [3, 19, 26] or embedding-based solutions [40]. While this text-to-text comparison is straightforward, assuming that the caption annotation is always available during the evaluation phase is unrealistic. In addition, languages can be expressed in various ways, allowing multiple descriptions for the same video. Treating the ground truth caption as the only answer may restrict the recognition of valid alternatives. To address this, recent work proposes to leverage pretrained visual-language embeddings [29] to assess the semantic alignment between the video and candidate caption ˆX. They are reference-free as humanannotated captions are not required during testing, while offering reasonable relative measure of caption quality. These metrics, however, do not provide values on an absolute scale (e.g., 0 to 1) that humans can easily interpret. In addition, the text encoders inherent context length limitation typically hinders comprehensive understanding of the input captions, which tend to be longer (tokens exceeding the length constraint are truncated). 3.2. Toward Factually Grounded Reference-free"
        },
        {
            "title": "Evaluation",
            "content": "In this paper, we aim to develop factually grounded and reference-free video caption quality evaluator that can genFigure 2. (left) We present data generation pipeline designed to systematically create synthetic video captions with diverse quality (right) scores, along with explanations for the assigned scores. This dataset was subsequently used for instruction tuning the VC-Inspector. son to reference captions. UMIC [15] and CLIP-based metrics [7, 9, 13, 17, 21, 30, 31, 34, 38] relax this constraint by employing contrastive learning to directly model the semantic alignment between the images and captions. Although promising for short captions, these embedding-based methods are constrained by the context length limitations of their text encoders. Recently, the strong contextual reasoning and instruction following capability of LLMs have been leveraged for evaluation across diverse tasks [22]. Building on this trend, Maeda et al. [25] employ an LLM to compare the candidate caption to the visual context extracted with VLM. However, these methods only consider static images and do not model the temporal dynamics of video, resulting in suboptimal performance on video caption evaluation. In summary, text-based methods require costly, highquality reference captions, while image-based extensions are suboptimal for video content. These challenges highlight the growing demand for reference-free video caption evaluation tailored for video inputs. However, it remains an underexplored topic in the community, compared to textbased and image-caption evaluation. EMScore [31] was an early attempt that supports evaluating video captions without reference. Although it considers both frame-level and video-level embeddings, these embeddings are still obtained from an image-based encoder [29], and the text encoder is limited by short context length. PAC-S [30] and FactVC [21] augment EMScore with positive and negative data synthesis, respectively. While they share some similarity to this work, they consider only single level of corruption, with binary (positive/negative) differentiation, whereas our method incorporates captions with varying degrees of quality, enabling more nuanced evaluation. In addition, these metrics produce single scalar score without offering explanations of their judgments, posing challenges for interpreting the quality evaluation. More recently, G-VEval [32] extended the G-Eval [22] approach to video by stitching together three frames from each video. 3 erate scores on an absolute scale, and is versatile for various domains and text lengths. We argue that an evaluation protocol is only reliable if it is factually grounded, meaning that the quality assessment must accurately reflect the correctness of object entities and actions in the caption with respect to the video. For example, the evaluator should assign lower score to candidate caption where an object is missing or an action is wrong in the video than correct one, and an even worse score for caption that has more errors. However, it remains unclear whether existing metrics can effectively capture these subtle changes in factual elements. To investigate this, we probe existing metrics with candidate captions that contain incorrect objects or/and actions. Table 1 and Figure 1 illustrate such cases for reference-based and reference-free metrics, respectively. These results reveal that current metrics are insensitive to these factual errors. Even when captions are semantically distinct (e.g., cat\" vs. lion\" and dancing\" vs. sleeping\"), high scores are assigned due to substantial syntactic overlap. these observations, we novel propose Motivated by VC-Inspector, factually grounded and reference-free model for inspecting the video caption quality. We leverage the recent advents of large multimodal models (LMMs) to handle long-context text reasoning and generalized video feature extraction, with the hypothesis that their established efficacy on visual-language joint reasoning can be applied to this task. While evaluation metrics are preferred to be lightweight, we employ the 3B/7B version of Qwen2.5-VL as model-based evaluator and equip the model with the factual grounding ability through instruction tuning, where the model receives video and candidate caption ˆX, and generates an integer score ranging from 1 to 5. We also request the model to provide an explanation for the assigned score in the output, similar to [32], which makes the evaluator more interpretable and can serve as supervision signal for factual grounding. The next section will cover how we systematically generate captions with incorrect factual elements, the corresponding quality estimates, and the explanations. 4. VC-Inspector: Instruction Tuning for Factually Grounded Evaluation In video captioning, two primary factors that can degrade caption quality are: (1) the inclusion of objects not present in the video, and (2) the introduction of actions not depicted in the video. Our preliminary study indicates that existing metrics fall short in capturing these factual elements in the caption. To address this, we propose to empower LMMs with this ability through instruction tuning. However, it is nontrivial to acquire annotated samples for training. Supervised video caption datasets are already scarce due to the high cost of labeling, and these datasets generally only include correct captions, lacking those of lower quality. To Figure 3. Data generation pipeline to create synthetic dataset for training VC-Inspector. While both talking and holding were identified as actions, only holding was sampled for replacement in the synthetic dataset. obtain considerable number of samples with diverse quality, we propose systematic way to create synthetic video caption dataset leveraging the internal knowledge of large language models (LLMs). 4.1. Data Generation To create captions with incorrect elements, we employed Llama-3.3-70B-Instruct to alter the objects and actions in ground truth caption from supervised video caption dataset. Specifically, we created the ActivityNet-FG-It dataset for instruction tuning utilizing the ActivityNet [14] training set. The data generation pipeline is summarized in Figure 3. Given ground truth caption X, we first prompted the LLM to extract the set of objects = {o1, ..., oM } and actions = {a1, ..., aN }, and randomly sampled nif (0, ) objects and nif (0, ) actions to be replaced, forming subset A, where = + and nif (a, b) denotes discrete uniform distribution over integers {a, ..., b}. Subsequently, for each object oi R, we instructed the LLM to generate an alternative object oi belonging to the same category but with distinct meaning. This approach encourages the model to discern subtle differences between objects, ensuring that the transformations were not trivial (e.g., replacing car with building). Similarly, for each action aj R, we acquired an alternative action aj with the LLM that the subject could perform, but with different meaning. For example, changing stand4 ing to jumping. The selected objects and actions in are then replaced with their corresponding alternatives with the LLM, resulting in the pseudo caption X. The list of incorrect objects and actions is assembled as additional information that can later be used in the explanation. After generating pseudo caption, it is critical to assign it reasonable quality score that is intuitive for humans so that it can better mirror humans expectations. To achieve this, one could utilize visual-language model (VLM) to estimate this score; however, VLMs are not robust enough to produce factually grounded score in specified range without calibration, and the scoring quality is confined by their capabilities. Instead, we employ deterministic scoring mechanism based on factual grounding, described as follows, model parameters in the LLM with low-rank adaptation [11] and the MLP-based vision-language merger. The model consumes video-caption pair (V, X), and generates quality score {1, ..., 5} with the corresponding explanation E, i.e., [S, E] = VC-Inspector(V, X), (2) where we format the explanation in text using the information collected in the data creation process, i.e., the list of changed objects and actions, detailed in the supplementary. This can serve as extra supervision for the model to implicitly learn factual grounding and provide interpretable reasoning for this model-based evaluator at test time. During training, the model is optimized with the language modeling loss, like in other instruction tuning work [20]. score = 1 = 1 # of changed objects & actions total # of objects & actions O + . 5. Experiments (1) 5.1. Experimental Settings Since and A = + A, the score will reside within the range of 0 and 1. It also guarantees that caption with more incorrect objects/actions receives lower score. We then convert the score to an integer in the 1-5 range for better interpretability during model training. We repeated the aforementioned process to create 10 pseudo captions per ground truth caption, resulting in total of 374K pseudo captions derived from 37,396 videocaption pairs. The prompts used to guide the generation process are detailed in the supplementary. The randomized replacement of objects and actions ensures coverage across the full range of the possible scores, as the number of replacements directly influences the semantic deviation from the original caption. However, it also naturally yields skewed and non-uniform score distribution. To mitigate the potential bias during training, we applied balanced sampling strategy, resulting in refined subset of approximately 218K pseudo captions with uniform representation across five score categories. However, due to computational constraints, training with the full 218K instances would require multiple weeks of runtime. Therefore, we further sampled 44K (8.8K for each label) caption subset from the balanced dataset for instruction tuning, which we refer to as ActivityNet-FG-It. This subset preserves category balance and maintains the diversity and structure of the original data, while offering tractable size for experimentation ( 32 GPU hours using A100). 4.2. Training We train VC-Inspector based on Qwen-2.5-VL [2] as the foundation model by finetuning the model with ActivityNet-FG-It. To preserve the generalized features, we freeze the video encoder, and only finetune the 5 Datasets. We train VC-Inspector with the proposed instruction tuning dataset, ActivityNet-FG-It, which comprises 44K video-caption pairs, along with their quality scores and explanations. To evaluate whether VC-Inspector can generalize to other follow the videos and visual domains, we further same data generation pipeline in section 4.1 to create two evaluation datasets, ActivityNet-FG-Eval and YouCook2-FG-Eval, according to the ActivityNet [14] test set and YouCook2 [41] validation set. However, these are not truly labeled by humans. To evaluate whether the quality scores given by VC-Inspector are aligned with human evaluators, we utilized the widely adopted VATEXEVAL dataset, specifically designed for evaluating video caption evaluation quality. It contains 6 captions with varying levels of quality per video, each rated by three human evaluators on scale of 1 to 5. Since some videos become unavailable on YouTube, we collect the remaining subset of 2,590 videos and the corresponding 15,540 candidate captions. Unless otherwise specified, all experiments were evaluated on the same dataset to ensure fair comparison. We further extend our evaluation to image caption datasets, Flickr8K-Expert and Flickr8K-CF [10], by viewing images as single-frame videos to test the generalization of the proposed metric. The former contains 17K image-caption pairs rated by three human experts on scale of 1 to 4, while the latter collects binary quality assessments for 48K imagecaption pairs from crowd sources. Baselines. The baselines are organized into three catei) Language-based metrics: The evaluation in gories: this category solely relies on the reference texts without considering the visual input. Representative metrics are BLEU [26], ROUGE [19], METEOR [3], CIDEr [33], BERTScore [39], SPICE [1], SPICE-Factual [18], Softii) Image-augmented metSPICE [18] and CLAIR [6]. rics: These approaches incorporate images as references (alongside reference captions), for better capturing the semantic alignment between the visual input and the candidate caption, e.g., CLIPScore [9], EMScore [31], PACS [30], FactVC [21], and G-VEVAL [32]. They usually support both reference-free and reference-based settings, where the former compares the candidate caption directly to the video without reference captions, while the latter uses both visual and textual references. We report results for both settings for completeness, but our primary focus is on the reference-free setting, which is more practical for realworld, in-the-wild videos. iii) Video-based metrics: These methods employ video encoder to incorporate full video sequences as references. To the best of our knowledge, no existing metric in the literature falls into this category. Therefore, we adapt CLIPScore [9] to the recent advent of ViCLIP [35] as stronger baseline, ViCLIPScore. Additionally, we compare VC-Inspector against the base model it builds upon, the vanilla Qwen2.5-VL model, to highlight the benefit of fine-tuning for evaluative reasoning. Metrics. In the caption evaluation literature, the effectiveness of evaluation metrics is typically assessed by measuring their correlation to human judgments. Following prior work [31, 32], we report both Kendalls correlation (τb) and Spearmans rank correlation (ρ). Implementation details. VC-Inspector is developed for two model sizes, 3B and 7B, initialized from their corresponding Qwen2.5-VL pretrained weights. In all experiments, we train the model on 4 NVIDIA-A100 GPUs with global batch size of 128 and learning rate of 1e-4. We set both alpha and rank to 32 for the low-rank adaptation with dropout rate of 0.05. During inference, we use temperature of 0.0 for reproducibility. 5.2. Generalization of Quality Estimation In this section, we evaluate the consistency of quality estimates using synthetic data, and validate the effectiveness of VC-Inspector by measuring its alignment with human judgments on the video caption evaluation dataset, VATEX-Eval. Given the limited availability of video caption datasets, we treat images as single-frame videos and extend our evaluation to image caption datasets, Flickr8KExpert and Flickr8K-CF, to assess the generalization. Evaluation with the synthetic data. Table 2 presents the correlation between metric scores and ground truth the ActivityNet-FG-Eval and annotations YouCook2-FG-Eval datasets, both of which contain pseudo captions with varying degrees of factual inacVC-Inspector, specifically finetuned for curacies. in ActivityNet-FG-Eval YouCook2-FG-Eval Metric EMScore [31] CLIPScore [9] Qwen2.5-VL-3B [2] VC-Inspector-3B τb 28.94 28.10 37.91 49.53 ρ 40.77 39.65 47.80 62.01 τb 20.21 18.00 37.16 44.29 ρ 29.24 26.14 47.17 55.31 2. Table synthetic ActivityNet-FG-Eval [4] and YouCook2-FG-Eval [14] datasets. The best score is bolded. Correlation scores the on factual grounding, consistently outperforms baselines in differentiating incorrect captions. Notably, although it is only trained on ActivityNet-FG-It, its quality estimation generalizes effectively across visual domains. This suggests that the proposed data generation pipeline produces stable and reliable quality scores, rather than noisy outputs that fail to converge. Evaluation on VATEX-Eval. Table 3 presents our primary results: human correlation scores on the VATEXEVAL [31] dataset. Baselines are grouped by reference type as outlined in section 5.1, and the three columnsections correspond to the No Reference, 1-Reference, and 9-Reference settings, respectively. Language-based metrics, which rely heavily on textual references, are not applicable to our target No Reference setting. Therefore, we focus our comparisons on multimodal methods that incorporate visual inputs. VC-Inspector consistently outperforms all evaluated metrics in the reference-free setting, particularly those based on CLIP embeddings, including CLIPScore, EMScore, FactVC, and PAC-S. When adapting CLIPScore to the recently introduced ViCLIP [35] (which adopts video encoder), we observe considerable gain over those image-CLIP-based approaches. Despite ViCLIPScore being stronger baseline, it still underperforms relatively to VC-Inspector and its underlying model, as the context length of the ViCLIP model (i.e., 32) is much shorter than the 32K context length of ours, which supports more flexible and comprehensive caption evaluation. We further compare VC-Inspector with G-VEval [32], recent LMM-based evaluation method. However, G-VEval is based on GPT-4o, proprietary, paid model with an estimated 200B parameters. In contrast, VC-Inspector is open-source, lightweight, and reproducible, with configurations available at 3B and 7B parameters depending on system requirements. While having compact size, our 7B model achieves the highest correlation to human evaluations and could potentially enable on-the-fly quality estimation during training, making it viable reward model in Reinforcement Learning (RL) applications. Although our primary focus is to compare with the reference-free metrics, it is noteworthy that VC-Inspector even surpasses the performance of most reference-based metrics. 6 No Reference 1-Reference 9-References Metric Data Synthesis Metric τb ρ τb ρ τb ρ EMScore [31] n/a - - - - - - - - - - - - - - - - - - - - Language-based BLEU_1 [26] BLEU_4 [26] ROUGE-L [19] METEOR [3] CIDEr [33] BERTScore [39] SPICE [1] SPICE-factual [18] Soft-SPICE [18] CLAIR [6] Multimodal - image-based CLIPScore [9] EMScore [31] FactVC [21] PAC-S [30] G-VEval [32] Multimodal - video-based ViCLIPScore [9, 35] 30.92 39.86 Qwen2.5-VL-3B [2] 31.29 36.43 Qwen2.5-VL-7B [2] 34.70 39.40 - - VC-Inspector-3B VC-Inspector-7B 37.99 42.45 42.58 45.99 12.65 16.52 28.70 36.88 12.44 36.28 22.76 29.56 12.94 16.89 23.94 31.06 16.68 21.80 27.64 35.76 17.62 23.02 27.92 36.18 15.24 19.82 25.05 32.37 14.80 18.78 27.41 35.40 13.59 17.04 26.05 35.58 21.25 27.61 36.31 46.41 36.00 34.80 - - 22.33 29.09 27.39 35.49 35.21 45.28 22.88 29.79 28.63 37.05 36.66 47.00 22.79 29.69 28.78 37.22 36.18 46.33 25.10 39.40 31.40 48.10 32.60 44.90 - - - - - - - - - - - - - - - - - - - - - - - - Table 3. Human correlation scores on the VATEX-EVAL [31] dataset. indicates results reported from [32] due to GPT-4 licensing issue. The best score for each column section is bolded. Please note that this work focuses on the No Reference setting. Our best model has outperformed all other models in this setting, while remaining competitive with metrics that rely on references."
        },
        {
            "title": "Metric",
            "content": "Flickr8K-Expert Flickr8K-CF Reference-based BLEU_1 [26] BLEU_4 [26] ROUGE [19] METEOR [3] CIDEr [33] SPICE [1] BERTScore [39] CLIPScore [9] PAC-S [30] Reference-free CLIPScore [9] PAC-S [30] VC-Inspector-3B VC-Inspector-7B 32.20 30.60 32.10 41.50 43.60 51.70 - 52.60 55.50 51.10 53.90 59.86 63.43 17.90 16.90 19.90 22.20 24.60 24.40 22.80 36.40 37.60 34.40 36. 39.00 45.97 Correlation score (τb) with human judgments on Table 4. Flickr8k-Expert and Flickr8k-CF [10] dataset. The overall best scores are bolded and best of each section is underlined. Our model has outperformed even the reference-based methods. Evaluation on Flickr8K. To expand our evaluation beyond the limited availability of video caption datasets, we treat images as an extreme case of short videos with sin7 τb 22. 36.40 33.23 37.99 ρ 29.79 41.20 39.63 42.45 VC-Inspector-3B Change objects only Change actions only Change both (Ours) Table 5. Ablation study on synthetic data generation strategies. Human correlation scores (τb, ρ) are reported on the VATEX-Eval. gle frame. Table 4 reports the human correlation score on two widely adopted benchmarks, Flickr8K-Expert and Flickr8K-CF. The former requires more fine-grained differentiation among the captions, with ratings distributed across 4 levels, whereas the latter employs binary judgments. VC-Inspector is instruction-tuned with captions that exhibit varying degrees of factual inaccuracies, enabling nuanced evaluation of caption quality. This allows the model to perform effectively across both benchmarks, despite their differing rating scheme. In these evaluations, VC-Inspector remains the best-performing method under the reference-free setting, substantially narrowing the gap to inter-human correlation (τb 73) [1], and even outperforming several reference-based metrics. These results demonstrate the strong generalization capability of VC-Inspector across visual domains and video lengths. 5.3. Ablation and Analysis We conduct ablation studies on VC-Inspector components on the VATEX-Eval benchmark: Data synthesis strategies. Since both objects and actions are informative factual elements for video, grounding the evaluator in factual understanding requires instructing the model to identify errors in these components. To this end, during the data generation process (outlined in section 4.1), we systematically altered both elements in ground truth captions to create pseudo captions for training. In this section, we ablate the impact of modifying these elements in the ActivityNet training set by evaluating three variants: i) Changing objects only, ii) Changing actions only, and iii) Changing both. As shown in Table 5, all variants achieve strong alignment with human ratings compared to EMScore. However, the variant that alters both objects and actions yields the best performance. These results highlight the importance of both factual elements, object and action, in capturing the context of video content for caption evaluation. They also demonstrate the robustness and generalization of the proposed paradigm across different factual errors. Role of explanations. During our data generation process, we systematically synthesize pseudo captions along with their corresponding quality estimates. This process yields an informative side product\": explanations that indicate where factual errors are located within the candidate VC-Inspector-3B Without Explanations With Explanations τb 34.29 37.99 ρ 38.18 42. Table 6. Impact of explanations on the model performance. In the Without Explanation\" setting, we have trained the model only on the score, removing the pseudo explanations. rior efficiency compared to existing methods. 5.4. Qualitative Results Figure 4 presents visual examples of VC-Inspector outputs on the ActivityNet-FG-Eval (top) and VATEXEval (middle, bottom) datasets. Without relying on reference captions, the model evaluates candidate captions based on the associated video content and produces quality scores that closely mirror human judgments. The explanation pinpoints the factual inaccuracies, such as incorrect objects and/or actions in the candidate captions, and assigns factually grounded scores accordingly. Additional qualitative results are provided in the supplementary. 6. Conclusion and Discussion This work addresses the challenge of evaluating video captions across diverse domains without relying on human-annotated reference captions. We identified the limitations of existing metrics, and proposed novel reference-free and factually grounded evaluation framework, VC-Inspector, based on LMMs. By delving into the factual analysis of captions, we empower the LMM with the factual grounding ability through instruction tuning, where we systematically create pseudo captions of diverse quality using an LLM. Experimental results across multiple domains demonstrate that VC-Inspector achieves high consistency with human evaluators, and outperforms existing metrics on detecting factual errors. Its versatility and interpretability make it practical tool for evaluating the factual accuracy of video captions in real-world settings. While our current focus is on factual errors, particularly object and action substitutions, the proposed paradigm can be potentially extended to simulate other types of errors (e.g., attribute replacements), as supported by our ablations on synthesizing only one type of error. Additionally, we recognize the importance of evaluating temporal coherence and narrative structure, which remain underexplored. Future work should expand this paradigm to simulate and detect broader range of errors, enabling more comprehensive and context-aware evaluation strategies for video captions."
        },
        {
            "title": "References",
            "content": "[1] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: Semantic Propositional Image Caption Evaluation, July 2016. arXiv:1607.08822 [cs]. 2, 6, 7 Figure 4. Visual examples from ActivityNet-FG-Eval (top) and VATEX-Eval (others). VC-Inspector produces quality assessments highly consistent with ground truth scores, along with explanatory insights into factual errors (highlighted in red). captions. We leverage these explanations as an auxiliary supervision signal during training, enabling the model to implicitly learn factual grounding, which has been shown to be effective compared to the variant without explanations in Table 6. Not only are explanations useful in training, but they also enhance the interpretability of the model-based metric, providing transparency of why the scores are assigned to given captions. Further evaluation of the explanation quality is provided in the supplementary. Metric stability. To evaluate the stability of our metric, following prior work [36], we computed the Pearson correlation between scores obtained across two runs. We find perfect correlation of 1.0 (as expected with temperature 0.0), confirming that VC-Inspector produces stable quality estimation across multiple runs. Computational efficiency. We assess the computational efficiency by comparing the average runtime per video clip. EMScore [31], ViCLIPScore [35], and VC-Inspector require 0.42, 0.34, and 0.30 seconds per clip, respectively, on single A100 GPU. VC-Inspector demonstrates supe- [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL Technical Report, Feb. 2025. arXiv:2502.13923 [cs]. 5, 6, 7 [3] Satanjeev Banerjee and Alon Lavie. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss, editors, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. 1, 2, 3, 5, 7 [4] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. 6 [5] David Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, and John Canny. CLAIR: Evaluating Image Captions In Houda Bouamor, Juan with Large Language Models. Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1363813646, Singapore, Dec. 2023. Association for Computational Linguistics. 2 [6] David Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Clair: Evaluating image arXiv preprint Darrell, and John Canny. captions with large language models. arXiv:2310.12971, 2023. 6, [7] Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, and Serge Belongie. Learning to Evaluate Image Captioning, June 2018. arXiv:1806.06422 [cs]. 3 [8] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video retrieval. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IV 16, pages 214229. Springer, 2020. 1 [9] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning, Mar. 2022. arXiv:2104.08718 [cs]. 3, 6, 7 [10] Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as ranking task: Data, models and Journal of Artificial Intelligence Reevaluation metrics. search, 47:853899, 2013. 2, 5, 7 [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5 [12] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. LITA: Language instructed temporal-localization assistant. In ECCV, 2024. [13] Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan Zhang, Zhe Gan, Jana Diesner, and Jianfeng Gao. TIGEr: Text-to-Image Grounding for Image Caption Evaluation, Sept. 2019. arXiv:1909.02050 [cs]. 3 [14] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-Captioning Events in Videos, May 2017. 1, 2, 4, 5, 6 [15] Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Trung Bui, and Kyomin Jung. Umic: An unreferenced metric for image captioning via contrastive learning. arXiv preprint arXiv:2106.14019, 2021. 3 [16] Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, and Kyomin Jung. ViLBERTScore: Evaluating Image Caption Using Vision-andLanguage BERT. In Steffen Eger, Yang Gao, Maxime Peyrard, Wei Zhao, and Eduard Hovy, editors, Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 3439, Online, Nov. 2020. Association for Computational Linguistics. 2 [17] Yebin Lee, Imseong Park, and Myungjoo Kang. FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using Large Multimodal Model, June 2024. arXiv:2406.06004 [cs]. [18] Zhuang Li, Yuyang Chai, Terry Yue Zhuo, Lizhen Qu, Gholamreza Haffari, Fei Li, Donghong Ji, and Quan Hung Tran. FACTUAL: Benchmark for Faithful and Consistent Textual Scene Graph Parsing, June 2023. arXiv:2305.17497 [cs]. 2, 6, 7 [19] Chin-Yew Lin. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. 1, 2, 3, 5, 7 [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 5, 12 [21] Hui Liu and Xiaojun Wan. Models see hallucinations: EvalarXiv preprint uating the factuality in video captioning. arXiv:2303.02961, 2023. 3, 6, [22] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, May 2023. arXiv:2303.16634 [cs]. 3 [23] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. 2 [24] Pranava Madhyastha, Josiah Wang, and Lucia Specia. Vifidel: Evaluating the visual fidelity of image descriptions. arXiv preprint arXiv:1907.09340, 2019. 2 [25] Koki Maeda, Shuhei Kurita, Taiki Miyanishi, and Naoaki Okazaki. Vision Language Model-based Caption Evaluation Method Leveraging Visual Context Extraction, Feb. 2024. arXiv:2402.17969 [cs]. 3 [26] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: Method for Automatic Evaluation of Machine Translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. 1, 2, 3, 5, 7 [27] Jeshmol P.J. and Binsu C. Kovoor. Video question answering: survey of the state-of-the-art. Journal of Visual Communication and Image Representation, 105:104320, 2024. 1 [40] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore: Evaluating Text Generation with BERT, Feb. 2020. arXiv:1904.09675 [cs]. 2, 3 [41] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In AAAI Conference on Artificial Intelligence, 2018. 1, 5 [28] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, TatSeng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained temporal reasoning. ArXiv, abs/2402.11435, 2024. 1 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, Feb. 2021. arXiv:2103.00020 [cs]. 2, 3 [30] Sara Sarto, Manuele Barraco, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Positive-augmented contrastive learning for image and video captioning evaluation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 69146924, 2023. 2, 3, 6, 7 [31] Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li, Weiming Hu, and Zheng-Jun Zha. EMScore: Evaluating Video Captioning via Coarse-Grained and Fine-Grained Embedding Matching, July 2022. arXiv:2111.08919 [cs]. 1, 2, 3, 6, 7, 8 [32] Tony Cheng Tong, Sirui He, Zhiwen Shao, and Dit-Yan Yeung. G-VEval: Versatile Metric for Evaluating Image and Video Captions Using GPT-4o. In AAAI, Mar. 2025. 3, 4, 6, [33] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based Image Description Evaluation, June 2015. arXiv:1411.5726 [cs]. 1, 2, 5, 7 [34] Yuiga Wada, Kanta Kaneda, Daichi Saito, and Komei Sugiura. Polos: Multimodal Metric Learning from Human Feedback for Image Captioning, Feb. 2024. arXiv:2402.18091 [cs]. 3 [35] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. InternVid: Large-scale Video-Text Dataset for Multimodal Understanding and Generation, Jan. 2024. arXiv:2307.06942. 6, 7, 8 [36] Ziang Xiao, Susu Zhang, Vivian Lai, and Q. Vera Liao. Evaluating evaluation metrics: framework for analyzing NLG evaluation metrics using measurement theory. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1096710982, Singapore, Dec. 2023. Association for Computational Linguistics. 8 [37] Yanzhi Yi, Hangyu Deng, and Jinglu Hu. Improving image captioning evaluation by considering inter references variance. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 985994, 2020. 2 [38] Zequn Zeng, Jianqiao Sun, Hao Zhang, Tiansheng Wen, Yudi Su, Yan Xie, Zhengjue Wang, and Bo Chen. HICEScore: Hierarchical Metric for Image Captioning Evaluation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 866875, Oct. 2024. arXiv:2407.18589 [cs]. [39] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generIn International Conference on Learning ation with bert. Representations, 2020. 1, 6,"
        },
        {
            "title": "Appendix",
            "content": "The appendix is organized as follows. Section lists the detailed prompts used in the proposed data generation pipeline, model fine-tuning, and explanation evaluation. Section provides the evaluation of explanations generated by VC-Inspector, and section presents additional visual examples with candidate captions of diverse quality. A. Prompts The data generation prompts are reported on the following blocks: Extract object Prompt A.1 Extract action Prompt A.2 Find similar object Prompt A.3 Find similar action Prompt A.4 Substitute object or action Prompt A.5 The fine-tuning prompt and the prompt for evaluating the generated explanations are reported in Prompt A.6 and Prompt A.7, respectively. Prompt A.1: Prompt to extract object from caption ### Instruction: Given the input text, generate list of objects in the caption in the format of [ \"Object1\", \"Object2\", ...]. Dont include any verbs. ONLY REPLY THE ANSWER. ### Input: {{caption}} ### Output Prompt A.2: Prompt to extract actions from caption ### Instruction: Given the input text, generate list of actions in the caption in the format of [\"Action1\", \"Action2\", ...]. ONLY REPLY THE ANSWER. ### Input: {{caption}} ### Output Prompt A.3: Prompt to find similar object given object ### Instruction: Find the parent class of the given object and generate one of its child class that has different meaning but shares the same parent. The new class cannot be synonym or similar terms to the original object. It can be an antonym or any co-hyponym. For example, generate \"dog\" for \"cat\". ONLY REPLY THE NEW CLASS. ### Input: {{object}} ### Output: Prompt A.4: Prompt to find similar action given an action ### Instruction: Find different action that the subject can perform that has different meaning than the input action. The new action cannot be synonym or similar terms to the original action. For example, generate \"put into\" for \"take out of\". ONLY REPLY THE NEW ACTION. ### Input: {{action}} ### Output: Prompt A.5: Prompt to substitute object or action given the caption and new object or actions ### Instruction: Substitute as {{new_obj_act}} . Keep the answer in the same format as {{caption}} . ONLY REPLY THE ANSWER. {{old_obj_act}} {{caption}} in ### Input: cap ### Output: Prompt A.6: Fine-tuning prompt ### USER: [[VIDEO]] <caption> {{caption}} </caption> You are given video and caption describing the video content. Please rate the helpfulness, relevance, accuracy, level of details of the caption. The overall score should be on scale of 1 to 5, where higher score indicates better overall performance. Please first output single line containing only one integer indicating the score. In the subsequent line, please provide comprehensive explanation of your evaluation, avoiding any potential bias. STRICTLY FOLLOW THE FORMAT. ### ASSISTANT: {{quality_score}} The caption does not accurately capture the video content. For example, the actions ( {{wrong_act}} ) are incorrect / the objects ( {{wrong_obj}} ) are incorrect/the objects ( {{wrong_obj}} ) and actions ( {{wrong_act}} ) are incorrect. Prompt A.7: Prompt for explanation evaluation [Context] {{ground_truth_caption}} [Caption] {{caption_to_evaluate}} [Groundtruth] {{ground_truth_explanation}} [End of Groundtruth]"
        },
        {
            "title": "LLM Score",
            "content": "ActivityNet-FG-Eval YouCook2-FG-Eval 0.79 0.70 93.11 90.97 Table 7. VC-Inspector-3B on two synthetic evaluation datasets. the explanations generated by Evaluation of [Assistant] {{predicted_explanation}} [End of Assistant] [System] We would like to request your feedback on the performance of an AI assistant in the response to the quality evaluation of the caption provided above with respect to video. For your reference, the visual content in the video is represented with few sentences describing the same video. You are also given ground truth evaluation to that caption. Please rate the helpfulness, relevance, accuracy, level of details of the response by comparing to the ground truth and referring to the context information. Provide an overall score on scale of 1 to 10, where higher score indicates better overall performance. Please first output single line containing the score. In the subsequent line, please provide comprehensive explanation of your evaluation, avoiding any potential bias. B. Evaluation on Explanation Quality We further assess the quality of the explanations generated by VC-Inspector on two synthetic datasets, as the VATEX-Eval dataset does not offer ground truth explanations. Table 7 presents two metrics for this experiment. The BERT score measures the semantic similarity between the generated explanations and the pseudo ground truth explanations, according to pretrained textual embeddings. On the other hand, the LLM Score is derived by using the prompt detailed in section to engage Llama-3-8B-Instruct in LLM-as-a-judge approach, similar to [20]. We report the relative score assigned by the LLM that rescales the score received by the predicted explanations with respect to the score given to the ground truth explanations. Results show that the explanations provided by VC-Inspector, along with its quality assessment, align closely with the (pseudo) ground truth explanations obtained during the data generation process. C. Additional Visual Examples 5 visual presents Figure of additional VC-Inspector applied to VATEX-Eval. Consistent with the results shown in Figure 4 of the main paper, VC-Inspector effectively identifies incorrect objects and actions in candidate captions, assigning quality scores that closely align with human judgments. examples Figure 5. Additional visual examples on VATEX-Eval. VC-Inspector identifies the incorrect objects (highlighted in red) and assigns scores aligned with human evaluators. Figure 6. Additional visualization of VC-Inspector results on ActivityNet-FG-Eval videos, with candidate captions of diverse quality. Incorrect objects and actions are identified by VC-Inspector and labeled in red. In Figure 6, we showcase two video examples from ActivityNet-FG-Eval, each paired with candidate captions containing progressively increasing number of factual inaccuracies. The model successfully detects these incorrect elements, provides detailed explanations, and yields scores that reflect the severity of the factual errors."
        }
    ],
    "affiliations": [
        "Intel Labs",
        "University of Maryland, Baltimore County"
    ]
}