{
    "paper_title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning",
    "authors": [
        "Zhongwei Wan",
        "Zhihao Dou",
        "Che Liu",
        "Yu Zhang",
        "Dongfei Cui",
        "Qinjian Zhao",
        "Hui Shen",
        "Jing Xiong",
        "Yi Xin",
        "Yifan Jiang",
        "Yangfan He",
        "Mi Zhang",
        "Shen Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful and instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose Multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization (SRPO), a two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct a high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model learn both reasoning and self-reflection. In the second stage, we introduce a novel reward mechanism within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarks, including MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 3 1 7 1 0 . 6 0 5 2 : r SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning Zhongwei Wan2 Zhihao Dou3 Che Liu4 Yu Zhang11 Dongfei Cui5 Qinjian Zhao6 Hui Shen7 Yangfan He9 Mi Zhang2 Jing Xiong10 Yi Xin12 Yifan Jiang8 Shen Yan1 3Case Western Reserve University 1ByteDance Seed 2The Ohio State University 4Imperial College London 5Duke University 6Kean University Minnesota 7University of Michigan 8University of Southern California 9University of 10The University of Hong Kong 11Tongji University wan.512@osu.edu, sheny@bytedance.com 12Nanjing University Correspondence to: https://srpo.pages.dev"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have shown promising capabilities in reasoning tasks, yet still struggle significantly with complex problems requiring explicit self-reflection and self-correction, especially compared to their unimodal text-based counterparts. Existing reflection methods are simplistic and struggle to generate meaningful, instructive feedback, as the reasoning ability and knowledge limits of pre-trained models are largely fixed during initial training. To overcome these challenges, we propose multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization SRPO, two-stage reflection-aware reinforcement learning (RL) framework explicitly designed to enhance multimodal LLM reasoning. In the first stage, we construct high-quality, reflection-focused dataset under the guidance of an advanced MLLM, which generates reflections based on initial responses to help the policy model to learn both reasoning and In the second stage, we introduce novel reward mechanism self-reflection. within the GRPO framework that encourages concise and cognitively meaningful reflection while avoiding redundancy. Extensive experiments across multiple multimodal reasoning benchmarksincluding MathVista, MathVision, Mathverse, and MMMU-Prousing Qwen-2.5-VL-7B and Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms state-of-the-art models, achieving notable improvements in both reasoning accuracy and reflection quality."
        },
        {
            "title": "Introduction",
            "content": "Multimodal reasoning is central to numerous real-world scenarios, such as interpreting scientific figures, geometric reasoning, and integrated complex image-text comprehension tasks [13]. Although recent approaches have attempted to transfer effective RL-based reasoning methods [47] from textual models to multimodal scenarios [811], these methods generally encounter considerable limitations. Specifically, existing MLLMs typically follow token-level Markov process [12, 13] during generation, which relies on local dependencies. This often leads to redundant, repetitive, or erroneous reasoning steps in their output [14]. Such issues hinder reasoning models from achieving significant improvements over fast-thinking models; in some cases, their performance is even inferior. For instance, GPT-o1, despite its explicitly structured reasoning pathways, achieves slightly lower Project Lead (work done during internship at ByteDance). Equal Contribution. Preprint. Figure 1: Left: Illustrative examples of reflection improving reasoning. Right: Quantitative comparison on benchmark datasets. accuracy (73.9%) on MathVista compared to Qwen2.5-VL-72B (74.8%) [1, 15]. The primary reason lies in the presence of incorrect and redundant steps, which negatively affect final performance. Previous studies have shown that self-reflection [16, 17] is an effective approach to address this issue. By explicitly encouraging the model to review, evaluate, and revise its own reasoning process, selfreflection helps eliminate unnecessary or incorrect steps, enhances logical coherence, and promotes deeper understanding [18]. However, recent empirical studies [1820] indicate that the upper bounds of reasoning capabilities in pre-trained models are largely established during the initial pre-training phase. Consequently, these studies indicate that reinforcement learning improves reasoning by activating decision-making within fixed structures, rather than enabling the acquisition of new knowledge or behaviors. To effectively surpass these inherent limitations, external interventions such as advanced reflective experiences or cognitively guided techniques are required. While previous approaches [18, 16, 20] have attempted to enhance self-reflective reasoning through direct prompting or reinforcement learning, their effectiveness remains limited by the constraints imposed during pre-training, making them insufficient for substantially improving reflective reasoning and overall reasoning performance. Therefore, designing effective enhancement strategies to improve the intrinsic reasoning capabilities of MLLMs beyond the constraints established during pre-training remains challenging problem. To address this question, inspired by cognitive science emphasizes that robust human reasoning involves active self-reflection and iterative self-correction steps [2123, 16], we integrate explicit reflective methods within both multimodal Supervised Fine-Tuning (SFT) and RL, enabling models to surpass their intrinsic reasoning boundaries established in the pre-training phase. Unlike previous studies [2426], which focus solely on enhancing reasoning ability by aligning with extended chainof-thought supervision, our goals are not only strengthens the models reasoning performance but also fosters its capacity for self-reflection. Motivated by these insights, we introduce SRPO (multimodal Self-Reflection enhanced reasoning with Group Relative Policy Optimization), novel two-stage reflective training framework specifically designed to promote explicit self-reflection and self-correction behaviors within MLLMs. (i) In the first stage, we utilize an advanced MLLM to generate reflection content based on the discrepancies between the policy models outputs and the ground truth. In this process, the model autonomously evaluates its multiple generated responses, identifies errors, and iteratively revises them through reflective reasoning. Subsequently, we leverage these high-quality reflection datasets to perform multimodal reflection-based supervised fine-tuning (SFT), providing cold-start initialization for subsequent reinforcement learning. (ii) In the second stage, we further propose reflection-aware RL method built upon the Group Relative Policy Optimization (GRPO) algorithm [4]. Our specifically designed reward function actively incentivizes concise, task-oriented reflection steps, explicitly punishing overly verbose or redundant reflections, thus effectively encouraging MLLMs to adopt meaningful reflective behaviors via RL stage. As illustrated in Figure 1, after two-stage training, SRPO enables MLLMs to autonomously generate reflective reasoning, effectively refine intermediate thinking steps, and consequently achieve improved reasoning performance across various benchmarks compared to the GRPO. We conduct comprehensive experiments across several widely adopted multimodal reasoning benchmarks, including MathVista [1], MathVison [27], and MMMU-Pro [3], utilizing representative multimodal models (e.g., Qwen-2.5-VL-7B and Qwen-2.5-VL-32B [28]). Results demonstrate that SRPO consistently and significantly outperforms current state-of-the-art models, achieving notable 2 improvements in reasoning accuracy, reflection quality, and cross-task generalization. These empirical findings provide strong evidence that explicit reflection-oriented training can effectively extend multimodal models reasoning capabilities beyond the inherent cognitive boundaries set during pre-training. Our core contributions are summarized as follows: Novel reflection-oriented SFT construction. We introduce novel reflective data generation pipeline that leverages the original models responses. By using large MLLM (e.g., GPT-o4mini [7]), we generate corresponding reflection processes aligned with the gold-standard answers. This pipeline is designed to teach the policy model both effective reasoning and reflective thinking. Reflection-aware reinforcement learning. We develop tailored GRPO-based RL method (SRPO) equipped with an explicit reward function to incentivize meaningful reflective reasoning. Empirical validation and insights. Extensive evaluations across various multimodal reasoning benchmarks confirm that SRPO achieves state-of-the-art performance, clearly demonstrating the effectiveness of self-reflection enhancements in multimodal reasoning contexts."
        },
        {
            "title": "2 Related Works",
            "content": "Reinforcement Learning for LLM Reasoning. Recent advancements in large-scale RL, such as DeepSeek-R1 [4], have demonstrated substantial progress in enhancing complex, human-like Chain-of-Thought (CoT) reasoning by utilizing result-oriented or formatting-specific reward signals. In parallel, several studies, including Open-Reasoner-Zero [29], SimpleRL-Zoo [30], AlphaMed [31], and Logic-RL [32], have explored directly fine-tuning base language models using RL without any supplementary supervised fine-tuning stages. Additionally, methods such as Light-R1 [33] and DeepScaler [34] introduce specially constructed cold-start datasets designed explicitly to encourage detailed step-wise reasoning during initial training phases. Meanwhile, recent analyses [18, 20, 19] have also shed light on intrinsic limitations of purely RL-based reasoning enhancement strategies. Furthermore, complementary approaches such as VAPO [35], DAPO [12], and Dr. GRPO [36] have sought to refine the Group Relative Policy Optimization (GRPO) framework by optimizing reward design and enhancing advantage estimation techniques, thus more effectively promoting deeper reasoning behaviors within language models. In contrast, our work specifically targets multimodal complex reasoning, explicitly emphasizing self-reflection or correction to enhance reasoning performance during both multimodal SFT and RL training phases. Reinforcement Learning for Multimodal LLM Reasoning. State-of-the-art multimodal reasoning capabilities are largely dominated by proprietary models, such as GPT-o3 and o4 [7], Gemini-2.5Pro-T [5], and Seed1.5-VL-T [37]. Recent studies aim to close this gap via reinforcement learning (RL) on open-source multimodal LLMs. LMM-R1 [9] introduces two-stage, rule-based RL, though mainly benefiting textual scenarios. Reason-RFT [38] leverages supervised fine-tuning (SFT) with Chain-of-Thought (CoT) data to initialize RL. Vision-R1 [26] enhances multimodal CoT datasets using DeepSeek-R1 and employs progressive thinking suppression in GRPO training. MM-Eureka [8] presents the MMK12 dataset alongside two-stage RL method, while VL-Rethinker [15] utilizes selective sample replay and explicit textual rethinking triggers to refine multimodal reasoning. R1V [39] explores RL primarily within visual-centric reasoning tasks but has limited generalization to broader multimodal domains. However, none of these approaches explicitly emphasize self-reflection or correction during both SFT and RL training phases, resulting in suboptimal reasoning performance. Furthermore, poorly designed reward functions leave these methods vulnerable to length redundancy."
        },
        {
            "title": "3 Method of SRPO",
            "content": "In the following sections, we present the detailed methodology of our SRPO training framework, emphasizing our two core contributions: (1) Novel reflection-oriented SFT data construction. In this stage, we construct reflection dataset to inject reflective capabilities into the policy model. Through training on this dataset, we aimed to achieve two goals: first, to enhance the policy models ability for self-reflection and self-correction during cold-start initialization; and second, to effectively transfer the reflective knowledge of large-scale MLLMs into the policy model, enabling it to learn how to reflect effectively gradually. (2) Reflection-aware reinforcement learning, where we propose tailored GRPO-based RL algorithm, SRPO, equipped with reflection-aware reward function that promotes reflective reasoning. 3 Figure 2: Pipeline of Self-Reflection SFT data construction, including CoT and self-reflection generation. 3.1 Reflection-oriented Cold-start Initialization 3.1.1 Self-Reflection SFT data construction Motivation. To address the limitations of local dependency in MLLM reasoning, often resulting in redundant, incoherent, or incorrect outputs, self-reflection [16] becomes essential for improving reasoning quality. However, the absence of reflection knowledge and skills can lead to low-quality or superficial self-correction. RL methods typically guide model towards selecting high-reward reasoning paths already represented within its intrinsic knowledge distribution, rather than inducing genuinely novel cognitive capabilities or knowledge [19]. In contrast, by incorporating external knowledge distillation, supervised fine-tuning (SFT) has been shown to effectively expand the cognitive boundaries of reasoning [20]. To this end, we propose supervised fine-tuning (SFT) approach that explicitly injects reflection knowledge into the policy model. By learning from highquality reflective examples, the model acquires the ability to identify, diagnose, and revise flawed reasoning, ultimately enhancing its coherence, efficiency, and self-awareness during the reasoning. Less is More. To support effective reflection learning, we construct high-quality dataset generated by advanced MLLMs, containing two types of reflective examples: one for refining correct CoTs by removing redundancy, and another for revising incorrect CoTs through error correction. While prior SFT approaches [2426] focus on mimicking correct reasoning, our reflection-oriented SFT explicitly injects reflection knowledge, enabling the model to detect flaws and refine its reasoning. These approaches are complementaryours enhances reasoning correction and self-improvement. As illustrated in Figure 2, we begin by curating high-quality subset of = 10,000 multimodal reasoning samples from three large-scale datasets: LLaVA-CoT [25] (100K), Mulberry [24] (260K), and MathV360K [40]. These samples cover diverse domains including physics, mathematics, and general knowledge. Based on this subset, we construct our self-reflection dataset through two complementary strategies: (1) Refinement of correct CoTs, and (2) Revision of incorrect CoTs. For each sample, we first obtain the initial response generated by the policy model through CoT prompting. Then, using the ground truth answer as guidance, we employ larger MLLM (e.g., GPT-o4-mini) to generate self-reflection that either revises flawed reasoning or streamlines correct but verbose outputs. Each final sample thus contains three components: the initial response, the generated self-reflection, and the ground truth answer. In our curated data, approximately 30% of initial responses are correct, while the remaining 70% contain reasoning errors, highlighting the necessity of self-reflection for both wrong solution correction and right question refinement. 3.1.2 Cold-start Initialization of Policy Model This phase equips the policy model πθ (initial MLLMs) with fundamental self-reflective reasoning capabilities, ensuring it can generate proper reflection-aware reasoning paths before reinforcement learning: Lcold-start = Eτ (cid:34) (cid:88) log (πinital(a1, < reflection > ... < /reflection >, a2 q)) . (cid:35) t=1 (1) Here, a1 is the policy models initial response, reflection.../reflection denotes the reflection generated by large LLM, and a2 is the ground truth answer. Given the input prompt and the policy model πinitial, the objective is twofold: (1) to train the model to revise a1 toward a2 using the reflection reflection, and (2) to leverage the reasoning and knowledge embedded in a2 to guide future predictions. This reflection-driven learning process equips the policy model with self-correction capabilities and improves alignment with correct reasoning trajectories. 3.2 Reflection-aware Reinforcement Learning Under the reinforcement learning framework, complex reasoning tasks often leverage Chain of Thought (CoT) steps to improve prediction accuracy and interpretability [4, 6]. However, simply encouraging CoT generation can result in redundant or misleading reasoning [1820]. To address this, recent work introduces self-reflection, allowing models to revise their reasoning after generation and improve overall quality. Yet without proper control, models may exploit reflectione.g., by inflating length or complexity for higher rewards without real gains. To counter this, we propose SRPO, framework that enhances both reasoning and reflection through carefully designed reward signals that discourage superficial behaviors. 3.2.1 Group Relative Policy Optimization (GRPO) We utilize Group Relative Policy Optimization (GRPO), following recent advances [4], to optimize our RL-based training. Unlike SFT, which uses token-level losses, GRPO leverages policy gradients calculated from reward losses for effective policy optimization. GRPO promotes exploration of richer and more diverse reasoning solutions by comparing generated responses within sampled groups. Formally, let be the question set, πθold be the policy model, and {o1, o2, . . . , oG} be group of responses from πθold for question q. Let πθref denote the frozen reference model. The GRPO optimization objective is defined as follows: JGRPO(θ) = qQ,{oi}G Ai, clip (cid:18) πθ(oi,tq) πθold (oi,tq) (cid:19) (cid:19) (2) , 1 ϵ, 1 + ϵ Ai βDKL(πθπref) 1 (cid:88) oi (cid:88) i=1 t=1 min i=1πθold (cid:18) πθ(oi,tq) πθold(oi,tq) Here, ϵ and β are clipping hyper-parameters and KL-divergence penalty coefficients, respectively. The advantage Ai for each response is computed as: Ai = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) , where {ri}G i=1 are rewards from the group. (3) GRPO thus replaces the critic model traditionally required in PPO with computationally efficient intra-group advantage estimation. 3.2.2 Enhanced GRPO with Reflection-Aware Rewards (SRPO) In SRPO, we aim to achieve two main goals: (1) enhance the policy models reasoning ability through RL, and (2) strengthen its capacity for self-reflection. To realize these goals, we introduce an enhanced reward function that specifically targets the reflection process within the CoT reasoning framework. The format of this response can be summarized as first solution reflection second refined solution. The total reward Rtotal can be shown as: Rtotal = Rtask + Rreflection. (4) Task Reward. The task-specific reward Rtask combines format reward and an accuracy reward. The format reward Rformat encourages the model to enclose its reasoning process within <think>...</think> tags. The accuracy reward Raccuracy verifies whether the predicted answer matches the ground truth, guiding the model to produce logically consistent and correct outputs for the first solution. This repeated supervision helps enhance the models reasoning ability. Formally, Raccuracy is defined as: Rtask = Rformat = (cid:26)0.5, 0, if format is correct otherwise +Raccuracy = (cid:26)0.5, 0, if first solution matches gt label otherwise (5) Reflection Reward. Let Iref {0, 0.25} indicates proper formatting of reflection segments (enclosed with <reflect> tags) , Lresponse is the total response token length, Ttarget is the optimal length for all responses and Tmax is maximum response lengths respectively. It is noted that the response contains the first solution, reflection and the second solution guided by reflection. The reward can be defined as: Rreflection = Ieff + Iref + α flen(Lresponse), (6) 5 where the reflective brevity reward flen(Lresponse) is explicitly defined to encourage appropriate lengths which can achieve exact and brief results: flen(Lresponse) = exp (cid:18) (cid:18) Ttarget Tmax Ttarget (cid:19)(cid:19) . (7) The reward flen peaks at target length, encouraging concise, informative reasoning, and decays smoothly toward zero as length approaches defined maximum. This softly constrains output within desirable range without hard cutoffs. We adopt this exponential form for its simplicity, differentiability, and stable gradient behavior during training. Additionally, the effectiveness indicator Ieff provides extra rewards if the reflection genuinely improves reasoning outcomes, measured by improvement in the correctness or accuracy of final answers postreflection: Ieff = 0.25, 0.5, 0, 0.25 if reflection keeps corrected answer, if reflection corrects the wrong answer, if reflection fails to correct the wrong answer, if reflection misconducts the right into wrong answer. (8) The proposed reward function Ieff focuses on the second solution results and assesses the reflections impact on answer correctness in four cases: preserving correct answer yields +0.25, successfully correcting an incorrect answer results in +0.5, failing to fix wrong answer receives no reward, and misleading correct answer incurs -0.25. This design encourages the model to treat reflection not as formality, but as tool for improving reasoning quality and avoiding redundancy. Advantages over Standard GRPO. Compared with standard GRPOwhich primarily relies on sparse task-level accuracy supervisionour enhanced reflection-aware SRPO framework introduces several critical improvements: (i) By enforcing structured reflection formatting through the Iref indicator, the model is guided to produce consistently well-organized and identifiable reflection segments. (ii) The introduction of smooth, differentiable length reward flen(Lref) encourages the generation of reflections that are concise yet informative, avoiding hard cutoffs while softly constraining verbosity. (iii) The effectiveness reward Ieff directly aligns reward signals with functional improvement, providing positive incentives only when the reflection corrects errors or preserves correctness, and penalizing harmful reflections. (iv) By explicitly rewarding reflection utility rather than mere presence, our approach discourages reward gaming behaviors such as empty or verbose reflections, leading to more meaningful reasoning supervision. Together, these enhancements enable SRPO to foster deeper self-correction capabilities, improve sample efficiency, and achieve superior performance in complex reasoning tasks compared to standard GRPO."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experiment Settings Training Dataset. (1) SFT: To construct the self-reflection SFT dataset for the cold-start initialization phase, we first curate samples from several established multimodal reasoning sources, including the Mulberry dataset (260K) [24], MathV360K [40], and LLaVA-CoT dataset (100K) [25]. We then apply the data construction procedure detailed in Section 3.1.1, ultimately resulting in refined SFT dataset comprising approximately 10K samples. (2) RL: For the subsequent reinforcement learning phase, we aggregate diverse collection of multimodal reasoning samples from multiple datasets, such as ScienceQA [41], Geometric Math QA [42], ChartQA [43], DVQA [44], AI2D [45], MATH [46], Virgo [47], R1-OneVision [11], MMK12 [8], and PhyX [48]. These datasets collectively encompass mathematical reasoning, general scientific reasoning, and general chart comprehension tasks. The RL training dataset consists of diverse, cross-domain reasoning samples. More details about SFT and RL training dataset collection are shown in Appendix B.1. Baselines and Benchmarks. To comprehensively evaluate SRPO, we compare against three groups of baselines: (1) Closed-source MLLMs: General-purpose models GPT-4o [49], Claude3.7-Sonnet [50], Gemini2-flash [51], and the reasoning-optimized GPT-o1 [52]; (2) Open-source general MLLMs: Instruction-tuned multimodal models InternVL2.5 [53] and Qwen-2.5-VL [28], ranging from 7B to 78B parameters; and (3) Open-source reasoning MLLMs: Explicitly fine-tuned reasoning models, 6 Table 1: Comparison between our 7B and 32B models, closed-source baselines, and other visionlanguage models. reproduced by us. The results of other baselines are obtained from their official reports. Bold indicates the best-performing open-source model. Model Math-Benchmark General-Benchmark MathVista MathVerse MathVision OlympiadBench WeMath MMMU-Pro MMMU EMMA Claude3.7-Sonnet GPT-4o GPT-o1 Gemini2-flash Seed1.5-VL-T InternVL2-8B InternVL2.5-8B QwenVL2-7B Llava-OV-7B Kimi-VL-16B QwenVL2.5-7B MM-Eureka-8B1 R1-VL-7B R1-Onevision-7B OpenVLThinker-7B VL-Rethinker-7B Vision-R1-7B MM-Eureka-7B (Ours - SRPO-7B) InternVL2.5-VL-38B Qwen-2.5-VL-32B InternVL2.5-38B-MPO MM-Eureka-32B (Ours - SRPO-32B) Closed-Source MLLMs 52.0 50.2 57.0 59.3 - 41.3 30.4 60.3 41.3 68. 48.9 35.0 68.0 51.0 65.0 Open-Source General MLLMs (7B-16B) 22.8 39.5 19.7 26.2 44.9 46.3 17.4 19.7 16.3 18.5 21.4 25.1 10.1 12.3 9.7 8.5 20.2 Open-Source Reasoning MLLMs (7B) 40.4 40.0 46.4 47.9 54.2 52.4 50.3 55.8 22.2 24.7 23.5 25.3 32.3 27.2 26.9 32.9 8.6 10.8 17.3 20.1 20.5 19.4 20.1 22. 72.6 68.8 98.7 71.4 - 47.2 53.5 51.6 49.9 62.1 55.7 53.8 61.8 64.3 70.2 62.9 66.1 71.6 Open-Source General and Reasoning MLLMs (32B) 49.4 48.5 46.5 56. 58.9 31.8 38.4 32.3 34.4 39.6 32.0 30.0 25.6 35.9 38.5 67.5 69.1 66.2 73. 76.4 66.8 63.8 73.9 70.4 85.6 58.3 64.4 58.2 63.2 68.7 68.2 67.1 63.5 64.1 70.2 74.9 73.5 73.0 75.8 71.9 74.7 73.8 74. 78.5 51.5 51.9 62.4 51.7 67.6 29.0 34.3 30.5 24.1 36.9 27.8 7.8 21.6 37.3 41.7 37.7 37.6 42.3 46.0 49.5 - 50. 51.3 68.3 69.1 78.2 70.7 77.9 51.2 56.0 54.1 48.8 55.7 54.3 49.2 44.5 52.5 56.7 54.7 55.2 57.1 57.6 59.4 62. 66.1 35.1 32.7 45.7 33.6 - 19.8 20.6 20.2 18.3 21.5 21.5 8.3 20.8 26.6 29.7 22.4 23.5 29.6 - 31.1 - 34. 38.2 including InternVL2.5-MPO [54], OpenVLThinker-7B [55], MM-Eureka-7B [56], VL-Rethinker7B [15], R1-Onevision-7B [11], and R1-VL-7B [39]. We evaluate SRPO across three categories of multimodal reasoning benchmarks: mathematical reasoning (MathVista [1], MathVerse [2], MathVision [27], OlympiadBench [57], WeMath [58]), general reasoning (MMMU-Pro [3], MMMU [59], EMMA [60]), and cross-disciplinary reasoning (MMK12 [8]), covering physics, chemistry, and biology tasks. Implementation Setup. For self-reflection cold-start SFT and subsequent RL training, Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-32B-Instruct models are trained on 8 and 32 NVIDIA H100 GPUs, respectively. We adopt 1 epoch for SFT to avoid overfitting. During RL, we adopt the OpenRLHF framework [61], training for 3 epochs on 30K samples with rollout and training batch sizes set to 128 (8 rollouts per sample), sampling temperature of 1.0, and Adam optimizer with learning rate of 1106. For the reflection reward parameter α, we set it to 0.1 to ensure training stability. Regarding the reflective brevity reward flen(Lresponse), to discourage excessively verbose outputs, we define Ttarget as 2 the length of the original response (i.e., reflection plus new reasoning equals the first think length), and set Tmax to 2.5 the original length (i.e., reflection plus new reasoning equals 1.5 the first think length). Additional hyper-parameter settings and detailed prompt configurations are provided in Appendix B.2 and Appendix B.3. Table 2: Performance comparison across different disciplines in MMK12. Model Math Phys Chem Bio Closed Models Claude3.7 GPT-4o o1 Gemini2 Open General MLLMs IntVL2.5-8B Qwen-2.5-7B IntVL2.5-38B Qwen-2.5-32B Qwen-2.5-72B 57.4 55.8 81.6 76.8 46.8 58.4 61.6 71.6 75.6 Open Reasoning MLLMs IntVL2.5-8B-MPO IntVL2.5-38B-MPO R1-OneVision MM-Eureka-7B OpenVLThinker MM-Eureka-32B SRPO-7B SRPO-32B 26.6 41.4 44.8 71.2 63.0 74.6 75.3 77. 53.4 41.2 68.8 53.6 35.0 45.4 49.8 59.4 64.8 25.0 42.8 33.8 56.2 53.8 62.0 60.6 64.2 55.4 47.0 71.4 64.6 50.0 56.4 60.4 69.6 69.6 42.4 55.8 39.8 65.2 60.6 75.4 70.3 77. 55.0 55.4 74.0 66.0 50.8 54.0 60.0 66.6 72.0 44.0 53.2 40.8 65.2 65.0 76.8 69.5 79.2 4.2 Main Results of Benchmarks Multimodal General Reasoning. We further evaluate our approach on general multimodal reasoning tasks to assess the effectiveness of our reflection-enhanced training strategy beyond mathematical Table 3: Ablation study of SRPO on RL training data size and self-reflection components. Model Components RL Data Size MathVista MathVerse MathVision MMMU-Pro Physics Avg. Qwen-2.5-VL-7B + GRPO (Ours - SRPO-7B) SRPO-7B SRPO-7B w/o Self-Reflection SFT w/o Self-Reflection RL - 37K 37K 15K 5K 37K 37K 68.2 72.3 75.8 74.5 73. 74.2 70.3 46.3 52.9 55.8 54.9 53.6 53.3 48.2 25.1 30.3 32.9 32.2 31. 30.3 27.2 36.9 39.9 42.3 41.4 40.3 39.7 38.7 45.4 53.5 60.6 60.1 57. 58.6 48.5 44.4 49.8 53.5 52.6 51.3 51.2 46.6 reasoning. As shown in Table 1, SRPO-7B consistently outperforms existing open-source MLLMs on three general-domain benchmarks: MMMU-Pro, MMMU, and EMMA. Notably, compared to state-of-the-art closed-source reasoning models, SRPO-32B still demonstrates highly competitive performance, exceeding Gemini2-flash by 4.6 on the EMMA benchmark. These results underscore the broader generalizability of reflection-enhanced training in improving multimodal reasoning capabilities. Multimodal Mathematical Reasoning. As presented in Table 1, SRPO achieves highly competitive performance on multiple mathematical reasoning benchmarks, even when compared to leading closed-source MLLMs. For instance, on the MathVista benchmark, SRPO obtains score of 78.5%, trailing the widely acknowledged state-of-the-art model, OpenAI O1, by only 73.9%. Moreover, SRPO consistently outperforms open-source general multimodal baselines by clear margin. Notably, when compared to state-of-the-art open-source reasoning models such as VL-Rethinker-7B and MM-Eureka-7B, SRPO demonstrates obvious advantages, even on complex, graduate-level reasoning datasets like OlympiadBench. These results strongly validate our claim that explicitly enhancing the models self-reflection capabilities during both the SFT and RL stages positively contributes to improved complex reasoning performance. Cross-disciplinary Reasoning. Beyond evaluating our model on widely-used multimodal mathematical and general reasoning benchmarks, we also investigate its capability for cross-disciplinary generalization to novel tasks not included in the training data, such as physics, chemistry, and biology. Results presented in Table 2 demonstrate that SRPO achieves superior cross-disciplinary reasoning performance, surpassing MM-Eureka-7B (trained solely via RL without self-reflection incentivization) by 5.1 points on Physics and OpenVLThinker-7B (SFT-enhanced reasoning) by 9.7 points on Chemistry. These findings highlight that integrating both reflection-enhanced SFT during the cold-start stage and employing reflection-aware reward function during the RL stage significantly improves the models generalization to previously unseen reasoning domains. 4.3 Ablation Study RL Train-Set Size. We analyze SRPOs performance sensitivity to the RL training set size by sampling subsets of 15K and 7K from our original 37K dataset. As shown in Table 4.3, SRPO consistently improves with more data. Remarkably, even at 5K samples, SRPO significantly outperforms Qwen-2.5-VL-7B and standard GRPO, exceeding GRPO by 7.1 points on the Physics benchmark. Thus, enhancing self-reflection within RL efficiently boosts reasoning even under limited data. Effectiveness of Self-Reflection Components. We further investigate individual self-reflection components within SRPO. Table 4.3 shows that removing Self-Reflection SFT notably reduces performance, yet still maintains 5.1-point advantage over standard GRPO on Physics. Conversely, eliminating Self-Reflection RL yields minimal improvements over Qwen-2.5-VL-7B, indicating that reflection training solely in the SFT stage is insufficient. Hence, explicitly rewarding reflection quality during RL is essential for optimal multimodal reasoning. More analysis about Self-Reflection RL component is illustrated in supplementary part. 4.4 Reasoning Qualitative Analysis Self-Reflection in RL Training. Figure 3 The left part compares samples generated during RL training with SRPO and standard GRPO using Qwen-2.5-VL-7B, specifically highlighting the intermediate reasoning steps. We observe that SRPO explicitly guides the model to engage in effective self-reflection on its initial reasoning paths and answers. During the reflection process, SRPO corrects wrong reasoning steps and provides concise revisions, leading to refined final answers. 8 Figure 3: Generated samples in RL training (left) and generated samples in real test case (right). In contrast, GRPO-generated samples typically contain reasoning steps without explicit reflective corrections, rarely revising the initial reasoning paths. We provide the complete version of samples in Appendix B.4. Self-Reflection in Test Case. Futhermore, figure 3 right part illustrates examples of reasoning outputs during inference. Responses generated by SRPO exhibit clear self-reflection patterns acquired during RL training, actively refining or correcting flawed reasoning steps and answers to improve overall accuracy. Conversely, models trained without self-reflection rarely adjust or rectify incorrect reasoning paths, resulting in persistent reasoning errors and reduced final performance. Figure 4: Training curves for SRPO and baselines: (a) training reward, (b) response length, and (c) upper clipping ratio. 4.5 Further Analysis RL Training Dynamics Analysis. We analyze training dynamics to highlight SRPOs advantages (Figure 4). SRPO and SRPO w/o self-reflection SFT converge faster and outperform standard GRPO, illustrating that reflection-enhanced initialization accelerates reflection skill acquisition and improves reasoning. Moreover, SRPO consistently generates longer responses (Figure 4(b)), indicating effective early-stage reflection training from cold-start initialization. Interestingly, SRPOs lower, smoother ratio clip upper curve (Figure 4(c)) reflects stable policy updates, avoiding excessively large gradients or step sizes, confirming enhanced training consistency from reflection-based RL. More training visualizations are shown in Appendix B.5. Figure 5: Performance of various RL methods with and without selfreflection. Combining Self-Reflection with Alternative RL Methods. To validate the generality of our self-reflection strategy, we incorporate it into PPO and DAPO algorithms within the OpenRLHF framework, following identical cold-start SFT initialization and evaluating checkpoints at 500 training steps. Figure 5 shows consistent improvements from selfreflection integration across all RL algorithms. Reflection-enhanced DAPO achieves performance comparable to SRPO, while SRPO slightly surpasses reflection-enhanced PPO. The result highlights the advantage of GRPOs group-based advantage estimation and reflection-oriented rewards over PPOs single-trajectory reward signals for effectively incentivizing self-reflection."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduced SRPO, reflection-aware reinforcement learning framework designed to enhance multimodal reasoning capabilities in mutlimodal large language models. By systematically generating high-quality reflection-focused training data and employing novel reward mechanism that explicitly incentivizes concise and effective self-reflection, our method successfully addresses the limitations of previous approaches, including insufficient data quality and lack of self-reflective behavior for refining response. Comprehensive experiments across multiple multimodal reasoning benchmarks demonstrated the significant effectiveness of SRPO, surpassing existing state-of-the-art models in both reasoning accuracy and reflection quality. Our results highlight the critical role of reflection-driven training strategies for robust multimodal reasoning. 5.1 Acknowledgment We thank Xuehan Xiong, Kunchang Li and Qinghao Ye for their support in technical discussions related to this work. We also thank Faming Wu for his assistance in addressing infrastructure issues."
        },
        {
            "title": "References",
            "content": "[1] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [2] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [3] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. ArXiv, abs/2409.02813, 2024. URL https://api.semanticscholar.org/CorpusID: 272397682. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Google. Gemini 2.5: Our most intelligent ai model. https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025/, March 2025. Accessed: 2025-04-18. [6] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [7] OpenAI. Openai o3 and o4-mini system card. https://openai.com/index/ o3-o4-mini-system-card/, April 2025. Accessed: 2025-04-18. [8] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. 2025. [9] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [10] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 10 [11] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [12] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [13] Jieyi Long. Large language model guided tree-of-thought. arXiv preprint arXiv:2305.08291, 2023. [14] Jinghan Zhang, Xiting Wang, Fengran Mo, Yeyang Zhou, Wanfu Gao, and Kunpeng arXiv preprint Entropy-based exploration conduction for multi-step reasoning. Liu. arXiv:2503.15848, 2025. [15] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vlrethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [16] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [17] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. [18] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [19] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [20] Darsh Shah, Peter Rushton, Somanshu Singla, Mohit Parmar, Kurt Smith, Yash Vanjani, Ashish Vaswani, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, et al. Rethinking reflection in pre-training. arXiv preprint arXiv:2504.04022, 2025. [21] Simon Herbert et al. The architecture of complexity. Proceedings of the American Philosophical Society, 106(6):467482, 1962. [22] Philip David Zelazo. Executive function: Reflection, iterative reprocessing, complexity, and the developing brain. Developmental Review, 38:5568, 2015. [23] Linda Flower and John Hayes. cognitive process theory of writing. College Composition & Communication, 32(4):365387, 1981. [24] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [25] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [26] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [27] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. 11 [28] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [29] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. [30] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [31] Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, and Rossella Arcucci. Beyond distillation: Pushing the limits of medical llm reasoning with minimalist rule-based rl. arXiv preprint arXiv:2505.17952, 2025. [32] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [33] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [34] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, et al. Deepscaler: Surpassing o1-preview with 1.5 model by scaling rl. Notion Blog, 2025. [35] Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [36] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [37] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [38] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [39] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [40] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, and Roy Ka-Wei Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. [41] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35: 25072521, 2022. [42] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. [43] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 12 [44] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. [45] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [46] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [47] Yifan Du, Zikang Liu, Yifan Li, Wayne Xin Zhao, Yuqi Huo, Bingning Wang, Weipeng Chen, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Virgo: preliminary exploration on reproducing o1-like mllm. arXiv preprint arXiv:2501.01904, 2025. [48] Hui Shen, Taiqiang Wu, Qi Han, Yunta Hsieh, Jizhou Wang, Yuyue Zhang, Yuxin Cheng, Zijian Hao, Yuansheng Ni, Xin Wang, et al. Phyx: Does your model have the\" wits\" for physical reasoning? arXiv preprint arXiv:2505.15929, 2025. [49] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [50] Anthropic. Claude 3.5 Sonnet Model Card Addendum. https://huggingface.co/ anthropic/claude-3-sonnet-20240229, 2024. Accessed: 2025-05-01. [51] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [52] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [53] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [54] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [55] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. [56] Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning, 2025. [57] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [58] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 13 [59] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95569567, 2023. [60] Trevor Standley, Ruohan Gao, Dawn Chen, Jiajun Wu, and Silvio Savarese. An extensible multi-modal multi-task object dataset with materials. In International Conference on Learning Representations, 2023. [61] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [62] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, et al. Efficient large language models: survey. arXiv preprint arXiv:2312.03863, 2023. [64] Hui Shen, Jingxuan Zhang, Boning Xiong, Rui Hu, Shoufa Chen, Zhongwei Wan, Xin Wang, Yu Zhang, Zixuan Gong, Guangyin Bao, et al. Efficient diffusion models: survey. arXiv preprint arXiv:2502.06805, 2025. [65] Yu Zhang, Jialei Zhou, Xinchen Li, Qi Zhang, Zhongwei Wan, Tianyu Wang, Duoqian Miao, Changwei Wang, and Longbing Cao. Enhancing text-to-image diffusion transformer via splittext conditioning. arXiv preprint arXiv:2505.19261, 2025. [66] Jing Xiong, Gongye Liu, Lun Huang, Chengyue Wu, Taiqiang Wu, Yao Mu, Yuan Yao, Hui Shen, Zhongwei Wan, Jinfa Huang, et al. Autoregressive models in vision: survey. arXiv preprint arXiv:2411.05902, 2024. [67] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024."
        },
        {
            "title": "A Limitation and Social Impacts",
            "content": "A.1 Limitation Our experiments primarily evaluated the effectiveness of SRPO on dense MLLMs at 7B and 32B scales, without conducting scaling experiments on Mixture-of-Experts (MoE) [62, 63] or diffusion LM [6466] architectures. Additionally, the reinforcement learning training data utilized in our experiments were selected exclusively from publicly available multimodal reasoning datasets, without exploration of larger-scale commercial reasoning datasets. Extending our method to MoE-based models and incorporating larger-scale RL training datasets remain promising avenues for future work, potentially leading to even broader improvements in multimodal reasoning capabilities. A.2 Social Impacts Our work offers clear positive contributions by significantly enhancing MLLMs capabilities in complex reasoning tasks. These advances can enable more accurate and reliable AI assistance in education, scientific discovery, and decision-making scenarios, ultimately contributing to broader accessibility of high-quality reasoning support. However, improved reasoning capabilities also carry potential risks, such as generating more convincing yet inaccurate or biased content if models reflect on misleading data. Consequently, careful curation of training datasets and explicit efforts in mitigating potential biases remain essential. Additionally, deploying advanced multimodal reasoning models without adequate safeguards might inadvertently reinforce existing societal inequalities if access to these advanced technologies remains restricted. Overall, responsible and transparent use of these enhanced multimodal reasoning frameworks is crucial to ensure positive societal outcomes."
        },
        {
            "title": "B Appendix",
            "content": "In this appendix, we provide supplementary details and extended analyses supporting the main findings presented in our paper. Specifically, we describe the datasets employed for self-reflection SFT and RL training (B.1), present detailed hyper-parameter settings used throughout our experiments (B.2), and provide explicit prompt templates for self-reflection SFT, RL training, and evaluation (B.3). Additionally, we illustrate training dynamics, including convergence trends and key metrics during the reinforcement learning phase (B.5). Finally, we offer qualitative analyses through representative samples generated during RL training and inference, demonstrating the effectiveness of our reflection-based strategies (B.4). (a) Self-reflection SFT data statistic (b) RL training data statistic Figure 6: Statistics of reasoning-type distribution in different stages of training. B.1 Training Dataset Self-reflection SFT Dataset. We primarily select samples from three established multimodal reasoning datasets: Mulberry Dataset (260K) [24]: multimodal reasoning dataset enriched through collective Monte Carlo tree search, specifically designed to enhance reflection and reasoning capabilities of multimodal LLMs. It features diverse reasoning problems requiring explicit cognitive processes. 15 MathV360K [40]: The dataset focuses on mathematical reasoning for multimodal LLMs. It systematically bootstraps multimodal reasoning by constructing high-quality mathematical reasoning prompts paired with visual contexts. LLaVA-CoT Dataset (100K) [25]: vision-language reasoning dataset explicitly designed for chain-of-thought (CoT) prompting. It consists of multimodal problems that encourage step-by-step logical reasoning aligned with visual inputs. From these datasets, we randomly sample 100K data points. For each sampled multimodal problem, we apply specialized CoT-generation template, feeding both the visual inputs and associated questions to two pretrained models: Qwen-2.5-VL-7B-Instruct and Qwen-2.5-VL-32B-Instruct [28], respectively, to generate corresponding reasoning steps. Subsequently, we use the DeepSeek-V3 [67] API to assess the quality of these generated reasoning paths, selecting high-quality subset of 10K samples containing approximately 30% correctly solved and 70% incorrectly solved reasoning paths. Next, leveraging these selected CoT samples and their associated ground truths, we utilize GPT-o4mini [7] with specifically designed self-reflection generation prompt. This approach yields concise, meaningful self-reflective feedback regarding the generated reasoning steps. Finally, we structure these samples into our self-reflection enhanced SFT dataset following the proposed Self-reflection SFT template. Self-reflection RL Dataset. We curate our reinforcement learning (RL) dataset by selectively sampling from several multimodal reasoning benchmarks, each featuring distinct reasoning characteristics and data modalities. These datasets include: ScienceQA [41]: Contains 21K multimodal science questions, encouraging explicit reasoning chains through visual contexts, textual explanations, and multiple-choice tasks. Geometric Math QA (GeoQA) [42]: Consists of 5K geometry-focused multimodal problems designed to evaluate numerical reasoning over geometric concepts. ChartQA [43]: Provides 9.6K questions for visual and logical reasoning based on diverse chart types such as bar, line, and pie charts. DVQA [44]: Includes 3.5K questions requiring comprehensive reasoning to interpret data visualizations effectively. AI2D [45]: Features 5K diagram-based science questions aimed at evaluating visual and conceptual understanding through diagrams. MATH [46]: Comprises 12.5K challenging mathematical problems across various difficulty levels, extensively utilized to measure mathematical reasoning capabilities. Virgo [47]: Offers around 10K multimodal reasoning examples intended to emulate the reasoning complexity of state-of-the-art models like OpenAI O1. R1-OneVision [11]: Contains 5K multimodal reasoning instances specifically designed for crossmodal reasoning and generalization. MMK12 [8]: Provides curated set of around 12K multimodal problems that involve rigorous rule-based reasoning across diverse domains. PhyX [48]: Comprises 3K carefully designed multimodal physics questions spanning thermodynamics, electromagnetism, mechanics, modern physics, optics, and acoustics, aimed at evaluating physical reasoning capabilities in realistic visual contexts. By systematically combining and sampling from these datasets, we obtain comprehensive selfreflection RL dataset containing high-quality multimodal reasoning instances for our experiments. B.2 Hyper-parameters In the reinforcement learning phase of SRPO, we highlight several critical hyper-parameters: we set both the rollout and training batch sizes to 128, generating 8 samples per prompt to ensure response diversity. Sampling is performed with temperature of 1.0. The learning rate is fixed at 1 106 using the Adam optimizer with parameter offloading enabled, and training is conducted using bf16 mixed-precision. We adopt group-normalized advantage estimation (specific to GRPO) to stabilize training and utilize the \"k3\" KL divergence estimator for controlled policy updates. Additionally, we freeze the visual encoder parameters during training, enable gradient checkpointing and flash attention for memory efficiency, and perform accuracy filtering (retaining samples with accuracy scores between 0.1 and 0.9) to maintain data quality throughout the RL process. B.3 Prompt Template Prompt Template for CoT Generation System: \"You are precise AI assistant and must strictly follow the following rules: First reason step-by-step, and wrap the thought process in <think> tags. The final answer must be wrapped in <answer> tags. Formatting requirements: Choice answers must be uppercase letters (A/B/C/D). Fill-in-the-blank answers should be digits. DO NOT EXPLAIN ANYTHING IN <answer>. You must provide both <think> and <answer>. Please strictly follow the formatting requirements and do not add any extra content! User: [type: \"text\", content: question, type: \"image_url\", image_url: image_url] Prompt Template for Self-Reflection Generation System: \"You are helpful math reasoning assistant. Think carefully. Output only JSON.\" User: You are an expert visual reasoning assistant. Your task is to reflect on the quality of chain-of-thought (CoT) reasoning given for visual question. The goal is to improve the CoT by identifying weaknesses and offering suggestions for refinement. Please follow this structure strictly: 1. \"reflection\": Provide detailed critique of the original CoT, pointing out: Logical flaws or inconsistencies Missing assumptions or information Any correct reasoning that could be made clearer or more robust Suggestions for improving the reasoning process Only return valid JSON object with \"reflection\" field. - Input - Question: {query} Image: {image_url} Original Chain of Thought: {cot} Predicted Answer (Based on CoT): {answer} Correct Answer (ground truth): {ground truth} 17 Prompt Template for Self-reflection SFT System: You are reasoning expert. Given an image and question, please generate two rounds of step-by-step reasoning: First, provide your initial chain of thought and answer. Then reflect on it. Finally, based on your reflection, give your final reasoning and answer. User: { \"messages\": [ { \"role\": \"system\", \"content\": \"You are reasoning expert. Given an image and question, please generate two rounds of step-by-step reasoning: First, provide your initial chain of thought and answer. Then reflect on it, and finally, based on your reflection, give your final reasoning and answer.\" \"role\": \"user\", \"content\": \"Question: <question>{query}</question>nImage: <image>\" \"role\": \"assistant\", \"content\": \"<think>{cot}</think>n<answer>{answer}</answer> n<reflection>{reflection}</reflection> n<answer>{ground_truth}</answer>\" }, { }, { } ], \"images\": [\"{image_url}\"] } Prompt Template for Self-reflection RL System: Solve the users question step by step. First, think about the reasoning process internally and write it inside <think>...</think> tags. Then, provide the first answer in LaTeX format wrapped with $...$, and the final result must be enclosed with boxed{}. Wrap this answer inside <answer>...</answer> tags. After that, perform critical self-reflection on the previous reasoning and answer, writing the reflection inside <reflection>...</reflection> tags. Then, based on the reflection, generate new reasoning process and new answer: The new reasoning is again placed inside <think>...</think>. The new answer is written inside <answer>...</answer> and uses LaTeX $...$ with boxed{} for the final output. Make sure both reasoning steps are clear and detailed. Even if the final answer does not change, the second reasoning must incorporate improvements based on the reflection. Format Example: <think> Since $1+1=2$, so the answer is $2$. </think> <answer> The answer is $boxed{2}$. </answer> <reflection> The reasoning is correct but too brief; could have explained the addition more explicitly. </reflection> <think> Adding $1$ and $1$ together results in $2$ because $1$ plus $1$ means taking one and adding another one, leading to $2$. </think> <answer> The answer is $boxed{2}$. </answer> All reasoning, answer, and reflection steps must be included without omission and in the exact sequence: <think>...</think> <answer>...</answer> <reflection>...</reflection> <think>...</think> <answer>...</answer>. 18 B.4 Generated Samples Analysis In this section, we present generated samples during RL training (Sample 1 and Sample 2) and from real test cases (Sample 3 and Sample 4). In the training examples (Sample 1 and 2), we observe that Qwen-2.5-VL-7B, under our proposed SRPO framework, exhibits the ability to perform self-reflection and subsequently revise its reasoning steps. Similarly, in downstream evaluation tasks, the model demonstrates reflective adjustments when answering incorrectly, and when answering correctly, it further refines and simplifies its reasoning through self-reflection. Generated Samples in RL Training of SRPO (Sample 1) Question: Given that the measure of angle 1 is 123 degrees in the figure, what is the measure of angle 11? (B) 67 (C) 113 (D) Ground truth: (D) 123 Choices: (A) 57 <think> Since angle 1 and angle 11 are corresponding angles formed by transversal intersecting two parallel lines, they are congruent. Therefore, the measure of angle 11 is the same as the measure of angle 1. </think> <Answer> The answer is 57 (Incorrect due to flawed assumptions) </Answer> <reflection> The original reasoning has significant flaw: it incorrectly identifies angle 1 and angle 11 as corresponding angles formed by transversal intersecting parallel lines. This assumption is unfounded given the image description, which does not specify that the lines are parallel. Moreover, the reasoning does not consider the possibility that angle 11 might be obtained from different relationship, such as vertical angle or an alternate interior angle formed by non-parallel configuration. To improve the reasoning, one should: 1) analyze the given visual elements more carefully rather than relying on assumptions about parallel lines; 2) re-evaluate the geometric relationships present in the figure, including the possibility of vertical angles or other angle pairs that could be equal; and 3) include intermediate steps or calculations that confirm why the measure of angle 11 should be different from angle 1. Clarifying these points would lead to more robust and accurate conclusion. </reflection> <think> (Generate the right answer after effective self-reflection) The correct approach here would be to notice that if angle 1 and angle 11 were somehow related in way that the sum of their measures was specific constant (for example, if they were supplementary or complementary in context), we could deduce the measure of angle 11 by subtracting from 180 degrees (since supplementary angles add up to 180 degrees). But since the problem does not specify this relationship, and considering the answer choices, the most logical deduction is that angle 11 is the same as angle 1 due to specific geometric configuration not detailed in the problem statement.Thus, the measure of angle 11 is 123. </think> <Answer> The answer is 123 </Answer> 19 Generated Samples in RL Training of SRPO (Sample 2) Question: Based on the upper image, what task is the robotic arm most likely performing? Ground truth: (D) Options: (A) Sorting playing cards based on predefined algorithm. (B) Performing magic trick with the cards. (C) Playing card game against human opponent. (D) Conducting an experiment on object recognition. <think> The image shows robotic arm with camera mounted on it, which suggests that it is involved in task that requires visual input and processing. The presence of playing cards on the table and the arms position above them indicate that the robot might be interacting with the cards in some way. Given the context of the setup, the most logical task for the robotic arm would be to sort the playing cards based on predefined algorithm, as this is common application for robotic arms equipped with cameras for visual recognition. </think> <answer> The answer is . (Incorrect) </answer> <reflection> The original reasoning is overall correct in observing the camera and the cards, but it overlooks deeper experimental context. The red and yellow point clouds and structured laser-like patterns shown in the lower part of the image are indicative of 3D perception or object localization, which are typical in object recognition experiments. Additionally, theres no evidence of sorting mechanisms or card repositioning. The reflection should thus account for the experimental nature of the setup, making option more consistent with the evidence provided. </reflection> <think> (Generate the right answer after effective self-reflection) Lets rethink, the image shows robotic arm with camera, indicating task that involves visual data collection or processing. The presence of playing cards on the table suggests an interaction with objects, and the alignment of the dots and lines might represent data points collected for object recognition or spatial analysis. Given these observations, the most likely task for the robotic arm is related to object recognition, where the camera would be used to identify and analyze the cards features. </think> <answer> The answer is . </answer> 20 Generated Samples in Real Test cases of GRPO (Sample 3) Question: farmer wants to build fence around the entire perimeter of his land, as shown in the diagram. The fencing costs 37 per metre. The lengths of EF and CD are metres and metres respectively. At 37 per metre, how much will it cost him to build the fence along the entire perimeter? Ground truth: 777 <think> To determine the cost of building the fence around the entire perimeter of the land, we first need to find the total perimeter of the land. The perimeter is the sum of the lengths of all the sides of the shapes forming the land. Given: - The height of the rectangle ABGF is 5 m. - The width of the rectangle ABGF is 1 m. - The height of the rectangle BCDE is 7 m. - The width of the rectangle BCDE is 2 m. - The length of EF is m. - The length of CD is m. The perimeter of the land is the sum of the lengths of all the sides: Perimeter = AB + BG + GE + EF + + DC + CE + EB From the diagram: Fencing cost is: Perimeter = 5 + 1 + 2 + + 1 + 2 + 3 + 1 = 15 + 555 + 37x (Incorrect) </think> <answer> The answer is 555 + 37x . </answer> <reflection> The initial reasoning misidentified the structure of the perimeter by incorrectly including internal segments such as BG, GE, and EB. These are auxiliary construction lines used solely to form right triangles for computing the diagonal lengths EF and CD, not actual parts of the outer boundary of the land that require fencing. This led to an overestimation of the total perimeter. To correct the reasoning, one must focus strictly on the visual geometry and trace only the external contour of the landspecifically, the true fencing path comprises segments AB, AF , EF , ED, DC, and CA. The diagonal lengths EF and CD should be computed using the Pythagorean theorem based on right triangle constructions derived from the diagram. This corrected understanding yields the true perimeter and thus the accurate total fencing cost. </reflection> <think> Tracing only the external boundary gives segments (Generate the right answer after effective self-reflection) AB = 5, AF = 1, = x, ED = 2, DC = y, CB = 2. Therefore the total perimeter is 5 + 1 + + 2 + + 2 = 10 + (x + y). From the diagrams length proportions (and the single 11 label spanning the two slanted edges), we see directly that Hence the full perimeter is 10 + 11 = 21 m, and the fencing cost is + = 11. </think> <answer> The answer is 777 . </answer> 37 21 = 777 . 21 Generated Samples in Real Test cases of GRPO (Sample 4) Question: M, W, X, Y, and are main-group elements in the same period. The number of valence electrons in is three times the number of electrons in the penultimate shell of W. The element is key player in inorganic non-metal materials. The structure of the compound formed by M, W, X, Y, and is shown. Which of the following statements is incorrect? Choices: A. Elemental is usually stored under kerosene. B. The oxide of dissolves in aqueous solution of Ys hydride. C. Every atom in the anion shown satisfies an octet. D. The boiling points of the simplest hydrides of X, Y, follow > > Z. <think> Determine the period: all elements are in period 3. Ws penultimate shell contains 8 electrons, so has 3 8 = 24 valence electrons; therefore, = Cl. Using known oxidation states: = Na, = Mg, = Al, = P. is most likely Si. Now evaluate the options: A. Sodium is not stored under kerosene but under oil; therefore, this statement is incorrect. B, C, and are chemically consistent. Therefore, the incorrect statement is . </think> <answer> The answer is . </answer> <reflection> Reflect on the redundancy of the prior reasoning and generate simplified version. The initial reasoning is correct but can be simplified. Instead of step-by-step element identification, we can streamline the logic into three clear steps: (1) From Ws inner shell (8 electrons), deduce that has 24 valence electrons, hence = Cl. (2) Identify = Na, = Mg, = Al, = from standard main-group patterns. (3) Eliminate each option: only fails (Na is not stored under kerosene). This improved structure yields the same answer more directly. </reflection> <think> From periodic positions and common valences: = Na, = Mg, = Al, = P. Only statement is chemically incorrect. Answer: . </think> <answer> The answer is . </answer> B.5 Training Dynamics We provide additional visualizations of RL training curves, including total response length, correct and incorrect response lengths, ratio clip lower, policy loss, and accuracy reward. Several key observations can be made: First, due to the explicit emphasis on self-reflection during training, SRPO consistently generates longer total responses and exhibits notably greater growth in response length compared to GRPO and SRPO without self-reflection. This is attributable to the models active engagement in self-reflection and subsequent correction of prior reasoning steps. Additionally, SRPO consistently achieves higher accuracy reward values than baselines, confirming that reinforcement of reflective reasoning effectively enhances the models reasoning capabilities. Furthermore, from the ratio clip lower and policy loss curves, we observe that SRPOwhether employing self-reflection in 22 both SFT and RL phases or solely in the RL phasemaintains stable clip lower values consistently below 0.005. This indicates that the integration of self-reflection contributes to stable policy updates with moderate gradient adjustments throughout training. Figure 7: More training dynamics of SRPO, GRPO, and SRPO w/o Self-reflection SFT."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "Case Western Reserve University",
        "Duke University",
        "Imperial College London",
        "Kean University Minnesota",
        "Nanjing University",
        "The Ohio State University",
        "The University of Hong Kong",
        "Tongji University",
        "University of Michigan",
        "University of Southern California"
    ]
}