{
    "paper_title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs",
    "authors": [
        "Mikhail Seleznyov",
        "Mikhail Chaichuk",
        "Gleb Ershov",
        "Alexander Panchenko",
        "Elena Tutubalina",
        "Oleg Somov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: https://github.com/AIRI-Institute/when-punctuation-matters."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 3 8 3 1 1 . 8 0 5 2 : r When Punctuation Matters: Large-Scale Comparison of Prompt Robustness Methods for LLMs Mikhail Seleznyov1,2, Mikhail Chaichuk1,5, Gleb Ershov 3, Alexander Panchenko1,2, Elena Tutubalina1,6,7, Oleg Somov1,4 1AIRI, 2Skoltech, 3Yandex, 4MIPT 5HSE University, 6Sber AI, 7ISP RAS Research Center for Trusted AI Correspondence: seleznev@airi.net, tutubalina@airi.net, somov@airi.net"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: https://github.com/AIRI-Institute/ when-punctuation-matters."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) today excel across wide range of tasks in both in-context learning (ICL) and supervised fine-tuning (SFT) paradigms (Brown et al., 2020; Gao et al., 2021; Dong et al., 2024; Le Scao et al., 2023; Yang et al., 2024a; Wu et al., 2024; Mosbach et al., 2023; Yang et al., 2024b; Chen et al., 2024). However, critical yet often overlooked challenge is the high sensitivity of LLMs to prompt formatting. Many large-scale task-rich benchmarks rely on single instruction format to evaluate all language models on wide-range of tasks, implicitly assuming that performance is independent of prompt format (Hendrycks et al., 2020; Srivastava et al., 2023). Recent work shows that even semantically neutral variations in prompt structure can lead to substantial changes in model predictions, 1 W R I L E W R LLM EVALUATION LANDSCAPE Many methods, tasks and models Under-explored Single-method, few tasks or few models LOW HIGH PROMPT DIVERSITY Figure 1: Most existing robustness methods are evaluated in isolation and in disparate settings, disallowing apples-to-apples comparison. Our work targets the under-explored upper-right quadrant by evaluating multiple prompt robustness techniques across wide range of prompt formats, LLM families, learning paradigms, and distribution shifts under unified framework. often exceeding the variability introduced by model architecture or inference method (Voronov et al., 2024a; Mizrahi et al., 2023). Prompt format (e.g. spacing, capitalization, punctuation) can dramatically influence performance, leading to inconsistent or unreliable outputs (Zhao et al., 2021; Min et al., 2022). This phenomenon, known as prompt sensitivity, can be mitigated using specialized robustness methods. number of robustness techniques have been proposed to address this issue, including Template Ensembling (Voronov et al., 2024b), SensitivityAware Decoding (Lu et al., 2024), Batch Calibration (Zhou et al., 2024), and Consistency Learning (Qiang et al., 2024b). However, these methods have primarily been evaluated in isolation, making it difficult for practitioners to assess their relative strengths or determine which method is best suited to given scenario. Our work addresses this gap through comprehensive, systematic evaluation of prompt robustness techniques under unified experimental framework. Specifically, we benchmark four widely cited robustness methods against standard few-shot prompting and fine-tuning with prompt format augmentation as baselines. We conduct experiments using representative subset of 52 tasks from the well-known Natural Instructions dataset, covering domains such as mathematics, logic, and text comprehension. As backbone models, we evaluate three modern LLM families GEMMA (Gemma Team, 2024), LLAMA (Dubey et al., 2024), and QWEN (Qwen et al., 2025) with sizes from 1.5B to 9B parameters. We additionally include closed-source models to study format sensitivity at scale. Within our framework, we answer the following research questions: RQ1: How do existing robustness methods compare in effectiveness across various models? RQ2: How distribution shifts affect the effectiveness of SFT-based and ICL-based methods? RQ3: How does greedy decoding affect robustness compared to choosing highest-probability answer option? RQ4: How sensitive are frontier models to format perturbations, and what methods can be applied in black-box setting to improve their robustness? To the best of our knowledge, this is the first study to offer side-by-side comparison of multiple prompt robustness methods under unified, largescale evaluation protocol spanning diverse prompt formats, model families, learning paradigms, distribution shifts and inference strategies. By bridging previously disconnected lines of work, our findings provide actionable insights for both practitioners and researchers interested in building more stable and reliable LLM-based systems. We also release our code to encourage systematic evaluation in the field of prompt sensitivity mitigation."
        },
        {
            "title": "2 Related Work",
            "content": "Recent work has highlighted the sensitivity of language models to subtle prompt variations, but current research remains fragmented (Zhuo et al., 2024; Pei et al., 2025). Adversarial-focused studies (Zhu et al., 2024; Zou et al., 2023) expose vulnerabilities to malicious or perturbed prompts, emphasizing safety but targeting directed threat models rather than benign formatting inconsistencies. Other works propose robustness-enhancing methods such as Consistency Learning (Qiang et al., 2024b), Batch Calibration (Zhou et al., 2024), and Template Ensembles (Voronov et al., 2024a), which improve stability either during training or inference. However, these approaches are evaluated in isolation, making it difficult to assess their relative effectiveness. Complementary studies (Lu et al., 2024; Zhao et al., 2021; Sclar et al., 2024) analyze prompt components and formatting artifacts, showing that even innocuous design choices (e.g., whitespace, punctuation) can introduce large performance shifts. This further underscores the need for unified, standardized evaluation protocols. In summary, while prior research has addressed different aspects of prompt sensitivity, there is lack of systematic, comparative evaluation across tasks, models, and learning paradigms. Our work fills this gap by benchmarking four robustness methods under unified framework across 52 diverse tasks, multiple LLM families and distribution shift scenarios, resulting in actionable takeaways for practitioners."
        },
        {
            "title": "3 Experimental Setup",
            "content": "To answer our research questions, we use subset of Natural Instructions with parametrized set of formats (Section 3.1) and implement the methods from Section 3.2. We evaluate performance and robustness using the metrics defined in Section 3.3. 3.1 Data & Format We use subset of 52 tasks from Natural Instructions (Wang et al., 2022) with diverse humanwritten formats and instructions, comprising 19 multiple-choice tasks and 33 classification tasks with 2, 3 or 4 answer options. Given that there are more than 1600 tasks we select relevant and socially impact tasks following Sclar et al. (2024) task selection criteria (refer to Appendix for details). Resulting tasks cover math and logic problems, text comprehension, detection of harassment and racial stereotypes. To evaluate the performance, we use subset of 1000 random examples from each task. Prompt format. We consider 6 types of format components, following Sclar et al. (2024). They are listed in Table 1. For each component there are between 4 and 16 possible values. To construct format, we select specific value of each component. For example, default 2 Descriptor transformation .title() .uppercase() .lowercase() Separator Space Text & option separator Option item style Option item wrapper : - n ; t A, B, C, ... 1, 2, 3, ... I, II, III, ... {}) {}. [{}] Table 1: Format components, with some example values. Descriptor transformation correspond to Python command making first character upper case (title), all letters upper case (uppercase) or all letters lower case (lowercase). For option item wrapper, {} is used as placeholder for option item (e.g. or 1). prompt for task could be structured as following: question:{}A){}B){}answer:{}, where {} denotes the placeholders for the task instruction and multiple-choice answer options (following (Wang et al., 2022) formatting). Then choosing the first values from first row of Table 1 to modify original prompt design results in Question: {} A) {} B) {} Answer: {} whereas taking values from second row forms another prompt design: QUESTION- {}n1.t{}n2.t{}nANSWER- {} For some tasks there are no multiple choice options in this case the format is defined only by descriptor transformation, separator and space. Complete list of format components is available in Appendix G. 3.2 Methods We consider five representative approaches for improving robustness to prompt formatting. These span both ICL and SFT paradigms. Below, we briefly describe each method. Few-shot (FS). As baseline, we use standard 2-shot prompting strategy. Since the selection and order of demonstration examples can significantly influence results (Lu et al., 2022), we fix the incontext examples and their order across all models and test samples. Demonstration examples are also formatted in the same way as the test sample, hinting to the model that formatting should not influence the prediction. Batch Calibration (BC). Batch Calibration (Zhou et al., 2024) is post-hoc correction technique. It estimates contextual bias across batch and adjusts predicted log-probabilities by subtracting the bias. While simple and efficient, it is limited to classification tasks. Template Ensembles (TE). Template Ensembling (Voronov et al., 2024b) improves robustness by averaging predicted class probabilities across prompt formats. This reduces variance caused by formatting changes, but increases inference cost linearly with . Sensitivity-Aware Decoding (SAD). This approach is inspired by Lu et al. (2024). It penalizes predictions that are sensitive to synthetic input perturbations. In our implementation, we use random token substitutions to estimate sensitivity. This approach helps to stabilize outputs but requires multiple forward passes per input. LoRA with format augmentations (LoRA). We apply parameter-efficient fine-tuning (PEFT) using LoRA on an instruction-following dataset augmented with formatting variations. This method exposes the model to diverse prompt styles during training with the aim of mitigating spurious correlations between answers and format components. LoRA with consistency loss (LoRA-JS). Following Qiang et al. (2024a), we add JensenShannon consistency loss between outputs of different prompt variants, encouraging format-invariant predictions. The total loss combines standard crossentropy with divergence term. Full implementation details for each method are provided in Appendices B, C. 3.3 Metrics & Inference Approach Evaluation Metrics. To evaluate model performance, we use accuracy as the primary metric. To assess sensitivity to prompt formatting, we report two measures: spread and standard deviation. Spread is defined as the difference between the maximum and minimum accuracy across set of prompt formats (Sclar et al., 2024), providing simple measure of output variability due to prompt variation. We also consider class-imbalance set3 Figure 2: Comparing format sensitivity mitigation methods in terms of their effect on accuracy and standard deviation over prompt formats. To aggregate accuracy, we first compute median accuracy over formats for each task, and then average over 52 tasks. Error bars are 2 (standard deviation over formats, averaged across tasks). ting. Since accuracy is often misleading on imbalanced tasks, we report Matthews correlation coefficient (MCC). We choose MCC instead of F1 score since the latter puts emphasis on the positive class and may change dramatically after permutation of classes. MCC treats classes symmetrically and accounts for all four confusion matrix components, including true negatives. Since our tasks usually do not have distinguished positive class, MCC is better suited for our evaluations. Inference Strategies. To obtain answers from the language model in multiple-choice and classification tasks, we use two common inference strategies: greedy decoding and probability ranking. Greedy decoding generates the answer token-by-token, selecting the most likely token at each step. The result string is normalized and evaluated to gold answer with exact match. In contrast, probability ranking computes the probability of each answer option, and selects the highest-ranked one."
        },
        {
            "title": "4 Results",
            "content": "Our experiments address the four research questions outlined in Section 1: methods comparison in default setting (RQ1, Section 4.1), effect of distribution shifts on fine-tuning and ICL-based methods (RQ2, Section 4.2), effect of inference strategy (RQ3, Section 4.3) and evaluations of frontier models (RQ4, Section 4.4). 4.1 Robustness Method Comparison RQ1: How existing robustness methods compare in effectiveness across different LLM families and sizes? To answer RQ1, we apply all methods under the default conditions (without distribution shift) to evaluate their impact on accuracy and robustness. We assess accuracy to check whether robustness methods negatively affect performance. Figure 2 plots accuracy of each method, averaged across 52 tasks, along with standard deviation over formats1. To compare methods effectiveness in improving robustness, we use the following procedure. For fixed model M0 (e.g. Llama 3.1 8B) and each task we estimate the spreads of baseline few-shot approach and the competing method over 10 formats, where {BC, TE, SAD, LoRA}. Then, 1We also tested LoRA with consistency loss. However, the results were less favorable compared to the other methods, especially LoRA with format augmentations. Some of these findings are available in Appendix A. 4 Method BC FS LoRA SAD TE 1.7 Default Unbalanced 3.2+0.6 2.70.2 1.7 2.9 2.6 4.3 4.00.3 3.30.2 3.5 Table 2: Rankings of methods across models by Matthews correlation coefficient (1 is best). Rankings are averaged across models and tasks. findings of Voronov et al. (2024a), which note that single suboptimal template may make the ensemble perform noticeably worse. Together, it suggests that logit averaging is sometimes brittle strategy, sensitive to outliers. LoRA with augmentations enhances accuracy, but struggles to consistently improve robustness. On Figure 2 we can see that LoRA with augmentations achieves much higher average accuracy compared to ICL-based approaches. This is expected, since LoRA is the only SFT-based method on the plot, and has access to training labels. Perhaps more surprisingly, augmentations have almost no impact on robustness: Figure 3 shows that LoRA improves spread compared to few-shot only on single model out of 8, with 6 ties and 1 loss. Takeaways: 1. In absence of distribution shifts, calibrationbased approach shows promise in improving robustness to prompt formats due to its ability to significantly reduce spread, positive effect on accuracy and low overhead. 2. While naive parameter-efficient finetuning with augmentations significantly improves accuracy, it turns ineffective in mitigating sensitivity to format changes. 3. Probability averaging strategy used in Template Ensemble helps to reduce spread, but may suffer from sensitivity to especially poorperforming formats. 4. Impact of Distribution Shifts RQ2: How do distribution shifts affect the effectiveness of SFT-based and ICL-based methods? As we see in Section 4.1, Batch Calibration helps to improve robustness while LoRA significantly stands out in terms of accuracy. To answer RQ2, we zoom in and inspect these methods in more detail to understand their limitations. Covariate shift (class imbalance). In Batch Calibration, predicted probabilities of each class are Figure 3: Comparing format sensitivity mitigation methods against regular few-shot in terms of spread on 8 language models. Method wins against few-shot for given model if it has statistically significantly lower spread than few-shot on 52 tasks. we consider differences SpreadDiffsM0,X = (cid:8)spread(FS)t,M0 spread(X)t,M0 (cid:9) (1) T, = 52, where is our selected 52 tasks from Natural Instructions. Finally, we run Students t-test with H0: mean of SpreadDiffsM0,X is equal to zero. If H0 is rejected, the method with lower mean spread is considered more robust on model M0. Otherwise, ties with few-shot. Such tests are run for each of 8 open-source models we evaluate, and the results are shown in Figure 3. Batch Calibration improves both accuracy and robustness across the board. From Figure 2 we can see that Batch Calibration achieves higher average accuracy compared to few-shot for all 8 opensource models. Meanwhile, Figure 3 shows that BC delivers statistically significant reduction of spread for 6/8 models. Since calibration methods do not require training data and have near-zero inference time overhead, these strong results put Batch Calibration as clear leader in terms of format robustness enhancement in absence of distribution shifts. Template Ensembles improve robustness at cost of reducing accuracy. Ensembling method proposed by Voronov et al. (2024a) also reduces spread, with statistically significant reductions for 4/8 models. However, it results in lower accuracy compared to few-shot baseline. To investigate the causes of the drop in performance, we inspected predictions of individual ensemble members. It turned out that for one format in the ensemble the accuracy sometimes underperforms, affecting average probabilities. This aligns with the original 5 Figure 4: Median Matthews Correlation of robustness methods in uniform vs. unbalanced settings for LLaMA 3.1 8B, Qwen 2.5 7B, and Gemma 2 9B. Red values indicate the drop in performance under the unbalanced setting relative to the uniform case. confirms this finding: Batch Calibration exhibits the largest change in average ranking. LoRA-finetuned models also degrade, as they were trained assuming uniform class distribution and thus are also subject to covariate shift. Compositional and cross-domain shifts. To evaluate the robustness of the LoRA method to distribution shifts, we consider two scenarios: compositional and cross-domain. Compositional shift. Inspired by the notion of systematic compositionality (Hupkes et al., 2020), this setting tests the models ability to generalize by recombining known elements in novel ways. In default scenario in Section 4.1, train and test formats are sampled uniformly. However, under compositional shift test formats contain new combinations of previously seen format components. An illustration of compositional train/test format split is shown in Figure 5. Cross-domain shift. To evaluate robustness to domain changes, this setting uses training data from an external dataset (see Appendix C). The prompt formats remain uniformly distributed during both training and testing like in Section 4.1. This setup probes the models and methods ability to disentangle semantics from format and generalize beyond the training domain. Analysis. Compositional shift with respect to formats does not affect accuracy and robustness much. We hypothesize this is due to the fact that LoRA with augmentations does not consistently improve robustness even in default scenario considered in RQ1, Section 4.1, and the complexity of composiFigure 5: Without distribution shift (left), train and test formats are sampled uniformly. Under the compositional distribution shift (right), the test set contains novel combinations of known components, requiring systematic generalization. Cross () stands for train samples, circle () for test. adjusted by subtracting mean probability of this class over batch. Naturally, predicted probabilities for classes that occur more often are adjusted more. They in turn are less often selected by argmax, which leads to more uniform predictive distribution. Thus, Batch Calibration implicitly assumes more uniform class distribution compared to baseline few-shot approach. While reasonable in balanced tasks, this assumption may lead to calibration errors under class imbalance. To investigate this, we construct an artificial imbalanced dataset by downsampling each of our 52 tasks such that the most frequent class constitutes 90% of the examples, with the remaining classes evenly splitting the remaining 10%. In Figure 4, we observe that all methods are affected by the unbalanced setting, but Batch Calibration suffers the most due to the models inductive bias toward uniform class distribution. Table 6 4.3 Greedy Decoding vs. Probability Ranking RQ3: How does greedy decoding affect robustness compared to choosing highest-probability answer option? To answer RQ3, we run experiments with two inference strategies, described in Section 3.3. Table 3 demonstrates, that generation is always less robust to format choice, with Gemma models exhibiting especially large instability. Generation approach is widely used in practical applications (chat-bots, API calls), so the problem of format sensitivity can be even more acute there. Takeaway: greedy decoding exacerbates models format sensitivity. When possible, opt for probability ranking. 4.4 Frontier Models & Format Robustness We split RQ4 into two questions. RQ4-1: How sensitive are frontier models to format perturbations? To answer the first part of RQ4, we perform experiments with two frontier models, GPT-4.1 and DeepSeek V3. We evaluate them on subset of 10 out of 52 tasks due to budget limitations. The results are presented in Table 4. We can see that large closed-source models show much better robustness. While DeepSeek V3 significantly outperforms GPT-4.1 in terms of accuracy, it is more sensitive to format changes. However, Figure 7 shows that on individual tasks, even frontier models might have spread of 8-10 accuracy points. RQ4-2: What methods can be applied in blackbox setting to improve their robustness? To answer the second part of RQ4, we need to consider each method assumptions. Batch Calibration, Sensitivity-Aware Decoding and Template Ensembles require logit access, which is not always available for closed source models. With SFT-based methods the problem is that the user usually has no control over what exact method and hyperparameters are used, even if company provides fine-tuning as service. For broadly applicable approach, we consider an adaptation of Template Ensembles which uses majority voting instead of probability averaging. Figure 7 confirms that this strategy effectively reduces spread, and in Table 4 we even see slight performance improvement, contrary to results of Template Ensembles in Section 4.1. We attribute this to the fact that mode, utilized in majority voting, is substantially more robust to outlier formats Figure 6: LoRA method under distribution shifts. To aggregate accuracy, we first compute median accuracy over formats for each task, and then average over 52 tasks. Error bars are 2 (standard deviation over formats, averaged over tasks). Inference Strategy Greedy Decoding Probability Ranking 0.48 0.28 0.55 0.32 0.58 0.06 0.66 0.03 Gemma 2 2B Gemma 2 9B Llama 3.1 8B Llama 3.2 1B Llama 3.2 3B 0.63 0.11 0.46 0.11 0.56 0.13 Qwen 2.5 1.5B 0.49 0.12 0.59 0.12 Qwen 2.5 3B 0.63 0.14 Qwen 2.5 7B 0.63 0.06 0.46 0.09 0.57 0.09 0.55 0.08 0.61 0.08 0.67 0.04 Table 3: Comparison of inference strategies: greedy decoding vs. probability ranking. To aggregate accuracy, we first compute median accuracy and standard deviation over formats for each task, and then average over 52 tasks. We report averaged median accuracy 2 averaged std. Higher standard deviation is in bold. tional shift remains hidden. Cross-domain transfer is challenging setup. While it is possible that with another configuration of training data and hyperparameters it might perform better, in our experiments cross-domain fine-tuning with augmentations decreases accuracy below the few-shot baseline. Takeaways: 1. Batch Calibration implicitly assumes more uniform prior on classes compared to baseline few-shot approach. This inductive bias backfires when the class distribution is skewed. 2. Cross-domain experiments confirm that high accuracy achieved by LoRA approach substantially relies on training dataset. (a) DeepSeek V3 (b) GPT-4.1 Figure 7: Spreads for selected tasks from Natural Instructions for two frontier models. Even at this scale, for some tasks spread still might reach 8-10 accuracy points. Using modified version of Template Ensembles with majority voting instead of probability averaging, we are able to reduce the spread in 19/20 cases, in 9 of which the reduction is at least 44%. Method Model Accuracy Std accuracy Spread Few-shot Llama 3.1 8B Qwen 2.5 7B DeepSeek V3 0324 GPT-4.1 Template Ensembles (majority voting) DeepSeek V3 0324 GPT-4. 0.563 0.605 0.741 0.624 0.742 0.625 0.052 0.058 0.015 0.010 0.009 0.005 0.161 0.190 0.045 0.032 0.028 0. Table 4: Evaluation of frontier models on subset of 10 out of 52 tasks. For reference, we also include couple of open-source models. To aggregate accuracy, we take median over formats and average over tasks. Standard deviation and spread are first computed over formats for each task individually and then averaged. than the mean, used in original version. Takeaways: 1. Frontier models are substantially more robust compared to small open-source models, suggesting that scaling improves robustness. 2. Occasionally, there are still cases where spread can reach 8-10 accuracy points. To deal with them, Template Ensembles with majority voting might be used."
        },
        {
            "title": "5 Conclusion",
            "content": "To the best of our knowledge, we have conducted the first comprehensive comparison of existing prompt sensitivity mitigation methods across multiple model families, sizes and distribution shifts. We provide actionable insights for practitioners. For example, excessive fragility of calibrationbased methods to class imbalance underscores the consequences of implicit assumptions, and hints that reliable estimate of prior is important. Meanwhile, ineffectiveness of light supervised finetuning with augmentations at improving robustness suggests that more research is needed to develop strong baseline in this paradigm. Finally, our experiments on frontier models confirm that scale is positively correlated with robustness. Still, on some tasks even large models exhibit 8-10 accuracy point differences solely due to format changes. Version of Template Ensembles with majority voting helps to mitigate this instability. We also release our code to facilitate research aimed to address the problem of format sensitivity."
        },
        {
            "title": "Limitations",
            "content": "Our study provides deep insights into classification and multiple-choice tasks, leaving more complex settings like text generation or multi-step reasoning out of the scope. Nonetheless, we underline that models exhibits substantial instability even in this simple setting. Some of considered robustness methods are harder to apply to frontier models. Batch Calibration, Sensitivity-Aware decoding and Template Ensembles require access to logits, which are sometimes unavailable. Finetuning approaches might be expensive at large scale. Additionally, when using finetuning API, users usually have limited control over the finetuning procedure."
        },
        {
            "title": "Ethics",
            "content": "The models and datasets used in this study are publicly available for research purposes, with licenses detailed in Appendix F. All experiments were performed on NVIDIA A100 80GB GPUs. Each finetuning or evaluation run was conducted on single GPU, and took between 1 and 24 hours, depending on the model size and methods efficiency. To optimize computation, we utilized up to 46 GPUs in parallel. In total, the experiments took approximately 15,000 GPU-hours. Our PyTorch/Hugging Face code will be released alongside the paper, and we expect no direct social or ethical concerns arising from this work. Use of AI Assistants We utilize Grammarly to enhance and proofread the text of this paper, correcting grammatical, spelling, and stylistic errors, as well as rephrasing sentences. Consequently, certain sections of our publication may be identified as AI-generated, AI-edited, or combination of human and AI contributions. We also used DeepSeek V3, Claude Sonnet 3.5 and ChatGPT to improve text fluency and implement some of the code for results visualization."
        },
        {
            "title": "References",
            "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. 1 Yunmo Chen, Tongfei Chen, Harsh Jhamtani, Patrick Xia, Richard Shin, Jason Eisner, and Benjamin Van Durme. 2024. Learning to retrieve iteratively for in-context learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 71567168. Association for Computational Linguistics. 1 Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, and Zhifang Sui. 2024. survey on in-context learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11071128, Miami, Florida, USA. Association for Computational Linguistics. 1 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. 2 Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 38163830, Online. Association for Computational Linguistics. Gemma Team. 2024. Gemma. 2 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. 1 Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. 2020. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67:757795. 6 Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: 176bparameter open-access multilingual language model. 1 9 Sheng Lu, Hendrik Schuff, and Iryna Gurevych. 2024. How are prompts different in terms of sensitivity? In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 58335856, Mexico City, Mexico. Association for Computational Linguistics. 1, 2, 3, 11, Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80868098, Dublin, Ireland. Association for Computational Linguistics. 3 Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53165330, Dublin, Ireland. Association for Computational Linguistics. 1 Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. 2023. State of what art? call for multi-prompt llm evaluation. arXiv preprint arXiv:2401.00595. 1 Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot fine-tuning vs. in-context learning: fair comparison and evaluation. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1228412314. Association for Computational Linguistics. 1 Aihua Pei, Zehua Yang, Shunan Zhu, Ruoxi Cheng, and Ju Jia. 2025. SelfPrompt: Autonomously evaluating LLM robustness via domain-constrained knowledge guidelines and refined adversarial prompts. In Proceedings of the 31st International Conference on Computational Linguistics, pages 68406854, Abu Dhabi, UAE. Association for Computational Linguistics. 2 Yao Qiang, Subhrangshu Nandi, Ninareh Mehrabi, Greg Ver Steeg, Anoop Kumar, Anna Rumshisky, and Aram Galstyan. 2024a. Prompt perturbation consistency learning for robust language models. In Findings of the Association for Computational Linguistics: EACL 2024, St. Julians, Malta, March 1722, 2024, pages 13571370. Association for Computational Linguistics. 3, Yao Qiang, Subhrangshu Nandi, Ninareh Mehrabi, Greg Ver Steeg, Anoop Kumar, Anna Rumshisky, and Aram Galstyan. 2024b. Prompt perturbation consistency learning for robust language models. In Findings of the Association for Computational Linguistics: EACL 2024, pages 13571370, St. Julians, Malta. Association for Computational Linguistics. 1, 2 Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. 2 Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2024. Quantifying language models sensitivity to spurious features in prompt design or: How learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. 2, 3, 13 Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on machine learning research. 1 Teknium. 2023. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants. Anton Voronov, Lena Wolf, and Max Ryabinin. 2024a. Mind your format: Towards consistent evaluation of in-context learning improvements. arXiv preprint arXiv:2401.06766. 1, 2, 5 Anton Voronov, Lena Wolf, and Max Ryabinin. 2024b. Mind your format: Towards consistent evaluation of in-context learning improvements. In Findings of the Association for Computational Linguistics: ACL 2024, pages 62876310, Bangkok, Thailand. Association for Computational Linguistics. 1, 3, 11, 12 Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. 2, 3 Minghao Wu, Thuy-Trang Vu, Lizhen Qu, and Reza Haf. 2024. Mixture-of-skills: Learning to optimize 10 data usage for fine-tuning large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1422614240, Miami, Florida, USA. Association for Computational Linguistics. Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng-Ann Heng, and Wai Lam. 2024a. Unveiling the generalization power of fine-tuned large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 884899, Mexico City, Mexico. Association for Computational Linguistics. 1 Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, and Yue Zhang. 2024b. Supervised knowledge makes large language In The Twelfth models better in-context learners. International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. 1 Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pages 1269712706. PMLR. 1, 2 Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine A. Heller, and Subhrajit Roy. 2024. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. 1, 2, 3, 11 Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Gong, and Xing Xie. 2024. Promptrobust: Towards evaluating the robustness of large In Prolanguage models on adversarial prompts. ceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis, LAMPS 2024, Salt Lake City, UT, USA, October 1418, 2024, pages 5768. ACM. 2 Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, and Kai Chen. 2024. ProSA: Assessing and understanding the prompt senIn Findings of the Association sitivity of LLMs. for Computational Linguistics: EMNLP 2024, pages 19501976, Miami, Florida, USA. Association for Computational Linguistics. Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. CoRR, abs/2307.15043."
        },
        {
            "title": "A Consistency loss",
            "content": "Figure 8 provides some results for LoRA with consistency loss. Compared to LoRA with format augmentations, it shows higher spread and lower accuracy for all models."
        },
        {
            "title": "B Extended Description of Methods",
            "content": "In this section, we provide more detailed description of the methods used in the paper. Batch Calibration (BC). Batch Calibration (Zhou et al., 2024) is post-hoc approach, which calibrates model prediction with the estimate of contextual bias term p(y C). The contextual bias for each class p(y = yjC) is estimated from batch of samples by marginalizing the output scores over all samples within the batch. The calibrated probabilities ˆyi are derived by shifting the log-probability log p(yxi, C) by the corresponding estimated mean of each class: yj : (2) 1 (cid:88) i=1 log p(yC)j = log p(y = yjx(i), C), ˆy(i) = argmax (cid:16) yY log p(yx(i), C) log p(yC) (cid:17) . The method was originally designed exclusively for classification, and is inapplicable to other tasks. Template Ensembles (TE). Template Ensembles (Voronov et al., 2024b) average the predictions probability across various prompt formats fi and select the class with the largest probability. ˆy = argmax yY 1 (cid:88) i=1 p(y x, fi) (3) The main drawback of this method is the need to run the model times, where is the ensemble size. Sensitivity-aware decoding (SAD). Sensitivityaware decoding (Lu et al., 2024) assesses model sensitivity to input data by evaluating the variance of predictions over modified inputs, using synthetic perturbations based on real data. The sensitivity value is then used as penalty in the greedy decoding process: ˆy = argmax [αP (yx) (1 α)s] , (4) yV where (yx) is the probability of an output given x, α is the reweighting hyperparameter and is vocabulary the set of all tokens. In this 11 Figure 8: Comparing LoRA with consistency loss (LoRA-JS) with other methods in terms of their effect on accuracy and standard deviation over prompt formats. To aggregate accuracy, we first compute median accuracy over formats for each task, and then average over 52 tasks. Error bars are 2 (standard deviation over formats, averaged across tasks). paper we use simplified version of sensitivityaware decoding, using random token substitutions as perturbation. This approach reduces model variance but also requires times more runs. LoRA with augmentations (LoRA). We finetune an instruction-finetuned model on small dataset Dformat containing augmented samples. Here augmentation refers to changing the formatting while maintaining the same content. To build Dformat, we select subset of samples from generic instruction-following dataset Dsource and insert augmented versions for each sample. Parameter-efficient finetuning on Dformat is conducted using standard language modeling crossentropy loss. Loss is only computed on answer tokens, while the prefix tokens are masked. LoRA with consistency loss (LoRA-JS). One way to enforce consistent predictions is to use an auxiliary loss during fine-tuning. To test this approach, we reproduce prompt perturbation consistency learning (Qiang et al., 2024a). The training objective contains supervised crossentropy losses for pairs of augmented examples x1, x2 that share the same target label y, along with the consistency loss, based on Jensen-Shannon divergence. Formally, the overall loss function is defined as: = CE(ˆy1, y) + CE(ˆy2, y) + βJS(ˆy1ˆy2), (5) where CE denotes the cross-entropy loss, β is the coefficient controlling the contribution of the consistency loss, JS is Jensen-Shannon divergence, ˆy1, ˆy2 are the response token probability distributions, and ˆy1 and ˆy2 are corresponding distributions averaged over response length. Learning rate LoRA α LoRA Rank Amount of epochs Batch size Weight decay 2e-4 16 16 1 64 0.01 Table 5: LoRA training hyperparameters."
        },
        {
            "title": "C Method hyperparameters",
            "content": "All LoRA models were trained with default hyperparameters, given in Table 5. LoRA fine-tuning data. For experiments in Section 4.1 we construct Dformat from our subset of Natural Instructions benchmark, choosing up to 1000 samples per task, disjoint with the test samples chosen before. For cross-domain experiments in 4.2 we use custom fine-tuning dataset built from subsample of the Open Hermes 2.5 dataset (Teknium, 2023). Open Hermes 2.5 contains synthetically generated tasks in the form of prompts for LLMs, covering various task types. Our research primarily focuses on classification tasks and multiple-choice questions. Although the dataset includes many such tasks, only small portion has clearly defined labels. To find such examples, we selected those where the GPT response length does not exceed 20 symbols, as suitable tasks typically feature simple and concise answers. This threshold was determined empirically. Resulting dataset has approximately 50k samples. Other methods hyperparameters. In experiments with Template Ensembles and Sensitivityaware decoding, we used ensembles of size 5, following (Voronov et al., 2024b). The α parameter in Sensitivity-aware decoding 12 was set to 0.7 based on Section A.7 in (Lu et al., 2024). To create synthetic inputs for sensitivity estimation, we replaced 15% of the original tokens with random tokens from the entire vocabulary. Format 1 Format 2 Format 3 Format 4 Question: {} Answer: {} Question:: {} Answer:: {} QUESTIONn{}nANSWERn{} question {} answer {} Table 6: Some of augmentations used during LoRA finetuning."
        },
        {
            "title": "D Task selection",
            "content": "We selected tasks from Super-Natural Instructions following the criteria outlined in (Sclar et al., 2024). total of 52 evaluation tasks were selected from Super-Natural Instructions using several heuristics. First, datasets were required to contain at least 1000 samples. Then, tasks with long instructions (over 3000 characters) and inputs (over 2000 characters) were excluded to ensure scalability. Additionally, tasks with predicted accuracy of 0% for LLaMA2-7B 1-shot were removed, and no more than 4 tasks from the same dataset were included. Furthermore, socially significant tasks and formats for them were added if they were missing. task065, task133, task162, task220, task296, task319, task325, task335, task607, task905, The selected tasks were the following 52: task070, task050, task158, task114, task213, task161, task280, task214, task316, task286, task322, task317, task327, task323, task385, task328, task609, task580, task904, task1283, task1284, task1297, task1347, task1387, task1419, task1420, task1421, task1423, task1502, task1612, task1678, task1724. task069, task155, task163, task279, task297, task320, task326, task337, task608, task1186, Figure 9: Empirical dependency between spread and amount of components in format. 90% confidence interval is based on percentiles."
        },
        {
            "title": "E Dependence between spread and",
            "content": "format complexity Figure 9 shows the relation between spread and format complexity, measured as the amount of prompt components. While the dependence is quite noisy, maximal spread values occur at formats of maximal length. Use of scientific artifacts. Artifact License Natural Instructions Open Hermes 2. Llama 3.1 8B Llama 3.2 (1B, 3B) Gemma 2 (2B, 9B) Qwen2.5 (1.5B, 3B, 7B) Apache 2.0 CC-BY-NC Llama 3.1 Community License Agreement Llama 3.2 Community License Agreement Apache License 2.0 Apache License 2.0 Table 7: Scientific artifacts used in this paper and their licenses. In Table 7 we list the artifacts used in this paper along with their licenses. To the best of our knowledge, using these artifacts for research purposes is consistent with their intended use. task190 fits all previous requirements, but was Complete list of format components. excluded due to labeling errors. task323, For evaluation of frontier models, we selected task114, task161, task296, task320, task322, task1419, task1420, task1423, since they cover math, text compherension, simple tasks like counting the number of words with given letter and socially significant topics like detecting racial stereotypes. task1387, Complete list of format components is given in Table 8."
        },
        {
            "title": "H Example of input prompt for frontier",
            "content": "models Example of input prompt used for frontier models in presented on Figure 10. 13 Modification Values Descriptor transformation lambda x: x.title(), lambda x: x.upper(), lambda x: x.lower(), lambda x: Separator Space , ::: , :: , : , nt, , : , - , , , nt, :, ::, - , , , n, n, , , ; n, , <sep> , , , , , , , , . , , Text & option separator , , , Option item style 1, 2, ...; A, B, ...; a, b, ...; I, II, ...; i, ii, ... Option item wrapper ({}); {}.; {}); {} ); [{}]]; <{}> Table 8: Descriptor transformation correspond to Python commands making first character upper case (title), all letters upper case (uppercase), all letters lower case (lowercase) or keeping input as is. Option item style includes Arabic numerals, uppercase and lowercase Latin letters and uppercase and lowercase Roman numerals. For option item wrapper, {} is used as placeholder for option item (e.g. or 1). System: In this task, you need to answer Yes if the given word is the longest word (in terms of number of letters) in the given sentence, else answer No. Note that there could be multiple longest words in sentence as they can have the same length that is the largest across all words in that sentence. PAY ATTENTION TO THE OUTPUT FORMAT ONLY OUTPUT THE ANSWER WITHOUT ANY OTHER TEXT, LIKE IN EXAMPLES. User: Sentence woman sitting on chair holding three teddy bears. Is the longest word in the sentence? Answer No Sentence large green plant with leaves and spiky flowers. Is flowers the longest word in the sentence? Answer Yes Sentence long white airplane covered with lot pastel hears on it. Is covered the longest word in the sentence? Answer Figure 10: Example of input prompt used for GPT-4.1 and DeepSeek V3 0324."
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE University",
        "ISP RAS Research Center for Trusted AI",
        "MIPT",
        "Sber AI",
        "Skoltech",
        "Yandex"
    ]
}