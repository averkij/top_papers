{
    "paper_title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting",
    "authors": [
        "Hanzhi Chang",
        "Ruijie Zhu",
        "Wenjie Chang",
        "Mulin Yu",
        "Yanzhe Liang",
        "Jiahao Lu",
        "Zhuoyuan Li",
        "Tianzhu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web"
        },
        {
            "title": "Start",
            "content": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting Hanzhi Chang1*, Ruijie Zhu1,2*, Wenjie Chang1, Mulin Yu2, Tianzhu Zhang1 Yanzhe Liang1, Zhuoyuan Li1, Jiahao Lu1, 5 2 0 2 5 ] . [ 1 1 1 8 7 1 . 8 0 5 2 : r 1University of Science and Technology of China 2Shanghai AI Laboratory Figure 1. Given sparse-view images as input, MeshSplat can directly predict the scene geometry and efficiently extract the scene mesh. Compared to MVSplat [10] and other state-of-the-art methods, Meshplat achieves more consistent and precise mesh extraction in generalizable sparse-view surface reconstruction."
        },
        {
            "title": "Abstract",
            "content": "Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also normal prediction network to *Equal contributions. Corresponding author. align the orientation of 2DGS with normal vectors predicted by monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang. github.io/meshsplat_web/. 1. Introduction Surface reconstruction of 3D scenes is fundamental task in 3D vision, with wide range of applications in downstream tasks such as AR/VR and embodied AI. Recently, based on NeRF [35] and 3DGS [26], numerous per-scene optimized surface reconstruction methods [6, 19, 21, 52, 67] have been proposed. However, these methods struggle to robustly reconstruct scenes with only sparse views as input. This is because sparse views can only provide limited multiview geometric constraints, which are insufficient to perform high-quality per-scene geometry optimization. Therefore, it is necessary to construct feed-forward pipeline to task. Compared to 3DGS, 2DGS is more sensitive to position and orientation estimation in mesh reconstruction, as illustrated in Figure 2 (c). For position, it can be iteratively optimized in per-scene optimization setting. However, in an end-to-end framework, the Gaussian positions depend on the predicted depth maps. Due to the thin nature of 2DGS, mispredictions in the depth map directly lead to noticeable position shifts, unlike 3DGS. For orientation, unlike 3DGS whose orientations can be more flexible, the orientation of 2DGS directly determines the surface normals of the scene. Errors in predicting Gaussian orientation will ultimately result in distorted scene surfaces. To address these challenges, we propose MeshSplat, generalizable sparse-view surface reconstruction framework based on 2DGS, which consists of the following key designs: (1) Given sparse images as input, we build an MVS-based feed-forward network to generate pixelaligned 2DGS, where the position and orientation of 2DGS are transformed according to the predicted depth and normal maps, and other attributes of 2DGS are decoded directly from convolutional Gaussian head. With the predicted 2DGS, we can synthesize novel views for supervision and finally perform mesh extraction. (2) To improve the accuracy of 2DGS position prediction, we design Weighted Chamfer Distance Loss to align point clouds generated from predicted depths across different views. Since adjacent views have varying overlaps, we introduce confidence map to identify overlapping regions and weight the Chamfer Distance Loss accordingly, assigning higher (3) To improve the weights to these overlapping areas. accuracy of 2DGS orientation prediction, we design an uncertainty-based normal prediction network that predicts per-view normals and converts them to rotation quaternions for 2DGS. The per-view normals are supervised by an off-the-shelf monocular normal estimator, which incorporates monocular geometry priors to assist surface reconstruction. Our main contributions can be summarized as follows: We propose MeshSplat, the first method, to the best of our knowledge, that focuses on generalizable sparse-view surface reconstruction via Gaussian splatting. By incorporating 2DGS instead of 3DGS as bridge, MeshSplat can learn generalizable geometric priors from the novel view synthesis task in self-supervised manner. To better integrate 2dgs into the feed-forward framework, we introduce Weighted Chamfer Distance Loss to align Gaussian position across views and design an uncertaintybased normal prediction network that provides monocular supervision to Gaussian orientations. Extensive experiments show that our method achieves state-of-the-art performance in generalizable sparse-view surface reconstruction and cross-dataset generalization. Figure 2. Motivation. (a) The ellipsoid shape of 3DGS leads to different intersection planes in different viewpoints, resulting in inconsistent surface. (b) 2DGS has consistent intersection planes in different viewpoints, which is more suitable for surface reconstruction. (c) When the positions and orientations of 2DGS are not regularized, there will be significant discrepancies between 2DGS and the contours of the surface, which hinders the reconstruction of scene surfaces. learn geometric priors from diverse scenarios for generalizable sparse-view surface reconstruction. To address this issue, several meaningful attempts have been made, which are mainly NeuS-based methods [52]. For instance, SparseNeuS [32] constructs geometric volumes to estimate implicit SDF fields, which are then transformed to mesh. However, due to the inefficiency of implicit SDF representation and neural rendering, NeuS-based methods are limited to object-centric scenes and suffer from long rendering times. Compared with NeuS, Gaussian splatting can achieve not only faster rendering speeds but also better rendering quality. However, the vanilla Gaussian splatting framework falls short in representing surface geometry [21]. Therefore, existing Gaussian splatting-based methods [3, 10, 56] mainly focus on generalizable novel view synthesis, while methods applying Gaussian splatting to generalizable sparse-view surface reconstruction remain unexplored. To realize high-quality surface reconstruction, the network requires not only realistic appearance rendering but also precise geometry recovery. More importantly, the former task usually has more available training data, while the latter requires expensive geometric labeling. This raises question: is it possible to learn generalizable geometric priors from novel view synthesis task? Motivated by recent work 2DGS [21], we find an effective way to address this issue: using 2DGS as bridge between novel view synthesis (NVS) and surface reconstruction. Compared with 3DGS, 2DGS is naturally more suitable for representing thin surfaces and can be easily used to extract surface mesh beyond rendering novel views, as shown in Figure 2 (a) and (b). To this end, we decide to construct feed-forward framework to predict pixel-aligned 2DGS for generalizable sparse-view surface reconstruction. However, we also find that integrating 2DGS into generalizable sparse-view 3D scene reconstruction is not trivial 2. Related Work 2.1. 3D Scene Representation Early 3D scene methods [18, 23, 37] employ signed distance functions or occupancy fields to represent 3D scenes. NeRF [35] utilizes neural networks to construct the 3D radiance field for scene representation and renders RGB images by volume rendering, which inspires many subsequent works in 3D vision [2, 20, 29, 36, 40, 41, 47]. However, NeRF-based methods suffer from long-time inefficient rendering. 3DGS [26] models 3D scenes by explicit ellipsoidal Gaussian primitives and introduces differentiable rendering method based on alpha-blending, achieving significant breakthroughs in novel view synthesis tasks. It has become the core 3D scene representation for many tasks, such as novel view synthesis [16, 33, 43], 3D assets generation [49, 65], 3D scene understanding [64, 75] and dynamic scene reconstruction [22, 34, 57, 61, 74]. Despite their success, original NeRF and 3DGS require per-scene optimization, lacking cross-scene generalization capabilities. 2.2. Generalizable 3D Reconstruction Extensive works have proposed generalizable 3D reconstruction frameworks by training neural networks on largescale datasets to learn cross-scene priors. Early approaches [4, 9, 53, 58, 66] primarily combined feed-forward neural networks with Neural Radiance Fields (NeRF). However, these methods often suffer from the inefficiencies inherent in NeRF training and rendering, which limit their performance. More recently, methods such as DUSt3R and its subsequent extensions [13, 31, 51, 54, 60] leverage Transformer-based architectures trained on large-scale 3D-annotated datasets to predict 3D point maps from input 2D images. While effective in generating point-based 3D representations, these approaches cannot generalize to novel views and are not suitable for surface reconstruction tasks. Methods like LVSM [24] support novel view synthesis (NVS) tasks, but they are limited to generating RGB images and thus cannot be directly applied to surface reconstruction. With the introduction of 3D Gaussian Splatting (3DGS), new class of methods [3, 10, 30, 48, 55, 70], where MVSplat [10] represents prominent approach, employ 3DGS as the 3D scene representation for generalizable 3D reconstruction. They usually first predict depth maps from input views, and then use back-projected 3D points as centers to construct pixel-aligned 3D Gaussian representations. However, despite their promising performance in NVS tasks, these generalizable frameworks remain inadequate for mesh extraction and surface reconstruction tasks. ture matching. With the rise of neural networks, many works [27, 37, 38, 42, 52, 62, 63] reconstruct scene surface by estimating neural occupancy fields or signed distance functions (SDF) by neural networks, and render novel views through surface rendering or volume rendering techniques. Recently, many works have extended Gaussian Splatting to mesh extraction. These methods can be categorized into two types: methods that directly use ellipsoidal 3DGS to extract meshes [19, 6769], and methods that use flattened 3DGS or 2DGS [57, 12, 15, 21, 50]. However, these methods fail to generate high-quality meshes under sparse viewpoint inputs and lack cross-scene generalization. SparseNeuS [32] and subsequent works [8, 28, 39, 44, 59], inspired by generalizable novel view synthesis approaches, achieve generalizable surface reconstruction based on NeuS [52]. These methods extract feature maps from input images to construct 3D geometry volume, which is used to obtain neural SDF fields and output the mesh of the scene. However, these methods exhibit poor efficiency in mesh extraction and are limited to objectcentric scenes. 2DGS [21] outperforms NeuS-based methods in mesh extraction tasks, but how to generalize 2DGS to different scenes with sparse inputs still remains unexplored. Therefore, we propose MeshSplat, an end-to-end generalizable sparse-view surface reconstruction framework based on 2DGS. 3. Methods 3.1. Overall Framework The framework of MeshSplat is illustrated in Figure 3. Given two images {Ii}2 i=1 and corresponding projection matrices{Πi}2 i=1, Πi = Ki[Riti], where Ki denotes for camera intrinsics, Ri denotes for camera rotation matrices and ti denotes for translation matrices, MeshSplat begins with an encoder composed of CNN and Multi-View Transformer to extract feature maps. From these multi-view feature maps, per-view cost volumes are constructed via plane sweeping. To enhance the multi-view consistency, we propose Weighted Chamfer Distance Loss to constrain the cost volumes. Subsequently, we use Gaussian Prediction Network, which includes normal prediction network to predict Gaussian orientations, and depth refinement network to predict Gaussian positions. In this way, we can obtain the pixel-aligned 2DGS to represent the scene. This process can be formulated as: {Ii, Πi}2 i=1 {µj, sj, rj, αj, cj}2HW j=1 (1) Finally, these predicted 2DGS can render novel views and reconstruct the scene mesh. 2.3. Surface Reconstruction 3.2. Cost Volume Construction Traditional surface reconstruction methods [17, 25] primarily model 3D scene geometry through explicit feaFollowing MVSplat [10], we first use CNN to extract feature map from each input view. Then, we employ Figure 3. Overall Architecture. Taken pair of images as input, MeshSplat begins with multi-view backbone to extract per-view feature maps. Then we construct per-view cost volumes via the plane-sweeping to generate coarse depth maps, which can be projected to 3D point clouds and be constrained by our proposed Weighted Chamfer Distance Loss. We further feed cost volumes into our gaussian prediction network, together with depth refinement network and normal prediction network, to obtain pixel-aligned 2DGS. Finally, we use these 2DGS to render novel view for supervision and reconstruct the scene mesh. Transformer that incorporates multi-view cross-attention to enable information exchanges across different views. As result, we obtain the feature map {Fi}2 i=1 for each view with size of 4 4 C. To intuitively obtain the matching correspondences between the two input images, we construct per-view cost volumes via plane sweeping. Specifically, for input view i, we first divide the depth ranges into several depth bins {dk}D k=1. The feature map Fj from the other view is then warped to view using the projection matrices Pi and Pj along with the current depth candidate dk, denoted as dk ji. Next, we compute the dot product between the two feature maps and concatenate the results from all depth candidates. After two convolutional layers ϕCV , we finally obtain the cost volume Vi with size of 4 4 D: Vi = ϕCV({V dk }D k=1), dk = Fi dk ji (2) Since the correlation calculation directly reflects the matching confidence between the input images, we can directly apply Softmax operation to compute the probabilities of the depth values to be each depth candidates. The coarse depth maps can be obtained by weighted summation: Wi = SoftmaxD(Vi), Dcoarse = dk (3) (cid:88) 3.3. Gaussian Prediction Network Next, we discuss how to derive the attributes of pixelaligned 2DGS from cost volumes. Following MVSplat [10], we first use U-Net with attention layers to refine the cost volume, taking feature maps as additional inputs. Using the refined cost volume, we can compute the final depth maps using equation (3), and then perform back-projection to obtain the 3D positions of the pixel-aligned 2DGS. In addition, 2DGS requires more precise orientation prediction than the ellipsoid-like 3DGS. Based on this, we introduce an additional normal prediction network to predict the normal maps of the input images, which are further converted to the 2DGS rotation quaternions. We use lightweight CNN ϕrot to predict the orientations of the pixel-aligned 2DGS. The network takes the refined cost volumes, feature maps, and original RGB images at three different scales as input, and processes them through convolutional layers with residual connections. Finally the rotation quaternions for all three scales are obtained, which are further converted into normal vectors for each scale, along with kappa κ which reflect the uncertainty of predicted normal vectors following [1]: {q, κ} = ϕrot(ViFiIi), = R(q) [0, 0, 1]T (4) where R(q) represents the rotation matrix computed from the rotation quaternion q. The resulting normal vectors and kappa κ will be used in subsequent loss calculations. For the remaining attributes of 2DGS, following [10], we predict them through two-layer convolutional gaussian head, taking the refined cost volumes, feature maps, and original RGB images as input. 3.4. Weighted Chamfer Distance Loss In the ideal case, the Gaussian positions predicted from adjacent views should have significant overlaps. Considering this property, one approach is to use the Chamfer Distance (CD) Loss as regularization on these 3D points. Chamfer Distance measures the overlaps between two point clouds, which can be expressed as: LCD = 1 2 ("
        },
        {
            "title": "1\nN1",
            "content": "min pi 1 pj 2+"
        },
        {
            "title": "1\nN2",
            "content": "N1(cid:88) i=1 N2(cid:88) i=1 min pi 2 pj 1) k}Nk (5) where {pi i=1 are the predicted point clouds calculated by the projection matrices Πk and coarse depth maps Dcoarse of the k-th view. The original CD Loss assigns equal weights to all points. However, due to occlusion and view differences, those pixels which do not have their corresponding pixels will have far chamfer distances. Applying CD loss uniformly to all points may result in incorrect and unreasonable constraints. To address this issue, we design confidence maps to measure the matching confidence of each pixel and use them to weight CD loss, resulting in our proposed Weighted Chamfer Distance Loss (WCD Loss). We argue that confidence maps can be derived from the cost volumes, since they have already captured feature similarities between pixels. The confidence map Mi for view is defined by taking the maximum value along depth dimension in the cost volume: Mi = max dk SoftmaxD(Vi), = 1, 2 (6) With the confidence maps, we define the WCD Loss as: N1(cid:88) M1(i) min pi 1 pj 2 LWCD = 1 ( 1 N1 i=1 + 1 N2 N2(cid:88) i=1 M2(i) min pi 2 pj 1) (7) where M1/2(i) denotes the confidence map value for pixel in the first or second view. Our proposed WCD loss ensures that the regions with higher matching confidence overlap as much as possible. This regularizes the generation of cost volumes, and leads to more accurate prediction of 2DGS positions. 3.5. Uncertainty-Guided NLL Normal Loss Due to the lack of regularization for normals, the predicted orientations of 2DGS still suffer from degradation, preventing the 2DGS from aligning precisely with the actual surfaces. To address this issue, we use the uncertainty-guided normal negative log-likelihood (NLL) loss proposed by [1] to supervise predicted normal maps at each scale, which is defined as: LAngMF(ni, ˆni, κi) = log(κ2 exp(κiπ)) + κi cos1 nT + 1) + log(1+ ˆni (8) where ni, κi, ˆni stands for the predicted normal vector, kappa and pseudo ground-truth normal vector of pixel i. Following [1], we sample pixels based on their κi, forming the sampled sets Psample, and only apply NLL loss on Psample. We use an off-the-shelf monocular normal estimation model Omnidata [14] to provide pseudo groundtruth normal maps as supervision. Finally, the normal loss Lnormal is computed as the average of NLL losses in sampled points. Details can be found in supplementary materials. 3.6. Training and Inference Finally, our overall training loss is defined as: = w1Lpho + w2LWCD + w3Lnormal (9) Lpho consists of MSE loss and LPIPS loss calculated from ground truth RGB image and rendered image ˆI: Lpho = w11 MSE(I, ˆI) + w12 LPIPS(I, ˆI) (10) During inference, 2DGS is first reconstructed through our network taken two images as input. Subsequently, we follow the methods of 2DGS original paper [21] to extract the scene mesh. 4. Experiments 4.1. Settings Datasets. We train and evaluate our model on Re10K [73] and Scannet [11] datasets. Re10K is large-scale home walkthrough multi-view dataset obtained from YouTube, containing 67,477 training scenes and 7,289 testing scenes. Scannet is multi-view real-world indoor dataset, and following [71], we use 100 scenes for training and 8 scenes for testing. To evaluate the model generalizability, we also perform cross-dataset evaluations from Re10K to Scannet and Replica [46], multi-view synthetic indoor dataset. Following [72], we use 8 scenes for testing in Replica dataset. Implementation Details. We employ different training strategies for Re10K and Scannet. For experiments on Re10K, we crop the input images to 256 256 and train for 200,000 steps with batch size of 12. For experiments on Scannet, we crop the input images to 512 384 and train for 75,000 steps with batch size of 4. Figure 4. Quanlitative Comparisons on Re10K Dataset. While the baseline methods provide meshes with holes and uneven surfaces, MeshSplat successfully reconstruct the scene with smoother and more complete meshes. Figure 5. Quanlitative Comparisons on Scannet Dataset. While the baseline methods provide meshes with holes and uneven surfaces, MeshSplat successfully reconstruct the scene with smoother and more complete meshes. 4.2. Main Results Mesh Reconstruction. To enable comparisons with the state-of-the-arts, we transfer four methods from generalizable novel view synthesis to mesh reconstruction as our comparison. For fair comparison, we also apply TSDF fusion to extract meshes for the compared methods. Table 1 presents the experimental results on the two datasets. The Table 1. Quantitative Comparisons. * denotes that we use the dense reconstruction results of COLMAP with dense view inputs as ground-truth point clouds of Re10K. ** denotes for the 300k-training-step version according to its original setting. Re10K* Precision Recall 0.0141 0.1390 0.1067 0.3986 0.3949 0. 0.1146 0.1548 0.0903 0.2633 0.2607 0.2953 CD 5.9736 0.6139 1.4423 0.4038 0.4015 0.3566 F1 0.0248 0.1407 0.0944 0.3139 0.3100 0.3758 Scannet Precision Recall 0.0001 0.1320 0.2599 - 0.1992 0.3901 0.0145 0.2053 0.3597 - 0.2282 0.3849 F1 0.0002 0.1514 0.2948 - 0.2095 0.3824 CD 11.93 0.5761 0.3285 - 0.3748 0. SparseNeuS [32] MVSNeRF [4] PixelSplat [3] MVSplat** [10] MVSplat [10] MeshSplat (Ours) Table 2. Quantitative Comparisons in Zero-Shot Transfer Experiments on Scannet and Replica Datasets. We use the models trained on Re10K only to perform cross-dataset generalization experiments. MeshSplat still shows promising results. Re10K Scannet Precision Recall 0.0001 0.1333 0.2868 0.0112 0.1591 0.3176 CD 13.16 0.4506 0.3148 F1 0.0001 0.1418 0.2956 Re10K Replica Precision Recall 0.0001 0.0528 0.0749 0.0104 0.0695 0. CD 15.21 1.089 0.921 F1 0.0002 0.0564 0.0809 SparseNeuS [32] MVSplat [10] MeshSplat (Ours) experiments demonstrate that our method significantly outperforms these methods in all metrics, indicating that our approach has strong capabilities in surface reconstruction with sparse-view input. Figures 4 and 5 show the visualizations on Re10K and Scannet. For the compared methods, their extracted meshes suffer from holes and uneven surfaces. In contrast, MeshSplat can accurately reconstruct the meshes of complex scenes with smoother surfaces, demonstrating that our model can capture scene structure well even with sparse-view inputs. More qualitative comparisons can be found in Appendix. Cross-Dataset Generalization. To validate the generalization ability, we conduct zero-shot transfer experiments on the Scannet and Replica datasets respectively, using the model trained on the Re10K dataset. As shown in Table 2 and Fig 6, our method provides significant improvements compared to the baselines, demonstrating the satisfactory generalizability of MeshSplat across diverse scenarios. Depth and Normal Maps Prediction. To further show that MeshSplat can capture scene geometry accurately, we show the experiments on depth and normal maps prediction in Table 3. MeshSplat surpasses MVSplat by large margin in all metrics, demonstrating that MeshSplat can better recover the scene geometry. We also show the visualizations of depth and normal maps in Figure 7, along with matching confidence maps in the WCD loss and kappa maps used in the normal loss. MeshSplat can accurately estimate depth and normal maps of the input views. The confidence maps clearly indicate regions of low confidence, such as textureTable 3. Results on Depth and Normal Prediction Experiments. It is clear that MeshSplat can predict depth and normal maps of input views correctly. Depth AbsRel AbsDiff MVSplat MeshSplat 0.1692 0. 0.3197 0.9153 Normal Mean <30 57.16 33.84 0.1357 0.6026 Table 4. Ablation Studies on Scannet Dataset. NPN stands for the normal prediction network and NLL loss. # 1 2 3 4 5 2DGS WCD Loss NPN CD 0.3748 0.2948 0.2769 0.2642 0.2606 less areas and non-overlapped regions. For kappa maps, the higher uncertainty areas are typically object borders. 4.3. Ablations Main Components. Table 4 and Figure 8 (a) present the results of our ablation studies conducted on the Scannet dataset. The first two lines of the table indicate that 2DGS is more suitable for mesh reconstruction tasks compared to Figure 6. Qualitative Comparisons in Zero-Shot Transfer Experiments on Scannet and Replica Datasets. Compared to MVSplat, MeshSplat can still extract smoother surfaces, demostrating its generalization across different datasets. Figure 7. Visualizations of output depth and normal maps, confidence maps used in WCD loss and kappa maps used in normal loss. The confidence maps reflect the unconfident matching areas like texture-less areas and non-overlapped areas between the two views. For kappa maps, areas with higher uncertainty typically correspond to object boundaries. 3DGS. Moreover, by incorporating our proposed WCD loss and normal prediction network, 2DGS can align better with the actual surfaces. These enhancements significantly improve surface fidelity and reduce geometric artifacts in the final output. Overall, these results demonstrate that each proposed component plays complementary role in boosting the reconstruction quality. Gaussian normals. To further demonstrate the necessities of the normal prediction network, we present comparisons on normal maps rendered by Gaussian Splatting in Figure 8 (b). Following [6], we define the normal vector of 3DGS as the direction of the axis which has the minimum value of scales. 2DGS suffer from more significant degradation in normals compared to 3DGS when no regularization is applied. After incorporating the normal prediction network and its corresponding regularization loss, the predicted orientations of 2DGS become more accurate. Figure 8. Qualitative Ablation Studies. (a) Ablation studies on our proposed modules. (b) Comparisons of normal maps rendered by 2DGS and 3DGS. 5. Limitations Although our model achieves promising results in generalizable sparse-view surface reconstruction, it still has some limitations. MeshSplat sometimes may predict discontinuous depth maps in weakly textured areas, as shown in the supplimentary materials, although the rendered RGB image is reliable. This is probably due to the ambiguity of feature matching in these areas. Additionally, MeshSplat cannot reconstruct surfaces in regions that are not visible from the input views. Incorporating generative approaches might be good choice in future work. 6. Conclusion We present MeshSplat, generalizable surface reconstruction framework designed for sparse-view inputs. We use 2DGS as scene representation and construct an end-to-end surface reconstruction network. To better meet the needs for more accurate prediction of 2DGS, we propose the WCD loss to regularize coarse point maps, and also use normal prediction network to predict the orientations of 2DGS. Experimental results demonstrate that MeshSplat outperforms existing methods in generalizable sparse-view mesh extraction. Future work could explore more efficient architectures to predict the attributes of 2DGS more accurately."
        },
        {
            "title": "References",
            "content": "[1] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Estimating and exploiting the aleatoric uncertainty in surface normal estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1313713146, 2021. 4, 5, 1 [2] Jonathan Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul Srinivasan. Mip-nerf: multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 58555864, 2021. 3 [3] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction. arXiv preprint arXiv:2312.12337, 2023. 2, 3, 7, 1 [4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1412414133, 2021. 3, 7, 1 [5] Danpeng Chen, Hai Li, Weicai Ye, Yifan Wang, Weijian Xie, Shangjin Zhai, Nan Wang, Haomin Liu, Hujun Bao, and Guofeng Zhang. PGSR: Planar-based Gaussian Splatting for Efficient and High-Fidelity Surface Reconstruction, 2024. eprint: 2406.06521. 3 [6] Hanlin Chen, Chen Li, and Gim Hee Lee. NeuSG: Neural Implicit Surface Reconstruction with 3D Gaussian Splatting Guidance, 2023. eprint: 2312.00846. 1, 8 [7] Hanlin Chen, Fangyin Wei, Chen Li, Tianxin Huang, Yunsong Wang, and Gim Hee Lee. VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction, 2024. eprint: 2406.05774. [8] Xi Chen, Jiaming Sun, Yiming Xie, Hujun Bao, and Xiaowei Zhou. NeuralRecon: Real-Time Coherent 3D Scene Reconstruction from Monocular Video. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 114, 2024. 3 [9] Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Explicit correspondence arXiv matching for generalizable neural radiance fields. preprint arXiv:2304.12294, 2023. 3 [10] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images. arXiv preprint arXiv:2403.14627, 2024. 1, 2, 3, 4, 5, 7 [11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 5, 1 [12] Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, and Weiwei Xu. High-quality Surface ReIn SIGGRAPH 2024 construction using Gaussian Surfels. Conference Papers. Association for Computing Machinery, 2024. 3 [13] Bardienus Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Revaud. Mast3rsfm: fully-integrated solution for unconstrained structurefrom-motion. arXiv preprint arXiv:2409.19152, 2024. 3 [14] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multitask mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1078610796, 2021. [15] Lue Fan, Yuxue Yang, Minxing Li, Hongsheng Li, and Zhaoxiang Zhang. Trim 3D Gaussian Splatting for Accurate Geometry Representation. arXiv preprint arXiv:2406.07499, 2024. 3 [16] Guangchi Fang and Bing Wang. Mini-splatting: Representing scenes with constrained number of gaussians. In European Conference on Computer Vision, pages 165181. Springer, 2024. 3 [17] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and IEEE transactions on pattern robust multiview stereopsis. analysis and machine intelligence, 32(8):13621376, 2009. 3 [18] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for In Proceedings of the IEEE/CVF conference on 3d shape. computer vision and pattern recognition, pages 48574866, 2020. 3 [19] Antoine Guedon and Vincent Lepetit. SuGaR: SurfaceAligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering. CVPR, 2024. 1, [20] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large Reconstruction Model for Single Image to 3D, 2024. eprint: 2311.04400. 3 [21] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2D Gaussian Splatting for Geometrically Accurate Radiance Fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024. 1, 2, 3, 5 [22] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes, 2024. eprint: 2312.14937. 3 [23] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Nießner, Thomas Funkhouser, et al. Local implicit grid representations for 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60016010, 2020. 3 [24] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.17242, 2024. 3 [25] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM Transactions on Graphics (ToG), 32(3):113, 2013. [26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics (ToG), 42(4):114, 2023. Publisher: ACM New York, NY, USA. 1, 3 [27] Zhaoshuo Li, Thomas Muller, Alex Evans, Russell H. Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-Fidelity Neural Surface Reconstruction, 2023. eprint: 2306.03092. 3 [28] Yixun Liang, Hao He, and Ying-cong Chen. Rethinking rendering in generalizable neural surface reconstruction: learning-based solution. arXiv preprint arXiv:2305.18832, 2 (6):7, 2023. 3 [29] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot One Image to 3D Object. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 92989309, 2023. 3 [30] Tianqi Liu, Guangcong Wang, Shoukang Hu, Liao Shen, Xinyi Ye, Yuhang Zang, Zhiguo Cao, Wei Li, and Ziwei Liu. Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo. arXiv preprint arXiv:2405.12218, 2024. 3 [31] Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, and Baoquan Chen. Slam3r: Realtime dense scene reconstruction from monocular rgb videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1665116662, 2025. [32] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. SparseNeuS: Fast Generalizable Neural eprint: Surface Reconstruction from Sparse Views, 2022. 2206.05737. 2, 3, 7, 1 [33] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d In Proceedings of gaussians for view-adaptive rendering. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2065420664, 2024. 3 [34] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Tracking arXiv preprint Deva Ramanan. by persistent dynamic view synthesis. arXiv:2308.09713, 2023. 3 Dynamic 3d gaussians: [35] Mildenhall, PP Srinivasan, Tancik, JT Barron, Ramamoorthi, and Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision, 2020. 1, [36] Thomas Muller, Alex Evans, Christoph Schied, and AlexanInstant neural graphics primitives with mulder Keller. tiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):115, 2022. Publisher: ACM New York, NY, USA. 3 [37] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35043515, 2020. 3 [38] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance In Proceedings of fields for multi-view reconstruction. the IEEE/CVF international conference on computer vision, pages 55895599, 2021. 3 [39] Rui Peng, Xiaodong Gu, Luyang Tang, Shihe Shen, Fanqi Yu, and Ronggang Wang. GenS: Generalizable Neural Surface Reconstruction from Multi-View Images, 2024. eprint: 2406.02495. 3 [40] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [41] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. 3 [42] Christian Reiser, Stephan Garbin, Pratul P. Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan T. Barron, Peter Hedman, and Andreas Geiger. Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based View Synthesis, 2024. eprint: 2402.12377. [43] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians. arXiv preprint arXiv:2403.17898, 2024. 3 [44] Yufan Ren, Fangjinhua Wang, Tong Zhang, Marc Pollefeys, and Sabine Susstrunk. VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable MultiView Reconstruction, 2023. eprint: 2212.08067. 3 [45] Johannes Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 501518. Springer, 2016. 1 [46] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 5, [47] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459 5469, 2022. 3 [48] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150, 2023. 3 [49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3 [50] Matias Turkulainen, Xuqian Ren, Iaroslav Melekhov, Otto Seiskari, Esa Rahtu, and Juho Kannala. DN-Splatter: Depth and Normal Priors for Gaussian Splatting and Meshing, 2024. eprint: 2403.17822. 3 [51] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 3 [52] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. 1, 2, 3 [53] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. IbrIn Pronet: Learning multi-view image-based rendering. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46904699, 2021. [54] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 3 [55] Yunsong Wang, Tianxin Huang, Hanlin Chen, and Gim Hee Lee. FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes, 2024. eprint: 2405.17958. 3 [56] Christopher Wewer, Kevin Raj, Eddy Ilg, Bernt Schiele, and latentSplat: Autoencoding Variational In Jan Eric Lenssen. Gaussians for Fast Generalizable 3D Reconstruction. arXiv, 2024. 2 [57] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528, 2023. 3 [58] Haofei Xu, Anpei Chen, Yuedong Chen, Christos Sakaridis, Yulun Zhang, Marc Pollefeys, Andreas Geiger, and Fisher Yu. Murf: Multi-baseline radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2004120050, 2024. 3 [59] Luoyuan Xu, Tao Guan, Yuesong Wang, Wenkai Liu, Zhaojie Zeng, Junle Wang, and Wei Yang. C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction, 2023. eprint: 2306.10003. 3 [60] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2192421935, 2025. 3 [61] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction, 2023. eprint: 2309.13101. 3 [62] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems, 33:24922502, 2020. 3 [63] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Voleprint: ume Rendering of Neural Implicit Surfaces, 2021. 2106.12052. 3 [64] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. In European conference on computer vision, pages 162179. Springer, 2024. 3 [65] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXiv preprint arXiv:2310.08529, 2023. 3 [66] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45784587, 2021. 3 [67] Mulin Yu, Tao Lu, Linning Xu, Lihan Jiang, Yuanbo Xiangli, and Bo Dai. GSDF: 3DGS Meets SDF for Improved Rendering and Reconstruction, 2024. eprint: 2403.16964. 1, 3 [68] Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian Opacity Fields: Efficient High-quality Compact Surface Reconstruction in Unbounded Scenes. arXiv preprint arXiv:2404.10772, 2024. [69] Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, and Ping Tan. Rade-gs: Rasterizing depth in gaussian splatting. arXiv preprint arXiv:2406.01467, 2024. 3 [70] Cheng Zhang, Haofei Xu, Qianyi Wu, Camilo Cruz Gambardella, Dinh Phung, and Jianfei Cai. Pansplat: 4k panorama synthesis with feed-forward gaussian splatting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1143711447, 2025. 3 [71] Xiaoshuai Zhang, Sai Bi, Kalyan Sunkavalli, Hao Su, and Zexiang Xu. Nerfusion: Fusing radiance fields for largescale scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54495458, 2022. 5 [72] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew Davison. In-place scene labelling and understanding In Proceedings of the with implicit scene representation. IEEE/CVF International Conference on Computer Vision, pages 1583815847, 2021. 5 [73] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. 5, Stereo magnification: [74] Ruijie Zhu, Yanzhe Liang, Hanzhi Chang, Jiacheng Deng, Jiahao Lu, Wenfei Yang, Tianzhu Zhang, and Yongdong Zhang. Motiongs: Exploring explicit motion guidance for deformable 3d gaussian splatting. Advances in Neural Information Processing Systems, 37:101790101817, 2024. 3 [75] Ruijie Zhu, Mulin Yu, Linning Xu, Lihan Jiang, Yixuan Li, Tianzhu Zhang, Jiangmiao Pang, and Bo Dai. Objectgs: Object-aware scene reconstruction and scene understanding via gaussian splatting, 2025. 3 MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Uncertainty-Guided"
        },
        {
            "title": "Negative",
            "content": "Log-"
        },
        {
            "title": "Likelihood Loss in Normal Prediction",
            "content": "In this section, we explain the uncertainty-guided negative log-likelihood (UG-NLL) loss we use in the normal map supervision, which is proposed by [1]. For each pixel i, with the predicted normal vector ni, the actual normal vector ˆni ideally follows the Angular von Mises-Fisher distribution: pAngMF(ˆnini, κi) = (κ2 + 1) exp(κi cos1 nT 2π(1 + exp(κiπ)) ˆni) (11) where κi > 0 is the kappa value of this pixel. Higher κi indicates lower uncertainty in normal vector prediction. From this, we can compute the negative log-likelihood (NLL) loss: LAngMF(ni, ngt , κi) = log(κ2 exp(κiπ)) + κi cos1 nT + 1) + log(1+ ngt (12) In order to ensure efficient training, UG-NLL loss is only applied to several sampled pixels based on their κ. Specifically, we first sample the top β pixels with the lowest κ at each scale. Then, we randomly sample (1 β) additional pixels, forming the set of all sampled pixels Psample. 8. Datasets Our experiments are based on three indoor multiview datasets, including Re10K [73], Scannet [11] and Replica [46] datasets. RE10K is licensed by Google LLC under Creative Commons Attribution 4.0 International License. Scannet is under MIT license. Replica is under custom license that only allows research or educational purpose. Since Re10K does not have ground-truth meshes or point clouds, we use COLMAP [45] to reconstruct dense point clouds for 20 scenes under dense-view inputs with provided camera parameters, and take these point clouds as ground truth. Additionally, since sparse input viewpoints cannot observe the entire content of the scene, we cull ground-truth meshes or point clouds based on the input view frustums. Visualizations of some of these point clouds can be found in Figure 11. 9. Additional Implementation Details Network Architecture. The backbone of our model follows the same structure and initialization methods as MVSplat [10]. For the cost volume construction, we sample Time (s) Params (M) MVSNeRF* [4] SparseNeuS* [32] pixelSplat [3] MVSplat [10] MeshSplat 0.76 7.0483 0.194 0.072 0.102 0.341 0.843 125.4 12.0 13.3 Table 5. Model Efficiency Analysis. We report rendering time and model sizes of MeshSplat and baseline methods. * denotes that these methods render only one image per forward pass. = 128 depth candidates between the near and far planes. For the normal prediction network, we use total of 10 convolutional layers to predict the 2D Gaussian orientations, and the pseudo ground-truth supervision is applied to normal maps at all scales including 1/4, 1/2 and full resolution. For the depth refinement network, they are implemented as U-Net with self-attention and cross-attention layers. Training Strategy. We use the Adam optimizer with maximum learning rate of 2 104 to train our network. The weights of all components of the loss function are set as follows: w1 = 1.0, w2 = 5.0 103, w3 = 5.0 103, w11 = 1.0, w12 = 0.1 and α = 0.1. For the uncertaintyguided random sampling applied in normal prediction network, we set β = 0.7, = 0.4 , where (H, ) is the image size at the current processing scale. All experiments are performed on single NVIDIA A800 GPU. Others. On the Re10K dataset, we set the camera near and far planes to dnear = 1.0 and dmax = 100.0, and we set the voxel size to 0.005 and the truncated threshold to 0.1 for TSDF fusion. For the Scannet and Replica datasets, we set the camera near and far planes to dnear = 0.5 and dmax = 15.0, and the voxel size and truncated threshold to 0.01 and 0.08 for TSDF fusion. 10. Additional Experimental results Model Efficiency Analysis. We report the rendering time and the size of the models in Table 5. All experiments are conducted on single NVIDIA 3090 GPU. Compared with NeRF-based methods, MeshSplat achieves significantly faster rendering speeds. Additionally, MeshSplat has only few extra model parameters and little extra rendering time compared to MVSplat. Additional Mesh Visualization. Figure 9, 10 and 12 show additional visualizations of reconstructed mesh. Please refer to corresponding captions of these figures for details. Additional Depth and Normal Visualization. Figure 13 shows the rendered depth maps and normal maps by 2DGS in the Scannet dataset. Additionally, we show depth maps and normal maps of context images predicted by the Gaussian prediction network in Figure 14. 11. Video Demo We apply video demo of MeshSplat to further show the mesh reconstruction results. Please refer to the supplementary materials for details. Figure 9. Additional Visualizations of Extracted Meshes in Re10K Datasets. Figure 10. Additional Visualizations of Extracted Meshes in Scannet Datasets. Figure 11. Visualizations of our ground-truth dense point clouds provided by COLMAP in Re10K datasets. Figure 12. Additional Visualizations of Extracted Meshes in Cross-Dataset Generalization Experiments. Figure 13. Visualizations of Rendered RGB Images, Depth Maps and Normal Maps in the Scannet Dataset. Figure 14. Predicted Depth Maps and Normal Maps of the Gaussian Prediction Network. Figure 15. Failure Cases of MeshSplat."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "University of Science and Technology of China"
    ]
}