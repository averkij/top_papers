{
    "paper_title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization",
    "authors": [
        "Zhengzhao Lai",
        "Youbin Zheng",
        "Zhenyang Cai",
        "Haonan Lyu",
        "Jinpu Yang",
        "Hongqing Liang",
        "Yan Hu",
        "Benyou Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha."
        },
        {
            "title": "Start",
            "content": "Can Multimodal LLMs See Materials Clearly? Multimodal Benchmark on Materials Characterization Zhengzhao Lai1, Youbin Zheng2, Zhenyang Cai1, Haonan Lyu3, Jingpu Yang2, Hongqing Liang3, Yan Hu1*, Benyou Wang1 1The Chinese University of Hong Kong, Shenzhen 2Northeastern University 3Zhejiang University {zhengzhaolai, huyan}@cuhk.edu.cn 5 2 0 2 1 1 ] . [ 1 7 0 3 9 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expertlevel domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/ FreedomIntelligence/MatCha."
        },
        {
            "title": "Introduction",
            "content": "Materials characterization serves as critical means of obtaining information about the physical world (Leng, 2013), offering insights that transcend the limitations of human sensory perception. It enables multi-scale analysis, providing rich data on material morphology, composition, and structure. This information, in turn, reveals the physical, chemical, and mechanical properties essential *Corresponding author. 1 for guiding new material design and optimization (Robertson et al., 2011). For instance, scanning electron microscopy (SEM) and transmission electron microscopy (TEM) images are instrumental in determining the underlying mechanisms of steel bar fracture in buildings (Inkson, 2016). Despite its importance, interpreting diverse and complex imaging data generated by various characterization techniques remains significant challenge that demands extensive domain expertise. Typically, even experienced materials researchers invest considerable time analyzing these multifaceted results. This analytical burden becomes particularly acute when handling high-throughput data, where efficiency bottlenecks arise. While convolutional neural networks (CNNs) have been utilized for various materials characterization tasks (Madsen et al., 2018; Maksov et al., 2019; Zaloga et al., 2020; Warmuzek et al., 2021; Leitherer et al., 2023), providing valuable insights, their application faces notable limitations. Predominantly, these CNNbased approaches are task-specific, exhibiting limited cross-task generalization. Furthermore, prior work has largely focused on morphological perception, resulting in shallow image content understanding that often falls short of the nuanced interpretations required by materials scientists in real-world scenarios. These constraints highlight the need for more versatile and deeply understanding models. Leveraging MLLMs (Zhu et al., 2023; Liu et al., 2023; Li et al., 2023; Achiam et al., 2023; Wang et al., 2024b; Liu et al., 2024) for these challenges offers distinct advantages. MLLMs have demonstrated strong performance and generalization in both natural and domain-specific image understanding (Li et al., 2025), and have spurred revolutionary changes in materials science, including property prediction (Rubungo et al., 2023; Antunes et al., 2024; Xie et al., 2023), new material design (Tang et al., 2025; Mishra et al., 2024; Xie et al., 2024), and autonomous scientific agents (Zhang et al., 2024; Ding et al., 2024). These models show great potential to assist materials scientists with diverse materials characterization tasks via natural language interaction, thereby facilitating new material development and boosting scientific productivity. To realize this potential, MLLMs must first be capable of accurately interpreting diverse materials characterization images, recognizing fundamental visual content, and performing reasoning based on visual cues. However, existing evaluations of MLLMs on scientific imaging data primarily focus on biomedical domains (He et al., 2020; Huang et al., 2023; Lozano et al., 2024), are confined to relatively simplistic figures and charts in limited scientific fields (Yue et al., 2024a,b; Chen et al., 2024a; Roberts et al., 2024), or lacking sufficient depth to represent the complexity of authentic materials research scenarios (Alampara et al., 2024; Li et al., 2024b; Verma et al., 2024). Consequently, comprehensive, expert knowledge-anchored multimodal benchmark specifically designed for materials science to rigorously assess current models is notably absent. This gap impedes the progress toward AI-assisted research and autonomous scientific discovery agents. To bridge this gap, we present MatCha, challenging multimodal benchmark for materials characterization imaging data understanding. The core strengths of MatCha are threefold: (1) practical and realistic task design: The design philosophy of MatCha originates from real-world scientific workflows. The tasks are derived directly from the research processes of materials scientists and are designed to reflect authentic challenges in practice. (2) task diversity and broad coverage: MatCha incorporates 21 sub-tasks, each representing concrete step within the scientific workflow. These tasks collectively cover wide range of characterization methods and corresponding problems. (3) expert-level difficulty: MatCha includes 1,500 multiple-choice questions of varying complexity, each requiring visual understanding and expertlevel scientific expertise. We first benchmark state-of-the-art MLLMs on MatCha under the zero-shot setting, observing substantial performance gap between models and human experts as well as noticeable performance degradation across different task stages due to limited generalization capability. Next, we further investigate whether these performance gaps can be bridged by in-context learning or by guiding the model through chain-of-thought (CoT) process. The results show that while some models do benefit from these strategies, others exhibit unstable or even degraded performance, and significant gap to human expert performance persists. In summary, our contributions are as follows: We introduce MatCha, the first multimodal perception and understanding benchmark for materials characterization, which comprises 21 expert-defined tasks reflecting real scientific challenges. We conduct extensive experiments on various proprietary and open-source models under different settings. We reveal their current limitations and directions for future enhancements through detailed quantitative and qualitative analyses."
        },
        {
            "title": "2 Related Works",
            "content": "Materials Characterization Analysis. Computer vision has revolutionized the extraction of visual information and quantitative analysis from material microscopy images. Applications include image-based structure recognition (Abouelatta, 2013; DeCost and Holm, 2015; Chowdhury et al., 2016; DeCost et al., 2017), detection of individual atomic sites and defects (Madsen et al., 2018; Li et al., 2018; Yang et al., 2021; Shen et al., 2021), and segmentation of microstructures or particles (DeCost et al., 2019; Roberts et al., 2019; Baskaran et al., 2020; Bals and Epple, 2023). However, existing studies largely focus on perception in specific electron microscopes, neglecting deeper imaging content and cross-modal data analysis, which results in gap with actual scientific research problems. Additionally, material science characterization data significantly differ from natural images, thus limiting generalization between tasks. Furthermore, the scarcity of large labeled datasets impedes supervised deep learning model training (Holm et al., 2020). Our research seeks to leverage and investigate the capabilities of MLLMs to address the diverse challenges in characterization data analysis and elucidation, thereby tackling real-world problems faced by materials scientists. Multimodal Benchmarks in Science. Recent advancements in MLLMs (Liu et al., 2023; Zhu et al., 2023; Li et al., 2023; Dai et al., 2023) have spurred the development of benchmarks to evaluate their scientific problem-solving capabilities. For example, MMMU (Yue et al., 2024a) 2 and MMMU-Pro (Yue et al., 2024b) present extensive multi-discipline college-level tasks. In materials and chemistry, MaScQA (Zaki et al., 2024) offers text-based question dataset, while MaCBench (Alampara et al., 2024) proposes multimodal benchmark. However, MaCBench (Alampara et al., 2024) concentrates on chemistry and general laboratory knowledge, with limited content on crystalline materials. SciFIBench (Roberts et al., 2024) targets scientific figure interpretation using arXiv papers, which do not cover materials science or chemistry extensively (Hsu et al., 2021; Li et al., 2024a) and, being non-peer-reviewed, may have quality concerns. While MMSci (Li et al., 2024b) sources its content from Nature Communications, its task formatscaptioning and figurecaption matchingare not representative of the queries posed in actual scientific research, which limits its practical applicability. MatCha, in contrast, is designed with problem formats that authentically mirror the challenges scientists face during the process of scientific discovery."
        },
        {
            "title": "3 The MatCha benchmark",
            "content": "We introduce MatCha, comprehensive and challenging benchmark for advancing multimodal materials characterization visual analysis and understanding. MatCha comprises 1,500 questions across 21 tasks, reflecting expert-level difficulty and grounded in real-world scientific scenarios faced by scientists. In the following sections, we elaborate on how we construct tasks (3.1), collect data (3.2), generate questions (3.3), and analyze its composition (3.4). The holistic construction pipeline is shown in Fig. 1."
        },
        {
            "title": "3.1 Task Construction",
            "content": "To cover wide range of domain-specific knowledge for comprehensive evaluation, we collaborate with experienced researchers in materials science. Drawing from the typical workflow of materials science research\"Processing\" \"Morphology\" \"Structure\" \"Property\"we designed corresponding chain of stages that reflects this logical progression. The details are as follows: Morphology Analysis (MA). This stage focuses on observing and assessing the surface or crosssectional of materials to gather morphological information. It measures the capability of models to perceive both macroand micro-scale visual characteristics in electron microscopy images. Structure Analysis (SA). It targets the interpretation of material structure at the microor atomicscale, which is essential for understanding the underlying mechanisms of various properties, such as mechanical behavior. It further assesses the ability to integrate and link cross-modal knowledge, for instance, linking spectral peaks to chemical bonds or functional groups. Property Analysis (PA). Since the structure of material determines its properties, this stage poses more difficult challenge, by evaluating the logical reasoning capabilities in connecting structural features with properties in physical and chemical. To concretize these research stages, we define several scientifically meaningful sub-tasks for each, as detailed in A. These stages represent the essential steps that materials scientists typically undertake, while the sub-tasks capture diverse range of real-world challenges encountered during new materials development. Evaluating MLLMs on these tasks is crucial to reveal their potential and limitations in scientific research, providing critical insights into their capability boundaries and guiding future advancement for applications."
        },
        {
            "title": "3.2 Data Curation",
            "content": "Data Collection. Given the diversity of characterization data, firstly, we define set of advanced search terms and their synonyms that exhaustively cover the types of data required across the various sub-tasks. With these terms, we employ Exsclaim (Schwenker et al., 2021) to search for and retrieve publicly accessible articles from the Nature platform under CC BY-4.0 license. The search results are sorted by relevance. For each article, we download the HTML file along with all associated figures and their corresponding captions. We crawl 340 articles containing 2,165 figures in total. Processing Correlation (PC). As foundational step in materials characterization, it involves identifying the characterization technique and its intended purpose. It evaluates the ability to accurately awareness characterization methods and their appropriate application contexts. Data Processing. First, we query GPT-4o with prompts presented in H.1 to segment the full caption of each figure into sub-captions corresponding to sub-figures. Second, Exsclaim (Schwenker et al., 2021) is applied to split each figure into its constituent sub-figures and assign the appropri3 Figure 1: MatCha construction pipeline. First, experts define scientifically meaningful, practical tasks and extract key terms. Second, data is collected and processed using these terms. Third, GPT-based generation and template-based conversion are employed to construct samples from the gathered data, followed by quality filtering and review. ate sub-captions to each one. The resulting subfigures are categorized. To ensure the authenticity of the dataset, we retain categories that accurately reflect real-world materials characterization scenarios and exclude other simulated types of subfigures, such as illustrations. To compensate for insufficient information in the subsequent VQA generation caused by the overly short sub-captions, we use parser from (Toland et al., 2023) to parse HTML files of the articles and extract the main body text. We then develop regular expression matching function to retrieve the context relevant to each sub-figure from the main content. Finally, these remaining article contents and images form the basis for the subsequent question generation. Data Supplementation. Figures in published papers often inevitably contain annotations or markings that may hint at microstructural characteristics. Moreover, although we download highresolution figures directly, their visual clarity can still be inferior to images captured directly from characterization instruments. To alleviate this and enhance benchmark diversity and challenge, we source additional non-simulated, human-annotated datasets. Following rigorous expert review and filtering, three datasets (Hecht et al., 2017; Baskaran et al., 2020; Dennler et al., 2021) are selected as supplementary data sources. These datasets consist of high-quality, real-world electron microscopy images, and are used to construct supplementary sub-tasks: surface microstructure analysis (Suppl. SMA), defect type classification (Suppl. DTC), and image content analysis (Suppl. ICA). Specifically, Suppl. SMA requires analyzing the microstructural features of Ti-6Al-4V alloy images; Suppl. DTC examines defect identification and distinguishment between different defect types; and Suppl. ICA assesses the analysis of primary microstructural components in low-carbon steel. These supplementary tasks focus on common microstructural 4 analyses in practical yet fundamental materials scenarios, evaluating the domain-specific knowledge and visual perception capabilities of MLLMs."
        },
        {
            "title": "3.3 Question Generation",
            "content": "We formulate our benchmark in closed-ended VQA format. This approach facilitates easier analysis compared to open-ended VQA and obviates the need for LLMs in automatic evaluation. Consequently, it eliminates subjectivity in answer assessment, ensuring that evaluation results more accurately reflect the true performance of MLLMs. Generated VQA. Equipped with collected (subfigure, sub-caption, context) triplets, we employ GPT-4o to generate multiple-choice questions. Considering the unique nature of images in each article and to prevent overly image-specific or divergent questions lacking generalizability, we carefully design prompt that constrains VQA generation within our predefined sub-task scopes. However, we do not restrict the number of answer choices, allowing the model to fully utilize the unique information in each data triplet, as detailed in H.2. Each generated VQA sample includes sub-figure, question within sub-task, and set of answer choices, where one is correct and the others are distractors generated by GPT-4o. This process finally yields 26,891 samples, which subsequently undergo data filtering and expert review. Converted VQA. For the three supplementary datasets, we convert their metadata and labels into multiple-choice questions using QA template. Similarly, each converted VQA sample consists of an image, question designed under certain sub-task, and several answer choices. One choice is the correct answer derived from the original label, while the remaining options are distractors sampled from the label set, excluding the correct one. Data filtering. For the generated VQA samples, we perform coarse-grained filtering using AI experts as the first step. Concretely, we use Qwen2.5-VL-7B (Wang et al., 2024a), InternVL38B (Chen et al., 2024c), and LLaMA-3.2-11BVision (Grattafiori et al., 2024) to each attempt the question three times. If all AI experts answer the question correctly in all attempts, the question is deemed too simple and removed. The remaining questions are retained. This process effectively filters out easy questions and preserves those both challenging and discriminative, ensuring quality. Experts review. Following coarse filtering by AI experts, two materials science expertsPh.D. candidates from the material science and engineering departmentconduct manual review to ensure: 1) each figure is real photograph or data-generated plot, not simulation or illustration, ensuring data authenticity; 2) each question is grounded in visual content, answerable solely through visual cues and reasoning with intrinsic domain knowledge, without requiring external contextual information, ensuring question validity; 3) overly simple optical character recognition (OCR)-style questions lacking domain-specific expertise are removed to emphasize professionalism and challenge, reflecting realistic scenarios for materials researchers. After this, 994 samples remain, forming the core of MatCha. Additionally, as the three supplementary datasets have already undergone manual validation and annotation, we randomly select 506 samples from their converted VQA set. Together, these comprise total of 1,500 samples that constitute the complete benchmark. In G.1, we provide some examples as illustrations."
        },
        {
            "title": "3.4 Analysis of MatCha Benchmark",
            "content": "Scientific discovery in materials science demands multiple and complex characterizations beyond basic natural image perception and shallow domain expertise. To investigate the diversity and representativeness of our benchmark, we count and analyze the distribution of sub-tasks, characterization techniques, and material types. detailed statistical breakdown is provided in B. The quantitative statistics show that the dataset encompasses wide array of methods. The material types include metallic materials, inorganic non-metallic materials, composites, and organic polymers. Furthermore, the collected source articles are retrieved from 14 different journals under the Nature platform, with publication dates ranging from 2015 to 2025, mitigating potential biases towards specific research topics or time periods. This broad scope ensures MatCha reflects wide range of real-world scientific challenges. The statistics of different subtask samples are shown in Fig. 2 and Fig. 3."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Evaluation models. We use random selection as baseline, where randomly chosen option is treated as the answer. We evaluate diverse set of 5 Statistics MatCha Instances Total Images Total QA samples Average size (px) Maximum size (px) Processing Correlation - questions - average length - options Morphology Analysis - questions - average length options - - # Among them, for the supplementary - questions - average length - options Structure Analysis - questions - average length options - Property Analysis - questions - average length - options Value 1, 260 1, 500 559 660 1876 1064 153 160 4 : 153 795 236 7 : 206, 5 : 4, 4 : 308, 3 : 275, 2 : 2 506 271 7 : 206, 4 : 33, 3 : 267 370 187 6 : 1, 5 : 6, 4 : 345, 3 : 15, 2 : 182 194 5 : 4, 4 : 163, 3 : 12, 2 : 3 Figure 2: Statistical overview of MatCha samples. Superscripts indicate the number of multiplechoice options for those questions. Figure 3: Composition of the MatCha benchmark, illustrating the proportion and distribution statistics of its 21 sub-tasks across the four progressive research stages: Processing Correlation (PC), Morphology Analysis (MA), Structure Analysis (SA), and Property Analysis (PA). MLLMs, including the following proprietary models: GPT-{4o (Achiam et al., 2023), 4o-mini (Hurst et al., 2024)}, Gemini-1.5-{Pro (Team et al., 2024), Flash (Team et al., 2024)}, Claude-3.5-Sonnet (Anthropic, 2024), Llama-4-Maverick (Meta, 2025). We also evaluate the following popular open-source models: LLaVa-1.5 series (Liu et al., 2024), Qwen2.5-VL series (Bai et al., 2025), InternVL-3 series (Chen et al., 2024b), Llama-3.2-Vision (Grattafiori et al., 2024), Janus-Pro (Chen et al., 2025), and Gemma-3 (Team et al., 2025). Notably, we choose their chat or instruction-tuned version for each model for better capability of instruction following. Implementation details. Since all questions are in multiple-choice format, we instruct the models to constrain their responses to the provided option letters. detailed prompt can be found in H.3. For proprietary models, inference is conducted via the API platform. For open-source models, we utilize the Transformers library (Wolf et al., 2020) and the LLaMA-Factory toolkit (Zheng et al., 2024) to perform inference on NVIDIA RTX 6000 GPUs. More details are presented in C. To better understand the performance gap between models and humans, we invite doctoral researchers familiar with various characterization techniques from the material science and engineering department to participate in tests as human baseline. They are presented with the same format of questions and instructions as the models to ensure fair comparison."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "We adopt generic zero-shot strategy for evaluation, and the quantitative results of 6 proprietary models and 9 open-source models with varying sizes and architectures are in Tab. 1. Considering the different sources of data, we report the Generated VQA subset and the Converted VQA subset, respectively. The main results are as follows: MatCha represents challenging benchmark. On the generated VQA subset, the best-performing model, GPT-4o, achieves only 62.58% accuracy, exhibiting 26.29% gap from human performance (88.87%). When facing the more difficult converted VQA subset, the top-performing model, LLaMA-4Maverick, scores only 57.71%, significantly lower than the 88.93% accuracy attained by human experts. These results demonstrate that MatCha provides well-calibrated level of difficulty and highlight the considerable performance gap between current MLLMs and human experts in materials characterization. no-image ablation study is also conducted in to validate that the questions in MatCha are strongly grounded in visual content. Proprietary models outperform open-source models. Considering the overall tasks in the generated VQA subset, the leading proprietary model outperforms its open-source counterpart by 9.96%"
        },
        {
            "title": "Model",
            "content": "Random Choice Human GPT-4o (Achiam et al., 2023) GPT-4o-mini (Hurst et al., 2024) Gemini-1.5-Flash (Team et al., 2024) Gemini-1.5-Pro (Team et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) LlaMA-4-Maverick (Meta, 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) InternVL3-8B (Chen et al., 2024b) InternVL3-38B (Chen et al., 2024b) LLaVA-1.5-7B (Liu et al., 2024) LLaVA-1.5-13B (Liu et al., 2024) Llama-3.2-11B-Vision (Grattafiori et al., 2024) Janus-Pro-7B (Chen et al., 2025) Gemma-3-4b-it (Team et al., 2025)"
        },
        {
            "title": "Generated VQA\nPA\nSA",
            "content": "Baselines"
        },
        {
            "title": "All",
            "content": "Suppl. SMA Suppl. DTC Suppl. ICA"
        },
        {
            "title": "MatCha\nAll",
            "content": "19.61 90.26 26.64 89.31 23.24 87.57 24.72 88.59 15.79 88.87 85.62 66.67 75.16 86.27 83.66 79. 66.01 69.28 41.83 67.97 22.22 43.14 60.13 48.37 60.13 70.59 61.59 62.63 70.24 71.28 62.98 57.44 66.44 50.87 66.44 26.99 40.14 40.48 49.13 47.06 Proprietary Models 71.62 57.57 56.76 67.03 66.49 60.81 66.48 51.65 54.95 62.64 68.68 59. 62.58 45.27 48.39 59.76 61.37 53.32 Open-source Models 54.05 60.81 48.38 58.38 20.00 38.92 40.27 51.08 43.24 53.85 66.48 48.90 60.99 29.12 37.36 41.21 53.85 41.21 43.46 52.62 36.12 49.70 15.90 29.18 31.39 39.54 34.41 34.83 94. 60.30 54.31 46.82 55.43 50.56 69.66 45.69 58.43 46.82 63.30 41.57 33.71 38.95 25.09 39.33 24.24 87.88 63.64 48.48 48.48 66.67 75.76 48.48 54.55 54.55 54.55 54.55 24.24 27.27 12.12 48.48 45.45 15.53 81. 23.91 88.93 24.73 88.89 39.81 18.93 31.07 39.81 46.60 43.69 20.87 25.24 21.36 23.79 14.56 14.56 18.45 19.90 25.73 52.17 39.53 40.51 49.80 50.59 57.71 36.17 44.66 36.96 46.64 29.45 25.49 28.85 24.51 34. 59.07 43.33 45.73 56.40 57.73 54.80 41.00 49.93 36.40 48.67 20.47 27.93 30.53 34.47 34.33 Table 1: Evaluation results of model performance on MatCha. Generated VQA: PC (processing correlation), MA (morphology analysis), SA (structure analysis), PA (property analysis). Converted VQA: Suppl. SMA (supplementary surface microstructure analysis), Suppl. ICA (supplementary image content analysis), Suppl. DTC (supplementary defect type classification). Bolded values signify the optimal in-class outcomes (open-source or proprietary) and underlined values indicate the suboptimal performance. (62.58% to 52.62%), indicating considerable performance gap. This disparity widens to 11.07% (57.71% to 46.64%) in the converted VQA subset. Overall, open-source models explicitly exhibit low performance across both generated and converted VQA subsets, with most models failing to correctly answer more than 40% of the questions. Performance and generalization capability disparities exist across models. Models show considerable differences in performance across task dimensions, and even within the same model, there are marked fluctuations across different research stages (e.g., PC vs. PA). These results suggest that current models struggle with task generalization and knowledge transfer within the materials science domain."
        },
        {
            "title": "4.3 Analysis",
            "content": "Most models still struggle with relatively simple perceptual tasks. Some proprietary models, such as Gemini-1.5-Pro, GPT-4o, and Claude-3.5Sonnet, lead in tasks from the PC and MA stages. Gemini-1.5-Pro, for instance, trails human performance by only 3.99% on the PC stage, indicating its reasonable ability to handle basic characterization techniques. However, even the bestperforming model on the MA stageClaude-3.5Sonnetstill lags behind human experts by approximately 18.03%. This highlights that, despite recent advancements in understanding natural images, current cutting-edge MLLMs remain insufficient for achieving expert-level pattern recognition in finegrained morphological analysis of materials imaging. We attribute this to the current lack of highquality scientific training corpora, which hinders models from generalizing the ability developed on natural images to these specialized tasks. Nevertheless, beyond these three models, the performance of most other proprietary and opensource models remains suboptimal. For example, most open-source models failed to exceed 50% accuracy for tasks in the MA stage. This suggests that many current open-source models still struggle to extract and interpret morphological features from electron microscopy images, particularly when recognizing multiscale structures. However, Qwen2.5-VL-32B and InternVL3-38B achieve notable 66.44% on the MA stage, outperforming certain proprietary models such as GPT-4o-mini (61.91%). This indicates the potential for opensource models to match or even surpass generalpurpose proprietary models in specialized scientific domains when properly optimized. Most models degrade on tasks requiring more expertise and reasoning ability. Benefiting from our stage-based task design, we can conduct fine-grained analysis of both proprietary and opensource models across different materials research 7 Figure 4: Performance analysis. Left: Confusion matrices of several models on the Suppl. DTC task. Right: Performance degradation trends of models across progressive stages. stages, as this design aligns well with the research workflow of materials scientists. As shown in the right figure of Fig. 4, we observe clear trend: as tasks progress from morphology analysis to structure interpretation and properties elucidation stages requiring deeper materials domain knowledge and reasoning abilitymost models exhibit significant decline in performance. Interestingly, this performance deterioration is more pronounced among proprietary MLLMs. On average, proprietary models show 15.96% accuracy degradation across the MA, SA, and PA stages. In contrast, open-source models demonstrate smaller average decline of 10.29%. Consequently, several open-source models, including Qwen2.5-VL-7B, Qwen2.5-VL-32B, and InternVL3-38B, have outperformed certain proprietary models in the MA, SA, and PA stages, in some cases approaching the accuracy of the best proprietary models. We hypothesize that this may be related to the incorporation of larger proportion of scientific imagery or domain-specific textual data during training, particularly in scientific discourse. These findings suggest that current mainstream MLLMs still lack sufficient materials science domain-specific understanding and multi-level reasoning capabilities, rendering them inadequate for research tasks that demand deep materials expertise and complex logical analysis. This limitation constrains their potential for broader application in scientific research support. terization images, current MLLMs encounter substantial challenges despite these tasks being relatively basic for human experts. In the Suppl. SMA task, LLaMA-4-Maverick achieves the best performance with an accuracy of 69.66%, still approximately 25.1% lower than human performance. Open-source models struggle to identify the microstructural hierarchy in alloy images, with most accuracies falling between 30% and 50%. In the Suppl. DTC task, which requires the detection and classification of subtle structural defects, both proprietary and open-source models exhibited significant difficulties. Many failed to determine the presence of defects or distinguish between defects from different origins, indicating breakdown in their recognition capabilities, as shown in the left figure of Fig. 4. The Suppl. ICA task poses an even greater challenge, demanding fine-grained differentiation of multiple microstructural components in low-carbon steel. This task places high demands on both visual discrimination and metallography knowledge. All open-source models achieve accuracies below 30%, and proprietary models also perform suboptimally, revealing severe limitations in the understanding and reasoning abilities of current models regarding materials microstructure. Notably, GPT-4o consistently ranked among the top three performers across all three supplementary tasks, suggesting relatively stronger generalization capability. This may be attributed to its more advanced multimodal alignment mechanisms and broader pretraining data coverage. Models fail to recognize microstructural details in more realistic and complex perceptual scenarios. In the converted VQA subset, which involves real-world materials electron microscopy charac-"
        },
        {
            "title": "4.4 Impact of Few-shot and Chain-of-Thought",
            "content": "To investigate whether the performance limitations observed in the zero-shot setting can be mitigated, 8 we conduct further experiments using few-shot incontext learning and CoT prompting. The detailed results of performance across various few-shot settings and CoT are presented in D. Few-shot Learning Our few-shot experiments reveal that the ability to leverage in-context examples varies dramatically across models. On the one hand, some models show significant positive scaling. GPT-4o, for instance, demonstrates remarkable improvement on the challenging Converted VQA subset, with its accuracy jumping from 52.17% (zero-shot) to 73.52% (16-shot). Other models like Claude-3.5-Sonnet and GPT-4o-mini also show clear upward trend. This suggests that for some models, in-context examples can effectively elicit domain-specific knowledge and improve pattern recognition. On the other hand, this benefit is not universal and can even be detrimental. For Gemini-1.5-Pro, performance is inconsistent and can even degrade at higher shot counts. The effect is more severe for Qwen2.5-VL-32B, which suffers notable performance drop when provided with examples. These cases indicate that for certain models, in-context examples may introduce noise or conflict with their internal knowledge, making the effective utilization of such prompts non-trivial challenge. Chain-of-Thought Prompting Our evaluation of CoT prompting reveals mixed results. On one hand, most models show slight improvement with step-by-step reasoning. Janus-Pro, for instance, sees the most significant benefit, with its accuracy increasing by nearly 6 percentage points to 40.27%. On the other hand, for top-performing models like Gemini-1.5-Pro and Llama-4-Maverick, CoT prompting actually hindered performance compared to the direct zero-shot approach. These contradictory outcomes suggest that CoT is not universally effective strategy. While it can help some models, it may conflict with the internal reasoning pathways of others. This reinforces our finding that simple prompting techniques, such as in-context learning and CoT, are insufficient to overcome fundamental gaps in intrinsic domain knowledge and visual perception. Therefore, other techniques, such as the retrieval mentioned in F, remain for future exploration. Table 2: Distribution of error types for three models. Model Visual Perception Error Lack of Material Knowledge Language & Logic Failure Image-Text Alignment Error GPT-4o (Achiam et al., 2023) GPT-4o-mini (Hurst et al., 2024) Gemini-1.5-Pro (Team et al., 2024) 27% 34% 32% 71% 59% 64% 1% 5% 2% 1% 2% 2% GPT-4o and categorize the primary cause of error using GPT-4o. We predefine four main error types: 1) Visual Perception Error, where the model fails to correctly identify visual features, such as structures, boundaries, or grains; 2) Lack of Material Knowledge, where the model misunderstands or lacks necessary materials domain concepts; 3) Language and Logic Failure, involving errors in explanation or the semantic understanding of the question and options; and 4) Image-Text Alignment Misunderstanding, where the model incorrectly links image content to the question or options text. As shown in Tab. 2, the predominant error category across all models is Lack of Material Knowledge, accounting for over 60-70% of failures, confirming that core limitation of current MLLMs is their deficiency in specialized scientific domain knowledge. Visual Perception Error is the second most common failure type, highlighting that even leading models struggle with the fine-grained and complex visual patterns in scientific imagery, conclusion that aligns with our previous analysis. We provide error cases for illustration in G.2."
        },
        {
            "title": "5 Conclusion",
            "content": "Automated interpretation of complex materials characterization imaging data remains significant bottleneck. In this work, we introduce MatCha, the first benchmark designed to assess the capability of current MLLMs to understand materials characterization imagery. We construct suite of realistic and scientifically meaningful tasks to evaluate state-ofthe-art MLLMs. Our comprehensive evaluations pinpoint their inadequacy for generalization, deep domain knowledge, complex morphology perception, and nuanced materials analysis. MatCha thus serves as critical tool for diagnosing these core deficiencies, aiming to guide the development of MLLMs that can truly accelerate materials research and enable autonomous scientific discovery."
        },
        {
            "title": "4.5 Error Analysis",
            "content": "To gain deeper insights into the nature of model failures, we sample 100 incorrect predictions from Although MatCha aims for broad coverage with 21 sub-tasks, the vast field of materials science means it cannot encompass every material, characteriza9 tion technique, experimental nuance, or specific research question. Certain emerging areas or highly specialized analyses might be underrepresented. Meanwhile, the dataset of 1,500 questions, while reviewed by experts, might inadvertently contain biases or not fully capture the statistical distribution of all real-world scenarios."
        },
        {
            "title": "Potential Risks",
            "content": "While MatCha is designed to rigorously assess the specialized knowledge and visual understanding capabilities of MLLMs within the domain of materials characterization, potential risk or limitation lies in its direct applicability to all professional scenarios without further consideration. High performance on MatCha indicates strong foundational capabilities, but evaluating MLLMs in specific operational laboratory or industrial settings may necessitate adaptation or fine-tuning to those unique professional contexts. Real-world applications often involve distinct instrument variations, proprietary data intricacies, evolving experimental procedures, or specific analytical goals not exhaustively covered by any standardized benchmark. Therefore, there is risk that strong MatCha scores might be overgeneralized, and deploying these MLLMs effectively into diverse, practical workflows will likely still require careful contextual adjustments and validation to ensure optimal and reliable performance in those specialized environments."
        },
        {
            "title": "Acknowledgments",
            "content": "and"
        },
        {
            "title": "Technology",
            "content": "supported by the ShenThis work was zhen Program (JCYJ20220818103001002), Shenzhen Doctoral Startup Funding (RCBS20221008093330065), Tianyuan Fund for Mathematics of National Natural Science Foundation of China (NSFC) (12326608), Shenzhen Science and Technology Program (Shenzhen Key Laboratory Grant No. ZDSYS20230626091302006), and Shenzhen Stability Science Program 2023, Shenzhen Key Lab of Multi-Modal Cognitive Computing. The authors thank the financial support from the National Natural Science Foundation of China (Grant no. 22302174) and the Natural Science Foundation of Zhejiang Province (Grant no. LZ25E030005). Dr. Hong-Qing Liang acknowledges gratefully the research startup package from Zhejiang University."
        },
        {
            "title": "References",
            "content": "Ossama Abouelatta. 2013. Classification of copper alloys microstructure using image processing and neural network. Journal of American Science, 9(6):213 223. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Nawaf Alampara, Indrajeet Mandal, Pranav Khetarpal, Hargun Singh Grover, Mara Schilling-Wilhelmi, NM Anoop Krishnan, and Kevin Maik Jablonka. 2024. Macbench: multimodal chemistry and maIn Proceedings of the terials science benchmark. 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf. Luis Antunes, Keith Butler, and Ricardo GrauCrespo. 2024. Crystal structure generation with autoregressive large language modeling. Nature Communications, 15(1):10570. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl technical report. Jonas Bals and Matthias Epple. 2023. Deep learning for automated size and shape analysis of nanoparticles in scanning electron microscopy. RSC advances, 13(5):27952802. Arun Baskaran, Genevieve Kane, Krista Biggs, Robert Hull, and Daniel Lewis. 2020. Adaptive characterization of microstructure dataset using two stage machine learning approach. Computational Materials Science, 177:109593. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and 1 others. 2024a. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. 2025. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, and 1 others. 2024b. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, and 1 others. 2024c. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198. Aritra Chowdhury, Elizabeth Kautz, Bülent Yener, and Daniel Lewis. 2016. Image driven machine learning methods for microstructure recognition. Computational Materials Science, 123:176187. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:4925049267. Brian DeCost, Toby Francis, and Elizabeth Holm. 2017. Exploring the microstructure manifold: image texture representations applied to ultrahigh carbon steel microstructures. Acta Materialia, 133:3040. Brian DeCost and Elizabeth Holm. 2015. computer vision approach for automated analysis and classification of microstructural image data. Computational materials science, 110:126133. Brian DeCost, Bo Lei, Toby Francis, and Elizabeth Holm. 2019. High throughput quantitative metallography for complex microstructures using deep learning: case study in ultrahigh carbon steel. Microscopy and Microanalysis, 25(1):2129. Nik Dennler, Antonio Foncubierta-Rodriguez, Titus Neupert, and Marilyne Sousa. 2021. Learning-based defect recognition for quasi-periodic hrstem images. Micron, 146:103069. Qianggang Ding, Santiago Miret, and Bang Liu. 2024. Matexpert: Decomposing materials discovery by mimicking human experts. arXiv preprint arXiv:2410.21317. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. 2020. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286. Matthew Hecht, Brian DeCost, Toby Francis, Elizabeth Holm, Yoosuf Picard, Bryan Webler, and 1 others. 2017. Ultrahigh carbon steel micrographs. Elizabeth Holm, Ryan Cohn, Nan Gao, Andrew Kitahara, Thomas Matson, Bo Lei, and Srujana Rao Yarasi. 2020. Overview: Computer vision and machine learning for microstructural characterization and analysis. Metallurgical and Materials Transactions A, 51(12):59855999. Ting-Yao Hsu, Lee Giles, and Ting-HaoKenneth Huang. 2021. Scicap: Generating captions for scientific figures. arXiv preprint arXiv:2110.11624. Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas Montine, and James Zou. 2023. visual language foundation model for pathology image analysis using medical twitter. Nature medicine, 29(9):23072316. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Beverley Inkson. 2016. Scanning electron microscopy (sem) and transmission electron microscopy (tem) for materials characterization. In Materials characterization using nondestructive evaluation (NDE) methods, pages 1743. Elsevier. Jakub Lála, Odhran ODonoghue, Aleksandar Shtedritski, Sam Cox, Samuel Rodriques, and Andrew White. 2023. Paperqa: Retrieval-augmented generative agent for scientific research. arXiv preprint arXiv:2312.07559. Andreas Leitherer, Byung Chul Yeo, Christian Liebscher, and Luca Ghiringhelli. 2023. Automatic identification of crystal structures and interfaces via artificial-intelligence-based electron microscopy. npj Computational Materials, 9(1):179. Yang Leng. 2013. Materials characterization: introduction to microscopic and spectroscopic methods. John Wiley & Sons. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024a. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231. Wei Li, Kevin Field, and Dane Morgan. 2018. Automated defect analysis in electron microscopic images. npj Computational Materials, 4(1):36. Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, and Yu Kong. 2025. Visual large language models for generalized and specialized applications. arXiv preprint arXiv:2501.02765. 11 Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, and 1 others. 2024b. Mmsci: dataset for graduate-level multidiscipline multimodal scientific understanding. Andre Niyongabo Rubungo, Craig Arnold, Barry Rand, and Adji Bousso Dieng. 2023. Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions. arXiv preprint arXiv:2310.14029. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306. Eric Schwenker, Weixin Jiang, Trevor Spreadbury, Nicola Ferrier, Oliver Cossairt, and Maria KY Chan. 2021. Exsclaim!an automated pipeline for the construction of labeled materials imaging datasets from literature. arXiv preprint arXiv:2103.10631. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Alejandro Lozano, Jeffrey Nirschl, James Burgess, Sanket Rajan Gupte, Yuhui Zhang, Alyssa Unell, and Serena Yeung. 2024. Micro-bench: microscopy benchmark for vision-language understanding. Advances in Neural Information Processing Systems, 37:3067030685. Jacob Madsen, Pei Liu, Jens Kling, Jakob Birkedal Wagner, Thomas Willum Hansen, Ole Winther, and Jakob Schiøtz. 2018. deep learning approach to identify local structures in atomic-resolution transmission electron microscopy images. Advanced Theory and Simulations, 1(8):1800037. Artem Maksov, Ondrej Dyck, Kai Wang, Kai Xiao, David Geohegan, Bobby Sumpter, Rama Vasudevan, Stephen Jesse, Sergei Kalinin, and Maxim Ziatdinov. 2019. Deep learning analysis of defect and phase evolution during electron beaminduced transformations in ws2. npj Computational Materials, 5(1):12. AI Meta. 2025. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. Vaibhav Mishra, Somaditya Singh, Dhruv Ahlawat, Mohd Zaki, Vaibhav Bihani, Hargun Singh Grover, Biswajit Mishra, Santiago Miret, NM Krishnan, and 1 others. 2024. Foundational large language models for materials research. arXiv preprint arXiv:2412.09560. Graham Roberts, Simon Haile, Rajat Sainju, Danny Edwards, Brian Hutchinson, and Yuanyuan Zhu. 2019. Deep learning for semantic segmentation of defects in advanced stem images of steels. Scientific reports, 9(1):12744. Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. 2024. Scifibench: Benchmarking large multimodal models for scientific figure interpretation. Advances in Neural Information Processing Systems, 37:1869518728. Mingren Shen, Guanzhao Li, Dongxia Wu, Yuhan Liu, Jacob RC Greaves, Wei Hao, Nathaniel Krakauer, Leah Krudy, Jacob Perez, Varun Sreenivasan, and 1 others. 2021. Multi defect detection and analysis of electron microscopy images with deep learning. Computational Materials Science, 199:110576. Yingheng Tang, Wenbin Xu, Jie Cao, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael Mahoney, Andy Nonaka, and Zhi Yao. 2025. Matterchat: multi-modal llm for material science. arXiv preprint arXiv:2502.13107. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, and 1 others. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Aubrey Toland, Huan Tran, Lihua Chen, Yinghao Li, Chao Zhang, Will Gutekunst, and Rampi Ramprasad. 2023. Accelerated scheme to predict ring-opening polymerization enthalpy: simulation-experimental data fusion and multitask machine learning. The Journal of Physical Chemistry A, 127(50):10709 10716. Prateek Verma, Minh-Hao Van, and Xintao Wu. 2024. Beyond human vision: The role of large vision language models in microscope image analysis. In 2024 IEEE International Conference on Big Data (BigData), pages 17001705. IEEE. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, and 1 others. 2024a. Qwen2vl: Enhancing vision-language models perception arXiv preprint of the world at any resolution. arXiv:2409.12191. Ian Robertson, Christopher Schuh, John Vetrano, Nigel Browning, David Field, Dorte Juul Jensen, Michael Miller, Ian Baker, David Dunand, Rafal Dunin-Borkowski, and 1 others. 2011. Towards an integrated materials characterization toolbox. Journal of Materials Research, 26(11):13411383. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, and 1 others. 2024b. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499. Małgorzata Warmuzek, Marcin Zelawski, and Tomasz Jałocha. 2021. Application of the convolutional neural network for recognition of the metal alloys microstructure constituents based on their morphological characteristics. Computational Materials Science, 199:110722. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and 1 others. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 3845. Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, and 1 others. 2023. Darwin series: Domain specific large language models for natural science. arXiv preprint arXiv:2308.13565. Tong Xie, Yuwei Wan, Yixuan Liu, Yuchen Zeng, Shaozhou Wang, Wenjie Zhang, Clara Grazian, Chunyu Kit, Wanli Ouyang, Dongzhan Zhou, and 1 others. 2024. Darwin 1.5: Large language models as materials science adapted learners. arXiv preprint arXiv:2412.11970. Sang-Hyeok Yang, Wooseon Choi, Byeong Wook Cho, Frederick Osei-Tutu Agyapong-Fordjour, Sehwan Park, Seok Joon Yun, Hyung-Jin Kim, Young-Kyu Han, Young Hee Lee, Ki Kang Kim, and 1 others. 2021. Deep learning-assisted quantification of atomic dopants and defects in 2d materials. Advanced Science, 8(16):2101099. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, and 1 others. 2024a. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556 9567. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, and 1 others. 2024b. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813. Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, and Bang Liu. 2024. Honeycomb: flexible llm-based agent system for materials science. arXiv preprint arXiv:2409.00135. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient finetuning of 100+ language models. arXiv preprint arXiv:2403.13372. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592. Sub-task Details We present each stage along with its corresponding sub-tasks and explanation in Tab. 3, all defined by materials scientists. These sub-tasks are designed to comprehensively encompass common challenges and characterization techniques encountered in materials characterization."
        },
        {
            "title": "B Dataset Statistics and Diversity",
            "content": "Based on our statistics, the papers used in MatCha are sourced from the following journals: Communications Earth & Environment, Communications Materials, Light: Science & Applications, NPG Asia Materials, Nature Biotechnology, Nature Communications, Nature Materials, Nature Photonics, Nature Synthesis, Polymer Journal, Scientific Data, Scientific Reports, npj Computational Materials, and npj Heritage Science. The distribution statistics below show that the dataset encompasses wide range of mainstream characterization techniques and material types. These types are annotated by instructing GPT-4o and are then classified and merged by human experts accordingly. Previous datasets in the materials characterization domain are largely limited to techniques such as SEM and XRD, while MatCha integrates data across the spectrum of mainstream methods, making it the most diverse dataset of its kind to date. Mohd Zaki, NM Anoop Krishnan, and 1 others. 2024. Mascqa: investigating materials science knowledge of large language models. Digital Discovery, 3(2):313327. B.1 Distribution of Characterization Types The data in MatCha cover wide range of characterization techniques, categorized as follows: Alexander Zaloga, Vladimir Stanovov, Oksana Bezrukova, Petr Dubinin, and Igor Yakimov. 2020. Crystal symmetry classification from powder x-ray diffraction patterns using convolutional neural network. Materials Today Communications, 25:101662. Microscopy: Includes transmission electron microscopy (147), scanning electron microscopy (550), scanning transmission electron microscopy (83), optical microscopy (42), X-ray imaging/tomography (1), scanning 13 Stage Processing Correlation Sub-task Characterization Technique Identification Characterization Purpose Inference Material Classification Image Content Analysis Surface Microstructure Assessment Morphology Analysis Surface Roughness Assessment Defect Type Classification Grain/Pore Size Classification Crystallographic Data Inference Crystallinity Classification Multiphase Interface Assessment X-ray diffraction (XRD) Pattern Analysis Structure Analysis Phase Analysis Elemental Mapping Analysis Element Distribution Homogeneity Assessment Material Morphology/Composition Uniformity Assessment Physical and Chemical Properties Inference Mechanical Properties Analysis Property Analysis Thermal Analysis Infrared (IR) and Raman (RS) Spectral Analysis Explanation Determine the characterization technique used (e.g., SEM, TEM, XRD, AFM). Deduce the scientific purpose of using particular characterization technique. Infer the general category of the material (e.g., metal, ceramic, polymer, composite). Analyze microstructural content from electron microscope images to extract material characteristics. Determine structural features such as the presence of surface defects, the order of the crystal structure, and the existence of layered structures, etc. Evaluate whether the material surface appears smooth or rough, or determine the level of surface roughness. Recognize and classify defect types, such as dislocations, vacancies, stacking faults, grain boundaries, etc. Categorize the size scale of grains or pores (e.g., nanometer, micrometer, or millimeter range). Utilize unit cell parameters and lattice spacings to determine structural features, such as symmetry, space group, and overall lattice architecture. Assess whether the material is amorphous, polycrystalline, or single crystalline based on the image. Examine the presence of multiple phases or interfaces within the image, and analyze their structural and compositional features. Extract and analyze key information from XRD spectra, including peak positions and other characteristic features. Include phase identification and classification, interpretation of phase composition, assessment of phase homogeneity, determination of crystal structure and polymorphic forms, etc. Identify elements represented by different colors or regions in elemental mapping images, (e.g., EDS or EELS maps). Analyze the image to assess whether elements are uniformly distributed across the material. Assess the uniformity of material morphology and composition, such as component ratios, particle size distribution, the homogeneity of internal microstructures, etc. Predict potential physical or chemical properties of materials. Extract key parameters such as yield strength, ultimate tensile strength, and ductility from the stress-strain curve to assess material performance. Extract critical information from various thermal analysis methods (e.g., TGA, DTA, DSC). Elucidate the molecular structure and chemical composition, identify functional groups, and bond types associated with specific spectral peaks, etc. X-ray Photoelectron Spectroscopy (XPS) Spectrum Analysis Analyze XPS spectra to identify peak positions, determine elemental composition, and chemical states, and infer chemical structures. Table 3: MatCha taxonomy of sub-tasks. probe microscopy (11), atom probe tomography (5), focused ion beam-SEM (6), and X-ray photoemission electron microscopy (2). Spectroscopy: Includes Raman spectroscopy (42), photoluminescence/fluorescence spectroscopy (22), infrared spectroscopy (23), energy-dispersive X-ray spectroscopy (21), Xray photoelectron spectroscopy (9), nuclear magnetic resonance (10), time-resolved spectroscopy (8), X-ray absorption spectroscopy (6), ultraviolet-visible spectroscopy (5), mass spectrometry (3), extended X-ray absorption fine structure (3), electroluminescence (3), electron probe micro-Analysis (2), Fouriertransform spectroscopy (1), atomic absorption spectroscopy (1), Auger electron spectroscopy (1), electron energy loss spectroscopy (1), electron paramagnetic resonance (1), and cathodoluminescence (1). Diffraction and Scattering: Includes electron diffraction (68), X-ray diffraction (58), electron backscatter diffraction (18), smallangle X-ray scattering (8), transmission Kikuchi diffraction (3), grazing-incidence Xray diffraction (2), neutron diffraction (2), and reciprocal space mapping (1). Electrochemical Analysis: Includes general electrochemical tests (15), cyclic voltammetry (9), electrochemical impedance spectroscopy (3), voltammetry (2), cycling stability test (2), performance evaluation (Faradaic/Coulombic efficiency, 2), galvanostatic intermittent titration technique (1), and galvanostatic chargedischarge (1). Computation and Simulation: Includes microscopy/diffraction simulation (7), general simulation/computation (7), quantum chemistry calculation (4), first-principles calculation (2), optical simulation (1), phase diagram/thermodynamic calculation (1), finite element method (1), and molecular dynamics (1). Mechanical Testing: Includes stress-strain test (8), general mechanical testing (5), fracture toughness (1), scratch test (1), and nanoindentation (1). Thermal Analysis: Includes differential scanning calorimetry (4), thermal imaging (1), and thermal conductivity measurement (1). Magnetic Characterization: Includes magnetic measurement (3), and X-ray magnetic circular dichroism (2). Other and Performance Evaluation: Includes data/image analysis (65), electrical/optoelectronic device performance (18), elemental/compositional mapping (10), and physical property measurement (9). 10%) above random guessing. This underscores the significant role of visual information in MatCha and highlights the rigorous demands on the visual understanding capabilities of models. B.2 Distribution of Material Types"
        },
        {
            "title": "MatCha\nAll",
            "content": "Ablation no-image drop The benchmark covers four major categories of materials: metallic materials (683), inorganic nonmetallic materials (435), composite materials (167), and organic polymer materials (98). GPT-4o (Achiam et al., 2023) Gemini-1.5-Pro (Team et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) LlaMA-4-Maverick (Meta, 2025) 34.33 29.53 34.60 5.93 -24.74 -26.87 -23.40 -20.47 Table 9: No-image ablation study on MatCha. Future Direction: The Potential Role of Retrieval-Augmented Generation primary conclusion from our benchmark is that MLLMs are significantly constrained by lack of specialized domain knowledge, common challenge in vertical domains. promising solution is Retrieval-Augmented Generation (RAG), which allows models to access external knowledge bases. RAG has already shown success in scientific domains, such as PaperQA (Lála et al., 2023), which answers complex questions by retrieving information from scientific articles. Our few-shot experiments indicate that providing in-context examples can, to some extent, elicit domain knowledge of model. Similarly, RAG is another powerful paradigm providing dynamic, external knowledge to model before it generates response. Given that both methods function by supplying context, we believe RAG can also further improve performance in materials characterization scenarios. This makes MatCha an ideal testbed for evaluating future multimodal RAG systems in materials science."
        },
        {
            "title": "C Parameters Settings",
            "content": "To reduce randomness, the temperature is fixed at 0 for models using API interface. For models executed with the Transformers library, the default setting is retained. Furthermore, in zero-shot and few-shot settings where models are required to directly output multiple-choice answers, the maximum generation length (max_new_tokens) is set to 32. In contrast, for the CoT experiments, the generation length is set to 8192. Detailed Few-shot and Chain-of-Thought Results We evaluate model performance under both fewshot and CoT prompting settings. For the few-shot experiments, we randomly sample 2, 4, 8, and 16 in-context examples for each test instance from its corresponding data source. For CoT prompting, we append the phrase Lets think step by step to the end of the original instruction. The results are shown in Tab. 4, Tab. 5, Tab. 6, Tab. 7, and Tab. 8, respectively. In the 8and 16-shot settings, several models, including LLaVA-1.5 (7B, 13B) and Janus-Pro-7B, fail to output valid option, likely due to challenges in processing the extended context."
        },
        {
            "title": "E Ablations Experiments",
            "content": "Followed by previous research (Goyal et al., 2017; Chen et al., 2024a), we conduct no-image ablation study to investigate the contribution of visual information, shown in Tab. 9. The notably low score of LLaMA-4-Maverick in the no-image condition is primarily due to its tendency to bypass the question and directly generate answers without adequately adhering to instructions in the absence of visual input. Other models also exhibited substantial performance degradation over 20%with accuracy only marginally (around"
        },
        {
            "title": "All",
            "content": "Suppl. SMA Suppl. DTC Suppl. ICA"
        },
        {
            "title": "MatCha\nAll",
            "content": "GPT-4o (Achiam et al., 2023) GPT-4o-mini (Hurst et al., 2024) Gemini-1.5-Flash (Team et al., 2024) Gemini-1.5-Pro (Team et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) LlaMA-4-Maverick (Meta, 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) InternVL3-8B (Chen et al., 2024b) InternVL3-38B (Chen et al., 2024b) LLaVA-1.5-7B (Liu et al., 2024) LLaVA-1.5-13B (Liu et al., 2024) Llama-3.2-11B-Vision (Grattafiori et al., 2024) Janus-Pro-7B (Chen et al., 2025) Gemma-3-4b-it (Team et al., 2025) 82.35 73.86 76.47 80.39 74.51 87.58 64.71 64.71 52.29 66.67 26.14 32.68 47.06 50.33 54.90 71.97 66.09 63.67 66.78 70.59 75.78 58.82 65.05 55.71 68.51 35.99 48.44 48.10 53.98 49. Proprietary Models 71.35 58.65 58.11 58.92 67.03 70.81 67.58 53.30 58.24 58.24 68.68 67.58 62.47 48.59 51.31 52.72 59.66 64.59 Open-source Models 53.24 60.81 51.89 57.57 34.32 42.70 40.00 57.57 42. 59.34 57.69 50.00 57.14 31.32 43.41 45.05 55.49 43.41 43.86 48.29 39.74 50.50 23.94 33.00 30.99 42.66 34.41 87.64 52.06 63.67 59.18 26.59 83.52 38.58 39.70 65.92 74.91 32.58 32.21 38.95 32.21 30.34 60.61 63.64 60.61 60.61 78.79 66.67 57.58 51.52 57.58 63.64 33.33 18.18 12.12 18.18 57. 46.12 13.59 33.01 50.97 57.77 56.80 24.76 24.27 35.92 29.13 12.62 12.62 9.71 15.05 21.84 68.97 37.15 50.99 55.93 42.69 71.54 34.19 34.19 53.16 55.53 24.51 23.32 25.30 24.31 28.66 64.67 44.73 51.20 53.80 53.93 66.93 40.60 43.53 44.27 52.20 24.13 29.73 29.07 36.47 32. Table 4: 2-shot results of model performance on MatCha. Bolded values signify the optimal in-class outcomes (open-source or proprietary). Model"
        },
        {
            "title": "All",
            "content": "Suppl. SMA Suppl. DTC Suppl. ICA"
        },
        {
            "title": "MatCha\nAll",
            "content": "GPT-4o (Achiam et al., 2023) GPT-4o-mini (Hurst et al., 2024) Gemini-1.5-Flash (Team et al., 2024) Gemini-1.5-Pro (Team et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) LlaMA-4-Maverick (Meta, 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) InternVL3-8B (Chen et al., 2024b) InternVL3-38B (Chen et al., 2024b) LLaVA-1.5-7B (Liu et al., 2024) LLaVA-1.5-13B (Liu et al., 2024) Llama-3.2-11B-Vision (Grattafiori et al., 2024) Janus-Pro-7B (Chen et al., 2025) Gemma-3-4b-it (Team et al., 2025) 84.31 77.78 78.43 83.01 87.58 88.24 62.75 63.40 50.33 70.59 21.57 32.03 47.06 52.29 50.98 73.01 66.44 62.63 70.59 74.39 71.28 62.63 67.13 56.75 69.90 23.88 24.22 54.67 54.33 47."
        },
        {
            "title": "Proprietary Models",
            "content": "74.05 60.27 57.84 65.41 69.46 72.70 65.93 55.49 56.04 63.19 69.23 67.03 63.88 51.11 50.30 58.15 64.59 63.98 Open-source Models 51.35 59.46 51.89 58.92 31.62 30.54 44.32 52.43 44.32 58.79 57.14 49.45 58.79 28.02 25.27 45.60 51.10 43. 43.76 48.79 41.35 51.41 17.81 18.91 35.21 39.64 32.80 85.02 47.57 35.21 59.55 81.65 85.39 46.82 48.69 50.19 60.67 32.21 32.21 23.97 33.33 33.33 57.58 54.55 57.58 66.67 75.76 66.67 51.52 48.48 51.52 57.58 21.21 24.24 18.18 27.27 42.42 53.88 16.50 33.01 47.09 68.93 50. 20.87 23.30 33.98 40.78 13.59 13.59 16.99 15.05 24.76 70.55 35.38 35.77 54.94 76.09 69.76 36.56 38.34 43.68 52.37 23.91 24.11 20.75 25.49 30.43 66.13 45.80 45.40 57.07 68.47 65.93 41.33 45.27 42.13 51.73 19.87 20.67 30.33 34.87 32.00 Table 5: 4-shot results of model performance on MatCha. Bolded values signify the optimal in-class outcomes (open-source or proprietary). Model PC MA Generated VQA PA SA All Suppl. SMA Suppl. DTC Suppl. ICA All Converted VQA MatCha All GPT-4o (Achiam et al., 2023) GPT-4o-mini (Hurst et al., 2024) Gemini-1.5-Flash (Team et al., 2024) Gemini-1.5-Pro (Team et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) LlaMA-4-Maverick (Meta, 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) InternVL3-8B (Chen et al., 2024b) InternVL3-38B (Chen et al., 2024b) LLaVA-1.5-7B (Liu et al., 2024) LLaVA-1.5-13B (Liu et al., 2024) Llama-3.2-11B-Vision (Grattafiori et al., 2024) Janus-Pro-7B (Chen et al., 2025) Gemma-3-4b-it (Team et al., 2025) 84.97 77.12 81.05 84.31 85.62 84.31 71.24 62.09 56.86 73.20 0.00 0.00 51.63 0.00 58. 73.36 63.32 64.71 69.20 75.43 73.36 61.94 67.47 57.44 67.47 0.00 0.00 50.87 0.00 49."
        },
        {
            "title": "Proprietary Models",
            "content": "72.70 59.73 57.03 60.00 67.84 71.89 68.13 57.14 57.14 66.48 69.23 66.48 64.19 49.70 50.70 56.94 63.68 63.38 Open-source Models 53.78 60.27 52.16 59.46 0.00 0.00 45.14 0.00 47.03 57.14 60.44 50.55 55.49 0.00 0.00 42.31 0.00 46. 45.47 50.00 42.05 51.51 0.00 0.00 32.49 0.00 37.02 86.52 63.67 52.81 64.42 89.89 90.26 32.96 38.58 60.30 58.43 0.00 0.00 36.70 0.00 36.33 60.61 54.55 33.33 66.67 84.85 60.61 48.48 57.58 48.48 54.55 0.00 0.00 9.09 0.00 39.39 46.12 18.93 43.20 43.69 62.14 52. 20.87 20.39 35.92 33.98 0.00 0.00 16.02 0.00 23.79 68.38 44.86 47.63 56.13 78.26 72.92 29.05 32.41 49.60 48.22 0.00 0.00 26.48 0.00 31.42 65.60 48.07 49.67 56.67 68.60 66.60 39.93 44.07 44.60 50.40 0.00 0.00 30.47 0.00 35.13 Table 6: 8-shot results of model performance on MatCha. Bolded values signify the optimal in-class outcomes (open-source or proprietary)."
        },
        {
            "title": "All",
            "content": "Suppl. SMA Suppl. DTC Suppl. ICA"
        },
        {
            "title": "MatCha\nAll",
            "content": "GPT-4o (Achiam et al., 2023) GPT-4o-mini (Hurst et al., 2024) Gemini-1.5-Flash (Team et al., 2024) Gemini-1.5-Pro (Team et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) LlaMA-4-Maverick (Meta, 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) InternVL3-8B (Chen et al., 2024b) InternVL3-38B (Chen et al., 2024b) LLaVA-1.5-7B (Liu et al., 2024) LLaVA-1.5-13B (Liu et al., 2024) Llama-3.2-11B-Vision (Grattafiori et al., 2024) Janus-Pro-7B (Chen et al., 2025) Gemma-3-4b-it (Team et al., 2025) 83.66 78.43 77.12 84.97 83.01 84.97 70.59 64.05 62.09 75.16 0.00 0.00 57.52 55.56 64.71 73.36 68.17 63.67 65.40 73.70 75.09 62.63 66.09 61.94 69.55 0.00 0.00 58.13 55.36 50. Proprietary Models 73.51 60.81 57.84 60.54 67.57 71.08 73.08 57.14 58.24 61.54 68.13 68.68 65.19 52.62 51.21 55.53 62.07 64.29 Open-source Models 54.05 62.97 52.97 61.08 0.00 0.00 45.68 61.35 49. 58.24 60.44 53.85 59.34 0.00 0.00 40.11 57.14 42.86 46.08 50.50 45.37 53.22 0.00 0.00 37.12 46.38 38.53 89.14 64.79 61.80 59.18 75.28 89.89 33.33 40.07 47.94 49.81 0.00 0.00 36.33 31.09 28.09 66.67 60.61 57.58 66.67 75.76 42.42 48.48 51.52 45.45 54.55 0.00 0.00 18.18 45.45 39. 54.37 16.99 44.66 34.47 41.26 62.62 10.19 24.76 33.01 36.41 0.00 0.00 10.68 21.36 24.76 73.52 45.06 54.55 49.60 61.46 75.69 24.90 34.58 41.70 44.66 0.00 0.00 24.70 28.06 27.47 68.00 50.07 52.33 53.53 61.87 68.13 38.93 45.13 44.13 50.33 0.00 0.00 32.93 40.20 34. Table 7: 16-shot results of model performance on MatCha. Bolded values signify the optimal in-class outcomes (open-source or proprietary). Model PC MA Generated VQA PA SA All Suppl. SMA Suppl. DTC Suppl. ICA All Converted VQA MatCha All GPT-4o (Achiam et al., 2023) GPT-4o-mini (Hurst et al., 2024) Gemini-1.5-Flash (Team et al., 2024) Gemini-1.5-Pro (Team et al., 2024) Claude-3.5-Sonnet (Anthropic, 2024) LlaMA-4-Maverick (Meta, 2025) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-32B (Bai et al., 2025) InternVL3-8B (Chen et al., 2024b) InternVL3-38B (Chen et al., 2024b) LLaVA-1.5-7B (Liu et al., 2024) LLaVA-1.5-13B (Liu et al., 2024) Llama-3.2-11B-Vision (Grattafiori et al., 2024) Janus-Pro-7B (Chen et al., 2025) Gemma-3-4b-it (Team et al., 2025) 84.97 71.90 78.43 83.66 84.31 60. 62.75 67.32 45.10 60.78 5.23 5.88 60.13 55.56 58.17 73.36 57.09 61.94 69.55 71.97 54.33 56.40 67.82 54.67 65.40 15.57 16.96 45.67 55.71 49."
        },
        {
            "title": "Proprietary Models",
            "content": "68.65 61.35 56.49 60.81 68.11 51.62 70.88 58.24 59.89 64.29 69.78 54.95 63.08 48.39 49.70 55.63 61.77 40.85 Open-source Models 50.81 61.08 52.16 60.27 15.95 16.49 47.03 61.35 49.46 57.69 64.29 56.04 61.54 22.53 21.98 46.70 57.69 45. 44.57 52.52 40.44 50.30 9.26 9.46 37.32 46.48 36.82 55.43 53.93 47.19 47.19 49.06 55.06 49.06 53.18 53.93 59.18 32.58 35.58 35.96 31.09 28.84 69.70 54.55 42.42 72.73 81.82 54.55 60.61 60.61 51.52 57.58 24.24 24.24 24.24 45.45 57.58 46.12 24.76 30.58 42.23 51.46 55. 22.82 32.52 29.61 41.75 10.68 15.05 22.82 21.36 24.27 52.57 42.09 40.12 46.84 52.17 55.14 39.13 45.26 43.87 51.98 23.12 26.48 29.84 28.06 28.85 59.53 46.27 46.47 52.67 58.53 45.67 42.73 50.07 41.60 50.87 13.93 15.20 34.80 40.27 34.13 Table 8: CoT results of model performance on MatCha. Bolded values signify the optimal in-class outcomes (open-source or proprietary)."
        },
        {
            "title": "G Cases",
            "content": "G.1 VQA Cases"
        },
        {
            "title": "Physical and Chemical Properties Inference",
            "content": "Question: What does the red circle in the 230 frame indicate regarding the nanorods crystallization? Choices: (A) The maximum diffraction intensity (B) Onset of the first diffraction spot (C) Completion of crystallization (D) Absence of any crystallization Answer: B"
        },
        {
            "title": "Crystallographic Data Inference",
            "content": "Question: What is the space group designation associated with the phase identified in the L-CTD XRD pattern? Choices: (A) Fm-3m (B) Pbcn (C) P63/mmc (D) Ia3d Answer: B"
        },
        {
            "title": "Surface Microstructure Assessment",
            "content": "Question: What structural feature is clearly visible in the HAADF-STEM image of type 2 p-ANHs? Choices: (A) Cylindrical canyons on the 100 facets (B) Entirely smooth surface (C) Spherical nanoparticle morphology (D) Randomly structured pores Answer: A"
        },
        {
            "title": "Material Morphology and Composition Uniformity Assessment",
            "content": "Question: What type of material morphology is observed in the SEM image? Choices: (A) Rod-like structures (B) Uniformly distributed spherical nanoparticles (C) Thin film coating (D) Porous sponge-like structures Answer:"
        },
        {
            "title": "Mechanical Properties Analysis",
            "content": "Question: What is the approximate value of the yield stress observed in the engineering stress-strain curve? Choices: (A) 230 MPa (B) 300 MPa (C) 200 MPa (D) 50 MPa Answer: Question: What is the approximate strain at which stress begins to decrease in the engineering stress-strain curve? Choices: (A) 0.08 (B) 0.20 (C) 0.01 (D) 0.50 Answer: A"
        },
        {
            "title": "Characterization Technique Identification",
            "content": "Question: What characterization technique was used to capture the diffraction patterns shown in Figure 5b? Choices: (A) SEM (B) XRD (C) TEM (D) STEM Answer: G.2 Error Cases"
        },
        {
            "title": "Lack of Material Knowledge",
            "content": "Question: How many distinct lattice structures are identified from the FFT pattern inset? (A) One (B) Two (C) Three (D) Four Correct Answer: (C) Model Answer: (B) Model Prediction Rationable: To determine how many distinct lattice structures are identified from the FFT (Fast Fourier Transform) pattern inset, lets analyze the image step by step: 1. Observation of the FFT Pattern: The inset FFT pattern shows three distinct sets of diffraction spots, each marked with different colored dashed lines (blue, orange, and green). These correspond to different lattice structures or orientations. 2. Identification of Lattice Structures in the Image: - The main image shows three regions labeled as \"2H,\" \"1T Grain 1,\" and \"1T Grain 2.\" - The \"2H\" region corresponds to one lattice structure. - The \"1T Grain 1\" and \"1T Grain 2\" regions correspond to the same lattice structure (1T) but with different grain orientations. 3. Conclusion: From the FFT pattern, we can identify two distinct lattice structures: - The \"2H\" lattice structure. 19 - The \"1T\" lattice structure (with two grains but the same structure). Thus, the correct answer is: (B) Two Error Analysis: The model incorrectly concluded that there are only two distinct lattice structures based on the FFT pattern and the labeled regions in the image. The mistake lies in the interpretation of the FFT pattern inset, which clearly shows three distinct sets of diffraction spots marked by different colored dashed lines (blue, orange, and green). These correspond to three distinct lattice structures or orientations. The model failed to recognize that the FFT pattern indicates three distinct lattice structures, not just two, despite the labeling of 1T grains in the main image. This error stems from lack of material knowledge and misunderstanding of how FFT patterns correspond to lattice structures."
        },
        {
            "title": "Visual Perception Error",
            "content": "Question: What are the main microstructure in the given scanning electron microscope (SEM) images of Ti-6Al-4V alloy? (A) acicular: acicular morphology (B) lamellar: α lamellae (C) duplex: duplex microstructure, bimodal morphology consisting of equiaxed grains and α-lamellae Correct Answer: (C) Model Answer: (B) Model Prediction Rationable: To identify the main microstructure in the given SEM image of the Ti-6Al-4V alloy, lets analyze the features step by step: 1. Observation of the Image: - The image shows clear pattern of elongated, parallel structures within larger grains. - These structures resemble lamellae, which are characteristic of lamellar microstructure. 2. Comparison with Descriptions: - Acicular: Acicular morphology typically consists of needle-like structures, which are not observed in this image. - Lamellar: The image shows α lamellae within the grains, matching the description of lamellar microstructure. - Duplex: duplex microstructure would show combination of equiaxed grains and α lamellae. However, the image does not show distinct equiaxed grains alongside lamellae. 3. Conclusion: - The microstructure in the SEM image corresponds to (B) lamellar: α lamellae. Error Analysis: The model incorrectly identified the microstructure as lamellar (B) instead of duplex (C). The SEM image clearly shows bimodal morphology consisting of equiaxed grains and α-lamellae, which is characteristic of duplex microstructure. The model failed to recognize the presence of equiaxed grains in the image, focusing only on the lamellae structures. This indicates visual perception error, as the model did not correctly identify the equiaxed grains in the image."
        },
        {
            "title": "H Prompt Setting",
            "content": "H.1 Sub-caption Segmentation Prompt System prompt: Subfigure labels are letters referring to individual subfigures within larger figure. Please separate the given full caption into the exact subcaptions and format as syntactically valid JSON format with keys the letter of each subcaption. If there is no full caption then return an empty JSON. User prompt: Caption: {caption} H.2 QA Generation Prompt System prompt: You are scientific expert. Based on the following figure-caption pair and its related context from the article in the field of materials science and characterization, generate 1 to 4 visual question answering (VQA)-style questionanswer pairs, depending on the amount of information provided. The question should be related to both the figure-caption pair and the context, but the answer should be able to be inferred and analyzed ONLY from the figure. {sub-tasks with explanation} You should generate multiple-choice questions, ensure that the answers are concise and clear. For multiple-choice questions, please generate plausible but incorrect options. The number of options is not limited, and enclose all options in parentheses (e.g., (A)) as part of the question. After providing the question and answer, also include the topic of this question, and ] } Example output in JSON format: { \"vqas\": [ template: { \"vqas\": [ \"question\": ... (A) xx (B) xx (C) xx (D) xx, \"answer\": \"B\", \"topic\": ... ] } \"question\": ..., \"answer\": ..., \"topic\": ... User prompt: Figure {sub-figure} Caption: {sub-caption} Related Context: {related context} H.3 Evaluation Prompt System prompt: You are helpful materials science assistant. Based on the figure, please answer the following question. The answer could be inferred from the figure and must be concise and clear. Answer directly without any explanation. User prompt: Question: {question with options} Answer with the options letter from the given choices directly:"
        }
    ],
    "affiliations": [
        "Northeastern University",
        "The Chinese University of Hong Kong, Shenzhen",
        "Zhejiang University"
    ]
}