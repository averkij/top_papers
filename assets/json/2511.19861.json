{
    "paper_title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI",
    "authors": [
        "GigaWorld Team",
        "Angen Ye",
        "Boyuan Wang",
        "Chaojun Ni",
        "Guan Huang",
        "Guosheng Zhao",
        "Haoyun Li",
        "Jiagang Zhu",
        "Kerui Li",
        "Mengyuan Xu",
        "Qiuping Deng",
        "Siting Wang",
        "Wenkang Qin",
        "Xinze Chen",
        "Xiaofeng Wang",
        "Yankai Wang",
        "Yu Cao",
        "Yifan Chang",
        "Yuan Xu",
        "Yun Ye",
        "Yang Wang",
        "Yukun Zhou",
        "Zhengyuan Zhang",
        "Zhehao Dong",
        "Zheng Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training."
        },
        {
            "title": "Start",
            "content": "2025-12-2 GigaWorld-0: World Models as Data Engine to Empower Embodied AI GigaAI Project Page: https://giga-world-0.github.io GigaWorld Team (alphabetical order): Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jiagang Zhu, Kerui Li, Mengyuan Xu, Qiuping Deng, Siting Wang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yankai Wang, Yu Cao, Yifan Chang, Yuan Xu, Yun Ye, Yang Wang, Yukun Zhou, Zhengyuan Zhang, Zhehao Dong, Zheng Zhu. 5 2 0 N 0 3 ] . [ 2 1 6 8 9 1 . 1 1 5 2 : r Figure 1: An overview of GigaWorld-0 applications. Using video generation, it dynamically alters appearance and viewpoints. GigaWorld-0 also facilitates converting human demonstration videos into robotic manipulation trajectories. Furthermore, through 3D scene generation and reconstruction, GigaWorld-0 supports physically realistic motion planning and produces geometrically consistent renderings to empower embodied AI. 2025 GigaAI. All rights reserved. GigaWorld-0: World Models as Data Engine to Empower Embodied AI"
        },
        {
            "title": "Abstract",
            "content": "World models are emerging as foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, unified world model framework designed explicitly as data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training. 1. Introduction World models have emerged as foundational component for advancing embodied AI (Agarwal et al., 2025; Zhu et al., 2024), functioning as high-fidelity simulators that bridge the gap between synthetic and realworld environments. By modeling the dynamics, appearance, and spatial structure of physical scenes, world models enable the efficient, scalable, and cost-effective generation of high-quality, diverse data under broad spectrum of controllable conditions. This capability significantly alleviates the data bottleneck traditionally faced by embodied agents, which rely heavily on expensive real-world data collection. In this work, we present GigaWorld-0 and underscore the role of the world model as powerful data engine, scalable, controllable, and photorealistic training data source that significantly empowers the learning of embodied AI. Accurate modeling of texture, geometry, and physical dynamics is fundamental to the fidelity and utility of any world model aiming to serve as high-quality proxy for the physical world. To address these requirements in scalable and controllable manner, the introduced GigaWorld-0 comprises GigaWorld-0-Video and GigaWorld-0-3D. GigaWorld-0-Video leverages video generation models to synthesize temporally coherent, photorealistic visual sequences with rich texture detail, diverse scene content, and fine-grained control over appearance (e.g., texture, material, light), object placement and camera viewpoints. This enables the efficient generation of large-scale, high-quality 2D observation data under broad distribution of real-world conditions. In parallel, GigaWorld0-3D explicitly enforces 3D geometric consistency and physical plausibility by integrating 3D reconstruction models and physics-aware simulation priors. It ensures spatial coherence across viewpoints, models object rigidity and deformability, and respects physical constraints such as collision, gravity, and contact dynamics. The coupling of GigaWorld-0-Video and GigaWorld-0-3D further yields unified data generation pipeline capable of producing embodied interaction data that is simultaneously texture-rich, geometrically consistent, physically grounded, and dynamically realistic. Specifically, the model suite of GigaWorld-0 is summarized in Tab. 1. The GigaWorld-0-Video comprises four generative models: GigaWorld-0-Video-Dreamer, GigaWorld-0-Video-AppearanceTransfer, GigaWorld-0-VideoViewTransfer, and GigaWorld-0-Video-MimicTransfer. Among them, GigaWorld-0-Video-Dreamer serves as our Video Foundation Modela Mixture-of-Experts (MoE) architecture for image-text-to-video (IT2V) generation, trained on large-scale corpus of embodied interaction data. Building upon this foundation, the three post-training adaptation models, GigaWorld-0-Video-AppearanceTransfer, GigaWorld-0-Video-ViewTransfer, and GigaWorld-0-Video-MimicTransfer, each incorporate dedicated controllable branches to enable generaliza2 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Table 1: Components of GigaWorld-0 and their functions. Model Name Function GigaWorld-0-Video-Dreamer GigaWorld-0-Video-AppearanceTransfer GigaWorld-0-Video-ViewTransfer GigaWorld-0-Video-MimicTransfer Image-text-to-video foundation model for embodied scenes. Text-guided appearance transfer, edits texture, material, lighting. Renders videos from user-specified camera extrinsics. Translates egocentric human demonstration to robot arm trajectories. GigaWorld-0-3D-FG GigaWorld-0-3D-BG GigaWorld-0-3D-Phys GigaWorld-0-3D-Act Generates 3D assets of foreground manipulable objects. Reconstructs backgrounds via 3D Gaussian Splatting (3DGS). Models object physics and performs differentiable system identification. Synthesizes executable, physically consistent arm motions. tion across appearance (e.g., texture and lighting), camera viewpoints, and action modalities, respectively. Notably, GigaWorld-0-Video-MimicTransfer translates first-person human manipulation demonstrations into robot-executable trajectories, facilitating cross-embodiment generalization. To better suit embodied manipulation scenarios, we further extend the training pipeline with multi-view video generation, FP8-precision training acceleration, denoising-step distillation and FP8-efficient inference. Complementing the video stream, GigaWorld-0-3D constructs physically grounded 3D scenes through modular pipeline: GigaWorld-0-3D-FG generates foreground assets via 3D generative model. GigaWorld-0-3D-BG reconstructs background environments using 3D Gaussian Splatting (3DGS). GigaWorld-0-3D-Phys models the physical properties of interactable objects and performs differentiable system identification for the robotic arm. GigaWorld-0-3D-Act calculates arm motions to produce complete, executable manipulation sequences. The large-scale training of GigaWorld-0-Video is efficiently enabled by GigaTrain, our training framework leveraging FP8-precision and sparse attention to accelerate the training process. We conduct extensive experiments to validate the effectiveness of GigaWorld-0 from multiple dimensions, including physical plausibility, geometric consistency, text-to-video alignment, multi-view coherence, and visual fidelity. Quantitative and qualitative results demonstrate that GigaWorld-0 achieves state-of-the-art performance across these metrics, setting new benchmark for synthetic data generation in embodied settings. Moreover, we evaluate policies trained on GigaWorld-0-generated data in real-world robotic environments, confirming that such synthetic data significantly enhances the training of Vision-Language-Action (VLA) models, leading to improved task success rates, robustness, and generalization under diverse conditions. The proposed GigaWorld-0 as powerful data engine for embodied AI. To foster community progress, we will open-source the models and data generation pipeline. We believe world models remain vast and underexplored frontier; promising directions such as World Models as Policy Environments and World Models for Policy Generation warrant collaborative exploration by the broader research community. 2. Related Work Recent progress in world model research (Agarwal et al., 2025; Alhaija et al., 2025; Assran et al., 2025; Jang et al., 2025; Jiang et al., 2025; Kong et al., 2024; Liao et al., 2025; Wang et al., 2025) has accelerated the adoption of generated data as training source to facilitate the learning of embodied AI (Zhu et al., 2024). In autonomous driving, generative world models are now routinely used to create complex, safety-critical traffic scenarios; representative frameworks include (Gao et al., 2023, 2024; Hu et al., 2023; Ni et al., 2024, 2025; Ren et al., 2025; Russell et al., 2025; Wang et al., 2024; Zhao et al., 2024, 2025,). Similarly, in robotics, where real-world data collection is constrained by hardware availability, safety, and labor costs, generative approaches offer scalable alternative. Several works (Du et al., 2023; Feng et al., 2025; Jang et al., 2025; Yang et al., 2023; Zhou et al., 2024) utilize natural language instructions to forecast plausible future observations, subsequently deriving low-level motor commands through inverse dynamics or action decoding. To enhance geometric and temporal fidelity, methods such as TesserAct (Zhen et al., 2025) and Robot4DGen (Liu et al., GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 2: The framework of GigaWorld-0-Video-Dreamer. 2025) propose unified multimodal generation pipelines that jointly synthesize aligned RGB, depth, surface normals, and 3D point clouds, enabling coherent 4D scene reconstructions that substantially improve policy learning over RGB-only training. Additional gains in environmental diversity are achieved via background inpainting techniques (Fang et al., 2025; Yuan et al., 2025), which modify scene textures, and video-to-video translation frameworks (Alhaija et al., 2025; Dong et al., 2025; Li et al., 2025; Liu et al., 2025; Wang et al., 2025; Xu et al., 2025) that adapt visual styles or dynamics across domains. Building on this trend, GigaWorld-0 harnesses world models to generate highly diverse synthetic data spanning variations in texture, material, lighting, object layout, and camera pose, thereby offering rich and generalizable training signal for VLA learning. 3. GigaWorld-0 Models This section first introduces the GigaWorld-0-Video series, followed by the GigaWorld-0-3D series. GigaWorld-0Video leverages video generation models to synthesize photorealistic sequences with control over appearance, object placement, and camera viewpoints, enabling large-scale, high-quality data generation under diverse real-world conditions. In parallel, GigaWorld-0-3D enforces 3D geometric consistency and physical plausibility via 3D representation, ensuring spatial coherence, modeling object rigidity/deformability, and respecting physical constraints like contact dynamics. 3.1. GigaWorld-0-Video Foundation models for video generation in embodied scenarios must exhibit profound understanding of such environments and be capable of efficiently synthesizing plausible embodied interaction videos conditioned on diverse control signals. In contrast to existing video generation models (Agarwal et al., 2025; Kong et al., 2024; Wang et al., 2025; Yang et al., 2024; Zheng et al., 2024), which primarily scale up model parameters, the GigaWorld-0-Video series achieve lower training costs and reduced inference latency through combination of sparse attention mechanisms, mixture-of-experts (MoE) architecture, FP8-precision training and inference, and diffusion step distillation. 3.1.1. GigaWorld-0-Video-Dreamer Model Details. GigaWorld-0-Video-Dreamer is our foundation video generation model, capable of achieving IT2V generation. Its overall architecture is illustrated in Fig. 2. We adopt flow-matching (Lipman et al., 2022) formulation for modeling the generative process: ğ‘‘zğ‘¡ ğ‘‘ğ‘¡ = vğœƒ(zğ‘¡, ğ‘¡, c), (1) 4 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 3: Qualitative comparison of action inference on the test set. Predicted joint trajectories from GigaWorld0-IDM closely align with ground-truth actions across all 12 arm joints and 2 gripper degrees of freedom, demonstrating high fidelity in recovering physically plausible manipulation policies from visual input alone. where zğ‘¡ denotes the latent at time ğ‘¡, represents the text and image conditioning, and vğœƒ is the velocity parameterized by our model. For the input representation, we employ the 3D-VAE architecture (Wang et al., 2025) to efficiently compress raw videos into latent representations with spatial-temporal compression ratio of 4, 8, 8 (temporal,height,width), resulting in 16-channel video latents. On top of this representation, we apply the same 1 2 2 patchification strategy to further compress the latent features. We encode these latents using 3D Rotary Position Embedding (3D-RoPE) (Su et al., 2023). For text conditioning, we utilize the T5 encoder (Raffel et al., 2020) to extract textual embeddings. The core generative backbone of GigaWorld-0-VideoDreamer is DiT built upon sparse attention mechanisms (Hassani et al., 2023). Additionally, we incorporate Mixture-of-Experts (MoE) architecture (Liu et al., 2024) into the feed-forward network (FFN) blocks of the DiT. Let ğ‘¢ğ‘¡ denote the FFN input of the ğ‘¡-th token, we compute the FFN output ğ‘¡ as follows: ğ‘¡ = uğ‘¡ + ğ‘ğ‘Ÿ ğ‘–= ğ‘”ğ‘–,ğ‘¡ FFNğ‘– (uğ‘¡) , { ğ‘ ğ‘–,ğ‘¡, ğ‘” ğ‘–,ğ‘¡ = 0, ğ‘ ğ‘–,ğ‘¡ = softmax (u ) . ğ‘¡ eğ‘– if ğ‘ ğ‘–,ğ‘¡ Topk ({ğ‘ ğ‘—,ğ‘¡ 1 ğ‘— ğ‘ğ‘Ÿ}, ğ¾ğ‘Ÿ) , otherwise, (2) (3) (4) Specifically, in contrast to DeepSeek-V2 (Liu et al., 2024), we do not include shared expert. Instead, we configure ğ‘ğ‘Ÿ = 4 routed experts and activate ğ¾ğ‘Ÿ = 2 experts per token. This design enables dynamic specialization across different semantic regions of the video without redundant parameter sharing. eğ‘– is the learnable vector of the ğ‘–-th routed expert. Besides, to ensure MoE load balance, we employ complementary balance loss from DeepSeek-V3 (Liu et al., 2024): â„’Load = ğ›¼ ğ‘ğ‘Ÿ ğ‘–=1 ğ‘“ğ‘–ğ‘ƒğ‘–, ğ‘‡ 1 (ğ‘ ğ‘–,ğ‘¡ Topk ({ğ‘ ğ‘—,ğ‘¡ 1 ğ‘— ğ‘ğ‘Ÿ} , ğ¾ğ‘Ÿ)) , ğ‘“ğ‘– = ğ‘ğ‘Ÿ ğ¾ğ‘Ÿğ‘‡ ğ‘  ğ‘–,ğ‘¡ = ğ‘ğ‘Ÿ ğ‘ƒğ‘– = 1 ğ‘‡ , ğ‘¡=1 ğ‘ ğ‘–,ğ‘¡ ğ‘—=1 ğ‘ ğ‘—,ğ‘¡ ğ‘‡ ğ‘  ğ‘–,ğ‘¡, ğ‘¡=1 (5) (6) (7) (8) 5 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 4: The control branch of GigaWorld-Video. where ğ›¼ = 0.01 is balance factor. 1() denotes the indicator function, ğ‘‡ denotes the number of tokens in video sequence. The balance loss encourages the expert load on each sequence to be balanced. Function as Data Engine. GigaWorld-0-Video-Dreamer serves as versatile foundation model for training downstream controllable video generation systems. Moreover, it functions as powerful data engine for training VLA models. As shown in Fig. 12 and Fig. 13, GigaWorld-0-Video-Dreamer can generate distinct future videos conditioned on the same initial frame but guided by different text prompts. We then train an Inverse Dynamics Model, GigaWorld-0-IDM, to infer the corresponding robotic arm actions from these generated videos. Specifically, given generated video sequence = {v1, v2, . . . , vğ‘‡ }, where vğ‘¡ Rğ»ğ‘Š 3 denotes the RGB frame at time ğ‘¡, GigaWorld-0-IDM estimates the joint-angle trajectory: ğœƒ1:ğ‘‡ = ğ‘“IDM(V), (9) ğ‘¡ ğ‘¡ , ğœƒ(2) ğ‘¡ , . . . , ğœƒ(ğ·) where ğœƒğ‘¡ = [ğœƒ(1) ] Rğ· represents the rotation angles of all ğ· joints of the robotic arm at timestep ğ‘¡. In contrast to prior IDM (Tan et al., 2025), the GigaWorld-0-IDM employs masked training. Specifically, we use (Ravi et al., 2024) to segment the robotic arm from the input video and feed only the segmented arm region into the IDM during training, thereby reducing the adverse impact of cluttered backgrounds on prediction accuracy. This strategy significantly enhances the models robustness and prediction accuracy under real-world visual ambiguities. As shown in Fig. 3, we collect manipulation data from unseen tasks for evaluation. GigaWorld-0-IDM successfully infers action sequences that closely align with the groundtruth trajectories, accurately predicting the states of all 12 arm joints and the 2 gripper degrees of freedom. The resulting paired dataset of generated videos and predicted actions (V, ğœƒ1:ğ‘‡ ) provides abundant, diverse, and temporally aligned supervision for training VLA models without requiring real-world robot interaction. 3.1.2. GigaWorld-0-Video-AppearanceTransfer Real-world data collected for training VLA models often suffers from limited diversity in texture, color, and lighting conditions. This limitation hinders the robustness of trained models when deployed in complex, visually rich real-world environments. While traditional simulation pipelines can generate vast amounts of data with diverse textures, colors, and lighting, the rendered appearances still exhibit significant sim2real gap, leading to low success rates in real robot deployment. To address this challenge, we propose GigaWorld-0-VideoAppearanceTransfer, an efficient framework that enables text-driven appearance modification of real-world videos and facilitates style transfer from simulation to reality, thereby narrowing the sim2real gap. Model Details. GigaWorld-0-Video-AppearanceTransfer allows controllable editing of texture, material, and GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 5: Training data pair of GigaWorld-0-Video-ViewTransfer. illumination in real-world video sequences using natural language prompts, while preserving geometric and motion consistency. Specifically, GigaWorld-0-Video-AppearanceTransfer is built upon the pre-trained GigaWorld0-Video-Dreamer, extended with lightweight control branch. As illustrated in Fig. 4, ControlNet (Zhang et al., 2023) is not used due to its high parameter overhead, especially problematic when the base model employs MoE architecture, where duplicating MoE layers would drastically increase model size. Instead, we introduce more parameter-efficient control mechanism. Given multiple video conditions (e.g., depth, surface normals), we first extract their latent representations using 3D VAE (Wang et al., 2025). These control latents are then concatenated channel-wise with the noise latents used in the diffusion process. The combined tensor is passed through series of channel-compressed MLP layers to produce the final latent input for the subsequent Transformer blocks. This design significantly reduces parameter count while remaining flexible across diverse video conditions. For appearance control, we leverage textual prompts to independently manipulate foreground and background attributes such as texture, material, and lighting. To obtain geometric priors, we use VideoDepthAnything (Chen et al., 2025) and LOTUS (He et al., 2024) to extract depth and normal maps from either real-world or simulation videos. These maps are normalized, repeated to form 3-channel inputs, and then encoded by the 3D VAE, following practices established in (Dong et al., 2025; Liu et al., 2025). Function as Data Engine. GigaWorld-0-Video-AppearanceTransfer functions as powerful data engine for diverse visual generalization. As shown in Fig. 14 and Fig. 7, it supports both real2real and sim2real appearance transfer: given real-world embodied interaction video or simulation trajectory, the model can synthesize photorealistic variants with user-specified textures, colors, and lighting conditions via text prompts. This capability enables large-scale generation of visually diverse training data without requiring additional realworld collection or expensive simulation rendering. VLA models trained on generated datasets demonstrate significantly improved robustness under appearance variations. 3.1.3. GigaWorld-0-Video-ViewTransfer Beyond appearance generalization, viewpoint generalization remains critical challenge for VLA models. model trained on data collected from viewpoint-A often fails to generalize to viewpoint-B. While multi-view data collection can mitigate this issue, it incurs prohibitive real-world annotation and operational costs. To address this, we propose GigaWorld-0-Video-ViewTransfer, framework that synthesizes diverse novel viewpoints from existing single-view robot interaction videos, while simultaneously transforming the associated robot actions to maintain task consistency. Model Details. Formally, consider robot operating in world coordinate frame ğ’²ğ´, capturing an egocentric video Vğ´ along with sequence of end-effector poses {Teebase expressed relative to the robot base. We aim to synthesize new observation Vğµ as if captured from different world frame ğ’²ğµ, where the robot base has been relocated (i.e., Tbase = I). Crucially, the absolute end-effector pose in the world frame must }ğ‘‡ ğ‘¡=1 ğ‘¡ ğ’²ğ´ğ’²ğµ 7 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 6: Training data pair of GigaWorld-0-Video-MimicTransfer. remain unchanged to preserve task semantics: Teeğ’² ğ‘¡ = Tbaseğ’²ğ´ Teebase ğ‘¡ = Tbaseğ’²ğµ Kğ‘¡, where Kğ‘¡ denotes the new end-effector pose relative to the relocated base in ğ’²ğµ. Solving for Kğ‘¡ yields: Kğ‘¡ = ( Tbaseğ’²ğµ )1 Tbaseğ’²ğ´ Teebase ğ‘¡ . (10) (11) The synthesized video Vğµ must be consistent with both the new camera viewpoint and the transformed action sequence = {Kğ‘¡}ğ‘‡ . ğ‘¡= GigaWorld-0-Video-ViewTransfer is built upon the pretrained GigaWorld-0-Video-Dreamer via post-training adaptation, featuring dual-condition control branch  (Fig. 4)  . To ensure 3D consistency under viewpoint changes, we decompose the control signal into two components: (i) background 3D consistency, enforced via video condition-1; (ii) robot arm 3D consistency, enforced via video condition-2. Since paired multi-view real-world videos (Vğ´, Vğµ) are unavailable, we employ double-reprojection strategy (Xu et al., 2025) to construct self-supervised training pairs. For video condition-1, we first estimate scaled depth in ğ’²ğ´ using MoGe (Wang et al., 2025), then warp Vğ´ into the target view ğ’²ğµ, and finally reproject it back to the original view (see video condition-1 in Fig.5). The reprojected video serves as the input condition, while the original Vğ´ acts as the ground truth. To isolate scene geometry from the moving robot, we mask out the robotic arm (Ravi et al., 2024) during the warping process. For video condition-2, we render the transformed action sequence in physics-aware simulator (Xiang et al., 2020) to generate an arm-only video that reflects the correct pose and kinematics in ğ’²ğµ (see video condition-2 in Fig.5). This rendered sequence provides explicit 3D guidance for arm motion consistency. The model is trained to generate the ground-truth video Vğ´ conditioned on both video condition-1 (background geometry under novel view) and video condition-2 (arm pose under transformed kinematics). More implementation details follow the similar framework (Xu et al., 2025). Function as Data Engine. After training, GigaWorld-0-Video-ViewTransfer functions as scalable viewpoint augmentation engine. As shown in Fig. 15, given single real-world interaction video, it can generate photorealistic observations from arbitrarily novel viewpoints, accompanied by geometrically consistent robot actions K. This enables massive expansion of the effective dataset with diverse egocentric perspectives, without additional real-world data collection. VLA models trained on this augmented data exhibit significantly improved robustness to viewpoint shifts during deployment, closing key gap between simulation and real-world generalization. 3.1.4. GigaWorld-0-Video-MimicTransfer Collecting real-world robot data via teleoperation is costly. An alternative and more efficient approach is to leverage first-person demonstration videos. However, significant gap exists between first-person human demonstration videos and actual robot execution videos, the most prominent being the appearance gap between human hands and robotic arms. To address this issue, we propose GigaWorld-0-Video-MimicTransfer, method 8 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 7: GigaWorld can generate multi-view consistent videos, thereby enabling 3D-aware training and improving spatial reasoning in downstream tasks. that translates first-person human-hand manipulation videos into robotic-arm manipulation videos, thereby reducing the aforementioned appearance gap and enhancing the usability of egocentric manipulation data. Model Details. GigaWorld-0-Video-MimicTransfer is post-trained model based on GigaWorld-0-Video-Dreamer, with its control branch illustrated in Fig. 4. During training, due to the scarcity of aligned video pairs showing both human-hand and robotic-arm manipulations, we construct the training data using only robotic-arm manipulation videos. Specifically, we employ two video conditions: video condition-1 controls the manipulation scene, while video condition-2 enforces the robotic arms motion to mimic that of human hand. As shown in Fig. 6, to form video condition-1, we mask out the robotic arm in the original manipulation video and retain only the background. For video condition-2, we drive simulated robotic arm using the original arms motion trajectories to generate synthetic video depicting human-like manipulation. The model is trained to reconstruct the original (unmasked) robotic-arm manipulation video from these two conditions. Additional training details and data construction procedures follow those described in (Li et al., 2025). Function as Data Engine. After training, GigaWorld-0-Video-MimicTransfer serves as data engine that translates first-person human demonstration videos into robotic-arm manipulation videos, as shown in Fig. 16. Specifically, the human hand is masked out from the input video to serve as video condition-1, preserving the scene context. Meanwhile, using annotated end-effector poses of the human hand, we solve for the corresponding joint angles of the robotic arm via inverse kinematics (IK), and render the resulting arm pose in simulator to generate video condition-2. Conditioned on these two inputs, the model synthesizes realistic robotic-arm manipulation video that mimics the original human action, thereby enabling scalable and cost-effective data augmentation for robot learning. Furthermore, to better support embodied manipulation scenarios,GigaWorld-0-Video incorporates multi-view video generation and generation acceleration techniques. For multi-view synthesis, we follow recent approaches (Dong et al., 2025; Liu et al., 2025; Zhao et al., 2025) by concatenating multi-view images along the width dimension into single panoramic input. This design preserves the original diffusion model architecture and leverages in-context learning capabilities: after fine-tuning on small set of multi-view data, the model gen9 GigaWorld-0: World Models as Data Engine to Empower Embodied AI erates temporally and spatially coherent videos across multiple viewpoints without architectural modifications. As shown in Fig. 7 and Fig. 13, the resulting outputs exhibit strong cross-view consistency, making them suitable for training vision systems requiring egocentric and third-person observations. To accelerate video generation, GigaWorld-0-Video employs denoising step distillation (Yin et al., 2024), reducing the sampling process from dozens of steps to single step. Combined with FP8-precision inference, these optimizations achieve over 50 speedup compared to standard diffusion models, enabling scalable data generation at deployment time. Despite these advances, generated videos may still contain hallucinations or artifacts that could impair downstream policy learning. To ensure data quality, GigaWorld-0-Video introduces comprehensive evaluation pipeline that assesses each video across multiple dimensions: geometric consistency, multi-view coherence (Liu et al., 2025), text-to-video alignment (Azzolini et al., 2025), and physical plausibility (Azzolini et al., 2025). composite quality score is computed for each sequence, determining its suitability for pre-training, fine-tuning, or rejection. 3.2. GigaWorld-0-3D While GigaWorld-0-Video leverages video generation models to synthesize texture-rich embodied scene data, high-quality embodied manipulation also demands strong geometric consistency and physical accuracy. To address these requirements, GigaWorld-0-3D adopts 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023) as its core scene representation, enabling the construction of spatially coherent and physically grounded 3D environments. Below, we detail the four key components of GigaWorld-0-3D: GigaWorld-0-3D-FG, GigaWorld-03D-BG, GigaWorld-0-3D-Phys, and GigaWorld-0-3D-Act. 3.2.1. GigaWorld-0-3D-FG Figure 8: Overall pipeline of GigaWorld-0-3D-FG. Traditional 3DGS reconstruction pipelines typically require dense multi-view inputs to produce high-fidelity assets. However, recent advances (Feng et al., 2025; Xiang et al., 2025; Yang et al., 2024; Zhang et al., 2024; Zhao et al., 2025) in generative modeling have enabled object-level 3D reconstruction from extremely sparse inputs, such as single image or even text prompt. Notable approaches in this direction include Trellis and Hunyuan3D. Despite its strong geometric modeling capabilities, Trellis suffers from several shortcomings that limit its applicability in embodied AI scenarios: the generated textures often display poor visual fidelity, especially due to over-saturated specular highlights that lead to unnatural whitening when baked onto the mesh. Moreover, the resulting assets are purely visual constructs lacking real-world scale, physically plausible geometry, or material properties, rendering them incompatible with physics-based simulators (Mittal et al., 2023; Todorov et al., 2012; Xiang et al., 2020). To bridge this gap, GigaWorld-0-3D-FG enhances asset quality through rigorous data curation, while its successor module, GigaWorld-0-3D-Phys, endows the assets with physical semantics suitable for simulation. As illustrated in Fig. 8, the GigaWorld-0-3D-FG pipeline accepts either real-world photograph or synthetic image generated via text-to-image models. Prior to 3D generation, an automated preprocessing stage performs quality control on the input. Specifically, we employ an aesthetic assessment module based on the Aesthetic-Checker (Ma et al., 2025), which correlates positively with texture richness. Recognizing that foreground segmentation accuracy critically influences 3D output quality, we introduce an ImageSegChecker powered by GPT-4o to evaluate 10 GigaWorld-0: World Models as Data Engine to Empower Embodied AI segmentation reliability. To ensure robustness across diverse object categories, the system integrates three segmentation backends (AI., 2025; Gatis, 2025; Ravi et al., 2024). If the ImageSegChecker flags segmentation failure, the pipeline triggers retry, either by prompting the user to capture new image or by regenerating the input using an alternative text-to-image model. For the image-to-3D conversion, we adopt open-source generative models to facilitate seamless integration with future community advancements. Among available options, Trellis (Xiang et al., 2025) is selected for its superior geometric coherence and its dual support for mesh and 3DGS representations. Following generation, postprocessing inspection module, termed MeshGeoChecker, renders the asset from four orthogonal viewpoints to evaluate geometric completeness and plausibility. Only assets that pass all quality gates are exported in URDF format and archived; those failing any inspection stage are automatically resubmitted to the corresponding generation step with modified parameters and random seeds for re-synthesis. Additional implementation details, including texture baking strategies, are provided in (Wang et al., 2025). 3.2.2. GigaWorld-0-3D-BG Figure 9: Visualization of novel view synthesis before and after view restoration. 3DGS (Kerbl et al., 2023) has emerged as mature technique for scene reconstruction. Conventional 3DGS relies on Elliptical Weighted Average (EWA) splatting, which approximates projection via Jacobian computation and is limited to pinhole camera models. In contrast, 3DGRUT (Wu et al., 2025) enhances camera compatibility by associating each 3D Gaussian with seven representative points (one center and six boundary points), enabling precise modeling of non-pinhole cameras, such as those with rolling shutters, commonly used in embodied AI settings. This leads to higher reconstruction fidelity in such scenarios. However, traditional 3DGS typically requires dense multi-view inputs to achieve high-quality reconstructions, which are often unavailable in real-world embodied settings. To address this limitation, we draw inspiration from recent generative approaches (Ni et al., 2024; Wu et al., 2025; Zhao et al., 2024, 2025) that synthesize novel views to enrich sparse observations. GigaWorld-0-3D-BG pipeline begins with sparse-view inputs and employs 3DGRUT for initial scene reconstruction. Yet, under sparse-view conditions, novel view synthesis (NVS) often suffers from geometric and photometric artifacts. To mitigate this, we adopt view restoration strategy inspired by (Ni et al., 2024), training dedicated view refinement model to hallucinate plausible intermediate views. As shown in Fig. 9, the refined views significantly reduce artifacts and provide dense, consistent visual observations. These synthesized views then serve as augmented inputs for second-stage, dense 3DGS reconstruction, yielding high-fidelity Gaussian representation of the background. Finally, we convert the resulting dense Gaussian Splats into watertight mesh using the Poisson Surface Reconstruction, producing realistic and geometrically consistent background assets suitable for embodied manipulation scenarios. 11 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 10: The learning pipeline of the differentiable physics network in GigaWorld-0-3D-Phys. 3.2.3. GigaWorld-0-3D-Phys While GigaWorld-0-3D-FG and GigaWorld-0-3D-BG construct foreground and background assets using 3DGS and mesh representations, these assets lack physical properties necessary for interactive simulation. To enable physically grounded interaction, GigaWorld-0-3D-Phys endows both robotic agents and manipulated objects with realistic physical attributes. For the robotic arm, precise physical parameters, such as joint friction and PD controller gains, are critical but rarely measurable in practice. Traditional system identification methods (e.g., manual tuning or simulated annealing (Li et al., 2024)) are slow and labor-intensive. In contrast, GigaWorld-0-3D-Phys leverages differentiable physics framework based on physics-informed neural networks (PINNs), enabling efficient, gradient-based parameter estimation. The pipeline operates in three stages, as shown in Fig. 10: (1) Real-world trajectories (ağ‘¡1, sğ‘¡1) are paired with randomly sampled physical parameters (ğ‘“, ğ‘, ğ‘‘), denoting friction, stiffness, and damping, and used to generate simulated rollouts. (2) surrogate model â„³ğ‘“,ğ‘,ğ‘‘ is trained to approximate the simulators dynamics by minimizing the MSE between predicted and simulated next states, yielding differentiable dynamics model. (3) With the surrogate model fixed, physical parameters are refined via gradient descent to minimize the discrepancy between simulated and real trajectories, converging to an optimal set (ğ‘“ *, ğ‘*, ğ‘‘*) that accurately replicates real-world behavior. Further details are provided in (Wang et al., 2025). For manipulated objects, we employ multimodal physics expert agent built on Qwen3-VL (Team, 2025) that infers physical properties from rendered orthographic views. The agent first estimates real-world scale by analyzing frontal view under text-guided constraints, resolving ambiguities in object size. Once scaled, it predicts mass, friction coefficient, and other physical attributes, associating them with semantic categories for downstream use, refer to (Wang et al., 2025) for more implementation details. For deformable objects, we extend the 3DGS representation by binding spring-mass systems to Gaussian particles, following the spirit of PhysTwin (Jiang et al., 2025). However, unlike PhysTwins per-scenario optimization, we are exploring feedforward approach that directly infers spring-mass parameters from monocular video, enabling fast, generalizable soft-body simulation. 3.2.4. GigaWorld-0-3D-Act While the community has developed numerous simulation-based approaches for automatic action generation, enabling task execution and large-scale expansion of robotic manipulation datasets, these methods often lack adaptability across scene complexity or generalization to novel object configurations. To address this, GigaWorld-0-3D-Act introduces two-tiered action generation pipeline tailored to both simple and complex manipulation scenarios, as shown in Fig. 11. For simple scenarios, GigaWorld-0-3D-Act first acquires small set of basic demonstrations via teleoperation or rule-based policies (Chen et al., 2025). These seed trajectories are then systematically extended to new object poses and scene layouts using the MimicGen framework (Mandlekar et al., 2023), enabling scalable and 12 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 11: The overall pipeline of GigaWorld-0-3D-Act. geometrically consistent action augmentation without additional human supervision. For complex scenarios involving multi-step reasoning or contact-rich interactions, GigaWorld-0-3D-Act leverages teleoperated demonstrations as cold-start data for reinforcement learning. It then employs fast online reinforcement learning (e.g., RLPD (Ball et al., 2023)) to rapidly bootstrap policy training. Once converged, the learned policy is deployed to generate large-scale, physically plausible, and diverse manipulation trajectories, effectively bridging high-level task structure with low-level motor control. 3.2.5. Function as Data Engine The GigaWorld-0-3D model suite, when integrated as cohesive pipeline, constructs geometrically consistent and physically realistic embodied manipulation scenes that serve as high-quality training data for VLA models, as shown in Fig. 17. Specifically, GigaWorld-0-3D-FG and GigaWorld-0-3D-BG generate foreground and background assets in complementary representations: 3DGS for photorealistic rendering, and meshes for accurate collision detection, dynamics simulation, and physical interaction. GigaWorld-0-3D-Phys then endows both the robotic manipulator and scene objects with physically grounded properties (e.g., mass, friction, elasticity) and performs differentiable system identification to calibrate actuation dynamics. Finally, GigaWorld-0-3D-Act synthesizes executable, collision-free manipulation trajectories that complete user-specified tasks. The resulting rendered data are directly usable for end-to-end VLA training. To further enhance data diversity, we integrate GigaWorld0-Video-AppearanceTransfer to perform text-guided editing of texture, color, material, and lighting across the rendered scenes. This enables zero-shot domain expansion while preserving geometric and physical consistency, dramatically increasing the visual and contextual richness of the training distribution without additional 3D asset creation. 4. GigaWorld-0 Training Our training data combines publicly available datasets with proprietary data collected from our in-house robotic platforms. Public sources include AgiBotWorld (Bu et al., 2025) and RoboMind (Wu et al., 2024), which provide foundational coverage of manipulation and locomotion tasks. In addition, we collected thousands of hours of proprietary data using the Agilex Cobot Magic and AgiBot G1 platforms across total area of 3,100 m2, spanning five broad environment categories: industrial, commercial, office, residential, and laboratory settings. These are further subdivided into 14 distinct real-world scenarios, including supermarkets, hotel lobbies, coffee shops, bubble tea stores, convenience stores, restaurants, warehouse material handling zones, industrial assembly lines, pantries, private residences, apartment interiors, meeting rooms, office workstations, and laboratories. The collected tasks range from basic pick-and-place operations to long-horizon sequential activities, mobile 13 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Table 2: Training efficiency of GigaWorld-0-Video-Dreamer under different distributed training configurations (8H20 GPUs, batch size 32). Act. Ckpt. indicates activation checkpoint of the feedforward network. OOM. = Out of Memory. Dist. Framework Act. Ckpt. FP8 Sparse Attn. w/ MoE Time (s/step) Memory (MB) DeepSpeed-Zero0 DeepSpeed-Zero2 FSDP-2 DeepSpeed-Zero0 DeepSpeed-Zero2 FSDP-2 DeepSpeed-Zero0 DeepSpeed-Zero2 FSDPDeepSpeed-Zero0 DeepSpeed-Zero2 FSDP-2 OOM. 32.84 33.19 29.61 29.75 29.53 25.54 25.44 25.38 OOM. 33.27 33. OOM. 95241 89355 89781 76419 71857 90077 76937 73131 OOM. 84699 73997 manipulation in dynamically changing layouts, and interactions with deformable objects. Training video foundation models is computationally intensive. To enable efficient and cost-effective training, GigaWorld-0-Video-Dreamer adopts sparse attention and FP8-precision training. Moreover, since most contemporary VLA models (Black et al., 2024; GigaAI et al., 2025; Intelligence et al., 2025) operate on 480p inputs, we train our model at resolution of 480768 for 61-frame sequences, striking balance between visual fidelity and training efficiency. Our training infrastructure, GigaTrain1 (GigaAI, 2025), is unified distributed framework designed for scalability and flexibility. It supports seamless multi-GPU/multi-node execution and integrates leading large-model training strategies, including: Distribution framework: DeepSpeed ZeRO (Stages 03), FSDP2; Mixed-precision training (FP16, BF16, FP8); Gradient accumulation, gradient checkpointing, and exponential moving average (EMA); Configurable optimizers, learning rate schedulers, and other training modules. This design enables both large-scale pretraining and resource-constrained post-training (e.g., fine-tuning with limited compute). To facilitate community adoption, we report resource consumption for various post-training configurations under modest hardware (e.g., 8H20 GPUs with batch size 32), providing practical guidance for users seeking to adapt our model and framework for downstream embodied tasks. As shown in Tab. 2, FSDP-2 achieves the best memory efficiency among distributed training frameworks, followed by DeepSpeed ZeRO-2 and then ZeRO-0, though stronger memory optimization comes at the cost of increased communication overhead and slightly longer per-step latency. The use of FP8 precision consistently reduces both memory consumption and training time across all frameworks, demonstrating its effectiveness for scalable video foundation model training. For attention efficiency, we adopt NATTEN (Hassani et al., 2023) as our sparse attention operator due to its superior speedup over SageAttention (Zhang et al., 2024), though it requires fine-tuning to avoid performance degradation in the absence of adaptation. Finally, when scaling to 4-expert MoE architecture, the increased parameter footprint necessitates the application of activation checkpointing, specifically on the feedforward networks, to maintain feasible memory usage during training, enabling stable convergence under constrained hardware. 1https://github.com/open-gigaai/giga-train GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 12: Visualization results of GigaWorld-0-Video-Dreamer conditioned on the same initial frame but different text prompts, demonstrating its ability to produce diverse, semantically consistent future trajectories. 5. Experiments To evaluate the capabilities of our foundation model GigaWorld-0-Video-Dreamer, we conduct comprehensive assessments on two benchmarks (DreamGen Bench (Jang et al., 2025), PBench (Ali et al., 2025)), measuring performance across multiple dimensions: physical plausibility, geometric consistency, text-to-video alignment, 15 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 13: Multi-view visualization results of GigaWorld-0-Video-Dreamer conditioned on the same initial frame but different text prompts. multi-view coherence, and visual fidelity. Quantitative results demonstrate that GigaWorld-0-Video-Dreamer achieves state-of-the-art performance in almost all metrics, significantly outperforming existing video generation models in simulating realistic embodied interactions. In addition to quantitative evaluation, we perform extensive visual analysis to validate the semantic correctness, temporal coherence, and contextual richness of the generated videos. These visualizations confirm that GigaWorld-0 produces physically grounded and spatially consistent scenes under diverse object configurations and environmental conditions. Ultimately, as scalable data engine for embodied AI, GigaWorld-0 demonstrates strong downstream utility. Policies trained on GigaWorld-0-generated data exhibit significant improvements in VLA task performance, particularly in generalization scenarios involving novel textures, novel object placements, and novel camera viewpoints. 5.1. Benchmark Results While wide variety of world model benchmarks (Ali et al., 2025; Duan et al., 2025; Huang et al., 2024; Jang et al., 2025; Zhang et al., 2025) exist, spanning general-purpose scene understanding, content creation, embodied navigation, and embodied manipulation, we specifically select PBench (Ali et al., 2025) and DreamGen Bench (Jang et al., 2025) for evaluation, as they are explicitly designed for embodied manipulation tasks and provide comprehensive metrics for assessing visual quality, physical plausibility, geometric consistency. As shown in Tab. 3, we compare our model against state-of-the-art video generation approaches (Ali et al., 2025; Wang et al., 2025), including Cosmos-Predict2-14B, Cosmos-Predict2.5-2B, and Wan2.2-5B and Wan2.2-14B. Despite having the smallest activated parameter, GigaWorld-0-Video-Dreamer achieves the highest overall score on PBench (Robot Set), demonstrating superior efficiency and generation quality for embodied AI applications. Additionally, we provide detailed evaluation on DreamGenBench. Following the official protocol, we fine16 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 14: Visualization results of GigaWorld-0-Video-AppearanceTransfer, which enables photorealistic editing of texture, material, and lighting in real-world or simulation-acquired videos while preserving scene geometry, object semantics, and temporal coherence. tune both our model and open-source baselines (Wan2.2 and Cosmos-Predict2/2.5) on the publicly released GR1 robot dataset, which comprises 29 sequences for GR1-Env, 50 for GR1-Obj, and 47 for GR1-Behavior. All models are fine-tuned using the same hyperparameters as in DreamGen (Jang et al., 2025): batch size 64 for 200 training steps. During evaluation, we use the official DreamGen Bench codebase. Notably, the prompts embedded in the publicly released code differ from those reported in the original paper, to ensure fair comparison, we adopt the exact prompts described in the DreamGen paper (Jang et al., 2025), which enables us to reproduce their reported results. Furthermore, following the benchmarks instructions, we compute the PA II score using the VideoPhy (Bansal et al., 2024) protocol and report the final PA score as the 17 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Table 3: Evaluation on PBench Robot Set (Ali et al., 2025). For each column, the highest score is bolded. Model #Param. Quality Score i2v-bg i2v-s aes img bg-con mot sub-con o-con Domain Overall Score Score 14B Cosmos-Predict2 14B Wan2.2 5B Wan2.2 2B Cosmos-Predict2.5 GigaWorld-0-Video-Dreamer 2B(Act.) 97.6 97.6 48.1 93.6 98.9 97.4 97.3 47.0 94.0 66.3 97.6 67.1 95.9 95.9 48.6 91.0 95.4 95.0 46.7 92.7 97.9 63.9 93.8 91.3 49.3 92.1 74.2 99.2 66.8 99.2 12.0 11.9 12.0 12.2 12.6 93.1 88.1 90.2 90.2 91.9 84.0 83.2 80.1 84.7 88.2 79.88 78.85 77.15 79.95 82. Table 4: Evaluation on DreamGen Bench (Jang et al., 2025). For each column, the highest score is bolded. Method Param. GR1-Env GR1-Object GR1-Behavior Qwen-IF GPT-IF PA Qwen-IF GPT-IF PA Qwen-IF GPT-IF PA Cosmos-Predict2 Wan2.2 Wan2.2 Cosmos-Predict2.5 GigaWorld-0-Video-Dreamer 2B(Act.) 14B 14B 5B 2B 0.966 0.900 0.790 0.930 0.966 0.552 0.586 0.760 0.549 0.531 0.340 0.534 0.480 0.529 0.586 0.840 0.700 0.720 0.920 0.920 0.760 0.471 0.780 0.531 0.780 0.522 0.503 0.240 0.481 0. 0.894 0.870 0.830 0.830 0.894 0.638 0.458 0.570 0.477 0.468 0.280 0.320 0.471 0.638 0.446 average of PA (Jang et al., 2025) and PA II. Results on DreamGen Bench are shown in Tab. 4. Despite the absence of extensive GR1 data in our pretraining corpus, GigaWorld-0-Video-Dreamer, with only 2B activated parameters, consistently outperforms the comparable Cosmos-Predict2.5-2B across all three scenarios (GR1-Env, GR1-Obj, and GR1-Behavior) in terms of instruction-following fidelity, demonstrating stronger generalization and controllability in embodied manipulation. 5.2. Visualiztaion Results Visualization serves as crucial qualitative tool for evaluating world models, offering direct insight into the fidelity, controllability, and physical plausibility of generated data. To this end, we conduct extensive visual analysis of videos synthesized by GigaWorld-0. As shown in Fig. 12, GigaWorld-0-Video-Dreamer generates diverse future trajectories from shared initial frame under varying text prompts, ranging from rigid-body manipulation (e.g., grasping objects) to deformable object interactions (e.g., folding laundry). The results demonstrate strong adherence to textual instructions, high visual fidelity, and stable temporal coherence across complex scenes. Additional video examples are available on our project page. Moreover, GigaWorld0-Video-Dreamer supports multi-view video generation, as illustrated in Fig. 13. The rendered views exhibit consistent geometry, appearance, and action dynamics across camera poses, while faithfully following high-level instructions. Such multi-view consistency is essential for training VLA policies, which typically rely on multiperspective observations. Empirically, we find that IDMs used for gripper state estimation critically depend on multi-view inputs to achieve accurate and robust predictions, highlighting the practical value of our multi-view generation capability. We further visualize the capabilities of GigaWorld-0-Video-AppearanceTransfer, as shown in Fig. 14. This module enables photorealistic, text-guided editing of texture, material, and lighting in both real-world collected videos and simulation-generated sequences. By transferring rich, diverse appearance attributes while preserving motion dynamics and geometric structure, it facilitates efficient, low-cost augmentation of training data for embodied AI, significantly expanding the visual and contextual diversity of the training distribution without requiring new physical recordings or 3D asset authoring. Additionally, Fig. 15 demonstrates the capability of GigaWorld-0-Video-ViewTransfer to synthesize photorealistic observations from arbitrary camera viewpoints, even when conditioned on real-world collected videos. This enables seamless augmentation of single-view datasets with multi-perspective analogs, significantly enriching 18 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 15: Visualization results of GigaWorld-0-Video-ViewTransfer, which synthesizes photorealistic videos from arbitrary camera viewpoints while simultaneously adapting robot arm trajectories to maintain physical plausibility and action consistency, enabling the generation of diverse embodied manipulation data. the spatial diversity of training data without requiring additional physical captures or costly 3D reconstruction. Furthermore, Fig. 16 illustrates the effectiveness of GigaWorld-0-Video-MimicTransfer in translating first-person human manipulation demonstrations into robot-executable trajectories. The generated motions exhibit precise spatial alignment between the human hand and the robot end-effector, while preserving natural dynamics and physical plausibility. This cross-embodiment transfer greatly enhances the utility of egocentric video data for training VLA models, enabling the use of abundant, low-cost human demonstration videos as scalable supervision signal for robotic policy learning. Finally, we visualize the 3D scenes generated by GigaWorld-0-3D, as shown in Fig. 17. The foreground objects are synthesized by GigaWorld-0-3D-FG, while the background is reconstructed from real-world captures using GigaWorld-0-3D-BG via 3DGS and mesh-based modeling. The resulting scene exhibits high geometric 19 GigaWorld-0: World Models as Data Engine to Empower Embodied AI consistency across viewpoints and seamless integration between foreground and background components. Combined with physically plausible object properties and articulated dynamics from GigaWorld-0-3D-Phys and GigaWorld-0-3D-Act, this enables the construction of photorealistic, spatially coherent, and simulation-ready embodied manipulation environmentsproviding robust foundation for training and evaluating VLA models. 5.3. Downstream Task Results The most effective validation of GigaWorld-0 as data engine for embodied AI lies in its ability to enhance the training of VLA models. To this end, we train GigaBrain-0 (GigaAI et al., 2025) on data synthesized by GigaWorld-0. The resulting policies demonstrate strong real-world performance and significantly improved generalization across diverse robotic tasks. GigaBrain-0 succeeds in dexterous manipulation tasks such as Laundry Folding  (Fig. 18)  and Paper Towel Preparation  (Fig. 19)  , long-horizon mobile manipulation tasks including Juice Preparation  (Fig. 21)  and Table Bussing  (Fig. 20)  , as well as dynamic mobile operations like Boxes Moving  (Fig. 22)  and Laundry Baskets Moving  (Fig. 23)  . These results highlight the fidelity, diversity, and task coverage of the synthetic data generated by GigaWorld-0, enabling robust policy learning without extensive real-world demonstrations. comprehensive quantitative analysis of task success rates, robustness, and ablation studies can be found in GigaBrain-0 (GigaAI et al., 2025). 6. Conclusion In this work, we presented GigaWorld-0, scalable and controllable world model designed to serve as high-fidelity data engine for embodied AI. By unifying photorealistic video generation with geometrically consistent and physically grounded 3D scene simulation, GigaWorld-0 enables the efficient synthesis of diverse, instruction-conditioned interaction data spanning novel textures, object configurations, and camera viewpoints, circumventing the cost and scalability bottlenecks of real-world data collection. Through rigorous evaluation across multiple embodied benchmarks, we demonstrated that training VLA policies on GigaWorld-0-generated data leads to significant improvements in task success, robustness, and zero-shot generalization in real-world robotic environments. Looking ahead, our work lays the foundation for several compelling directions in world model research for robotics. First, while GigaWorld-0 currently functions as powerful data engine, natural evolution is to deploy it as an interactive policy environment for model-based reinforcement learningenabling agents to safely explore, plan, and refine behaviors in simulation before real-world execution. Second, world models like GigaWorld-0 may ultimately learn universal priors over physical dynamics, semantic affordances, and task structure, allowing them to transition from passive data generators to active policy co-designers that propose plausible action sequences or decompose complex tasks into executable subgoals. Finally, closing the loop between real-world experience and synthetic generation, where robot rollouts continuously improve the world model, which in turn produces higher-quality training data, could enable self-improving robotic systems capable of lifelong, autonomous learning. We hope GigaWorld-0 accelerates progress toward this vision and inspires broader community exploration of world models as central infrastructure for the next generation of embodied intelligence. 20 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 16: Visualization results of GigaWorld-0-Video-MimicTransfer, which translates first-person human demonstration videos into robot-executable manipulation trajectories, enabling scalable synthesis of crossembodiment training data for VLA models. 21 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 17: Visualization results of GigaWorld-0-3D, showcasing geometrically consistent rendering and physically realistic robot actions. Figure 18: Deployment of GigaBrain-0 on the G1 humanoid robot for real-world laundry folding. Figure 19: Deployment of GigaBrain-0 on the PiPER arms for real-world paper towel preparation. 22 GigaWorld-0: World Models as Data Engine to Empower Embodied AI Figure 20: Deployment of GigaBrain-0 on PiPER arms for real-world table bussing. Figure 21: Deployment of GigaBrain-0 on G1 humanoid robot for real-world juice preparation. Figure 22: Deployment of GigaBrain-0 on the G1 humanoid robot for real-world paper towel preparation. Figure 23: Deployment of GigaBrain-0 on the PiPER arms for real-world laundry baskets moving. 23 GigaWorld-0: World Models as Data Engine to Empower Embodied AI References [1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. 2, 3, 4 [2] RIA AI. Rmbg-1.4: Background removal model, 2025. URL https://huggingface.co/briaai/RMBG-1.4. 11 [3] Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, et al. Cosmos-transfer1: Conditional world generation with adaptive multimodal control. arXiv preprint arXiv:2503.14492, 2025. 3, 4 [4] Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, et al. World simulation with video foundation models for physical ai. arXiv preprint arXiv:2511.00062, 2025. 15, 16, 18 [5] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 3 [6] Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. 10 [7] Philip Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. In ICML, 2023. 13 [8] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. 17 [9] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. ğœ‹0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 14 [10] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Xindong He, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. In IROS, 2025. 13 [11] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In CVPR, 2025. [12] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. 12 [13] Zhehao Dong, Xiaofeng Wang, Zheng Zhu, Yirui Wang, Yang Wang, Yukun Zhou, Boyuan Wang, Chaojun Ni, Runqi Ouyang, Wenkang Qin, et al. Emma: Generalizing real-world robot manipulation via generative visual transfer. arXiv preprint arXiv:2509.22407, 2025. 4, 7, 9 [14] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. NeurIPS, 2023. 3 [15] Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. 24 GigaWorld-0: World Models as Data Engine to Empower Embodied AI [16] Yu Fang, Yue Yang, Xinghao Zhu, Kaiyuan Zheng, Gedas Bertasius, Daniel Szafir, and Mingyu Ding. Rebot: Scaling robot learning with real-to-sim-to-real robotic video synthesis. arXiv preprint arXiv:2503.14526, 2025. 4 [17] Jiashi Feng, Xiu Li, Jing Lin, Jiahang Liu, Gaohong Liu, Weiqiang Lou, Su Ma, Guang Shi, Qinlong Wang, Jun Wang, et al. Seed3d 1.0: From images to high-fidelity simulation-ready 3d assets. arXiv preprint arXiv:2510.19944, 2025. 10 [18] Yao Feng, Hengkai Tan, Xinyi Mao, Guodong Liu, Shuhe Huang, Chendong Xiang, Hang Su, and Jun Zhu. Generalist bimanual manipulation via foundation video diffusion models. arXiv preprint arXiv:2507.12898, 2025. 3 [19] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601, 2023. [20] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. arXiv preprint arXiv:2405.17398, 2024. 3 [21] Daniel Gatis. tool to remove images background. arXiv preprint arXiv:2203.06173, 2025. 11 [22] GigaAI. Gigatrain: An efficient and scalable training framework for ai models. https://github.com/ open-gigaai/giga-train, 2025. 14 [23] GigaAI, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, et al. Gigabrain-0: world model-powered vision-language-action model. arXiv preprint arXiv:2510.19430, 2025. 14, 20 [24] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In CVPR, 2023. 5, 14 [25] Jing He, Haodong Li, Wei Yin, Yixun Liang, Leheng Li, Kaiqiang Zhou, Hongbo Zhang, Bingbing Liu, and Ying-Cong Chen. Lotus: Diffusion-based visual foundation model for high-quality dense prediction. arXiv preprint arXiv:2409.18124, 2024. 7 [26] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. 3 [27] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 16 [28] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. ğœ‹0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. 14 [29] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. Dreamgen: Unlocking generalization in robot learning through video world models. arXiv preprint arXiv:2505.12705, 2025. 3, 15, 16, 17, [30] Hanxiao Jiang, Hao-Yu Hsu, Kaifeng Zhang, Hsin-Ni Yu, Shenlong Wang, and Yunzhu Li. Phystwin: Physics-informed reconstruction and simulation of deformable objects from videos. arXiv preprint arXiv:2503.17973, 2025. 12 25 GigaWorld-0: World Models as Data Engine to Empower Embodied AI [31] Yuxin Jiang, Shengcong Chen, Siyuan Huang, Liliang Chen, Pengfei Zhou, Yue Liao, Xindong He, Chiming Liu, Hongsheng Li, Maoqing Yao, et al. Enerverse-ac: Envisioning embodied environments with action condition. arXiv preprint arXiv:2505.09723, 2025. 3 [32] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM ToG, 2023. 10, [33] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3, 4 [34] Haoyun Li, Ivan Zhang, Runqi Ouyang, Xiaofeng Wang, Zheng Zhu, Zhiqin Yang, Zhentao Zhang, Boyuan Wang, Chaojun Ni, Wenkang Qin, et al. Mimicdreamer: Aligning human and robot demonstrations for scalable vla training. arXiv preprint arXiv:2509.22199, 2025. 4, 9 [35] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 12 [36] Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al. Genie envisioner: unified world foundation platform for robotic manipulation. arXiv preprint arXiv:2508.05635, 2025. 3 [37] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [38] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. 5 [39] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 5 [40] Liu Liu, Xiaofeng Wang, Guosheng Zhao, Keyu Li, Wenkang Qin, Jiaxiong Qiu, Zheng Zhu, Guan Huang, and Zhizhong Su. Robotransfer: Geometry-consistent video diffusion for robotic visual policy transfer. arXiv preprint arXiv:2505.23171, 2025. 4, 7, 9, 10 [41] Zeyi Liu, Shuang Li, Eric Cousineau, Siyuan Feng, Benjamin Burchfiel, and Shuran Song. Geometry-aware 4d video generation for robot manipulation. arXiv preprint arXiv:2507.01099, 2025. 3 [42] Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. In ICCV, 2025. 10 [43] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. arXiv preprint arXiv:2310.17596, 2023. 12 [44] Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay Mandlekar, Buck Babich, Gavriel State, Marco Hutter, and Animesh Garg. Orbit: unified simulation framework for interactive robot learning environments. RAL, 2023. 10 [45] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, et al. Recondreamer: Crafting world models for driving scene reconstruction via online restoration. arXiv preprint arXiv:2411.19548, 2024. 3, 11 26 GigaWorld-0: World Models as Data Engine to Empower Embodied AI [46] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Xinze Chen, Guanghong Jia, Guan Huang, and Wenjun Mei. Recondreamer-rl: Enhancing reinforcement learning via diffusion-based scene reconstruction. arXiv preprint arXiv:2508.08170, 2025. 3 [47] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 2020. 5 [48] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 6, 8, 11 [49] Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, et al. Cosmos-drive-dreams: Scalable synthetic driving data generation with world foundation models. arXiv preprint arXiv:2506.09042, 2025. 3 [50] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv preprint arXiv:2503.20523, 2025. 3 [51] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv2104.09864, 2023. 5 [52] Hengkai Tan, Yao Feng, Xinyi Mao, Shuhe Huang, Guodong Liu, Zhongkai Hao, Hang Su, and Jun Zhu. Anypos: Automated task-agnostic actions for bimanual manipulation. arXiv preprint arXiv:2507.12768, 2025. 6 [53] Qwen Team. Qwen3-vl, 2025. URL https://github.com/QwenLM/Qwen3-VL. 12 [54] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In IROS, 2012. 10 [55] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 4, 5, 7, [56] Boyuan Wang, Xinpan Meng, Xiaofeng Wang, Zheng Zhu, Angen Ye, Yang Wang, Zhiqin Yang, Chaojun Ni, Guan Huang, and Xingang Wang. Embodiedreamer: Advancing real2sim2real transfer for policy training via embodied world modeling. arXiv preprint arXiv:2507.05198, 2025. 4, 12 [57] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang. Moge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training supervision. In CVPR, 2025. 8 [58] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-drive world models for autonomous driving. In ECCV, 2024. 3 [59] Xinjie Wang, Liu Liu, Yu Cao, Ruiqi Wu, Wenkang Qin, Dehui Wang, Wei Sui, and Zhizhong Su. Embodiedgen: Towards generative 3d world engine for embodied intelligence. arXiv preprint arXiv:2506.10600, 2025. 11, 12 [60] Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstructions with single-step diffusion models. In CVPR, 2025. 27 GigaWorld-0: World Models as Data Engine to Empower Embodied AI [61] Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation. arXiv preprint arXiv:2412.13877, 2024. 13 [62] Qi Wu, Janick Martinez Esturo, Ashkan Mirzaei, Nicolas Moenne-Loccoz, and Zan Gojcic. 3dgut: Enabling distorted cameras and secondary rays in gaussian splatting. In CVPR, 2025. 11 [63] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, et al. Sapien: simulated part-based interactive environment. In CVPR, 2020. 8, [64] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In CVPR, 2025. 10, 11 [65] Yuan Xu, Jiabing Yang, Xiaofeng Wang, Yixiang Chen, Zheng Zhu, Bowen Fang, Guan Huang, Xinze Chen, Yun Ye, Qiang Zhang, et al. Egodemogen: Novel egocentric demonstration generation enables viewpoint-robust manipulation. arXiv preprint arXiv:2509.22578, 2025. 4, 8 [66] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023. 3 [67] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 10 [68] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [69] Tianwei Yin, MichaÃ«l Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. NeurIPS, 2024. 10 [70] Chengbo Yuan, Suraj Joshi, Shaoting Zhu, Hang Su, Hang Zhao, and Yang Gao. Roboengine: Plug-andplay robot data augmentation with semantic robot segmentation and background generation. arXiv preprint arXiv:2505.18738, 2025. 4 [71] Jiahan Zhang, Muqing Jiang, Nanru Dai, Taiming Lu, Arda Uzunoglu, Shunchi Zhang, Yana Wei, Jiahao Wang, Vishal Patel, Paul Pu Liang, et al. World-in-world: World models in closed-loop world. arXiv preprint arXiv:2510.18135, 2025. 16 [72] Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. arXiv preprint arXiv:2410.02367, 2024. 14 [73] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM ToG, 2024. 10 [74] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 7 [75] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, et al. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. arXiv preprint arXiv:2410.13571, 2024. 3, 11 [76] Guosheng Zhao, Xiaofeng Wang, Chaojun Ni, Zheng Zhu, Wenkang Qin, Guan Huang, and Xingang Wang. Recondreamer++: Harmonizing generative and reconstructive models for driving scene representation. arXiv preprint arXiv:2503.18438, 2025. 3, 11 28 GigaWorld-0: World Models as Data Engine to Empower Embodied AI [77] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. In AAAI, 2025. 3, [78] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 10 [79] Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan. Tesseract: Learning 4d embodied world models. arXiv preprint arXiv:2504.20995, 2025. 3 [80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 4 [81] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024. 3 [82] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024. 2,"
        }
    ],
    "affiliations": [
        "GigaAI"
    ]
}