{
    "paper_title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground",
    "authors": [
        "Dmitrii Stoianov",
        "Danil Taranets",
        "Olga Tsymboi",
        "Ramil Latypov",
        "Almaz Dautov",
        "Vladislav Kruglikov",
        "Nikita Surkov",
        "German Abramov",
        "Pavel Gein",
        "Dmitry Abulkhanov",
        "Mikhail Gashkov",
        "Viktor Zelenkovskiy",
        "Artem Batalov",
        "Aleksandr Medvedev",
        "Anatolii Potapov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications."
        },
        {
            "title": "Start",
            "content": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground Gen-T Team T-Tech, Moscow, Russia Correspondence: anatolii.s.potapov@gmail.com 5 2 0 2 1 1 ] . [ 1 0 3 4 0 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications. hf.co/collections/t-tech/t-pro-"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have progressed from basic text generation to systems capable of multi-step reasoning and efficient inference. Recent foundation models show that reasoningoriented training (DeepSeek-AI et al., 2025a; Yang et al., 2025) and improved decoding methods (Chen et al., 2023; Li et al., 2024e) can substantially boost both accuracy and speed. In the Russian open-source space, progress remains limited. Most strong models are closed and accessible only through APIs (Mamedov et al., 2025; Zmitrovich et al., 2023), while open models are typically small adaptations of multilingual systems (Nikolich et al., 2024). There is no unified ecosystem for studying Russian-language reasoning: high-quality evaluation sets are scarce, and, to the best of our knowledge, there are currently few public demos that let users compare direct answer1 ing and step-by-step reasoning, inspect inferencetime optimizations, or observe how decoding speed impacts user experience. To address these gaps, we introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and an interactive demo platform. The model supports two complementary modesdirect answering and explicit reasoning tracesenabling applications to balance speed and accuracy within single deployed system. Our training setup combines Cyrillic-dense tokenizer derived from Qwen3 (Yang et al., 2025), large-scale instructional midtraining, supervised fine-tuning focused on both reasoning and nonreasoning, preference optimization, and an adaptation of EAGLE-style speculative decoding (Li et al., 2024e) to accelerate Russian-language inference. To sum up, our main contributions are: T-pro 2.0, an open-weight Russian hybridreasoning LLM with improved inference efficiency via an optimized Cyrillic tokenizer and EAGLE-style speculative decoding. T-Wix, the largest open Russian hybridreasoning SFT dataset to date ( 500k samples) covering general instruction following, long-context tasks, and teacher-generated reasoning traces. T-Math, benchmark of Russian high-school olympiad-level mathematics problems for curriculum-aligned reasoning evaluation. An interactive web demo that exposes T-pro 2.0 as research-oriented live system1,2 , enabling side-by-side comparison of reasoning and non-reasoning modes, running tasks from our datasets and benchmarks, and viewing telemetry for inference-time optimizations. 1The interactive demonstration interface is publicly accessible at http://t-pro-2-0.streamlit.app 2The web demo video is available on YouTube. All model-related components (T-pro 2.0, EAGLE weights, and the T-Math benchmark) are released under the Apache-2.0 license, while the TWix corpus is released under the ODC-By open data license."
        },
        {
            "title": "2 Related Work",
            "content": "The development of Russian LLMs primarily follows two tracks: monolingual pre-training and adaptation of multilingual models. Early decoderonly baselines like ruGPT (Kuratov and Arkhipov, 2019; Zmitrovich et al., 2023) and commercial systems such as YandexGPT3 and GigaChat (Mamedov et al., 2025) focus on Russian-centric pretraining. While achieving promising results on Russian benchmarks, early versions face capability gap compared to leading multilingual LLMs like Qwen (Yang et al., 2024) and Llama (Dubey et al., 2024). To mitigate these limitations, T-pro 1.04 adopts continued pre-training strategy on large-scale Russian corpora, reaching state-of-the-art results on MERA (Fenogenova et al., 2024) among open Russian models. Its release aligns with broader shift toward strengthening open-source Russian LLMs, alongside projects such as Saiga (Gusev, 2023), RuAdapt (Tikhomirov and Chernyshev, 2024), and Vikhr (Nikolich et al., 2024). These works emphasize the value of mitigating English-centric tokenizer limitations (Petrov et al., 2024) and extending pre-training on Russian data. This direction continues to grow: although YandexGPT-5-Lite5 is fully pre-trained model rather than an adaptation, its recent open release further expands the set of publicly available Russian foundation models. Russian Instruction Datasets. Existing Russian instruction datasets vary in provenance and domain coverage. Saiga (Gusev, 2023) applies self-instruct (Wang et al., 2023) pipelines producing ru_turbo_saiga, GrandMaster-PROMAX (Nikolich et al., 2024) aggregates sources across coding and general knowledge, and RuAdapt (Tikhomirov and Chernyshev, 2024) combines translated and native Russian samples. However, these datasets are usually small and contain few reasoning-intensive tasks. 3https://ya.ru/ai/gpt 4https://huggingface.co/t-tech/T-pro-it-1.0 5https://huggingface.co/yandex/ YandexGPT-5-Lite-8B-pretrain Efficient Inference. Speculative decoding accelerates autoregressive inference (Leviathan et al., 2023). EAGLE (Li et al., 2024d) uses lightweight head to generate draft token trees verified in parallel, achieving 23 speedup. Multi-Token Prediction (MTP) (Gloeckle et al., 2024) trains models to predict multiple tokens simultaneously and is deployed successfully in DeepSeek-V3 (DeepSeekAI et al., 2025b). GigaChat models (Mamedov et al., 2025) also adopt MoE architecture for increased efficiency on training and inference stages. Speculative decoding remains underexplored for general-purpose Russian LLMs, with few publicly documented deployments."
        },
        {
            "title": "3 T-pro 2.0",
            "content": "3.1 System and Demonstration Description We provide public web demo of T-pro 2.0 that exposes the model as an interactive hybrid-reasoning assistant and makes our inference optimizations directly observable. The service is stateless and does not store user prompts or completions. The interface supports multi-turn chat in Russian and English and side-by-side comparison with baseline models (by default Qwen3-32B-Instruct), allowing users to inspect both answers and reasoning traces under identical serving conditions. The demo currently supports text-only interactions and does not perform additional server-side content filtering beyond what is built into the underlying models. Architecture. The demo is single-page web application backed by lightweight Python HTTP server. The server exposes simple JSON API, attaches configuration options (model, decoding mode, generation parameters) received from the UI, and forwards requests to two serverless SGLang endpoints (Gu et al., 2024). Each endpoint runs on single NVIDIA H100 GPU: one hosts Tpro 2.0 with an EAGLE-style speculative decoding pipeline (draft head + 32B verifier), and the other hosts the Qwen3-32B baseline with standard autoregressive decoding. The deployment is tuned for interactive use and supports around 20 concurrent users per model while keeping per-request latency low. User interface and functionality. Figure 1 shows the main layout. The central comparison view presents parallel completions from two systems. For each side, users can independently choose between standard and reasoning modes. 2 Figure 1: Screenshot of the system demo of the T-pro 2.0 EAGLE. Outputs are streamed token by token, making differences in latency, verbosity, and reasoning structure directly visible. control panel above the chat area lets users select models, toggle reasoning per model, and adjust decoding parameters such as temperature, maximum length, and sampling options. All decoding settings used for given interaction are displayed in the UI, so that qualitative comparisons can be reproduced outside the demo. typical interaction consists of selecting preset prompt (or entering custom query), choosing reasoning and generation settings, and launching both models to compare their outputs and telemetry. To support systematic probing, the interface provides small library of predefined prompts grouped by domain (general questions, math and science, code, etc.). Several presets are derived from our evaluation suites, including T-Math and other Russian reasoning benchmarks, so that users can quickly examine T-pro 2.0 on challenging tasks without reconstructing benchmark-style prompts. Performance telemetry and usage patterns. telemetry panel reports, for every request and model, the number of generated tokens, end-toend latency, streaming throughput in tokens per second, and the acceptance ratio of speculative tokens for T-pro 2.0. Relating these statistics to the visible outputs illustrates how speculative decoding affects both accuracy and perceived responsiveness for short conversational turns and long reasoning traces, complementing the benchmark results in Section 4. 3.2 Training recipe This section describes the T-pro 2.0 training pipeline, integrating tokenizer adaptation, instructional midtraining, general post-training, and EAGLE-based speculative decoding. At all stages, we perform MinHash deduplication against benchmarks to prevent data leakage. Cyrillic-dense tokenizer We address the systematic under-tokenization of Russian in multilingual models by replacing 34k low-frequency nonCyrillic tokens in the Qwen3 (Yang et al., 2025) vocabulary with Cyrillic ones while keeping the total size fixed. To build the expansion set, we extract 35.7k candidate tokens containing at least one Cyrillic character from four donor tokenizers (Qwen3, (Tikhomirov and Chernyshev, 2024) , RuAdapt cl100k_base (OpenAI et al., 2024), MGPT (Shliazhko et al., 2023)). For each candidate, we evaluate its decomposition under the current merge graph and iteratively add those merges required to make two-piece decompositions fully reachable. Four refinement passes make approximately 95% of candidates reachable. Tokens containing Cyrillic, pure Latin tokens, punctuation, and all 12symbol units are preserved, while the 34k removed tokens are selected via log-smoothed frequency 3 scoring on the midtraining mix. Our modification T-pro Qwen3 GigaChat Ruadapt-Qwen3 2. 3.63 2.89 3.26 Table 1: Average tokens per word on Wikipedia for eight Cyrillic languages (ru, uk, be, bg, sr, mk, kk, ky). Lower values indicate more efficient segmentation of Cyrillic text. Indicates https://huggingface.co/ai-sage/ GigaChat-20B-A3B-instruct model. yields substantial compression gains: on Russian Wikipedia, the share of Russian words tokenized into at most two tokens rises from 38% to 60% (see Table 7), and Tables 1, 9 further demonstrates that this improvement generalizes consistently across eight Cyrillic languages, with all of them exhibiting shorter average segmentations under our tokenizer. full set of tokenizer evaluation metrics is provided in Appendix B. Instructional midtraining To adapt the Qwen332B model to the new dense Russian tokenizer and enhance reasoning, we employ an intermediate stage on 40B tokens drawn from curated opensource instructions, synthetic tasks, and parallel corpora. The mixture is dominated by Russian (49%) and English (36%) text; in terms of domains, it is dominated by Reasoning (34.6%), General QA (28.8%), and Math (16.2%), supplemented by grounded synthetic Question Answering (QA), code, and real user dialogues. The data mixture undergoes rigorous domain-specific local sensitive hashing (LSH) deduplication and InsTag-based semantic deduplication (Lu et al., 2024; Abbas et al., 2023) to balance diversity. To ensure high quality and stylistic consistency, all assistant responses are regenerated using Qwen3-235B-A22B teacher. Training utilizes 32k context window, stabilizing the model for downstream supervised fine-tuning (SFT). Ablations show that the instruct-only midtraining outperforms mixtures retaining the fraction of raw pre-training data on reasoning tasks, improving ruAIME 2024 (T-Tech, 2025e) from 0.60 to 0.67. Separately, 8B-scale experiments confirm the tokenizer transition, with the T-pro tokenizer reaching higher MERA (Fenogenova et al., 2024) macro-average (0.574) than the original Qwen3 tokenizer (0.560). Full details and ablations are provided in Appendix C. Reward Model (RM) construction To support the T-pro 2.0 post-training pipeline, dedicated reward model is trained (see Appendix F). The RM is initialized from Qwen3-32B with scalar regression head and trained with BradleyTerry preference objective on sequences up to 32K tokens using Ulysses-style sequence parallelism. Synthetic preference data are generated using knockdown tournaments over completions from multiple instructand reasoning-oriented model groups of different scales, substantially reducing the number of pairwise evaluations relative to an exhaustive pairwise scheme. For each instruction, completion pairs are judged, pairs with positional bias are discarded, and transitive tournament relations are added to improve preference coverage. To assess downstream performance, we design an Arena-Hard Best-of-N benchmark based on the BoN (best@N worst@N) metric, on which our RM outperforms existing open-source reward models; full details are provided in Appendix F.1. General Post-Training The T-pro 2.0 posttraining pipeline is implemented through general and reasoning SFT, and on-policy Direct Preference Optimization (DPO), with all filtering procedures detailed in Appendices D-E. For the general part of the T-Wix SFT dataset, approximately 14M raw instructions from opensource corpora are reduced to 468k samples using deduplication, multi-stage filtering pipeline, and domain/complexity balancing across six domains (Math, Code, Science, General Instruction, General Knowledge, Writing) and three difficulty tiers (School, Student, Professor). Each instruction is expanded with 8 candidate completions generated by Qwen3-235B-A22B or DeepSeek-V3 (DeepSeekAI et al., 2025b) and then passed through an RMguided selection step. The resulting mixture is low-noise, domain-balanced, and predominantly Russian, with approximately 10% English data retained to preserve bilingual competence. For the reasoning component, approximately 30K samples are drawn from 450k English pool covering general reasoning, mathematics, natural sciences, and code. After translation and deduplication, candidate solutions are generated by the Qwen3-235B-A22B teacher model and midtraining student checkpoint and then filtered via RM-based rejection. For verifiable tasks, the highest-scoring factually correct teacher output is selected; for open-ended tasks, the shortest valid trace among the teachers top RM-ranked candidates is chosen. DPO is performed on 100k instructions sampled from the T-Wix dataset, with 90/10 general-toreasoning ratio. For each instruction, 16 on-policy completions are RM-scored, and one high-contrast preference pair (best vs. worst) is formed, so that observed failure modes are directly targeted and alignment is improved without the overhead of online RL. Speculative Decoding We integrated an EAGLEbased speculative decoding module into T-pro 2.0, where lightweight draft model proposes candidate tokens that are verified by the frozen 32B target model. The draft model consists of single Llama-2-based decoder layer with an FR-Spec component (Zhao et al., 2025), trained on hiddenstate reconstruction (smoothed L1) and token distribution alignment (KL divergence) losses. During inference, we employ EAGLE-2s dynamic drafttree mechanism via SGLang. As shown in Tables 2 and 3, at temperature 0.8 the module achieves an average speedup of 1.85 in standard mode, showing similar speedups for both standard and reasoning modes. STEM domains consistently outperform humanities (1.99 vs 1.62), due to more predictable token distributions in technical content. See Appendix for training pipeline details. Benchmark ruMT-Bench1 ruAlpaca2 ruCodeEval3 T-Math4 Average Speedup Acceptance Length Standard Reasoning Standard Reasoning 1.79 1.61 2.15 1.85 1.69 1.57 1.84 2. 1.83 3.31 2.94 3.93 3.39 3.10 2.85 3.34 4.01 3.33 Table 2: Aggregated T-pro-2.0-eagle performance at temperature 0.8. Comparison of relative speedup and average acceptance length for the standard and reasoning modes. 1T-Tech (2025c), 2T-Tech (2025a), 3Fenogenova et al. (2024), 4T-Tech (2025g) Domain Category Speedup Acceptance Length STEM & Business Social & Humanities Average 1.99 1.62 1. 3.57 2.88 3.19 Table 3: Aggregated acceleration results on ruMMLUPro. Includes Math, Chem, Eng, Bus, Phys, CS. Includes Econ, Bio, Health, Psych, Phil, Hist, Law."
        },
        {
            "title": "4 Evaluation",
            "content": "4.1 Benchmarks We evaluate along three axes: factual knowledge, dialogue, and reasoning capabilities. common-knowledge Russian benchmarks MERA, MaMuRAMu (Fenogenova et al., 2024), and ruMMLU-Pro (T-Tech, 2025f), targeting world knowledge and logical competence. Dialogue benchmarks Arena Hard Ru (T-Tech, 2025b), Arena Hard 2 (Li et al., 2024b,c), and WildChat Hard Ru (Kukushkin, 2024) (curated native Russian queries). WildChat uses o3-mini responses as baseline; DeepSeek-V3.1-Terminus (DeepSeekAI et al., 2025b) serves as judge across all arenas and DeepSeek-V3.1 (DeepSeek-AI et al., 2025b) for WildChat. Reasoning benchmarks AIME 24/25 (Zhang and Math-AI, 2024) (Zhang and Math-AI, 2025), MATH-500 (Hendrycks et al., 2021), GPQA Diamond (Rein et al., 2023), Vikhr Math/Physics (Kuleshov et al., 2025), and LiveCodeBench v4_v5 (Jain et al., 2024). English benchmarks are professionally localized (ruAIME, ruMATH-500, ruGPQA, ruLCB (T-Tech, 2025d)). Vikhr benchmarks use Math-Verify scoring 6. from All-Russian T-Math benchmark We introduce T-Math331 and Moscow problems olympiads (19982025), automatically extracted and human-verified. Details are provided in Appendix H. 4.2 Results General knowledge and dialogue abilities Table 4 shows the results on Russian generalknowledge and dialogue benchmarks. T-pro 2.0 performs consistently well across all evaluations, scoring 0.66 on MERA and 0.697 on ruMMLUPro. These numbers put it close to GPT-4o (0.714) and above Russian-adapted baselines such as RuadaptQwen3-32B-Instruct (0.652). On dialogue tasks, the model reaches 91.1 on Arena Hard Ru and 72.6 on WildChat Hard Ru, outperforming all open-source systems and most proprietary ones. On Arena Hard 2, T-pro 2.0 scores 53.5 on Hard Prompts and 64.8 on Creative Writing, showing that it reliably follows instructions across different task types. These results directly reflect 6https://github.com/huggingface/Math-Verify 5 Model MERA MaMuRAMu ruMMLU-Pro Arena WildChat Hard Ru Hard Ru Arena Hard 2 CW HP Open Source Models (27B-32B class) T-pro 2.0 (Ours) Qwen3-32B RuadaptQwen3-32B-Instruct1 Gemma 3 27B2 DeepSeek-R1-Distill-Qwen-32B 0.66 0.582 0.574 0.577 0.508 0.851 0.833 0.823 0.808 0.787 0.697 0.677 0.652 0.665 0.537 91.1 / 90.36 72.6 / 76.4 53.5 / 46.2 64.2 / 62.8 83.95 / 84.66 59.6 / 51.9 46.4 / 32.9 53.7 / 41.5 68.4 / 64.76 41.5 / 39.4 13 / 14.2 19.4 / 12.7 82.66 58. 34.07 / 22.83 12.1 / 8.7 23.5 7.2 / 7.2 74.7 5.9 / 3.5 Open Source Larger Scale & Proprietary Models DeepSeek-V3 DeepSeek-R13 YandexGPT5-Pro4 GigaChat 2 Max5 o4-mini6 (medium) GPT-4o7 1Tikhomirov and Chernyshev (2024), 2Team et al. (2025), 3DeepSeek-AI et al. (2025a), 4https://ya.ru/ai/gpt, 5Mamedov et al. (2025), 6OpenAI (2025), 7OpenAI et al. (2024). 0.736 0.604 0.649 0.714 0.875 0.864 0. 0.677 0.67 0.642 92.67 95.74 19.13 61.44 95.63 85.14 45.8 73.6 3.8 8.5 67 20.0 66.8 90.3 12.1 10.1 74.4 41.4 59.9 90.3 2.6 27.1 49.8 44.2 Table 4: Comparison of models on Russian language understanding and dialogue benchmarks. In Arena Hard 2, subsets are Hard Prompt (HP) and Creative Writing (CW). For entries reported as a/b, the first value corresponds to the reasoning setting and the second to the non-reasoning setting. o4-mini and DeepSeek-R1 are omitted from MERA as it does not support reasoning model mode, while YandexGPT5-Pro is omitted from MERA due to licensing restrictions. Model T-Math ruAIME ruAIME ruMATH-500 ruGPQA Diamond 2025 2024 ruLCB Vikhr Vikhr Math Physics Open Source Models (27B-32B class) T-pro 2.0 (Ours) Qwen3-32B RuadaptQwen3-32B-Instruct Gemma 3 27B DeepSeek-R1-Distill-Qwen-32B 0.541 0.529 0.444 0.208 0.254 Open Source Larger Scale & Proprietary Models DeepSeek-V3 DeepSeek-R1 YandexGPT5-Pro GigaChat 2 Max o4-mini (medium) GPT-4o 0.278 0.619 0.13 0.142 0.634 0.106 0.704 0.706 0.575 0.248 0. 0.319 0.800 0.062 0.102 0.781 0.090 0.646 0.625 0.450 0.231 0.402 0.285 0.800 0.046 0.062 0.771 0.069 0.94 0.938 0.450 0.860 0.898 0.882 0.972 0.682 0.702 0.958 0.766 0.591 0.606 0.591 0.439 0. 0.657 0.763 0.354 0.475 0.773 0.510 0.563 0.537 0.500 0.261 0.493 0.444 0.69 0.265 0.272 0.705 0.131 0.799 0.809 0.528 0.548 0.462 0.613 0.864 0.372 0.372 0.834 0.372 0.51 0.531 0.337 0.276 0. 0.367 0.469 0.252 0.245 0.408 0.296 Table 5: Comparison of models on Russian advanced reasoning benchmarks. the structure of the T-Wix corpus, which mixes general instruction-following with long-context tasks and distilled reasoning traces from stronger teacher models. Reasoning capabilities Table 5 summarizes performance on T-Math and localized reasoning benchmarks. On T-Math, the model scores 0.541, indicating strong performance on original olympiad-style Russian problems. On ruAIME 2024 and 2025 it reaches 0.704 and 0.646, sharply outperforming DeepSeek-V3 (0.319/0.285), GPT-4o (0.090/0.069) and all proprietary Russian models. Results on ruMATH-500 (0.94) and Vikhr Math (0.799) further confirm the models ability to perform mathematical reasoning in Russian under varied setups. These results also show that T-Math is challenging benchmark that reveals meaningful performance differences that are obscured by translated or adaptation-based alternatives. Crucially, the Russian-focused training does not hurt English performance. As shown in Table 23, T-pro 2.0 remains competitive on English reasoning benchmarks, with 0.765 on AIME 2024, 0.966 on MATH-500, and 0.556 on LiveCodeBench, on par with or better than other open-source models of similar scale. detailed breakdown of English results is provided in Appendix I."
        },
        {
            "title": "5 Conclusion",
            "content": "We present T-pro 2.0, an open-weight Russian language model tailored for hybrid reasoning and efficient inference. The combination of Cyrillic6 dense tokenizer, reasoning-focused midtraining stage, and an adapted EAGLE-style speculative decoding pipeline allows the model to deliver strong performance on Russian tasks without increasing model size and without notable degradation on English benchmarks. Along with the model, we release T-Wix, large-scale SFT dataset enriched with reasoning traces, and T-Math, benchmark designed to probe mathematical and analytical abilities in Russian. These results point to two broader takeaways. First, careful, targeted adaptation of strong multilingual backbones remains practical and reproducible route for building high-quality models for languages with limited resources. Second, tokenizer design and inference optimization are not optional details but key components for deploying reasoning-capable models beyond English. We expect the released models, datasets, evaluation code, and public demo to support research on Russianlanguage reasoning LLMs and to contribute to more transparent and consistent evaluation practices in this area."
        },
        {
            "title": "Ethical Statement",
            "content": "Possible Misuse. Our work may enable generation of misleading, offensive, or otherwise harmful content if deployed without appropriate safeguards. We do not support applications that restrict access to information, facilitate disinformation, target individuals or groups, or automate harmful actions. To mitigate these risks, we apply toxicity and safety filtering during post-training and provide usage guidelines for responsible deployment. Biases and Data Quality. The datasets used for pre-training and fine-tuning contain publicly available Russian and English text, which may include stereotypes, factual inaccuracies, or cultural biases. While automated filtering and manual checks are applied, residual biases may remain. We recommend additional evaluation when transferring the model to domains or communities that are underrepresented in the training data. Human Subjects and Privacy. This work does not involve human-subjects research or the collection of personally identifiable information. All data sources comply with their respective licenses and usage policies."
        },
        {
            "title": "Limitations",
            "content": "Despite the strong performance of T-pro 2.0, several limitations should be acknowledged, which are planned to be addressed in future work. Limited Agentic Capabilities No dedicated improvements for tool use or agentic behavior were incorporated into the model. Optimizations for function calling or complex multi-turn interactions were not performed, and as result, performance in these areas is expected to be comparable to or slightly below that of the base Qwen3-32B model. Enhancements in this direction are prioritized for future development. Offline-Only Reinforcement Learning Model alignment was conducted exclusively through offline DPO. Online reinforcement learning methods such as PPO (Schulman et al., 2017) or GRPO (Shao et al., 2024) were not employed. Although DPO is computationally efficient, the absence of interactive feedback may limit robustness and lead to performance degradation on out-ofdomain tasks. Unverified Long-Context Performance All training stages for T-pro 2.0 were carried out with fixed context window of 32k tokens, consistent with the base Qwen3-32B configuration. While support for up to 128k tokens is theoretically enabled via RoPE scaling, the models ability to maintain coherence and retrieve information over such extended contexts has not been empirically validated. Reproducibility Issues Full reproducibility is restricted by the use of proprietary datasets in midtraining and the DPO stage. However, the curated SFT dataset is being released to support and encourage further research, particularly in the development of high-quality Russian-language language models."
        },
        {
            "title": "Author Contributions",
            "content": "Administration and Supervision: Anatolii Potapov Training Pipelines Team: German Abramov, Pavel Gein Post-training Team: Dmitrii Stoianov, Olga Tsymboi, Danil Taranets, Ramil Latypov, Almaz Dautov, Dmitry Abulkhanov Inference Team: Vladislav Kruglikov, Nikita Surkov 7 Evaluation Team: Mikhail Gashkov, Viktor Zelenkovskiy, Artem Batalov, Alexandr Medvedev"
        },
        {
            "title": "References",
            "content": "2025. Turbo-alignment. https://github.com/ turbo-llm/turbo-alignment. GitHub repository. Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari S. Morcos. 2023. Semdedup: Dataefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540. Berk Atil, Sarp Aykent, Alexa Chittams, Lisheng Fu, Rebecca J. Passonneau, Evan Radcliffe, Guru Rajan Rajagopal, Adam Sloan, Tomasz Tudrej, Ferhan Ture, Zhe Wu, Lixinyu Xu, and Breck Baldwin. 2025. Nondeterminism of \"deterministic\" llm settings. Preprint, arXiv:2408.04667. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. Preprint, arXiv:2502.13923. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk, and 114 others. 2025. Llamanemotron: Efficient reasoning models. Preprint, arXiv:2505.00949. RALPH ALLAN BRADLEY and MILTON E. TERRY. 1952. Rank analysis of incomplete block designs: The method of paired comparisons. Biometrika, 39(34):324345. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model Preprint, decoding with speculative sampling. arXiv:2302.01318. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2025. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400. Peng Cui and Mrinmaya Sachan. 2025. Investigating the zone of proximal development of language models for in-context learning. Preprint, arXiv:2502.06990. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, 8 Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025b. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and 1 others. 2024. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783. Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Anastasia Minaeva, Denis Dimitrov, Alexander Panchenko, and Sergey Markov. 2024. Mera: comprehensive llm evaluation in russian. arXiv preprint arXiv:2401.04531. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, and 1 others. 2024. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737. Shiyang Gu and 1 others. 2024. Sglang: Efficient serving of llms with speculative decoding and continuous batching. GitHub repository. Igor Gusev. 2023. sian llama models. IlyaGusev/saiga. Saiga: Instruction-tuned rushttps://huggingface.co/ Dan Hendrycks and 1 others. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Hugging Face. 2025. Open r1: fully open reproduction of deepseek-r1. Shawn Im and Sharon Li. 2025. Can dpo learn diverse human values? theoretical scaling law. Preprint, arXiv:2408.03459. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. 2024. System optimizations for enabling training of extreme long sequence transformer models. In Proceedings of the 43rd ACM Symposium on Principles of Distributed Computing, PODC 24, page 121130, New York, NY, USA. Association for Computing Machinery. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Preprint, arXiv:2403.07974. Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, and Yahui Zhou. 2025a. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352. Diederik P. Kingma and Jimmy Ba. 2017. Adam: method for stochastic optimization. Preprint, arXiv:1412.6980. Alexander Kukushkin. 2024. wildchat-hard-ru. https: //github.com/kuk/wildchat-hard-ru. GitHub repository. Ilya Kuleshov, Pavel Ilin, Nikolay Kompanets, Ksenia Sycheva, and Aleksandr Nikolich. 2025. Doom: Difficult olympiads of math. ArXiv preprint. Yuri Kuratov and Alexey Arkhipov. 2019. Adaptation of deep bidirectional multilingual transformers for russian language. arXiv preprint arXiv:1912.11283. Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning (ICML 23), pages 1927419286. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024a. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. Preprint, arXiv:2308.12032. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. 2024b. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024c. From live data to high-quality benchmarks: The arena-hard pipeline. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024d. Eagle-2: Faster inference of language models with dynamic draft trees. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024e. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2025. Eagle-3: Scaling up inference acceleration of large language models via training-time test. arXiv preprint arXiv:2503.01840. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2025b. Pairjudge rm: Perform bestof-n sampling with knockout tournament. arXiv preprint arXiv:2501.13007. In progress work. Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Johan ObandoCeron, Siran Yang, Jiamang Wang, Wenbo Su, and Bo Zheng. 2025c. Part i: Tricks or traps? deep dive into rl for llm reasoning. Preprint, arXiv:2508.08221. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2024. #instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations. Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah A. Smith, Hannaneh Hajishirzi, and Nathan Lambert. 2025. Rewardbench 2: Advancing reward model evaluation. Preprint, arXiv:2506.01937. Valentin Mamedov, Evgenii Kosarev, Gregory Leleytner, Ilya Shchuckin, Valeriy Berezovskiy, Daniil Smirnov, Dmitry Kozlov, Sergei Averkiev, Lukyanenko Ivan, Aleksandr Proshunin, Ainur Israfilova, Ivan Baskov, Artem Chervyakov, Emil Shakirov, Mikhail Kolesov, Daria Khomich, Daria Latortseva, Sergei Porkhun, Yury Fedorov, and 14 others. 2025. GigaChat family: Efficient Russian language modeling through mixture of experts architecture. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 93106, Vienna, Austria. Association for Computational Linguistics. Aleksandr Nikolich, Konstantin Korolev, Sergei Bratchikov, Igor Kiselev, and Artem Shelmanov. 2024. Vikhr: The family of open-source instructiontuned large language models for russian. arXiv preprint arXiv:2405.13929. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, and 401 others. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. OpenAI. 2025. Introducing o3 and o4-mini. https:// openai.com/index/o3-o4-mini-system-card/. Accessed: 2025-05-15. 9 Aleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Biber. 2024. Language model tokenizers introduce unfairness between languages. arXiv preprint arXiv:2305.15425. T-Tech. 2025d. ru-reasoning-benchmarks. https://huggingface.co/collections/ t-tech/ru-reasoning-benchmarks. Face Datasets; accessed 2025. Hugging Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. T-Tech. 2025e. ruaime-2024. https://huggingface. Hugging co/datasets/t-tech/ruAIME-2024. Face Datasets; accessed 2025. T-Tech. 2025f. rummlu-pro. https://huggingface. co/datasets/t-tech/ruMMLU-pro. Hugging Face Datasets; accessed 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Preprint, arXiv:2305.18290. David Rein and 1 others. 2023. Gpqa: google-proof q&a benchmark for large language models. arXiv preprint arXiv:2311.16452. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxPreprint, Alec Radford, and Oleg Klimov. 2017. imal policy optimization algorithms. arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. 2023. mgpt: Few-shot learners go multilingual. Preprint, arXiv:2204.07580. Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, Graham Neubig, and Aditi Raghunathan. 2025. Overtrained language models are harder to fine-tune. In Forty-second International Conference on Machine Learning. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen, and Xia Hu. 2025. Stop overthinking: survey on efficient reasoning for large language models. Preprint, arXiv:2503.16419. T-Tech. 2025a. ru-alpaca-eval. https: //huggingface.co/datasets/t-tech/ ru-alpaca-eval. accessed 2025. Hugging Face Datasets; T-Tech. 2025b. ru-arena-hard. https://huggingface. co/datasets/t-tech/ru-arena-hard. Hugging Face Datasets; accessed 2025. T-Tech. 2025c. ru-mt-bench. https://huggingface. Hugging co/datasets/t-tech/ru-mt-bench. Face Datasets; accessed 2025. 10 T-Tech. 2025g. T-math. co/datasets/t-tech/T-math. Datasets; accessed 2025. https://huggingface. Hugging Face Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Yuan Tang, Alejandro Cuadron, Chenguang Wang, Raluca Popa, and Ion Stoica. 2025. Judgebench: benchmark for evaluating LLM-based judges. In The Thirteenth International Conference on Learning Representations. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025. Gemma 3 technical report. Preprint, arXiv:2503.19786. Mikhail Tikhomirov and Daniil Chernyshev. 2024. Facilitating large language model russian adaptation with learned embedding propagation. arXiv preprint arXiv:2412.21140. Tianchun Wang, Zichuan Liu, Yuanzhou Chen, Jonathan Light, Weiyang Liu, Haifeng Chen, Xiang Zhang, and Wei Cheng. 2025a. On the effect of sampling diversity in scaling llm inference. Preprint, arXiv:2502.11027. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, and 1 others. 2023. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560. Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, HooChang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, and Oleksii Kuchaiev. 2025b. HelpSteer3-Preference: Open human-annotated preference data across diverse tasks and languages. Preprint, arXiv:2505.11475. Wei Xiong, Jiarui Yao, Yuhui Xu, Bo Pang, Lei Wang, Doyen Sahoo, Junnan Li, Nan Jiang, Tong Zhang, Caiming Xiong, and Hanze Dong. 2025. minimalist approach to llm reasoning: from rejection sampling to reinforce. Preprint, arXiv:2504.11343. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. An Yang, Baosong Yang, Binyuan Hui, and 1 others. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. 2024. Mammoth2: Scaling instructions from the web. In Advances in Neural Information Processing Systems, volume 37, pages 9062990660. Curran Associates, Inc. Yifan Zhang and Team Math-AI. 2024. American invitational mathematics examination (aime) 2024. Yifan Zhang and Team Math-AI. 2025. American invitational mathematics examination (aime) 2025. Weilin Zhao, Tengyu Pan, Xu Han, Yudi Zhang, Ao Sun, Yuxiang Huang, Kaihuo Zhang, Weilun Zhao, Yuxuan Li, Jie Zhou, Hao Zhou, Jianyong Wang, Zhiyuan Liu, and Maosong Sun. 2025. FR-spec: Accelerating large-vocabulary language models via frequencyranked speculative sampling. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 39093921, Vienna, Austria. Association for Computational Linguistics. Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem Snegirev, Vitalii Kadulin, Sergey Markov, Tatiana Shavrina, Vladislav Mikhailov, and Alena Fenogenova. 2023. family of pretrained transformer language models for russian. arXiv preprint arXiv:2309.10931."
        },
        {
            "title": "A Released Resources and Licenses",
            "content": "All released components use permissive, researchfriendly licenses. The T-pro 2.0 model, its EAGLE draft weights, and the T-Math benchmark are distributed under Apache-2.0, allowing broad academic and commercial use. The T-Wix 500k corpus is released under ODC-By. Full license details appear in Table 6."
        },
        {
            "title": "B Tokenizer adaptation statistics",
            "content": "Russian and English corpora. Table 7 reports tokenization statistics for Russian and English on both Wikipedia and our in-domain SFT corpus (TWix). For Russian, the Cyrillic-dense T-pro 2.0 tokenizer substantially reduces the average number of tokens per word and increases the share of words segmented into at most two tokens, while English compression is essentially unchanged. Table 8 extends this analysis to eight Cyrillic languages using Wikipedia. In all cases the new tokenizer reduces tokens per word, with particularly large gains for Kyrgyz, which is poorly served by generic multilingual tokenizers. Comparison with other Cyrillic-rich tokenizers. Finally, Table 9 compares T-pro 2.0 against several strong Cyrillic-focused baselines. T-pro 2.0 achieves the lowest tokens-per-word for seven out of eight languages (ru, uk, be, bg, sr, mk, ky) and remains competitive on Kazakh, demonstrating that our tokenizer design is competitive with specialized alternatives."
        },
        {
            "title": "C Instructional midtraining",
            "content": "We employ an intermediate instructional midtraining stage between generic large-scale pre-training and downstream alignment. Starting from the publicly available Qwen3-32B dense model (Yang et al., 2025), already pre-trained on 36T tokens, we perform continual training on 40B tokens of instruction-style data. The goals of this stage are: (i) adapt the model to denser, Russian-centric tokenizer, (ii) learn useful representations for new subword units, and (iii) further strengthen Russian language and reasoning skills without degrading the base models capabilities. Training setup. Midtraining uses 4M-token global batch and 40B total tokens ( 9,750 steps). We train with AdamW using peak learning rate of 1 105 and cosine decay to 1 106, with 100 warmup batches. Data are formatted in the same chat-style schema and packed up to 32K context window without length curriculum. Training is performed in bf16 with FSDP full-shard and activation checkpointing. Remaining hyperparameters are listed in Table 10. Midtraining datamix All midtraining samples are in instruction format. The datamix combines (i) public instruction datasets from the Hugging Face Hub, (ii) web and forum data (e.g., questionanswer style threads), (iii) real userassistant dialogues, and (iv) synthetic instructional data and reasoning traces grounded in pre-training corpora (books, Common Crawl, code) via WebInstructstyle pipeline (Yue et al., 2024). Compared to the SFT stage, the midtraining datamix is intentionally larger and less curated: we trade some noise for broader coverage of tasks and domains. All instructions are derived from public sources; internal data are used only as anonymized targets in dialogue-style responses. Table 11 reports the category-level breakdown of the 40B-token corpus. In terms of language, the corpus is predominantly Russian and English: roughly 49% of tokens are Russian (19.6B), 36% English (14.4B), 9.3% code (3.7B) and 5.5% come from parallel RussianEnglish data (2.2B). InsTag deduplication. To control redundancy while preserving diversity across sources, we apply #INSTAG-based deduplication (Lu et al., 2024) independently within each component of the datamix (reasoning, general QA, code, etc.). The tagger is applied only to user utterances; all tags from multi-turn sample are unioned into single tag set. We then perform exact-match and semantic deduplication at the tag level, followed by greedy diversity sampling over tagged samples. This procedure gives macro-level control over the balance between different categories instead of deduplicating the raw pool as whole. On large components such as reasoning and general QA, only about 10 30% of the raw candidates are retained (the remaining 7090% are discarded), whereas for smaller, less repetitive sources we keep 8090% of samples. For each retained sample, the final assistant turn is regenerated with stronger teacher, Qwen3-235B, which improves answer quality and stylistic consistency while keeping the original user input and context intact. Ablations on datamix design We compare two variants of the midtraining corpus, both trained 12 Resource Type Location License Model T-pro 2.0 EAGLE weights Model component Benchmark dataset T-Math Instruction corpus T-Wix 500k https://huggingface.co/t-tech/T-pro-it-2.0 Apache-2.0 https://huggingface.co/t-tech/T-pro-it-2.0-eagle Apache-2.0 https://huggingface.co/datasets/t-tech/T-math Apache-2.0 https://huggingface.co/datasets/t-tech/T-Wix ODC-By Table 6: Released resources and licenses. (a) T-pro (b) Qwen (c) Ruadapt-Qwen3 (d) GigaChat Figure 2: Qualitative comparison of Russian tokenization. 220-character text is tokenized by T-pro 2.0, the original Qwen3 tokenizer, and other Cyrillic-optimized models. T-pro 2.0 encodes the text into just 55 tokens compared to 76 for Qwen3, demonstrating superior compression efficiency. Corpus Tokenizer tok/ word 1 tok (%) 2 tok (%) >2 tok (%) Russian ruWiki Qwen3 ruWiki T-pro 2.0 T-Wix Qwen3 T-Wix T-pro 2.0 3.12 2.38 2.70 2. 20.3 28.7 31.8 39.3 38.2 60.1 52.4 65.5 English enWiki Qwen3 enWiki T-pro 2.0 1.68 1.68 61.2 61. 83.7 83.7 61.8 39.9 47.6 34.5 16.3 16.3 Table 7: Tokenization density statistics for Russian and English on Wikipedia and our SFT corpus (T-Wix). We compare the original Qwen3 and Cyrillic-dense T-pro 2.0 tokenizers. Columns show: average tokens per word (tok/ word), percentage of words tokenized into exactly 1 token, at most 2 tokens, and more than 2 tokens. Tokens/Word % Words (2 tok) Lang Qwen3 T-pro Qwen T-pro ru uk be bg sr mk kk ky 3.12 3.70 3.97 2.99 3.26 3.04 4.60 4.35 2.38 2.80 2.94 2.35 2.62 2.41 3.07 3.09 38.20 31.17 30.15 43.42 37.65 42.42 15.30 21.27 60.13 45.79 41.36 59.60 51.79 57.19 37.69 39. Table 8: Tokenization density on Wikipedia for Cyrillic languages. Tokens/Word: average tokens per word; % Words (2 tok): percentage of words tokenized into at most 2 tokens. 13 Figure 3: Midtraining training loss as function of optimization steps. Loss drops steeply during the first 1k steps (4B tokens) and then gradually plateaus, indicating that most adaptation to the new tokenizer happens early in the run. Lang T-pro GigaChat Ruadapt-Qwen3 gpt-oss Hyperparameter Value ru uk be bg sr mk kk ky Avg 2.38 2.80 2.94 2.35 2.62 2.41 3.07 3.09 2.71 2.49 3.09 3.32 2.58 2.97 2.67 2.67 3. 2.89 2.43 3.29 3.54 2.50 3.07 2.70 4.60 3.97 3.26 2.70 2.92 3.03 2.56 2.73 2.59 3.11 3.17 2.85 Table 9: Tokenization density (tokens/word) on Wikipedia for T-pro and other Cyrillic-rich tokenizers. Lower is better. Indicates https://huggingface.co/ ai-sage/GigaChat-20B-A3B-instruct model. for 40B tokens with identical optimization settings  (Table 10)  : Pre-train + instruct: mixture including generic pre-training-style data (Common Crawl, Wikipedia, code) alongside instructionformatted examples. Instruct-only: the same instruction pool but without additional raw pre-training sources, i.e., all examples follow an explicit instructionresponse schema. The instruct-only variant thus allocates more of the 40B-token budget to high-quality instruction data, whereas the mixed variant spends fraction of tokens on generic web/code continuation. We evaluate both models on suite of Russian and multilingual math/reasoning benchmarks, including ruGlobal batch size (tokens) Total tokens Steps Max context length 4M (128 seq 32K) 40B 9,750 32K Optimizer Adam betas Adam ϵ Weight decay LR schedule Peak / min LR Warmup Gradient clipping Precision Parallelism AdamW (0.9, 0.95) 108 106 cosine 1 105 / 1 106 100 batches max norm 1.0 BF16 FSDP full-shard, act. checkpointing Table 10: Midtraining optimization setup. Category Share Tokens (B) Reasoning General Math Real chat IF Grounded QA synth Code Forum Summarization ICL 34.5% 29.3% 16.3% 5.5% 5.0% 3.8% 2.8% 1.7% 0.7% 0.4% 13.8 11.7 6.5 2.2 2.0 1.5 1.1 0.7 0.3 0. Table 11: Midtraining datamix by category (40B tokens total). AIME24/25, ruMATH500, ruGPQA, ruLCB, TMath, and Arena-style pairwise comparisons. All evaluations are zero-shot; AIME-style metrics are computed as avg@8 over 30 problems, and other benchmarks are run once due to computational 14 cost. Table 12 shows representative subset of metrics. Across most math and reasoning benchmarks, the instruct-only datamix outperforms or matches the mixed variant despite using the same token budget, and even early checkpoints from the instructonly run are ahead of the mixed model. This is consistent with recent evidence that heavily pretrained models are harder to adapt via continual pre-training (Springer et al., 2025), especially when the additional data distribution differs from downstream tasks. Benchmark PT+I I-only ruAIME24 ruAIME25 ruMATH500 ruGPQA ruLCB T-Math Arena hard (think) Arena wildchat ru (think) 0.60 0.47 0.93 0.58 0.53 0.49 43.7 55.0 0.67 0.63 0.94 0.66 0.55 0.50 44.5 55.1 Table 12: Ablation on midtraining datamix design (zeroshot). PT+I denotes the pre-training+instruct mixture; I-only uses only instruction-formatted data. We did not run additional ablations such as training on original (non-regenerated) answers or disabling InsTag deduplication. In the first case, instruction data come from heterogeneous sources with uneven answer quality and formats, and we found it undesirable to train on such outof-distribution completions. In the second case, skipping deduplication would require regenerating answers for much larger pool of raw samples, significantly increasing computational cost; we leave this exploration for future work. Tokenizer adaptation and MERA results key objective of midtraining is to adapt the model to new, denser tokenizer for Russian without degrading downstream quality. To quantify the impact of the tokenizer choice, we train two 8B models on the same midtraining datamix with identical optimization hyperparameters, differing only in the tokenizer (original Qwen3 vs. T-pro 2.0). Table 13 reports MERA scores for these two variants. The T-pro 2.0 tokenizer attains macroaverage score comparable to the original one (0.574 vs. 0.560), with only small per-task differences in both directions. In other words, replacing the tokenizer with denser Cyrillic segmentation does not degrade general Russian-language performance on MERA, which is the primary design goal of midtraining. Task USE MaMuRaMu ruWorldTree ruCodeEval RCB MathLogicQA ruOpenBookQA RWSD CheGeKa LCS PARUS MultiQ ruMultiAr ruTiE ruModAr Qwen tokenizer T-pro 2.0 tokenizer 0.198 0.784 0.966/0.966 0.173/0.45/0.585 0.557/0.479 0.710 0.897/0.897 0.446 0.30/0.368 0.102 0.868 0.381/0.517 0.402 0.788 0.515 0.191 0.796 0.966/0.966 0.454/0.689/0.756 0.564/0.47 0.731 0.922/0.923 0.250 0.31/0.384 0.096 0.912 0.344/0.478 0.400 0.798 0.627 AVG 0. 0.574 Table 13: MERA scores for an 8B model with the original Qwen3 tokenizer and the Cyrillic-dense T-pro 2.0 tokenizer. Bold marks the better value per row. The midtraining loss curve in Figure 3 further illustrates the adaptation process. Training loss decreases sharply over the first 1k steps ( 4B tokens) before plateauing, indicating that substantial token budget at relatively high learning rate is required to adapt the model to the new tokenizer beyond what the smaller, lower-LR SFT budget alone could provide. T-Wix SFT dataset D.1 General part of T-Wix The general part of the dataset consists of 468k diverse prompts collected from open-source data and high-quality translations of English-language datasets, subsequently deduplicated. The dataset is assembled to enhance the models capabilities in coding, mathematics, dialogue, and other competencies expected from modern LLM. D.1.1 Data Preparation First and foremost, corpus of 14M instructions (mostly in English) is compiled from various opensource datasets. To select the most useful samples, data filtering pipeline is developed. It consists of several consecutive stages aimed at deduplication and ensuring high thematic, qualitative, and complexity diversity of the SFT dataset. We also perform deduplication against benchmark datasets to ensure that no benchmark examples leak into the training corpus. 15 LSH and Embedding-Based Deduplication. At the initial stage, simple deduplication is performed using locality-sensitive hashing (LSH) and embedding-based similarity search to eliminate duplicated samples originating from different opensource datasets. Thematic Tag Filtering. To ensure thematic balance, the #INSTAG-based filtering approach (Lu et al., 2024) is employed. The pipeline uses trained tagging model to extract thematic tags from each instruction. For the present work, the tagger is trained using the Qwen2.5-7B (Qwen et al., 2025) model on multilingual data with context length of up to 32k tokens, allowing tagging in both Russian and English, including long-context data. This modification substantially reduces translation overhead, as tagging and filtering can be applied directly to multilingual raw data without prior translation into English. To improve thematic balance, an additional domain-balancing stageDomain & Complexity Balancingis included, as the tagger primarily produces low-level thematic annotations. Domain & Complexity Balancing. In addition to fine-grained thematic filtering, higher-level balance is introduced across major knowledge domains and difficulty levels within each domain. To achieve this, six domains Math, Code & Programming, Science, General Instruct, General Knowledge, Writing and three complexity levels School, Student, Professor are defined. Using large-scale LLM-assisted annotation, approximately 14M samples are automatically labeled with both domain and complexity tags. Subsequently, the dataset is balanced across domains and further normalized by difficulty within each domain to regulate the models output capabilities and ensure uniform skill distribution. This stage enables finer control over the resulting models generalization behavior, preventing overrepresentation of specific topics or difficulty levels. Reward Model Filtering. In the subsequent stage, samples are filtered according to prompt quality using scores from the Reward Model (RM) described in F. For each of the datasets comprising the 14M instructions, an RM score is computed, and the bottom 10% of samples with the lowest scores within each dataset are filtered out. This step effectively removes noisy or lowquality samples that could negatively impact downstream model performance, preserving only highquality and instructionally meaningful examples. Instruction Following Difficulty Filtering. further filtering stage based on Instruction Following Difficulty (IFD) scores is incorporated, following the approach introduced in (Li et al., 2024a). These scores quantify the difficulty language model faces in following given instruction. For the present work, IFD scores are computed relative to midtraining checkpoint to reflect the models actual instruction-following capability. Samples with excessively high IFD values (>1.0) are discarded as overly complex or ambiguous, while those with very low IFD scores (<0.7) are filtered out as trivially simple. This selective filtering makes it possible to retain the most challenging and instructionally rich examplesthose that contribute most to improving the models instruction-following abilitywhile removing both overly simple and excessively difficult samples. Multilingual Filtering and Translation. The multilingual setup enables filtering to be conducted directly on mixed-language raw data, reducing both the cost and time associated with preliminary translation. Only the final curated dataset is translated into Russian to ensure cross-lingual consistency. Rejection Sampling and Generation. High dataset quality is further ensured through the use of top LLMs and rejection sampling. Each final training completion is produced using DeepSeekV3 and Qwen-235B-A22B models, generating 8 candidate responses per instruction. These candidates are then filtered using RM scores to select the highest-quality outputs. This approach not only eliminates translation artifacts present in the raw multilingual data but also results in substantially higher-quality responses compared to the original samples, thereby improving the overall consistency and instructional value of the dataset. Overall, the combined multistage filtering pipeline ensures that the final SFT dataset is diverse, balanced, and composed of high-quality, instructionally valuable samples, free from data leakage and redundancy. This approach allows the training process to remain balanced across domains (e.g., code and 16 math) without bias toward any particular category. D.2 Long Context To enhance the models ability to process extended inputs, dedicated long-context dataset is constructed. diverse collection of long texts is selected from publicly available data for the pre-training phase, covering various domains such as education, technology, business, scientific literature, and fiction. The dataset is distributed across multiple context lengths ranging from 8k to 32k tokens, enabling the model to learn robustly across different input sizes. Using DeepSeek-V3 and Qwen-235B-A22B, variety of prompts and responses are generated for each text, encompassing summarization, openand closed-domain QA tasks, as well as reasoningoriented datasets. The resulting long-context dataset increment constitutes about 1% of the total SFT training data in samples and 7.7% in tokens, providing valuable coverage for instruction tuning under extended context conditions. D.3 Parallel Corpora To maintain strong English proficiency, parallel corpora are added to the SFT dataset that is, instructional samples presented in English alongside their Russian counterparts. series of experiments on the language share within the dataset shows that an optimal ratio is approximately 10% English data relative to the total SFT mix. D.4 Reasoning part of T-Wix To enhance reasoning capabilities in Large Language Models (LLMs) for the Russian language, high-quality, reasoning-focused dataset is constructed through targeted distillation pipeline. Rather than maximizing data volume, the pipeline prioritizes instructional value and appropriate task difficulty, ensuring that each retained sample provides meaningful learning potential for the target student model. The process starts from broad collection of English-language reasoning instructions, which are subsequently translated into Russian, deduplicated, and carefully balanced across domains to support diverse and robust reasoning behaviors. Initial Pool Generation and Deduplication. The initial pool of data is constructed from approximately 450k high-quality English-language reasoning instructions, drawn from established opensource datasets (e.g., Open-R1 (Hugging Face, 2025), Nvidia/AceReason-Math (Chen et al., 2025), Nvidia/Nemotron (Bercovich et al., 2025)), covering general knowledge, mathematics, natural sciences, and code generation. Domain Distribution: 60% general knowledge and open-ended reasoning (to establish fluent, structured reasoning in Russian), 10% verifiable mathematics (e.g., arithmetic, algebra), 10% open-ended mathematics (e.g., proofs, conceptual explanations), 15% natural sciences (physics, chemistry, biology), 5% code-related reasoning. After the initial collection, these Englishlanguage instructions are translated into Russian. As in the general part of T-Wix, deduplication is applied to eliminate near-duplicates and ensure sample uniqueness. Reward-Based Completion Evaluation. To mitigate the stochasticity inherent in LLM generation (Wang et al., 2025a; Atil et al., 2025), 8 diverse completions are generated per instruction by both: The teacher model (Qwen3-235B-A22B), The student model (midtraining checkpoint). This yields 16 completions per instruction, which are independently scored by trained reward model (RM). The inclusion of student generations enables direct assessment of the models current reasoning capability on each task, while the teacher generations provide high-quality reference behaviors. This multi-generation approach provides more robust statistical picture of the models performance on each instruction. Statistical Filtering Based on Reward Stability. Instructions exhibiting high variance in RM scores across generations are discarded, as they reflect ambiguous or unstable evaluation signals. Additionally, instructions for which the student model consistently receives very low RM scoreseven 17 with low varianceare excluded, as they lie beyond the students current learning capacity and are unlikely to support effective knowledge transfer (Xiong et al., 2025; Liu et al., 2025c). D.5 T-Wix dataset analytics The final distribution of data in the SFT dataset (T-Wix) are presented in Figure 4. The total size is 500k samples. Mean Reward-Based Selection Within the Zone of Proximal Development. To operationalize the pedagogical principle of the zone of proximal development (ZPD) (Cui and Sachan, 2025), the RM scores for the 8 teacher and 8 student completions per instruction are aggregated by computing their respective means. The average reward of the teacher responses and the average reward of the student responses are then used to estimate the reasoning gap. Samples are selected based on the absolute difference between these mean rewards. small difference indicates that the student already performs comparably to the teacher (suggesting limited learning potential), whereas very large difference implies that the task lies beyond the students capabilities (making distillation ineffective). Only samples with moderate gap in mean RM scoresneither too small nor too largeare retained. This ensures that the selected samples are challenging enough to drive improvement, yet sufficiently within reach for successful knowledge transfer. Final Completion Selection. For the final training targets, teacher-generated completions are selected as follows: For verifiable instructions (e.g., mathematical problems), factually incorrect completions are first filtered out; among the remaining correct ones, the completion with the highest RM score is chosen. For open-ended instructions, the shortest reasoning trace among the top-3 RM-ranked teacher completions is selected. This encourages the student to learn concise and non-redundant reasoning patterns (Sui et al., 2025). This pipeline yields high-quality reasoning dataset of approximately 30k samples, consisting of 90% Russian and 10% English instructions, consistent with the overall language strategy of T-Wix. By design, the dataset emphasizes stable, diverse, and pedagogically optimal reasoning traces in Russian across multiple domains, effectively balancing task difficulty with learning potential. Figure 4: Token distribution in T-Wix. Token counts were computed using tiktoken with o200k base tokenizer. D.6 SFT Training Recipe The SFT stage took 9 hours on 4 nodes with 8H100 GPUs for T-pro 2.0, using gradient checkpointing and FSDP, as well as packing samples into 32k-token contexts without truncation. After series of experiments, the optimal fine-tuning hyperparameters were selected, as described in Table 14. Hyperparameter Global batch size (samples) Max context length Number of training epochs Optimizer Adam betas Adam ϵ Learning rate Learning rate scheduler Warmup ratio Gradient clipping Precision Value 32 32k 2 Adam (Kingma and Ba, 2017) (0.9, 0.95) 1012 1e-6 cosine 0.1 max norm 2.0 BF16 Table 14: Hyperparameters used for SFT training."
        },
        {
            "title": "E Preference tuning",
            "content": "To enhance alignment beyond supervised finetuning, an on-policy Direct Preference Optimization (DPO) procedure is applied. Recent work shows that on-policy preference optimization offers more stable and reliable alignment gains than off-policy alternatives, as it learns directly from the models own generative distribution and therefore avoids distribution shift while targeting realistic error modes (Rafailov et al., 2024; Im and Li, 2025). 18 For each instruction, the SFT-trained model produces 16 completions. All candidates are scored using the RM described in App. F, and preference pairs are constructed by selecting the highestand lowest-scoring completions. This contrastive selection yields stable and informative training pairs by removing low-signal, ambiguous comparisons. The DPO dataset is constructed from filtered SFT data (T-Wix). total of 100k preference pairs is formed, consisting of: 90k sampled from the General SFT part, 10k sampled from the Reasoning SFT part. In addition, cross-subset augmentation is applied to enrich preference diversity: 4k samples from the General subset are paired with reasoning-style reformulations, while 6k samples from the Reasoning subset are converted into general-style instructions. This yields smoother distribution of reasoning complexity without altering the intended emphasis of each subset. The resulting on-policy DPO stage improves the models alignment, coherence, and reasoning structure while preserving broad general-purpose capabilities. E.1 Preference Training Recipe The DPO stage required 28 hours of training on 4 nodes with 8H100 GPUs. The training was carried out using sequence parallelism, which enabled efficient distribution of computation across devices. The hyperparameters listed in Table 15 were identified as optimal. Hyperparameter Global batch size (samples) Max context length Number of training epochs Optimizer Adam betas Adam ϵ Weight decay Learning rate Learning rate scheduler Warmup ratio Gradient clipping Precision Loss type DPO beta Value 128 32k 1 AdamW (0.9, 0.95) 1012 0.01 1e-7 cosine 0.05 max norm 2.0 BF16 DPO 0. Table 15: Hyperparameters used for preference tuning"
        },
        {
            "title": "F Reward Model",
            "content": "Tournament-Based Synthetic Preference Data Generation To construct high-quality reward model, it is essential to obtain reliable preference data pairs of model completions ranked according to their relative quality. Direct pairwise annotation across all available completions, however, is computationally expensive and inefficient. To address this, similar to the knockout-tournament method introduced by Liu et al. (2025b), we propose tournament-based preference generation approach that substantially reduces the number of required comparisons while preserving the informativeness of the resulting preference signal. Each tournament comprises participants, randomly sampled from the pool of available models. For each instruction, every model generates completion, and the tournament bracket is constructed according to model category for instance, smallscale models (7B13B) are paired against models of similar scale, and reasoning-oriented models compete within the same subclass. This grouping strategy ensures that comparisons are made between models of comparable generative quality, encouraging the reward model to learn finegrained distinctions rather than relying on trivial cases where one output is clearly superior (e.g., when large model is compared with small size model, comparison might yield an obvious outcome the larger model would consistently produce more coherent and contextually appropriate responses, leaving little room for the reward model to learn subtle differences). Each round of the tournament consists of single instruction and the corresponding completions generated by the competing models. An external LLM, not participating in the tournament, is employed as judge to determine the preferred completion for each matchup. To avoid positional bias, each pair of completions is evaluated in both possible orders, and samples exhibiting positional bias are excluded from the final training set. At the completion of each single-elimination tournament with participants, total of 2 log2 preference pairs are obtained. This result comes from the hierarchical structure of the tournament: in each round, half of the remaining participants compete, producing 2 new pairwise outcomes (both direct and transitive). Because tournament with participants requires log2 rounds to determine winner, the total number of inferred preference pairs accumulates to 2 log2 . Each round contributes the same number of new known preferences because every winners new victory also establishes transitive relationships over all opponents defeated in earlier rounds. For instance, if player beats player in the final, it is implied that outperform every player that previously defeated. Consequently, even though only n1 matches are directly played, the tree-like transitive structure allows many additional indirect comparisons to be inferred. This process produces preference set dense enough to capture comparative information among many participants, yet far more efficient than exhaustively comparing every possible pair (which would require n(n1) This tournament-based approach yields an informative preference dataset while significantly reducing annotation complexity. comparisons). 2 Reward Model Training The reward model is based on Qwen3-32B (Yang et al., 2025) with regression head to produce single preference score for each completion. Training follows the BradleyTerry (BRADLEY and TERRY, 1952) formulation, which models the probability of one completion being preferred over another as logistic function of their respective scores. All training is conducted with maximum sequence length of 32k tokens, leveraging Ulysses sequence parallelism (Jacobs et al., 2024) to efficiently support longcontext optimization. Data preprocessing, batching, and distributed training are managed through the TurboAlignment library (tur, 2025). Evaluation For intrinsic evaluation, we adapt RewardBench 2 (Malik et al., 2025) to Russian by translating the original benchmark and report standard leaderboard metrics. For downstream evaluation, we additionally construct Best-of-N selection benchmark on top of the Arena-Hard-RU instruction set to assess the reward model under realistic generation scenarios. In this setting, the base model produces candidate completions per instruction, and the reward model selects the highestscoring (best@N) and lowest-scoring (worst@N) outputs. These selections are then evaluated using Arena-Hard, allowing us to measure the alignment between reward-model rankings and externally validated quality. We further report the BoN metric (best@N worst@N) to quantify discriminative capacity. Although our model performs compara20 bly to existing open-source reward models on the translated RewardBench 2, it demonstrates clear advantage on our Best-of-N Arena-Hard benchmark. As shown in Table 16, our model obtains the highest BoN score, reflecting the strongest separation between highand low-quality completions. RM-model best@8 worst@8 Qwen3-32B-RM (Ours) Llama-3.3-Nemotron-70B-Reward-Multilingual1 Skywork-Reward-Gemma-2-27B2 Skywork-Reward-V2-Llama-3.1-8B3 Llama-3.1-Tulu-3-70B-SFT-RM-RB 92.69 (-0.99) 85.93 (-1.93) 89.05 (-1.6) 90.49 (-1.43) 87.37 (-1.86) 70.48 (+2.34) 84.91 (+1.85) 74.35 (+2.07) 77.31 (+1.77) 78.47 (+1.76) BoN 22.21 1.02 14.70 13.18 8.90 Table 16: Best-of-N (N = 8) evaluation on ArenaHard-RU. We report win rates for the highest- (best@8) and lowest-scoring (worst@8) completions selected by each reward model, and their difference BoN = best@8 worst@8, which measures discriminative capacity. 1Wang et al. (2025b), 2Liu et al. (2024), 3Liu et al. (2025a), 4Malik et al. (2025) Prompt selection Furthermore, in the process of synthetic data generation, we evaluated range of prompting strategies derived from the JudgeBench (Tan et al., 2025). Empirical analysis indicates that the Google Vertex prompt yields superior evaluation quality in different benchmarks (see Table 18), particularly on RewardBench 2 (RU). This improvement underscores the sensitivity of LLM-based evaluators to prompt design and highlights the importance of selecting domainappropriate judging configurations for reliable preference data generation. F.1 Reward Model Analysis experiments that DeepSeekreveal Our V3 (DeepSeek-AI et al., 2025b) demonstrates superior judgment capabilities in open-domain and conversational (chat) tasks, whereas Qwen3235B-A22B exhibits stronger performance in mathematical, code-related and other domains (see Table 17). Ablation on Transitive Samples. An ablation study was conducted to evaluate the contribution of transitive preference pairs. Removing transitive samples led to consistent degradation across all evaluation metrics (see Table 17), suggesting that inferred pairwise relationships enrich the preference signal and improve the models generalization to unseen instructions. Conversely, adding additional transitive samples beyond the first closure continued to yield marginal but positive improvements. Category Model Comparison with Existing RMs Qwen3-32B-RM (Ours) Skywork-Reward-V2-Llama-3.1-8B Skywork-Reward-Gemma-2-27B Llama-3.1-Tulu-3-70B-SFT-RM-RB2 Llama-3.3-Nemotron-70B-Reward-Multilingual Judge Model Ablation Qwen-3-RM-8B-DeepSeek-V3 Qwen-3-RM-8B-Qwen3-235B-A22B Transitive Samples Ablation Qwen3-8B-RM w/o transitive Qwen3-8B-RM w/ transitive RewardBench 2 (RU) Fact. Focus Math Prec. IF Safety Total 0.66 0.68 0.69 0.72 0.73 0.478 0.467 0.467 0.505 0.87 0.88 0.88 0.74 0.85 0.756 0.324 0.324 0. 0.62 0.65 0.64 0.69 0.62 0.598 0.688 0.688 0.704 0.42 0.45 0.40 0.41 0.41 0.341 0.350 0.350 0. 0.89 0.79 0.92 0.76 0.86 0.736 0.840 0.840 0.860 0.69 0.69 0.7 0.66 0.69 0.581 0.533 0.533 0. Table 17: Evaluation results on RewardBench 2 (RU). We compare our Qwen3-32B-RM against existing reward models (top), analyze the impact of different judge models for preference annotation (middle), and study the effect of tournament-derived transitive preference samples during training (bottom). Bold indicates best performance within each category. Prompt RewardBench 2 (RU) Fact. Focus Math Prec. IF Safety Total Skywork Arena Hard Google Vertex Prometheus 2 Chat-Eval 0.636 0.653 0.741 0.600 0.667 0.782 0.638 0.846 0.622 0.781 0.834 0.762 0.830 0.743 0. 0.394 0.349 0.549 0.432 0.478 0.881 0.899 0.915 0.790 0.831 0.706 0.660 0.776 0.637 0.710 Table 18: Assessing the role of prompt selection in RewardBench 2 (RU). Length Sensitivity and Distribution Effects. further observation concerns the length distribution between chosen and rejected completions. RewardBench 2 (RU) exhibits substantial drop in evaluation quality when the distribution becomes skewed specifically, when longer or shorter completions dominate. This imbalance appears to induce length-based bias in the reward model, leading it to systematically favor responses of particular size rather than quality. For instance, Qwen3-235B-A22B as judge displayed pronounced length bias, consistently preferring longer completions regardless of their semantic quality. This highlights the importance of maintaining balanced length distribution during preference data generation and tournament construction to prevent undesirable inductive shortcuts in the reward model."
        },
        {
            "title": "G Speculative Decoding Implementation",
            "content": "To mitigate the sequential latency of autoregressive generation, we integrate an EAGLE-based speculative decoding module (Li et al., 2024e) into T-pro 2.0. This setup employs lightweight draft model to propose candidate tokens in parallel, which are subsequently verified by the target model to ensure the output distribution remains identical to standard decoding (Leviathan et al., 2023). Architecture and Objective. Our draft model utilizes single decoder layer augmented with an FR-Spec component (Zhao et al., 2025), based on the Llama 2 architecture and implemented via SGLang (Gu et al., 2024). Unlike standard approaches that replicate the full target architecture, this model approximates essential hiddenstate dynamics. The training objective combines smoothed L1 loss (MAE and MSE) for hidden state reconstruction with KL divergence to align the draft token distribution with the target model. Data and Training Pipeline. We evaluated three data pipelines: offline labeling (I/O bound), chunked streaming (network bound), and online labeling. We adopted Online Labeling for the final setup. Although this increases HBM footprint by requiring the frozen target model to reside in memory, it yields the highest Tensor Core utilization. Training was performed on single node with 8H100 GPUs. The frozen verifier used Tensor Parallelism, while the EAGLE draft model utilized Distributed Data Parallelism. Full training hyperparameters are listed in Table 19. Deployment and Results. Deployed via SGLang using EAGLE-2s dynamic draft tree (Li et al., 2024d), the system achieves significant latency reductions. Table 20 highlights speedups up to 2.28 on reasoning tasks (T-Math) and consistent gains on ruMT-Bench. Table 21 illustrates domain-specific performance on ruMMLUPro, where Math and Engineering domains show the highest acceptance lengths (3.7) and speedups 21 Hyperparameter Value 8H100 (80GB) Hardware Verifier parallelism TP=2 Draft model parallelism DDP (world size=8) Batch size Learning rate Number of epochs Learning rate scheduler Warmup steps Weight decay Optimizer Data type TF32 32 3e-5 4 cosine 100 0.01 AdamW BF16 enabled Table 19: Hyperparameters used for EAGLE draft model training. (2.0). Future work will focus on draft model quantization and integrating EAGLE 3 (Li et al., 2025). Benchmark Temp. Mode Speedup Acceptance Length ruMT-Bench ruAlpaca ruCodeEval T-Math 0 0 0.8 0. 0 0 0.8 0.8 0 0 0.8 0.8 0 0.8 No Think Think No Think Think No Think Think No Think Think No Think Think No Think Think Think Think 2.05 1.86 1.79 1.69 1.78 1.77 1.61 1.57 2.26 2.07 2.15 1.84 2.28 2.25 3.55 3.37 3.31 3. 3.23 3.20 2.94 2.85 4.09 3.76 3.93 3.34 4.14 4.01 Table 20: Performance metrics for T-pro-2.0-eagle across different benchmarks, temperatures, and reasoning modes. Comparison of Speedup and Acceptance Length with and without Eagle. T-Math benchmark T-Math8 is Russian math reasoning benchmark constructed from high-school olympiad problems. It contains 331 tasks drawn from the All-Russian School Olympiad and the Moscow Olympiad in mathematics over the period 19982025. All items are single-answer problems with numeric gold solutions, which makes the benchmark suitable for automatic evaluation of long-chain mathematical reasoning. Problem statements and ground-truth answers are extracted from PDF collections using the 8https://huggingface.co/datasets/t-tech/ T-math Domain Speedup Accept. Length TPS w/o Eagle w/ Eagle Biology Business Computer Sci. Economics Engineering Health History Law Math Philosophy Physics Psychology Chemistry 1.68 2.00 1.89 1.72 2.00 1.67 1.52 1.51 2.06 1.62 1.96 1.65 2.04 3.00 3.63 3.37 3.07 3.60 2.98 2.72 2.69 3.70 2.88 3.50 2.85 3.66 108.22 107.83 107.99 108.26 106.96 108.29 108.15 108.03 107.88 108.37 107.60 108.38 107. 181.86 216.49 204.22 185.80 214.37 181.00 164.17 163.17 221.96 175.29 210.60 179.03 219.20 Table 21: Performance metrics for T-pro-2.0-eagle across ruMMLUPro domains (Temperature 0.8, Thinking mode, Batch size=1). Qwen2.5-VL-72B-Instruct (Bai et al., 2025) model. The raw pool is then filtered with an LLM-based checker to discard (i) tasks requiring multiple answers, (ii) problems without unique correct answer, (iii) theorem-style questions where the main goal is to prove statement, (iv) tasks whose solutions are non-numeric, and (v) items that cannot be solved without access to auxiliary figures. Next, medium-difficulty tasks on which Qwen3-8B achieves near-perfect pass@16 are removed to focus the benchmark on genuinely challenging instances. Finally, both the question texts and the verifiable answers are manually reviewed against the original olympiad sources. Evaluation uses standardized answer format (final answer wrapped in boxed{}) and the math_verify library9 to compare predicted and reference expressions. Table 24 reports pass@1 scores for several strong reasoning models. Although frontier systems such as o4-mini-high, DeepSeek-R1 and Gemini 2.5 Pro achieve competitive performance, the benchmark remains far from saturated, with none of the models exceeding 0.75 pass@1."
        },
        {
            "title": "I Additional Evaluations",
            "content": "As shown in Table 23, the model preserves strong English reasoning ability despite being primarily optimized for Russian. Within the 27B32B class, it remains closely aligned with the Qwen3-32B baseline: on MATH-500 it slightly improves accuracy, and on AIME 2024/2025 and GPQA the margins stay narrow. Performance is also competitive 9https://github.com/huggingface/Math-Verify 22 # 1 3 Problem statement (translated from Russian for readability)7 Combinatorics / logic. In tournament there are 20 players and 10 referees. After each game, the participants of that game take photograph together with the referee. After the tournament it turned out that for some people it is impossible to determine whether they are player or referee (based only on the set of photos they appear in). What is the maximum possible number of such people? Answer 2 Number theory / arithmetic constructions. Using any number of coins of denominations 1, 2, 5 and 10 roubles, together with (free) parentheses and the four arithmetic operations, construct an expression whose value is 2009, while spending as little money as possible. In the answer, write the minimum possible total value of the coins used (i.e., the minimum amount of money you need to spend). Geometry, olympiad level. In triangle ABC with side lengths AB = 3, BC = 4, CA = 5, we mark pairs of points on its sides: points C1 and C2 on side AB, points A1 and A2 on side BC, and points B1 and B2 on side CA. Inside triangle ABC there is point such that triangles A1A2, B1B2 and C1C2 are congruent and equilateral. Find the area of the convex hexagon with vertices A1, A2, B1, B2, C1, C2. If necessary, round your answer to two decimal places. 23 3.34 Table 22: Example problems from the T-Math benchmark. Statements are translated from the original Russian for readability; see the dataset for the original wording and full benchmark specification. Model AIME 2024 AIME 2025 MATH-500 GPQA Diamond LCB Open Source Models (27B-32B class) T-pro 2.0 (Ours) Qwen3-32B RuadaptQwen3-32B-Instruct Gemma 3 27B DeepSeek-R1-Distill-Qwen-32B 0.765 0.808 0.692 0.260 0.706 Open Source Larger Scale & Proprietary Models DeepSeek-V3 DeepSeek-R1 YandexGPT5-Pro GigaChat 2 Max o4-mini (medium) GPT-4o 0.52 0.914 0.117 0.110 0.800 0. 0.679 0.725 0.604 0.221 0.573 0.285 0.875 0.090 0.058 0.819 0.065 0.966 0.961 0.948 0.882 0.950 0.942 0.983 0.776 0.742 0.974 0.762 0.641 0.668 0.596 0.515 0.621 0.655 0.813 0.434 0.449 0.783 0. 0.556 0.546 0.489 0.246 0.572 0.405 0.770 0.272 0.272 0.757 0.246 Table 23: Comparison of models on English advanced reasoning benchmarks. Model pass@1 o4-mini-high DeepSeek-R1-0528 Gemini-2.5-Pro Claude Sonnet 4 T-pro 2.0 Qwen3-32B 0.73 0.71 0.70 0.56 0.54 0.53 Table 24: Pass@1 accuracy on the T-Math benchmark (331 problems). with reasoning-distilled systems such as DeepSeekR1-Distill-Qwen-32B, outperforming them on several metrics. Overall, these results indicate that the Cyrillic-focused tokenizer and our training pipeline do not meaningfully degrade English performance, maintaining robust cross-lingual generalization with minimal loss on advanced benchmarks."
        }
    ],
    "affiliations": [
        "T-Tech, Moscow, Russia"
    ]
}