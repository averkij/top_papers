{
    "paper_title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations",
    "authors": [
        "A. Bochkov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational \"meaning vectors.\" This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including a novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to \"representational interference\" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformer's compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research."
        },
        {
            "title": "Start",
            "content": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations A. Bochkov1 1Moscow Institute of Physics and Technology (MIPT), Moscow, Russia Corresponding author: A. Bochkov (e-mail: andrey.bochkov@gmail.com). Abstract Understanding the locus of semantic representation in large language models (LLMs) is crucial for interpretability and architectural innovation. The dominant paradigm posits that trainable input embeddings serve as foundational \"meaning vectors.\" This paper challenges that view. We construct Transformer models where the embedding layer is entirely frozen, with vectors derived not from data, but from the visual structure of Unicode glyphs. These non-semantic, precomputed visual embeddings are fixed throughout training. Our method is compatible with any tokenizer, including novel Unicode-centric tokenizer we introduce to ensure universal text coverage. Despite the absence of trainable, semantically initialized embeddings, our models converge, generate coherent text, and, critically, outperform architecturally identical models with trainable embeddings on the MMLU reasoning benchmark. We attribute this to \"representational interference\" in conventional models, where the embedding layer is burdened with learning both structural and semantic features. Our results indicate that high-level semantics are not inherent to input embeddings but are an emergent property of the Transformers compositional architecture and data scale. This reframes the role of embeddings from meaning containers to structural primitives. We release all code and models to foster further research. INDEX TERMS emergent semantics, frozen embeddings, interpretability, language models, multilingual NLP, transformers, visual representations, Unicode 5 2 0 2 7 ] . [ 1 6 8 8 4 0 . 7 0 5 2 : r"
        },
        {
            "title": "INTRODUCTION",
            "content": "Understanding where and how semantic abstractions arise in transformer language models is both theoretical and practical concern for the development of scalable, robust, and interpretable AI systems. Traditionally, input token embeddingslearned representations from large corporaare viewed as the primary locus of meaning: their algebraic properties have been cited as evidence of encodable semantic relationships (king - man + woman = queen), and advancing architectures routinely attribute model improvements to smarter or larger embedding matrices. This paradigm has shaped assumptions about what is required for intelligent representation learning. The robustness of modern Large Language Models (LLMs) to orthographic variations, such as in can wRiTe, highlights fundamental question. The models understanding is not derived from single, semantically rich token for \"write\". Instead, it often relies on composing meaning from sequence of semantically poor characteror subword-level tokens (e.g., w, R, i, T, e). If the input vectors for these atomic units lack inherent semantic content, where does the models understanding originate? This paper argues and empirically demonstrates that semantics are not property of input embeddings but an emergent phenomenon of the Transformer architecture itself. However, recent advances in byte and characterlevel modeling, as well as modular and multi-lingual architectures, suggest that the relationship between embeddings and high-level meaning is far subtler. In pursuit of deeper understanding, we investigate the extreme case: Transformer language models where the input embedding layer is never trained and, in fact, deliberately devoid of semantic optimization. Instead, we fix this layer to deterministic mapping based on visual Unicode glyph representations and token-level n-gram image composites. Our experimental framework draws embeddings not from data-driven optimization, but from precomputed character images spanning the entire Unicode range. For multicharacter tokens (as in byte-pair and unigram models), we employ compositional image techniques to generate fixed-length visual feature vector per token, achieving universal text coverage. The tokenizer is similarly Unicode-centric, balancing character-level granularity with multi-script n-gram tokens to maximize overlap across languages. Instead of pursuing state-of-the-art benchmark scores, which often depend on massive scale, our primary goal is to isolate and test fundamental hypothesis: can language abstraction emerge without trainable, semantically-rich input embeddings? We deliberately constrain model and dataset sizes to create controlled environment for comparing the learning dynamics of frozenversus trainableembedding models. Our focus is on the conditions under which meaning emerges, not on outperforming large-scale systems. Critically, we observe that despite this radical departure from conventional practice, these frozenembedding models not only converge but display robust convergence and, surprisingly, outperform architecturally identical counterparts with trainable embeddings on reasoning benchmarks like MMLU, suggesting representational interference in conventional approaches. Emergent abstraction appears within the transformer blocks themselves, not in the input vectors. This has profound implications: it implies that semantic structure is an architectural phenomenon, not an initialization artifact. Our main contributions are: demonstrate that semantic understanding in Transformers can emerge without trainable input embeddings introduce universal visual embedding scheme compatible with any tokenizer identify \"representational interference\" as potential limitation of trainable embeddings release open-source implementations enabling reproducibility The rest of this paper is organized as follows: Section II reviews related work; Section III details our method for constructing visual embeddings and Unicode tokenization; Section IV describes the experimental setup; Section presents results and analysis; Section VI discusses implications for future architectures; and Section VII concludes. Code and all artifacts are released to foster further progress in both science and industry1."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Semantic representation in neural language models has received extensive attention. Early word2vec [1] and GloVe [2] models established the conceptual centrality of meaningful dense vector spaces, with well-documented algebraic properties in vector arithmetic. Subsequent transformer-based LMs [3] extended this paradigm to context-dependent embeddings, further entrenching the notion that trainable input vectors are the source of semantic capability. However, several lines of research challenge this view. Character - and byte-level models, such as ByT5 [4] and CANINE [5], forego word-level tokenization for universal coverage; yet, even here, embed1Repository: https://github.com/AVBochkov/ Embeddings (https://huggingface.co/Bochkov) ding matrices are trainable and morph into semantic vector spaces during pretraining. Cross-lingual and modular approaches pursue shared embedding spaces for transfer [6, 7], but rely on substantial alignment learning. Mixture-of-experts [8, 9] and adapter fusion [10] aim to modularize reasoning, yet remain dependent on trainable embeddings. Our input embedding matrix is never trained and strictly visual/surface oriented. More radically, recent work in visual-language models [11] investigates multimodal representations by grounding language in visual contexts. Models like CLIP [12] or Flamingo [13] learn joint embeddings for text and images. However, these approaches typically combine separate, powerful encoders for each modality and aim to align two rich semantic streams, rather than enforcing nonsemantic, frozen vector space for language as we propose. Our work differs in two key aspects: We forego data-driven embedding learning altogether, initializing the input layer with fixed, non-semantic, visual features derived from Unicode glyphs. Our primary contribution is demonstrating that high-level abstraction emerges even when the input layer is restricted to fixed, non-semantic, visual features, isolating the compositional layers as the locus of meaning. ply Principal Component Analysis (PCA). PCA model is fit on the raw vectors of the entire vocabulary, and the top d_model principal components are used to transform each raw vector into d_modeldimensional embedding. This preserves the maximum variance of the visual features within the target dimensionality (floating points components). PCA was chosen for its determinism, computational efficiency, and ability to capture dominant visual variance in fixed-dimensional space without introducing trainable parameters. This aligns with our core hypothesis of investigating emergent semantics absent of any data-driven feature learning at the input layer. While learned visual encoder like CNN could potentially extract richer features, it would re-introduce trainable parameters, confounding our experimental goal of isolating the Transformers compositional layers. Normalization and Freezing: The resulting embedding vectors are L2-normalized. The final matrix is loaded into the Transformers embedding layer, and its weights are frozen (i.e., requires_grad=False) for all subsequent training stages. The approach supports tokenizers from arbitrary LMs (e.g., Mistral Nemo tokens text used in bvv241nemo tokenizer), as new tokens are mapped via their textual form."
        },
        {
            "title": "3 METHOD",
            "content": "3.1. FROZEN VISUAL UNICODE EMBEDDINGS Our method generates fixed embedding matrix of shape (V, d_model), where is the vocabulary size and d_model is the models hidden dimension. The vector for each token is derived through deterministic, multi-step process: Glyph Rendering: Each token in the vocabulary is rendered into bitmap. Single-character tokens are rendered directly. Multi-character n-gram tokens are rendered by horizontally concatenating the glyphs of their constituent characters. For example, the token \"ing\" is rendered as single image containing the sequence of i, n, glyphs. We use standardized font (e.g., unifont-14.0.01) at fixed-point size. Image Standardization: The resulting variablewidth bitmaps are resized to fixed square resolution (e.g., 32x32 for d_model=1024) using bilinear interpolation. This creates standardized visual representation for every token. Vectorization: Each standardized HxH bitmap is flattened into H²-dimensional raw vector (binary components). Projection: To project the high-dimensional image vector into the models latent space, we ap3.2. UNICODE-CENTRIC TOKENIZER CONSTRUCTION To test our hypothesis across diverse languages and existing token schemes, we required method to generate visual embeddings for any given vocabulary. We developed Unicode-centric tokenization framework, bvv241, to serve this purpose. The core approach is based on bijective mapping of Unicode codepoints to individual tokens, whenever possible, ensuring 1:1 relationship between characters and tokens across wide range of scripts. Reserved, surrogate, and private Unicode ranges are systematically exploited to store custom and multicharacter tokens, allowing for efficient representation of commonly occurring substrings and entire wordsparticularly beneficial for high-frequency terms shared across models and languages. The tokenizers hybrid design draws from frequent multi-character patterns identified in topperforming, state-of-the-art models: approximately 19,000 such tokens are incorporated. This structure enables robust handling of diverse languages and scripts, mitigating the fragmentation common in byte-pair encoding (BPE) and unigram tokenizers especially for underrepresented scripts and codemixed text. The bvv241-nemo variant is provided as demonstration of the flexibility of the proposed methodol-"
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "4.1. TRAINING DATASETS The training corpus comprises multilingual mix including subsets of Wikipedia (en, ru, zh), and SFT datasets (10% insertion in pretrain) SQuADv2, TriviaQA, HotpotQA, NQ-Open, BoolQ, CommonsenseQA, and ARC, totalling approximately 9B tokens. Wikipedia data constitutes roughly 5% of its total size. Model Name Parameters and Vocab Size Training Corpus and Embedding Type pro_bvv_en pro_bvv_unfrozen pro_bvv_ru pro_bvv_zh best_bvv_ru best_bvv_unfrozen_ru best_bvv_zh best_bvv_unfrozen_zh max_bvv_ru max_bvv_zh nemo_bvv_ru nemo_bvv_zh 0.2 B, 65536 0.2 B, 65536 0.2 B, 65536 0.2 B, 65536 0.5 B, 131072 0.5 B, 131072 0.5 B, 131072 0.5 B, 131072 0.4 B, 131072 0.4 B, 131072 0.4 B, 131072 0.4 B, English, Frozen English, Trainable EN+Russian, Frozen EN+Chinese, Frozen EN+Russian, Frozen EN+Russian, Trainable EN+Chinese, Frozen EN+Chinese, Trainable EN+Russian, Frozen EN+Chinese, Frozen EN+Russian, Frozen EN+Chinese, Frozen Table 1: Model Parameters and Vocabulary Statistics. Overview of models used in experiments. Frozen models use our fixed visual embeddings. Trainable models are baselines with standard, randomly initialized, trainable embeddings. All other architectural parameters (d_model=1024, layers, etc.) are held constant within each size class (0.2B, 0.4B, 0.5B) between frozen and trainable embeddings mode. The prefixes pro, max, and best correspond to models with 0.2B, 0.4B, and 0.5B parameters, respectively. 4.2. BASELINES AND COMPARISONS Baselines include classic transformer LMs with trainable embedding layers matched for architecture, parameter count, and tokenizer coverage. Our primary comparisons are between models with Frozen embeddings and their Trainable counterparts (e.g., best_bvv_ru vs. best_bvv_unfrozen_ru), which share identical architecture and training data. 4.3. EVALUATION We report standard language model metrics: Figure 1: Average characters per token. Lower values indicate lower compression efficiency. Our bvv241 variants prioritize character-level granularity. ogy. While the underlying Unicode-centric mapping scheme is applicable to any tokenizer, bvv241-nemo specifically replicates the vocabulary, token structure, and exact token enumeration of the SOTA Mistral Nemo tokenizer, using its public vocabulary files as template. This showcases how the approach can serve as drop-in replacement or augmentation for existing BPE or unigram schemes. Furthermore, since all token indices and their corresponding Unicode/codepoint representations are deterministic and exhaustively specified, this methodology enables the direct creation of preinitialized embedding matrix of shape vocab_size n_embed. Such matrix can be either used as fixed (non-trainable) embedding or as highquality cold start, reducing the need for extensive embedding pre-training. Combining strict Unicode mapping with multi-character tokens, the bvv241 family achieves highly stable performance for Asian, Arabic, and underrepresented scriptswhere differences from SOTA are minimal. For Indo-European (Latin and partially Cyrillic) languages, compression is less aggressive, but the model gains in predictability, recoverability, and linguistic inclusivity. 3.3. MODEL ARCHITECTURE Train and validation loss/perplexity, We employ standard decoder-only Transformer architecture, similar to GPT-2. All models use hidden dimension d_model=1024, multi-head attention, and GELU activations. The key modification is that the token embedding layer (nn.Embedding) has its weights frozen to the precomputed visual matrix. For comparison, we train baseline models with identical architectures but with standard, randomly initialized and trainable embedding layer. MMLU, ARC and CommonsenseQA. All metrics are computed on held-out test sets. 4.4. IMPLEMENTATION DETAILS Training was performed on H100 80 Gb with batch sizes up to 16 and block sizes of 1024. Gradient accumulation and mixed precision were used for efficiency."
        },
        {
            "title": "5 RESULTS",
            "content": "Before presenting our results, it is crucial to frame their context. This study does not aim to achieve state-of-the-art performance on benchmarks like MMLU. Our models are intentionally constrained in scale (up to 0.5B parameters) and trained on modest 9B token dataset. The primary objective is to demonstrate the feasibility of learning with nonsemantic embeddings and to compare the learning dynamics between our proposed frozen-embedding models and traditional trainable-embedding baselines under identical conditions. The following metrics should therefore be interpreted as evidence of emergent abstraction, not as challenge to largescale models. 5.1. MODEL CONVERGENCE primary concern was whether model with nonsemantic, frozen embeddings could converge at all. Figure 2 shows the training loss curves for 0.5B parameter frozen-embedding model (best_bvv_ru) and Figure 3 shows its trainable-embedding counterpart (best_bvv_unfrozen_ru). Both models exhibit stable convergence profiles, with nearly identical loss trajectories. This demonstrates that the Transformer architecture is capable of learning effectively even when the input layer provides only fixed structural information. Figure 3: Learning curves (best_bvv_unfrozen_ru) for trainable embedding. Figure 4 and Figure 5, while our frozen-embedding models demonstrate robust convergence across all tasks, they show striking and consistent performance advantage on the MMLU reasoning benchmark compared to their architecturally identical counterparts with trainable embeddings. Figure 2: Learning curves (best_bvv_ru) for frozen embedding. 5.2. PERFORMANCE OF FROZEN VS. TRAINABLE EMBEDDING MODELS Our primary finding challenges the conventional wisdom that trainable, semantically-rich embeddings are necessary for high-level reasoning. As shown in Figure 4: Performance Comparison: MMLU sub tasks with score greater than 27%. For instance, best_bvv_ru (0.5B params, frozen embeddings) achieves an MMLU score of 22.29, whereas best_bvv_unfrozen_ru (0.5B params, trainable embeddings) scores only 11.37 nearly 2x performance gap. This pattern holds across different model sizes and languages. We hypothesize this phenomenon, which we term \"representational interference,\" stems from the dual burden placed on trainable embeddings. trainable embedding layer must simultaneously learn low-level orthographic features (e.g., character shapes, token boundaries) and high-level semantic abstractions within the same parameter space. This creates conflict during optimization, leading to suboptimal representation that is neither perfect structural encoder nor pure semantic vector. In contrast, our frozen visual embeddings pro5 Figure 6: t-SNE visualization of input token embeddings. In the trainable Mistral Nemo embeddings, semantic groups (e.g., numbers, professions, animals) tend to lie closer to one another, yet, globally, the point cloud remains mostly uniform. Figure 5: Performance Comparison: Frozen vs. Trainable Embedding Models. vide stable, information-rich structural foundation. By offloading the task of representing token form to fixed, deterministic layer, we free the Transformers attention and feed-forward layers to focus exclusively on their core competency: composing these structural primitives into high-level meaning. The model does not waste capacity learning what token looks like; it only learns what to do with it. This hypothesis is further supported by the tSNE visualizations of the embedding spaces below. 5.3. EMERGENT SEMANTICS VISUALIZATION OF INPUT TOKEN EMBEDDINGS Our fixed visual embedding vectors naturally cluster by surface/formal features (e.g., length), rather than by meaning (Figure 7). In contrast, trainable embeddings demonstrate soft semantic grouping, though the distributed latent space remains globally unstructured (Figure 6). This illustrates that any emergence of true semantic structure must occur entirely within the networks compositional layers. Figure 7: t-SNE visualization of input token embeddings. In our frozen, visual Unicode embeddings (bvv241-nemo), clusters are sharp and distinct, but reflect only surface or formal properties (such as token length), with no alignment to the semantic categories. This highlights that emergent meaning in our models must arise above the embedding layer."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Our findings suggest that the role of input embeddings in Transformers may be widely misunderstood. Meaning appears not to be property of the embeddings, but process enacted by the architecture. 6.2."
        },
        {
            "title": "IMPLICATIONS FOR\nARCHITECTURE AND\nEFFICIENCY",
            "content": "This paradigm opens several avenues for future architectures: 6.1. RE-EVALUATING EMBEDDINGS:"
        },
        {
            "title": "FROM MEANING CONTAINERS\nTO STRUCTURAL PRIMITIVES",
            "content": "The success of our frozen-embedding models, particularly on MMLU, challenges the view of embeddings as \"meaning vectors.\" Instead, they can be seen as \"structural primitives.\" The \"representational interference\" hypothesis suggests fundamental inefficiency in standard LLMs. By tasking the embedding layer with both structural and semantic learning, we may be creating an optimization bottleneck that harms downstream reasoning. Forcing the model to learn, for instance, that the token \"king\" is semantically close to \"queen\" but structurally dissimilar from \"monarch\" within the same vector space is difficult, potentially conflicting task. Our method resolves this by offloading the structural representation to fixed, deterministic layer. The model doesnt waste capacity learning what token looks like; it only learns what to do with its form in context. This may be particularly advantageous for tasks requiring orthographic or morphological awareness, which could be implicitly tested in MMLUs diverse sub-tasks. By contrast, our frozen visual embeddings resolve this conflict. They provide stable, informationrich, and purely structural foundation. The model does not expend capacity learning what token looks like; it only learns what to do with it. This frees the Transformers attention and MLP layers to focus exclusively on their core competency: composing these structural primitives into high-level meaning. This suggests hidden inefficiency in standard models. Consider task requiring orthographic reasoning (e.g., \"How many es in strawberries ?\"). If \"strawberries\" is single token, its semantic embedding contains no direct information about its character composition. The model must perform costly \"internal detokenization,\" using its deep layers to implicitly reconstruct the words structure from memory. Our approach mitigates this by providing structural information at the input layer, potentially making the model more computationally efficient for class of reasoning tasks. The performance gap on MMLU suggests that decoupling structural representation from semantic composition is more effective learning strategy, especially under constrained data and compute. Modular and Standardized Embeddings: universal, algorithmically-derived visual embedding matrix could become standard component, allowing researchers to swap out different Transformer backbones for direct comparison without confounding factors from different embedding initializations. Efficient MoE and Model Fusion: Mixtureof-Experts (MoE) models could share common frozen embedding layer, reducing parameter count and simplifying routing logic. Improved Cross-Lingual Transfer: single Unicode-based visual embedding space is inherently multilingual. This could provide more robust foundation for zero-shot crosslingual transfer, as the model learns to operate on visual script features common to many languages. 6.3. LIMITATIONS AND FUTURE WORK This study was intentionally constrained in scale. Future work should test whether these findings hold for models at the 10B+ parameter scale. Additionally, while our method covers the entire Unicode text space, its generalization to truly novel modalities (e.g., mathematical formulas, chemical structures) remains an open question."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We have demonstrated that transformer LMs with frozen, purely visual Unicode-based embeddings can learn rich linguistic and logical abstraction. Semantics in neural LMs is not property of the embedding matrix, but an emergent phenomenon of the models layered composition and self-attention. This insight opens new avenues for model standardization, modularity, and efficient scaling in multi-lingual and multi-domain NLP."
        },
        {
            "title": "References",
            "content": "[1] T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space, 2013. [2] J. Pennington, R. Socher, and C. D. Manning, Glove: Global vectors for word representation, 7 [12] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in Proceedings of the 38th International Conference on Machine Learning (ICML), 2021, pp. 8748 8763. [13] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, N. de Freitas, O. Vinyals, and A. Zisserman, Flamingo: visual language model for few-shot learning, 2022. [14] Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu, H. Tian, and H. Wu, ERNIE: Enhanced representation through knowledge integration, in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), 2019, pp. 14411451. [15] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, ALBERT: lite BERT for self-supervised learning of language representations, in International Conference on Learning Representations (ICLR), 2019. in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 15321543. [3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, Language models are unsupervised multitask learners, OpenAI, OpenAI Blog Post 1, 2019. [4] L. Xue, A. Barua, N. Constant, R. Al-Rfou, S. Narang, M. Kale, A. Roberts, and C. Raffel, ByT5: Towards token-free future with pretrained byte-to-byte models, in Transactions of the Association for Computational Linguistics, vol. 10, 2022, pp. 291306. [5] J. H. Clark, D. Garrette, I. Turc, and J. Wieting, CANINE: Pre-training an efficient tokenization-free encoder for language representation, in Transactions of the Association for Computational Linguistics, vol. 10, 2022, pp. 726745. [6] A. Conneau, G. Lample, M. Ranzato, L. Denoyer, and H. Jégou, Word translation without parallel data, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018. [7] G. Lample and A. Conneau, Cross-lingual language model pretraining, in Advances in Neural Information Processing Systems (NeurIPS), vol. 32, 2019. [8] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, Outrageously large neural networks: The sparselygated mixture-of-experts layer, in International Conference on Learning Representations (ICLR), 2017. [9] W. Fedus, B. Zoph, and N. Shazeer, Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, in Journal of Machine Learning Research, vol. 23, no. 120, 2022, pp. 139. [10] J. Pfeiffer, A. Kamath, A. Rücklé, K. Cho, and I. Gurevych, AdapterFusion: Non-destructive task composition for transfer learning, in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, 2020, pp. 36373650. [11] H. Tan and M. Bansal, Vokenization: Improving language understanding with contextualized, visual-grounded supervision, in Findings of the Association for Computational Linguistics: EMNLP 2022, 2022, pp. 41414156."
        }
    ],
    "affiliations": [
        "Moscow Institute of Physics and Technology (MIPT), Moscow, Russia"
    ]
}