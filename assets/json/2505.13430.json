{
    "paper_title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization",
    "authors": [
        "Sifeng Shang",
        "Jiayi Zhou",
        "Chenyu Lin",
        "Minxian Li",
        "Kaiyang Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As the size of large language models grows exponentially, GPU memory has become a bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within a unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous quantization scale for gradient estimation and uses a directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within a single 24GB GPU."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 0 3 4 3 1 . 5 0 5 2 : r Fine-tuning Quantized Neural Networks with Zeroth-order Optimization Sifeng Shang1 Jiayi Zhou1 Chenyu Lin1 Minxian Li2 Kaiyang Zhou1,(cid:0) 1Hong Kong Baptist University 2Nanjing University of Science and Technology https://github.com/maifoundations/QZO"
        },
        {
            "title": "Abstract",
            "content": "As the size of large language models grows exponentially, GPU memory has become bottleneck for adapting these models to downstream tasks. In this paper, we aim to push the limits of memory-efficient training by minimizing memory usage on model weights, gradients, and optimizer states, within unified framework. Our idea is to eliminate both gradients and optimizer states using zeroth-order optimization, which approximates gradients by perturbing weights during forward passes to identify gradient directions. To minimize memory usage on weights, we employ model quantization, e.g., converting from bfloat16 to int4. However, directly applying zeroth-order optimization to quantized weights is infeasible due to the precision gap between discrete weights and continuous gradients, which would otherwise require de-quantization and re-quantization. To overcome this challenge, we propose Quantized Zeroth-order Optimization (QZO), novel approach that perturbs the continuous quantization scale for gradient estimation and uses directional derivative clipping method to stabilize training. QZO is orthogonal to both scalar-based and codebook-based post-training quantization methods. Compared to full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by more than 18 for 4-bit LLMs, and enables fine-tuning Llama-2-13B and Stable Diffusion 3.5 Large within single 24GB GPU."
        },
        {
            "title": "Introduction",
            "content": "Pre-trained large language models (LLMs) [1, 2, 25, 26] have demonstrated great potential in numerous downstream applications, ranging from sentiment classification and text summarization, to more challenging open-ended question answering and creative writing. However, with the model size growing at an exponential rate, adapting LLMs to downstream tasks presents significant challenges to computational resources. For instance, fine-tuning Llama-7B model stored in bfloat16 typically requires 56GB GPU memory: 14GB for model weights, 14GB for gradients, and another 28GB for optimizer states when adaptive gradient-based optimization methods are used (e.g., the first and second moments in AdamW [8], which cost twice the size of gradients). Such an enormous memory cost makes it infeasible for researchers and practitioners with limited computational resources to fine-tune LLMs. In general, there are four key components that determine memory usage: (1) model weights, (2) gradients (typically the same size as weights), (3) optimizer states (often twice the size as gradients), and (4) activations cached for gradient computation. Since activations are mostly affected by the size of mini-batch, existing memory-efficient training methods mainly target the first three components [10, 6, 9]. In this work, we aim to push the limits of memory-efficient training by (cid:0) Corresponding author Preprint. Under review. minimizing memory usage on model weights, gradients, and optimizer states, within unified framework. Our main idea is to eliminate gradients and optimizer states using zeroth-order optimization [21], which gets rid of backpropagation by approximating gradients solely through forward passes (i.e., perturbing model weights to identify gradient directions). When it comes to model weights, the optimal approach is to quantize the weights, e.g., converting from bfloat16 to int4 can significantly cut the memory cost by 4. However, directly applying zeroth-order optimization to quantized weights is non-trivial because (1) quantized weights cannot be perturbed in the continuous space, and (2) the gradients estimated by zeroth-order optimizer are continuous and therefore cannot be used to update discrete quantized weights (which would otherwise require de-quantization and re-quantization). Figure 1: Memory profiling on SST-2 [27] with (per-device) batch size set to 1. Fine-tuning is done with fully-sharded data parallel. To overcome the aforementioned challenges, we propose novel approach called Quantized Zerothorder Optimization (QZO), which enables quantized neural networks to be fine-tuned with zerothorder optimization, hence achieving maximum reduction in memory consumptioncompared to full-parameter fine-tuning in bfloat16, QZO significantly reduces the total memory cost by 18 for 4-bit LLMs (see Figure 1). Specifically, QZO approximates the gradients of quantized weights by perturbing the continuous quantization scale parameter(s) rather than the discrete weights, which are kept fixed throughout training. To further stabilize training, we propose gradient clipping method and provide theoretical proof to justify that the clipping method essentially reduces the variance of the gradient estimate. We evaluate QZO on different families of LLMs including OPT [1] and Llama [25, 26], as well as using diverse set of quantization methods. The experiments are conducted on five popular NLP benchmarks including both classification and generation tasks. Using 4-bit LLMs, QZO significantly outperforms both quantized and un-quantized zero-shot models while performing on par with MeZO [9], which applies zeroth-order optimization to un-quantized models. In the extreme quantization case where the model is quantized to 2-bit, QZO still beats the zero-shot baseline by large margin, demonstrating the effectiveness of QZO in fine-tuning quantized models. In addition to LLMs, we also evaluate QZO on fine-tuning state-of-the-art text-to-image model, i.e., Stable Diffusion 3.5 Large, which requires more than 86.43GB of memory for regular fine-tuning. QZO successfully fine-tunes Stable Diffusion 3.5 Large on stylized images, using only 12.4GB of memorywhich can even fit into consumer-grade GPU like Nvidia RTX 4090."
        },
        {
            "title": "2 Related Work",
            "content": "Memory-Efficient Training Fine-tuning LLMs often requires significant amount of GPU memory, making it challenging for model adaptation on resource-constrained hardware. In general, current memory-efficient training methods mainly focus on reducing GPU memory usage for the following components: (1) learnable model weights, (2) gradients, (3) optimizer states storing additional gradient information, and (4) activations cached for gradient computation. The seminal work known as LoRA [10] cuts the number of learnable parameters by learning low-rank matrices while keeping the original model weights frozen, and as result, the memory usage for gradients and optimizer states 1By quantization, we refer to post-training quantization throughout this work, unless specified otherwise. 2 is simultaneously reduced as well. To save memory cost for optimizer states, GaLore [6] projects the first and second moments of gradients in AdamW [8] onto low-rank subspace. MeZO [9] eliminates gradients and optimizer states by using zeroth-order optimizer [21], which estimates gradients using only forward passes and therefore keeps the memory cost the same as inference. CoLM [11] uses small mini-batches whose gradients match those of large mini-batches, leading to huge memory reduction in activations. Our approach further pushes the limits of memory-efficient training by fine-tuning quantized LLMs with zeroth-order optimization, which significantly cuts memory usage across all components requiring GPU memory. LLM Quantization Post-training quantization (PTQ) is popular paradigm for compressing LLMs. Most PTQ methods [14, 12, 13, 15, 16] reduce the bit width for each model parameter by representing the numerical range with low-precision integers while using full precision for quantization parameters. These methods can achieve up to 4-bit quantization, resulting in up to 4 reduction in memory usage compared to the widely-used BF16 representation. Different from the popular scalar-based quantization paradigm, recent research [17, 18, 19] has explored using codebooks for storing fullprecision numbers, which are indexed with integers to represent the original model weights. These codebook-based methods can achieve extreme quantization in 2 or 3 bits without observing significant performance drops. Typically, quantized LLMs are not suitable for fine-tuning because continuous gradients cannot be directly applied to updating discrete quantized weights (which would require de-quantization and re-quantization). Our approach seamlessly combines memory-efficient training with quantization to enable fine-tuning on quantized LLMs, achieving maximal reduction on GPU memory usage. More importantly, our approach is orthogonal to most PTQ methods, including both 4-bit and 2-bit quantization methods."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Background: Zeroth-order Optimization Zeroth-order optimization (ZO) methods are often used in cases where gradients and higher-order derivatives of the objective cannot be directly computed or are unreliable [20]. The pioneering work, Simultaneous Perturbation Stochastic Approximation (SPSA) [21], is defined as follows, Definition 3.1 (Simultaneous Perturbation Stochastic Approximation, SPSA [21]). Given model parameterized by θ Rd and loss function L, SPSA estimates the gradients of θ on mini-batch using the following formula: ˆθL(θ; B) = L(θ + ϵz; B) L(θ ϵz; B) 2ϵ zzθL(θ; B), (1) where Rd is random vector sampled from (0, d), and ϵ the perturbation scale. Built on top of SPSA, recent work [9] proposed memory-efficient zeroth-order optimization (MeZO) for LLMs. In particular, MeZO uses random seeds as trick to eliminate the storage cost of z, and as result, the memory footprint is kept the same level as inference. MeZO also replaces the regular SGD [22] with zeroth-order stochastic gradient descent (ZO-SGD), which is defined below: Definition 3.2 (Zeroth-Order Stochastic Gradient Descent, ZO-SGD [9]). Given learning rate η, ZO-SGD updates the parameters θt at t-th step using gradients estimated by SPSA as follows: θt+1 = θt η ˆθtL(θt; Bt) (2) where Bt denotes the input mini-batch at step t. 3.2 QZO: Quantized Zeroth-order Optimization QZO minimizes the memory usage not only on gradients and optimizer states but also on model weightsthis can save huge memory cost when using large models of more than 10B parameters, e.g., when using bfloat16, 10B models weights consume 20GB of memory, while using int4, the weights only take 5GB of memory. QZO consists of two core modules: Quantized Simultaneous Perturbation Stochastic Approximation (Q-SPSA), and directional derivative clipping. The former extends SPSA to quantized weights while the latter stabilizes training by reducing the variance of gradient estimation. 3 3.2.1 From SPSA to Q-SPSA SPSA (Eq. 1 cannot be directly applied to quantized weights because (1) quantized weights are discrete and therefore cannot be perturbed in the continuous space, and (2) the continuous gradients cannot be used to update discrete weights, which would otherwise require de-quantization and re-quantization. To overcome these challenges, we propose Quantized Simultaneous Perturbation Stochastic Approximation (Q-SPSA), which only applies perturbation to the continuous quantization scale. We begin by introducing quantization and de-quantization, which are two essential steps in model quantization. Concretely, for each single element in weight set W, these two steps can be formulated as = , (3) = w, (4) where denotes an element-wise quantization scale, and the quantized counterpart stored using lower bits. The weight set is determined by the choice of quantization group, while the implementation of varies among different quantization methods. For example, when = absmax(W) 2k11 , Eqs. 3 and 4 refer to the standard scalar-based quantization in k-bit. Since the de-quantization process in Eq. 4 aligns with the normal forward propagation, we decompose the model parameters θ in Eq. 1 into θ, and perturb the scaling component while keeping the discrete weights θ fixed. Therefore, Q-SPSA can be formulated as Definition 3.3 (Quantized Simultaneous Perturbation Stochastic Approximation, Q-SPSA). Given quantized model with integer parameters θ Rd and quantization scales , and loss function L, Q-SPSA estimates the gradients of over mini-batch using the following formula: ˆL( θ; B) = L(( + ϵz) θ; B) L(( ϵz) θ; B) 2ϵ zzL( θ; B), (5) where Rd is random vector sampled from (0, d), ϵ the perturbation scale, and the Hadamard product. Similar to MeZO, all quantization scales within linear layer are perturbed to save computation. In practice, one may choose to fine-tune the continuous quantization scale only, or combine Q-SPSA with SPSA to jointly update the unquantized counterparts. It is worth noting that Q-SPSA can be applied to both scalar-based and codebook-based quantization methods: in the experiments we show that our approach can successfully fine-tune both 4-bit LLMs quantized by the scalar-based GPTQ [12] and 2-bit LLMs quantized by the codebook-based AQLM [18] (in this case both the channel-wise scales and un-quantized weights are updated). 3.2.2 DDC: Directional Derivative Clipping Gradient estimation via ZO is notorious for causing unstable training due to large gradient variance [9]. This was also observed when combining Q-SPSA with the vanilla ZO-SGD method in our preliminary experiments where training often collapsed. To mitigate this problem, we propose Directional Derivative Clipping (DDC) and apply this method before updating the model with ZO-SGD at each optimization step. Specifically, the gradient estimate in Eq. 5 can be viewed as product of the random vector and the estimated directional derivative of loss function along w.r.t. (which is essentially scalar). Let denote the estimated directional derivative, Eq. 5 can be re-written as ˆL( θ; B) = z. Then, DDC applies clipping to by: = C, d, C, if > [C, C] if < (6) where is non-negative constant. The gradient estimate then becomes ˆL( θ; B) = z, which is plugged into ZO-SGD. Note that ˆL( θ; B) is an unbiased estimate of the full gradients. Since d2 d2 always holds, DDC essentially reduces the gradient variance: E[ ˆL( θ; B)2] = E[d z2] E[d z2] = E[ ˆL( θ; B)2]. (7) 4 Algorithm 1 Quantized Zeroth-order Optimization Require: quantization scales Rd, quantized weights θ Rd, loss function : Rd learning rate ηt, optimization steps , perturbation scales ϵ, clipping threshold C. 1st forward pass 2nd forward pass Directional derivative clipping, Eq. 6 Ensure non-negative scales for = 1...T do Sample batch of inputs and random seed PERTURB_SCALES(, ϵ, s) ℓ+ L( θ; B) PERTURB_SCALES(, 2ϵ, s) ℓ L( θ; B) PERTURB_SCALES(, ϵ, s) (ℓ+ ℓ)/(2ϵ) CLIP(d, C, C) Reset random number generator with seed for do (0, 1) max(i ηt z, 0) end for end for procedure Perturb_Scales(, ϵ, s) Reset random number generator with seed for do (0, 1) i + ϵz end for end procedure Due to space limit, the full proof of this part is provided in Appendix A. 3.2.3 Algorithm We summarize QZO in Algorithm 1. Note that although the quantization scales are perturbed per parameter in the pseudo code, in practice one may perturb the entire quantization scales of linear layer to save training time [9]. Remarks QZO seamlessly combines ZO with quantization and therefore leads to maximum reduction in memory usage: gradients and optimizer states are eliminated while model weights are compressed. To further cut memory usage on activations, one can divide the batch size while increasing the total number of optimization steps, or release activations during forward passes since ZO does not need to cache activations for gradient computation. It is worth mentioning that QZO is not limited to fine-tuning LLMs: in the experiments, we successfully fine-tune Stable Diffusion 3.5 Large [37] quantized by BitsAndBytes [24] on stylized images using single Nvidia RTX 4090 24GB GPU."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiments on LLMs Models and Datasets We evaluate our approach using three 7B-level LLMs, namely OPT-6.7B [1], Llama-2-7B [25], and Llama-3.1-8B [26], and one large-sized model with 13B parameters, i.e., Llama2-13B [25]. For QZO, the 7B models are quantized to 4-bit while the 13B model to 2-bit to test QZOs effectiveness under extreme quantization. Following prior work [9], we evaluate our approach on five popular NLP datasets covering both classification and generation tasks. Specifically, for classification, we use SST2 [27] and three subsets from SuperGLUE collection [28], i.e., RTE [29, 30, 31, 32], CB [33] and BoolQ [34]. For generation, we use SQuAD [35], which is question answering dataset. Following the common practice, we randomly sample 1,000 examples for training, 500 examples for 5 Table 1: Experiments based on OPT-6.7B, Llama-2-7B, and Llama-3.1-8B. Zero-Shot and Zero-ShotQ serve as the lower-bound, while LoRA is the upper-bound. QZO works well across different model architectures on all datasets, with fine-tuning memory significantly lower than MeZO and LoRA. Model Precision Memory Profiling Classficiation SST-2 RTE CB BoolQ Generation SQuAD OPT-6.7B Llama-2-7B Llama-3-8B Zero-Shot LoRA MeZO Zero-Shot-Q QZO Zero-Shot LoRA MeZO Zero-Shot-Q QZO Zero-Shot LoRA MeZO Zero-Shot-Q QZO 16 bits 16 bits 16 bits 4 bits 4 bits 16 bits 16 bits 16 bits 4 bits 4 bits 16 bits 16 bits 16 bits 4 bits 4 bits - 61.2 14.04GB 95.6 14.81GB 93.0 60.1 87.6 - 4.82GB - 58.1 14.23GB 95.4 14.79GB 83.5 58.5 90.0 - 4.99GB - 59.6 16.73GB 95.4 20.45GB 92.5 58.7 93.0 - 6.3GB 55.2 51.8 83.8 71.4 64.6 67.9 53.8 51.8 61.7 67.9 61.7 32.1 88.1 82.1 58.1 67.9 53.4 35.7 59.2 69.6 45.8 46.4 89.2 89.3 70.0 91.1 50.2 37.5 66.8 69.6 59.5 80.1 66.8 59.1 66. 66 85.9 69.6 64.6 68.2 66.1 87.6 83.4 65.0 78.2 36.5 85.8 79.6 35.9 78.5 55.6 89.9 80.7 53.6 85.5 64.8 89.5 86.9 59.2 88.3 validation, and 1,000 examples for testing. We report accuracy for classification tasks, whereas the metric for generation tasks is F1 score. Baseline Methods wide range of baseline methods are chosen for comparison to justify QZOs effectiveness. Specifically, QZO is compared with: (1) Zero-Shot, and Zero-Shot-Q, the original and quantized zero-shot models, respectively, which are viewed as the lower-bound; (2) LoRA [10] fine-tuning on un-quantized models, which is considered as the upper-bound;2 (3) MeZO [9], which applies ZO to un-quantized models. Implementation Details For 4-bit quantization, we apply GPTQ [12] to the 7B-level LLMs (i.e., OPT-6.7B, Llama-2-7B, and Llama-3.1-8B).3 The quantization group in GPTQ is set to 128. For extreme quantization in 2-bit, we apply AQLM [18] with 1 codebook of 16 bits to Llama-213B.4 We use QZO to fine-tune the channel-wise scales in AQLM. Following prior work [18, 17], the un-quantized parts are jointly fine-tuned using the regular SPSA and ZO-SGD. To accelerate QZO fine-tuning in 2-bit, we also modify AQLMs Triton inference kernel to disentangle matrix reconstruction and matrix-vector multiplication.5 For QZO, we set the learning rate to 107, the batch size to 16, training steps to 20k, the perturbation scale ϵ to 103, and the clipping threshold to 100. single Nvidia RTX 4090 GPU (24GB) is used for all experiments (except LoRA, which requires two GPUs). For MeZO, we adopt the official code.6 For LoRA, we use HuggingFaces implementation, which supports fp16 training.7 4.1.1 Main Results QZO on 4-bit Quantization Table 1 compares QZO with different baselines across three model architectures on the five NLP datasets. The detailed training statistics are shown in Table 2. Following MeZO, memory profiling measures the peak memory usage during the first 100 optimization steps. The dataset used for memory profiling is SST2 and the (per-device) batch size is set to 1 to test the minimum VRAM requirement. We summarize our main findings below. 2Due to limited budget on computational resources, we do not compare with full-parameter fine-tuning and instead treat LoRA as the upper-bound. 3https://github.com/ModelCloud/GPTQModel 4https://huggingface.co/ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf 5https://github.com/triton-lang/triton 6https://github.com/princeton-nlp/MeZO 7https://github.com/huggingface/peft 6 Table 2: Training statistics collected on SST-2. Overall, QZO is both memory-efficient and computation-efficient. OPT-6.7B Llama-2-7B Llama-3.1-8B LoRA MeZO QZO LoRA MeZO QZO LoRA MeZO QZO Trainable Paramters 4.19 106 6.65 109 5.03 107 4.19 106 6.74 109 5.06 3.41 106 8.03 109 5.45 107 Total FLOPs (SST-2) 1.36 1016 9.91 1017 8.19 1013 1.54 1016 1.13 1018 2.26 1016 1.59 1016 1.13 1018 7.9 1016 Table 3: Experiments based on Llama-2-13B. Zero-Shot-Q is the lower-bound while LoRA is the upper-bound. QZO demonstrates strong potential under extreme quantization. Model Precision Memory Profiling Classification Generation SST-2 RTE CB BoolQ SQuAD Llama-2-13B LoRA Zero-Shot-Q QZO 16 bits 2 bits 2 bits 26.26GB 95.5 57.6 80.5 - 5.78GB 89.2 94.6 53.1 46.4 54.5 55.4 86.4 69.2 70. 91.1 55.4 59.4 QZO demonstrates effectiveness consistently across all model architectures and NLP tasks. Specifically, QZO achieves significant improvements over Zero-Shot-Q, meaning that QZO successfully fine-tunes these quantized LLMs. On most datasets, QZO performs on par with MeZO, despite using 3 less memory; sometimes QZO even beats MeZO with noticeable margins, e.g., 85.5 vs. 80.7 on SQuAD when using Llama-2-7B. It is worth highlighting that MeZO is based on 16-bit models while QZO is based on 4-bit models with much lower precision. Compared with the upper-bound, i.e., LoRA, the gap is still huge. This makes sense because ZO methods rely merely on forward passes for gradient estimation, which would be much less accurate than that of backpropagation. QZO demonstrates both memory-efficiency and computation-efficiency. QZO pushes memoryefficiency to the extreme by eliminating gradients and optimizer states while reducing weights precision. Therefore, the memory usage is minimal compared to the baselines like MeZO and LoRA. Table 2 compares QZO with MeZO and LoRA on learnable parameter count and FLOPs. It is worth noting that QZO uses only about 1% of trainable parameters and 1% of FLOPs of MeZO. This is because QZO only fine-tunes the continuous quantization scale while leaving most weights (which are quantized) fixed. We expect the difference to be further increased when more powerful quantization methods are used. QZO on 2-bit Quantization Table 3 shows that QZO beats the zero-shot model with significant margins. The results strongly justify QZOs effectiveness under extreme quantization. QZO has the potential to be applied to on-device learning scenarios for edge devices. 4.1.2 Ablation Study and Further Analyses In this section, we mainly evaluate the DDC component. Recall that DDC (Directional Derivative Clipping, Eq. 6) clips abnormal directional derivatives estimated via QZO (i.e., in Eq. 6). We use QZO to train two Llama-2-7B models, with and without using DDC, and record the directional derivatives and loss values for the first 1,000 steps. Figure 2 shows that without DDC the directional derivative often gets abnormal values that go beyond the range of [C, C] (C is the clipping threshold in Eq. 6), leading to NaN value for the loss (which means the training collapses). 7 Figure 2: Directional derivatives (left) and loss values (right) collected during the early 1,000 training steps. Without DDC, the training is extremely unstable, often leading to abnormal directional derivatives and eventually NaN values for the loss. 4.2 Experiments on Stable Diffusion Model and Dataset We evaluate our approach on Stable Diffusion 3.5 Large [37], the current state-of-the-art text-to-image model (the largest among the 3.5 series). We choose the Styled Image Dataset [38] for evaluation, which includes the Frosting Lane, PS1, Tarot, and Yarn styles, with 10,000 image-caption pairs per style. For each style, the images of 512512 resolution are split using the 8:2 train-test ratio. Implementation Details For QZO, 4-bit quantization is applied to Stable Diffusion 3.5 Large using BitsAndBytes [24]. The batch size is set to 16. The learning rate is set to 1e-6. The perturbation scale ϵ is set to 1e-3. The total number of training steps is 20k. Following the common practice, only the DiT part in Stable Diffusion 3.5 Large is fine-tuned. Memory Usage Stable Diffusion 3.5 Large consists of VAE, DiT, and three text encoders (CLIP-ViT/G, CLIP-ViT/L, and T5-XXL). For regular training in fp16/bf16, this model requires 0.37GB for the VAE, 21.26GB for the text encoders, 16.2GB for the DiT, 16.2GB for gradients, and 32.4GB for optimizer states, totaling 86.43GB of memory usage (without even considering other overheads like caches and buffers). In contrast, QZO takes only 12.4GB of memory for fine-tuning, which can easily fit into single Nvidia RTX 4090 GPU (24GB). To our knowledge, this is the first work showing that fine-tuning Stable Diffusion 3.5 Large can be done on single consumer-grade GPU. Qualitative Results The results are visualized in Figure 3. Overall, the results are encouraging: the data distribution generated by QZO is visually closer to the ground truth than the zero-shot model, which suggests that QZO works to some extent for fine-tuning quantized text-to-image models. However, there is still noticeable gap between QZOs images and the ground truths. More results of different styles are provided in Appendix B. We discuss two reasons that may explain why QZO does not produce the same level of performance as in LLMs. First, unlike discrete probability modeling as in LLMs, Stable Diffusion is essentially regression model that predicts continuous noise values. This architectural difference leads to sensitivity issue: the deviations of the estimated gradients via ZO are directly manifested as differences at the pixel level during image generation, and such errors are propagated through continuous output, resulting in fidelity degradation. Second, recall that ZO introduces noise perturbations in latent representations. Consider linear layer without bias, = a, the forward call in QZO leads to = (W + ηd z)a after one optimization step, where η denotes the learning rate and the estimated directional derivative discussed in Eq. 6. This update injects additional Gaussian noise ηd into the activations, which is propagated through the denoising process and thus disrupts the pre-configured noise schedule (it acts as conflicting noise 8 Figure 3: Tarot style image generation results. Using the same test prompt, the model fine-tuned with QZO generates slightly better results than the zero-shot model. It is worth noting that QZO takes only 12.4GB of memory in single Nvidia RTX 4090 GPU for fine-tuning Stable Diffusion 3.5 Large, which would otherwise require more than 86.43GB of memory for regular fine-tuning. patterns). The diffusion model is unable to simultaneously remove the scheduled and ZO-induced noise, thus resulting in incomplete denoising."
        },
        {
            "title": "5 Conclusion, Limitations, and Future Work",
            "content": "QZO enables fine-tuning quantized neural networks via ZO, which greatly reduces memory usage related to model weights, gradients, and optimizer states. We show that QZO works for wide range of LLMs and is compatible with both scalar-based and codebook-based quantization methods. When using 4-bit LLMs, QZO achieves performance on par with MeZO, while using 3 less GPU memory. In the extreme quantization scenario, QZO successfully fine-tunes 2-bit LLama-2-13B across different NLP datasets. The results indicate that QZO has the potential to be applied to on-device learning for edge devices. We also show that QZO can be used to fine-tune Stable Diffusion 3.5 Large, taking only 12.4GB of memory in single Nvidia RTX 4090 GPU. There are few limitations worth mentioning: (1) QZOs performance depends on how good the quantization method is. Specifically, if the quantization method has large quantization error, this makes the forward passes in ZO noisy and therefore makes the gradient estimation less accurate. However, improving quantization is beyond the scope of this work. (2) The performance on diffusion models lags behind LLMs, which may be caused by the mismatch in the noise scheduling between ZO and diffusion. One potential solution is to redesign the noise scheduling in ZO such that it aligns with diffusion. (3) The gap between QZO and full-precision fine-tuning is still big. To reduce this gap, we need to significantly improve the gradient estimation accuracy in ZO."
        },
        {
            "title": "References",
            "content": "[1] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer language models, arXiv preprint arXiv:2205.01068, 2022. [2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [3] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language models to follow instructions with human feedback, Advances in neural information processing systems, vol. 35, pp. 2773027744, 2022. [4] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen, Evaluating large language models at evaluating instruction following, in 12th International Conference on Learning Representations, ICLR 2024, 2024. [5] A. Gholami, Z. Yao, S. Kim, C. Hooper, M. W. Mahoney, and K. Keutzer, Ai and memory wall, IEEE Micro, 2024. [6] J. Zhao, Z. Zhang, B. Chen, Z. Wang, A. Anandkumar, and Y. Tian, Galore: Memory-efficient llm training by gradient low-rank projection, in International Conference on Machine Learning, pp. 6112161143, PMLR, 2024. [7] W. Guo, J. Long, Y. Zeng, Z. Liu, X. Yang, Y. Ran, J. R. Gardner, O. Bastani, C. De Sa, X. Yu, et al., Zeroth-order fine-tuning of llms with extreme sparsity, in Workshop on Efficient Systems for Foundation Models II@ ICML2024, 2024. [8] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101, 2017. [9] S. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen, and S. Arora, Fine-tuning language models with just forward passes, Advances in Neural Information Processing Systems, vol. 36, pp. 5303853075, 2023. [10] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al., Lora: Low-rank adaptation of large language models., ICLR, vol. 1, no. 2, p. 3, 2022. [11] D. Nguyen, W. Yang, R. Anand, Y. Yang, and B. Mirzasoleiman, Mini-batch coresets for memory-efficient language model training on data mixtures, in The Thirteenth International Conference on Learning Representations, 2025. [12] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, OPTQ: Accurate quantization for generative pre-trained transformers, in The Eleventh International Conference on Learning Representations, 2023. [13] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, Awq: Activation-aware weight quantization for llm compression and acceleration, in MLSys, 2024. [14] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, GPT3.int8(): 8-bit matrix multiplication for transformers at scale, in Advances in Neural Information Processing Systems (A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, eds.), 2022. [15] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, SmoothQuant: Accurate and efficient post-training quantization for large language models, in Proceedings of the 40th International Conference on Machine Learning, 2023. [16] S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, P. Cameron, M. Jaggi, D. Alistarh, T. Hoefler, and J. Hensman, Quarot: Outlier-free 4-bit inference in rotated LLMs, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [17] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa, Quip #: Even better llm quantization with hadamard incoherence and lattice codebooks, in International Conference on Machine Learning, pp. 4863048656, PMLR, 2024. [18] V. Egiazarian, A. Panferov, D. Kuznedelev, E. Frantar, A. Babenko, and D. Alistarh, Extreme compression of large language models via additive quantization, in International Conference on Machine Learning, pp. 1228412303, PMLR, 2024. [19] Y. Liu, J. Wen, Y. Wang, S. Ye, L. L. Zhang, T. Cao, C. Li, and M. Yang, Vptq: Extreme low-bit vector post-training quantization for large language models, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 81818196, 2024. [20] A. R. Conn, K. Scheinberg, and L. N. Vicente, Introduction to derivative-free optimization. SIAM, 2009. [21] J. C. Spall, Multivariate stochastic approximation using simultaneous perturbation gradient approximation, IEEE transactions on automatic control, vol. 37, no. 3, pp. 332341, 1992. [22] H. Robbins and S. Monro, stochastic approximation method, The annals of mathematical statistics, pp. 400407, 1951. [23] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and D. Lee, Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization, Advances in Neural Information Processing Systems, vol. 36, pp. 3618736207, 2023. [24] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, Advances in neural information processing systems, vol. 36, pp. 1008810115, 2023. [25] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [26] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [27] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, Recursive deep models for semantic compositionality over sentiment treebank, in Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 16311642, 2013. [28] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman, Superglue: stickier benchmark for general-purpose language understanding systems, Advances in neural information processing systems, vol. 32, 2019. [29] I. Dagan, O. Glickman, and B. Magnini, The pascal recognising textual entailment challenge, in Machine learning challenges workshop, pp. 177190, Springer, 2005. [30] R. B. Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor, The second pascal recognising textual entailment challenge, in Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, vol. 7, pp. 785794, 2006. [31] D. Giampiccolo, B. Magnini, I. Dagan, and W. B. Dolan, The third pascal recognizing textual entailment challenge, in Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 19, 2007. [32] L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo, The fifth pascal recognizing textual entailment challenge., TAC, vol. 7, no. 8, p. 1, 2009. [33] M.-C. De Marneffe, M. Simons, and J. Tonhauser, The commitmentbank: Investigating projection in naturally occurring discourse, in proceedings of Sinn und Bedeutung, vol. 23, pp. 107124, 2019. [34] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, Boolq: Exploring the surprising difficulty of natural yes/no questions, in Proceedings of NAACL-HLT, pp. 29242936, 2019. [35] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, Squad: 100,000+ questions for machine comprehension of text, in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, 2016. [36] S. Malladi, A. Wettig, D. Yu, D. Chen, and S. Arora, kernel-based view of language model fine-tuning, in International Conference on Machine Learning, pp. 2361023641, PMLR, 2023. [37] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al., Scaling rectified flow transformers for high-resolution image synthesis, in Forty-first international conference on machine learning, 2024. [38] A. Ganjdanesh, R. Shirkavand, S. Gao, and H. Huang, Not all prompts are made equal: Prompt-based pruning of text-to-image diffusion models, arXiv preprint arXiv:2406.12042, 2024."
        },
        {
            "title": "A Proof of Clipped Gradient Estimate Variance Upper Bound",
            "content": "A.1 Preliminary Formulations In the SGD algorithm, the stochastic gradient of mini-batch is given by θL(θ; B) = 1 (cid:88) kB ℓθ(θ; xk) In our QZO, following Definition 3.3, the estimated gradient is formulated as ˆL( θ; B) = dz zzL( θ; B) (0, d) where = L((+ϵz)θ;B)L((ϵz)θ;B) z. Note that θL(θ; B) and ˆL( θ; B) are un2ϵ biased estimate of the full gradient θL(θ) and full gradient of loss w.r.t quantization scales L( θ), respectively. A.2 Variance Upper Bound Let be the clipping threshold, the clipped gradient estimate can be reformulated by: ˆL( θ; B) = clip(d, C, C)z = dz Theorem 1. Clipped gradient estimate ˆL( θ; B) is an unbiased estimate of the full gradient of loss w.r.t quantization sclaes L( θ). (8) Proof. Suppose that the mini-batch is sampled from the dataset D, and denotes the number of mini-batches in the dataset. EB,z[ ˆL( θ; B)] = Ez[ 1 (cid:88) iz] = Ez[ = Ez[ = Ez["
        },
        {
            "title": "1\nN",
            "content": "1 iD (cid:88) iD,di<C (cid:88) iD,di<C (cid:88) iD,di<C iz + dz +"
        },
        {
            "title": "1\nM",
            "content": "(cid:88) iz] iD,di>C (cid:88) Cz] iD,di>C (9) zzL( θ; B)] + Ez[ (cid:88) z] (10) iD,di>C = Ez[µiD,di<C(zzL( θ; B))] + 0 = Ez[zz]EB[L( θ; B)] = L( θ) (11) (12) Eq. 9 equals Eq. 10 as ϵ 0. In Eq. 11, µ represents the sample mean of the observations, and the transition from Eq. 11 to Eq. 12 holds because the sample mean µ is an unbiased estimate of the expectation. The squared norm of the clipped estimator satisfies: E[ ˆL( θ; B)2] = E[d2z2] E[d2z2] = E[ ˆL( θ; B)2] (13) since d2 d2 holds. We have the variance upper bound: ar[ ˆL( θ; B)] = E[ ˆL( θ; B)2] E2[ ˆL( θ; B)] E[ ˆL( θ; B)2] E2[ ˆL( θ; B)] = ar[ ˆL( θ; B)] + E2[ ˆL( θ; B)] E2[ ˆL( θ; B)] = ar[ ˆL( θ; B)] + [L( θ)]2 E2[ ˆL( θ; B)] (14) By Theorem 1, ar[ ˆL( θ; B)] ar[ ˆL( θ; B)] holds almost surely."
        },
        {
            "title": "B Stable Diffusion Experiment Details",
            "content": "B.1 More Results In this section, we present additional visualization results for fine-tuning the Stable Diffusion 3.5 Large. Figure 4 illustrates the Yarn style outcomes, while Figure 5 shows the PS1 style visuals. Meanwhile, the results in the Frosting Lane style can be found in Figure 6. 12 Figure 4: Yarn style generation comparison. Figure 5: PS1 style generation comparison. Figure 6: Frosting Lane style generation comparison."
        }
    ],
    "affiliations": [
        "Hong Kong Baptist University",
        "Nanjing University of Science and Technology"
    ]
}