{
    "paper_title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning",
    "authors": [
        "Yuji Wang",
        "Wenlong Liu",
        "Jingxuan Niu",
        "Haoji Zhang",
        "Yansong Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model."
        },
        {
            "title": "Start",
            "content": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning Yuji Wang1,2, Wenlong Liu2, Jingxuan Niu1, Haoji Zhang1, Yansong Tang1 1Tsinghua Shenzhen International Graduate School, Tsinghua University 2International Digital Economy Academy (IDEA) 5 2 0 2 6 ] . [ 1 3 7 3 6 0 . 2 1 5 2 : r Figure 1. In the left case, VG-Refiner performs explicit reasoning over the tool outputs via the CoT process, whereas REVPT merely confirms the tool feedback without any analytical examination, leading to its inability to detect tool-induced errors. The baseline model Qwen2.5-VL-7B of REVPT and VG-Refiner can locate the true object by its own capabilities without CoT. The right part shows that our VG-Refiner achieves grounding accuracy comparable to the 32B model across the average of five test splits on the RefCOCO series, under different tool conditions in the PiTER evaluation protocol."
        },
        {
            "title": "Abstract",
            "content": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce two-stage thinkrethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We Corresponding author adopt small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model. Our code is available at https://github.com/VoyageWang/VG-Refiner. 1. Introduction Tool-integrated visual reasoning (TiVR) can significantly improve the performance of large vision-language models (LVLMs) across diverse specialized domains, such as table or document question-answering (QA) [5, 38, 49], visual search and reasoning [17, 31], and general visual QA [14, 44, 59]. Building upon recent advances that incorporate verbal chain-of-thought (CoT) reasoning to strengthen LVLMs [2, 10, 26, 28], existing approaches often employ reinforcement learning (RL) to train models for tool-based reasoning tasks [29, 48, 62, 63]. This training paradigm not only reduces the need for extensive manual annotations but also enables the models to achieve stronger generalization capabilities across diverse tasks [56]. However, this agentic RL paradigm focuses on teaching the model how to select suitable tools from fixed toolkit, such as detection [20, 34], depth [53, 54], and edge tools, which overlooks situations where the selected tools outputs are unreliable or incorrect. The existing TiVR model can be easily disturbed by this poor feedback and then generate hallucinatory response to explain and reasoning based on it. This issue is particularly prominent in referring expression comprehension (REC) tasks [33], where detection tools frequently produce inaccurate or misleading results. Thus, it further limits the applications in downstream tasks, such as spatial grounding [16, 46, 55, 58] or counting [7]. To be more specific, as shown in Figure 1, we require the TiVR model REVPT [63], to locate the referred object and prompt it with the incorrect tool results. Despite the explicit verbal CoT process, the model fails to discover the irrationality of the prompted tool result, even though the original Qwen2.5-VL-7B model [1] can locate the true object. We attribute this scenario to the notable deficiency in their ability to discover and correct tool-induced errors. To overcome this limitation, we propose tool-refined referring grounded reasoning (TrRGR) framework, VGRefiner, that can rethink the feedback from the expert REC tools and decide whether to accept or refine the results through explicit analysis trained based on agentic RL. Rather than relying on tools as infallible sources of information, VG-Refiner performs secondary reasoning stage to analyze the contextual consistency between the visual evidence, the textual query, and the tool-generated predictions. Although VG-Refiner is primarily designed to address unreliable or erroneous tool outputs, it also demonstrates strong follow-correct rate when tools are reliable. Technically, we propose novel thinkrethink two-stage refinement framework, where the model first performs independent reasoning and then re-evaluates its conclusions by integrating feedback from external tools to produce more reliable and accurate results. Secondly, we design novel refinement reward to encourage the model to recognize and accept reliable feedback from external tools while reinforcing its ability to detect and refine erroneous predictions. In addition, to comprehensively evaluate the refinement ability of different models, we establish prompt-integrated tool enhancement and refinement (PiTER) evaluation protocol and create two tool-refined metrics that assess model performance from two key dimensions: refinement capability and refinement quality. The PiTER protocol injects the tool results into the system prompt and outputs with JSON grounding results fairly. Overall, our contributions are summarized as follows: We identify key limitation of existing TiVR methods in REC tasks and propose VG-Refiner, two-stage thinkrethink framework equipped with refinement reward that enhances both reasoning and correction while preserving trust in reliable tool feedback. We define the TrRGR reasoning paradigm and propose two novel metrics for assessing the refinement capability of all LVLMs, and establish unified PiTER protocol to ensure fair comparison. Preserving the general capabilities of the pre-trained model, our model outperforms SOTA methods on RefCOCO/+/g in accuracy and surpasses the Qwen2.5-VL32B model in refinement ability, shown in Figure 1, while trained on small amount of task-specific data. 2. Related Work Tool-integrated Visual Reasoning. Tool-integrated visual reasoning (TiVR) significantly improves the reasoning abilities of LVLMs. The effectiveness of TiVR hinges on well-curated and diverse tools, which can be categorized into three groups [39]. Firstly, to mimic human visual attention, current works, including DeepEyes [31], Mini-O3 [17], Simple-o3 [45], and Pixel Reasoner [37] focus on iterative zoom-in and region-of-interest (RoI) selection, equipping LVLMs with active perception capabilities. The second group includes OCR [? ] and code interpretation tools [29, 49], which empower LVLMs to extract textual information from images and perform numerical or logical reasoning through executable Python code. Finally, REVPT [63], OpenThinkIMG [38], and Active-O3 [64] employ visual perception tools, such as depth estimation and detection modules, to enhance perception capabilities, e.g., counting. However, without carefully designed refinement mechanisms, these models are easily disturbed by the imperfect or noisy outputs of detection tools in REC. LVLMs for REC Tasks. The referring expression comprehension aims to locate the visual element aligned with the linguistic input phrase, which has many applications across diverse tasks [12, 23, 24, 46, 47, 55, 60]. With the rapid development of LVLMs [16, 19, 41, 51], many series of works have used the LVLMs for REC tasks. LLAVA-grounding [58] and Grounding-GPT [18] construct large-scale datasets using the supervised fine-tuning (SFT) method to enhance grounding capabilities. Recently, studies like VLM-R1 [35], Visual-RFT [28], Visionreasoner [27], and Ground-R1 [3] have demonstrated that the RL can make stronger grounding reasoning abilities than SFT with verbal CoT process. Rex-Thinker [11] and Rex-Seek [13] are also trained with RL and employ proposal detection tool, upon which the LVLM performs reasoning over the proposed bounding boxes to retrieve the true target object. Therefore, no existing methods employ agentic RL for the REC with respect to external references provided by tools. Expert REC Models. We define good expert REC model as one that is fine-tuned on referring datasets such as RefCOCO, RefCOCO+, and RefCOCOg [57], while Figure 2. The overall framework of VG-Refiner. In our reward design, we consider the quality of tool feedback IoUt. For different circumstances, we adopt different levels of reward to encourage the model to refine the tools incorrect results or accept the reliable results. We use GRPO to optimize the policy model, which produces various rollouts during training. After the think process, the model queries referring visual toolkit for additional reference outputs. In GRPO, KL divergence constrains strategy deviation from the frozen reference model to ensure stable optimization. weak model refers to one that possesses detection capabilities but has not been fine-tuned on any referring dataset. Current state-of-the-art expert models, such as EVF-SAM [61] and InstanceVG [6], employ bi-directional encoders BEIT-3 [42] to achieve effective languagevision fusion. In summary, existing methods overlook the issue of erroneous tool outputs in REC scenarios. Therefore, we propose the tool-refined referring grounded reasoning paradigm. 3. Method In this section, we present comprehensive description of our proposed VG-Refiner. We begin by introducing the special features of TrRGR and the reasoning paradigm, followed by detailed explanation of its training strategy. Finally, we describe the evaluation protocols for assessing refinement capabilities and the corresponding metrics. 3.1. Tool-refined Referring Grounded Reasoning Concept Constructions. We propose tool-refined referring grounded reasoning (TrRGR), new paradigm for REC tasks. We differentiate our approach from two prevailing paradigms: (1) conventional TiVR methods [38, 63] that merely integrate detection tool invocation and feedback without deeper reasoning against unreliable tool results, and (2) verbal CoT-based models [2, 3, 26] that rely solely on verbal CoT reasoning without external tool collaboration. In contrast, built upon internal reasoning, TrRGR introduces an additional explicit rechecking and refinement mechanism. Through structured CoT process, the model analyzes and verifies the outputs of external expert tools, enabling it to reason about and correct unreliable results. Moreover, it optimizes reliable tool outputs to achieve higher Intersection-over-Union (IoU) scores. Reasoning Paradigm. The overall architecture of our VG-Refiner is shown in Figure 2. We employ Qwen2.5VL-7B as the refiner policy model due to its strong general reasoning capabilities demonstrated by [26, 27, 63]. To enhance reasoning reliability under uncertain tool predictions, we design two-stage thinkrethink framework that mimics the human cognitive process from recognizing, verify3 Figure 3. User prompt for the PiTER evaluation process. This prompt is shared across all model types, requiring the model to produce grounding results in JSON format through single-stage conversation, without any CoT reasoning or tool interaction. The placeholder {Question} is replaced with the referring expression, while {tool results} is substituted with the feedback from either strong or weak tool corresponding to the given question. ing the tool feedback, to correcting. In the first-round think stage, the policy model πm first take the input image and referring phrase Pq with the system prompt Psys and output the thinking CoT reasoning PCoT, which is included between the <think> and </think> and the tool calling action PA, shown below: CoT, PA = π1 1 m(I, Pq, Psys), (1) where π1 refers to the first conversation. This stage aims for broad coverage rather than precision, providing an initial hypothesis of the referred object. The action phrase will be parsed and used for querying external tool results. Next, the expert model produces its detection result Bt based on and , which serves as external feedback for the refinement process. In the second-round rethink stage, the model re-evaluates both initial predictions and performs targeted correction analysis included between <rethink> and </rethink>, shown below: Bf , 2 CoT = π m(I, 1 CoT, Bt), (2) where Bf is the final refined result produced by the policy model. Through this reflective process, the refiner learns when to trust or override the expert model. This iterative reasoning effectively combines tool-assisted perception with self-corrective reflection, leading to more accurate and interpretable REC outcomes. Tool Model. In the REC task, we define strong tool as one that has already achieved state-of-the-art (SOTA) performance on mainstream REC benchmarks. In contrast, weak tool refers to model that has not been fine-tuned on any referring expression dataset. During inference, we employ the strong tool to achieve superior performance, surpassing both the tool itself and the base MLLM. However, when benchmarking the models refinement capability, we adopt the weak tool to better assess how effectively the model can correct suboptimal tool predictions. Each referring visual tool takes the same inputs (I, Pq) and outputs its predicted bounding boxes Bt. 3.2. Agentic RL Training Group Relative Policy Optimization (GRPO). We enable our VG-Refiner to evolve via agentic RL, where the model actively invokes external tool results during the RL process. We do not rely on the cold start data for first supervised finetuning, as the baseline model already demonstrates strong refinement ability  (Table 3)  . To realize this training scheme, we adopt GRPO [10] on the pretrained model. During each update, the policy model performs multiple rollouts to generate set of reasoning trajectories. Each trajectory oi in the TrRGR paradigm is represented as {Ti, Ai, Fi, Ri}. We then evaluate the generated trajectories using our rulebased reward function, which assigns scalar reward to every oi {o1, o2, . . . , oG}. To further enhance the models refinement capability, we incorporate two complementary types of rewards described below. Format Reward. To encourage the model to produce structured and interpretable reasoning traces, we introduce format reward that verifies whether the generated output follows the required reasoning schema. Specifically, the required output is <think> 1 CoT </think>, <rethink> 2 CoT </rethink>, and <answer> {bbox 2d: [x1, y1, x2, y2]} </answer>. positive reward of 1 is assigned for Rf ormat if all tags are correctly formatted; otherwise, the reward is set to zero. This encourages the model to generate explicit refinement reasoning in structured manner. Refinement Reward. Since tool feedback may be reliable or erroneous, the reward must reflect these differing levels of difficulty. Notably, correcting an incorrect tool output is more challenging than confirming correct one. To guide the model in both recognizing trustworthy predictions and refining unreliable ones, we introduce two components: tool-confirmation reward RG ref ine and tool-refinement reward RW ref ine. Combined, they form our refinement reTable 1. Evaluation results of visual grounding in Acc@0.5 on RefCOCO, RefCOCO+, and RefCOCOg datasets."
        },
        {
            "title": "Method",
            "content": "Qwen2.5-VL-7B [1] Qwen2.5-VL-72B [1] Ground-R1 [3] CogVLM [43] CogCoM [32] UniVG-R1 [2] Vitron [8] UniPixel [25] EVF-SAM (tool) [61] VG-Refiner (Ours) vs. EVF-SAM (tool) vs. Qwen2.5-VL-7B"
        },
        {
            "title": "RefCOCO",
            "content": "RefCOCO+"
        },
        {
            "title": "RefCOCOg",
            "content": "val 90.0 92.7 92.9 92.5 92.3 91.6 90.9 92.0 92.5 93.2 +0.7 +3.2 testA testB 92.5 94.6 93.9 94.0 94.6 93.1 93.2 92.5 94.2 95.0 +0.8 +2.5 85.4 89.7 88.0 88.7 89.2 87.2 89.3 88.1 90.3 90.7 +0.4 +5. val 84.2 88.9 86.5 87.5 88.2 85.9 83.7 87.2 86.4 88.5 +2.1 +4.3 testA testB 89.1 92.2 90.8 91.8 92.8 90.5 89.1 91.9 90.2 92.7 +2.5 +3.6 76.9 83.7 78.8 81.4 82.1 80.0 76.9 82.1 81.7 83.0 +1.3 +6. val 87.2 89.9 90.1 89.5 89.3 88.7 86.4 88.6 87.7 89.8 +2.1 +2.6 test 87.2 90.3 90.2 90.1 90.5 88.6 87.0 88.7 88.9 90.6 +1.7 +3.4 Avg. 86.6 90.3 88.9 89.4 89.9 88.2 87.1 88.9 88.9 90.5 +1.6 +3. Table 2. Evaluation results of visual grounding in Acc@0.5 on RefCOCO, RefCOCO+, and RefCOCOg datasets. 3.3. Refinement Ability Evaluation"
        },
        {
            "title": "RefCOCO",
            "content": "RefCOCO+"
        },
        {
            "title": "RefCOCOg",
            "content": "testA testB testA testB Qwen2.5-VL-7B [1] Qwen2.5-VL-72B [1] EVF-SAM (tool) [61] VG-Refiner (Ours) vs. EVF-SAM (tool) vs. Qwen2.5-VL-7B Rex-omni (tool) [12] VG-Refiner (Ours) vs. Rex-omni vs. Qwen2.5-VL-7B UNINEXT-H (tool) [52] VG-Refiner (Ours) vs. UNINEXT vs. Qwen2.5-VL-7B 92.5 94.6 94.2 95.0 +0.8 +2.5 89.4 92.6 +3.2 +0.1 94.3 95.6 +1.3 +3.1 85.4 89.7 90.3 90.7 +0.4 +5.3 83.6 85.8 +2.2 +0.4 91.5 92.3 +0.8 +6.9 89.1 92.2 90.2 92.7 +2.5 +3.6 85.1 89.9 +4.8 +0.8 89.6 92.8 +3.2 +3.7 76.9 83.7 81.7 83.0 +1.3 +6.1 72.9 76.6 +3.7 -0.3 79.9 83.2 +3.3 +6. test 87.2 90.3 88.9 90.6 +1.7 +3.4 83.9 88.2 +4.3 +1.0 89.2 91.4 +2.2 +4.2 ward, which enables the model to adaptively utilize tool feedback based on its quality. Let IoUt and IoUf denote the IoU scores of the tool prediction and the final model prediction with the GT, respectively, shown in Figure 2. RG refine = RW refine = (cid:40)0.5, IoUt 0.5 and IoUf 0.5, 0, otherwise, (cid:40)1, IoUt < 0.5 and IoUf 0.5, 0, otherwise. (3) We adopt this coarse piecewise reward to ensure stability and avoid reward hacking. fixed 0.5 reward prevents overfitting to noisy IoU fluctuations when the tool is already correct, while 1 reward is given only for correcting wrong tool outputs, which represents truly substantive refinement. PiTER Evaluation Protocol. We propose the PiTER evaluation protocol to systematically assess the refinement capability of different models. To ensure fair comparison, we design unified system prompt shown in Figure 3, Prompt-injected Tool for Enhancement and Refinement (PiTER), which directly injects the tool results as external references in the input stage. All models are required to perform inference in single stage, regardless of their original paradigms or how they integrate tool feedback during reasoning. This removal of all intermediate verbal reasoning steps can reveal the intrinsic refinement capability of each model. Additionally, we also require them to generate predictions in JSON format consistent with the Qwen2.5-VL pretraining stage to accomplish the grounding tasks. Evaluation Metrics. We propose two complementary metrics to comprehensively evaluate the refinement performance of the TrRGR model: critical correct rate (CCR) and normalized signed relative IoU (NSRI). The former metric measures the proportion of successful refinements made by the model on tool-failed cases. In contrast, the latter evaluates the IoU improvement relative to the maximum possible gain, reflecting how effectively the model utilizes the available refinement space. For each sample i, we define the total tool wrong cases as Sw = {i IoUi < 0.5} and the CCR can be calculated shown below: CCR = (cid:80) iSw 0.5] I[IoUi Sw , (4) where I[] is the indicator function. In parallel, we define an 5 Table 3. Performance comparison of different models under weak and strong tool conditions across RefCOCO, RefCOCO+, and RefCOCOg benchmarks under PiTER . Methods with PiTER RefCOCO testA (%) RefCOCO testB (%) RefCOCO+ testA (%) RefCOCO+ testB (%) RefCOCOg test (%) Acc SRIw CCR Acc SRIw CCR Acc SRIw CCR Acc SRIw CCR Acc SRIw CCR Grounding DINO [20] 49.9 Qwen2.5-VL-7B Qwen2.5-VL-32B REVPT VG-Refiner EVF-SAM Qwen2.5-VL-7B Qwen2.5-VL-32B REVPT VG-Refiner 90.8 89.9 71.2 92.9 94. 92.7 95.1 88.9 94.8 71.7 69.4 36.9 75.0 22.2 14.7 -3.6 30.6 82.9 80.3 46.7 87.2 39.9 24.9 7.5 47.8 37.8 81.9 82.1 61.4 85.6 90. 86.3 90.8 82.9 89.9 Weak Tool Conditions 72.4 71.7 41.5 78.4 50.0 86.1 84.2 64.1 89. 63.9 59.2 24.4 68.9 Strong Tool Conditions 27.7 17.5 3.2 30.5 90. 89.8 92.4 87.2 92.5 30.1 19.8 2.0 36.4 62.0 61.1 31.4 67.0 10.6 7.6 -7.8 17.5 75.0 69.0 32.0 80.7 48.4 33.3 10.5 54.5 38. 70.3 71.3 53.2 75.8 81.7 77.4 83.6 77.1 81.9 45.7 45.1 19.5 51.7 10.9 8.9 -1.6 16.3 54.7 54.0 27.9 62.6 27.9 20.7 8.0 32.4 54. 82.7 84.1 67.3 86.6 88.9 86.1 90.6 86.4 89.3 56.3 54.5 23.4 61.9 19.5 14.7 0.9 25.7 66.7 65.9 32.1 73.5 33.9 23.6 9.0 39.4 Table 4. Comparison of reasoning grounding performance. The tools used in PiTER and VG-Refiner are both EVF-SAM."
        },
        {
            "title": "Method",
            "content": "VLM-R1-3B Qwen2.5-VL-7B Qwen2.5-VL-7B (PiTER) EVF-SAM (tool) VG-Refiner (Ours) LISA-Test 63.1 67.1 65.7 48.9 68.5 NSRI change gi as: gi ="
        },
        {
            "title": "IoUi",
            "content": "f IoUi 1 IoUi t"
        },
        {
            "title": "IoUi",
            "content": "f IoUi IoUi , , if IoUi > IoUi if IoUi < IoUi (5) 0, otherwise. This normalization ensures that gi [1, 1], where positive value represents the possible improvement, vice versa. To focus on the models behavior when the tool fails, we compute the average value of gi over the subset Sw as final metric, defined as SRIw. 4. Experiment 4.1. Experimental Settings Datasets. Following the VLM-R1 [35], to test the indomain performances, we evaluate our VG-Refiner on the RefCOCO/+/g datasets and test the out-of-domain performances on LISA-grounding [16] test sets, which are two classic referring reasoning grounding datasets. To further demonstrate that our model has the general QA capabilities, we benchmark it on MMbench [21], OCRBench [22], RealWordQA [50], ChartQA [30], and MMStar [4]. Implementation Details. Following REVPT [63], we adopt Qwen2.5-VL-7B as our baseline model. Our training framework is built upon the open-source verl [36] and vLLM [15] implementations. Following [26], we select total of 9K samples from the RefCOCOg dataset for training, where the tool outputs are partially derived from 6 strong expert model, EVF-SAM [61], and partially from weak tool, no fine-tuned Grounding DINO (GD-T) [20]. Although EVF-SAM is designed for referring comprehension segmentation (RES), its generated bounding boxes converted from masks are sometimes imprecise, being either too tight or too loose around the target due to mask quality artifacts. This property makes it particularly suitable for our refinement setting, as the model can further adjust such imperfect yet correct predictions to achieve higher IoU scores. For the training configuration, we perform 8 rollouts per sample, with training batch size of 128 and rollout batch size of 32. We set the learning rate to 1.0 106 and use sampling temperature of 1.0. The model is trained for one epoch to preserve its general QA capabilities and can be applied out of domain. All the experiments are conducted on 4 NVIDIA A100 GPUs. Metrics. We adopt the standard Acc@0.5 metric, measuring the proportion of predictions with IoU above 0.5 across all the test samples. To assess refinement ability, we use SRIw and CCR, focusing on samples where the tool outputs are incorrect (i.e., IoU < 0.5). All models are evaluated with official checkpoints under unified evaluation protocol and codebase for fairness. 4.2. Main Results Table 1 presents the referring grounding results on the RefCOCO series benchmarks. During inference, VG-Refiner calls the EVF-SAM model as an expert tool and explicitly analyzes its outputs in the rethink stage. By integrating tool feedback with its own reasoning process, VG-Refiner achieves SOTA performance, comparable even to 72B-scale Figure 4. Visualization of VG-Refiner handling three representative types of tool-induced errors in TrRGR. The first two grounding error categories often occur in good tool, EVF-SAM [61], whereas the third occurs in the not fine-tuned Grounding DINO [20]. Table 5. General QA evaluation across challenging benchmarks. Table 6. Ablation study on reward design under PiTER protocol using EVF-SAM tool result."
        },
        {
            "title": "Dataset",
            "content": "Qwen2.5-VL-7B REVPT VG-Refiner MMBench-ENDEV MMBench-CNDEV MMBench-ENDEV-V1.1 MMBench-CNDEV-V1.1 OCRBench Chart QA Realworld QA MMStar 80.0 82.2 80.0 81.4 886 86.2 68.1 60.8 50.7 53.9 52.5 55.1 880 84.0 51.4 45.8 79.0 82.5 79.1 81.5 886 86.2 67.5 60.1 Dataset IoU Reward Refinement Reward Acc (%) SRIw (%) Acc (%) SRIw (%) RefCOCO testA RefCOCO testB RefCOCO+ testA RefCOCO+ testB RefCOCOg test 93.9 88.8 91.4 80.1 87.2 24.4 16.6 32.2 15.2 22. 94.8 89.9 92.4 81.9 89.2 30.6 17.5 36.4 16.3 25.7 models. With the assistance of the tool, our model exhibits substantial performance gains over the original Qwen2.5VL-7B baseline, consistently outperforming the tools outputs across all benchmarks. These results demonstrate that VG-Refiner is capable of discerning when to follow or refine the tool predictions, rather than blindly trusting expert tool outputs. This TrRGR paradigm surpasses traditional verbal reasoning and SFT approaches, achieving co-evolution between the tool and the refiner. As shwon in Table 2, to further evaluate the robustness of VG-Refiner under different strong tool conditions, we additionally test it using tool predictions from Rex-Omni [12] and UNINEXT-H [52], both of which achieve strong performance on the REC task. Despite being trained solely on EVF-SAM tool outputs, our model demonstrates strong scalability and delivers consistent improvements across all five test sets. Notably, when provided with high-quality tool results such as those from UNINEXT-H, it can even surpass the performance of models at the 72B scale. Table 3 reports the refinement capabilities under the PiTER protocol across five test sets of the RefCOCO series benchmarks. Under weak tool conditions (i.e., use not finetuned GD-T tool), VG-Refiner significantly outperforms both 32Band 7B-sacle models, demonstrating superior robustness to unreliable tool predictions. In contrast, REVPT shows limited refinement ability due to the absence of explicit error-handling mechanisms. Notably, our model can effectively refine weak tool outputs to reach performance comparable to its own grounding ability. Under strong tool conditions (i.e., use EVF-SAM), REVPT fails to preserve the tools original performance and even degrades results, as reflected by negative SRIw, indicating that it tends to optimize tool outputs in harmful direction. This further highlights VG-Refiners advantage in adaptively handling both reliable and unreliable tool feedback. We further report the out-of-domain performances on the LISA-Grounding test set, as shown in Table 4, where the referring phrases require stronger reasoning capabilities. In such cases, EVF-SAM serves as weak tool since it has not 7 Figure 5. Visualization of the overall reasoning paradigm, first performing self-thinking and then re-thinking based on the tool outputs. Table 7. Ablation study on Think-Rethink mechanism in Acc (%)."
        },
        {
            "title": "RefCOCO",
            "content": "RefCOCO+"
        },
        {
            "title": "RefCOCOg",
            "content": "testB testA testB 1st stage tool prompt 2nd stage tool prompt think think-rethink 86.3 89.8 88.7 90.7 89.8 90.9 90.2 92.7 77.4 81.9 78.6 83. test 86.1 88.9 88.7 90.5 Table 8. Evaluation on using Qwen2.5-VL-7B as tool in PiTER protocol on RefCOCOg test subset. The 84.2 accuracy is our reproduced results of Qwen2.5-VL-7B using JSON format output."
        },
        {
            "title": "Method in PiTER",
            "content": "Acc (%) SRIw (%) CCR (%) Qwen2.5-VL-32B 86.0 vs 84.2 83.7 vs 84.2 Qwen2.5-VL-7B 85.7 vs 84.2 VG-Refiner 11.0 8.2 12.6 14.9 11.4 16.0 been fine-tuned on this dataset. We evaluate all models in zero-shot setting. Our VG-Refiner still surpasses the original Qwen2.5-VL-7B baseline, demonstrating its ability to identify and refine incorrect tool outputs, even under unseen and reasoning-intensive conditions. 4.3. Quality Assessment Tool Refinement. Figure 4 presents qualitative visualizations of our VG-Refiner under various tool feedback conditions. For incorrect tool predictions, our model can rethink the tools feedback and perform self-analysis to produce more accurate and robust grounding results. Rethink Analysis. Figure 5 illustrates that the rethink CoT process serves in refining and achieving the final correct predictions, highlighting the key advantage of the TrRGR paradigm. For instance, in the boundary ambiguity case, although VG-Refiner initially fails to detect the pizza under big piece of pizza in vessel during the think stage, the additional reference provided by the tool introduces contextual visual cues, enabling it to localize the true referred object accurately. This observation aligns with recent work TreeVGR [40], which enhances visual grounded reasoning by generating an evidence box to provide an additional spatial focus. Secondly, for completely incorrect tool predictions, VG-Refiner can reject the unreliable feedback and identify the true target during the rethinking process, demonstrating strong robustness against disturbances from erroneous tool outputs. 4.4. Ablation Study General Visual QA. Since we only use small amount of training data to enhance the refinement capability of VGRefiner, we further investigate its original general ability. As shown in Table 5, we compare the visual QA performance of our model with the baseline Qwen2.5-VL-7B and the TiVR model REVPT. Our VG-Refiner preserves the general visual QA capability and even achieves slight performance gains on some benchmarks. Reward Design. The design of the refinement reward plays crucial role in enhancing the correction capability of VG-Refiner. As shown in Table 6, both reward types are verifiable rewards computed by comparing the prediction with the GT. However, unlike the direct IoU reward, the proposed refinement reward explicitly considers the circumstances of tool feedback. When the tool prediction is incorrect but the models final prediction is correct, the model receives the full reward. Conversely, if the tool prediction is correct but the model only marginally refines it, the reward is set to 0.5. This hierarchical reward mechanism encourages the model to focus on correcting inaccurate tool outputs, leading to higher overall accuracy and NSRIw. The results confirm the effectiveness of this design, showing con8 sistent performance gains across all benchmarks. Think-rethink Paradigm. As shown in the first and second rows of Table 7, we first investigate the effect of tool feedback positioning using the baseline model Qwen2.5VL-7B. The first row presents the evaluation under the PiTER setting, while the second row corresponds to twostage conversation setup. In both conversations, the model is required to output the bounding box in JSON format, but the tool results are introduced only in the second stage as external feedback. These results demonstrate that incorporating the second-stage tool prompt yields substantial performance improvement, as the model can jointly exploit its initial recognition and the tools prediction to achieve more accurate grounding. Furthermore, under two rounds of conversations, the third and fourth rows highlight the significance of the rethink stage, which allows the model to further refine the tool outputs, revealing stronger selfcorrection and reasoning capabilities. Self Improvement. Our VG-Refiner is capable of refining arbitrary tool outputs. In this experiment shown in Table 8, we use the reproduced baseline results of Qwen2.5-VL7B as the tool feedback under the PiTER protocols. While the original Qwen2.5-VL-7B fails to improve when provided with its own predictions as prompts, our VG-Refiner successfully identifies and corrects these suboptimal results, leading to consistent performance gains. 5. Conclusion In this paper, we propose tool-refined referring grounded reasoning paradigm to address the hallucination and negative optimization issues caused by unreliable or erroneous tool feedback in tool-integrated reasoning for visual grounding tasks. We introduce VG-Refiner, an agentic reinforcement learning model trained with limited amount of data. By perceiving general visual question answering capabilities, VG-Refiner enhances its refinement ability toward external tool feedback through carefully designed refinement reward and two-stage thinkrethink framework. Our model can serve as foundation for future multi-round toolcalling frameworks, enabling more robust reasoning and fine-grained comprehension of small objects."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 5, 1 [2] Sule Bai, Mingxing Li, Yong Liu, Jing Tang, Haoji Zhang, Lei Sun, Xiangxiang Chu, and Yansong Tang. Univg-r1: Reasoning guided universal visual grounding with reinforcement learning. arXiv preprint arXiv:2505.14231, 2025. 1, 3, 5 [3] Meng Cao, Haoze Zhao, Can Zhang, Xiaojun Chang, Incentivizing Ian Reid, and Xiaodan Liang. Ground-r1: grounded visual reasoning via reinforcement learning. arXiv preprint arXiv:2505.20272, 2025. 2, 3, 5 [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. 6 [5] Qian Chen, Xianyin Zhang, Lifan Guo, Feng Chen, and Chi Zhang. Dianjin-ocr-r1: Enhancing ocr capabilities via reasoning-and-tool interleaved vision-language model. arXiv preprint arXiv:2508.13238, 2025. [6] Ming Dai, Wenxuan Cheng, Jiang-Jiang Liu, Lingfeng Yang, Zhenhua Feng, Wankou Yang, and Jingdong Wang. Improving generalized visual grounding with instance-aware joint IEEE Transactions on Pattern Analysis and Malearning. chine Intelligence, 2025. 3 [7] Siyang Dai, Jun Liu, and Ngai-Man Cheung. Referring exIn Proceedings of the IEEE/CVF Conpression counting. ference on Computer Vision and Pattern Recognition, pages 1698516995, 2024. 2 [8] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. Advances in neural information processing systems, 37:57207 57239, 2024. 5 [9] Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1498714997, 2025. 1 [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633 638, 2025. 1, 4 [11] Qing Jiang, Xingyu Chen, Zhaoyang Zeng, Junzhi Yu, rearXiv preprint and Lei Zhang. ferring via chain-of-thought reasoning. arXiv:2506.04034, 2025. 2 Rex-thinker: Grounded object [12] Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, and Lei Zhang. Detect anything via next point prediction. arXiv preprint arXiv:2510.12798, 2025. 2, 5, 7 [13] Qing Jiang, Lin Wu, Zhaoyang Zeng, Tianhe Ren, Yuda Xiong, Yihao Chen, Liu Qin, and Lei Zhang. Referring to any person. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2166721678, 2025. 2 [14] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [15] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao 9 Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. 6 [16] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 2, 6 [17] Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969, 2025. 1, 2 [18] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou, Junting Pan, Zefeng Li, Van Tu Vu, et al. Groundinggpt: Language enhanced multi-modal grounding model. arXiv preprint arXiv:2401.06071, 2024. [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2 [20] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 2, 6, 7, 1 [21] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 6 [22] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. 6 [23] Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, and Yansong Tang. Universal segmentation at arbitrary granularity with language instruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34593469, 2024. 2 [24] Yong Liu, Zhuoyan Luo, Yicheng Xiao, Yitong Wang, Shuyan Li, Xiu Li, Yujiu Yang, and Yansong Tang. Semantic-assisted object clustering for multi-modal referring video segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [25] Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Unipixel: Unified object referring and segmentation for pixel-level visual reasoning. arXiv preprint arXiv:2509.18094, 2025. 5 [26] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 1, 3, 6 [27] Yuqi Liu, Tianyuan Qu, Zhisheng Zhong, Bohao Peng, Shu Liu, Bei Yu, and Jiaya Jia. Visionreasoner: Unified visual perception and reasoning via reinforcement learning. arXiv preprint arXiv:2505.12081, 2025. 2, 3 [28] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualarXiv preprint rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. 1, 2 [29] Ziyu Liu, Yuhang Zang, Yushan Zou, Zijian Liang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual agentic reinforcement fine-tuning. arXiv preprint arXiv:2505.14246, 2025. 1, 2 [30] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 22632279, 2022. 6 [31] Nicola Pezzotti, Thomas Hollt, Jan Van Gemert, Boudewijn PF Lelieveldt, Elmar Eisemann, and Anna Deepeyes: Progressive visual analytics for Vilanova. IEEE transactions on designing deep neural networks. visualization and computer graphics, 24(1):98108, 2017. 1, 2 [32] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. Cogcom: Train large vision-language models diving into details through chain of manipulations. 2024. 5 [33] Yanyuan Qiao, Chaorui Deng, and Qi Wu. Referring expression comprehension: survey of methods and datasets. IEEE Transactions on Multimedia, 23:44264440, 2020. 2 [34] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance arXiv preprint the edge of open-set object detection. arXiv:2405.10300, 2024. 2 [35] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 2, 6 [36] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pages 12791297, 2025. 6 [37] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. [38] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 1, 2, 3 [39] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. 2 [40] Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian 10 Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999, 2025. 8 [41] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [42] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as foreign language: Beit pretraining for vision and visionlanguage tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19175 19186, 2023. 3 [43] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37:121475121499, 2024. 5 [44] Yiqin Wang, Haoji Zhang, Jingqi Tian, and Yansong Tang. Ponder & press: Advancing visual gui agent towards general computer control. arXiv preprint arXiv:2412.01268, 2024. 1 [45] Ye Wang, Qianglong Chen, Zejun Li, Siyuan Wang, Shijie Guo, Zhirui Zhang, and Zhongyu Wei. Simple o3: Towards interleaved vision-language reasoning. arXiv preprint arXiv:2508.12109, 2025. 2 [46] Yuji Wang, Jingchen Ni, Yong Liu, Chun Yuan, and Yansong Tang. Iterprime: Zero-shot referring image segmentation with iterative grad-cam refinement and primary word emphasis. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 81598168, 2025. 2 [47] Yuji Wang, Haoran Xu, Yong Liu, Jiaze Li, and Yansong Tang. Sam2-love: Segment anything model 2 in languageaided audio-visual scenes. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 28932 28941, 2025. 2 [48] Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025. [49] Mingyuan Wu, Jingcheng Yang, Jize Jiang, Meitang Li, Kaizhuo Yan, Hanchao Yu, Minjia Zhang, Chengxiang Zhai, and Klara Nahrstedt. Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. arXiv preprint arXiv:2505.19255, 2025. 1, 2 [50] xAI. Grok-1.5 vision preview. https://x.ai/news/ grok-1.5v, 2024. 6 [51] Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. 2 [52] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance percepIn Proceedings of tion as object discovery and retrieval. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1532515336, 2023. 5, 7 [53] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1037110381, 2024. 2 [54] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. 2 [55] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1815518165, 2022. 2 [56] Huanjin Yao, Ruifei Zhang, Jiaxing Huang, Jingyi Zhang, Yibo Wang, Bo Fang, Ruolin Zhu, Yongcheng Jing, Shunyu Liu, Guanbin Li, et al. survey on agentic multimodal large language models. arXiv preprint arXiv:2510.10991, 2025. 2 [57] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn European conference on computer vision, pages sions. 6985. Springer, 2016. 2 [58] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Leizhang, Chunyuan Li, et al. Llava-grounding: Grounded visual chat with large multimodal models. In European Conference on Computer Vision, pages 1935. Springer, 2024. 2 [59] Haoji Zhang, Xin Gu, Jiawen Li, Chixiang Ma, Sule Bai, Chubin Zhang, Bowen Zhang, Zhichao Zhou, Dongliang He, and Yansong Tang. Thinking with videos: Multimodal toolaugmented reinforcement learning for long video reasoning. arXiv preprint arXiv:2508.04416, 2025. 1 [60] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, and Xiaojie Jin. Flash-vstream: Efficient realtime understanding for long video streams. arXiv preprint arXiv:2506.23825, 2025. [61] Yuxuan Zhang, Tianheng Cheng, Lianghui Zhu, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Evf-sam: Early vision-language fusion for text-prompted segment anything model. arXiv preprint arXiv:2406.20076, 2024. 3, 5, 6, 7, 1 [62] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. 1 [63] Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv preprint arXiv:2509.01656, 2025. 1, 2, 3, 6 [64] Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, et al. Active-o3: Empowering multimodal large arXiv language models with active perception via grpo. preprint arXiv:2505.21457, 2025. 2 VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning"
        },
        {
            "title": "Supplementary Material",
            "content": "Table 10. Performance comparison of different models under self tool conditions on RefCOCO+ (testA and testB) benchmarks under PiTER. Methods with PiTER RefCOCO+ testA (%) RefCOCO+ testB (%) Acc SRIw CCR Acc SRIw CCR Self Tool Conditions Qwen2.5-VL-7B Qwen2.5-VL-32B VG-Refiner-7B 88.0 88.0 88.8 89.0 7.4 10.4 11.1 10.8 13.5 15.0 74.2 73.5 75.8 75.6 6.8 7.3 8.5 8.6 10.7 10.5 the RefCOCO/+/g test sets, it achieves an average mean accuracy of only around 0.4, indicating that its predictions are unreliable and relatively random, as shown in the Table. Such behavior makes it well-suited to serve as the weak tool in our refiner framework. 7. More Benchmark Results REVPT Benchmark Results. We benchmark the REVPT model [63] using two types of expert tools, following its multi-round tool-calling paradigm. The overall performance remains relatively low, primarily because the model often fails to generate outputs in the required format and the REC model inputs are not sufficiently specific when calling parameters. Moreover, REVPTs predictions tend to closely track the tool outputs rather than refine them, which aligns with our earlier observations. More Results on Self-Improvement Experiments. The prompt tool results used in the self-improvement experiment are generated by our reproduced Qwen2.5-VL-7B, which is required to output in the JSON format. We further evaluate on additional test sets to demonstrate that our VG-Refiner framework can be applied to any tool outputs, even those from the original baseline, while achieving performance comparable to 32B-scale model. Refiner Compliance and Stability Metrics. To assess whether the refiner faithfully preserves correct tool predictions while avoiding harmful degradation, we introduce two metrics: the Follow Correct Rate and the Worsen Rate. All definitions rely on the IoU relationship between the tool prediction IoUi and the refiner output IoUi . We first define the tool-correct set as Sc = (cid:8) IoUi 0.5 (cid:9) . Within this set, the refiner is considered to follow the tool when its IoU remains nearly unchanged, i.e., = (cid:8)i (cid:12) (cid:12) (cid:12) < ϵ, (cid:9) forming the follow set F. In parallel, we define the global worsen set (cid:9) , which includes all samples in = (cid:8) IoUi IoUi (cid:12) IoUi < IoUi Figure 6. (a)The common three errors in the referring expression comprehension task. (b) The output mask of the RES model with expression: lady is walking away from the surfer. Table 9. Evaluation results of visual grounding in ACC@0.5 on RefCOCO, RefCOCO+, and RefCOCOg datasets."
        },
        {
            "title": "RefCOCO",
            "content": "RefCOCO+"
        },
        {
            "title": "RefCOCOg",
            "content": "testA testB testA testB Grounding DINO [20] Qwen2.5-VL-7B [1] REVPT [63]+LLMDET [9] REVPT+EVFSAM [61] VG-Refiner+EVF-SAM 49.9 92.5 77.3 90.1 95.0 37.8 85.4 68.5 75.1 90.7 50.0 89.1 71.1 75.5 92.7 38.7 76.9 59.4 66.6 83. test 54.6 87.2 67.2 75.3 90.6 6. The Choice of Tools Tool Error Category. As shown in Figure 6, the referring expert tool may fail in three ways: (1) wrong object localization, (2) boundary imprecision where the predicted mask has only minor overlap with the target, and (3) missing object localization. For objects that are indeed present in the image, the weak tool often outputs null box because it is not fine-tuned to understand complex referring expressions. In contrast, even strong tool can occasionally localize an incorrect object, as no tool is perfect. RES Model as Strong Tool. We adopt EVF-SAM [61] as the strong referring expression segmentation (RES) tool, given its state-of-the-art performance. Since RES outputs masks, we convert them into bounding boxes by taking the extreme left, top, right, and bottom pixel coordinates. However, in ambiguous scenarios with multiple similar objects, the RES model may produce masks spanning several regions or containing artifacts, shown in Figure 6(b). Such cases lead to boundary imprecision when converting masks to boxes, thereby increasing the difficulty for the refiner model. Weak Tool. We use the non-finetuned Grounding DINO-T as the weak tool, as it struggles with complex and long referring expressions while still performing reasonably well on simple category-name queries. When evaluated on 1 Table 11. Comparison of Refiner 7B and Qwen32B on multiple referring datasets. Dataset VG-Refiner 7B Qwen2.5-VL-32B Acc (%) FCR (%) WR (%) Acc (%) FCR (%) WR (%) RefCOCO testA RefCOCO testB RefCOCOg test RefCOCO+ testA RefCOCO+ testB 95.0 90.7 90.5 92.7 83.0 96.7 96.7 95.7 95.5 94.3 2.2 2.6 2.9 3.4 4. 95.1 90.8 90.6 92.4 83.6 96.7 93.0 91.4 95.0 90.4 2.5 5.0 5.7 3.7 7.2 which the refiner degrades the tool output. The Follow Correct Rate (FCR) measures how reliably the refiner preserves correct tool predictions: FCR = Sc 100%. The Worsen Rate (WR) quantifies the overall fraction of degradations across the entire dataset: WR = 100%, where is the total number of samples. We report the FCR and WR results in Table 11. Compared with Qwen2.5-VL-32B under the PiTER protocol, our VG-Refiner more faithfully follows reliable tool outputs while avoiding further degradation of the tool predictions. 8. Training Details Reward Changes During Training We visualize the reward curves during training in Figure 7. The refinement reward remains consistently above 0.5, indicating that the model effectively corrects incorrect tool outputs and receives the corresponding tool-refinement reward. The tools overall IoU hovers around 0.6, providing an appropriate level of difficulty and contributing to stable training dynamics. The format reward stays near 1 throughout training, demonstrating that the model reliably adheres to the required structured output format. Training Datasets. We construct 9k-sample training set by mixing inference results from EVF-SAM and Grounding DINO T. Specifically, we randomly sample half of the data from EVF-SAM outputs and the other half from Grounding DINO to form balanced mixture. 9. PiTER protocol Analysis PiTER evaluation protocol removes intermediate reasoning, tool-calling, and iterative zooming, which are typically integral components of TiVR methods. However, this design choice is purposeful in the context of our experimental setup. All evaluated Tool-use models are tuned from the same pretrained backbone, Qwen2.5-VL-7B, meaning Figure 7. The reward changes with the training steps. We show the mean value across the batch. that their foundational visuallanguage capabilities and architectural priors are identical. The objective of PiTER is therefore not to compare full agentic pipelines, but to isolate the incremental refinement ability contributed by each fine-tuning strategy beyond the shared base model. Allowing each method to operate with its native multi-step interaction style would introduce confounding factors, as performance would reflect differences in external scaffolding rather than the refinement competence learned by the model 2 Figure 8. More visualization results. itself. By enforcing single-stage, tool-injected inference format, PiTER normalizes the interaction interface across all methods and evaluates their intrinsic ability to utilize imperfect external tool outputs. This uniform setting ensures that any performance differences stem from the models learned refinement mechanisms rather than disparities in procedural reasoning frameworks. 10. More Case Study Figure 8 presents additional examples where our VGRefiner successfully corrects tool failures. Most of these cases originate from the non-finetuned Grounding DINO T, which exhibits relatively weak capability in modeling spatial relationships and objectattribute associations."
        }
    ],
    "affiliations": [
        "International Digital Economy Academy (IDEA)",
        "Tsinghua Shenzhen International Graduate School, Tsinghua University"
    ]
}