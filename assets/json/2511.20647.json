{
    "paper_title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
    "authors": [
        "Tahira Kazimi",
        "Connor Dunlop",
        "Pinar Yanardag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 7 4 6 0 2 . 1 1 5 2 : r Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization Tahira Kazimi Connor Dunlop Pinar Yanardag Virginia Tech {tahirakazimi, cdunlop, pinary}@vt.edu diverse-video.github.io Figure 1. Given an input prompt, DPP-GRPO enables generation of diverse sets of videos spanning cinematic factors such as camera motion or scene layout. We formulate diversity as set-level policy optimization with DPP-based diminishing-returns term to suppress similar videos as the set grows. For brevity, we display only the first frame of each clip; see the supplement for frameby-frame comparisons and videos. We note that our results are uncurated and shown in the exact order produced by our method."
        },
        {
            "title": "Abstract",
            "content": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from single text prompt. We tackle this challenge by formulating it as set-level policy optimization problem, with the goal of training policy that can cover the diverse range of plausible outcomes for given prompt. To address this, we introduce DPPGRPO, novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and new benchmark dataset of 30,000 diverse prompts to support future research. 1. Introduction Text-to-video diffusion models have rapidly improved, enabling the generation of visually coherent and high-fidelity short videos across range of applications including entertainment, education, advertising, and social media content creation [6, 22, 34, 39]. However, despite these advances, current video generators often suffer from limited diversity in their outputs. They tend to produce videos that conform to narrow distribution of styles or scenarios. For instance, although model may generate accurate scenes of giraffe bending to sip water from sunlit Savannah pool (see Fig. 1), it repeatedly favors similar types of scenes and motions, 1 overlooking many other plausible variations. The lack of diversity in generative outputs has been long-standing issue, especially in image synthesis research where output distributions collapse toward few modes [16, 33]. Prior efforts tackled diversity with strategies such as entropy-based sampling [13], datasetcoverage objectives [4], group sampling [25], or noiseinjection schemes [28]. However, these techniques are largely tailored to static imagery and face key limitations in video: many rely on expensive test-time optimization and memory-intensive caches of prior latents [13] which is ill-suited for video. On the other hand, methods that require access to the full training set [4] or mandating architectural changes [25] are infeasible due to significant computational overhead. Moreover, these methods overlook challenges unique to video. Beyond visual appearance, generated videos must vary along temporal and cinematic dimensions: such as object motion, camera movement, scene structure. As result, users often rely on prompt engineering, and exhaustive sweeps of seeds/guidance settings to find diverse generations. While these strategies can surface occasional diversity, they require substantial time and compute costs with inconsistent gains. How can we generate set of outputs that varies factors such as motion, scene composition, and camera language while remaining faithful to the input prompt? In this paper, we approach this problem as set-level policy optimization task and introduce DPP-GRPO, diversity-aware framework that couples DPP [17, 21] with GRPO [31] objective. Our DPP component injects diminishing returns, explicitly discounting similar samples so the first instance of choice (e.g., dolly shot) is rewarded while redundant variants add diminishing returns. Then GRPO supplies groupwise feedback over batch of candidates to prefer sets that jointly span semantically diverse dimensions in user input. Our method is model-agnostic and plug-and-play: it can be used in open-source (e.g., WAN, CogVideoX) or blackbox models (e.g., Veo) to enable generation of faithful, high-quality videos with significantly improved diversity. Our contributions are as follows. We formulate diverse video generation as set-level optimization problem and introduce novel policy optimization framework that combines GRPOs groupwise optimization with DPP-based diminishingreturns diversity objective, enabling policy that generates semantically faithful yet non-redundant prompt sets. To our knowledge, this is the first work tackling diversity problem in video generation. Our method is model agnostic and plug-and-play, and requires no architectural changes and applies to open models (Wan, CogVideoX) and to black-box APIs (e.g., Veo). Figure 2. Frame-level examples illustrating motion, subject, and camera diversity achieved by DPP-GRPO. Given the same input prompt, our method generates videos that vary in subject appearance, environment, and camera movement while maintaining semantic consistency with the original prompt. Across standard T2V benchmarks and human studies, we show consistent improvements in set diversity while preserving fidelity and temporal coherence. We release curated dataset of 30K diverse promptvariant pairs designed specifically for diverse video generation, providing the first benchmark resource for this emerging problem. 2. Related Work Diversity in Generative Models Balancing fidelity and diversity is an important challenge in generative modeling. In diffusion models, guidance mechanisms like classifier-free guidance (CFG) [9, 29] improve prompt alignment but reduce sampling stochasticity, often supIn the image domain, several pressing diversity [13]. strategies have been proposed; [30] sample from lowdensity regions to encourage diversity, but their method operates in pixel space and does not transfer to latent diffusion. Reinforcement learningbased methods [23] optimize for diversity via reward signals, yet require full retraining. Several other works [2, 12, 13] incorporate diversity-aware objectives into generation but depend on internal model access, limiting scalability and deployment. DreamDistribution [40] promotes variation by learning composite prompt embeddings from few-shot examples, while [4] formulate it as dataset coverage problem requiring training data access. To the best of our knowledge, no prior work directly targets diversity in video generation, setting that introduces challenges unique to the video domain. RL-based Video Generation Prior work adapts preference learning to video along several axes. VideoDPO [20] extends DPO to text-to-video diffusion with an omni-preference objective balancing visual 2 an L-ensemble if, for any subset Y, Pr(Y = S) det(LS), where LS is the principal submatrix of indexed by elements in S. If the entries of the matrix are interpreted as similarity scores (e.g., cosine similarity) between elements in the ground set, then the determinant det(LS) can be viewed as the squared volume spanned by the feature vectors of the elements in subset S. This volume increases when the vectors are diverse (i.e., linearly independent) and shrinks when they are similar or redundant. Hence, DPPs naturally favor subsets with high internal diversity. Group Relative Policy Optimization GRPO [31] is value-free reinforcement learning framework that computes advantages through group-based normalization of sampled outputs rewards, eliminating the need for separate critic network. Given query q, the method samples responses p1, . . . , pG from the old policy πθold and updates the current policy πθ to maximize: JGRPO(θ) = Ea,{oi} (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 min (cid:18) πθ(oiq) πθold(oiq) Ai, clip (cid:18) πθ(oiq) πθold (oiq) (cid:19) (cid:19) , 1ϵ, 1+ϵ Ai (cid:35) βDKL(πθπref) (1)"
        },
        {
            "title": "Advantages Ai are computed by normalizing rewards",
            "content": "ri within the group: Ai = ri mean(r1:G) std(r1:G) (2) Standard GRPO (Eq. 1) objective optimizes policies to maximize expected reward, with advantage normalization encouraging high probability on sample with maximum reward. While effective for quality optimization, this can lead to diversity collapse where policies consistently generate similar high-reward outputs rather than exploring diverse valid responses. 4. Methodology Given user prompt q, T2V model G, and desired output size K, our goal is to generate set of prompts Pq = {p1, . . . , pK} whose corresponding videos Vq = {G(pi)}K i=1 are diverse yet faithful to the intent of q. We approach this through set-level policy optimization, where each generated response is evaluated based on the additional diversity it contributes to the partially constructed set, encouraging exploration of complementary variations while avoiding redundancy. To accomplish Figure 3. Framework Overview. The model generates group of candidates (C1, . . . , CG) for each prompt. Each candidate is scored by composite reward combining DPP marginal gain (S Ci) and relevance Rrel(Ci). Groupwise normalization produces advantages (Ai), which update the policy under the DPPGRPO objective. quality and text relevance. Flow-DPO [19] aligns flowmatching video generators using multi-dimensional reward, improving motion smoothness and prompt adherence over SFT and alternative RLHF variants. More fine-grained supervision has also been explored via DenseDPO [36], which targets temporal locality by densifying video preferences across time. Complementary directions include GRPO variants tailored to video control, such as DanceGRPO [37] for learning temporally coherent, motion-centric policies from groupwise relative feedback. Prompt-a-Video [15] explores prompt-level control and conditioning strategies for textto-video models. However, none of these methods address diversity of generated outputs in videos which is the main focus of our work. 3. Background Determinantal Point Processes We quantify the diversity of set of generated responses using concepts DPPs, which are well-suited for modeling diversity and negative correlation in subsets [17]. In particular, we focus on the L-ensemble formulation [3, 17], subclass of DPPs widely used in machine learning. Definition 3.1. Let = {1, 2, . . . , } denote finite ground set, and let be random subset. Suppose RN is real symmetric positive semidefinite matrix. Then defines DPP in the form of 3 this, we introduce DPP-GRPO, model-agnostic alignment framework that trains policy using diversityaware DPP [17, 21] reward in combination with the GRPO objective [31]. Our DPP component enforces diminishing-returns property: the first appearance of novel factor (e.g., dolly shot) receives higher reward, while subsequent redundant variants yield progressively smaller gains. Then, GRPO supplies group-wise relative feedback over batch of candidates, guiding the policy toward sets that jointly span diverse dimensions such as camera motion or scene layout. While our method could be applied inside T2V generator such as by training the model to diversify outputs according to our objective, we instead optimize prompt-based policy for three practical reasons: (i) efficiency since we do not perform backprop through video sampling, which yields significantly shorter training time, (ii) cinematic expressivity since factors such as camera motion, scene composition are naturally controllable via prompt tokens, and (iii) plug-and-play deployment since the prompts generated by our method can work with open-source and black-box models without architectural changes or access to latents/gradients. 4.1. DPP-GRPO DPP-GRPO balances two competing objectives: (1) maximizing diversity, and (2) maintaining semantic alignment with the users input prompt. We formulate diverse video generation as set-level optimization problem: given target set size K, our goal is to generate set of video prompts that captures wide range of cinematic variations including motion, composition, and perspective which can then be used with any T2V generator. To quantify and promote diversity within this set, we incorporate DPPs, which has been shown to be more effective than simple pairwise distance metrics in modeling diverse subsets [24]. Following Def. 3.1, we define diversity of set p1:k as: Div(p1:k) = log det(Lϕ(p1:k) + I), (3) where Lϕ[pi, pj] = (ϕ(pi), ϕ(pj)), is kernel function and ϕ(.) is selected embedding model [27] which can map response into high-dimensional semantic space. We define to be normalized cosine similarity and regularize Lϕ + I, to avoid singular matrices, following [17]. The log-determinant measures the log-volume spanned by {ϕ(p1), . . . , ϕ(pk)}, increasing when prompts are diverse and decreasing when overly similar. According to GRPO training, we sample responses per query and assign each pi marginal diversity gain relative to reference set Rq: (pi Rq) = Div(Rq {pi}) Div(Rq) (4) Formally, response pi has higher marginal gain if pi spans dimension not covered by elements in Rq, increasing the volume spanned by existing set members. Here, Rq consists of curated ground-truth variants for that capture distinct semantic and cinematic modes (e.g., subject changes, camera motions, or scene layouts). Each pair ei, ej Rq is both semantically aligned with the query and mutually diverse, so optimizing marginal gains relative to Rq teaches the policy to distribute candidates across these modes rather than collapsing onto single one. As result, the final diversityaware reward signal takes the final closed-form, (pi Rq) = log det(Lϕ(Rq{pi}))log det(Lϕ(Rq)) (5) which is used by GRPO objective (+I term is omitted for simplicity in notation). Relevance. To ensure that generated prompts remain semantically faithful to the users intent, we define relevance term in our reward mechanism that encourages alignment with both the user-provided query and elements in Rq. For instance, given the input dog is playing with ball at the beach, high-quality variant like dolly shot of poodle is playing with red ball at the beach should receive high relevance score, while trivial copies (e.g., dog is playing near the beach with ball) or unrelated variants should be penalized. We compute this via considering two similarities: one between the generated prompt pi and the user query q, and another between pi and elements in Rq: Rrel = 1 Rq (cid:88) gRq cos(ϕ(pi), ϕ(q)) cos(ϕ(pi), ϕ(g)) (6) This formulation enforces joint constraint: pi must maintain high similarity to both the original query and valid variations. Our final objective is then composite reward mechanism in training that balances diversity and semantic alignment: R(p q, g) = λdiv(pi Rq) + λrelRrel (7) where λdiv and λrel are weighting coefficients. Each component targets specific failure mode: prevents mode collapse through diversity enforcement, while Rrel maintains semantic fidelity to the original query and ground-truth samples. Training. We follow two-stage post-training by applying supervised finetuning to initialize the policy model before reinforcement learning. The finetuning corpus = {(x, y)} consists of user prompts paired with diverse variants generated via chain-of-thought prompting [35] (see details below). Finetuning is performed for up to 50 iterations. After warm start, following GRPO [31], given user prompt q, we sample responses p1, . . . , pG from the old policy πθold and update the current policy πθ to maximize the GRPO objective (Eq. 1). Each training sample Di = (s, qi, Rq) contains system prompt s, user query qi, and the reference set Rq. Rewards for sampled outputs p1, . . . , pG are computed using Eq. 7 and used to update πθ via Eq. 1. Through this process, the policy learns transferable rule: how to propose new candidate that expands the semantic coverage of an existing reference set while remaining faithful to the original query. An ablation on reference set Rq is provided in SM. Inference. Given an input prompt and target output size K, we initialize an empty reference set Rq = . The first sample p1 is generated conditioned only on (s, q) and added to the reference set. In subsequent steps, we iteratively provide (s, q, Rq) to generate p2, p3, . . ., expanding Rq with each new variant until prompts are produced. This autoregressive rollout mirrors the structure used during training: each new sample is evaluated relative to the partially constructed set, allowing the learned diversification strategy to operate without requiring access to the curated reference sets used during training. Dataset. We curate diverse video-prompt dataset using GPT-5-nano [1]. Data construction proceeds in two stages: (1) we generate 3K base prompts using GPT5-nano, and (2) for each base prompt, rather than relying on simple expansion prompts, we design an agentic pipeline that iteratively refines diversity. An architect agent proposes diverse variations, while critic agent evaluates each candidate using video-level diversity and quality metrics (TIE, TCE, CLIP), ensuring that resulting prompts are grounded in video behavior rather than text alone. We synthesize 10 semantically diverse variations per base prompt, yielding 30K samples in total. 5. Experiments We showcase the effectiveness of DPP-GRPO on two state-of-the-art T2V models Wan2.1 [34] and CogVideoX [39], as well as black-box T2V model VEO3 [6]1. Since no existing method directly addresses diverse video set generation, we benchmark against RL-driven prompt optimization baselines Promptist [7], Prompt-A-Video [14]) and GPT-5. Experimental Setup All experiments are conducted on four NVIDIA L40S GPUs. We first perform supervised fine-tuning for 50 iterations, followed by GRPO training for approximately 1,200 iterations. The learning rate is set to 2 105 for supervised fine-tuning 1Since VEO3 is prohibitively expensive (on fal.ai, it costs $3,000 to produce 1,000 videos required by our quantitative analysis), our evaluation on Veo3 is restricted to qualitative comparisons. and 2 107 during GRPO optimization, with λdiv = λrel = 0.5 by default. Our dataset comprises 30K user querysample pairs designed to capture diverse generative outcomes. For video generation, we utilize Wan2.1 [34] and CogVideoX [39] as our backbone models, and Qwen2-7b-Instruct [38] as our alignment model. Unless otherwise specified, we adopt guidance scale of 6, with 40 inference steps and 81 output frames per video. Qualitative Experiments We present qualitative results across range of prompts, comparing DPP-GRPO to video baselines, CogVideoX [39] and Wan [34]. Because our method optimizes set-level diversity with DPP-based diminishing-returns objective, it directly produces diverse sets without post-hoc cherry-picking. All examples are shown uncurated and in the exact generation order, demonstrating that our outputs are diverse by design. Fig.1 shows comparison between our method and Wan model, where our method consistently yields sets with substantial variation in factors such as scene composition while preserving semantic fidelity to the prompt (please refer to SM for frame-by-frame comparisons, high-quality videos, and prompts associated with each sample.). Moreover, we provide qualitative comparisons across multiple models, including Wan, CogVideoX, Promptist, and Prompt-A-Video (see Fig. 4). We highlight two failure modes that other methods suffer: homogeneous output generation and weak semantic grounding. For instance, in the abstract prompt fantasy landscape, base models and prompt optimization methods produce either repetitive or non-fantastical nature scenes, demonstrating both overly-similar outputs and poor text alignment. In contrast, our method generates diverse outputs, spanning mystical forests, and supernatural landscapes, that both are visually diverse and better align with the prompts intent. Additionally, our method demonstrates subject-level diversity: lantern example demonstrates varied lantern types, weather conditions and locations, while other methods show limited diversity in these areas and do not align with the given prompt correctly. Furthermore, we can see diverse human subjects across age, gender, and ethnicity in kayaking scenes (please refer to Fig.4 ). Furthermore, DPP-GRPO diversifies across features unique to video models (e.g., motion and camera types). Fig. 2 demonstrates varied skateboarding maneuvers and varied camera movements, while Fig. 4 shows diverse perspectives including aerial views, panning, and closeups across kayaking, coffee, and cat scenes. Importantly, increased diversity does not compromise semantic fidelity: all generated videos accurately depict the core subjects and actions of the input prompt. This demonstrates that DPP-GRPO balances exploration and relevance while maintaining user intent. 5 Figure 4. Qualitative Comparison. DPP-GRPO diversifies several cinematic factors such as subject, scene, motion, and cameraview diversity while preserving the prompt alignment and achieves more diverse and semantically faithful videos compared to baselines. (a) For clarity, we provide the first frames of each video (b) Detailed frame-by-frame comparisons of the same videos are given (please kindly zoom-in for details). Please visit our SM for more comparisons and high-quality videos."
        },
        {
            "title": "TIE VENDI CLIP VQ TC DD TVA FC",
            "content": "SC BC"
        },
        {
            "title": "VBench\nIQ",
            "content": "AQ MS DD 39.7 19.79 37.58 16.71 29.27 15.72 24.10 31.95 49.09 Wan2.1 Original prompts 19.76 Promptist GPT-5 Prompt-A-Video Ours CogVideoX Original prompts 22.21 40.33 23.45 41.67 Promptist 17.38 24.80 GPT-5 17.25 25.78 Prompt-A-Video 27.59 45.46 Ours 9.2 8.26 8.15 8.45 11. 8.10 8.39 8.84 7.62 10.30 2.83 2.76 3.00 0.28 0.29 2.81 2.71 2.96 0.305 2.69 2.56 2.81 0.304 2.74 2.59 2.95 0.311 3.37 3.32 3.55 0.292 3.59 3.52 3.73 0.294 3.60 3.54 3.70 0.296 2.76 2.64 2.87 0.300 2.86 2.93 2.70 0.310 4.08 4.14 3.78 2.81 2.78 2.62 2.78 3.27 3.37 3.34 2.50 2.72 3.42 2.59 0.973 0.961 0.611 0.692 2.54 0.975 0.966 0.621 0.691 2.35 0.971 0.973 0.634 0.691 2.39 0.977 0.966 0.652 0.703 3.22 0.976 0.966 0.655 0. 0.392 0.991 0.992 0.373 0.992 0.385 0.990 0.369 0.992 0.387 3.45 0.961 0.961 0.528 0.626 3.48 0.954 0.955 0.524 0.591 2.53 0.964 0.961 0.561 0.646 2.76 0.963 0.971 0.651 0.734 4.05 0.964 0.979 0.635 0.717 0.990 0.990 0.985 0.987 0.991 0.510 0.493 0.295 0.433 0.495 Table 1. Quantitative comparison of our framework with baseline T2V models under two model families (Wan2.1 and CogVideoX). Quantitative Experiments We evaluate our method against 200 prompts sampled from the VBench [10] dataset, with 20 videos generated per prompt, resulting in 4,000 videos per method. Since there are currently no works directly targeting diverse video generation, we include several representative baselines that enhance generation quality in both image and video domains [7, 14] as well as GPT-5 [1]. We assess diversity, alignment, and visual quality across wide range of standard metrics. For diversity, we employ three complementary measures: Truncated CLIP Entropy (TCE) [11] captures semantic variation across samples, Truncated Inception Entropy (TIE) [11] measures perceptual diversity in visual attributes (texture, color, lighting), and VENDI [5] quantifies distributional diversity via eigenvalue entropy. Embeddings for TCE/TIE/VENDI are computed by sampling 8 uniformly spaced frames per video where metrics are calculated on individual frames and averaged. For generation quality, we use VideoScore [8] and VBench [10] metrics. Video Diversity and Quality. Our method substantially outperforms all diversity baselines across both models (see Table 1). On Wan2.1, we compare DPP-GRPO with alignment methods, Promptist [18] and Prompt-A-Video [15] as well as GPT-5 [1]. we achieve substantial improvement on diversity metrics (TCE, TIE, and VENDI) over all compared methods. Similarly, on CogVideoX, diversity metrics demonstrate significant gain in our method. Beyond diversity metrics, our approach yields competitive performance on VideoScore and VBench, indicating that increased diversity does not come at the cost of perceptual quality or fidelity. VBench results show that semantic consistency and appearance quality remain stable or slightly improved. Similar trends appear for CogVideoX, where our method achieves the strongest overall VideoScore while maintaining high semantic consistency. These results demonstrate that Figure 5. Visual comparison of T2V generations from various base models, with and without our method. For each baseline (CogVideoX, Wan, VEO), we show two generated videos and their representative frames. Our method enhances the diversity and quality of the generated videos across integrated models. diversity-oriented optimization can be achieved without degrading visual or temporal coherence. Text Alignment. DPP-GRPO improves CLIP alignment alongside diversity  (Table 1)  . CLIP scores in both CogVideoX and Wan2.1 improves compared to base models. VideoScores TVA metric similarly improves as result of applying our method. Computational Efficiency. We measure average inference time per video across methods in Table 2 where we report inference time and overhead relative to base models: Wan2.1 (85.48s baseline) for video methods and SDXL [26] (42.2s baseline) for image methods. Our method achieves 0.58s inference time with only"
        },
        {
            "title": "Method",
            "content": "Inference (s) Overhead SDXL (Base) SPARKE Wan2.1 (Base) Promptist Prompt-A-Video GPT-5 Ours 42.2 +5.25 85.48 +0.51 +11.02 +22.42 +0.58 +12.4% +0.60% +12% +26% +0.67% Table 2. Computational efficiency comparison. Time per video in seconds. Our method achieves minimal overhead (+0.67%)."
        },
        {
            "title": "Diversity Alignment",
            "content": "CogVideoX Prompt-A-Video Promptist Wan2.1 Ours 2.55 2.70 3.02 3.10 4.07 3.75 3.83 3.97 3.80 4.28 Table 3. Our method achieves the highest ratings for both diversity and alignment in human evaluation. 0.67% overhead, second only to Promptist [18] (0.51s, 0.60% overhead). Test-time optimization methods incur significantly higher costs: SPARKE [13], an imagebased method built on SDXL, adds 12% overhead per image due to iterative optimization. API-based methods also suffer from high latency: GPT-5 requires 22.42s, while Prompt-A-Video [15], which uses Llama3-Instruct models (8B-70B parameters), takes 11.02s. These results confirm that our approach achieves nearbaseline efficiency. User Study. We ran randomized user study to evaluate perceived set-level diversity and prompt alignment with participants recruited from Prolific.com. For each of 10 prompts, participants viewed one set per method composed of 4 clips, with method labels hidden and both prompt and video orders counterbalanced to mitigate position and fatigue effects. After viewing each set, participants rated Diversity and Alignment on 5-point Likert scales. DPP-GRPO received significantly higher Diversity scores than all baselines, indicating broader setlevel variety. It also achieved higher Alignment on average, while competing methods performed closely, as expected given their strong prompt adherence. Please refer to SM for more details. 5.1. Ablation Studies. We conduct series of ablations to analyze the contribution of different components of DPP-GRPO and validate the robustness of our design choices. Ablation on different backbones To evaluate modelagnostic performance, we ablate across two state-of-theart text-to-video backbones: Wan2.1 and CogVideoX. As shown in Table 4, DPP-GRPO consistently improves Figure 6. CFG ablation. Diversity (TCE/TIE) and fidelity (CLIP) across different CFG values for Wan and our DPP GRPO model. both semantic and visual diversity metrics across architectures, demonstrating that our framework generalizes well regardless of the underlying diffusion model. The improvements are particularly notable in metrics such as TCE, TIE, and VENDI, confirming that our diversity modeling is not tied to any specific video model. Moreover, Fig. 5 presents side-by-side comparison applying our method to Wan, CogVideoX, and Veo 3. As can be seen from the visuals, the results demonstrate diverse outputs across various backbones. Ablation on reward terms We further analyze the impact of individual reward components by isolating the diversity and relevance terms in our composite objective in Eq. 7. Using only the relevance term yields higher CLIP alignment scores, as expected, yet significantly reduces diversity, often collapsing to repetitive outputs similar to base models. Conversely, using only DPP-based diversity term increases variation across motion and scene attributes but leads to modest decline in textvideo consistency. Our full model, which balances both terms, achieves the best balance between semantic fidelity and diversity, improving across both dimensions simultaneously. This confirms that the dual-term reward formulation is essential for diverse video generation. Ablation on generation set size We analyze the impact of the generation set size during inference, with results shown in Fig. 7. Figure plots the running TIE, TCE, and average CLIP scores as the set of generated videos grows, comparing DPP-GRPO to the Wan baseline. The diversity plots reveal two key findings: Our method consistently achieves higher semantic (TCE) and perceptual (TIE) diversity scores than the baseline at every set size. While both methods exhibit diminishing returns, where the marginal diversity gain from each new video"
        },
        {
            "title": "TIE VENDI CLIP VQ TC DD TVA FC",
            "content": "SC BC"
        },
        {
            "title": "VBench\nIQ",
            "content": "AQ MS DD Wan2.1 Ours (SFT) 14.05 17.13 Ours (only relevance) 20.06 35.66 27.05 45.21 Ours (only DPP) 31.95 49.09 Ours (full) 11.05 8.87 11.72 11.29 0.251 2.83 2.72 2.99 0.283 2.60 2.55 2.85 0.255 2.74 2.58 2.86 0.311 3.37 3.32 3. 2.79 2.61 2.92 3.27 2.56 0.962 0.960 0.623 0.700 2.45 0.971 0.964 0.596 0.702 2.15 0.961 0.972 0.597 0.705 3.22 0.976 0.966 0.655 0.722 0.989 0.266 0.981 0.247 0.247 0.984 0.992 0.387 Table 4. Ablation studies conducted with Wan backbone over different reward terms, supervised-finetuned version, and our full complete model Figure 7. Impact of generation set size on diversity and alignment. The plots compare our DPP-GRPO method against the Wan baseline on (Left) Average CLIP alignment, (Middle) semantic diversity (TCE), and (Right) perceptual diversity (TIE). DPP-GRPO consistently outperforms the baseline in both alignment and diversity as the set grows. larity scores. Both models exhibit the expected behavior in which CLIP scores peak at moderate guidance scales (CFG 6-8). Importantly, our method maintains fidelity comparable to the baseline across all settings, and achieves the highest CLIP score at CFG = 6. 6. Discussion Limitations and Broader Impact Our approach inherits base model limitations in complex temporal dynamics as temporal dynamics are governed by the models temporal attention values rather than prior conditioning [32]. Figure 8 shows when the base model struggles with fine-grained motions in complex actions (e.g., peeling actions), our method also struggles in overcoming this constraint, Nevertheless, our work aims to make T2V generation diverse by design, reducing the need for compute-intensive trial-and-error. This can reduce operational cost and improve accessibility for creators or small teams that lack large compute budgets. Conclusion We introduced DPP-GRPO, set-level policy optimization framework that treats diversity as an explicit optimization objective for text-to-video generation. Our method combines group-relative policy optimization with DPP-based diminishing-returns term, encouraging complementary coverage across cinematographic and semantic factors while preserving prompt fidelity. Our approach is model-agnostic and plug-andplay; operating in prompt space to work with both open models (Wan, CogVideoX) and black-box systems (e.g., Veo) and directly yields diverse sets without post-hoc re-ranking or cherry-picking. Experiments on several benchmarks, human studies, and qualitative comparisons demonstrate consistent gains in set-level diversity with equal or better alignment and perceptual quality. Figure 8. An example failure case of our method where the temporal video quality depends on the base models ability. decreases as the set expands, the baseline models diversity plateaus much earlier. In contrast, DPP-GRPO continues to discover varied outputs, achieving significantly higher overall diversity. Diversity gain do not compromise prompt alignment. The CLIP Alignment plot shows that DPP-GRPO maintains stable average CLIP score compared to the Wan baseline. As result, our method successfully expands diversity while simultaneously maintaining its semantic fidelity. Effect of classifier-free guidance Figure 6 presents an ablation study measuring the effect of classifier-free guidance (CFG) values on both diversity and fidelity for the baseline Wan model and our DPP-GRPO method. We sweep the CFG scale over {2, 4, 6, 8, 10, 12} and report (1) the joint behavior of TCE and TIE, and (2) CLIP alignment scores. The upper plot in Figure 6 shows that our method consistently achieves higher TCE and TIE across all CFG values. While both models exhibit non-monotonic trajectories as CFG increases, the DPP-GRPO frontier shifts upward in the TCE-TIE plane, demonstrating that our objective is robust to abrupt CFG adjustments. This indicates that our method remains effective under wide range of decoding hyperparameters and does not depend on carefully tuned CFG settings. The lower plot in Figure 6 reports CLIP simi-"
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 5, 7 [2] Reyhane Askari Hemmat, Melissa Hall, Alicia Sun, Candace Ross, Michal Drozdzal, and Adriana RomeroSoriano. Improving geo-diversity of generated images In Eurowith contextualized vendi score guidance. pean Conference on Computer Vision, pages 213229. Springer, 2024. 2 [3] Alexei Borodin and Eric Rains. Eynardmehta theorem, schur process, and their pfaffian analogs. Journal of statistical physics, 121(3):291317, 2005. 3 [4] Mischa Dombrowski, Weitong Zhang, Sarah Cechnicka, Image generHadrien Reynaud, and Bernhard Kainz. In Proation diversity issues and how to tame them. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 30293039, 2025. 2 [5] Dan Friedman and Adji Bousso Dieng. The vendi score: diversity evaluation metric for machine learning. arXiv preprint arXiv:2210.02410, 2022. 7 [6] Google DeepMind. Veo: text-to-video generation system (veo 3 tech report). https://storage. googleapis . com / deepmind - media / veo / Veo-3-Tech-Report.pdf, 2025. Accessed: 202511-12. 1, [7] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. Advances in Neural Information Processing Systems, 36:66923 66939, 2023. 5, 7 [8] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained huarXiv preprint man feedback for video generation. arXiv:2406.15252, 2024. 7 [9] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [10] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21807 21818, 2024. 7 [11] Francisco Ibarrola and Kazjon Grace. Measuring diversity in co-creative image generation. arXiv preprint arXiv:2403.13826, 2024. [12] Mohammad Jalali, Azim Ospanov, Amin Gohari, and Farzan Farnia. Conditional vendi score: An information-theoretic approach to diversity evaluation arXiv preprint of prompt-based generative models. arXiv:2411.02817, 2024. 2 [13] Mohammad Jalali, Haoyu Lei, Amin Gohari, and Farzan Farnia. Sparke: Scalable prompt-aware diversity guidance in diffusion models via rke score. arXiv preprint arXiv:2506.10173, 2025. 2, 8 [14] Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian Ge, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, et al. Prompt-a-video: Prompt your video diffusion model via preference-aligned llm. arXiv preprint arXiv:2412.15156, 2024. 5, 7 [15] Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian Ge, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, et al. Prompt-a-video: Prompt your video diffusion model via preference-aligned llm. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1872518735, 2025. 3, 7, 8 [16] Youssef Kossale, Mohammed Airaj, and Aziz Darouichi. Mode collapse in generative adversarial networks: An overview. In 2022 8th International Conference on Optimization and Applications (ICOA), pages 16. IEEE, 2022. 2 [17] Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 5(23):123286, 2012. 2, 3, 4 [18] WeiJie Li, Jin Wang, and Xuejie Zhang. Promptist: Automated prompt optimization for text-to-image syntheIn CCF international conference on natural lansis. guage processing and Chinese computing, pages 295 306. Springer, 2024. 7, [19] Jie Liu, Gongye Liu, Jiajun Liang, Ziyang Yuan, Xiaokun Liu, Mingwu Zheng, Xiele Wu, Qiulin Wang, Improving video Menghan Xia, Xintao Wang, et al. arXiv preprint generation with human feedback. arXiv:2501.13918, 2025. 3 [20] Runtao Liu, Haoyu Wu, Ziqiang Zheng, Chen Wei, Yingqing He, Renjie Pi, and Qifeng Chen. Videodpo: Omni-preference alignment for video diffusion generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 80098019, 2025. 2 [21] Odile Macchi. The coincidence approach to stochastic point processes. Advances in Applied Probability, 7(1): 83122, 1975. 2, 4 [22] Andrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren, and Helge Ritter. Video diffusion models: survey. arXiv preprint arXiv:2405.03150, 2024. 1 [23] Zichen Miao, Jiang Wang, Ze Wang, Zhengyuan Yang, Lijuan Wang, Qiang Qiu, and Zicheng Liu. Training diffusion models towards diverse image generation with reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1084410853, 2024. 2 [24] Jack Parker-Holder, Aldo Pacchiano, Krzysztof Choromanski, and Stephen Roberts. Effective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33:18050 18062, 2020. 4 [25] Gaurav Parmar, Or Patashnik, Daniil Ostashev, KuanChieh Wang, Kfir Aberman, Srinivasa Narasimhan, Scaling group inference for diand Jun-Yan Zhu. arXiv preprint verse and high-quality generation. arXiv:2508.15773, 2025. 2 [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 5 [39] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 5 [40] Brian Nlong Zhao, Yuhang Xiao, Jiashu Xu, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet, and Yunhao Ge. Dreamdistribution: Prompt distribution learning for text-to-image diffusion models. 2023. 2 [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 7 [27] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. 4 [28] Seyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann Weber. Cads: Unleashing the diversity of diffusion models through conditionannealed sampling. arXiv preprint arXiv:2310.17347, 2023. 2 [29] Seyedmorteza Sadat, Manuel Kansy, Otmar Hilliges, and Romann Weber. No training, no problem: Rethinking classifier-free guidance for diffusion models. arXiv preprint arXiv:2407.02687, 2024. [30] Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, and Cristian Canton. Generating high fidelity data from low-density regions using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1149211501, 2022. 2 [31] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 3, 4, 5 [32] Ariel Shaulov, Itay Hazan, Lior Wolf, and Hila Flowmo: Variance-based flow guidance for arXiv preprint Chefer. coherent motion in video generation. arXiv:2506.01144, 2025. 9 [33] Hoang Thanh-Tung and Truyen Tran. Catastrophic forgetting and mode collapse in gans. In 2020 international joint conference on neural networks (ijcnn), pages 110. IEEE, 2020. [34] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced arXiv preprint large-scale video generative models. arXiv:2503.20314, 2025. 1, 5 [35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 4 [36] Ziyi Wu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ashkan Mirzaei, Igor Gilitschenski, Sergey Tulyakov, and Aliaksandr Siarohin. Densedpo: Fine-grained temporal preference optimization for video diffusion models. arXiv preprint arXiv:2506.03517, 2025. 3 [37] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 3 11 Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Additional Qualitative Examples Please refer to the end of supplementary webpage (.html file) or the supplementary folder for all the videos in our main paper, as well as additional videos."
        },
        {
            "title": "CLIP",
            "content": "2 5 8 10 15.131 16.570 16.959 15.546 22.939 23.936 24.464 23.040 0.308 0.308 0.306 0.306 Table 5. Ablation on reference set size Rq. Small multireference sets (58 examples) yield the best diversity, while larger sets show diminishing returns. (λdiv, λrel)"
        },
        {
            "title": "CLIP",
            "content": "(0.9, 0.1) (0.5, 0.5) (0.1, 0.9) 16.910 16.709 16.002 24.256 23.945 23.735 0.285 0.302 0.305 Table 6. Ablation on reward weights (λdiv, λrel). We vary the balance between diversity and relevance during training. 8. Video Prompts Due to space limitations, we could not include the full prompts in the main paper. However, we provide the complete prompts for several videos generated by our method as well as and system prompt used in tables 10, 9, 14, 16, 13, 11, 15, 12, and 7. 9. User Study Details We provide screenshot of our user study in Fig. 9. Users are shown 4 videos generated by given method, and asked to rate the diversity and text alignment on Likert scale 1-5. 10. Additional Ablations 10.1. Reference Set Size during Training We study how the size of the reference set Rq influences training dynamics and diversity. Table 5 shows that using small multi-reference set (5-8 examples) yields the higher improvements in TCE and TIE, 1 confirming that exposing the policy to several videogrounded modes produces more reliable marginal-gain signal. Performance degrades when the set becomes too large (Rq = 10), consistent with the diminishingreturns property of the DPP log-determinant and the increased variance of similarity estimates in larger matrices. CLIP alignment remains stable across settings, with only mild drop for larger sets. Overall, modest reference set (5-8 samples) provides the best balance between diversity gain and semantic stability. 10.2. Ablation on λ hyperparameters Table 6 shows how varying the reward weights (λ, λdiv) affects performance. Increasing the diversity weight (λdiv = 0.9) yields the highest TIE/TCE, indicating stronger diversity, while moderately CLIP alignment. Conversely, prioritizing relevance (λrel = 0.9) improves CLIP but reduces diversity. The balanced setting (0.5, 0.5) provides middle ground across all metrics. Overall, the ablation highlights the expected tradeoff: higher diversity weight improves motion and variation, while higher relevance weight preserves semantic fidelity. 11. Dataset Generation Our dataset is constructed in two stages. First, we extend the VBench prompt categories by using chainof-thought prompting to generate approximately 350 prompts for each of the 7 VBench categories. We additionally include the original VBench prompts as part of our evaluation set. This ensures that our prompt space remains grounded in widely used video-generation benchmark. In the second stage, each curated base prompt is expanded into 10 diverse variants using an iterative, two-agent reasoning framework. An architect agent proposes candidate expansions (system prompt shown in Table 7), and critic agent evaluates the videos produced from these expansions using established video-level metrics: TCE/TIE for temporal diversity, CLIP for semantic alignment, and VideoScore for perceptual quality. Only expansions that satisfy diversity and alignment criteria are retained. This procedure ensures that the final dataset is not only textually diverse but also grounded in video-level behavior. Figure 9. screenshot of our user study where 4 videos from given method are shown to the users. Table 7. System Prompt"
        },
        {
            "title": "System Prompt Content",
            "content": "You are an expert at prompt expansion. Given base prompt, expand it into at most four sentences. 1. 2. Preserve the main object(s), number, and action (no semantic drift). Expand along dimensions: environment, object attributes, subject variation, time/season, perspective, narrative/action, character appearance, or cultural context. Use English only. 4. Example: Base prompt: Good expansion: cat sitting on windowsill. \"A fluffy Siamese cat with bright blue eyes sits on wooden windowsill, watching the rainy evening street in warm painterly style.\" Bad expansion: \"A group of cats running through sunny garden, chasing butterflies.\" (changes number, action, setting). You will be given one user query. Your task: write new expansion that is diverse from the references while still preserving meaning. 2 Table 8. Video Prompts"
        },
        {
            "title": "Water Lily rests on a calm pond",
            "content": "Video 1: \"A minimalist depiction of white water lily resting at the edge of calm pond, bold outlines, restrained greens, and crisp lines.\" Video 2: \"A soft watercolor scene showing water lily resting on calm pond, the bloom painted in gentle pink tones with soft wash and gentle backdrop of water reflections.\" Video 3: \"A top-down, aerial view of water lily resting on calm pond, the outline tracing perfect circle while the pools mirror-like surface reflects the scene in photorealistic detail.\" \"A minimalist vector style renders water lily resting at the edge of calm pond, Video 4: the lotus represented as clean silhouette against calm, mirror-like surface with subtle radial symmetry.\" Table 9. Video Prompts"
        },
        {
            "title": "A cat eating food out of a bowl on a sidewalk",
            "content": "Video 1: \"A shot of cat on sunlit sidewalk eating from shallow ceramic bowl, food bright color, street life and people blurred in the background.\" Video 2: \"In street cafe atmosphere, Siamese cat eats from ceramic bowl on sunlit sidewalk, the bowl catching the glow and the cats fur shimmering with the passing traffic in soft watercolor.\" \"In sunlit courtyard, fluffy Persian cat sits on bright cobblestone, eating kibble Video 3: from ceramic bowl while the sun casts warm glow across the scene.\" Video 4: \"From ground-level perspective, cat in bold patterned jacket eats from bright ceramic bowl on sunlit city sidewalk, the bowls rim catching glint of sun while the background bustles with activity\" Table 10. Video Prompts"
        },
        {
            "title": "A boat gliding across a lake at twilight",
            "content": "Video 1: \"A moody urban scene: sleek glass-bottomed ferry glides across glittering lake at twilight, the city skyline glowing behind and silhouettes etched in the glass, captured in digital painting with dramatic color gradient.\" Video 2: \"A cinematic time-lapse across lake at twilight showing boat gliding across the water, the skys changing light refracting on the surface and the water reflecting pale, dreamlike color.\" \"Across lake, boat glides at twilight; the camera tracks the craft from low-angle Video 3: perspective on grassy bank, the water shifting from silver to pink and the horizon turning deep indigo.\" Video 4: \"A hyperreal CGI rendering of sleek boat gliding across glassy lake at twilight, reflective water, starry skies, and cool blue-green hues to capture the moment.\" Table 11. Video Prompts"
        },
        {
            "title": "A giraffe bending to sip water from a sunlit savanna pool",
            "content": "Video 1: \"From low-angle view, giraffe bends to sip water from sunlit savannah pool, long eyelashes brushing the surface and the sunlit grasses shimmering in the background\" Video 2: \"A painterly color-graded shot with warm sunset hues shows giraffe bending to sip water from sunlit savanna pool, distant grasses glow and subtle horizon line guides the eye.\", Video 3: \"A painterly color-graded frame turning the sunlit savanna into amber tones, giraffe bending to sip water from sunlit savanna pool, rendered in digital gouache with thick brushstrokes.\", Video 4: \"A photorealistic close-up of giraffe bending to sip water from sunlit savanna pool, as the sunbeams split the grasses and the water shimmers with color.\" 3 Table 12. Video Prompts"
        },
        {
            "title": "A skateboarder performs jumps",
            "content": "Video 1: \" \"On sunlit park plaza, teenage Black girl with vibrant hair performs set of jumps and grinds on street-style board, ground reflects light.\" Video 2: \"On concrete skatepark plaza at sunset, Black teenage skateboarder performs series of jumps from deck, the crowd watching as the skateboarder leaps with confidence, captured in cinematic shot.\" Table 13. Video Prompts"
        },
        {
            "title": "A fantasy landscape",
            "content": "Video 1:\"A panorama-style shot of forest glade at twilight, holographic flowers and glow moss lighting the scene as breeze moves the leaves.\" Video 2: \"A surreal yet coherent dreamlike landscape where frost-draped trees shimmer under aurora, the moonlight catching tiny frost crystals, and faint, otherworldly air pervades the scene.\" Video 3: \"In world where magic meets science, sun-dappled desert rises around mysterious castle; crystals glow with luminosity as water droplets create pools across the scene.\" Video 4: \"A dawn panorama across fantastical valley with river, shimmering dragonfly-like wing across the sky, and the air thick with mist, rendered in pastel watercolor with warm glow.\" Table 14. Video Prompts"
        },
        {
            "title": "A coffee cup sitting on a wooden table",
            "content": "Video 1:\"A cinematic top-down view in modern kitchen shows porcelain coffee cup on wooden table, single grain visible in the wood and single drop of condensation forming on the rim.\" Video 2: \"An artisanal coffee mug with rustic ceramic rim rests on wooden table, the scene rendered in warm watercolor with soft brushstrokes and delicate steam curl above the mug\" Video 3: \"A studio macro on glass coffee cup on wooden table, the cup resting in the center while the wooden grain and the steam in the air form high-contrast backdrop.\", Video 4: \"A vintage-inspired setting shows glass coffee cup on wooden table, sunbeams filtering through lace curtains and dust motes dancing in the air as the cup glows softly.\" Table 15. Video Prompts"
        },
        {
            "title": "A person kayaking or canoeing",
            "content": "Video 1:\"A South Asian woman in wetsuit paddles bright pink kayak across tranquil river, sunlight turning the water to copper and casting warm halo around the boat.\" Video 2: \"A dawn scene where person paddles kayak toward misty marsh; soft washes of pink and teal, with shallow, warm glow on the water.\" Video 3: \"A sunlit meadow scene where person in neon-green paddling jacket is canoeing along quiet river, wildflowers blooming nearby and the sun casting warm glow across the water.\" Video 4: \"A person in kayak glides along tranquil river at dawn, sun rising and the water shimmering with golds, rendered in dreamy mood with loose brushwork and soft edges.\" Table 16. Video Prompts"
        },
        {
            "title": "A dog playing ball on the beach",
            "content": "Video 1:In sunset beach scene, beagle with bright coat and long ears romps after ball on the sand, waves lapping and light breeze curling the hair, watercolor mood.\" Video 2: \"A cinematic color-graded shot with vibrant warm hues and cool shadows of golden retriever at foggy weather, playing with ball on sunlit beach, high-angle view from above.\" Table 17. Video Prompts"
        },
        {
            "title": "A fox walking through a forest",
            "content": "Video 1:\"A photorealistic dawn scene of red fox walking through forest, warm light filtering through canopy of birches and needles, dewy moss and distant stream catching the early sun.\" Video 2:\"A white fox walking through sunlit forest, soft brushwork greens and browns; the scene is dreamlike with slight hint of surrealism.\" Video 3: \"A surreal concept envisions fox walking through dreamlike forest, trees bending to form path and light washing the fur in soft tones.\" Video 4: \"A hyper-realistic CGI depiction of fox walking through forest at dawn, thick fur rendered with photoreal texture, golden hour light highlighting the forest floor.\" Table 18. Video Prompts"
        },
        {
            "title": "A cyclist riding along a lakeside trail",
            "content": "Video 1:\"A sunset sequence shows cyclist riding along lakeside trail, the sun casting warm light on the rider and the water reflecting hues of pink.\" Video 2:\"A vintage style rendering of cyclist riding along lakeside trail, bold shapes and warm sunset hues, retro typography for the route and destination.\" Video 3: \"A painterly color-graded frame turning the greens to emerald and the sky to pale cerulean, as the cyclist glides along lakeside trail, water reflections shimmering in the frame\" Video 4: \"A cyclist on lakeside trail glides toward the camera, reflections on the water shimmering and ripples spreading as tranquil backdrop renders the scene cinematic.\" Table 19. Video Prompts"
        },
        {
            "title": "A lantern swaying softly on windy night",
            "content": "Video 1:\"A painterly color-graded scene where the lantern shifts to warm amber, the night breeze makes the light flutter and the background blurs into dreamlike wash.\" Video 2:\"A painterly night scene showing lantern swaying softly on windy night in quiet street, the glow diffusing through the wind and casting warm halo across the cobblestones\" Video 3: \"Isometric landscape vignette: lantern floats in serene park at twilight; ornate, graphic lanterns define natural shapes in geometric forms; soft pastel sky overhead adds cozy, calm mood.\" Video 4: \"A cinematic long-shot with lantern swaying softly on windy night, coastal boardwalk as the backdrop, mist rolling and the lantern creating warm halo around the walker.\""
        }
    ],
    "affiliations": [
        "Virginia Tech"
    ]
}