{
    "paper_title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "authors": [
        "Leon Liangyu Chen",
        "Haoyu Ma",
        "Zhipeng Fan",
        "Ziqi Huang",
        "Animesh Sinha",
        "Xiaoliang Dai",
        "Jialiang Wang",
        "Zecheng He",
        "Jianwei Yang",
        "Chunyuan Li",
        "Junzhe Sun",
        "Chu Wang",
        "Serena Yeung-Levy",
        "Felix Juefei-Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 9 7 2 2 1 . 2 0 6 2 : r UniT: Unified Multimodal Chain-of-Thought Test-time Scaling Leon Liangyu Chen1,2, Haoyu Ma2, Zhipeng Fan2, Ziqi Huang2,3, Animesh Sinha2, Xiaoliang Dai2, Jialiang Wang2, Zecheng He2, Jianwei Yang2, Chunyuan Li2, Junzhe Sun2, Chu Wang2, Serena Yeung-Levy1, Felix Juefei-Xu2 1Stanford University, 2Meta Superintelligence Labs, 3Nanyang Technological University Unified models can handle both multimodal understanding and generation within single architecture, yet they typically operate in single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, framework for multimodal chain-of-thought test-time scaling that enables single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models. Date: February 13, 2026 Paper: https://ai.meta.com/research/publications/unit-unified-multimodal-chain-of-thought-test-time-scaling Correspondence: Leon Liangyu Chen at liangyuc@stanford.edu"
        },
        {
            "title": "1 Introduction",
            "content": "Unified multimodal models (Deng et al., 2025b; Wu et al., 2024a; Zhou et al., 2024c) aim to merge vision, language, and more modalities into single architecture capable of both understanding and generation. Unlike modular pipelines where separate models handle perception, verification, and generation, unified models handle all modalities within one coherent conversation, enabling richer cross-modal grounding, continuous contextual tracking, and seamless interleaving of understanding and generation. However, despite this potential, existing unified models still operate mostly in single-pass mode: they produce an output once, without explicit mechanisms for evaluating, reflecting on, or refining their predictions. This limitation becomes fundamental for tasks that intrinsically require multi-step reasoning and self-correction, such as compositional generation, multi-turn editing, and complex visual reasoning - settings where both humans and AI naturally benefit from extended reasoning. Recent advances in language models have demonstrated that test-time scaling (TTS)allocating additional computational resources during inference through extended chain-of-thought reasoning, verification, and iterative refinement (OpenAI, 2024; DeepSeek-AI et al., 2025; Snell et al., 2024a)enables substantial performance gains on complex reasoning tasks in mathematics (Cobbe et al., 2021), coding (Chen et al., 2021), and logic (Srivastava et al., 2023). Early work on multimodal chain-of-thought has shown similar benefits for single-round visual understanding and generation (Shao et al., 2024; Fang et al., 2025; Xiao et al., 2025; Huang et al., 2025a; Chern et al., 2025). Yet, test-time scaling for unified multimodal models remains largely 1 Figure 1 Multimodal chain-of-thought enables test-time scaling through emergent cognitive behaviors. We propose the UniT framework for unified multimodal models, which induces subgoal decomposition for compositional tasks and unlocks content understanding and memory for multi-turn editing. Controlling the number of test-time images, chain-of-thought sequential scaling outperforms best-of-N parallel scaling across generation and reasoning benchmarks. User input Model output unexplored. The challenge is nontrivial: test-time scaling requires capabilities that currently scatter across specialized models (image generation models for generation, vision-language models for verification, image editing models for refinement). Bridging this gap requires unified framework that systematically integrates data synthesis, model training, and inference mechanisms for multimodal test-time scaling. This motivates the central question: How to enable scalable multimodal inference that allows unified models to iteratively generate, reflect, and refine? We introduce UniT, unified framework for multimodal chain-of-thought test-time scaling. Scalable multimodal inference requires the tight integration of three components: (i) Agentic data synthesis to induce cognitive behaviors through multi-round trajectories. We develop an automated pipeline  (Fig. 2)  where vision-language models iteratively critique and image editing models refine generated images with explicit chain-of-thought reasoning. This naturally produces training data exhibiting three critical cognitive behaviors (Gandhi et al., 2025): verificationevaluating outputs against instructions; subgoal decompositionbreaking complex instructions into sequential planning steps; content memorymaintaining understanding of visual content across rounds through unified multimodal context. (ii) Unified model training to enable the model to internalize multimodal reasoning patterns. We collect approximately 12K multi-round trajectories and fine-tune the Bagel unified multimodal model (Deng et al., 2025b) for 700 H100 hours, enabling it to perform both understanding and refinement without switching models. (iii) Multimodal test-time scaling at inference with flexible computational budget. The trained model performs all reasoning, generation, and refinement iteratively through explicit multimodal chain-of-thought thinking, allocating more rounds to more challenging tasks. 2 The synergy of these components enables the model to act as single, coherent multimodal reasoner capable of self-evaluation and iterative improvement. The UniT framework exhibits strong test-time scaling behavior  (Fig. 1)  with emergent capabilities. Most notably, models trained on shorter reasoning trajectories (averaging 3.6 rounds) effectively generalize to longer inference chains at test time (averaging 4.7 rounds)  (Fig. 5)  , echoing patterns previously seen only in text-only models (Snell et al., 2024a). Furthermore, chain-of-thought sequential scaling substantially outperforms best-of-N parallel sampling, achieving comparable performance with 2.5 less computational cost  (Fig. 1)  . This demonstrates that iterative refinement with explicit reasoning provides more efficient use of inference compute than parallel sampling. Critically, UniT achieves 5.56% improvement on CompBench multi-object editing, 2.95 human preference scores on ImgEdit multi-turn editing, and 10.34% on OneIG instruction following compared to single-pass generation. Moreover, it improves out-of-distribution visual reasoning on MIRA by 53.33%, establishing multimodal chain-of-thought test-time scaling as unified paradigm that benefits both generation and comprehension. We summarize our contributions as follows: Unified multimodal test-time scaling. We propose UniT, unified framework for multimodal chain-ofthought test-time scaling, integrating agentic data synthesis, unified model training, and test-time scaling mechanisms. Emergent extrapolation to longer reasoning chains. We demonstrate that models trained on shorter trajectories generalize to longer inference chains at test time, extrapolating beyond the training distribution. Broad improvements across multimodal tasks. UniT achieves substantial gains on compositional generation/editing, multi-turn editing, and visual reasoning, establishing chain-of-thought test-time scaling as unified paradigm for both generation and understanding tasks."
        },
        {
            "title": "2 Related Works",
            "content": "Test-time scaling. Test-time scaling allocates additional computation during inference to improve model performance. We distinguish two primary approaches: parallel and sequential methods. Parallel scaling generates multiple independent candidates and selects the best via criteria such as best-of-N sampling (Brown et al., 2024; Levi, 2024) or majority voting (Irvine et al., 2023), typically guided by outcome reward models (Xin et al., 2024; Ankner et al., 2024). Sequential scaling (Snell et al., 2024b; Hou et al., 2025; Lee et al., 2025) enables iterative refinement where models critique and improve outputs across multiple rounds. Self-refinement approaches (Madaan et al., 2023) exemplify this by having models explicitly reason about deficiencies and produce progressively better solutions. Tree-based methods such as Monte Carlo Tree Search (Liu et al., 2024a; Zhang et al., 2023; Zhou et al., 2024a; Choi et al., 2023) and REBASE (Wu et al., 2024b) occupy middle ground, using process reward models (Lightman et al., 2023; Wang et al., 2024a,c) to guide structured search. Recent breakthroughs including o1 (OpenAI, 2024) and DeepSeek-R1 (DeepSeek-AI et al., 2025) demonstrate that reinforcement learning enables effective utilization of extended inference computation. Budget forcing (Snell et al., 2024b; Muennighoff et al., 2025) trains models to produce reasoning chains of controllable cost by varying computational budgets during training. While most test-time scaling research focuses on text-only reasoning, recent work has explored search-based methods for image and video generation (He et al., 2025; Liu et al., 2025a). However, unified multimodal test-time scaling interleaving text and images remains largely unexplored, which we address in this work. Unified multimodal models. Unified models that jointly handle understanding and generation have attracted substantial interest. Autoregressive approaches (Wu et al., 2024a; Chen et al., 2025c; Lu et al., 2024; Qu et al., 2024; Team, 2024; Wang et al., 2024b) extend next-token prediction to both text and discrete image tokens. Additional diffusion methods (Dong et al., 2024; Tong et al., 2024b; Pan et al., 2025; Tong et al., 2024a) augment language models with external diffusion modules for image generation. Unified integrated transformers (Deng et al., 2025b; Yu et al., 2024a; Ma et al., 2024; Shi et al., 2024; Zhou et al., 2024b) deeply integrate language modeling and diffusion within single architectures. Our work builds upon Bagel (Deng et al., 2025b), pretrained on large-scale interleaved text-image sequences. Our framework generalizes to all three paradigms as they naturally handle interleaved multimodal inputs and outputs. Multimodal chain-of-thought. Chain-of-thought reasoning (Wei et al., 2022) has proven effective for text-based 3 Figure 2 Agentic framework for synthesizing chain-of-thought training data. Starting from user prompt, an image generation model generates an initial image. vision-language model then performs verification - evaluating whether the output satisfies the prompt. When unsatisfactory, the VLM engages in explicit subgoal decomposition through thinking tokens, planning concrete improvements, and rewriting editing instructions. This iterative loop continues until verification succeeds, generating multi-turn reasoning trajectories that teach unified models to refine outputs through test-time computation. The explicit reasoning traces of the three models capture how cognitive behaviors emerge from the interplay between generation, verification, and planning. problem-solving, motivating extensions to multimodal tasks. Visual chain-of-thought methods (Shao et al., 2024; Zhang et al., 2024; Hu et al., 2024; Liu et al., 2024b; Fan et al., 2024; Huang et al., 2025b) incorporate visual representations into reasoning steps for multimodal understanding. Recent work explores interleaved reasoning (Huang et al., 2025a; Gu et al., 2025a) across text and visual modalities. Uni-CoT (Qin et al., 2025) further demonstrates unified model that couples macroand micro-level reasoning for vision-language understanding, but it does not study compute scaling or iterative editing. In text-to-image generation, studies investigate whether explicit reasoning improves generation quality (Fang et al., 2025; Xiao et al., 2025; Deng et al., 2025b; Jiang et al., 2025; Gu et al., 2025b). Reflection-based approaches (Zhuo et al., 2025; Wu et al., 2025; Chern et al., 2025) iteratively critique and refine generated images. Our work differs by focusing on both semantic correctness and visual quality through test-time scaled refinement, while demonstrating that multimodal chain-of-thought benefits both generation and understanding tasks as unified paradigm."
        },
        {
            "title": "3 Method",
            "content": "We extend test-time compute scaling from text-only reasoning to unified multimodal models. As illustrated in Fig. 2, we develop an agentic framework to automatically collect chain-of-thought training data, then control test-time computational budget through iterative refinement rounds. Sequential chain-of-thought scaling outperforms parallel best-of-N sampling  (Fig. 1)  while inducing emergent cognitive behaviorsverification and subgoal decomposition. 4 Figure 3 UniT enables iterative refinement for compositional instructions through multimodal chain-of-thought reasoning. UniT exhibits: (i) error verification and correctionidentifying and fixing constraint violations that Bagel misses (top: correcting leash placement and dog action); (ii) subgoal decomposition with subject consistencysequentially addressing instructions while maintaining subject identity across rounds (middle: preserving bear features through style transformation, bottom: skateboard consistency); (iii) quality preservationmaintaining visual fidelity through iterative refinement rather than degradation (top: reduced artifacts and haloing). Key distinction. The multi-model agentic framework described in Sec. 3.1 is used solely for synthesizing training data. At inference time (Sec. 3.3), we use only the single unified BAGEL model (Deng et al., 2025b), which performs all planning, generation, reflection, and refinement operations without external models."
        },
        {
            "title": "3.1 Multimodal Chain-of-Thought Data\nAgentic data collection pipeline. Our automated pipeline synthesizes multimodal chain-of-thought trajectories\nthrough iterative reflection-editing (Fig. 2):",
            "content": "1. Prompt generation: Llama-4-Scout-17B-16E (Meta, 2025) generates 20K diverse prompts covering compositional attributes, spatial relations, and complex multimodal tasks based on (T2I-CoReBench Team, 2025). 2. Initial generation: Flux Pro (Black Forest Labs, 2024) produces initial images from prompts. For complex prompts, the VLM (Qwen3-VL) decomposes the prompts into subgoals and executes the first step in initial generation. 3. Reflection: Qwen3-VL (Qwen Team, 2025a) evaluates whether the image satisfies the prompt. If not, it generates explicit chain-of-thought reasoning, identifying deficiencies, planning improvements, and specifying editing instructions. 4. Refinement: Flux Kontext (Labs, 2025) or Qwen-Image-Edit (Qwen Team, 2025b) applies editing instructions. 5. Iteration: Steps 3-4 repeat until the VLM determines the output satisfies the prompt. The VLMs produce interleaved text and image tokens with explicit thinking tokens. The reflection step generates detailed reasoning about why outputs fall short and how to improve them, rather than simply issuing new instructions. Cognitive behaviors. As demonstrated in Fig. 1, 2, this agentic pipeline naturally induces three critical cognitive behaviors: (i) verificationVLMs evaluate outputs against specifications to determine when refinement is 5 Figure 4 Qualitative examples of chain-of-thought test-time scaling. Representative trajectories showing progressive refinement across different tasks and computational budgets. Examples demonstrate how explicit chain-of-thought reasoning enables the model to iteratively improve compositional generation. needed; (ii) subgoal decompositioncomplex compositional tasks are planned in sequential editing steps; (iii) content memorythe model maintains understanding of image content across refinement rounds through unified multimodal context. Data filtering. We apply quality filtering to ensure training efficiency: Length constraint: Trajectories exceeding 8 rounds are removed to balance efficiency with reasoning depth. Quality regression: Trajectories where the final image has worse instruction-following quality than any of the first three images (measured by Qwen3-VL) are removed. Relevance filtering: Individual rounds with editing prompts semantically irrelevant to the original task (measured by Llama-4-Scout-17B-16E) are removed. Minimal visual changes: Rounds with perceptual editing distance below LPIPS < 0.03 (Zhang et al., 2018) between consecutive images are removed. Benchmark deduplication: Training prompts are deduplicated from evaluation benchmarks using 5-gram matching to prevent data leakage. This filtering retains 12k high-quality trajectories for training."
        },
        {
            "title": "3.2 Training and Inference\nTraining. We use Bagel (Deng et al., 2025b), a unified multimodal architecture with understanding and\ngeneration capabilities, trained on the dataset from Sec. 3.1 for 700 H100 hours. To simulate user prompts for\nmulti-turn editing, 10% of intermediate image editing instructions don’t require losses.",
            "content": "Inference. We adopt framework that incorporates two complementary classifier-free guidance (CFG) schemes applied in nested manner: (1) text CFG, conditioning with versus without the current text instruction, and (2) image CFG, conditioning with versus without all images in the generation history (including both the initial input image if exists and previously generated outputs). Formally, let vt denote the fully conditional prediction, vt,unc the text-unconditional prediction, and vi,unc the image-unconditional 6 Method NP T&P Short Medium Long Overall Alignment Janus-Pro (Chen et al., 2025b) BLIP3-o (Chen et al., 2025a) Bagel (Deng et al., 2025a) Bagel+CoT (Deng et al., 2025a) 0.557 0.719 0.776 0.798 0.533 0.671 0.734 0.767 0.609 0.754 0.782 0. 0.548 0.712 0.769 0.793 0.515 0.674 0.759 0.767 0.552 0.706 0.764 0.790 UniT 0.853 0. 0.859 0.849 0.812 0.843 Table 1 Compositional generation, OneIG-Bench. NP denotes the natural language prompt. T&P denotes the tag-based and phrase-based prompt. Short, Medium and Long represent the length of the prompts, where Short denotes the number of words is less than 30, Medium denotes the number between 30 and 60, and Long denotes the number exceeding 60. Bagel+CoT indicates Bagel with text-only chain-of-thought. prediction. We apply CFG sequentially: first text guidance vtext = vt,unc + st(vt vt,unc), then image guidance vfinal = vi,unc +si(vtext vi,unc), with scales st=4.0 and si=2.0. The nested applicationwhere image guidance is applied on top of the text-guided predictionenables independent control over prompt adherence and visual consistency. This strategy helps maintain strong alignment with text instructions while preserving structural coherence across multi-turn editing sequences, with notable benefits for generation quality and consistency in iterative refinement workflows. The original untrained Bagel model can also be forced to reason chain-of-thought with our inference code. However, the image quality degrades quickly as context images scale and the Bagel model cant verify its image outputs properly. It usually hallucinates visual content according to user prompts. Thus the untrained Bagel model is not feasible for practical multimodal chain-of-thought reasoning. Training is required to enable effective multimodal chain-of-thought reasoning."
        },
        {
            "title": "3.3 Budget Forcing for Test-time Scaling",
            "content": "We adapt budget forcing (Muennighoff et al., 2025) from text-only to multimodal test-time scaling. While text-based methods control reasoning tokens, we control image generation rounds, which dominate inference latency. At test time, the unified BAGEL model performs all operations autonomouslyplanning, generation, reflection, and refinementwithout relying on external models. Controlling computational budget. At inference, we specify computational budget as the number of image generation rounds. Each round consists of textual chain-of-thought reasoning followed by image generation or editing. To enforce budget C: Forcing extended reasoning: If the model terminates before rounds, we suppress EOS, append Lets edit the image\", wait for reasoning completion, then force image generation. Budget constraint: If the model generates more than images, we use only the final image from round C. This enables studying sequential chain-of-thought scaling (iterative refinement building on previous outputs) versus parallel best-of-N scaling (generating independent images and selecting the best). We present detailed scaling analysis in Sec. 5.1. Beyond-training generalization. Models trained on trajectories averaging 3.6 rounds generalize to longer inference chains averaging 4.7 rounds at test time  (Fig. 5)  , exhibiting problem-solving capabilities beyond their training distribution. This establishes test-time compute as general paradigm for unified multimodal models."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate on text-to-image generation, compositional editing, multi-turn editing, and visual reasoning benchmarks. Chain-of-thought TTS achieves substantial gains across both generation and understanding 7 Figure 5 Training vs. inference round distribution demonstrates beyond-training generalization. The model is trained on trajectories averaging 3.6 refinement rounds, but effectively generalizes to longer inference chains averaging 4.7 rounds at test time. This distribution shift reveals the models emergent ability to extend inference beyond its training distribution, key property of effective test-time scaling. Method Bagel (Deng et al., 2025a) Bagel+CoT (Deng et al., 2025a) UniT Content Memory Content Understand Version Backtrack Overall 1.76 2.24 4. 1.34 2.67 5.02 0.82 0.84 3.48 1.31 1.92 4. Table 2 Multi-turn editing, ImgEdit. Human evaluation score from 0-10, normalized over 3 turns of editing. tasks."
        },
        {
            "title": "4.2 Compositional Generation and Editing",
            "content": "We evaluate on OneIG-Bench-EN (Chang et al., 2025) for compositional generation  (Table 1)  , achieving 10.34% improvement over base model at C=10. On CompBench (Jia et al., 2025) multi-object compositional editing subset  (Table 3)  , we achieve 5.56% improvement from C=1 to C=10. Fig. 1 demonstrates that iterative refinement with explicit reasoning enables better compositional understanding and generation."
        },
        {
            "title": "4.3 Multi-Turn Editing",
            "content": "We evaluate on ImgEdit (Ye et al., 2025) multi-turn editing subset  (Table 2)  , achieving 225.19% improvement from C=1 to C=4. This demonstrates that maintained context and reasoning chains are critical for multi-turn 8 Method Foreground Background LC-T LC-I PSNR(dB) SSIM LPIPS Overall 19.163 0.757 HQ-Edit (Hui et al., 2024) 20.022 0.795 UltraEdit (Zhao et al., 2024) AnyEdit (Yu et al., 2024b) 19.875 0.809 19.092 0.795 SEED-X (Ge et al., 2024) GoT (Fang et al., 2025) 19.919 0.804 20.213 0.828 Step1X-Edit (Liu et al., 2025b) FLUX.1 Kontext (Labs, 2025) 20.983 0.836 Qwen-Image-Edit (Qwen Team, 2025b) 21.058 0.836 20.434 0.842 Bagel (Deng et al., 2025a) Bagel+CoT (Deng et al., 2025a) 20.658 0.846 12.987 22.326 22.789 20.638 21.296 22.696 24.013 21.927 24.370 24.691 0.412 0.719 0.697 0.788 0.826 0.873 0.938 0.810 0.917 0.926 0.421 0.164 0.129 0.138 0.127 0.089 0.064 0.121 0.069 0.065 0.007 0.609 0.622 0.595 0.668 0.844 0.965 0.782 0.936 0.956 UniT 21.127 0.854 25.442 0.942 0.055 0.988 Table 3 Multi-object editing, CompBench. LC-T denotes local CLIP scores between the edited foreground and the local description. LC-I refers to the CLIP image similarity between the foreground edited result and ground truth (GT) image. Overall scores are computed using min-max normalization for each metric. interactions. The models content memory through unified multimodal context enables coherent interactions as computational budget increases  (Fig. 1)  ."
        },
        {
            "title": "4.4 Visual Reasoning",
            "content": "On MIRA (Zhou et al., 2025) for out-of-distribution visual reasoning  (Table 4)  , we achieve 53.33% improvement from C=1 to C=10  (Fig. 1)  . The remaining gap to frontier models (GPT-5, Qwen2.5-VL-72B) reflects base model capability differences: these models benefit from significantly larger scale and proprietary training data. Our key contribution is methodological, demonstrating that TTS successfully transfers to multimodal domains; as base unified models improve, the UniT framework directly benefits. This establishes multimodal chain-of-thought test-time scaling as unified paradigm enhancing both generation and comprehension tasks. The cognitive behaviors induced by our agentic framework  (Fig. 2)  verification, subgoal decomposition, and content memorytransfer from generation to understanding tasks."
        },
        {
            "title": "4.5 Qualitative Results",
            "content": "Fig. 1 and Fig. 3 demonstrate how multimodal chain-of-thought test-time scaling progressively refines outputs through iterative reasoning, revealing the three cognitive behaviorsverification, subgoal decomposition, and content memory. Compositional generation. For complex compositional prompts, Round 1 produces partial solutions with compositional errorsmissing objects, incorrect attributes, or violated spatial constraints. The models explicit reasoning identifies deficiencies, and through subgoal decomposition, breaks corrections into sequential steps. By Round 10, the output achieves precise alignment with all requirements, demonstrating systematic error correction through explicit reasoning. Visual reasoning. For MIRA geometry tasks, the models chain-of-thought reveals how verification supports iterative problem-solving. Early rounds may produce incorrect analyses, but the reasoning shows self-critique, allowing identification of flaws and revision in subsequent rounds. Subgoal decomposition breaks complex reasoning into steps. These examples demonstrate that cognitive behaviors transfer from generation to understanding tasks."
        },
        {
            "title": "5 Discussion",
            "content": "We analyze factors contributing to effective multimodal test-time scaling: sequential versus parallel scaling, cognitive behaviors, and data quality. 9 Figure 6 Chain-of-thought visual reasoning on MIRA. The model decomposes the puzzle into subgoals (zoom in, identify patterns) before selecting the matching piece, demonstrating cognitive behaviors transferring from generation to understanding tasks. Experimental protocol. Unless specified otherwise, we use computational budget C=10 (C=4 for ImgEdit) and report: alignment score for OneIG-Bench, overall normalized score for CompBench, human evaluation score (0-10) for ImgEdit, and accuracy for MIRA."
        },
        {
            "title": "5.1 Sequential vs. Parallel Scaling",
            "content": "We compare chain-of-thought sequential scaling against best-of-N parallel scaling. Following Muennighoff et al. (2025), sequential scaling builds on intermediate results through iterative refinement, while parallel scaling generates outputs independently. Setup. For both approaches, we control the number of generated images (C=N ) from 1 to 10. Sequential scaling uses budget forcing (Sec. 3.3). Parallel scaling generates independent images and selects the best via HPSv3 (Ma et al., 2025). Compute accounting. We use the number of generated images as our compute metric (C=N ), which accurately reflects computational cost because: (i) image generation via diffusion models dominates wall-clock timeas noted in Sec. 3.3, image generation rounds control inference latency; (ii) marginal text tokens from VLM reflection have negligible effect on latency compared to diffusion sampling; (iii) computational cost scales linearly with the number of images generated. We exclude selection costs (HPSv3 for parallel scaling, VLM verification for sequential scaling) from our comparison as these represent arbitrary model choices rather than fundamental algorithmic requirements. Under this metric, sequential scaling achieves comparable performance to parallel with 2.5 fewer generated images (e.g., C=4 sequential matches =10 parallel on OneIG-Bench). Results. Sequential scaling consistently outperforms parallel scaling across all tasks  (Fig. 1)  . At C=10 (C=4 for ImgEdit), sequential achieves 4.85% improvement over parallel on OneIG-Bench, 3.89% on CompBench, 71.77% on ImgEdit, and 33.72% on MIRA. This advantage manifests through: Steeper scaling slopes: Sequential achieves larger performance improvements per additional image, indicating more efficient test-time compute use. Sustained improvements: Sequential shows continued gains up to C=10, while parallel plateaus after few samples. Iterative refinement with explicit chain-of-thought reasoning provides more effective test-time compute scaling than independent sampling. The advantage stems from sequential scaling accumulating successful edits and learning from previous iterations: each round builds on prior images with explicit CoT corrections, leveraging expanded textual context (reflections, plans). Parallel scaling generates independent samples without inter-sample learning, explaining why it plateaus earlierit cannot systematically refine toward the target. 10 Method EG PBR ASLP CT Overall GPT-5 (OpenAI, 2025) 14.5 29.9 Qwen2.5-VL (72B) (Bai et al., 2025) 14.5 21.7 7.9 Bagel (Deng et al., 2025a) Bagel+CoT (Deng et al., 2025a) 9. 9.7 8.0 10.8 11.1 3.5 4.8 UniT 12.5 11.2 6.1 17.9 8.6 12.3 14. 16.3 16.5 13.1 7.5 9.2 11.5 Table 4 Multimodal reasoning, MIRA, with direct input. We report results across four reasoning categories: EG (Geometry), PBR (Physics), ASLP (Puzzles), and CT (Causal), along with the overall average score. Configuration OneIG Align (%) CompBench Overall (%) ImgEdit Score MIRA Acc (%) All behaviours w/o Verification w/o Subgoal Decomp. w/o Content Memory 84.3 81.2 (-3.1) 80.5 (-3.8) 82.8 (-1.5) 98.8 96.8 (-2.0) 96.3 (-2.5) 97.8 (-1.0) 4.26 3.55 (-0.71) 3.75 (-0.51) 2.45 (-1.81) 11.5 9.6 (-1.9) 10.3 (-1.2) 10.8 (-0.7) Table 5 Cognitive behavior ablation. Impact of removing verification, subgoal decomposition, or content memory from our agentic framework. Latency considerations. Sequential and parallel scaling serve complementary use cases: sequential optimizes performance while parallel optimizes latency. Sequential scaling also benefits from unique acceleration techniques including speculative decoding, KV-cache reuse across rounds, and early stopping when the model determines satisfaction, which can significantly reduce the latency gap in practice. Unified model vs. modular pipeline. At inference, UniT performs all reasoning, generation, and refinement within single model, unlike the multi-model agentic pipeline used for data synthesis (Sec. 3.1). While the teacher pipeline (Flux Pro + Qwen3-VL) scores slightly higher due to frontier-scale components, UniT offers faster inference by eliminating inter-model communication overhead, seamless multimodal context within single architecture, and practical single-model deployability."
        },
        {
            "title": "5.3 Data Quality Analysis",
            "content": "We analyze the impact of data quality by ablating individual filters from our curation pipeline (Sec. 3.1). Setup. We train three ablated models, each removing one quality filter: (1) w/o Quality regression filter, including trajectories where refinement degrades quality; (2) w/o Relevance filtering, including rounds with semantically irrelevant editing prompts; (3) w/o Minimal visual changes filter, including rounds with negligible visual refinement. Results. Table 6 shows task-specific sensitivities. Removing relevance filtering causes the largest degradation on compositional tasks (3.1% on OneIG-Bench, 2.5% on CompBench), as off-topic edits undermine maintaining 11 Data Configuration OneIG Align (%) CompBench Overall (%) ImgEdit Score MIRA Acc (%) Full curated dataset w/o Quality regression filter w/o Relevance filtering w/o Min. visual changes filter 84.3 82.5 (-1.8) 81.2 (-3.1) 83.5 (-0.8) 98.8 97.5 (-1.3) 96.3 (-2.5) 98.0 (-0.8) 4.26 3.30 (-0.96) 3.60 (-0.66) 3.10 (-1.16) 11.5 10.0 (-1.5) 10.3 (-1.2) 10.9 (-0.6) Table 6 Data quality ablation. Impact of removing individual curation filters (Sec. 3.1) from the training data pipeline. compositional constraints. Removing the minimal visual changes filter most significantly hurts multi-turn editing (1.16 points on ImgEdit), demonstrating that learning meaningful incremental progress is critical for sustained interactions. For visual reasoning (MIRA), removing the quality regression filter has the largest impact (1.5%), as learning from degraded trajectories impairs converging toward correct answers. Different quality dimensions matter for different capabilities; effective test-time scaling requires curating data along multiple axes."
        },
        {
            "title": "5.4 Failure Cases",
            "content": "Despite strong performance, our approach exhibits limitations in specific scenarios. First, tasks requiring precise physical reasoning or fine-grained spatial relationships occasionally fail, as iterative refinement may struggle to correct fundamental physics violations or attribute binding errors inherited from the base generation/editing models (e.g., incorrect leash-dog assignment or wrong helmet placement and sizes in Fig. 3). Second, we observe occasional degradation loops where reflection incorrectly identifies non-existent issues, leading to unnecessary edits that harm quality rather than improve ita verification hallucination bottleneck particularly evident when the VLMs verification capabilities are insufficient to accurately assess subtle visual attributes. Third, extremely complex compositional prompts with many interacting constraints can lead to subgoal conflicts during decomposition, where satisfying one constraint inadvertently violates another. Finally, testtime scaling cannot overcome fundamental capability gaps in the base model; if the underlying diffusion or VLM components lack certain semantic understanding, additional inference compute provides diminishing returns. These failure modes suggest directions for future work, including more robust verification mechanisms, physics-aware refinement strategies, and constraint satisfaction planning. We further discuss scaling limits beyond C=10 in Sec. D. Failure visualizations are presented in Sec. E."
        },
        {
            "title": "6 Conclusion",
            "content": "We have presented unified approach to multimodal chain-of-thought test-time scaling that extends inferencetime compute from text-only reasoning to models handling both visual understanding and generation, establishing paradigm that benefits both generation and comprehension across modalities. Our key contributionsan agentic framework that induces cognitive behaviors (verification, subgoal decomposition, content memory), budget forcing for beyond-training generalization, and evidence that sequential reasoning outperforms parallel samplingyield substantial gains across compositional generation, multi-turn editing, and visual reasoning, demonstrating that iterative refinement through explicit reasoning unlocks significant performance improvements on complex multimodal tasks. Limitations. While our approach demonstrates strong results, test-time scaling inherently requires additional computational resources at inference. Future work should explore more efficient reflection mechanisms and adaptive budget allocation strategies that minimize computational overhead while preserving quality gains. Future directions. Promising directions include extending our approach to additional modalities (audio, video), augmenting reflection with explicit physical reasoning to enforce implicit constraints (e.g., object sizing, perspective, occlusion), investigating reinforcement learning from human feedback to further improve reflection quality, and exploring how test-time scaling interacts with other inference-time techniques such as self-consistency and verifier-guided generation."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Hao Tang, Mark Endo, and Alejandro Lozano for their helpful discussions."
        },
        {
            "title": "References",
            "content": "Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models, 2024. URL https://arxiv.org/abs/2408.11791. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Black Forest Labs. Flux 1.1 pro: State-of-the-art text-to-image generation model. https://bfl.ai/models/flux-pro, 2024. High-speed text-to-image generation with strong prompt adherence. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Yuhang Chang, Fei Fang, Wenjing Wang, Haochen Chen, Guangtao Zhang, et al. Oneig-bench: Omni-dimensional nuanced evaluation for image generation. arXiv preprint arXiv:2506.07977, 2025. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025c. Steffi Chern et al. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. Sehyun Choi, Tianqing Fang, Zhaowei Wang, and Yangqiu Song. Kcts: Knowledge-constrained tree search decoding with token-level hallucination detection, 2023. URL https://arxiv.org/abs/2310.09044. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025a. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025b. Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Dreamllm: Synergistic multimodal comprehension and creation. In The Twelfth International Conference on Learning Representations, 2024. Wenshan Fan, Shijie Chen, Jianqiang Zhang, and Donglin Liu. Visualization-of-thought: Eliciting spatial reasoning through sketch-based visual reasoning. arXiv preprint arXiv:2404.03622, 2024. 13 Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, and Jifeng Dai. Got: Unleashing reasoning capability of multimodal large language model for visual generation and editing. arXiv preprint arXiv:2503.10639, 2025. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, and Yu Cheng. Thinkmorph: Emergent properties in multimodal interleaved chain-of-thought reasoning. arXiv preprint arXiv:2510.27492, 2025a. Zeqi Gu, Markos Georgopoulos, Xiaoliang Dai, Marjan Ghazvininejad, Chu Wang, Felix Juefei-Xu, Kunpeng Li, Yujun Shi, Zecheng He, Zijian He, et al. Improving chain-of-thought efficiency for autoregressive image generation. arXiv preprint arXiv:2510.05593, 2025b. Haoran He, Jiajun Liang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Ling Pan. Scaling image and video generation via test-time evolutionary search. arXiv preprint arXiv:2505.17618, 2025. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. Advancing language model reasoning through reinforcement learning and inference scaling, 2025. URL https://arxiv.org/abs/ 2501.11651. Yushi Hu, Weijia Yin, Xingyu Shi, Anirudh Kumar, Ranjay Krishna, Dorsa Sadigh, and Trevor Darrell. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, et al. Interleaving reasoning for better text-to-image generation. arXiv preprint arXiv:2509.06945, 2025a. Ziqi Huang, Ning Yu, Gordon Chen, Haonan Qiu, Paul Debevec, and Ziwei Liu. Vchain: Chain-of-visual-thought for reasoning in video generation. arXiv preprint arXiv:2510.05094, 2025b. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Ziyi Zhu, Vineet Mudupalli, Aliaksei Korshuk, Zongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, Xiaoding Lu, Thomas Rialan, and William Beauchamp. Rewarding chatbots for real-world engagement with millions of users, 2023. URL https://arxiv.org/abs/2303.06135. Bohan Jia, Wenxuan Huang, Yuntian Tang, Junbo Qiao, Jincheng Liao, Shaosheng Cao, Fei Zhao, Zhaopeng Feng, Zhouhong Gu, Zhenfei Yin, et al. Compbench: Benchmarking complex instruction-guided image editing. arXiv preprint arXiv:2505.12200, 2025. Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. Black Forest Labs. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, and Xinyun Chen. Evolving deeper llm thinking, 2025. URL https://arxiv.org/abs/2501.09891. Noam Levi. simple model of inference scaling laws, 2024. URL https://arxiv.org/abs/2410.16377. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. Video-t1: Test-time scaling for video generation. arXiv preprint arXiv:2503.18942, 2025a. 14 Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding, 2024a. URL https://arxiv.org/abs/2309.15028. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025b. Yang Liu, Hao Zhang, Xiang Wang, and Jiaming Chen. Mvot: Multi-view thinking for enhanced visual reasoning. arXiv preprint arXiv:2407.12345, 2024b. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2643926455, June 2024. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2024. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1508615095, 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. ArXiv, abs/2303.17651, 2023. Meta. Llama 4 scout: https://huggingface.co/meta-llama/ Llama-4-Scout-17B-16E, April 2025. Multimodal mixture-of-experts language model with 17B activated parameters. 17b parameter mixture-of-experts model. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. OpenAI. Openai o1 system card, 2024. OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025. Accessed: 2025-11-08. Xichen Pan, Yan Cai, Junyi Li, Yuheng Lu, Xicheng Huang, Xu Jia, Juefei Xu, Runxin Xu, Peng Li, Zheng Ge, Zhenguo Ying, Zheng Liu, Shuicheng Huang, Jiazhi Zhao, and Yuanhan Peng. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. Qwen Team. Qwen3-vl: Technical report. Technical Report, Alibaba Cloud, 2025a. Available at https://github.com/ QwenLM/Qwen3-VL. Qwen Team. Qwen-image-edit: Ai-powered image editing model. Technical Report, Alibaba Cloud, 2025b. 20B parameter image editing model. Hao Shao, Shengju Wang, Liang Li, Xiang Wang, and Liu Yang. Visual-cot: Unleashing chain-of-thought reasoning in multi-modal language models. arXiv preprint arXiv:2403.16999, 2024. Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024a. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024b. URL https://arxiv.org/abs/2408.03314. 15 Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2023. T2I-CoReBench Team. T2i-corebench: comprehensive benchmark for evaluating composition and reasoning in text-to-image generation. https://t2i-corebench.github.io/, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. doi: 10.48550/arXiv.2405.09818. URL https://github.com/facebookresearch/chameleon. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024a. Shengbang Tong et al. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint, 2024b. Placeholder for wu2024next - update with correct citation if different. Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y. Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations, 2024a. URL https://arxiv.org/abs/2312.08935. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models, 2024c. URL https://arxiv.org/abs/2406.08673. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models, 2024b. URL https://arxiv.org/abs/2408. 00724. Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, and Ying Shan. Mindomni: Unleashing reasoning generation in vision language models with rgpo. arXiv preprint arXiv:2505.13031, 2025. Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data, 2024. URL https://arxiv.org/abs/2405.14333. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Haofei Yu, Zhengyang Qi, Lawrence Jang, Ruslan Salakhutdinov, Louis-Philippe Morency, and Paul Pu Liang. Mmoe: Enhancing multimodal models with mixtures of multimodal interaction experts. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1000610030, 2024a. Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 586595, 2018. 16 Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language models for code generation, 2023. URL https://arxiv.org/abs/2303.05510. Yiyang Zhang, Wei Chen, Xiang Wang, and Jun Li. Chain-of-image: Advancing visual reasoning via image sequence generation. arXiv preprint arXiv:2312.06141, 2024. Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models, 2024a. URL https://arxiv.org/abs/2310.04406. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024b. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024c. Yiyang Zhou, Haoqin Tu, Zijun Wang, Zeyu Wang, Niklas Muennighoff, Fan Nie, Yejin Choi, James Zou, Chaorui Deng, Shen Yan, et al. When visualizing is the first step to reasoning: Mira, benchmark for visual chain-of-thought. arXiv preprint arXiv:2511.02779, 2025. Le Zhuo et al. From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning. arXiv preprint arXiv:2504.16080, 2025. 17 Figure 7 Data synthesis pipeline architecture. Three model roles coordinate via information flows: Image Gen Model produces initial images, Vision-language model verifies image and performs planning/prompt rewriting with content memory, Image Editing Model applies refinements. Trajectories loop until satisfied, producing interleaved text-image chain-of-thought data."
        },
        {
            "title": "A Data Synthesis Pipeline",
            "content": "We provide implementation details for our automated data collection pipeline that generates 12K multimodal chain-of-thought training trajectories. A.1 Pipeline Architecture The agentic framework coordinates three model roles in an iterative loop  (Fig. 7)  : Image Gen Model produces initial images from user prompts, Vision-language Model evaluates satisfaction and performs verification with content memory and subgoal decomposition, and Image Editing Model applies refinements based on VLM planning. This loop continues until the VLM determines the image satisfies all requirements, producing interleaved text-image chain-of-thought trajectories that are filtered for quality. A.2 Model Components Prompt generation: Llama-4-Scout-17B-16E generates 20K diverse prompts covering compositional attributes, spatial relations, and multi-object generation tasks based on T2I-CoReBench. Image generation: Flux Pro produces initial images from prompts. For complex prompts, Qwen3-VL decomposes prompts into subgoals and executes the first step in initial generation. Verification: Qwen3-VL evaluates whether images satisfy prompts. If not, it generates explicit chain-of-thought reasoning, identifying deficiencies, planning improvements, and specifying editing instructions. Editing: Flux Kontext or Qwen-Image-Edit applies editing instructions based on VLM planning. A.3 Example Trajectory Fig. 8 shows concrete bookshelf generation trajectory demonstrating the three cognitive behaviors induced by our framework. The VLM performs verification by identifying that books are present when the prompt specifies no books, only picture frames. It exhibits subgoal decomposition by breaking the correction into sequential stepsfirst removing books, then adding frames. Finally, it demonstrates content memory by explicitly referencing and comparing Images #1, #2, and #3 to track cumulative progress across refinement rounds. A.4 Training Data Statistics After quality filtering, we obtain 12K trajectories with the following characteristics: training trajectories average 3.6 refinement rounds (range 1-8 rounds). Training on this data requires 700 H100 GPU hours. 18 Figure 8 Detailed chain-of-thought trajectory demonstrating cognitive behaviors. This bookshelf generation example shows the models explicit reasoning through <think> blocks across three refinement rounds. Verification: the model identifies that books are present when the prompt specifies no books, only picture frames. Subgoal decomposition: the model breaks the correction into sequential stepsfirst removing books, then adding picture frames. Content memory: the model explicitly references and compares Images #1, #2, and #3 to track cumulative progress. The reasoning demonstrates how chain-of-thought enables iterative self-correction through explicit evaluation and planning. A.5 VLM Prompt Design The vision-language model uses structured prompt template  (Table 7)  to induce cognitive behaviors during data synthesis. The prompt guides the VLM through three steps: (1) detailed image description with explicit object counts and spatial relationships, (2) comparison analysis against user requirements with reflection on previous attempts, and (3) decision making between editing, backtracking, or completion. This structured reasoning naturally produces trajectories exhibiting verification, subgoal decomposition, and content memory."
        },
        {
            "title": "B Additional Qualitative Results",
            "content": "Additional qualitative examples are provided in the main paper  (Fig. 4)  , showing representative trajectories across different task types and computational budgets."
        },
        {
            "title": "C Generalization Preservation",
            "content": "Fine-tuning on 12K reasoning-heavy trajectories does not cause catastrophic forgetting of the base Bagel models general capabilities. We compare Bagel before and after fine-tuning on our data (without test-time scaling at inference): the fine-tuned model achieves 0.783 alignment on OneIG-Bench (vs. 0.764 for vanilla Bagel) and 2.26 on ImgEdit (vs. 1.31), indicating that the chain-of-thought training data improves rather than degrades the base models instruction-following capabilities even without multi-round inference. Scaling Beyond C=10 We cap evaluation at C=10 due to GPU memory constraints. We observe that image quality collapses when editing rounds produce minimal visual changes (LPIPS < 0.03 between consecutive images), as accumulated autoregressive noise degrades fidelity. Such non-improving editing steps are infrequent, so scaling remains effective up to C=10. Beyond this point, we expect TTS performance to saturate or degrade once quality collapse dominates. The exact inflection point depends on the base generation and editing model capabilities. Potential mitigations include: (a) perceptual thresholding to skip rounds with minimal changes, (b) reset 19 You are an intelligent and honest image evaluation agent. ORIGINAL USER REQUEST: prompt} [Previous fied/TODO features] images information with satis- {user_STEP 1 - IMAGE DESCRIPTION: First, describe what you see in the current image in detail: - List ALL objects present with exact counts - Describe their positions and spatial relationships - Note colors, materials, lighting, and style - Describe the overall scene composition VERIFY colors, materials, and other STEP 2 - COMPARISON ANALYSIS: Compare your image description with the user request: 1. COUNT all objects explicitly 2. CHECK spatial relationships 3. specific details 4. wrong objects to remove 5. REFLECT on previous attempts - making progress or stuck? 6. simpler language IDENTIFY correct objects to retain and If instruction failed multiple times, try STEP 3 - DECISION: Choose ONE action: ACTION: EDIT_IMAGE EDIT_INSTRUCTION: tion, focus on ONE change] SATISFIED: [features matching request with counts] TODO: [features still needed with counts] [5-18 word instrucACTION: BACKTRACK_TO_IMAGE [image number, BACKTRACK_TO: \"Image #2\"] e.g., ACTION: SATISFIED_COMPLETE SATISFIED: verification] [all requirements met with Table 7 VLM verification and planning prompt. Structured template guiding the visionlanguage model through image description, comparison analysis, and action decision. This three-step reasoning naturally induces verification, subgoal decomposition, and content memory behaviors during trajectory generation. rounds that regenerate from scratch using accumulated reasoning, and (c) adaptive noise scheduling to counteract quality degradation. 20 Figure 9 Representative failure modes. Example 1: Compositional constraints with precise object counts and spatial arrangements (napkin count). Example 2: Complex spatial relationships requiring specific geometric configurations (forks encircling plate). Examples 3,4: Layout change from the intermediate images (people count)."
        },
        {
            "title": "E Failure Analysis",
            "content": "While our approach achieves strong performance, we observe limitations in specific scenarios. Fig. 9 visualizes representative failure modes where chain-of-thought test-time scaling struggles to produce satisfactory outputs even with extended computational budget. These cases reveal fundamental challenges in precise compositional reasoning, complex spatial arrangements, and fine-grained attribute control that warrant future investigation."
        }
    ],
    "affiliations": [
        "Meta Superintelligence Labs",
        "Nanyang Technological University",
        "Stanford University"
    ]
}