{
    "paper_title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning",
    "authors": [
        "Yuanhao Wu",
        "Juntong Song",
        "Hanning Zhang",
        "Tong Zhang",
        "Cheng Niu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 3 3 5 7 1 . 6 0 5 2 : r DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning Yuanhao Wu1, Juntong Song1, Hanning Zhang2, Tong Zhang2, and Cheng Niu1 1NewsBreak 2University of Illinois Urbana-Champaign {yuanhao.wu, cheng.niu}@newsbreak.com"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose DuaShepherd, novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. unified, multi-head architecture was explored to train the two reward models in multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved remarkable success across various NLP tasks (Brown et al., 2020), yet they continue to struggle in domains that require complex, multi-step reasoning, such as mathematical problem-solving (Hendrycks et al., 2021). Recent advances have shown that prompting LLMs to think step-by-step, known as Chain-of-Thought (CoT) prompting, can significantly enhance their reasoning capabilities (Wei et al., 2023). To further improve LLMs internal capabilities in mathematical reasoning, reinforcement learning (RL) has been applied, yielding promising results. Uesato et al. (2022) demonstrated that both outcome-based and process-based reward models achieve comparable performance on mathematical 1 tasks. However, Lightman et al. (2024) showed that with larger models and more fine-grained humanlabeled data, process-based reward models (PRMs) can achieve significantly better results. Despite its effectiveness, constructing step-wise supervision data, as described in Lightman et al. (2024), requires substantial human effort to evaluate each step in an LLM-generated solution. Recent studies, like Math-Shepherd, suggests that Monte Carlo sampling can be leveraged to estimate the probability of given step leading to the correct final answer, offering scalable alternative for building PRMs while maintaining competitive performance (Wang et al., 2024b). Besides its lower construction cost, we argue that synthetic PRM data generated via Monte Carlo sampling carries different implicit meaning compared to OpenAIs PRM800K data. Although PRM800K requires steps labeled as positive to progress towards the solution, when considering neutral and positive steps as single class, their primary distinction from negative steps lies in the correctness of the step. On one hand, although PRM800K requires steps labeled as positive to progress towards the solution, when considering neutral and positive steps as single class, their primary distinction from negative steps lies in the correctness of the step. On the other hand, PRM data obtained through Monte Carlo methods, such as Math-Shepherd, emphasizes the likelihood or potential of step leading to the correct final answer. Based on this distinction, key challenge that arises is whether these two different signals can be effectively integrated into more powerful process reward model that leverages the strengths of both approaches. In this paper, we propose novel approach, DuaShepherd, that effectively leverages these two different types of reward signals. Our experiments demonstrate that simply multiplying these two scores as compound probability yields promising results. Besides, we develop an automated pipeline to construct PRM training dataset that incorporates both types of reward signals. We also find that multi-task training schema, similar to ArmoRM (Wang et al., 2024a), where single base model is shared to learn both PRM reward signals, ultimately enhances the reward models overall performance. This further confirms that these two reward can complement each other. Our analysis shows that the two types of PRM reward signals indeed exhibit different reward patterns, and unifying them into single framework leads to notable improvements in mathematical reasoning tasks. By leveraging two existing datasets, PRM800K and Math-Shepherd, and without introducing additional sampling or annotation, the DuaShepherd model proposed in this paper achieves significant improvements in MATH500 and ProcessBench, reaching new state-of-the-art under comparable conditions. These findings underscore the potential of combining multiple reward dimensions to build more capable reward models for reasoning. Our main contributions are as follows: We propose novel method that leverages two types of reasoning rewards: one that emphasizes identifying reasoning errors, and the other that focuses on evaluating the potential to arrive at the correct final answer.. We present an automated dataset construction approach that enables large-scale generation of process supervision data with two types of reward signals for mathematical reasoning. We validate our reward fusion approach across multiple tasks and datasets, demonstrating its state-of-the-art performance."
        },
        {
            "title": "2.1 LLM on Reasoning Task",
            "content": "Despite demonstrating remarkable reasoning capabilities, particularly in solving math and coding problems (Huang and Chang, 2023; Zhang et al., 2024a; Guan et al., 2025), LLMs still remain suboptimal when dealing with complex reasoning tasks, e.g. overlooking logical fallacies or calculation errors (Zhong et al., 2024; Li et al., 2024; Hendrycks et al., 2021). Researchers have found that prompting LLMs to reason step by step, known as Chain-of-Thought prompting, significantly enhances their performance across various reasoning tasks, including mathematical problemsolving (Wei et al., 2023; Chu et al., 2024). Similarly, self-reflection mechanisms, which allows LLMs evaluate and refine their own reasoning processes, have been demonstrated to enhance reasoning accuracy by identifying and correcting errors (Renze and Guven, 2024). Furthermore, integrating LLMs with external tools has been shown to further improve their mathematical reasoning capabilities (Gou et al., 2024; Wang et al., 2023)."
        },
        {
            "title": "Mathematical Reasoning Abilities",
            "content": "The mathematical reasoning capabilities of LLMs can be enhanced during two distinct training stages. Pretraining Pretraining LLMs on large amounts of math-related corpora has been proven effective in enhancing their mathematical reasoning abilities (OpenAI et al., 2024; DeepSeek-AI et al., 2024; Grattafiori et al., 2024). Post-training The pretrained models capabilities can be further improved during the posttraining stage. DeepSeekMath demonstrated that carefully curated and diverse instruction-tuning dataset can significantly enhance mathematical reasoning, even through straightforward supervised fine-tuning (Shao et al., 2024). Several studies have also shown that reinforcement learning is highly effective in strengthening LLMs internal mathematical reasoning abilities (Uesato et al., 2022; Lightman et al., 2024). Recent studies have indicated that through post-training, LLMs can exhibit emergent reasoning patterns, such as step-bystep reasoning and self-reflection, leading to significant performance improvements (Qwen, 2024b; DeepSeek-AI et al., 2025)."
        },
        {
            "title": "2.3 Reward Models",
            "content": "The reward model not only plays crucial role in reinforcement training but also enhances the models reasoning ability during inference stage by assisting in selecting high-quality generated results. In the domain of reasoning, reward models are typically categorized into two types: Outcomebased Reward Models (ORM) and Process-based Reward Models (PRM) (Uesato et al., 2022). The ORMs provides reward signals based on the correctness of the final result and has been shown to significantly improve mathematical reasoning performance (Cobbe et al., 2021). However, Lightman et al. (2024) discovered that with larger models and 2 Figure 1: Overview of the DuaShepherdpipeline. We first trained reward models from PRM800K and MathShepherd dataset separately, then we used these reward models to automatically annotate the sampled reasoning trajectories to obtain the DuaShepherd dataset with both potential and correctness reward labels. Finally, we trained multi-head model with multi-task learning on the DuaShepherd dataset. The DuaShepherd approach yields substantial performance improvement. more fine-grained human feedback signals, PRMs can achieve significantly better results. Nevertheless, collecting fine-grained human feedback often incurs substantial costs. Fortunately, recent studies have demonstrated the feasibility of automatically constructing such datasets for PRM training. Math Shepherd (Wang et al., 2024b; Kazemnejad et al., 2024) introduced novel approach that uses Monte Carlo sampling to automatically generate reward labels by evaluating whether given trajectory leads to the correct result. Building on this idea, more sophisticated algorithms, such as Monte Carlo Tree Search (MCTS) (Chen et al., 2024), have been proposed to generate step-wise preference data. Additionally, techniques like self-reflection have been incorporated into tree search methods to dynamically enhance the quality of responses (Zhang et al., 2024b). Setlur et al. (2024b) took this concept step further by not only assessing whether the current step can lead to the final result but also evaluating whether each additional step meaningfully contributes to the overall success rate. Moreover, recent work from Setlur et al. (2024a) has explored an alternative approach by directly constructing both correct and incorrect trajectories in mathematical reasoning. By leveraging reinforcement learning on these structured datasets, they demonstrated that training on perstep incorrect responses can significantly improve model generalization."
        },
        {
            "title": "3.1 PRM",
            "content": "PRMs enhance LLMs by evaluating each step of the reasoning process rather than just the final outcome. This stepwise supervision enables the identification and correction of errors as they occur, leading to improved performance on complex tasks."
        },
        {
            "title": "The PRM is often trained with the following loss",
            "content": "function: = (cid:88) i=1 [ysi log rsi +(1ysi) log(1rsi)] (1) where ysi [0, 1] represents the correct label (golden answer) for si, the i-th reasoning step of solution s. The value rsi denotes the sigmoid score assigned to si by PRM, and is the total number of reasoning steps involved for (Wang et al., 2024b). In this study, we also utilize this loss function to train the models."
        },
        {
            "title": "3.2 Construction of the DuaShepherd Dataset",
            "content": "As shown in Figure 1, to construct dataset with both potential and correctness reward labels, we leverage two existing datasets, PRM800K (Lightman et al., 2024) and Math-Shepherd (Wang et al., 2024b), along with reward models trained on each dataset separately. First, we trained correctness reward model on the PRM800K dataset. According to original label definition, during training, we treated both the neutral label and the positive label as = 1 (correct), 3 while the negative label was considered = 0 (incorrect). Next, we used the Math-Shepherd dataset to train the potential reward model with its original binary label. Unlike the PRM800K dataset, which requires substantial manual annotation costs, the MathShepherd dataset has significant advantage in that it allows for the automated generation of large volumes of high-quality data. To preserve this efficiency in our approach, we employed the model trained with the PRM800K dataset to predict the correctness of each reasoning step in the MathShepherd dataset. This process generates pseudolabels for the correctness reward. To simultaneously learn both reward using single dataset, soft labels are preferred to prevent prematurely truncating potential trajectories during joint training. Since the Math-Shepherd dataset only provides binary hard labels, we used the potential reward model to generate corresponding pseudo-soft labels as potential rewards. Through this approach, we obtained independent correctness and potential reward labels for every sample in the Math-Shepherd dataset. We named this dataset DuaShepherd, and used this dataset to train reward models combining both correctness and potential. Specifically, since the DuaShepherd dataset is constructed using Monte Carlo sampling and pseudo-labeling, its scale is easily expandable."
        },
        {
            "title": "3.3 The DuaShepherd Model",
            "content": "We employ multi-head model in conjunction with multi-task learning to simultaneously learn two types of reward labels, as shown in Figure 1. Specifically, we add separate fully connected layers upon shared base LLM to independently predict two distinct rewards: Rcorrectness = σ(wT Rpotential = σ(wT 1 + b1) 2 + b2) (2) (3) where σ denotes the sigmoid function, while w1, b1 and w2, b2 represent the weights and biases of two fully connected layers, respectively. The variable refers to the hidden state obtained from the last layer of base LLM. This design leverages the shared foundational LLM to capture general language representations, while the individual fully connected layers specialize in estimating each specific reward. For the multi-head model, we use the sum of the binary 4 cross-entropy losses from the two rewards as the training loss: Lmixed = L(yc., rc.) + L(yp., rp.) (4) Multiple dimensions of rewards offer rich and detailed signals. Rcorrectness evaluates past reasoning steps, representing the probability that the existing steps are correct. Meanwhile, Rpotential looks ahead, estimating the likelihood that the reasoning will lead to successful final solution. Given the orthogonal nature of these two metrics, we propose to use the compound probability as the final DuaShepherd reward applying the chain rule of probability. RDuaShepherd = Rcorrectness Rpotential (5)"
        },
        {
            "title": "4.1 Evaluation Datasets and Metrics",
            "content": "We conduct our experiments using two distinct datasets. The first dataset is the widely adopted MATH benchmark (Hendrycks et al., 2021), which serves as standard for evaluating mathematical problem-solving capabilities. The second dataset, ProcessBench (Zheng et al., 2024), is specifically designed to assess mathematical reasoning within the PRM framework. For the MATH dataset, in the verification scenario, we leverage computationally efficient subset, MATH500, which corresponds precisely to the test set used in Lightman et al. (2024) and Wang et al. (2024b). To evaluate different reward models, generator produces 64 candidate solutions for each test problem. Each reward model then selects the highest-scoring solution from the generated candidates. The final evaluation reports the average best-of-N accuracy across different test problems for each method. In ProcessBench, models are required to detect the earliest step that contains an error or confirm that all steps are correct. We generally follow the evaluation instructions provided by the authors. However, for models that directly output scalar value, we do not select the threshold based on the GSM8K subset of the data. Instead, we fix it at 0.5. We then report the overall performance based on this standardized procedure. Therefore, our evaluations assess both the reward models ability to select the correct final solution GENERATOR NO. MODEL ACCURACY Mistral-7B: MetaMath pass@1: 0.266 maj@64: 0.362 pass@64: 0.750 DeepSeekMath-Instruct-7B pass@1: 0.412 maj@64: 0.556 pass@64: 0.862 Qwen-2.5-Math-Instruct-7B pass@1: 0.540 maj@64: 0.840 pass@64: 0.890 1 2 3 4 1 2 3 1 2 3 4 Qwen2.5-Math-7B: PRM800K (our trained) Qwen2.5-Math-7B: Math-Shepherd (our trained) Mixing 1 & 2 with compound probability DuaShepherd (ours) DuaShepherd correctness head (ours) DuaShepherd potential head (ours) Qwen2.5-Math-7B: PRM800K (our trained) Qwen2.5-Math-7B: Math-Shepherd (our trained) Mixing 1 & 2 with compound probability DuaShepherd (ours) DuaShepherd correctness head (ours) DuaShepherd potential head (ours) Qwen2.5-Math-7B: PRM800K (our trained) Qwen2.5-Math-7B: Math-Shepherd (our trained) Mixing 1 & 2 with compound probability DuaShepherd (ours) DuaShepherd correctness head (ours) DuaShepherd potential head (ours) 0.472 0.472 0.498 0.526 0.506 0.504 0.594 0.544 0.602 0.622 0.626 0. 0.808 0.820 0.824 0.824 0.804 0.824 Table 1: Best-of-N accuracy of different generators on MATH500 with different verification models. For each prompt, we sampled 64 solution candidates. We also provide the pass@1, maj@64 (majority voting among 64 samplings) and pass@64 (upper bound) accuracies of each generator. and its capability to identify errors in the reasoning process. H100 GPUs with total of 8*80 GB of GPU memory."
        },
        {
            "title": "5 Experimental Results",
            "content": "To construct dataset with both potential and correctness reward labels, the Rcorrectness model and the Rpotential model were fine-tuned on the PRM800K and Math-Shepherd datasets respectively for total of one epoch based on the Qwen2.5-Math-7B model (Qwen, 2024a) using learning rate of 2 105. The final DuaShepherd reward model was also trained from the Qwen-2.5-Math-7B model with learning rate of 2 105. Three 7B models, Mistral-7B (Jiang et al., 2023), DeepSeekMath-Instruct-7B (Shao et al., 2024) and Qwen-2.5-Math-Instruct-7B (Yang et al., 2024) are used as answer generators in the experiment. Following the practice in Math-Shepherd, we fine-tune Mistral-7B on MetaMATH (Yu et al., 2024) for three epochs to further improve its mathematical reasoning capability. learning rate of 5 106 is used in the training. DeepSeekMath-Instruct-7B and Qwen-2.5-Math-Instruct-7B are specifically designed for mathematical reasoning, and so no further fine-tuning is needed. We selected these three models to evaluate our reward models effectiveness across generators with varying reasoning abilities. We conduct our experiments using 8 NVIDIA"
        },
        {
            "title": "5.1 Effectiveness of Compound Probability",
            "content": "As shown in Table 1, by comparing experiments 1, 2, and 3 for all the generators, it can be observed that compounding the rewards from two separately trained reward models yields better results than either reward model alone. For the DuaShepherd model, this trend holds for two out of the three generators, achieving performance comparable to or better than the stronger head. The exception is DeepSeekMath, where the significant performance gap between the two heads results in the compounded reward performing worse than the best individual head. As shown in Table 2, in the ProcessBench dataset, the model trained with PRM800K demonstrates significant advantage over the model trained with Math-Shepherd. By multiplying the two rewards, the overall performance improves from 57.5% of the PRM800K model to 63.0%. The compound probability of the DuaShepherd model also shows significant improvement over each head of the model."
        },
        {
            "title": "5.2 Effectiveness of Multi-Task Training",
            "content": "We can observe that multi-task training further enhances the capability of the reward model. Al5 MODEL GSM8K MATH OLYMPIADBENCH OMNIMATH AVG. Process Reward Models Skywork-PRM-7B (o1 Team, 2024) Llama3.1-8B-PRM-Deepseek-Data (Xiong et al., 2024) Llama3.1-8B-PRM-Mistral-Data (Xiong et al., 2024) Qwen2.5-Math-7B: PRM800K (our trained) Qwen2.5-Math-7B: Math-Shepherd (our trained) Mixing above two models rewards with compound probability Qwen2.5-Math-7B: DuaShepherd (ours) Qwen2.5-Math-7B: DuaShepherd correctness head (ours) Qwen2.5-Math-7B: DuaShepherd potential head (ours) 70.8 38.8 50.4 61.2 60.3 74.5 78.7 63.9 65.0 53.6 33.8 33.4 66.1 27.8 67.7 68.3 64.2 29.5 Language models, prompted as Critic Models Qwen2.5-72B-Instruct (Qwen, 2024a) Qwen2.5-Math-72B-Instruct (Qwen, 2024a) QwQ-32B-Preview (Qwen, 2024b) GPT-4o-0806 o1-mini 76.2 65.8 88.0 79.2 93.2 61.8 52.1 78.7 63.6 88.9 22.9 16.9 13.8 51.6 8.9 54.7 60.2 55.2 9.5 54.6 32.5 57.8 51.4 87.2 21.0 16.9 15.8 51.0 4.1 55.2 54.9 52.7 4. 52.2 31.7 61.3 53.5 82.4 42.1 25.6 28.4 57.5 25.3 63.0 65.5 59.0 27.2 61.2 45.5 71.5 61.9 87.9 Table 2: Performance of different models on ProcessBench. Our model significantly outperforms PRM models of the same size and even surpasses many large critic models with long reasoning chains. * indicates results sourced from the ProcessBench paper. though the two reward signals focus on different aspects of reasoning steps, they seem to positively influence each other. In the MATH500 dataset with the Mistral-7B generator, using only the correctness head of Model 4 as the verifier, we achieve an accuracy of 0.506, which is an improvement of 3.4% over Model 1. Using only the potential head as the verifier, the accuracy reaches 0.504, surpassing its counterpart, Model 2, by 3.2%. By comparing Experiments 3 and 4, when the rewards from both heads are multiplied to form the final verification score, the accuracy reaches 0.526, which improves upon the multiplication of labels used to train this model (i.e., Experiment 3) by 1.8%. These results also hold for more powerful generators, namely DeepSeekMathInstruct-7B and Qwen-2.5-Math-Instruct-7B. For the ProcessBench dataset, the correctness head of DuaShepherd achieves an average F1 score that surpasses its teacher model by 1.5%. Additionally, the potential head outperforms its teacher model with an average F1 improvement of 1.9%. These results highlight the benefits of joint optimization over the two reward signals, demonstrating that multi-task learning enables the reward model to capture richer reasoning patterns and enhance verification accuracy. pass@1. This result indicates that our method remains effective even for models with strong reasoning capabilities, leading to substantial improvement in reasoning accuracy. However, it is also important to note that as the generators inherent capability improves, the advantage of DuaShepherd!over majority voting diminishes. Although DuaShepherd consistently achieves higher accuracy than models trained on the two baseline datasets across all generators, we observe that as the generators capability improves, the performance gain of DuaShepherd also narrows. We argue that this phenomenon arises because the solutions in the training data for the PRM model were generated by relatively outdated models, leading to significant discrepancy between these solutions and those produced by the latest generators. Specifically, we observe that solutions generated by Qwen-2.5-Math-Instruct-7B contain substantial amount of Chinese text and garbled characters, which are absent in the PRM800K and MathShepherd datasets. Using model more aligned with the target generator to sample and generate PRM training data should improve the performance of the PRM model."
        },
        {
            "title": "5.3 Performance with Different Generators",
            "content": "As shown in Table 1, for all generators, DuaShepherd achieves an accuracy significantly higher than Figure 2 presents performance comparison of different strategies applied to varying candidate solution counts, ranging from 1 to 64, sampled from 6 This result further supports the effectiveness of our correctness reward model in capturing critical reasoning signals. These metrics indicate that our pseudo-labels are not only aligned with strong automated baselines but also capture reasoning correctness with high degree of reliability. This high label quality is crucial to the effectiveness of our reward modeling and downstream applications."
        },
        {
            "title": "6.2 Relationship of the two Rewards",
            "content": "We calculated the Pearson correlation coefficient between the two rewards of Mistral-7B MATH500 solutions candidates. The correlation coefficient for correct solutions and incorrect solutions are 0.712 and 0.392 respectively. The overall correlation coefficient is 0.812. We plotted the best Mistral-7B solutions by different reward models of MATH500 test set in Figure 3. Among all the reward models, points representing incorrect solutions cluster in the lower-left quadrant, while those representing correct solutions concentrate in the upper-right quadrant. This distribution indicates general consensus between the two reward metrics in distinguishing correct from incorrect solutions. In Figure 3(a), when only the Math-Shepherd reward model is used for verification, large number of incorrect solutions cluster around Rcorrectness = 0. This observation corroborates our earlier hypothesis that erroneous steps typically reflect low-quality (incorrect) reasoning chains. Conversely, when only the PRM800K reward model is employed for verification, numerous incorrect solutions gather around Rpotential = 0, as shown in Figure 3(b). This highlights limitation of PRM800K reward model: it struggles to select solutions with higher potential when all solution candidates are prone to errors. As illustrated in Figure 3(c), once the two rewards are compounded in DuaShepherd, the performance in these two extreme scenarios is substantially improved. Notably, within the incorrect solutions, the PRM (correctness) score demonstrates stronger predictive capability, as evidenced by the majority of points aligning along the left side of the plot. Conversely, among the correct solutions, several samples exhibit lower PRM scores. This observation suggests that erroneous intermediate steps can still lead to correct final outcomes, particularly as contemporary models exhibit enhanced self-reflection abilities. This is likely one of the Figure 2: Performance of Mistral-7B using different verification models across different numbers of solution candidates on MATH500. Mistral-7B generator on the Math500 benchmark. When varying the number of candidate solutions, combining the rewards from PRM800K and MathShepherd yields higher accuracy than using either reward individually, and our DuaShepherd model consistently further enhances this accuracy."
        },
        {
            "title": "6.1 Quality of the DuaShepherd Dataset",
            "content": "As shown in Table 3, We conducted quantitative evaluation of our annotating models, and the results suggest that they are of high quality: Potential RM For pseudo-labels generated by the potential reward model, we computed Precision, Recall, and F1 score against the original MathShepherd binary labels. Although these labels were themselves generated via Monte Carlo sampling, they provide strong baseline. The pseudo-labels from our model achieved an F1 score of 0.9346, demonstrating high consistency with the original labeling. Correctness RM For pseudo-labels from the correctness reward model, we constructed reference annotations using OpenAIs o1-mini model on 1,000 randomly sampled reasoning trajectories, following the prompt from the ProcessBench paper. This model has been reported to achieve an F1 score of 87.9 in identifying the first error in reasoning trajectory, making it reliable proxy for human judgment. Our pseudo-labelsdefined as the steps leading up to the first errorachieved an F1 score of 0.796 against the o1-mini annotations. 7 (a) Math-Shepherd (b) PRM800K (c) DuaShepherd Figure 3: Reward distribution of the best solutions picked by different reward models in the MATH500 test set. As verifier, Math-Shepherd exhibits most of its errors near the Pcorrectness = 0 region. As for the PRM800K model, it focuses on correctness; however, many of its errors arise from failing to select solutions with high Ppotential when correctness probabilities are generally low. By integrating these two reward signals, DuaShepherd effectively mitigates both types of errors, leading to significant improvement in performance. MODEL GROUND TRUTH PRECISION RECALL F1 Potential RM Correctness RM o1-mini annotation MathShepherd 94.16% 80.00% 92.77% 79.20% 93.46% 79.60% Table 3: Performance of annotating models. reasons why recent works, such as DeepSeekR1 (DeepSeek-AI et al., 2025), have abandoned the use of the process reward model."
        },
        {
            "title": "6.3 Other Reward Mixing Approaches",
            "content": "An alternative reward mixing approach involves training model by directly using the product of the two rewards as the target label. This method simplifies the training process by consolidating the reward signals into single objective, potentially enhancing the models ability to learn the combined effect of both rewards. When using this method for training, the mixed reward label is required to be obtained by multiplying two soft labels. If the original binary labels are used, it can lead to reward collapse phenomenon. Specifically, when one label is 0, the other label is completely overridden; whereas when one label is 1, it effectively results in only the other label being active. As shown in Table 4, directly distilling the product of two models labels achieves comparable but slightly worse results. Unlike learning from two independent labels, in this scenario, the model learns more abstract label, which we speculate may influence the final performance."
        },
        {
            "title": "6.4 The Weak to Strong Phenomenon",
            "content": "We also observe clear pattern similar to the weakto-strong generalization phenomenon described by OpenAI (Burns et al., 2023). We conducted the same task as Section 6.2 but using much larger 72B model. We trained Qwen-2.5-Math-72B-Instruct model with learning rate of 1 104 for 3 epoches. Due to computational constraints, the 72B model was fine-tuned using the LoRA approach (Hu et al., 2022). The LoRA configuration was set as follows: rank = 16, scaling factor lora_alpha = 32, dropout rate lora_dropout = 0.05, with target modules specified as q_proj, k_proj, v_proj. This model results in best-of-N accuracy of 56.2% in MATH500 and 67.5% average F1 in ProcessBench, which is significant higher than the the corresponding 7B reward models. We believe this phenomenon aligns with OpenAIs weak-to-strong generalization framework. The multiplication method can be seen as very weak model, and when its generated data is used to fine-tune more capable model, we consistently observe improvements as the models overall reasoning capability increases. We argue that behind the multiplication method, there must be deeper, more fundamental signal or rule that governs reasoning. Fine-tuning stronger model on data generated by weak model likely 8 APPROACH MATH500 PROCESSBENCH Multiply two rewards from two models Train single-head model with multiplication of two rewards Train multi-head model then multiply 0.498 0.514 0.526 63.0% 64.4% 65.5% Table 4: Performances of different reward mixing approaches. helps elicit and uncover parts of this underlying capability."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we have presented novel framework for integrating correctness-based and potentialbased reward signals to enhance stepwise supervision in mathematical reasoning tasks. By unifying feedback on both the correctness of individual steps and the likelihood of reaching the correct final answer, our approach captures complementary facets of the problem-solving process. Empirical evaluations on MATH500 and ProcessBench demonstrate that this combined reward substantially outperforms either reward type alone and achieves state-of-the-art under comparable resource constraints. Moreover, our multi-task training strategy shows that correctness and potential rewards can be jointly modeled in single, shared representation, further improving verification accuracy. This result confirms that the two reward signals focus on different aspects of reasoning: correctness flags stepwise errors, while potential leverages forward-looking guidance. Our findings indicate that balancing these two perspectives not only improves performance but also yields finer-grained insights into the reasoning chain. Taken together, this work offers scalable pipeline for constructing high-quality stepwise reward modeling data and flexible methodology for fusing multiple reward dimensions. Future research could investigate more advanced reward mixing techniques, better integration with treesearch-based methods, and applications to broader domains requiring multi-step reasoning. We anticipate that our dual-reward paradigm will spur further innovation in fine-grained reward modeling."
        },
        {
            "title": "Limitations",
            "content": "Despite its demonstrated improvements, our framework has limitations. First, it relies on pseudo labels generated by reward models trained on Qwen2.5-Math-7B, whose accuracy directly affects final performance. Larger or more capable models could yield more reliable labels. Second, the PRM800K dataset primarily focuses on correctness, which may not fully capture the self-reflective and adaptive reasoning of advanced LLMs. Future work might explore more flexible annotations to accommodate alternate reasoning paths. Finally, while we integrate correctness and potential signals through direct multiplication, other mixing strategies (e.g., adaptive weighting or RL-based approaches) remain underexplored and could further enhance stepwise supervision."
        },
        {
            "title": "References",
            "content": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. 2023. Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024. Step-level value preference optimization for mathematical reasoning. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, and Ting Liu. 2024. Navigate through enigmatic labyrinth survey of chain of thought reasoning: Advances, frontiers and future. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2024. Deepseek-v3 technical report. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2024. Tora: tool-integrated reasoning agent for mathematical problem solving. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit San11 gani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 10491065, Toronto, Canada. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. 2024. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. Yanda Li, Dixuan Wang, Jiaqing Liang, Guochao Jiang, Qianyu He, Yanghua Xiao, and Deqing Yang. 2024. Reason from fallacy: Enhancing large language models logical reasoning through logical fallacy understanding. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Skywork o1 Team. 2024. Skywork-o1 open series. https://huggingface.co/Skywork. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie 12 Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. Team Qwen. 2024a. Qwen2.5: party of foundation models. Team Qwen. 2024b. Qwq: Reflect deeply on the boundaries of the unknown. Matthew Renze and Erhan Guven. 2024. Self-reflection in llm agents: Effects on problem-solving performance. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. 2024a. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024b. Rewarding progress: Scaling automated process verifiers for llm reasoning. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcomebased feedback. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024a. Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024b. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. 2024. An implementation of generative prm. https: //github.com/RLHFlow/RLHF-Reward-Modeling. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Fei Yu, Anningzhe Gao, and Benyou Wang. 2024. OVM, outcome-supervised value models for planning in mathematical reasoning. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 858875, Mexico City, Mexico. Association for Computational Linguistics. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts* : Llm self-training via process reward guided tree search. In Advances in Neural Information Processing Systems, volume 37, pages 6473564772. Curran Associates, Inc. Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. 2024b. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. 13 Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2024. Processbench: Identifying process errors in mathematical reasoning. Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, and Bo Du. 2024. Achieving >97% on gsm8k: Deeply understanding the problems makes llms better solvers for math word problems."
        }
    ],
    "affiliations": [
        "NewsBreak",
        "University of Illinois Urbana-Champaign"
    ]
}