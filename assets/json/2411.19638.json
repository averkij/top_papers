{
    "paper_title": "LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification",
    "authors": [
        "Taja Kuzman",
        "Nikola Ljubešić"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers' access to relevant content. To address this challenge, we propose a teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs a Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits a high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with a relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema."
        },
        {
            "title": "Start",
            "content": "Date of current version 29 November, 2024. Digital Object Identifier - N/A - This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: Case Study in IPTC News Topic Classification 4 2 0 2 TAJA KUZMAN1,2, NIKOLA LJUBEŠIĆ1,3 1Department of Knowledge Technologies, Jožef Stefan Institute, 1000 Ljubljana, Slovenia 2Jožef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia (e-mail: taja.kuzman@ijs.si) 3University of Ljubljana, 1000 Ljubljana, Slovenia (e-mail: nikola.ljubesic@ijs.si) Corresponding author: Taja Kuzman (e-mail: taja.kuzman@ijs.si). This work was supported by the project Embeddings-based techniques for Media Monitoring Applications (L2-50070, co-funded by the Kliping d.o.o. agency), and the research programme Language resources and technologies for Slovene (P6-0411), both funded by the Slovenian Research and Innovation Agency (ARIS). 9 2 ] . [ 1 8 3 6 9 1 . 1 1 4 2 : r ABSTRACT With the ever-increasing number of news stories available online, classifying them by topic, regardless of the language they are written in, has become crucial for enhancing readers access to relevant content. To address this challenge, we propose teacher-student framework based on large language models (LLMs) for developing multilingual news classification models of reasonable size with no need for manual data annotation. The framework employs Generative Pretrained Transformer (GPT) model as the teacher model to develop an IPTC Media Topic training dataset through automatic annotation of news articles in Slovenian, Croatian, Greek, and Catalan. The teacher model exhibits high zero-shot performance on all four languages. Its agreement with human annotators is comparable to that between the human annotators themselves. To mitigate the computational limitations associated with the requirement of processing millions of texts daily, smaller BERT-like student models are fine-tuned on the GPT-annotated dataset. These student models achieve high performance comparable to the teacher model. Furthermore, we explore the impact of the training data size on the performance of the student models and investigate their monolingual, multilingual and zero-shot cross-lingual capabilities. The findings indicate that student models can achieve high performance with relatively small number of training instances, and demonstrate strong zero-shot cross-lingual abilities. Finally, we publish the best-performing news topic classifier, enabling multilingual classification with the top-level categories of the IPTC Media Topic schema. INDEX TERMS Multilingual text classification, IPTC, large language models, LLMs, news topic, topic classification, training data preparation, data annotation. I. INTRODUCTION"
        },
        {
            "title": "T OPIC classification is a significant asset in the news",
            "content": "industry, enabling the automatic identification of news topics and facilitating readers access to content that aligns with their interests. However, developing robust news topic classifier is challenging, particularly because of the absence of manually-annotated data, which are especially scarce for non-English languages. Manual annotation, although invaluable, is an expensive and labor-intensive process that cannot be quickly extended to numerous languages. Despite the emergence of zero-shot approaches employing Generative Pretrained Transformers (GPTs) that have demonstrated impressive results in various natural language processing tasks in diverse languages [1], [2], [3], their application in settings with extensive data to be processed remains impractical because of their high computational demands. more suitable technology for this setting is smaller BERTlike language model; however, such models require thousands of annotated instances for effective fine-tuning for certain task. In this paper, we introduce an approach to developing annotated training data and multilingual BERT-like news topic classifiers through teacher-student framework based on large language models (LLMs). Our methodology leverages the power of GPTs that we use as teacher models to automatically annotate news articles with top-level Media Topic labels, VOLUME , 2024 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification FIGURE 1. Pipeline for the preparation of training and test datasets for the news topic classification. introduced by the International Press Telecommunications Council (IPTC) [4]. This automatically annotated dataset is then used to train smaller XLM-RoBERTa-based student model, addressing the computational demands of the news media industry, where topic annotation needs to be scaled to millions of data points. In our experiments, we train and evaluate the models on multilingual training and test datasets comprising four diverse languages: Catalan, Croatian, Greek, and Slovenian. The selection of these languages for evaluation is on one side based on their availability in the MaCoCu corpora collection [5] to ensure high level of comparability between languagespecific datasets. Furthermore, these languages exhibit varying degrees of linguistic relatedness, with Croatian and Slovenian being closely related and the other two languages belonging to separate branches of the Indo-European language family. This distinction provides valuable insights for crosslingual experiments. In this work, we aim to address the following research questions, pertaining to the task of IPTC news topic classification: (RQ1) Can the teacher LLM, used as data annotator for news topic classification, achieve annotation quality comparable to that of human annotators? (RQ2) How much data, annotated by the teacher model, is required for the student model to achieve comparable performance to the teacher model? (RQ3) Is it necessary to include the target language in the training data, or does the fine-tuned student model exhibit satisfactory zero-shot cross-lingual capabilities? (RQ4) Does fine-tuning student model on target-language monolingual data yield better outcomes than training on multilingual dataset of equal size? In summary, this study makes the following contributions. We investigate the feasibility of the LLM teacherstudent framework for the development of accurate and computationally efficient multilingual news topic classifiers for languages that do not have readily accessible manuallyannotated training data suitable for the task. Specifically, we focus on single-label multi-class text classification employing top-level categories of the IPTC NewsCodes Media Topic schema. This study assesses the applicability of GPT model for data annotation by comparing the level of agreement between the model and human annotators to the level of agreement among the human annotators themselves. Furthermore, we examine the classification performance of the student model relative to its teacher model, while also exploring the impact of training data size on the student models performance, as well as its zeroshot cross-lingual and multilingual capabilities. Motivated by these positive results, we publish the best-performing model on the Hugging Face repository (https://huggingface. co/classla/multilingual-IPTC-news-topic-classifier), providing an IPTC news topic classification model that can be applied to any of the 100 languages covered by the XLMRoBERTa model. To the best of our knowledge, this is the first openly-available multilingual topic classifier that uses the (top-level) IPTC Media Topic categories. The remainder of this paper is organized as follows. Section II provides an overview of the existing literature on IPTC news topic classification and the application of GPT models to data annotation. In Section III, we introduce our methodology for the automatic annotation of training data by leveraging the zero-shot capabilities of large language models. In Section IV, we describe the manual annotation campaign that was undertaken to develop test set for evaluating the performance of both teacher and student models, and we evaluate the performance of the GPT model as data annotator in comparison to human annotators. Section presents the experiments involving fine-tuning BERT-like models on varying sizes of training data, and the analysis of their monolingual, multilingual, and zero-shot crosslingual performance. Section VI concludes the paper with discussion of the main findings and suggestions for future research. Additionally, the Appendix provides more details on the annotation guidelines, and the description of the toplevel IPTC Media Topic labels (Section A), and the prompt used for automatic data annotation (Section B). II. RELATED WORK The International Press Telecommunications Council (IPTC) is global organization dedicated to the development and 2 VOLUME , 2024 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification promotion of industry standards for the exchange of news data. Among its notable contributions is the maintenance of IPTC NewsCodes controlled vocabularies that are widely adopted by major media providers, including Agence FrancePresse, Associated Press, and Reuters [6]. The NewsCodes vocabularies include the Media Topic vocabulary, which has been established as global standard to ensure consistent coding of news metadata across diverse news providers [7]. The Media Topic taxonomy is structured hierarchically, with 17 topic labels at the top level. The entire taxonomy encompasses over 1,000 topic labels, organized across up to five levels of granularity [8]. Multiple studies researched news topic classification [9], there is no manually- [10], [11], [12], [13]. However, annotated reference dataset that can be used directly for the training and evaluation of IPTC Media Topic classifiers. The existing topic datasets have several limitations: 1) Inconsistent Topic Schemata: Many datasets use their own topic schemata [9], [12], [13], rendering the results of their experiments highly dataset-dependent, and consequently, incomparable with other studies. 2) Lack of Manual Annotation: Many topic datasets did not undergo manual annotation. Instead, topic categories were derived from source media websites [12], [13] or automatically annotated using topic modeling [9] based on the word2vec algorithm [14]. 3) Use of Deprecated IPTC Schema: Some datasets use the outdated IPTC Subject Codes schema (see, for instance, [15] and [11]), which was replaced by the IPTC Media Topic schema in 2010. Additionally, the IPTC Media Topic schema itself is subject to frequent updates, occurring every two to three months [16], which poses additional challenges for the development of stable reference dataset and classifiers. Recently, two news topic datasets have been introduced, both annotated with the IPTC Media Topic schema: the MNDS dataset [17], comprising approximately 10,000 English news articles, and the EventDNA dataset [18], containing around 1,800 Dutch news articles. Despite their potential, both datasets have faced criticism regarding the reliability of manually-annotated labels, as highlighted in related studies employing these datasets for machine learning experiments [19], [20]. Beyond problems with reliability, these datasets exhibit additional limitations. The MN-DS dataset was derived from the NELA-GT-2018 dataset [21], which was originally developed for misinformation research. As result, it includes sources known to disseminate fake news, which may compromise the performance of models trained on it when applied to mainstream news texts. The EventDNA dataset, on the other hand, is limited by its annotations being performed at the lowest levels of the IPTC Media Topic taxonomy. Specific sublabels do not always align well with top-level labels, which restricts the utility of the dataset for automatic text classification on top-level labels. Additionally, while EventDNA is freely accessible, restrictions on thirdparty sharing pose challenges for conducting experiments with closed-source large language models. Prior research has investigated range of machine learning models for the automatic classification of texts into IPTC news topics. The experiments encompass both traditional non-neural models, such as logistic regression, the Naive Bayes classifier, and support vector machines [17], [20], as well as deep learning techniques, particularly BERT-like Transformer models [11], [17], [19]. Most of these studies have demonstrated that Transformer-based models consistently achieve state-of-the-art performance, indicating their potential in the domain of news topic classification [17], [19]. Recently introduced Generative Pretrained Transformer (GPT) models, commonly known as large language models (LLMs), have demonstrated impressive performance across range of text classification tasks, even when used in zeroshot prompting fashion that does not require any training data [1], [22], [23]. Furthermore, recent research in the field of natural language processing (NLP) has embraced LLMs as catalysts for redefining the landscape of data annotation in machine learning and NLP [24]. Specifically, researchers are exploring the potential of GPT-based annotation of training data to replace time-consuming manual annotation, with the aim of using these annotations to fine-tune or improve the performance of other GPT or BERT-like models [24], [25], [26], [27]. Although LLMs have been widely adopted to generate and annotate instruction-tuning datasets [28], [29], [30], their use in the annotation of text classification data has been less explored. Meng et al. [31] contributed to this area by employing GPT model to generate both texts and labels for training datasets through zero-shot prompting approach, and by fine-tuning smaller language model on the generated training dataset. In contrast to this approach, our study refrains from generating synthetic data to mitigate the potential LLM biases. Instead, we use authentic news articles sourced from the web and employ GPT model exclusively for classifying these texts into Media Topic labels. This GPT-based data annotation approach was also shown to be promising in the domain of sentiment analysis, where student models fine-tuned on datasets annotated by GPT model exhibited comparable performance to models finetuned on datasets annotated by human annotators [27]. recent investigation into the applicability of GPT models for IPTC news topic classification was conducted by Fatemi et al. [20] who evaluated GPT model on the MN-DS dataset [17]. The applicability of GPT models for topic annotation was evaluated by training various machine learning models on the GPT-annotated dataset and comparing their performance against models trained on the same dataset, but manuallyannotated. The findings indicate the promising performance of the GPT model used as data annotator. However, it is important to note that the performance of the model was not directly evaluated on manually-annotated test set. Consequently, the accuracy of the GPT model in this task remains undetermined. Building on existing research, our study explores the use VOLUME , 2024 3 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification of GPT-based data annotation to fine-tune BERT-like topic classifier. This research addresses multiple gaps in the literature. First, we assess the performance of both the teacher GPT model and fine-tuned student models on reliably manuallyannotated data. Second, we compare the annotation reliability of the GPT model with the reliability of human annotators. Additionally, our research extends machine learning experiments across multiple languages, offering valuable insights into the models multilingual and cross-lingual performance. III. LLM-BASED DATASET DEVELOPMENT In this section, we present our methodology for the automatic annotation of training data aimed at fine-tuning multilingual Transformer model that employs top-level IPTC Media Topic labels. The proposed methodology leverages three resources that have become available recently: MaCoCu Corpora [5]: The MaCoCu corpora collection comprises comparable web corpora in 10 lessresourced European languages, including Slovenian, Croatian, Catalan, Greek, Turkish, Icelandic, Ukrainian, and others. Developed through web crawling, MaCoCu corpora represent some of the largest text collections available for these languages. They encompass highquality texts [32] from wide range of sources and text genres [33]. X-GENRE Classifier [1]: This multilingual Transformerbased genre classifier categorizes texts in various languages into set of genre labels, such as News, Promotion, and Opinion/Argumentation. The X-GENRE classifier allows us to efficiently extract relevant news content from general web corpora, which is crucial for the present task that is focused on news articles. Multilingual GPT Models: The multilingual Generative Pretrained Transformer (GPT) models have demonstrated impressive zero-shot classification capabilities across numerous languages and tasks. Inter alia, these models have shown high performance on South Slavic languages, including Croatian and Slovenian that are included in our experiments [34]. Specifically, we employ the GPT-4o model (version gpt-4o-2024-05-13) [35] provided by the OpenAI API, which outperformed other openand closed-source GPT models in our preliminary experiments. A. EXTRACTION OF NEWS TEXTS AND PRE-PROCESSING The pipeline for developing training and test data for news topic classification using the LLM teacher-student framework is illustrated in Fig. 1. The datasets used in this study are sourced from the Catalan (ca) [36], Croatian (hr) [37], Greek (el) [38], and Slovenian (sl) [39] MaCoCu web corpora that have been annotated with genres (genre-annotated datasets are available as part of the MaCoCu-Genre corpus collection [40]). Given the focus of the IPTC topic classification on news articles, we extract from the web corpora the subset of news articles, identified with the X-GENRE classifier [1]. Furthermore, the dataset undergoes additional pre-processing, wherein the texts are truncated to the initial 512 words, which is constraint imposed by the BERT-like models. B. AUTOMATIC ANNOTATION Automatic annotation involves using the GPT-4o model to assign 17 top-level IPTC Media Topic labels to news texts. We use labels from the Media Topic schema version from October 24, 2023. The annotation is applied to two separate samples, randomly extracted from the pre-processed news text collection described in the previous section: first batch of 21,000 texts is annotated for the development of training and development datasets, and second batch of 8,000 texts is annotated to create test set. Both batches comprise equal numbers of instances in the four languages, namely Catalan (ca), Croatian (hr), Greek (el), and Slovenian (sl). The GPT-4o model is used in zero-shot prompting manner. It is instructed to assign to each text one of the 17 main IPTC Media Topic labels, such as politics and society. The prompt (see Appendix B) was constructed based on the results of the preliminary experiments. It includes the task description and list of labels with their descriptions (provided in Appendix A). The annotation of 29,000 texts cost approximately 230=C and took approximately six hours. This cost and time investment are deemed minimal compared to the resources that would be required for the manual annotation of similar volume of instances. C. TRAINING AND DEVELOPMENT DATASETS To construct the training and development datasets, the first annotated batch of 21,000 instances is split into subsets of 20,000 and 1,000 texts, respectively, while maintaining stratification based on the GPT-assigned labels. The resulting dataset, which comprises both splits, has been published under the name the EMMediaTopic dataset in the CLARIN.SI repository (http://hdl.handle.net/11356/ 1991) [41]. The distribution of labels within the EMMediaTopic dataset is shown in Fig. 2. The dataset is shown to be relatively balanced by labels, with the most prevalent label (sport) accounting for approximately 15% of the instances. Moreover, the distribution of labels is consistent across the four languages. The least represented labels are weather, accounting for less than 1% of instances, and religion and labour, each assigned to less than 2% of texts. D. TEST DATASET The test dataset is derived from the second batch of 8,000 automatically annotated texts, comprising texts in the same four languages as the training data, namely Slovenian, Croatian, Catalan, and Greek. To assess the models performance across all 17 top IPTC Media Topic labels, the test set is balanced by GPT-assigned labels which is achieved by selecting 18 instances per label per language (or fewer if there are fewer than 18 label instances in the batch). The selection of texts to be manually-annotated comprised 1,199 instances with balanced distribution across languages and GPT-assigned 4 VOLUME , 2024 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification FIGURE 2. Distribution of labels in the GPT-annotated EMMediaTopic dataset comprising training and development splits (in percentages). TABLE 1. GPT-4o performance on each language in the human-labeled test set in micro-F1 and macro-F1 scores, averaged across three prediction iterations, with standard deviation included. Hr Ca Sl El Entire Test Set Micro-F1 0.714 0.002 0.703 0.002 0.741 0.000 0.730 0.009 0.722 0.002 Macro-F1 0.721 0.001 0.702 0.001 0.748 0.001 0.738 0.009 0.731 0.003 labels. Finally, the test set was manually annotated to obtain human gold labels. Details on the annotation campaign are provided in the next section. E. TEACHER MODEL PERFORMANCE The first application of the manually-annotated test set was to measure the performance of the GPT-4o model that was used for automatic annotation. The model demonstrates consistently high performance, with an average macro-F1 score of 0.731 and micro-F1 score of 0.722. Its performance across different languages is stable, with 5-point difference between the highest macro-F1 score (on Slovenian) and the lowest (on Catalan), as shown in Table 1. IV. MANUAL ANNOTATION OF TEST DATA The test sample, balanced by GPT-assigned labels and the four languages, is manually annotated by single annotator. The annotator was provided with annotation guidelines with the description of the labels (see Appendix A) and received brief training on the annotation process. The description of the labels is based on the official definitions provided by the IPTC [42]. However, as these definitions offer only high-level understanding of the distinctions between labels, we enriched the descriptions with examples of their lower-level labels to facilitate more precise and consistent annotation. In addition to the standard 17 top-level IPTC Media Topic labels, three additional labels have been introduced to assist the annotator in identifying and excluding texts that are unsuitable for the purposes of our task: do not know (highly challenging instance lacking clear topic), not news (text of different genre, e.g., Promotion), and multiple (presence of multiple texts in single instance). The manual annotation process excluded 70 instances (5.83%) as unsuitable. Among these, 4% were excluded because of the absence of clear topic, 2% because they contained multiple texts in single instance, and 0.1% because of incorrect genre classification. The final test sample consists of 1,129 text instances. The test dataset is available on request from the corresponding author. It is not publicly released to prevent its integration into large language models (LLMs) during their pretraining or fine-tuning phase to maintain the integrity of future model evaluations. A. INTER-ANNOTATOR AGREEMENT To assess the reliability of human and LLM annotations, subset of 339 instances from the test set, balanced by labels and languages, was annotated by second annotator. The nominal Krippendorffs alpha [43] between the two annotators reached 0.728, surpassing the acceptable threshold of 0.667, as defined by Krippendorff [43]. This moderate level of agreement indicates that the annotation campaign was successful while also highlighting the challenging nature of this task. As shown in Table 2, the inter-annotator agreement VOLUME , 5 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification TABLE 2. Pair-wise inter-annotator agreement in terms of the nominal Krippendorffs alpha. Annotators 1st ann & 2nd ann 1st ann & GPT-4o 2nd ann & GPT-4o Krippendorffs alpha 0.728 0.693 0.752 FIGURE 3. Label-level inter-annotator agreement in terms of the nominal Krippendorffs alpha. between each of the human annotators and the LLM model was on par with the agreement between the two human annotators. This finding addresses Research Question 1 (RQ1), demonstrating that the LLM model is as capable of data annotation for this task as the human annotators. We also assess the agreement between human and LLM annotators at the label level. Fig. 3 illustrates the substantial variation in inter-annotator agreement across topic labels, highlighting labels that are less clearly defined, which leads to confusion among annotators. The labels sport and weather are well comprehended by both human and LLM annotators, achieving high inter-annotator agreement above 0.90 in terms of Krippendorffs alpha. Labels religion, lifestyle and leisure, and society present challenges for the LLM, with human annotators demonstrating significantly higher agreement. The label economy, business and finance is particularly problematic for both humans and the LLM, with an average interannotator agreement of 0.43. TABLE 3. Intra-annotator agreement in terms of the nominal Krippendorffs alpha for the human annotator (1st annotator) and the LLM teacher model. Annotator GPT-4o Human annotator (1st ann) Krippendorffs alpha 0.934 0.796 B. INTRA-ANNOTATOR AGREEMENT To further assess the reliability of the annotations provided by humans and LLMs, we instruct both human annotator and the GPT-4o model to re-annotate the subset that was previously used to calculate the inter-annotator agreement. The annotators are given the same instructions as in the initial annotation process, and the labels previously assigned to the texts are hidden from them. Intra-annotator agreement is evaluated using Krippendorffs alpha measuring consistency between two annotation runs conducted by the same annotator. Table 3 presents the intra-annotator agreement for the human and LLM annotators. The human annotator achieved relatively high Krippendorffs alpha of approximately 0.80, indicating reliable level of self-agreement. The GPT-4o model demonstrated even greater consistency with Krippendorffs alpha of 0.93, highlighting the reliability of large language models in producing consistent responses. However, while such consistency is also advantageous, more diverse responses, similar to those obtained from the human annotator, can be beneficial for various research directions, including learning from disagreement [44]. V. STUDENT MODEL FINE-TUNING EXPERIMENTS In this section, we describe the experiments involving the fine-tuning of student BERT-like models on training data that were automatically annotated by the teacher GPT model. In Section V-A, we present the student model is used in the experiments that examine (1) the impact of training data size on classification performance (Section V-B), (2) the models label-level performance (Section V-C), and (3) the models monolingual, multilingual, and cross-lingual capabilities (Section V-D). A. STUDENT MODEL As the student model, we select the large-sized XLMRoBERTa model [45] (available on the Hugging Face repository: https://huggingface.co/FacebookAI/xlm-roberta-large), which we fine-tune on the teacher-annotated EMMediaTopic dataset. This BERT-like encoder model, pretrained on hundred languages, has demonstrated strong multilingual and cross-lingual performance across various text classification tasks, which also involved the languages used in our experiments [1], [32], [46]. The optimal hyperparameters were identified via hyperparameter search performed on the development dataset. Appendix provides comprehensive description of the selected hyperparameters. The models, fine-tuned on the teacher-annotated training dataset, are evaluated on the test dataset via micro-F1 and macro-F1 scores to assess the performance at both the instance and label levels, respectively. Each model was trained and tested at least three times to gain an understanding of the performance consistency. The reported scores represent the mean values calculated across all iterations and are accompanied by standard deviation. In the experiments related to the training data size, five iterations were performed per data 6 VOLUME , 2024 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification FIGURE 4. Performance in micro-F1 and macro-F1 scores of the XLM-RoBERTa (XLM-R) model fine-tuned on various sizes of training data, compared to the zero-shot GPT-4o performance as the upper limit. The scores are averaged across five iterations of fine-tuning and evaluation, each using different random sample of specified size, drawn from the training dataset. point owing to the addition of the subset selection process, which was an additional source of variation of the results. B. IMPACT OF THE TRAINING DATA SIZE In this section, we address Research Question 2 (RQ2). First, we explore the feasibility of achieving high performance of BERT-like models on the news topic classification task when fine-tuned on automatically annotated data. Second, we investigate the minimum amount of training data required to attain performance level comparable to that of the teacher model. To this end, we fine-tune the XLM-RoBERTa model on the subsets from the original training set, which are stratified by label size and balanced across languages. The smaller training datasets comprise 1,000, 2,500, 5,000, 10,000, and 15,000 instances. Fig. 4 shows the impact of the training data size on the model performance. The performance of the GPT-4o model is considered to be an upper limit, as it served as the teacher model for annotating the training dataset for the student model. The level of improvement begins to plateau after 15,000 instances. Notably, student models trained on datasets comprising 15,000 data points or more exhibit performance levels that are comparable to those of the teacher GPT model. There is less than one-point difference in macro-F1 and micro-F1 scores between the student and teacher models. These findings confirm the feasibility of achieving high performance in the news topic classification task using student model trained on data annotated by teacher model. Fine-tuning student models on training datasets comprising 15,000 data points or more provides optimal results; however, the findings also reveal that the models exhibit substantial performance even with minimal training data. Specifically, FIGURE 5. Confusion matrix for the best-performing XLM-RoBERTa student model. student models trained on datasets comprising 1,0005,000 instances achieve average micro-F1 scores ranging from 0.678 to 0.704 and macro-F1 scores between 0.692 and 0.716. Notably, the student model trained on only 1,000 instances demonstrates performance that is only four points lower than that of the teacher GPT model. C. LABEL-LEVEL PERFORMANCE The best-performing version of the XLM-RoBERTa student model, namely, version trained on 15,000 instances, has been made available in the Hugging Face repository (accessible at https://huggingface.co/classla/ multilingual-IPTC-news-topic-classifier). This model attains macro-F1 score of 0.746 and micro-F1 score of 0.734. Further insights into the performance of the model at the label level can be obtained from the confusion matrix depicted in Fig. 5, which displays the true versus predicted classifications. The labels that are most accurately predicted by the model are weather, sport, and religion, with label-level F1 scores above 0.89. These findings are consistent with the labellevel inter-annotator agreement shown in Fig. 3 (see Section IV-A), which identified these labels as those with the highest consensus among human annotators. Interestingly, despite the weather label being represented by less than 1% of the instances of the training dataset, this did not negatively impact the performance of the model. The model demonstrated near-perfect classification accuracy for this label, achieving an F1 score of 0.94. In contrast, the most challenging labels for the XLMRoBERTa student model are lifestyle and leisure, human VOLUME , 2024 TABLE 4. Performance in macro-F1 scores of monolingual and multilingual models trained on 5,000 instances and evaluated on each language in the test split, and on the entire test split. The results of the monolingual training and evaluation are highlighted in gray color. The highest score for each language-specific test set is highlighted in bold. GPT-4o performance is added as the upper limit, as the models were trained on its predictions. The reported scores represent the average results across three iterations of training and evaluation, with the standard deviation provided. Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification Hr Model Ca Model Sl Model El Model Multilingual GPT-4o Hr Test 0.701 0.017 0.706 0.012 0.711 0.007 0.674 0.004 0.707 0.011 0.721 0.001 Ca Test 0.672 0.005 0.671 0.008 0.660 0.016 0.662 0.012 0.656 0.024 0.702 0.001 Sl Test 0.733 0.014 0.728 0.005 0.736 0.014 0.716 0.011 0.741 0.007 0.748 0. El Test 0.739 0.011 0.715 0.014 0.721 0.013 0.706 0.013 0.729 0.004 0.738 0.009 Entire Test Set 0.716 0.009 0.710 0.007 0.713 0.005 0.695 0.006 0.714 0.009 0.731 0.003 interest, and economy, business and finance, as evidenced by label-level F1 scores ranging from 0.59 to 0.62. This is not surprising, given that these categories have also been identified as challenging for both human and LLM annotators. As shown in Fig. 5, the label economy, business and finance is frequently misclassified as science and technology when the texts pertain to the sales of technology products; or as environment in instances where the texts address electricity infrastructure. The label human interest is often mistaken for arts, culture, entertainment and media, because both categories can encompass content related to celebrities. The former is typically associated with tabloid-style presentations of celebrity lives, whereas the latter is intended to cover more serious topics. Similarly, the label lifestyle and leisure frequently overlaps with both human interest and arts, culture, entertainment and media, as it encompasses individuals recreational activities, which may intersect with topics represented by the other two labels. These overlaps illustrate the complexities inherent in IPTC topic classification, which pose challenges to both manual annotation and automatic classification. These challenges arise primarily from the broad nature of top-level labels, which often leads to intersections among categories. potential solution to these issues is to conduct classification with labels from the lower levels of the Media Topic schema, which are more specific but also more challenging due to their high number. D. MONOLINGUAL, MULTILINGUAL AND CROSS-LINGUAL PERFORMANCE The preceding section demonstrated the potential for achieving high performance using the LLM teacher-student setup for the task of news topic classification. This section explores the applicability of student models to languages on which they were not fine-tuned. We analyze the performance of fine-tuned models in monolingual, multilingual, and crosslingual settings, addressing Research Questions 3 (RQ3) and 4 (RQ4). Specifically, we fine-tune the XLM-RoBERTa model on monolingual subsets extracted from the original training dataset comprising only Croatian (hr), Slovenian (sl), Catalan (ca), or Greek (el) instances. Each monolingual subset consists of 5,000 instances in the target language, and the subsets have similar label distributions. Additionally, the evaluation includes the multilingual model, trained on samples comprising 5,000 instances (as described in Section V-B), distributed equally across the four languages. The training dataset for the multilingual model is limited to 5,000 instances to ensure that the multilingual model is trained on dataset of equivalent size to that of the monolingual models and to eliminate any potential influence of the dataset size on the results. The models are evaluated on the manually-annotated test dataset, split into target language subsets. Each monolingual test subset comprises approximately 300 instances. The reported scores are averaged across the three iterations of finetuning and evaluation. Table 4 presents the performance of the monolingual and multilingual models across four languages included in the test set. In zero-shot cross-lingual scenario where the models are evaluated on language different from the one they were fine-tuned on , the models demonstrate relatively high macro-F1 scores, ranging from 0.66 to 0.74. Moreover, the zero-shot cross-lingual performance is often comparable to, and in most cases even exceeds, the models performance in monolingual and multilingual settings where the models are evaluated on the language which is included in their (either monolingual or multilingual) fine-tuning dataset. These findings, pertaining to Research Question 3 (RQ3), demonstrate that student models exhibit high performance even when applied to languages that are not part of the finetuning training dataset. Second, we compare the performance of student models on target language when they are fine-tuned on 5,000 instances of (1) monolingual data in the target language, or (2) multilingual data that are balanced across the target language and three additional languages. Through these experiments, we address Research Question 4 (RQ4), which focuses on investigating whether the development of language-specific models offers any advantages over the development of multilingual model trained on an equivalent amount of data, consisting of instances in the target language and other languages. If the experiments would show that monolingual results are significantly higher, this might require to host potentially many language-specific models for cases where top performance is important enough. The results presented in Table 4, where the monolingual performance is highlighted in gray, however, reveal that the multilingual model achieves performance that is comparable to, or on certain languagespecific test sets surpasses that of the model fine-tuned only 8 VOLUME , Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification on the target language. This finding highlights the benefits of training student models on multiple languages, through which the robustness of the model is quite likely to improve. More importantly, this result allows the final inference procedure to be very simple, with single multilingual model that can be applied to any text written in one of the 100 supported languages on which the XLM-RoBERTa model was pretrained [45]. VI. CONCLUSION This paper presents novel methodology for developing annotated training data using teacher-student framework based on large language models. By fine-tuning smaller Transformer model on the data annotated by GPT model, this methodology allows for scalable classification of news topics without the need for manual annotation of the training data. The code for the development of GPTannotated training data and XLM-RoBERTa topic classifiers is publicly accessible (https://github.com/TajaKuzman/ IPTC-Media-Topic-Classification). The IPTC topic training dataset is developed by extracting news articles from web corpora in four languages (Slovenian, Croatian, Greek, and Catalan) based on automatic genre identification using the X-GENRE classifier [1] and automatic topic annotation using the GPT model as teacher model. The EMMediaTopic dataset [41] has been made freely available in the CLARIN.SI repository (http://hdl.handle.net/11356/ 1991). The GPT model, used in zero-shot prompting fashion, demonstrates high performance across all languages and labels, with an average macro-F1 score of 0.731 and microF1 score of 0.722. The inter-annotator agreement between the two human annotators is comparable to the agreement between the teacher LLM model and each of the human annotators. Furthermore, the intra-annotator agreement analysis reveals that the GPT model demonstrates significantly higher consistency in topic labeling than the human annotator. The results indicate that large language models possess comparable ability to human annotators in labeling training data with news topic categories, but with less variation in the output. GPT models are computationally too demanding for scaling topic annotation to millions of data points that regularly need to be annotated, as is the case in the news media industry. Thus, we develop smaller BERT-like models, considered as student models, by fine-tuning them on the GPT-annotated EMMediaTopic training dataset. We show that the developed training data are of sufficient quality to achieve high performance of the student models. This finding confirms the feasibility of the LLM teacherstudent framework for the development of multilingual news topic classifiers. Specifically, the XLM-RoBERTa-based student models, when fine-tuned on 15,000 or more data points, demonstrate performance comparable to that of the GPT teacher model, with difference of less than one point in both the micro-F1 and macro-F1 scores. Motivated by positive results, we publish the best-performing student model on Hugging Face (https://huggingface.co/classla/ multilingual-IPTC-news-topic-classifier), providing the first open-source IPTC Media Topic classification model for the top layer of the Media Topic schema that can be applied to any language covered by the XLM-RoBERTa model. The model was trained on 15,000 instances in four languages, and it achieves micro-F1 score of 0.734 and macro-F1 score of 0.746. Furthermore, we delve deeper into the constraints of the performance of the student models, investigating their ability to generalize in zero-shot cross-lingual scenario. The experiments with student models fine-tuned either on monolingual or multilingual training data and evaluated on four languages show high zero-shot cross-lingual performance, yielding macro-F1 scores between 0.66 to 0.74. Moreover, the zero-shot cross-lingual performance exceeds the models performance in monolingual and multilingual settings. These results indicate that incorporating the target language within the training dataset is not essential for achieving high topic classification performance in that language. Further experiments concerning the scalability of IPTC classification across multiple languages investigate the potential advantages of developing language-specific student models compared with constructing multilingual model finetuned on the same amount of data in four languages, thereby less data in the target language. An analysis of the performance of monolingual versus multilingual models across four target languages reveals that the multilingual model achieves performance that is either comparable or superior to that of models fine-tuned exclusively on the target language. This finding suggests that multilingual models are more effective approach to catering to multiple languages. These models provide high performance across multiple languages while requiring less annotated data in the target language and incurring smaller processing and storage costs compared to the development of individual models for each language. In this study, we employ only the 17 top-level categories of the IPTC Media Topic hierarchical schema. The analysis of the agreement between human annotators and the LLM as data annotator for each category, as well as the confusion matrix for the fine-tuned model, highlighted challenges associated with the ambiguity of the top-level labels, which, in certain instances, are not clearly delineated. In future work, we plan to extend our experiments to include more fine-grained categories from the lower levels of the IPTC schema, which are more useful for some users and news media providers. We consider following the hierarchical approach proposed by [20] and [19], where the top label is assigned to the text first with the classifier presented in this paper, and then based on the predicted label, more specialized lower-layer classifier is selected to make deeper level-two predictions. Motivated by positive results, we intend to expand our experiments involving the LLM teacher-student framework to topic classification across various domains, such as parliamentary proceedings and web corpora. Furthermore, we aim VOLUME , 9 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification to explore the applicability of the proposed methodology to additional text classification tasks, including automatic genre identification, which is recognized as challenging task for human annotators [47]. 10 VOLUME , 2024 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification APPENDIX ANNOTATION GUIDELINES a: General Instructions Annotate the provided texts with the IPTC topic labels, provided below. If it is hard to choose between two labels, you can also help yourself by searching through the taxonomy of sublabels (https://www.iptc.org/std/NewsCodes/treeview/ mediatopic/mediatopic-en-GB.html) where you might find the specific topic of your text and see under which of out toplevel labels it fits (but do not spend too much time on it, if it is very hard to decide, rather annotate it as do not know). b: Additional Labels for Hard Cases We are interested only in reasonable and informative instances. That is why we added to the list of labels three more categories for you to discard the text: do not know: if it is impossible to decide under which label this instance fits. not news: if the text is not news article, for example, it is an advertisement, recipe, legal text, blog post, etc. multiple: if the instance is not single text, but consists of multiple texts. IPTC LABEL DESCRIPTION Table 5 presents the IPTC Media Topic labels along with their descriptions, as provided to the human annotators and GPT model. APPENDIX PROMPT FOR AUTOMATIC ANNOTATION Fig. 6 shows the prompt presented to the GPT model for the purpose of automatic annotation. The prompt includes descriptions of the labels, presented in Table 5. VOLUME , 2024 11 TABLE 5. IPTC Media Topic labels and their descriptions, which have been constructed by the authors of this paper. Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification Label arts, culture, entertainment and media conflict, war and peace crime, law and justice Description News about cinema, dance, fashion, hairstyle, jewellery, festivals, literature, music, theatre, TV shows, painting, photography, woodworking, art exhibitions, libraries and museums, language, cultural heritage, news media, radio and television, social media, influencers, and disinformation. News about terrorism, wars, wars victims, cyber warfare, civil unrest (demonstrations, riots, rebellions), peace talks and other peace activities. News about committed crime and illegal activities, the system of courts, law and law enforcement (e.g., judges, lawyers, trials, punishments of offenders). Man-made or natural events resulting in injuries, death or damage, e.g., explosions, transport accidents, famine, drowning, natural disasters, emergency planning and response. health labour education environment human interest lifestyle and leisure disaster, accident and emergency incident economy, business and finance News about companies, products and services, any kind of industries, national economy, international trading, banks, (crypto)currency, business and trade societies, economic trends and indicators (inflation, employment statistics, GDP, mortgages, ...), international economic institutions, utilities (electricity, heating, waste management, water supply). All aspects of furthering knowledge, formally or informally, including news about schools, curricula, grading, remote learning, teachers and students. News about climate change, energy saving, sustainability, pollution, population growth, natural resources, forests, mountains, bodies of water, ecosystem, animals, flowers and plants. News about diseases, injuries, mental health problems, health treatments, diets, vaccines, drugs, government health care, hospitals, medical staff, health insurance. News about life and behaviour of royalty and celebrities, news about obtaining awards, ceremonies (graduation, wedding, funeral, celebration of launching something), birthdays and anniversaries, and news about silly or stupid human errors. News about employment, employment legislation, employees and employers, commuting, parental leave, volunteering, wages, social security, labour market, retirement, unemployment, unions. News about hobbies, clubs and societies, games, lottery, enthusiasm about food or drinks, car/motorcycle lovers, public holidays, leisure venues (amusement parks, cafes, bars, restaurants, etc.), exercise and fitness, outdoor recreational activities (e.g., fishing, hunting), travel and tourism, mental well-being, parties, maintaining and decorating house and garden. News about local, regional, national and international exercise of power, including news about election, fundamental rights, government, non-governmental organisations, political crises, non-violent international relations, public employees, government policies. News about religions, cults, religious conflicts, relations between religion and government, churches, religious holidays and festivals, religious leaders and rituals, and religious texts. News about natural sciences and social sciences, mathematics, technology and engineering, scientific institutions, scientific research, scientific publications and innovation. News about social interactions (e.g., networking), demographic analyses, population census, discrimination, efforts for inclusion and equity, emigration and immigration, communities of people and minorities (LGBTQ, older people, children, indigenous people, etc.), homelessness, poverty, societal problems (addictions, bullying), ethical issues (suicide, euthanasia, sexual behaviour) and social services and charity, relationships (dating, divorce, marriage), family (family planning, adoption, abortion, contraception, pregnancy, parenting). News about sports that can be executed in competitions, e.g., basketball, football, swimming, athletics, chess, dog racing, diving, golf, gymnastics, martial arts, climbing, etc.; sport achievements, sport events, sport organisation, sport venues (stadiums, gymnasiums, ...), referees, coaches, sport clubs, drug use in sport. News about weather forecasts, weather phenomena and weather warning. science and technology weather religion politics society sport FIGURE 6. Prompt used for automatic annotation of data with the GPT-4o model. 12 VOLUME , 2024 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification APPENDIX FINE-TUNING HYPERPARAMETERS FOR THE XLM-ROBERTA MODELS TABLE 6. Number of epochs used for fine-tuning the XLM-RoBERTa model, depending on the size of the training data (number of instances). Training Data Size 20,000 15,000 10,000 5,000 2,500 1, Epoch Number 3 5 9 10 22 24 In the fine-tuning experiments, we use the following hyperparameters that were determined through hyperparameter search on the development dataset: learning rate of 8e6, train batch size of 32, and maximum sequence length of 512 tokens. The optimal number of training epochs was found to depend on the size of the training data, as shown in Table 6. VOLUME , 2024 13 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification REFERENCES [1] T. Kuzman, I. Mozetič, and N. Ljubešić, Automatic Genre Identification for Robust Enrichment of Massive Text Collections: Investigation of Classification Methods in the Era of Large Language Models, Machine Learning and Knowledge Extraction, vol. 5, no. 3, pp. 11491175, 2023. [2] H. Wibowo, E. Fuadi, M. Nityasya, R. E. Prasojo, and A. Aji, COPALID: Indonesian Language Reasoning with Local Culture and Nuances, in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 2024, pp. 14041422. [3] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research, vol. 24, no. 240, pp. 1113, 2023. [4] IPTC, Media Topics, https://iptc.org/standards/media-topics/, accessed October 29, 2024. [5] M. Bañón, M. Esplà-Gomis, M. L. Forcada, C. García-Romero, T. Kuzman, N. Ljubešić, R. van Noord, L. P. Sempere, G. Ramírez-Sánchez, P. Rupnik et al., MaCoCu: Massive collection and curation of monolingual and bilingual data: focus on under-resourced languages, in 23rd Annual Conference of the European Association for Machine Translation, 2022, pp. 301302. [6] IPTC, NewsCodes, https://iptc.org/standards/newscodes/, accessed October 29, 2024. [7] IPTC, What is IPTC? https://iptc.org/about-iptc/, accessed October 29, 2024. [8] IPTC, Groups of NewsCodes, https://iptc.org/standards/newscodes/ groups/#descrncd, accessed October 29, 2024. [9] H. Roberts, R. Bhargava, L. Valiukas, D. Jen, M. M. Malik, C. S. Bishop, E. B. Ndulue, A. Dave, J. Clark, B. Etling et al., Media cloud: Massive open source collection of global news on the open web, in Proceedings of the International AAAI Conference on Web and Social Media, vol. 15, 2021, pp. 10341045. [10] A. N. Chy, M. H. Seddiqui, and S. Das, Bangla news classification using Naive Bayes classifier, in 16th Intl Conf. Computer and Information Technology. IEEE, 2014, pp. 366371. [11] M. Pranjić, M. Robnik Šikonja, and S. Pollak, An evaluation of BERT and Doc2Vec model on the IPTC subject codes prediction dataset, Proceedings of the 24th International Multiconference Information Society IS 2021: Volume - Data Mining and Data Warehouses Conference SiKDD, p. 2528, 2021. [12] R. Misra, News category dataset, arXiv preprint arXiv:2209.11429, 2022. [13] I. Kosem, J. Čibej, K. Dobrovoljc, T. Kuzman, and N. Ljubešić, Spremljevalni korpus Trendi in avtomatska kategorizacija, Slovenščina 2.0: empirične, aplikativne in interdisciplinarne raziskave, vol. 11, no. 1, pp. 161 188, 2023. [14] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, Distributed representations of words and phrases and their compositionality, Advances in Neural Information Processing Systems, vol. 26, 2013. [15] STT, Finnish News Agency Archive 1992-2018, source, accessed October 29, 2024. [Online]. Available: http://urn.fi/urn:nbn:fi:lb-2019041501 [16] IPTC, NewsCodes Guidelines, https://www.iptc.org/std/NewsCodes/ guidelines/, accessed October 29, 2024. [17] A. Petukhova and N. Fachada, MN-DS: multilabeled news dataset for news articles hierarchical classification, Data, vol. 8, no. 5, p. 74, 2023. [18] C. Colruyt, O. De Clercq, T. Desot, and V. Hoste, EventDNA: dataset for Dutch news event extraction as basis for news diversification, Language Resources and Evaluation, vol. 57, no. 1, pp. 189221, 2023. [19] O. De Clercq, L. De Bruyne, and V. Hoste, News topic classification as first step towards diverse news recommendation, Computational Linguistics in the Netherlands Journal, vol. 10, pp. 3755, 2020. [20] B. Fatemi, F. Rabbi, and A. L. Opdahl, Evaluating the Effectiveness of GPT Large Language Model for News Classification in the IPTC News Ontology, IEEE Access, 2023. [21] J. Nørregaard, B. D. Horne, and S. Adalı, NELA-GT-2018: large multilabelled news dataset for the study of misinformation in news articles, in Proceedings of the International AAAI Conference on Web and Social Media, vol. 13, 2019, pp. 630638. [22] N. Ljubešić, N. Galant, S. Benčina, J. Čibej, S. Milosavljević, P. Rupnik, and T. Kuzman, DIALECT-COPA: Extending the Standard Translations of the COPA Causal Commonsense Reasoning Dataset to South Slavic Dialects, in Proceedings of the Eleventh Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial 2024), 2024, pp. 8998. [23] F. Huang, H. Kwak, and J. An, Is ChatGPT better than human annotators? Potential and limitations of ChatGPT in explaining implicit hate speech, in Companion Proceedings of the ACM Web Conference 2023, 2023, pp. 294297. [24] Z. Tan, A. Beigi, S. Wang, R. Guo, A. Bhattacharjee, B. Jiang, M. Karami, J. Li, L. Cheng, and H. Liu, Large language models for data annotation: survey, arXiv preprint arXiv:2402.13446, 2024. [25] Y. Zeng, Y. Mu, and L. Shao, Learning reward for robot skills using large language models via self-alignment, arXiv preprint arXiv:2405.07162, 2024. [26] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang, and C. Gan, Principle-driven self-alignment of language models from scratch with minimal human supervision, Advances in Neural Information Processing Systems, vol. 36, 2024. [27] B. Ding, C. Qin, L. Liu, Y. K. Chia, B. Li, S. Joty, and L. Bing, Is GPT-3 Good Data Annotator? in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 11 17311 195. [28] Z. Wang, C.-L. Li, V. Perot, L. Le, J. Miao, Z. Zhang, C.-Y. Lee, and T. Pfister, CodecLM: Aligning Language Models with Tailored Synthetic Data, in Findings of the Association for Computational Linguistics: NAACL 2024, 2024, pp. 37123729. [29] C. Xu, D. Guo, N. Duan, and J. McAuley, Baize: An open-source chat model with parameter-efficient tuning on self-chat data, in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 62686278. [30] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi, Self-Instruct: Aligning Language Models with Self-Generated Instructions, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 13 484 13 508. [31] Y. Meng, J. Huang, Y. Zhang, and J. Han, Generating training data with language models: Towards zero-shot language understanding, Advances in Neural Information Processing Systems, vol. 35, pp. 462477, 2022. [32] R. van Noord, T. Kuzman, P. Rupnik, N. Ljubešić, M. Esplà-Gomis, G. Ramírez-Sánchez, and A. Toral, Do Language Models Care about Text Quality? Evaluating Web-Crawled Corpora across 11 Languages, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 52215234. [33] T. Kuzman, P. Rupnik, and N. Ljubešić, Get to know your parallel data: Performing English variety and genre classification over MaCoCu corpora, in Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023), 2023, pp. 91103. [34] N. Ljubešić, T. Kuzman, P. Rupnik, I. Vulić, F. Schmidt, and G. Glavaš, JSI and WüNLP at the DIALECT-COPA Shared Task: In-Context Learning From Just Few Dialectal Examples Gets You Quite Far, in Proceedings of the Eleventh Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial 2024), 2024, pp. 209219. [35] OpenAI, Hello GPT-4o, https://openai.com/index/hello-gpt-4o/, 2024, accessed September 11, 2024. [36] M. Bañón, M. Chichirau, M. Esplà-Gomis, M. L. Forcada, A. GalianoJiménez, C. García-Romero, T. Kuzman, N. Ljubešić, R. van Noord, L. Pla Sempere, G. Ramírez-Sánchez, P. Rupnik, V. Suchomel, A. Toral, and J. Zaragoza-Bernabeu, Catalan web corpus MaCoCu-ca 1.0, 2023, Slovenian Language Resource Repository CLARIN.SI. [Online]. Available: http://hdl.handle.net/11356/1837 [37] , Croatian web corpus MaCoCu-hr 2.0, 2023, Slovenian Language Resource Repository CLARIN.SI. [Online]. Available: http://hdl.handle. net/11356/ [38] , Greek web corpus MaCoCu-el 1.0, 2023, Slovenian Language Resource Repository CLARIN.SI. [Online]. Available: http://hdl.handle. net/11356/1839 [39] , Slovene web corpus MaCoCu-sl 2.0, 2023, Slovenian Language Resource Repository CLARIN.SI. [Online]. Available: http://hdl.handle. net/11356/1795 [40] T. Kuzman and N. Ljubešić, Genre-enriched web corpora MaCoCuGenre, 2024, Slovenian Language Resource Repository CLARIN.SI. [Online]. Available: http://hdl.handle.net/11356/1969 [41] , Multilingual IPTC Media Topic dataset EMMediaTopic 1.0, 2024, Slovenian Language Resource Repository CLARIN.SI. [Online]. Available: http://hdl.handle.net/11356/1991 14 VOLUME , Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification [42] IPTC, IPTC Media Topic NewsCodes Tree View, https://www. iptc.org/std/NewsCodes/treeview/mediatopic/mediatopic-en-GB.html, accessed October 29, 2024. [43] K. Krippendorff, Content analysis: An introduction to its methodology. Sage Publications, 2018. [44] A. N. Uma, T. Fornaciari, D. Hovy, S. Paun, B. Plank, and M. Poesio, Learning from disagreement: survey, Journal of Artificial Intelligence Research, vol. 72, pp. 13851470, 2021. [45] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, É. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, Unsupervised Cross-lingual Representation Learning at Scale, in 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 8440 8451. [46] M. Ulčar, A. Žagar, C. S. Armendariz, A. Repar, S. Pollak, M. Purver, and M. Robnik-Šikonja, Evaluation of contextual embeddings on lessresourced languages, arXiv preprint arXiv:2107.10614, 2021. [47] T. Kuzman and N. Ljubešić, Automatic genre identification: survey, Language Resources and Evaluation, pp. 134, 2023. VOLUME , 2024 Kuzman and Ljubešić: LLM Student-Teacher Framework for IPTC News Topic Classification TAJA KUZMAN received the M.S. degree in translation between Slovenian, French and English from the University of Ljubljana, Slovenia, in 2019. Since 2021, she is pursuing her Ph.D. in information and communication technologies at the Jožef Stefan International Postgraduate School, Ljubljana, Slovenia. Since 2021, she is Research Assistant at the Department of Knowledge Technologies at the Jožef Stefan Institute in Ljubljana, Slovenia. Her research interests involve language resource collection and curation, and the development of technologies for automatic annotation using deep learning, including text classification tasks, such as automatic genre identification. Ms. Kuzman is co-leader of CLASSLA, the CLARIN ERIC knowledge center, which offers expertise on language resources and technologies for South Slavic languages. She is one of the main developers of the CLASSLAweb corpora, the largest text collections for these languages, and conducts evaluations of NLP models on South Slavic languages and dialects. As an author or co-author of 67 resources, she is one of the main contributors of text datasets and language technologies for Slovenian and other European languages at the CLARIN.SI repository. NIKOLA LJUBEŠIĆ received his PhD at the University of Zagreb on the topic of event detection in news data streams in 2009. Currently he works as senior researcher at the Department of Knowledge Technologies at the Jožef Stefan Institute in Ljubljana, Slovenia. He is also affiliated with the Faculty for Computer and Information Science at the University of Ljubljana, as well as with the Institute for Contemporary History in Ljubljana. His research interests lie in the fields of computational linguistics and computational social science, with an emphasis on South Slavic linguistic and cultural space. He is co-author of the CLASSLA-Stanza linguistic processing pipeline and CLASSLA-web corpora, both covering the range of South Slavic languages. He also co-authored the training data and various benchmarks for the same language group. His general interest in language variation has motivated him to expand his focus from textual modality to speech modality, inside which he is developing methodology to scale spoken data collection and automatic enrichment. He is co-leading the CLASSLA knowledge center for South Slavic languages with the first author. 16 VOLUME ,"
        }
    ],
    "affiliations": [
        "Department of Knowledge Technologies, Jožef Stefan Institute, 1000 Ljubljana, Slovenia",
        "Jožef Stefan International Postgraduate School, 1000 Ljubljana, Slovenia",
        "University of Ljubljana, 1000 Ljubljana, Slovenia"
    ]
}