{
    "paper_title": "RecTok: Reconstruction Distillation along Rectified Flow",
    "authors": [
        "Qingyu Shi",
        "Size Wu",
        "Jinbin Bai",
        "Kaidong Yu",
        "Yujing Wang",
        "Yunhai Tong",
        "Xiangtai Li",
        "Xuelong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io."
        },
        {
            "title": "Start",
            "content": "RecTok: Reconstruction Distillation along Rectified Flow Qingyu Shi1,3, Size Wu2, Jinbin Bai1, Kaidong Yu3, Yujing Wang1, Yunhai Tong1, Xiangtai Li2, Xuelong Li3 1Peking University 2Nanyang Technological University 3TeleAI 5 2 0 D 5 1 ] . [ 1 1 2 4 3 1 . 2 1 5 2 : r Figure 1. (a) presents the core insights of our approach. Unlike previous works, we enhance semantic information along the forward pass of the rectified flow via reconstruction distillation. Fig. (b) shows that increasing the latent space dimension consistently improves the generation performance of RecTok, indicating that the dimensional bottleneck no longer constrains the semantic information encoded in the latent features. (c) compares the gFID convergence across training epochs, where our method converges 7.75 faster than prior works and achieves final gFID of 1.34 without classifier-free guidance, the state-of-the-art gFID performance to date."
        },
        {
            "title": "Abstract",
            "content": "Visual tokenizers play crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, fundamental trade-off is inherent between dimensionality and generation quality, constrainlatent spaces. ing existing methods to low-dimensional Although recent works have leveraged vision foundation models (VFMs) to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstructionalignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distill the semantic inforThis work was completed at TeleAI. Project lead. Corresponding author. mation in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-theart results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https:// shi-qingyu.github.io/rectok.github.io/. 1. Introduction Diffusion modeling [13, 16, 21, 27, 35, 41] has become the dominant paradigm for image and video generation. As crucial component, the visual tokenizer [25, 65] projects images from raw pixels to compact latent space. Since the denoising network [5, 34, 38] is trained entirely in the latent space, computational cost is significantly reduced. However, the latent space is typically restricted to low feature dimensions to simplify diffusion training [13, 27, 41], 1 which in turn limits both reconstruction fidelity and semantic expressiveness [50, 62]. Therefore, expanding the latent space while maintaining generative stability becomes fundamental challenge in training visual tokenizers. To address this limitation, previous methods [7, 60, 62] distill semantic information from vision foundation models (VFMs) [17, 26, 37, 53] into the latent space, aiming to enrich representation capacity and accelerate generative convergence. However, their generation quality in high dimensions still lags behind their low-dimensional counterparts. Thus, these approaches remain constrained to low-dimensional latent spaces (e.g., dimension 32). Recently, RAE [69] increases DiT width to accommodate high-dimensional latents for diffusion training, achieving promising generative performance. However, its reconstruction performance lags behind previous methods limitation that is detrimental to tasks such as editing [1, 2, 14, 18, 48] and personalized generation [42, 47]. Furthermore, RAE does not systematically explore how dimensionality affects reconstruction, generation, and semantic representation. In this work, we revisit this question and present principled framework for training high-dimensional visual tokenizers without compromising performance. Unlike previous works [6, 7, 62] that directly inject semantics to the un-noised latent x0, we take more trainingconsistent perspective: Since DiT is trained on the forward flow {xt [0, 1]} rather than on x0, we enhance the semantics of all flow states xt. To understand the importance of semantic consistency along the flow, we first evaluate the discriminative capability of latent features across xt. As shown in Fig. 2, the linear probing accuracy of several representative tokenizers [25, 61, 62] drops remarkably as the latent is propagated along the forward flowthe very representations that DiT receives during diffusion training. This degradation highlights the need for semantic enhancement throughout the entire flow, not just at x0. In this work, we propose RecTok with two key innovations to enhance semantic consistency along the forward flow, simultaneously improving dimensionality and generative quality."
        },
        {
            "title": "The first",
            "content": "innovation is Flow Semantic Distillation (FSD). Our key insight is to distill the semantics of VFMs into the forward flow trajectory {xt [0, 1]}, which represents the interpolation of clean data x0 and noise x1. We utilize lightweight semantic decoder to extract semantic features from points along the flow. These features are supervised by the corresponding representations from VFMs, as illustrated in Fig. 1 (a). FSD explicitly encourages the forward flow path {xt [0, 1]} to remain semantically discriminative. Consequently, our RecTok exhibits even better accuracy on the flow than the latent features, as shown in Fig. 2. The second innovation is Reconstruction and Alignment Distillation (RAD). Inspired by masked image modeling methods [17, 56, 71], which obtain semantically rich features through pixel or feature reconstruction, we introduce reconstructive target during FSD. Specifically, we apply random masks to the input image and reconstruct the missing regions based on the visible noisy latent features. We align the reconstructed latent features with full image features extracted from VFMs. Following previous works [61, 62, 69], we train and evaluate our tokenizer and DiT [69] on the ImageNet-1K dataset [43]. As the latent dimensionality increases, we observe consistent improvements across reconstruction, generation, and linear probing tasks. Compared to other distillation or VFM-based visual tokenizers, our approach exhibits clear advantage in convergence speed and generation quality, especially under without classifier-free guidance [20] setting. To summarize, our key contributions include: We identify the significance of enhancing semantics of forward flow trajectories, and introduce FSD and RAD that effectively expedite diffusion training. Our tokenizer achieves an effective balance among reconstruction, generation quality, and semantic representation. We demonstrate that all three aspects mentioned above can be consistently improved by increasing the dimensionality of the latent space. 2. Related Work Visual Tokenizers for Image Generation. Broadly, visual tokenizers fall into two categories: discrete and continuous. Discrete tokenizers quantize image features with learnable codebook [12, 36, 40, 63], and later works focus on enlarging the codebook and improving utilization [64, 65]. Despite these advances, their reconstruction quality remains inferior to continuous tokenizers, limiting downstream generative performance [3, 4, 46, 59]. Continuous tokenizers instead map images into continuous latent space. Representative models such as VAE [25] regularize this latent space using KL loss, while subsequent works [12] introduce perceptual and adversarial losses to improve reconstruction quality. Recent studies [7, 62] have also shown the advantage of aligning the latent space with the features of Vision Foundation Models (VFMs) [37, 53], which accelerates convergence and improves downstream generation quality. However, these approaches still restrict the latent representation to low-dimensional space, constraining semantic expressiveness and reconstruction fidelity. In contrast, our work further expands the dimensionality of the latent space and observes continued improvements in generation quality. High-dimensional Latent Space for Diffusion Models. high-dimensional latent space is crucial for high-fidelity reconstruction and preserving rich semantic information. However, scaling latent dimensionality presents an inherent optimization challenge that often degrades generative per2 the velocity field: xt = (1 t) x0 + x1, vt = dxt dt = x1 x0. (1) During training, neural network vθ(x, t) is optimized on the forward flow {xt (0, 1]} to approximate the velocity field as: LRF = t, x0, x1 (cid:104)(cid:13) (cid:13) vθ(xt, t) (x1 x0) (cid:13) 2 (cid:13) 2 (cid:105) , (2) During generation, vθ(x, t) predicts the velocity that gradually transforms noisy data into clean data x0 through the ODE solver [32, 33]. Visual Tokenizers. To reduce the training cost of generation models, prior works [13, 27, 41] project images into compact latent space via an encoder-decoder image tokenizer [25]. Given an input image RHW 3, the encoder Eθ produces latent x0 Rhwc, and the decoder Dϕ reconstructs ˆI: x0 = Eθ(I), ˆI = Dϕ(x0). (3) Both the encoder and decoder are typically based on CNN [28] or ViT [11] architectures. Considering computational efficiency and scalability [69], we adopt ViT-based encoderdecoder design in this work. The training of visual tokenizers typically involves multiple objectives, including reconstruction loss, perceptual loss, GAN loss [36], and KL loss [25]. Moreover, recent studies [62] have shown that distilling semantic information from Vision Foundation Models (VFMs) [17, 37] into the latent space can accelerate the convergence of downstream generative models and further improve image quality. Overall, the general loss function of the visual tokenizer can be formulated as follows: = λrec Lrec+λper Lper+λGAN LGAN+λKL LKL+λsem Lsem. (4) 3.2. Our Method: RecTok Our motivation is illustrated in Fig. 2. Although previous methods improve the semantic information in x0, the discriminative ability of xt deteriorates significantly when training the diffusion transformers (DiTs). Our key insight is to enhance the semantic information not only in x0, but also the forward flow {xt [0, 1]}, where the DiTs are trained. In the following section, we present two key innovations that enhance semantic representation throughout the forward flow. Flow Semantic Distillation (FSD). Our goal is to make every point xt along the forward flow discriminative and semantically rich. Fortunately, the forward flow from data x0 to noise ϵ is independent of the velocity network vθ(x, t), (a) RecTok, L.P. Acc.=55.61% (b) VA-VAE, L.P. Acc.=18.36% (c) DeTok, L.P. Acc.=10.2% (d) VAE, L.P. Acc.=5.4% Figure 2. Linear probing results on xt. We evaluate the discriminative ability of representative tokenizers on the forward flow through linear probing on xt. Specifically, we fix = 0.5. As shown in Fig. 2b2d, both the t-SNE visualization and the linear probing accuracy demonstrate that their latent features perform poorly during the training of DiT. In contrast, our RecTok exhibits clear advantage even under noise interpolation. formance. Although VA-VAE [62] alleviates part of this difficulty via VFM loss, its convergence in high dimensions remains noticeably slower than that of low-dimensional variants. Another line of works [6, 68] initialize the visual encoder with VFMs. Yet, these methods still project highdimensional features into low-dimensional latents (e.g., dimension 32), inevitably discarding rich semantics in the VFMs. More recently, RAE [69] makes the first attempt to perform diffusion directly in the high-dimensional feature space of VFMs. However, because the VFM is kept frozen, this approach inevitably loses fine-grained details, leading to reconstruction artifacts. In this work, we develop highdimensional visual tokenizer that simultaneously excels in reconstruction fidelity, generative capability, and semantic representation. 3. Method 3.1. Rectified Flow in Image Generation Flow Matching. Flow matching methods [31] construct distribution transformation between data x0 and noise x1 through forward and reverse flows. As representative approach, Rectified Flow [32] adopts forward flow defined by linear interpolation, which simplifies the formulation of 3 Figure 3. Pipeline of RecTok. During the training of RecTok, we apply random mask to the input image and encode the visible regions using the encoder to obtain x1. We then sample time step and use the forward flow to generate the corresponding xt. Subsequently, xt is fed into two decoders: the Semantic Decoder reconstructs the features of VFMs, while the Pixel Decoder reconstructs the pixel space. After training, both the Semantic Decoder and VFMs are discarded, ensuring the efficiency of RecTok during inference. allowing us to obtain xt = (1 t)x0 + tϵ, [0, 1] easily through interpolation between the encoded x0 = Eθ(I) and Gaussian noise ϵ. Each xt is then decoded by lightweight semantic decoder Dsem to obtain semantic features, which are supervised by Vision Foundation Models (VFMs) EVFM: Lsem = 1 cos(Dsem(xt), EVFM(I)) (5) Specifically, the lightweight semantic decoder Dsem adopts transformer architecture with only 1.5M parameters. lightweight design enforces the encoder to capture richer semantic representations, as an overly powerful semantic decoder would otherwise draw away the semantic information from the encoder. We remove the normalization on Dsem(xt) and EVFM(I) for simplicity. During FSD, we need to sample the timestep t. Considering the redundancy in high-dimensional latent spaces, we apply dimension-dependent shift to the distribution of t, following RAE [69], and sample it as follows: = st 1 + (s 1)t , U(0, 1), = (cid:114) 4096 r2d (6) where r, is the resolution and dimension of the latent feature, respectively. Reconstruction and Alignment Distillation (RAD). Inspired by masked image modeling methods [7, 17, 61], which enforce the model to learn robust representations by predicting unseen image patches. To further enhance the semantics along the flow. We introduce reconstruction target = (1 t)xvis in the FSD. Specifically, we apply random masks to the input image. We use random mask ratio between -0.1 and 0.4. negative ratio means that no mask is applied. After encoding the visible image into latent feature xvis 0 , we utilize semantic decoder to reconstruct VFM features based on the xvis 0 + tϵ. To ensure compatibility with the reconstruction task, we utilize transformer-based semantic decoder Dsem. The semantic loss Lsem is applied to both masked and unmasked regions. Our ablation study demonstrates that jointly performing semantic alignment and reconstruction yields the best overall performance. Dimension of Latent Space. fundamental limitation of previous tokenizers in generative models is their confinement to low-dimensional latent spaces. Although semantic distillation [62] and channel regularization [8] partially alleviate this issue, the best practice remains restricted to 32 dimensions. We progressively increase the dimensionality of the latent space. As shown in Tab. 2. Interestingly, this leads to consistent improvements in reconstruction (rFID, PSNR), generation (gFID, IS), and semantics (linear probing). This finding suggests the emergence of shared latent space in higher dimensions that effectively supports lowlevel and high-level tasks. Decoder Finetuning. After joint pixel and VFM-feature training, we freeze the encoder to preserve the learned latent semantics and finetune only the pixel decoder for image reconstruction. We disable the FSD and RAD and remove the losses LKL and Lsem. While we do not claim this as our primary contribution, it is crucial step to improve reliability 4 Method Epochs Params Generation@256 w/o guidance Generation@256 w/ guidance gFID IS Prec. Rec. gFID IS Prec. Rec. Autoregressive VAR [52] MAR [30] l-DeTok [61] Pixel Diffusion ADM [10] RIN [22] PixelFlow [9] PixNerd [54] Latent Diffusion DiT [38] MaskDiT [70] SiT [34] MDTv2 [15] VA-VAE [62] AFM [6] REPA [66] DDT [55] REPA-E [29] RAE [69] RecTok (Ours) 350 800 800 400 480 320 160 1400 1600 1400 1080 80 800 800 80 80 400 80 800 80 800 80 600 2.0B 943M 479M 554M 410M 677M 700M 675M 675M 675M 675M 675M 675M 675M 675M 675M 839M 839M 1.92 2.35 1.86 323.1 227.8 238.6 10.94 3.42 - - 9.62 5.69 8.61 - 4.29 2.17 2.04 7.94 5.90 6.62 6.27 3.46 1.83 2.16 1. 2.09 1.34 101.0 182.0 - - 121.5 177.9 131.7 - - 205.6 206.2 121.3 157. 135.2 154.7 159.8 217.3 214.8 242.9 198.6 254.6 0.82 0.79 0.82 0.69 - - - 0.67 0.74 0.68 - - 0.77 0.76 0.69 0.70 0.69 0.68 0.77 0. 0.82 0.79 0.79 0.78 0.59 0.62 0.61 0.63 - - - 0.67 0.60 0.67 - - 0. 0.67 0.64 0.69 0.67 0.69 0.63 0.66 0.59 0.63 0.62 0. 1.73 1.55 1.35 3.94 - 1.98 2.15 2.27 2.28 2.06 1.58 - 1.35 1.37 - 1. 1.52 1.26 1.67 1.26 1.13 1.48 1.13 350.2 303.7 304.1 215.8 - 282.1 297. 278.2 276.6 270.3 314.7 - 295.3 293.6 - 305.7 263.7 310.6 266.3 314. 262.6 223.8 289.2 0.82 0.81 0.81 0.83 - 0.81 0.79 0.83 0.80 0.82 0.79 - 0. 0.79 - 0.80 0.78 0.79 0.80 0.79 0.78 0.79 0. 0.60 0.62 0.62 0.53 - 0.60 0.59 0.57 0.61 0.59 0.65 - 0.65 0.65 - 0. 0.63 0.65 0.63 0.66 0.67 0.65 0.67 Table 1. Class-conditional performance on ImageNet 256256. RecTok reaches an FID of 1.34 and an IS of 254.6 without guidance, outperforming previous methods by large margin. With AutoGuidance [23], it achieves an FID of 1.13 and an IS of 289.2 using only 600 epochs, representing the best overall performance. and quality of the reconstruction. We show the performance improvement in Tab. 11. 4. Experiment 4.1. Implementation Details Visual Tokenizer. We adopt an architecture and training strategy largely following l-DeTok [61]. Specifically, we employ ViT-B [11] backbone equipped with ROPE [51], SwiGLU [45], and RMSNorm [67] for both encoder and decoder. To investigate the impact of dimension on semantics, generation, and reconstruction, we train models with latent dimensions of 16, 32, 64, and 128. Note that this only affects the dimensionality of the ViTs linear head, so the resulting changes in parameter count and computational cost are negligible. Reparameterization and KL divergence are used to regularize the latent space. We train our tokenizer on the ImageNet-1K training set for 200 epochs. We 5 Table 2. Results across different feature dimensions. L.P. Acc. (L) denotes linear probing accuracy on latent features, while L.P. Acc. (SL) refers to linear probing accuracy on second-last layer features. As the feature dimension increases, discriminative ability, reconstruction, and generation show consistent gains. Table 3. Tokenizer comparison on ImageNet-1K. We compare RecTok with representative tokenizers in terms of parameters, GFLOPs, reconstruction, and generation. RecTok achieves the best performance among ViT-based tokenizers. Dim L.P. Acc. (L) L.P. Acc. (SL) rFID PSNR gFID 16 32 64 128 24.1 38.8 47.2 55.4 62.9 63.7 65.0 68. 0.74 0.71 0.66 0.65 22.75 24.08 24.93 25.28 2.75 2.64 2.57 2.27 set λrec = 1.0, λper = 1.0, λadv = 0.5, λkl = 1 106, and λsem = 1. The learning rate is set to 4 104, with linear warmup during the first 50 epochs followed by cosine decay schedule for the remaining 150 epochs. We use global batch size of 1024 and an EMA rate of 0.999. We evaluate our tokenizer through rFID [19] and PSNR on the ImageNet-1K validation set. Diffusion Model. For the diffusion model, inspired by the advanced architecture of DiTDH [69], we utilize DiTDH-XL as our diffusion transformers. We train DiTDH-XL on ImageNet-1K using rectified flow with timestep shift strategy. The model is trained for 800 epochs with an initial learning rate of 2 104 and global batch size 1024, followed by linear decay to 2105 after 40 epochs. During training, we apply gradient clipping with value of 1.0 and no weight decay. We use an EMA rate of 0.995, and all evaluations are conducted using the EMA-weighted model. For the ablation studies, the model trains for 80 epochs using the same training strategy. We evaluate the diffusion models on the ImageNet-1K validation set, measuring gFID, Inception Score (IS) [44], Precision, and Recall. We utilize AutoGuidance [23] as the classifier-free guidance method. The bad version in the AutoGuidance is DiTDH-S trained on ImageNet-1K for 30 epochs. During inference, we sample 150 steps using the Euler solver with timestep shift; In the ablation studies, we sample only 50 steps and skip decoder finetuning to reduce computational cost. All experiments are conducted on 32 H100 GPUs. Training the RecTok requires roughly 19 hours, while DiTDH requires 10 hours for 80 epochs and 3 days for 600 epochs. 4.2. Main Results Tokenizer Performance. In Tab. 3, we systematically compare RecTok with other representative tokenizers [7, 41, 61, 62, 69] in terms of parameter numbers, computational cost, reconstruction, and generation performances. Although RecTok has more parameters than CNN-based tokenizers, it achieves the lowest computational cost thanks to its efficient ViT architecture. Moreover, ViT-based tokenizers benefit from modern acceleration methods. In terms of reconstruction, RecTok significantly outperforms other ViTTokenizer Params GFlops ImageNet rFID PSNR gFID SD-VAE VA-VAE MAETok DeTok RAE RecTok 84M 70M 176M 176M 395M 176M 26.04 26.30 23.61 23.53 18.98 26.16 Numbers reported from the original papers. 445 310 54.2 44.4 128.9 44. 0.62 0.28 0.48 0.52 0.57 0.48 8.30 2.17 2.21 1.86 1.51 1.34 Figure 4. Visualization of latent features. We present the PCA projection and cosine similarity heatmap of the latent features from RecTok and VA-VAE. RecTok exhibits more semantically rich latent space. based methods, while also achieving state-of-the-art performance in generation. Furthermore, RecTok yields semantically richer latent space, outperforming prior tokenizers in linear probing  (Fig. 2)  . In Fig. 4, we visualize the PCA and similarity heatmap, where RecTok exhibits more semantically structured latent space compared with VA-VAE. Overall, RecTok achieves the best trade-off among reconstruction fidelity, generation quality, and semantic representation. In terms of latent dimension, we gradually expand the dimensionality of the latent space. As shown in Tab. 2, we observe clear trend that the reconstruction, generation, and discriminative performances consistently improve as the dimension increases. To the best of our knowledge, RecTok is the first work to demonstrate such improvement across all three aspects. Generation Comparison. As shown in Tab. 1, RecTok with DiTDH-XL achieves the best gFID=1.34 without classifier-free guidance [20]. When employing classifierfree guidance, we adopt the AutoGuidance strategy and achieve gFID of 1.13, matching the gFID of RAE [69] while showing clear advantage in Inception Score Figure 5. Qualitative results on ImageNet-1K 256 256. We show selected examples of class-conditional generation using DiTDH-XL with AutoGuidance. Table 4. Ablations on flow semantic distillation. We compare our FSD with the x0 semantic distillation using cosine similarity and VF loss. The experimental results demonstrate that FSD yields significant improvement. Setting Sem Loss L.P. Acc. rFID gFID IS w/o FSD Cos Sim VF Loss [62] FSD Cos Sim 44.35 37. 55.40 0.69 0.72 0.65 3.35 3.91 2.27 157.3 142. 196.4 served. This suggests that distilling semantic information along the flow matching path benefits both generation performance and semantic representation. In Tab 5, we compare different λsem, considering overall performance, we set λsem = 1. Ablations on Noise Schedule. As shown in Tab. 8, we compare different noise sampling strategies during RecTok training, including the Dimension-dependent Shift (referred to as Shift), Uniform, and Logit-Normal (Lognorm) methods. We observe that uniform sampling achieves the best reconstruction performance but performs the worst in generation quality. In contrast, the shift strategy yields slightly lower reconstruction scores but delivers the best generative results. Considering that the reconstruction quality can be further improved through decoder finetuning, as shown in Tab. 11, we adopt the Shift noise schedule as our default configuration. Ablations on VFMs. We explore the impact of different VFMs in FSD, including DINOv3 [49], DINOv2 [37], SigLIP2 [53], RADIOv2 [39], and SAM [26]. All models use the large-size variants to align with the VA-VAE experiments. We find that DINOv3 achieves the best performance in high-dimensional latent spaces, while DINOv2 performs best in low-dimensional latent spaces. In particular, we also (a) FID-10K and IS-10K vs Training Epochs. FID converges around 600 epochs (between 4.0 4.1), while IS keeps increasing. (b) FID-50K vs Model Parameters. Larger models yield lower gFID; evaluated with the 80 epochs checkpoint. (c) FID-10K and IS-10K vs Sampling Steps. Strong performance is achieved by 60 steps (trained for 600 epochs). (d) FID-10K and IS-10K vs CFG. Performance across classifier-free guidance scales. Figure 6. FID and IS under different settings. (IS) [44]. In Fig. 6a, we plot the gFID-10K and IS-10K curves over training epochs. We observe that gFID-10K stabilizes around 600 epochs, and therefore use the checkpoint at epoch 600 for reporting final results. The scaling results are presented in Fig. 6b, where the latent space of RecTok demonstrates strong scaling capability. Figs. 6c6d illustrate how different inference settings affect gFID and IS. Considering the overall performance, we sample 150 steps with guidance scale of 1.29. 4.3. Ablation Studies Ablations on FSD. In Tab. 4, we study the effectiveness of FSD (i.e., applying distillation on xt). When we only align the latent features x0 to VFM features, notable degradation of performance in generation and linear probing is ob7 Table 5. Ablations on semantic loss weight λsem. λsem = 1 achieves the best generation performance. Table 8. Comparison of different sampling distributions. Since the reconstruction quality can be improved through decoder finetuning, we adopt the Shift schedule. λsem L.P. Acc. rFID gFID IS 0.5 1 54.8 55.4 56.1 0.59 0.65 0.87 2.78 2.27 2.43 179.5 196.4 199.7 Noise Schedule L.P. Acc. rFID PSNR gFID IS Uniform Lognorm Shift 55.1 53.5 55.4 0.53 0.57 0.65 26.71 25.03 25.28 2.50 2.37 2. 191.6 199.7 196.4 Table 6. Ablations on vision foundation models (VFMs). DINOv2 excels in low-dimensional latents (e.g., 16), while DINOv3 performs best in higher dimensions (e.g., 128). Table 9. Ablations on encoder initialization methods. We notice that using the randomly initialized encoder yields better generation performance. Dim=16 Dim=128 Encoder Initialization L.P. Acc. rFID gFID IS VFM DINOv3 [49] DINOv2 [37] SigLIP 2 [53] RADIOv2 [39] SAM [26] Two VFMs rFID gFID IS rFID gFID IS 0.81 0.74 0.71 0.79 0. 0.69 2.86 2.75 3.59 2.97 4.96 3.26 195.2 183.3 172.3 193.1 141.1 164.7 0.65 0.53 0.51 0.64 0. 0.49 2.27 2.38 3.14 2.59 4.47 2.51 196.4 184.7 178.6 193.4 157.2 181.3 Using two VFMs simultaneously (DINOv3 and SigLIP 2) Table 7. Ablations on reconstruction and alignment distillation. RAD improves the generation performance, and the gain does not originate from the transformer architecture. Setting Sem Dec rFID gFID IS Align. only Align. only Rec. only Rec. + Align. MLP Transformer Transformer Transformer 0.76 0.57 0.75 0.65 3.02 2.52 2.97 2.27 175.3 184.5 174.2 196.4 try using two VFMs during semantic learning. However, this leads to degradation in generation performance. Considering these results, we use DINOv2 for experiments with 16 and 32 dimensions, and DINOv3 for experiments with 64 dimensions and above. Ablations on RAD. We conduct four groups of ablation studies, including (1) alignment only, (2) reconstruction only, and (3) joint reconstruction and alignment. For the alignment only setting, we experiment with two types of semantic decoders: an MLP (4M parameters) and lightweight Transformer (1.5M parameters). As shown in Table 7, the joint reconstruction and alignment strategy achieves the best overall performance, obtaining the lowest gFID, and the highest IS score. Ablations on Semantic Decoder. In Tab. 10, we compare different architectural designs of the Semantic Decoder. We observe that using transformer architecture consistently outperforms an MLP across all metrics. However, increasing the transformers capacity leads to degraded performance in both linear probing accuracy and generation quality. Therefore, lightweight transformer design provides 8 VFM Random 54.5 55.4 0.57 0.65 2.37 2.27 189.2 196. Table 10. Comparison of the performance on different semantic decoders. lightweight transformer achieves the best generation performance. Sem Dec Params L.P. Acc. rFID gFID IS MLP Transformer Transfromer 4M 10M 1.5M 51.2 47.3 55.4 0.76 0.63 0.65 3.02 3.13 2.27 175.3 170.5 196.4 Table 11. Overall ablation study of FSD, RAD, and Decoder Finetuning. Each method provides clear improvement. Method L.P. Acc. rFID PSNR gFID IS Baseline + FSD + RAD + Dec FT 7.1 52.7 55.4 55. 0.22 0.57 0.65 0.48 29.76 25.62 25.28 26.16 12.07 2.52 2.27 2.23 57.5 184.5 196.4 198.2 the best overall trade-off. Ablations on Encoder Initialization. We attempt to initialize the encoder of RecTok with VFMs, as shown in Tab. 9. We employ FSD and RAD as self-distillation [58] like methods to train the VFM initialized RecTok. However, the overall performance lags behind that of the randomly initialized encoder. Overall Ablation Study. In Tab. 11, we present an ablation study on the two key innovations and the decoder finetuning. Each component brings clear performance gain. 5. Conclusion In this work, we address the fundamental challenge posed by the latent dimensionality of visual tokenizers through RecTok. Building on our core insightenhancing semantic consistency along the forward flow rather than only at the un-noised latents. We introduce two key innovations: Flow Semantic Distillation (FSD) and Reconstruction and Alignment Distillation (RAD). Together, FSD and RAD effectively enrich the semantics of RecToks latent space, and we observe consistent improvements as the latent dimension increases. Experiments on ImageNet-1K demonstrate that RecTok achieves state-of-the-art generation performance while maintaining strong reconstruction quality and semantic representation. We hope this work inspires future research on high-dimensional visual tokenizers."
        },
        {
            "title": "References",
            "content": "[1] Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian Ye, and Kaicheng Zhou. Integrating view conditions for image synthesis. arXiv preprint arXiv:2310.16002, 2023. 2 [2] Jinbin Bai, Wei Chow, Ling Yang, Xiangtai Li, Juncheng Li, Hanwang Zhang, and Shuicheng Yan. Humanedit: highquality human-rewarded dataset for instruction-based image editing. arXiv preprint arXiv:2412.04280, 2024. 2 [3] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. In The Thirteenth International Conference on Learning Representations, 2024. 2 [4] Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, and MingHsuan Yang. From masks to worlds: hitchhikers guide to world models. arXiv preprint arXiv:2510.20668, 2025. 2 [5] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: ViT backbone for diffusion models. 2023. 1 [6] Bowei Chen, Sai Bi, Hao Tan, He Zhang, Tianyuan Zhang, Zhengqi Li, Yuanjun Xiong, Jianming Zhang, and Kai Zhang. Aligning visual foundation encoders to tokenizers for diffusion models. arXiv preprint arXiv:2509.25162, 2025. 2, 3, 5, 1 [7] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In ICML, 2025. 2, 4, 6, [8] Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, and Han Cai. Dc-ae 1.5: Accelerating diffusion model convergence with structured latent space. ICCV, 2025. 4 [9] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. 5 [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. 5 [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 3, 5 [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 2 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 1, 3 [14] Aosong Feng, Weikang Qiu, Jinbin Bai, Zhen Dong, Kaicheng Zhou, Xiao Zhang, Rex Ying, and Leandros Tas9 siulas. An item is worth prompt: Versatile image editing with disentangled control. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1655916567, 2025. 2 [15] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. 5 [16] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis, 2022. 1 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2021. 2, 3, [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022. 2 [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 6 [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. 2, 6 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [22] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. In ICML, 2023. 5 [23] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. In NeurIPS, 2025. 5, 6 [24] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. 1 [25] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 1, 2, 3 [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 2, 7, 8, [27] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 3 [28] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 1998. 3 [29] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. In ICCV, 2025. 5 [30] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. 5 [31] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2023. [32] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 3, 2 [33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022. 3 [34] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. 1, 5 [35] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021. 1 [36] Aaron van den Oord, Oriol Vinyals, and Koray Neural discrete representation learning. Kavukcuoglu. In NeurIPS, 2017. 2, 3 [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. TMLR, 2023. 2, 3, 7, 8 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 1, 5 [39] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In CVPR, 2024. 7, 8 [40] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In NeurIPS, 2019. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 6 [42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2 [43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211252, 2015. 2 [44] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In NeurIPS, 2016. 6, 7 [45] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [46] Qingyu Shi, Jinbin Bai, Zhuoran Zhao, Wenhao Chai, Kaidong Yu, Jianzong Wu, Shuangyong Song, Yunhai Tong, Xiangtai Li, Xuelong Li, et al. Muddit: Liberating generation beyond text-to-image with unified discrete diffusion model. arXiv preprint arXiv:2505.23606, 2025. 2 [47] Qingyu Shi, Lu Qi, Jianzong Wu, Jinbin Bai, Jingbo Wang, Yunhai Tong, and Xiangtai Li. Dreamrelation: Bridging customization and relation generaion. In CVPR, 2025. 2 [48] Qingyu Shi, Jianzong Wu, Jinbin Bai, Jiangning Zhang, Lu Qi, Yunhai Tong, and Xiangtai Li. Decouple and track: Benchmarking and improving video diffusion transformers In Proceedings of the IEEE/CVF Infor motion transfer. ternational Conference on Computer Vision, pages 10995 11005, 2025. 2 10 [63] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022. 2 [64] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. 2 [65] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. 1, [66] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. 5 [67] Biao Zhang and Rico Sennrich. Root mean square layer normalization. NeurIPS, 2019. 5 [68] Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, and Xiaojuan Qi. Vision foundation models as effective visual tokenizers for autoregressive image generation, 2025. 3 [69] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 2, 3, 4, 5, 6 [70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. TMLR, 2023. 5 [71] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer. 2021. [49] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. 7, 8, 3 [50] Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. In ICML, 2025. 2 [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 5 [52] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. 5 [53] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features, 2025. 2, 7, 8, 3 [54] Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025. 5 [55] Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer, 2025. 5 [56] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. In CVPR, 2022. 2 [57] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. 3 [58] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, and Chen Change Loy. Clipself: Vision transformer distills itself for open-vocabulary dense prediction. In ICLR, 2024. 8 [59] Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, et al. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. 2 [60] Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation, 2025. 2 [61] Jiawei Yang, Tianhong Li, Lijie Fan, Yonglong Tian, and Yue Wang. Latent denoising makes good visual tokenizers, 2025. 2, 4, 5, [62] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. 2, 3, 4, 5, 6, 7 11 RecTok: Reconstruction Distillation along Rectified Flow"
        },
        {
            "title": "Supplementary Material",
            "content": "Overview. In this supplementary file, we present more details in addition to the main paper. Here are the details: Sec. 6: Introduction video, we strongly recommend that Table 12. Implementation details. We report the detailed architectural specifications, training hyperparameters, and sampling settings for different model variants. reviewers take look. Sec. 7: Implementation details. Sec. 8: More ablation studies. Sec. 9: More qualitative results. Sec. 10: More analysis on RecTok. Sec. 11: Limitations and future works. Sec. 12: Broader impacts. 6. Introduction Video To help readers quickly grasp the primary idea of our work, we provide 6-minute introduction video. Please refer to introduction video.mp4 in the supplementary file. 7. Implementation Details In the Tab. 12, we provide detailed training configurations for both RecTok and DiT, including the hyperparameters for different DiT sizes, training epochs, learning-rate schedules, sampling schedules, and other related settings. 8. Additional Ablation Studies Ablations on Mask Ratio in RAD. We ablate the mask ratio used in reconstruction and alignment distillation (RAD). As shown in Tab. 13, we evaluate four mask ratio settings and report their effects on reconstruction, generation, and semantics. The results show that increasing the mask ratio improves generation quality while degrading reconstruction performance. Since reconstruction can be further enhanced through decoder finetuning, we set the upper bound of the mask ratio to 0.4. Ablations on Noise Intensity in FSD. It is worth noting that the end point state of the forward flow in FSD, x1, does not need to follow normal Gaussian distribution. As an alternative, we can instead place x1 in more expressive high-intensity noise space: x1 = γ ϵ, ϵ (0, 1), (7) where ϵ denotes standard Gaussian noise, and γ controls the intensity of the noise, equivalently, the variance of x1. larger γ corresponds to stronger collisions at time t. We view this mechanism as analogous to the timestep shift introduced in our main paper, as both aim to mitigate information redundancy in high-dimensional settings. Therefore, we focus our analysis on the effect of γ and leave alternative formulations of x1 to future work. architecture depth hidden dim heads DDT depth DDT hidden dim DDT heads training epochs warmup epochs decay epochs optimizer batch size learning rate learning rate schedule weight decay ema rate noise schedule class token drop (for CFG) sampling ODE solver ODE steps time steps CFG scale CFG interval DiTDH-S DiTDH-B DiTDH-L DiTDH-XL 12 384 6 12 768 24 1024 16 28 1152 16 2 2048 16 RecTok DiTDH 200 50 (linear) 50 - 80 (ablation), 600 0 40 - 800 Adam [24], β1, β2 = 0.9, 0.95 1024 4e-4 cosine decay 1e-4 0.999 st 1+(s1)t , None = 2e-4 linear decay 0.0 0.9995 U(0, 1), = 0. (cid:113) 4096 r2d Euler 50 (ablation), 150 shift in [0.0, 1.0] according to noise schedule 1.29 [0, 1] (not used) Ablations on KL Loss. We conduct an ablation study on the use of the KL loss as shown in Tab. 15. Although recent works [6, 7] remove the KL term and adopt an autoencoder (AE), our ablations show that incorporating KL regularization improves the generation performance. detailed analysis of Tab. 15 reveals distinct trade-off between reconstruction and generation performance. Specifically, the deterministic AE setting excels in reconstruction, achieving lower rFID (0.35) and higher PSNR (29.89). This indicates that without the regularization constraint, the model can more freely encode high-frequency details into the latent space. However, this unconstrained latent space fall short in the generation stage, as evidenced by the degraded gFID (5.19). In contrast, enforcing the KL loss promotes smooth and compact latent manifold. Although this results in slight drop in reconstruction metrics, it significantly facilitates the learning process for the subsequent generative model, improving the gFID by over 50% (5.19 2.27) and boosting the Inception Score by large margin (+44.2). Since our work prioritizes the generation task, we retain the KL loss and use variational autoencoder (VAE). 1 (a) t=0.2, acc.=52.8% (b) t=0.4, acc.=54.9% (c) t=0.6, acc.=55.5% (d) t=0.8, acc.=55.2% (e) t=1.0, acc.=55.4% Figure 7. t-SNE visualizations under different timesteps (t). Our RecTok shows clear advantage in semantic consistency on the forward flow, even with high level of noise and disturbance. c ] 2 6 [ V - Figure 8. Visualizations of latent feature through cosine similarity. We compare the latent features of RecTok (top row) and VA-VAE (bottom row). The RecTok features exhibit stronger semantic localization on foreground objects compared to VA-VAE. Table 13. Ablation on mask ratio. We observe that higher mask ratio of 0.4 yields the best overall generative performance (gFID and IS), suggesting that more challenging task benefits RAD."
        },
        {
            "title": "Mask Ratio",
            "content": "rFID PSNR gFID IS 0.3 0.4 0.5 0.6 0.60 0.65 0.66 0.67 25.45 25.28 25.24 25.22 2.35 2.27 2.28 2. 192.8 196.4 197.1 192.7 Table 14. Effect of γ on reconstruction and generation quality. We observe that γ = 1.0 achieves the optimal trade-off. While larger γ values marginally improve IS, they lead to degradation in both reconstruction fidelity (rFID/PSNR) and generative distribution alignment (gFID). γ 1.0 2.0 3.0 Noise Schedule rFID PSNR gFID IS Shift Shift Shift 0.65 0.69 0.72 25.28 24.79 24.45 2.27 2.34 2.39 196.4 198.1 200. 9. Additional Qualitative Results Reconstruction Results. In Fig. 13, we present additional reconstruction results. RecTok accurately preserves the structure, color, and fine details of the input images. Generation Results. In Fig. 10, Fig. 11, and Fig. 12. We provide additional generation results produced by DiTDH XL model trained for 600 epochs. We show outputs both with and without classifier-free guidance. 10. Additional Analysis on RecTok The Discriminative Ability along the Flow. We visualize the features along the forward flow xt using t-SNE, uniformly sampling timesteps [0, 1]. As shown in Fig. 7, our RecTok exhibits strong semantic consistency throughout the forward flow. The t-SNE visualization and linear probing accuracy demonstrate the stable discriminative ability of xt. more discriminative xt also encourages forward trajectories to avoid intersections, which aligns with the objective of the original rectified flow [32]. Figure 9. Visualizations of latent feature through PCA. latent dimensionality and refining the KL regularization. 12. Broader Impacts RecTok further expands the dimensionality of visual tokenizers while delivering consistent improvements in reconstruction, generation, and semantic representation. Its effectiveness suggests that the community may benefit from exploring higher-dimensional latent spaces that excel at both generative and understanding tasks. Such latent space serves as real unified representation, removing the need for two separate image tokenizers as in prior unified models [57]. shared feature space for both generation and understanding can promote mutual benefits across tasks, making unified models more coherent and meaningful. Table 15. Ablation on the KL loss. We compare the deterministic AE (w/o KL) and the VAE (w KL) settings. While removing the KL term improves reconstruction fidelity (rFID 0.35), it results in disjointed latent space that hinders generation (gFID 5.19). Incorporating KL regularization significantly boosts generative performance (gFID 2.27), validating its necessity for our framework."
        },
        {
            "title": "Setting",
            "content": "rFID PSNR gFID IS w/o KL KL 0.35 0.65 29.89 25.28 5.19 2. 152.2 196.4 Latent Feature Visualization. We visualize the latent features of RecTok using cosine similarity. Specifically, we extract the latent features using RecTok and obtain global feature via spatial pooling. We then compute the cosine similarity between the global and latent features. We also show the PCA results, as shown in Fig. 8 and Fig. 9, the heatmaps reveal that the learned latent features possess distinct semantic localization capabilities. Even without explicit segmentation supervision, the high similarity regions (indicated in warm colors) consistently align with the parts of the foreground objects, such as the head or the body structure. Conversely, background clutter and irrelevant textures are effectively suppressed. This suggests that RecToks tokenization process preserves spatial semantic integrity, ensuring that the aggregated global feature is highly representative of the core visual content and discriminative for downstream tasks. 11. Limitations In terms of semantics, although RecTok enhances semantic structure by increasing the latent dimensionality, its discriminative capability still lags behind that of VFMs. For example, DINOv3 [49], SigLIP 2 [53], and SAM [26]. Regarding reconstruction, while the KL loss smooths the latent space and improves generation quality, it inevitably weakens reconstruction ability, resulting in RecTok performing worse than an AE model with the same architecture. We leave these challenges as open questions for future work. We believe they can be addressed by further increasing the 3 Figure 10. Supplementary generations (1/3). 4 Figure 11. Supplementary generations (2/3). 5 Figure 12. Supplementary generations (3/3). 6 Figure 13. Supplementary reconstruction (1/1). We put the original images on the left and the reconstructed images on the right."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Peking University",
        "TeleAI"
    ]
}