{
    "paper_title": "The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models",
    "authors": [
        "Lukas Gienapp",
        "Christopher Schröder",
        "Stefan Schweter",
        "Christopher Akiki",
        "Ferdinand Schlatt",
        "Arden Zimmermann",
        "Phillipe Genêt",
        "Martin Potthast"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, limiting the development of truly open models. This problem is exacerbated for non-English languages, where openly licensed text remains critically scarce. We introduce the German Commons, the largest collection of openly licensed German text to date. It compiles data from 41 sources across seven domains, encompassing legal, scientific, cultural, political, news, economic, and web text. Through systematic sourcing from established data providers with verifiable licensing, it yields 154.56 billion tokens of high-quality text for language model training. Our processing pipeline implements comprehensive quality filtering, deduplication, and text formatting fixes, ensuring consistent quality across heterogeneous text sources. All domain subsets feature licenses of at least CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and redistribution. The German Commons therefore addresses the critical gap in openly licensed German pretraining data, and enables the development of truly open German language models. We also release code for corpus construction and data filtering tailored to German language text, rendering the German Commons fully reproducible and extensible."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 9 9 3 1 . 0 1 5 2 : r The German Commons 154 Billion Tokens of Openly Licensed Text for German Language Models Lukas Gienapp University of Kassel, hessian.AI, and ScaDS.AI Kassel, Germany Ferdinand Schlatt Friedrich-SchillerUniversität Jena Jena, Germany Christopher Schröder InfAI and ScaDS.AI Leipzig, Germany Stefan Schweter Independent Researcher Holzkirchen, Germany Arden Zimmermann German National Library Leipzig, Germany Philippe Genêt German National Library Frankfurt, Germany Christopher Akiki Leipzig University and ScaDS.AI Leipzig, Germany Martin Potthast University of Kassel, hessian.AI, and ScaDS.AI Kassel, Germany Abstract Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, limiting the development of truly open models. This problem is exacerbated for non-English languages, where openly licensed text remains critically scarce. We introduce the German Commons, the largest collection of openly licensed German text to date. It compiles data from 41 sources across seven domains, encompassing legal, scientific, cultural, political, news, economic, and web text. Through systematic sourcing from established data providers with verifiable licensing, it yields 154.56 billion tokens of high-quality text for language model training. Our processing pipeline implements comprehensive quality filtering, deduplication, and text formatting fixes, ensuring consistent quality across heterogeneous text sources. All domain subsets feature licenses of at least CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and redistribution. The German Commons therefore addresses the critical gap in openly licensed German pretraining data, and enables the development of truly open German language models. We also release code for corpus construction and data filtering tailored to German language text, rendering the German Commons fully reproducible and extensible. Data: https://huggingface.co/datasets/coral-nlp/german-commons Code: https://github.com/coral-nlp/llmdata"
        },
        {
            "title": "1 Introduction\nOpen language models are increasingly rivaling commercial sys-\ntems in terms of their effectiveness and/or efficiency on key bench-\nmarks, with expanding coverage of languages and tasks. Yet, the\ndegree of openness of open models is often lacking [1]. While the\nweights of open models are being released under open licenses,\nthe licensing of the training data of many models remains un-\nclear. However, as most pretraining datasets used are derived from\nlarge-scale web crawls, this creates legal and ethical barriers to\nthe development of fully open language models, despite the web’s\nlong-established value in science [2] and industry [3]. This is be-\ncause (1) the provenance of web content is hard to establish and\n(2) obtaining consent from original authors or copyright holders\nis infeasible at web scale; (3) re-publishing web data without such\nconsent as part of a training dataset infringes upon copyright and",
            "content": "Table 1: Overview of number of documents, number of tokens and median sequence length per source domain comprised in the German Commons corpus."
        },
        {
            "title": "19.89 B 12.87 %\n2.31 %\n3.57 B\n2.99 B\n1.94 %\n72.67 B 47.02 %\n0.07 %\n0.11 B\n54.49 B 35.25 %\n0.54 %\n0.84 B",
            "content": "(cid:205) Total"
        },
        {
            "title": "154.56 B",
            "content": "privacy; in particular since (4) the data may contain personally identifiable information (PII), and (5) generative models, even if published openly, may reproduce sensitive or copyrighted text. These shortcomings limit the usefulness of many open-weight models. Practitioners can only trust, but not verify, that, for example, the web crawls used for training have not been contaminated by benchmark test data or that no copyrighted material is included. To minimize these risks and make open language models usable for both scientific and commercial purposes without reservations, it is important to limit their training data to verifiably open texts. However, this is challenging for non-English languages, as the available open alternatives to web data need to be carefully compiled. We therefore introduce the German Commons, the largest pretraining-scale collection of explicitly openly licensed text in German language  (Table 1)  . It encompasses 154.56 billion tokens of open text across 35.78 million documents spanning seven thematic domains. This renders it the largest openly licensed German corpus (Section 2 and 3). Alongside the corpus, we release our data processing library llmdata to ensure full reproducibility (Section 4). Moreover, we provide an analysis of the corpus properties (Section 5). In the Appendix, datasheet compliant with the recommendation of Gebru et al. [4] is included."
        },
        {
            "title": "2 Related Work\nTo motivate our choices for constructing the German Commons, we\nrevisit three categories of existing training corpora: (1) the web-\nscraped datasets that dominate current LLM training, (2) smaller-\nscale, German-specific resources, and (3) emerging openly licensed\nalternatives, predominantly in English.",
            "content": "Web Corpora for LLM Training Modern LLM training relies on web-scraped content as large-scale text source, with collections such as C4 [5]/mC4 [6], The Pile [7], OSCAR [8], ROOTS [9], RedPajama [10], Dolma [11], HPLT [12], and FineWeb [13, 14]. However, these datasets derive almost exclusively from Common Crawl1, which creates dependency on single source and lacks explicit licensing metadata. While C4Corpus [15] identifies licensed content through substring matching, license scope remains unclearcontent may be open but not verifiable. Additional risks include terms of service restrictions that prohibit model training [16] and considerable amount of PII despite preprocessing [17]. The heterogeneous nature of web data further necessitates extensive quality filtering [11, 18, 19]. This creates fundamental tension: web-scraped corpora provide scale but introduce legal, ethical, and quality risks. The German Commons aim to address these shortcomings by collecting verifiably licensed, high-quality text content from non-web sources. German Text Datasets Web datasets naturally include German subsets, however, with identical licensing and quality issues. Several additional large-scale German text corpora are available, but mostly predate LLM training efforts, such as the Leipzig Corpora Collection [20], OPUS [21], and HPLT [12]. Moreover, since they are also sourced from the web, their licenses are frequently unclear and not verifiable as well. While openly licensed German corpora exist (see Section 3), their individual volumes are substantially smaller than web-scraped alternatives, and they remain fragmented without centralized access. Furthermore, multiple of these datasets are not available in plain text form, and thus need to undergo text extraction and preprocessing first. The German Commons aim to instead provide unified, comprehensive source of text data suitable for large language model training. Openly Licensed Training Corpora Scaling openly licensed text corpora faces verification challenges, leading datasets to include questionable content [1]. While code datasets can rely on the explicit machine-readable licensing present in code repositories [22], such annotations are rarely available for natural-language text from web or print sources, which consequently proves more difficult to verify. Therefore, data collection efforts turn to trusted providers of licensed and open-domain content, such as government agencies, GLAM institutions, and collaborative projects such as Wikimedia. Current large-scale openly-licensed collections for language model training include: (1) the Open License Corpus [23] is an aggregation from existing corpora covering open domain and attribution licensed content from the legal, scientific, conversational, books, and news domain, amounting to 228B tokens of multi-domain English text; (2) the KL3M project [16] assembled 1.35 trillion tokens of English text sourced primarily from open domain government records in English language; it explicitly excludes content licenses 1https://commoncrawl.org/ Gienapp et al. with share-alike clause such as CC-BY-SA and thus removes, e.g., Wikipedia from consideration. (3) the Common Pile [24] compiles approximately upwards of two trillion tokens of English text with stringent licensing requirements, and is the largest, yet monolingual resource of verifiably open text; and (4) the Common Corpus [25] also provides 2 trillion multilingual tokens, including 112 billion German tokens, which serve as the starting point for our data collection efforts."
        },
        {
            "title": "3.2 What do we consider openly licensed?\nThe German Commons require explicit licenses for each document.\nWe exclude datasets with ambiguous licensing, i.e., where the ag-\ngregation or metadata carries open licenses while underlying text\ncontent retains unspecified copyright restrictions. This excludes\nmost web-crawled datasets that conflate aggregation with content\nlicensing rights [24].",
            "content": "We follow Kandpal et al. [24] in adopting the Open Knowledge Foundations Open Definition 2.13. Unlike Open License Corpus and KL3M, this covers licenses with share-alike provision. Table 3 lists the accepted licenses found in our data. Licenses are grouped into the categories of (1) public domain equivalent (2) attribution 2 Text+ Zenodo Huggingface German National Library (DNB) Austrian National Library (ÖNB) German Digital Dictionary (DWDS) Leibniz-Institute for German Language (IDS) OpenAlex Wikimedia 3https://opendefinition.org/od/2.1/en/ https://text-plus.org https://zenodo.org https://huggingface.co/datasets https://dnb.de https://www.onb.ac.at https://www.dwds.de/ https://www.ids-mannheim.de https://openalex.org https://www.wikimedia.de The German Commons 154 Billion Tokens of Openly Licensed Text for German Language Models Table 2: Overview on datasets constituting the German Commons corpus. Token counts are measured using GPT-2 tokenizer. Various licenses are open, but differ per document as per original source. Thematic Subset & Constituent Corpora Ref."
        },
        {
            "title": "Web Commons",
            "content": "Wikipedia Wikivoyage Wikipedia Discussions Youtube-Commons One Million Posts Corpus The Stack (Markdown and TXT Subsets)"
        },
        {
            "title": "Political Commons",
            "content": "Reichtagsprotokolle German Political Speeches Corpus der Drucksachen des Deutschen Bundestages C. d. Plenarprotokolle des Deutschen Bundestages EuroVoc"
        },
        {
            "title": "Legal Commons",
            "content": "Corpus des Deutschen Bundesrechts OpenLegalData Corpus der Entscheidungen des BFH Entscheidungen des BGH in Strafsachen des 20. Jhd. Corpus der Entscheidungen des BGH Corpus der Entscheidungen des BVerfG Corpus der Entscheidungen des BpatG Corpus der Entscheidungen des BVerwG Corpus der amtl. Entscheidungssammlung des BVerfG Corpus der Entscheidungen des BAG EurLEX [26] [27] [28] [25] [29] [22] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44]"
        },
        {
            "title": "Cultural Commons",
            "content": "DiBiLit-Korpus DiBiPhil-Korpus Wikisource German-PD BLBooks MOSEL SBB Fulltexts Wikiquote"
        },
        {
            "title": "Total",
            "content": "CC-BY-SA-4.0 Various CC-BY-SA-4.0 Travel CC-BY-SA-4.0 Online Discussions Various CC-BY-4.0 Various"
        },
        {
            "title": "Video Subtitles\nOnline Discussions\nVarious",
            "content": "Docs (# / %) 15.48 43.26 % Tokens (# / %) 19.89 12.87 %"
        },
        {
            "title": "2.95 B\n0.04 B\n1.22 B\n14.48 B\n0.09 B\n1.11 B",
            "content": "1.91 % 0.03 % 0.79 % 9.37 % 0.06 % 0.72 %"
        },
        {
            "title": "3.57 B",
            "content": "2.31 % CC-BY-SA-4.0 Parliamentary Protocols CC-BY-4.0 CC0-1.0 CC0-1.0 EUPL"
        },
        {
            "title": "0.70 B\n0.03 B\n0.53 B\n0.32 B\n1.99 B",
            "content": "0.46 % 0.02 % 0.34 % 0.20 % 1.29 % CC0-1.0 CC0-1.0 CC0-1.0 CC0-1.0 CC0-1.0 CC0-1.0 CC0-1.0 CC0-1.0 CC0-1.0 CC0-1.0 CC-BY-4."
        },
        {
            "title": "German Federal Laws\nCourt Decisions\nCourt Decisions\nCourt Decisions\nCourt Decisions\nCourt Decisions\nCourt Decisions\nCourt Decisions\nCourt Decisions\nCourt Decisions\nEuropean Union Laws",
            "content": "News Articles CC0-1.0 News Articles CC0-1.0 CC0-1.0 News Articles CC-BY-SA-4.0 News Articles"
        },
        {
            "title": "2.99 B",
            "content": "1.94 %"
        },
        {
            "title": "0.00 B\n1.92 B\n0.07 B\n0.09 B\n0.29 B\n0.04 B\n0.19 B\n0.12 B\n0.02 B\n0.05 B\n0.20 B",
            "content": "0.00 % 1.24 % 0.04 % 0.06 % 0.19 % 0.03 % 0.12 % 0.08 % 0.02 % 0.03 % 0.13 %"
        },
        {
            "title": "0.11 B",
            "content": "0.07 % [25] CC0-1."
        },
        {
            "title": "0.11 B",
            "content": "0.07 % [45] [25] [46] [47] [48] [49] [50] [51] CC-BY-SA-4.0 Literature CC-BY-SA-4.0 Literature CC-BY-SA-4.0 Various CC0-1.0 CC0-1.0 CC-BY-4.0 CC-BY-4.0 CC-BY-SA-4.0 Quotes & Proverbs"
        },
        {
            "title": "Literature\nLiterature\nSpeech Transcripts\nLiterature",
            "content": "CC-BY-SA-4.0 Educational Books CC-BY-SA-4.0 Scholarly Papers Scholarly Books Various Various Scholarly Papers CC-BY-SA-4.0 Educational Content Various"
        },
        {
            "title": "0.00 M 0.01 %\n0.00 M 0.00 %\n0.24 M 0.67 %\n0.12 M 0.35 %\n0.00 M 0.01 %\n3.13 M 8.74 %\n2.61 M 7.28 %\n0.01 M 0.02 %",
            "content": "0.14 % 0.22 0.02 % 0.03 0.23 % 0.35 49.33 31.92 % 0.65 % 1.01 2.06 % 3.18 0.23 % 0.36 0.00 % 0.01 B"
        },
        {
            "title": "0.84 B",
            "content": "0.54 %"
        },
        {
            "title": "0.05 B\n0.18 B\n0.17 B\n0.00 B\n0.03 B\n0.41 B",
            "content": "0.03 % 0.12 % 0.11 % 0.00 % 0.02 % 0.27 %"
        },
        {
            "title": "154.56 B",
            "content": "Table 3: Licenses of data constituting German Commons."
        },
        {
            "title": "License",
            "content": ". CC0-1.0 / Public Domain q - Unlicense MIT-0 0BSD t r tt MIT BSD-2-Clause BSD-2-Clause-FreeBSD BSD-3-Clause BSD-Source-Code Apache-1.1 Apache-2.0 BSD-4-Clause-UC BSD-4-Clause CC-BY-2.0 CC-BY-3.0 CC-BY-4.0 ft CC-BY-SA-4.0 EUPL-1.2 Artistic-2.0 p C"
        },
        {
            "title": "CopyrightNotice\nAttribution",
            "content": "SourceProvision Share-Alike licenses; and (3) copyleft licenses. All of the selected licenses permit redistribution, modification, and commercial use. They thus support data commons principles for sustainable open model development [52, 53]. The latter two require attribution and/or license indication, while copyleft licenses have to be redistributed under the same license terms (share-alike). Each document in the German Commons is tagged with its corresponding SPDX-canonical license4, linking to its original license text. We exclude licenses with non-commercial clauses, research-only provisions, and other use-limiting conditions. However, we advice practitioners to review individual license compatibility before use. We acknowledge that this approach, i.e., trusting provided licenses without independent auditing, carries inherent misattribution risks. However, given the relative scarcity of open German text, and the institutional nature of the data providers contributing to the German Commons, we consider it viable method for large-scale data collection. While we apply less strict criteria than Kandpal et al. [24], who independently audit entries in their data, to reasonably mitigate risks, we consequently limit inclusion of data to established institutional providers: national libraries, academic institutions, government agencies, and verified open-source platforms, excluding sources lacking clear licensing protocols. 4https://spdx.org/licenses/ Gienapp et al."
        },
        {
            "title": "3.3 Detailed Provenance Information\nThe German Commons are divided into seven thematic domains:\nWeb spans collaborative platforms, user-generated content and\ndiscussions, and open-source repositories; Political encompasses\nparliamentary protocols, publications, and speeches; Legal includes\ncourt decisions, proceedings, and regulatory frameworks from Ger-\nman and European judicial systems. While previous datasets group\npolitical and legal text as one [24], we argue that legal text has a\ndistinct writing style not overlapping with the more general text\npublished by political institutions and thus chose to differentiate\nbetween both. The News content draws primarily from historical\nnewspaper archives maintained by cultural heritage institutions.\nEconomics includes documents from business and trade publica-\ntions, Cultural includes books and general speech transcripts, and\nScientific includes scholarly and educational content. The constitut-\ning data of each domain is detailed in the following paragraphs.",
            "content": "Web Commons For Wikipedia and Wikivoyage, we use the TEIencoded version supplied by the DWDS [26, 27]. The complementary corpus Wikipedia Discussions of user discussions on Wikipedia is supplied by the IDS [28]. The One Million Posts Corpus consists of user comments posted to the Austrian newspaper Der Standard, collected and openly licensed in collaboration with the newspaper [29]. Youtube-Commons [25] encompasses audio transcripts of over 2 million videos shared on YouTube under CC-BY license. These include both automatically translated and manually curated texts. Finally, we filter German-language website sources and README files hosted on GitHub by filtering the Markdown and TXT subsets of The Stack [22] corpus for our allowed licenses. Political Commons We aggregate official publications from the German federal parliament (Corpus der Drucksachen des Deutschen Bundestags, [32]) and the EU parliament (EuroVoc). Alongside that, we include parliamentary protocols and speeches from the German federal parliament (Corpus der Plenarprotokolle des Deutschen Bundestags, [33]) and the historic Reichstag (Reichtagsprotokolle, [30]). German parliamentary documents are exempt from copyright as official works. EU documents are licensed under the European Union Public License (EUPL), which is compatible with CC BY-SA 3.0 license.5 Additionally, we include corpus of political speeches in German language [31]. Legal Commons The largest portion of legal documents comprises court proceedings from German courts, collected by OpenLegalData [35].6 Proceedings of federal courts are made available in dedicated corpora, namely for the Bundesfinanzhof (BFH, [36]), Bundesgerichtshof (BGH, [37, 38]), Bundesverfassungsgericht (BVerfG, [39, 42]), Bundespatentgericht (BPatG, [40]), Bundesverwaltungsgericht (BVerwG, [41]), and Bundesarbeitsgericht (BAG, [43]). Court decisions and decrees of German courts are exempt from copyright. Additionally, we include European laws made available via the EUR-Lex7 platform, incorporating the official data dump. 5https://eupl.eu/1.2/en 6https://openlegaldata.io 7https://eur-lex.europa.eu/ The German Commons 154 Billion Tokens of Openly Licensed Text for German Language Models News Commons The Deutsches Zeitungsportal8 corpus covers over 4 million German newspaper editions of nearly 2000 newspapers from 1671 to 1994, incorporating all public domain releases. The ANNO corpus provides equivalent coverage for Austrian newspapers, spanning around 1600 newspapers in the public domain. Additionally, we include the Europeana Newspapers archive, which contains more than 4 million individual documents across multiple European languages from the 18th to early 20th century, using the plain text version created by the BigLAM initiative.9 Wikinews is wiki project for news articles written by community members; we obtain current dump and parse plain text from it. Economics Commons The TEDEUTenders [25] dataset comprises European public procurement notices from the online version of the EUs Official Journal Supplement, providing structured access to public tender information across EU member states. Cultural Commons The DiBiLit [45] dataset provides literary corpus spanning the 16th through early 20th centuries, containing primarily German literary works (poetry, drama, prose) alongside select humanities texts. DiBiPhil covers the 15th to 20th centuries, encompassing literary works with philosophical content. Both corpora feature TEI-encoded, homogenized texts. Wikisource is collaborative digital library for public domain source texts, including historical documents, literary works, and reference materials, transcribed and proofread by volunteer contributors. GermanPD [25] systematically aggregates German monographs and periodicals from Internet Archive and European national libraries, including both OCR-sourced content and text extracted from machinereadable format. BLBooks [46] comprises digitized pages from the British Library, primarily concentrated on the 18th-19th century and covering diverse subject areas. While BLBooks is originally split into pages, we concatenate pages of the same in the provided order to re-assemble complete texts. MOSEL [47] aggregates multilingual open-source speech recordings with Whisper-generated transcriptions across 24 EU languages. SBB Fulltexts [48] is collection of the fulltexts available in the digitized collections of the Berlin State Library (SBB), covering approximately 25 000 unique German literary works split into individual page scans. Wikiquote collaboratively collects quotations and proverbs that fall under public domain. Scientific Commons The Digitalisierung des Polytechnischen Journals is historic technical periodical, digitized through OCR [50]. Wikibooks is free repository of open-content textbooks and educational materials, converted to TEI [49]. We crawl the Directory of Open Access books (DOAB) for scholarly open-access book publications and filter for German language, similar to the English counterpart assembled by Kandpal et al. [24]. We further include German scholarly articles published on arXiv which explicitly indicate open licensing. For each article, we only include the latest version to reduce deduplication overhead. Finally, we obtain all fulltexts from OpenAlex [51], metadata aggregator for open-access scholarly papers, and filter for German texts under open licenses. Since OpenAlex does not provide fulltexts, we instead crawl all linked PDFs and extract plain text from those. 8https://www.deutsche-digitale-bibliothek.de/newspaper 9https://huggingface.co/biglam"
        },
        {
            "title": "4 Data Processing\nWe apply several processing steps to transform the heterogeneous\ninput data into a consistent output format, apply quality heuristics\nand filters, and fix text formatting. The individual steps are detailed\nin this section and are executed in the listed order. We include\ndetailed statistics on removed documents and tokens per processing\nstep and source dataset in Appendix A.4. Table 4 shows the final\ndataset schema. The dataset is distributed in Parquet format, with\npartial files partitioned for each thematic domain and source dataset\nto allow selective loading.",
            "content": "Plain Text Extraction Most of the compiled source data sets already feature readily available plain text. In instances where the original corpus is only available in PDF format, we use Grobid [54] (for scholarly sources) or OlmOCR [55] (for other types of PDF) to obtain plain text. In sources where TEI or similar formats featuring structural markup are used, we convert to plain text and systematically exclude editorial elements such as title pages, page numbering and breaks, footnotes, bibliographic references, and textual apparatus, preserving only the core textual content. Markdown syntax is left intact when encountered. For wiki markup, we use mwparserfromhell10 to convert to plain text. Text Formatting To address artifacts introduced during optical character recognition present in large portion of the data, we further apply minimal amount of text formatting. We apply the standard formatters of the FTFY suite [56], including UTF-8 encoding fixes, removal of HTML entities, control characters and terminal escape sequences, ligature decomposition, character width and surrogate pair fixes, quote character normalization, and Unicode NFC normalization. In addition, we apply series of custom regex-based transformations to address common whitespace issues resulting from OCR-sourced data. It collapses multiple spaces and tabs into single spaces, reduces excessive newlines while preserving paragraph boundaries, removes hyphenation on line breaks, and removes leading and trailing whitespace from individual lines. Language & Length Filtering For datasets that indicate language in their metadata, we pre-filter using each datasets own language tags to reduce redundant classification work. Then, we employ the FastText language identification model [57] to automatically detect the language of the input texts. We apply the compressed model version [58], which supports 176 languages, to text snippets truncated to 4096 characters for computational efficiency. The classifier treats the input by replacing newlines with spaces to improve the prediction accuracy, as the FastText model was trained on single-line text samples. We discard text not classified as primarily German with probability of at least 0.65. We use the GPT-2 tokenizer [59] to obtain token counts for all sequences in the corpus. We discard sequences shorter than 32 tokens, as the majority of those are extraction artifacts. The token count is persisted alongside the text data for downstream filtering. Quality Filtering Following the consideration of other large-scale pretraining datasets, such as the BigScience ROOTS corpus [9] for BLOOM [60], Gopher [61], and CulturaX [62], we implement several quality indicators to remove low-quality text. Those include 10https://github.com/earwig/mwparserfromhell Table 4: Dataset schema Value Document identifier as found in the original dataset Source dataset this document stems from Thematic subset of this document Cleaned full text of this document Canonical SPDX license URL(s) of this document Key id source subset text license num_tokens Number of GPT-2 tokens in this document perplexity ocr_score"
        },
        {
            "title": "Perplexity of this document estimated by Wikipedia KenLM\nOCR quality as calculated by ocroscope",
            "content": "word count, average word length, symbol-to-word ratios (hash and ellipsis), bullet/ellipsis line ratios, alphabetic word ratio, stop word count, text repetition through duplicate line/paragraph fractions, character-level duplicate fractions, and n-gram repetition analysis (2-4 grams for top frequency, 5-10 grams for duplication). We select the exclusion parameters for these indicators using percentile-based thresholds [62], calculating the value distributions on languagefiltered and formatted data, and removing documents either below the 5th or above the 95th percentile, depending on indicator. As large subset of our data originates from OCR text, we additionally apply OCR-specific filtering heuristics to exclude errors not previously addressed through formatting fixes, namely character casing anomalies, fragmented words, and special character density. Exact parameters for all applied filters can be found in Appendix A.1. For all parameters, their choice was manually validated by cursory inspection of removed content for different percentile thresholds. Deviations from parameters used in prior work for English webtext [61] are little, and reasonable given the difference in language and domains of our data. Deduplication For deduplication, we rely on the LSH bloom filter implementation of Dolma [11]. We perform paragraph-level deduplication, splitting each document into chunks at newline characters. MinHash collisions are detected using 20-gram shingling and collision rate of 0.8, i.e., for two paragraphs to be deemed duplicates, 80% of their ngrams have to be identical. All but one chunk are removed from the corpus in case of collisions. The bloom filter was parametrized for false positive rate of 1e-4. We make available the bloom filter file for others to deduplicate new data against German Commons. PII Removal We remove personally identifiable information (PII) using combination of regex-based filters and the Presidio framework [63]. Types of PII removed are email addresses, phone numbers, IP addresses, credit-card numbers, IBAN numbers, and URL. To keep the semantic structure of sentences intact, we replace each of them with respective generic information. list of the replacements strings used is included in Appendix A.3, which allows lookup for full redaction or other replacements downstream. License Mapping and Filtering We map the diverse license identifiers indicated in the data to canonical set of SPDX license URLs pointing to the corresponding license of each document in the corpus. We then filter licenses to only open licenses as listed in Table 3. For cases where multiple licenses are given for document, we require all of them to be permissive. Gienapp et al. Table 5: Split Composition by domain for different training context lengths 𝑠. Assumes disjoint splits, i.e., 𝑠 8192 contains all sequences 2048 < 𝑠 8192. Percentage for (cid:205) is in relation to full dataset. 𝑠 2048 𝑠 8192 𝑠 32768 𝑠 >"
        },
        {
            "title": "3.20 B 26.47 % 0.68 B 1.37 % 0.40 B 0.84 % 49.73 B 83.72 %\n0.03 B 0.26 % 0.06 B 0.12 % 0.02 B 0.03 % 0.01 B 0.01 %\n0.17 B 1.44 % 1.08 B 2.16 % 1.48 B 3.11 % 0.24 B 0.41 %\n3.27 B 27.05 % 39.17 B 78.60 % 30.24 B 63.38 % 0.00 B 0.00 %\n0.14 B 1.17 % 0.27 B 0.55 % 0.51 B 1.06 % 2.64 B 4.44 %\n0.04 B 0.30 % 0.13 B 0.25 % 0.19 B 0.39 % 0.49 B 0.82 %\n5.23 B 43.31 % 8.45 B 16.95 % 14.88 B 31.18 % 6.30 B 10.61 %",
            "content": "(cid:205) 12.07 7.14 % 49.83 29.48 % 47.71 28.23 % 59.40 35.15 % Figure 1: Cumulative proportion of tokens by document overall (dashed), length in corpus, normalized by domain; news, and by subset for economic, cultural, legal, political, scientific, web."
        },
        {
            "title": "5 Corpus Statistics\nWe analyze corpus composition across thematic subsets and license\ntypes, demonstrate the efficacy of our filtering pipeline, and in-\nvestigate different text properties, in order to illustrate German\nCommons’ suitability for language model pretraining.",
            "content": "Token and Document Distribution By Thematic Domain. Figure 1 shows cumulative document proportions by length across thematic domains in German Commons. Distinct length distributions are apparent: cultural and web content concentrate in shorter documents, with cultural content showing rapid cumulative growth below 1,000 tokens due to page-level book segmentation in some source corpora. News content exhibits substantially longer documents, while legal, scientific, and political domains occupy intermediate positions similar to the overall corpus trend. Table 5 quantifies the practical implications of these length distributions for language model training contexts. When partitioning documents into subsets of short (𝑠 2048), medium (𝑠 8192), long (𝑠 32 768) and very long (𝑠 > 32 768) context lengths, domain distribution varies across partitions. At short contexts, web content dominates with 43.31% of available tokens, followed by news (27.05%) and cultural content The German Commons 154 Billion Tokens of Openly Licensed Text for German Language Models Table 6: Number of documents and tokens per license type."
        },
        {
            "title": "13.95 M 35.96 % 126.61 B\n34.48 B\n12.97 M 33.44 %\n7.93 B\n11.87 M 30.60 %",
            "content": "74.91 % 20.40 % 4.69 % Table 7: Number of tokens per license type and domain."
        },
        {
            "title": "Subset",
            "content": "Public-Domain Attribution"
        },
        {
            "title": "Cultural\nEconomic\nLegal\nNews\nPolitical\nScientific\nWeb",
            "content": ""
        },
        {
            "title": "0.20 B 0.58 %",
            "content": "(26.47%). For medium and long contexts, news articles provide the majority of tokens at 78.60% and 63.38% respectively, with web content ranging from to 16.95% to 31.18%. At very long contexts, most tokens (83.73%) originate from book-length documents of the cultural domain. Scientific, legal, and political domains maintain smaller representation across all context lengths, ranging from 0.53% each. All thematic domains remain represented in each length partition, enabling subor oversampling at different context lengths to control model exposure toward text genres. Token and Document Distribution By License Type. Table 6 presents document counts and token volume across license categories. Public domain equivalent licenses dominate token count with 126.61B tokens (74.291% of corpus) despite representing only 35.96% of documents, reflecting substantially longer average sequence lengths. This length disparity stems primarily from news articles and cultural content under public domain licensing. Attribution-type licenses contribute 34.48B tokens (20.40% of corpus) across 33.4% of documents, while copyleft licenses provide 7.93B tokens (4.69% of corpus) from 30.60% of documents. With licenses being derived from source datasets, license types exhibit strong correlations with thematic domains. Public domain content concentrates heavily in cultural (39.36% of public domain tokens) and news domains (57.39%), with minimal representation in web content (0.08%). Attribution licenses are predominantly found in web content (88.76% of attribution tokens), followed by cultural content (10.27%). Copyleft licenses span web sources (52.29%, including share-alike licensed Wikimedia projects) and political text (33.84%, primarily from EUPLlicensed Eurovoc). All thematic domains feature public domain data, and public domain data yields 75% of tokens. Filtering Statistics. Our data filtering pipeline removes noisy data in three sequential stages (also see Section A.4). (1) Quality filtering removed 46.41% of initial data, with the majority of that being nonGerman text eliminated from multilingual source corpora (e.g., The Stack, arXiv) and very short texts being removed (e.g., Wikipedia Table 8: Proportion of toxicity degree and corresponding toxicity label across paragraph sample. Differences for domains are minimal. No values above 3 are observed."
        },
        {
            "title": "Kind",
            "content": "Ability Gender/Sex Race/Origin Religion Violence Non-Toxic"
        },
        {
            "title": "Mildly Toxic",
            "content": "0 0.99 0.98 0.94 0.97 0.86 1 0.00 0.01 0.01 0.02 0.11 2 0.00 0.01 0.04 0.01 0.03 3 0.00 0.00 0.01 0.00 0.01 4 0.00 0.00 0.00 0.00 0.00 redirect pages and failed PDF extractions). (2) Deduplication only removes an additionaly 2.7% of text, concentrated in web and news corpora which exhibit both within-corpus and cross-corpus overlaps. Other domains showed minimal duplication. (3) Final license compliance and PII filtering removed negligible volumes (0.08%). Source corpora contained minimal personally identifiable information, occurring predominantly in web sources and economics content, while other domains required virtually no PII filtering. Overall retention reached 50.73% of input, which, however, is attributable to select multilingual corpora, with the majority of source corpora retaining between 70% and 95% of their content. This aligns with our filtering objective of eliminating noise while maximizing text retention at consistent quality. Only trace amounts of duplicates and PII were present in source corpora and subsequently removed. Text Properties. We employ four pretrained encoder-based German text classifiers11to assess text properties across on stratified random sample of 10 000 paragraphs per source corpus (385 467 total, due to less paragraphs available in some sources). The toxicity classifier grades content on 0-7 scales across five categories (ability, gender/sex, race/origin, religion, and violence), with scores 0-3 considered non-toxic. Results listed in Table 8 show no paragraphs scoring above 3 in any category, with on average 95% of paragraphs scoring 0. Remaining scores fall between 1-3, with negligible differences between thematic domains. The German Commons is thus deemed to contain only minimal amounts of harmful or toxic content. Language complexity classification (Figure 2) identifies four grades: plain (2%), easy (3%), everyday (65%), and special language (30%). Figure 2 reveals expected domain variations: scientific content exhibits highest special language proportion (63.8%), while web content shows highest everyday language (81.4%), with similar distributions found in Political, Economic, and Legal domains. News content demonstrates intermediate complexity with balanced distribution across categories. Cultural is nearly evenly divided between everyday (52.2%) and special language (39.9%). Overall, balanced complexity distribution across domains enables learning across linguistic registers. 11Toxicity [64]: https://hf.co/PleIAs/celadon Sentiment [65]: https://hf.co/oliverguhr/german-sentiment-bert Complexity: Fluency: https://hf.co/krupper/text-complexity-classification https://hf.co/EIStakovskii/bert-base-german-cased_fluency Gienapp et al. Third, standard German dominates content, diminishing linguistic diversity against non-standard varieties like Swiss, Austrian, or Low German dialects. Demographic biases may be present; socioeconomic stratification manifests through overrepresentation of formal registers from institutional sources. Cultural representation likely exhibits Western Protestant bias consistent with broader German NLP resources [68]. Targeted inclusion of dialectal and minority language varieties can improve this situation, if they become available under open licenses. Finally, privacy protection through PII removal provides limited security. Our regexand Presidio-based approaches constitute surface-level modifications. However, the historical skew and data sources from the public record diminish the potential adverse effects should PII be contained in the data. To enable informed downstream usage, we provide comprehensive documentation following established frameworks [4, 69] (Section A.4). The corpus includes document-level metadata preserving provenance and license information. Publishing the deduplication bloom filters enables cross-corpus contamination detection. Thematic partitioning supports selective usage based on application requirements. Together with the corpus, we also release highly scalable preprocessing pipeline with specific considerations for German language. This enables future additions and community contributions to the collection."
        },
        {
            "title": "7 Conclusion\nThe German Commons is a data collection intended to address a\nfundamental challenge in open German language model develop-\nment: the scarcity of large-scale, verifiably licensed training data\nfor German language. By systematically aggregating 154.56 billion\ntokens of openly licensed German text from institutional sources,\nthis work represents the largest collection of open German text to\ndate and enables the development of language models without the\nlicensing issues prevalent in web-scraped alternative corpora.",
            "content": "The corpus encompasses seven thematic domains, with text from web, political, legal, news, economics, cultural, and scientific sources. We apply systematic quality filtering, deduplication, and PII removal. Through detailed corpus statistics, we show the suitability of the included data for model training, and verify the high quality of the provided text. Every document in the corpus is further tagged with an explicit canonical SPDX license URL, enabling unambiguous downstream use. The German Commons thus represent critical step toward sustainable, ethically compliant development of German language models. Acknowledgments This work has been partially funded by the German Federal Ministry of Research, Technology, and Space (BMFTR) under Grants 01IS24077A, 01IS24077B, and 01IS24077D; by the ScaDS.AI Center for Scalable Data Analytics and Artificial Intelligence, funded by the BMFTR and by the Sächsische Staatsministerium für Wissenschaft, Kultur und Tourismus under Grant ScaDS.AI; and by the OpenWebSearch.eu project, funded by the European Union under Grant GA 101070014. Figure 2: Proportion of text complexity ( simple, special ) across paragraph sample, per subset. everyday, easy, Figure 3: Proportion of text sentiment classes ( neutral, positive ) across paragraph sample, per domain. negative, Sentiment analysis (Figure 3) categorizes text as negative (16.4%), neutral (80.5%), or positive (3.1%). Web content exhibits highest negative sentiment (31.8%), while cultural content shows most positive sentiment (5.0%). News content demonstrates the highest proportion of neutral sentiment (92.0%). The remaining domains maintain proportions similar to the overall distribution. Mostly neutral text prevents systematic model biases w.r.t. sentiment."
        },
        {
            "title": "6 Limitations and Ethical Considerations\nThe German Commons inherits fundamental limitations from its\nconstituent sources and curation methodology. We identify four\nprimary limitations requiring explicit acknowledgment and propose\nfuture mitigation strategies.",
            "content": "First, the corpus exhibits temporal bias toward historical content. News (47.02%) and cultural domains (35.25%) comprise 82.27% of tokens, with cultural content predominantly sourced from 18th20th century digitized texts. This historical skew induces nostalgia bias [66]. Scientific (0.54%) and economic (0.07%) domains remain critically underrepresented. Adding contemporary German text to rebalance the temporal distribution is paramount for future extensions of the corpus. Second, the predominance of OCR-extracted text may introduce errors. German diacritics exhibit heightened vulnerability to misrecognition [67]. While our pipeline applies OCR-specific filtering, residual errors may persist particularly in older texts. We did not apply LLM-based correction methods for error reduction, due to substantial computational expenditure, hallucination risks, and misinterpretation of historical texts, however, would be future improvement if specialized correction models become available. The German Commons 154 Billion Tokens of Openly Licensed Text for German Language Models References [1] [2] [3] [4] [5] [6] [7] [8] [9] Longpre et al. 2023. The Data Provenance Initiative: Large Scale Audit of Dataset Licensing & Attribution in AI. CoRR, abs/2310.16787. doi:10 . 48550 /ARXIV.2310.16787. Kilgarriff and Grefenstette. 2003. Introduction to the Special Issue on the Web as Corpus. Comput. Linguistics, 29, 3, 333348. doi:10.1162/089120103322711569. Brin, Motwani, Page, and Winograd. 1998. What can you do with Web in your Pocket? IEEE Data Eng. Bull., 21, 2, 3747. Gebru, Morgenstern, Vecchione, Vaughan, Wallach, III, and Crawford. 2021. Datasheets for datasets. Commun. ACM, 64, 12, 8692. doi:10.1145/3458723. Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu. 2020. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. J. Mach. Learn. Res., 21, 140:1140:67. Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant, Barua, and Raffel. 2021. mT5: Massively Multilingual Pre-trained Text-to-Text Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the ACL: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021. ACL, 483498. doi:10.1 8653/V1/2021.NAACL-MAIN.41. Gao et al. 2021. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. CoRR, abs/2101.00027. Abadji, Suárez, Romary, and Sagot. 2022. Towards Cleaner Document-Oriented Multilingual Crawled Corpus. In Proc. of LREC. European Language Resources Association, 43444355. Laurençon et al. 2022. The BigScience ROOTS Corpus: 1.6TB Composite Multilingual Dataset. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. [11] [12] [10] Weber et al. 2024. RedPajama: an Open Dataset for Training Large Language Models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Soldaini et al. 2024. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. In Proceedings of the 62nd Annual Meeting of the ACL (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024. ACL, 1572515788. doi:10.18653/V1/2024.ACL-LONG.840. De Gibert et al. 2024. New Massive Multilingual Dataset for High-Performance Language Technologies. In Proc. of LREC/COLING. ELRA and ICCL, 11161128. Penedo, Kydlícek, Allal, Lozhkov, Mitchell, Raffel, von Werra, and Wolf. 2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Penedo et al. 2025. FineWeb2: One Pipeline to Scale Them All - Adapting Pre-Training Data Processing to Every Language. CoRR, abs/2506.20920. doi:10 .48550/ARXIV.2506.20920. [14] [13] [15] Habernal, Zayed, and Gurevych. 2016. C4Corpus: Multilingual Web-size Corpus with Free License. In Proc. of LREC. European Language Resources Association (ELRA). II, Bommarito, and Katz. 2025. The KL3M Data Project: Copyright-Clean Training Resources for Large Language Models. CoRR, abs/2504.07854. doi:10.4 8550/ARXIV.2504.07854. [16] [18] [17] Hong, Hutson, Agnew, Huda, Kohno, and Morgenstern. 2025. Common Pool of Privacy Problems: Legal and Technical Lessons from Large-Scale Web-Scraped Machine Learning Dataset. CoRR, abs/2506.17185. doi:10.48550 /ARXIV.2506.17185. Longpre et al. 2024. Pretrainers Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity. In Proceedings of the 2024 Conference of the North American Chapter of the ACL: Human Language Technologies (Volume 1: Long Papers). ACL, 32453276. doi:10.18653/v1/2024.na acl-long.179. [19] Wang, Zhong, Wang, Zhu, Mi, Wang, Shang, Jiang, and Liu. 2024. Data Man- [20] [21] [22] agement For Training Large Language Models: Survey. (2024). Goldhahn, Eckart, and Quasthoff. 2012. Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages. In Proc. of LREC. European Language Resources Association (ELRA), 759765. Tiedemann. 2012. Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012. European Language Resources Association (ELRA), 22142218. Kocetkov et al. 2023. The Stack: 3 TB of permissively licensed source code. Trans. Mach. Learn. Res., 2023. [23] Min, Gururangan, Wallace, Shi, Hajishirzi, Smith, and Zettlemoyer. 2024. SILO Language Models: Isolating Legal Risk In Nonparametric Datastore. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Kandpal et al. 2025. The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text. CoRR, abs/2506.05209. doi:10.48550/ARXIV.2506.05 209. [24] [25] [26] [27] Langlais et al. 2025. Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training. CoRR, abs/2506.01732. doi:10.48550/ARXIV.2506.01732. Nolda. Wikipedia-Korpus: Korpusquellen der deutschsprachigen Wikipedia im TEI-Format. Version 20250101. Zenodo. doi:10.5281/zenodo.14748605. Nolda. Wikivoyage-Korpus: Korpusquellen der deutschen Sprachversion von Wikivoyage im TEI-Format. Version 20250101. Zenodo. doi:10.5281/zenodo.14 748553. [28] Margaretha and Lüngen. 2014. Building Linguistic Corpora from Wikipedia [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] Articles and Discussions. J. Lang. Technol. Comput. Linguistics, 29, 2, 5982. Schabus, Skowron, and Trapp. 2017. One Million Posts: Data Set of German Online Discussions. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017. ACM, 12411244. doi:10.1145/3077136.3080711. Boenig, Haaf, Nolda, and Wiegand. Reichstagsprotokoll-Korpus. Version v1.0.2. Zenodo. doi:10.5281/zenodo.10225467. Barbaresi. German Political Speeches Corpus. Version v4.2019. Zenodo. doi:10 .5281/zenodo.3611246. Fobbe. Corpus der Drucksachen des Deutschen Bundestages (CDRS-BT). Version 2021-04-02. Zenodo. doi:10.5281/zenodo.4643066. Fobbe. Corpus der Plenarprotokolle des Deutschen Bundestages (CPP-BT). Version 2021-02-17. Zenodo. doi:10.5281/zenodo.4542662. Fobbe. Corpus des Deutschen Bundesrechts (C-DBR). Version 2025-01-07. Zenodo. doi:10.5281/zenodo.14592346. Ostendorff, Blume, and Ostendorff. 2020. Towards an Open Platform for Legal Information. In JCDL 20: Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, Virtual Event, China, August 1-5, 2020. ACM, 385388. doi:10.1145/3383583.3398616. Fobbe. Corpus der Entscheidungen des Bundesfinanzhofs (CE-BFH). Version 202501-14. Zenodo. doi:10.5281/zenodo.14622341. Fobbe and Swalve. Entscheidungen des Bundesgerichtshofs in Strafsachen aus dem 20. Jahrhundert (BGH-Strafsachen-20Jhd). Version 1.0.0. Zenodo. doi:10.5 281/zenodo.4540377. Fobbe. Corpus der Entscheidungen des Bundesgerichtshofs (CE-BGH). Version 2024-09-25. Zenodo. doi:10.5281/zenodo.12814022. Fobbe. Corpus der Entscheidungen des Bundesverfassungsgerichts (CE-BVerfG). Version 2024-07-24. Zenodo. doi:10.5281/zenodo.12705674. Fobbe. Corpus der Entscheidungen des Bundespatentgerichts (CE-BPatG). Version 2024-07-09. Zenodo. doi:10.5281/zenodo.10849977. Fobbe. Corpus der Entscheidungen des Bundesverwaltungsgerichts (CE-BVerwG). Version 2024-03-13. Zenodo. doi:10.5281/zenodo.10809039. Fobbe. Corpus der amtlichen Entscheidungssammlung des Bundesverfassungsgerichts (C-BVerfGE). Version 2024-03-08. Zenodo. doi:10.5281/zenodo.107831 77. Fobbe. Corpus der Entscheidungen des Bundesarbeitsgerichts (CE-BAG). Version 2020-08-28. Zenodo. doi:10.5281/zenodo.4006645. Chalkidis, Fergadiotis, and Androutsopoulos. 2021. MultiEURLEX - multilingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. In ACL, 69746996. doi:10. 18653 / V1 /2021 . EMNLP - MAIN.559. Boenig and Hug. DiBiLit-Korpus. Version v3.0. Zenodo. doi:10.5281/zenodo.57 86725. Labs. 2021. Digitised Books. c. 1510 - c. 1900. JSONL (OCR derived text + metadata). https://doi.org/10.23636/r7w6-zy15. (2021). Gaido, Papi, Bentivogli, Brutti, Cettolo, Gretter, Matassoni, Nabih, and Negri. 2024. MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages. In Proc. of EMNLP. ACL, 13934 13947. Labusch and Lehmann. Fulltexts of the Digitized Collections of the Berlin State Library (SBB). Version 1. Staatsbibliothek zu Berlin - Berlin State Library. doi:10.5281/zenodo.7716098. Nolda. Wikibooks-Korpus: Korpusquellen von deutschsprachigen Wikibooks im TEI-Format. Version 20250101. Zenodo. doi:10.5281/zenodo.14748586. [51] [52] [50] Hug, Kassung, and Meyer. 2010. Dingler-Online The Digitized Polytechnisches Journal on Goobi Digitization Suite. In Digital Humanities DH2010. Conference Abstracts, 311313. Priem, Piwowar, and Orr. 2022. OpenAlex: fully-open index of scholarly works, authors, venues, institutions, and concepts. CoRR, abs/2205.01833. doi:1 0.48550/ARXIV.2205.01833. Lee, Cooper, and Grimmelmann. 2024. Talkin Bout AI Generation: Copyright and the Generative-AI Supply Chain (The Short Version). In Proceedings of the Symposium on Computer Science and Law, CSLAW 2024, Boston, MA, USA, March 12-13, 2024. ACM, 4863. doi:10.1145/3614407.3643696. Tarkowski. 2025. Data Governance in Open Source AI. 2025. GROBID. https://github.com/kermitt2/grobid. (2025). Poznanski, Borchardt, Dunkelberger, Huff, Lin, Rangapur, Wilhelm, Lo, and Soldaini. 2025. olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models. CoRR, abs/2502.18443. doi:10.48550/ARXIV.2502.18443. [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] Speer. 2019. ftfy. Zenodo. (2019). doi:10.5281/zenodo.2591652. Joulin, Grave, Bojanowski, Douze, Jégou, and Mikolov. 2016. FastText.zip: Compressing text classification models. CoRR, abs/1612.03651. Joulin, Grave, Bojanowski, and Mikolov. 2017. Bag of Tricks for Efficient Text Classification. In Proc. of EACL. ACL, 427431. doi:10.18653/V1/E17-2068. Radford, Wu, Child, Luan, Amodei, Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1, 8, 9. Scao et al. 2022. BLOOM: 176B-Parameter Open-Access Multilingual Language Model. CoRR, abs/2211.05100. doi:10.48550/ARXIV.2211.05100. Rae et al. 2021. Scaling Language Models: Methods, Analysis & Insights from Training Gopher. CoRR, abs/2112.11446. Nguyen, Nguyen, Lai, Man, Ngo, Dernoncourt, Rossi, and Nguyen. 2024. CulturaX: Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy. ELRA and ICCL, 42264237. [63] Mendels, Peled, Vaisman Levy, Hart, Rosenthal, Lahiani, et al. 2018. Microsoft Presidio: Context aware, pluggable and customizable PII anonymization service for text and images. Microsoft, (2018). Gienapp et al. [64] [65] [66] [67] [68] [69] Arnett, Jones, Yamshchikov, and Langlais. 2024. Toxicity of the Commons: Curating Open-Source Pre-Training Data. arXiv preprint arXiv:2410.22587. Guhr, Schumann, Bahrmann, and Böhme. 2020. Training broad-coverage german sentiment classification model for dialog systems. In Proceedings of The 12th Language Resources and Evaluation Conference. European Language Resources Association, 16201625. Zhu, Chen, Gao, and Wang. 2024. Evaluating llms at evaluating temporal generalization. CoRR, abs/2405.08460. doi:10.48550/ARXIV.2405.08460. Kanerva, Ledins, Käpyaho, and Ginter. 2025. OCR error post-correction with llms in historical documents: no free lunches. CoRR, abs/2502.01205. doi:10.485 50/ARXIV.2502.01205. Kurpicz-Briki. 2020. Cultural differences in bias? origin and gender bias in pre-trained german and french word embeddings. In Proceedings of the 5th Swiss Text Analytics Conference and the 16th Conference on Natural Language Processing, SwissText/KONVENS 2020, Zurich, Switzerland, June 23-25, 2020 [online only] (CEUR Workshop Proceedings). Vol. 2624. CEUR-WS.org. Bender and Friedman. 2018. Data statements for natural language processing: toward mitigating system bias and enabling better science. Trans. Assoc. Comput. Linguistics, 6, 587604. doi:10.1162/TACL_A_00041. Excl. if > 0.99 > 0.70 > 0.30 > 0.10 > 0.10 < 6.00 > 0.25 > 0.15 > 0.30 > 0. > 0.39 > 0.39 > 0.38 > 0.38 > 0.37 > 0.37 > 0.07 > 0.10 > 0.13 The German Commons 154 Billion Tokens of Openly Licensed Text for German Language Models"
        },
        {
            "title": "A Data Filtering",
            "content": "A.1 Quality Filtering Parameter Values Parameter Alphabetic Word Ratio Bullet Line Ratio Ellipsis Line Ratio Ellipsis Ratio Hash Ratio Stop-word Count Explanation Ratio of whitespace-separated words consisting of only alphabetic characters Ratio of lines starting with or - characters Ratio of lines ending in ... or . . . Ratio of ... or . . . substrings occuring to overall whitespace-separated words Ratio of # character occuring to overall whitespace-separated words Number of stopwords in text; for stop words used, see below Amount of duplicated lines in document, measured as ratio of lines. Duplicated Line Fraction Amount of duplicated lines in document, measured as ratio of characters. Duplicated Lines Character Fraction Amount of duplicated paragraphs in document, measured as ratio of paragraphs. Duplicated Paragraph Fraction Duplicated Paragraph Character Fraction Amount of duplicated paragraphs in document, measured as ratio of characters. Duplicate 5-gram Character Fraction Duplicate 6-gram Character Fraction Duplicate 7-gram Character Fraction Duplicate 8-gram Character Fraction Duplicate 9-gram Character Fraction Duplicate 10-gram Character Fraction Top-2-Gram Character Fraction Top-3-Gram Character Fraction Top-4-Gram Character Fraction"
        },
        {
            "title": "Spacing Anomaly Ratio\nCase Anomaly Ratio\nWord Fragment Ratio\nLine Artifact Ratio\nSpecial Character Density\nRepeated Character Ratio\nNumeric Context Errors",
            "content": "Text accounted for by duplicated n-grams, measured as ratio of characters. Text accounted for by most frequent n-gram, measured as ratio of characters. Ratio of spacing anomalies; missing spaces, excessive spaces, spaced words > 0.15 Ratio of case anomalies such as random capitalization and mixed case within words > 0.10 Ratio of likely OCR word fragments (1-2 character words excluding common words) > 0.20 > 0.25 Ratio of lines that are likely OCR artifacts (single characters, page numbers) > 0.03 Density of unusual unicode characters > 0.05 Ratio of text consisting of repeated character sequences and repeated patterns > 0.08 Ratio of numbers inappropriately embedded within words (excluding ordinals) Avg. Word Length (min) Avg. Word Length (max) Word Length Standard Deviation (min) Word Length Standard Deviation (max) Very Short Words Ratio Very Long Words Ratio Average length of whitespace-separated words in characters Average length of whitespace-separated words in characters Standard deviation of word lengths in characters Standard deviation of word lengths in characters Ratio of words with length 1 character after removing punctuation Ratio of words with length 15 characters after removing punctuation < 4.80 > 7.30 < 1.00 > 5.00 > 0.10 > 0. A.2 Stopwords"
        },
        {
            "title": "Category\nDefinite articles\nIndefinite articles\nConjunctions\nCommon Verbs\nPrepositions\nPronouns\nAdverbs\nSubordinating Conjunctions\nContractions\nQuestion words\nQuantifiers\nModal Verbs\nParticles",
            "content": "Words der, die, das, den, dem, des ein, eine, einen, einem, einer und, oder, aber ist, sind, hat, haben, wird, werden, von, zu, mit, in, auf, für, bei, nach, vor, über, unter, durch, gegen, ohne, um ich, du, er, sie, es, wir, ihr, sich, sein, seine, ihrer, ihren, mich, dich nicht, auch, nur, noch, schon, hier, dort, da, dann, jetzt, heute sehr, mehr, weniger, ganz, gar, etwa dass, wenn, als, wie an, am, im, ins, zum, zur, vom, beim was, wer, wo, wann, warum, wie, welche, welcher alle, viele, einige, andere, jede, jeden, jeder kann, könnte, muss, soll, will, würde ja, nein, doch, so, also, nun, mal A.3 PII Generic Replacement Values"
        },
        {
            "title": "Generic Replacement",
            "content": "4242 4242 4242 4242 192.0.2.255 name@beispiel.de +49 123 45678910 DE02 1203 0000 0000 2020 51 https://www.beispiel.de"
        },
        {
            "title": "Comment",
            "content": "VISA testing number; valid but unused RFC 5737 Test Block 1 Example domain Invalid number with correct format DKB testing number; valid but unused Example domain A.4 Processing Step Statistics Data Source Wikipedia Wikivoyage Wiki Discussions Youtube-Commons One Million Posts Corpus The Stack Reichtagsprotokolle German Political Speeches C. d. Drucksachen d. dt. BT C. d. Plenarprotok. d. dt. BT EuroVoc Deutsches Bundesrecht OpenLegalData C. d. Ents. d. BFH Ents. d. BGH (20. Jhd.) C. d. Ents. d. BGH C. d. Ents. d. BVerfG C. d. Ents. d. BpatG C. d. Ents. d. BVerwG C. d. amtl. E.-S. BVerfG C. d. Ents. d. BAG EurLEX Deutsches Zeitungsportal Europeana Newspapers Wikinews Anno TEDEUTenders DiBiLit-Korpus DiBiPhil-Korpus Wikisource German-PD BLBooks SBB Fulltexts Wikiquote MOSEL Initial Filtered Deduplicated Final # Docs # Tokens # Docs # Tokens # Docs # Tokens # Docs # Tokens"
        },
        {
            "title": "0.216 B 74.35 % 0.002 M 99.04 %\n0.032 B 97.72 % 0.000 M 99.99 %\n0.348 B 49.59 % 0.241 M 41.77 %",
            "content": "Dig. d. Polytechn. Journals Wikibooks DOAB arXiv Wikiversity OpenALEX"
        },
        {
            "title": "71.473 M 304.629 B 37.594 M 52.60 % 163.266 B 53.59 % 35.835 M 50.14 % 154.799 B 50.81 % 35.778 M 50.06 % 154.558 B 50.73 %",
            "content": "Percentages indicate remaining of initial. After filtering using the source datasets metadata, if available. Includes PII replacement and final license filtering. The German Commons 154 Billion Tokens of Openly Licensed Text for German Language Models Datasheet: German Commons Motivation Why was the dataset created? German Commons addresses the critical gap in large-scale open German text for language model training. Existing German corpora either lack explicit licensing, contain web-scraped content of uncertain provenance, or provide insufficient scale. Has the dataset been used already? This represents the initial release of German Commons. No external usage has occurred prior to publication. What (other) tasks could the dataset be used for? Beyond language model pretraining, German Commons supports all German NLP research requiring clean, licensecompliant text, multilingual model development, or linguistic analysis of German text across domains. The diverse domain coverage (legal, cultural, scientific, etc.) further enables domain-specific model development and crossdomain evaluation studies. Who funded the creation of the dataset? Dataset compilation was supported by German and European research grants: German Federal Ministry of Research, Technology, and Space (BMFTR) under Grants 01IS24077A, 01IS24077B, and 01IS24077D, by the ScaDS.AI Center for Scalable Data Analytics and Artificial Intelligence, funded by the BMFTR and by the Sächsische Staatsministerium für Wissenschaft, Kultur und Tourismus under Grant ScaDS.AI, and by the OpenWeb-Search.eu project, funded by the European Union under Grant GA 101070014. Constituent datasets originate primarily from state-funded institutions across Germany and Austria. Dataset Composition What are the instances? Each instance represents single German-language document with associated metadata and licensing information. How many instances are there in total? The dataset contains 35,778,211 documents comprising 154,558,196,961 GPT-2 tokens. What data does each instance consist of? Each instance includes: unique identifier for source crossreferencing, source dataset name, quality-filtered and paragraph-deduplicated raw text, canonical SPDX license URL, thematic domain key, GPT-2 token count, perplexity score from German Wikipedia KenLM model, and OCR quality score. Is there label or target associated with each instance? No supervised labels exist. However, each instance contains metadata labels for thematic domain classification, licensing information, and document length statistics. Is any information missing from individual instances? Paragraph-level deduplication may alter texts from their original form. Personally identifiable information has been systematically removed. Does the dataset contain all possible instances or is it sample (not necessarily random) of instances from larger set? The dataset represents filtered subset of source collections. Filtering removes OCR errors, extraction artifacts, and low-quality or duplicated content, creating curated selection. Are there recommended data splits? No predefined splits are provided. All data is intended for pretraining. Are there any errors, sources of noise, or redundancies in the dataset? Despite quality filtering and deduplication, residual issues may remain: (1) cross-corpus text duplicates from overlapping sources, and (2) extraction artifacts from OCR and PDF-to-text processing. Is the dataset self-contained, or does it link to or otherwise rely on external resources? The dataset is selfcontained and centrally downloadable. The Source dataset references provided enable reproducible reconstruction. Collection Process What mechanisms or procedures were used to collect the data? Data collection employed multiple automated procedures: (1) direct download from institutional repositories and open platforms, (2) programmatic crawling via APIs where available, and (3) automated text extraction from PDF and other document formats using specialized libraries. Then, the open source processing pipelines were applied for quality filtering and deduplication all sources. Validation occurred through manual inspection of sample outputs, cross-verification against source repositories, and automated consistency checks. How was the data associated with each instance acquired? All text data represents directly observable content from original sources; no inference or derivation occurred. Metadata (licensing, thematic classification, source attribution) was extracted directly from source repository information or explicitly provided by institutional datasets. Where PDF extraction was required, raw text underwent validation against source documents to verify accuracy. If the dataset is sample from larger set, what was the sampling strategy? Sampling was deterministic based on explicit criteria: (1) German language content as per automated classification (2) explicit open licensing, (3) quality thresholds, and (4) institutional source verification. No probabilistic sampling occurred; all content meeting inclusion criteria was retained after deduplication. Who was involved in the data collection process and how were they compensated? Data collection was conducted by the author team using automated systems. No crowdworkers, contractors, or external annotators were employed. All processing occurred through programmatic methods without manual content creation or labeling requiring compensation. Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances? Collection occurred between January and August 2025, using source dataset versions available through August 31st, 2025. The underlying content creation spans multiple centuries, representing temporal range that significantly predates and extends beyond the collection timeframe. Data Preprocessing Was any preprocessing/cleaning/labeling of the data done? Comprehensive preprocessing included: (1) text extraction from PDFs and OCR sources with encoding normalization, (2) language detection and filtering for German content, and (3) quality filtering targeting digitization artifacts and extraction errors, (4) paragraph-level deduplication using content hashing, (5) systematic PII removal, (6) format standardization across all source types. Thematic domain classification was applied based on source dataset. Was the raw data saved in addition to the preprocessed/cleaned/labeled data? Raw data is not provided since all constituent source datasets remain publicly accessible through their original repositories. Is the software used to preprocess/clean/label the instances available? All preprocessing software is open source and available at https://github.com/coral-nlp/germ an-commons and https://github.com/coral-nlp/llmdata , ensuring complete reproducibility of the dataset. Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? Yes. The procedure successfully addresses the identified gap by: (1) providing the largest collection to-date of openly licensed German text, (2) enabling open German language model development without licensing uncertainties, and (2) establishing reproducible methodology for future dataset construction. This directly fulfills the stated motivation of creating license-compliant, large-scale German training data. How will the dataset be distributed? The dataset is distributed as Parquet files through multiple public repositories for redundancy. Primary distribution occurs via Hugging Face Hub at https://huggingface.co/datasets/coralnlp/german-commons. When will the dataset be released/first distributed? What license (if any) is it distributed under? Public release occurred on 2025/10/14. Dataset metadata and compilation are licensed under ODC-BY 1.0. Individual document texts retain their original licenses as specified in each instances SPDX URL field, creating heterogeneous but fully documented licensing structure. Are there any copyrights on the data? Yes. Each document retains copyright under its original creator or institutional provider, governed by the specific license indicated in the instance metadata. The compilation itself does not claim additional copyright over constituent texts. Are there any fees or access/export restrictions? The dataset is freely accessible without fees or registration requirements. However, users must comply with individual document licenses, which may include attribution requirements or share-alike provisions. Commercial use is permitted by all constituent licenses. Dataset Maintenance Who is supporting/hosting/maintaining the dataset? The dataset is maintained by the authors of this report. Will the dataset be updated? If so, how often and by whom? Updates may occur when significant new German open-source collections become available. The original authors will coordinate updates, with community contributions welcomed through the open-source pipeline. How will updates be communicated? Updates will be announced through: (1) versioned releases on hosting platforms with detailed changelogs, (2) academic publication updates when substantial changes occur. If the dataset becomes obsolete how will this be communicated? Obsolescence will be communicated through deprecation notices on all hosting platforms. Is there repository to link to any/all papers/systems that use this dataset? No centralized usage repository will be maintained. Usage tracking occurs through standard academic citation of the dataset paper. Users are encouraged to cite the dataset publication when reporting results or building derivative works. If others want to extend/augment/build on this dataset, is there mechanism for them to do so? The open-source llmdata pipeline enables community extensions through standardized data ingestion protocols for new sources and automated quality assessment and deduplication using established filtering criteria. Community contributions undergo review by the maintenance team. Ethical Considerations Were any ethical review processes conducted? No formal institutional review board process was conducted. The dataset relies exclusively on pre-existing, publicly available, and explicitly licensed materials from established institutional sources. Data processing incorporated ethical considerations including systematic PII removal and exclusion of sources lacking clear licensing frameworks. Does the dataset contain data that might be considered confidential? No. All included content derives from explicitly open-licensed institutional sources. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? Potentially yes. The dataset spans centuries of German text documents, which may include historical perspectives, political viewpoints, or language that could be considered offensive by contemporary standards. The scale and temporal range make comprehensive content moderation infeasible. Users should exercise appropriate caution. Does the dataset relate to people? The dataset may contain publicly available information relating to individuals in various contexts including historical documents, biographical information, academic citations, and government records."
        }
    ],
    "affiliations": [
        "Friedrich-Schiller-Universität Jena",
        "German National Library",
        "Independent Researcher",
        "InfAI and ScaDS.AI",
        "Leipzig University and ScaDS.AI",
        "University of Kassel, hessian.AI, and ScaDS.AI"
    ]
}