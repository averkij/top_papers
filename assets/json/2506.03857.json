{
    "paper_title": "Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation",
    "authors": [
        "Mingxuan Xia",
        "Haobo Wang",
        "Yixuan Li",
        "Zewei Yu",
        "Jindong Wang",
        "Junbo Zhao",
        "Runze Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine a single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose a novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop a teacher-student framework CanDist that distills candidate annotations with a Small Language Model (SLM). We further provide a rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https://github.com/MingxuanXia/CanDist."
        },
        {
            "title": "Start",
            "content": "Prompt Candidates, then Distill: Teacher-Student Framework for LLM-driven Data Annotation Mingxuan Xia1, Haobo Wang1*, Yixuan Li2, Zewei Yu1, Jindong Wang3, Junbo Zhao1, Runze Wu4 1Zhejiang University 2University of Wisconsin Madison 3William & Mary 4NetEase Fuxi AI Lab {xiamingxuan,wanghaobo,22451274,j.zhao}@zju.edu.cn sharonli@cs.wisc.edu,jwang80@wm.edu,wurunze1@corp.netease.com 5 2 0 2 4 ] . [ 1 7 5 8 3 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recently, Large Language Models (LLMs) have demonstrated significant potential for data annotation, markedly reducing the labor costs associated with downstream applications. However, existing methods mostly adopt an aggressive strategy by prompting LLM to determine single gold label for each unlabeled sample. Due to the inherent uncertainty within LLMs, they often produce incorrect labels for difficult samples, severely compromising the data quality for downstream applications. Motivated by ambiguity aversion in human behaviors, we propose novel candidate annotation paradigm wherein large language models are encouraged to output all possible labels when incurring uncertainty. To ensure unique labels are provided for downstream tasks, we develop teacher-student framework CanDist that distills candidate annotations with Small Language Model (SLM). We further provide rigorous justification demonstrating that distilling candidate annotations from the teacher LLM offers superior theoretical guarantees compared to directly using single annotations. Extensive experiments across six text classification tasks validate the effectiveness of our proposed method. The source code is available at https: //github.com/MingxuanXia/CanDist."
        },
        {
            "title": "Introduction",
            "content": "Various NLP tasks require collecting high-quality labeled data for model training (e.g. text classification (Kowsari et al., 2019), named entity recognition (Li et al., 2022a), and sentiment analysis (Wankhade et al., 2022)), which typically involves human experts meticulously providing high-quality target labels, process that is notoriously timeconsuming and labor-intensive. With the development of Large Language Models (OpenAI, 2023; Anil et al., 2023; Dubey et al., 2024), LLM-driven automatic data annotation approaches have been * Corresponding author. Figure 1: When facing uncertainty, humans instinctively behave ambiguity aversion to avoid risk, which motivated us to prompt LLM for candidate annotations (multiple possible answers), increasing the likelihood of providing the correct labels. proposed (Gilardi et al., 2023; Tan et al., 2024; Long et al., 2024), relieving the burden of the costprohibitive human annotation. Although LLMs excel at general language understanding and generation, their knowledge of downstream tasks remains limited (Li et al., 2024). As result, LLMs may be uncertain about some samples during annotation. Nevertheless, existing LLMdriven annotation methods prompt LLMs with single annotation, which forces the model to assign specific label to each unlabeled sampleeven when it is unsure. This often leads to completely wrong annotations, which is not only waste of computational resources but also affects downstream training (Zhu et al., 2022). Moreover, it necessitates further error localization and re-labeling, which is both costly and time-consuming. This raises critical question: Can we induce LLMs to provide more valuable annotation rather than completely wrong label when they are uncertain? To answer this question, we first draw an anallabels gradually emerge from those false positive ones. Theoretically, we justify that distilling from candidate annotations from the teacher LLM offers superior theoretical guarantees than directly using the single annotations from the teacher LLM. Empirically, we evaluate CanDist on six text classification tasks, where CanDist achieves state-of-the-art among various LLM and SLM baselines."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 LLM for Data Annotation LLM-driven data annotation has been applied in various NLP tasks, such as text classification (Gilardi et al., 2023), relation extraction (Ding et al., 2023), named entity recognition (Ye et al., 2024), question answering (He et al., 2024b), semantic parsing (Shin et al., 2021), and multilingual text generation (Choi et al., 2024). Advanced approaches adopt techniques like in-context learning (Brown et al., 2020; Xiao et al., 2023; Liu et al., 2024), chain-of-thought prompting (Wei et al., 2022; He et al., 2024b; Yuan et al., 2024), and collaboration with fine-tuned SLMs (Xiao et al., 2023; Xu et al., 2024; Yang et al., 2024) to boost LLMs zero-shot performance for annotations. However, these approaches limit LLMs to provide single annotations, which inevitably introduce completely wrong labels. In contrast, we investigate more conservative strategy by prompting LLMs for candidate annotations, which offers greater value. Besides, while FreeAL (Xiao et al., 2023), the pioneering work of SLM-collaborated annotation, has demonstrated the effectiveness of distilling the SLM from LLMs single annotations, we propose that distilling from candidate annotations yields superior results and we rigorously provide its theoretical guarantees. 2.2 Generate and Aggregate Multiple Answers with LLM Recently, solving NLP tasks by generating multiple diverse answers using LLMs and then aggregating them to extract their essences has been increasingly popular. Sampling-based strategy first samples diverse set of reasoning paths during LLM decoding, and then integrate them through methods such as trained ranking models (Cobbe et al., 2021; Shen et al., 2021; Thoppilan et al., 2022), majority voting (Wang et al., 2023; Fu et al., 2023; Li et al., 2022b), LLMs (Chen et al., 2023; Weng et al., 2023; Zhang et al., 2024b), or human feedback (Li, Figure 2: Comparison of 1 α-error and F1-score between single annotations (SA) and candidate annotations (CA) by GPT-3.5. Higher metric values indicate better results. See section 3.2 for details. ogy to human behaviorwhen faced with uncertainty, humans often behave conservatively instead of being overconfidentan instinctive psychological phenomenon known as Ambiguity Aversion (Fox and Tversky, 1995; Maccheroni et al., 2006). This behavior helps people mitigate severe risks and ensures the lower bound of the gains. Motivated by this, we propose to induce LLMs to exhibit ambiguity aversion during annotation, by prompting them to provide multiple possible labels for each unlabeled sample, i.e., candidate annotations. As shown in Figure 1, although the LLM may fail to provide correct answer with single label, answering with candidate labels successfully includes the correct one. We further demonstrate in Figure 2 that, on macro level, candidate annotations are more likely to cover correct labels (higher 1 α-error) and retain more value (higher F1-score) than single annotations. Note that, unlike methods such as Self-Consistency (Wang et al., 2023), prompting candidates is asking for the inherent uncertainty rather than randomness, see Table 4 for detailed discussion. Despite its great potential, however, candidate annotations cannot be directly applied to downstream tasks, as they require one specific label for each sample. To address this issue, we draw inspiration from knowledge distillation (Hinton et al., 2015) where the student model is distilled from the teachers output distribution and exhibits better generalization on downstream tasks (Phuong and Lampert, 2019), and propose teacher-student framework called CanDist that distills high-quality knowledge from the teacher LLMs candidate annotations to student Small Language Model (SLM) to achieve data annotation. Specifically, we introduce distribution refinery (DR) mechanism during distillation that dynamically adjusts the training target based on SLMs predictions, where correct Table 1: Key prompts of prompting single (SA) and candidate (CAadd and CAall) annotations on the TREC dataset. Strategy Prompt SA CAadd CAall Given question: . . . What does the question ask about? Please identify the question into one of the following types: Abbreviation; Description and abstract concepts; Entities; Human beings; Locations; Numeric values. Given question: . . . What does the question ask about? Please identify the question into one of the following types: Abbreviation; Description and abstract concepts; Entities; Human beings; Locations; Numeric values. If you are unsure about your answer, please include other potential choices. Given question: . . . What does the question ask about? Please identify the question with all possible choices of the following types: Abbreviation; Description and abstract concepts; Entities; Human beings; Locations; Numeric values. 2024). Ensemble-based methods generate multiple answers by gathering outputs from different prompt designs, such as different prompt formats (Zhou et al., 2022; Yue et al., 2023; Zhang et al., 2024a) or different permutations of few-shot examples (Zhao et al., 2021; Lu et al., 2022; Lazaridou et al., 2022). Additionally, few approaches propose to directly prompt candidates, in the applications of model calibration (Tian et al., 2023; Xiong et al., 2024) and open-domain QA (Kim et al., 2024). However, sampling and ensemble-based methods rely on the randomness of LLMs, making them costly and inefficient in providing enough valuable annotations compared to prompting candidates. Moreover, this paper proposes novel aggregation strategy that leverages an SLM to distill high-quality annotations from the multiple labels provided by the LLM."
        },
        {
            "title": "3 Proposed Method",
            "content": "3.1 Preliminaries In this paper, we consider the task of text classification, where an unsupervised dataset = {xi}n i=1 with samples is provided. Given the label space = {1, . . . , C} with corresponding semantic meanings, each sample is associated with ground-truth label Y, which is inaccessible. In LLM-driven data annotation, an LLM serves as the annotator, providing labels for the unlabeled samples in D. Most existing methods prompt LLMs to provide Single Annotation (SA), i.e., specific label yi for each xi. 3.2 Prompt Candidate Annotations by LLM However, LLMs knowledge of downstream tasks remains limited (Li et al., 2024), making them uncertain about some samples during data annotation. In this case, prompting with single annotations may force the LLM to behave over-confidently and generate completely incorrect answers, which not only wastes computational resources but also harms downstream processes. To tackle this problem, we propose to prompt LLM with Candidate Annotations (CA), namely, set of multiple possible labels Y, = . Our motivation stems from human psychological phenomenon known as Ambiguity Aversion (Fox and Tversky, 1995; Maccheroni et al., 2006), where people tend to behave conservatively when facing uncertainty, which helps mitigate severe risks and ensures the lower bound of the gains. Prompting candidate annotations can inject ambiguity aversion into LLMs, which increases the likelihood of including correct labels in LLMs output, see examples in Figure 3. Specifically, we investigate two strategies for querying candidates: 1) CAadd prompts the LLM to generate one answer first and then provide additional answers if it is not sure; 2) CAall prompts the LLM to generate all possible answers. Table 1 shows the key prompts of different prompting strategies on the TREC dataset and the full prompts can be found in Appendix D. (cid:80)n CA Exhibits Better Statistical Properties. In this paragraph, we directly assess the value of candidate annotations. Regarding the annotation process as label space pruning, we employ the metrics introduced in (He et al., 2024a): 1) 1 αI[yi / si], measuring error, where α = 1 i=1 how the candidates include the correct labels; 2) Csi β-coverage, where β = 1 C1 , measuring how the answers shrink the original search space; 3) F1-score, which comprehensively considers both metrics, namely, F1 = 2(1α)β 1α+β . Figure 2 demonstrates the assessment results of 1 α-error and F1-score on three text classification tasks annotated by GPT-3.5, where both CAadd and (cid:80)n i=1 Figure 3: The overall framework of CanDist, which first prompts the LLM to provide candidate annotations, and then distills an SLM to identify the correct labels. Examples on the TREC dataset annotated by GPT-3.5 demonstrate that though the LLM fails to provide correct answer with single label, answering with candidate labels successfully includes the correct one. We also provide theoretical guarantees for our proposed CanDist framework. CAall improves the two metrics compared to SA. Notably, by prompting all possible labels, CAall outperforms SA by margins of 18.01%, 26.71%, 14.06% of 1 α-error on the three datasets, indicating the strong ability to include gold labels of prompting candidate annotations. The higher F1-scores further illustrate that while containing more correct labels, CA also effectively shrinks the search space, indicating its great value. The full assessment results are in Appendix B.1. 3.3 Distill Candidate Annotations by SLM Though candidate annotations demonstrate great potential, they cannot be directly applied to downstream tasks where specific labels are required. To address this, we propose teacher-student framework CanDist that trains an SLM student on the teacher LLMs candidate annotations, allowing the SLM to provide unique annotations. This is inspired by knowledge distillation (Hinton et al., 2015), where the student model is distilled from the teacher models output distribution and can better generalize to downstream tasks (Phuong and Lampert, 2019; Jeong and Chung, 2025). The overall framework of CanDist is shown in Figure 3. Nevertheless, with multiple false positive labels, training the SLM on the uniform distribution of candidate labels is suboptimal. Therefore, we propose Distribution Refinery (DR) strategy, which dynamically adjusts the target distribution based on the SLMs prediction. This is motivated by the memorization effect of deep neural networks (DNNs) (Zhang et al., 2017), where the SLM can first remember easy patterns, making proportion of true labels emerge from those false positive ones. Formally, the refined distribution qi for sample xi at each training iteration is computed as the renormalized prediction among candidate labels: qt ij = (cid:40)I(j si) 1 si , ij / (cid:80) I(j si) pt1 pt1 ik , ksi = 0 > 0 (1) where pt denotes the SLMs softmax output of sample xi at iteration t. qi is the distribution vector which is initialized from uniform distribution. Filter Out-of-Candidate Samples. Although candidate annotations are more likely to include the correct labels, there are still few samples whose true label lies outside the candidate set, which can disrupt SLM distillation. To this end, we filter out these samples by judging whether the SLMs max prediction lies beyond the candidate set: Dout = {xi arg max cY pic / si} (2) Distribution Sharpening for Reliable Samples. We further propose to select reliable samples in Din = Dout and sharpen their target distributions to guide the distillation process. To assess the reliability, we again leverage the memorization effect of DNNs where clean samples always pose small losses (Han et al., 2018). Specifically, we select small loss samples in class-wise manner to Algorithm 1 Pseudo-code of CanDist Input: Unlabeled dataset D, teacher LLM , and student SLM 1: Generate candidate annotations using by prompting strategy CAadd or CAall 2: for epoch = 1, 2, . . . , do 3: Filter out-of-candidate samples by Eq.(2) Select class-wise reliable samples by Eq.(3) Select high confidence samples by Eq.(4) for batch = 1, 2, . . . , do Compute pseudo-labels by Eq.(1) and (5) Calculate training loss Ldr by Eq.(5) Train by optimizing Ldr 4: 5: 6: 7: 8: 9: end for 10: 11: end for Output: Student SLM for annotation ensure balanced training progress across all classes. Formally, the reliable set is calculated as: sl, where Dc Dsl = cY sl = {xili Lc Dc δ, li = lce(pi, qi)} (3) and lce denotes the cross-entropy loss, and Lc δ denotes the top-δ percent smallest losses of samples whose max prediction is class c. For samples in Dsl, we use pre-defined temperature γ to sharpen their re-normalized distribution. Besides, we regard those samples in Dout that gradually pose high confidence as reliable samples: Dhc = {xi max cY pic > τ } Dout (4) where we use their predicted class as the training target. τ is pre-defined high threshold. Overall Distillation Object. The overall training objective of Distribution Refinery is formalized as: lack of theoretical understanding of LLMs, we simplify this problem by treating the LLM as traditional teacher model, focusing on whether the SLM can distill better results from candidate labels. While most existing knowledge distillation theories illustrate the advantages of distilling from the teachers output distribution (Phuong and Lampert, 2019; Das and Sanghavi, 2023), we analyze distilling from the teachers candidate annotations (top-k outputs), wherein the student SLM distilled from teacher LLMs candidate annotations demonstrate more noise-tolerant than the teacher LLM, as well as the SLM distilled from LLMs single annotations. Theorem 1 Considering the scenario that both the teacher LLM and student SLM are composed of feature extractor g() : (cid:55) Rd (with different scales) and classifier RdC. The teacher LLM is pre-trained on an inaccurate dataset = {xi, yi}m i=1 with noise rates {Rc,c}C,C 1, where denotes the number of samples in the dataset and Rc,c indicates the probability of label being flipped to c. After pretraining, the student SLM is then trained based on the teacher LLMs single (top-1) or candidate (top-2) annotations on D. Suppose the models are trained by l2-regularized cross-entropy loss with regularization parameter λ, and the feature extractors are fixed. Besides, we consider that the feature similarity between different samples from the same class and different classes are and respectively, with 1 > > > 0. c=1,c=1 Then, with , the condition of achieving 100% accuracy (correctly predicting all training data) for the teacher LLM, as well as the student SLM distilled from LLMs top-1 prediction is: Rc,c + (cid:88) i=c Rc,i < 1 θ ϕ θ , c, = where θ = 1 Cmλ Cmλ + 1 , Cmλ (6) Cmλ + (a b) + 1 and the condition of that for the student SLM distilled from LLMs top-2 prediction is: (cid:88) Rc,i < 1, c, = Rc,c + (7) Ldr = lce(pi, ˆqi), where (cid:88) i=1 1 ij / (cid:80) q1/γ qij, I(j = arg max cY q1/γ ic cY ˆqij = , xi Dsl xi Din Dsl ϕ = 1 pij), xi Dhc (5) Algorithm 1 shows the pseudo-code of CanDist."
        },
        {
            "title": "4 Theoretical Analysis",
            "content": "In this section, we further theoretically explain why prompting and then distilling candidate annotations leads to better results. Since there is still i=c 1Due to LLMs strong general capabilities, we assume that, for specific task, LLMs can consistently output label distribution that is relatively close to the true distribution . Under this assumption, LLMs appear to act like teacher pre-trained on dataset with distribution . Table 2: Comparisons of Accuracies (%) on the training and testing sets of different tasks. CanDistadd and CanDistall apply CAadd and CAall to prompt candidates respectively. The best results are bold and the second best is underlined. Method Training Set Testing Set TREC MA DBP AGN RCT BANK TREC MA DBP AGN RCT BANK 62.84 Zero-shot 71.07 Few-shot 71.88 CoT SC 71.06 AnnoLLM 73.73 76.05 SuperICL 76.04 Distillation 78.24 FreeAL 80.87 CanDistadd 79.73 CanDistall 62.03 62.28 60.05 62.29 59.71 62.81 62.45 62.89 63.31 63. 93.33 95.41 91.85 95.60 95.62 97.55 97.52 97.76 98.67 98.54 87.72 88.73 83.23 88.80 85.52 89.16 89.13 89.58 89.91 89.29 61.41 65.18 60.06 65.50 68.13 66.80 66.86 67.57 68.69 68.90 65.19 66.08 57.54 66.08 67.04 69.91 69.83 71.38 72.92 72.94 72.20 77.20 80.60 76.00 79.60 81.60 81.00 82.33 83.13 87.80 63.12 63.40 61.15 63.26 59.56 63.75 63.54 64.13 64.23 64. 93.94 95.40 92.44 95.42 95.34 97.63 97.61 97.92 98.72 98.65 87.24 88.05 83.05 87.96 85.39 88.79 88.29 88.64 89.46 88.78 61.83 65.85 60.43 65.85 68.53 67.82 67.66 68.32 69.77 70.57 68.41 68.86 60.97 68.99 70.29 73.25 72.40 74.58 76.27 75.97 SFT - - - - - - 97. 64.54 98.78 92.29 84.52 93.31 The proof is provided in Appendix C. The theorem illustrates that the SLM distilling top-2 predictions from the teacher LLM achieves 100% accuracy with more tolerant condition on label noise than using the top-1 prediction, which theoretically demonstrates the great potential of the paradigm that first generates candidates by the teacher LLM and then distilling them using student SLM."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we report our empirical results to show the superiority of CanDist. We refer the readers to the Appendix for more details and results. 5.1 Setup Datasets. We conduct experiments on the following six text classification datasets, namely, TREC (Li and Roth, 2002) for topic classification, Medical Abstract (MA) (Schopf et al., 2022) for medical diagnosis classification, DBpedia (DBP) for ontology classification (Zhang et al., 2015), AGNews (AGN) (Gulli, 2005) for news topic classification, RCT (Dernoncourt and Lee, 2017) for content type classification in medical abstracts, and Banking (BANK) (Casanueva et al., 2020) for intent classification in banking dialogues. Baselines. We adopt the following LLM-based or SLM-based baselines: Zero-shot and Few-shot (Liu et al., 2022) directly prompt for single annotations without/with few-shot examples; CoT (Kojima et al., 2022) employs chain-of-thought prompting by adding \"Lets think step by step\" before each answer; Self-Consistency (SC) (Wang et al., 2023) samples diverse reasoning paths and generates the answer by majority voting; AnnoLLM (He et al., 2024b) provides explanations for few-shot examples to boost performance; SuperICL (Xu et al., 2024) first trains an SLM using labeled data and uses its output and confidence as references during LLM annotation; Distillation distill an SLM from LLMs single annotation and use the SLM to provide the final annotation; FreeAL (Xiao et al., 2023) introduces robust training mechanism to improve generalization when distilling the SLM from single annotations, where we apply 1 round of annotation-distillation for fair comparison. Note that few-shot examples are applied to CanDist and all baselines except Zero-shot and CoT. Besides, for SuperICL, LLMs single annotations are leveraged to train the plug-in SLM. Performance Evaluation. We evaluate the annotation accuracy on both the training and testing set. For SLM-based methods (Distillation, FreeAL, and our method), the unlabeled training set is first annotated by the LLM, and then the SLM is trained on this training set to provide annotations. We also report the testing results of supervised fine-tuning (SFT) where the SLM is trained on the humanlabeled training dataset. For all experiments, we run three times and report the averaged results. Implementation Details. We exploit GPT-3.5 as the LLM annotator (see results of more advanced LLMs in Appendix B.2) and RoBERTa-Base (Liu et al., 2019) as the SLM for all tasks except MA, where BioMed-RoBERTa-Base (Gururangan et al., 2020) is used to boost performance for the medical Table 3: Comparison with selecting answers from candidates using LLM on the training sets. Results of single annotations (Few-shot) are also listed for the sake of comparison. Ablation TREC MA DBP AGN RCT BANK Avg. CanDistadd with LLM Select 80.87 72.87 (-8.00) 63.31 63.42 (+0.11) 98.67 96.38 (-2.29) 89.91 88.33 (-1.58) 68.69 63.17 (-5.52) 73.50 68.33 (-5.16) 79.16 75.42 (-3.74) CanDistall with LLM Select 79.73 70.95 (-8.78) 63.76 63.18 (-0.58) 98.54 96.30 (-2.24) 89.29 88.23 (-1.06) 68.90 63.67 (-5.23) 72.94 67.42 (-5.52) 78.86 74.96 (-3.90) Few-shot 71.07 62.28 95. 88.73 65.18 66.08 74.79 task. We set the number of few-shot examples as 10 for all tasks except 5 for MA due to limited context length. Since we cannot access labeled samples, the few-shot examples are LLM-generated (Xiao et al., 2023). For sampling-based baseline SC, we sample the decoding path 5 times with temperature of 0.5. For other LLM generation processes, the temperature is set to lower value of 0.3. More details of training SLM are in Appendix A.3. 5.2 Main Results The comparison results on the training and testing sets are shown in Table 2 where the best results are shown in bold and the second best is underlined. Overall, CanDist outperforms all baselines on all tasks. For example, on the testing set of TREC, CanDist improves the best baseline by large margin of 5.47%. Also, in the tasks of MA and DBpedia, CanDist achieves competitive testing performance on par with supervised fine-tuning. The superior results against all baselines imply the effectiveness of our proposed CanDist framework. Specifically, CanDist largely improves Zero-shot and Few-shot, where CanDistadd and CanDistall outperform Few-shot by averaged improvements of 5.48% and 6.10% on the testing set, and 7.03% and 6.63% on the training set. Though effective in reasoning tasks, CoT prompting performs poorly in most annotation tasks and self-consistency achieves similar results with Few-shot. AnnoLLM improves Few-shot in several tasks by providing explanations on input examples. However, these LLMbased methods underperform SLM-based methods, where SLM can distill the high-quality task-related knowledge from the LLMs annotation. Regarding the knowledge of SLM as reference, SuperICL slightly improves the performance of Distillation. FreeAL further improves Distillation through robust training objective that tackles label noise. For CanDist, we declare that there is trade-off between the number of candidates and the accuracy Table 4: Comparison with other candidate generation strategies on TREC, where 1 α-error, average number of labels (#Labels), and testing accuracy are reported Strategy 1 α #Labels Accuracy 5 sampled paths 10 sampled paths 20 sampled paths 40 sampled paths 5 example orders 5 prompt formats CanDistadd CanDistall 77.59 79.92 82.36 84.30 79.15 83.82 74.65 89.09 1.17 1.25 1.32 1.39 1.21 1. 1.07 1.70 81.40 81.73 82.27 82.33 81.27 82.67 83.13 87.80 since more candidates are more challenging to identify while fewer candidates contain fewer correct labels. Though CAall generally retrieves more labels than CAadd, we suppose that the performance of different prompting strategies depends on tasks, and both strategies achieve state-of-the-art results. 5.3 Analysis Comparison with Other Candidate Generation Strategies. To show the superiority of generating candidates by prompting, we compare the following two candidate generation strategies: 1) sampling-based strategy (Wang et al., 2023) samples = 5, 10, 20, 40 paths and gathers them into candidate set; 2) ensemble-based strategy gathers the answers from diverse prompting results, where we consider prompting with 5 few-shot example orders (Zhao et al., 2021) and 5 prompting formats (Gao et al., 2021). To evaluate the generated candidates, we report their 1 α-error, average number of labels, and the testing accuracy of SLM trained by our proposed Distribution Refinery objective. Table 4 demonstrates that by retrieving more candidate labels, CanDistall enjoys much higher 1 α-error than other methods and achieves the highest testing accuracy. Moreover, CanDistadd Table 5: Key prompt for selecting answers from candidate annotations on the TREC dataset. Prompt of selecting the answer from candidates Given question: . . . What does this question ask about? It is known that the answer belongs to one of the following classes: .... Please select the correct answer from them. Table 6: Ablation study on Distribution Refinery mechanism on the testing set of TREC and Banking. Ren. Out. Sha. Cla. Hig. TREC BANK 82.47 85.47 86.60 87.07 87.40 87.80 71.40 74.88 75.13 74.99 75.70 75.97 also outperforms the sampling and ensemble-based methods even if it retrieves fewer candidates, indicating that directly prompting candidates results in more valuable annotation. For the sampling-based method, though incorporating more sampled paths offers higher 1 α-error, the increment in testing accuracy remains limited. Besides, sampling and ensemble-based strategies suffer from more costs in querying LLMs while promoting candidates only need to prompt and sample once. Comparison with Selecting Answers from Candidates using LLM. To validate the effectiveness of our proposed teacher-student framework for identifying the correct label from candidate labels, we compare CanDist with its variant, CanDist with LLM Select, which directly queries LLM to select the correct label from the given candidate annotations. The key prompt for selecting the answer from candidates is shown in Table 5. As shown in Table 3, LLM selection suffers from performance drops compared with CanDist on most tasks, which demonstrates the superiority of our proposed teacher-student framework. Moreover, we found that CanDist with LLM Selection slightly outperforms single annotations (Few-shot), indicating that the paradigm of prompting candidates and then selecting from them is better than direct prompting for single label. Ablation Study on Distribution Refinery. To demonstrate the effectiveness of different components in DR, we run CanDistall with varying combinations of the components. We denote the comFigure 4: Comparison of 1 α on TRECs training set (left) and accuracy on the testing set (right) between different collaboration strategies with self-consistency. ponents in DR as 1) Ren. for the re-normalization function in Eq.(1); 2) Out. for filtering out-ofcandidate samples; 3) Sha. for whether employing distribution sharpening for reliable samples; 4) Cla. for whether select small loss samples in class-wise manner; 5) Hig. for whether using high confidence samples as reliable samples. As shown in Table 6, distilling from re-normalized distribution improves the vanilla version (trained on cross-entropy loss) by large margin, i.e., 3.00% for TREC and 3.70% for Banking. DR also helps by filtering out-of-candidate samples and sharpening the target distribution, where class-wise selection is essential for employing distribution sharpening, which balances the training progress across all classes. High-confidence label assignment further improves the performance by maximizing the utility of the out-of-candidate samples. j=1 Synergism with Self-Consistency. We further show that our vanilla method can work collaboratively with Self-Consistency (SC). Specifically, we first prompt LLMs with candidate labels and sample = 40 answers {sj}K j=1, and then calculate the frequency for each class by (cid:80)K I(c sj) to filter the top-k frequent labels as candidate annotations. We name this strategy as SC-k and we also define SC-All as using all the appeared labels as candidate labels. As shown in Figure 4, the comparison on 1 α-error illustrates that collaborating with SC further increases the diversity of candidate labels which includes more correct labels, and this also yields higher accuracy for the final annotation, as shown on the right. Further discussion on SC-1 can be found in Appendix B.3."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we study LLM-driven data annotation by proposing novel teacher-student framework, CanDist, which first prompts the teacher LLM to generate candidate labels and then distill student SLM to identify the true labels. We illustrate that candidate annotations exhibit better statistical properties and theoretically justify that distilling from LLMs candidate annotations is more noisetolerant. Empirically, we show that CanDist outperforms various LLM and SLM-based methods. We hope our work will inspire future research to exploit candidate annotations with weak annotators."
        },
        {
            "title": "Limitations",
            "content": "Despite the effectiveness of our proposed CanDist framework for data annotation, there is still much potential for further improvement. On the one hand, as the Distribution Refinery mechanism is specifically designed for classification, the application of CanDist is currently limited to text classification tasks, and we aim to explore its potential in text generation tasks in our future works. On the other hand, the derivation of our proposed theory is based on the assumption that the LLM is traditional encoder model, which is not the case for the prevailing decoder-only LLMs. Besides, there is still lack of theoretical understanding of LLMs in the community and we hope that this field will further develop in the near future."
        },
        {
            "title": "Ethical Considerations",
            "content": "While the datasets used in our paper are all publicly available and are widely adopted by researchers, utilizing LLMs for data annotation and generating few-shot examples may include bias and unfairness. Allowing LLMs to output multiple annotations may further amplify such issues, although we did not observe such phenomena in our experiments. Nevertheless, if CanDist is used with such biased annotations, it may unpleasantly yield unfair and biased predictions based on characteristics like race, gender, disabilities, LGBTQ, or political orientation. To alleviate this issue, we recommend that potential users first use bias reduction and correction techniques to remove biased text and predictions so as to improve overall fairness and ethical standards."
        },
        {
            "title": "Acknowledgments",
            "content": "Haobo Wang is supported by the NSFC under Grants (No. 62402424) and (No. U24A201401)."
        },
        {
            "title": "References",
            "content": "Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Iñigo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. 2020. Efficient intent detection with dual sentence encoders. CoRR, abs/2003.04807. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Universal self-consistency for large language model generation. CoRR, abs/2311.17311. Juhwan Choi, Eunju Lee, Kyohoon Jin, and YoungBin Kim. 2024. Gpts are multilingual annotators for sequence generation tasks. In Findings of the Association for Computational Linguistics: EACL 2024, St. Julians, Malta, March 17-22, 2024, pages 1740. Association for Computational Linguistics. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. Rudrajit Das and Sujay Sanghavi. 2023. Understanding self-distillation in the presence of label noise. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 71027140. PMLR. Franck Dernoncourt and Ji Young Lee. 2017. Pubmed 200k RCT: dataset for sequential sentence classification in medical abstracts. In Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 - December 1, 2017, Volume 2: Short Papers, pages 308313. Asian Federation of Natural Language Processing. Bosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken Chia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023. In Proceedings Is GPT-3 good data annotator? of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1117311195. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Craig Fox and Amos Tversky. 1995. Ambiguity aversion and comparative ignorance. The quarterly journal of economics, 110(3):585603. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 38163830. Association for Computational Linguistics. Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms crowd-workers for textannotation tasks. CoRR, abs/2303.15056. Antonio Gulli. 2005. The anatomy of news search In Proceedings of the 14th international engine. conference on World Wide Web, WWW 2005, Chiba, Japan, May 10-14, 2005 - Special interest tracks and posters, pages 880881. ACM. Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Dont stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 83428360. Association for Computational Linguistics. Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. 2018. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 85368546. Shuo He, Chaojie Wang, Guowu Yang, and Lei Feng. 2024a. Candidate label set pruning: data-centric perspective for deep partial-label learning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. 2024b. Annollm: Making large language models to be better crowdsourced annotators. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 165190. Association for Computational Linguistics. Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in neural network. CoRR, abs/1503.02531. Alex Holub, Pietro Perona, and Michael C. Burl. 2008. Entropy-based active learning for object recognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops 2008, Anchorage, AK, USA, 23-28 June, 2008, pages 18. IEEE Computer Society. Hyeonsu Jeong and Hye Won Chung. 2025. Rethinking self-distillation: Label averaging and enhanced soft label refinement with partial labels. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin. 2024. Sure: Summarizing retrievals using answer candidates for open-domain QA of llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura E. Barnes, and Donald E. Brown. 2019. Text classification algorithms: survey. Inf., 10(4):150. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot prompting for open-domain question answering. CoRR, abs/2203.05115. Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2022a. survey on deep learning for named enIEEE Trans. Knowl. Data Eng., tity recognition. 34(1):5070. Jiyi Li. 2024. Human-llm hybrid text answer aggreIn Proceedings of gation for crowd annotations. the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1560915622. Association for Computational Linguistics. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. The dawn after the dark: An empirical study on factuality hallucination in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1087910899. Association for Computational Linguistics. Xin Li and Dan Roth. 2002. Learning question classifiers. In 19th International Conference on Computational Linguistics, COLING 2002, Howard International House and Academia Sinica, Taipei, Taiwan, August 24 - September 1, 2002. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022b. Making large language models better reasoners with stepaware verifier. arXiv preprint arXiv:2206.02336. Chaoqun Liu, Qin Chao, Wenxuan Zhang, Xiaobao Wu, Boyang Li, Anh Tuan Luu, and Lidong Bing. 2024. Zero-to-strong generalization: Eliciting strong capabilities of large language models iteratively without gold labels. CoRR, abs/2409.12425. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, pages 100114. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692. Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On llmsdriven synthetic data generation, curation, and evalIn Findings of the Association uation: survey. for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1106511082. Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086 8098. Association for Computational Linguistics. Fabio Maccheroni, Massimo Marinacci, and Aldo Rustichini. 2006. Ambiguity aversion, robustness, and the variational representation of preferences. Econometrica, 74(6):14471498. Katerina Margatina, Giorgos Vernikos, Loïc Barrault, and Nikolaos Aletras. 2021. Active learning by acquiring contrastive examples. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 650663. Association for Computational Linguistics. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL 2005, 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 25-30 June 2005, University of Michigan, USA, pages 115124. The Association for Computer Linguistics. Mary Phuong and Christoph Lampert. 2019. Towards understanding knowledge distillation. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 51425151. PMLR. Tim Schopf, Daniel Braun, and Florian Matthes. 2022. Evaluating unsupervised text classification: Zeroshot and similarity-based approaches. In Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval, NLPIR 2022, Bangkok, Thailand, December 16-18, 2022, pages 615. ACM. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics. Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate & rank: multi-task framework for math word probIn Findings of the Association for Compulems. tational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 16-20 November, 2021, pages 22692279. Association for Computational Linguistics. Richard Shin, Christopher H. Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 76997715. Association for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, meeting of SIGDAT, Special Interest Group of the ACL, pages 16311642. ACL. Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large language models for data annotation and synthesis: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 930957. Association for Computational Linguistics. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Agüera Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications. CoRR, abs/2201.08239. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 54335442. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Mayur Wankhade, Annavarapu Chandra Sekhara Rao, and Chaitanya Kulkarni. 2022. survey on sentiment analysis methods, applications, and challenges. Artif. Intell. Rev., 55(7):57315780. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. 2023. Large language models are better reasoners In Findings of the Associawith self-verification. tion for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 25502575. Association for Computational Linguistics. Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, and Haobo Wang. 2023. Freeal: Towards human-free active learning in the era of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1452014535. Association for Computational Linguistics. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2024. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian J. McAuley. 2024. Small models are valuable plug-ins for large language models. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 283294. Association for Computational Linguistics. Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, and Yue Zhang. 2024. Supervised knowledge makes large language In The Twelfth models better in-context learners. International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Junjie Ye, Nuo Xu, Yikun Wang, Jie Zhou, Qi Zhang, Tao Gui, and Xuanjing Huang. 2024. LLM-DA: data augmentation via large language models for few-shot named entity recognition. CoRR, abs/2402.14568. Bo Yuan, Yulin Chen, Yin Zhang, and Wei Jiang. 2024. Hide and seek in noise labels: Noise-robust collaborative active learning with llms-powered assistance. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1097711011. Association for Computational Linguistics. Murong Yue, Jie Zhao, Min Zhang, Liang Du, and Ziyu Yao. 2023. Large language model cascades with mixture of thoughts representations for cost-efficient reasoning. CoRR, abs/2310.03094. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. 2017. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, and Weiming Lu. 2024a. Self-contrast: Better reflection through inconsistent solving perspectives. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 36023622. Association for Computational Linguistics. Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 649657. Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024b. Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 19461965. Association for Computational Linguistics. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 1269712706. PMLR. Chunting Zhou, Junxian He, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2022. Prompt consistency for zero-shot task generalization. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 26132626. Association for Computational Linguistics. Dawei Zhu, Michael A. Hedderich, Fangzhou Zhai, David Ifeoluwa Adelani, and Dietrich Klakow. 2022. Is BERT robust to label noise? study on learning with noisy labels in text classification. In Proceedings of the Third Workshop on Insights from Negative Results in NLP, Insights@ACL 2022, Dublin, Ireland, May 26, 2022, pages 6267. Association for Computational Linguistics."
        },
        {
            "title": "A Additional Experimental Setup",
            "content": "A.1 Statistics of Datasets Table 7: Statistics of the used datasets. #Class denotes the number of classes. #Train and #Test indicate the size of the training and testing set. Dataset Task #Class #Train #Test Topic cls TREC MA Medical cls DBpedia Ontology cls AGNews RCT Banking Topic cls Content cls Intent cls 6 5 14 4 5 77 5,452 11,550 10,000 10,000 10,000 9,003 500 2,888 70,000 7,600 30,135 3, Table 7 shows the statistics of datasets used in our experiments. Given the extensive size of the original training sets for DBpedia, AGNews, and Table 8: Comparisons on 1 α-error, average number of labels (#La.), and F1-score between different prompts. Method SA CAadd CAall TREC MA BANK AGN RCT DBP 1 α #La. 1 α #La. F1 1 α #La. F1 1 α #La. 1 α #La. F1 1 α #La. F1 71.07 74.65 89.09 1.00 1.07 1. 83.1 85.0 87.5 62.28 79.06 88.99 1.00 1.56 1.95 76.8 82.4 82.1 66.08 76.99 80.14 1.00 1.74 2. 79.6 86.6 88.5 88.73 94.47 97.19 1.00 1.30 1.70 94.0 92.2 85.7 65.18 75.18 79.15 1.00 1.56 1. 78.92 80.26 79.51 95.41 98.59 99.25 1.00 1.37 1.75 97.7 97.9 96.7 RCT, we randomly selected 10,000 examples from each as their respective training sets. Note that the most competitive baseline, FreeAL, primarily evaluates binary classification datasets, which are easier to annotate and do not need to apply candidate annotations, whereas we conduct experiments on more challenging tasks. A.2 More Details of SLM Distillation During SLM distillation, we incorporate consistency regularization and mixup training to boost performance following FreeAL. Consistency regularization encourages the model to produce similar outputs for different augmented views of the same sample. Specifically, we adopt back-translation (Sennrich et al., 2016) to augment each sample xi into xaug . Then, for samples in Din and Dout, the consistency regularization are formulated as: the few-shot examples for each unlabeled sample are retrieved based on embedding similarity with the bert-base-uncased model. For SLM distillation, we use Nvidia RTX A5000 GPU to train the model for 50 epochs with AdamW optimizer with learning rate selected from {3e 5, 1e 5, 3e 6} and weight decay of 0.01. The batch size is fixed as 32 with maximum sequence length of 128. We warm up the model by training on the re-normalized distribution for few epochs to achieve high-quality selection in the Distribution Refinery mechanism. For hyperparameters, the small loss ratio δ is selected from {0.4, 0.5, 0.6}. The sharpen parameter γ is fixed as 0.85 and the high confidence threshold is selected from {0.95, 0.99, 1.0}. Note that we employ the default validation set for each dataset for parameter selection. The loss weight parameter η is linearly ramped up from 0 to 1 to avoid overfitting false labels at the start. (cid:88) lce(paug , ˆqi) Lin cr = 1 Din Lout cr = 1 Dout xiDin (cid:88) (8) lkl(paug , pi)"
        },
        {
            "title": "B Additional Experimental Results",
            "content": "xiDout B.1 Full Assessment Results where lkl denotes the KL-divergence. For mixup training, we create virtual training samples by linearly interpolating both: g(xm) = ω g(xi) + (1 ω) g(xj) ˆqm = ω ˆqi + (1 ω) ˆqj (9) where g(xi) is the embedding of xi. ω Beta(ς, ς) where ς is simply set as 4. The mixup loss Lmix is then defined by the cross-entropy loss between the SLMs prediction on g(xm) and ym. The total loss for SLM distillation is aggregated as: In this section, we demonstrate the assessment results of single annotations and candidate annotations on all tasks (training sets), where we use the average number of labels (#La.) to represent βcoverage since it is more intuitive to understand. As shown in Table 8, CAadd and CAall improve 1 α-error on all datasets with average number of labels no more than two. Candidate annotations also achieve higher F1-scores on all tasks except for AGNews. These results statistically demonstrate that candidate annotations are more likely to include the correct labels and offer great potential. Ltotal = Ldr + η (Lin cr + Lout cr + Lmix) (10) B.2 Results of Different LLMs A.3 More Implementation Details In our main experiments, we use the gpt-3.5-turbo0125 version for the LLM API. For generating fewshot examples, we follow the setting in FreeAL which first queries the LLM to generate an example pool of size 100 with corresponding labels. Then, In this section, we evaluate the annotation results using two other LLMs: Llama 3.1 (Llama-3.1-8BInstruct) and GPT-4o. As shown in Table 9 and 10, Llama 3.1 achieves results at the same level as GPT3.5, and using the more advanced GPT-4o boosts the performance of all data annotation methods. Table 9: Assessment results of different prompting strategies on TREC using Llama 3.1 and GPT-4o. Method Llama 3.1 GPT-4o 1 α #La. F1 1 α #La. SA CAadd CAall 68.80 85.34 89.56 1.00 1.87 2.06 81.52 83.98 83.80 87.53 94.42 96.28 1.00 1.20 1. 93.35 95.17 93.63 Table 10: Comparisons on the training set and testing set of TREC using Llama 3.1 and GPT-4o. Method Few-shot FreeAL CanDistadd CanDistall Llama 3.1 GPT-4o Train 68.80 76.60 76.99 77.66 Test 77.00 82.67 83.40 85.60 Train 87.53 89.14 89.53 90. Test 87.60 93.80 95.60 96.40 Still, CanDist improves GPT-4os single annotations (Few-shot) by large margin of 8.80% and outperforms the most competitive baseline FreeAL by margin of 2.60% on the testing set. B.3 Synergism with Self-Consistency (cid:80)K Following the setting in paragraph 5.3, we further show that the collaboration of prompting candidates and majority voting (i.e. SC-1) also brings great potential by outperforming voting on single annotations. Specifically, after sampling = 40 candidate annotations, we use majority voting to obtain the final annotation: ˆy = I(c sj). Figure 5 demonarg maxc strates the comparison results on the training set of TREC and Banking, where we found that voting on candidate annotations results in higher performance than voting on single annotations. Notably, as the number of sampled paths increases, the accuracy of voting on candidates grows more significantly, especially from 1 to 5. This further indicates the great value of prompting candidate annotations. j=1 B.4 Comparison of Different ICL Strategies for Prompting Candidates In this section, we further investigate how the design of in-context learning (ICL) examples for prompting candidate annotations affects the results of CanDist. Note that we employ Self-generated (Single) for our method following FreeAL, which leverages sample-single label pairs generated by LLM as ICL examples. We further explore the Figure 5: Comparison of different prompting strategies for self-consistency shows the synergism between prompting candidates with self-consistency. Table 11: Comparison of different ICL strategies for prompting candidate annotations. Example Type TREC BANK Zero-shot Self-generated (Single) Self-generated (Candidate) Supervised 87.00 87.80 89.60 90.47 68.47 75.97 74.71 76.04 effect of two other types of ICL examples: Selfgenerated (Candidate) which leverages samplecandidate label pairs generated by LLM as examples; Supervised adopt human-labeled training data as examples. For both methods, we first gather an example pool of size 100 and retrieve ICL examples for each unlabeled sample based on embedding similarity with the bert-base-uncased model. As shown in Table 11, CanDist using self-generated examples outperforms zero-shot CanDist, and using supervised ICL can make further improvements. Besides, CanDist using examples with selfgenerated single labels outperforms the one with candidate labels on Banking but underperforms it on TREC. This suggests that whether to use single labels or candidate labels as ICL examples depends on the specific task and we simply adopt the former, which achieves state-of-the-art results. B.5 Comparison with Traditional Active Learning Methods To compare the effectiveness of CanDist with human annotation, we further evaluate some active learning (AL) baselines, including 1) AL-Random, which acquires to-be-labeled data randomly; 2) ALEntropy (Holub et al., 2008), which is the most commonly used uncertainty-based method that acquires samples with highest predictive entropy; 3) AL-CAL (Margatina et al., 2021) is recent active learning method that acquires contrastive examples. We also report Supervised Fine-tuning which acTable 12, which demonstrates CanDist is in the same magnitude as FreeAL. Proof of Theorem 1 In this section, we provide the proof of Theorem 1, which illustrates that the SLM distilled from the LLMs candidate annotations enjoys better theoretical guarantees than the LLM as well as the SLM distilled from the LLMs single annotations. Theorem 1 Considering the scenario that both the teacher LLM and student SLM are composed of feature extractor g() : (cid:55) Rd (with different scales) and classifier RdC. The teacher LLM is pre-trained on an inaccurate dataset = i=1 with noise rates {Rc,c}C,C {xi, yi}m c=1,c=1, where denotes the number of samples in the dataset and Rc,c indicates the probability of label being flipped to c. After pre-training, the student SLM is then trained based on the teacher LLMs single (top-1) or candidate (top-2) annotations on D. Suppose the models are trained by l2-regularized crossentropy loss with regularization parameter λ, and the feature extractors are fixed. Besides, we consider that the feature similarity between different samples from the same class and different classes are and respectively, with 1 > > > 0. Then, with , the condition of achieving 100% accuracy (correctly predicting all training data) for the teacher LLM as well as the student SLM distilled from LLMs top-1 prediction is: Rc,c + (cid:88) i=c Rc,i < 1 θ ϕ θ , c, = where θ = 1 Cmλ Cmλ + 1 , Cmλ (11) ϕ = 1 Cmλ + (a b) + 1 and the condition of that for the student SLM distilled from LLMs top-2 prediction is: Rc,c + (cid:88) i=c Rc,i < 1, c, = (12) Proof. Closed-form Solutions of Models Prediction. Denote the training objective of the models as: L(W ) = 1 (cid:88) i=1 lce(pi, qi) + λW 2 (13) Figure 6: Comparison between active learning methods and CanDist on TREC where CanDistall is applied. Table 12: Running time (in seconds) of one SLM training epoch of baseline FreeAL and CanDist. Method TREC MA DBP AGN RCT BANK FreeAL CanDist 80.2 79.1 172.7 174.0 148.5 149.4 147.8 148.1 146.2 146.5 131.7 132. quires annotation for the whole training set and CanDist-hybrid which incorporates randomly acquired human annotations into CanDist. For all methods, we first train the SLM on the annotated training set and evaluate its testing accuracy. Figure 6 demonstrates the comparison results under different annotation budgets on the TREC datasets. Firstly, CanDist, without human annotation, outperforms most traditional AL baselines under 10% human annotations. Also, incorporating merely 20% human annotations, CanDisthybrid achieves comparable performance with AL baselines under 50% human annotations. Furthermore, CanDist-hybrid with 50% human annotations achieves competitive performance on par with supervised fine-tuning. These results yield the superiority of our proposed CanDist framework. Besides, though FreeAL shows that LLM-driven active learning surpasses traditional active learning and achieves competitive results with supervised fine-tuning on the SST-2 (Socher et al., 2013) and MR (Pang and Lee, 2005) datasets, we show that on harder task, LLM-driven active learning still requires small proportion of human annotations to achieve near-supervised performance. B.6 Time Complexity Analysis To analyze the time complexity of the SLM distillation process in our proposed CanDist, we compare the empirical running time (in seconds) of SLM distillation in CanDist and the baseline FreeAL in where pi = softmax(W g(xi)) is the models prediction distribution and qi denotes the training target. When pre-training the teacher LLM, qi = e(yi) where e(y) denotes the one-hot form of specific label y; When distilling the student SLM from teacher LLMs top-1 prediction, qi is onehot vector where the value on the max prediction index equals 1 and otherwise 0; When distilling the student SLM from teacher LLMs top-2 prediction, qi is vector where the value on the top-2 prediction index equals 0.5 and otherwise 0. dL(W ) The optimal classifier satisfies the condition of dW = 1 i=1 g(xi)(pi qi) + λW = 0. Thus, the optimal classifier can be formalized as: (cid:80)m = 1 mλ (cid:88) (qi pi)g(xi) (14) i=1 To derive the relation between the training target qi and models prediction pi, we define ai = qi pi and derive as follows: ai = qi pi = qi softmax(W g(xi)) = qi softmax( = qi softmax( 1 mλ 1 mλ (cid:88) ajg(xj)g(xi)) j=1 (cid:88) g(xi), g(xj)aj) j=1 Due to the non-linearity of the softmax function, directly solving ai is challenging. To this end, we employ linear approximation of the softmax function following (Hinton et al., 2015): softmax(v)i = (cid:80)C exp(vi) j=1 exp(vj) 1 + vi + (cid:80)C j=1 vj (15) 1 + vi Note that this linear approximation, originally introduced by Hinton et al. (2015), is based on applying softmax with high temperature > 0, i.e., softmax(v/T ). Therefore, when = 1, the approximation in Eq.(15) becomes valid when the logits are of sufficiently small magnitude. By applying the above approximation, we have: ai = qi 1 1C 1 Cmλ (cid:88) j=1 g(xi), g(xj)aj (16) where 1C C-dimensional all-ones vector. Denoting = [a1, . . . , am] RCm, = [q1, . . . , qm] RCm, and Rmm with Si,j = g(xi), g(xj), Eq.(16) can be expressed as: = 1 1Cm 1 Cmλ AS (17) With the definition of and the symmetry of S, and denote = [p1, . . . , pm] RCm as the output matrix, the relation between the training target and the models prediction can be derived as: = (cid:18) 1Cm (cid:19) Im + = 1 1 Cmλ 1 Cmλ 1 AS; 1Cm; (cid:18) = 1 (cid:19) (cid:18) 1Cm Im + (cid:19)1 1 Cmλ (cid:18) 1 (cid:19) (cid:18) 1Cm (cid:19) 1Cm = 1 (cid:18) 1 (cid:19) (cid:18) 1Cm Im + (cid:19)1 1 Cmλ ; ; 1 (cid:32) (cid:18) 1Cm = (cid:19) 1Cm 1 (cid:18) Im Im + (cid:19)1(cid:33) 1 Cmλ (18) where Im is an m-dimensional identity matrix. To further simplify the above expression, we apply eigen-decomposition for the similarity matrix as = ΛV 1 with eigenvalue-eigenvector pairs {λi, vi}m i=1. Then, by applying Woodburys matrix identity, Eq.(18) can be simplified as: 1 (cid:32) (cid:18) 1Cm = (cid:19) 1Cm 1 (cid:18) Im Im + (cid:19)1(cid:33) 1 Cmλ ΛV (cid:18) = (cid:19) 1Cm 1 V (cid:0)CmλΛ1 + Im (cid:1) 1 (19) Quantification of the Similarity Matrix. In the following derivations, we further simplify the closed-form solution of through the quantification of the similarity matrix S. Specifically, we assume that the feature similarity of different samples depends on classes, i.e.: Si,j = 1, a, b, = = j, yi = yj yi = yj , where < < 1 (20) Denote the class-wise similarity matrix RCC with Zi,j = when = and Zi,j = when = j, and let = [e(y1), . . . , e(ym)] RCm be the ground-truth label matrix, the similarity matrix can be expressed as: = ZY + (1 a)Im = (b1CC + (a b) IC) + (1 a)Im (21) Lemma 1 Suppose the symmetric matrix Rnn is composed of the sum of rank-m (m < n) matrix and multiple of the identity matrices: = ΞU + λIn where = [u1, . . . , um] Rnm is an orthonormal matrix satisfying = Im. Ξ = diag(ξ1, . . . , ξm) Rmm containing the eigenvalues ξi. Then, has the following two types of eigenvalue-eigenvector pairs {σi, vi}n i=1: 1) eigenvalues that are shifts of the original eigenvalues from the rank-m matrix: σi = ξi + λ, = 1, . . . , with corresponding eigenvectors vi = ui. 2) (n m) eigenvalues from the identity matrix: σi = λ, = + 1, . . . , with corresponding eigenvectors orthogonal to the columns of . Proof. The eigenvalue equation is given by: (U ΞU + λIn)v = σv Decompose into components + v, where is in the column space of and is orthogonal to the column space of , and we have = β and = 0 for some β Rm. Then, multiplying on both sides of the eigenvalue equation yields: ΞU + λU = σU v; ΞU (v + v) + λU (v + v) = σU (v + v); (Ξ + λU )β = σβ; (Ξ + λI)β = σβ which indicates σi = ξi + λ for = 1, . . . , with corresponding eigenvectors given by vi = ui. The remaining eigenvalues arise from λI, with eigenvectors orthogonal to the columns of . With Lemma 1, we can reformulate in Eq.(21). For = b1CC + (a b) IC, it has two types of eigenvalue-eigenvector pairs {σi, ui}C i=1: 1) one pair with eigenvalue: σ1 = Cb + (a b) and eigenvector u1 = 1 1C; 2) 1 pairs with eigenvalues: σi = b, = 2, . . . , and the corresponding eigenvectors ui. Denoting Σ = diag(σ1, . . . , σC) and = [u1, . . . , uC] RmC, thus: = ZY + (1 a)Im = ΣU + (1 a) Im (cid:114) = U (cid:17) Σ (cid:16) (cid:32)(cid:114) Y (cid:33) (22) + (1 a) Im where we assume (cid:80)m j=1 Yi,j = m/C, namely, the dataset is balanced. Again, by applying Lemma 1, has three types of eigenvalue-eigenvector pairs {λi, vi}m i=1: 1) one pair with eigenvalue: = mb + σ1 + (1 a) λ1 = (a b) + (1 a) and eigenvector v1 = 1C; 2) 1 pairs with eigenvalues for = 2, . . . , C: u1 = 1 λi = = C σi + (1 a) (a b) + (1 a) and the eigenvectors vi = (cid:113) ui; 3) pairs with eigenvalues: λi = (1 a), = + 1, . . . , and the corresponding eigenvectors vi. Denoting = (cid:0)CmλΛ1 + Im (cid:1) 1 in Eq.(19), and denoting θ, ϕ, ψ according to the following equations: θ = 1 ϕ = 1 ψ = 1 Cmλ Cmλ + 1 Cmλ Cmλ + (a b) + 1 Cmλ (a b) + 1 Cmλ + mb + (cid:113) v1v 1 + (cid:88) i=2 λi Cmλ + λi viv viv ϕC (cid:88) i=2 uiu we have: (cid:88) = i=1 λi Cmλ + λi viv = λ1 Cmλ + λ1 (cid:88) + i=C+1 λi Cmλ + λi = ψC u1u 1 + (cid:88) + θ viv i=C+1 1CCY = ψ + ϕC (cid:32) (cid:18) IC (cid:19) 1CC 1 + θ Im (cid:88) uiu (cid:33) = ψ ϕ 1mm + i=1 (ϕ θ)C Y + θIm (23) Finally, the models prediction pi is quantified as: (cid:19) (cid:18) pi = 1Cm 1 :,i + 1 1C = θqi + (ϕ θ) + (ψ ϕ) 1 (cid:88) j=1 qj (cid:88) j:yi=yj qj + (1 ψ) 1 1C = θqi + (ϕ θ) (cid:88) j:yj =yi qj + (1 ϕ) 1C 1 (24) where we assume the target is also balanced which indicates 1 j=1 qj = 1C. (cid:80)m Condition for Achieving Correct Prediction. Recall that the teacher model is trained on an inaccurate dataset = {xi, yi}m i=1 with noise rates {Rc,c}C,C c=1,c=1, and we have qi = e(yi) when training the teacher model. Then, when , the second term in Eq.(24) can be expressed as yi,:, which yields: qj = j:yj =yi (cid:80) pi = θe(yi) + (ϕ θ)R yi,: + (1 ϕ) 1C (25) Then, we aim to find the conditions for the prediction pi to have the maximum value at the true label position yi, indicating correct prediction. On the one hand, if sample xi is clean, i.e., yi = yi: [pi]c = (cid:40) θ + (ϕ θ)Ryi,yi, (ϕ θ)Ryi,c, = yi = yi (26) where the condition for arg maxc [pi]c = yi is Rc,c > Rc,c θ ϕθ , c, = c; On the other hand, if sample xi is noisy, i.e., yi = yi: [pi]c = (ϕ θ)Ryi,yi, θ + (ϕ θ)Ryi,yi, (ϕ θ)Ryi,c, = yi = yi = yi, yi (27) c=1,c=1 for still satisfies Rq where the condition is Rc,c > Rc,c + θ ϕθ , c, = c. Overall, since we have ϕ > θ, the most stringent condition for correct prediction of the teacher LLM is Rc,c > Rc,c + θ ϕθ , c, = c. Note that if Rc,c < Rc,c + θ ϕθ for some and = c, the teacher models top-1 prediction on those samples with yi = and yi = remains noisy, which indicates that when distilling from the teacher models top-1 prediction Q, the noise rates {Rq}C,C c,c < Rq c,c + θ = c. To this end, the condition for achieving correct prediction for the student SLM distilled from the teacher LLMs top-1 prediction coincides with the condition of the teacher LLM, i.e., Rc,c > Rc,c + θ ϕθ , c, = c. In the following paragraph, we justify when Rc,c > Rc,c, c, = c, the student SLM distilled from the teacher LLMs top-2 prediction can achieve correct prediction. With Eq.(27), we have when Rc,c > Rc,c, c, = c, the teacher models top-2 prediction always includes the true label yi. Denote yi as: ϕθ for those and yi = arg maxc=yi Ryi,c the training target qi for distilling the teacher models top-2 prediction can be expressed as: qi = (cid:40) 2 e(yi) + 1 2 e(yi) + 1 1 2 e(yi), xi is clean 2 e(yi), xi is noisy (28) Then, with the balance assumption, the second term in Eq.(24) is given as: (cid:88) qj = j:yj =yi 1 2 + e(yi) + 1 2 Ryi,yie(yi) 1 2 (cid:88) c=yi Ryi,ce(c) (29) Thus, if sample xi is clean then [pi]c = θ θ ϕθ 2 + 1ϕ , (Ryi,yi + Ryi,yi) + 1ϕ , 2 + ϕθ 2 + ϕθ 2 Ryi,c + 1ϕ , = yi; = yi; = yi, yi. (30) where obliviously the max prediction is yi since (cid:80)C c=1C Ryi,c = 1. Then, if sample xi is noisy and yi = yi, [pi]c =: θ θ 2 + 1ϕ , 2 + ϕθ 2 + ϕθ 2 Ryi,c + 1ϕ , ϕθ 2 (Ryi,yi + Ryi,yi) + 1ϕ , = yi; = yi; = yi, yi. (31) and when yi = yi, [pi]c =: θ θ ϕθ 2 ϕθ 2 + ϕθ 2 + ϕθ 2 + 1ϕ , 2 Ryi,yi + 1ϕ , (Ryi,yi + Ryi,yi) + 1ϕ , 2 Ryi,c + 1ϕ , = yi; = yi; = yi. = yi, yi, yi. (32) Eq.(31) and (32) also yield yi as the max prediction of pi, which indicates the student SLM distilled from the teacher LLMs top-2 prediction achieves accurate predictions. To sum up, the condition of achieving accurate prediction, i.e., achieving 100% accuracy for either the pre-trained teacher LLM or the SLM distilled from the teacher LLMs top-1 prediction is: Rc,c > Rc,c + θ ϕ θ , c, = (33) and the condition of achieving 100% accuracy for the SLM distilled from the teacher LLMs top-2 prediction is: Rc,c > Rc,c, c, = (34) Since Rc,c reflects the clean probability, we replace Rc,c in Eq.(33) and (34) by 1 (cid:80) i=c Rc,i that reflects the noise rates, which directly yields the conclusion in Eq.(11) and (12). These illustrate that the SLM distilled from LLMs top-2 prediction achieves 100% accuracy with more tolerant condition on label noise, providing the theoretical foundation of our proposed CanDist framework."
        },
        {
            "title": "D Full Prompt Design",
            "content": "The full prompt designs of single annotations and candidate annotations are listed in Table 13. Table 13: Full prompts of prompting single (SA) and candidate (CAadd and CAall) annotations on the TREC dataset. Strategy Prompt SA CAadd CAall You are helpful assistant for the task of question classification on the TREC (The Text REtrieval Conference Question Classification) dataset. You reply with brief, to-the-point answers with no elaboration as truthfully as possible. TREC dataset contains 5452 questions, each question is identified as one of the 6 types with respect to what it asks for: DESC; ENTY; ABBR; HUM; LOC; NUM, which stand for Abbreviation; Description and abstract concepts; Entities; Human beings; Locations; Numeric values, respectively. Each of these 6 classes contains non-overlapping set of fine-grained sub-classes as follows: ABBR (Abbreviation): [Abbreviation and Expression abbreviated], DESC (Description and abstract concepts): [Definition of something. Description of something. Manner of an action and Reason.], ENTY (Entities): [Animal. Organ of body; Color; Invention, book and other creative piece; Currency name; Disease and medicine; Event; Food; Musical instrument; Language; Letter like a-z; Other entity; Plant; Product; Religion; Sport; Element and substance. Symbols and sign. Techniques and method. Equivalent term. Vehicle. Word with special property.], HUM (Human beings): [Group or organization of persons; Individual; Title of person; Description of person], LOC (Locations): [City; Country; Mountain; Other location. State], NUM (Numeric values): [Postcode or other code; Number of something; Date; Distance, linear measure; Price; Order, rank; Other number; Lasting time of something; Percent, fraction; Speed; Temperature; Size, area and volume; Weight]. Your task is to classify the the given question as one of the 6 given coarse classes (ABBR, DESC, ENTY, HUM, LOC and NUM) based on what is asked and type of the answer. Given question: . . . What does this question ask about? Please identify the question into one of the six mentioned types. You are helpful assistant for the task of question classification on the TREC (The Text REtrieval Conference Question Classification) dataset. You reply with brief, to-the-point answers with no elaboration as truthfully as possible. TREC dataset contains 5452 questions, each question is identified as one of the 6 types with respect to what it asks for: DESC; ENTY; ABBR; HUM; LOC; NUM, which stand for Abbreviation; Description and abstract concepts; Entities; Human beings; Locations; Numeric values, respectively. Each of these 6 classes contains non-overlapping set of fine-grained sub-classes as follows: ABBR (Abbreviation): [Abbreviation and Expression abbreviated], DESC (Description and abstract concepts): [Definition of something. Description of something. Manner of an action and Reason.], ENTY (Entities): [Animal. Organ of body; Color; Invention, book and other creative piece; Currency name; Disease and medicine; Event; Food; Musical instrument; Language; Letter like a-z; Other entity; Plant; Product; Religion; Sport; Element and substance. Symbols and sign. Techniques and method. Equivalent term. Vehicle. Word with special property.], HUM (Human beings): [Group or organization of persons; Individual; Title of person; Description of person], LOC (Locations): [City; Country; Mountain; Other location. State], NUM (Numeric values): [Postcode or other code; Number of something; Date; Distance, linear measure; Price; Order, rank; Other number; Lasting time of something; Percent, fraction; Speed; Temperature; Size, area and volume; Weight]. Your task is to classify the the given question as one of the 6 given coarse classes (ABBR, DESC, ENTY, HUM, LOC and NUM) based on what is asked and type of the answer. Given question: . . . What does the question ask about? Please identify the question into one of the six mentioned types. If you are unsure about your answer, please include other potential choices. You are helpful assistant for the task of question classification on the TREC (The Text REtrieval Conference Question Classification) dataset. You reply with brief, to-the-point answers with no elaboration as truthfully as possible. TREC dataset contains 5452 questions, each question is identified as one of the 6 types with respect to what it asks for: DESC; ENTY; ABBR; HUM; LOC; NUM, which stand for Abbreviation; Description and abstract concepts; Entities; Human beings; Locations; Numeric values, respectively. Each of these 6 classes contains non-overlapping set of fine-grained sub-classes as follows: ABBR (Abbreviation): [Abbreviation and Expression abbreviated], DESC (Description and abstract concepts): [Definition of something. Description of something. Manner of an action and Reason.], ENTY (Entities): [Animal. Organ of body; Color; Invention, book and other creative piece; Currency name; Disease and medicine; Event; Food; Musical instrument; Language; Letter like a-z; Other entity; Plant; Product; Religion; Sport; Element and substance. Symbols and sign. Techniques and method. Equivalent term. Vehicle. Word with special property.], HUM (Human beings): [Group or organization of persons; Individual; Title of person; Description of person], LOC (Locations): [City; Country; Mountain; Other location. State], NUM (Numeric values): [Postcode or other code; Number of something; Date; Distance, linear measure; Price; Order, rank; Other number; Lasting time of something; Percent, fraction; Speed; Temperature; Size, area and volume; Weight]. Your task is to classify the the given question as one of the 6 given coarse classes (ABBR, DESC, ENTY, HUM, LOC and NUM) based on what is asked and type of the answer. Given question: . . . What does the question ask about? Please identify the question with all possible choices of the six mentioned types."
        }
    ],
    "affiliations": [
        "NetEase Fuxi AI Lab",
        "University of Wisconsin Madison",
        "William & Mary",
        "Zhejiang University"
    ]
}