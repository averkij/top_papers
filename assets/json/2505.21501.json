{
    "paper_title": "Vision Transformers with Self-Distilled Registers",
    "authors": [
        "Yinjie Chen",
        "Zipeng Yan",
        "Chong Zhou",
        "Bo Dai",
        "Andrew F. Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly \"absorb\" the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 0 5 1 2 . 5 0 5 2 : r Vision Transformers with Self-Distilled Registers Yinjie Chen*1 Zipeng Yan*2 Chong Zhou3 Bo Dai2 Andrew F. Luo2 1 Zhejiang University 2 University of Hong Kong 3 Nanyang Technological University * Equal contribution Co-First authors chen.yinjie@zju.edu.cn, kelvinyzp@gmail.com Corresponding author: aluo@hku.hk"
        },
        {
            "title": "Abstract",
            "content": "Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with the local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is to the addition of register tokens to ViTs, which implicitly absorb the artifact term during training. Given the availability of various large-scale pre-trained ViTs, in this paper we aim at equipping them with such register tokens without the need of re-training them from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (PH-Reg), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teachers inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing. Our code is publicly available at this repository."
        },
        {
            "title": "Introduction",
            "content": "Vision Transformers (ViTs) are now the dominant architecture in visual modeling, delivering strong performance across classification, detection, and segmentation. Unlike convolutional networks with their built-in locality inductive bias, ViTs process images by spatially splitting them into patches and applying self-attention to enable global feature interactions. This architectural design leads to superior scalability, particularly with contrastive or self-supervised pre-training objectives, and facilitates more flexible representation learning, as it is less constrained by the translation invariance assumptions inherent in CNNs. This flexibility enables remarkable emergent capabilities. Models like CLIP, trained solely on image-text alignment, achieve competitive open-vocabulary segmentation through zero-shot dense queries; while self-supervised approaches learn semantically rich features directly from unlabeled images. However, the same data-driven attention mechanisms that enable ViTs representation power can also lead to the emergence of artifact tokens. These are outlier features often discordant with local image semantics, meaning they fail to correspond to locally meaningful image structures. The propensity for ViTs to generate such tokens is exacerbated by their lack of strong, built-in spatial priors, which can result in inconsistent dense representations. Ultimately, the presence of these artifact tokens Preprint. Under review. Figure 1: Effect of PH-Reg on open-vocabulary segmentation. For each image, we compare four methods: MaskCLIP which directly takes the value features from the last attention layer; SCLIP which adds correlative self-attention; NACLIP which further enforces locality bias; and our PH-Reg method with self-distilled registers. We utilize the same OpenAI CLIP ViT-B/16 weights for all three methods. For each method, we visualize the UMAP of the dense features and heatmap of one text query. Our method yields noticeably cleaner dense features and high quality localizations, and requires only small set of additional register parameters compared to the original network. disrupts fine-grained localization, critical capability for tasks demanding high spatial precision, such as detailed semantic segmentation or part identification. Recent work has sought to mitigate artifact tokens via architectural modifications, where register tokens are added to the network. These register tokens are randomly initialized, with learnable parameters that participate in the self-attention process similar to the [CLS] token, but are not otherwise used during the output. Although these register tokens are not explicitly supervised during training, they effectively absorb the artifact term and learn to attend to global objects. While effective, introducing register tokens constitutes fundamental architectural modification that requires training from scratcha time-consuming and computationally demanding process. This significantly limits their applicability, especially given the vast ecosystem of existing, high-performing pre-trained vision models. We present solution to this issue with Post Hoc Registers (PH-Reg), an efficient self-distillation framework that requires no labeled data or full retraining. We illustrate OpenAI CLIP with PH-Reg in Figure 1. In PH-Reg, both teacher and student networks are initialized from the same pre-trained model weights. And the only extra parameters are the register tokens added to the student network. Our proposed framework freezes the teacher during training. Images provided to the teacher undergo test-time augmentation (e.g. random offsets and horizontal flips). This augmentation strategy effectively denoises the teachers dense features without requiring gradient-based updates on the teacher itself, yielding stable dense targets. The denoised dense features are used as distillation target for the student network, where only small set of parameters are optimized. This entire process requires only modest set of unlabeled images, enabling significant enhancements to pretrained models with minimal computational overhead. Concretely our contributions are as follows: 1. We propose test-time augmentation scheme that can effectively denoise dense features in vision transformers. Our denoiser does not require costly neural fields and does not require gradient based optimization. 2. We elucidate the underlying components in student model that contribute to learning clean dense feature map. We show that by finetuning select weights, we are able to achieve clean dense features with minimal additional parameters. 3. We demonstrate that PH-Reg effectively improves the consistency of dense feature representations in ViTs, leading to quantifiable improvements on downstream tasks that rely on fine-grained spatial understanding (e.g., semantic segmentation or depth prediction). Our method preserves the original utility of dense features without inducing unwanted distribution shift, and functions well with zero-shot language-based dense queries. 2 Figure 2: Learning framework of PH-Reg. (a) Our framework begins by creating two networks from the same set of weights. In the teacher, the weights are frozen and unmodified. In the student, the only additional parameters are learnable register tokens. The teacher creates learning target using denoised representations. (b) An image undergoes augmentation by function with random augmentation parameters consisting of random offsets and horizontal flips.(c) Given an RGB image, we utilize UMAP to visualize the features, and heatmap using CLIP text query. Our method can produce significantly cleaner dense representations with minimal additional inference cost."
        },
        {
            "title": "2 Related work",
            "content": "Transformers in Visual Learning. Building upon the success of self-attention in language modeling, architectures that leverage transformer based token-mixing have been proposed for visual generation [1, 2, 3] and recognition tasks [4, 5], cumulating in the ViT architecture which relies on very few locality biases [6]. In the years since, many improvements and variants have been proposed [7, 8, 9, 10, 11, 12]. The improvements have largely focused on data [13] and compute efficiency [14, 15, 16, 17, 18, 19, 20, 21, 22]. In general, vision transformers tokenize an image into set of patches, where each patch is first processed using an MLP or convolution block [23, 24, 25, 26], the patches are further processed with self-attention which enables global token interactions beyond those in convolutional networks. As self-attention is permutation invariant, positional information is typically injected using learnable positional embeddings or relative positions [27, 28, 29, 16, 30, 31, 32]. Positional embeddings have been suggested to play role in the emergence of dense ViT artifacts, as networks with positional embeddings removed have smooth feature maps [33]. Representation Learning with Vision Transformers. The lack of restrictive local inductive biases in Vision Transformers enables strong scaling behavior across diverse set of tasks. Beyond traditional supervised learning on categorical datasets such as ImageNet, methods have been proposed to learn on large scale datasets by leveraging language contrastive objectives [34, 35], or selfsupervised image-level objectives [36, 37, 38] and patch-level objectives [39, 40, 41, 42, 43, 44, 45, 46]. While the training objectives are very different, these methods enable strong zero-shot and linear-probe performance across diverse set of tasks, suggesting that these methods effectively learn the underlying statistics of visual input. While these methods lack explicit dense supervision, the dense features from these models have been shown to have strong zero-shot emergent behavior with language-based segmentation [47], object part correspondence [48], and structural understanding [49]. Artifacts in Vision Transformer Dense Features. Recent work on DINOv2 [46] has found that Vision Transformers can have artifacts in their dense features. It has been proposed that artifacts can be mitigated with register tokens [50, 51]. These register tokens are effectively randomly initialized embeddings that are analogous to the [CLS] token. While registers participate in the self-attention process, they are discarded during output. This approach requires model to be trained from scratch. The nature and the mechanisms that cause the emergence of artifact tokens are unclear, and there exists conflicting results on what information (global or no information) these artifact tokens contain [50, 52]. Recent work has further investigated the mechanism of register 3 Figure 3: Denoising teacher representations with augmentations. For each model, we visualize the UMAP of dense features before and after applying test-time augmentation. The results show that our proposed method produces noticeably cleaner dense feature representations without requiring gradient-based learning. Please zoom in for details. tokens [53]. Our own results have found that artifact tokens are not necessarily high-norm, and can be low-norm as well. Unlike the observation by [33], we find that positional embeddings alone cannot account fully for the artifacts. Regardless of why artifact tokens emerge, removing these artifacts is an active area of research, with proposals based on registers [50], magnitude smoothness priors [52], and the foundational work on leveraging neural fields to denoise ViTs with static artifact component [33, 54]. concurrent line of work has sought to remove artifacts for open-vocabulary segmentation with training-free attention modifications [47, 55, 56, 57, 58, 59]. Our framework can be applied to existing pretrained networks, introduces minimal additional parameters, can be applied to tasks beyond open-vocabulary segmentation, and makes no assumptions on the magnitude or static nature of the artifacts."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we will describe the PH-Reg framework, which we illustrate in Figure 2. This framework enables existing pretrained ViTs to benefit from register tokens, yielding significantly cleaner dense representations. During training, PH-Reg requires only unlabeled images for the self-distillation process. In section 3.1, we will first describe the denoising process of teacher network outputs. Unlike prior work that rely on neural field/hash-grid, this method denoises dense features without the use of expensive gradient-based learning. In section 3.2, we will describe how we initialize and modify the student architecture. This approach only introduces small set of additional parameters to the network. Finally in section 3.3 we will describe our distillation process. 3.1 Efficient denoising of teacher representations , Our denoising process starts from the observation that artifact tokens are not static relative to image content. Put another way, if an image is shifted by certain amount (with the gaps padded with whitespace), the artifacts do not shift by the same amount. As shown in Figure 2, given an RGB image RHW 3, we randomly sample random augmentation parameters (θ1, θ2, ..., θn), where each θi defines horizontal/vertical offset (xi, yi) and boolean flipi {0, 1} defined horizontally. For each image, we also compute the image space coordinate grid = (u-coords, v-coords). Where (u, v) are respectively in range [0, 1]: defines the left-right axis, and defines the top-down axis. The coordinates help us keep track of the original location of an image region after augmentation. In practice, as we are working with ViT model with patch size k, where the tokenization process yields ( ) grid of image tokens, we define our parameters using offsets that are integer multiples of to facilitate efficient indexing. Together, the image I, the coordinates C, and augmentation parameter θi are provided to transform function to yield an augmented image Ii and new coordinates Ci: (I, C, θi) (Ii, Ci). teacher model fteacher is based on frozen set of original network weights, without any additional parameters. Given an augmented image Ii, this network outputs feature Fi. We restore the features to the original location within an image using the inverse transform function 1(Fi, Ci). The restored features are additively accumulated across different augmentation parameters, while keeping track of the number of occurrences for each location. At the end, the dimension-wise sample mean is computed for the accumulated features. The patch-wise expected value of this representation is the same as the optimal value when optimizing discrete grid of representations to minimize mean squared error (as used in DVT [33] and traditional neural field based methods). However, as we do not require gradients, this denoising process can be done in less than 200ms, roughly two magnitudes 4 faster than neural field based denoising in DVT. The comparison between raw and denoised dense feature visualizations is shown in Figure 3. Figure 4: Visualization of Open Vocabulary Segmentation. We compare against MaskCLIP, SCLIP, NACLIP, and find that our method yields clean feature maps free of artifacts. 3.2 Design of the student network Our objective is to preserve maximimal computational efficiency of the student model, while leveraging the knowledge of the pre-trained weights. For this purpose we introduce number of register tokens, providing minimally invasive enhancement to the base architecture. After the addition of register tokens, total of + 1 + tokens participate in the self-attention process. Unlike prior work that trains registers from scratch [50], this approach updates only these registers and selectively unfreezes specific components during distillation, preserving the majority of the ViTs pretrained weights. Through ablation studies, we identify optimal unfreezing strategies, such as adjusting convolution layers, positional embeddings, or the last transformer block. 3.3 Learning and Optimization of the Student We employ multi-objective distillation strategy, combining cosine similarity and mean squared error losses to ensure both directional and magnitude alignment between teacher and student representations. Our final loss is: Losstotal = 1 cossim(target, predicted) + MSE(target, predicted)."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we comprehensively evaluate the performance of PH-Reg on diverse set of dense tasks, first using zero-shot setup for open-vocabulary segmentation in section 4.1, followed by linear probe based segmentation and depth tasks in section 4.2. Finally we perform ablation studies to explore design decisions and investigate the nature of artifacts across different models in section 4.3. All implementation details are provided in the appendix. 4.1 Open-vocabulary Semantic Segmentation Evaluation Datasets. In this section, we follow prior works [56, 59, 58] to evaluate our approach on six semantic segmentation datasets, with their names abbreviated (in parentheses) to conserve table space: PASCAL VOC 2012 (VOC21) [60], PASCAL Context (PC 60) [61], COCO-Object (Object) [62], COCOStuff (Stuff) [63], Cityscape (City) [64], ADE20K-150 (ADE) [65]. In addition to these standard benchmarks, we also evaluate on two commonly used variants, PASCAL VOC 2012 (VOC20) and PASCAL Context (PC 59), in which the background class is excluded from the evaluation. For all experiments, we utilize the same evaluation harness for all methods, and apply sliding window inference strategy for non-square images. We also resize input images such that the shorter side is fixed to specific resolutions, accommodating the varying original image sizes across datasets. Baselines. We compare our method against several relevant approaches in open-vocabulary semantic segmentation, including MaskCLIP [47], SCLIP [56], ClearCLIP [58], and NACLIP [59]. We also include vanilla CLIP as baseline in our comparison, as it can be adapted for semantic segmentation. 5 Table 1: Open-vocabulary semantic segmentation quantitative evaluation results on 8 benchmarks. While the first 3 benchmarks (VOC21, PC60 and Obejct) include background class, the remaining benchmarks do not. We report the mean Intersection over Union (mIoU, %) metric, where higher values indicate better performance, for our method and all baseline models. The best result for each dataset is highlighted in bolded. Additional results in supplemental. Method CLIP [34] MaskCLIP [47] SCLIP [56] ClearCLIP [58] NACLIP [59] MaskCLIP + DVT [47, 33] NACLIP + DVT [59, 33] Ours (PH-Reg) VOC21 18.60 49.27 59.62 59.76 58.88 44.29 60.25 63.01 PC60 Object VOC20 49.05 6.50 7.84 66.56 26.94 25.46 81.53 33.52 31.74 84.56 32.77 32.56 79.70 33.15 32.20 65.88 20.89 25.08 80.26 32.89 32.73 35.27 34.52 83.05 PC59 11.17 28.62 34.46 35.91 35.16 29.50 35.91 37.88 Stuff 7.19 18.8 22.65 23.89 23.30 17.10 23.41 24.66 City 6.65 28.33 32.34 30.04 35.48 30.89 36.31 37.17 ADE 3.16 13.70 16.45 16.65 17.42 14.06 17.54 19. Avg. 13.77 32.21 40.08 39.52 39.41 30.96 39.91 41.85 Unless otherwise specified, all visual encoders use the widely adopted pretrained ViT backbone with the same OpenAI CLIP ViT-B/16 weights to ensure fair comparison. We also include denoised versions of MaskCLIP and NACLIP produced by DVT, as DVT represents closely related method to our approach. Unless otherwise noted, for CLIP we adopt the most basic MaskCLIP framework (direct output without any attention modifications) as our student model. Notably, we re-implemented all baselines using the same prompt templates as in [56, 59]. All reported results are obtained without any post-processing refinement. Quantitative Results. Table 1 summarizes the quantitative comparison results of various openvocabulary semantic segmentation models. We observe that PH-Reg CLIP consistently outperforms all compared methods on 7 out of 8 evaluated benchmarks, with particularly strong results in VOC21 (63.01%) and COCO Object (35.27%). Moreover, PH-Reg CLIP surpasses the denoised versions of MaskCLIP and NACLIP, where DVT fails to yield significant performance gains. We believe this is caused by the residual estimator in DVT, which assumes stationary artifacts an assumption that does not hold consistently for training-free open vocabulary segmentation methods based CLIP. We note that ClearCLIP slightly outperforms our method on the VOC20 dataset. This may be attributed to its use of correlative self-attention, i.e. q-q attention, which incorporates feature localization cues. This explanation is plausible, as SCLIP, which also employs q-q attention, similarly outperforms NACLIP on the VOC20 dataset. Qualitative results. Figure 4 presents qualitative comparison between our PH-Reg CLIP and three baseline models: MaskCLIP, SCLIP, and NACLIP. We visualize the UMAP of the dense features produced by each model, as well as the corresponding heatmaps generated from different text queries. Our qualitative observations are as follows: 1. Artifact tokens are frequently observed in the UMAP visualizations of MaskCLIP, SCLIP, and NACLIP. While some artifact tokens are reduced in the heatmap of MaskCLIP, they remain prevalent in the heatmaps of both SCLIP and NACLIP. 2. The presence of artifact tokens hinders the models ability to maintain fine-grained spatial alignment with the text queries, leading to suboptimal localization. 3. In contrast, PH-Reg CLIP consistently produces cleaner UMAPs and more fine-grained, semantically aligned heatmaps, which correspond well to meaningful local image structures. These visualizations demonstrate that our method effectively preserves the consistency of dense feature representations and enhances semantic alignment between visual and textual modalities. 4.2 Linear Probe Based Segmentation & Depth Evaluation Datasets & Baselines. In this section, we evaluate our approach in two semantic segmentation datasets: PASCAL VOC 2012 (VOC21) [60] and ADE20K-150 (ADE) [65] and one monocular depth estimation dataset: NYUv2-Depth dataset (NYUd) [66]. Since DVT [33] targets similar objective with our approach, we include both the vanilla models and their denoised versions produced by DVT as comparison baselines. We adopt the same linear probe experimental setup as in [33], and train linear layer integrated into the backbones, as decode head to predict pixel-wise segmentation or depth logits from patch tokens. Table 2 summarizes the main experiment results. Semantic segmentation results. As shown in Table 2, We observe significant and consistent improvements, outperforming in 5 out of 6 denoised ViT backbones across the evaluated datasets. 6 Table 2: Linear probing evaluation results on segmentation and depth. PH-Reg improves pretrained ViT backbones across various dense prediction tasks. For semantic segmentation, we report the mean Intersection over Union (mIoU, %) metric and mean accuracy (mAcc, %). For monocular depth estimation, we report Root Mean Squared Error (RMSE), absolute relative error (Abs Rel), and accuracy under threshold δ1. The best result for each dataset is highlighted in bold. Method CLIP [34] CLIP + DVT [34, 33] NACLIP [59] NACLIP + DVT [59, 33] MaskCLIP [47] MaskCLIP + DVT [47, 33] Ours (PH-Reg) OpenCLIP [67] OpenCLIP + DVT [67, 33] OpenCLIP + Ours DFN-CLIP [68] DFN-CLIP + DVT [68, 33] DFN-CLIP + Ours DINOv2 [46] DINOv2 + DVT [46, 33] DINOv2 + Ours VOC21 ADE NYUd mIoU() mAcc() mIoU() mAcc() RMSE() Abs Rel() 73.88 74.74 74.01 74.47 70.28 71.38 75.32 71.31 71.03 73.25 71.98 70.47 72.97 84.13 85.43 84.85 83.37 84.33 83.16 82.98 79.06 80.49 84.96 80.64 81.02 83.99 82.07 81.34 82.48 92.00 93.37 92.46 35.78 36.39 37.06 36.91 34.43 34.43 38.07 37.68 36.69 39.32 36.81 35.77 39.15 47.82 48.86 48.66 47.3 48.14 48.33 48.56 44.74 44.86 49.58 49.8 48.73 51.24 47.83 46.45 50.61 60.5 61.61 61. 0.6843 0.6800 0.6852 0.6845 0.6645 0.6792 0.6746 0.6853 0.6811 0.6784 0.6860 0.6852 0.6768 0.4566 0.4329 0.4306 0.2115 0.2089 0.2082 0.2122 0.2030 0.2091 0.1995 0.2113 0.2159 0.2019 0.2118 0.2092 0.2052 0.1391 0.1289 0.1216 δ1() 64.93 65.07 64.52 65.11 67.71 64.96 68.17 64.86 64.73 65.32 64.50 64.65 65.26 82.92 85.23 86.35 While DVT consistently enhances the performance of DINOv2 and vanilla CLIP, it provides only limited improvements for other ViT backbones derived from other CLIP models. In contrast, our approach yields substantial performance boosts across these backbones, especially notable +5.04% mIoU on VOC21 and +3.64% mIoU on ADE20k. These results demonstrate that our method can be robustly adopted to enhance the performance of diverse ViT backbones in semantic segmentation. Notably, DVT relies on neural fields and requires gradient-based optimization, making the iterative denoising process applied to each image individually highly time-consuming. Our method leverages test-time augmentation for denoising, enabling the generation of cleaner dense feature representations without incurring excessive computational overhead. However, our results also show that the residual estimator as introduced in DVT may be beneficial to some model types (DINOv2) moreso than others. These results highlight that PH-Reg achieves superior performance in suppressing artifact tokens through more robust and efficient design. Depth estimation results. Following prior work [46, 33] we adopt AdaBins [69] for monocular depth evaluation. As shown in Table 2, our method consistently improves the performance of pretrained ViT backbones whereas the DVT assumption of stationary artifacts mostly hold true for DINOv2. Additionally, DVT achieves performance gains using an additional transformer block with 0.08 the parameters of the base models [33], our method achieves superior results with only negligible increase in parameter count introduced by the register tokens. These results demonstrate the efficiency of our approach, yielding noticeable performance gains with minimal model overhead. 4.3 Ablation Studies and Investigating Artifacts In this section, we conduct an ablation study on OpenAIs CLIP ViT-B/16 to investigate various architectural and training components, focusing on both model performance and training feasibility. Number of register tokens. We evaluate the influence of the number of register tokens on the cosine similarity between the student models outputs and the target values using the COCO Caption dataset. Specifically, we distill the student model with 0, 1, 2, 4, 8, or 16 register tokens. As illustrated in Figure 5a, the cosine similarity increases as the number of register tokens grows, indicating improved alignment with the target representations. However, the performance gain becomes marginal when increasing the number of registers from 4 to 8 and from 8 to 16. Based on this observation, we use 16 register tokens in all subsequent experiments. Distillation architectural settings. We further evaluate the impact of architectural configurations during distillation by analyzing cosine similarity on the COCO Caption dataset. In this setting, 7 (a) Registers behavior. This plot illustrates adding registers improve PH-Reg teacher performance. In the blue settings, only registers are unfreezed. The green settings represent the improvements when positional embeddings are unlocked additionally. The red settings represent performance of unlocking more layers. (b) Augmentations improves cosine similarity. The plot illustrates how increasing the number of augmentations improves the alignment of the models predictions with the target of 200 augmentations features, as measured by cosine similarity. Figure 5: Ablation on number of registers and augmentations Figure 6: Comparison of original and PH-Reg features and norms. While prior work has noted artifact tokens in DINOv2 as having higher norm than other tokens, we observe this is not the case for all models. Some models have artifact tokens with lower magnitude. we vary the number of register tokens from 0 to 16 while allowing the positional embeddings to be updated during training. As shown in Figure 5a, the improvement in the cosine similarity from unlocking the position embedding becomes less pronounced as the number of register tokens increases. Nonetheless, unlocking positional embeddings continues to provide positive effect on alignment. This result suggests that in contrast to DVT, the positional embedding itself is unlikely to fully explain the artifact tokens. Next, we fix the number of register tokens to 16 and evaluate the effect of unlocking additional layers, including the convolutional patch embedding layer and the later attention layers. For all experiments, we report 50th, 70th, 90th, 95th, and 99th percentiles (of cosine similarity to capture the distribution of the most dissimilar features.) Our analysis reveals that incorporating even single register leads to substantial improvements. In particular, the 99th percentile of feature cosine similarity in the 1-register configuration exceeds the 50th percentile (median) of the raw case without 8 Figure 7: Patch Norms. This figure illustrate norms of patch tokens of different backbones. Our method effectively reduces the variance of token norms and reduces the outliers, regardless if the artifacts are lower/higher norm. registers. This indicates that registers significantly enhance the quality of feature representations across the distribution, not only in extreme cases. As suggested by [52, 33], attention layers close to the output also play an important role, which we confirm in our experiments. As shown in Figure 5a, unlocking the last attention layer significantly increases the cosine similarity between the student models outputs and the target values. While unlocking the convolutional patch embedding layer alone slightly reduces cosine similarity value, the overall value improves when both the convolutional patch embedding and later attention layers are unlocked, compared to the baseline with only unlocked the position embeddings and later attention layers with 16 registers. Therefore, we unlock the positional embeddings, the convolutional patch embedding layer, and the final attention layer during distillation. The number of augmentations. We evaluate our approach using cosine similarity on the COCO Caption dataset to investigate the effect of the number of augmentations. The student model is distilled with 1 to 10 augmentations, where one of them is always an identical input. As shown in Figure 5b, high convergence threshold is observed at the 99th percentile where even the most dissimilar cases exhibit cosine similarity values above 0.95, indicating substantial reduction in feature space outliers. As result, we employ 10 augmentations to generate high-quality features efficiently, thereby reducing computational overhead. Evaluation of Artifacts and Registers. While prior work has noted that artifact tokens are high magnitude in DINOv2 [50], in Figure 6 we find that this is not always the case. In OpenAIs CLIP and OpenCLIP, the artifacts are generally lower norm than their surrounding patches. In contrast, in DFN-CLIP and DINOv2, the artifacts are higher norm. This illustrates that there may be elements of the training dynamic at play, as the artifact norms can differ even when the training objective is very similar. In Figure 7 we visualize the norms of the original network and those with registers added, we find that our method effectively reduces the variance of the patch norms."
        },
        {
            "title": "5 Discussion",
            "content": "Limitations and Future Work In this work we proposed PH-Reg, method to reduce the artifact tokens in existing pre-trained vision transformers. We show that PH-Reg can eliminate artifact tokens in ViTs effectively and generate clean dense feature maps, enhancing the performance in downstream dense prediction tasks. This approach relies on test time augmentation to denoise dense feature presentations in the teacher model. While our method generally outperforms DVT on CLIP based models, we sometimes underperform when using the DINOv2 backbone. We believe this is due to the static artifact estimator present in DVT. The assumption of static artifacts holds true for some models (DINOv2), but not for others (CLIP). potential avenue for additional investigation is how to dynamically determine the artifacts without strong stationary assumptions. Conclusion We introduce novel post-training method PH-Reg, for learning clean dense feature representations in ViTs through an efficient self-distillation framework that does not require additional labeled data. Our approach leverages test-time augmentation to denoise the teacher model, and guide the student model to optimize the dense feature representations. This enables us to eliminate artifact tokens effectively by integrating learnable registers into existing pretrained models, without the need for training from scratch. We demonstrate that the distilled ViTs generate fine-grained dense feature maps, enhancing the consistency of feature representations in ViTs. We further show that cleaner dense feature maps in ViTs leads to quantifiable improvements on dense prediction tasks. Finally, we 9 illustrate that the distilled ViTs can accurately capture meaningful semantic structures in images, as shown by heatmaps generated from CLIP text queries. We validate our conclusions with extensive evaluations across multiple dense prediction benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pages 40554064. PMLR, 2018. [2] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. [4] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-attention in vision models. Advances in neural information processing systems, 32, 2019. [5] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-attention and convolutional layers. arXiv preprint arXiv:1911.03584, 2019. [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [7] Tobias Christian Nauen, Sebastian Palacio, Federico Raue, and Andreas Dengel. Which transformer to favor: comparative analysis of efficiency in vision transformers. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 69556966. IEEE, 2025. [8] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local self-attention for parameter efficient visual backbones. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1289412904, 2021. [9] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. Advances in neural information processing systems, 34: 2426124272, 2021. [10] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint arXiv:2102.08602, 2021. [11] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention for transformer models. In International conference on machine learning, pages 1018310192. PMLR, 2021. [12] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou. Going deeper with image transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3242, 2021. [13] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In International Conference on Machine Learning, volume 139, pages 1034710357, July 2021. [14] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In International conference on machine learning, pages 94389447. PMLR, 2020. [15] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [16] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 11 [17] Alaaeldin Ali, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, et al. Xcit: Crosscovariance image transformers. Advances in neural information processing systems, 34:20014 20027, 2021. [18] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:5368, 2021. [19] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 1110611115, 2021. [20] Hongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. In Proceedings of the IEEE/CVF A-vit: Adaptive tokens for efficient vision transformer. conference on computer vision and pattern recognition, pages 1080910818, 2022. [21] Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo, and Tao Mei. Wave-vit: Unifying wavelet and transformers for visual representation learning. In European conference on computer vision, pages 328345. Springer, 2022. [22] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not all patches are what you need: Expediting vision transformers via token reorganizations. arXiv preprint arXiv:2202.07800, 2022. [23] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2231, 2021. [24] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision, pages 558567, 2021. [25] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian. Visformer: The vision-friendly transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 589598, 2021. [26] Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multiscale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, pages 357366, 2021. [27] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243 1252. PMLR, 2017. [28] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [29] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: visual representation for neon genesis. Image and Vision Computing, 149:105171, 2024. [30] Zeyu Lu, Zidong Wang, Di Huang, Chengyue Wu, Xihui Liu, Wanli Ouyang, and Lei Bai. Fit: Flexible vision transformer for diffusion model. arXiv preprint arXiv:2402.12376, 2024. [31] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. [32] Dahun Kim, Anelia Angelova, and Weicheng Kuo. Region-aware pretraining for openvocabulary object detection with vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1114411154, 2023. 12 [33] Jiawei Yang, Katie Luo, Jiefeng Li, Congyue Deng, Leonidas Guibas, Dilip Krishnan, Kilian Weinberger, Yonglong Tian, and Yue Wang. Denoising vision transformers. In European Conference on Computer Vision, pages 453469. Springer, 2024. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. [35] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id= Ee277P3AYC. [36] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. [37] Xinlei Chen*, Saining Xie*, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021. [38] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Efficient self-supervised vision transformers for representation learning. International Conference on Learning Representations (ICLR), 2022. [39] Trieu H. Trinh, Minh-Thang Luong, and Quoc V. Le. Selfie: Self-supervised pretraining for image embedding, 2019. URL https://arxiv.org/abs/1906.02940. [40] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 16911703. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/chen20s.html. [41] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: simple framework for masked image modeling. In International Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [42] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv:2111.06377, 2021. [43] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. International Conference on Learning Representations (ICLR), 2022. [44] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEit: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=p-BhZSz59o4. [45] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with jointembedding predictive architecture. arXiv preprint arXiv:2301.08243, 2023. [46] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [47] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European Conference on Computer Vision, pages 696712. Springer, 2022. [48] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. ECCVW What is Motion For?, 2022. 13 [49] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1249012500, June 2024. [50] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. [51] Mikhail Burtsev, Yuri Kuratov, Anton Peganov, and Grigory Sapunov. Memory transformer. arXiv preprint arXiv:2006.11527, 2020. [52] Haoqi Wang, Tong Zhang, and Mathieu Salzmann. Sinder: Repairing the singular defects of dinov2. In European Conference on Computer Vision, pages 2035. Springer, 2024. [53] Alexander Lappe and Martin A. Giese. Register and cls tokens yield decoupling of local and global features in large vits, 2025. URL https://arxiv.org/abs/2505.05892. [54] Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, et al. Emernerf: Emergent spatial-temporal scene decomposition via self-supervision. arXiv preprint arXiv:2311.02077, 2023. [55] Yi Li, Hualiang Wang, Yiqun Duan, Jiheng Zhang, and Xiaomeng Li. closer look at the explainability of contrastive language-image pre-training. Pattern Recognition, 162:111409, 2025. ISSN 0031-3203. doi: https://doi.org/10.1016/j.patcog.2025.111409. URL https: //www.sciencedirect.com/science/article/pii/S003132032500069X. [56] Feng Wang, Jieru Mei, and Alan Yuille. Sclip: Rethinking self-attention for dense visionlanguage inference, 2024. URL https://arxiv.org/abs/2312.01597. [57] Walid Bousselham, Felix Petersen, Vittorio Ferrari, and Hilde Kuehne. Grounding everything: Emerging localization properties in vision-language transformers, 2023. URL https://arxiv. org/abs/2312.00878. [58] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Clearclip: Decomposing clip representations for dense vision-language inference. In European Conference on Computer Vision, pages 143160. Springer, 2024. [59] Sina Hajimiri, Ismail Ben Ayed, and Jose Dolz. Pay attention to your neighbours: Training-free open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2025. [60] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: retrospective. International journal of computer vision, 111:98136, 2015. [61] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 891898, 2014. [62] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [63] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12091218, 2018. [64] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. [65] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019. [66] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Computer VisionECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 746760. Springer, 2012. [67] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. [68] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. [69] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 40084017, 2021. doi: 10.1109/CVPR46437.2021.00400."
        },
        {
            "title": "Sections",
            "content": "1. Implementation details for Self-Distillation (section B) 2. Implementation details for Quantitative Evaluation (section C) 3. Additional Qualitative Examples for Segmentation (section D) 4. Additional Qualitative Heatmaps for PH-Reg Zero-Shot (section E) 5. Proof of Test Time Augmentation (section F) 16 Implementation details for Self-Distillation In this section, we provide detailed overview of how we implement self-distillation in PH-Reg. Our self-distillation framework consists of one teacher model and one student model. While both the teacher and student model are initialized from the same weights, the teacher is frozen, while additional register parameters are added to the student network. Our codes is available in https://github.com/0raiser0/PH-Reg B.1 Model Architectures Teacher Model Architecture. For CLIP based models, since we focus on zero-shot open-vocabulary segmentation, we utilize the NACLIP modification to the final layer. This modification does not introduce any additional weights to the teacher network, and is training-free. Our empirical analysis in Section B.4 shows that NACLIPs neighborhood attention mechanism improves feature consistency. For DINOv2, we directly use the final output layer, without any modification to the teacher network. Student Model Architecture. Based on the results from the ablation studies we integrate 16 register tokens into the student model. For the CLIP based student, to ensure representational alignment we directly take the head from the output layer (the MaskCLIP output). For DINO based students, we do not apply such modifications. We bicubicly upsample the positional embedding so it matches the input image. Unless otherwise specified, this modification is applied consistently, while all other layers remain unchanged. B.2 Model Implementation Table S.1: Model implementation libraries and weights. We compare models trained using different datasets and objectives. Model CLIP OpenCLIP DFN-CLIP DINOv2 Library clip (OpenAI) open_clip open_clip transformers (Hugging Face) Weight ViT-B-16 hf-hub:laion/CLIP-ViT-B-16-laion2B-s34B-b88K hf-hub:apple/DFN2B-CLIP-ViT-B-16 facebook/dinov2-base We provide the model weights and the corresponding implementation libraries in Table S.1. B.3 Optimization In the distillation process, the shorter side of each input image is resized using bicubic interpolation to 448 for CLIP-based models and 518 for DINOv2. The resized image is then randomly cropped into square of size (448, 448) or (518, 518), respectively. For each input image, we generate = 10 augmentations using random shifts and horizontal flips. Assuming an image length of 1, we uniformly sample the shift for both the horizontal and vertical axes from [0.15, 0.15]. While the horizontal flip is sampled with probability 0.5. To ensure each patch is covered, we do not apply any augmentation to the first image of the 10. All shifted images are concatenated and fed into the teacher model, while the original (unshifted) images are used as input to the student model. The target feature is computed as the average of these 10 augmentations. To accommodate the resized input images for both the teacher and student models, we consistently resize the positional embeddings using bicubic interpolation. During training, the weights of the teacher model are frozen. In the student model, we allow updates to registers, the positional embeddings, the convolutional patch embedding layer, and the final transformer layer containing the self-attention mechanism. The distillation framework is implemented in PyTorch, with distributed training managed via PyTorch Accelerate. Training is conducted on 4 NVIDIA Ada 6000 GPUs, with mixed-precision optimization to balance computational efficiency and numerical stability. Detailed training configurations are provided in Table S.2 and Table S.3. Table S.2: Configs for CLIP-based models. Table S.3: Configs for DINOv2. Value AdamW 3e-4 1e-5 1e-2 β1=0.9, β2=0.999 Config optimizer initial learning rate final learning rate weight decay optimizer momentum learning rate scheduler Exponential Scheduler batch size training epochs augmentation 16 100 RandomSquareCrop Value AdamW 1e-4 5e-6 1e-2 β1=0.9, β2=0. Config optimizer initial learning rate final learning rate weight decay optimizer momentum learning rate scheduler Exponential Scheduler batch size training epochs augmentation 8 100 RandomSquareCrop B.4 Pearson Analysis of Zero-Shot Segmentation Table S.4: Open-vocabulary semantic segmentation quantitative comparison on 7 datasets. We report the Pearson correlation coefficient for the zero-shot query against the one-hot ground truth labels. The results are averaged within each image, then averaged across images. Compared to mIoU, pearson does not require knowledge of all of the categories present an image (via softmax). The value ranges from -1 to 1, where 1 = perfect positive correlation, -1 = perfect negative correlation, and 0 = no linear correlation. The best result for each dataset is highlighted in bolded. Method SCLIP ClearCLIP NACLIP NACLIP+DVT Ours (PH-Reg) VOC21 -0.005 0.012 0.011 0.003 0. PC60 VOC20 0.409 0.349 0.489 0.428 0.470 0.422 0.487 0.438 0.494 0.468 PC59 0.443 0.543 0.543 0.551 0.590 Stuff 0.323 0.393 0.392 0.395 0.424 City 0.291 0.336 0.363 0.367 0.381 ADE20k Avg. 0.303 0.374 0.375 0.381 0.404 0.308 0.418 0.425 0.427 0. In this section we present additional evaluation results on zero-shot open-vocabulary semantic segmentation via the pearson metric. Results are illustrated in Table S.4. Overall, PH-Reg CLIP significantly outperforms the baseline models on 7 datasets. Even in the absence of prior category knowledge, PH-Reg CLIP achieves an average performance of 0.404, representing clear improvement over the second-best method, DVT enhanced NACLIP, with an average performance of 0.381. These results highlight that our approach improves the consistency of dense feature representations by reducing artifact tokens, thereby offering robust and generalizable enhancement over existing methods. We further observe that both ClearCLIP and NACLIP achieve competitive results; however, NACLIP significantly outperforms ClearCLIP on ADE20K and Cityscapes. The former requires the model to handle large number of categories, while the latter demands fine-grained localization of small objects. Based on this observation, we choose NACLIP as our primary teacher model, leveraging its neighbor attention mechanism to enhance the student models performance on these challenging tasks. 18 Table S.5: Dataset specific details for zero-shot open-vocabulary semantic segmentation. We list the per-dataset resolution, crop size, and stride used for each dataset. We maintain the same settings for all methods within given dataset."
        },
        {
            "title": "Dataset\nResize resolution\nCrop size\nStride",
            "content": "VOC21 448 336 112 PC 60 Object VOC20 336 448 336 336 112 112 336 336 112 PC 59 448 336 112 Stuff City ADE 448 560 448 336 224 336 112"
        },
        {
            "title": "C Implementation details for Quantitative Evaluation",
            "content": "In this section, we provide detailed implementation information for our quantitative evaluation experiments. In section C.1, we present the evaluation details for zero-shot open-vocabulary semantic segmentation (OVSS). In section C.2, we describe the evaluation details for linear probe based semantic segmentation and monocular depth estimation. C.1 Implementation details of zero-shot open-vocabulary semantic segmentation. We follow SCLIP and NACLIP in the setup for the open-vocabulary semantic segmentation evaluation. For fairness, we utilize the same parameters for all models. We resize input images such that the shorter side is scaled to specific resolution, while maintaining the original aspect ratio for the longer side. Additionally, we set fixed crop sizes and strides during evaluation. All evaluation parameters are summarized in Table S.5, while all other settings follow their default configurations. C.2 Implementation details of linear probe based evalution. Our linear probe evaluation follows prior work (Vision Transformers Need Registers, Denoising Vision Transformers), where linear layer is trained as decoding head to predict pixel-wise segmentation or depth logits. Semantic Segmentation. We extract the final output features from the frozen backbone and, if applicable, pass them through the denoiser (for the DVT baseline). single learnable linear layer is then trained to predict the segmentation logits. For CLIP-based models, both training and testing images are resized to (448, 448), while for DINOv2, the images are resized to (518, 518). Monocular Depth Estimation. Similar to semantic segmentation, we extract features from the backbone, and pass them through the denoiser if applicable. Following the method in DVT and DINOv2, we then append the [CLS] token to each patch token to enrich the feature representations for all methods. linear layer is trained using SigLoss and gradient loss (scaled by factor of 0.5) to predict depth values into 256 uniformly distributed bins. We adopt DVTs learning rate of 5e-3 for all experiments."
        },
        {
            "title": "D Additional Qualitative Examples",
            "content": "Figure S.1: Open-vocabulary semantic segmantation qualitative comparision between different baseline models on ADE20K. 20 Figure S.2: Open-vocabulary semantic segmantation qualitative comparision between different baseline models on Pascal Context59. 21 Figure S.3: Open-vocabulary semantic segmantation qualitative comparision between different baseline models on COCO Obejct. Additional Qualitative Heatmaps for PH-Reg Zero-Shot Figure S.4: Zero-shot heatmap results. Our results have fewer artifacts than other methods. 23 Figure S.5: Zero-shot heatmap results. Our results have fewer artifacts than other methods. 24 Figure S.6: Zero-shot heatmap results. Our results have fewer artifacts than other methods. 25 Figure S.7: Zero-shot heatmap results. Our results have fewer artifacts than other methods."
        },
        {
            "title": "F Optimal Feature Aggregation",
            "content": "Let f1, . . . , fn Rd be augmented feature vectors from different transformations of an input image I. We seek the optimal aggregated feature that minimizes the total squared error: (1) (2) (3) (4) (5) (6) Expanding the objective: = arg min (cid:88) i= fi 2 2 (cid:88) (f fi 2f + ) Dropping constant terms that do not affect the result and simplifying: i=1 = nf 2 (cid:32) (cid:88) (cid:33) fi i=1 We multiply and divide the right side by n: = nf 2n (cid:32) (cid:88) i=1 1 (cid:33) fi Dividing the equation by as whole shows us that we need to minimize: 1 n (cid:88) i=1 fi2 2 So it can be derived that the mean of the feature vectors is the minimizer under MSE loss: = 1 n (cid:88) i=1 fi"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "University of Hong Kong",
        "Zhejiang University"
    ]
}