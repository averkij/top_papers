{
    "paper_title": "TransMLA: Multi-head Latent Attention Is All You Need",
    "authors": [
        "Fanxu Meng",
        "Zengwei Yao",
        "Muhan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern large language models (LLMs) often encounter communication bottlenecks on current hardware, rather than purely computational constraints. Multi-head Latent Attention (MLA) tackles this challenge by using low-rank matrices in the key-value (KV) layers, thereby allowing compressed latent KV states to be cached. This approach significantly reduces the KV cache size relative to traditional multi-head attention, leading to faster inference. Moreover, MLA employs an up-projection matrix to increase expressiveness, trading additional computation for reduced communication overhead. Although MLA has demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers still rely on Group Query Attention (GQA) and have not announced any plans to adopt MLA. In this paper, we show that GQA can always be represented by MLA while maintaining the same KV cache overhead, but the converse does not hold. To encourage broader use of MLA, we introduce **TransMLA**, a post-training method that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen, Mixtral) into MLA-based models. After conversion, the model can undergo additional training to boost expressiveness without increasing the KV cache size. Furthermore, we plan to develop MLA-specific inference acceleration techniques to preserve low latency in transformed models, thus enabling more efficient distillation of Deepseek R1."
        },
        {
            "title": "Start",
            "content": "TransMLA: Multi-head Latent Attention Is All You Need Fanxu Meng1,2*, Zengwei Yao3 , Muhan Zhang1,2 1Institute for Artificial Intelligence, Peking University 2State Key Laboratory of General Artificial Intelligence, Peking University 3Xiaomi Corp., Beijing, China https://github.com/fxmeng/TransMLA"
        },
        {
            "title": "Abstract",
            "content": "Modern large language models (LLMs) often face communication bottlenecks on current hardware, rather than purely computational limitations. Multi-head Latent Attention (MLA) addresses this issue by utilizing low-rank matrices in the key-value layers, enabling the caching of compressed latent keyvalue (KV) states. This design significantly reduces the KV cache size compared to traditional multi-head attention, thus accelerating inference. Furthermore, MLA incorporates an upprojection matrix to enhance expressiveness, effectively trading extra computation for reduced communication overhead. Despite its demonstrated efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers continue to rely on Group Query Attention (GQA), with no public plans to adopt MLA. This paper shows that GQA can always be represented by MLA with the same KV cache overheadthe reverse does not hold. To encourage broader adoption, we introduce TransMLA, post-training method that converts widely used GQA-based pre-trained models (e.g. LLaMA, Qwen, Mixtral) into MLA-based models. After this conversion, further training is performed to boost the models expressiveness without increasing the KV cache size. Additionally, we plan to develop MLA-specific inference acceleration strategies to maintain low latency in transformed models, thereby facilitating more effective distillation of Deepseek R1. 5 2 0 2 1 1 ] . [ 1 4 6 8 7 0 . 2 0 5 2 : r (a) Group Query Attention (GQA) (b) Multi-Head Attention (MHA) (c) Multi-head Latent Attention (MLA) Figure 1: The process of equivalently converting GQA to MLA. (a) In GQA, is repeated to match the number of heads before computing attention. (b) Instead, WK is repeated; after multiplying with X, it directly produces , which is equivalent to GQA. (c) Decomposing yields an r-dimensional representation that matches the KV cache dimension in GQA while maintaining equivalence."
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) have become indispensable tools for productivity (OpenAI, 2024; Anthropic, 2024; Team et al., 2024a), with open-source models (AI@Meta, 2024; Mistral, 2024; Qwen, 2024; Liu et al., 2024a; Team et al., 2024b; Abdin et al., 2024) narrowing the *These authors contributed equally to this work. Corresponding author: <muhan@pku.edu.cn> performance gap to closed-source alternatives. The effectiveness of LLMs is primarily due to Next Token Prediction (Radford, 2018; Brown et al., 2020), where tokens are predicted sequentially, with attention computed between each token and its preceding ones. To avoid redundant calculations, keyvalue (KV) pairs are cached. However, as model sizes grow, caching overhead increases, leading to memory and communication bottlenecks. For example, LLaMA-65B (Touvron et al., 2023), with 8-bit key-value quantization, requires over 86GB of 1 GPU memory to store 512K tokens, surpassing the capacity of single H100-80GB GPU (Sun et al., 2024). Several attention modification methods have been proposed to address this issue. Multi-Query Attention (MQA) (Shazeer, 2019) reduces the number of attention heads by using single head for both the Key and Value layers. Another approach, Group-Query Attention (GQA) (Ainslie et al., 2023), groups Query heads, with each group sharing single Key and Value head. While both methods reduce KV Cache requirements, they sacrifice performance. Additionally, techniques like post-training pruning (Yu et al., 2024) and KV compression (Liu et al., 2024c; Hooper et al., 2024) can reduce KV size but may harm model accuracy, often necessitating fine-tuning to recover performance. Multi-head Latent Attention (MLA), introduced in Deepseek V2 (DeepSeek-AI, 2024) and extended in Deepseek V3 (DeepSeek-AI, 2024) and Deepseek R1 (Guo et al., 2025), strikes balance between speed and effectiveness. However, these studies did not offer theoretical proof or perform ablation experiments comparing MLAs performance with that of GQA. This paper presents theoretical proof showing that for the same KV Cache overhead, MLA consistently offers greater expressive power than GQA. We then convert several popular GQA-based modelssuch as LLaMA-3 (AI@Meta, 2024), Qwen-2.5 (Qwen, 2024), Mistral (Jiang et al., 2023), Mixtral (Mistral, 2024), Gemma-2 (Team et al., 2024b), and Phi-4 (Abdin et al., 2024)into equivalent MLA models with the same KV Cache overhead. Through fine-tuning experiments, we demonstrate that the enhanced expressive power of MLA leads to superior performance on downstream tasks. This work aims to shift the focus of mainstream LLM attention design from GQA to MLA, offering low-cost migration strategy that enables model developers to maximize the potential of existing models while reducing both resource consumption and carbon emissions."
        },
        {
            "title": "2 Related Work",
            "content": "In large language models (LLMs), the key-value (KV) cache becomes significant bottleneck due to the quadratic scaling of self-attention with sequence length. As each tokens KV pair must be recomputed for every new token, the memory demands of the cache grow quickly, limiting the abil2 ity to process long sequences. To address this issue, several techniques have been proposed to reduce the memory footprint of the KV cache, each with its own benefits and trade-offs. One approach is linear attention, as seen in methods like Linear Transformer (Katharopoulos et al., 2020; Wang et al., 2020), RWKV (Peng et al., 2023), and Mamba (Gu and Dao, 2023), which replace the standard attention mechanism with one that scales linearly with sequence length. While linear attention significantly reduces memory demands, it can reduce model expressivity, leading to performance degradation in tasks requiring complex, long-range token dependencies. Another technique is dynamic token pruning, employed by LazyLLM (Fu et al., 2024), A2SF (Jo and Shin, 2024), and SnapKV (Li et al., 2024). These methods selectively prune less important tokens from the KV cache, reducing memory usage without sacrificing performance. Although dynamic pruning improves efficiency, it risks discarding essential tokens, especially for tasks requiring detailed understanding of distant context. Additionally, dynamic pruning introduces complexity in determining which tokens to prune and often requires fine-tuning or retraining. Pruning head dimensions, seen in approaches like SliceGPT (Ashkboos et al., 2024), Sheared (Xia et al., 2023), and Simple Pruning (Sun et al., 2023), reduces the number of attention heads or the dimensionality of each head. By eliminating less important heads or dimensions, memory usage is reduced, but excessive pruning can impair the models ability to capture important token relationships. Sharing KV representations across layers, as in YONO (Sun et al., 2024), MiniCache (Liu et al., 2024b), and MLKV (Zuhri et al., 2024), reduces memory by reusing the same KV cache across multiple layers. This can drastically lower memory usage and speed up inference, but sharing caches across layers with different attention patterns can negatively affect performance. Finally, KV quantization techniques like GPTQ (Frantar et al., 2022), Kivi (Liu et al., 2024c), and KVQuant (Hooper et al., 2024) reduce the precision of the KV vectors by storing them in lower-bit formats. This reduces memory usage and computational overhead, enabling longer contexts and faster inference with minimal performance loss."
        },
        {
            "title": "3 TransMLA",
            "content": "This section begins by presenting the following theorem: Theorem 1. The expressiveness of MLA is greater than that of GQA when both have the same size of KV cache. Proof: Sections 3.1, 3.2, and 3.3 show that any GQA configuration can be equivalently transformed into MLA with the same size of KV cache. In Section 3.4, we demonstrate that there are cases where MLA cannot be represented by GQA. 3.1 Replicating the Keys in GQA Figure 1a illustrates the typical structure of Group Query Attention (GQA). In GQA, the query is split into nq = heads, each of dimension dh. In dh order to reduce the number of heads for the keys and values, the key is defined to have nk heads (with nk < nq), each of dimension dh. Let RT be the input sequence of length and hidden dimension D, and let WK RD(nk dh) be the projection matrix for the keys. Then, = WK RT (nk dh). Because standard multi-head attention requires and (as well as V) to have the same number of heads, we must expand from nk heads to nq heads. Define the replication factor = nq . nk Partition along its columns into nk blocks, each corresponding to one head: K(i) = K[:, dh:(i+1) dh], = 0, 1, . . . , nk 1, where each block K(i) RT dh. Replicating each K(i) by times and concatenating yields the expanded matrix RT D: Then, replicate each W(i) times and concatenate them in sequence to form new projection matrix K = nq nk RDD: (cid:105) . , . . . , W(nk1) (cid:125) (cid:123)(cid:122) times (cid:104) = W(0) (cid:124) , . . . , W(0) (cid:123)(cid:122) (cid:125) times , . . . , W(nk1) (cid:124) Applying to directly yields = K. This approach is mathematically equivalent to first computing and then replicating its heads. 3.3 Low-Rank Factorization Form MLA Figure 1c demonstrates that K, formed by replicating WK, has at most nk dh degrees of freedom. Consequently, its rank is at most nk dh. To see this more formally, we factorize using the Singular Value Decomposition (SVD): = UK SK K, where UK and VK are DD orthogonal matrices, and SK is diagonal matrix of singular values. Only the top nk dh (or fewer) of these singular values can be nonzero. Hence, we can truncate the SVD to keep just the top-r singular values, with nk dh: K = UK [:,:r] SK [:r,:r] [:r,:]. Define Wa = UK [:,:r] (cid:113) SK [:r,:r] RDr, Wb = (cid:113) SK [:r,:r] [:r,:] RrD. = (cid:104) K(0), . . . , K(0) (cid:123)(cid:122) (cid:125) (cid:124) times , . . . , K(nk1), . . . , K(nk1) (cid:125) (cid:124) (cid:123)(cid:122) times (cid:105) . Then 3.2 Moving Replication to the Parameter Side Figure 1b shows an alternative approach to replicating keys. Instead of computing and then replicating each head, we can replicate the projection matrix WK before computing K. First, split WK along its columns into nk parts, RDdh corresponds to one where each W(i) original key head: W(i) = WK [:,:, dh:(i+1) dh], = 0, 1, . . . , nk1. 3 = Wa Wb and = Wa Wb K. This construction shows how the GQA repeat KV scheme can be interpreted as low-rank factorization reminiscent of the MLA approach. In practice, when caching the key and value matrices, one only needs to store the low-rank representation Wa K. During the actual attention computation, the representation can be expanded by multiplying with Wb K, thus regaining the full dimensionality and enhancing expressiveness. with Wb 3.4 MLA Not Representable in GQA Consider scenario where the vectors in Wb are orthogonal. In this case, the output of each channel, after multiplying XWa K, remains distinct across the channels. However, in GQA, within each group, the heads are replicated, meaning that the output for all heads within group is identical. This difference in structure implies that there are certain cases in MLA that cannot be captured by GQA, as MLA allows for more diversity in the outputs across the channels. Based on the steps outlined above, we have proven Theorem 1. By transforming GQA into an equivalent MLA representation, we can enhance the expressiveness of the model. The next section will present experimental results to validate this claim."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experiment Setting This section demonstrates how to convert GQAinto an MLA model using the based model Qwen2.5 framework, and compares the training performance of the two models on downstream tasks. The Qwen2.5-7B model has 28 query heads and 4 key/value heads per layer, with each head having dimension of 128 and KV cache dimension of 1024. Similarly, the Qwen2.5-14B model features 40 query heads and 8 key/value heads per layer, each with dimension of 128, and KV cache dimension of 2048. Upon transforming the Qwen2.5-7B model into the MLA model, the output dimensions of the weight matrices and are adjusted to 512, while the KV cache dimension remains unchanged at 1024. Unlike the original setup of the GQA model, the TransMLA approach projection the dimensions of from 512 to 28 128 = 3584 dimensions. Since the 28 query heads can now interact with 28 distinct queries, forming diverse functional representations, this adjustment significantly enhances the models expressive power. and This transformation allows TransMLA to improve the GQA models expressive ability without increasing the KV cache size. Notably, the increase in parameters is minimal. Specifically, for the QK pair, an additional 512 3584 matrix is introduced, while the original matrix had dimensions of 3584 3584 + 512 3584. Therefore, the added parameters account for only 1/8 of the original ma- (a) Training Loss over Steps (b) Test Accuracy Figure 2: Comparison of Training Loss and Accuracy between TransMLA and GQA-based Qwen Models trix size. Similarly, for the V-O pair, the additional parameters also represent just 1/8 of the original. As result, the total parameter count of the model increases slightly from 7.6 billion to 7.7 billion, which is negligible rise. 4.2 Comparing The Fine-Tuning Performance To evaluate the performance improvement of the transformed MLA model, we train both the original GQA-based Qwen model and the TransMLA model on an instruction fine-tuning dataset, SmolTalk (Allal et al., 2025). The SmolTalk dataset includes rich set of instructiontuning data, including mathematical tasks (MetaMathQA (Yu et al., 2023)) and coding tasks (SelfOSS-Starcoder2-Instruct (Wei et al., 2024)). The training was conducted using the torchtune (torchtune maintainers and contributors, 2024) framework, with batch size of 16, learning rate of 2e-5, and 2 epochs. To minimize the impact on the original model, only the Key-Value layers were trained. For the GQA model, only WK and WV were trained, while for the TransMLA model, K, were trained. The loss during training and the models performance after training are shown in Figure 2. , and K, As shown in Figure 2a, the transformed MLA 4 model (TransMLA) exhibits significantly lower loss during training, indicating stronger data fitting capabilities. Figure 2b further demonstrates that the TransMLA model outperforms the original GQAbased model in terms of accuracy across both 7B and 14B model settings, particularly in math and code tasks. This suggests that TransMLA not only enhances the models expressiveness but also leads to notable performance improvements on specific tasks. The performance boost is attributed not only to the enlarged Key-Value dimensions but also to the use of orthogonal decomposition. To further validate this, comparison experiment was conducted in which the dimensionality expansion was implemented using an identity map initialization, without the orthogonal decomposition. The resulting model, trained on the GSM8K dataset, achieved an accuracy of 82.11%, which was only 0.15% higher than the original GQA-based model (81.96%). This marginal improvement suggests that merely adding trainable parameters does not explain the significant performance boost; the orthogonal decomposition method plays crucial role in enhancing the models effectiveness. Further experiments are ongoing to explore the underlying causes of this phenomenon and to validate the contribution of orthogonal decomposition to model performance."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "In this paper, we demonstrate that the expressive power of Multi-Head Linear Attention (MLA) surpasses that of Group Query Attention (GQA). We prove that any GQA model can be equivalently transformed into an MLA model. Through extensive experiments, we validate that the MLA model, once transformed and fine-tuned, exhibits significantly enhanced performance. This work fills the gap left by the DeepSeek paper, which lacked both theoretical and empirical comparison between GQA and MLA. In future work, we aim to extend our approach by adapting large-scale models such as LLaMA, Qwen, and Mistral to MLA-based architectures. Additionally, we plan to employ DeepSeek R1 distillation to further optimize the performance of the converted models, ultimately achieving even greater results."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. AI@Meta. 2024. Llama 3 model card. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. 2025. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737. Anthropic. 2024. Claude 3.5 sonnet. Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. DeepSeek-AI. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434. Jamie Frantar, Irina Pustokhina, Artem Kotik, and Sergey Kuzmin. 2022. Gptq: Efficient quantization for transformers in the gpt family. Proceedings of the 38th International Conference on Machine Learning (ICML). Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, and Mahyar Najibi. 2024. Lazyllm: Dynamic token pruning for efficient long context llm inference. arXiv preprint arXiv:2407.14057. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. 5 James Hooper, Li Dai, Zhen Zhang, and Seung-Hwan Lee. 2024. Kvquant: Quantization for efficient keyvalue caching in transformer models. arXiv preprint arXiv:2402.12345. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. 2023. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv:2310.06825. Hyun-rae Jo and Dongkun Shin. 2024. A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder. arXiv preprint arXiv:2407.20485. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. 2024. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024a. arXiv preprint Deepseek-v3 technical report. arXiv:2412.19437. Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, and Bohan Zhuang. 2024b. Minicache: Kv cache compression in depth dimension for large language models. arXiv preprint arXiv:2405.14366. Zhiyang Liu, Dong Zhang, Xinyi Li, and Ji Wu. 2024c. Kivi: Quantized key-value representation for efficient long-context transformers. arXiv preprint arXiv:2402.06732. Mistral. 2024. Cheaper, better, faster, stronger: Continuing to push the frontier of ai and making it accessible to all. OpenAI. 2024. Hello GPT-4o. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. 2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048. Qwen. 2024. Qwen2.5: party of foundation models. Alec Radford. 2018. Improving language understanding by generative pre-training. Noam Shazeer. 2019. One write-head is all you need. arXiv:1911.02150. Fast transformer decoding: arXiv preprint Tian Sun, Li Zhang, and Shuang Wu. 2024. You only need one: Efficient kv sharing across transformer layers. Proceedings of the 42nd International Conference on Machine Learning (ICML). Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024a. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024b. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. torchtune maintainers and contributors. 2024. torchtune: Pytorchs finetuning library. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768. Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro Von Werra, Arjun Guha, and Lingming Zhang. 2024. Selfcodealign: Self-alignment for code generation. arXiv preprint arXiv:2410.24198. Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2023. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694. Hao Yu, Zelan Yang, Shen Li, Yong Li, and Jianxin Wu. 2024. Effectively compress kv heads for llm. arXiv preprint arXiv:2406.07056. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284. Zayd Muhammad Kawakibi Zuhri, Muhammad Farid Adilazuarda, Ayu Purwarianti, and Alham Fikri Aji. 2024. Mlkv: Multi-layer key-value heads for memory efficient transformer decoding. arXiv preprint arXiv:2406.09297."
        }
    ],
    "affiliations": [
        "Institute for Artificial Intelligence, Peking University",
        "State Key Laboratory of General Artificial Intelligence, Peking University",
        "Xiaomi Corp., Beijing, China"
    ]
}