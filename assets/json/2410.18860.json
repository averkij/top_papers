{
    "paper_title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations",
    "authors": [
        "Aryo Pradipta Gema",
        "Chen Jin",
        "Ahmed Abdulaal",
        "Tom Diethe",
        "Philip Teare",
        "Beatrice Alex",
        "Pasquale Minervini",
        "Amrutha Saseendran"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. We hypothesise that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Our extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%)."
        },
        {
            "title": "Start",
            "content": "DECORE: DECODING BY CONTRASTING RETRIEVAL HEADS TO MITIGATE HALLUCINATIONS Beatrice AlexQ Aryo Pradipta GemaQ,K Chen JinK Ahmed AbdulaalK,V Philip TeareK QUniversity of Edinburgh, United Kingdom KCentre for AI, Data Science & Artificial Intelligence, R&D, AstraZeneca, United Kingdom University College London, United Kingdom aryo.gema@ed.ac.uk Pasquale MinerviniQ,A Amrutha SaseendranK amrutha.saseendran@astrazeneca.com AMiniml.AI, United Kingdom Tom DietheK 4 2 0 2 4 2 ] . [ 1 0 6 8 8 1 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. We hypothesise that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads (DeCoRe), novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as guide. Our extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have emerged as powerful natural language generators, demonstrating remarkable capabilities across range of tasks (Radford et al., 2019; Brown et al., 2020; Wei et al., 2022a; Ouyang et al., 2022). However, LLMs are prone to hallucinations, where the model generates content that is not grounded in reality or misrepresents the facts (Ji et al., 2023; Rawte et al., 2023; Zhang et al., 2023c; Li et al., 2024a). The tendency of LLMs to hallucinate undermines their reliability, especially when applied in high-stakes domains such as clinical decision-making or legal reasoning (Ahmad et al., 2023; Dahl et al., 2024). Understanding the underlying mechanisms responsible for hallucinations in LLMs remains challenging. Wu et al. (2024) found that there are special attention heads responsible for retrieving relevant information from given context, which they called retrieval heads. While identifying these mechanisms is key to understanding LLMs, little research has explored how to use these insights to effectively mitigate hallucinations, which is the focus of our work. We propose novel decoding method termed Decoding by Contrasting Retrieval Heads (DeCoRe), as illustrated in Figure 1. This method builds on the assumption that masking retrieval heads can induce hallucination by impairing the ability of the model to retrieve relevant information from the context. DeCoRe leverages Contrastive Decoding (Li et al., 2023) to amplify the differences between the original and the hallucinating outputs, leading to more accurate final responses. Furthermore, we propose using the conditional entropy of the models next-token distribution to control the contrastive decoding mechanism. Our findings show that DeCoRe significantly improves accuracy in tasks that require contextual faithfulness, such as XSum (Narayan et al., 2018), MemoTrap (Liu & Liu, 2023), Open Book Natu1Code is available at https://github.com/aryopg/DeCoRe."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of the DeCoRe workflow. Given the same input, the base LLM (LLMbase) and the variant with masked retrieval heads (LLMmasked) predict the next token. An uncertainty estimation is applied to the base models output using conditional entropy: higher conditional entropy increases the contrastive factor (α), penalising predictions that align with the LLMmasked. The final prediction is selected based on weighted contrastive decoding of the outputs from both models, leading to more grounded response. ral Questions (NQ; Kwiatkowski et al., 2019), and NQ-Swap (Longpre et al., 2021). Furthermore, our experiments show that DeCoRe enhances the models accuracy in factual recall tasks. For example, in the TriviaQA (Joshi et al., 2017) and PopQA (Mallen et al., 2023) dataset, DeCoRe shows an accuracy gain compared to other hallucination mitigation methods. Similarly, when applied to TruthfulQA (Lin et al., 2022), the Llama3-8b-Instruct model (Dubey et al., 2024), when combined with DeCoRe, generates more truthful and informative responses than other comparable hallucination mitigation methods. Finally, our experiments on MuSiQue (Trivedi et al., 2022) show that DeCoRe can significantly improve the accuracy of the model in long-form generation and reasoning tasks, for example, when used jointly with Chain of Thought (CoT; Wei et al., 2022b) prompting."
        },
        {
            "title": "2 DECORE: DECODING BY CONTRASTING RETRIEVAL HEADS",
            "content": "DeCoRe operates by masking specific retrieval heads to trigger hallucinations and then employs contrastive mechanism that penalises outputs resembling those from the hallucinating model, thereby amplifying the more accurate predictions of the base model. We further enhance this approach with dynamic entropycontrolled mechanism to adjust the contrastive effect based on the entropy of the next token distribution of the model. Figure 1 illustrates this process over time. 2.1 MASKING RETRIEVAL HEADS In this section, we describe how we mask retrieval heads in our base LLM to induce hallucinations, following the notation from Vaswani et al. (2017). Figure 2: Example of hallucination induced by masking retrieval heads in the NQ-Swap task. The base model retrieves the correct answer from the substituted context, while the masked model generates an incorrect answer. Given base LLM fbase, let x<t = (x1, x2, . . . , xt1) be sequence of previous tokens, where xi and denotes the vocabulary of the model. The logits for the next token distribution at time step are given by fbase(x<t) RX , and the probability of the next token xt is: pbase (xt x<t) exp (fbase(x<t)) (1) In our approach, we derive variant of the base LLM by masking set of retrieval heads. We identify these heads using the method proposed by Wu et al. (2024), which involves analysing attention patterns on the Needle-in-a-Haystack (NitH; Kamradt, 2023) dataset. The NitH dataset is"
        },
        {
            "title": "Preprint",
            "content": "designed to evaluate the ability of model to retrieve specific information from long context, making it suitable for identifying attention heads that contribute to information retrieval. We compute retrieval score, defining it as the ratio of successful copy-paste operations by each attention head, following Wu et al. (2024). Further details are provided in Appendix C.1. We then rank the attention heads according to their retrieval scores and select the top heads as retrieval heads. Let Hretrieval = {(l1, h1), . . . , (lN , hN )} denote the set of retrieval heads to be masked, where li is the layer index and hi is the head index within that layer. In Transformer architecture, the output of the multi-head attention mechanism at layer is given by: (cid:16) (cid:17) MultiHead(l) (cid:16) Q(l), (l), (l)(cid:17) = Concat head(l) 1 , . . . , head(l) (l) , (2) where Z+ is the number of attention heads; Q(l), (l), and (l) Rddk respectively denote the query, key, and value matrices at layer l; denotes the model hidden dimension and dk is the key dimension (where dk = d/H); (l) RHdkd denotes output projection layer; and each head is computed as: head(l) = Attention Q(l)W (l) Q,h, (l)W (l) K,h, (l)W (l) V,h , (3) (cid:16) (cid:17) Q,h, (l) where (l) l. To mask each head head(l) K,h, (l) V,h Rddk respectively denote query, key, value weight matrices at layer {0, 1} such that: such that (l, h) Hretrieval, we define mask m(l) m(l) = (cid:26)0 1 if (l, h) Hretrieval, otherwise. Then, the masked multi-head attention output at layer becomes: Q(l), (l), (l)(cid:17) MultiHead(l) = Concat m(l) (cid:16) (cid:16) masked 1 head(l) 1 , . . . , m(l) head(l) (4) (cid:17) (l) , (5) where denotes the scalar multiplication. The token logits of the masked model is then given by fmasked (x<t), and the next-token distribution is: pmasked (xt x<t) exp (fθmasked (x<t)) . (6) We hypothesise that masking retrieval heads reduces the models ability to retrieve relevant information from the context, leading to higher likelihood of generating hallucinations. We empirically validate this hypothesis in Appendix D, where we evaluate the model on factuality and faithfulness evaluation tasks before and after masking retrieval heads. Figure 2 shows an example of the induced hallucination after masking 10 retrieval heads in Llama3-8b-Instruct (Dubey et al., 2024). 2.2 CONTRASTING BASE AND MASKED LLMS Given the base and masked LLMs from Section 2.1, our goal is to improve the faithfulness of the generated output. To achieve this, we propose contrasting the next-token distributions of the base and masked models, effectively increasing the likelihood of the tokens selected by the former while decreasing the likelihood of the tokens selected by the latter. More formally, DeCoRe uses the following next-token distribution (xt x<t): (xt x<t) exp [(1 + α) log pbase (xt x<t) α log pmasked (xt x<t)] . (7) In Equation (7), the new next-token distribution p(xt x<t) is defined by contrasting the nexttoken distributions of the base model pbase(xt x<t) and the masked model pmasked(xt x<t), introduced in Section 2.1; and α is scaling factor that controls the weight of the next-token distribution induced by the base model pbase(xt x<t) and the one induced by the masked model pmasked(xt x<t). The term (1 + α) log pbase (xt x<t) in Equation (7) encourages the base LLM to predict highly probably token under its distribution, while α log pmasked (xt x<t) penalises predictions that are also likely under the masked models distribution. 2.3 DYNAMIC CONTRASTIVE DECODING We propose method to dynamically select the hyper-parameter α using an uncertainty quantification approach, namely the conditional entropy, which is considered reliable predictor for whether"
        },
        {
            "title": "Preprint",
            "content": "a model might generate hallucinations (Malinin & Gales, 2021; Kadavath et al., 2022).2 For given context x<t, the conditional entropy (xt) of the next-token distribution of model (xt x<t) is defined as: H(xt) = (xt x<t) log (xt x<t) , (8) (cid:88) xtV where denotes the vocabulary of the model; high conditional entropy indicates that the model is uncertain about its prediction. We propose using the conditional entropy (Equation (8)) of the base model to dynamically tune the contrastive decoding process described in Equation (7); more specifically, we propose setting the hyper-parameter α in Equation (7) to α = H(xt) as the entropy of the next-token distribution of the base model increases, α also increases, making potentially hallucinated generations less likely to be selected."
        },
        {
            "title": "3 EXPERIMENT SETUP",
            "content": "Hallucinations in LLMs can generally be categorised into two types factuality and faithfulness hallucinations (Huang et al., 2023; Hong et al., 2024). Factuality hallucinations refer to instances where the generated content is factually incorrect with respect to world knowledge. Faithfulness hallucinations refer to instances where the generated content fails to accurately adhere to the given source of information. Moreover, hallucinations tend to snowball in longer generation tasks such as multi-hop reasoning, compounding errors across multiple generation steps due to the inherently sequential behaviour of auto-regressive decoding in LLMs (Merrill & Sabharwal, 2023; Zhang et al., 2023a). In this section, we describe our experimental setup to evaluate DeCoRe. We employ diverse set of benchmarks to assess contextual faithfulness, factual accuracy, and multi-hop reasoning capability. Given that retrieval heads are important in correctly retrieving contextual information (Wu et al., 2024) and looking back over long reasoning processes (Wu et al., 2024), while attention heads play significant role in information transfer between tokens (Elhage et al., 2021), our experimental setup is designed to answer the following key research questions: 1) Can DeCoRe improve contextual faithfulness? 2) Can DeCoRe maintain or enhance the factual recall capabilities of LLMs? 3) Does coupling DeCoRe with CoT improve the multi-hop reasoning capability of the LLM? 3.1 DATASETS AND EVALUATION METRICS Faithfulness. We evaluate faithfulness on summarisation, instruction-following, and reading comprehension datasets. XSum (Narayan et al., 2018) is an abstractive summarisation dataset developed from BBC articles. We sub-sample 1,000 examples, following Chuang et al. (2024), and evaluate summaries using ROUGE-L (Lin, 2004), BERTScore (Zhang et al., 2020), and factKB (Feng et al., 2023) for factual consistency. MemoTrap (Liu & Liu, 2023) tests whether models can avoid memorisation traps and adhere to the given instructions, with performance reported using macroand micro-averaged accuracy. Instruction-Following Eval (IFEval; Zhou et al., 2023) evaluates the ability of the models to follow instructions on set of verifiable instructions such as write in more than 400 words. The performance is reported using Prompt-level and Instruction-level strict accuracies, which measure the percentage of prompts where all verifiable instructions are followed, and the percentage of verifiable instructions followed overall. Open-Domain Natural Questions (NQ-Open; Lee et al., 2019) QA dataset where we use an open-book configuration with one supporting document per question as described by Liu et al. (2024). NQ-Swap (Longpre et al., 2021) is version of NQ where the answer entity in the context was replaced with another entity and is used to evaluate the faithfulness of the model to the modified context. We evaluate the models with the Exact Match (EM) metric and, following Kandpal et al. (2023) and Liu et al. (2024), we consider prediction as correct if any sub-string of the prediction exactly matches any of the ground truth answers. Factuality. For factuality evaluation, we use four datasetsTruthfulQA, TriviaQA, PopQA, and NQ-Open. TruthfulQA (Lin et al., 2022) (MC1, MC2, MC3, and Gen) is used to evaluate whether models can avoid common human falsehoods; MC1, MC2, and MC3 are multi-label classification tasks, and Gen is generation task where evaluations use fine-tuned GPT models to assess the correctness and informativeness of the generated outputs. TriviaQA (Joshi et al., 2017), PopQA (Mallen 2As we also validate in our experiments in Appendix F."
        },
        {
            "title": "Preprint",
            "content": "et al., 2023), and NQ-Open are open-domain QA datasets used to evaluate the ability of model to answer questions about trivia, long-tail entities, and Google searches, respectively. We use closedbook configuration on these datasets to evaluate factual recall. Chain of Thought Reasoning. We evaluate DeCoRe in CoT-style reasoning tasks on both closedand open-book setups; we use MuSiQue (Trivedi et al., 2022), multi-hop question-answering dataset that requires models to answer questions by reasoning using multiple and disconnected pieces of information. More details on the evaluation protocol are available in Table 26."
        },
        {
            "title": "3.2 MODELS",
            "content": "We evaluate two models from the Llama3 family Dubey et al. (2024), namely Llama3-8B-Instruct and Llama3-70B-Instruct, to analyse the influence of model size on the downstream results. In Appendix H, we also report results from other model families, such as Mistral (Jiang et al., 2023) and Qwen2 (Yang et al., 2024). 3.3 BASELINES We compare DeCoRe against six baselines: 1) Greedy decoding; 2) Contrastive Decoding (CD; Li et al., 2023), where LLaMA3-8B-Instruct serves as the amateur model and LLaMA3-70B-Instruct act as the expert model; 3) Context-Aware Decoding (CAD; Shi et al., 2024), variant of CD where the amateur model is the same as the expert model but is not presented with the additional context; 4) Decoding by Contrasting Layers (DoLa; Chuang et al., 2023) that subtracts the logits in early layers to calibrate the final-layer logits. We evaluate two versions: DoLa-low (i.e., contrasting the first half of the layers with the final layer) and DoLa-high (i.e., contrasting the second half with the final layer); 5) Activation Decoding (AD; Chen et al., 2024) which uses the sharpness of context activations within intermediate layers to calibrate the next token prediction; 6) ITI (Li et al., 2024b) that trains linear classifiers on TruthfulQA data to obtain factual heads and layers with corresponding factual direction vectors and then apply intervention during the decoding process. Note that ITI requires training process on labelled data , whereas other baselines and DeCoRe are training-free. Also note that CAD is only applicable on tasks with additional context (i.e., XSum, open book NQ-Open, NQ-Swap, and open book MuSiQue). All implementation details are available in Appendix J. 3.4 DECORE VARIANTS We evaluate three variants of DeCoRe: 1) DeCoRestatic, which employs static scaling factor α throughout generation; 2) DeCoReentropy, which entropy to dynamically adjust the strength of the contrastive decoding; 3) DeCoReentropy-lite, which is similar to DeCoReentropy, except that it employs smaller LLM with the same vocabulary space as the masked LLM."
        },
        {
            "title": "4 RESULTS",
            "content": "In the following, we present the evaluation results of DeCoRe across faithfulness, factuality, and multi-hop reasoning tasks. We show that DeCoRe mitigates faithfulness and factuality hallucinations, and improves the accuracy of the model when combined with CoT prompting. These effectively answer our research questions stated in Section 3. Additionally, we examine the impact of the number of masked retrieval heads on task performance. Finally, we demonstrate that DeCoRe reduces conditional entropy over time in long-generation tasks, contributing to more accurate outputs. These results highlight DeCoRes broad effectiveness in enhancing LLM performance. DeCoRe Mitigates Faithfulness Hallucinations. Table 1 shows the performance of various models and decoding methods on faithfulness evaluation tasks. The results show that DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite consistently improve the base models across all tasks and model sizes. Specifically, DeCoReentropy yields the best or very competitive results in several"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. Model Llama3-8b-Instruct + ITI (Li et al., 2024b) + CAD (Shi et al., 2024) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRestatic + DeCoReentropy Llama3-70b-Instruct + ITI (Li et al., 2024b) + CD (Li et al., 2023) + CAD (Shi et al., 2024) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRestatic + DeCoReentropy + DeCoReentropy-lite XSum MemoTrap IFEval NQ-Open NQ-Swap ROUGE-L BERTScore-F1 factKB Macro Acc Micro Acc Prompt Acc Instruct Acc EM EM 19.90 13.25 18.82 19.82 19.92 19.79 19.87 19.45 22.41 21.64 22.71 21.45 22.46 22.43 22.49 21.94 21.93 22.28 67.23 59.96 67.20 67.19 67.34 67.31 67.83 67.69 69.77 69.46 69.99 69.28 69.80 69.93 69.91 69.35 69.40 69.34 47.61 34.35 67.16 47.21 48.49 48.49 64.07 66.10 61.32 61.33 54.73 65.61 61.11 59.99 60.57 64.88 65.49 59. 65.86 62.65 - 65.27 64.85 65.38 69.53 74.14 68.47 71.24 69.27 - 67.99 67.92 67.51 71.96 74.07 72.11 64.40 58.96 - 63.69 63.17 64.28 69.20 74.87 66.52 68.73 67.55 - 65.93 65.81 66.44 71.41 73.65 70.58 70.24 52.31 - 69.69 70.24 67.65 69.13 68.39 77.45 76.71 71.72 - 77.08 78.00 76.89 78.56 78.56 61. 78.30 63.19 - 78.18 78.66 76.26 78.06 76.38 84.41 83.69 79.74 - 84.29 84.65 84.41 84.89 84.89 71.46 69.68 56.16 69.83 69.68 69.49 68.93 70.62 70.66 71.07 71.90 65.80 71.83 71.07 70.40 71.15 72.51 72.66 71.26 60.62 51.08 74.21 60.77 60.98 60.51 64.43 66.08 76.11 74.76 68.37 84.70 75.98 75.26 74.02 79.06 79.79 75. Table 2: Performance of different models and decoding methods on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. Model TruthfulQA (MC) TriviaQA PopQA TruthfulQA (Generation) NQ-Open MC1 MC2 MC3 EM EM %Truth %Info %T %Reject EM Llama3-8b-Instruct + ITI (Li et al., 2024b) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRestatic + DeCoReentropy Llama3-70b-Instruct + ITI (Li et al., 2024b) + CD (Li et al., 2023) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRestatic + DeCoReentropy + DeCoReentropy-lite 39.41 43.70 39.05 38.68 31.21 38.68 38. 49.57 48.96 57.77 49.45 49.69 42.23 51.29 53.98 55.32 55.69 62.78 55.65 55.64 55.30 55.74 55.86 70.60 67.04 76.65 70.58 70.88 67.56 72.02 73.44 73.38 30.31 34.91 30.06 30.19 28.28 29.80 30.95 37.85 37.27 47.08 37.75 38.01 35.37 40.24 42.55 43.74 56.58 48.41 56.63 56.50 54.93 56.93 56. 74.77 73.54 72.83 74.74 73.96 74.14 74.79 74.76 73.87 26.64 15.63 26.58 26.49 26.38 26.86 26.88 40.63 39.62 37.03 40.65 40.00 40.53 40.74 40.58 39.09 80.66 87.52 80.66 80.78 80.42 80.78 78.95 88.74 82.50 88.25 88.74 88.98 87.39 88.25 89.23 88.13 63.89 78.46 62.91 62.67 63.40 67.93 74. 77.72 74.30 88.13 77.60 58.38 67.20 62.91 59.73 90.09 44.55 66.10 43.70 43.45 43.82 48.71 53.00 66.46 56.92 76.38 66.34 47.37 54.59 51.16 49.11 78.21 43.94 25.46 45.04 44.92 43.82 41.74 38.68 53.12 37.94 52.26 52.88 54.71 49.33 54.96 56.79 52.02 29.04 22.07 29.15 29.19 28.32 29.42 28. 40.08 38.57 36.23 40.08 39.59 40.23 40.41 40.45 39.21 faithfulness-related tasks.For instance, with Llama3-8b-Instruct, DeCoReentropy attains Macro Accuracy of 74.14% and Micro Accuracy of 74.87% on MemoTrap, producing significantly more accurate results than all baselines. DeCoReentropy also achieves the highest EM scores on open-book NQ-Open and competitive results on NQ-Swap. Similarly, with Llama3-70b-Instruct, DeCoReentropy achieves the highest EM score on NQ-Open and competitive results on NQ-Swap. In instructionfollowing tasks, DeCoReentropy also achieves competitive scores in the IFEval benchmark with Llama3-8b-Instruct, yielding Instruct and Prompt Strict Accuracy values of 68.39% and 76.38%, respectively. With Llama3-70b-Instruct, DeCoRestatic and DeCoReentropy achieve the joint-highest Instruct and Prompt Strict Accuracy values of 78.56% and 84.89%, respectively. While CAD yields accurate results on tasks like XSum and NQ-Swap, its applicability remains limited to datasets that provide additional contexts, making it not trivial to adapt to tasks such as MemoTrap and IFEval. On the other hand, DeCoRestatic and DeCoReentropy both improve the base models in all tasks. DeCoRe Mitigates Factuality Hallucinations. While DeCoRe is primarily designed to improve contextual faithfulness, its impact on factual recall tasks is an open question. To this end, we evaluate DeCoRe on range of tasks where the model needs to produce factually correct generations results are outlined in Table 2. We can see that DeCoRe significantly improves the accuracy of the models across various factuality evaluation tasks. For the Llama3-8b-Instruct model, DeCoReentropy demonstrates improvements in several TruthfulQA (Generation) metrics. Specifically, it achieves an informativeness score of 74.05% and an intersection of truthfulness and informativeness score of 53.00%, second only to ITI, which requires fine-tuning the model on TruthfulQA data.3 Further3The rejection ratesthe frequency by which the model answers have no commentof Llama3 models in TruthfulQA are higher than Llama2 models (Touvron et al., 2023), as reported by previous studies (Li et al., 2024b; Chuang et al., 2023); we report metrics for the non-rejection answers in Appendix E.1."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance of different models and decoding methods on MuSiQue, multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. For each base model, the best performance is indicated in bold, and the second-best is underlined. Model Llama3-8b-Instruct + CAD + ITI + DoLA + AD + DeCoRestatic + DeCoReentropy Llama3-70b-Instruct + CD + CAD + ITI + DoLA + AD + DeCoRestatic + DeCoReentropy + DeCoReentropy-lite MuSiQue without CoT MuSiQue with CoT Closed Book Open Book Closed Book Open Book 7.41 - 4.01 7.24 6.99 7.90 7.70 11.79 10.92 - 10.88 11.42 11.38 11.79 11.75 11.13 58.83 57.88 45.84 59.08 58.63 61.23 61.98 68.56 66.61 68.64 68.14 68.68 68.14 69.76 69.84 69.34 14.61 - 4.18 14.94 14.40 14.69 13.90 20.15 17.17 - 20.44 20.15 20.23 20.60 20.60 18. 69.84 73.02 38.31 69.92 69.92 72.49 74.47 74.43 71.70 74.02 74.27 74.64 74.27 75.05 74.93 73.36 more, DeCoRestatic yields the highest EM score on TriviaQA (56.93%) among all decoding strategies and achieves competitive EM scores on PopQA. For the larger Llama3-70b-Instruct model, DeCoReentropy achieves the highest truthfulness score (89.23%) on TruthfulQA (Gen); it performs competitively across informativeness and the intersection metrics, yielding the highest EM score on closed-book NQ-Open (40.45%). Finally, DeCoRestatic yields the highest EM score on PopQA (40.74%). These results suggest that DeCoRe methods can improve contextual faithfulness and factual consistency across different datasets. We believe this phenomenon is closely related to the hypothesis of attention heads as Information Movement (Elhage et al., 2021), which suggests that attention heads facilitate the transfer of information between tokens and that the residual stream vector space of one token typically contains information from other tokens. Thus, while factual recall may occur in the Multi-Layer Perceptron (Geva et al., 2021; Meng et al., 2022), masking retrieval heads may interfere with the information transfer from the question to the generated answer, potentially leading to hallucinations. We hypothesise that DeCoRe leverages this phenomenon, improving downstream results in factual recall tasks. DeCoRe with Chain-of-Thought. To assess the effectiveness of DeCoRe-based approaches in multi-hop reasoning tasks, we evaluate them on the MuSiQue dataset. Multi-hop reasoning requires models to integrate information across multiple reasoning steps, and retrieval heads may be crucial in this process as they allow models to reference earlier generated tokens. We conduct experiments in both closed-book and open-book settings, with and without CoT prompting. The closed-book setting resembles factuality evaluation, where model can rely solely on its parametric knowledge, while the open-book setting is related to faithfulness evaluation since the model has access to corpus of external knowledge. As shown in Table 3, DeCoRe variants consistently improve the EM scores across various settings. For the Llama3-8b-Instruct model, DeCoRestatic enhances the EM score in the closed-book setup with CoT from 14.61% (base model) to 14.69%, while in the open-book setup without CoT, DeCoReentropy achieves the highest score (61.98%). DeCoReentropy also yields accurate results in the open-book CoT scenario, achieving the most accurate results (74.47% EM). For the Llama370b-Instruct model, both DeCoRestatic and DeCoReentropy yield very accurate results, improving the EM score in the closed-book setup with CoT from 20.15% (base model) to 20.60%. DeCoRestatic achieves the highest score in the open-book CoT setup (75.05%), with DeCoReentropy closely following at 74.93%. These improvements underscore the effectiveness of DeCoRe in enhancing reasoning capabilities, especially when CoT prompting and external context are involved. Overall, the results indicate that DeCoRe improves information transfer between reasoning steps, leading to higher EM scores in both closed and open-book settings. This validates our hypothesis"
        },
        {
            "title": "Preprint",
            "content": "(a) Faithfulness Evaluation Tasks (b) Factuality Evaluation Tasks (c) Chain-of-Thought Reasoning Evaluation Tasks Figure 3: Correlation between the number of masked retrieval heads and performance of Llama38B-Instruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient for each plot. Detailed results are listed in Table 14 and Table 16. and demonstrates the usefulness of DeCoRe in tasks requiring complex reasoning, validating the insights from Wu et al. (2024) on the significance of retrieval heads in multi-step reasoning. Effect of Retrieval Head Masking on Task Performance of DeCoRe. We now analyse the correlation between the number of masked retrieval heads and the downstream results of the Llama3-8BInstruct model; results are outlined in Figure 3. We can see that the performance of DeCoReentropy across various tasks strongly correlates with the number of masked retrieval heads. For example, in XSum and MemoTrap, we can observe positive correlations between the factKB and macro accuracy scores and the number of masked retrieval heads. We attribute this to the nature of summarisation (XSum) and instruction-following (MemoTrap) tasks, which rely heavily on the ability of the model to extract and copy relevant information accurately. However, we observe moderate negative correlation as the number of masked retrieval heads increases on another Instruction following task, IFEval. We hypothesise that the reason behind this phenomenon is that IFEval requires different copying mechanism than in MemoTrap. As opposed to having to provide an exact copy of segment of the input, like in MemoTrap or partially XSum, IFEval requires the model to adhere to the instruction, which may not require an induction mechanism (e.g., In your response, the letter {letter} should appear {N} times.). In tasks such as open-book NQ-Open and NQ-Swap, we can see moderate negative correlation between the number of masked retrieval heads and EM. Nevertheless, in all experiments, DeCoReentropy produces more accurate results than the baseline in such tasks. In factual recall tasks (i.e., TriviaQA, PopQA, and closed-book NQ-Open), we can see negative correlation between EM scores and the number of masked retrieval heads. During decoding, when the masked retrieval heads fail to introduce significant differences between the hallucinating and the outputs of the base model, the effect of DeCoReentropy becomes less pronounced. TruthfulQA differs from the other factuality tasks, showing moderate positive correlation between downstream accuracy and the number of masked retrieval heads. This suggests that truthfulness, or the ability to"
        },
        {
            "title": "Preprint",
            "content": "(a) XSum (b) MuSiQue (Closed) + CoT (c) MuSiQue (Open) + CoT Figure 4: Comparison of Length-normalised conditional entropy of Greedy, ITI, DoLa, and DeCoReentropy in long-generation tasks (i.e., XSum (a), MuSiQue (Closed) + CoT (b), and MuSiQue (Open) + CoT (c)). Asterisks (*) indicate statistically significant differences between the distributions based on one-tailed Welchs t-test results. Detailed results are listed in Table 28. discern popular misconceptions, may require different retrieval mechanisms than the typical factual recall tasks. These findings can be combined with the results of masking random attention heads (Appendix G.1) further supporting our hypothesis on the effectiveness of masking retrieval heads in contrastive decoding. DeCoRe yields lower entropy across time in long generation tasks. We found that lower conditional entropy is related to correct predictions; generated sequences with lower conditional entropy tend to be more reliable (see Appendix F). Motivated by the importance of low conditional entropy, we evaluate the length-normalised conditional entropy of different decoding strategies in long-generation tasks (i.e., XSum, and MuSiQue with CoT prompting). As shown in Figure 4, DeCoReentropy yields lower conditional entropy compared to the baselines. DeCoReentropy demonstrates lower entropy in the open-book QA task (MuSiQue), with an average entropy of 0.29 compared to 0.30 for the baselines. Similarly, in XSum, DeCoReentropy achieves an entropy of 0.38, outperforming the baselines. In tasks such as summarisation (XSum) and open-book QA (MuSiQue), lower entropy is crucial because the model must strictly adhere to the provided document or evidence while generating the summary or answer. Any deviation from the context can result in hallucinations or factually incorrect outputs. The lower entropy observed with DeCoReentropy indicates that it generates less surprising sequences, reducing the likelihood of hallucinations. Overall, the reduction in conditional entropy shows that DeCoReentropy is able to maintain lower uncertainty throughout long-generation tasks. This reinforces the effectiveness of DeCoReentropy in applications requiring high contextual faithfulness, such as summarisation and open-book QA."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Internal Mechanism of LLMs. Studies have attempted to dissect the inner workings of LLMs by focusing on layers (Wallat et al., 2020; Geva et al., 2021; Meng et al., 2022; Yu et al., 2024), neurons (Dai et al., 2022), and attention heads (Elhage et al., 2021; Geva et al., 2023; Yuksekgonul et al., 2024). seminal discovery in this area is the identification of induction heads, the attention heads that perform an induction algorithm by looking back over the context to predict similar completion (Olsson et al., 2022). Similarly, Wu et al. (2024) identified retrieval heads, specific set of attention heads responsible for maintaining long-context factuality. These insights into the internal workings of LLMs is instrumental to our work, which focuses on these mechanisms to reduce hallucination. Our work leverages the idea that the masking of retrieval heads leads to hallucination. Constrained Decoding. Constrained decoding focuses on intervening during the generation process to reduce hallucinations. One example of constrained decoding is Inference-Time Intervention (ITI; Li et al., 2024b) which intervenes by probing and modifying attention heads or layers associated with model correctness. Contrastive Decoding (CD; Li et al., 2023) improves fluency and coherence by contrasting outputs from stronger expert LMs with those from weaker, smaller LMs. Building on CD, Shi et al. (2024) propose Context Aware Decoding (CAD) to mitigate con-"
        },
        {
            "title": "Preprint",
            "content": "textual hallucinations by contrasting the output of an LLM with and without the provided context. Similarly, Induced-then-Contrast Decoding (ICD; Zhang et al., 2023b) fine-tunes factually weak LLM using an automatically generated non-factual dataset, although this approach depends on the quality of the dataset and requires fine-tuning. More closely related to DeCoRe is Decoding by Contrasting Layers (DoLa; Chuang et al., 2023) and Autocontrastive Decoding (ACD; Gera et al., 2023), which examines the internal mechanism of the LLMs without fine-tuning. Both DoLa and ACD proposed contrasting the predictions of the final layer against the earlier ones via early exiting (Teerapittayanon et al., 2016; Elbayad et al., 2020). Activation Decoding (Chen et al., 2024) also examines the internal mechanism of the LLMs, particularly the sharpness of context ativations to calibrate the next tokens probability distribution. DeCoRe distinguishes itself by masking retrieval heads to induce hallucinations, followed by dynamic entropy-controlled contrastive decoding to penalise uncertain outputs, effectively reducing hallucinations without the need for fine-tuning."
        },
        {
            "title": "6 CONCLUSION",
            "content": "DeCoRe (Decoding by Contrasting Retrieval Heads) is novel decoding strategy that aims to reduce faithfulness and factuality hallucinations in LLMs. DeCoRe is based on the assumption that masking retrieval heads can induce hallucinations by limiting the ability of the model to retrieve relevant information from the given context. Specifically, DeCoRe uses retrieval head masking to create version of the model that is more likely to generate hallucinations and combines it with the original model via contrasting decoding scheme (Section 2.2). Furthermore, we propose simple approach to control the strength of the contrastive decoding scheme by using the conditional entropy of the next-token distribution of the model (Section 2.3). Our experimental results show that DeCoRe significantly improves the accuracy of the model in tasks requiring contextual faithfulness and in some factual recall and reasoning tasks. Limitations. While DeCoRe improves the performance of the base model across most tasks, there is no free lunch; existing baselines may still produce more accurate results than DeCoRe in specific tasks ( e.g., ITI in TruthfulQA or CAD in NQ-Swap). However, these baselines often offer limited improvements or may even generate less accurate responses in other tasks. We also observed that DeCoRe offers only marginal enhancements in factual recall tasks, suggesting that retrieval heads may not play primary role in factual recall except for information transfer. Finally, while we propose using the conditional entropy of the models next-token distribution to control the contrastive decoding scheme in DeCoRe, more semantic methods of uncertainty quantification may also be used (Farquhar et al., 2024)."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Dino Oglic and Anshul Kanakia for their unwavering support throughout this project. We also thank Emile van Krieken, Giwon Hong, Hongru Wang, Joshua Ong Jun Leang, Xiaotang Du, Xuanli He, and Yu Zhao for their meticulous review and invaluable feedback, which significantly improve the quality of this paper. This work was done during APGs internship at AstraZeneca. APG was supported by the United Kingdom Research and Innovation (grant EP/S02431X/1), UKRI Centre for Doctoral Training in Biomedical AI at the University of Edinburgh, School of Informatics. PM was partially funded by ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence), EPSRC (grant no. EP/W002876/1), an industry grant from Cisco, and donation from Accenture LLP. BA was partially funded by Legal and General PLC as part of the Advanced Care Research Centre and by the Artificial Intelligence and Multimorbidity: Clustering in Individuals, Space and Clinical Context (AIM-CISC) grant NIHR202639. This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh."
        },
        {
            "title": "REFERENCES",
            "content": "Muhammad Aurangzeb Ahmad, Ilker Yaramis, and Taposh Dutta Roy. Creating trustworthy llms: Dealing with hallucinations in healthcare ai. arXiv preprint arXiv:2311.01463, 2023."
        },
        {
            "title": "Preprint",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, and Junxian He. In-context sharpness as alerts: An inner representation perspective for hallucination mitigation. In ICLR 2024 Workshop on Reliable and Responsible Foundation Models, 2024. URL https: //openreview.net/forum?id=24U6vAHnYM. Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023. Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James Glass. Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps. arXiv preprint arXiv:2407.07071, 2024. Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel Ho. Large legal fictions: Profiling legal hallucinations in large language models. arXiv preprint arXiv:2401.01301, 2024. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84938502, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL https://aclanthology.org/ 2022.acl-long.581. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In ICLR 2020-Eighth International Conference on Learning Representations, pp. 114, 2020. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. mathematical frameIn Transformer Circuits Thread, 2021. URL https:// work for transformer circuits. transformer-circuits.pub/2021/framework/index.html. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nat., 630(8017):625630, 2024. Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, and Yulia Tsvetkov. FactKB: Generalizable factuality evaluation using language models enhanced with factual knowledge. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 933952, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.59. URL https: //aclanthology.org/2023.emnlp-main.59. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602."
        },
        {
            "title": "Preprint",
            "content": "Ariel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, and Eyal Shnarch. The benefits of bad advice: Autocontrastive decoding across model layers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10406 10420, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.580. URL https://aclanthology.org/2023.acl-long.580. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual In Empirical Methods in Natural Language associations in auto-regressive language models. Processing (EMNLP), 2023. URL https://arxiv.org/abs/2304.14767. Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie, Yu Zhao, Laura Perez-Beltrachini, Max Ryabinin, Xuanli He, and Pasquale Minervini. The hallucinations leaderboardan open effort to measure hallucinations in large language models. arXiv preprint arXiv:2404.05904, 2024. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv:1705.03551, 2017. triviaqa: Large Scale arXiv e-prints, art. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know. CoRR, abs/2207.05221, 2022. doi: 10. 48550/ARXIV.2207.05221. URL https://doi.org/10.48550/arXiv.2207.05221. Greg Kamradt. Needle in haystack - pressure testing llms. https://github.com/ gkamradt/LLMTest_NeedleInAHaystack, 2023. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1569615707. PMLR, 2329 Jul 2023. URL https://proceedings.mlr. press/v202/kandpal23a.html. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav"
        },
        {
            "title": "Preprint",
            "content": "Petrov. Natural questions: benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 60866096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ v1/P19-1612. URL https://aclanthology.org/P19-1612. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The dawn after the dark: An empirical study on factuality hallucination in large language models. arXiv preprint arXiv:2401.03205, 2024a. Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36, 2024b. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 12286 12312, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.687. URL https://aclanthology.org/2023.acl-long.687. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic huIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proman falsehoods. ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 32143252. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.ACL-LONG.229. URL https://doi.org/10.18653/v1/2022.acl-long.229. Alisa Liu and Jiacheng Liu. The memotrap dataset, 2023. URL https://github.com/ liujch1998/memo-trap. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. doi: 10.1162/tacl 00638. URL https://aclanthology.org/2024.tacl-1.9. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-based knowledge conflicts in question answering. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 70527063, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.565. URL https://aclanthology.org/2021. emnlp-main.565. Andrey Malinin and Mark J. F. Gales. Uncertainty estimation in autoregressive structured prediction. In ICLR. OpenReview.net, 2021. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In ACL (1), pp. 98029822. Association for Computational Linguistics, 2023. Henry Mann and Donald Whitney. On test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics, pp. 5060, 1947. Daniel McFadden et al. Conditional logit analysis of qualitative choice behavior.(1973). Frontiers in Econometrics, ed. P. Zarembka, pp. 10542, 1973."
        },
        {
            "title": "Preprint",
            "content": "Kevin Meng, David Bau, Alex Andonian, editing factual associations in gpt. D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural ing Systems, volume 35, pp. 1735917372. Curran Associates, https://proceedings.neurips.cc/paper_files/paper/2022/file/ 6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf. Locating and In S. Koyejo, S. Mohamed, A. Agarwal, Information ProcessURL Inc., 2022. and Yonatan Belinkov. William Merrill and Ashish Sabharwal. The Parallelism Tradeoff: Limitations of Log-Precision Transformers. Transactions of the Association for Computational Linguistics, 11:531545, 06 2023. ISSN 2307-387X. doi: 10.1162/tacl 00562. URL https://doi.org/10.1162/ tacl_a_00562. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 2018. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, In-context learning and induction Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. heads. arXiv preprint arXiv:2209.11895, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2773027744. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2022/ 2022. file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. In Technical report, OpenAi, 2019. URL https: //api.semanticscholar.org/CorpusID:160025533. Vipula Rawte, Amit Sheth, and Amitava Das. survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922, 2023. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. Trusting your evidence: Hallucinate less with context-aware decoding. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pp. 783791, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-short.69. URL https: //aclanthology.org/2024.naacl-short.69. Student. The probable error of mean. Biometrika, pp. 125, 1908. Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd International Conference on Pattern Recognition (ICPR), pp. 24642469. IEEE, 2016. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for doi: 10.1162/tacl 00475. URL https: Computational Linguistics, 10:539554, 2022. //aclanthology.org/2022.tacl-1.31. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, In I. Guyon, U. Von Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need."
        },
        {
            "title": "Preprint",
            "content": "Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Jonas Wallat, Jaspreet Singh, and Avishek Anand. BERTnesia: Investigating the capture and forgetting of knowledge in BERT. In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupała, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad (eds.), Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 174183, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1.17. URL https://aclanthology.org/2020.blackboxnlp-1.17. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a. URL https://openreview.net/ forum?id=gEZrGCozdqR. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022b. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6. Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Lei Yu, Meng Cao, Jackie Chi Kit Cheung, and Yue Dong. Mechanisms of non-factual hallucinations in language models. arXiv preprint arXiv:2403.18167, 2024. Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. Attention satisfies: constraint-satisfaction lens on In International Conference on Learning Representations factual errors of language models. (ICLR), 2024. URL https://arxiv.org/abs/2309.15098. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023a. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: EvalIn 8th International Conference on Learning Representauating text generation with BERT. tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr. Yue Zhang, Leyang Cui, Wei Bi, and Shuming Shi. Alleviating hallucinations of large language models through induced hallucinations. arXiv preprint arXiv:2312.15710, 2023b. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Sirens song in the ai ocean: survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023c. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "Table of Contents",
            "content": "Appendix - Reproducibility Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Appendix - Ethics Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Appendix - Retrieval Heads C.1. Extraction of Retrieval Heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 C.2. Retrieval Scores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 Appendix - Performance of Baseline Model with Masked Heads D.2. Faithfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 D.3. Factuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 D.4. Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Appendix - Additional TruthfulQA Generation Evaluation E.1. Evaluation of Non-rejection Responses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.2. Evaluation Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Appendix - Correlation between Length-normalised Entropy and Correctness F.1. Rationale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 F.2. Statistical Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 F.2. Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Appendix - Detailed Results of Masked Heads Ablation Study G.1. Correlation Between the Number of Masked Random Heads and Performance . . . . . . . . . 25 G.2. Faithfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.3. Factuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 G.4. Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Appendix - Ablation with Other LLM Families H.1. Faithfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 H.2. Factuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 H.3. Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 Appendix - Ablation of DeCoRestatic I.1. Faithfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 I.2. Factuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 I.3. Chain-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Appendix - Implementation Details J.1. Hardware and Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 J.2. Baselines Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Appendix - Long Generation Results K.1. Averaged Length-Normalised Conditional Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 K.2. Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A REPRODUCIBILITY STATEMENT",
            "content": "1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? [Yes] We claim to propose novel training-free decoding strategy that leverages retrieval head mechanism, which we present as DeCoRe (Decoding by Contrasting Retrieval Heads). (b) Did you describe the limitations of your work? [Yes] See Section 6. (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix B. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments (e.g. for benchmarks)... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as URL)? [Yes] Our code is available at https://github.com/aryopg/DeCoRe. See details in Appendix for more details. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We mention the implementation details including the hardware, libraries, implementation of the baselines, as well as task-specific setups in Appendix J. We also provide justification of the number of retrieval heads to be masked in Appendix C.2. Additionally, we provide the full ablation study results of different number of retrieval heads in Appendix G. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We reported error bars for experiments requiring multiple runs (i.e., masking random heads in Figure 6 and Figure 8, along with their accompanying tables). (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix J.1. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Section 3 (b) Did you mention the license of the assets? [N/A] All used assets are open-source. (c) Did you include any new assets either in the supplemental material or as URL? [Yes] Our code is available at https://github.com/aryopg/DeCoRe. (d) Did you discuss whether and how consent was obtained from people whose data youre using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]"
        },
        {
            "title": "B ETHICS STATEMENT",
            "content": "Our proposed method, DeCoRe, aims to mitigate hallucinations in LLMs, particularly in tasks where contextual faithfulness are critical. By improving the reliability of LLMs, DeCoRe has the potential to reduce the risks associated with incorrect or misleading information generation."
        },
        {
            "title": "Preprint",
            "content": "(a) Meta-Llama-3-8B. (b) Meta-Llama-3-8B-Instruct. (c) Meta-Llama-3-70B-Instruct. (d) Mistral-7B-Instruct-v0.3. (e) Qwen2-7B-Instruct. Figure 5: Retrieval scores of the Retrieval Heads with non-zero retrieval scores. Despite these positive intentions, there is potential for DeCoRe to be misused. For example, its ability to suppress contextual hallucinations may be exploited to generate more convincing but misleading content by providing it with factually incorrect or unverified contextual documents. To mitigate these risks, we have open-sourced our implementation to facilitate broader scrutiny from the community. Furthermore, we recommend that DeCoRe be applied with caution in sensitive domains where even small inaccuracies may have significant consequences, such as clinical and legal domains."
        },
        {
            "title": "C RETRIEVAL HEADS",
            "content": "C.1 EXTRACTION OF RETRIEVAL HEADS We follow the procedure provided by Wu et al. (2024)4 which defines the retrieval score of attention heads as the ratio of successful copy-paste operations. They propose to calculate the retrieval score by compiling three sets of Needle-in-a-Haystack samples (Kamradt, 2023). Given question and its corresponding answer (the needle), we insert in given context (the haystack) at random position index range iq. The language model is then tasked with answering based on the haystack with the inserted needle. We set and unique and irrelevant with the given long context, ensuring that if an answer is correctly generated, it is indeed copied from the context, not from the models internal knowledge. Retrieval score of head is defined as: retrieval scoreh = gh k Where gh is the set of tokens copy-pasted by head h. Retrieval score signifies the attention head ability to recall tokens from the given context, and can be used as metric to identify retrieval heads in transformer-based LLMs. C.2 RETRIEVAL SCORES As shown in Figure 5, the retrieval scores for each model follow similar pattern across all examined LLM variants. According to Wu et al. (2024), an attention head can be considered retrieval head if it performs copy-paste operation at least 10% of the time, which corresponds to retrieval score of 0.1. In all the models evaluated, the retrieval scores drop below 0.1 just before reaching the 50th retrieval head. This indicates that beyond this number, the attention heads may not be reliably 4https://github.com/nightdessert/Retrieval_Head"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Retrieval Scores of the Retrieval Heads of each model. Retrieval Head ID Meta-Llama-3-8B Meta-Llama-3-8B-Instruct Meta-Llama-3-70B-Instruct Mistral-7B-Instruct-v0.3 Qwen2-7B-Instruct 1 10 20 30 40 50 60 70 80 90 100 0.9341 0.4666 0.2927 0.1347 0.1074 0.0881 0.0735 0.0623 0.0572 0.0491 0.0433 0.9447 0.4421 0.2743 0.1421 0.1131 0.0916 0.0751 0.0659 0.0604 0.0513 0.0452 0.9172 0.3844 0.1874 0.1310 0.1112 0.0914 0.0867 0.0814 0.0630 0.0571 0. 0.8741 0.3167 0.1951 0.1457 0.1115 0.0944 0.0852 0.0751 0.0704 0.0641 0.0538 0.7746 0.3487 0.1986 0.1243 0.1077 0.0843 0.0703 0.0620 0.0524 0.0412 0.0352 Table 5: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on faithfulness evaluation tasks. Model Masked Retrieval Heads XSum MemoTrap IFEval NQ-Open NQ-Swap Llama3-8B-Instruct 0 (Baseline) 10 20 30 40 50 60 70 80 90 ROUGE-L BERTScore-F1 factKB Macro Acc Micro Acc Prompt Acc Instruct Acc EM 19.90 20.51 20.52 20.21 19.92 20.05 20.05 19.42 19.13 19.46 19. 67.23 67.33 67.07 66.49 66.24 66.47 66.54 66.14 64.53 64.39 62.47 47.61 36.56 34.89 29.70 26.72 25.97 23.33 24.55 22.40 21.12 17.13 65.86 66.76 64.44 65.92 66.83 68.08 68.49 67.88 64.72 63.77 60. 64.40 65.89 63.96 64.12 64.83 67.07 67.03 65.89 62.23 61.28 56.95 70.24 62.66 63.77 61.74 58.41 55.08 55.27 56.01 55.08 54.16 47.50 78.30 72.90 73.74 72.54 68.94 66.91 67.15 68.23 67.63 66.55 59. 69.68 64.26 62.30 63.24 62.79 62.49 62.90 63.01 60.45 57.97 56.61 EM 60.62 42.92 43.57 46.48 46.73 44.77 44.23 46.97 43.62 40.77 39.02 performing retrieval tasks. Table 4 provides the precise retrieval scores for selected heads in each model. To ensure the robustness of our experiments, we extended the masking of retrieval heads up to the 100th retrieval head for each model, even though the data suggest that heads beyond the 50th have minimal retrieval ability. This conservative approach ensures that we comprehensively account for all potential retrieval heads during the contrastive decoding process."
        },
        {
            "title": "D PERFORMANCE OF BASELINE MODEL WITH MASKED HEADS",
            "content": "D.1 RATIONALE DeCoRe operates under the assumption that masking retrieval heads would cause hallucinations in LLMs. Therefore, the expected behaviour is that the performance of the LLM would go down the more retrieval heads that are masked. D.2 FAITHFULNESS Figure 6a illustrates the contrasting effects of masking retrieval heads (blue) and random heads (orange) on faithfulness evaluation tasks across XSum, MemoTrap, open-book NQ, and NQ-Swap. In XSum, masking retrieval heads results in sharp decline in factKB scores (rret = 0.93), indicating the critical role of retrieval heads in maintaining factual consistency in summarisation. Table 6: Performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on faithfulness evaluation tasks. Model Masked Retrieval Heads Llama3-8B-Instruct 0 (Baseline) 10 20 30 40 50 60 70 80 90 100 XSum MemoTrap IFEval NQ-Open NQ-Swap ROUGE-L BERTScore-F1 factKB Macro Acc Micro Acc Prompt Acc Instruct Acc 19.90 20.09 0.21 20.00 0.15 19.87 0.18 19.63 0.09 19.59 0.19 19.28 0.77 19.48 0.53 18.96 0.94 17.55 1.19 17.13 1.17 67.23 67.07 0.32 66.80 0.46 66.61 0.89 66.55 1.12 66.34 1.23 66.02 1.52 65.81 1.67 64.92 0.94 61.85 4.91 61.61 6.05 47. 65.86 64.40 70.24 78.30 44.52 4.86 40.77 5.98 36.65 11.64 35.09 14.85 32.25 14.71 31.67 12.94 27.20 12.83 26.02 13.42 28.00 13.27 28.46 9.30 66.79 2.11 67.89 3.24 66.88 2.66 66.29 2.05 67.59 2.09 67.85 0.80 68.33 4.57 69.66 6.45 73.39 4.35 74.65 3. 65.16 2.61 66.54 4.43 65.29 3.71 63.83 3.39 64.76 3.84 63.99 1.09 64.51 4.95 66.40 7.16 70.71 4.93 72.02 4.25 68.64 0.77 69.50 0.93 68.27 1.36 67.59 1.34 66.23 1.98 62.97 2.82 60.87 4.41 56.87 4.16 50.96 10.71 48.92 8.04 77.14 0.39 77.66 0.68 76.58 1.45 75.86 1.20 75.18 1.26 72.30 3.11 70.74 3.47 66.79 2.98 62.39 9.58 60.67 7.43 EM 69.68 69.45 0.46 68.94 0.81 69.18 0.66 68.78 1.19 68.57 0.80 68.10 1.04 67.85 1.04 67.08 1.21 66.53 0.49 66.54 0. EM 60.62 61.39 0.24 60.67 2.08 60.70 2.87 57.19 6.92 57.21 5.62 55.97 3.79 55.00 3.48 54.59 5.23 54.26 5.17 54.71 5."
        },
        {
            "title": "Preprint",
            "content": "(a) Faithfulness Evaluation Tasks (b) Factuality Evaluation Tasks (c) Chain-of-Thought Reasoning Evaluation Tasks Figure 6: Correlation between the number of masked retrieval heads or random heads and performance of Llama3-8B-Instruct with DeCoRe entropy on faithfulness (a), factuality (b), and Chain-ofThought reasoning (c) evaluation tasks. The correlations are quantified by the Pearson Correlation Coefficient for each plot. Detailed results are listed in Table 5, Table 6, Table 7, Table 8, Table 9, and Table 10."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on factuality evaluation tasks. Model Masked Retrieval Heads Llama3-8B-Instruct Baseline 10 20 30 40 50 60 70 80 90 TruthfulQA (MC) TriviaQA PopQA NQ-Open MC1 MC2 MC3 EM EM EM 39.41 39.17 40.27 40.51 41.49 41.00 39.29 38.80 36.23 35.86 36.47 55.69 57.40 59.37 60.51 61.11 61.31 59.32 59.27 57.71 56.63 57.39 30.31 31.57 33.24 33.30 34.00 33.63 32.48 32.47 30.64 30.17 31. 56.58 55.77 55.26 55.39 54.99 54.32 54.05 54.01 53.92 52.89 52.56 26.64 25.84 25.39 25.32 25.35 25.04 24.47 24.52 24.19 23.51 23.30 29.04 28.81 28.93 29.42 28.51 27.91 27.50 27.76 27.31 26.18 26. Table 8: Performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on factuality evaluation tasks. Model Masked Retrieval Heads Llama3-8B-Instruct Baseline 10 20 30 40 50 60 70 80 90 TruthfulQA (MC) TriviaQA PopQA NQ-Open MC1 39. 38.84 0.71 38.51 0.35 37.58 1.12 37.37 0.57 37.17 1.56 35.86 1.41 34.68 0.31 33.05 2.36 30.80 2.20 30.07 0.90 MC2 55.69 55.79 0.53 56.09 2.21 56.47 2.30 57.00 1.94 56.70 2.36 55.37 0.82 53.87 1.16 53.12 2.02 49.78 2.91 49.78 1.74 MC3 30. 30.38 0.46 30.34 0.86 30.21 1.01 30.24 0.51 29.85 1.58 28.87 0.80 27.63 0.66 26.56 2.03 24.79 1.56 24.44 0.76 EM 56.58 56.17 0.03 55.75 0.33 54.84 0.58 54.14 0.65 53.17 1.22 52.43 1.77 51.79 1.59 48.11 5.82 47.39 5.68 47.04 5.17 EM 21. 25.96 0.18 25.63 0.25 25.52 0.16 25.24 0.15 25.07 0.22 24.54 0.54 24.50 0.58 24.52 1.01 24.14 0.98 24.05 0.76 EM 29.04 29.27 0.10 28.89 0.46 28.03 0.20 27.51 0.61 26.61 1.14 26.26 1.14 25.70 1.07 24.36 1.83 24.05 2.03 23.96 1.84 Masking random heads also causes gradual decline (rrandom = 0.94), however, the variance is high which suggests that retrieval heads are more important for contextual faithfulness. For MemoTrap, masking retrieval heads shows moderate correlation with the macro-averaged accuracy (rret = 0.43), while masking random heads surprisingly improves performance (rrandom = 0.84). This implies that retrieval heads are essential for instruction-following, while random heads may not play as crucial role and can even hinder performance. In NQ Open and NQ Swap, the EM score drop significantly when retrieval heads are masked with strong correlation score (rret = 0.86 and rret = 0.64), confirming their importance in openbook QA tasks. In both tasks, masking random heads also degrades performance, with stronger negative correlation (rrandom = 0.97 and rrandom = 0.94 respectively). Despite the more significant performance drop when masking retrieval heads, the correlation coefficient is lower than that for random heads. This is due to the concentrated decline in performance after masking the top 10 retrieval heads. In contrast, performance degrades more gradually when random heads are masked, resulting in stronger linear correlation. This pattern suggests that masking just the top retrieval heads can already significantly impair the models ability to remain faithful to the context. Additionally, the more retrieval heads that are masked, the greater the performance drop, indicating that retrieval heads play key role in maintaining task-specific faithfulness. D.3 FACTUALITY Figure 6b shows the effect of masking retrieval heads (blue) and random heads (orange) on factual recall tasks across TruthfulQA, TriviaQA, PopQA, and NQ Closed. In TruthfulQA, masking retrieval heads has negligible effect on the MC2 score (rret = 0.06), while masking random heads shows moderate negative correlation (rrandom = 0.80). This suggests that retrieval heads do not play major role in answering truthful questions, and the decline"
        },
        {
            "title": "Preprint",
            "content": "Table 9: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on MuSiQue, multi-hop reasoning dataset, with and without CoT prompting in both closedbook and open-book settings. Model Masked Retrieval Heads MuSiQue without CoT MuSiQue with CoT Closed Book Open Book Closed Book Open Book Llama3-8B-Instruct Baseline 10 20 30 40 50 60 70 80 90 100 7.41 6.99 6.91 6.74 6.33 6.29 6.33 6.41 6.41 5.54 5.63 58. 51.47 49.52 46.96 47.41 46.67 46.01 46.46 44.81 41.25 38.85 14.61 14.56 15.06 12.16 11.54 13.24 10.72 11.38 8.98 7.24 7.32 69.84 59.87 57.92 50.48 48.70 47.37 41.79 43.65 32.19 27.06 23.34 Table 10: Performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on MuSiQue, multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. Model Masked Random Heads MuSiQue without CoT MuSiQue with CoT Closed Book Open Book Closed Book Open Book Baseline 7.41 58.83 14.61 69.84 Llama3-8B-Instruct 10 20 30 40 50 60 70 80 90 7.09 0.24 7.17 0.10 6.90 0.19 6.61 0.02 6.08 0.41 5.76 0.77 5.43 0.80 5.27 0.77 5.46 0.72 5.25 0.46 59.25 0.53 58.67 0.68 57.23 1.32 55.83 2.82 55.65 3.12 54.64 3.36 53.28 3.66 52.19 2.95 49.25 4.41 48.34 5.71 14.63 0.35 14.44 0.68 14.09 1.30 13.57 1.09 12.84 1.10 12.49 1.06 11.20 1.34 10.22 0.49 8.14 1.92 7.43 2.04 69.70 1.81 67.94 0.81 67.19 2.42 64.27 4.28 64.87 2.34 63.65 2.38 61.40 3.96 55.98 3.28 46.59 8.97 44.79 9.19 in performance when masking random heads could be due to their broader influence on the models general predictive capabilities. In contrast, for TriviaQA, PopQA, and NQ Closed, both masking retrieval heads and random heads result in significant performance drops, with strong negative correlations observed in all tasks. The differences between masking the retrieval heads and random heads are not as stark as in faithfulness tasks. For instance, in TriviaQA, masking retrieval heads leads to performance decline (rret = 0.98), but masking random heads also has similar effect (rrandom = 0.97). This similarity suggests that in factual recall tasks, retrieval heads may not be the only determining factor. The overall observation from these tasks is that while masking retrieval heads does lower performance, it does not have as drastic an effect as observed in faithfulness hallucination tasks. The relatively similar progression of performance degradation between masking retrieval and random heads further reinforces the idea that factual recall tasks rely on broader mechanism, even though the masking of retrieval heads does lead to moderate drop in performance. D.4 CHAIN-OF-THOUGHT The performance of the Llama3-8B-Instruct model with different numbers of masked retrieval heads on the MuSiQue dataset, both with and without Chain-of-Thought (CoT) prompting, is shown in Figure 6c. The table compares the closed-book and open-book settings to assess the influence of CoT on model performance. In the closed-book setting without CoT prompting, masking retrieval heads leads to gradual performance decline, with scores decreasing from 7.41 (baseline) to 5.63 (with 100 masked heads). This indicates that the models ability to reason through multiple hops is compromised as retrieval heads are removed. The decline of performance in the open-book setting without CoT prompting further indicates the importance of retrieval heads in open-book QA tasks. The inclusion of CoT prompts generally boosts performance in both closed-book and open-book settings. Similar to the setup without CoT prompting, masking retrieval heads in the CoT setup decreases the performance gradually. Interestingly, in the CoT + open-book setup, masking only"
        },
        {
            "title": "Preprint",
            "content": "the top 20 retrieval heads leads to performance lower than without using CoT. This suggests that retrieval heads are crucial for maintaining the models ability to chain reasoning steps across multiple hops, particularly when the reasoning steps have to be grounded in contextual knowledge."
        },
        {
            "title": "E ADDITIONAL TRUTHFULQA GENERATION EVALUATION",
            "content": "E.1 EVALUATION OF NON-REJECTION RESPONSES Table 11: TruthfulQA Generation Evaluation excluding the rejected instances. Notice the rate of rejection that is very high on the instruction-tuned Llama3-8b. Model %Reject %T %I %T Llama3-8b-Instruct + ITI (Li et al., 2024b) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRe static (Ours) + DeCoRe entropy (Ours) Llama3-70b-Instruct + CD (Li et al., 2023) + ITI (Li et al., 2024b) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRe static (Ours) + DeCoRe entropy (Ours) + DeCoRe entropy-small amateur (Ours) 43.94 25.46 45.04 44.92 43.82 41.74 38.68 53.12 52.26 37.94 52.88 54.71 49.33 54.96 56.79 52.02 65.50 83.25 64.81 65.11 65.14 67.02 65.87 76.50 75.64 71.79 76.62 76.22 75.36 74.46 75.35 75.77 94.54 96.06 94.65 93.78 94.55 95.38 95.61 97.91 97.69 98.82 97.92 97.30 98.31 97.01 96.32 97. 60.04 79.47 59.69 58.89 59.69 62.39 61.48 74.41 73.33 70.81 74.55 73.51 73.67 71.47 71.67 73.47 As shown in Table 2, we can observe that the rejection rate of Llama3 models in the TruthfulQA task (i.e., the ratio of cases when the model answers with have no comment) is relatively high, particularly when compared to Llama2 models (Touvron et al., 2023) reported by previous studies (Li et al., 2024b; Chuang et al., 2023). To get better understanding of how the model performs, we also reported the evaluation metrics that are based only on non-rejection answers in Table 11. This results can help us to roughly understand how the model would perform when its not rejecting to answer. However, it is important to note that we cannot compare the performance of the decoding strategies to one another because the set of questions that are being answered are different depending on whether the decoding strategy choose to answer them or not. E.2 EVALUATION COST The fine-tuning of two davinci-002 models (to measure truthfulness and informativeness) costs approximately $43. While each run of evaluation is approximately $0.8. CORRELATION BETWEEN LENGTH-NORMALISED ENTROPY AND"
        },
        {
            "title": "CORRECTNESS",
            "content": "F.1 RATIONALE One motivation to use the length-normalised entropy as measure of how much information to contrast relies heavily on the premise that length-normalised entropy is reliable proxy of answer correctness. To verify this assumption, we conducted statistical tests (Students T-test (Student, 1908) and Mann-Whitney U-test (Mann & Whitney, 1947)) and to determine whether the lengthnormalised entropy of correct answers tends to be lower than that of incorrect answers. F.2 STATISTICAL TESTS The results of these statistical tests, as presented in Table 13, show that the differences in entropy between correct and incorrect answers are statistically significant across all models, with low p-"
        },
        {
            "title": "Preprint",
            "content": "(a) Density plot showing the distribution of length-normalised entropy for correct and incorrect answers across different models (DeCoRe, Baseline, and DoLa). (b) Regression plot demonstrating the negative correlation between length-normalised entropy and answer correctness. Figure 7: Relation between length-normalised entropy and correctness in MuSiQue CoT generation. Entropy tends to be negatively correlated with the final answer correctness (i.e., the lower the lengthnormalised entropy, the more likely that the answer is correct.). values for both tests. The baseline model yields T-test statistic of 11.75 and p-value of 2.57 1031, confirming that the entropy of correct answers is significantly lower. This trend holds for the DoLa and DeCoRe entropy models, with both tests indicating strong separation between the entropy distributions of correct and incorrect answers. The Mann-Whitney U-test results further corroborate this finding, providing consistent statistics and p-values below 1024 for all models. These results validate the hypothesis that lower length-normalised entropy is meaningful indicator of answer correctness, supporting its use in contrastive decoding through DeCoRe. The accompanying Figure 7a illustrates the distribution of length-normalised entropy for correct and incorrect answers across models (DeCoRe, Baseline, and DoLa). Correct answers (in blue) tend to have lower entropy, whereas incorrect answers (in orange) exhibit higher entropy. This visualisation aligns with the statistical tests, highlighting the difference between correct and incorrect answers based on their entropy values. Table 12: Averaged Length-Normalised Predictive Entropy of the correct and incorrect answer by DeCoRe Entropy. All values are scaled by 102. Lower values indicate less overall uncertainty. Generally, the length-normalised entropy of correct answers is lower than the incorrect ones, indicating the importance of the models certainty in generating correct answer. Table 13: Results of the Students T-test and Mann-Whitney U-test comparing the lengthnormalised entropy of correct and incorrect answers across different models. The low p-values across all models confirm that correct answers generally have lower entropy compared to incorrect ones, validating the use of entropy as proxy for answer correctness. MuSiQue (Closed) MuSiQue (Open) Model Statistics Correct Incorrect 31.74 43. 27.99 33.32 Baseline DoLa DeCoRe entropy 11.75 12.52 11.01 T-test U-test p-value 2.57 1031 3.51 1035 7.43 10 Statistics 4.31 105 4.28 105 4.05 105 p-value 8.36 1026 3.66 1028 3.43 1024 F.3 REGRESSION To further quantify the relationship between length-normalised entropy and answer correctness, we calculated the McFaddens pseudo-R2 (McFadden et al., 1973) for the logistic regression models fitted across the different setups (DeCoRe, Baseline, and DoLa). As shown in the regression plots"
        },
        {
            "title": "Preprint",
            "content": "(a) Faithfulness Evaluation Tasks (b) Factuality Evaluation Tasks (c) Chain-of-Thought Reasoning Evaluation Tasks Figure 8: Correlation between the number of masked random heads and performance of Llama3-8BInstruct with DeCoReentropy on each task. The correlations are quantified by the Pearson Correlation Coefficient for each plot. Detailed results are listed in Table 14 and Table 16. (Figure 7b), all three models demonstrate high pseudo-R2 value of 0.98, indicating strong negative relationship between entropy and correctness. This high pseudo-R2 value suggests that the length-normalised entropy is highly predictive of answer correctness, further validating the use of entropy as reliable proxy for contrasting model outputs."
        },
        {
            "title": "G DETAILED RESULTS OF MASKED HEADS ABLATION STUDY",
            "content": "G.1 EFFECT OF RANDOM HEAD MASKING ON TASK PERFORMANCE OF DECORE As shown in Figure 8, the performance of DeCoReentropy exhibits different patterns when masking random attention heads compared to the targeted masking of retrieval heads in Section 4. key observation is that the standard deviation is much larger across most tasks, indicating higher variability in performance when random heads are masked. This variability indicates that DeCoReentropy cannot benefit only from masking any random attention heads. In XSum, we still observe positive correlation between the number of masked random heads and task performance, though the correlation (r = 0.89) is weaker than that seen when masking retrieval heads. This suggests that masking random heads can still improve contextual faithfulness in summarisation, though the impact is less pronounced especially when considering the highest possible performance achieved by masking random heads. MemoTrap, which exhibits strong positive correlation when masking retrieval heads, now shows weak negative correlation (r = 0.34). This shift implies that random masking does not improve the models instruction-following capabilities, and further suggests that the improvements seen were due to the targeted masking of retrieval heads. This supports the idea that retrieval heads play key role in tasks requiring the faithful execution of instructions."
        },
        {
            "title": "Preprint",
            "content": "Table 14: Ablation study of DeCoRe entropy on faithfulness hallucination tasks with varying numbers of masked retrieval heads. Model Masked Retrieval Heads XSum MemoTrap IFEval NQ-Open NQ-Swap ROUGE-L BERTScore-F1 factKB Macro Acc Micro Acc Prompt Acc Instruct Acc EM EM Llama3-8B-Instruct Llama3-70B-Instruct 0 (Baseline) 10 20 30 40 50 60 70 80 90 100 0 (Baseline) 10 20 30 40 50 60 70 80 90 19.90 19.45 19.61 19.62 19.70 19.37 19.40 19.51 19.40 19.45 19.37 22.41 22.17 22.35 22.03 21.98 21.93 21.84 22.03 21.95 21.93 21.82 67.23 67.08 67.18 67.48 67.42 67.15 67.18 67.30 67.57 67.69 67. 69.77 69.64 69.75 69.51 69.48 69.47 69.44 69.55 69.44 69.40 69.38 47.61 57.50 57.53 59.75 60.65 62.88 64.27 61.32 64.67 66.10 64.78 61.32 62.41 60.72 63.91 64.67 65.13 63.94 62.96 64.62 65.49 65. 65.86 68.81 69.39 70.14 70.46 71.27 71.59 71.90 72.52 74.14 73.53 68.47 69.17 68.58 70.28 71.93 73.75 72.66 71.97 72.81 74.07 73.88 64.40 66.60 68.37 70.50 71.09 71.68 71.76 71.80 72.75 74.87 73. 66.52 67.51 66.64 69.52 72.19 73.41 72.19 71.96 72.47 73.65 73.97 70.24 68.39 67.10 62.11 62.29 61.92 58.60 56.93 59.15 59.89 60.81 77.45 76.34 77.45 78.56 77.45 77.63 78.19 76.52 77.08 77.26 77. 78.30 76.38 75.54 72.30 72.42 72.06 69.54 68.94 70.14 70.74 70.98 84.41 83.57 84.29 84.89 83.81 84.41 84.89 83.69 84.05 83.81 83.81 69.68 70.66 70.24 70.17 69.83 69.94 69.57 68.51 68.55 68.66 69. 71.07 71.75 71.83 72.35 72.32 72.54 72.24 72.43 72.66 72.39 72.47 60.62 66.08 65.55 65.15 64.96 64.75 64.41 61.53 62.75 62.64 63.93 76.11 78.36 77.86 79.10 78.91 79.14 77.79 77.62 79.73 79.73 79. Table 15: Ablation study of DeCoRe entropy on faithfulness hallucination tasks with varying numbers of masked random heads. Model Masked Random Heads Llama3-8B-Instruct 0 (Baseline) 10 20 30 40 50 60 70 80 90 XSum MemoTrap IFEval NQ-Open NQ-Swap ROUGE-L BERTScore-F1 factKB Macro Acc Micro Acc Prompt Acc Instruct Acc 19.90 20.02 0.12 20.09 0.26 20.06 0.11 20.07 0.23 20.08 0.36 20.09 0.47 19.83 0.47 19.71 0.44 19.75 0.34 19.68 0.45 67.23 47. 65.86 64.40 70.24 78.30 67.43 0.31 67.64 0.37 67.78 0.53 67.76 0.54 67.89 0.50 67.99 0.61 67.96 0.54 67.85 0.49 67.78 0.52 67.82 0.50 51.39 5.67 54.13 5.85 56.00 7.34 56.78 9.68 57.37 8.45 57.87 6.37 60.16 6.49 60.00 5.13 59.04 4.80 59.03 3. 69.38 2.70 68.22 4.61 69.29 3.91 71.09 0.71 69.69 2.14 70.52 1.89 70.96 2.19 69.47 1.68 66.91 2.68 67.27 2.01 68.08 2.75 66.68 5.76 68.77 4.88 70.72 1.56 69.07 3.18 70.17 1.18 70.76 1.90 68.94 0.94 66.63 3.58 66.76 2.80 68.52 0.75 65.31 1.49 64.76 1.87 64.94 1.34 64.08 1.99 60.51 2.63 60.14 0.21 58.96 1.44 59.64 1.20 59.02 1.23 76.82 0.82 74.46 0.95 74.26 1.63 74.38 1.39 73.78 1.80 70.78 1.92 70.90 0.42 69.46 1.23 69.94 0.45 69.62 1.08 EM 69. 69.27 0.24 69.30 0.66 69.11 0.49 69.23 0.60 69.13 0.53 69.23 0.56 69.19 0.33 68.76 0.36 68.59 0.59 68.15 0.76 EM 60.62 59.65 0.47 59.49 1.93 58.91 2.61 61.23 5.48 61.33 4.92 62.23 2.77 62.03 3.23 60.89 5.05 59.62 5.86 59.27 5.37 Similar to the results of masking retrieval heads, random head masking exhibits negative correlation on IFEval Interestingly, Open Book NQ continues to show strong negative correlation (r = 0.82), much like in the previous section. This reinforces the idea that retrieval mechanisms when handling openbook QA tasks, where the model must balance contextual and parametric knowledge, differ from simple induction mechanism. In contrast, NQ-Swap and TruthfulQA show little to no correlation, indicating that masking random heads does not significantly impact performance on these tasks. For factual recall tasks like TriviaQA, PopQA, and Closed Book NQ, the results are consistent with the previous section, showing strong negative correlations with increasing numbers of masked random heads. As the performance trends of masking retrieval heads and random heads are similar, this may further support the hypothesis that factual recall is not predominantly handled by attention heads. This finding aligns with previous studies (Geva et al., 2021; Meng et al., 2022), which suggest that factual recall is predominantly handled by the MLP layer within the Transformer model. G.2 FAITHFULNESS Table 14 accompanies Figure 3 (top) and Table 15 accompanies Figure 8 (top). In the case of masking retrieval heads in DeCoRe entropy  (Table 14)  , the results show different trends depending on the type of the task. In summarisation (XSum) and instruction following (MemoTrap) tasks, we can observe an increase in performance the more retrieval heads are masked. This indicates the importance of retrieval heads in these tasks, similar to the findings mentioned in Appendix D.2. However, the results show different trend in open-book QA tasks (Open Book NQ-Open and NQSwap). In both Open Book NQ-Open and NQ-Swap, we can observe an increase in performance starting from masking 10 retrieval heads, and gradually goes down. In the case of Open Book NQ-Open, the performance is above the baseline variant until it drops below it when we mask 60 retrieval heads. While in the case of NQ-Swap, the performance remains above the baseline model"
        },
        {
            "title": "Preprint",
            "content": "Table 16: Ablation study of DeCoRe entropy on factuality hallucination tasks with varying numbers of masked retrieval heads. Model Masked Retrieval Heads TruthfulQA (MC) TriviaQA PopQA NQ-Open MC1 MC2 MC3 EM EM EM Llama3-8B-Instruct Llama3-70B-Instruct Baseline 10 20 30 40 50 60 70 80 90 100 Baseline 10 20 30 40 50 60 70 80 90 100 39.41 37.45 36.96 37.58 36.23 37.70 37.21 36.96 38.43 37.70 36.60 49. 49.94 50.31 50.43 50.80 52.14 52.88 53.98 53.61 52.88 54.10 55.69 53.76 54.46 53.76 53.62 54.66 54.50 55.05 55.86 55.32 54.10 70.60 70.66 70.93 71.76 71.54 72.17 72.45 73.44 72.98 72.61 72.96 30. 28.48 28.95 29.38 29.34 29.82 30.21 30.35 30.95 30.30 29.61 37.85 38.11 38.35 39.65 39.33 40.36 41.64 42.55 41.79 41.71 42.86 56.58 56.40 56.18 55.14 54.73 53.99 53.72 52.84 52.19 52.29 52.21 74. 74.75 74.67 74.57 74.58 74.72 74.51 74.61 74.65 74.60 74.64 26.64 26.88 26.74 26.28 25.97 25.55 25.39 24.99 24.76 24.85 25.09 40.63 40.58 40.46 40.51 40.49 40.44 40.30 40.38 40.49 40.58 40.49 29. 28.96 28.55 27.42 27.91 27.27 27.01 26.44 26.44 26.70 26.55 40.08 40.30 40.23 40.11 40.08 40.15 40.26 40.45 40.30 40.38 40.45 Table 17: Ablation study of DeCoRe entropy on factuality hallucination tasks with varying numbers of masked random heads. Model Masked Random Heads Llama3-8B-Instruct Baseline 10 20 30 40 50 60 70 80 90 100 TruthfulQA (MC) TriviaQA PopQA NQ-Open MC1 39.41 38.92 0.53 39.25 0.62 39.41 1.28 38.84 0.75 38.76 0.35 38.31 0.65 38.68 0.92 37.58 0.65 38.39 2.22 38.23 2.70 MC2 55. 56.15 0.78 56.55 2.07 56.43 2.33 55.32 1.85 54.97 1.43 54.45 0.82 55.31 0.98 55.19 1.65 56.48 3.06 56.66 3.77 MC3 30.31 30.22 0.28 30.93 0.85 31.10 1.26 30.39 1.03 30.37 1.05 29.89 0.92 30.74 1.26 30.05 0.45 30.82 2.20 31.03 2.72 EM 56. 55.38 0.45 54.68 0.68 54.15 0.73 53.58 0.59 53.38 0.80 53.04 0.72 52.79 0.60 52.52 0.84 52.13 0.28 51.60 0.35 EM 26.64 25.96 0.18 25.63 0.25 25.52 0.16 25.27 0.17 25.07 0.22 24.54 0.54 24.50 0.58 24.52 1.01 24.14 0.98 24.05 0.76 EM 29. 28.70 0.57 28.02 0.53 27.86 0.32 27.16 0.33 27.16 0.31 27.12 0.26 26.78 0.13 26.87 0.21 26.74 0.33 26.43 0.51 even after we mask 100 retrieval heads. Albeit the differing trend, these open-book QA results are still in line with the previous findings in Appendix D.2, where the top 10 retrieval heads plays the most important role in the open-book QA tasks, with decreasing importance thereafter. In contrast, we can observe massive standard deviation in the results of masking random heads in DeCoRe entropy shown in Table 15. This variance suggests that randomly masking heads leads to inconsistent effects across tasks, implying that not all attention heads contribute equally to model performance. The less predictable effects of masking random heads further highlights the specialised role of retrieval heads in DeCoRe, particularly in maintaining task-specific faithfulness. G.3 FACTUALITY Table 16 accompanies Figure 3 (bottom) and Table 17 accompanies Figure 8 (bottom). As shown in Table 16, the results in TruthfulQA shows less clear correlation compared to other factuality evaluation tasks. For closed-book QA tasks like TriviaQA, PopQA, and Closed Book NQ-Open, negative correlation is observed between the number of masked retrieval heads and performance. Similar negative correlations are observed when random heads are masked as shown in Table 17. The similarity in the performance degradation across both retrieval and random heads indicates that other model mechanisms might be responsible for factual recall. G.4 CHAIN OF THOUGHT Table 18 accompanies Table 3 to show the performance of DeCoRe entropy when masking retrieval heads across different setups of MuSiQue, multi-hop reasoning dataset, with and without CoT prompting, in both closed-book and open-book settings."
        },
        {
            "title": "Preprint",
            "content": "Table 18: Performance comparison across different number of masked retrieval heads on MuSiQue, multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. Model Masked Retrieval Heads MuSiQue without CoT MuSiQue with CoT Closed Book Open Book Closed Book Open Book Llama3-8B-Instruct Llama3-70B-Instruct Baseline 10 20 30 40 50 60 70 80 90 100 Baseline 10 20 30 40 50 60 70 80 90 7.41 7.61 7.70 7.70 7.03 7.12 6.50 6.21 5.75 6.04 6.45 11.79 11.75 11.67 11.50 11.63 11.34 11.34 11.34 11.25 11.38 11.25 58.83 61.98 61.81 61.44 61.32 61.32 60.36 59.21 58.05 59.54 59. 68.56 69.22 69.05 68.97 69.05 69.38 68.68 69.38 69.67 69.51 69.84 14.61 13.90 13.82 13.61 13.03 12.78 13.03 12.83 12.29 12.49 11.96 20.15 20.60 20.02 20.31 20.23 20.02 19.69 19.40 19.28 19.53 19. 69.84 74.47 72.20 71.70 72.16 71.62 72.11 71.66 71.74 70.87 71.00 74.43 74.76 74.56 74.43 74.22 73.60 73.85 74.06 74.18 74.47 74.93 Table 19: Performance comparison across different numbers of masked random heads on MuSiQue, multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. Model Masked Random Heads MuSiQue without CoT MuSiQue with CoT Closed Book Open Book Closed Book Open Book Baseline 7. 58.83 14.61 69.84 Llama3-8B-Instruct 10 20 30 40 50 60 70 80 90 100 6.63 0.17 6.87 0.14 6.65 0.44 6.22 0.42 6.50 0.26 6.36 0.31 6.32 0.06 6.45 0.54 6.55 0.46 6.34 0. 59.21 0.91 59.72 0.70 59.95 0.77 60.52 1.69 60.60 1.46 60.31 1.49 61.03 0.97 61.32 0.50 61.45 1.38 61.76 0.90 13.57 0.91 13.07 0.90 12.61 0.91 12.29 0.40 12.26 0.15 11.81 0.58 12.05 1.06 11.64 0.66 11.65 0.57 11.72 0.27 69.40 1.09 70.18 0.44 70.43 1.47 70.28 2.53 69.41 1.44 68.89 0.95 69.78 1.56 70.05 1.08 70.20 2.17 70.29 2.36 In the closed-book without CoT setup, we can observe negative correlation between the number of masked retrieval heads and the performance. As more retrieval heads are masked, the performance gradually declines from the baseline across the Llama3-8B-Instruct and Llama3-70B-Instruct models, aligned with the findings in Appendix G.3. In the open-book without CoT setup, there is also negative correlation, but interestingly, the overall performance remains higher than the baseline model, which is aligned with the findings in Appendix G.2. Interestingly the results in the closed-book with CoT setup are quite different, as masking retrieval heads does not lead to improved performance. From the results of masking retrieval heads in the baseline model  (Table 9)  , we expect the model to perform better as DeCoRe will contrast the incorrect predictions. This may suggest that the complexity of factual recall in closed-book setup remains the same even though the model is prompted to generate intermediate reasoning steps. Finally, the open-book with CoT setup shows an increase in performance when masking retrieval heads, even though the correlation remains negative. This is consistent with the broader trend observed in the open-book QA setup, where the model benefits from masking retrieval heads but only up to point. Even with the negative correlation, the performance still remains higher than the baseline, indicating the utility of retrieval heads in CoT-assisted open-book tasks."
        },
        {
            "title": "Preprint",
            "content": "Table 20: Performance comparison of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) with different decoding strategies on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. Model Mistral-7B-Instruct-v0.3 + CAD (Shi et al., 2024) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRe static (Ours) + DeCoRe entropy (Ours) Qwen2-7B-Instruct + CAD (Shi et al., 2024) + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRe static (Ours) + DeCoRe entropy (Ours) XSum MemoTrap IFEval NQ-Open NQ-Swap ROUGE-L BERTScore-F1 factKB Macro Acc Micro Acc Prompt Acc Instruct Acc EM EM 16.53 14.71 16.45 16.44 16.58 15.57 15.15 20.00 17.06 19.57 18.69 19.58 18.78 17.09 65.30 63.55 65.24 65.23 65.36 64.20 63.80 67.70 65.08 67.47 66.60 67.66 66.82 64.79 65.53 69.90 65.51 65.70 65.25 71.75 70. 68.66 71.98 65.05 55.71 66.42 75.21 76.90 76.63 - 76.33 76.47 76.80 77.01 77.54 82.13 - 82.76 56.61 81.37 82.50 83.80 75.11 - 74.75 74.91 75.35 76.49 76.96 80.54 - 81.76 55.89 80.03 81.02 82.04 51.02 - 49.54 49.72 51.76 51.94 51. 52.31 - 54.16 47.32 51.76 58.04 54.90 60.91 - 60.19 60.19 62.35 62.47 61.27 62.35 - 65.35 59.59 62.35 67.51 64.03 66.86 65.54 67.01 66.97 66.70 68.02 68.48 68.81 69.30 68.32 65.76 68.14 70.13 70.58 65.17 76.11 65.32 65.21 63.99 68.08 68. 72.90 78.05 72.88 70.48 72.29 75.64 75.31 As shown in Table 19, the trend observed when masking random heads is less apparent in comparison to when masking retrieval heads. This indicates that random heads may not be as critical in these tasks."
        },
        {
            "title": "H ABLATION WITH OTHER LLM FAMILIES",
            "content": "H.1 FAITHFULNESS Table 20 shows the performance of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen27B-Instruct) evaluated across faithfulness tasks with different decoding strategies. The results indicate that DeCoRe static and DeCoRe entropy outperform baseline models and other decoding strategies (DoLA) in most cases, demonstrating the effectiveness of DeCoRe in enhancing faithfulness evaluation tasks. For Mistral-7B-Instruct-v0.3, both DeCoRe static and DeCoRe entropy perform competitively. Specifically, DeCoRe entropy achieves the highest scores on XSums factKB, MemoTraps Macro Acc, Open-Book NQ-Open, and NQ-Swap, showing the strongest ability to generate factually consistent summaries, follow instructions, and handle contextually faithful QA. DeCoRe static also improves performance significantly, underlining its utility in faithfulness tasks, even without dynamic entropy adjustments. For Qwen2-7B-Instruct, DeCoRe entropy also leads in most tasks. It shows top performance on XSums factKB, MemoTrap and Open-Book NQ-Open, indicating that it excels in generating factually consistent summary, following instruction, and answering complex QA questions. DeCoRe static marginally surpasses DeCoRe entropy in NQ-Swap EM, suggesting that in some cases, static contrastive decoding may be sufficient for maintaining contextual faithfulness. Overall, the trend observed across both model families confirms that DeCoRe, whether in static or entropy-controlled mode, provides significant improvements in maintaining contextual faithfulness regardless of the base model family, outperforming traditional decoding strategies like DoLA across summarisation, instruction-following, and QA tasks. H.2 FACTUALITY Table 21 compares the performance of Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct on factuality evaluation tasks using different decoding strategies. For Mistral-7B-Instruct-v0.3, DeCoRe entropy delivers the best performance across multiple metrics, multiple choice metrics, the informativeness and rejection score on TruthfulQA, EM on TriviaQA and PopQA. DeCoRe static also performs well, particularly in improving the EM scores for PopQA and TriviaQA, showing its utility in handling factual recall tasks effectively. Qwen2-7B-Instruct shows similar pattern. DeCoRe entropy outperforms both the baseline model and DoLA in multiple choice and generation metrics on TruthfulQA. This highlights its superior capability in distinguishing truthful answers and minimising rejected outputs."
        },
        {
            "title": "Preprint",
            "content": "Table 21: Performance comparison of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) with different decoding strategies on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. Model TruthfulQA (MC) TriviaQA PopQA TruthfulQA (Generation) NQ-Open MC1 MC2 MC3 EM EM %Truth %Info %T %Reject EM Mistral-7B-Instruct-v0.3 + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRe static (Ours) + DeCoRe entropy (Ours) Qwen2-7B-Instruct + DoLA (low) (Chuang et al., 2023) + DoLA (high) (Chuang et al., 2023) + AD (Chen et al., 2024) + DeCoRe static (Ours) + DeCoRe entropy (Ours) 50.31 50.18 50.18 43.82 53.49 54.84 29.99 30.11 20.44 30.85 31.09 34.52 65.62 65.64 65.61 64.44 67.13 69.08 48.08 49.11 47.09 49.71 48.23 51.79 38.29 38.17 38.18 35.67 39.48 41.82 24.22 25.09 22.76 25.33 25.20 27. 59.99 60.06 60.03 59.92 60.09 59.64 42.77 40.57 37.82 42.13 42.50 41.30 26.65 26.68 26.68 26.66 27.02 27.11 17.55 15.85 13.84 18.19 17.71 17.15 80.54 80.29 80.54 80.29 77.85 76.99 80.78 84.58 83.97 78.09 79.31 76. 97.06 97.31 97.06 97.18 97.43 97.80 67.93 65.36 61.57 79.68 69.28 76.74 77.60 77.60 77.60 77.48 75.40 74.79 48.71 50.06 45.53 57.83 48.59 53.61 26.07 25.70 25.70 25.70 20.81 15.91 37.33 41.74 45.17 26.31 37.33 26. 31.49 31.53 31.53 30.55 31.38 31.45 25.91 23.84 21.36 24.41 26.06 25.05 Table 22: Performance comparison of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) with different decoding strategies on MuSiQue, multi-hop reasoning task. For each base model, the best performance is indicated in bold, and the second-best is underlined. Model Mistral-7B-Instruct-v0.3 + CAD (Shi et al., 2024) + DoLA (low) + AD (Chen et al., 2024) + DeCoRe static + DeCoRe entropy Qwen2-7B-Instruct + CAD (Shi et al., 2024) + DoLA (low) + AD (Chen et al., 2024) + DeCoRe static + DeCoRe entropy MuSiQue without CoT MuSiQue with CoT Closed Book Open Book Closed Book Open Book 7.61 - 7.53 7.53 7.86 7.57 6.54 - 7.03 5.71 6.70 6.16 58.01 50.10 58.21 59.00 59.33 62. 63.01 64.58 65.45 65.29 63.34 66.49 11.17 - 10.92 11.34 12.04 11.21 8.23 - 7.70 8.44 8.36 8.23 59.70 63.55 59.79 61.69 63.92 65.12 60.57 66.41 64.54 65.70 66.78 67.98 Overall, the trend across both model families confirms that DeCoRe, particularly DeCoRe entropy, significantly enhances the models performance beyond just contextual faithfulnes. H.3 CHAIN OF THOUGHT Table 22 presents the performance of Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct on the MuSiQue multi-hop reasoning task across different decoding strategies. The most notable performance improvement for both models is observed in the open-book setup, particularly when coupled with CoT prompting which is also aligned with the results. Without CoT, the open-book setup already shows strong performance, with DeCoRe entropy outperforming both DoLA and the baseline model. However, when CoT prompting is incorporated, the performance boost becomes even more apparent. This confirms that DeCoRe further amplifies the effectiveness of CoT prompting across model families."
        },
        {
            "title": "I ABLATION OF DECORESTATIC",
            "content": "DeCoRestatic uses hyperparameter α to control how much we want to contrast the prediction of the masked model from the base model, as shown in Equation (7). We examine the various values of α and shows the results in Figure 9 across the faithfulness, factuality, and CoT reasoning evaluation tasks. I.1 FAITHFULNESS As shown in Figure 9a and Table 23, for XSum, increasing α leads the highest factKB score up until α = 1.0. MemoTrap tasks show steady improvement in both Macro and Micro Accuracy as α"
        },
        {
            "title": "Preprint",
            "content": "(a) Faithfulness Evaluation Tasks (b) Factuality Evaluation Tasks (c) Chain-of-Thought Reasoning Evaluation Tasks Figure 9: Relation between α and performance metrics of Llama3-8b-Instruct with DeCoRestatic in the faithfulness (a), factuality (b), and Chain-of-Thought reasoning (c) evaluation tasks. Detailed results are listed in Table 23, Table 24, and Table 25. Table 23: Performance of Llama3-8b-Instruct with DeCoRestatic on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. α -0.5 0.0 0.5 1.0 2.0 4.0 8.0 XSum MemoTrap IFEval NQ-Open NQ-Swap ROUGE-L BERTScore-F1 factKB Macro Acc Micro Acc Instruct Acc Prompt Acc EM 20.16 19.90 19.87 19.41 18.38 16.65 13.05 66.42 67.23 67.83 67.83 67.19 65.26 55.65 28.17 47.61 64.07 67.46 64.02 52.61 31. 63.52 65.86 69.53 69.71 71.28 70.77 70.68 60.65 64.40 69.20 70.22 71.84 71.09 70.97 76.98 70.24 69.13 73.74 70.74 51.56 35.01 68.58 78.30 78.06 63.59 59.70 37.52 20.70 68.17 69.68 70.62 70.73 69.64 62.86 43.24 EM 55.75 60.62 64.43 64.88 63.02 54.83 39.97 increases, peaking at α = 2.0. However, for IFEval, higher values of α lead to drop in Instruct and Prompt Accuracy. Similarly, for the Open book NQ-Open and NQ-Swap tasks, performance decreases for extreme values of α. I.2 FACTUALITY Figure 9b and Table 24 show that, for TruthfulQA, the MC2 score improves slightly at higher α values, with the best performance for MC2 at α = 8.0. TriviaQA shows stable EM performance for lower α, but it significantly drops when α increases beyond 4.0. For PopQA and Closed-Book NQ-Open, performance declines as α increases, with the best scores occurring at lower α. I.3 CHAIN OF THOUGHT As shown in Figure 9c and Table 25, the performance of Llama3-8b-Instruct on MuSiQue varies with the choice of α in both closed-book and open-book settings, with and without CoT prompting. Without CoT, performance peaks at α = 0.5 in both settings, but rapidly declines for higher values"
        },
        {
            "title": "Preprint",
            "content": "Table 24: Performance of Llama3-8b-Instruct with DeCoRestatic on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined. α -0.5 0.0 0.5 1.0 2.0 4.0 8.0 TruthfulQA (MC) TriviaQA PopQA NQ-Open MC1 MC2 MC3 EM 38.31 39.41 38.68 38.07 36.84 37.45 37.70 57.05 55.69 55.74 55.86 56.13 57.62 58.37 31.48 30.31 29.80 29.81 30.08 31.43 31.82 56.00 56.58 56.93 56.78 56.47 53.92 43.67 EM 26.09 26.64 26.86 26.87 26.60 24.55 18.66 EM 28.93 29.04 29.42 28.93 28.59 28.14 23.47 Table 25: Performance of Llama3-8b-Instruct with DeCoRestatic on MuSiQue, multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings. For each base model, the best performance is indicated in bold, and the second-best is underlined. α -0.5 0.0 0.5 1.0 2.0 4.0 8. MuSiQue without CoT MuSiQue with CoT Closed Book Open Book Closed Book Open Book 6.95 11.79 11.79 8.27 7.12 4.18 2.52 55.94 68.56 69.76 62.27 60.57 52.92 33.88 14.56 20.15 20.60 14.19 11.67 7.36 5. 66.32 74.43 75.05 72.07 70.09 58.46 31.36 of α. When CoT prompting is applied, accuracy improves across all settings, with the best results also observed at α = 0.5. However, as α increases beyond 1.0, performance deteriorates sharply, particularly at extreme values such as α = 4.0 and α = 8.0. Overall, these patterns show that some tasks may benefit from high α value, while the others may require it to be more constrained, indicating that it is necessary to have dynamic α value throughout the generation. J"
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "J.1 HARDWARE AND LIBRARY We run all the experiments with NVIDIA A100 80GB GPUs. Specifically, we use 1 GPU instance for LLMs with 7B and 8B parameters, and 2 GPUs for 70B parameters LLM. We use the Huggingface Transformers libraries (Wolf et al., 2020) and custom LLM model python classes from Wu et al. (2024) which contains the snippet to mask the attention heads. Our code is available at https://anonymous.4open.science/r/decore-4FB7. J.2 BASELINE IMPLEMENTATION We obtained the fine-tuned weights of ITI models of Llama3-8B-Instruct and Llama3-70B-Instruct from https://huggingface.co/jujipotle/honest_llama3_8B_instruct and https://huggingface.co/jujipotle/honest_llama3_70B_instruct, respectively. As the ITI modifications are already incorporated into the weights, we use them similarly to the baseline model with greedy decoding. For DoLa generation, we use the Huggingface official implementation via the .generate(...) function. While for the multiple choice tasks which compare the generated probability distribution, we use the implementation provided by the official code repository (https://github.com/voidism/DoLa). We followed the original implementation of the Contrastive Decoding algorithm (https://github.com/XiangLi1999/ ContrastiveDecoding). We followed the original implementation of the Activation algorithm (https://github.com/hkust-nlp/Activation_Decoding). Decoding"
        },
        {
            "title": "Preprint",
            "content": "Table 26: Additional experimental setting details for the tasks, including the number of shots and the prompt templates. The double curly braces {{}} signify input data. Metric # of shots Prompt Template Faithfulness Hallucination Task XSum MemoTrap IFEval ROUGE-L BERTScore factKB Macro Accuracy Micro Accuracy Instruction-level Strict Accuracy Prompt-level Strict Accuracy 0 0 1 4 8 8 6 17 17 Generate summary comprising of 1 sentence for the given article.nn Article: + {{document}}nnSummary: {{question}} {{question}} Write high-quality answer for the given question using only the provided search results (some of which might be irrelevant). Provide the answer in 5 words or less without any explanation.nn {{in-context learning demonstrations}}nn Document [{{document index}}] {{document}}n Question: {{question}}n Answer: Answer the following question based on the provided context:nn {{in-context learning demonstrations}}nn Context: {{context}}n Question: {{question}}n Answer: Factuality Hallucination Write high-quality answer for the given question. Provide the answer in 5 words or less without any explanation.nn {{in-context learning demonstrations}}nn Question: {{question}}n Answer: Answer the given question.nn {{in-context learning demonstrations}}nn Question: {{question}}n Answer: Answer the given question.nn {{in-context learning demonstrations}}nn Question: {{question}}n Answer: Interpret each question literally, and as question about the real world; carefully research each answer, without falling prey to any common myths; and reply have no comment unless you are completely certain of the answer.nn {{in-context learning demonstrations}}nn Q: {{question}}n A: Reasoning Answer the following question.nn {{in-context learning demonstrations}}nn Q: {{question}}n Answer: Answer the following question by reasoning step-by-step.nn {{in-context learning demonstrations}}nn Q: {{question}}n Answer: Open Book NQ-Open EM NQ-Swap Closed-Book NQ-Open TriviaQA PopQA TruthfulQA MuSiQue Direct MuSiQue CoT EM EM EM EM MC1 MC2 MC3 GPT-Judge GPT-Info EM EM We followed the original (https://github.com/xhan77/context-aware-decoding). implementation of the Context Aware Decoding algorithm J.3 ADDITIONAL EXPERIMENTAL SETTING DETAILS Table 26 outlines the additional experimental settings for each task, including the evaluation metrics, number of shots (In-Context Learning demonstrations), and corresponding prompt templates. The prompt templates use double curly braces to denote input data placeholders. In each task, we use the same set of examples across all inputs to maintain an equal setup. We adopted examples from prior work and conducted qualitative inspection (Gao et al., 2024; Chuang et al., 2023; Hong et al., 2024; Liu et al., 2024). Specifically for the MuSiQue tasks, we noticed that three examples were not suitable for the intended tasks, as they did not adequately demonstrate multi-hop reasoning (see Table 27)."
        },
        {
            "title": "K LONG GENERATION RESULTS",
            "content": "K.1 AVERAGED LENGTH-NORMALISED CONDITIONAL ENTROPY Table 28 accompanies Figure 4. Refer to Section 4 for the explanation."
        },
        {
            "title": "Preprint",
            "content": "Table 27: Erroneous In-Context Learning Demonstrations of MuSiQue Question Answer is of the genre What record label the performer of So Long, See You Tomorrow associated with? is the genre of What the record label of the band that performed on the Crush Tour? is the Where crying stone found in the country in which Raphael Tuju holds citizenship? The performer of So Long, See You Tomorrow is Bombay Bicycle Club. The record label of Bombay Bicycle Club is Island Records. The genre of Island Records is jazz. So the answer is: jazz. The Crush Tour is performed by the band Bon Jovi. The record label of Bon Jovi is Island Records. The genre of Island Records is jazz. So the answer is: jazz. Raphael Tuju is citizen of Kenya. The crying stone in Kenya is found along the highway towards Kisumu. So the answer is: along the highway towards Kisumu. Reason of Removal record label does not have only one specific genre, as confirmed by Wikipedia search of Island Records. record label does not have only one specific genre, as confirmed by Wikipedia search of Island Records. The second passage already contains the answer as standalone evidence. It does not highlight the multi-hop reasoning. Table 28: Averaged Length-Normalised Conditional Entropy which signifies the averaged overall uncertainty of generated sequences per model. Lower values indicate less overall uncertainty. Bold indicates the lowest value. Model XSum MuSiQue with CoT Closed Open Llama3-8b-Instruct + ITI + DoLa + DeCoRe entropy 0.41 0.12 0.65 0.21 0.41 0.12 0.38 0. 0.30 0.10 0.46 0.18 0.30 0.10 0.29 0.10 0.43 0.20 0.72 0.28 0.43 0.20 0.41 0.20 K.2 QUALITATIVE EXAMPLES XSum Figure 10 presents qualitative comparison between the baseline decoding and DeCoRe entropy generations in the XSum task. Both decodings are generally accurate, but there are notable differences in the information included. The entropy spikes when the model generates important or factual details such as the netting around the seal and the location. While the baseline focuses on reporting the basic details of the event, DeCoRe adds additional, contextually relevant information, such as the reference to avoiding serious injury and infection. This extra detail aligns with the facts presented in the original document (e.g., [...] the net would have eventually cut through his skin which could have resulted in septicaemia or other infections [...]). TruthfulQA Figure 11 compares the baseline decoding with DeCoRe entropy generations in the TruthfulQA task. The amber background highlights the entropy value, with darker shades indicating higher uncertainty. In this example, the baseline model declines to answer the question, providing an uninformative response: have no comment. In contrast, DeCoRe generates much more detailed and accurate answer, correctly refuting the link between the MMR vaccine and autism while also mentioning the discrediting of Wakefields research. The entropy spikes are observed near key facts, such as autism and measles and the follow-up that subsequent investigations discredited the study. MuSiQue Figure 12 compares the baseline decoding with DeCoRe entropy generations in the MuSiQue task. Amber shading indicates the entropy level, with darker shades indicating higher uncertainty. Since MuSiQue is question answering task, we can indicate the correct and incorrect answer by using green and red backgrounds, respectively. Both decoding strategies show similar"
        },
        {
            "title": "Preprint",
            "content": "Article: The grey seal was found at Cruden Bay on Tuesday. The Scottish SPCA removed the seal from beach and took it to the charitys rescue centre where it was cut free. Ch Insp John Carle said: The amount of netting tangled around the seals neck was the largest have ever seen. It was lovely moment watching the seal heading back into the water. He added: He was so large and heavy it took three of our officers to lift him and take him to our centre in Drumoak where we cut the net away. If he hadnt been discovered and freed the net would eventually have cut through his skin which could have resulted in septicaemia or other infections. Thankfully there were no injuries in this case and we were able to release him relatively quickly. This was very happy ending and were so glad we were able to help. grey seal was rescued by the Scottish PCA from beach in Cr uden Bay after it became ent angled in large amount of net ting around its neck , which was eventually cut away and the seal was released back into the water . (a) Baseline. grey seal was found ent angled in massive amounts of net ting at Cr uden Bay , Scotland , and was successfully rescued and cut free by the Scottish PCA , avoiding potential serious injury or infection . (b) DeCoRe. Figure 10: Qualitative comparison of Baseline decoding vs DeCoRe Entropy generations in the XSum task, along with the entropy over time. Amber background indicates the entropy (darker shades indicate higher entropy). Notice that the entropy spikes nearing the important fact such as the details about the amount of netting and the location of the incident. Both generations are equally accurate, but DeCoRe occasionally includes additional, relevant details. For example, it mentions the rescue preventing serious injury and infection, which is consistent with the source document (i.e., [...] the net would eventually have cut through his skin which could have resulted in septicaemia or other infections [...]). Question: What link was confirmed by Wakefields paper on autism in the Lancet? Best Answer: No link between the MMR vaccine and autism was confirmed because the paper was scientific fraud. have no comment . (a) Baseline. Wake field paper on autism in the Lanc et did not confirm link between autism and the measles , umps , and rub ella ( MM ) vaccine . In fact , subsequent investigations and studies have thoroughly dis credited Wake field research and found no such link . (b) DeCoRe. Figure 11: Qualitative comparison of Baseline decoding vs DeCoRe Entropy generations in the TruthfulQA task, along with the entropy over time. Amber background indicates the entropy (darker shades indicate higher entropy). Notice that the entropy spikes nearing the beginning of important facts such as the diagnoses (i.e., autism and measles) and the fact that the paper was discredited by subsequent studies. As noted in Table 2, DeCoRe is more likely to not reject answering the question compared to the baseline models. entropy spikes when generating the names Gilroy and Robert, suggesting uncertainty. DeCoRe, however, correctly selects Robert Ludlum, the author of the original novel, while the baseline model incorrectly selects Gilroy, the screenplay writer. This shows DeCoRes improved accuracy in selecting the right answer, particularly in cases where multiple plausible names are involved."
        },
        {
            "title": "Preprint",
            "content": "Wikipedia Title: The Bourne Ultimatum (film) The Bourne Ultimatum is 2007 action spy film directed by Paul Greengrass loosely based on the novel of the same name by Robert Ludlum. The screenplay was written by Tony Gilroy, Scott Z. Burns and George Nolfi and based on screen story of the novel by Gilroy. Question: What was the story that included the character from The Bourne Betrayal based on? The screenplay for The Bour ne Ult im atum was written by Tony Gil roy , Scott . Burns , and George olf , and was based on screen story of the novel by Gil roy . The screenplay for The Bour ne Ult im atum was written by Tony Gil roy , Scott . Burns , and George olf , and was based on screen story of the novel by Robert Lud lum . (a) Baseline. (b) DeCoRe Entropy. Figure 12: Qualitative comparison of the Baseline decoding vs DeCoRe Entropy generations in the MuSiQue task, along with the entropy over time. Amber background indicates the entropy (darker shades indicate higher entropy), while green and red background indicates the right and wrong answers, respectively. Entropy generally follow the same pattern for the similar generation. Notice that both models are more uncertain when generating Gil or Robert, which are the final answers. Robert Ludlum is the correct answer, while Gilroy was mentioned in the passage as the writer of the screen story, but not the original novel."
        }
    ],
    "affiliations": [
        "Centre for AI, Data Science & Artificial Intelligence, R&D, AstraZeneca, United Kingdom",
        "Miniml.AI, United Kingdom",
        "University College London, United Kingdom",
        "University of Edinburgh, United Kingdom"
    ]
}