{
    "paper_title": "Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models",
    "authors": [
        "Sharut Gupta",
        "Shobhita Sundaram",
        "Chenyu Wang",
        "Stefanie Jegelka",
        "Phillip Isola"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in a target modality? We introduce UML: Unpaired Multimodal Learner, a modality-agnostic training paradigm in which a single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of a shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalities -- such as text, audio, or images -- consistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 2 9 4 8 0 . 0 1 5 2 : r Better Together: Leveraging Unpaired Multimodal Data for Stronger Unimodal Models Sharut Gupta, Shobhita Sundaram, Chenyu Wang, Stefanie Jegelka , Phillip Isola MIT CSAIL, TU Munich {sharut, shobhita, wangchy, stefje, phillipi}@mit.edu Figure 1: Text provides complementary information beyond images, even when not paired directly; We introduce Unpaired Multimodal Learner (Uml) whereby sharing model weights across modalities (e.g., image and text) extracts synergies and enhances unimodal representations, outperforming methods that rely only on single modality (such as images above)."
        },
        {
            "title": "Abstract",
            "content": "Traditional multimodal learners find unified representations for tasks like visual question answering, but rely heavily on paired datasets. However, an overlooked yet potentially powerful question is: can one leverage auxiliary unpaired multimodal data to directly enhance representation learning in target modality? We introduce Uml: Unpaired Multimodal Learner, modality-agnostic training paradigm in which single model alternately processes inputs from different modalities while sharing parameters across them. This design exploits the assumption that different modalities are projections of shared underlying reality, allowing the model to benefit from cross-modal structure without requiring explicit pairs. Theoretically, under linear data-generating assumptions, we show that unpaired auxiliary data can yield representations strictly more informative about the data-generating process than unimodal training. Empirically, we show that using unpaired data from auxiliary modalitiessuch as text, audio, or imagesconsistently improves downstream performance across diverse unimodal targets such as image and audio. Our project page: https://unpaired-multimodal.github.io/"
        },
        {
            "title": "1 Introduction",
            "content": "It is often taken for granted that to model modality well, one must train on data from that modality. For instance, if one wants an accurate image classifier, one trains on images; if one wants language model, one trains on text. Recent advances in multimodal learning, however, suggest that multiple modalities can benefit one another: in particular, using text captions paired with images yields richer representations, often surpassing their unimodal counterparts in zero-shot transfer, cross-modal retrieval, and downstream classification [Radford et al., 2021, Singh et al., 2022, Mizrahi et al., 2023, Girdhar et al., 2023a, Bachmann et al., 2022, Li et al., 2023, Bachmann et al., 2024, Jia et al., 2021]. However, these gains have been realized largely through paired data, where one has access to aligned examples (x, y) pX ,Y , such as an image and its corresponding caption y. Such paired supervision allows aligning modalities into shared latent space, where cross-modal correlations can be captured and transferred to downstream tasks. This reliance on paired corpora also poses bottleneck. Collecting and curating aligned datasets is expensive and domain-limited, whereas unpaired dataindependent samples from pX and pY are naturally abundant. For example, vast image collections and vast text corpora exist independently, but without explicit alignment. This raises more fundamental question: Can unpaired data from secondary modality enhance representations of the target modality X, even in the absence of (x, y) correspondences? There is growing evidence that the answer may be yes. Recent work posits the existence of shared statistical model of reality, an empirical parallel to Platos concept of ideal Forms, suggesting that as deep networks scale, their embeddings across modalities converge toward unified representation of the underlying world [Huh et al., 2024, Huang et al., 2021]. This convergence implies that when paired supervision is available, model can leverage natural co-occurrences between modalities and thus more accurately capture shared semantics. Critically, however, achieving convergence does not necessarily require explicit pairs; as long as each modality samples from the same underlying ground-truth latent space, it may be sufficient to align their marginal distributions to uncover the common semantic structure [Timilsina et al., 2024, Sturma et al., 2023]. Intuitively, even unpaired data from another modality can provide complementary cues to better estimate the underlying reality (Figure 1). In this work, we formalize this idea through Unpaired Multimodal Representation Learning, framework for improving unimodal representations by leveraging unpaired data across modalities. Theoretically, under linear assumptions, we derive conditions under which incorporating unpaired samples yields strictly more informative representations than unimodal training alone. Notably, in some regimes, single sample from yields greater per-sample value than sample from itself when the goal is to model X. Building on these insights, we introduce Uml: Unpaired Multimodal Learner, shared-network framework that applies the same set of parameters to inputs from different modalities. By nothing more than weight sharing, i.e., without surrogate objectives or inferred alignments, the model learns modality-agnostic features and naturally transfers information across modalities [Sutskever, 2023]. Empirically, we evaluate Uml on diverse imagetext tasks in healthcare, and affective computing, as well as 10 standard visual benchmarks. Unpaired data from auxiliary modalities consistently improves unimodal representations across self-supervised and supervised regimes, in both few-shot and full-data regimes, and with diverse encoders including CLIP, DINOv2, OpenLLaMA, and others. These benefits compound when moving from two to three modalities, with audio, vision, and text each adding complementary signals. We further demonstrate effective cross-modal transfer without paired data by initializing vision models with pretrained language-model weights. Finally, we quantify the exchange rate between modalities, mapping how many words equate to one image (and vice versa) for optimal performance. To summarize, the key contributions of our work are: 2 We introduce Uml, modality-agnostic framework that leverages unpaired data to improve unimodal models. We test it across self-supervised and supervised encoders (CLIP, DINOv2, OpenLLaMA, and others), in both few-shot and full-data regimes, showing consistent gains over range of image-text benchmarks and extensions to audio. We theoretically characterize, under linear assumptions, the conditions where unpaired data yield strictly more informative representations than unimodal training. Remarkably, the conversion ratio between modalities can fall below one: in certain regimes, single sample from contributes more to modeling than an additional sample from itself. We quantify conversion ratios between images and text; i.e., how many words is an image worth for training vision models and show that unpaired multimodal data systematically widens inter-class margins and aligns modalities in weights. We further demonstrate that individual neurons develop strong cross-modal coupling, revealing multimodal neurons that respond coherently across vision and text, all without any paired supervision."
        },
        {
            "title": "2 Paired Multimodal Representation Learning",
            "content": "A useful way to conceptualize learning across modalities is to posit shared ground-truth reality, denoted , which manifests through multiple projections such as images, text, or audio recordings [Huh et al., 2024, Timilsina et al., 2024, Sturma et al., 2023]. The goal of representation learning is to learn an embedding space that captures the structure of , whether from single modality or from several jointly. Thus, unimodal representations are inherently limited by what one projection alone can reveal: single camera view may contain occlusions; an audio recording lacks visual details; textual descriptions may lack layout information. These limitations motivate multimodal representation learning, which extends the unimodal setting to learn from multiple modalities together. By integrating heterogeneous projections, multimodal learning leverages their interplay to recover more complete view of the shared latent reality. For brevity, consider two modalities with observable data denoted by (e.g., images) and (e.g., text) and samples and Y. Mathematically, Multimodal representation learning seeks encoders fX : and fY : mapping each modality into shared embedding space Z. Different frameworks optimize for desirable properties of Z. Contrastive methods encourage instances of the same concept (such as an image and its text label) to be close together [Radford et al., 2021, Zhai et al., 2023, Jia et al., 2021]. Fusion approaches learn shared layers for multimodal inputs with reconstruction [Singh et al., 2022, Li et al., 2019, Lu et al., 2019, Bachmann et al., 2022] or contrastive objectives [Roy et al., 2025]. Generative methods often structure to enable translation between modalities[Li et al., 2022, Rombach et al., 2022]. Other methods disentangle common factors from modality-specific factors [Wang et al., 2024, Liang et al., 2023]. Common to all of these prior work, however, is requirement for pre-existing knowledge of one-to-one correspondences between the modalities, i.e., samples (x, y) PX,Y (as shown in Figure 2(a)), which are assumed to be aligned views of the same underlying entity in . We ask whether it is possible to recover this shared structure without such correspondencesnamely, when only unpaired samples from the marginals PX and PY are available, as shown in Figure 2(c)and, if so, how this affects downstream performance individually on the primary modality X."
        },
        {
            "title": "3 Unpaired Multimodal Representation Learning",
            "content": "Let DX = {xi}Nx j=1 be datasets of Nx and Ny samples respectively, drawn from marginal distributions PX and PY as shown in Figure 2(c). The joint distribution PX,Y is unknown. Critically, in i=1 and DY = {yj}Ny 3 Figure 2: (a) Paired learning uses (xi, yi) with known correspondences. We instead study Unpaired learning: (b) with labels, using (xi, ci) and (yj, ˆcj), where ci and ˆcj denote labels for xi and yj, but no cross-modal correspondences; and (c) without any labels or correspondences, using {xi} and {yj}. contrast to paired multimodal learning, for any given xi DX and yj DY , there is no assumption that xi and yi are projections of the same underlying entity Z. This is in contrast with paired multimodal learning, which assumes access to labeled one-to-one correspondences between DX and DY . Our objective is to learn mappings fX : and fY : from each modality to common embedding space that captures the shared structures between the unpaired datasets. We refer to this as Unpaired Multimodal Representation Learning. In contrast to previous works [Lin et al., 2023, Lee and Yoon, 2025, Girdhar et al., 2022, Chada et al., 2023] (discussed in Section A), we leverage unpaired data without inferring alignment, incorporating paired data, or assuming pre-aligned embeddings. Intuitively, without known DX -toDY correspondences, even unpaired data can be useful if modalities capture different axes of information pertaining to the underlying . We formalize this intuition in Section 3.1, within the popularly studied framework of linear models, and introduce modality-agnostic algorithm Uml in Section 3.2 for learning representations from unpaired data."
        },
        {
            "title": "3.1 Theoretical Perspectives",
            "content": "We adopt the widely used modeling choice that each modality can be expressed as linear combination of shared and modality-specific components [Sturma et al., 2023, Timilsina et al., 2024, Huang et al., 2021]. This formulation inherently accounts for informational differences across modalities by incorporating modality-specific features [Wang et al., 2024]. Prior works [Sturma et al., 2023, Timilsina et al., 2024] have developed sufficient conditions for recovering joint distributions and the shared causal structure under this linear model. Building on this framework, we ask how joint training with unpaired modalities can still enhance representations within single modality. Data Generating Process. Assume that all factors of variation in reality live in single d-dimensional space, , i.e., θ Rd, modeled using linear data-generating pipeline. This parameter can further be decomposed as θ [θc, θx, θy] where θc Rdc , θx Rdx , θy Rdy and dc + dx + dy = d. Here, θc captures the common (shared) parameters that affect both modalities, θx denotes the parameters that only affect modality X, and θy denotes the parameters that only affect modality Y. We observe two datasets, one from each modality {Xi}Nx j=1 Rn, each reflecting partial measurements of the ground i=1 Rm and {Yj}Ny 4 truth latent space : Xi = Ac,i θc + Ax,i θx + ϵX,i, Yj = Bc,j θc + By,j θy + ϵY,j, ϵX,i (cid:16) ϵY,j (cid:16) 0, σ2 (cid:17) Im (cid:17) 0, σ2 In . (1) (2) Here, Ac,i, Ax,i, Bc,j, By,j are known design blocks capturing how each sample probes the latent factors and ε X,i, εY,j represent the independent measurement noise. (cid:105) (cid:104) 2 θ ℓ(θ) In our linear setting, estimating the true latent state θand hence the underlying reality is governed by the Fisher information matrix I(θ) = [Fisher, 1922, 1925], which measures how sharply the likelihood curves around the true θ. High curvature along particular axis means the data tightly constrain that component, driving down estimator variance there. Because and samples are independent given , their curvature contributions add pointwise, resulting in the joint Fisher information being simply the sum of the unimodal blocks. Thus, loosely speaking, any nonzero contribution from the unpaired Y-samples strictly increases curvature and thus strictly tightens the variance bound along those directions in θc. We formalize these insights in Theorem 1 and Theorem 2. Our results also generalize naturally to more than two modalities since the total contribution of all auxiliary modalities (excluding the primary modality X) can be obtained by summing their individual Fisher information matrices. Complete proofs and additional background are provided in Section C. Theorem 1 (Variance Reduction with Unpaired Multimodal Data). Let ˆθX, ˆθY be the least-squares estimators for θ using only {Xi} and only {Yj} and let ˆθX,Y be the joint estimator using both unpaired datasets. Then, under the assumption that at least one Bc,j, where {1, 2, ...Ny}, has full rank, the common-factor covariance satisfies the strict Loewner ordering i.e. Var or equivalently, the Fisher information on θc strictly increases when combining both modalities, despite not having sample-wise pairing: (IX + IY)θc,θc (IX)θc,θc . Var ˆθX,Y ˆθX θc,θc θc,θc (cid:17) (cid:17) (cid:16) (cid:16) , Theorem 1 says that although there is no sample-wise pairing, simply adding an unpaired modality Y, which carries non-degenerate information about every direction in the shared subspace, can only tighten our uncertainty about the shared parameters θc. Geometrically, the uncertainty ellipsoid for θc shrinks in every direction where contributes any curvature, and never expands elsewhere. In practice, however, no single data sample covers every latent axis. In these settings, while global shrinkage (Theorem 1) no longer applies, we still get directional gains. We capture this in Theorem 2. Theorem 2 (Directional Variance Reduction with Unpaired Multimodal Data). Let all notation be as in Theorem 1, and let Rdc {0}. If there exists at least one index {1, 2, ...Ny} such that Bc,jv = 0, then (IX + IY)θc,θc > (IX)θc,θc v. Further, the variance of the estimator in direction is strictly reduced if / range((IX)θc,θc ). Building on Theorem 2s insights that any nonzero curvature in given direction yields strict variance reduction, Corollary 1 and Corollary 2 in the Appendix (i) quantify the precise contraction factor when both modalities inform the same direction, and (ii) demonstrate how an otherwise ill-posed direction becomes well-posed once the second modality contributes. Thus far, we have studied incorporating data from an auxiliary modality without considering sample size constraints. natural next question is how to allocate fixed budget of additional samples: is it better to collect them from the same modality X, or from complementary modality Y? We formally address this in Theorem 3 below. Theorem 3 (Data from Auxiliary Modality Can Outperform More of the Same). Define for any m, (m) Y (m) = , then there exists nonzero Rdc c,jBc,j. If range c,i Ac,i and range j=1 (m) (cid:17) (cid:16) (cid:16) (cid:17) i=1 (m) such that Y (m) = (m) v. > Theorem 3 shows that if the second modality covers blind spot of the first, adding additional samples from the first modality does not increase the Fisher information in that direction; however, unpaired samples from the second modality provide strictly positive information along that axis. To empirically validate the above result, we design an illustrative Gaussian experiment (Section E.14) in which both modalities are generated as noisy linear projections of the same underlying Gaussian latent variable θc. We allocate fixed budget of samples, splitting evenly between and (details in Section E.14). Joint training with Uml significantly improves reconstruction fidelity on (Figure 3) compared to training on alone. Strikingly, this also reveals that the effective exchange rate between modalities is below one, meaning single additional sample from is worth more than an additional sample from X, even when the test task is on X. In Section 4.4, we extend this idea by quantifying the exchange rate between images and text on real world benchmarks."
        },
        {
            "title": "3.2 UML: Unpaired Multimodal Learner",
            "content": "Figure 3: Adding unpaired samples boosts reconstruction more than adding extra samples. Figure 4: (Left) Inputs from different modalities (e.g., images or text) are tokenized into patch or token embeddings using pretrained encoders or processed features; (Right) Uml can be trained under two settings: (a) Self-supervised, where patch/token embeddings are passed through shared network and modality-specific decoders to perform next-token/patch prediction; (b) Supervised, where mean/CLS embeddings are fed through the shared classifier to predict labels within each modality. The theory developed above establishes that adding an unpaired auxiliary modality strictly increases Fisher information along shared directions, thereby reducing estimator variance. We now translate these insights into practical framework: Uml (Unpaired Multimodal Learner). The central idea is to share parameters across modalities: since both and are projections of the same underlying reality , forcing them through shared weights can extract synergies by accumulating training gradients on the same parameters. This accumulation could be viewed as the practical analogue of Fisher information addition, ensuring that even without paired samples, auxiliary modalities tighten estimates of shared structure. While prior work has explored shared weights in partially paired settings [Sturma et al., 2023], for related visual streams such as video and depth [Girdhar et al., 2022], or within already-aligned spaces like CLIP [Lin et al., 2023], we study and show their pronounced impact in the most general case of unpaired modalities with unaligned encoders. 6 Consider unpaired samples or preprocessed features PX and PY, each encoded into embeddings zX = fX(x) and zY = fY(y) as shown in Figure 4. These embeddings are passed through shared network h, producing rX = h(zX) and rY = h(zY). This shared is the sole coupling between modalities and the locus of cross-modal transfer. We consider two regimes to train these networks: (a) self-supervised learning for fully unpaired data (Figure 2(c)) and (b) supervised learning for unpaired data with labels as shown in Figure 2(b). In the self-supervised setting, each modality has its own decoder (gX and gY) trained to reconstruct the input or predict the next token/patch depending on the form of embeddings zX and zY (refer to Figure 4(a)). The training objective (Pseudocode in the appendix: Algorithm 1) is LUML-SSL = ExPX ℓ (cid:16) gX(rX), (cid:17) + EyPY ℓ (cid:16) gY(rY), (cid:17) , where ℓ is the mean squared error for continuous targets or cross-entropy for discrete tokens. In the supervised setting, when labels are available, cX for and cY for y, shared classifier c() is trained on top of rX and rY. Since both c() and h() are shared across modalities, they can be viewed as single shared head (refer to Figure 4(b)). The supervised loss (Pseudocode in the appendix: Algorithm 2) is LUML-Sup = (x,cX ) ℓCE (cid:16) c(rX), cX (cid:17) + (y,cY) ℓCE (cid:16) c(rY), cY (cid:17) . In both scenarios, although supervision is modality-specific, the shared head receives updates from both modalities. Consequently, gradients from also flow into fX, effectively transferring information from fY and thus even without paired samples. At inference, we discard the auxiliary pathway and use rX as the representation for the target modality, training linear probe on top for downstream tasks. Uml also naturally extends to more than two modalities by incorporating additional modality-specific encoder/decoder heads or classification layers."
        },
        {
            "title": "4 Experimental Results",
            "content": "In this section, we empirically evaluate Uml across diverse benchmarks and configurations, leading to three main takeaways: (i) auxiliary modalities consistently boost target image representations in both self-supervised (Section 4.1) and supervised regimes (Section 4.2), with particularly strong gains in fine-grained and low-shot tasks; (ii) the benefits compound as we move from two to three modalities, with audio, vision, and text each contributing complementary signals (Section 4.2); and (iii) weight sharing across modalities generalizes beyond co-training, as pretrained language model weights transfer effectively to vision (Section 4.3). Finally, we quantify marginal rate of substitution between modalities, asking how many text samples equate to an image (Section 4.4) and report the emergence of multimodal neurons that implicitly align modalities (such as vision and language), even under unpaired supervision (Section 4.5)."
        },
        {
            "title": "4.1 UML in Self-Supervised Setting",
            "content": "To study Uml in the self-supervised setting, we use the real-world multimodal benchmark from MultiBench [Liang et al., 2021], which includes curated datasets such as text and images, with downstream labels covering variety of domains, including healthcare, affective computing, and multimedia research and three standard classification benchmarks [Parkhi et al., 2012, Cimpoi et al., 2014, Soomro et al., 2012]. We follow the same setting (dataset splitting, encoder architecture, pre-extracted features) as [Liang et al., 2023, 2021]. For training (refer to Figure 4(a)), we use linear encoders ( fX and fY) and decoders (gX and gY) and use an autoregressive transformer as the shared network (h). At inference, we average the transformer output embeddings and use the resulting embedding for linear probing on the downstream classification tasks. We report test accuracy using embeddings from the primary modality (image), and to ensure fair comparisons, we perform rigorous hyperparameter tuning for all methods and repeat each experiment with three random seeds. For more details, refer to Section B.3.1 and Section B.4.1. As shown in Table 1, 7 Table 1: Uml (Ours) achieves higher top-1 linear probe accuracy than unimodal baselines on learned representations, averaged over three seeds, across both MultiBench and visiontext benchmarks. Dataset MultiBench Standard vision-text a M Method M s s y f - t r O 1 0 1 D Unimodal Ours 59.66 63.28 55.16 57.10 70.62 71.98 56.17 58.16 56.99 57.34 85.04 86.32 79.86 80.98 78.13 78.49 Uml significantly outperforms its unimodal counterpart across all benchmarks, particularly on Mustard where unique information from the text modality expresses sarcasm, such as ironic tone of voice."
        },
        {
            "title": "4.2 UML in Supervised setting",
            "content": "We evaluate Uml on 9 widely-used visual classification benchmarks [Fei-Fei et al., 2004, Parkhi et al., 2012, Krause et al., 2013, Nilsback and Zisserman, 2008, Bossard et al., 2014, Xiao et al., 2010, Cimpoi et al., 2014, Soomro et al., 2012] in three settings: (1) Full fine-tuning: initializing from pretrained vision backbone and updating all parameters on the target dataset. (2) Few-shot linear probing: freezing the vision backbone and training linear classifier on labeled samples per class (k = 1, 2, 4, 8, 16). (3) Full-dataset linear probing: freezing the vision backbone and training linear classifier on the entire training dataset, discussed in Section E.1.2. In all cases, we enrich image representations with unpaired text embeddings, using ViT-S/14 DINOv2 as the vision encoder and OpenLLaMA-3B as the frozen text encoder. To construct conceptually related yet unpaired text data, we generate text templates with varying amounts of semantic information about the dataset. For further details and specific prompts, refer to Section B.3. To train Uml, we initialize the classifier with the average text embedding of each class, giving strong prior for imageclass alignment (refer to Section E.1 for ablation). Unpaired Textual Data Improves Visual Classification. As shown in Table 2, across both full fine-tuning and few-shot linear probing, Uml consistently improves over unimodal baselines across all datasets, with the largest gains on fine-grained tasks (e.g., Stanford Cars, FGVC Aircraft) where unpaired text sharpens class boundaries, and in low-shot regimes where textual cues help disambiguate visually similar classes. Additional results on other shots, datasets, model scales, and prompt variants are reported in Section E. We also evaluate the robustness of Uml-trained models under test-time distribution shifts. 16-shot linear probe with DINOv2 is trained on ImageNet and tested across four distribution-shifted target datasets: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R. Uml consistently outperforms the unimodal baseline (Figure 5), showing that language priors yield more transferable features. Additional robustness results are provided in Section E.3. Finally, for all these settings, we also replace the independent vision (DINOv2) and text encoders (OpenLLaMA) with those of CLIP; since CLIP embeddings are already aligned, the gains from Uml are even stronger (Section E.2.1, Section E.2.2, Section E.2.3,). Additional Ablations. For all experiments, we keep the text encoder frozen; Unfreezing the text encoder yields slightly weaker gains but still outperforms the unimodal baseline (Section E.12). Swapping in different text encoders yields consistent gains (Section E.6), while richer, more diverse captions provide especially strong boosts in few-shot settings (Section E.7). Further, training with semantically unrelated modalities (Section E.9) does not improve over the unimodal counterpart, confirming that gains stem Table 2: Uml (Ours) outperforms unimodal baseline on image classification with ViT-S/14 DINOv2 and OpenLLaMA-3B in both settings: (i) Full finetuning and (ii) Few-shot linear probing (k = 1, 2, 4). For complete results, refer to Section E.1. C f S 7 9 3 t c C Dataset 1 0 1 U 1 0 1 F s o r O P f 1 0 1 t e e Shot Method Full-finetuning Unimodal Ours 79.45 86.39 66.20 66.03 66.99 73.44 72.16 74.27 83.18 84.69 80.65 81.97 90.67 91.72 99.18 99.82 95.45 97.60 81.54 83.99 Few-shot Linear Probing Unimodal Ours 1 13.18 16.49 2 Unimodal Ours Unimodal Ours 24.68 28.65 38.76 43.17 34.15 41.79 47.88 53.15 57.51 60.89 14.09 15.63 23.09 24.78 32.10 33.86 36.60 42.04 47.75 53.25 59.69 62.43 46.74 52.33 56.81 63.86 67.75 71.13 35.18 42.27 48.54 54.44 60.79 63.88 63.51 73.59 75.32 81.41 83.89 87.36 89.62 93.64 96.02 97.63 98.59 99.17 76.66 84.52 86.90 90.55 93.48 94.96 45.52 51.36 56.33 60.85 65.84 68.53 from semantic correlations rather than spurious ones (Section E.9). Finally, unpaired multimodal data systematically widens inter-class margins and aligns modalities in weights (Section F). Figure 5: Our approach Uml is much more robust than its unimodal counterpart across four test time distribution shifted target test sets. All results are averaged across three random seeds. Unpaired Image and Textual Data Improve Audio classification. We extend Uml to an audiovisiontext setting using the ImageNet-ESC benchmark [Lin et al., 2023], which links ImageNet objects and captions with ESC-50 environmental sounds. The benchmark has two versions: ImageNet-ESC-27 and ImageNetESC-19. For example, the barking ESC-50 class maps to ImageNet dog breeds. The benchmark has two versions: ImageNet-ESC-27 (loosely matched visual-audio pairs) and ImageNet-ESC-19 (only accurate visual-audio matches). For audio encoding, we use AudioCLIP with an ES-ResNeXT backbone [Guzhov et al., 2021], while images and text are encoded by DINOv2 and OpenLLaMA-3B encoders. For further details, refer to Section B.3. Unpaired image and text data consistently improve audio classification (Figure 6), with larger gains from CLIPs aligned encoders (Section E.13.3). Conversely, both audio and text also enhance image classification, showing that transfer works in both directions (Section E.13). Finally, we study the full three-modality case, where we treat two modalities as auxiliaries and one as the targetfor example, using both image and text as auxiliaries for audio classification. As shown in Section E.10, performance improves monotonically with each added auxiliary modality, with the 9 Figure 6: Uml (Ours) improves audio classification using unpaired image and text samples on ImageNet-ESC-19 and ImageNet-ESC-27. All results are averaged across three random seeds. strongest gains achieved when all three modalities (audio, vision, and text) are used together."
        },
        {
            "title": "4.3 Transfer Learning",
            "content": "Thus far, we have explored how sharing model weights while co-training with multiple unpaired modalities improves the learned representation. But weight sharing need not be restricted to cotraining: if modalities capture the same underlying latents, then pretrained weights from one should also serve as useful initialization for another. We therefore study if transferring knowledge from one modality can enhance performance in another by initializing the transformer layers of ViT [Dosovitskiy et al., 2020] with pretrained BERT [Devlin et al., 2019] weights and evaluating on ImageNet (details in Section B.4.4). Patch embeddings are learned from scratch through linear layer, augmented by CLS token and positional embeddings. As shown in Figure 7, initializing with BERT boosts performance for both frozen and unfrozen backbones. Our results indicate that the semantic knowledge of language models provides strong initialization for vision compared to training from scratch. Figure 7: Image classifier trained from BERT initialization outperforms training from scratch for (left) full fine-tuning; (right) frozen backbone"
        },
        {
            "title": "4.4 Marginal Rate-of-Substitution Between Modalities",
            "content": "Having established that unpaired modalities boost representation learning, robustness, and transfer, we now ask more fundamental question: what is the relative value of each modality? If both images and text provide views of the same semantic space, can we quantify their exchange rateHow many words is an image worth? Figure 8 and Figure 9 visualize image-text conversion ratios using test accuracy isolines on Oxford-Pets [Parkhi et al., 2012] dataset. These map the number of texts equivalent to an image for the same performance. Aligned CLIP encoder (1 image 228 words) is more efficient than non-aligned DINOv2 and OpenLLaMA encoders (1 image 1034 words). Indeed, in some cases, an image may quite literally be worth thousand words. We also observe little or no additional benefit from adding more text beyond few samples. This is likely because we do not control for increasing complexity, so adding sentences does not guarantee extra information. Further results on additional datasets and key details, refer to Section E.4. 10 Figure 8: 1 img 228 words (CLIP) Figure 9: 1 img 1034 words (DINOv2)"
        },
        {
            "title": "4.5 Existence of Multimodal Neurons",
            "content": "Figure 10: Existence of Multimodal Neurons without Paired Supervision. Bars show overall (green), correlations conditioned on label=1 (sarcastic; blue), and correlations conditioned on label=0 (nonsarcastic; pink) between visual and textual activations for subset of neurons. Most neurons exhibit strong cross-modal correlation and specifically higher alignment for non-sarcastic samples, where visual and verbal cues are naturally congruent. While the previous section quantified the exchange rate between modalities, our next question concerns the mechanism that enables such exchange. Models like CLIP, trained with paired imagetext supervision, are known to develop multimodal neurons [Goh et al., 2021]: units that respond coherently to the same concept across both modalities. We ask whether similar cross-modal units can emerge even without such paired supervision, when the model is exposed only to unpaired but semantically related data. To test this hypothesis, we analyze neuron-level activations on the Mustard [Castro et al., 2019] dataset, multimodal sarcasm detection benchmark containing 690 video clips paired with corresponding utterances from television shows, each labeled for the presence or absence of sarcasm. For each sam- (t) ple and neuron j, let and ij denote activations in the visual and textual pathways, respectively. We compute the Pear- (v) (t) ij }i) to quantify cross-modal ij }i, {z son correlation rj = corr({z alignment. To examine label-specific effects, we further calcu- : yi = y}), for each class late {sarcastic, non-sarcastic}, assigning zero to undefined cases. As shown in Figure 10, even without paired supervision, several (y) = corr({z : yi = y}, {z (v) ij (v) ij (t) ij 11 Figure 11: Uml increasingly infers cross-modal correspondences with training, without paired supervision. neurons exhibit strong cross-modal coupling between vision and text, significantly higher than the highest correlation across neurons for an untrained network (baseline). This coupling steadily improves with training epochs, indicating that the model progressively infers more correspondences between modalities all without any paired supervision (refer to Figure 11). This correspondence, however, varies across labels. Most neurons show higher correlations for non-sarcastic samples, consistent with natural communication where facial expressions and verbal cues typically align. Sarcastic utterances, by contrast, deliberately break this alignment; words and expressions convey opposing meanings, resulting in weaker correlations between them. For detailed results across multiple neurons, refer to Section F.1."
        },
        {
            "title": "5 Conclusions and Limitations",
            "content": "Conclusions. We propose and investigate Unpaired Multimodal Representation Learning for enhancing unimodal representations with unpaired multimodal data. Under linear assumptions, we theoretically show that unpaired data from multiple modalities strictly increases Fisher information along shared directions, resulting in more accurate representation of the underlying world. Mechanistically, Unpaired Multimodal Learner (Uml) achieves this by accumulating gradients from different modalities on shared weights, which can be viewed as an operational analogue of the Fisher information gain. Empirically, we show performance gains across vision, text and audio benchmarks, and estimate conversion ratios between modalities. Uml provides new perspective on how to harness the abundance of unpaired data to learn better representations. This may be especially useful in domains such as medical imaging, scientific data, and robotics, which contain rich auxiliary modalities like text, audio, or metadata that are not often paired with every instance of the primary modality. Limitations. While our experiments study learning from unpaired multimodal data under both the selfsupervised and supervised settings, downstream evaluations are conducted primarily for classification. Investigating evaluation tasks, such as generation, offers rich avenue for future work. Furthermore, we evaluate how multimodal data enhances image and audio classification; it remains to show if they can, in turn, offer useful information for textual tasks."
        },
        {
            "title": "6 Reproducibility Statement",
            "content": "We provide complete proofs and relevant background for all theoretical results in Section C.3. For experiments, we detail the setup, training protocols, and algorithm implementations in Section B.4 and Section D. All datasets are publicly available, with additional details in Section B.3. The code can be found at https://github.com/Sharut/Unpaired-Multimodal-Learning/."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This research was sponsored by the Department of the Air Force Artificial Intelligence Accelerator under Cooperative Agreement Number FA8750-19-2-1000, in part by the NSF AI Institute TILOS (NSF CCF2112665) and the Alexander von Humboldt Foundation. This work was also supported by Packard Fellowship to P.I., and by ONR MURI grant N00014-22-1-2740. S.G. is supported by the MathWorks Engineering Fellowship. S.S. is supported by an NSF GRFP fellowship. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Department of the Air Force or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein."
        },
        {
            "title": "References",
            "content": "A. Almahairi, S. Rajeshwar, A. Sordoni, P. Bachman, and A. Courville. Augmented cyclegan: Learning many-to-many mappings from unpaired data. In International conference on machine learning, pages 195204. PMLR, 2018. R. Bachmann, D. Mizrahi, A. Atanov, and A. Zamir. Multimae: Multi-modal multi-task masked autoencoders. In European Conference on Computer Vision, pages 348367. Springer, 2022. R. Bachmann, O. F. Kar, D. Mizrahi, A. Garjani, M. Gao, D. Griffiths, J. Hu, A. Dehghan, and A. Zamir. 4m-21: An any-to-any vision model for tens of tasks and modalities. Advances in Neural Information Processing Systems, 37:6187261911, 2024. L. Bossard, M. Guillaumin, and L. Van Gool. Food-101mining discriminative components with random forests. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13, pages 446461. Springer, 2014. S. Castro, D. Hazarika, V. Perez-Rosas, R. Zimmermann, R. Mihalcea, and S. Poria. Towards multimodal sarcasm detection (an obviously perfect paper). arXiv preprint arXiv:1906.01815, 2019. R. Chada, Z. Zheng, and P. Natarajan. Momo: shared encoder model for text, image and multi-modal representations. arXiv preprint arXiv:2304.05523, 2023. M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36063613, 2014. In P. Demetci, R. Santorella, B. Sandstede, W. S. Noble, and R. Singh. Scot: Single-cell multi-omics alignment with optimal transport. Journal of Computational Biology, 29(1):318, 2022. doi: 10.1089/cmb.2021.0446. URL https://doi.org/10.1089/cmb.2021.0446. PMID: 35050714. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178178. IEEE, 2004. R. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical transactions of the Royal Society of London. Series A, containing papers of mathematical or physical character, 222(594-604):309368, 1922. R. A. Fisher. Theory of statistical estimation. In Mathematical proceedings of the Cambridge philosophical society, volume 22, pages 700725. Cambridge University Press, 1925. P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better visionlanguage models with feature adapters. International Journal of Computer Vision, 132(2):581595, 2024. X. Geng, H. Liu, L. Lee, D. Schuurmans, S. Levine, and P. Abbeel. Multimodal masked autoencoders learn transferable representations. arXiv preprint arXiv:2205.14204, 2022. 13 R. Girdhar, M. Singh, N. Ravi, L. Van Der Maaten, A. Joulin, and I. Misra. Omnivore: single model for many visual modalities. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1610216112, 2022. R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1518015190, 2023a. R. Girdhar, A. El-Nouby, M. Singh, K. V. Alwala, A. Joulin, and I. Misra. Omnimae: Single model masked pretraining on images and videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1040610417, 2023b. G. Goh, N. C. , C. V. , S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah. Multimodal neurons in artificial neural networks. Distill, 2021. doi: 10.23915/distill.00030. https://distill.pub/2021/multimodalneurons. A. Guzhov, F. Raue, J. Hees, and A. Dengel. Esresne (x) t-fbsp: Learning robust time-frequency transformation of audio. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 18. IEEE, 2021. M. K. Hasan, W. Rahman, A. Zadeh, J. Zhong, M. I. Tanveer, L.-P. Morency, et al. Ur-funny: multimodal language dataset for understanding humor. arXiv preprint arXiv:1904.06618, 2019. Y. Huang, C. Du, Z. Xue, X. Chen, H. Zhao, and L. Huang. What makes multi-modal learning better than single (provably). Advances in Neural Information Processing Systems, 34:1094410956, 2021. M. Huh, B. Cheung, T. Wang, and P. Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR, 2021. A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark. Mimic-iii, freely accessible critical care database. Scientific data, 3(1): 19, 2016. C. Jose, T. Moutakanni, D. Kang, F. Baldassarre, T. Darcet, H. Xu, D. Li, M. Szafraniec, M. Ramamonjisoa, M. Oquab, et al. Dinov2 meets text: unified framework for image-and pixel-level vision-language alignment. arXiv preprint arXiv:2412.16334, 2024. J. Y. Koh, R. Salakhutdinov, and D. Fried. Grounding language models to images for multimodal inputs and outputs. In International Conference on Machine Learning, pages 1728317300. PMLR, 2023. J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554561, 2013. G. Lample, M. Ott, A. Conneau, L. Denoyer, and M. Ranzato. Phrase-based & neural unsupervised machine translation. arXiv preprint arXiv:1804.07755, 2018. J.-J. Lee and S. W. Yoon. Can one modality model synergize training of other modality models? In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=5BXWhVbHAK. J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang. Visualbert: simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 14 Y. Li, H. Fan, R. Hu, C. Feichtenhofer, and K. He. Scaling language-image pre-training via masking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2339023400, 2023. P. P. Liang, Y. Lyu, X. Fan, Z. Wu, Y. Cheng, J. Wu, L. Chen, P. Wu, M. A. Lee, Y. Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation learning. Advances in neural information processing systems, 2021(DB1):1, 2021. P. P. Liang, Z. Deng, M. Q. Ma, J. Y. Zou, L.-P. Morency, and R. Salakhutdinov. Factorized contrastive learning: Going beyond multi-view redundancy. Advances in Neural Information Processing Systems, 36: 3297132998, 2023. Z. Lin, S. Yu, Z. Kuang, D. Pathak, and D. Ramanan. Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1932519337, 2023. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. Advances in neural information processing systems, 30, 2017. I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. M. Maniparambil, R. Akshulakov, Y. A. D. Djilali, M. El Amine Seddik, S. Narayan, K. Mangalam, and N. E. OConnor. Do vision and language encoders represent the world similarly? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1433414343, 2024. S. Menon and C. Vondrick. Visual classification via description from large language models. arXiv preprint arXiv:2210.07183, 2022. J. Merullo, L. Castricato, C. Eickhoff, and E. Pavlick. Linearly mapping from image to text space. arXiv preprint arXiv:2209.15162, 2022. D. Mizrahi, R. Bachmann, O. Kar, T. Yeo, M. Gao, A. Dehghan, and A. Zamir. 4m: Massively multimodal masked modeling. Advances in Neural Information Processing Systems, 36:5836358408, 2023. L. Moschella, V. Maiorca, M. Fumero, A. Norelli, F. Locatello, and E. Rodol`a. Relative representations enable zero-shot latent space communication. arXiv preprint arXiv:2209.15430, 2022. M.-E. Nilsback and A. Zisserman. Automated flower classification over large number of classes. In Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. A. Norelli, M. Fumero, V. Maiorca, L. Moschella, E. Rodola, and F. Locatello. Asif: Coupled data turns unimodal models to multimodal without training. Advances in Neural Information Processing Systems, 36: 1530315319, 2023. O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012. S. Pratt, I. Covert, R. Liu, and A. Farhadi. What does platypus look like? generating customized prompts for zero-shot image classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1569115701, 2023. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. K. Roth, J. M. Kim, A. Koepke, O. Vinyals, C. Schmid, and Z. Akata. Waffling around for performance: Visual classification with random words and broad concepts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1574615757, 2023. S. Roy, F. Ogidi, A. Etemad, E. Dolatabadi, and A. Afkanpour. shared encoder approach to multimodal representation learning. arXiv preprint arXiv:2503.01654, 2025. J. Ryu, C. Bunne, L. Pinello, A. Regev, and R. Lopez. Cross-modality matching and prediction of perturbation responses with labeled gromov-wasserstein optimal transport. arXiv preprint arXiv:2405.00838, 2024. Y. Shi, V. De Bortoli, A. Campbell, and A. Doucet. Diffusion schr odinger bridge matching. Advances in Neural Information Processing Systems, 36:6218362223, 2023. A. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela. Flava: foundational language and vision alignment model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1563815650, 2022. K. Soomro, A. R. Zamir, and M. Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. S. Srivastava and G. Sharma. Omnivec: Learning robust representations with cross modal sharing. 2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 12251237, 2023. URL https://api.semanticscholar.org/CorpusID:265128674. S. Srivastava and G. Sharma. Omnivec2 - novel transformer based network for large scale multimodal and multitask learning. In CVPR, pages 2740227414, 2024. URL https://doi.org/10.1109/CVPR52733. 2024.02588. N. Sturma, C. Squires, M. Drton, and C. Uhler. Unpaired multi-domain causal representation learning. Advances in Neural Information Processing Systems, 36:3446534492, 2023. I. Sutskever. An Observation on Generalization. YouTube video, Aug 2023. URL https://www.youtube. com/watch?v=AKMuA_TVz3A. Accessed: 2025-09-24. S. Timilsina, S. Shrestha, and X. Fu. Identifiable shared component analysis of unpaired multimodal mixtures. arXiv preprint arXiv:2409.19422, 2024. C. Wang, S. Gupta, X. Zhang, S. Tonekaboni, S. Jegelka, T. Jaakkola, and C. Uhler. An information criterion for controlled disentanglement of multimodal data. arXiv preprint arXiv:2410.23996, 2024. J. Xi, J. Osea, Z. Xu, and J. S. Hartford. Propensity score alignment of unpaired multimodal data. Advances in Neural Information Processing Systems, 37:141103141128, 2024. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 34853492. IEEE, 2010. A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency. Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos. arXiv preprint arXiv:1606.06259, 2016. 16 X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1812318133, 2022. X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. Y. Zhang, K. Gong, K. Zhang, H. Li, Y. Qiao, W. Ouyang, and X. Yue. Meta-transformer: unified framework for multimodal learning. arXiv preprint arXiv:2307.10802, 2023. J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 22232232, 2017."
        },
        {
            "title": "Appendix",
            "content": "A Further Related Works Supplementary Experimental Details and Assets Disclosure B.1 Assets . . . . . . . . . . B.2 Hardware and setup . B.3 Datasets . . . . . . . B.3.1 MultiBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.2 Image Classification Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.3 Constructing text templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3.4 ImageNet-ESC Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Training Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.1 Unpaired Multimodal Representation Learning under Self-Supervision . . . . . . . B.4.2 Image Classification using Image and Unpaired Texts . . . . . . . . . . . . . . . . . B.4.3 Evaluation on ImageNet-ESC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4.4 Transfer Learning from Language to Vision . . . . . . . . . . . . . . . . . . . . . . . Proofs of Theoretical Results C.1 Background and Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Maximum Likelihood Estimators and Fisher Contributions . . . . . . . . . . . . . . . . . . C.3 Theorems and Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . UML: Algorithm Pseudocode Additional Experiments E.1 Improving Image Classification using Unpaired Texts (Unaligned encoders) . . . . . . . . . E.1.1 Supervised Finetuning (across architectures) . . . . . . . . . . . . . . . . . . . . . . . E.1.2 Linear Probing (across architectures) . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1.3 Few-shot Linear Probing (across architectures) . . . . . . . . . . . . . . . . . . . . . . E.2 Improving Image Classification using Unpaired Texts (Aligned encoders) . . . . . . . . . . E.2.1 Supervised Finetuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 20 20 21 21 21 21 22 22 22 24 24 25 25 26 27 32 35 35 38 41 44 E.2.2 Linear Probing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.3 Few-shot linear Probing (across architectures) . . . . . . . . . . . . . . . . . . . . . . E.3 Improving Visual Robustness Using Unpaired Texts . . . . . . . . . . . . . . . . . . . . . . . E.4 Marginal Rate-of-Substitution Between Modalities . . . . . . . . . . . . . . . . . . . . . . . . E.4.1 Unaligned Encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4.2 Aligned Encoders (CLIP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.5 Impact of Scaling Vision Backbone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Impact of Varying Text Encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 Learning with Coarse-Grained vs. Fine-Grained Textual Cues . . . . . . . . . . . . . . . . . E.8 Impact on Performance with Increasing Unpaired Text Prompts . . . . . . . . . . . . . . . . E.9 Effect of Unrelated Auxiliary Modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.10 Extension to More Than Two Modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.11 Effect of Ratio of Modality Batches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.12 Effect of Freezing vs. Unfreezing the Text Encoder . . . . . . . . . . . . . . . . . . . . . . . . E.13 Additional Experiments for Audio-Visual Setting . . . . . . . . . . . . . . . . . . . . . . . . 44 45 49 49 50 51 52 53 54 55 55 56 E.13.1 Improving Image Classification with Unpaired Audio and Text (Unaligned encoders) 57 E.13.2 Improving Image Classification with Unpaired Audio and Text (Aligned encoders) E.13.3 Improving Audio Classification with Unpaired Image and Text (Aligned encoders) E.14 Gaussian Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Analysis of the Learned Predictor F.1 Multimodal Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Change in Decision Boundaries with Unpaired Data from Another Modality . . . . . . . . F.3 What do models learn from unpaired data? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 58 59 59"
        },
        {
            "title": "A Further Related Works",
            "content": "Unpaired Multimodal Learning. Unpaired data has long been used for image-to-image [Zhu et al., 2017, Liu et al., 2017, Almahairi et al., 2018, Shi et al., 2023] and text-to-text translation [Lample et al., 2018] . More recently, several works have also proposed learning from unpaired data by inferring coarseor finegrained alignments through distribution matching or optimal transport objectives [Xi et al., 2024, Demetci et al., 2022, Ryu et al., 2024]. In contrast, we leverage unpaired data for learning representations without the need for explicit or inferred alignment. [Timilsina et al., 2024, Sturma et al., 2023] theoretically analyze the problem of identifying shared latent components and causal structures in unaligned multimodal mixtures. Most closely related to our work is [Lin et al., 2023], which leverages coarse-grained text data such as class names to improve image classification on CLIP using shared linear head. Another related line of works [Roth et al., 2023, Pratt et al., 2023, Menon and Vondrick, 2022, Gao et al., 2024] leverage prompting templates and pretrained LLMs to generate descriptive class captions, showing improved image classification performance with CLIP. Nonetheless, these methods operate on CLIP with pre-aligned representation spaces, whereas our approach also learns from unpaired data without assuming prior alignment. Several works have also proposed learning large multitask multimodal models with joint encoders and unified embedding spaces [Srivastava and Sharma, 2024, 2023, Zhang et al., 2023, Girdhar et al., 2022, Geng et al., 2022], often using joint training over separate tasks and/or masked prediction objectives. In similar vein, [Chada et al., 2023] uses stage-wise training strategy with both unpaired and paired data, and [Girdhar et al., 2023b] trains single model across visual modalities. However, most of these methods rely on some amount of paired data for preliminary alignment and then leverage abundant modality-specific unpaired data for further improvement. In contrast, our approach demonstrates that model can implicitly learn cross-modal correlations from purely unpaired data, without requiring explicit alignment as prerequisite. Multimodal Representation Alignment. Our method relies on the notion of shared information and structure between unaligned modalities. Closely related to this are works demonstrating that unimodal representations trained without multimodal data are nevertheless converging. Huh et al. [2024] presents evidence that better-performing language models exhibit increased alignment to self-supervised vision models. Similarly, [Maniparambil et al., 2024] shows latent space alignment between vision and text encoders across backbones and training paradigms, and uses the CKA metric to connect unaligned encoders zero-shot. Earlier works also note alignment between models trained with different datasets and modalities [Moschella et al., 2022, Norelli et al., 2023]. Several works have also shown that linear projection or MLP is sufficient to stitch together the latent spaces of pretrained vision and language models [Merullo et al., 2022, Liu et al., 2023, Koh et al., 2023]. Zhai et al. [2022] extends this to training text encoder to align to frozen pretrained image model; this method was in turn used to integrate DINOv2, large self-supervised vision model, with text encoder [Jose et al., 2024]."
        },
        {
            "title": "B Supplementary Experimental Details and Assets Disclosure",
            "content": "B.1 Assets We do not introduce new data in the course of this work. Instead, we use publicly available, widely used image datasets for the purposes of benchmarking and comparison. 20 B.2 Hardware and setup Each experiment was conducted on 1 NVIDIA Tesla V100 GPUs, each with 32GB of accelerator RAM. The CPUs used were Intel Xeon E5-2698 v4 processors with 20 cores and 384GB of RAM. All experiments were implemented using the PyTorch deep learning framework. B.3 Datasets B.3.1 MultiBench We evaluate our approach on diverse set of multimodal fusion datasets from MultiBench [Liang et al., 2021] spanning healthcare, sentiment, and humor detection: CMU-MOSEI [Moschella et al., 2022]: The largest sentence-level multimodal sentiment and emotion dataset, containing 23,000 annotated monologue videos (over 65 hours of content from more than 1,000 speakers across 250 topics). Each video is labeled with sentiment intensity in the range [3, 3], which we cast into binary positive/negative sentiment classification. CMU-MOSI [Zadeh et al., 2016]: related multimodal sentiment dataset with 2,199 YouTube video clips, reflecting real-world opinionated speech. Sentiment intensities are annotated in the range [3, 3], and we again formulate the task as binary sentiment classification. UR-FUNNY [Hasan et al., 2019]: large-scale humor detection dataset derived from over 16,000 TED talk videos. It includes 8,257 humorous punchlines identified by laughter markers, paired with 8,257 negative examples drawn from non-humorous contexts, forming balanced binary humor classification task. MUSTARD [Castro et al., 2019]: multimodal sarcasm detection dataset with 690 video clips from TV shows. Each sample is annotated for the presence or absence of sarcasm, yielding challenging binary classification task. MIMIC-III [Johnson et al., 2016]: large-scale clinical dataset with records of over 40,000 ICU patients. It combines time-series physiological measurements (recorded hourly over 24-hour window) with static demographic and tabular features. We use it for binary classification of patients into group of ICD-9 codes. B.3.2 Image Classification Benchmarks We evaluate on the following widely-used classification benchmarks: ImageNet [Deng et al., 2009], StanfordCars [Krause et al., 2013], UCF101 [Soomro et al., 2012], Caltech101 [Fei-Fei et al., 2004], Oxford Flowers [Nilsback and Zisserman, 2008], SUN397 [Xiao et al., 2010], DTD [Cimpoi et al., 2014], FGVCAircraft [Maji et al., 2013], OxfordPets [Parkhi et al., 2012], and Food101 [Bossard et al., 2014]. More details about the dataset and splits is provided in Table 3. B.3.3 Constructing text templates To construct conceptually related yet unpaired text data, we generate text templates that capture varying granularities of information about the dataset. Our first approach (Vanilla) uses the straightforward template photo of {}\" with natural language label for each category, resulting in basic text description for each class. However, this simple textual corpus lacks fine-grained information necessary Table 3: Detailed statistics of the 10 datasets for image classification. Dataset Classes Train Val Test Caltech101 [Fei-Fei et al., 2004] OxfordPets [Parkhi et al., 2012] StanfordCars [Krause et al., 2013] Oxford Flowers [Nilsback and Zisserman, 2008] Food101 [Bossard et al., 2014] FGVCAircraft [Maji et al., 2013] SUN397 [Xiao et al., 2010] DTD [Cimpoi et al., 2014] UCF101 [Soomro et al., 2012] ImageNet [Deng et al., 2009] 100 37 196 102 101 100 397 47 101 1,000 2,465 1,649 4,128 3,669 736 2,944 8,041 1,635 6,509 2,463 1,633 4,093 30,300 20,200 50,500 3,333 3,333 3,334 19,850 3,970 15,880 1,692 1,128 2,820 7,639 3,783 1,898 1.28M N/A 50,000 to distinguish between visually similar subcategories or to resolve contextually ambiguous terms. To address this, for the second template, we draw from the extensive literature on improving text prompts for zero-shot classification in CLIP [Gao et al., 2024, Menon and Vondrick, 2022, Pratt et al., 2023, Roth et al., 2023]. Specifically, for the second approach (GPT-3 Descriptions), we adopt the text prompt generation strategy developed by Pratt et al. [2023], using large language models such as GPT-3 to generate diverse and contextually rich prompts for each image category. We use three generic hand-written sentences across the datasets: Describe what a/the {} looks like: Describe a/the {} : What are the identifying characteristics of a/the {}? The blank portion of each template is populated with the category name, along with the category type for specialized datasets (e.g., pet + {} for Oxford Pets or aircraft + {} for FGVC Aircraft). The type specification is important for disambiguating categories with multiple interpretations. Some examples of these descriptions are provided in Table 4 for the Oxford Pets dataset. B.3.4 ImageNet-ESC Dataset Experimental Setup. We extend our results beyond vision and language to an audiovisual-language dataset: the ImageNet-ESC benchmark [Lin et al., 2023]. This benchmark combines ImageNet (1000 object categories) and ESC-50 (50 environmental sound classes) by matching classes that logically correspond. For example, the dog (barking) class from ESC-50 aligns with various dog breeds from ImageNet, while the clock-alarm sound maps to both analog clock and digital clock. This alignment captures the relationship between visual objects, their sounds, and their textual descriptions. The benchmark consists of two versions: 1) ImageNet-ESC-27: broader set including loosely matched visual-audio pairs (e.g., drinkingsipping to water bottle); 2) ImageNet-ESC-19: more precise subset containing only accurate visual-audio matches. B.4 Training Protocol B.4.1 Unpaired Multimodal Representation Learning under Self-Supervision MultiBench. For the MIMIC dataset, which contains tabular and medical time-series inputs, we train models directly on the raw modality inputs. For the four video datasets (MOSEI, MOSI, UR-FUNNY, Table 4: Sample text descriptions per class for Oxford Pets dataset Class Examples Wheaten Terrier Great Pyrenees Sphynx Birman Pomeranian wheaten terrier is small, shaggy dog with soft, silky coat. wheaten terrier has soft, wheat-colored coat that is low-shedding and hypoallergenic. The wheaten terrier is medium-sized, hypoallergenic dog breed. pet Wheaten Terrier usually has an intelligent expression and soft, wheat-colored coat. great pyrenees is large, white, shaggy-coated dog. Great Pyrenees is large, fluffy dog with calm, gentle disposition. The great pyrenees was originally bred to protect livestock from predators. Great Pyrenees are known for being very large, white dogs with thick fur. pet Sphynx typically has small, wrinkled head and hairless body. Sphynx is hairless cat breed known for its soft, warm skin. Sphynx often displays large ears, pronounced cheekbones, and no fur. Sphynx are unique cats characterized by their lack of coat and wrinkled skin. Birman is long-haired, color-pointed cat with mask of darker fur on its face. Birman has silky, pale cream to ivory fur with deep sealor lilac-colored points. Birman cats possess striking blue eyes and contrasting white gloves on their paws. They are known for being gentle, affectionate, and smooth-coated companions. Pomeranian is small, fluffy dog with thick double coat. Pomeranians are toy-sized, alert dogs with fox-like faces and plumed tails. pet Pomeranian often comes in orange, black, white, or mixed coat colors. They are lively, outgoing, and known for their bold, friendly personalities. and MUSTARD), we train models on standard pre-extracted features from text, video, and audio modalities [Liang et al., 2021]. All models are trained for 100 epochs, with hyperparameter search over learning rates {102, 103, 104}. We report the best performance across learning rates, averaged over three random seeds. To train Uml, each modality is first projected into shared embedding space of dimension {10, 40, 150, 300} via learned linear transformation. The projected inputs are processed by shared 5-layer, 5-head autoregressive Transformer encoder, followed by linear projection back to the original modality dimension. Training uses next-token/patch embedding prediction objective, with both modalities sharing the Transformer backbone to encourage cross-modal synergies in the latent space. At inference time, we average the Transformer outputs across sequence length and use the resulting embeddings for linear probing on downstream classification tasks. All models (Unimodal baseline and Uml) are trained for 100 epochs, with hyperparameter search over learning rates {102, 103, 104}. For Uml, in addition, we perform hyperparameter search over curriculum parameter step {0, 30, 50, 70}. This parameter controls whether training begins on alone for the first step epochs before switching to joint training, with step = 0 corresponding to joint training from the start. For each dataset, we select the best-performing model on the validation set and report test accuracy averaged over three random seeds. Standard Vision-Text Benchmarks. We extract image and text embeddings using ViT-B/14 DINOv2 and OpenLLaMA-3B, respectively. As in MultiBench, patch and token embeddings are projected to shared 256-dimensional space via modality-specific linear layers. 4-layer, 4-head transformer serves as the shared encoder, with outputs projected back to the original embedding dimensions using modality-specific linear projections. We perform rigorous hyperparameter tuning for both the unimodal baseline and Uml, and report average test accuracy of the best model across three seeds. Full hyperparameter ranges are listed in Table 5. 23 B.4.2 Image Classification using Image and Unpaired Texts For text, we use OpenLLaMA-3B as our default encoder and ablate against BERT-Large, RoBERTa-Large, GPT-2 Large, and the pre-aligned CLIP text encoder, keeping the text encoder frozen. For images, our main backbone is ViT-S/14 DINOv2, with ablations across other DINOv2 variants and the CLIP vision encoder. In the linear-probe setting, all encoder weights stay fixed and we train only single linear classification head; in full fine-tuning, we jointly update the image backbone and that head, while still freezing the text encoder. We optimize cross-entropy loss via AdamW [Loshchilov and Hutter, 2017] and perform an extensive grid search over learning rate, weight decay, cosine learning rate scheduling with linear warmup, dropout, and learnable, modality-specific scaling on the logits. The results are reported for the best-performing model on the validation dataset. We report results for the model achieving highest validation accuracy; the full hyperparameter ranges are in Table 5. For full fine-tuning, we jointly update the image backbone and classification head with fixed learning rate of 5 105, batch size 64, and omit learnable modality-specific scaling, since it showed no benefit in this setting. Table 5: Hyperparameter grid for linear probing. Hyperparameter Values Optimizer Learning rate Weight decay LR scheduler Batch size Max iterations Warmup iterations Warmup type Warmup min LR Dropout Modality-specific learnable scaling Early-stop patience adamw {0.001, 1e-4} {0.0, 0.01, 0.001} cosine {8, 32} 12,800 50 linear 1e-5 {0.0} {False, True} 10 B.4.3 Evaluation on ImageNet-ESC Similar to our vision-language experiments, we perform few-shot evaluation using the 5-fold splits defined in the benchmark. Each fold contains 8 samples per class, with one fold used for training and validation and the remaining four for testing. We repeat the process over 5 random splits and report the average performance. For audio encoding, we use AudioCLIP with an ES-ResNeXT backbone [Guzhov et al., 2021]. AudioCLIP is pretrained on AudioSet and generates audio embeddings in the same representation space as CLIP. Following the instructions in [Guzhov et al., 2021, Lin et al., 2023], we use train() mode in Pytorch to extract the features since eval() mode yields suboptimal embeddings. We evaluate our models on two tasksaudio classification and image classificationcomparing the unimodal baseline against two multimodal variants in which the primary modality is each time augmented by one of the other modalities. 24 B.4.4 Transfer Learning from Language to Vision To adapt language model to image classification, we embed image patches using linear projection and add positional encodings to capture spatial structure. We then use transformer layers initialized from pretrained BERT, and finally, 2-layer MLP classification head. Specifically, we split each image of size 224 224 into patches of size 16 16 with 196 patch tokens. Each patch is then projected into the models embedding space of dimension d(e.g. d=768 for GPT-2, = 1024 for BERT) via learned linear layer. We then prepend learnable [CLS] token, add learned positional embeddings of shape (N + 1) d, and apply dropout with probability = 0.1. This (N + 1) sequence is passed into the pretrained transformer stack (either GPT-2 or BERT), using full bidirectional attention mask over all patch tokens and the CLS token. We extract the final hidden state corresponding to the CLS token and feed it through two-layer MLP classification head. During training, we evaluate two scenarios: 1) one where the pretrained backbone is frozen and only the patch embedding and linear head are trained, and 2) another where the backbone is initially frozen to align the trainable layers (patch embedding and head) with the pretrained language backbone, and then unfrozen after 2000 steps for end-to-end training. This approach allows us to test whether the semantic richness captured by language models provides strong initialization, leading to better convergence and performance compared to training ViT from scratch."
        },
        {
            "title": "C Proofs of Theoretical Results",
            "content": "In this section, we present complete derivations and proofs of the main theoretical claims. Section C.1 gathers all definitions and background required for our arguments. Section C.2 formalizes the linear datagenerating model, derives closed-form maximum-likelihood estimators for each modality and their joint estimator, and computes the corresponding block-wise Fisher information. Finally, Section C.3 provides the detailed proofs of our variance-reduction claims, showing rigorously how unpaired multimodal estimation strictly lowers estimator variance. C.1 Background and Definitions In this section we revisit the mathematical definitions used in our theoretical analysis, including matrixorderings, characterization of symmetric matrices and Fisher information. Definition 1 (Positive Semidefinite Matrix). real symmetric matrix Rdd is positive semidefinite if for all vectors Rd, v 0. Equivalently, all eigenvalues of are nonnegative. We denote the set of all symmetric, positive-semidefinite matrices as Sd 0. Definition 2 (Positive Definite Matrix). real symmetric matrix Rdd is positive definite if for every nonzero Rd, v > 0. Equivalently, all eigenvalues of are strictly positive. We denote the set of all symmetric, positive definite matrices as Sd 0. Definition 3 (Loewner Order). For two real symmetric matrices A, Rdd, we write is positive semidefinite and is positive definite. This defines partial order on the cone of symmetric matrices. Definition 4 (Fisher Information Matrix). Given parametric family of densities p(x; θ) on data x, the Fisher information matrix at parameter θ is I(θ) = (cid:104) xp(;θ) θ log p(x; θ) θ log p(x; θ)(cid:105) . Equivalently, for regular models, I(θ) = (cid:104) 2 θ log p(x; θ) (cid:105) . C.2 Maximum Likelihood Estimators and Fisher Contributions In this section we revisit our linear datagenerating model, introduce notations for the Xonly, Yonly and joint likelihoods, derive the closed-form MLEs (cid:98)θX, (cid:98)θY and (cid:98)θX,Y, and formalize their information contributions towards estimating the ground truth parameters θ [θc, θx, θy]. Data Generating Process. Recall our linear data-generating process: Assume that all factors of variation in reality live in single d-dimensional space θ Rd modeled using linear data-generating pipeline. This parameter can further be decomposed as θ [θc, θx, θy] where θc Rdc , θx Rdx , θy Rdy and dc + dx + dy = d. Here, θc captures the common (shared) parameters that affect both modalities, θx denotes the parameters that only affect modality X, and θy denotes the parameters that only affect modality Y. We observe two independent datasets, one from each modality {Xi}Nx j=1 Rn, each reflecting partial measurements of the ground truth latent space : (cid:16) i=1 Rm and {Yj}Ny (cid:17) Xi = Ac,i θc + Ax,i θx + ϵX,i, Yj = Bc,j θc + By,j θy + ϵY,j, ϵX,i (cid:16) ϵY,j 0, σ2 Imi (cid:17) 0, σ Inj (3) . (4) Here, Ac,i, Ax,i, Bc,j, By,j are known design blocks capturing how each sample probes the latent factors and ε X,i,εY,j represent the independent measurement noise. In our linear setting, estimating the true latent state θand hence the underlying reality is governed by the Fisher information matrix I(θ) = , which measures how sharply the likelihood curves around the true θ. High curvature along particular axis means the data tightly constrain that component, driving down estimator variance there. θ ℓ(θ) 2 (cid:104) (cid:105) Unimodal Estimators. We first estimate θ using only the Xdataset. Stacking {Xi}Nx i=1 yields design matrix with block rows [Ac,i, Ax,i, 0]. The least-squares solution (cid:98)θX = arg min θ Nx i=1 (cid:13) (cid:13) (cid:13)Xi Ac,i θc Ax,i θx (cid:13) (cid:13) (cid:13) 2 omits θy entirely. Consequently, the Fisher information on θy vanishes, making it unidentifiable. Analogously, stacking {Yj}Ny j=1 defines with block rows [Bc,j, 0, By,j] and yields (cid:98)θY = arg min θ Ny j=1 (cid:13) (cid:13) (cid:13)Yj Bc,j θc By,j θy (cid:13) (cid:13) (cid:13) 2 . This estimator doesnt depend on θx, providing zero coverage for that component. Thus, each unimodal estimator entirely fails to recover the parameters exclusive to the omitted modality. Multimodal Estimators. Despite the lack of one-to-one pairing, both {Xi} and {Yj} share the common parameters θc. Since the two distributions are conditionally independent, the joint likelihood factorizes as Nx i=1 p(Xi θc, θx) Ny j= p(Yj θc, θy). Maximizing this yields the combined estimator (cid:98)θX,Y = arg min θc,θx,θy (cid:40) Nx i=1 Xi Ac,i θc Ax,i θx2 + Yj Bc,j θc By,j θy2 . (cid:41) Ny j=1 26 Intuitively, there is no requirement to match up individual (Xi, Yj) pairs. Instead, the estimate for θc is improved by both modalities while remaining unpaired. Fisher Information. In our linear model, each dataset contributes block-structured Fisher information. For the Xdataset: and for the Ydataset: IX = Nx i=1 c,i Ac,i A x,i Ac,i 0 c,i Ax,i x,i Ax,i 0 , 0 0 0 IY = Ny j=1 c,jBc,j 0 y,jBc,j 0 c,jBy,j 0 0 0 y,jBy,j . Because and samples are independent, their curvature contributions add pointwise, resulting in the joint Fisher information being simply the sum of the unimodal blocks. IX,Y = IX + IY = A c,jBc,j c,i Ac,i + B A x,i Ax,i 0 0 B y,jBy,j , where denotes the cross-modal blocks. In particular, we have the shared-parameter block as (IX,Y)θc,θc = Nx i=1 c,i Ac,i + Ny j=1 c,jBc,j, C.3 Theorems and Proofs The aim of this section is to detail the proofs of the theoretical results presented in the main manuscript The key theoretical tools driving our analysis are already prepared in Section C.1 and Section C.2. Core to our theoretical analysis are few lemmas around the Loewner-order monotonicity result for inverses that we prove below. Lemma 1 (Loewner Order reversal for inverses). Let M, Sd N1 M1 (or N1 M1) . 0 with (or N). Then Proof. Since 0, N1/2 exists and is nonsingular. Define := N1/2 MN1/2 I. Because congruence with an invertible matrix preserves positive-definiteness, 0; hence C1 is well defined and C1 (the scalar map (cid:55) x1 is strictly decreasing on (0, )). Undoing the congruence gives M1 = N1/2C1N1/2 N1/2 N1/2 = N1. Lemma 2 (Inversemonotonicity of the MoorePenrose pseudoinverse). Let M, Sd ker = ker =: K. Then their pseudoinverses obey M. 0 satisfy and Proof. Set := and let := PS be the orthogonal projector onto S. Because and vanish on K, we have the decompositions = PMP and = PNP. Restricted to both matrices are positivedefinite: := PMP, := PNP Sdim 0 , N. Apply Lemma 1 to M, to obtain N1 M1 on S. The MoorePenrose pseudoinverse equals the ordinary inverse on and is zero on K: Therefore = N1P M1P = M. = M1P, = N1P. Lemma 3 (Directional Loewner Order reversal). Let M, Sd Mv < Nv, then 0 with N. If non-zero vector satisfies 1. For the vector v, it holds that M1v N1v, with strict inequality M1v > N1v if and only if (N M)M1v = 0. 2. There exists non-zero vector Rd such that M1u > N1u. Proof. Denote the Loewner gap := 0. Then, the assumption Nv > Mv is equivalent to vv > 0. Introduce the congruenceinvariant normalisation := M1/2M1/2 0. Now, using = M1/2CM1/2 and properties of inverse, = M1/2(I + C)M1/2, N1 = M1/2(I + C)1 M1/2, since + 0 (because 0 and 0). Thus, (cid:20) M1 N1 = M1/2 (I + C)1 (cid:21) M1/2 = M1/2C(I + C)1 M1/2, because (I (I + C)1)(I + C) = C. Finally, evaluating in the direction v, we have v(M1 N1)v = M1/2(I + C)1CM1/2v = u(I + C)1Cu (where = M1/2v) Now, since (I + C)1 S0 and S0 commute, the matrix (I + C)1C is positive semidefinite and it has exactly the same kernel as C. Thus, if = diag(λi)Q (λi 0), we have uC(I + C)1u = This expression is strictly positive exactly when has component in any eigen-subspace with λi > 0 i.e when ker(C). Since M1/2 S0, Cu = 0 = M1/2M1/2u = 0 = M1v = 0. Thus, this expression is strictly positive if M1v = 0. (Qu)2 0. λi 1 + λi Now, from the premise vv > 0, it follows that = 0. Since 0, M1/2 is invertible, is also not the zero matrix. Since 0, this means that must have at least one strictly positive eigenvalue. Let λ > 0 be such an eigenvalue, and let = 0 be corresponding eigenvector. Define, := M1/2z = 0. Thus, we have x(M1 N1)x = zC(I + C)1z = λ 1+λ z2 > 0, showing the existence of non-zero vector such that M1x > N1x. Theorem 1 (Restatement of Theorem 1). Let ˆθX, ˆθY be the least-squares estimators for θ using only {Xi} and only {Yj} and let ˆθX,Y be the joint estimator using both unpaired datasets. Then, under the assumption that at least one Bc,j where {1, 2, ...Ny} has full rank, the common-factor covariance satisfies the strict Loewner ordering i.e. or equivalently, the Fisher information on θc strictly increases when combining Var both modalities, despite not having sample-wise pairing: (IX + IY)θc,θc (IX)θc,θc . Var ˆθX,Y ˆθX θc,θc θc,θc (cid:16) (cid:17) (cid:17) (cid:16) , Proof. For any statistic S(θ) = θ log p(x; θ) and vector v, I(θ) = vE[S(θ)S(θ)] = (cid:104) (vS(θ))2(cid:105) 0. Thus, Fisher Information Matrix is positive semidefinite matrix. 28 In our linearGaussian model, the Xdataset contributes (IX)θc,θc = Nx gives (IY)θc,θc = Ny the θc subspace. Now, if at least one Bc,j Rmdc has full column rank dc, then for any Rdc {0}, c,i Ac,i and the Ydataset c,jBc,j. Since at least one Bc,j has full column rank, (IY)θc,θc is positive-definite on i=1 j=1 vB c,jBc,j = Bc,jv2 > 0. Hence, each summand in (IY)θc,θc is positive semidefinite and at least one is positive definite, so their sum B c,jBc,j is positive definite on the θc subspace. Thus, (IX)θc,θc (IX)θc,θc + (IY)θc,θc = (IX + IY)θc,θc Now, for regular exponential families (including Gaussian linear models), the covariance matrix of the maximum likelihood estimator (cid:98)θ near the true θ0 is (asymptotically) the inverse of the Fisher information matrix i.e. Var((cid:98)θ) I(θ0)1. Precisely, as the sample size , we have: n( ˆθ θ0) (0, I(θ0)1), where θ0 is the true parameter value, I(θ0) is the Fisher Information Matrix evaluated at θ0 and (0, I(θ0)1) denotes multivariate normal distribution with mean 0 and covariance matrix I(θ0)1. Thus, we compare variances via the MoorePenrose pseudoinverse of the information matrices. Let MX = (IX)θc,θc , MY = (IY)θc,θc and MX,Y = (IX + IY)θc,θc . Since MY 0, MX,Y = MX + MY is X,Y. We have established MX MX,Y. ), we apply Lemma 1 to get M1 X,Y also positive definite (as MX,Y MY 0). Thus, Var( ˆθX,Y) = M1 Assuming MX is positive definite (to define the matrix Var M1 ˆθX,Y θc,θc (cid:17) (cid:16) . Thus, (cid:16) Var ˆθX,Y (cid:17) θc,θc = X,Y M1 (cid:16) = Var ˆθX (cid:17) , θc,θc (cid:16) it has finite variance along such i.e. Var This proves the statement under the condition that MX is positive definite. Note here that, on spaces (cid:16) = . Since MX,Y is positive definite, unidentifiable by X-alone i.e. ker(MX), we have Var < , thus strictly reducing the variance of the θc,θc estimator. Thus, adding the unpaired Y-modality strictly reduces the variance (or, dually, increases the Fisher information) on the common factors θc. ˆθX,Y ˆθX θc,θc (cid:17) (cid:17) Theorem 2 (Restatement of Theorem 2). Let all notation be as in Theorem 1, and define MX := (IX)θc,θc , MY := (IY)θc,θc , and MXY := MX + MY. Let Rdc {0}. If there exists at least one index {1, 2, ...Ny} such that Bc,jv = 0, then the following hold: 1. The Fisher information strictly increases in direction i.e. MXY > MXv. (cid:17) (cid:16) 2. The variance of the estimator in direction is strictly reduced i.e Var v, if range(MX). For range(MX), this strict inequality holds for under an additional invertibility condition and is always guaranteed for some range(MX) i.e. s.t. Var < Var < Var ˆθX,Y ˆθX,Y ˆθX θc,θc θc,θc θc,θc u. (cid:16) (cid:16) (cid:17) (cid:17) ˆθX (cid:16) (cid:17) θc,θc Proof. Define MX := (IX)θc,θc , MY := (IY)θc,θc , andMXY := MX + MY. By assumption, such that Bc,jv = 0. Thus: MYv = Ny j=1 Bc,jv2 Bc,jv2 > 0. 29 Hence MY is positive-definite in direction v, implying MX,Y MX in this direction: MXYv = MXv + MYv > MXv, thus proving the first part of the theorem. Case 1: / Range(MX). If / Range(MX), then has non-zero component in ker(MX). Let = vS + vK, where vS Range(MX) and vK ker(MX) with vK = 0. The linear combination of parameters vθc = θc. Since vK ker(MX), the component θc is not identifiable by the X-only model. Consequently, the asymptotic variance of an unbiased estimator for vθc using only the X-dataset is infinite. We denote this as vVar( ˆθX)θc,θc = . θc + The strict inequality MXYv > 0, ensures that / ker(MXY), and thus Range(MXY). Since XYv is well-defined. Furthermore, because MXY is positive semidefinite, XY is also positive semidefinite and shares the same kernel as MXY (since MXY is symmetric). As = 0 Range(MXY) and = 0, M and / ker(MXY), thus / ker(M XYv is finite positive value. Thus, XY), which ensures M vVar( ˆθX,Y)θc,θc < . Comparing this to the variance from the X-only model in this case: vVar( ˆθX,Y)θc,θc < = vVar( ˆθX)θc,θc v, and the strict inequality holds. Case 2: Range(MX). Let := Range(MX) and let PS be the orthogonal projector onto S. Because MX = MX PS and MXY = MX + MY, the restrictions MX := PS MX PS, MXY := PS MXY PS = MX + PS MY PS are positive-definite on S; To see this, take any non-zero S. Since range(MX), PSw = w; hence MXw = MXw > 0 (PS is identity when restricted to S) Thus MX 0 on S. Because PS MY PS 0, adding it preserves positive-definiteness, so MXY = MX + PS MY PS MX 0 on S. Applying Lemma 3(1) to MX and MXY on gives us M1 holds if and only if the condition Cv := (( MXY MX) M1 M1 Cv holds, the directional variance along this constrained space is strictly reduced: v. Strict inequality M1 XYv < = 0) is met. Therefore, if condition XYv M1 vVar( ˆθX,Y)θc,θc = M1 XYv < M = vVar( ˆθX)θc,θc v1. Further, from Lemma 3(2), there exists some non-zero vector such that M1 XYu < M1 u. Thus we have, Thus, completing the proof. uVar( ˆθX,Y)θc,θc < uVar( ˆθX)θc,θc u. 1We note that true asymptotic variance defined as vVar( ˆθX,Y)θc,θc = M XYv, M XYv = M1 XYv if is an invariant subspace of MXY and MXY is block-diagonal with respect to and (i.e., PS MXY PS = 0, which implies PS MY PS = 0). 30 Corollary 1. Assume direction Rdc {0} with = v(IX)θc,θc > 0 and = v(IY)θc,θc > 0 where is the common eigenvector of (IX)θc,θc and (IY)θc,θc . Then the variance in direction contracts by the factor vVar( ˆθX,Y) vVar( ˆθX) = 1/(a + b) 1/a = + < 1, So the joint estimator achieves strictly lower error along v. Proof. Let MX = (IX)θc,θc and MY = (IY)θc,θc . By assumption, is common eigenvector of MX and MY. Thus, MXv = λXv and MYv = λYv for some eigenvalues λX and λY. From the assumptions, we have λX = a/v2 > 0 and λY = b/v2 > 0. Since MX is symmetric and MXv = λXv with λX > 0, the pseudoinverse acts as v. Therefore, the variance in direction for the X-only estimator is Xv = λ vVar( ˆθX)θc,θc = M Xv = v(λ1 v) = λ1 v2 = a1v4. Since is common eigenvector, it is also an eigenvector of MXY = MX + MY: (MX + MY)v = MXv + MYv = λXv + λYv = (λX + λY)v. The corresponding eigenvalue is λXY = λX + λY. Since λX > 0 and λY > 0, λXY > 0. Thus, (MX + MY)v = (λX + λY)1v. The variance in direction for the joint estimator is vVar( ˆθX,Y)θc,θc = v(MX + MY)v = (λX + λY)1v2 = (a + b)1v4. Now, we form the ratio of these variances: vVar( ˆθX,Y)θc,θc vVar( ˆθX)θc,θc = λX λX + λY = + < 1. Corollary 2. Assume direction Rdc {0} with v(IX)θc,θc = 0 and v(IY)θc,θc > 0. Then vVar( ˆθX) = and vVar( ˆθX,Y) < i.e. direction unidentifiable from alone becomes well-posed with even unpaired data from Y. Proof. This corollary follows directly from Case 1 of Theorem 2. The condition v(IX)θc,θc = 0 for = 0 implies ker((IX)θc,θc ), and thus range((IX)θc,θc ). Given the additional condition v(IY)θc,θc > 0, the conclusions of Case 1 of the theorem apply directly. Corollary 3 (Variance Reduction for Eigenvectors of MX). Let Rdc {0} be an eigenvector of MX = (IX)θc,θc with corresponding eigenvalue λX > 0. If the Y-dataset provides information in this direction (i.e., MYv > 0, where MY = (IY)θc,θc ), then the variance in direction is strictly reduced by incorporating the Y-dataset: vVar( ˆθX,Y)θc,θc < vVar( ˆθX)θc,θc v. Specifically, vVar( ˆθX)θc,θc = λ1 v2. Proof. Let MX = (IX)θc,θc and MY = (IY)θc,θc . Since is an eigenvector of MX with positive eigenvalue λX > 0, it follows that Range(MX). Let = Range(MX). The variance using only the X-dataset in direction is given by vVar( ˆθX)θc,θc = M Xv. Because is an eigenvector of MX with λX > 0, Xv = λ1 v. Thus, vVar( ˆθX)θc,θc = v(λ1 v) = λ1 v2. 31 This scenario falls under Case 2 of Theorem 2, specifically its conclusion regarding S. According to that theorem, strict variance reduction vVar( ˆθX,Y)θc,θc < vVar( ˆθX)θc,θc occurs if the condition Cv = ((PS MY PS)(MXS)1v = 0) holds. Here, PS is the orthogonal projector onto S, and MXS is the restriction of MX to S, so (MXS)1v = λ1 v. The condition Cv thus becomes (PS MY PS)(λ1 v) = 0. Since λX > 0, this is equivalent to PS MY PSv = 0. We are given that MYv > 0. As S, PSv = v. Therefore, MYv = vPS MY PSv > 0. Let AS = PS MY PS restricted to S. AS is positive semidefinite operator on S. The condition ASv > 0 for S, = 0 implies that ASv = 0 (because if ASv = 0, then ASv = 0, which contradicts ASv > 0). Thus, PS MY PSv = 0, which means the condition Cv is satisfied. Since and the condition Cv for strict inequality is met, by Theorem 2, it follows that vVar( ˆθX,Y)θc,θc < vVar( ˆθX)θc,θc v. Theorem 3 (Restatement of Theorem 3). Define for any m, c,i Ac,i and (m) , then there exists nonzero Rdc such that Y (m) = i=1 (m) = (m) v. > j=1 c,jBc,j. If range (cid:16) (cid:17) (m) range (cid:16) (cid:17) (m) (cid:17) (cid:16) Proof. Let RX := range . By the assumption RY RX, choose vector RY RX. Since Rdc is finite dimensional inner product space and RX is its finite dimensional subspace, we can decompose = + with RX and . Because / RX, the orthogonal component is non-zero. (cid:16) , RY := range (m) (m) (cid:17) (i) Term from (m) . From the Fundamental Theorem of Linear Algebra, for any symmetric matrix S, ker = range(S); hence = ker (m) . Thus (m) = 0. (ii) Term from (m) . Because RY = range(I (m) (m) = v = 0. Then ker contradiction, that v2 > 0 because while = 0. This contradicts w; therefore semidefiniteness, (m) (m) ), there exists with = u. Suppose, for , so w. But = (w + v) = + v2 = (m) = 0 and, by positive (m) > 0. Combining the above inequalities yields (m) > (m) v, with = 0, which is the desired inequality. UML: Algorithm Pseudocode In this section we present the full pseudocode for Uml for both the self-supervised and supervised settings as shown in Algorithm 1 and Algorithm 2 respectively. 32 Algorithm 1 Pytorch Pseudocode for Uml in the self-supervised setting 1 # f_img , f_text : image encoder , text encoder 2 # g_img , g_text : image decoder , text decoder 3 # : shared backbone 4 5 while not converged : # training loop 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 x_img = fetch_next ( image_loader ) x_text = fetch_next ( text_loader ) # image minibatch # text minibatch ( random / unaligned ) x_img = patc hi fy_and_embed ( x_img ) # input image patch embeddings x_text = to kenize_and_embed ( x_text ) # input text token embeddings z_img = f_img ( x_img ) z_text = f_text ( x_text ) # image patch embeddings # text token embeddings y_img = g_img ( ( z_img ) ) y_text = g_text ( ( z_text ) ) # predict text patch embeddings # predict image patch embeddings # Next Token / Patch Embedding Prediction Loss loss_img = MSE ( y_img [: ,: -1 ,:] , x_img [: ,1: ,:]) loss_text = MSE ( y_text [: ,: -1 ,:] , x_text [: ,1: ,:]) loss = loss_img + lambda * loss_text # total loss loss . backward () update (h , f_img , f_text , g_img , g_text ) # back - propagate # SGD update 25 26 # Define Mean Squared Error loss 27 def MSE ( pred , target ) : 28 return (( pred - target ) ** 2) . mean () Algorithm 2 Pytorch Pseudocode for Uml in the supervised setting 1 # f_img : image encoder ( frozen or trainable ) 2 # f_text : text encoder ( frozen ) 3 # is_trainable : True if f_img is trainable else False 4 # : classification head 5 6 while not converged : # training loop 7 9 10 11 12 13 15 16 17 18 19 21 x_img = fetch_next ( image_loader ) x_text = fetch_next ( text_loader ) # image minibatch # text minibatch ( random / unaligned ) z_img = f_img ( x_img ) z_text = f_text ( x_text ) # image embeddings # text embeddings logits_img = ( z_img ) logits_text = ( z_text ) # predict image labels # predict text labels loss_img = CE ( logits_img , labels_img ) loss_text = CE ( logits_text , labels_text ) loss = loss_img + lambda * loss_text # total loss # image classification loss # text classification loss loss . backward () update (h , f_img ) # back - propagate if is_trainable else update ( ) # SGD update 22 23 # Define Cross - Entropy loss 24 def CE ( logits , labels ) : return - sum ( labels * log_softmax ( logits , dim =1) ) / len ( labels )"
        },
        {
            "title": "E Additional Experiments",
            "content": "E.1 Improving Image Classification using Unpaired Texts (Unaligned encoders) In this section we report image-classification results on ten benchmarks (see Section B.3), covering three settings: 1. Full-dataset fine-tuning: train both the vision backbone and classification head (Section E.1.1). 2. Full-dataset linear probe: train only the classification head (Section E.1.2). 3. Few-shot linear probe: train only the classification head under few-shot conditions (Section E.1.3). In each setting, we compare Uml with baselines across all datasets and multiple DINO-initialized vision backbones. Our method has two variants: Ours (Uml), where we alternately train with both image and unpaired text data (see Algorithm 2), and Ours (init) where we initialize the classifier with the average text embedding of each class, providing strong prior to align image and class level information. E.1.1 Supervised Finetuning (across architectures) In this section, we fine-tune both the vision backbone and the linear classifier on ten downstream tasks, comparing Uml against strong image-only baselines. We evaluate four DINO-initialized backbones: ViT-B/16 in Table 6 ViT-B/8 in Table 7 DINOv2 ViT-S/14 in Table DINOv2 ViT-B/14 in Table 9 Results for DINOv2 ViT-L/14 are omitted due to computational constraints. Across all backbones, Uml consistently improves over the image-only baseline by leveraging unpaired text embeddings. For some backbones such as DINOv2 VIT-B/16, our head-initialization variant (Ours (init)) outperforms training using unpaired multimodal data from scratch (Ours), while in others it does not. 35 Table 6: Full finetuning on classification with ViT-B/16 DINO and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when fine-tuning on the target dataset. All vision encoders are initialized from DINO weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. C f S 7 9 3 S a i G F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 78.41 82.56 81.95 63.99 67.04 67.12 62.12 67.38 68.29 74.17 76.42 73.84 81.43 84.06 84. 82.38 81.79 81.12 92.00 93.20 92.60 98.24 98.98 98.73 96.31 97.04 96.84 81.01 83.16 82.76 Table 7: Full finetuning on classification with ViT-B/8 DINO and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when fine-tuning on the target dataset. All vision encoders are initialized from DINO weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. a o t 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 85.67 87.95 87.44 68.04 70.28 70.03 72.60 75.31 76.09 76.65 77.19 76.24 83.94 85.59 86. 85.32 84.83 84.71 93.06 93.05 93.81 99.22 99.43 99.27 96.82 97.12 97.16 84.59 85.64 85.69 Table 8: Full finetuning on classification with ViT-S/14 DINOv2 and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when fine-tuning on the target dataset. All vision encoders are initialized from DINOv2 weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. a o t 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 79.45 84.87 86.39 66.20 66.72 66.03 66.99 71.54 73.44 72.16 74.14 74.27 83.18 84.77 84. 80.65 81.16 81.97 90.67 91.87 91.72 99.18 99.55 99.82 95.45 97.03 97.60 81.54 83.52 83.99 Table 9: Full finetuning on classification with ViT-B/14 DINOv2 and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when fine-tuning on the target dataset. All vision encoders are initialized from DINOv2 weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. C f S 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 D 1 0 1 F w d x e o 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 89.62 90.93 90.73 71.45 70.97 70.92 77.29 80.02 80.23 73.88 75.83 75.87 88.00 87.52 87. 82.94 86.25 83.43 94.55 94.74 94.47 99.88 99.88 99.80 97.69 97.57 97.93 86.14 87.08 86.77 E.1.2 Linear Probing (across architectures) In this section, we train only the linear classifier, on top of the frozen vision and language backbone, on ten downstream tasks, comparing Uml against strong image-only baselines. We evaluate five DINO-initialized backbones: ViT-B/16 in Table 10 ViT-B/8 in Table 11 DINOv2 ViT-S/14 in Table 12 DINOv2 ViT-B/14 in Table DINOv2 ViT-L/14 in Table 14 Across all backbones, Uml consistently improves over the image-only baseline by leveraging unpaired text embeddings. For all backbones, our head-initialization variant (Ours (init)) outperforms training using unpaired multimodal data from scratch (Ours). Table 10: Full linear probing on classification with ViT-B/16 DINO and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when training linear probe on the target dataset. All vision encoders are initialized from DINO weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. C f S 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A Unimodal Ours Ours (init) 67.10 68.71 68.60 64.63 65.14 65.59 56.02 57.42 57.98 72.42 72.95 73. 81.27 82.06 82.40 74.96 75.30 75.73 93.07 93.18 93.62 98.32 98.46 98.42 95.01 96.19 96.35 78.08 78.82 79. 38 Table 11: Full linear probing on classification with ViT-B/8 DINO and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when training linear probe on the target dataset. All vision encoders are initialized from DINO weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. C f S 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 72.01 72.93 72.81 67.19 68.17 68.36 62.02 63.49 64.09 76.18 77.13 76.48 82.95 83.16 83. 78.57 79.87 80.01 91.99 92.59 92.50 98.78 98.50 98.74 96.23 96.47 96.43 80.66 81.37 81.46 Table 12: Full linear probing on classification with ViT-S/14 DINOv2 and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when training linear probe on the target dataset. All vision encoders are initialized from DINOv2 weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. a o t 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A 77.48 78.45 78.58 70.72 71.53 72.24 66.28 67.33 67.50 78.25 78.70 79.51 82.64 83.51 83. 84.39 84.67 84.74 94.29 94.70 94.78 99.62 99.82 99.89 97.00 97.11 97.15 83.40 83.98 84."
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) Table 13: Full linear probing on classification with ViT-B/14 DINOv2 and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when training linear probe on the target dataset. All vision encoders are initialized from DINOv2 weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. C f S 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 85.46 85.40 85.74 75.42 75.22 75.70 72.34 75.22 74.17 79.73 80.73 81.32 87.26 87.21 87. 88.70 89.02 88.78 95.56 95.83 95.78 99.76 99.88 99.88 97.81 97.85 97.93 86.89 87.37 87.40 Table 14: Full linear probing on classification with ViT-L/14 DINOv2 and OpenLLaMA-3B. We compare our proposed approach with the image-only baseline when training linear probe on the target dataset. All vision encoders are initialized from DINOv2 weights, and our approach leverages unpaired text data using OpenLLaMA-3B embeddings. C f S 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 88.16 88.45 87.99 77.26 77.20 77.75 74.32 76.93 77.20 81.56 82.39 82.51 89.82 90.19 90. 90.95 91.09 91.29 96.27 96.51 96.32 99.84 99.92 99.92 97.97 98.01 97.93 88.46 88.97 89.01 E.1.3 Few-shot Linear Probing (across architectures) In this section, we train only the linear classifier, on top of the frozen vision and language backbone, for few-shot classification on ten downstream tasks, comparing Uml against strong image-only baselines. We evaluate five DINO-initialized backbones: ViT-B/16 in Table 16, ViT-B/8 in Table 15, DINOv2 ViT-S/14 in Table 17, DINOv2 ViT-B/14 in Table 19, DINOv2 ViT-L/14 in Table 19. Across all backbones, Uml consistently improves over the image-only baseline by leveraging unpaired text embeddings. For all backbones, our head-initialization variant (Ours (init)) outperforms training using unpaired multimodal data from scratch (Ours). Table 15: Linear evaluation of frozen features on 11 fine-grained benchmarks for few-shot learning. We compare our proposed approach with the image-only baseline by training linear classifier on top of frozen VIT-B/8 DINO features. Our method leverages unpaired text data using OpenLLaMA-3B C f S 7.40 7.71 9.24 14.43 15.71 16. 25.67 27.30 28.54 41.04 43.76 44.16 57.72 60.11 60.36 r A F 12.16 13.56 14.49 20.28 21.04 22. 29.39 31.43 31.31 40.03 42.56 42.30 52.63 54.53 54.81 7 9 3 26.37 28.01 34.23 37.96 40.74 45. 49.23 51.23 53.68 56.86 58.14 59.80 61.74 63.21 64.26 Dataset 1 0 1 39.75 42.08 47. 53.03 55.86 59.02 64.27 66.72 67.47 72.39 73.13 74.30 76.18 78.13 78.76 28.62 33.22 36. 39.80 43.74 45.43 52.52 54.31 56.13 61.15 63.12 64.46 67.69 69.33 70.27 1 0 1 F 19.23 21.13 24. 30.62 33.52 35.89 43.82 45.58 47.40 54.47 56.30 57.07 62.63 63.74 64.13 e I 42.81 43.27 46.75 54.75 54.49 56.78 61.64 61.51 62.84 66.10 66.36 67.18 68.87 69.44 70.05 w d x 58.22 58.61 61.59 77.59 77.18 77.94 87.41 87.96 88.29 93.95 94.25 94.00 96.41 96.89 96.63 P f 54.97 55.85 60.09 68.12 69.86 71.57 75.85 77.51 79.10 82.30 84.27 84.85 87.31 87.73 88.23 1 0 1 t C 74.13 77.51 80.23 81.91 84.52 86.06 90.36 91.36 91.98 92.28 92.71 93.24 94.27 94.54 94.73 r A 36.37 38.10 41.52 47.85 49.67 51.70 58.02 59.49 60.67 66.06 67.46 68.14 72.54 73.76 74.22 Train Shot Method 1 2 4 8 16 Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) 41 Table 16: Linear evaluation of frozen features on 11 fine-grained benchmarks for few-shot learning. We compare our proposed approach with the image-only baseline by training linear classifier on top of frozen VIT-B/16 DINO features. Our method leverages unpaired text data using OpenLLaMA-3B a o t 6.28 7.89 8.96 12.64 14.38 15.99 22.60 24.83 25.83 37.68 39.31 40.50 52.48 55.84 55. a i g 9.72 10.41 12.12 14.98 17.00 17.65 24.27 25.76 26.35 33.67 35.56 35.64 45.34 47.70 48. 7 9 3 22.43 26.08 31.34 35.64 38.62 42.31 45.95 48.62 51.01 52.94 55.31 57.03 58.27 60.57 61. Dataset 1 0 1 37.85 40.27 44.32 51.14 54.28 56.46 63.00 64.39 65.86 70.62 71.88 73. 75.72 76.81 77.39 29.22 32.45 34.22 38.93 40.37 42.89 50.30 52.64 55.06 59.18 60.48 62. 64.81 66.21 67.02 1 0 1 F 15.40 18.14 21.46 26.05 29.24 32.15 38.51 40.74 42.69 49.48 50.46 51. 56.24 58.26 58.76 e I 38.67 39.28 42.68 50.34 50.83 52.90 57.99 57.96 59. 62.97 63.08 64.09 66.36 66.47 67.08 w d x 54.62 58.32 60.37 75.61 77.14 77.32 85.60 87.20 87. 92.83 93.23 93.59 95.90 96.55 96.62 P f 60.12 60.88 66.39 70.84 72.88 74.82 80.14 80.92 82. 85.26 86.25 86.93 88.57 89.60 90.53 1 0 1 t 73.25 75.66 79.74 83.16 85.95 87.34 89.67 91.17 91. 93.17 93.47 93.71 94.27 95.12 94.98 r 34.76 36.94 40.16 45.93 48.07 49.98 55.80 57.42 58. 63.78 64.90 65.84 69.80 71.31 71.81 Train Shot Method 1 2 8 16 Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Table 17: Linear evaluation of frozen features on 11 fine-grained benchmarks for few-shot learning. We compare our proposed approach with the image-only baseline by training linear classifier on top of frozen VIT-S/14 DINOv2 features. Our method leverages unpaired text data using OpenLLaMA-3B C f S 13.18 14.95 16.49 24.68 26.93 28.65 38.76 41.69 43. 54.56 56.27 57.91 67.96 69.42 70.32 r A F 14.09 14.88 15.63 23.09 24.29 24.78 32.10 33.38 33. 45.05 45.98 47.40 55.89 58.54 58.74 7 9 3 34.15 37.25 41.79 47.88 49.65 53.15 57.51 58.87 60. 63.00 64.57 65.82 67.35 68.50 69.19 Train Shot Method 1 2 8 16 Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Dataset 1 0 1 F 35.18 37.91 42.27 48.54 51.77 54.44 60.79 62.69 63. 68.06 69.22 69.71 73.24 73.80 74.51 e I 36.48 38.35 42.69 50.41 51.31 54. 58.73 59.69 61.38 64.53 65.14 66.40 68.14 68.70 69.44 w d x 89.62 91.42 93.64 96.02 96.90 97. 98.59 98.84 99.17 99.27 99.27 99.54 99.63 99.72 99.82 P f 63.51 68.92 73.59 75.32 79.44 81. 83.89 86.27 87.36 88.68 89.78 90.29 90.73 91.87 92.47 1 0 1 t 76.66 84.04 84.52 86.90 89.80 90. 93.48 94.56 94.96 94.35 95.42 95.84 96.43 96.63 96.80 r 44.62 47.58 50.50 55.73 58.28 60. 65.12 66.71 67.82 71.65 72.71 73.67 76.22 77.80 78.81 1 0 1 46.74 49.18 52.33 56.81 61.67 63. 67.75 69.60 71.13 74.19 75.19 75.99 77.92 78.69 79.58 36.60 38.93 42.04 47.75 50.99 53. 59.69 61.58 62.43 64.78 66.31 67.81 71.36 72.24 73.17 42 Table 18: Linear evaluation of frozen features on 10 fine-grained benchmarks for few-shot learning with DINOv2 ViT-B/14. We compare our proposed approach with the image-only baseline by training linear classifier on top of frozen VIT-B/14 DINOv2 features. Our method leverages unpaired text data using OpenLLaMA-3B C f S 22.42 23.10 25.47 35.17 37.38 38.78 51.40 54.26 55.01 66.01 68.53 67.91 77.31 78.92 78.52 r A F 15.79 16.22 16.83 25.54 25.88 26.00 34.25 35.52 35.14 48.17 50.88 51.26 62.38 64.51 65.81 7 9 3 S 43.03 45.12 48.56 55.41 56.98 59.81 63.68 64.65 66.55 68.88 69.75 70.66 72.17 72.80 73.18 Dataset 1 0 1 58.57 61.05 63.53 69.49 70.61 71.38 76.32 76.87 77.57 79.92 81.44 81.85 83.80 84.62 84. D 38.85 42.69 45.31 51.16 54.65 55.61 61.25 62.63 63.97 66.67 68.46 69.56 73.76 75.16 75. 1 0 1 F 48.71 51.30 54.16 62.13 63.89 66.54 71.60 72.33 73.25 76.26 77.34 77.95 80.74 81.00 81. n m 52.26 52.45 55.56 62.35 63.21 65.06 68.86 69.14 70.30 72.48 73.12 73. 75.15 75.46 75.82 w d x 97.12 98.08 97.94 99.58 99.70 99.62 99.76 99.70 99.57 99.80 99.70 99. 99.81 99.59 99.78 P f 76.47 78.14 81.08 84.31 85.50 86.49 89.05 90.00 90.31 90.97 92.39 92. 93.34 92.92 93.28 1 0 1 t 83.64 87.68 88.13 89.55 92.02 92.79 94.51 95.51 95.65 95.54 96.20 96. 97.40 97.38 97.57 r 53.69 55.58 57.66 63.47 64.98 66.21 71.07 72.06 72.73 76.47 77.78 78. 81.59 82.24 82.56 Train Shot Method 1 2 4 16 Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) 43 Table 19: Linear evaluation of frozen features on 10 fine-grained benchmarks for few-shot learning with DINOv2 ViT-L/14. We compare our proposed approach with the image-only baseline by training linear classifier on top of frozen VIT-L/14 DINOv2 features. Our method leverages unpaired text data using OpenLLaMA-3B C f S 24.89 25.88 27.90 39.95 41.22 42.93 56.49 58.19 58. 70.00 71.63 72.02 80.84 81.85 82.76 r A F 17.69 18.08 18.95 26.87 27.15 28.14 38.59 39.57 39. 51.57 55.13 55.49 64.13 69.45 69.42 7 9 3 48.36 49.63 52.86 58.95 60.82 63.36 66.37 67.36 68. 70.71 71.59 72.51 73.83 74.39 74.80 Dataset 1 0 1 66.46 69.18 70.98 75.79 76.61 77. 80.84 81.36 81.50 83.84 84.47 84.57 87.43 87.35 87.65 38.77 42.93 43.18 50.18 53.01 54. 59.08 61.78 62.77 66.47 67.91 69.03 73.96 74.70 74.88 1 0 1 F 59.27 60.12 63.17 70.74 72.07 73. 77.39 78.19 78.99 81.69 82.12 82.52 84.58 84.58 84.96 e I 57.50 58.37 60. 67.14 67.90 69.20 72.41 72.82 73.63 76.02 76.43 76.78 77.78 78.35 78.58 w d x 98.13 98.42 98. 99.74 99.72 99.81 99.73 99.76 99.88 99.89 99.88 99.89 99.91 99.89 99.81 P f 79.83 83.51 83. 84.71 86.07 87.13 89.90 90.99 90.74 93.53 93.62 93.80 94.69 94.59 94.42 1 0 1 t 82.96 86.23 88. 89.82 91.95 91.71 94.44 95.27 96.02 95.55 96.36 96.73 97.36 97.61 97.62 r 57.39 59.24 60. 66.39 67.65 68.88 73.52 74.53 75.02 78.93 79.91 80.33 83.45 84.28 84.49 Train Shot Method 2 4 8 16 Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) E.2 Improving Image Classification using Unpaired Texts (Aligned encoders) E.2.1 Supervised Finetuning In this section, we fine-tune both the vision backbone and the linear classifier on nine downstream tasks, comparing Uml against strong image-only baselines. We evaluate two different backbones: ResNet-50 and VIT-B/16. As shown in Table 20, across all backbones, Uml consistently improves over the image-only baseline by leveraging unpaired text embeddings. Further, our head-initialization variant (Ours (init)) outperforms training using unpaired multimodal data from scratch (Ours). E.2.2 Linear Probing In this section, we train only the linear classifier, on top of the frozen vision and language backbone from CLIP, on ten downstream tasks, comparing Uml against strong image-only baselines. As shown in Table 21, Uml consistently improves over the image-only baseline by leveraging unpaired text embeddings. Further, our head-initialization variant (Ours (init)) outperforms training using unpaired multimodal data from scratch (Ours). Table 20: Supervised finetuning on 9 fine-grained classification benchmarks with CLIP. We compare our proposed approach with the image-only baseline when fine-tuning on the target dataset. All vision encoders are initialized from CLIP ResNet50 weights, and our approach leverages unpaired text data using the corresponding CLIP text encoder. C f S r A F 7 9 3 S"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 d 1 0 1 F w d x e o 1 0 1 t C a A 36.12 37.00 72.75 25.93 24.05 62.33 37.70 41.34 66.58 51.06 55.67 56.50 52.49 60.48 67. 69.24 69.77 76.95 63.17 74.49 86.97 88.42 92.57 94.80 83.61 84.79 87.95 56.42 60.02 74."
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) Table 21: Full linear probing on classification with CLIP ResNet-50 Image Encoder and Text encoder. We compare our proposed approach with the image-only baseline when training linear probe on the target dataset. All vision encoders are initialized from ResNet-50 weights, and our approach leverages unpaired text data using the corresponding CLIP text embeddings. C f S 7 9 3 t c C F"
        },
        {
            "title": "Dataset",
            "content": "1 0 1 1 0 1 F s o r O P f 1 0 1 t C a A"
        },
        {
            "title": "Method",
            "content": "Unimodal Ours Ours (init) 76.36 77.23 79.14 70.97 71.18 73.83 41.88 42.66 42.81 72.81 71.81 73.76 81.23 81.81 82. 81.60 81.51 82.44 88.39 87.84 90.90 97.89 97.65 97.69 92.78 93.01 94.19 78.21 78.30 79.65 E.2.3 Few-shot linear Probing (across architectures) In this section, we train only the linear classifier, on top of the frozen vision and language backbone from CLIP, for few-shot classification on ten downstream tasks, comparing Uml against strong image-only baselines. We evaluate two different backbones: ResNet-50 and VIT-B/16. As shown in Table 22 and Table 23, across both backbones, Uml consistently improves over the image-only baseline by leveraging unpaired text embeddings. Further, our head-initialization variant (Ours (init)) outperforms training using unpaired multimodal data from scratch (Ours). E."
        },
        {
            "title": "Improving Visual Robustness Using Unpaired Texts",
            "content": "In this section, we evaluate the robustness of models trained with Uml to test-time distribution shifts. We train k-shot linear probe (where {1, 2, 4, 8}) with DINOv2 on ImageNet and evaluate across four distribution-shifted target datasets: ImageNet-V2, ImageNet-Sketch, ImageNet-A, and ImageNet-R. Our 45 Table 22: Linear evaluation of frozen features on 10 fine-grained benchmarks for few-shot learning. We compare our proposed approach with the image-only baseline by training linear classifier on top of frozen CLIP ResNet50 features. Our method leverages unpaired text data using the corresponding CLIP text encoder Dataset C f S 23.24 36.32 57. 38.37 46.64 61.86 51.34 55.21 65.80 61.74 62.75 69.78 70.94 71.58 74.56 r A F 12.38 16.84 22. 18.63 20.81 24.19 23.08 24.77 27.49 30.22 30.69 31.62 35.91 36.23 37.13 7 9 3 29.14 45.40 64. 43.83 53.53 65.90 54.38 59.48 68.11 61.47 63.70 69.61 65.53 67.08 71.33 1 0 1 37.55 53.19 65. 53.25 62.01 70.39 64.06 67.65 73.62 70.16 70.74 77.24 75.13 76.09 78.66 1 0 1 F 27.26 49.76 76. 44.60 56.67 77.07 57.29 62.68 77.79 64.63 67.73 78.58 70.67 71.63 79.06 30.24 40.92 50. 40.33 48.35 55.30 52.07 56.78 60.13 60.15 61.84 64.13 64.30 65.62 68.09 P f 34.61 53.03 86. 47.75 60.64 87.40 61.32 67.31 86.54 68.94 73.62 89.07 78.49 79.52 89.71 e I 21.36 36.48 60.92 32.62 42.21 61.40 41.72 47.04 62.37 49.48 52.14 63.34 55.07 56.92 64.31 w d x 59.07 68.56 81.08 75.03 77.97 86.20 86.16 86.46 91.60 92.20 92.31 94.21 95.21 95.44 96.17 1 0 1 t C 66.52 76.80 83.79 78.90 84.58 85.94 85.41 87.23 87.57 89.14 89.89 91.58 91.26 91.94 93.31 r A 34.14 47.73 65.06 47.33 55.34 67.57 57.68 61.46 70.10 64.81 66.54 72.92 70.25 71.20 75.23 Train Shot Method 1 2 4 8 16 Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) method consistently improves robustness over the unimodal baseline (Figure 12, Figure 13, Figure 14 and Figure 15) across different training shots, indicating that language priors help capture more transferable features. Figure 12: Robustness under test-time distribution shifts. Our approach (trained on 1-shot) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets. 46 Table 23: Linear evaluation of frozen features on 10 fine-grained benchmarks for few-shot learning. We compare our proposed approach with the image-only baseline by training linear classifier on top of frozen CLIP VIT-B/16 features. Our method leverages unpaired text data using the corresponding CLIP text encoder Dataset C f S 31.53 48.28 67.76 48.45 57.89 70. 61.64 66.24 74.58 71.76 72.77 78.43 78.76 79.40 82.38 r A F 17.76 22.06 32.26 23.38 27.19 33. 31.01 32.98 37.38 38.47 39.09 41.77 44.74 45.06 47.14 7 9 3 33.51 53.44 70.13 48.70 59.95 71. 60.66 65.56 73.54 66.67 69.50 75.07 71.49 72.19 76.51 1 0 1 43.64 63.40 75.02 60.08 69.60 78. 70.49 74.16 81.10 77.11 79.01 83.41 80.43 81.97 84.66 1 0 1 F 39.40 63.92 84.25 58.30 71.18 85. 71.91 76.19 86.05 78.16 80.07 86.87 82.08 82.12 86.60 31.72 47.04 55.16 42.04 52.27 60. 54.37 59.95 64.30 61.96 64.89 68.50 68.79 69.41 72.13 P f 37.43 60.95 90.91 53.56 66.78 90. 69.35 75.92 91.64 78.25 80.85 92.55 85.16 85.92 92.68 e I 27.65 47.35 69. 41.68 54.24 70.19 52.15 58.50 70.89 59.90 62.63 71.97 63.87 64.93 72.79 w d x 67.95 77.82 87. 82.01 87.43 92.18 90.99 91.32 94.80 95.20 94.98 96.94 96.97 96.49 97.70 1 0 1 t 71.68 83.14 88. 83.20 90.20 90.09 91.08 93.23 93.70 92.98 94.36 95.27 94.54 95.28 96.08 r 40.23 56.74 72. 54.14 63.67 74.33 65.36 69.40 76.80 72.05 73.82 79.08 76.68 77.28 80.87 Train Shot Method 2 4 8 16 Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Unimodal Ours Ours (init) Figure 13: Robustness under test-time distribution shifts. Our approach (trained on 2-shots) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets. 47 Figure 14: Robustness under test-time distribution shifts. Our approach (trained on 4-shots) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets. Figure 15: Robustness under test-time distribution shifts. Our approach (trained on 8-shots) is much more robust than its unimodal counterpart across four distribution-shuffled target test sets. 48 E.4 Marginal Rate-of-Substitution Between Modalities How many words is an image worth? In this section, we extend our results to evaluate image-text conversion ratios using test accuracy isolines on the remaining eight datasets. We measure these global equivalence ratios by fitting plane to the accuracy values given the number of image and text shots. Figures 16 to 23 demonstrate the conversion ratios for DINOv2 VIT-S/14 as the vision backbone and OpenLLaMa-3B as the text backbone (unaligned encoders). Analogously, Figures 24 to 31 show the same ratios for CLIP ResNet-50 as the vision and text encoders (aligned encoders). As expected, with the fully aligned CLIP backbone, each image equates to far fewer text prompts than under the unaligned DINO setting, showing the higher efficiency of aligned embeddings. E.4.1 Unaligned Encoders Figure 16: SUN397. 1 img 1568 words Figure 17: Caltech101. 1 img 1248 words Figure 18: Stanford Cars. 1 img 1799 words Figure 19: DTD. 1 img 2309 words 49 Figure 20: FGVC Aircraft. 1 img 3220 words Figure 21: Oxford Flowers. 1 img 1895 words Figure 22: Food101. 1 img 2608 words Figure 23: UCF101. 1 img 2617 words E.4.2 Aligned Encoders (CLIP) Figure 24: SUN397. 1 img 221 words Figure 25: Caltech101. 1 img 256 words Figure 26: Stanford Cars. 1 img 649 words Figure 27: DTD. 1 img 228 words Figure 28: FGVC Aircraft. 1 img 691 words Figure 29: Oxford Flowers. 1 img 851 words Figure 30: Food101. 1 img 202 words Figure 31: UCF101. 1 img 393 words E."
        },
        {
            "title": "Impact of Scaling Vision Backbone",
            "content": "In this section, we study how our methods performance scales with the size and architecture of the vision backbone. In addition to ViT-S/14 DINOv2, we extend our analysis to range of ViT-based architectures, including ViT-B/14 and ViT-L/14 DINOv2 and ViT-B/16 and ViT-B/8 DINO models. To ensure fair comparison, we follow the same training protocol as in previous experiments. Our method consistently 51 outperforms the unimodal baselines in every setting. In few-shot linear probing across ViT-B/8, ViT-B/16, DINOv2-ViTs and ViT-L/14 backbones (Tables 15 to 19), we see clear gains. The same holds for full-dataset end-to-end fine-tuning of both encoder and head (Tables 6 to 9), and even when only the linear classifier is trained on the full splits (Tables 10 to 14). E."
        },
        {
            "title": "Impact of Varying Text Encoders",
            "content": "In this section, we study how our methods performance varies with different language models used for generating text embeddings. Through this experiment, we aim to understand how differences in embedding quality and model capacity affect the integration of textual information in our multimodal setup. Specifically, we cover LLMs with diverse architectures and scales, including BERT-Large, RoBERTaLarge and GPT-2 Large. As shown in Figure 32, adding unpaired text embeddings shows significant boost in 1-shot accuracy and still decent gains at 16 shots on SUN397 dataset. Overall, OpenLLaMA-3B outperforms all other language models. Figure 32: Few-shot classification accuracy on SUN397 using Uml with unpaired, frozen embeddings from various pretrained language models. E.7 Learning with Coarse-Grained vs. Fine-Grained Textual Cues Understanding the type of information extracted from textual cues is crucial to assessing the effectiveness of our multimodal approach. key question is whether the model merely utilizes class names or goes beyond to capture richer, more descriptive features. To investigate this, we compare the performance of our method using two types of text templates: vanilla template that consists solely of the class name (e.g., photo of [class]) and descriptive templates generated from GPT-3, as detailed in Section Section B. As shown in Figure 33 and Figure 34, both multimodal approaches consistently outperform the unimodal baseline, with descriptions from GPT-3 offering more substantial performance gain. This shows that leveraging richer, contextually diverse text cues can significantly enhance model performance, even in low-shot learning scenarios. 52 Figure 33: Few-shot SUN397 accuracy with Uml using two levels of textual granularity: (a) vanilla class descriptions and (b) GPT-3generated fine-grained descriptions. Figure 34: Few-shot SUN397 accuracy with Uml (a) (init) using two levels of textual granularity: vanilla class descriptions and (b) GPT-3generated fine-grained descriptions. E."
        },
        {
            "title": "Impact on Performance with Increasing Unpaired Text Prompts",
            "content": "Here, we investigate how classification accuracy evolves as we augment each image with an increasing number of unpaired text prompts . Figure 35 shows these accuracy curves as we vary the number of unpaired text prompts per image shot across five image-shot budgets. In every regime, our multimodal initialization (Ours (init)) outperforms training the head from scratch, with most of the gain coming from the first few prompts and gains tapering off thereafter. Note that we do not enforce diversity or novelty in the unpaired text promptssimply adding more sentences does not guarantee additional information. 53 Figure 35: Classification accuracy as function of the number of text prompts per image shot for the SUN397 Dataset. E.9 Effect of Unrelated Auxiliary Modalities While our main experiments investigate the benefits of incorporating semantically related auxiliary modalities, an important question is what happens when the auxiliary data is unrelated or even adversarial. In our unpaired setting, this requires reasoning not about individual mismatched pairs, but about the relationship between entire data distributions. To meaningfully study the adversarial case, one would need principled definition of negative correlation between modality distributions, as well as metrics for quantifying it. We leave such rigorous framework to future work. Instead, here, we evaluate simpler but informative scenario: the independent case, where the auxiliary modality is semantically unrelated to the target modality. We use our few-shot learning setup to train Uml with image data from SUN397 and text data from Stanford-Cars, two semantically unrelated datasets. Table 24 reports results for few-shot image classification on the SUN397 dataset. When the auxiliary text is unrelated to the image domain, performance does not improve over the unimodal baseline. In contrast, semantically related text provides consistent gains across all shot counts. Table 24: Training Uml with unrelated auxiliary text from the Stanford Cars dataset does not yield performance gains on image classification on the SUN397 dataset. However, semantically correlated text (from SUN397) consistently improves accuracy across few-shot settings. Method 1-shot 2-shot 4-shot 8-shot 16-shot Unimodal (Image) Uml (Image + Unrelated Text) Uml (Image + Related Text) 34.15 35.27 41.79 47.88 47.12 53.15 57.51 57.50 60.89 63.00 62.45 65. 67.35 67.25 69.19 54 E.10 Extension to More Than Two Modalities Our framework naturally extends beyond two modalities. We validate this by extending our image and audio classification experiments on ImageNet-ESC to use all three modalities: image, audio, and text. Training alternates batches from each modality while applying modality-specific classification losses, consistent with our two-modality setup. Tables 25 and 26 summarize results on audio and image classification. In both cases, incorporating additional unpaired modalities consistently improves performance over unimodal or pairwise settings. These findings demonstrate that the performance benefits of Uml extend robustly to more than two modalities. From theoretical perspective, our results also generalize directly. Since modality-specific observations are conditionally independent given the ground-truth latent , their joint contribution to the Fisher information reduces to the sum of the unimodal blocks. Consequently, the total contribution of all auxiliary modalities (excluding the primary modality X) can be obtained by summing their individual Fisher information matrices. Table 25: Training Uml with all three modalities from ImageNet-ESC outperforms unimodal or pairwise training on audio classification. Numbers in parentheses denote relative improvements over the unimodal baseline. Dataset Method 1-shot 2-shot 4-shot ESC-19 Audio-Only Audio + Image Audio + Text Audio + Image + Text ESC-27 Audio-Only Audio + Image Audio + Text Audio + Image + Text 28.78 34.59 35.47 44.46 (+54.4%) 25.65 37.15 41.97 44.68 (+74.2%) 39.85 44.13 52.19 51.48 (+29.2%) 35.99 42.86 47.02 48.03 (+33.4%) 52.22 50.00 52.90 56.57 (+8.3%) 44.79 51.15 53.85 54.16 (+20.9%) Table 26: Training Uml with all three modalities from ImageNet-ESC outperforms unimodal or pairwise training on image classification. Numbers in parentheses denote relative improvements over the unimodal baseline. Dataset Method 1-shot 2-shot 4-shot ESC-19 ESC-27 Image-Only Image + Audio Image + Text Image + Audio + Text Image-Only Image + Audio Image + Text Image + Audio + Text 60.28 64.63 88.84 90.55 (+50.2%) 55.33 59.75 86.39 88.22 (+59.5%) 74.10 76.17 89.71 91.08 (+22.9%) 65.60 70.93 88.91 88.96 (+35.6%) 78.70 85.02 92.07 91.72 (+16.5%) 77.85 78.14 90.14 91.78 (+17.9%) E.11 Effect of Ratio of Modality Batches In our main experiments, Uml was trained with simple 1:1 alternation of batches across modalities. To study the effect of this schedule, we ablate the ratio of text to image batches, denoted by r, on SUN397 55 using ViT-S/14 DINOv2 (vision) and OpenLLaMA-3B (text). We evaluate both in the (a) linear probe setting and the (b) full finetuning setting. Across both settings and for both Uml and Uml (init), we observe that the choice of has little impact on performance. The gains primarily arise from the presence of auxiliary information rather than the exact frequency of its appearance during training. Table 27: Few-shot linear probing with different ratios of text-to-image batches on SUN397 using ViT-S/14 DINOv2 and OpenLLaMA-3B. Performance remains stable across ratios r, indicating robustness of Uml to batch scheduling. Method Uml Uml (init) Shot = 0.25 = 0.5 = 1. = 2.0 = 4.0 2-shot 4-shot 8-shot 16-shot 1-shot 2-shot 4-shot 8-shot 16-shot 49.03 59.05 64.63 68.55 42.60 52.83 61.10 65.29 69. 49.23 59.06 65.24 68.79 42.13 53.01 61.09 65.32 68.91 49.65 58.87 64.57 68.50 41.79 53.15 60.89 65.82 69.19 49.56 58.92 64.98 68.86 41.95 53.04 60.63 65.15 68. 49.97 58.70 65.29 68.36 42.04 52.81 60.99 65.21 68.71 Table 28: Full finetuning of the vision encoder and linear head with different text-to-image batch ratios on SUN397. Results show that performance is largely insensitive to r. Method Uml Uml (init) = 0.25 = 0. = 1.0 = 2.0 = 4.0 66.44 66.58 66.80 65.41 66.72 66. 67.60 65.86 66.44 64.25 E.12 Effect of Freezing vs. Unfreezing the Text Encoder In all our main experiments, we freeze the text encoder. This design choice allows us to isolate the role of the auxiliary modality, ensuring that improvements in the primary modality (e.g., vision) arise from cross-modal transfer rather than joint training of both encoders. In principle, however, one can also unfreeze the text encoder and update it during training. To study this, we ablate freezing versus unfreezing on Stanford Cars and SUN397 using ViT-S/14 DINOv2 (vision) and OpenLLaMA-3B (text). As shown in Table 29, unfreezing the text encoder improves performance on Stanford Cars, but slightly reduces performance on SUN397likely due to the larger number of trainable parameters and increased optimization complexity. Table 29: Effect of freezing versus unfreezing the text encoder when training with Uml. Freezing stabilizes training and often yields slightly stronger gains. Method Stanford Cars SUN397 Unimodal (Image Only) Uml (Unfrozen Text Encoder) Uml (Frozen Text Encoder) 79.45 84.23 84.87 66.20 65.80 66. 56 E.13 Additional Experiments for Audio-Visual Setting In this section, we extend our unpaired multimodal framework to the tri-modal ImageNetESC benchmark, examining how unpaired audio and text signals can enhance image classification under both aligned (Section E.13.2) and unaligned encoders(Section E.13.1). We then reverse the setting, showing that unpaired visual and textual context likewise improves audio classification (Section E.13.3). E.13.1 Improving Image Classification with Unpaired Audio and Text (Unaligned encoders) Figure 36: Uml improves image classification using unpaired audio and text samples on both ImageNetESC-19 and ImageNet-ESC-27 benchmarks when trained on top of DINOv2 VIT-S/14 and OpenLLaMa-3B. E.13.2 Improving Image Classification with Unpaired Audio and Text (Aligned encoders) Figure 37: Uml improves image classification using unpaired audio and text samples on both ImageNetESC-19 and ImageNet-ESC-27 benchmarks when trained on top of CLIP ResNet-50 image and text encoders E.13.3 Improving Audio Classification with Unpaired Image and Text (Aligned encoders) Figure 38: Uml improves audio classification using unpaired image and text samples on both ImageNetESC-19 and ImageNet-ESC-27 benchmarks when trained on top of CLIP ResNet-50 image and text encoders E.14 Gaussian Experiments Here, we shift our attention to more nuanced and intriguing question: can incorporating unpaired multimodal data actually improve the reconstruction quality of single modality? At first glance, this seems unlikelywhy would adding data from different modality make reconstruction better than training with X? Moreover, we push this question further: can incorporating data from different modality, while keeping the total dataset size fixed, still improve the reconstruction of compared to using the same number of samples dataset alone? This setup isolates the importance of multimodal information from mere data scaling, and surprisingly, our experiments show that this improvement is indeed possible. To investigate this, we design synthetic experiment inspired by our theoretical framework in Section 3.1. We generate data from two partially overlapping modalities, and Y, derived from shared latent space θc, while also containing unique components (θx and θy). The observations follow the same linear structure as in our theory: Xi = Ac,iθc + Ax,iθx + ϵX,i Yj = Bc,jθc + By,jθy + ϵY,j The training data are generated from Gaussian latents with dimensions dim(θc) = 10, dim(θx) = 5, and dim(θy) = 5. For X, only the first 10% of the shared components in θc are retained at full strength, while the rest are downscaled by 0.05; observes all shared components at full strength. This asymmetry makes informative about structure that is only weakly present in X. The validation set is constructed from the same projections but without attenuation, so both modalities fully observe θc. Observations are 50-dimensional with Gaussian noise ϵX, ϵY (0, 0.09I). In the unimodal setting, training on alone uses 10,000 samples from X. When training Uml on unpaired and Y, we instead use 5,000 samples from each modality to keep the total sample budget fixed, ensuring fair comparison. Our architecture is shared autoencoder. Each modality R50, R50 is projected into common space of dimension 128 through modality-specific linear layers. shared encoder (two linear layers with ReLU) maps into latent space of dimension 10, followed by shared decoder (two linear layers) that expands back to dimension 128. Finally, modality-specific heads reconstruct the original inputs. The shared pathway enforces cross-modal alignment, while the separate adapters preserve modality fidelity. As shown in Figure 39, the surprising outcome is that training on both modalities, even when they are unpaired, consistently improves the reconstruction of compared to training solely on X. More strikingly, this improvement holds even when the total number of training samples is fixed, with half the data coming from and half from Y; showing that the model is not just benefiting from increased data quantity but from the diversity and complementary information provided by the second modality. 58 Figure 39: Training on N/2 samples from and N/2 unpaired samples from improves test reconstruction on X, more than training on samples from X."
        },
        {
            "title": "F Analysis of the Learned Predictor",
            "content": "F.1 Multimodal Neurons Figure 40 tracks how cross-modal correlations between vision and text embeddings evolve during training for the top-performing neurons in our sarcasm detection model. Each subplot shows single neurons correlation values across 30 training epochs, separately measured across all samples (green), correlations conditioned on label=1 (sarcastic; blue), and correlations conditioned on label=0 (non-sarcastic; pink). The neurons are ranked by their final overall correlation magnitude, revealing both positive correlation neurons (#50, #144, #46, #140, #124, #16) and negative correlation neurons (#136, #89, #114) that encode the same semantic pattern with opposite signs. The most striking observation across neurons is that sarcasm consistently appears as cross-modal discord. Non-sarcastic samples exhibit strong correlations (around 0.60.8) as training progresses as opposed to sarcastic samples, which exhibit weaker correlations. Across all neurons, the same trend holds: non-sarcastic correlations are highest, overall correlations fall in the middle, and sarcastic correlations remain lowest. This pattern shows that the model may have learnt to recognize sarcasm not by what is said or shown alone, but by detecting when the two disagree or agree. 59 Figure 40: Evolution of cross-modal correlation during unpaired co-training. Each curve shows the correlation between visual and textual activations over training epochs for sarcastic (label=1) and nonsarcastic (label=0) samples. Most neurons develop positive correlations for non-sarcastic content, while others exhibit negative correlations, suggesting functional specialization for multimodal alignment and the detection of incongruence. F.2 Change in Decision Boundaries with Unpaired Data from Another Modality Our decision boundary visualizations are constructed by projecting the high-dimensional embedding space of given classifier to 2D plane. Axis 1 is computed as the normalized difference between the classifier weights of the two selected classes, representing the primary decision direction. Axis 2 is chosen to be orthogonal to Axis 1, constructed from the difference between the class mean embeddings after removing the component parallel to Axis 1. This orthogonalization ensures that the two axes capture complementary aspects: Axis 1 reflects the primary model decision boundary, while Axis 2 captures the variation orthogonal to that decision. The final 2D projection matrix combines these two vectors as columns, and embedding vectors are then mapped to this plane using simple dot product. Figure 41 , Figure 42 and Figure 43 show the change in decision boundary when adding unpaired textual information for 2-shot classification on top of frozen CLIP ResNet-50 features for Oxford Pets, DTD and Oxford Flowers datasets. 60 Figure 41: Impact of unpaired text on decision boundaries (CLIP ResNet50). (Left) Visual features alone learn ambiguous class boundaries between Russian Blue and Abyssinian cats. (Right) Adding unpaired text sharpens the boundary, leveraging semantic cues to better distinguish similar categories. Figure 42: Impact of unpaired text on decision boundaries (CLIP ResNet50). (Left) Visual features alone learn ambiguous class boundaries between knitted and cobwebbed. (Right) Adding unpaired text sharpens the boundary, leveraging semantic cues to better distinguish similar categories F.3 What do models learn from unpaired data? To understand what the model is truly learning and how its weights evolve, we develop and analyze three key metrics: functional margin, silhouette score, and class-prototype vectors. These metrics inform on how well the model distinguishes between classes and how text information influences the structure of feature-space Functional margin. This quantifies how confidently model separates given sample from the decision boundary. For sample belonging to class y, we calculate the margin relative to the next highest competing class. Specifically, we identify the second-highest logit among the incorrect classes, denoted as class j, and compute the functional margin as γi = xi wT wT xi wy wj 2 (5) where wT xi represents the highest logit among the competing classes. Larger margins indicate more confident and robust classification, while smaller margins xi represents the logit for the true class, while wT 61 Figure 43: Impact of unpaired text on decision boundaries (CLIP ResNet50). (Left) Visual features alone learn ambiguous class boundaries between ball moss and passion flower. (Right) Adding unpaired text sharpens the boundary, leveraging semantic cues to better distinguish similar categories imply that the sample lies closer to misclassification boundary. As shown in Figure 44, both Ours and Ours (init) exhibit substantially larger classification margins than the unimodal baseline, demonstrating that augmenting primary-modality training with unpaired multimodal data improves confidence in predictions over the primary modality. Figure 44: Functional margin of the linear head trained on SUN397 dataset for few-shot classification significantly increases when training with both Uml and Uml with linear head initialization. Silhouette Score and DB-Index. The Silhouette Score indicates how well-separated the clusters are, while the DB-Index measures intra-class compactness versus inter-class separation. Higher silhouette and lower DB-Index values mean better-defined clusters, indicating that text helps tighten intra-class spread and widen inter-class gaps. As shown in Figure 45 and Figure 46, both Ours and Ours (init) exhibit reduced intra-class distances and increased inter-class separations, further confirming improved class separability. Class-Prototype Vectors. These vectors are the rows of the final linear layers weight matrix, representing the class centroids in the shared embedding space. We compute heatmap of inner products between class prototypes and average text embeddings of the corresponding class to assess how well text features align with class centers. This helps reveal how the model organizes multimodal information. Figure 47 shows pronounced diagonal structure, indicating that each classs text embedding aligns closely with the learned weights of the model. 62 Figure 45: Silhouette Score of the linear head trained on SUN397 dataset for few-shot classification significantly increases when training with both Uml and Uml with linear head initialization. Figure 46: DB-Index of the linear head trained on SUN397 dataset for few-shot classification significantly improves when training with both Uml and Uml with linear head initialization. Figure 47: Inner products between each linear-head weight vector and its classs mean text embedding, demonstrating that text features align well with class prototypes."
        }
    ],
    "affiliations": [
        "MIT CSAIL",
        "TU Munich"
    ]
}