{
    "paper_title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries",
    "authors": [
        "Ming Yin",
        "Dinghan Shen",
        "Silei Xu",
        "Jianbing Han",
        "Sixun Dong",
        "Mian Zhang",
        "Yebowen Hu",
        "Shujian Liu",
        "Simin Ma",
        "Song Wang",
        "Sathish Reddy Indurthi",
        "Xun Wang",
        "Yiran Chen",
        "Kaiqiang Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 0 6 7 5 1 . 8 0 5 2 : r LIVEMCP-101: STRESS TESTING AND DIAGNOSING MCP-ENABLED AGENTS ON CHALLENGING QUERIES Ming Yin1,2 Dinghan Shen2 Silei Xu2 Jianbing Han2 Sixun Dong2 Mian Zhang2 Yebowen Hu2 Shujian Liu2 Simin Ma2 Song Wang2 Sathish Reddy Indurthi2 Xun Wang2 Yiran Chen1 Kaiqiang Song2 1Duke University 2Zoom Video Communications"
        },
        {
            "title": "ABSTRACT",
            "content": "Tool calling has emerged as critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides powerful standardized framework for tool integration, there is significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve success rate below 60%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use."
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1: Construction and Evaluation framework of LiveMCP-101. The ability to interact with external tools and services is cornerstone of autonomous AI agents (Schick et al., 2023; Qin et al., 2023a), enabling them to extend their capabilities beyond Work done during internship at Zoom Corresponding author. 1 static knowledge and engage dynamically with the real world. Recent advancements in tool integration frameworksmost notably the Model Context Protocol (MCP) (Anthropic, 2024)have standardized how models discover, invoke, and coordinate tools across diverse domains, from web search and data analysis to API interactions and file manipulation. These developments promise new generation of AI agents capable of executing complex, multi-step tasks with minimal human intervention (Yao et al., 2022; Shinn et al., 2024). However, reliability remains key barrier to real-world deployment, as systems that perform well in prototypes often fail on diverse user queries and in real production environments (Lu et al., 2024; Yao et al., 2024; Barres et al., 2025). Understanding why agents fail (Zhang et al., 2025b; Cemri et al., 2025) in realistic, temporally evolving production environments can offer valuable insights for improving the corresponding models and system architectures. However, existing benchmarks (Li et al., 2023a; Tang et al., 2023; Xu et al., 2023; Patil et al., 2024; Liu et al., 2025) focus only on single-step tool calls, synthetic environments (normally mock database), or limited tool sets, failing to capture the complexity and dynamism of real-world scenarios. In practice, agents must interact with practical tools that may produce varying responses over time and span entirely different domains. Moreover, user queries can carry nuanced contexts and specific constraints (Zhong et al., 2025), requiring accurate reasoning across dozens of tool calls to complete task. As result, current benchmarks cannot fully reveal the gaps in current agent systems when deployed in real production environments. In this work, motivated by the goal of stress testing frontier LLMs and agents in realistic, challenging scenarios, we introduce LiveMCP-101a benchmark of 101 carefully designed tasks requiring coordinated use of diverse MCP-enabled tools (Anthropic, 2024) (e.g., web browsing, file operations, mathematical reasoning, data analysis). Notably, user queries are refined through multiple iterations of LLM rewriting and manual review (Wang et al., 2022) to enhance complexity while preserving practicality, enabling us to reveal where even state-of-the-art agentic models and systems fall short. To ensure robust evaluationgiven that MCP-enabled tools may return varying responses to the same API call over timewe propose novel setup that runs two agents in parallel, one following the ground-truth execution plan and the other operating autonomously, and compute score based on their real-time outputs. Our experiments reveal that even state-of-the-art models struggle with the demands of complex tool orchestration, achieving less than 60% success rate. It highlights significant gap between current agent capabilities and the robustness required for truly autonomous task execution. More importantly, by digging deep into the error cases with various models, we are able to glean useful insights into different models agentic capabilities. By carefully analyzing the agent trajectories (Chen et al., 2023), we identify seven common failure modes in frontier models, shedding light on how to further improve these systems. Moreover, we observe striking log-shaped curve in token efficiency: closed-source models gain rapidly and then plateau, while open-source models fail to turn tokens into reliable evidence. We release this benchmark to accelerate development in tool-augmented AI systems and foster innovations in planning, reasoning, and long-horizon task execution (Xi et al., 2023). Concurrently, (Liu et al., 2025) also proposes to evaluate AI agents connected to MCP tools and servers, but their setup is limited to user queries within single MCP server and involves low task complexity. Consequently, different models achieve similar success rates, offering limited insight into their distinct strengths and weaknesses. Moreover, their evaluation compares the agents trajectory only against static ground-truth execution plan, failing to account for variations in MCP server responses over time, which are critical for assessing robustness in dynamic environments. In summary, our contributions are as follows: We introduce LiveMCP-101, benchmark of 101 diverse real-world tasks requiring coordinated use of multiple MCP tools (from diverse domains), with user queries refined through iterative LLM rewriting and manual review. We propose novel evaluation approach that leverages ground-truth execution plans, rather than raw API responses or end results, to account for the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve task success rate below 60%, underscoring major challenges in real-world tool orchestration. Detailed ablation studies and error analysis draw insights on how to further improve current models, from the perspectives of different agent failure modes and token efficiency."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Agents with Tool Use Chain-of-Thought (CoT) prompting (Wei et al., 2022) first demonstrated that large language models could remarkably improve performance on complex questions by making their intermediate reasoning steps explicit. Its success fueled the rapid rise of reasoning models (OpenAI, 2024; Guo et al., 2025), which consistently outperform baselines across diverse domains. The ReAct (Yao et al., 2022) framework extends CoT by decoupling reasoning from tool calls, enabling LLM-based agents to ground their reasoning in external information, correct errors mid-process, and adapt their plans dynamically. Building on these foundations, recent LLM-agent research explores diverse strategies for autonomous tool use. One line of work focuses on fine-tuning LLMs with large and diverse tool collections, enabling them to operate effectively over extensive real-world APIs (Schick et al., 2023; Du et al., 2024; Qin et al., 2023b). Another approach adopts modular and hierarchical architectures that decompose agents into specialized roles for proposing, planning, executing, and evaluating, improving robustness and compositional reasoning (Zhuang et al., 2023; Zhou et al., 2024; Shi et al., 2024). Retrieval-augmented methods further enhance tool access by incorporating improved retrieval, documentation compression, and reranking techniques (Yuan et al., 2024; Zheng et al., 2024). The emergence of Model Context Protocol (MCP) (Anthropic, 2024) marks turning point for agentic tool use. MCP provides standardized, JSON-RPC-based API layer for integrating LLMs with external tools. Since its release, MCP has been quickly adopted across all major AI players and has attracted significant attention from the research community (Hou et al., 2025; Ehtesham et al., 2025). Evaluation of Agentic Tool Use Evaluating agentic tool use in LLMs presents inherent challenges, and substantial research effort has been devoted to developing benchmarks that capture different dimensions of this capability. Early work primarily focused on assessing single-turn function-calling abilities of LLMs, where the model is required to invoke the correct tool in response to given query (Yan et al., 2024; Qin et al., 2023b; Guo et al., 2024; Li et al., 2023b; Wu et al., 2024; Patil et al., 2024). Subsequent benchmarks have extended this scope to multi-turn conversational settings, where effective tool use requires maintaining context and reasoning across dialogue turns (Wang et al., 2023; Song et al., 2023; Lu et al., 2024; Yao et al., 2024; Barres et al., 2025). Following the emergence of MCP, various datasets and benchmarks have been proposed to evaluate the MCP ecosystem. Luo et al. (2025) introduced MCPBench, the first MCP-specific evaluation. Building on this, Gao et al. (2025) proposed MCP-RADAR, which adopts multi-dimensional evaluation approach; while Liu et al. (2025) presented MCPEval, an automated, fine-grained MCP evaluation framework. These early MCP benchmarks have limited scope with only around 10 MCP servers. Fei et al. (2025) presented MCP-Tools, large MCP-tool retrieval dataset (308 servers, 2,797 tools). However, it does not provide benchmark for evaluation. concurrent work by Mo et al. (2025) introduced LiveMCPBench, which evaluates agents against dynamic, real-time MCP servers, and employs LLM-as-judge evaluator for scoring. However, tasks in LiveMCPBench are relatively simple, averaging only 2.7 tool calls and 2.8 steps per example. In addition, its gold annotations specify only the tool names without detailed parameters, and these annotations are not used as reference during scoring. This leads to results that may not fully capture relative model capabilities, for instance, GPT-4.1-mini outperforming GPT-4.1. In contrast, our LiveMCP-101 benchmark introduces three-tiered difficulty structure (easy, medium, hard), with tasks requiring an average of 5.4 tool-calling steps, making it significantly more challenging benchmark for LLMs. Furthermore, we provide detailed gold-standard tool invocation chains, which serve as explicit references to ensure scoring consistency and closer alignment with human judgments."
        },
        {
            "title": "3 LIVEMCP-101",
            "content": "3.1 CONSTRUCTION Query Generation To generate challenging queries that require agents to leverage tools across different domains, we first sample diverse application domains from the overall MCP tool pool, which spans 41 MCP servers and 260 tools, using GPT-4.1. As the next step, we employ OpenAI o3 model to generate queries of varying complexity, conditioned on domain context and detailed tool specifications (names, descriptions, and parameters). However, even with carefully tuned prompts, 3 some generated queries are either not solvable with the provided tools or have end states whose results are not easily verifiable. To ensure the dataset is clean and rigorous, we perform multiple rounds of LLM rewriting and manual revision (Wang et al., 2022) to guarantee clarity, balanced difficulty, solvability with the given tools, and objectively verifiable outcomes. All queries are divided into three difficulty tiers: Easy (30), Medium (30), and Hard (41). Figure 2 illustrates representative examples from each tier. Execution Plan Generation Because tasks interact with live, time-varying MCP services, answers may change over time. Thus, having fixed ground-truth results may not serve as reliable way to evaluate agent results at test time. To resolve this issue, for every curated query, we draft an execution plan with o3 given the query and tool specifications. As the next step, we revise it using the reference agents execution trajectory and outputs, combining LLM-assisted edits with manual adjustments to correct logical, tool-selection, parameter, and data-processing errors. Approximately 120 PhD-hours were required for this revision. Each task was validated across multiple trials with human verification of correctness. The finalized plan deterministically yields the reference output when followed. The distribution of tool-chain lengths in the execution plans is shown in Figure 3. 3.2 EVALUATION Evaluation framework For each task, we launch two parallel executions: (1) real-time reference execution, where the reference agent strictly follows the validated execution plan using only the MCP tools specified therein to produce the reference output; (2) real-time test execution, where the agent being evaluated receives only the natural-language query and predefined per-task MCP tool pool, and must independently analyze the query, select tools, schedule calls, and process intermediate results. The test execution proceeds until the agent declares completion or reaches the maximum iteration rounds. The per-task pool contains all task-essential tools plus set of additional MCP tools (to make the task more challenging for the agents). These additional tools approximate real-world choice breadth, and enable assessment of tool discovery and selection under distractors. This setup mitigates temporal drift and enables fair comparison between the evaluated agents output and the reference. In addition, the reference trajectory provides reference for analyzing the evaluated agents trajectory, enabling fine-grained diagnosis of planning, tool-selection, parameter, and output-handling errors. Evaluation metrics Both the result and trajectory of the evaluated agent are scored by an LLM judge (Zheng et al., 2023) using 1-5 Likert scale (Liu et al., 2023) and then mapped to {0.00, 0.25, 0.50, 0.75, 1.00}. Prompts are provided in the Appendix A.1 and Appendix A.2. Result Metrics: we use Task Success Rate (TSR), the proportion of instances with score 1.00, and Average Result Score (ARS), the mean score across instances. TSR measures the proportion of tasks successfully solved, while ARS reflects the overall quality of the solutions. Trajectory Metric: Average Trajectory Score (ATS) of the evaluated agent is reported across all tasks. This metric evaluates trajectories for logical coherence, completeness, and correctness, capturing the quality of the solution process complementary to result metrics. Average Token Consumption: in order to measure the token efficiency, for each task, we sum the agents output tokens across all the iteration rounds; the reported value is the mean of these per-task totals over the evaluation set. Average Tool Calls: for each task, we count the tool invocations across the full trajectory; the reported value is the mean of these per-task counts over the evaluation set."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP Models We evaluate diverse set of 18 widely used and popular LLMs on LiveMCP-101: OpenAI (GPT-5, GPT-5-mini, GPT-4.1, GPT-4o, GPT-4.1-mini, GPT-4o-mini, o3, o4-mini), Anthropic (Claude-4.1-Opus, Claude-4-Sonnet, Claude-3.7-Sonnet), Google (Gemini-2.5-Pro, Gemini-2.5Flash), and open-source (Qwen3-235B-A22B, Qwen3-32B, Qwen3-8B, Llama-3.3-70B-Instruct, Llama-3.1-8B-Instruct). For OpenAI reasoning models (OpenAI, 2025b), the reasoning effort is set to 4 Easy During recent weekly meeting, my mentor highlighted the need for improved DevOps monitoring. Please prepare Markdown file named k8s issues report.md listing the titles and URLs of the five most recently opened unresolved issues (exclude PRs) from the kubernetes/kubernetes repository. Medium As part of recent initiative at the fictional consultancy firm BrightPath Analytics, commissioned by the renowned artist Lucia Moretti for an upcoming exhibition in Zurich, you are tasked with supporting market research on the digital art landscape. Lucia is specifically interested in public engagement with YouTube content for AI-generated art tools. Retrieve the first five search results returned for this query. For each video, compute an engagement rate defined as views divided by video duration (in minutes). Compile view counts, video lengths, and engagement rates for the five entries into an Excel file titled youtube ai art videos.xlsx for forwarding to Lucias Zurich studio. Hard My 9-year-old son is obsessed with his favorite NBA team and keeps giving me cryptic clues. Yesterday at dinner he said, Dad, did you know our teams name owes huge debt to Spielberg sci-fi masterpiece? Hes been begging me to see home game at their arena. Id like to surprise him with tickets for game exactly 60 days from today (local time). Well fly in the night before and need accommodation for one night. Since its just the two of us and we want to be close to the action, please list all available Airbnb properties within 12-minute brisk walk (assuming 5 km/h) of the teams home arena. My budget is strictly $150$160 USD per night. Please retrieve official team information and produce comprehensive Markdown report titled nba game trip.md. This report should present the following information cohesively: first, the exact team name; second, detailed team information including the team name, conference, division, founded year, home arena, arena location, arena capacity, team colors, and championships; and finally, all qualifying accommodation options including the name, listing ID, nightly price, distance to the arena, walking time, and booking link. Figure 2: Example queries by difficulty level. These queries require the multi-step composition of heterogeneous MCP tools, with proper parameterization and output handling. Figure 3: Distribution of tool-chain lengths in the LiveMCP-101 execution plans. medium. For Anthropic models, we test both standard and extended thinking (ET) models (Anthropic, 2025). For Qwen3 models, thinking is enabled by default (Qwen Team, 2025). Settings Each agent is limited to maximum of 30 iteration rounds. For reference execution, we employ GPT-4.1 due to its low latency (OpenAI, 2025a) and strong instruction-following capabilities (OpenAI, 2025a; Zhang et al., 2025a), strictly adhering to the validated execution plan to produce the reference output. For each task, per-task MCP pool is constructed by combining all task-essential servers with randomly sampled MCP servers, yielding total of 15 MCP servers and 76125 tools available per task. We adopt the widely used ReAct prompting (Yao et al., 2023) and use GPT-4.1 as the LLM judge (Zheng et al., 2023) to score both the final output and the execution trajectory. Metrics As described in our evaluation framework, we report the following metrics for each model: task success rate (TSR), average result score (ARS), average trajectory score (ATS), average tool calls and average tokens used."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "As shown in Table 1, GPT-5 achieves the best overall performance on LiveMCP-101, leading across all difficulty tiers. Next are o3, GPT-5-mini, Claude-4.1-Opus (ET), and Claude-4-Sonnet (ET), indicating that stronger reasoning effort can yield meaningful improvements for dynamic, multi-step problem-solving and MCP tool calling. Among mid-tier proprietary models, GPT-4.1, Gemini-2.5Pro, and Claude-3.7-Sonnet perform reasonably well but remain behind the top performers. Opensource models lag behind proprietary models. Among them, Qwen3-235B-A22B (22.77%/42.57%) is the strongest among them yet remains far from the frontier. Llama models underperform notably on LiveMCP-101, and more detailed analysis is provided in Section 5.2. Performance degrades substantially with task difficulty for all models. Notably, even the strongest model attains only 39.02% TSR on Hard. Rankings by TSR and ARS are broadly consistent. Figure 4a visualizes the relationship among TSR, ARS, and ATS. The color-encoded ATS increases with both ARS and TSR, with higher-ATS models clustering toward the upper-right region. This suggests that better trajectories usually yield better outputs. Higher ATS corresponds to more reliable tool selection, parameterization, and post-processing, which helps satisfy task success criteria. Figure 4b shows the relationship between TSR, the average number of tokens, and the average number of tool calls. Closed-source models exhibit mild upward trend with tokens, yet planning quality remains the primary driver.Open-source models exhibit two characteristic inefficiencies. Llama variants cluster in the low-token, low-tool region, under-exploring tool affordances and often stopping early, which yields low ARS and TSR. Qwen variants trend toward the opposite extreme, producing longer outputs and invoking more tools without commensurate gains compared to the closed-source models. Extended-thinking variants consistently shift the efficiency frontier upward at comparable token budgets, suggesting gains from improved planning and error recovery rather than verbosity. Model GPT-5 o3 GPT-5-mini Claude-4.1-Opus (ET) o4-mini Claude-4-Sonnet (ET) Claude-4.1-Opus Claude-4-Sonnet GPT-4.1 Claude-3.7-Sonnet (ET) Gemini-2.5-Pro Claude-3.7-Sonnet Qwen3-235B-A22B GPT-4o GPT-4.1-mini Qwen3-32B GPT-4o-mini Gemini-2.5-Flash Qwen3-8B Llama-3.3-70B-Instruct Llama-3.1-8B-Instruct Overall Easy Medium Hard TSR ARS TSR ARS TSR ARS TSR ARS 58.42 46.53 43.56 41.58 40.59 43.56 39.60 37.62 35.64 29.70 27.72 26.73 22.77 21.78 17.82 18.81 8.91 10.89 3.96 1.98 0.99 73.02 64.60 63.12 61.88 61.63 60.40 59.41 55.69 55.94 47.77 46.78 42.57 42.57 41.09 35.15 34.41 27.48 22.48 11.63 6.93 2. 86.67 66.67 63.33 56.67 53.33 63.33 60.00 63.33 60.00 43.33 36.67 46.67 43.33 40.00 36.67 36.67 16.67 26.67 10.00 3.33 3.33 89.17 80.00 82.50 79.17 77.50 79.17 83.33 78.33 76.67 66.67 61.67 61.67 63.33 62.50 56.67 59.17 40.83 44.17 26.67 15.83 9.17 56.67 46.67 43.33 43.33 46.67 46.67 33.33 46.67 36.67 26.67 30.00 20.00 26.67 20.00 13.33 16.67 6.67 10.00 3.33 3.33 0.00 72.50 65.83 64.17 61.67 62.50 62.50 49.17 65.00 55.83 46.67 46.67 40.83 45.00 37.50 31.67 32.50 31.67 22.33 8.33 5.83 0.00 39.02 31.71 29.27 29.27 26.83 26.83 29.27 12.20 17.07 21.95 19.51 17.07 4.88 9.76 7.32 7.32 4.88 0.00 0.00 0.00 0.00 61.59 52.44 48.17 49.39 49.39 45.12 49.39 32.32 40.85 34.76 35.98 29.88 25.61 28.05 21.95 17.68 14.63 6.71 3.05 1.22 0. Table 1: Task success rate (TSR, %) and average result score (ARS, %) overall and by difficulty (Easy/Medium/Hard). Shaded rows mark the top-3 models by overall TSR. Bold indicates columnbest. ET denotes extended thinking enabled for Anthropic models. 6 (a) (b) Figure 4: Results on LiveMCP-101, showing model performance in terms of task success rate (TSR), average result score (ARS), average trajectory score (ATS), average token consumption, and average tool calls. (a) TSR (%) vs. ARS (%), with color encoding ATS (%). (b) TSR (%) vs. average tokens per task, with color encoding average tool calls. 4.3 ABLATION STUDY We conduct ablations on GPT-5, Claude-4.1-Opus (ET), GPT-4.1, Gemini-2.5-Pro, Qwen3-235BA22B, and Qwen3-8B, covering frontier and mid-tier closed-source models and open-source models. Impact of maximum iteration rounds In LiveMCP-101, the longest validated execution plan requires 15 tool calls. By default, each agent is limited to 30 iteration rounds, where each round may involve one or more tool invocations. To study sensitivity to the iteration budget, we vary this limit to 15, 20, 30, and 50 rounds. The results in Figure 5 (a) and (b) highlight two key phenomena. First, increasing the max iteration limit from 15 to about 25 rounds consistently improves task success, as the added budget enables more thorough tool exploration and error recovery (Yuan et al., 2025; Zhang et al., 2025c). Notably, although the longest validated execution plan comprises 15 tool calls (with an average of 5.4), the continued gains when raising the round limit from 15 to around 25 indicate that agents often expend extra rounds on error recovery or redundant deliberation even on correctly solved instances, revealing substantial headroom for execution efficiency. Second, beyond 25 rounds, the benefits saturate: performance becomes constrained by model capabilityparticularly planning quality and tool-use competencerather than iteration capacity. Additional rounds yield diminishing returns and can even introduce noise or compound errors, leaving performance essentially flat. Figure 5: Ablation study results. (a) TSR (%) vs. max iteration rounds: all models improve from 15 to around 25 rounds, then plateau. (b) Relative TSR change w.r.t. 15-round setting shows diminishing returns beyond about 25. (c) TSR (%) vs. number of MCP servers: top-tier models remain largely stable, while weaker or mid-tier models degrade as distractors grow. (d) Relative change w.r.t. 6-server setting shows that larger pools affect weaker models more, consistent with long-context sensitivity and tool-selection noise. 7 Impact of the number of MCP servers In the default setting, the most demanding task requires up to 6 MCP servers, and we expose pool of 15 servers to the evaluated agent. To study sensitivity to MCP server breadth, we vary the pool size to 6, 10, 12, and 15. We set 15 as the upper limit because larger pools could hit API limits (e.g., 128 tools per request) (OpenAI, 2025) or exceed context length. This choice keeps the setup realistic and comparable to real-world deployments. As the pool grows, the expanded tool search and tool call space increases selection overhead and the likelihood of spurious tool usage. We find weaker and mid-tier models more sensitive to this effect, often showing declines as noise accumulates and planning bandwidth is diluted. In contrast, top-tier systems (e.g., GPT-5, Claude-4.1-Opus (ET)) remain largely stable: stronger planning and tool-screening mitigate distractors, so performance changes are negligible."
        },
        {
            "title": "4.4 ANALYSIS OF LLM-AS-A-JUDGE",
            "content": "We apply an LLM-as-a-Judge to score both final outputs and execution trajectories. To assess reliability, we conduct blinded humanexpert study on stratified subset of tasks for six representative models: GPT-5, Claude-4.1Opus (ET), GPT-4.1, Gemini-2.5-Pro, Qwen3235B-A22B, and Qwen3-32B. Experts follow the same rubric and judge prompts. We compare human and LLM-judge decisions at the Figure 6: HumanLLM agreement (Cohens κ, %) instance level and report inter-rater agreement on result and trajectory evaluation for six models. using Cohens κ (Cohen, 1960). We evaluate Blue bars denote scores for the result evaluation, and sampled set of 30 tasks in total with 10 per pink bars denote scores for the trajectory evaluation. difficulty tier (Easy, Medium, Hard). Across all six models, the human vs. LLM-judge agreement (quadratic-weighted Cohens κ) exceeds 85% for the result evaluations and 78% for the trajectory evaluations respectively, indicating that the LLM judge yields consistent, human-aligned ratings (Landis & Koch, 1977; Fleiss, 1981)."
        },
        {
            "title": "5 DISCUSSION",
            "content": "5.1 TOKEN EFFICIENCY We observe that closed-source models exhibit striking log-shaped pattern: task success rate (TSR) rises rapidly with small token budgets, then plateaus (Figure 4b). Intuitively, early tokens drive high-value actionsplanning, probing tools, checking constraintsyielding large gains. But as budgets grow, extra tokens mostly add redundancy (longer explanations, repeated self-checks) rather than new evidence, and returns diminish. This curve reflects models token efficiency: even the strongest struggle to make effective use of additional tokens beyond certain point. How to maximize intelligence per token, or per dollar, remains an intriguing open challenge for MCP-based agents. In contrast, open-source models break this trend: despite equal or greater token use, TSR barely improves, revealing failure to turn tokens into reliable evidence and thus lower token efficiency. 5.2 FAILURE ANALYSIS To diagnose failure modes in MCP-based tool use, we carefully analyze execution logs across different models and identify three error categories including seven subtypes: tool planning and orchestration errors (14), parameter errors (56), and output handling errors (7). (1) Ignoring requirement: the agent misses an explicitly stated requirement and does not select any relevant tool. Typical signs include no corresponding thinking process and tool call, early termination, or generic final answer that does not address the requirement. This often occurs when the agent fails to extract key requirements from the prompt or loses track of them during execution. (2) Overconfident self-solving: the agent recognizes the requirement but attempts to answer from its own knowledge or using its own reasoning and capabilities without calling the needed tool. Symptoms include: no corresponding tool call, generic or hallucinated answers, and premature termination. (3) Unproductive thinking: the agent acknowledges that tool is needed and may discuss plans or parameters, but never initiates the call and does not propose any solution that addresses the requirement. It loops in unproductive or verbose thinking and eventually times out or gives up. Symptoms include repeated plan rewrites without execution, token-consuming thinking, and reaching the round limit with zero calls for the requirement. (4) Wrong tool selection: the agent calls tool but chooses an inappropriate one, leading to erroneous intermediate states or final outputs. This can happen as single misselection or repeated wrong calls until the budget is exhausted. Symptoms include irrelevant responses, repeated mistakes, or missing required fields in outputs. (5) Syntactic errors: parameters provided to tool are malformed, such as having incorrect types, missing or wrong field names, or invalid schema. These errors prevent the MCP server from correctly parsing the request, leading to failure. (6) Semantic errors: parameters are well-formed but do not match the task intent. Common cases include mis-scoped query strings, wrong identifiers or entity references, and incorrect contextual constraints. These errors often arise from mistakes in intermediate reasoning used to generate parameters. (7) Output parsing errors: the tool returns correct result, but the agent mishandles it during parsing, causing incorrect intermediate states or final answers. We further evaluate several popular models spanning range of capabilities, and their core error-type distributions are shown in Figure 7. Several interesting observations can be inferred from the results: Semantic errors dominate: even strong models show rates of 1625%, while smaller ones exceed 40% (e.g., GPT-4.1-mini), pinpointing content grounding and constraint enforcement as the primary bottleneck in live tool use. Syntactic errors are negligible for frontier models but catastrophic for Llama-3.3-70B-Instruct 48%. likely cause is limited MCP-specific trainingMCP adoption surged (Ehtesham et al., 2025) after the Llama-3 release (Meta Llama Team, 2024)suggesting that targeted fine-tuning on MCP function-call schemas could substantially cut such errors and boost overall performance. Overconfident self-solving is common in mid-tier models: they often skip tool calls because planning and screening remain brittle under large tool pools and long contexts, making reliance on internal knowledge (Chhikara, 2025) seem safer than attempting uncertain tool selection and parameterization. Figure 7: Error classification heatmap across models. The leftmost column (Correct) aligns with TSR, while the remaining columns decompose failures into 7 fine-grained subtypes."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce LiveMCP-101, benchmark of challenging user queries for evaluating AI agents ability to plan and execute multi-step tool use in realistic, dynamic environments via the Model Context Protocol (MCP). The benchmark comprises 101 diverse tasks, refined through iterative LLM-based rewriting and manual review, spanning domains including web search, file operations, mathematical reasoning, and data analysis. We further propose an evaluation methodology based on ground-truth execution plans, providing more reliable measure of agent performance in evolving environments. Experiments show that even frontier LLMs achieve success rate below 60%, underscoring key challenges in tool orchestration, adaptive reasoning, and token efficiency. Detailed ablations and error analysis reveal distinct failure modes and highlight opportunities for improvement. By releasing LiveMCP-101, we establish rigorous and scalable framework to benchmark and advance the development of more capable autonomous AI agents."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Model context protocol, 2024. URL https://modelcontextprotocol.io/. Anthropic. Extended thinking in claude. https://docs.anthropic.com/en/docs/ build-with-claude/extended-thinking, 2025. ET (extended thinking) models and usage; Accessed 2025-08-21. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. τ 2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982, 2025. Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. Qiantong Chen, Hongyu Zhang, Xueliang Liu, Jian Feng, Longtao Wang, et al. Tooleval: arXiv preprint An automatic evaluation framework for tool-augmented language models. arXiv:2307.10813, 2023. Prateek Chhikara. Mind the confidence gap: Overconfidence, calibration, and distractor effects in large language models. arXiv preprint arXiv:2502.11028, 2025. J. Cohen. coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):3746, 1960. Yu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective, hierarchical agents for large-scale api calls. arXiv preprint arXiv:2402.04253, 2024. Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, and Saket Kumar. survey of agent interoperability protocols: Model context protocol (mcp), agent communication protocol (acp), agent-to-agent protocol (a2a), and agent network protocol (anp). arXiv preprint arXiv:2505.02279, 2025. Xiang Fei, Xiawu Zheng, and Hao Feng. Mcp-zero: Proactive toolchain construction for llm agents from scratch. arXiv preprint arXiv:2506.01056, 2025. Joseph L. Fleiss. Statistical methods for rates and proportions. John Wiley & Sons, New York, 2nd edition, 1981. Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: multi-dimensional arXiv preprint benchmark for evaluating tool use capabilities in large language models. arXiv:2505.16700, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714, 2024. Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025. Richard Landis and Gary Koch. The measurement of observer agreement for categorical data. Biometrics, 33(1):159174, 1977. Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023a. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023b. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. 10 Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Huan Wang, Shelby Heinecke, et al. Mcpeval: Automatic mcp-based deep evaluation for ai agent models. arXiv preprint arXiv:2507.12806, 2025. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, et al. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682, 2024. Zhiling Luo, Xiaorong Shi, Xuanrui Lin, and Jinyang Gao. Evaluation report on mcp servers. arXiv preprint arXiv:2504.11094, 2025. Meta Llama Team. Llama-3.3-70b-instruct: Model card. https://huggingface.co/ meta-llama/Llama-3.3-70B-Instruct, 2024. Accessed 2025-08-21. Guozhao Mo, Wenliang Zhong, Jiawei Chen, Xuanang Chen, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, and Le Sun. Livemcpbench: Can agents navigate an ocean of mcp tools? arXiv preprint arXiv:2508.01780, 2025. OpenAI. Openai o1 system card, 2024. openai-o1-system-card/. Accessed: 2024-12-01. URL https://openai.com/index/ OpenAI. Assistants api deep dive. https://platform.openai.com/docs/assistants/ deep-dive, 2025. Use the tools parameter to give the Assistant access to up to 128 tools.. OpenAI. Gpt-4.1. https://openai.com/index/gpt-4-1/, 2025a. Accessed 2025-08-21. OpenAI. Openai platform: Reasoning models. https://platform.openai.com/docs/ guides/reasoning, 2025b. Describes reasoning effort levels (low/medium/high); Accessed 2025-08-21. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37: 126544126565, 2024. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023a. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023b. Qwen Team. Qwen3: Think deeper, act faster. https://qwenlm.github.io/blog/qwen3/, 2025. Accessed 2025-08-21. Timo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren, Suzan Verberne, and Zhaochun Ren. Learning to use tools via cooperative and interactive agents. arXiv preprint arXiv:2403.03031, 2024. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, et al. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624, 2023. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023. 11 Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Sharan, Aakanksha Goodman, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. In CCF International Conference on Natural Language Processing and Chinese Computing, pp. 372384. Springer, 2024. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864, 2023. Cheng Xu, Dazhen Guo, Nan Duan, and Julian McAuley. Tool learning with large language models: survey. arXiv preprint arXiv:2405.17935, 2023. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. 2024. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint arXiv:2401.06201, 2024. Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training language model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425, 2025. Mian Zhang, Shujian Liu, Sixun Dong, Ming Yin, Yebowen Hu, Xun Wang, Steven Ma, Song Wang, Sathish Reddy Indurthi, Haoyun Deng, et al. Complex logical instruction generation. arXiv preprint arXiv:2508.09125, 2025a. Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, et al. Which agent causes task failures and when? on automated failure attribution of llm multi-agent systems. arXiv preprint arXiv:2505.00212, 2025b. Zhisong Zhang, Tianqing Fang, Kaixin Ma, Wenhao Yu, Hongming Zhang, Haitao Mi, and Dong Yu. Enhancing web agents with explicit rollback mechanisms. arXiv preprint arXiv:2504.11788, 2025c. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. Toolrerank: Adaptive and hierarchy-aware reranking for tool retrieval. arXiv preprint arXiv:2403.06551, 2024. 12 Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. Complexfuncbench: exploring multi-step and constrained function calling under long-context scenario. arXiv preprint arXiv:2501.10132, 2025. Yifei Zhou, Qianlan Yang, Kaixiang Lin, Min Bai, Xiong Zhou, Yu-Xiong Wang, Sergey Levine, and Erran Li. Proposer-agent-evaluator(pae): Autonomous skill discovery for foundation model internet agents. 2024. URL https://arxiv.org/abs/2412.13194. Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan Rossi, Somdeb Sarkhel, and Chao Zhang. Toolchain*: Efficient action space navigation in large language models with a* search. arXiv preprint arXiv:2310.13227, 2023."
        },
        {
            "title": "A PROMPTS",
            "content": "A.1 RESULT EVALUATION PROMPT You are senior evaluator judging how well an AI agent solves task. Task Query: {query}. EVALUATION INSTRUCTION: Given the query above and the reference answer, evaluate how well the agent solves the task. LIKERT-STYLE DISCRETE SCORING (15) - 5 (Excellent): Agent output conveys the same results and information as reference; task fully satisfied; differences in formatting or wording are fine - 4 (Good): Mostly correct with minor omissions or small inaccuracies - 3 (Fair): About half of the results is correct but some requirement not met or with noticeable inaccuracies - 2 (Poor): Only small portion is correct, substantially incomplete or with significant inaccuracies - 1 (Fail): No correct or relevant results (off-topic, fabricated, or entirely incorrect) You MUST snap to one of these exact Likert values: 1, 2, 3, 4, or 5. CRITICAL RULES: 1. DO NOT excuse material differences due to dynamic data or timing 2. Focus on the content in both the reference and agent output that fulfills the Querys requirements and intent. 3. Structure and wording variations are acceptable 4. NUMERICAL TOLERANCE: For values that may easily fluctuate briefly (e.g., driving times, prices, view counts): - Minor variations plausibly due to short-term fluctuation should be considered correct - Example: $120 vs $118 for specific room price is acceptable - Example: 25 minutes drive vs 2327 minutes is acceptable - Example: 6900 view counts vs 6908 view counts is acceptable Provide your evaluation in the following JSON format: { } \"likert\": <integer 1-5>, \"feedback\": \"Detailed explanation for the chosen rating\" A.2 TRAJECTORY EVALUATION PROMPT You are senior evaluator judging the overall quality of the agents tool chain (trajectory) for solving the task. Reference Tool Chain (for context): {reference tool chain} Agents Actual Tool Chain: {agents actual tool chain} LIKERT-STYLE DISCRETE SCORING (15): - 5 (Excellent): The trajectory is logically sound, efficient, complete, and demonstrates strong reasoning. All necessary steps are present, no major mistakes, and the approach is either optimal or clearly valid alternative - 4 (Good): The trajectory is mostly correct, reasonable, and relevant; steps are generally appropriate and accurate, with noticeable but non-critical omissions or inefficiencies; no critical errors - 3 (Fair): Some correct, relevant steps, but with gaps in logic/completeness or several questionable/inefficient choices - 2 (Poor): Few correct steps; substantially incomplete or contains clearly wrong tool usage that undermines progress - 1 (Failed): The trajectory does not include any correct or relevant steps toward solving the task, is illogical or largely incorrect, and does not meaningfully advance the task; not directly usable You MUST snap to one of these exact Likert values: 1, 2, 3, 4, or 5. Note: The agents approach does not need to match the reference exactly. Please focus on the overall quality, efficiency, and logic of the agents tool chain. Provide your evaluation in the following JSON format: { } \"likert\": <integer 1-5>, \"feedback\": \"Detailed explanation for the chosen rating\""
        }
    ],
    "affiliations": [
        "Duke University",
        "Zoom Video Communications"
    ]
}