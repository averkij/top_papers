{
    "paper_title": "AXLearn: Modular Large Model Training on Heterogeneous Infrastructure",
    "authors": [
        "Mark Lee",
        "Tom Gunter",
        "Chang Lan",
        "John Peebles",
        "Hanzhi Zhou",
        "Kelvin Zou",
        "Sneha Bangalore",
        "Chung-Cheng Chiu",
        "Nan Du",
        "Xianzhi Du",
        "Philipp Dufter",
        "Ruixuan Hou",
        "Haoshuo Huang",
        "Dongseong Hwang",
        "Xiang Kong",
        "Jinhao Lei",
        "Tao Lei",
        "Meng Li",
        "Li Li",
        "Jiarui Lu",
        "Zhiyun Lu",
        "Yiping Ma",
        "David Qiu",
        "Vivek Rathod",
        "Senyu Tong",
        "Zhucheng Tu",
        "Jianyu Wang",
        "Yongqiang Wang",
        "Zirui Wang",
        "Floris Weers",
        "Sam Wiseman",
        "Guoli Yin",
        "Bowen Zhang",
        "Xiyou Zhou",
        "Danyang Zhuo",
        "Cheng Leong",
        "Ruoming Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We design and implement AXLearn, a production deep learning system that facilitates scalable and high-performance training of large deep learning models. Compared to other state-of-the-art deep learning systems, AXLearn has a unique focus on modularity and support for heterogeneous hardware infrastructure. AXLearn's internal interfaces between software components follow strict encapsulation, allowing different components to be assembled to facilitate rapid model development and experimentation on heterogeneous compute infrastructure. We introduce a novel method of quantifying modularity via Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains constant complexity as we scale the components in the system, compared to linear or quadratic complexity in other systems. This allows integrating features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred of modules with just 10 lines of code, compared to hundreds as required in other systems. At the same time, AXLearn maintains equivalent performance compared to state-of-the-art training systems. Finally, we share our experience in the development and operation of AXLearn."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 1 1 4 5 0 . 7 0 5 2 : r AXLearn: Modular Large Model Training on Heterogeneous Infrastructure Mark Lee Tom Gunter Chang Lan John Peebles Hanzhi Zhou Kelvin Zou Sneha Bangalore Chung-Cheng Chiu Nan Du Xianzhi Du Philipp Dufter Ruixuan Hou"
        },
        {
            "title": "Jiarui Lu",
            "content": "Zhiyun Lu Yiping Ma David Qiu Vivek Rathod Senyu Tong Zhucheng Tu Jianyu Wang Yongqiang Wang Zirui Wang Floris Weers Sam Wiseman Guoli Yin Bowen Zhang Xiyou Zhou Danyang Zhuo Cheng Leong Ruoming Pang Apple Duke University Abstract We design and implement AXLearn, production deep learning system that facilitates scalable and high-performance training of large deep learning models. Compared to other state-of-art deep learning systems, AXLearn has unique focus on modularity and support for heterogeneous hardware infrastructure. AXLearns internal interfaces between software components follow strict encapsulation, allowing different components to be assembled to facilitate rapid model development and experimentation on heterogeneous compute infrastructure. We introduce novel method of quantifying modularity via Lines-of-Code (LoC)-complexity, which demonstrates how our system maintains constant complexity as we scale the components in the system, compared to linear or quadratic complexity in other systems. This allows integrating features such as Rotary Position Embeddings (RoPE) into AXLearn across hundred of modules with just 10 lines of code, compared to hundreds as required in other systems. At the same time, AXLearn maintains equivalent performance compared to state-of-the-art training systems. Finally, we share our experience in the development and operation of AXLearn."
        },
        {
            "title": "1 Introduction\nLarge-scale deep learning models are now integral to soci-\nety—they power chatbots such as ChatGPT and Gemini [14,\n28], enhance video conferencing [47], and support modern\ncoding tools [7]. Modern deep learning systems prioritize\nperformance and scalability to accommodate large models.\nNumerous techniques have been explored to this end, in-\ncluding parallelization strategies [27, 45], memory optimiza-\ntions [35, 44], and model-specific kernel optimizations [8, 9].\nAs one of the largest consumer electronics and online ser-\nvice companies, we have integrated many AI models into",
            "content": "Mark Lee is the first author. denotes the core authors. Danyang Zhuo contributed to this work as visiting scholar at Apple. Ruoming Pang is the corresponding author. 1 our products, catering to billions of users worldwide. We have two requirements for our deep learning systems besides training performance and scalability. First, we aim to empower our model engineers to experiment with diverse model architectures and training techniques. They should write only minimal amount of code to configure complex model definitions and training methods. We call this modularity of the deep learning system. Second, as large technology company, we cannot rely on single hardware vendorany hardware can run into supply issues and vary in pricing. Our design goal is be compatible with heterogeneous backends, such as GPU, TPU, and AWS Trainium. This allows us to train on major cloud providers (e.g., AWS, Google Cloud, Azure) as well as our on-premises servers. To facilitate modularity, the core design decision of AXLearn is to enforce strict encapsulation. While encapsulation is well known principle in object oriented programming, we find that its often neglected in ML frameworks as almost all deep learning systems rely on subtyping (see 2). The prevalence of subtyping can be partially attributed to the lack of formal analysis on how to quantify the modularity and extensibility of system, where conventional design principles or rules of thumb\" are non-exhaustive and hard to measure. To this end, we propose framework for quantifying the complexity of system by measuring LoC changes incurred by the addition of new feature. Such framework is necessary as the traditional mechanistic way of counting LoC can only capture complexity under frozen snapshot, when extensibility is really about how system evolves over time. We argue that under this framework, it can be demonstrated that subyting significantly hurts extensibility, and that applying encapsulation strictly can overcome these limitations. We also show that the framework is consistent with more traditional ways of counting LoC via the case study of integrating Rotary Positional Embeddings (RoPE) [39] and Mixture of Experts (MoE) [38] into AXLearn as compared to other state-of-the-art deep learning systems. To our knowledge, AXLearn is the only training framework that adheres strictly to encapsulationany module is replaceable, including the input pipeline, checkpointer, trainer loopallowing complex features to be implemented without increasing the complexity of the overall system. To enable execution on heterogeneous infrastructure, we build AXLearn on top of XLA [1] and GSPMD [42]. Our native integration with XLA allows parallelism strategies to be automatically generated, but still allows hand-crafting kernel code for specific accelerators. For instance, on each hardware backend, we replace the attention layer with custom kernel like FlashAttention [8, 9]. We believe this design strikes an ideal balance: by leveraging the XLA ecosystem, we can seamlessly support multiple hardware accelerators without sacrificing high performance. Due to its modular design, AXLearn also enables succinct user configurations to customize the parallelization, rematerialization, and quantization strategies, further simplifying the scaling experience. Table 1 provides list of deep learning systems designed for large models. Besides modularity and support for heterogeneous hardware, the table indicates whether each system is model-agnostic or supports broader parallelism strategies such as 3D parallelism [27]to enable efficient training across diverse model architectures. We summarize several important observations. Some systems only support specific architectures: for example, Maxtext [26] provides LLM implementations but does not extend easily to custom architectures as its design encourages fork-and-modify rather than reusing building blocks. Other systems are designed for specific backends: for instance, Megatron-LM [27] has carefully designed optimizations for transformers on GPU, but these optimizations do not directly apply to other accelerators. Finally, while some systems decouple parallelism from layer implementations, none follow strict encapsulation and are not truly extensible (7). Our company uses diverse set of model architectures across range of cloud backends, so we cannot directly use these systems. Over the past few years, weve deployed AXLearn to train thousands of models involving hundreds of engineers. The rapid adoption of AXLearn largely owes to its modularity and unique ability to scale on various public clouds, including Google Cloud TPU, AWS GPU or Trainium2. We share how our experience evolved our design choices over several years. AXLearn is open-sourced under Apache 2.0 at https://gith ub.com/apple/axlearn."
        },
        {
            "title": "2 Motivation\nIn the section, we quantify modularity and extensibility, and\ndiscuss why systems that do not follow strict encapsulation\nsuffer from compounding complexity. Second, we motivate\nthe need for heterogeneous hardware backends and why\nusing existing compilers (e.g., XLA [1]) is not sufficient.",
            "content": "2.1 Modularity In many deep learning systems, neural network layers are implemented by subtyping: layer inherits from some base 2 Figure 1. Specifying MoE transformer in AXLearn. Red components are reused from the the specification of standard transformer. In AXLearn, user script that defines MoE only needs to specify the green parts of the neural network. layer, instantiates child layers as instance attributes, and implements forward method that handles the layers logic. To motivate the discussion, consider the changes required to replace feed-forward network (FFN) of Transformer architecture with Mixture-of-Experts (MoE) layer under the subtyping paradigm. Taking the example from the DeepSpeed documentation [10], one applies such change by replacing the instance attribute for the original FFN: - self.fc3 = nn.Linear(84, 10) + self.fc3 = nn.Linear(84, 84) + self.fc3 = deepspeed.moe.layer.MoE(...) + self.fc4 = nn.Linear(84, 10) At glance, this seems like simple 4 LoC change. However, consider how its applied in practice: one would subtype self.fc3s parent layer to apply such change. This effectively reduces the problem of replacing the FFN with MoE, to the problem of replacing the parent layer with the new subtyped layer. By induction, its easy to see how such change can compound to changes to multiple modules across the subtype hierarchy. Indeed, this happens in practice: between DeepSpeeds QwenV2 and QwenV2 MoE implementations [11] over 200 LoC are required to apply the MoE layer, not accounting for the MoE layer itselfa far cry from the 4 LoC that we may have hoped. Conceptually, MoE is small changeideally, one should be able to swap the FFN directly with an MoE equivalent, without incurring the side-effects of propagating changes up to ancestors like the Transformer layer. In AXLearn, this can be achieved by exploiting the compositional nature of neural networks: by implementing the FFN and MoE layers with compatible input/output interfaces, and encapsulating all MoE specific details within its layer implementation, we realize this ideal scenario of treating MoE as drop-in replacement. Section 4 shows how 10-line System Megatron-LM [27] DeepSpeed [35] PyTorch FSDP [44] PyTorch XLA FSDP [33] TorchTitan [24] Haiku [16] Flax [17] Pax [32] MaxText [26] AXLearn (Ours) Underlying Framework Model Agnostic 3D Parallelism Modular GPU TPU Trainium PyTorch PyTorch PyTorch PyTorch PyTorch JAX JAX JAX JAX JAX partial partial partial partial partial Table 1. Comparing AXLearn with state-of-the-art deep learning systems. We define partial\" as systems that decouple parallelism from model implementation via device abstractions (e.g., XLA), but in practice do not apply strict encapsulation. code snippet can apply MoE to any model in AXLearn, as depicted in Figure 1. Note that by naively comparing the LoC between implementations, the benefits of composition cannot be fully observed. Specifically, the 190 LoC difference between the two implementations does not account for higher-order effects: if we consider that production codebase may instead contain tens to hundreds of variants of the same model, the 200 LoC change quickly becomes thousands. In other words, LoC only provides snapshot of the current complexity of the system, but tells us nothing about how extensible the system is, including how the LoC would change if we introduced different MoE implementation. We instead propose to measure the extensibility of system in terms of the asymptotic LoC changes required by some re-parameterization of its API. When we refer to the parameterization of an API, we refer to possible configurations under an existing implementation signaturee.g., the possible arguments to each function or class, or the possible attributes of each object. When we refer to the re-parameterization of the API, we essentially pose the question of how the LoC changes when we reconfigure the system to support different implementation signaturesuch as adding new functionality like the MoE layer. We denote such metric LoC-complexity(x): given some new functionality 𝑥, we measure the LoC changes that are necessary to re-parameterize the system to integrate 𝑥, as the number of components in the system scales asymptotically. This is motivated by the idea that an implementation that exhibits asymptotic growth in LoC is generally more complex than one that stays constant. This framework can be used to explain why composition should be favored over subtyping. The LoC-complexity(MoE) for DeepSpeed is lower-bounded by 𝑂 (𝑁 ), where 𝑁 denotes the number of modules in the system (7): we require (1) at least one LoC change for each attention variant to subtype forward implementation that replaces FFN with the MoE equivalent; (2) at least one LoC change in each ancestor module of an attention layer to incorporate the new subtyped layer. In contrast, the LoC-complexity(MoE) in AXLearn is 𝑂 (1): the code snippet in 4 can be used to integrate MoE without changing any model. In practice, we can validate the LoC-complexity with concrete LoC counts. Indeed, in AXLearn we use the same 10line snippet to configure MoE in over 1,000 different experiments2. On the other hand, if we were to adopt DeepSpeeds strategy of integrating MoE to our internal codebase, we would incur over 4,000 LoC just to modify different model variants to support their MoE counterparts. LoC-complexity allows us to reason about the complexity of an implementation without resorting to counting, which is code-version dependent and often subjective (e.g., one could argue that certain changes, like the shared experts in DeepSpeeds QwenV2 MoE, are not actually essential to the code changes and therefore should not contribute to the LoC count). Utilizing an asymptotic analysis obviates the need for debating counting methodologies entirely, and we hope that future work can continue to build on this metric. 2.2 Support for Heterogeneous Hardware As one of the largest technology companies, we cannot feasibly rely on single hardware platform for all of our machine learning workloads. Aside from supply constraints, our company is positioned to benefit from not committing to any single platform: Megatron-LM has vested interest in optimizing for Nvidia GPUs, while Haiku, Flax, Pax, and MaxText are mostly optimized for Google TPUs. Instead, we take the alternate approach of developing cloud-agnostic system that can adapt seamlessly to multiple platforms, which allows our engineers to take advantage of the most cost-effective solution for their training workload. For example, while AWS Trainium2 did not exist when we first began development, AXLearn is one of the first deep learning system that supports Trainium2 at scale. While JAX/XLA is key component to support heterogeneous hardware, it is not sufficient. XLA often provides 2An experiment refers to training job configuration, such as the model architecture and hyper-parameters. 3 reasonable out-of-the-box performance, but additional steps are needed to achieve the best performanceincluding providing hints to the XLA compiler at various points in the layer graph, using hand-tuned kernels for less mature compilation targets like GPU or Trainium2, or customizing the rematerialization strategy for each workload. In addition, JAX/XLA is not layer or orchestration library. To run training at scale in multiple public clouds, it is necessary to build cloud-agnostic and resilient orchestration systems that would otherwise not be necessary in homogeneous, internally managed cluster. For example, such systems must recover from opaque hardware failures, detect SDCs (silent data corruption), automate hang recovery (which may be due to internal failures in the cloud provider), and more. These built-in resiliency mechanisms allows us to quickly deploy to new cloud providers, including nascent ones like Trainium2 (and previously Google Cloud Platform), which may be less robust early in their lifecycle. This motivates the need for modular, cloud-agnostic library built on top of JAX/XLA."
        },
        {
            "title": "3 Overview\nOur design goal is to allow model engineers to rapidly exper-\niment with various model architecture and training methods\nacross heterogeneous hardware backends. AXLearn’s devel-\noper interface is a hierarchical configuration of modules. A\nmodule can be viewed abstractly as a node in an object tree.\nIn training, the root module is typically the trainer itself,\nwith child modules such as the model, learner, and input,\neach of which may have its own children. A module’s defini-\ntion follows a consistent structure, including a config object\nthat encapsulates all configurable parameters of the module.\nOur view is that for ML researchers, writing configs and\ncomposing modules is much easier than implementing sub-\ntyped modules from existing ones, a view that is shared by\nTorchTitan [24]. The pure composition approach allows re-\nplacing individual components and makes it easy to reuse\ncomponents between teams or 3rd party libraries.",
            "content": "Similar to TorchTitan, our model definition is independent of any specific parallelization strategy or trainer loop. However, from the developer experience point of view, AXLearn has several key differences in how modules are parameterized. First, AXLearn encourages hierarchical config composition over config flattening. In the example shown in Figure 1 and 4, user script can build on top of standard transformer architecture by defining only the green components, and selectively replacing FFN modules with MoE equivalents without needing to inspect the details of the base transformer architecture. In contrast, TorchTitan adopts monolithic approach where config files flatten all possible configurations, including model, optimizer, checkpointer, and trainer configs. As we demonstrate in 7, flattening has significant ramifications in the extensibility of TorchTitan. Figure 2. AXLearns system diagram. The blue components belong to AXLearn. Second, AXLearn uses an entirely Python-based interface, which has the benefit of allowing configs to be expressed with Python constructs like functions, loops, and recursion; as well as the advantage of being able to be directly unittested. Figure 2 shows AXLearns system diagram and workflow. There are two key components in AXLearn: (1) AXLearn composer and (2) AXLearn runtime. user typically uses the layer library in AXLearn and potentially third-party layers to define training configuration. Given such script, the AXLearn composer first materializes full JAX program, which includes selecting the appropriate mesh shape for the desired accelerator instance, applying sharding annotations for certain layers, performing auto-tuning of XLA compilation options for the target hardware, selecting appropriate attention kernels for the backend, and applying appropriate rematerialization strategies based on tagged points in the module hierarchy. These annotations are crucial for training to run efficiently. The JAX program and compiler options are then passed to the XLA compiler to generate the accelerator program (including e.g. CUDA kernels), which is then orchestrated via the AXLearn runtime on distributed hardware (e.g., Kubernetes) using accelerator-specific runtime (e.g., CUDA runtime). AXLearn runtime monitors the execution of the accelerator program and provides additional capabilities like efficient checkpointing, monitoring, and fault tolerance."
        },
        {
            "title": "4 AXLearn Composer\nWe first describe the hierarchical and modular configuration\nfront-end in AXLearn. We then describe how configurations\nare transformed into JAX programs.",
            "content": "4 4.1 Modular Configuration Many libraries use languages such as YAML to represent configurations, which do not easily support reuse across configurations and often leads to maintenance overhead as project grows. Other libraries (e.g. T5X [36]) use domain specific languages (DSL) like gin [18] to define configurations. This often introduces differences in writing and testing code, and DSLs are usually limited in functionality such as support for loops or functions. Therefore, we choose Python-based approach to configuration. Hierarchical Configuration TorchTitan, DeepSpeed, Flax, and most other libraries utilize flat config layouts that provide birds-eye view of all configurations. However, flat layout can become unwieldy as the number of layers grows, or when switching implementations of layers (in which case, one must flatten multiple combinations of configurations). In AXLearn, users instead specify training configurations via composition, which logically forms tree. As an example, Transformer layers configuration consists of multiple child layer configurations: class TransformerLayer(Module): class Config(Module.Config): self_attention: AttentionLayer.Config feed_forward: FeedForwardLayer.Config ... Child configurations are encapsulated and can be specified independently of the parent: TransformerLayers config does not directly specify the hyperparameters of its child layers, allowing users to switch between different implementations of child layers. For instance, one can decide to replace feed_forward with MoE without changing the config of TransformerLayer. Further, since layers can be reused between models, the configs are often initially partially specified. As concrete example, we can build partial configuration for TransformerLayer with SwiGLU [37] activations: # Build TransformerLayer config. cfg = TransformerLayer.default_config() # Configure the FeedForwardLayer. cfg.feed_forward.hidden_dim = scaled_hidden_dim(scale=8/3) cfg.feed_forward.activation = (\"linear\", \"nn.silu\") Note that we have not configured an input dimension on feed_forward; instead, hidden_dim is configured to be function of the (as of yet unspecified) input dimension. This allows the parent TransformerLayer to set input_dim when the layer is instantiated: class TransformerLayer(Module): def __init__(self, cfg): # feed_forward uses the same input_dim. cfg.feed_forward.set(input_dim=cfg.input_dim) self._add_child(\"feed_forward\", cfg.feed_forward) By partially defining configs and then propagating from parent to child, configurations often only need to be specified once, commonly at the layers near the root of the tree. So long as the parent and child agree on configuration interface (often just input and output dimensions), arbitrarily complex configs can be constructed while keeping layers modular. Config traversal Because configs are hierarchical and encapsulated, one can apply arbitrary modifications by traversing the config tree. The illustrate, the following snippet recursively replaces any target config with new_cfg: def replace_config(cfg, tgt, new_cfg): def enter_fn(child): for key, value in child.items(): if isinstance(value, tgt.Config): new_cfg.set(**value.items()) child.set(key, new_cfg) cfg.visit(enter_fn=enter_fn) This can be used to apply MoE in the following manner: # Replace any FFN with MoE. replace_config( trainer_cfg, target=FeedForwardLayer, new_cfg=MoELayer.default_config().set(...), ) Indeed, this roughly 10-line code snippet is used to apply MoE to over 1,000 experiment configs, without any additional changes to other modules. Interoperating with 3rd party modules. One of the key benefits of compositional system is interoperability with modules from other deep learning systems. The configuration system exposes two APIs for 3rd party modules: config_for_function: An API that takes an arbitrary function and constructs config object from the function signature. config_for_class: An API that takes an arbitrary class and constructs config object from the __init__ signature. We have used these two APIs to adopt modules from T5X, Hugging Face, and Flax, and optax. Note that while its possible to use 3rd party modules for quick experimentation, such modules often do not natively specify XLA sharding annotations, which means the efficiency of program may be at the mercy of the XLA compiler. Therefore, users often opt to use AXLearns own layers, which provide annotations by default and expose configurations for customizing them. 4.2 Generating JAX Programs AXLearn is built on top of JAX [4], which enables automatic differentiation for NumPy programs and leverages XLA to produce efficient hardware-accelerated programs. Nevertheless, in some cases users still need to manually provide hints\" to the XLA compiler or implement custom kernels to achieve the best performance. In other JAX libraries like Flax and Haiku, these tasks are often left to the user, which can lead to additional work to transition model from proof of concept that passes unit tests to one suitable for large-scale training. AXLearn instead aims to make parallelism as simple as possible, so that researchers can focus on model iteration. Config-based parallelism AXLearn natively integrates parallelism support in every relevant layer for all common parallelism strategies, including fully-sharded data parallelism [35], pipeline parallelism [19], expert-parallelism [13], sequence parallelism [23], and tensor model parallelism [27]. This means that users do not need to implement parallelism directly, but instead specify their desired parallelism strategy via configuration. This contrasts with Flax or PyTorch where sharding is not native concept in the layer library, and thus code changes may be necessary depending on parallelism strategy. For instance, to configure mix of FSDP and tensor parallelism in AXLearn, users simply configure mesh topology and sharding via named axes: cfg = Trainer.default_config() cfg.mesh_shape = (4, 2) cfg.mesh_axis_names = (\"fsdp\", \"model\") For an 8-device setup, this configures training for 4-way fsdp parallelism, and 2-way model parallelism. For basic cases, this is often enoughAXLearns default layer implementations are already mesh-aware, and XLA will complete any intermediate shardings as needed to minimize overall device memory usage and communication overhead. In this particular instance, by specifying fsdp as one of the named axes, certain layers will partition weights over the leading batch dimension, and XLA will naturally employ strategy involving overlapped all-gather of weights and computation of layer outputs or gradients. Users also have granular control over how specific parameters in specific layers are partitioned. For instance, user can employ mix of FSDP and tensor parallelism for Linear layer in this fashion: cfg = Linear.default_config() cfg.param_partition_spec = (\"fsdp\", \"model\") Specified in such way, the linear layer will partition weights over the named axes (\"fsdp\", \"model\") and biases over the single axis \"model\". In more advanced cases, such as when writing custom layer implementations, JAX APIs like with_sharding_constraint() can be utilized to further constrain the sharding of intermediate tensors or activations. Note that AXLearns Linear layer implementation automatically infers the bias sharding from the sharding of the model weights, which minimizes communication costs. More generally, arbitrary config modifications to different modules in the hierarchy can be expressed as configuration modifiers, so that sharding, hyperparameters, and architecture can be tuned in the same manner. Memory optimizations AXLearn has several built-in mechanisms for optimizing high-bandwidth-memory (HBM) usage which are necessary for large-scale training. 6 First, AXLearn layers natively support configuring rematerialization (a.k.a., activation checkpointing or remat). Depending on the hardware, the remat strategy must be tuned to selectively save or recompute certain activations in the backward pass. In AXLearn, common remat points (such as the attention QKV projections and output) are tagged with names, such that users can selectively target remat points to decide which activations to save in accelerator memory, offload to CPU memory (if supported by hardware), or recompute. Users can also employ programmatic remat strategies, such as only saving the output of linear layers. Second, AXLearn implements optimizer state offloading to CPU memory. This is essential for training models with more than hundreds of billions of parameters on certain platforms like TPU v5e where HBM is limited, and where sharding beyond certain point can be inefficient. Hardware-dependent optimizations Different hardware platforms exhibit different characteristics, including interconnect topology, FLOPS, and HBM per chip. For example, Nvidia H100s use hierarchical network topologies with fast intra-node NVLink between 8 GPUs but slow inter-node network. On the other hand, TPU v5es have fast inter-chip interconnect (ICI) for up to 256 chips within slice, but slower data-center network (DCN) across slices. Additionally, each hardware platform supports different kinds of low precision training. For example, TPUs support INT8 quantized training, while H100s support both INT8 and FP8. In order to support efficient heterogeneous training, it is necessary to apply target-dependent parallelism strategies. AXLearn introduces the concept of mesh rules\" to address this problem. mesh rule describes mapping from accelerator types to config modifiers, which allows users to express complex per-target optimizations with succinct and self-contained configs. AXLearn uses these rules to automatically apply the appropriate modifiers based on the target platform. Appendix shows roughly 10 line code snippet that configures training to use FSDP within TPU v5e slices and data-parallel across slices, offloading activations from dot products to host memory and enabling INT8 training. On the other hand, when running on H100s, we instead switch to 8-way tensor parallelism within node and FSDP across nodes, saving query, key, value, and output (QKVO) projections to HBM, as well as enabling FP8 training with delayed scaling. No other code changes are required for efficient training between these two fundamentally different platforms, owing to the compositional nature of the library. While the XLA compiler can fuse most operations to produce optimized target-specific code, in some cases we can achieve better performance with custom kernels. Again owing to the modular nature of AXLearn layers, enabling custom kernels only requires simple configuration changes. For instance, AXLearn provides FlashAttention [8, 9] layer, which can be used as drop-in replacement for the default attention layer. Like above, this can be expressed as config modifier in mesh rule. Behind the scenes, the FlashAttention layer transparently dispatches kernels based on the backend: on GPU, cuDNN [6] is used when possible, falling back to custom Pallas [31] kernel for cases like block-sparse attention where cuDNN is not supported; on AWS Trainium, the Nki kernel from AWS Neuron Toolkit [2] is used; and on TPU, the SplashAttention Pallas kernel in JAX [4] is used. We note that efficient heterogeneous training is largely possible because of several core design choices in AXLearn. First, AXLearn layers are already sharding and remat aware. This allows complex strategies to be expressed with configuration rather than code. Second, all components are implemented as strictly encapsulated modules. This allows expressing optimizations like quantization as replacement of DotGeneral layers [41] with their quantization-aware equivalents. Third, configurations are entirely Python-based. This allows utilizing constructs like recursion to traverse and modify configs, which would otherwise not be possible with DSL. AOT compilation While mesh rules largely address the problem of specifying which optimizations to apply, in practice large portion of iteration time can be attributed to actually measuring performance on accelerators. core advantage of compilation-based approach is that many of the errors that would otherwise be encountered in full scale distributed run can be checked entirely locally, from the convenience of single host environment. AXLearn provides native support for JAX Ahead-of-Time (AOT) compilation, which allows users to analyze the memory and FLOPS utilization of training program without executing single line of the program, including catching errors like OOMs that would otherwise result in wasted resources. Because the same codepath is used for AOT and actual training, users can be confident that program that AOT-compiles will run at larger scale. This saves considerable time and resource costs when scaling development to large teams. 4.3 Maintaining States across Module Boundaries. In order to be transformed by JAX primitives such as jit and grad, JAX programs must be purely functional, which means that they must be stateless. However, neural network training is inherently stateful: it is often necessary to store model parameters, maintain pseudo-random number generators (PRNGs), collect training summaries, and aggregate outputs. The canonical way to maintain state in functional system is for the caller to pass in side inputs (such as layer parameters) along with the main inputs for the method. The outputs include not only the method results, but also side outputs such as summaries and state updates. Rather than requiring the user to maintain these side inputs and outputs, AXLearn introduces an abstraction called the InvocationContext, depicted in Figure 3. When parent module invokes child module, an InvocationContext for the child module and state is automatically pushed to the stack, which transparently splits the PRNG key and creates new data store for any training summaries or outputs saved by the child module. When child module returns, the context is popped off the stack, which transparently collects child summaries and outputs into the parent data store. Analogously to the traditional call stack, user code does not need to be aware of the InvocationContext. This allows users to implement modules in familiar imperative fashion while retaining the functional properties demanded by JAX. subtle design decision is that InvocationContexts contain references to modules, but not vice-versa. This allows contexts to be accessed outside of the module hierarchy, including from arbitrary function calls that may not have references to module, allowing deep integration with 3rd party libraries that are not natively aware of the AXLearn state system (e.g., optax optimizers [29]); as well as compatibility with codepaths with unique execution behavior, such as JAXs custom_vjp backward pass. The design of InvocationContext is key factor allowing complex architectures to be well-encapsulated. In other libraries, users may be required to perform nested instance attribute access in order to access shared state between layers. However, this requires that module implementations to be intricately aware of other modules being used and where they are in the hierarchy. In AXLearn, the system layer can instead transparently traverse the InvocationContext hierarchy to retrieve shared states, which allows module implementations to remain completely unaware of other modules in the system."
        },
        {
            "title": "5 AXLearn Runtime\nAXLearn’s runtime orchestrates distributed execution on\npublic clouds and on-premise clusters, providing essential\nfeatures like monitoring, checkpointing, and fault tolerance.",
            "content": "Monitoring and profiling. AXLearns runtime provides observability at several layers. At the hardware layer, AXLearn natively integrates with JAXs profiler to provide insights into inefficiencies introduced by input pipelines, sharding strategies, compiler behavior, or other aspects of the program. Users can remotely attach to in-flight programs so that profiles can be collected in an on-demand fashion. For large scale distributed training, it is often insufficient to profile at the per-host level, as issues can arise from interhost networking, firmware bugs, disk failures, or other broad failures. AXLearn supports generic measurement interface that can be used to record arbitrary events such as the start of training or the start of step. These events can be used to measure end-to-end inefficiencies such as those introduced by hardware provisioning or checkpointing recovery, which can be captured via metrics like overall job goodput. 7 Figure 3. Invocation Context. Module invocations push contexts to the stack, which retrieve child states, split PRNG keys, and create child output collections. Upon returning, contexts are popped, collecting outputs into the parent collection. The context stack can be programmatically traversed to retrieve shared state, allowing features like tied weights to preserve encapsulation. Checkpointing. AXLearns default checkpointing capabilities are similar to that of orbax [30], but supports multiple cloud storage backends, including AWS S3 and Google Cloud Storage (GCS), as well as provides additional memory optimizations for large-scale checkpointing. Specifically, the checkpointer supports data-sharded serialization, where checkpoint state is further partitioned over data-parallel workers rather than naively assigning each replicated shard to its 0th replica for serialization; as well as concurrency-bounded serialization, which enforces limit on the maximum number of in-flight shards being copied into host memory. We find these optimizations to be critical for large-scale training where having too many shards in-flight can cause certain third party storage backends to exhaust the host memory. Checkpoints are saved asynchronously during training to avoid bottlenecks on slow networks, blocking only in rare cases where the checkpointer is waiting on prior serialization to complete. gargage collector runs in the background to remove old checkpoints according to user-configurable policy to reduce storage costs. In situations where the default checkpointer is insufficient, checkpointing is entirely customizableusers can configure storage-layer implementations, or even replace the entire checkpointing system. Failure detection. Failures are inevitable for large-scale training spanning thousands of chips. core problem is therefore identifying when and where failure happens. To facilitate detection, AXLearn provides several components. First, AXLearn runtime has configurable watchdog that monitors the step time and hardware utilization of host. Upon observing low hardware utilization or abnormal step times, the watchdog can be configured to force restart of the host, alert an on-call for manual intervention, or dump stack traces for debugging. We find this to be essential as large fleet is expected to encounter hardware failures several times day, which can surface in surprising, opaque ways. AXLearn also has built-in support for XLA silent data corruption (SDC) checks, which is configurable to run at certain intervals. The SDC detector performs number of consistency checks, such as repeating single communication multiple times to check for interconnect problems, and alternating kernel execution on devices with multiple cores to check result consistency. Failure recovery. Even few minutes of downtime at large scales is incredibly costly. Efficient recovery is therefore critical to ensure high hardware utilization and goodput. Largescale training usually involves multiple data-parallel replicas with replicated weights. In these cases, the default async checkpointer can be swapped with orbax multi-tier checkpointing, where checkpoints are distributed across nodelocal storage such as memory or hard disk, and are only synced to remote storage periodically. This allows checkpointing to happen at significantly shorter intervals as saves are no longer bounded by the bandwidth to remote storage, reducing progress lost after restart. Upon failure of dataparallel replica, the checkpoint can be restored directly from healthy replica and broadcasted through the fast interconnect to all other healthy or newly provisioned replicas. This allows for considerably faster recovery time as compared to having all replicas restore checkpoints directly from remote storage. Applying similar strategy, persistent compilation caches can be configured to eliminate any startup compilation time, as compilation artifacts can be entirely reused across restarts of the same model. Finally, AXLearn builds on native Kubernetes features to support slice-level hot-swap. In this case, the AXLearn scheduler over-provisions spare replicas within the same cluster, allowing failed nodes in an ongoing training job to be rapidly substituted with healthy nodes. In the meantime, the over-provisioned hardware can still run low-priority jobs to reduce resource waste, or be sent for inspection and repair. Combining the above strategies allows us to reduce the restart time of 32,768 chip job from hours to less than ten minutes."
        },
        {
            "title": "6 Unifying Training and Inference\nOne surprising discovery during the process of building\nAXLearn is that we can get an efficient inference engine\nby reusing a substantial subset of AXLearn components.\nThough inference performance is not our design goal, AXLearn\nachieves significantly higher performance than a state-of-\nthe-art inference engine, vLLM [22], on TPUs. (See §7 for\ndetails.)",
            "content": "AXLearns design allows modules to easily be adapted to optimize for decoding. For example, because the attention layers KV cache is an encapsulated component, an attention layer can incorporate inference-friendly cache layouts without changing the attention layer. This allows incorporating new inference techniques like continuous batching [43], disaggregated prefill and decode [46], and paged KV cache [22] without re-implementing models and layers. While we currently support TPU, we believe with additional effort we can support unified training and inference on other backends."
        },
        {
            "title": "7 Evaluation\nIn this section, we analyze the extensibility of AXLearn\nand other systems, evaluate performance across heteroge-\nneous hardware, and describe our experience of deploying\nAXLearn.",
            "content": "7.1 Modularity of Configurations We analyze modularity using LoC-Complexity, motivated at length in 2. Specifically, we analyze the asymptotic LoC changes required to re-parameterize each system to support RoPE and MoE. When measuring extensibility, we focus on LoC changes incurred in existing modules in the system as opposed to the new functionality itself. This allows for comparable analysis, and in practice, the same feature across systems tends to incur similar LoC up to some constant. RoPE and MoE are useful examples as they are commonly used in modern architectures (e.g., Llama3 [15], DeepSeekV3 [25]), which allows us to point to concrete changes that have already been made by each system. This gives more objective estimate as compared to hypothesizing changes. Table 2 shows the summary of LoC-Complexities across systems. We omit PyTorch FSDP, which is not an LLM library; Haiku, which has no implementation of RoPE or MoE; and Pax, is replaced with its layer library Praxis. For concrete evaluation, we also provide LoC estimates based on the 9 changes required in production codebase. While many of the listed systems provide few sample implementations, production codebase can be orders of magnitude larger. As large technology company, we cannot disclose specific details of the scale of our internal model architectures and hyper-parameters. Therefore, we propose estimates under realistic setting of codebase with 20 model variants (e.g., variants of GPT model) and 10 variants of attention (i.e., kernels, kv cache strategies, etc.). Here we provide the summary of our LoC analysis. See Appendix for more details. Notation Let 𝑁 be the number of modules in system, and 𝑀 be the number of variants of the feature to be added. For example, MoE has variants of routing or gating mechanisms. AXLearn (ours) In AXLearn, RoPE and MoE are strictly encapsulated. 4 provides 10-line code snippet to integrate both features into any experiment config. Internally we use such code snippet to configure over 1,000 experiments to use RoPE, MoE, or both. As we scale the number of modules or RoPE or MoE variants, we require no changes to any existing interfaces, achieving constant LoC-Complexity. Megatron-LM Megatron-LM makes notable attempt to apply composition via transformer submodules. However, it does not apply composition strictly, as the constructor of each model flattens RoPE-specific configs. For each of 𝑀 new RoPE variants, one must modify 𝑂 (𝑁 ) constructor signatures. Thus, LoC-complexity(RoPE) scales with 𝑂 (𝑁 𝑀). Like AXLearn, Megatron-LM defines MoE layer in place of the standard MLP. However, each MLP introduces MoEspecific fields in its signature, so that any module composing linear submodule incurs at least one LoC change for MoEspecific arguments. This scales with 𝑂 (𝑁 ). We use Megatron-LMs implementations to estimate the LoC changes in the production settings. Our conservative estimate is that approximately 300 LoC is needed to reparameterize the system for RoPE, and 20 LoC for MoE. DeepSpeed In DeepSpeed, every model subtype defines an embedding type property to configure whether RoPE should be used. Further, each attention layer is made aware of the embedding type. The cross product of attention and embedding types means LoC-Complexity(RoPE) scales quadratically with 𝑂 (𝑁 𝑀). In the MoE case, DeepSpeed subtypes each model from custom MoE base class; this requires reimplementing most methods, which incurs 100s of LoC. We estimate 400 LoC for RoPE and 4,000 LoC for MoE. TorchTitan TorchTitan introduces RoPE-specific configs to each model config. Each attention layer uses these configs to choose the child RoPE layer. This interaction between attention and RoPE results in 𝑂 (𝑁 𝑀) LoC-Complexity. Layers also conditionally instantiate MoE or FeedForward layers based on the model config. Because TorchTitan transformer layers are model specific, this results in 𝑂 (𝑁 𝑀) as we must System Megatron-LM [27] DeepSpeed [35] TorchTitan [24] Flax [17] Praxis [32] MaxText [26] AXLearn LoC-Complexity(RoPE) 𝑂 (𝑁 𝑀) 𝑂 (𝑁 𝑀) 𝑂 (𝑁 𝑀) 𝑂 (𝑁 𝑀) 𝑂 (𝑁 𝑀) 𝑂 (𝑁 𝑀) 𝑂 (1) LoC-Complexity(MoE) 𝑂 (𝑁 ) 𝑂 (𝑁 𝑀) 𝑂 (𝑁 𝑀) N/A 𝑂 (𝑀) 𝑂 (𝑁 𝑀) 𝑂 (1) Table 2. LoC analysis of deep learning systems. 𝑁 denotes number of modules in system, and 𝑀 denotes number of variants of feature. LoC Estimates represent changes necessary within each systems API interfaces to integrate single variant of RoPE or MoE under standard production setting. Note that in AXLearn, 0 LoC changes to existing interfaces are necessary. LoC Estimate (RoPE) 400 320 240 600 300 200 0 LoC Estimate (MoE) 20 4000 400 N/A 5 300 0 Model Hardware Llama2-7B Llama2-70B 32 H100-8 tpu-v5p-512 64 Trainium2-16 64 H100-8 tpu-v5p-1024 64 Trainium2System PyTorch FSDP Megatron-LM* MaxText AXLearn PyTorch XLA FSDP MaxText AXLearn AXLearn PyTorch FSDP Megatron-LM* MaxText AXLearn PyTorch XLA FSDP MaxText AXLearn AXLearn Iteration Time (s) MFU Throughput (tokens/s) 2.6 1.4 1.4 1.4 3.5 2.7 2.5 1.2 10.6 7.3 9.4 9.2 12.3 11.6 11.2 29.9% 53.4% 54.7% 54.2% 46.7% 61.6% 66.2% 24.2% 34.7% 50.7% 39.1% 40.0% OOM 64.4% 68.0% 25.0% 1.6M 3.0M 3.0M 3.0M 1.2M 1.6M 1.7M 3.5M 396K 578K 446K 456K 341K 360K 374K Table 3. Comparing AXLearn with state-of-art systems on training performance. *Megatron-LMs results are from NVIDIA. All tests use the same batch size (1024) except that Megatron-LM on Llama2-7B uses batch size of 4K. modify transformer layer for each MoE variant, resulting in 240 LoC for RoPE and 400 LoC for MoE. Flax Flaxs Gemma config similarly flattens RoPE parameters, while the transformer module propagates them down to the attention layer where RoPE is implemented. LoC-Complexity(RoPE) is 𝑂 (𝑁 𝑀) as for each variant, one must update the RoPE parameters in the top-level config and then propagate those configs down an arbitrary number of modules to the attention layer. Unfortunately, Flax has no public example of MoE so we exclude it from our analysis. Praxis Like AXLearn, Praxis uses template composition approach, which allows LoC-Complexity(MoE) to be 𝑂 (𝑀): each MoE variant incurs small 5 LoC change. However, Praxis doesnt apply strict encapsulation; it flattens RoPE configs into each attention layer. Since RoPE variants may have different configs, LoC-Complexity(RoPE) is 𝑂 (𝑁 𝑀); each may require modifications to each attention layer. MaxText MaxTexts Attention conditions on configs (like attention type) to choose the RoPE module. The interactions between attention and RoPE variants scales with 𝑂 (𝑁 𝑀). Similarly, MaxTexts MoE implementation details are flattened into each models decoder. MaxTexts trainer also includes MoE-specific logic: each loss function uses MoE configs to apply auxiliary losses. Every variant incurs 𝑂 (𝑁 ) changes over loss functions, and 𝑂 (𝑁 𝑀) overall. This results in roughly 300 LoC. 7.2 Performance on Heterogeneous Hardware We compare AXLearn training performance with PyTorch FSDP [44], Megatron-LM [27], and MaxText [26], which achieve state-of-the-art performance on GPU and TPU. We evaluate two models, Llama2 7B and 70B, on three different hardware backends: (1) 256/512 H100 GPUs (32/64 AWS P5d instances, each with 8 H100 GPUs); (2) TPU-v5p-512/1024 (64/128 GCP Cloud TPU hosts, each with 4 chips); and (3) 1024 Trainium2 (64 AWS trn2 instances, each with 16 Trainium2 chips). All runs use global batch size of 1024. Note that since Pytorch FSDP does not run on TPU, we use Pytorch XLA FSDP as its closest replacement [33]. Megatron-LM is GPU-only system, so we only report results on GPU. None of the baselines support Trainium, so we only report results for AXLearn. 10 Model 7B 70B System vLLM AXLearn vLLM AXLearn TTFT (ms) TPOT (ms) 538.6 40.1 80213.6 150.5 22.42 9.1 189.8 28. Table 4. Comparing AXLearn with vLLM on inference latency for Llama 2 models on Time-To-Fist-Token (TTFT) and Time-Per-Output-Token (TPOT) on TPUs. Table 3 summarizes the performance results. For Llama2 70B, AXLearn outperforms PyTorch FSDP and MaxText on H100 GPUs. We believe PyTorch FSDP especially lags in MFU for several reasons. First, Pytorch FSDP has less fine-grained control over activation checkpointing. Without intrusive modifications to its implementation, checkpoints occur at the decoder block level, meaning that activations within decoder layer must be either fully recomputed or fully saved. On the other hand, AXLearn can save only the most expensive operations (e.g. the output of linear layers), allowing more of these operations to be saved in HBM, thus reducing the recomputation time. Second, torch.compile does not work well with FSDP, while AXLearns first class compilation support allows memory bound operations such as RMSNorm and RoPE to be fused without any hand-written kernels. We hypothesize that Megatron-LMs strong performance on H100s over AXLearn can be attributed to several factors. First, JAX/XLA on GPU is relatively nascent, where running at the 70B scale was not viable just over year ago. This suggests that while XLA on GPU lags behind, we believe it is not fundamental limitation and that future efforts can close the gap. Additionally, Megatron-LMs numbers are obtained from Nvidias own DGX H100 clusters, while in our experience the achievable bandwidth on public cloud can often lag behind advertised numbers. On TPUs, AXLearn achieves state-of-the-art performance, where MaxText lags slightly behind, likely due to choices on rematerialization. On the other hand, PyTorch XLA FSDP fails to run entirely with out-of-memory errors. To demonstrate AXLearns scalability, we perform weakscaling study of two production models. Model is 70B model with 4,096 context length, and Model is 150B model with 8,192 context length. Fixing the per-device batch size, for Model A, scaling from 256 to 4,096 chips reduces MFU from 63.0% to 52.4%; and for Model B, scaling from 8,192 to 32,768 chips reduces MFU from 40.6% to 37.6%. These numbers demonstrate that AXLearn achieves very close to linear scaling, shown in Figure 4. We note that MFU for the 150B model is lower due to the need to limit the global batch size at 32,768 chip scale for good training convergence. All scaling experiments for the 150B model has 1/16 per chip sequence length compared to 70B experiments. (a) Model (70B) (b) Model (150B) Figure 4. AXLearns training performance and MFU when scaling the number of TPUs. Figure 5. Comparing AXLearn with vLLM on inference throughput on TPUs. AXLearn also achieves state-of-the-art inference performance on TPUs. We compare the inference performance of Llama2 7B and 70B-parameter models between AXLearn and vLLM. We use the ShareGPT dataset for the prompts and measure latency and throughput in the batched inference settings. For latency, we measure the time to the first token (TTFT) and the time per output token (TPOT). For throughput, we measure the output tokens per second. For the 7B model, the benchmark is performed on Google Cloud TPU v5p-8 VM instance, with maximum input length of 1024 and maximum output length of 256. For the 70B model, the benchmark is on Google Cloud TPU v6e-8 VM instance3, with maximum input length of 1,800 and maximum output length of 256. Table 4 and Figure 5 show the inference performance comparison. AXLearn is significantly faster than vLLM in terms of latency and throughput, achieving 500x and 6x speedup in TTFT and TPOT over vLLM, respectively. In terms of throughput, AXLearn is 2.8x faster for 7B model inference and 1.6x faster for 70B model inference. We suspect this is due to vLLMs implementation issues on TPU, as TPU support for vLLM is still experimental. 7.3 Experience of using AXLearn in production JXTrain began development in late 2021 using PyTorch with small team of engineers. At the time, signs of the Transformer architecture becoming de facto choice were evident (e.g., BERT [12], T5 [34], GPT3 [5]). Architectural convergence meant that most layers in many new models can be reused if implemented modularly. However, due to limited support for automatic parallelization, layer implementations ultimately could not be fully modularizedin many cases, different 3vLLM does not support the 70B model on v5p-8 VM by the time of benchmark. 11 parallelization strategies meant that logic had to be rewritten, with inevitable interactions across modules. GSPMD [42] (published late 2021) meant that communication can instead be injected by compiler. We chose to deeply integrate with XLA as bet on the compiler-first approach. First, layers can be modularized as sharding propagation across the graph can be automatically and transparently handled. Second, we believed eventual adoption would push hardware providers to optimize compilers on their platforms, which means performance improvements can be achieved without single LoC. As it turns out, while JAX/XLA did not initially support GPUs, today it achieves competitive performance across AWS and GCP; and is now natively supported by platforms like Trainium2. Where compilers fall short, we surmise custom kernels can close the gap [31]. Adopting JAX/XLA meant several paradigm shifts. First, it shifts from imperative to functional programming, which posed challenge as familiar patterns like mutable state were prohibited. Instead, one manually specified parameters as inputs to each module, and bubbled up outputs through the call stack. This hurt modularity as it required implementations to be intricately aware of state structure, and hurt usability as users lost the comfort of the PyTorch imperative style. This motivated the design of InvocationContext, which allowed state to be decoupled, outputs to be collected across the module hierarchy, and layer implementations to be implemented in familiar PyTorch fashion. Second, it shifts focus to the cloud. JAX/XLA initially only supported Google Cloud TPU, which meant that number of internal components could not be directly used. As GCP was in nascent state with limited TPU capacity, we knew we could not rely on it entirely, and hoped to leverage our internal compute infrastructure. To that end, we designed the system to allow all components of training to be configurable. For example, we initially leveraged Flaxs GCS-compatible checkpointer to bootstrap the layer library, which we later replaced to support internal storage backends. This migration was completely seamless with no changes to user code, which was only possible due to the modular design. As we scaled development, we faced additional barriers. First, resource contention was crucial problem due to limited TPU capacity within GCP. Aside from leveraging other backends, we realized that significant portion of resources were consumed by jobs with low resource utilization or no progress due to preventable errors. However, XLA compilation inherently operates on device abstractionsmany errors (e.g., OOMs due to suboptimal sharding) can be theoretically caught from local machine. We therefore capitalized on JAXs compiler-first approach by deeply integrating AOTcompilation, which allowed users to debug training entirely on CPU. This was key to allowing development to scale even with limited capacity. Second, we learned the hard way that ML testing practices are often insufficient. While its common to unit test layers, ML experiments heavily depend 12 on correctness of configs. We extensively adopted Pythonbased configs so that configs can be unit-tested along with layer implementations, but this was not enough. With large codebase, changes to one experiment can inadvertently affect others, which are difficult to catch with unit or integration testing (e.g., subtle changes in training dynamics). To address this issue, AXLearn introduces the notion of golden configuration tests: key training configs are serialized into human readable format and committed along with code changes. This allows changes to produce consistent and reviewable diffs, to trigger relevant code-owner reviews, avoid codecontribution-level conflicts, and provide traceable commit history of experiments. Weve since extended this idea to other golden tests, including golden checkpoint, initialization, or regularizer tests. Lastly, public cloud infrastructure can fail in opaque ways. In contrast to an internally managed cluster, we learned that we must account for opaque failures often out of our control, including hardware failures, ICI failures, SDCs, kernel panics, file system throttling, and more. While we have built many layers of resiliency, debugging and mitigating many of these failures required close collaboration with partners at Google, Amazon, and Nvidia. Today, AXLearn has grown from handful of developers training models at million parameter scales, to hundreds of developers training models at billion-to-trillion parameter scales. It actively supports over 10,000 experiments under development at given time, running across tens of different heterogeneous hardware clusters. Some of the models trained with AXLearn now power features used by over billion users, including intelligent assistants, multimodal understanding and generation, and code intelligence."
        },
        {
            "title": "8 Related Work",
            "content": "Training systems for large models. High-performance large model training is an active research area. FSDP [44] and DeepSpeed [35] use sharding to reduce GPU memory usage. Megatron-LM [27] combines data parallelism, tensor-model parallelism, and pipeline parallelism to train LLMs efficiently on GPUs. MegaScale [20] further scales LLM training to more than 10,000 GPUs and deals with straggler and failures in GPU clusters. AXLearns focus is complementary to these systems, with the goal of enabling engineers to use these techniques with minimal effort. Software modularity. Maintaining modular structure is standard software engineering practice in any complex computer system, facilitating maintenance and integration of new features [3, 21]. Modularity is especially important for fast-moving machine learning system: in our company, we iterate on new model architectures, hyper-parameters, and training methods every day. To facilitate this development, configurations must be separated from system details like parallelism or rematerialization in the simplest way possible, so that researchers can focus on model iteration. Configuration systems for distributed workloads. One of our contributions is the ability to configure distributed deep learning execution in modular and automated manner. Configuration for distributed systems is topic beyond deep learning. Metas configuration system [40] can enable easy A/B testing, similar to how to we conduct A/B testing on model accuracy and training performance by comparing AXLearns configurations. However, our main design goal for the configuration system is to enable modules to be reparametrized and composed to easily integrate with new features (e.g., RoPE, MoE)."
        },
        {
            "title": "9 Conclusion\nWe have developed AXLearn, a modular deep learning sys-\ntem for scalable training of large models on heterogeneous\nhardware. We present a new method to measure modular-\nity using LoC-complexity, showing our system maintains\nconstant complexity as the number of modules scales. Fur-\nther, AXLearn has comparable performance compared to\nother state-of-the-art training systems. Finally, we shared\nour development and operation experiences with AXLearn.",
            "content": "References [1] 2025. XLA: Optimizing Compiler for Machine Learning. https: //openxla.org/xla/tf2xla [2] AWS. 2025. Neuron Documentation. https://awsdocs-neuron.readthe docs-hosted.com/en/v2.21.0.beta/general/nki/api/generated/nki.ker nels.flash_fwd.html [3] B. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer, M. E. Fiuczynski, D. Becker, C. Chambers, and S. Eggers. 1995. Extensibility safety and performance in the SPIN operating system. In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (Copper Mountain, Colorado, USA) (SOSP 95). Association for Computing Machinery, New York, NY, USA, 267283. https://doi.org/10.1145/22 4056.224077 [4] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs. http://gith ub.com/jax-ml/jax [5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/2005.14165 [6] Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John Tran, Bryan Catanzaro, and Evan Shelhamer. 2014. cuDNN: Efficient Primitives for Deep Learning. arXiv:1410.0759 [cs.NE] https://arxiv.org/abs/1410.0759 [7] Cursor. 2025. The AI Code Editor. https://www.cursor.com/ [8] Tri Dao. 2023. FlashAttention-2: Faster Attention with Better PararXiv:2307.08691 [cs.LG] https: allelism and Work Partitioning. //arxiv.org/abs/2307.08691 [9] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135 [cs.LG] https://arxiv.org/abs/2205.1 4135 [10] DeepSpeed. 2025. Mixture of Experts - DeepSpeed. https://www.de epspeed.ai/tutorials/mixture-of-experts/#moe-layer-api [11] DeepSpeed. 2025. Model Implementations. https://github.com/deeps peedai/DeepSpeed/tree/a21e5b9db68adf25e9fc797d0e67fdb5879f60 69/deepspeed/inference/v2/model_implementations [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics. https://api.semanticscholar.org/Corp usID:52967399 [13] Xianzhi Du, Tom Gunter, Xiang Kong, Mark Lee, Zirui Wang, Revisiting Aonan Zhang, Nan Du, and Ruoming Pang. 2024. MoE and Dense Speed-Accuracy Comparisons for LLM Training. arXiv:2405.15052 [cs.LG] https://arxiv.org/abs/2405.15052 [14] Google. 2025. Gemini: Our most intelligent AI models, built for the agentic era. https://deepmind.google/technologies/gemini/ [15] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [16] Haiku. 2025. Haiku: Sonnet for JAX. https://github.com/googledeepmind/dm-haiku [17] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. 2023. Flax: neural network library and ecosystem for JAX. http: 13 //github.com/google/flax [18] Dan Holtmann-Rice, Sergio Guadarrama, and Nathan Silberman. 2020. Gin-Config: lightweight configuration library for Python. https: //github.com/google/gin-config [19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems 32 (2019). [20] Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, and Xin Liu. 2024. MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24). USENIX Association, Santa Clara, CA, 745760. https: //www.usenix.org/conference/nsdi24/presentation/jiang-ziheng [21] Eddie Kohler, Robert Morris, Benjie Chen, John Jannotti, and M. Frans Kaashoek. 2000. The click modular router. ACM Trans. Comput. Syst. 18, 3 (Aug. 2000), 263297. https://doi.org/10.1145/354871.354874 [22] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz, Germany) (SOSP 23). Association for Computing Machinery, New York, NY, USA, 611626. https://doi.org/10.1145/3600006.3613165 [23] Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. 2023. Sequence Parallelism: Long Sequence Training from System Perspective.. In ACL. Association for Computational Linguistics, 2391 2404. [24] Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, Sanket Purandare, Gokul Nadathur, and Stratos Idreos. 2024. TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training. arXiv:2410.06511 [cs.CL] https://arxiv.org/abs/24 10.06511 [25] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [26] MaxText team. 2025. MaxText. https://github.com/AI-Hypercompu ter/maxtext [27] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021. Efficient large-scale language model training on GPU clusters using megatron-LM. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (St. Louis, Missouri) (SC 21). Association for Computing Machinery, New York, NY, USA, Article 58, 15 pages. https://doi.org/10.1145/3458817.3476209 [28] OpenAI. 2022. Introducing ChatGPT. https://openai.com/index/cha tgpt/ [29] Optax Authors. 2025. . https://github.com/google-deepmind/optax [30] Orbax Authors. 2022. . https://github.com/google/orbax [31] pallas. 2025. Pallas: JAX kernel language. https://docs.jax.dev/en/la test/pallas/index.html [32] PAX team. 2022. Framework to configure and run machine learning experiments on top of Jax. http://github.com/google/paxml [33] PyTorch. 2025. Fully Sharded Data Parallel in PyTorch XLA. https: //pytorch.org/xla/master/perf/fsdp.html 14 [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res. 21, 1, Article 140 (Jan. 2020), 67 pages. [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Atlanta, Georgia) (SC 20). IEEE Press, Article 20, 16 pages. [36] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2022. Scaling Up Models and Data with t5x and seqio. arXiv:2203.17189 [cs.LG] https://arxiv.org/abs/2203.17189 [37] Noam Shazeer. 2020. GLU Variants Improve Transformer. arXiv:2002.05202 [cs.LG] https://arxiv.org/abs/2002.05202 [38] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.. In ICLR (Poster). http://dblp.uni-trier.de/db/conf/iclr/iclr2017.html# ShazeerMMDLHD17 [39] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomput. 568, (Feb. 2024), 12 pages. https: //doi.org/10.1016/j.neucom.2023. [40] Chunqiang Tang, Thawan Kooburat, Pradeep Venkatachalam, Akshay Chander, Zhe Wen, Aravind Narayanan, Patrick Dowell, and Robert Karl. 2015. Holistic configuration management at Facebook. In Proceedings of the 25th Symposium on Operating Systems Principles (Monterey, California) (SOSP 15). Association for Computing Machinery, New York, NY, USA, 328343. https://doi.org/10.1145/2815400.2815401 [41] XLA. 2025. Operation Semantics - OpenXLA Project. https://openxla. org/xla/operation_semantics [42] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen. 2021. GSPMD: General and Scalable Parallelization for ML Computation Graphs. arXiv:2105.04663 [cs.DC] https://arxiv.org/abs/2105.04663 [43] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: Distributed Serving System for Transformer-Based Generative Models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). USENIX Association, Carlsbad, CA, 521538. https://www.usenix.org/confere nce/osdi22/presentation/yu [44] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. 2023. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. Proc. VLDB Endow. 16, 12 (Aug. 2023), 38483860. https://doi.org/10.14778/36115 40.3611569 [45] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, and Ion Stoica. 2022. Alpa: Automating Interand Intra-Operator Parallelism for Distributed Deep Learning. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). USENIX Association, Carlsbad, CA, 559578. [46] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). USENIX Association, Santa Clara, CA, 193 210. https://www.usenix.org/conference/osdi24/presentation/zhongyinmin [47] Zoom. 2025. Work happy with Zoom AI Companion. https://www.zo om.com/en/products/ai-assistant/ Mesh Rules In AXLearn, users can specify per-target platform configuration changes using mesh rules. These rules are mappings from instance type regular expressions to config modifiers discussed in 4. In the example below, when launching training on TPU v5e, we configure training to use FSDP withinslices and data-parallel across slices, offloading activations from dot products to host memory and enabling INT8 training. On the other hand, when launching training on H100s, we instead switch to 8-way tensor parallelism within node and FSDP across nodes, saving query, key, value, and output (QKVO) projections to HBM, as well as enabling FP8 training with delayed scaling. These configs are all that are necessary to apply per-target optimizations, allowing users to scale training on heterogeneous platforms with ease. [(\"tpu-v5e-256-*\", [MeshShapeModifier.default_config().set( mesh_shape=mesh(data=-1, fsdp=256)), RematSpecModifier.default_config().set( remat_policies={ \"model.decoder.transformer.layer\": RematSpec(policy=offload_dots)}), INT8ConfigModifier.default_config()]), (\"gpu-H100-*\", [MeshShapeModifier.default_config().set( mesh_shape=mesh(fsdp=-1, model=8)), RematSpecModifier.default_config().set( remat_policies={ \"model.decoder.transformer.layer\": RematSpec(policy=save_qkvoflash)}), FP8ConfigModifier.default_config().set( fp8_amax_history_length=128)])] LoC Analysis We provide additional details and rationale of how we derived the LoC estimates in Table 2. Megatron-LM Megatron-LMs Transformer implementation composes TransformerBlockSubmodules. However, the RoPE-specific parameters are flattened in the init signature of each model implementation, e.g. rotary_percent, rotary_base, rotary_scaling, and position_embedding _type in GPTModel. Additionally, these parameters are propagated to submodules like TransformerBlock, Transformer Layer, or Attention. This means to integrate RoPE variant, one potentially incurs LoC changes to each module in each model implementation, as at minimum the RoPE parameters must be propagated down an arbitrary number of modules to the Attention layer. Additionally, each models init implementation includes branching logic to instantiate the RoPE embedding layer variant depending on the desired position_embedding_type. For example, RotaryEmbedding should be instantiated if the 15 embedding type is rope, while MultimodalRotaryEmbedd ing should be instantiated when embedding type is mrope. Therefore, if we additionally consider RoPE variants, the LoC-complexity scales quadratically to 𝑂 (𝑁 𝑀), as in the worst case each module must account for each variant in its init signature if it receives an embedding type. To integrate MoE, Megatron-LM is able to leverage composition via TransformerBlockSubmodules to specify MoE layer in place of the standard MLP. However, once again the encapsulation is not applied strictly. Each MLP layer implementation introduces an is_expert field in its init signature, which is propagated to the linear submodules. Any module that uses linear submodule therefore needs to incur at least one LoC change, which scales with 𝑂 (𝑁 ). Indeed, MegatronLM itself has number of modules that are impacted, including ColumnParallelLinear, RowParallelLinear, Att ention, CrossAttention, and more. If we assume the production setting of 20 model variants, with each conservatively incurring at least 20 LoC changes to integrate RoPE4, we incur at least 400 LoC. For MoE, if we assume 10 MLP variants5, each incurring at least 1 LoC change to integrate support for is_expert, we incur at minimum 10 LoC change. In addition, if we assume at least 10 modules in the system using linear submodule (corresponding to each linear variant), each requiring 1 LoC change to build_module, we incur additional 10 LoC. DeepSpeed DeepSpeed applies the config flattening\" methodology discussed in 4. RoPE configs like rotary_dim and rope_theta are grouped under monolithic config class, like DeepSpeedInferenceConfig. Every model implementation reads the config and overrides the method positional _embedding_type to indicate whether RoPE should be enabled for the model by returning specific value of the embedding type. With this design, we can already observe that LoC-Complexity(RoPE) must be at least 𝑂 (𝑁 ), as each model implementation incurs LoC changes to override the necessary methods to enable RoPE6. In addition, the base model implementation propagates this embedding type (and additional RoPE configs) to child layers like the self attention layer. As consequence, every attention layer implementation, such as DSDenseBlocked Attention, must first update its init signature to handle the input embedding type. It must also update its forward implementation to apply RoPE based on the embedding type. In similar way as Megatron-LM, LoC-complexity(RoPE) scales quadratically with 𝑂 (𝑁 𝑀), because we must propagate embedding type and RoPE parameters down an arbitrary number of modules to the attention layer, and because each attention module in the worst case receives and must be prepared to handle all possible values of embedding type. 4Based on the changes required in GPTModel. 5Based on Megatron-LMs own Linear variants. 6E.g., positional_embedding_type and positional_embedding_config If we assume conservatively 20 models with at least 6 LoC per model7, we incur 120 LoC; in addition, with 10 attention variants each requiring approximately 20 LoC8 we incur another 200 LoC. If the MoE case, DeepSpeed requires subclassing each model from custom subclass DSMoETransformerModelBase, which requires in some cases re-implementation of most methods. For 20 model variants, each can incur 100s of LoC in the case of DeepSpeeds QwenV2MoE, more than 200 LoCwhich conservatively incurs 4,000 LoC changes. TorchTitan TorchTitan adopts similar design as DeepSpeed by using monolithic BaseModelArgs subclass that flattens all configs for each model. In the case of RoPE, each such config subclass introduces RoPE specific configurations like rope_theta and rope_scaling. Similar to prior analysis, such design already incurs at least 𝑂 (𝑁 ) LoCComplexity as each model adopting RoPE must accordingly modify its config class signature. In addition, each model has its own Attention implementation. For example, the deepseek_v3 Attention implementation conditions on the value of RoPE configs (e.g. rope_scaling) to decide which child RoPE layer to instantiate. In the worst case, each RoPE variant may incur the same conditional logic for each model implementation that intends to support it. This causes the LoC-Complexity to degrade to 𝑂 (𝑁 𝑀) if we consider RoPE variants. With 20 model variants, each incurs 2 LoC on average to update its corresponding ModelArgs class to incorporate RoPE configs, as well as 10 LoC for each attention implementation. In total, this incurs 240 LoC, although we note that this varies greatly between TorchTitan model implementations9. In the MoE case, TorchTitan conditionally instantiates either MoE child layer or standard FeedForward layer, based on the config field moe_enabled. Because TorchTitan implements customized layers for each model (e.g., Transfor merBlock for Llama, DecoderLayer for DeepSeekV3), this results in 𝑂 (𝑁 𝑀) complexity as we must modify the corresponding transformer layer for each model for each MoE variant. Like with RoPE, we incur at least 10 LoC for each ModelArgs (due to more complex configuration of MoE), as well as 10 LoC for each attention layer implementation10. Over 20 model variants, this results in 400 LoC changes. Flax Flax implements the Gemma model via mostly selfcontained Transformer modules which are not shared with other implementations. To integrate RoPE, the Gemma Trans formerConfig is modified to add the RoPE parameters; the Gemma Transformer module is modified to propagate the 7To override two properties. 8Based on changes in DSDenseBlockedAttention. 9Based on DeepSeekV3 and Llama4 implementations. 10Based on TransformerBlock and DecoderLayer. 16 as subtype, its RoPE logic must also be handled in its parent Attention. For each RoPE variant and model, we can expect at least 10 LoC14, which results in 200 LoC across 20 variants. In similar way, MaxTexts MoE implementation details are flattened into each models decoder. Each model implements its own DecoderLayer, while monolithic Decoder selects decoder layer implementation based on the name of the model in the config. With 10 LoC per decoder15, this results in 200 LoC for 20 model variants. MaxTexts trainer also includes MoE-specific logic: each loss function uses MoE configs to apply auxiliary losses. With 5 LoC for each loss function16, across 20 model variants this results in additional 100 LoC. RoPE parameters from the config to its Block submodules; each Block submodule is modified so that its init signature can accommodate additional RoPE parameters; and each Attention module must take these RoPE parameters to finally implement the RoPE logic. It is easy to see that the LoC-Complexity(RoPE) in this case is 𝑂 (𝑁 𝑀), as for each RoPE variant, one would need to correspondingly update RoPE parameters in the top-level TransformerConfig, and then propagate those configs down an arbitrary number of modules to the Attention layer implementation. For single variant, the Gemma model incurs at least 30 LoC across TransformerConfig, Transformer, Block, and Attention parameterization modifications, not including the actual RoPE implementations. For 20 model variants, this results in at least 600 LoC changes. Praxis Praxis is the layer library of Pax. It internally uses fiddle, config system similar to the one in AXLearn, which allows it to express certain re-parameterizations using composition. For example, Praxis uses template approach to configure the MoE layer in each transformer layer stack. This template is configured along with several MoE configs (like num_experts) in the StackedTransformerLayer definition, which in theory allows the integration of MoE to scale with 𝑂 (𝑀), or the number of MoE variants. Because some MoE configs are still flattened, although not to the extent of causing quadratic interactions, each MoE variant incurs 5 LoC change11. However, using composable config system doesnt guarantee strict encapsulation. In particular, Praxis flattens number of RoPE-specific configs (e.g., use_rotary_position _emb) into each attention layer implementation, which means that it incurs at least 𝑂 (𝑁 ) LoC-Complexity for single RoPE variant. As it turns out, Praxis attention layers compose the actual RoPE implementation itself via defining RoPE layer template rotary_position_emb_tpl. However, because each RoPE variant may have different configuration interfaces, the flattening ultimately means that LoC-Complexity under RoPE variants is 𝑂 (𝑁 𝑀), as each RoPE variant may nevertheless require modifications to each attention layer. For 10 attention variants12, each incurring approximately 30 LoC13, this incurs at least 300 LoC. MaxText MaxText builds on top of Flax modules, and follows similar layer design and analysis. MaxTexts Attention conditions on configs (like attention_type and rope_type) to choose the RoPE module. This has undesirable interactions between the attention and RoPE variants; in the worst case, Attention must account for the cross-product of attention and RoPE variants. We can already observe this with MaxTexts MLA implementation; while MLA implemented 11Based on the number of flattened configs. 12Based on number of attention layers in Praxis 13Based MultiQueryDotProductAttention on DotProductAttention 14Based on Attention implementation for llama3.1\", yarn\", etc. 15Based on changes in Decoder. 16Based on loss_fn and dpo_loss_fn. and"
        }
    ],
    "affiliations": [
        "Apple",
        "Duke University"
    ]
}