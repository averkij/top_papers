{
    "paper_title": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes",
    "authors": [
        "Tianyang Xu",
        "Haojie Zheng",
        "Chengze Li",
        "Haoxiang Chen",
        "Yixin Liu",
        "Ruoxi Chen",
        "Lichao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building a knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, a graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures a fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at https://github.com/Terry-Xu-666/NodeRAG."
        },
        {
            "title": "Start",
            "content": "NodeRAG: Structuring Graph-based RAG with Heterogeneous Nodes Tianyang Xu1, Haojie Zheng2, Chengze Li1, Haoxiang Chen1 Yixin Liu3, Ruoxi Chen, Lichao Sun3 1 Columbia University, 2 University of Pennsylvania, 3 Lehigh University tx2240@columbia.edu, haojiez@seas.upenn.edu"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-augmented generation (RAG) empowers large language models to access external and private corpus, enabling factually consistent responses in specific domains. By exploiting the inherent structure of the corpus, graph-based RAG methods further enrich this process by building knowledge graph index and leveraging the structural nature of graphs. However, current graph-based RAG approaches seldom prioritize the design of graph structures. Inadequately designed graph not only impede the seamless integration of diverse graph algorithms but also result in workflow inconsistencies and degraded performance. To further unleash the potential of graph for RAG, we propose NodeRAG, graph-centric framework introducing heterogeneous graph structures that enable the seamless and holistic integration of graph-based methodologies into the RAG workflow. By aligning closely with the capabilities of LLMs, this framework ensures fully cohesive and efficient end-to-end process. Through extensive experiments, we demonstrate that NodeRAG exhibits performance advantages over previous methods, including GraphRAG and LightRAG, not only in indexing time, query time, and storage efficiency but also in delivering superior question-answering performance on multi-hop benchmarks and open-ended head-to-head evaluations with minimal retrieval tokens. Our GitHub repository could be seen at this link. 5 2 0 2 5 1 ] . [ 1 4 4 5 1 1 . 4 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Retrieval-augmented generation (RAG) has emerged as solution to the challenges posed by the rapid evolution of real-world knowledge domains (Fan et al., 2024), coupling large language models (LLMs) with an external retrieval mechanism to ensure the generation of factually consistent and contextually relevant information (Tonmoy et al., 2024; Shrestha et al., 2024; Liu 1 et al., 2024). Despite recent progress, current RAG methods face notable shortcomings in handling multi-hop reasoning (Luo et al., 2023; Wang et al., 2024b) and summary-level queries (Han et al., 2024a; Wen et al., 2023) due to their insufficient utilization of data structures and lack of high-level understanding of the text corpus. Graph-based RAG methods (Tian et al., 2024; Park et al., 2023) have been proposed to enhance retrieval and question-answering performance, specifically addressing the two main challenges faced by traditional RAG approaches. Leveraging LLMs to decompose raw data into graph structures (Jiménez Gutiérrez et al., 2024; He et al., 2024) for utilizing structural information, as well as employing LLMs for summary-based enhancements (Edge et al., 2024; Guo et al., 2024) to derive insights beyond the original text, has gradually become mainstream approaches. However, previous Graph-based RAG works (Trajanoska et al., 2023; Jiménez Gutiérrez et al., 2024) have rarely considered the critical role of graph structures, i.e., what forms of graph better support RAG. Among existing approaches, knowledge graphs (Sanmartin, 2024; Wang et al., 2024b) extract triples, with the graph containing only structural information, yet retrieval context remains confined to text chunks, which often lack semantic coherence and include unrelated information. While current methods attempt to incorporate more information into the graph and extract deeper insights, they suffer from inefficiencies and inconsistencies due to inadequately designed structures. For instance, as illustrated in Figure 1, GraphRAG (Edge et al., 2024), adopt tightly coupled entity-event homogeneous structure, hindering the integration of original context and summary information into the graph. This results in inconsistencies in retrieval methods (separating local and global retrieval) and leads to coarse-grained retrieval, where retrieving an entity indiscriminately includes all Figure 1: Comparsions between NodeRAG and other RAG systems. NaïveRAG retrieving fragmented text chunks, leads to redundant information. HippoRAG introduces knowledge graphs but lacks high-level summarization. GraphRAG retrieves community summaries but may still produce coarse-grained information. LightRAG incorporates one-hop neighbors but retrieves redundant nodes. In contrast, NodeRAG utilizes multiple node types, including high-level elements, semantic units, and relationships, enabling more precise, hierarchical retrieval while reducing irrelevant information. associated events, adding irrelevant information. To address these limitations, we propose NodeRAG, which is built around well-designed Heterogeneous Graph, comprehensively considering the entire process of graph indexing and searching, enabling fine-grained retrieval. The heterograph adheres to the principle of unfolding and flattening, decomposing different types of information to construct heterogeneous fully nodalized graph where nodes serve distinct functions and roles. This means that entities, relationships, original text chunks, independently decomposed events from text chunks, and summaries extracted by LLMs are all represented as nodes within the graph. The heterograph not only encapsulates information from the original corpus but also extends beyond it, incorporating enriched insights such as key node attributes, and high-level discoveries. Each node in heterograph consists of unstructured content, while preserving structural connections between nodes, striking balance between structural integrity and flexibility. As illustrated in Figure 1, for multi-hop question, NodeRAG can retrieve semantically coherent, independent event (semantic unit) and high-level discoveries (high-level elements) related to key entities such as Harry, Neville, and the three-headed dog using graph algorithms, providing explainable and fine-grained retrievals as well as high-level understanding. The key contributions of our work can be summarized in three main aspects. (1) Better Graph Structure for RAG The graph structure serves as the foundation for graph-based RAG where significance has been overlooked. Our work emphasizes its importance and introduces graph structure that better supports RAG. (2) Fine-grained and Explainable Retrieval The heterograph enables fine-grained and functionally distinct nodes, allowing graph algorithms to effectively and reasonably identify key multi-hop nodes. This leads to more relevant retrieval with minimal retrieval context, enhancing both precision and interoperability. (3) Unified-Level Information Retrieval Decomposed information from documents and extracted insights from LLMs are not treated as separate layer but are instead unified as nodes within the heterograph. This integration allows for cohesive framework capable of handling information needs across different levels. In addition, extensive experiments demonstrate that NodeRAG not only outperforms previous graph-based RAG methods on multi-hop tasks but also exhibits superior performance in open-ended head-to-head evaluations. With minimal retrieval tokens, it achieves highly precise retrieval while also demonstrating system-level efficiency advantages, including improvements in indexing time, query time, and storage efficiency, as shown in appendix A."
        },
        {
            "title": "2 NodeRAG",
            "content": "The NodeRAG pipeline is built on foundational graph structure defined as the heterograph, which will be introduced in Section 2.1. The workflow is divided into two primary stages, graph indexing and graph searching. Graph indexing comprises three components, graph decomposition, graph augmentation, and graph enrichment, which are discussed in Sections 2.2, 2.3, and 2.4, respectively. This stage integrates various types of nodes and edges into the heterograph by leveraging LLMs and graph algorithms. The subsequent stage, graph searching, is detailed in Section 2.5 and combines the structural advantages of the heterograph with graph algorithms to efficiently retrieve relevant information. Moreover, the fundamental concepts and implementation details of the graph algorithms used in the pipeline are provided in Appendix C, while the prompting instructions for LLMs can be found in Appendix for reference."
        },
        {
            "title": "2.1 Heterograph",
            "content": "The concept of the heterograph embodies the principle of comprehensive unfolding and flattening of information into fully nodalized structure. This structure achieves its granularity through the integration of seven hetero node types: entity (N ), relationship (R), semantic unit (S), attribute (A), high-level elements (H), high-level overview (O), and text (T ). Each node type is tailored to represent specific roles and characteristics of the information, enabling fine-grained and functional decomposition of data. Mathematically, the heterograph is defined as: = (V, E, Ψ), where is the heterograph, represents the set of nodes, is the set of edges, and Ψ : Types is mapping function that assigns each node to specific type. The set of node types, corresponds to the seven predefined types: Types = {N, R, S, A, H, O, }. For any node v, Ψ(v) defines its type, with each node type performing distinct and well-defined function, as detailed in subsequent sections and appendix C. For each E, the default weight of is set to 1, representing basic connection between two nodes. Furthermore, we define Vtypes as the subset of nodes corresponding to subset set types Types, formally expressed as: Vtypes = {v Ψ(v) types}. For instance, V{N,R,S} represents the subset containing only entity, relationship, and semantic unit nodes. V{T,S,A,H} contain rich informational content and are classified as retrievable nodes. In contrast, V{N,O}, which represent names or titles, act solely as critical linkage and entry points within the graph but are not directly retrievable. For example, VH provides detailed context for high-level concepts, while VO represents the corresponding title and keywords but does not contribute directly to the retrieved content. Additionally, VR, is nodalized edge, acting as connector nodes and secondary retrievable nodes, contributing to the retrieval context but not serving as graph entry points."
        },
        {
            "title": "2.2 Graph Decomposition\nFirst, we define a null heterograph G0. The initial\nstep involves employing a LLM to decompose text\nchunks from the source corpus into three primary\nnode types: semantic units (S), entities (N ), and\nrelationships (R). These nodes are then intercon-\nnected to construct the initial heterograph. This\nprocess can be formalized as:\nG1 = G0∪{v ∈ V, ed, er ∈ E | Ψ(v) ∈ {S, N, R}},\nWhere e represents the connecting edges between\nsemantic units and entity nodes, as well as between\nrelationship nodes and their corresponding source\nand target entities. For instance, if “Hinton was\nawarded the Nobel Prize for inventing backprop-\nagation” serves as v ∈ VS derived from a text\nchunk, then Hinton, Nobel Prize, and backpropaga-\ntion represent v ∈ VN nodes, with ed denoting their\nconnections to v ∈ VS. An example of v ∈ VR\nwould be “Hinton received Nobel Prize”, where\ner represents the edge connecting the source node\nHinton to the target node Nobel Prize.",
            "content": "Semantic unit (S) The semantic unit acts as local summary, representing an independent event unit in paraphrased form. It serves as the core node for graph augmentation and improving search quality. Since the division of text chunks is not based on semantics, unrelated or unassociated content may coexist within single chunk. This context noise increases entropy, leading to degraded quality when using text chunks for graph augmentation or searching due to their coarse granularity and irrelevant information. Entity (N ) and Relationship (R) Entities (N ) are nodes that exclusively represent entity names, 3 Figure 2: Main indexing workflow of NodeRAG. It illustrates the step-by-step construction of the heterograph, including the process of graph decomposition, graph augmentation, and graph enrichment while relationships (R) are also transformed into nodes that connect source and target entities. These entities and relationships are directly connected to semantic units (S), as VS serves as the smallest, contextually meaningful representation of events within text chunks. This connection ensures that entities and relationships remain decoupled from specific events, allowing them to function independently while still being anchored to relevant contexts. Such design prevents redundant information and enables flexible graph structure."
        },
        {
            "title": "2.3 Graph Augmentation\nThe heterograph G1 provides a foundational low-\nlevel structure. However, it lacks high-level orga-\nnization and contextual insights. To further aug-\nment the graph, we implement two primary meth-\nods: node importance-based augmentation and\ncommunity detection-based aggregation, which re-\nspectively capture the perspectives of individual\nnode significance and structural cohesion within\nthe graph.",
            "content": "Node Importance Based Augmentation We prioritize the selection of structurally significant and functionally pivotal entities. These key entities, along with their associated semantic units and relationships, are processed through LLMs to generate attribute summaries. This approach mirrors human reading behavior, where all relevant content associated with critical entity is reviewed before synthesizing its attributes. The summarization specifically focuses on the important entities identified within the corpus, rather than processing all entities, ensuring both precision and efficiency. The selection of important entities, , is guided by two complementary metrics: K-core decomposition (Seidman, 1983; Kong et al., 2019) and betweenness centrality (Brandes, 2001). K-core identifies nodes in densely connected subgraphs that are critical to graph cohesion, while betweenness centrality highlights nodes that act as bridges for information flow. These metrics are denoted as K(G1) and B(G1), where K() and B() represent the selected entity nodes from the graph. The final set of important entities is defined as: = K(G1) B(G1). Entity attributes are constructed directly from relationships and semantic units, bypassing raw texts to avoid redundancy. Each generated attribute node is added to the graph and connected to its corresponding entity node via the edge ea. This update to the graph is represented as: G2 = G1 {v V, ea Ψ(v) {A}}. Community Detection Based Aggregation We first apply the Leiden algorithm (Traag et al., 2019) to G2 to perform community detection, segment4 ing the graph into closely related substructures, denoted as communities. Each node G2 is assigned to specific community Cn, where Cn represents the n-th community identified by the algorithm. Within each community Cn, an LLM is utilized to analyze the aggregated content, extracting high-level elements (H) that encapsulate the core information of the community, such as summaries, sentiment analysis, and other significant insights. For each generated high-level element node VH , it is essential to establish meaningful connections eh with relevant nodes from G2 to preserve the graphs structural coherence. To accomplish this, we propose semantic matching within community algorithm. This algorithm identifies the most semantically related nodes within the same community Cn for each high-level element node. To achieve this, K-means clustering (MacQueen et al., 1967) is applied to the embeddings of V{S,A,H}. The number of clusters is V{S,A,H}, where V{S,A,H} repdetermined as resents the total number of nodes labeled S, A, or H. An edge eh(v, v) exists between V{S,A,H} and VH if both and belong to the same semantic cluster Sk and the same community Cn. Additionally, the LLM can extract keyword title for each high-level element, referred to the highlevel overview (O), which is used for dual search as elaborated in Section 2.5. Each VH and VO will have corresponding connection eo. The updated graph G3 incorporates high-level elements (H) and their corresponding connections (eh, eo). It is defined as: G3 = G2 {v V, eh, eo ψ(v) = {H, O}}. (cid:113)"
        },
        {
            "title": "2.4 Graph Enrichment",
            "content": "In the previous process of generating the heterograph, G3 already contains wealth of information. However, certain unique and additional details can still further enrich the heterograph, enabling it to not only preserve the entirety of the original texts information but also gain enhanced features and insights that go far beyond the source material. Text Insertion As mentioned earlier, text chunks are not directly incorporated into during graph augmentation due to their semantic incoherent nature. However, original text chunks hold significant value as they contain detailed information, which is often lost during the LLM transformation process. Therefore, it is essential to ensure that the original information remains searchable within graph. G4 = G3 {v, es Ψ(v) = }, where es denotes the edges connecting text chunks to their relevant semantic units. Embedding As mentioned in Section 2.1, V{T,A,S,H} contains rich informational context where vector similarity is highly effective. Conversely, V{N,O}, which includes names and titles represented as words or phrase, is less suitable for vector similarity methods. To address this limitation, we developed dual search mechanism. During the embedding process, we selectively embed only subset of the graphs data, specifically V{T,A,S,H}. This targeted embedding step is crucial for reducing storage overhead while preserving efficient search capabilities. HNSW Semantic Edges The Hierarchical Navigable Small World (HNSW) algorithm (Malkov and Yashunin, 2018) is an approximate nearest neighbor search method that organizes data into multi-layer graph structure to efficiently retrieve semantically similar nodes. It represents the data as layered graph = {L0, L1, . . . , Lm}, where L0 is the base layer containing the densest semantic similarity connections, and higher layers (Li, > 0) are sparsely connected to facilitate coarse-grained navigation. is built iteratively. When new node is added, it is inserted into random level and all layers below it, connecting to similar neighbors based on cosine similarity. Higher layers remain sparse with long-range connections, while L0 focuses on dense local relationships. The search starts at the sparsely connected top layer, and progressively descends to L0. In our work, the base layer L0 of the HNSW graph, which encodes semantic relations between nodes, is integrated with the heterograph G. The updated graph, denoted as G5, is expressed as: G5 = G4 L0. The inclusion of L0 enhances the heterographs search capabilities by incorporating semantic dense proximity edges, augmenting its structural information in the graph. When an edge already exists in G4, adding the corresponding edge from L0 increases its weight by 1, reinforcing frequently occurring connections."
        },
        {
            "title": "2.5 Graph Searching",
            "content": "We first apply dual search mechanism to identify entry points within the heterograph. Subsequently, 5 Figure 3: This figure focuses on the querying process, where entry points are extracted from the original query, followed by searching for related nodes that need to be retrieved in the heterograph. shallow Personalized PageRank (PPR) algorithm is employed to extract cross nodes. The combination of entry point nodes and cross nodes is then filtered to produce the final retrieval. Dual Search Dual search combines exact matching on title nodes and vector similarity search on rich information nodes to identify entry points in the heterograph G. Given query, the LLM extracts entities and embeds the query into vector (q). The entry points are defined as: Ventry = {v Φ(v, q, q)}, where the condition function Φ(v, q, q) is defined as: Φ(v, q, q) = (cid:40) V{N,O} M(N q, v), V{S,A,H} R(q, v, k). Here, the exact matching function M(v, v) returns true if node matches one of the extracted entities by word level string matching. Additionally, the similarity-ranking function R(q, v, k) returns true if node ranks among the top-k most similar to based on the HNSW algorithm. By leveraging the non-retrievability of V{N,O}, they serve exclusively as entry points to the graph without contributing directly to the retrievable content. Only nodes identified through the shallow PPR as closely related to all entry points are included in the retrieval results as cross nodes. This ensures that the effects of noisy or ambiguous queries, which may lead to errors in exact matching, do not directly impact the retrieval process. Any indirect effects are further minimized by the graph algorithm, enhancing the robustness of the retrieval system. Shallow PPR Personalized PageRank (PPR) identifies relevant nodes in the heterograph by simulating biased random walk starting from set of entry points. In our approach, we use shallow PPR, limiting the number of iterations to ensure that relevance remains localized to the neighborhoods of the entry points. This early stop strategy prevents excessive diffusion to distant or irrelevant parts of the graph, focusing instead on multi-hop nodes near the entry points. Let be the normalized adjacency matrix of G, where Pij represents the transition probability from node to node j. The PPR process starts with personalization vector RV, where pi = 1/Ventry if vi Ventry, and pi = 0 otherwise. The PPR score vector π(t) after iterations is computed iteratively as: π(t) = αp + (1 α)P π(t1), π(0) = p, where α (0, 1) is the teleport probability that balances restarting at entry points and propagating through the graph. After iterations, the top-k nodes with the highest PPR scores for each type are selected as cross nodes, denoted as Vcross. In our default setting, we use α = 0.5 and = 2 to achieve balance between exploration and convergence. Filter Retrieval Nodes Finally, the retrieval nodes are filtered from the union of entry nodes and cross nodes to include only retrievable nodes of V{T,A,S,H,R}. V{N,O}, which contain only keywords without informational content, are excluded from the retrieval context. The final set of retrieval nodes is therefore defined as: Vretrieval = { Ventry Vcross ψ(v) {T, S, A, H, R}}"
        },
        {
            "title": "3 Evaluation",
            "content": "We evaluate NodeRAGs performance across three different multihop benchmarks, HotpotQA (Yang et al., 2018), MuSiQue (Trivedi et al., 2022b), MultiHop-RAG (Tang and Yang, 2024), and an open-ended head to head evaluation RAG-QA Arena (Han et al., 2024b) across six domains. And we compare our method against several strong and widely used RAG methods as baseline models, including NaiveRAG (Lewis et al., 2020), HyDE Part I: General comparisons"
        },
        {
            "title": "MultiHop",
            "content": "Arena-Writing Arena-Tech Arena-Science Arena-Recreation Arena-Lifestyle Arena-FiQA Acc. #Token Acc. #Token Sco. #Token W+T #Token W+T #Token W+T #Token W+T #Token W+T #Token W+T #Token NaiveRAG 87.50% 9.8k 39.43%"
        },
        {
            "title": "HyDE",
            "content": "73.00% 10.0k 33.14% LightRAG 79.00% GraphRAG 89.00% NodeRAG 89.50% 7.1k 6.6k 5.0k 36.00% 41.71% 46.29% 9.6k 9.8k 7.4k 6.6k 5.9k 0.56 0. 0.50 0.53 0.57 8.9k 9.4k 7.9k 7.4k 6.1k 0.663 0.789 0.754 0. 0.794 9.4k 9.6k 6.3k 6.4k 3.3k 0.689 0.863 0.937 0.943 0.949 9.1k 9.3k 6.9k 6.7k 3.8k 0.526 0. 0.840 0.863 0.903 9.0k 9.3k 7.1k 6.7k 4.2k 0.720 0.777 0.800 0. 0.886 9.3k 9.5k 6.2k 6.6k 3.4k 0.817 0.829 0.817 0.863 0.949 9.1k 9.3k 6.8k 6.8k 3.3k 0.926 0. 0.937 0.960 0.977 9.1k 9.3k 7.7k 6.8k 3.4k Part II: Pairwise Comparisons"
        },
        {
            "title": "Domain",
            "content": "M1 vs M2 Win (M1) Tie Win (M2)"
        },
        {
            "title": "Domain",
            "content": "M1 vs M2 Win (M1) Tie Win (M2) Domain M1 vs M2 Win (M1) Tie Win (M2)"
        },
        {
            "title": "Lifestyle",
            "content": "NodeRAG vs GraphRAG NodeRAG vs LightRAG NodeRAG vs NaiveRAG NodeRAG vs HyDE GraphRAG vs LightRAG GraphRAG vs NaiveRAG GraphRAG vs HyDE LightRAG vs NaiveRAG LightRAG vs HyDE NaiveRAG vs HyDE NodeRAG vs GraphRAG NodeRAG vs LightRAG NodeRAG vs NaiveRAG NodeRAG vs HyDE GraphRAG vs LightRAG GraphRAG vs NaiveRAG"
        },
        {
            "title": "HyDE vs NaiveRAG",
            "content": "0.520 0.486 0.749 0.531 0.320 0. 0.491 0.711 0.514 0.611 0.640 0. 0.800 0.526 0.429 0.680 0.354 0. 0.349 0.709 0.126 0.103 0.034 0. 0.303 0.092 0.132 0.106 0.143 0. 0.114 0.131 0.040 0.205 0.120 0. 0.097 0.046 0.120 0.028 0.354 0. 0.217 0.314 0.377 0.154 0.377 0. 0.343 0.326 0.246 0.246 0.160 0. 0.451 0.246 0.549 0.291 0.531 0."
        },
        {
            "title": "Science",
            "content": "NodeRAG vs GraphRAG NodeRAG vs LightRAG NodeRAG vs NaiveRAG NodeRAG vs HyDE GraphRAG vs LightRAG GraphRAG vs NaiveRAG GraphRAG vs HyDE LightRAG vs NaiveRAG LightRAG vs HyDE NaiveRAG vs HyDE NodeRAG vs GraphRAG NodeRAG vs LightRAG NodeRAG vs NaiveRAG NodeRAG vs HyDE GraphRAG vs LightRAG GraphRAG vs NaiveRAG"
        },
        {
            "title": "HyDE vs NaiveRAG",
            "content": "0.531 0.526 0.800 0.440 0.406 0. 0.377 0.691 0.349 0.674 0.497 0. 0.829 0.423 0.361 0.829 0.354 0. 0.308 0.840 0.126 0.143 0.017 0. 0.154 0.080 0.137 0.063 0.171 0. 0.200 0.208 0.085 0.280 0.343 0. 0.172 0.119 0.189 0.074 0.343 0. 0.183 0.371 0.440 0.206 0.486 0. 0.480 0.257 0.303 0.254 0.086 0. 0.296 0.063 0.474 0.053 0.503 0."
        },
        {
            "title": "Tech",
            "content": "NodeRAG vs GraphRAG NodeRAG vs LightRAG NodeRAG vs NaiveRAG NodeRAG vs HyDE GraphRAG vs LightRAG GraphRAG vs NaiveRAG GraphRAG vs HyDE LightRAG vs NaiveRAG LightRAG vs HyDE HyDE vs NaiveRAG NodeRAG vs GraphRAG NodeRAG vs LightRAG NodeRAG vs NaiveRAG NodeRAG vs HyDE GraphRAG vs LightRAG GraphRAG vs NaiveRAG"
        },
        {
            "title": "HyDE vs NaiveRAG",
            "content": "0.691 0.651 0.851 0.349 0.297 0. 0.177 0.731 0.211 0.857 0.543 0. 0.777 0.543 0.400 0.657 0.463 0. 0.463 0.606 0.120 0.115 0.018 0. 0.303 0.092 0.126 0.080 0.178 0. 0.154 0.137 0.046 0.160 0.234 0. 0.143 0.075 0.097 0.051 0.189 0. 0.131 0.423 0.400 0.217 0.697 0. 0.611 0.103 0.303 0.366 0.177 0. 0.366 0.246 0.394 0.234 0.440 0. Table 1: Part I: General Comparisons evaluates NaiveRAG, HyDE, LightRAG, GraphRAG, and NodeRAG on HotpotQA and MuSiQue (accuracy and average tokens) and in the Arena using Win+Tie ratios and average tokens. Part II: Pairwise Comparisons shows the fraction of wins\" (Win(M1)), ties\" (Tie), and losses\" (Win(M2)) when comparing one RAG method against another (e.g., NodeRAG vs. GraphRAG). Bold values highlight the best performance. (Gao et al., 2022a), GraphRAG (Edge et al., 2024), LightRAG (Guo et al., 2024). The details of these datasets and baseline models are introduced in Appendix B."
        },
        {
            "title": "3.1 Metrics",
            "content": "General Comparison In the first part, we evaluate NaiveRAG, HyDE, LightRAG, GraphRAG, and NodeRAG across four benchmark datasets. For HotpotQA and MuSiQue benchmarks, we assess accuracy (Acc) to measure effectiveness and the average number of retrieved tokens (#Token) to evaluate efficiency. For the MultiHop-RAG benchmark, we adopt its original evaluation metric, Score (Sco), while still using #Token to gauge retrieval efficiency. Lastly, for the RAG-QA Arena benchmark, we continue to track #Token for efficiency and employ win and tie ratio (W+T) against gold responses as measure of performance across different methods. Pairwise Comparsion In this part, the evaluation focuses exclusively on the RAG-QA Arena benchmark, covering six domains: FiQA, Recreation, Writing, Lifestyle, Science, and Technology. We conduct comprehensive pairwise comparisons among all method combinations and calculate the corresponding win and tie rates for each matchup, thereby identifying the better RAG system. 3."
        },
        {
            "title": "Implementation details",
            "content": "By default, all these RAG methods are implemented with GPT 4o-mini, and the temperature is set to 0 across the entire evaluation. Meanwhile, we identify potential unfairness in the current evaluation setup, evident in several key areas. Notably, the baselines vary in their choice of prompts used to synthesis the final response based on retrieved information. Therefore, we standardized response prompts for every method. Our initiative to standardize these settings also benefits other methods like GraphRAG, improving their performance compared to their default setting, underscoring the broader value of establishing fair and consistent evaluation standards."
        },
        {
            "title": "3.3 Results",
            "content": "General Comparison As shown in Part of Table 1, NodeRAG consistently outperforms competing methods on HotpotQA, MuSiQue, and MultiHopRAG, demonstrating the highest accuracy while retrieving noticeably fewer tokens. For example, for MuSiQue, NodeRAG attains an accuracy of 46.29%, surpassing GraphRAG (41.71%) and LightRAG (36.00%). In HotpotQA, while NodeRAG achieves slightly higher accuracy (89.50% vs. 89.00% for GraphRAG), it does so with only 5k retrieved tokens, which is 1.6k fewer than GraphRAG. In the RAG-QA Arena benchmark, graph-enhanced RAG systems exhibit clear advantage over traditional approaches. Notably, NodeRAG achieves the highest win and tie ratio in each of the five domains while keeping retrieval costs minimal. For example, it attains ratio of 94.9%, notably surpassing GraphRAGs 86.3% and LightRAGs 81.7% in the Lifestyle domain, and does so with less than half the retrieved tokens compared to the other models. It can also be noticed that graph-enhanced RAG systems generally retrieve fewer tokens than traditional RAG across all benchmarks. These results confirm NodeRAGs remarkable effectiveness and efficiency, demonstrating that our heterograph can significantly boost RAG performance across diverse tasks. Pairwise Comparsion Across all the six domains, NodeRAG consistently achieves higher win ratios against GraphRAG, LightRAG, NaiveRAG, and HyDE, demonstrating notable dominance, for instance, in the Lifestyle domain, NodeRAG achieves 0.640 win rate against GraphRAG, 0.623 against LightRAG, 0.800 against NaiveRAG and 0.526 against HyDE. GraphRAG, LightRAG, NaiveRAG, and HyDE show scattered successes, such as LightRAG edging out NaiveRAG (0.649 vs. 0.246) in Recreation, GraphRAG beats LightRAG (0.361 vs. 0.296) in Science, yet their overall win rates remain lower when compared to NodeRAG. Notably, these trends persist across other domains like Writing, Recreation, Science, and Tech, further underscoring NodeRAGs leading position, followed by LightRAG and GraphRAG, showing the superiority of our method. In general, NodeRAG not only achieves the highest accuracy rate and the lowest retrieval token count in general benchmarks but also outperforms all other baselines in preference evaluation comparisons. This unparalleled performance in both accuracy and computational efficiency makes NodeRAG the optimal choice for wide range of RAG tasks, from research applications to deployments in resource-constrained environments."
        },
        {
            "title": "4 Ablation experiments",
            "content": "Figure 4: Ablation analysis on PPR iterations. We conducted ablation experiments on the MuSiQue dataset, adhering to the same settings and evaluation metrics described earlier. We specifically examined the impact of four key submodules: shallow PPR, cross-node interactions, HNSW semantic edges, and dual search. We first investigated the variation in PPR iterations and examined whether shallow PPR offers advantages. PPR, with few iterations, performs better than deep PPR because it highlights important nodes that are closer to the entry points. Moreover, early stopping reduces unnecessary computational overhead, leading to improved retrieval efficiency. Moreover, we evaluate the performance of applying the top-k vector similarity method to all node data in the graph. Although increasing the retrieval context, its performance remains lower than the basic version. This confirms the necessity of cross-nodes in our method, as they help identify important multi-hop nodes. Second, performing vector similarity solely on node data consistently outperforms the naive RAG approach of similarity on text chunks, demonstrating the advantages brought by graph-based data augmentation. In addition, without integration of accurate search in dual search, accuracy drops to 44.57%, and the token count increases to 9.7k. This is because losing entity and high-level overview nodes as entry points causes nodes with long texts, such as text nodes, to have higher weights after shallow PPR. Since vector similarity entry nodes are more frequently connected to nodes, while accurate entry nodes are more connected to S, A, and 8 nodes, the absence of accurate search disrupts this balance. Finally, we investigate the effect of HNSW. HNSW introduces semantic edges to the heterograph, and removing this integration results in performance degradation. This is because HNSW enhances connectivity between semantically related nodes, enabling more efficient and meaningful retrieval. process extensive external documents holistically (Jiang et al., 2024b). RAG has been applied to various domain-specific knowledge bases, such as BioRAG and MedicalRAG (Wang et al., 2024a; Wu et al., 2024; Jiang et al., 2024a). RAG also struggles with corpus-wide understanding tasks, like query-focused abstractive summarization, which require synthesizing knowledge across large datasets. Method NodeRAG (Ours) Accuracy 46.29% w/o HNSW w/o Dual Search w/o Cross Node Top-k = 10 Top-k = 20 Top-k = 30 41.71% 44.57% 41.71% 43.43% 42.29% Time (s) Tokens (k) 4.05 4.92 4.72 4.15 4.70 4.80 5. 6.78 9.70 4.27 7.89 11.62 Table 2: Ablation study of NodeRAG components."
        },
        {
            "title": "5 Related Works",
            "content": "generation RetrievalRetrieval-augmented Augmented Generation (RAG) systems (Gupta et al., 2024) enhance the performance of large language models (LLM) by retrieving relevant information from external documents, grounding responses in domain-specific knowledge. Traditional RAG approaches (Zhao et al., 2024) embed user queries and entries from knowledge base into shared vector space and then compare query vectors to knowledge base vectors to retrieve the top-K most similar contexts based on cosine similarity or similar variants (Fan et al., 2024; Lewis et al., 2020). While effective, naive RAG methods face several limitations, prompting various enhancements in subsequent works. JPR (Min et al., 2021) improves multi-answer retrieval by refining passage selection, while IR-CoT (Trivedi et al., 2022a) integrates chain-of-thought reasoning for multi-hop question answering. Similarly, Tree of Clarifications (Kim et al., 2023) constructs tree-based disambiguation structure to resolve ambiguous queries. HyDE (Gao et al., 2022b) also enhances the performance of dense retrieval by generating hypothetical documents. Other works examine how different document types influence RAG effectiveness and LLM performance (Hsia et al., 2024). Despite these advancements, traditional RAG systems still face significant challenges. The context window limitations(Cheng et al., 2024; Su et al., 2024) of LLMs constrain their ability to 9 RAG over Hierarchical Index To overcome the limitations of traditional RAG, advanced systems integrate hierarchical indexing to incorporate document summaries and enhance retrieval performance. Dense Hierarchical Retrieval (DHR) (Liu et al., 2021) improves passage representations by combining macroscopic document semantics with microscopic passage details. Expanding on this, Hybrid Hierarchical Retrieval (HHR) (Arivazhagan et al., 2023) fuses sparse and dense retrieval techniques for both documentand passage-level retrieval, achieving greater precision. Other methods leverage hierarchical data structures to facilitate complex document summarization. For instance, RAPTOR (Sarthi et al., 2024) employs tree-based structures to integrate knowledge across lengthy documents, synthesizing information at various levels of abstraction. Graph-based RAGs (Trajanoska et al., 2023; Zhang et al., 2024) extend this by constructing knowledge graphs (KGs) (Chen et al., 2020) at the indexing stage and applying graph algorithms during querying (Haveliwala et al., 2003). Notable examples include HippoRAG (Jiménez Gutiérrez et al., 2024) and KAPING (Baek et al., 2023), which refine knowledge organization and retrieval efficiency. Similarly, GraphRAG (Edge et al., 2024) introduces graphbased text indexing using LLMs and generates community-based summaries (Blondel et al., 2008; Traag et al., 2019), inspiring subsequent works such as LightRAG (Guo et al., 2024), which integrates both highand low-level information while optimizing indexing costs. While these approaches effectively leverage hierarchical data structures, they do not fully exploit the synergy between LLMs and graph-based methods. Our proposed framework addresses these gaps by refining graph structures and incorporating advanced graph algorithms, leading to superior retrieval accuracy and efficiency."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this paper, we introduce NodeRAG, novel framework designed to enhance RAG performance by optimizing graph structures in indexing for more effective and fine-grained retrieval. NodeRAG constructs well-defined heterograph with functionally distinct nodes, balancing fine-grained understanding with global perspective of the knowledge corpus. Experimental results demonstrate that NodeRAG outperforms existing methods across multi-hop reasoning benchmarks and open-ended retrieval tasks. As the saying goes,A strong foundation supports higher structure\". In the realm of graph-based RAG, the graph structure serves as this very foundation. The introduction of NodeRAG underscores the critical role of graph structures, encouraging renewed emphasis on their design and optimization."
        },
        {
            "title": "References",
            "content": "Manoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng Huang. 2023. Hybrid hierarchical retrieval for open-domain question answering. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1068010689. Jinheon Baek, Alham Fikri Aji, and Amir Saffari. 2023. Knowledge-augmented language model prompting for zero-shot knowledge graph question answering. arXiv preprint arXiv:2306.04136. Vincent Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. 2008. Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008. Ulrik Brandes. 2001. faster algorithm for betweenness centrality. Journal of mathematical sociology, 25(2):163177. Zhe Chen, Yuehan Wang, Bin Zhao, Jing Cheng, Xin Zhao, and Zongtao Duan. 2020. Knowledge graph Ieee Access, 8:192435 completion: review. 192456. Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2024. Lift yourself up: Retrieval-augmented text generation with selfmemory. Advances in Neural Information Processing Systems, 36. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint 2404.16130. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meeting llms: Towards retrieval-augmented large language models. In International Conference on Knowledge Discovery and Data Mining (KDD), pages 64916501. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022a. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022b. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. Lightrag: Simple and fast retrievalaugmented generation. arXiv preprint 2410.05779. Shailja Gupta, Rajesh Ranjan, and Surya Narayan Singh. 2024. comprehensive survey of retrievalaugmented generation (rag): Evolution, current arXiv preprint landscape and future directions. 2410.12837. Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar, Ryan Rossi, Subhabrata Mukherjee, Xianfeng Tang, et al. 2024a. Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309. Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, and Vittorio Castelli. 2024b. Rag-qa arena: Evaluating domain robustness for long-form retrieval arXiv preprint augmented question answering. arXiv:2407.13998. Taher Haveliwala, Sepandar Kamvar, and Glen Jeh. 2003. An analytical comparison of approaches to personalizing pagerank. Technical report, Stanford. Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. arXiv preprint arXiv:2402.07630. Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, and Graham Neubig. 2024. Ragged: Towards informed design of retrieval augmented generation systems. arXiv preprint 2403.09040. Xinke Jiang, Yue Fang, Rihong Qiu, Haoyu Zhang, Yongxin Xu, Hao Chen, Wentao Zhang, Ruizhe Zhang, Yuchen Fang, Xu Chu, et al. 2024a. Tcrag: Turing-complete rags case study on medical llm systems. arXiv preprint arXiv:2408.09199. Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024b. Longrag: Enhancing retrieval-augmented generarXiv preprint ation with long-context arXiv:2406.15319. llms. 10 Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large language models. arXiv preprint arXiv:2405.14831. Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. 2023. Tree of clarifications: Answering ambiguous questions with retrievalaugmented large language models. arXiv preprint arXiv:2310.14696. Yi-Xiu Kong, Gui-Yuan Shi, Rui-Jie Wu, and Yi-Cheng Zhang. 2019. k-core: Theories and applications. Physics Reports, 832:132. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and Philip Yu. 2021. Dense hierarchical retrieval for open-domain question answering. arXiv preprint arXiv:2110.15439. Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061. James MacQueen et al. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281297. Oakland, CA, USA. Yu Malkov and Dmitry Yashunin. 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4):824836. Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, and Hannaneh Hajishirzi. 2021. Joint passage ranking for diverse multi-answer retrieval. arXiv preprint arXiv:2104.08445. Jinyoung Park, Ameen Patel, Omar Zia Khan, Hyunwoo Kim, and Joo-Kyung Kim. 2023. Graphguided reasoning for multi-hop question answerarXiv preprint ing in large language models. arXiv:2311.09762. Diego Sanmartin. 2024. Kg-rag: Bridging the gap between knowledge and creativity. arXiv preprint arXiv:2405.12035. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher Manning. 2024. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059. Stephen Seidman. 1983. Network structure and minimum degree. Social networks, 5(3):269287. Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, and Siqi Deng. 2024. Fairrag: Fair human generation via fair retrieval augmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11996 12005. Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models. arXiv preprint arXiv:2403.10081. Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multihop queries. arXiv preprint arXiv:2401.15391. Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh Chawla, and Panpan Xu. 2024. Graph neural prompting with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1908019088. SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313. Vincent Traag, Ludo Waltman, and Nees Jan Van Eck. 2019. From louvain to leiden: guaranteeing wellconnected communities. Scientific reports, 9(1):1 12. Milena Trajanoska, Riste Stojanov, and Dimitar Trajanov. 2023. Enhancing knowledge graph construction using large language models. arXiv preprint arXiv:2305.04676. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022a. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022b. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Chengrui Wang, Qingqing Long, Meng Xiao, Xunxin Cai, Chengjun Wu, Zhen Meng, Xuezhi Wang, and Yuanchun Zhou. 2024a. Biorag: rag-llm framework for biological question reasoning. arXiv preprint arXiv:2408.01107. 11 Yu Wang, Nedim Lipka, Ryan Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024b. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1920619214. Yilin Wen, Zifeng Wang, and Jimeng Sun. 2023. Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. arXiv preprint arXiv:2308.09729. Junde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min Xu, Filippo Menolascina, and Vicente Grau. 2024. Medical graph rag: Towards safe medical large language model via graph retrieval-augmented generation. arXiv preprint arXiv:2408.04187. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600. Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, and Chen Wang. 2024. Causal graph discovery with retrieval-augmented generation based large language models. arXiv preprint arXiv:2402.15301. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. 2024. Retrievalaugmented generation for ai-generated content: survey. arXiv preprint arXiv:2402.19473."
        },
        {
            "title": "Node",
            "content": "Graph-L Graph-G Light Node Graph-L Graph-G"
        },
        {
            "title": "Node",
            "content": "HotpotQA Musique MultiHop Arena-Fiqa Arena-Lifestyle Arena-Recreation Arena-Science Arena-Tech Arena-Writing 1.93M 1.84M 1.41M 1.65M 1.64M 0.93M 1.43M 1.72M 1.82M 66min 76min 50min 45min 52min 34min 43min 54min 50min 39min 90min 58min 49min 59min 33min 46min 54min 71min 21min 25min 24min 19min 18min 10min 17min 14min 13min 227MB 461MB 214MB 255MB 492MB 250MB 141MB 276MB 137MB 112MB 240MB 117MB 138MB 278MB 125MB 89MB 80MB 172MB 116MB 236MB 111MB 133MB 276MB 139MB 151MB 309MB 157MB 2.66s 2.94s 4.15s 8.95s 7.54s 5.10s 8.05s 7.35s 5.65s 26.69s 22.65s 34.45s 28.94s 33.09s 23.10s 35.79s 28.64s 40.12s 5.58s 6.53s 7.10s 13.35s 10.43s 8.01s 14.28s 8.89s 10.70s 3.98s 4.05s 4.89s 8.86s 6.79s 6.90s 8.85s 6.74s 5.40s 6680.65 6616.84 7367.54 6819.45 6860.26 6669.95 6759.15 6755.46 6477.72 810529 1111073 920780 713560 895964 564636 778051 741690 7176.73 7458.34 8920.00 7721.73 6822.32 6249.31 7111.80 6922.55 6364.59 5079.40 5960.25 5259.99 3381.72 3350.35 3448.38 4284.13 3821.78 3373.34 Table 3: Performance metrics for RAG methods, including Index Time, Storage Usage, Query Time, and Retrieval Tokens across various datasets. Graph denotes GraphRAG, with Graph-l representing its local mode and Graph-G its global mode. Light refers to LightRAG in hybrid mode, while Node represents our proposed method. The table 3 presents the system performance of mainstream graph-based RAG methods and our proposed approach. Compared to previous work, our method demonstrates superior performance across multiple datasets and in open-ended head-to-head evaluations, while also achieving better system-level efficiency. All evaluations in the table were conducted using the default indexing settings of each RAG method, with the query settings and the prompt details provided in Appendix B.2. Notably, our method demonstrates significant advantage in indexing time, which is crucial for practical deployment. This advantage is attributed to the construction process of our Hetero Graph, which not only creates more fine-grained and semantically meaningful graph structure but also carefully considers the algorithmic complexity of the retrieval process. NodeRAG also exhibits relatively better storage efficiency. Although the total number of nodes in our expanded graph is significantly larger than in previous graph structures, the combination of selective embedding and dual search effectively reduces the number of embedded nodes, leading to more efficient storage strategy. Moreover, our unified information retrieval approach results in reduced query time. While the GraphRAG local search (Graph-l) relies purely on vector similaritysimilar to our \"without cross-node\" setting mentioned in Section 4and achieves faster search speeds, its global mode (Graph-G) experiences significantly higher query times, exceeding 20 seconds with concurrency of 16. This is due to its reliance on LLM-based traversal of all community information, leading to substantial number of retrieval tokens. Given the considerable time and computational overhead associated with Graph-G queries, we conducted full evaluation only on the MuSiQue dataset. For other datasets, query time and retrieval token statistics were estimated based on sample of 20 selected queries. Further details on the ablation study of GraphRAG can be found in the Appendix B.4. In contrast, our method leverages the heterograph and graph algorithms to achieve unified information retrieval, effectively capturing meaningful information needs across multiple levels within single framework while maintaining efficient query speed. Finally, the nodes within the heterograph are connected in fine-grained structure, ensuring that more relevant text is retrieved with relatively fewer retrieval tokens."
        },
        {
            "title": "B Experiment details",
            "content": "B.1 Datasets We evaluate Node RAGs performance across four different benchmarks: HotpotQA, MuSiQue, MultiHopRAG and RAG-QA Arena. However, the original question formats of HotpotQA and MuSiQue required selecting the most relevant passages from multiple documents, incorporating multi-hop reasoning details. This setup no longer aligns with mainstream RAG methods, as modern approaches perform indexing over an entire corpus and subsequently retrieve information from the indexed data. To adapt to this paradigm, we concatenate all passages into unified corpus, transforming the task into retrieving multi-hop relevant information from the entire corpus. This modification makes the task more challenging compared to the 13 original setting. The evaluation metrics for HotpotQA and MuSiQue are divided into two aspects: the quality of the retrieved documents and the accuracy of the final answer, measured by metrics such as F1 score. However, current RAG methods retrieve not only text chunks but also more flexible forms of information, making it difficult to assess retrieval quality using traditional top-k document evaluation. Moreover, metrics like F1 score have become less effective in evaluating answers generated by modern generative models. Therefore, we adopt the LLM-as-a-Judge approach, leveraging LLMs to assess the final accuracy of the generated answers.The MultiHop and RAG-QA Arena dataset settings provide strong evaluation framework for current RAG methods. Therefore, we follow the original benchmarks proposed testing methodology and evaluation metrics. Further details regarding the benchmark settings are described below. HotpotQA is multi-hop question-answering dataset where each question requires combining information It encourages deeper reasoning by providing from multiple documents to find the correct answer. supporting factsspecific sentences from the texts that lead to the solution. Questions range widely across domains and often involve bridging or comparison to ensure more complex, multi-step reasoning. This makes HotpotQA critical benchmark for evaluating advanced reading comprehension models. We sampled 200 questions from the final dataset for evaluation. MuSiQue is also multi-hop question-answering dataset that challenges models to combine information across multiple documents in structured, step-by-step manner. Each question is designed to require several reasoning steps, ensuring that simple shortcut approaches do not suffice. As result, MuSiQue serves as rigorous test of advanced reading comprehension, demanding that systems accurately connect disparate pieces of evidence to arrive at correct answers. We also sample 175 questions for the evaluation MultiHop-RAG is multi-hop question-answering dataset that includes four distinct question types: comparison query, null query, inference query, and temporal query. From this dataset, we curated 375 questions to evaluate our approach. Each query in MultiHop requires synthesizing information from multiple sources, testing models ability to perform bridging inferences, handle temporal relationships, and make higher-order logical connections. This diversity in question types provides rigorous benchmark for assessing whether RAG methods can integrate scattered pieces of evidence. RAG-QA Arena is new evaluation framework designed to assess the quality of retrieval-augmented generation (RAG) systems on long-form question answering. It builds on Long-form RobustQA (LFRQA), dataset of 26K queries across seven domains including writing, tech, science, recreation and lifestyle. Each LFRQA entry features coherent, human-written answer grounded in multiple documents. RAGQA Arena leverages LLMs as evaluators, directly comparing systems generated answer with the gold long-form answer from LFRQA. Experimental results show that these model-based comparisons correlate highly with human judgments, making it challenging yet reliable benchmark for testing both cross-domain robustness and the ability to produce integrated, long-form responses. B.2 Baselines We compare NodeRAG against several strong and widely used RAG methods. By default, all these RAG methods implement their indexing process using GPT-4o-mini. However, we identify potential unfairness in the current evaluation setup, particularly in several key areas. To ensure the correctness and validity of the evaluation data, it is crucial to standardize both the final answer response prompt and the model temperature settings. Using different response prompts or varying temperature settings for answer generation introduces inconsistencies, as higher temperature setting may yield responses that receive better LLM preference score compared to those generated with lower temperature. critical point to consider is that, as RAG methods, the primary focus of evaluation should be the quality of the retrieved context rather than the final generated answer. Therefore, to ensure that final accuracy metrics accurately reflect the quality of the retrieved context, the final answer generation process and model settings should remain consistent across all methods. Hence, we set the temperature to 0 across the entire evaluation and standardized response prompts for every method. The unified prompt is illustrated 14 in appendix E. Our initiative to standardize these settings also benefits other methods, such as GraphRAG, improving their performance compared to their default settings. This underscores the broader value of establishing fair and consistent evaluation standards. Additionally, traditional evaluation methods such as top-k retrieval comparison have become increasingly difficult to apply uniformly, as retrieval is no longer restricted to isolated text chunks. To address this challenge, we propose new evaluation standard that leverages retrieval tokens as an efficiency metric. This approach ensures that retrieval methods achieve better effectiveness while utilizing fewer retrieval tokens, promoting more efficient and fair comparison framework. Current methods can only control the number of retrieval tokens through hyperparameter tuning. Although precise control over the exact number of tokens is not possible, we consider maintaining the average number of retrieval tokens within the range of 5K to 10K to be reasonable and fair comparison criterion. Below, we provide detailed introduction to each method along with its specific settings for reference. Naive RAG This method serves as standard baseline among all existing RAG systems. It first divided input document into several text chunks and encoded them into vector space utilizing text embeddings. Then retrieve related text chunks based on similarity of query representations. The number of retrieval tokens can be adjusted through the top-k parameter. HyDE HyDE serves as an improved method over traditional RAG systems. It first generates \"hypothetical\" texts that capture the essence of query. It then uses this generated text to retrieve relevant documents from large corpus, employing vector similarity in an embedding space. This method modifies the input query at the frontend without altering the text chunks or their embeddings. Therefore, we can still use the top-k parameter to control the number of retrieval tokens. GraphRAG This approach starts by segmenting the input text into chunks and extracting the entities and relationships within them, forming graph structure. This graph is then divided into multiple communities at different levels. At query time, GraphRAG identifies the relevant entities from the question and synthesizes answers by referencing these corresponding community summaries. Compared to traditional RAG methods, GraphRAG provides more structured and high-level understanding of the entire document. Through our experiments, we observed that under the default settings, the number of queries in GraphRAGs local mode resulted in higher retrieval token count than the naive retrieval approach. To ensure fair comparison, we proportionally reduced its parameters and standardized its prompt to match our unified prompt. The ablation study in Appendix B.4 demonstrates that after these adjustments, GraphRAGs accuracy improved, further validating the fairness of our evaluation methodology. Additionally, we analyzed both the local and global modes of GraphRAG. Our findings indicate that the global mode introduces significant additional overhead in terms of time and computational cost while providing only marginal improvements compared to the local mode. This result is further supported by our ablation study, which shows that the local mode achieves better efficiency and effectiveness. LightRAG LightRAG is an improved approach based on GraphRAG, designed to minimize computational overhead while enhancing the comprehensiveness of retrieved information through dual-level retrieval. This leads to more efficient retrieval and better balance between effectiveness and speed compared to GraphRAG. Similar to GraphRAG, the default settings of LightRAG result in higher retrieval token count than the Naïve approach. To ensure fair comparison, we proportionally adjusted its hyperparameters to maintain the number of retrieval tokens within the range of 5K to 10K. B.3 NodeRAG Graph Statistics The table 4 presents the number of each type of node in the indexed graph for each dataset, including entity (N ), relationship (R), semantic unit (S), attribute (A), high-level elements (H), high-level overview (O), and text (T ). These counts are detailed in the type statistics section. Additionally, the graph statistics provide information on the total number of nodes, the number of non-HNSW edges, HNSW edges, and"
        },
        {
            "title": "Graph Statistics",
            "content": "HotpotQA MuSiQue MultiHop-RAG Arena-Fiqa Arena-Lifestyle Arena-Recreation Arena-Science Arena-Tech Arena-Writing 1.93M 1.84M 1.41M 1.65M 1.64M 0.93M 1.43M 1.72M 1.82M R 1985 1907 1532 1821 1794 1003 1583 1910 1937 15905 18714 10986 9027 9400 5542 8010 10837 11008 88863 99840 43184 32470 39464 26382 32232 37724 42723 56578 61964 29286 27422 27895 16938 23092 29691 29338 684 795 685 508 518 413 551 534 Nodes Non-HNSW Edge HNSW Edge"
        },
        {
            "title": "Edge",
            "content": "4479 5700 2289 1714 2221 1969 2515 2633 4435 4479 5700 2289 1714 2221 1969 2515 2633 4435 172603 193922 90144 74605 83461 54180 70425 85888 94259 283543 316029 171410 143916 149225 93228 127719 167950 149552 487731 583126 203199 154109 174461 117915 149424 193159 298565 759812 888966 367486 295165 318073 207449 276963 354033 Table 4: Comprehensive dataset statistics, detailing corpus size, type statistics (T, S, N, R, A, O, H), and graph statistics. The graph statistics include the number of document compilation nodes, HNSW semantic edges, and total edges. Each value represents key metric relevant to graph-based document processing and retrieval. the total number of edges. The data indicate that the number of HNSW edges is comparable to that of nonHNSW edges, highlighting the integration of semantic connections within the graph. Notably, overlapping edges are removed when merging non-HNSW and HNSW edges. For instance, in the MultiHop-RAG benchmark, there are 171,410 non-HNSW edges and 203,199 HNSW edges. However, the total number of edges after merging is 367,486, which is only 7,123 fewer than the sum of both edge types. This indicates the uniqueness of these two types of edges and highlights the effectiveness of the HNSW algorithm. B.4 Graph RAG Ablation"
        },
        {
            "title": "Method",
            "content": "Accuracy Avg. Processing Time Avg. Tokens GraphRAG (default) Graph-L Graph-G 37.14% 41.71% 33.14% 4.82s 2.94s 22.65s 10.4k 6.6k 1.11M Table 5: Performance Comparison of GraphRAG Variants. Default is the default setting. Local and global represent the local and global modes under unified prompt and hyperparameter settings. The default setting of GraphRAG, along with its own prompting mechanism, is not standardized for evaluation, as both the number of retrieval tokens and the choice of prompts significantly impact performance. Hence, we introduce unified prompt and adjust the hyperparameters of GraphRAG to ensure fair comparison within specific range. As shown in the table B.4, GraphRAG with our unified prompt achieves higher performance, demonstrating that the original prompting strategy is not optimal for this task. This further ensures fairness in comparison, as performance is influenced solely by the quality of the retrieved context. Moreover, the global mode of GraphRAG requires significantly longer processing time and incurs higher computational costs due to the LLM analyzing all community summaries, leading to increased complexity and resource consumption. Additionally, for multi-hop question answering, this approach results in degraded performance. Therefore, we conducted an exploratory ablation study only on the MuSiQue dataset, while for other datasets, we estimated query time and retrieval token statistics based on sampled queries."
        },
        {
            "title": "C Algorithm details",
            "content": "C.1 Terminology Abbr."
        },
        {
            "title": "Function",
            "content": "T"
        },
        {
            "title": "Text",
            "content": "Full-text chunks from the original source. It contains rich detailed information, although it integrates large amount of unrelated semantic information. Retrievable; Entry points from vector similarity Example \"Hinton was awarded the Nobel Prize in 2023 for his groundbreaking contributions to artificial intelligence, particularly in deep learning. His pioneering work on backpropagation laid the foundation for modern neural networks, influencing both academia and industry. The recognition came amid increasing discussions on the ethical implications of AI, with Hinton himself advocating for responsible AI development and regulation.\" S"
        },
        {
            "title": "Semantic Unit",
            "content": "Local summaries that are independent and meaningful events summarized from text chunks. They serve as middle layer between text chunks and entities, acting as the basic units for graph augmentation and semantic analysis. Retrievable; Entry points from vector similarity. \"Hinton was awarded the Nobel Prize for inventing backpropagation.\" A"
        },
        {
            "title": "Attribute",
            "content": "Attributes of key entities, derived from relationships and semantic units around important entities. Retrievable; Entry points from vector similarity. High-Level Element Insights summarizing graph communities. Encapsulates core information or any high level ideas from community. Retrievable; Entry points from vector similarity. \"Geoffrey Hinton, often referred to as the \"Godfather of Deep Learning,\" is pioneer in the field of artificial intelligence. In 2024, he was awarded the Nobel Prize for his contributions to AI and deep learning. \" \"Due to the increasing importance of AI, the Nobel Prize is awarded to scholars who have made tremendous contributions to the field of AI.\" High-Level Overview N"
        },
        {
            "title": "Entity",
            "content": "Titles or keywords summarizing high-level elements. Connections between entities represented as nodes. Acts as connector nodes and secondary retrievable node. Named entities such as people, places, or concepts. Non-Retrievable; Entry points from accurate search. \"AI significance\" Retrievable; Non-Entry points \"Hinton received the Nobel Prize.\" Non-Retrievable; Entry points from accurate search.. \"Hinton,\" \"Nobel Prize\" Table 6: Node Types in the heterograph C.2 K-core & Betweenness centrality In this subsection, we present the methodology for identifying important entities and generating their attribute summaries, ensuring alignment with the mathematical framework established in the main text. The selection of important entities, denoted as , is based on two fundamental structural graph metrics: K-core decomposition and betweenness centrality. These metrics collectively ensure that the selected 17 nodes are not only structurally integral but also play pivotal role in facilitating information flow. The K-core decomposition, denoted as K(G1), identifies nodes within densely connected subgraphs, ensuring that selected entities contribute significantly to the structural cohesion of the graph. Meanwhile, betweenness centrality, denoted as B(G1), highlights nodes that serve as critical intermediaries between different regions of the graph, capturing entities essential for information dissemination. The process of identifying important entities follows the steps outlined in Algorithm 1. Algorithm 1 Identification of Important Entities Input: Graph G1 = (V, E) Output: Important entity set Step 1: Compute K-core decomposition Compute the core threshold: kdefault = log(V) (cid:18) (cid:80) vV deg(v) (cid:19)1/2 Extract the K-core subgraph: K(G1) = {v degG1(v) kdefault} Step 2: Compute betweenness centrality for each do Approximate betweenness centrality using shortest-path sampling: b(v) = betweenness_centrality(G1, = 10) end for Compute the average betweenness centrality: Compute the scale factor: = (cid:80) vV b(v) scale = log10(V) Step 3: Select important nodes for each do if b(v) > scale then Add to B(G1) end if end for Compute the final set of important entities: = K(G1) B(G1) Return C.3 Semantic Matching within Community To establish meaningful semantic relationships among high-level element nodes, we propose the Semantic Matching within Community algorithm. This algorithm ensures that entities with strong semantic similarities are connected within their respective communities. The motivation behind this approach is Algorithm 2 Semantic Matching within Community Input: Graph = (V, E), node embeddings Φ(V), community partition {Cn} Output: Semantic edges Eh Step 1: Select high-level element nodes Extract nodes with labels S, A, or H: V{S,A,H} = {v ψ(v) {S, A, H}} Step 2: Apply K-means clustering to node embeddings Set number of clusters: (cid:113) = V{S,A,H} Perform K-means clustering on V{S,A,H}), obtaining clusters {Sk} Step 3: Establish semantic edges within communities for each community Cn do for each cluster Sk do Identify nodes within the community and cluster: VCn,Sk = V{S,A,H} Cn Sk for each pair (v, v) where {S, A}, do Add semantic edge: eh(v, v) Eh end for end for end for Return Eh to organically integrate nodes into the graph structure by establishing connections with semantically related nodes within the same community. Formally, the process is summarized in Algorithm 2. The algorithm begins by identifying nodes that belong to three specific categories: structure nodes (S), attribute nodes (A), and high-level nodes (H). These nodes are collectively defined as: V{S,A,H} = {v ψ(v) {S, A, H}} Since these nodes exhibit inherent semantic relationships, we cluster them based on their embeddings, which capture their contextual meaning. To partition the nodes into semantically similar groups, we apply the K-means clustering algorithm (MacQueen et al., 1967) to the embedding representations of V{S,A,H}. which balances computational efficiency and granularity. This clustering process results in partitioning of nodes into semantic clusters, denoted as Sk, where each cluster contains nodes with closely related semantic representations. After clustering, the algorithm establishes edges between semantically related nodes within the same community. Communities are predefined structural subgroups in the graph, denoted as Cn, ensuring that local relationships are preserved. For each community-cluster pair, semantic edges are introduced between nodes in V{S,A} and nodes in VH . Specifically, for any node pair (v, v), where V{S,A} and VH , an edge eh(v, v) is established if both nodes belong to the same community and the same semantic cluster. 19 By integrating semantic matching within community constraints, this algorithm enhances the structural integrity of the graph while maintaining computational feasibility. The choice of K-means clustering efficiently groups nodes with similar semantic properties, while the enforcement of community constraints ensures that edges are only formed between nodes that naturally belong to the same substructure. Consequently, the proposed method balances semantic consistency and graph locality, making it well-suited for applications requiring structured knowledge representation and retrieval. C.4 Dual Search To efficiently locate relevant entry points within the Hetero Graph G, we propose the Dual Search algorithm, which integrates exact matching on structured nodes and vector similarity search on rich information nodes. This hybrid approach ensures balance between precision and recall by leveraging both symbolic and dense representations. The core idea is to utilize exact string matching for well-structured nodes while employing approximate nearest neighbor search for nodes containing rich contextual information. By doing so, the algorithm improves both retrieval accuracy and robustness to query variations. Given query, LLM extracts set of relevant entities, denoted as q, and embeds the query into vector representation q. Entry points in the graph are then determined by: where the condition function Φ(v, q, q) determines whether node qualifies as an entry point: Ventry = {v Φ(v, q, q)}, Φ(v, q, q) = (cid:40) V{N,O} M(N q, v), V{S,A,H} R(q, v, k). Here, the exact matching function M(N q, v) returns true if node matches one of the extracted entities in q. This ensures that titles or named nodes such VN,O are retrieved deterministically. Meanwhile, the similarity-ranking function R(q, v, k) applies HNSW, selecting the top-k most similar nodes to in vector space. This is particularly beneficial for context-rich nodes for VS,A,H , which may not contain exact query terms but remain semantically relevant. One of the major strengths of Dual Search is its ability to mitigate errors from noisy or ambiguous queries. Since and nodes serve exclusively as entry points and are not directly retrieved, any errors in exact matching do not propagate into the retrieval results. Furthermore, Personalized PageRank (PPR) is applied to refine retrieval by selecting highly weighted cross nodesnodes that are both structurally significant and semantically aligned with multiple entry points. This graph-based expansion helps compensate for potential errors in entity extraction and ensures robustness against query variations. Another advantage of this approach is its scalability. By employing HNSW, the algorithm avoids the computational overhead of exhaustive similarity searches, making it efficient even on large-scale graphs."
        },
        {
            "title": "D Case Study",
            "content": "Question1 Why isnt there public transport on Christmas day in the UK?"
        },
        {
            "title": "LLM\nDecision",
            "content": "The absence of public transport on Christmas Day in the UK can be attributed to combination of historical, cultural, and economic factors. Historically, public transport services were more prevalent on Christmas Day before World War II, as many people worked on that day and required transportation. However, over the years, societal norms have shifted, leading to significant reduction in demand for public transport services on Christmas Day. In contemporary society, Christmas is widely recognized as time for family gatherings and celebrations, which diminishes the need for public transport. Many people choose to spend the day at home or with relatives, resulting in lower demand for transit services. Consequently, transport companies have adapted to this change by suspending services on Christmas Day, allowing transit workers to enjoy the holiday with their families as well. Additionally, the operational costs associated with running public transport services on Christmas Day can be significant. With minimal passenger demand, the financial viability of providing such services becomes questionable. Railways and other transport providers often use this time for maintenance and engineering work, taking advantage of the reduced disruption to carry out necessary improvements. Public transport in the UK is significantly limited on Christmas Day due to combination of historical, cultural, and operational factors. Traditionally, Christmas Day is public holiday in the UK, and many businesses, including public transport services, close to allow employees to spend time with their families and celebrate the holiday. This practice is rooted in the cultural significance of Christmas, which emphasizes family gatherings and community celebrations. The limited operation of public transport on Christmas Day is also influenced by the low demand for services. Historically, fewer people travel on this day compared to other holidays or weekends, as many individuals prefer to stay home or visit family. As result, transport providers often find it economically unfeasible to run full services when the number of passengers is expected to be minimal. In recent years, some transport services have begun to offer limited services on Christmas Day, particularly in major cities like London, where there may be demand for travel to events or gatherings. However, these services are typically not as extensive as those available on other public holidays or regular days. Overall, the combination of cultural practices, low demand, and operational considerations contributes to the lack of public transport on Christmas Day in the UK. In most parts of the world, official or religious holidays are taken very seriously. On such days, employers typically have to pay higher rates to staff who are willing to work, but that effort often doesnt pay off due to low customer turnout. Families tend to arrange their travel plans by Christmas Eve to avoid the need for transportation on Christmas Day. Christmas avoids unnecessary travel because its typically day filled with home activities, and it allows workers in various roles to have day off. The lack of public demand and the staffs resistance contributed to the lack of services on Christmas Day. Christmas Day is also an opportune time to perform engineering work on railways with minimal disruption. The railroad industry is capable of completing extensive projects during this period, thanks to thorough planning and pre-staging. ORDER: {1\": GraphRAG\", 2\": NodeRAG\"} Both answers provide comprehensive explanation, but answer 2 adds historical context about public transport before World War II and mentions the use of the holiday for maintenance work, offering slightly more depth. ORDER: {1\": LightRAG\", 2\": NodeRAG\"} Both answers provide comprehensive explanation, but answer 2 adds historical context about the change in public transport services since World War II, which enriches the explanation. Therefore, answer 2 is slightly better. Table 7: Sample Table with Multi-line Text Question2 What are rabbit hides used for in Minecraft?"
        },
        {
            "title": "LLM\nDecision",
            "content": "In Minecraft, rabbit hides serve couple of useful purposes, particularly in crafting and resource management. One of the primary uses of rabbit hides is to craft leather. Players can combine four rabbit hides in 2x2 crafting grid to create one piece of leather. This is especially beneficial in Survival Multiplayer (SMP) scenarios where finding other animals for leather can be challenging. Additionally, rabbits can drop unique item known as the rabbits foot\", which is an important ingredient for brewing potions. The rabbits foot is specifically used in the crafting of the Potion of Leaping, which grants players the ability to jump higher for limited time. Thus, rabbit hides and their associated drops contribute significantly to crafting and potion-making in the game, enhancing the overall gameplay experience. In Minecraft, rabbit hides are primarily used to craft leather. Players can combine four rabbit hides in 2x2 crafting grid to create one piece of leather. This crafting option is particularly useful for players who may not have access to cows, which are the traditional source of leather in the game. Additionally, rabbit hides can be beneficial in multiplayer servers that utilize entity clears, as finding animals can be challenging. By using rabbit hides to craft leather, players have an alternative method to obtain this valuable resource, which is essential for crafting various items, including armor and books. In Minecraft, Rabbit Hides are primarily used as crafting resource. They can be combined with other materials to create leather, which is an essential item in the game. Leather is used for crafting various items, including armor (like leather armor), books, item frames, and more. The process of obtaining Rabbit Hides involves hunting rabbits, which can be found in various biomes. Once collected, players can use the hides in their crafting recipes to enhance their gameplay experience through resource management and item creation. This highlights the importance of Rabbit Hides within the broader crafting mechanics of Minecraft. ORDER: {1\": TGRAG\", 2\": GraphRAG\"} Both answers are truthful, but answer 1 provides more detailed information about the uses of rabbit hides, including the additional use of rabbits foot for potion-making Therefore, answer 1 is better. ORDER: {1\": TGRAG\", 2\": LightRAG\"} Both answers are truthful, but answer 1 provides more detailed information about the uses of rabbit hides, including the crafting of leather and the rabbits foot for potions. Therefore, answer 1 is better. Table 8: Sample Table with Multi-line Text"
        },
        {
            "title": "E Overview Prompting in NodeRAG",
            "content": "23 24"
        }
    ],
    "affiliations": [
        "Columbia University",
        "Lehigh University",
        "University of Pennsylvania"
    ]
}