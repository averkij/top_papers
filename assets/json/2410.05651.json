{
    "paper_title": "ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler",
    "authors": [
        "Serin Yang",
        "Taesung Kwon",
        "Jong Chul Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from a single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce a novel, bidirectional sampling strategy to address these off-manifold issues without requiring extensive re-noising or fine-tuning. Our method employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On a single 3090 GPU, our method can interpolate 25 frames at 1024 x 576 resolution in just 195 seconds, establishing it as a leading solution for keyframe interpolation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 1 5 6 5 0 . 0 1 4 2 : r Preprint. VIBIDSAMPLER: ENHANCING VIDEO INTERPOLATION USING BIDIRECTIONAL DIFFUSION SAMPLER Serin Yang1, Taesung Kwon2, Jong Chul Ye1 1 Kim Jae Chul Graduate School of AI, KAIST, {yangsr, star.kwon, jong.ye}@kaist.ac.kr 2 Dept. of Bio & Brain Engineering, KAIST Figure 1: Keyframe interpolation results using our ViBiDSampler. (a) The images in the first and last rows are keyframes, and the intermediate frames are generated using ViBiDSampler. (b) comparison of results with three baseline methodsFILM, TRF, and Generative Inbetweening (GI)demonstrates that these baselines exhibit artifacts or unnatural appearances. In contrast, our method generates clear and realistic frames."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent progress in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models has greatly enhanced video generation, especially in terms of keyframe interpolation. However, current image-to-video diffusion models, while powerful in generating videos from single conditioning frame, need adaptation for two-frame (start & end) conditioned generation, which is essential for effective bounded interpolation. Unfortunately, existing approaches that fuse temporally forward and backward paths in parallel often suffer from off-manifold issues, leading to artifacts or requiring multiple iterative re-noising steps. In this work, we introduce novel, bidirectional sampling strategy to address these offmanifold issues without requiring extensive re-noising or fine-tuning. Our method 1 Preprint. employs sequential sampling along both forward and backward paths, conditioned on the start and end frames, respectively, ensuring more coherent and on-manifold generation of intermediate frames. Additionally, we incorporate advanced guidance techniques, CFG++ and DDS, to further enhance the interpolation process. By integrating these, our method achieves state-of-the-art performance, efficiently generating high-quality, smooth videos between keyframes. On single 3090 GPU, our method can interpolate 25 frames at 1024576 resolution in just 195 seconds, establishing it as leading solution for keyframe interpolation. Project page: https://vibid.github.io/"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in large-scale text-to-video (T2V) and image-to-video (I2V) diffusion models (Blattmann et al., 2023a;b; Wu et al., 2023; Xing et al., 2023; Bar-Tal et al., 2024) have made it possible to generate high-quality videos that closely match given text or image conditions. Various efforts have been made to leverage the powerful generative capabilities of these video diffusion models, especially in the context of keyframe interpolation, to improve perceptual quality significantly. Specifically, diffusion-based keyframe interpolation (Voleti et al., 2022; Danier et al., 2024; Huang et al., 2024; Feng et al., 2024; Wang et al., 2024) focuses on generating intermediate frames between two keyframes, aiming to create smooth and natural motion dynamics while preserving the keyframes visual fidelity and appearance. Image-to-video diffusion models are particularly wellsuited for this task because they are designed to maintain the visual quality and consistency of the initial conditioning frame. While image-to-video diffusion models are designed for start-frame conditioned video generation, they need to be adapted for start and end frame conditioned video generation for keyframe interpolation. One line of works (Feng et al., 2024; Wang et al., 2024) addresses this issue by introducing new sampling strategy that fuses the intermediate samples of the temporally forward path, conditioned on the start frame, and the temporally backward path, conditioned on the end frame. The fusing strategy ensures smooth and coherent frame generation in-between two keyframes using image-to-video diffusion models in training-free (Feng et al., 2024) or lightweight fine-tuning manner (Wang et al., 2024). In the geometric view of diffusion models (Chung et al., 2022), the sampling process is typically described as iterative transitions Mt Mt1, = T, , 1, moving from the noisy manifold MT to the clean manifold M0. From this perspective, fusing two intermediate sample points through linear interpolation on noisy manifold can lead to an undesirable off-manifold issue, where the generated samples deviate from the learned data distribution. TRF (Feng et al., 2024) reported that this fusion strategy often results in undesired artifacts. To address these discrepancies, they apply multiple rounds of re-noising and denoising to the fused samples, which may help correct the offmanifold deviations. Unlike the prior works, here we introduce simple yet effective sampling strategy to address offmanifold issues. Specifically, at timestep t, we first denoise xt to obtain xt1 along the temporally forward path, conditioned on the start frame (Istart). Then, we re-noise xt1 back to xt using stochastic noise. After that, we denoise t1 along the temporally backward path, conditioned on the end frame (Iend), where the notation indicates that the sample has been flipped along the time dimension. Unlike the fusing strategy, which computes two conditioned outputs in parallel and then fuses them, our bidirectional diffusion sampling strategy samples two conditioned outputs sequentially, which mitigates the off-manifold issue. to get Furthermore, we incorporate advanced on-manifold guidance techniques to produce more reliable interpolation results. First, we employ the recently proposed CFG++ (Chung et al., 2024), which addresses the off-manifold issues inherent in Classifier-Free Guidance (CFG) (Ho & Salimans, 2021). Second, we incorporate DDS guidance (Chung et al., 2023) to ensure proper alignment of the last frame of the generated samples with the given frames, as the ground-truth start and end frames are already provided. By combining bidirectional sampling with these guidance techniques, our method achieves stable, state-of-the-art keyframe interpolation performance without requiring fine-tuning or multiple re-noising steps. Thanks to its efficient sampling strategy, our method can interpolate between two keyframes to generate 25-frame video at 1024576 resolution in just 195 seconds on 2 Preprint. single 3090 GPU. Since our method is designed for high-quality and vivid video keyframe interpolation using bidirectional diffusion sampling, we refer to it as Video Interpolation using BIdirectional Diffusion (ViBiD) Sampler."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Video interpolation. Video interpolation is task that generates the intermediate frames based on two bounding frames. Conventional interpolation methods have utilized convolutional neural networks (Kong et al., 2022; Li et al., 2023; Lu et al., 2022; Huang et al., 2022; Zhang et al., 2023b; Reda et al., 2019), which are typically trained in supervised manner to estimate the optical flows for synthesizing an intermediate frame. However, they primarily focus on minimizing L1 or L2 distances between the output and target frames, emphasizing high PSNR values at the expense of perceptual quality. Furthermore, the train datasets generally consist of high frame rate videos, limiting the models ability to learn extreme motion effectively. Diffusion-based methods and time reversal sampling. Diffusion-based methods have been proposed (Danier et al., 2024; Huang et al., 2024; Voleti et al., 2022) to leverage the generative priors of diffusion models to produce high-quality perceptual intermediate frames. Although these methods demonstrate improved perceptual performance, they still struggle with interpolating frames that contain significant motion. However, video keyframe interpolation methods that build on the robust performance of video diffusion models have been more successful in handling ambiguous and nonlinear motion (Xing et al., 2023; Jain et al., 2024), largely due to the incorporation of the temporal attention layers in these models (Blattmann et al., 2023a; Ho et al., 2022; Chen et al., 2023; Zhang et al., 2023a). Recent advancements in video diffusion models, particularly for image-to-video diffusion, have introduced new sampling techniques that leverage temporal and perceptual priors. These techniques reverse video frames in parallel during inference and fuse bidirectional motion from both the temporally forward and backward directions. TRF (Feng et al., 2024) proposed method that combines forward and backward denoising processes, each conditioned on the start and end frames. Similarly, Generative Inbetweening (Wang et al., 2024) introduced method that extracts temporal selfattention maps and rotates them to sample reversed frames, enhancing video quality by fine-tuning diffusion models for reversed motion. However, these methods rely on fusion strategy that often results in an off-manifold issue. Moreover, although methods such as multiple noise injections and model fine-tuning have been employed to address these challenges, they continue to exhibit off-manifold issues and substantially increase computational costs. In contrast, we introduce simple yet effective sampling strategy that eliminates the need for multiple noise injections or model fine-tuning."
        },
        {
            "title": "3 VIDEO INTERPOLATION USING BIDIRECTIONAL DIFFUSION",
            "content": "Although our method is applicable to general video diffusion models, we employ Stable Video Diffusion (SVD) (Blattmann et al., 2023a) as proof of concept in this paper. By introducing SVD, we aim to provide clearer understanding of our approach. SVD is latent video diffusion model employed in EDM-framework (Karras et al., 2022) with micro-conditioning (Podell et al., 2023) on frame rate (fps). For the image-to-video model, SVD replaces text embeddings with the CLIP image embedding (Radford et al., 2021) of the conditioning. In EDM-framework, the denoiser Dθ computes the denoised estimate from the U-Net ϵθ: Dθ(x; σ, c) = cskip(σ)x + cout(σ)ϵθ (cin(σ)x; cnoise(σ), c) , (1) where cskip, cout, cin, and cnoise are σ-dependent preconditioning parameters and is the condition. In practice, the denoiser Dθ takes concatenated inputs [x, x] to return c-conditioned estimate and null-conditioned estimate [ ˆxc(x), ˆx(x)], where ˆxc is then updated using ω-scale classifier-free guidance (CFG) (Ho & Salimans, 2021): For sampling, SVD employs Euler-step to gradually denoise from Gaussian noise xT to get x0: ˆxc(x) ˆx(x) + ω[ ˆxc(x) ˆx(x)]. (2) (3) xt1(xt; σt, c) := ˆxc(xt) + 3 σt1 σt (xt ˆxc(xt)), Preprint. Figure 2: Comparison of denoising processes. (a) Time Reversal Fusion method and (b) bidirectional sampling (Ours). where ˆxc(xt) is the denoised estimate from (2) and σt is the discretized noise level for each timestep [0, ]. 3.1 BIDIRECTIONAL SAMPLING Prior approaches such as TRF (Feng et al., 2024) and Generative Inbetweening (Wang et al., 2024) have employed fusion strategy that linearly interpolates between samples from the temporally forward path, conditioned on the start frame (Istart), and the temporally backward path, conditioned on the end frame (Iend): xt1,cstart = xt1(xt; σt, cstart), = xt1(x t; σt, cend), t1,cend (4) (5) xt1 = λxt1,cstart + (1 λ)(x (6) where the notation indicates that the sample has been flipped along the time dimension, λ denotes interpolation ratio, cstart and cend denotes the encoded latent condition of Istart and Iend, respectively. However, as the authors in TRF (Feng et al., 2024) reported, the vanilla implementation of this fusion strategy suffers from random dynamics and unsmooth transitions. This occurs because linearly interpolating between two distinct sample points in the noisy manifold Mt can cause the deviation from the original manifold, as illustrated in Fig. 3 (a). t1,cend ), In this work, we aim to leverage the image-to-video diffusion model (SVD) for keyframe interpolation tasks, eliminating the multiple noise injections or model fine-tuning. Notably, our key innovation lies in the sequential sampling of the temporally forward path of xt and the temporally backward path of := flip(xt) by integrating single re-noising step between them: xt1,cstart = xt1(xt; σt, cstart), xt,cstart = xt1,cstart + t1 = xt1(x (cid:1) xt1 = (cid:0)x . t,cstart (cid:113) t1ϵ, σ2 σ2 ; σt, cend), (7) (8) (9) (10) This approach effectively constrains the sampling process for bounded generation between the start frame (Istart) and the end frame (Iend). As depicted in Fig. 3 (b), our method seamlessly connects the temporally forward and backward paths so that the sampling trajectory stays within the SVD manifold, resulting in smooth and coherent transitions throughout the interpolation process. 4 Preprint. Figure 3: Comparison of diffusion sampling paths. (a) Existing methods encounter off-manifold issues due to the averaging of two sample points. (b) In contrast, our bidirectional sampling sequentially connects the temporally forward and backward paths, ensuring that the process remains within the manifold. 3.2 ADDITIONAL MANIFOLD GUIDANCES We further employ recent advanced manifold guidance techniques to enhance the interpolation performance of the bidirectional sampling. First, we introduce additional frame guidance using DDS (Chung et al., 2023). Then, we replace traditional CFG (Ho & Salimans, 2021) with CFG++ (Chung et al., 2024) to mitigate the off-manifold issue of CFG in the original implementation of SVD (Blattmann et al., 2023a). Last frame guidance with DDS. DDS (Chung et al., 2023) synergistically combines the diffusion sampling and Krylov subspace methods (Liesen & Strakos, 2013) such as the conjugate gradient (CG) method, guaranteeing the on-manifold solution of the following optimization problem: ℓ(x) := A(x)2, min xM (11) where is the linear mapping, is the condition, and represents the clean manifold of the diffusion sampling path. Here, we leverage the DDS framework to guide start-frame-conditioned sampling path of SVD to startand end-frame-conditioned sampling path. Specifically, for the temporally forward path, conditioned on the start frame (Istart), we take the DDS step on denoised estimate ˆxcstart(xt) to guide the last frame of ˆxcstart(xt) to align with cend. For the temporally backward path, conditioned on the end frame (Iend), we take the DDS step on denoised estimate ˆx t) to guide the last frame of ˆx t) to align with cstart. In practice, we set A(x) := xlast as last frame extractor, as matched condition which are cend for temporally forward path and cstart for temporally backward path: (x (x cend cend xcstart := arg min ˆxcstart +Kl cend A(x)2, cend := arg min ˆx +Kl cend cstart A(x)2, (12) where Kl is the l-th order Krylov subspace, in which Krylov subspace methods seek an approximate solution. By leveraging this DDS framework, we effectively guide the sampling process toward path conditioned by both the start and end frames, which is particularly effective for keyframe interpolation. Better Image-Video alignment with CFG++. Recent advances of CFG++ (Chung et al., 2024) tackles the inherent off-manifold issue in CFG (Ho & Salimans, 2021). Specifically, CFG++ mitigates this undesirable off-manifold issue using the unconditional score instead of the conditional score in re-noising process of CFG. By using the unconditional score, CFG++ can overcome the off-manifold phenomena in CFG-generated samples, resulting in better text-image alignment for text-to-image generation tasks. While SVD replaces text embedding with CLIP image embedding, we can still use CFG++ for image-to-video diffusion models to ensure better image-video alignment. Specifically, after applying CFG++ into SVD sampling framework, the Euler-step of SVD (3) now reads: xt1(xt; σt, c) := ˆxc(xt) + σt1 σt (xt ˆx(xt)), (13) 5 Preprint. where the last term in (3) is replaced by ˆx(xt). In practice, we apply DDS guidance before CFG++ update, so ˆxc(xt) in (13) should be replaced with xc(xt) as in (12). We experimentally found that incorporating DDS and CFG++ guidance improves the interpolation performance of bidirectional sampling. The overall sampling method effectively steers the SVD sampling path to perform keyframe interpolation in an on-manifold manner, fully leveraging the generation capabilities of SVD. The detailed algorithm is provided in Algorithm 1. The vanilla bidirectional sampling can be implemented by removing DDS guidance (orange) and replacing the CFG++ update (blue) with traditional CFG update. The detailed algorithm of the vanilla bidirectional sampling is provided in Appendix A. t=1 Algorithm 1 Bidirectional sampling (Full) Require: xT (0, I), Istart, Iend, {σt}T 1: cstart, cend encode(Istart, Iend) 2: for = : 1 do 3: 4: 5: ˆxcstart, ˆx Dθ(xt; σt, cstart) xcstart DDS( ˆxcstart, cend) xt1,cstart xcstart + σt1 σt (cid:113) xt,cstart xt1,cstart + flip(xt,cstart) Dθ(x ˆx cend t1 xt1 flip(x , ˆx DDS( ˆx t,cstart , cstart) (x cend + σt1 σt t1) t,cstart t,cstart cend cend 7: 8: 9: 10: 11: 12: end for 13: return x0 ; σt, cend) 6: (xt ˆx) σ2 σ2 t1ϵ ˆx ) EDM denoised estimate with cstart DDS guidance for end-frame matching CFG++ update Re-noise Time reverse EDM denoised estimate with cend DDS guidance for start-frame matching CFG++ update Time reverse"
        },
        {
            "title": "4 EXPERIMENTAL RESULTS",
            "content": "4.1 EXPERIMENTAL SETTING Dataset. The high-resolution (1080p) video datasets used for evaluation are sourced from the DAVIS dataset (Pont-Tuset et al., 2017) and the Pexels dataset1. For the DAVIS dataset, we preprocessed 100 videos into 100 video-keyframe pairs, with each video consisting of 25 frames. This dataset includes wide range of large and varied motions, such as surfing, dancing, driving, and airplane flying. For the Pexels dataset, we collected 45 videos, primarily featuring scene motions, natural movements, directional animal movements, and sports actions. We used the first and last frames from each video as keyframes for our evaluation. Implementation Details. For the sampling process, we used the Euler scheduler with 25 timesteps for both forward and backward sampling. The motion bucket ID was fixed at 127, and the decoding frame number was set to 4 due to memory limitations on an NVIDIA RTX 3090 GPU. All other parameters followed the default settings from SVD. Since micro-condition fps is sensitive to the data, we applied lower fps for cases with large motion and higher fps for cases with smaller motion. While both DDS and CFG++ generally improve the results, the choice between them depends on the specific use case. All evaluations were performed on single NVIDIA RTX 3090. 4.2 COMPARATIVE STUDIES We conducted comparative study with four different keyframe interpolation baselines, including FILM (Reda et al., 2019), conventional flow-based frame interpolation method, and three frame interpolation methods based on video diffusion models: TRF (Feng et al., 2024), DynamiCrafter (Xing et al., 2023), and Generative Inbetweening (Wang et al., 2024). We conducted these studies using the official implementations with default values, except for TRF, which has not been open-sourced yet. 1https://www.pexels.com/ 6 Preprint. Figure 4: Qualitative evaluation compared to three baselines: FILM, TRF, and Generative Inbetweening. The start and end frames (I0, I24) are used as keyframes. While FILM encounters difficulties in capturing motion when there is significant discrepancy between the two keyframes, and TRF and Generative Inbetweening experience decline in perceptual quality due to the blurring of objects within the image, our method successfully captures motion while maintaining high fidelity in the generated images. 7 Preprint. Method DAVIS Pexels LPIPS FID FVD LPIPS FID FVD FILM TRF 2 DynamiCrafter Generative Inbetweening Ours (Vanilla) Ours (Vanilla w/ CFG++) 0.2697 0.3102 0.3274 0.2823 0.3031 0.2571 40.241 60.278 46.854 36. 52.452 41.960 833.80 622.16 538.36 490.34 543.31 434.41 0.0821 0.2222 0.1922 0.1523 0.2074 0.1524 25.615 80.618 49.476 40. 63.241 41.347 559.16 880.97 604.20 746.26 717.37 478.35 Ours (Full) 0.2355 35. 399.15 0.1366 37.341 452.34 Table 1: Quantitative evaluation on DAVIS and Pexels datasets. We compared our method against four different baselines and conducted ablation studies to assess the impact of CFG++ and DDS. Ours (Vanilla) refers to the bidirectional sampling method utilizing traditional CFG update without DDS guidance. Ours (Vanilla w/ CFG++) refers to the bidirectional sampling method with CFG++ update, also without DDS guidance. Bold and underline refer to the best and the second best, respectively. Figure 5: Ablation study on the effects of CFG++ and DDS. The inclusion of CFG++ and DDS results in improved perceptual quality in the generated frames. Qualitative evaluation. As illustrated in Fig. 4, our model clearly outperforms the other methods in terms of motion consistency and identity preservation. Other baselines struggle to accurately predict the motion between the two keyframes when there is significant difference in content. For example, in Fig. 4, the first frame shows the tip of an airplane, while the last frame reveals the airplanes body. In this case, FILM fails to produce linear motion path, instead showing the airplanes shape converging toward the middle frame from both end frames, resulting in the airplanes body being disconnected by the 18th frame. While TRF and Generative Inbetweening show sequential movement, the airplanes shape becomes distorted. In contrast, our method preserves the airplanes shape while effectively capturing its gradual motion. Furthermore, it can be observed from the second and third cases from Fig. 4 that our method generates temporally coherent results while semantically adhering to the input frames. In TRF, the shapes of the robot and the dog become blurred due to the denoising paths deviating from the manifold during the fusion process, leading to artifacts in the image. While Generative Inbetweening mitigated this off-manifold issue through temporal attention rotation and model fine-tuning, artifacts still persist. In contrast, our method preserves the shapes of both the robot and the dog, generating frames with strong temporal consistency. Quantitative evaluation. For quantitative evaluation, we used LPIPS (Zhang et al., 2018) and FID (Heusel et al., 2017) to assess the quality of the generated frames, and FVD (Unterthiner et al., 2019) to evaluate the overall quality of the generated videos. As shown in Table 1, our method surpasses the other baselines in terms of fidelity. Moreover, it achieves superior perceptual quality, particularly in scenarios involving dynamic motions (DAVIS), indicating that our approach effectively addresses the issue of deviations from the diffusion manifold, resulting in improved video generation quality. 2Unofficial implementation: https://github.com/YingHuan-Chen/Time-Reversal 8 Preprint. Method NFE Train Inference time (s) Frame # Resolution TRF DynamiCrafter Generative Inbetweening Ours 120 50 300 443 42 1,222 195 25 16 25 25 1024 576 512 320 1024 576 1024 576 Table 2: comprehensive comparison of our method with other diffusion-based approaches. Figure 6: Effect of CFG++ guidance scale. The rows, from top to bottom, correspond to the CFG++ scales of 0.6, 0.8, and 1.0. Metrics LPIPS FID FVD 0.6 0.8 1.0 525.36 52.5059 0. 424.03 40.4968 0.2394 399.15 35.6594 0.2355 Table 3: Quantitative analysis on CFG++ guidance scale ω. Effective results are obtained at the scale of 1.0. 4.3 COMPUTATIONAL EFFICIENCY We performed comparative studies on the computational cost of diffusion models, as presented in Table 2. In the training stage, DynamiCrafter undergoes additional training with large-scale image-to-video model for the frame interpolation task, while Generative Inbetweening also necessitates SVD model fine-tuning, both of which demands significant computational resources. During the inference stage, both TRF and Generative Inbetweening generate videos in 25 50 steps for each forward and backward direction, with additional noise injection steps that further increase the number of function evaluations (NFE) and inference time. However, our method does not require additional training or fine-tuning and completes the process in just 25 steps per direction, without requiring multiple re-noising. 4.4 ABLATION STUDIES Bidirectional sampling and conditional guidance. The effectiveness of bidirectional sampling can be validated in the vanilla version without any conditional guidance, such as CFG++ or DDS. As demonstrated in Table 1, our vanilla model outperforms TRF across all three metrics, supporting the claim that fusing time-reversed denoising paths leads to off-manifold issues, which our method addresses through bidirectional sampling. In addition, with conditional guidance from CFG++ and DDS, we could achieve even better results and outperform DynamicCrafter and Generative Inbetweening which further train the image-to-video models. This is consistent with Fig. 5, which illustrates that frames generated by TRF exhibit blurry shapes of the golfer and unnecessary camera movement. In contrast, the body shape of the golfer and the golf club are progressively better preserved as additional conditional guidance is incorporated. CFG++ guidance scale. As shown in Fig. 6, at higher CFG++ scales, the semantic information of the input frames is better preserved in the generated intermediate frames, resulting in improved 9 Preprint. Figure 7: Application to keyframe interpolation with various boundary conditions. The end image is identical to the start image. End images b-c represent dynamic boundaries sampled from different time points. fidelity. For instance, while the small person in the first input frame disappears in the intermediate frames at CFG++ scales of 0.6 and 0.8, the person remains visible across all the intermediate frames at scale of 1.0. Additionally, as the CFG++ scale decreases, the blurriness of the chairlift in the output frames gradually worsens. This aligns with the findings presented in Table 3. The LPIPS, FID, and FVD values are lowest at CFG++ scale of 1.0 and highest at scale of 0.6, indicating that CFG++ contributes to improving the perceptual quality of the generated videos. 4.5 IDENTICAL AND DYNAMIC BOUNDS Our method is applicable not only to dynamic bounds, where the start and end frames are different, but also to static bounds, where the start and end frames are identical. As illustrated in Fig. 7, our method successfully generates temporally coherent videos with identical start and end images (a). For example, the wave line also consistently fluctuates with the progression of time. Furthermore, as seen in the fifth and sixth rows of Fig. 7, our method effectively generates intermediate frames based on varying end frames (b and c). Given that the end images of the two rows differ, the resulting intermediate frames are generated accordingly."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We present Video Interpolation using Bidirectional Diffusion Sampler (ViBiDSampler), novel approach for keyframe interpolation that leverages bidirectional sampling and advanced manifold guidance techniques to address off-manifold issues inherent in time-reversal-fusion-based methods. By performing denoising sequentially in both forward and backward directions and incorporating CFG++ and DDS guidance, ViBiDSampler offers reliable and efficient framework for generating high-quality, temporally coherent, and vivid video frames without requiring fine-tuning or repeated re-noising steps. Our method achieves state-of-the-art performance in keyframe interpolation, as evidenced by its ability to generate 25-frame video at high resolution in short processing time. 10 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575, 2023b. Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverse problems using manifold constraints. Advances in Neural Information Processing Systems, 35:2568325696, 2022. Hyungjin Chung, Suhyeon Lee, and Jong Chul Ye. Decomposed diffusion sampler for accelerating large-scale inverse problems. arXiv preprint arXiv:2303.05754, 2023. Hyungjin Chung, Cfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv:2406.08070, 2024. Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. arXiv preprint Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 14721480, 2024. Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael Black, and Xuaner Zhang. Explorative inbetweening of time and space. arXiv preprint arXiv:2403.14611, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633 8646, 2022. Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In European Conference on Computer Vision, pp. 624642. Springer, 2022. Zhilin Huang, Yijie Yu, Ling Yang, Chujun Qin, Bing Zheng, Xiawu Zheng, Zikun Zhou, Yaowei Wang, and Wenming Yang. Motion-aware latent diffusion models for video frame interpolation. arXiv preprint arXiv:2404.13534, 2024. Siddhant Jain, Daniel Watson, Eric Tabellion, Ben Poole, Janne Kontkanen, et al. Video interpolation with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73417351, 2024. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. 11 Preprint. Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang. Ifrnet: Intermediate feature refine network for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19691978, 2022. Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: All-pairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 98019810, 2023. Jorg Liesen and Zdenek Strakos. Krylov subspace methods: principles and analysis. Numerical Mathematics and Scie, 2013. Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation with transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 35323542, 2022. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and arXiv preprint Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pp. 87488763, 2021. Fitsum Reda, Deqing Sun, Aysegul Dundar, Mohammad Shoeybi, Guilin Liu, Kevin Shih, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Unsupervised video interpolation using cycle consistency. In Proceedings of the IEEE/CVF international conference on computer Vision, pp. 892900, 2019. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Advances in neural information processing systems, 35:2337123385, 2022. Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steven Seitz. Generative inbetweening: Adapting image-to-video models for keyframe interpolation. arXiv preprint arXiv:2408.15239, 2024. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 76237633, 2023. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-tovideo generation. arXiv preprint arXiv:2309.15818, 2023a. Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 56825692, 2023b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. 12 EDM denoised estimate with cstart Re-noise Time reverse EDM denoised estimate with cend Time reverse Preprint."
        },
        {
            "title": "A ALGORITHM",
            "content": "Algorithm 2 Bidirectional sampling (Vanilla) Require: xT , Istart, Iend, {σt}T 1: cstart, cend encode(Istart, Iend) 2: for = : 1 do 3: 4: t=1 (xt ˆxcstart) σ2 σ2 t1ϵ ˆxcstart, ˆx Dθ(xt; σt, cstart) xt1,cstart ˆxcstart + σt1 σt (cid:113) xt,cstart xt1,cstart + flip(xt,cstart) t,cstart ˆx cend, ˆx Dθ(x cend + σt1 t1 ˆx σt xt1 flip(x t1) t,cstart (x t,cstart ; σt, cend) ˆx 5: 6: 7: 8: 9: 10: end for 11: return cend) 13 Preprint."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "Figure 8: Additional comparison with baseline methods. 14 Preprint. Figure 9: Additional comparison with baseline methods. 15 Preprint. Figure 10: Additional comparison with baseline methods."
        }
    ],
    "affiliations": [
        "Dept. of Bio & Brain Engineering, KAIST",
        "Kim Jae Chul Graduate School of AI, KAIST"
    ]
}