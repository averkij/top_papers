{
    "paper_title": "Dynamic View Synthesis as an Inverse Problem",
    "authors": [
        "Hidir Yesiltepe",
        "Pinar Yanardag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we address dynamic view synthesis from monocular videos as an inverse problem in a training-free setting. By redesigning the noise initialization phase of a pre-trained video diffusion model, we enable high-fidelity dynamic view synthesis without any weight updates or auxiliary modules. We begin by identifying a fundamental obstacle to deterministic inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and resolve it by introducing a novel noise representation, termed K-order Recursive Noise Representation. We derive a closed form expression for this representation, enabling precise and efficient alignment between the VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions resulting from camera motion, we introduce Stochastic Latent Modulation, which performs visibility aware sampling over the latent space to complete occluded regions. Comprehensive experiments demonstrate that dynamic view synthesis can be effectively performed through structured latent manipulation in the noise initialization phase."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 4 0 0 8 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Dynamic View Synthesis as an Inverse Problem",
            "content": "Hidir Yesiltepe hidir@vt.edu Pinar Yanardag pinary@vt.edu Virginia Tech https://inverse-dvs.github.io/ Figure 1: From real-world complex scenes to AI-generated videos, our method preserves identity fidelity and synthesizes plausible novel views by operating entirely in noise initialization phase."
        },
        {
            "title": "Abstract",
            "content": "In this work, we address dynamic view synthesis from monocular videos as an inverse problem in training-free setting. By redesigning the noise initialization phase of pre-trained video diffusion model, we enable high-fidelity dynamic view synthesis without any weight updates or auxiliary modules. We begin by identifying fundamental obstacle to deterministic inversion arising from zeroterminal signal-to-noise ratio (SNR) schedules and resolve it by introducing novel noise representation, termed K-order Recursive Noise Representation. We derive closed form expression for this representation, enabling precise and efficient alignment between the VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions resulting from camera motion, we introduce Stochastic Latent Modulation, which performs visibility aware sampling over the latent space to complete occluded regions. Comprehensive experiments demonstrate that dynamic view synthesis can be effectively performed through structured latent manipulation in the noise initialization phase."
        },
        {
            "title": "Introduction",
            "content": "Dynamic view synthesis (DVS) [13, 35, 43, 53, 59] from monocular videos [10, 11, 26, 60, 48, 2, 15, 58, 55] is computer vision task that aims to generate new, dynamic perspectives of scene using only single video as input. This process involves predicting how scene would appear from angles not captured in the original footage, requiring the inference of depth, occluded regions, and unseen details. In the film industry, DVS can revolutionize post-production by enabling virtual camera sweeps through scene or producing additional shots from different angles, eliminating the need for expensive reshoots. In robotics, it supports advanced perception systems by generating synthetic Preprint. Under review. viewpoints that train algorithms for navigation, manipulation, and active perception tasks in complex environments. Historically, DVS has relied on explicit 3D reconstruction methods, such as Neural Radiance Fields (NeRF) [34], its dynamic extension D-NeRF [40], K-Planes [9], and 3D/4D Gaussian Splatting [22, 52]. These seminal volumetric and point-based approaches model scenes as continuous representations or point configurations. However, they impose strict prerequisites: multi-view supervision, computationally intensive per-scene optimization, and precise camera calibration. Recently, paradigm shift has emerged, leveraging video-diffusion [5, 57, 49, 4, 29] priors to address these limitations. Diffusion models appeal for DVS because they implicitly capture geometry and appearance in their latent space, inherently provide temporal consistency, and bypass the need for explicit 3D modeling. Under this diffusion paradigm, two dominant approaches have surfaced. The first involves attention-sharing architectures, as seen in Generative Camera Dolly [48], TrajectoryAttention [55], TrajectoryCrafter [58], and ReCamMaster [2]. These methods integrate camera-aware branches such as pixel-trajectory attention, dual streams, or 3D attention layers to enable fine-grained camera control. However, they require additional architectural modules and extensive retraining on large synthetic datasets like Unreal Engine 5 [8] or Kubric [14], leading to domain-gap issues when applied to natural settings. The second recipe employs LoRA-based fine-tuning, exemplified by ReCapture [60] and Reangle-A-Video [20]. These approaches attach spatial and temporal Low-Rank Adaptations (LoRAs) [18, 7, 42], and perform per-video fine-tuning leveraging masked losses. Across both strategies, shared limitations persist: they require updating backbone parameters or adding layers, depend on curated synthetic data or video-specific fine-tuning, and suffer from pitfalls when the inversion process misaligns with the models forward noise schedule. These constraints underscore critical open question: Can we achieve 6-DoF monocular DVS without any weight updates, auxiliary modules, or synthetic pre-training purely by manipulating the initial noise fed into video-diffusion model? In this work, we pioneer fundamentally different approach to DVS from monocular videos. We demonstrate that by solely manipulating the initial noise fed into video diffusion model, we can achieve state-of-the-art performance without any weight updates or auxiliary modules. This novel perspective shifts the focus from architectural redesign or resource-intensive retraining to efficient noise design, distinguishing our method from existing approaches. Our approach is centered around two key innovations. First, we identify and formalize the Zero-Terminal SNR Collapse Problem, which arises when training schedules enforce zero signal-to-noise ratio at the terminal timestep, causing collapse in information content and obstructing deterministic inversion. To resolve this, we propose the K-order Recursive Noise Representation (K-RNR), which recursively refines the initial noise in alignment with the models forward schedule, enabling stable and faithful reconstruction of the original scene. We derive closed-form expressions for this refinement process and stabilize generation with an adaptive variant that prevents scale explosion. Second, to address the synthesis of newly visible content due to camera motion, we introduce Stochastic Latent Modulation, visibility-aware sampling mechanism that directly completes occluded latent regions using contextaware latent permutations. This enables plausible scene completion in the noise initialization phase. Together, these components form unified framework that achieves high-fidelity reconstruction and physically consistent view synthesis from monocular input. Our contributions can be summarized as follows: We identify and formalize the Zero-Terminal SNR Collapse Problem, showing that while zero terminal SNR schedules improve generation quality, they inherently break injectivity, preventing deterministic inversion and hindering faithful reconstruction. We propose K-order Recursive Noise Representation (K-RNR) to resolve the obstruction caused by the Zero-Terminal SNR Problem. By defining recursive refinement relation between the VAE-encoded latent and the positive-SNR DDIM-inverted latent, we derive closed-form noise expression, enabling high-fidelity reconstructions of original scenes. We introduce Stochastic Latent Modulation (SLM), novel latent-space completion mechanism that infers content for newly visible regions by performing visibility-aware sampling and contextual latent permutation, enabling physically plausible synthesis in occluded areas without modifying the model. 2 Figure 2: Approaches to Zero-Terminal SNR Collapse Problem. (b) Low strength preserves source content but renders unseen regions as black. (c) High strength improves propagation into unseen areas but causes identity drift. (d) DDIM inverted latent as initial noise leads to washed-out, high saturation generation. (f) Our K-RNR (k = 6) with Stochastic Latent Modulation preserves identity and completes newly visible regions with plausible content."
        },
        {
            "title": "2 Related Work",
            "content": "This section reviews prior research in two closely related areas relevant to our work. The first is novel view synthesis for dynamic scenes, and the second is video-to-video translation with camera control. Novel View Synthesis for Dynamic Scenes. Novel view synthesis seeks to generate unseen perspectives from available visual data, with substantial advancements driven by neural rendering. For static scenes, Neural Radiance Fields (NeRF) [34] and 3D Gaussian Splatting [22] provide detailed 3D reconstructions. Dynamic scene extensions, such as D-NeRF [40], K-Planes [9], HexPlane [6], and HyperReel [1], depend on synchronized multi-view inputs, which are often impractical for casual settings. Monocular video methods, including Neural Scene Flow Fields [27], DynIBaR [28], Robust Dynamic Radiance Fields [32], and Dynamic View Synthesis [10], utilize depth-based warping or neural encodings but face challenges with occlusions and extrapolation beyond input views. Recent approaches, such as 4D Gaussian Splatting [52], Dynamic Gaussian Marbles [46], and GaussianFlow [12], enhance efficiency with 3D Gaussian representations, yet require robust multi-view data or significant input camera motion, restricting broader applicability. Video-to-Video Translation with Camera Control. Early video-to-video translation efforts, such as World Consistent Video to Video [33] and Few Shot Video to Video [51], targeted tasks like outpainting. Generative Camera Dolly [48] trains on synthetic multiview videos from Kubric, but domain gaps limit generalizability in natural settings. ReCapture [60] uses two stage pipeline that first generates an anchor video with CAT3D [13] multiview diffusion or point cloud rendering, followed by refinement using spatial and temporal LoRA modules. However, per video optimization hampers scalability. Methods like DaS [15] and GS DiT [3] enforce 4D consistency through 3D point tracking with tools such as SpatialTracker [54] and Cotracker [21], though tracking inaccuracies in complex scenes limit effectiveness. ReCamMaster [2] proposes generative rerendering within pre-trained text to video models using with frame-conditioning attention sharing mechanism using large Unreal Engine 5 [8] dataset, but struggles with high computational cost as the number of tokens are doubled in the 3D attention mechanism. TrajectoryCrafter [58] decouples view transformation and content generation using dual stream diffusion model conditioned on point clouds and source videos, but remains constrained large camera shifts. Trajectory Attention [55] applies pixel trajectory attention for camera motion control and long range consistency, however, it is sensitive to sparse or fast motions and lacks full 3D consistency."
        },
        {
            "title": "3 Background",
            "content": "In this section, we review the base video diffusion model in 3.1, followed by common noise initialization strategies used in current video models for I2V and V2V applications in 3.2. 3.1 Base Video Diffusion Model Following prior works [58, 15], our work builds upon the I2V variant of the CogVideoX [57]. CogVideoX is transformer-based video diffusion model operating in latent space with 4 temporal and 8 spatial compression. The model takes single RGB image RHW3 as input and generates video RFHW3 with frames. The image is first encoded by 3D VAE [23] 3 Figure 3: Overview of Our Method. (Left) We lift monocular video into dynamic 3D point cloud and render novel views under target camera trajectories, revealing unseen regions. (Right) Our method synthesizes coherent outputs by initializing noise with K-order Recursive Noise Representation, and Stochastic Latent Modulation without modifying the video model. into spatial latent zimg of size it is broadcast along the temporal dimension and concatenated with tensor x0 of size the channel dimension, yielding the initial noisy input xinit of size 8 , with = 16. To extend this representation across time, 4 1 zero latents, forming 8 . Finally, x0 is concatenated with noise tensor ϵ (0, I) along 8 for the I2V task. 4 2C 4 8 8 8 3.2 Noise Initialization Strategies Current video generation models [50, 24, 49, 57] for Image-to-Video (I2V) and Video-to-Video (V2V) tasks typically employ specific noise initialization strategies. These strategies can be broadly categorized into two main groups: deterministic inversion and schedule-consistent interpolation. Deterministic Inversion. In models such as ModelScope [50], the network is conditioned on discrete sequence of timesteps {t = 0, . . . , }, with each timestep associated with strictly positive cumulative signal coefficient αt > 0. In this setting, the clean latent representation can be deterministically mapped to the noise manifold using DDIM Inversion [45]. Schedule-consistent Interpolation. In contrast, standard DDIM inversion is not directly applicable when the network is conditioned on continuous sequence of timesteps, as in models like SVD [4]. In such cases, the initial noisy latent is initialized as xinit = x0 + γ ϵ, where γ is noise augmentation parameter that controls the strength of the initial image perturbation. In the Flow Matching-based [31] video model HunyuanVideo [24], the initial noisy latent at discrete timestep {0, . . . , } is given by xinit = ϵ + (1 t) x0 for I2V applications. Similarly, in Wan [49], another Flow Matching model, the noise initialization is defined as xt = σt ϵ + (1 σt) x0, where σt is schedule-dependent weighting factor. CogVideoX [57] is trained with zero terminal signal-to-noise ratio (SNR), which makes DDIM inversion not directly applicable as we discuss in 4.1. In V2V translation tasks, it initializes the noisy latent as xinit = 1 αtϵ with signal-to-noise-ratio SNR(t) = at 1at αtx0 + ."
        },
        {
            "title": "4 Methodology",
            "content": "Dynamic view synthesis involves simultaneously (1) preserving scene fidelity and (2) completing newly visible regions as the camera moves. This requires not only faithfully reconstructing identities and actions over time but also plausibly synthesizing previously unseen regions. To address the former, we first define the zero terminal SNR collapse problem, which reveals the incompatibility between deterministic inversion and schedule-consistent interpolation in models like CogVideoX trained with zero terminal SNR (4.1). We resolve this with K-order Recursive Noise Representation , which enables effective use of DDIM-inverted latents in such settings (4.2). To address the latter, we 4 Figure 4: K-RNR Analysis (a) Cosine similarity between ϵ(k) and VAE-encoded latent x0. (b) For increasing values, the mean and (c) the variance of ϵ(k) explodes. propose stochastic latent modulation strategy that infers unseen regions resulting from camera motion (4.4). 4.1 Zero Terminal SNR Collapse We begin our discussion by identifying key issue that hinders the direct use of DDIM-inverted latents during the noise initialization phase under zero-terminal SNR noise schedules. Lin et al. [30] argue that noise schedules should enforce zero SNR at the final timestep and that sampling should always start from = to ensure alignment between diffusion training and inference. Based on this principle, CogVideoX [57] adopts zero terminal SNR during training following the noise schedule used in [41]. While this setup improves generation quality and ensures consistency between training and inference, we show that it causes breakdown in injectivity. Proposition 4.1. Let {αt}T αt = (cid:81)t forward diffusion map t=0 be variance-preserving noise schedule with cumulative products s=1(1 βs), such that the schedule enforces zero terminal SNR with αT = 0. Define the ΦT (x0, ϵ) = Then, for every pair of latents x0, ϵ (0, I). αT x0 + 1 αT ϵ, 0 Rd and every noise sample ϵ, ΦT (x0, ϵ) = ΦT (x 0, ϵ) = ϵ. Hence ΦT (, ϵ) is not injective in x0. Consequently, deterministic inversion methods such as DDIM inversion cannot uniquely recover x0 from xT . Proposition 4.1 implies that noise schedule with zero terminal SNR forces the schedule-consistent latent at the last time step to be xT = αT x0 + 1 αT ϵ = ϵ, ϵ (0, I), which collapses to pure noise because αT = 0. No information from the original frame x0 survives, so the resulting video-to-video translation cannot remain aligned with the source content. common workaround is to begin sampling from an earlier index < for which αt > 0. However, it shortens the diffusion trajectory and therefore limits translation diversity, which is an important component of dynamic view synthesis. As shown in Fig.2(b), this also results in the reconstruction of regions that are unseen after camera transformation. Even when αt is very small but non-zero, the stochastic term ϵ introduces perturbations that accumulate during generation and ultimately lead to identity drift as demonstrated in Fig.2(c). Figure 5: Expected Norm Deviation 4.2 K-order Recursive Noise Representation (K-RNR) An alternative workaround to the zero-terminal SNR collapse problem is to perform DDIM inversion with positive terminal SNR, allowing the resulting latent to initialize the diffusion process for downstream tasks. However, as shown in Fig.2(d), this approach still results in images with washed-out appearance. We attribute this issue to mismatch between the scale of the expected initial noise and that 5 1 αt ϵinv, evaluated produced by schedule-consistent interpolation, given by xinit = at = 0.95T . This discrepancy is visualized in Fig.5 along the = 1 axis. Moreover, applying normalization or standardization to the xinit introduces trajectory drift, leading to degraded results, as demonstrated in Supplementary Material. αt x0 + Given the limitations of existing workarounds for the zero-terminal SNR collapse problem, we propose new noise initialization mechanism, K-order Recursive Noise Representation , which aligns deterministic inversion with schedule-consistent interpolation. In this formulation, we treat the VAE-encoded latent x0 as the pivot latent, and define the initial noise as xinit = ϵ(k). Throughout the paper, we use superscripts enclosed in parentheses to denote recursion order, while superscripts without parentheses indicate exponentiation. Proposition 4.2. Let x0 Rd be the pivot latent and let αt > 0 denote the cumulative signal coefficient at timestep t. Define the recursive noise initialization by: and for > 1, ϵ(1) = αt x0 + 1 αt ϵinv, ϵ(k) = αt x0 + 1 αt ϵ(k1). Then, for discrete recursion depth N>0, the closed-form expression for ϵ(k) is: ϵ(k) = (cid:32) (cid:88) i=1 (cid:0) αt 1 αt (cid:1)i1 (cid:33) x0 + (cid:0) 1 αt (cid:1)k ϵinv. Which can be generalized to continuous recursion depth R>0 as: (cid:32) αt ϵ(k) = 1 (cid:0) 1 1 αt 1 αt (cid:1)k (cid:33) x0 + (cid:0) 1 αt (cid:1)k ϵinv. (1) (2) Refer to the Appendix for the proofs of Eq.(1-2). By treating x0 as pivot latent and recursively updating the noise latent ϵ(i), the resulting initialization xinit = ϵ(k) becomes increasingly aligned with the structure of x0. We quantify the alignment by measuring the cosine similarity between x0 and ϵ(k), as shown in Fig.4(a). To isolate the effect of the inverted latent ϵinv, we initialize the recursion with ϵ(1) = 1 αt ϵ, where ϵ (0, I), and apply the recursive formulation with discrete depth as described in Proposition 4.2. As increases, the similarity steadily improves, indicating that K-RNR progressively enhances structural fidelity by injecting more of the original latent structure into the initialized noise. αt x0 + Importantly, this growing similarity is not the only factor contributing to improved reconstruction quality. As shown in Fig.5, the expected scale of ϵ(k) also becomes better aligned with the reference distribution scale as increases, up to certain threshold. This alignment is achieved without applying explicit normalization or standardization which results in high saturation generations. However, K-RNR on its own suffers from exploding mean and variance, as demonstrated in Fig.4(bc). This indicates that as the recursion order increases, the scale In practice this of the initialized noise grows rapidly. problem leads to high contrast outputs with exploded RGB colors in the generated video. To address this issue, we introduce Adaptive K-RNR, which stabilizes the recursion by incorporating scale information from Specifically, given an intermediate recursion step. total recursion depth k, we select an intermediate index δ {1, . . . , k}, compute the intermediate noise representation ϵ(δ), and apply xinit = AdaIN (cid:2)ϵ(k), ϵ(δ)(cid:3). This operation preserves the structural benefits of K-RNR while suppressing the scale explosion that leads to visual artifacts. 6 Figure 6: Adaptive K-RNR Figure 7: Stochastic Latent Modulation Motivation. To evaluate the models capacity for physical plausibility in unseen regions, we modify the rendered input with occlusion-filling strategies. (a) Camera motion trajectory. (b) Original render frame. (c) Occluded regions are filled by repeating background patch. (d) Resulting frame generated by combining the filled render with ϵinv using K-RNR, demonstrating plausible yet artifact-prone content synthesis in unseen areas. 4.3 Conditioning on Camera Information Following prior works [58, 60, 20, 59, 55, 15], we incorporate explicit camera conditioning into our framework to enable precise control over novel view synthesis. Given source video = {Ii}n i=1, where each frame Ii RCHW, we first estimate sequence of depth maps = {Di}n i=1 using monocular depth prediction models, with each Di RHW. Using camera intrinsics R33, we lift each RGB-D pair (Ii, Di) into point cloud Pi R3(HW) via unprojection function Π1(), forming dynamic point cloud sequence = {Pi}n i=1: Pi = Π1(Ii, Di, K), where Π1 denotes inverse projection from 2D image space to 3D camera space. Next, we define i=1, where each Ti R44 represents the desired relative set of target camera poses = {Ti}n transformation from the source view. Using these poses, we render novel view sequence = {I i=1 from the transformed point clouds via forward projection Π(): i}n (3) = Π(Ti Pi, K), where Π is the standard perspective projection from 3D points to the image plane. In addition to the rendered novel views I, we generate corresponding visibility masks = {M i=1 to capture occluded or out-of-frame regions resulting from the new camera trajectory. i}n (4) 4.4 Stochastic Latent Modulation (SLM) Having addressed the fidelity aspect of dynamic view synthesis, we now turn to the second core requirement: completing regions that become newly visible as the camera moves. As shown in Fig.3, we apply DDIM inversion to videos rendered under novel camera trajectories and interpolate between the VAE-encoded latent x0 and the DDIM-inverted latent ϵinv using Adaptive K-RNR. However, regions that are occluded in the rendered input remain occluded in both x0 and ϵinv, causing these areas to be regenerated as black in the output. To investigate this limitation, we examine whether the base model possesses meaningful physical understanding of the scene that allows it to plausibly infer content in unseen regions. We conduct an analysis on 100 randomly sampled videos from the OpenVid dataset [36], with particular focus on cases where the input render video lies outside the training distribution or violates basic physical realism. The central question is whether the model can still produce outputs that are plausible and consistent with the rules of the physical world. Although unseen areas are also encoded occluded in the inverted latent, we keep ϵinv unchanged, as it retains semantic cues due to attention across visible tokens during the forward trajectory. Instead, we modify the rendered frames by experimenting with different occlusion-filling strategies. As illustrated in Fig.7(c), one approach involves repeating background patch across the occluded regions. When passed through the 3D VAE and combined with ϵinv through K-RNR, this leads to plausible propagation of visual information into previously unseen areas, as shown in Fig.7(d) with visible visual artifacts. Motivated by this discovery, we propose Stochastic Latent Modulation, where instead of completing unseen regions at the input level, we perform stochastic modulation directly in the latent space. Specifically, given binary occlusion mask {0, 1}BFCHW, where = 1 indicates occluded regions, and depth-based background mask {0, 1}BFCHW, where = 1 7 Figure 8: Qualitative Comparison. K-RNR with SLM better preserves subject identity and ensures that synthesized regions remain consistent with the original scene. Method Visual Quality Camera Accuracy View Synchronization FID FVD CLIP-T CLIP-F RotErr TransErr Mat. Pix. (K) FVD-V CLIP-V GCD 89.12 TrajectoryAttention 78.91 71.44 DaS 62.77 TrajectoryCrafter 58.12 ReCamMaster 482.73 342.19 201.83 162.67 118.82 Ours 53.15 103.44 28.64 30.53 32.91 34.13 35.02 35.37 91.02 93.67 96.03 97.48 98.89 98.54 3.67 3.09 2.72 2.39 1. 1.31 6.12 5.64 5.21 4.89 4.52 4.33 603.25 620.83 638.77 823.91 863.54 881.43 429.52 310.78 182.41 108.38 82. 75.17 82.45 84.21 86.72 88.36 89.91 92.04 Table 1: Quantitative comparison of visual quality, camera pose accuracy, and view synchronization on 1000 randomly selected samples from the OpenVid-1M [36] dataset. marks background areas, we define visibility-aware sampling mask as = (1 M) which identifies spatial locations that are both visible and lie on background surfaces. We define stochastic permutation operator PS : RBFCHW RBFCHW that samples latent values from positions indicated by and randomly redistributes them to the occluded positions indicated by M. Our ϵinv = PS(ϵinv) where and ϵ are the modulated modulation function is given by x0 = PS(x0), content and noise latents. This operation stochastically fills occluded regions in latent space with contextually relevant signals sampled from visible background areas, enabling the model to synthesize plausible completions aligned with physical scene structure."
        },
        {
            "title": "5 Experiments\nImplementation. Our framework is built on the pretrained CogVideoX-5B-I2V model. Inference\nis performed with 50 steps at a strength of 0.95 to ensure ¯aT > 0. For all quantitative evaluations,\nwe set the classifier-free guidance (CFG) scale to 6.0 and use a recursion order of k = 10 and\nadaptive order of δ = 3. 3D dynamic point clouds are generated using DepthCrafter [19], fol-\nlowing the procedure described in [58]. We apply DDIM inversion with a positive terminal-SNR\nnoise schedule using 30 steps, and adopt v-prediction in all cases. For quantitative evaluations,\nwe use CogVideoX’s modified DDIM sampling method in the reverse trajectory. The output res-\nolution is fixed at 480 × 720, and all experiments are conducted on a single NVIDIA L40 GPU.",
            "content": "Evaluation Set. We construct dataset of 1100 videos to evaluate performance across varying content and motion complexity: 1000 from OpenVid-1M [36], 50 from DAVIS [38], and 50 AI-generated videos. OpenVid-1M provides semantically rich scenes, DAVIS offers highmotion content for testing temporal stability, and AI-generated samples assess generalization to synthetic inputs. Each video is rendered under 10 canonical camera trajectories including translations, pans, tilts, and arcs, to evaluate robustness under diverse viewpoint shifts. 8 Method FID CLIP-T CLIP-V PSNR Random Noise 74.86 DDIM Inversion + K-RNR w.o AS + K-RNR AS 102.54 71.80 61.43 + K-RNR SLM 53. 37.12 19.98 31.25 33.46 35.37 73.74 63.39 86.78 89.12 92. 12.06 5.43 14.99 15.64 16.28 Figure 9: Ablation on K-RNR, Adaptive Scaling, and Stochastic Latent Modulation Method PSNR SSIM LPIPS OpenVid [36] DAVIS [39] Synthetic Mean OpenVid [36] DAVIS [39] Synthetic Mean OpenVid [36] DAVIS [39] Synthetic Mean GCD TrajectoryAttention DaS TrajectoryCrafter ReCamMaster Ours 9.87 10.11 11.37 13.02 15.84 16.28 8.32 9.70 10.14 10.89 11.31 12. 10.57 11.04 12.27 13.94 14.17 14.59 9.58 10.28 11.26 12.61 13.77 14.50 0.212 0.241 0.309 0.428 0.610 0.623 0.191 0.211 0.259 0.306 0.339 0.354 0.227 0.272 0.348 0.501 0.623 0.617 0.210 0.241 0.305 0.411 0.524 0. 0.739 0.685 0.586 0.366 0.421 0.397 0.754 0.708 0.621 0.646 0.588 0.561 0.681 0.618 0.545 0.537 0.517 0.504 0.724 0.670 0.584 0.516 0.508 0.487 Table 2: Quantitative comparison on our curated benchmark. We report PSNR (), SSIM (), and LPIPS (), averaged over 10 canonical camera trajectories per video. Comparison Baselines. We compare our method against five baselines: GCD [48], TrajectoryAttention [55], RecamMaster [2], TrajectoryCrafter [58], and Diffusion-as-Shader (DaS) [15]. GCD and TrajectoryAttention are built on SVD [4], RecamMaster is based on Wan [49], while TrajectoryCrafter, DaS, and our method are based on CogVideoX. Evaluation Metrics. We evaluate our method for camera pose accuracy, source-target synchronization, and visual quality. For camera accuracy, we use GLOMAP [37] to extract estimated camera trajectories and report rotation and translation errors (RotErr, TransErr) following [16, 2]. Synchronization is measured using GIM [44] by counting matched pixels with high confidence (Mat. Pix.), along with FVD-V [56] and CLIP-V [25], which compute CLIP similarity between source and target frames at corresponding timestamps. Visual quality is evaluated using FID [17], FVD [47], CLIP-T, and CLIP-F, capturing fidelity, text alignment, and temporal consistency, respectively. We additionally compute the full reference metrics PSNR, SSIM, and LPIPS on the OpenVid-1M, DAVIS, and Sora-generated videos [5] to quantify per-frame visual fidelity with respect to ground truth frames. Main Results. As reported in Table 1, our method achieves state-of-the-art performance across all quantitative evaluation axes, encompassing visual fidelity, camera pose accuracy, and view synchronization. The results demonstrate that our framework consistently preserves semantic content and visual coherence while maintaining accurate geometric alignment under camera transformations. Compared to existing baselines, our approach yields improved consistency across frames and more precise reconstruction of dynamic scenes, validating the effectiveness of our noise-space formulation. Furthermore, Table 2 reports full-reference metrics, where our method exhibits robust reconstruction quality across diverse datasets and camera trajectories, further confirming its generalizability and resilience under varying content complexity and motion dynamics. We show identity preservation quality of our method and the baselines in Fig.8. Our framework produces visually coherent results under various viewpoints and demonstrates strong temporal alignment with the source footage. For video samples, please refer to the Supplementary Material. Ablation Studies. Table 9 shows the impact of our proposed methods: K-RNR, Adaptive K-RNR, and K-RNR with Stochastic Latent Modulation. Directly using DDIM-inverted latents leads to poor results, often producing oversaturated and washed-out outputs, as seen in Fig. 2(d). Initializing with random noise also results in weak view synchronization. In contrast, our methods significantly improve both view alignment and reconstruction quality, as reflected in PSNR and FID scores."
        },
        {
            "title": "6 Discussion",
            "content": "Limitations and Broader Impact Our method provides training-free framework for generative camera control in real-world videos, making it broadly accessible for creative editing. However, it inherits biases from the base diffusion model which may limit performance in scenes with uncommon objects, or heavy occlusion. Stochastic latent modulation can also produce unstable or incoherent results when large regions become newly visible. The ability to generate realistic synthetic content raises concerns, highlighting the need for future safeguards such as attribution or model auditing. Conclusion In this paper, we introduce training-free framework for dynamic view synthesis from monocular videos. Our key contributions (1) the identification of the Zero-Terminal SNR Collapse Problem, (2) the development of the K-order Recursive Noise Representation for the use of deterministic inversion, and (3) the Stochastic Latent Modulation technique for occlusion-aware scene completion. Together, they enable high-fidelity synthesis of novel views without fine-tuning or architectural changes. Through rigorous theoretical analysis and empirical validation, we demonstrate that structured manipulation of the noise space alone can unlock new capabilities in generative models, offering principled and practical path toward controllable, efficient dynamic scene generation."
        },
        {
            "title": "References",
            "content": "[1] Attal, B., Huang, J.B., Richardt, C., Zollhoefer, M., Kopf, J., OToole, M., Kim, C.: Hyperreel: High-fidelity 6-dof video with ray-conditioned sampling. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1661016620 (2023) [2] Bai, J., Xia, M., Fu, X., Wang, X., Mu, L., Cao, J., Liu, Z., Hu, H., Bai, X., Wan, P., et al.: Recammaster: Camera-controlled generative rendering from single video. arXiv preprint arXiv:2503.11647 (2025) [3] Bian, W., Huang, Z., Shi, X., Li, Y., Wang, F.Y., Li, H.: Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690 (2025) [4] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023) [5] Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., Ng, C., Wang, R., Ramesh, A.: Video generation models as world simulators (2024), https://arxiv.org/abs/2403.17181 [6] Cao, A., Johnson, J.: Hexplane: fast representation for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 130141 (2023) [7] Chefer, H., Zada, S., Paiss, R., Ephrat, A., Tov, O., Rubinstein, M., Wolf, L., Dekel, T., Michaeli, T., Mosseri, I.: Still-moving: Customized video generation without customized video data. ACM Transactions on Graphics (TOG) 43(6), 111 (2024) [8] Epic Games: Unreal unreal-engine-5 (2022), accessed: 2025-05-03 engine 5. https://www.unrealengine.com/en-US/ [9] Fridovich-Keil, S., Meanti, G., Warburg, F.R., Recht, B., Kanazawa, A.: K-planes: Explicit radiance fields in space, time, and appearance. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1247912488 (2023) [10] Gao, C., Saraf, A., Kopf, J., Huang, J.B.: Dynamic view synthesis from dynamic monocular video. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 57125721 (2021) [11] Gao, H., Li, R., Tulsiani, S., Russell, B., Kanazawa, A.: Monocular dynamic view synthesis: reality check. Advances in Neural Information Processing Systems 35, 3376833780 (2022) [12] Gao, Q., Xu, Q., Cao, Z., Mildenhall, B., Ma, W., Chen, L., Tang, D., Neumann, U.: Gaussianflow: Splatting gaussian dynamics for 4d content creation. arXiv preprint arXiv:2403.12365 (2024) [13] Gao, R., Holynski, A., Henzler, P., Brussee, A., Martin-Brualla, R., Srinivasan, P., Barron, J.T., Poole, B.: Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314 (2024) [14] Greff, K., Belletti, F., Beyer, L., Doersch, C., Du, Y., Duckworth, D., Fleet, D.J., Gnanapragasam, D., Golemo, F., Herrmann, C., et al.: Kubric: scalable dataset generator. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 37493761 (2022) [15] Gu, Z., Yan, R., Lu, J., Li, P., Dou, Z., Si, C., Dong, Z., Liu, Q., Lin, C., Liu, Z., et al.: Diffusion as shader: 3d-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847 (2025) [16] He, H., Xu, Y., Guo, Y., Wetzstein, G., Dai, B., Li, H., Yang, C.: Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101 (2024) [17] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems 30 (2017) [18] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al.: Lora: Low-rank adaptation of large language models. ICLR 1(2), 3 (2022) [19] Hu, W., Gao, X., Li, X., Zhao, S., Cun, X., Zhang, Y., Quan, L., Shan, Y.: Depthcrafter: Generating consistent long depth sequences for open-world videos. arXiv preprint arXiv:2409.02095 (2024) [20] Jeong, H., Lee, S., Ye, J.C.: Reangle-a-video: 4d video generation as video-to-video translation. arXiv preprint arXiv:2503.09151 (2025) [21] Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., Rupprecht, C.: Cotracker: It is better to track together. In: European Conference on Computer Vision. pp. 1835. Springer (2024) [22] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph. 42(4), 1391 (2023) [23] Kingma, D.P., Welling, M., et al.: Auto-encoding variational bayes (2013) [24] Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., et al.: Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603 (2024) [25] Kuang, Z., Cai, S., He, H., Xu, Y., Li, H., Guibas, L.J., Wetzstein, G.: Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems 37, 1624016271 (2024) [26] Li, Z., Chen, Z., Li, Z., Xu, Y.: Spacetime gaussian feature splatting for real-time dynamic view synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 85088520 (2024) [27] Li, Z., Niklaus, S., Snavely, N., Wang, O.: Neural scene flow fields for space-time view synthesis of dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 64986508 (2021) [28] Li, Z., Wang, Q., Cole, F., Tucker, R., Snavely, N.: Dynibar: Neural dynamic image-based rendering. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 42734284 (2023) [29] Lin, B., Ge, Y., Cheng, X., Li, Z., Zhu, B., Wang, S., He, X., Ye, Y., Yuan, S., Chen, L., et al.: Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131 (2024) [30] Lin, S., Liu, B., Li, J., Yang, X.: Common diffusion noise schedules and sample steps are flawed. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 54045411 (2024) [31] Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., Le, M.: Flow matching for generative modeling. arXiv preprint arXiv:2210.02747 (2022) [32] Liu, Y.L., Gao, C., Meuleman, A., Tseng, H.Y., Saraf, A., Kim, C., Chuang, Y.Y., Kopf, J., Huang, J.B.: Robust dynamic radiance fields. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1323 (2023) [33] Mallya, A., Wang, T.C., Sapra, K., Liu, M.Y.: World-consistent video-to-video synthesis. In: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VIII 16. pp. 359378. Springer (2020) [34] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM 65(1), 99106 (2021) [35] Müller, N., Schwarz, K., Rössle, B., Porzi, L., Bulò, S.R., Nießner, M., Kontschieder, P.: Multidiff: Consistent novel view synthesis from single image. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1025810268 (2024) 11 [36] Nan, K., Xie, R., Zhou, P., Fan, T., Yang, Z., Chen, Z., Li, X., Yang, J., Tai, Y.: Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371 (2024) [37] Pan, L., Baráth, D., Pollefeys, M., Schönberger, J.L.: Global structure-from-motion revisited. In: European Conference on Computer Vision. pp. 5877. Springer (2024) [38] Pont-Tuset, J., Perazzi, F., Caelles, S., Arbeláez, P., Sorkine-Hornung, A., Van Gool, L.: The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675 (2017) [39] Pont-Tuset, J., Perazzi, F., Caelles, S., Arbeláez, P., Sorkine-Hornung, A., Gool, L.V.: The 2017 davis challenge on video object segmentation. arXiv: Computer Vision and Pattern Recognition (2017) [40] Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-nerf: Neural radiance fields for dynamic scenes. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1031810327 (2021) [41] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1068410695 (2022) [42] Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2250022510 (2023) [43] Sargent, K., Li, Z., Shah, T., Herrmann, C., Yu, H.X., Zhang, Y., Chan, E.R., Lagun, D., Fei-Fei, L., Sun, D., et al.: Zeronvs: Zero-shot 360-degree view synthesis from single image. arXiv preprint arXiv:2310.17994 (2023) [44] Shen, X., Cai, Z., Yin, W., Müller, M., Li, Z., Wang, K., Chen, X., Wang, C.: Gim: Learning generalizable image matcher from internet videos. arXiv preprint arXiv:2402.11095 (2024) [45] Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502 (2020) [46] Stearns, C., Harley, A., Uy, M., Dubost, F., Tombari, F., Wetzstein, G., Guibas, L.: Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In: SIGGRAPH Asia 2024 Conference Papers. pp. 111 (2024) [47] Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly, S.: Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717 (2018) [48] Van Hoorick, B., Wu, R., Ozguroglu, E., Sargent, K., Liu, R., Tokmakov, P., Dave, A., Zheng, C., Vondrick, C.: Generative camera dolly: Extreme monocular dynamic novel view synthesis. In: European Conference on Computer Vision. pp. 313331. Springer (2024) [49] Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., et al.: Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314 (2025) [50] Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., Zhang, S.: Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571 (2023) [51] Wang, T.C., Liu, M.Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot video-to-video synthesis. arXiv preprint arXiv:1910.12713 (2019) [52] Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang, X.: 4d gaussian splatting for real-time dynamic scene rendering. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2031020320 (2024) 12 [53] Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan, P.P., Verbin, D., Barron, J.T., Poole, B., et al.: Reconfusion: 3d reconstruction with diffusion priors. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 2155121561 (2024) [54] Xiao, Y., Wang, Q., Zhang, S., Xue, N., Peng, S., Shen, Y., Zhou, X.: Spatialtracker: Tracking any 2d pixels in 3d space. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2040620417 (2024) [55] Xiao, Z., Ouyang, W., Zhou, Y., Yang, S., Yang, L., Si, J., Pan, X.: Trajectory attention for fine-grained video motion control. arXiv preprint arXiv:2411.19324 (2024) [56] Xie, Y., Yao, C.H., Voleti, V., Jiang, H., Jampani, V.: Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. arXiv preprint arXiv:2407.17470 (2024) [57] Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al.: Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 (2024) [58] YU, M., Hu, W., Xing, J., Shan, Y.: Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. arXiv preprint arXiv:2503.05638 (2025) [59] Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.T., Shan, Y., Tian, Y.: Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048 (2024) [60] Zhang, D.J., Paiss, R., Zada, S., Karnad, N., Jacobs, D.E., Pritch, Y., Mosseri, I., Shou, M.Z., Wadhwa, N., Ruiz, N.: Recapture: Generative video camera controls for user-provided videos using masked video fine-tuning. arXiv preprint arXiv:2411.05003 (2024)"
        },
        {
            "title": "Table of Contents",
            "content": "A Symbols and Notations Elaboration on Proposition 4.1 B.1 Forward Diffusion Map Under Zero-Terminal SNR . . . . . . . . . . . . . . . . . B.2 Breakdown of Injectivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Implications for Deterministic Inversion . . . . . . . . . . . . . . . . . . . . . . . Elaboration on Proposition 4. C.1 Proof for the Discrete Case: N0 . . . . . C.2 Proof for the Continuous Case: R0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Elaboration on Stochastic Latent Modulation D.1 Technical Details of Stochastic Latent Modulation . . . . . . . . . . . . . . . . . . D.2 Algorithm for Stochastic Latent Modulation . . . . . . . . . . . . . . . . . . . . . More Ablation Studies E.1 Noise Initialization Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Discrete K-order Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Adaptive Reference Latent Index δblations . . . . . . . . . . . . . . . . . . . . . . Discussion on Quantitative Results"
        },
        {
            "title": "A Symbols and Notations",
            "content": "1 1 1 2 2 3 4 5 5 5 6 6 7 7 In this section, we present the symbols and notations used throughout the paper to ensure clarity and consistency in our mathematical and algorithmic descriptions. Elaboration on Proposition 4. t=0 with cumulative products defined as αt = s=1(1 βs), where the schedule enforces zero terminal signal-to-noise ratio (SNR), such that Consider variance-preserving noise schedule {αt}T (cid:81)t αT = 0. The forward diffusion map is given by: ΦT (x0, ϵ) = αT x0 + 1 αT ϵ, where x0 RFCHW is the initial latent variable, and ϵ (0, I) is noise sample drawn from standard normal distribution. B.1 Forward Diffusion Map Under Zero-Terminal SNR Since the zero-terminal SNR noise schedule specifies αT = 0, substitute this into the definition of ΦT : ΦT (x0, ϵ) = αT x0 + 1 αT ϵ = 0x0 + 1 0ϵ = 0 x0 + 1 ϵ = ϵ. Thus, ΦT (x0, ϵ) = ϵ, which depends solely on the noise ϵ and is independent of the initial latent x0. For any two initial latents x0, 0 RFCHW and fixed noise sample ϵ, it follows that: ΦT (x0, ϵ) = ϵ 0, ϵ) = ϵ. 0, ϵ) = ϵ, regardless of whether x0 = 0 or x0 = 0. and ΦT (x Therefore, ΦT (x0, ϵ) = ΦT (x 1 Symbol i=1 Ii = {Di}n Di Pi Ti M Description Video and Frame Symbols Source video Individual frame of the source video Sequence of depth maps Depth map for frame Ii Camera intrinsics matrix Point cloud for frame Ii Target camera pose for frame Rendered novel view for frame Visibility masks for novel views Latent Space Symbols Φt αt αt xinit x0 ϵ ϵinv ϵ(k) PS x0 ˆϵinv Diffusion for timestep Cumulative signal coefficient at timestep Cumulative product of αt Initial noise for diffusion process VAE-encoded latent also our pivot latent Noise sample from standard normal distribution DDIM inverted latent K-order recursive noise representation Mask and Modulation Symbols Binary occlusion mask Depth-based near depth mask Visibility-aware sampling mask Stochastic permutation operator Modulated content latent Modulated noise latent Table 1: List of symbols used in the paper. B.2 Breakdown of Injectivity function : is injective if, for all a, A, (a) = (a) implies = a. Consider the map ΦT (, ϵ) : RFCHW RFCHW with ϵ fixed. From B.1, for any distinct x0, 0 RFCHW where x0 = 0, we have: ΦT (x0, ϵ) = ϵ = ΦT (x 0, ϵ). Since ΦT (x0, ϵ) = ΦT (x 0, the condition for injectivity is violated. Hence, ΦT (, ϵ) is not injective in x0, as multiple (indeed, all) initial latents x0 map to the same output ϵ for given ϵ. 0, ϵ) holds even when x0 = B.3 Implications for Deterministic Inversion In diffusion models, the terminal state is denoted xT = ΦT (x0, ϵ), which, under the condition αT = 0, simplifies to xT = ϵ. Deterministic inversion methods, such as DDIM inversion, aim to recover the original latent x0 from xT by reversing the forward diffusion process. These methods assume that the forward map ΦT can be inverted uniquely, which requires ΦT to be injective. However, since ΦT (, ϵ) is not injective, multiple distinct x0 produce the same xT = ϵ. Consequently, given only xT , it is impossible to determine which x0 among the infinitely many possible initial latents was the original, rendering unique recovery via deterministic inversion unfeasible. Elaboration on Proposition 4.2 In this section, we prove the closed-form expressions associated with the recursive noise initialization process K-RNR outlined in Proposition 4.2. The recursive process is defined as follows: for an initial step where = 1, the expression is given by ϵ(1) = αtx0 + 1 αtϵinv, 2 and for subsequent steps where > 1, the expression becomes ϵ(k) = αtx0 + 1 αtϵ(k1). Here, x0 RFCHW represents the pivot latent variable, αt > 0 denotes the cumulative signal coefficient at timestep t, and ϵinv is the initial noise term. The proposition posits two closed-form expressions. For the discrete recursion depth, where N0, the expression is ϵ(k) = (cid:32) (cid:88) i=1 (cid:0) αt 1 αt (cid:1)i1 (cid:33) x0 + (cid:0) 1 αt (cid:1)k ϵinv. For the continuous recursion depth, where R0, the expression is (cid:32) αt ϵ(k) = 1 (cid:0) 1 1 αt 1 αt (cid:1)k (cid:33) x0 + (cid:0) 1 αt (cid:1)k ϵinv. The proof is divided into two parts: the discrete case is addressed in C.1, and continuous case is addressed in C.2. C.1 Proof for the Discrete Case: N0 To verify the closed-form expression for discrete values of k, mathematical induction is employed as method of proof. For the initial step, consider the case where = 1. The recursive definition states that ϵ(1) = αtx0 + 1 αtϵinv. To confirm this, the proposed closed-form expression is evaluated at = 1: ϵ(1) = (cid:32) 1 (cid:88) i=1 (cid:0) αt 1 αt (cid:1)i1 (cid:33) x0 + (cid:0) 1 αt (cid:1)1 ϵinv. The summation involves only one term, corresponding to = 1. This term is calculated as follows: (cid:0) (cid:1)1 (cid:0) (cid:1)0 = αt 1 αt = αt 1 = αt. αt 1 αt Thus, the closed-form expression becomes ϵ(1) = αtx0 + 1 αtϵinv, which is identical to the recursive definition. This establishes the validity of the expression for the base case. Next, suppose that for some positive integer 1, the closed-form expression holds true: ϵ(n) = (cid:32) (cid:88) i=1 (cid:0) αt 1 αt (cid:1)i1 (cid:33) x0 + (cid:0) 1 αt (cid:1)n ϵinv. The objective is now to demonstrate that this expression remains valid for the next integer, = + 1. According to the recursive definition, 1 αtϵ(n). The inductive hypothesis is substituted into this equation, yielding (cid:33) ϵ(n+1) = αtx0 + ϵ(n+1) = αtx0 + 1 αt (cid:34)(cid:32) (cid:88) (cid:0) αt 1 αt (cid:1)i1 x0 + (cid:0) 1 αt (cid:1)n (cid:35) ϵinv . The factor in 1 αt is applied to each term within the brackets. For the summation term, this results i=1 1 αt (cid:88) (cid:0) αt 1 αt (cid:1)i1 = (cid:88) (cid:0) αt 1 αt (cid:1)i , i= i=1 3 and for the noise term, 1 αt (cid:0) Thus, the expression for ϵ(n+1) is written as 1 αt (cid:1)n = (cid:0) 1 αt (cid:1)n+1 . ϵ(n+1) = αtx0 + (cid:32) (cid:88) (cid:0) αt 1 αt (cid:33) (cid:1)i x0 + (cid:0) 1 αt (cid:1)n+ ϵinv. The terms involving x0 are then grouped together: i=1 ϵ(n+1) = (cid:32) (cid:88) (cid:0) αt αt + 1 αt (cid:33) (cid:1)i x0 + (cid:0) 1 αt (cid:1)n+1 ϵinv. To express this as single summation, it is noted that allows the expression to be rewritten by adjusting the summation indices: αt can be written as i=1 αt (cid:0) 1 αt (cid:1)0 . This αt + (cid:88) (cid:0) αt 1 αt (cid:1)i = (cid:88) (cid:0) αt 1 αt (cid:1)i . i=1 i=0 This summation from = 0 to corresponds exactly to the desired form when re-indexed: (cid:88) (cid:0) αt 1 αt (cid:1)i = n+1 (cid:88) (cid:0) αt 1 αt (cid:1)i , i=0 i=1 since each term aligns appropriately with the change in index. Therefore, the expression becomes ϵ(n+1) = (cid:32)n+1 (cid:88) i=1 (cid:0) αt 1 αt (cid:1)i (cid:33) x0 + (cid:0) 1 αt (cid:1)n+1 ϵinv, which matches the proposed closed-form expression for = + 1. This step confirms the inductive hypothesis for the next integer, and by the principle of mathematical induction, the closed-form expression is valid for all positive integers which completes the proof C.2 Proof for the Continuous Case: R0 To extend the result to real values of k, the discrete cases summation is analyzed as geometric series. Let the ratio = 1 αt, where, given 0 < αt < 1, it follows that 0 < < 1. The summation in the discrete expression is expressed as (cid:88) αtri1 = αt i=1 k1 (cid:88) i=0 ri. The formula for the sum of finite geometric series is applied here: i=0 This allows the summation to be rewritten as k1 (cid:88) Substituting = i=0 1 αt back into the expression, it becomes k1 (cid:88) ri = 1 rk 1 . αt ri = αt 1 rk 1 . αt 1 (cid:0) 1 1 αt 1 αt (cid:1)k . Incorporating this into the discrete closed-form expression, the result is (cid:32) αt ϵ(k) = 1 (cid:0) 1 1 αt 1 αt (cid:1)k (cid:33) x0 + (cid:0) 1 αt (cid:1)k ϵinv. This formulation is well-defined for all real 0, as the exponential terms are continuous functions over the real numbers which completes the proof"
        },
        {
            "title": "D Elaboration on Stochastic Latent Modulation",
            "content": "In this section, we provide detailed technical elaboration of the Stochastic Latent Modulation (SLM) mechanism, key component of our approach to dynamic view synthesis. SLM addresses the challenge of synthesizing plausible content for regions that become newly visible due to camera motion, operating directly in the latent space of pre-trained video diffusion model. This process modulates both the VAE-encoded latent x0 and the inverted latent ϵinv using single binary occlusion mask and depth map, ensuring consistent and efficient strategy for handling occlusions. By leveraging visibility-aware sampling and stochastic permutation, SLM enables the diffusion model to infer content for occluded regions without requiring architectural changes or additional training. D.1 Technical Details of Stochastic Latent Modulation The SLM process modulates the latents and ϵ by filling their occluded regions with values sampled from visible, depth-specific areas, using single mask and depth map to guide the operation. This begins with the computation of visibility mask, defined as = (1 M) (D), which identifies regions that are both visible (where = 0) and depthwise near (where D). These regions serve as the source pool for sampling, as they contain stable and contextually relevant latent values from the scene. The target regions, where content synthesis is needed, correspond to the occluded areas where = 1. The modulation proceeds by identifying the spatial indices of the source and target regions. The set of source indices, Isource, consists of all positions where = 1, while the set of target indices, Itarget, includes all positions where = 1. For each latent, SLM counts the number of occluded elements (i.e., the size of Itarget) and randomly selects an equal number of indices from Isource. These randomly chosen source values are then assigned to the target positions. Specifically, for x, the values at indices Itarget are replaced with values from randomly selected indices Isource, such that xi = xj. The same process is applied to ϵ, where ϵi = ϵj for corresponding pairs of indices. This stochastic sampling ensures that the occluded regions of both latents are populated with plausible content drawn from the visible, near-depth areas of the scene. The use of single mask and depth map for both and ϵ ensures that the source and target regions remain consistent across the two latents, while the independent application of the sampling process to each latent preserves their distinct roles in the diffusion pipeline. The randomness in selecting source indices introduces variability, allowing the diffusion model to explore diverse completions for the occluded regions, all while maintaining coherence with the visible parts of the scene. D.2 Algorithm for Stochastic Latent Modulation Algorithm 1 Stochastic Latent Modulation 1: Input: RBFCHW, ϵ RBFCHW, {0, 1}BFCHW, RBFCHW 2: Output: Modulated x, Modulated ϵ 3: Compute visibility mask = (1 M) 4: Let Isource = {i Vi = 1} 5: Let Itarget = {i Mi = 1} 6: for each Itarget do 7: 8: 9: 10: end for 11: return x, ϵ Sample Uniform(Isource) Set ϵi = ϵj Set xi = xj"
        },
        {
            "title": "E More Ablation Studies",
            "content": "In this section, we present additional ablation studies to further analyze the components of our approach. In E.1, we assess the effectiveness of K-RNR in comparison to standard noise initialization strategies for video reconstruction. In E.2, we examine the impact of varying the discrete recursion depth k, and in E.3, we analyze the role of the adaptive normalization latent depth δ. 5 E.1 Noise Initialization Ablations Method PSNR SSIM LPIPS 12.03 15.97 9.08 10.16 23. Random Noise Encoded Video + Random Noise DDIM Inversion Encoded Video + DDIM Inversion Random Noise + KV Caching The results presented in Figure 1 provide comparative evaluation of various initialization strategies for video reconstruction in the absence of camera transformations. The baseline method that begins generation with standard normal noise (ϵ) underperforms across all metrics, which is expected due to the lack of structured guidance during synthesis. Injecting signal via linear combination of VAE-encoded video latents (x0) and noise, as in the Encoded Video + Random Noise strategy, yields noticeable improvements, indicating the benefit of directly incorporating source video content into the initial conditions. In contrast, DDIM Inversion, which initializes with an inverted latent but without scheduler-consistent interpolation, achieves the lowest reconstruction quality, yielding high saturation, washed-out generations. The marginal improvement obtained by combining the encoded latent with DDIM inversion further underscores the sensitivity of the diffusion trajectory to initialization fidelity. Figure 1: Ablation on noise initialization strategies for video reconstruction without camera transformation. 0.313 0.674 0.315 0.324 0.824 0.486 0.539 0.904 0.907 0.118 K-RNR 29.56 0.910 0.063 Random Noise + KV Caching introduces mechanism where the generation initiated from noise is guided by attending to key-value pairs derived from parallel DDIM-inverted path, integrating cross-stream structural memory. This strategy shows some gains, particularly in perceptual quality as measured by LPIPS with the expense of reduced efficiency since 2 parallel attention computation over the extended sequence dimension is performed. Our proposed K-RNR approach that achieves the highest performance across all metrics, with PSNR, SSIM, and LPIPS values of 29.56, 0.910, and 0.063 respectively. These results confirm the effectiveness of recursive noise representationfor high-fidelity video reconstruction. The superior quantitative outcomes suggest that K-RNR is capable of leveraging structured priors in noise space more effectively than existing baselines. Results are demonstrated in Figure 4 and corresponding videos are shared in website.html E.2 Discrete K-order Ablations Figure 2 presents an ablation study on the discrete recursion depth in K-RNR, following the application of adaptive scaling. The results demonstrate clear performance trend as increases. For shallow recursion depths (k = 1 and = 2), the model exhibits poor reconstruction quality across all metrics, indicating that insufficient recursive refinement fails to recover meaningful structure in the video content. substantial performance jump is observed at = 3, suggesting that minimum level of recursive processing is necessary to capture the underlying temporal and spatial consistency required for high-fidelity generation. K-Depth PSNR SSIM LPIPS = 1 = 2 = 3 = 4 = 5 = 6 = 7 = 8 7.82 8.85 15.91 15.94 16.00 16.39 16.34 15.30 0.221 0.231 0.550 0.550 0.550 0.555 0.558 0.545 0.896 0.871 0.465 0.468 0.489 0.471 0.474 0.483 Figure 2: Ablation on the recursion depth in K-RNR with after applying adaptive scaling. As increases beyond 3, PSNR and SSIM metrics improve steadily, peaking at = 6 and = 7 respectively. The LPIPS metric reaches its lowest value at = 3 (0.465), indicating optimal perceptual similarity at moderate recursion depth, though values remain competitive through = 7. Notably, performance begins to degrade at = 8, likely due to over-recursion, which may introduce noise or overfitting artifacts into the refinement process. These findings suggest that while increasing recursion depth generally enhances reconstruction, there exists sweet spot around = 6 to = 7 that balances iterative refinement with stability. This trade-off is essential to consider when tuning K-RNR for optimal video reconstruction performance. 6 E.3 Adaptive Reference Latent Index δblations Figure 3 presents an ablation study on the choice of the adaptive latent index δ, which determines the reference noise level used for adaptive normalization between the k-th order noise and the δ-order noise. In all our experiments, we set δ = 3, and the results in this ablation empirically validate this design choice. When δ = 3, the model achieves the highest reconstruction quality across all evaluation metrics, with PSNR of 24.97, SSIM of 0.885, and LPIPS of 0.078. δ Index PSNR SSIM LPIPS = 1 = 2 = 3 = 4 = 5 = 6 = 10.32 19.23 24.97 15.29 13.92 12.66 11.28 0.342 0.748 0.885 0.592 0.468 0.333 0.244 0.883 0.148 0.078 0.240 0.329 0.451 0.604 Figure 3: Ablation on the adaptive reference latent index δ. Performance degrades notably when δ deviates from this setting. For instance, lower values of δ such as 1 and 2 lead to insufficient regularization, producing reconstructions with low fidelity and poor perceptual quality. Conversely, higher values of δ (i.e., δ 4) introduce excessive deviation in the normalization reference, which appears to destabilize the refinement process and result in less consistent outputs. This pattern suggests that δ = 3 offers an optimal trade-off by aligning the reference noise distribution closely with the target generation stage, enabling more effective adaptive normalization. These findings confirm that careful selection of the latent reference index is critical for preserving quality in recursive refinement."
        },
        {
            "title": "F Discussion on Quantitative Results",
            "content": "Table 1 and Table 2 in the main paper present comprehensive quantitative evaluation of our framework against recent methods across multiple axes, including visual quality, camera pose accuracy, view synchronization, and reconstruction fidelity. The baseline methods span three architectural families: GCD and TrajectoryAttention are built upon the Stable Video Diffusion backbone, Diffusion as Shader (DaS) and TrajectoryCrafter share the CogVideoX foundation with our method, and ReCamMaster is based on the Wan architecture. In our experiments, we observe that methods relying on Stable Video Diffusion, such as GCD and TrajectoryAttention, consistently underperform in preserving the identity and motion dynamics of the original videos when camera transformations are introduced. This can be attributed to the limited expressiveness of the Stable Video Diffusion architecture compared to the more semantically rich representations offered by CogVideoX and Wan. Among the CogVideoX-based approaches, Diffusion as Shader struggles to maintain action fidelity, often generating semantically coherent frames that fail to reflect the intended motion trajectory. TrajectoryCrafter achieves stronger balance between action fidelity and identity preservation; however, we note that identity consistency tends to degrade toward the latter segments of the video. ReCamMaster, while effective in its synthesis, incurs significant inefficiency due to its reliance on concatenating source and target video frames along the frame channel. This design increases the overall token sequence length, which not only limits scalability but also results in considerably slower inference speeds. In contrast, our proposed method retains both high fidelity and identity consistency across the video while maintaining efficient inference. The quantitative comparisons are shared in website.html. 7 Figure 4: Video Reconstruction Strategies. We perform quantitative and qualitative evaluation on video reconstruction without camera transformation application. Video results can be found in the supplementary material."
        }
    ],
    "affiliations": [
        "Virginia Tech"
    ]
}