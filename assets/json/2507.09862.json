{
    "paper_title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation",
    "authors": [
        "Youliang Zhang",
        "Zhaoyang Li",
        "Duomin Wang",
        "Jiahe Zhang",
        "Deyu Zhou",
        "Zixin Yin",
        "Xili Dai",
        "Gang Yu",
        "Xiu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audio-visual dyadic interactive virtual human generation. Totaling over 8,743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types (dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into a large-scale pre-training subset and a curated, high-quality subset for Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by a dedicated set of metrics and test data to serve as a benchmark VidChatBench for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/"
        },
        {
            "title": "Start",
            "content": "SpeakerVid-5M: Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation Youliang Zhang1,2 Deyu Zhou2,3 Zhaoyang Li2 Duomin Wang2 Zixin Yin2,4 1Tsinghua University. Xili Dai Gang Yu2 2StepFun. Jiahe Zhang Xiu Li1 5 2 0 2 4 ] . [ 1 2 6 8 9 0 . 7 0 5 2 : r 3The Hong Kong University of Science and Technology (Guangzhou). 4The Hong Kong University of Science and Technology. zhangyou24@mails.tsinghua.edu.cn, lizhaoyang@mail.ustc.edu.cn (wangduomin,daixili.cs, jiaahe0111)@gmail.com zyinaf@connect.ust.hk, dzhou861@connect.hkust-gz.edu.cn yugang@stepfun.com, li.xiu@sz.tsinghua.edu.cn Figure 1. Overview of the audio-visual dyadic generation task and the SpeakerVid-5M dataset. The primary task (top row) is to generate coherent audio-visual response based on the input of initiator. Our SpeakerVid-5M (bottom left) provides over 8.7K hours data to facilitate this research. Each clip is enriched with detailed multi-modal annotations (right panel), enabling fine-grained generation."
        },
        {
            "title": "Abstract",
            "content": "The rapid development of large-scale models has catalyzed significant breakthroughs in the digital human domain. These advanced methodologies offer high-fidelity solutions for avatar driving and rendering, leading academia to focus on the next major challenge: audio-visual dyadic interactive virtual human. To facilitate research in this emerging area, we present SpeakerVid-5M dataset, the first large-scale, high-quality dataset designed for audioProject Lead Corresponding author visual dyadic interactive virtual human generation. Totaling over 8, 743 hours, SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It covers diverse scales and interaction types, including monadic talking, listening, and dyadic conversations. Crucially, the dataset is structured along two key dimensions: interaction type and data quality. First, it is categorized into four types(dialogue branch, single branch, listening branch and multi-turn branch) based on the interaction scenario. Second, it is stratified into large-scale pre-training subset and curated, high-quality subset for Supervised FineTuning (SFT). This dual structure accommodates wide array of 2D virtual human tasks. In addition, we provide an autoregressive (AR)-based video chat baseline trained on this data, accompanied by dedicated set of metrics and test data to serve as benchmark (VidChatBench) for future work. Both the dataset and the corresponding data processing code will be publicly released. Project page: https://dorniwang.github.io/SpeakerVid-5M/. 1. Introduction In the era of rapid advancements in large-scale models, large video models have acquired the capability to generate high-fidelity video, presenting unprecedented advantages for the generation and driving of high-quality virtual humans. Pioneering works based on Generative Adversarial Networks (GANs) [41, 53, 55], established the foundation for realistic 2D virtual human driving and rendering. However, the advent of large video models exemplified by diffusion-based works [10, 25, 27, 44, 46] has achieved state-of-the-art realism, significantly elevating the authenticity of both driving and rendering. This leap in fidelity has enabled broader industrial adoption, facilitating commercial-grade applications in areas such as automated lip-syncing, digital newscasting, and virtual actors. Nevertheless, more ambitious goal has captured the attention of researchers in both academia and industry: the creation of proactive, interactive virtual humans. This endeavor is akin to equipping an avatar with brain, advancing towards virtual beings that are not just passively driven but can engage autonomously. Such technology holds compelling potential for applications like more realistic virtual assistants, live-streaming e-commerce, and online education. Several works have approached this task from system-building perspective [26, 33, 42, 58], have constructed functional interactive agents by integrating existing, off-the-shelf modules. In contrast to earlier systems that lack the ability for multimodal perception and understanding, BodyofHer [4] presented an end-to-end trained interactive agent based on autoregressive LLM paradigm that accepts full suite of multimodal inputs. These works collectively represent pioneering explorations in this direction. In the current research paradigm, which is increasingly centered on foundation models, the development of task-specific applications necessitates large-scale training datasets. However, notable scarcity of open-source datasets focused on interactive virtual humans persists within the academic community. Training foundation models for interactive virtual humans requires vast amounts of specialized data, yet such resources are scarce. To address this critical need, we introduce SpeakerVid-5M, the first large-scale, high-quality dataset for audio-visual dyadic interaction, featuring richly annotated and meticulously aligned audiovisual pairs. SpeakerVid-5M features: (1) Large-scale data: The dataset contains 770K high-quality dynadic conversion audiovisual pairs (totaling 1.8K hours) and 5.2M high-quality single-speaker clips (totaling 8.7K hours). (2) High resolution: 93% of the videos are in 1080P or higher, ensuring detailed and clear visual input for generation tasks. (3) Rich annotation types: Each clip is accompanied by structured textual annotations, skeletal sequences, ASR transcriptions, and blur scores, supporting wide range of multimodal (4) High-quality: Precise synchrolearning objectives. nization between audio and video is ensured, with rigorous filtering applied to both modalities to guarantee clean and reliable training data. (5) Body compositional diversity: Data instance is captured with labels spanning full-body, half-body, head-only, and side-view profiles, enabling finegrained control over framing analysis. In addition, we select 500 videos from out-of-distribution speaker IDs to construct the VidChatBench benchmark for the audio-visual dyadic interactive virtual human task, which focuses on video quality, audio-visual consistency, dialogue coherence, and identity preservation for model performance evaluation. SpeakerVid-5M is designed to facilitate research in interactive virtual human task. The rich annotations also make it valuable resource for variety of related tasks, such as human animation, talking head generation, multimodal dialogue modeling, etc. all of which face the challenge of lack of large-scale, high-quality datasets. In addition, we conduct an initial exploration of implementing audio-visual dyadic interactive virtual human generation under an autoregressive paradigm. Given input video and audio, the model jointly generates the speakers response in both audio and visual modalities. Our contributions can be summarized as follows: We propose SpeakerVid-5M, the first large-scale dataset designed specifically for the audio-visual dyadic interactive virtual human task. It includes 1M high-quality dialogue audiovisual pairs, with supporting for multi-turn conversations. The VidChatBench is also provided for better evaluation. SpeakerVid-5M contains 5M single-speaker audiovisual clips, making it the largest talking human dataset. It covers wide range of annotated visual formats, including talking heads, half-body, full-body, and side-view videos. We open-source the entire dataset, including the raw data, annotations, and data processing pipeline, providing full transparency and reproducibility for the community. 2. Related Work 2.1. Audio-Visual Human Video Generation. Lip-sync [20, 31, 32] and talking head generation [39, 41, 49, 53, 55] are foundational tasks in audio-driven human video generation, among those, PD-FGC [41] was pioneering work that first achieved the generation of talking heads with vivid facial expressions. Additionally, tasks such as learning to listen [29, 30] also fall under the category of audio-driven portrait generation, focusing on responsive non-verbal behaviors. Subsequently, with the rapid development of foundational video models, the field has bifurcated into two primary research directions. One branch has pushed for higher quality and broader scope [10, 25, 27, 39, 44, 46], with EMO [39] introducing diffusion video models to achieve state-of-the-art realism, and works like OmniHuman-1 [25], MoCha [46], and Veo3 [1] expanding generation to full-body, multi-agent performances, and direct text-to-audiovisual synthesis. The second branch focuses on creating interactive virtual humans [4, 26, 33, 42, 58]. This includes modular systems like AgentAvatar [42] that integrate LLMs as planners, and end-to-end paradigms like BodyofHer [4] that enable direct multimodal understanding for autonomous reactions. These pioneering efforts in interactivity shift the focus from pure driving alignment to agent autonomy. 2.2. Audio-Visual Human Video Datasets. Early research in talking head and lip-sync generation initially leveraged datasets from related fields such as lip reading [2, 3, 12] and speaker recognition [13, 28], Subsequently, datasets specifically curated for these tasks began to emerge [45, 52, 54]. ViCo [56] introduced dataset for the learning to listen task, though it was focused on monadic scenarios. However, these early datasets were often limited in scale and of lower quality, rendering them inadequate for the demands of current high-quality, dataintensive models. While massive datasets exist, they are either too general-purpose and of variable quality (e.g., ACAV-100M [19]) or proprietary and unreleased (e.g., the data used by OmniHuman-1 [25]). The recent release of OpenHumanVid [21] provided valuable resource, but it was only partially released and remains focused on monadic, talking-head scenarios. This leaves critical void for public, large-scale dataset centered on audio-visual dyadic interactions for virtual human research. Our work, SpeakerVid-5M, is designed to fill this void. 3. Dataset Curation The construction process of our SpeakerVid-5M dataset mainly includes four parts: Source data collection, Data pre-processing, Data annotation, and Quality filter. The entire construction process is illustrated in Figure 2. 3.1. Source Data Collection We manually collected high-quality videos featuring twoperson dialogue from YouTube. The sources mainly include interviews, news reports, seminars, television programs, variety shows, debates, and educational videos. The collected videos exhibit diverse formats such as full-body, half-body, frontal, and side views, all with clear facial visibility and In total, we gathered 153K audiocorresponding audio. visual videos, amounting to 64, 386 hours of raw data, with 93% of the videos in 1080P resolution or higher. The temporal scope of the dataset extends from 2006 to the present day (June 2025). This collection spans diverse range of genres, mainly including entertainment, people and blogs, comedy, news and politics, education, science, and sports. 3.2. Data Pre-processing Scene splitting. We employed SceneDetect [6] for scene splitting, which identifies visually significant transitions by analyzing changes in color and brightness to determine scene boundaries. Based on the initial segmentation results, we conducted further post-processing by discarding clips shorter than 3 seconds and splitting those longer than 14 seconds. As result, we obtained video clips ranging from 3 to 14 seconds in length. We record the temporal order and the human ID of each segment, allowing clips to be easily concatenated to form longer sequences when needed. The segments resulting from this stage are denoted by Ssp. Speaker diarization. We utilize 3D-Speaker [11] to perform speaker diarization, segmenting the original audio (extracted directly from video before scene splitting) into multiple segments and recording the timestamps and corresponding speaker IDs for each segment. For each original audio, we identified the two primary speaker IDs based on the frequency and duration of their speech. Segments associated with other speaker IDs were discarded in the subsequent processing steps. The segments resulting from this stage are denoted by Ssv. Human detection. We apply YOLO [18] for human tracking within each video clip, and aggregate the tracking results over time to form single spatiotemporal visual track for each individual. Because the scene boundaries provided by SceneDetect can be inaccurate, we further refine each clip by temporally and spatially cropping it based on YOLO detection results to obtain usable single-speaker video clips. This stage may yield multiple clips, each originating from different spatial regions within the same temporal window of single video clip Ssp. The resulting segments from this stage are denoted by Srsp. Lip synchronization. For each video clip obtained from the last stage, we first calculate the temporal overlap between Srsp and Ssv, this yields the overlapped segment Sol. We then employ SyncNet [36] to perform audio-visual synchronization within each segment Sol. The SyncNet confidence score is then used to associate each speaker ID with specific visual bounding box detected by YOLO. In cases where two individuals are present and conversing within the same temporal window, we assign speaker ID to the bounding box with the highest confidence score. Figure 2. The SpeakerVid-5M curation pipeline. The process consists: (1) Source data collection from YouTube; (2) Multi-step audiovisual pre-processing; (3) Rich multi-modal annotation using models like Qwen-VL; (4) Rigorous quality filtering stage for data fidelity. ID correction. To further verify and refine the speaker IDs obtained through the collaboration of Speaker diarization and lip-sync analysis, we employ vision-based method, ArcFace [15], for further correction. For multiple clips extracted from the same original video, the speaker IDs derived from audio segmentation should be consistent with those inferred from visual information. We first assign speaker IDs based on audio segmentation, then compute facial cosine similarity across clips belonging to the same speaker ID. Outliers identified via low similarity scores are compared against other speaker IDs using ArcFace. If higher similarity is found with another ID, the outliers ID is updated accordingly, completing the correction process. 3.3. Data Annotation Structured Textual Caption. Textual captions are provided in detailed structure formats to ensure comprehensive representation of the video content. We leverage the powerful multimodal model Qwen2.5-VL [5] to generate textual annotations for our video data, which demonstrates remarkable capabilities in visual understanding and instruction following. The structured annotations include camera movement patterns, list of entities present in the video, body orientation (front or side view), and whether the person is shown in half-body or full-body. Additionally, we provide detailed descriptions of human body movements and facial expressions. Furthermore, we use Qwen-3 [50] to summarize automatic speech recognition results across multiple clips originating from the same source video. Based on these summaries, we annotate the topic category of the two-person dialogue presented in the video. Audio Annotation. Each data sample in our dataset consists of speaker video with well-aligned audio and visual modalities. To support tasks related to audio generation and control, we annotate the audio with several key features. First, we apply Whisper [35] for automatic speech recognition to obtain text transcriptions of the audio. Second, we annotate each audio segment with its corresponding SyncNet metrics, including the audio-visual offset, synchronization confidence score, and audio-visual embedding distance. Finally, we use 3D-Speaker to assign speaker IDs to multiple audio segments from single original audio. For each clip, in addition to the original audio, we provide cleaned version in which segments not belonging to the target speaker ID are replaced with silent audio. Skeleton Sequence. Building upon the spatiotemporal human bounding boxes obtained from YOLO, we further utilize DWpose [51] for human pose estimation. The resulting skeletal sequences include keypoints of the face, hands, and body. Clips without detected face are filtered out. Face and Hand Blur Score. We extract face and hand bounding boxes from the DWpose annotations. For each frame, we crop the pixel regions corresponding to the face and both hands, then resize these crops to resolution of 128 128. Next, we calculate the Laplacian variance of the cropped face and hand images using the Laplacian operator. higher variance typically indicates sharper, clearer image, which we use as clarity score. Since videos containing limb movements often exhibit motion blur during rapid movements, incorporating this clarity score as conditioning factor may enhance the models performance in handling such scenarios [24]. Motion score. The range of human motion plays crucial role in generating human-centered videos. Compared to actions in general videos, human movements are more subjective and varied. Therefore, we employed Qwen2.5-VL to simulate human judgment and score the motion magnitude of individuals in the videos. To better simulate the behavior of different annotators, we employ multiple prompts, each embodying distinct persona, to rate each video. The ratings are on 1-to-5 scale, where score of 1 indicates minimal movement and score of 5 denotes high motion amplitude. After excluding outliers, the average score is taken as the final motion magnitude annotation for the video. 3.4. Quality Filter Luminance Filtering. Following OpenHumanVid [21], we calculate the luminance score to filter overly dark or bright videos. The luminance score is calculated using the formula: 0.2126R + 0.7152G + 0.0722B, where R, G, and denote the pixel values of the red, green, and blue channels, respectively. Videos with luminance score below 10 or above 210 are filtered out. Video Quality Filtering. To ensure the visual quality of the video data, we employed the DOVER [47] model for video quality assessment. DOVER decomposes each video into aesthetics-related and technical-related components and evaluates them separately. This approach enhances the consistency between the models assessment results and human subjective perception of video quality. We filtering out videos with fused scores below 0.25. Clear score Filtering. Resolution alone is insufficient to assess the visual clarity of video, as some high-resolution videos may still exhibit blurriness and other low-quality characteristics. Bitrate reflects the amount of information carried per frame and provides an informative quality meaW H, where sure. We compute clarity score as B/ and means the resolution of video and is the bitrate. Videos with clarity scores in the bottom 5% are filtered out. Blur Filtering. Human-centered videos often suffer from motion blur, especially when the subject exhibits large movements. In our dataset, such blur typically affects the face and hands. To address this, we compute the average blur score over face and hand for each frame in video. Videos with an average blur score(face or hand) below 0.1 are filtered out to ensure data quality. Audio Filtering. Previous audio-visual datasets have typically focused solely on visual quality while neglecting the quality of the audio component. We additionally focus on four key metrics in the ASR process, confidence score, no-speech probability, compression ratio, and detected language. These indicators reflect the reliability of the ASR output, whether the audio contains human speech, and whether there are corrupted or non-speech signals in the audio. At this stage, we filter out audio clips with confidence score(average log probability) lower than 1.5, nospeech probability greater than 0.8, or compression ratio exceeding 2.5. To further ensure the quality of the detected results, we filter out samples in which the detected language mismatches the language labeled in the videos metadata. 4. Dataset Statistics and Analysis 4.1. Dataset Statistical Comparison. As shown in Table 1, we present comparative analysis of our dataset against previous general and human video datasets. Compared to traditional audioor text-driven digital human generation, our dataset is the first to extend this task into audio-visual dyadic interactive scenarios, featuring complete audio-visual pairs of both questions and responses. This provides rich, high-quality training data for end-to-end audio-visual dyadic interactive virtual human generation. Moreover, our dataset represents the largest collection of single-speaker audio-video pairs to date. Each clip features well-aligned audio and video, capturing clear and uninterrupted instance of speech. Figure. 3 shows statistical analysis of SpeakerVid-5M dataset. The blur score distribution, resolution and DOVER quality distribution demonstrate the high quality of our proposed dataset. Video topic and year distribution and the caption words distribution shows that the dataset exhibits diversity in monologue or conversation content. Key features of our dataset include: (1) Large-scale data. The dataset comprises 5.2M single-speaker audiovideo clips totaling 8.7K hours, and 770K two-person conversational audiovisual pairs totaling 1.8K hours. (2) High resolution. 93% of the videos are in 1080P or higher, and 98% exceed 720P, ensuring high visual fidelity. (3) Rich annotations. Each clip is accompanied by fine-grained structured textual annotations, ASR transcriptions, human pose keypoint sequences, motion magnitude scores, and motion blur scores, enabling detailed modeling and anal- (4) Diverse data formats. The dataset includes ysis. single-speaker monologues, two-person dialogues, multiturn conversational dialogues, and listening-human scenarios, supporting wide range of human-centric generation (5) High-quality audio-visual data: Multiple filtasks. tering strategies across both audio and video modalities ensure that all clips exhibit clean, well-synchronized audiovisual pairs. (6) Tiered dataset design: The dataset is organized into large-scale pretraining subset and curated high-quality subset (1.3K hours), based on multiple quality indicators, facilitating research under varying levels of training resources and computational constraints. (7) Body compositional diversity: Data instance is captured with labels spanning full-body, half-body, head, and side-view profiles, enabling fine-grained control over framing analysis. (a) Face and Hand Blur Score (b) Duration and Resolution (c) Topic and Year Distribution (d) Caption Word Cloud (e) Sync Conf Score Distribution (f) Caption Words Distribution (g) DOVER Quality Score Distribution Figure 3. Statistics of our dataset from multiple aspects, including blur score, sync score, caption, etc. Datasets Domain Clips Duration (hours) Generation Person num audio pose Speaker ID Blur anno Body composition Caption type UCF-101 [38] ActivityNet [7] NTU RGB+D [37] TikTok-v4 [8] Openhumanvid [21] VoxCeleb [28] VoxCeleb2 [13] MEAD [43] CelebV-HQ [57] CelebV-Text [52] Human Human Human Human Human Head Head Head Head Head SpeakerVid-5M SpeakerVid-5M (Dialogue) Human Human 13.3K 100K 114K 350 13.4M 21.2K 150.4K 281.4K 35.6K 70K 5.2M 770K 26.7 849 3.7 1 16.7K 352 2.4K 39 68 279 8.7K 1.8K Conditioned Conditioned Conditioned Conditioned Conditioned Conditioned Conditioned Conditioned Conditioned Dyadic N/A N/A single single multi single single single single single single single Text Text - - Structured - - - Structured Structured Structured Structured IDs N/A N/A N/A N/A N/A 1.2k 6.1K 60 15.6K N/A 83K 16K Resolution 240P N/A 1080P N/A 720P 224P 224P 1080P 512P 512P 1080P 1080P Table 1. Comparative analysis of SpeakerVid-5M with existing human video datasets. Person num indicates the number of individuals present in given clip. For each clip, only one person is retained, ensuring clean alignment between the audio and visual streams. Blur anno represents the degree of blurriness of the hands and face in each frame. Body composition means the fine-grained annotations for body composition (full-body, half-body, head-only) and camera perspective (frontal, side), features that are absent in most prior work. 4.2. SpeakerVid-5M Dialogue Branch To support the training of audio-visual dyadic interactive virtual human generation, we introduce two-person dialogue branch within the dataset, where each sample consists of two audio-visual pairs, one serving as the input and the other as the target response. Unlike conditioncontrolled generation tasks such as talking head, or monadic generation tasks such as learning-to-listen, dyadic generation requires the model to generate both audio and video responses based on comprehensive understanding of the input multi-modal content, enabling interaction with the environment in both listening and behaving. This setting goes beyond conventional modality-aligned generation, demanding stronger comprehension and reasoning capabilities from generative models. Our two-person dialogue branch is primarily sourced from real-world conversational scenarios such as interviews, podcasts, news segments, educational videos, and debates. It consists of 770K clip pairs (totaling 1.8K hours) and includes 16K unique speaker IDs. The dialogue topics span wide range of categories(Fig. 3c 3d, including entertainment, people and blogs, comedy, news and politics, education, and science. To ensure dialogue coherence, we only select temporally continuous clips and extract data exclusively from original videos in which the two main speakers account for over 80% of the total speaking time. 4.3. SpeakerVid-5M Single Branch The single-speaker video branch consists of 5.2M clips with 83K unique speaker IDs, totaling 8.7K hours. It covers diverse range of camera framings and angles, including Figure 4. Examples of dyadic dialogue and body composition in SpeakerVid-5M. The top rows illustrate typical dyadic human generation sample (initiator and responder). The bottom rows demonstrate the variety of body compositions annotated in our dataset, including close-up headshots, half-body, and full-body views, which are critical for controllable generation. full-body, half-body, frontal, and side-view shots. Each clip is annotated with skeletal keypoint sequences, structured textual descriptions, and automatic speech recognition (ASR) transcriptions, making it well-suited for wide array of conditioned generation tasks, such as audio-skeleton driven portrait animation, lip synchronization, and talking head generation. In terms of scale, this branch represents the largest speaker-specific dataset to date, with volume comparable to that of more general-purpose video datasets. 4.4. SpeakerVid-5M Listening Branch Considering the real-world applications of digital humans, models are expected not only to generate meaningful responses but also to exhibit appropriate listening behaviors. To support this capability, we specifically collect two types of data representing listening states: (1) Co-present listening dialogue (e.g., live interviews, news commentary, seminars, podcasts). For naturally occurring two-person onscreen dialogues, we filter clips based on audio segmentation results and SyncNet confidence scores. Given clip with two individuals and (before cropping), if only one speaker is active and the SyncNet scores between the two differ larger than predefined threshold, if is the lower one, is determined to be in listening state. (2) Non-copresent listening dialogue. For dialogue scenarios where speakers are not simultaneously visible, we similarly rely on SyncNet scores. person (suppose as A) is classified as being in listening state if the following conditions are met: the ASR result is valid, the transcription confidence is above given threshold, and the detected SyncNet score for that persons video is below pre-defined threshold. In both cases, the resulting listening pair is composed of the speakers audio track and the listeners silent video track. 4.5. SpeakVid-5M Multi-turn Branch To enable multi-turn dialogue capabilities in dyadic interactive scenarios, we select multiple clips extracted from the same original video, assigning them with sequential index, and preserve their temporal order. We specifically collect and organize two types of data for multi-turn dialogue. For given pair of two-person dialogue clips, we define the dialogue start timestamp as and the maximum history temporal length as . All clips occurring within the interval [x T, x] are considered as preceding turns of the current dialogue: (1) Contextual multi-turn dialogue. The corresponding ASR transcriptions of these preceding turns are aggregated to form the multi-turn dialogue context for the current response prediction. (2) Sequential multi-turn dialogue. We exam the temporal gap between consecutive clips. If the gap between two adjacent clips from preceding turns is less than predefined threshold δt, we consider them part of the same continuous conversation. The audiovisual data from all such clips are then utilized for predicting the current response. (We discard clips shorter than 3s during pre-processing.) To increase the amount of multiturn data, we relax the filtering criteria for clips that fall between two valid adjacent dialogue segments. Relaxed criterion facilitates the construction of longer and more natural conversational sequences, enabling the training of dialogue models capable of coherent, multi-turn interactions. 4.6. Data Stratification Beyond the four categorical divisions, we also graded the data by quality. We created high-quality supervised finetuning (SFT) subset (571K clips, 1368 hours) by selecting samples with hand motion blur score above 0.5 and face blur score above 0.7, DOVER score above 0.6. We also constrain the motion score to above 2 and filtering the asr quality with confidence score above 1. The remaining data forms the large-scale pretraining subset (7375 hours). 5. Autoregressive Talking Human Generation We design baseline method based on an autoregressive framework tailored for audio-visual dyadic human generation. As illustrated in Fig 5, we incorporate Qwen2.5-Omni [48] to enable multimodal understanding of the input video and audio. Subsequently, next-chunk prediction autoregressive model is employed to jointly generate audio and video tokens. These tokens are then used as conditioning signals for shallow diffusion MLP, which produces videos with enhanced detail and realism. 5.1. Autoregressive generation of video and audio Following Qwen2.5-Omni, we feed both the hidden states produced by the Qwen2.5-Omni thinker and the embeddings of the original audio-video inputs into the autoregressive generation head. Besides, we employ open-source 3D variational autoencoder (VAE) with temporal stride of 4 and spatial stride of 8 to encode the video frames to the latent space [23]. These latents are then divided into patches and encoded as token embeddings. For audio, we borrow audio tokenizer from CosyVoice2 [17] to encode raw audio into discrete tokens. We consider latent map from the 3D-VAE and its corresponding audio tokens to constitute single chunk. During attention computation, both audio and video tokens attend to all preceding tokens and the tokens within their current chunk. Video tokens are augmented with combined 1D temporal and 2D spatial positional encoding. Audio tokens utilize dual-level 1D positional encoding scheme, which encodes both the tokens position within its chunk and the chunks position in the overall sequence. The reference image is also encoded by 3D VAE. Reference image tokens are appended after the input audio and video and before visual generation tokens. To mitigate error accumulation in autoregressive generation, we introduce random noise [40] to the visual tokens during training, which encourages the model to learn more robust representations and leads to improved generation quality. 5.2. Visual Optimization Inspired by MAR [22], we also employ diffusion loss and masked-token prediction to enhance visual generation. For tokens in each generated chunk, another spatial transformer from NOVA [14] takes them as input and generate detailed visual token set-by-set. The simultaneous generation of audio-video tokens via next-chunk prediction, combined with carefully designed frame-wise audio injection into the spatial transformer, ensures high audio-visual consistency. The visual tokens generated by the spatial transformer are temporally and spatially aligned with the latents encoded by the 3D VAE, each token corresponds to specific patch of visual latent feature and is used as condition signal for diffusion MLP, which performs denoising process to generate refined visual latents that can be directly decoded by the 3D VAE. CosyVoice flow matching vocoder are used to decode the generated audio tokens into audio. Figure 5. Our autoregressive audio-visual generation method. 5.3. Progressive Training Process The model training process is divided into three stages: visual pretraining, audio-visual joint training, and highquality dyadic dialog fine-tuning. In the visual pretraining stage, we utilize single-speaker data, where the ASR transcription of the target audio and textual captions (including both motion and expression captions) are used as condition signals to generate video. This stage aims to train the models basic capability in visual content generation. In the joint audio-visual training stage, after filtering the audio data, we continue to use the ASR transcription and captions of the target audio as inputs, but extend the generation targets to include both video and audio modality. This stage enables the model to learn synchronized audio-visual generation. In the high-quality dyadic dialog fine-tuning stage, we select premium dialogue audio-video pairs to further fine-tune the model. The goal of this stage is to enhance its multimodal understanding capabilities and its ability to generate coherent, emotionally aligned conversations. During training, the visual objective is optimized using diffusion loss, while the audio objective is supervised with cross-entropy loss for next-chunk prediction. Figure 6. Qualitative results of our dyadic generation model. From left to right, the input video of the initiator, the reference image providing the target identity, and the modes generated audio-visual response. Method Audio Spatial Noise FID FVD PSNR SSIM ArcFace CLIPdialog Syncconf FIDEmotion SIM-o Conditioned Conditioned Conditioned Conditioned Dyadic Dyadic Dyadic Dyadic 56.82 57.03 38.53 34.72 49.97 49.86 35.67 32.35 55.06 55.16 34.64 30. 47.23 36.90 31.28 28.82 15.26 15.31 16.79 17.39 15.74 15.63 17.44 17.55 0.62 0.62 0.64 0.65 0.62 0.62 0.65 0.66 0.638 0.630 0.732 0. 0.637 0.635 0.749 0.772 0.642 0.643 0.643 2.063 2.459 2.655 2.239 2.541 2.698 3.45 3.45 3.36 3. 3.48 3.43 3.33 3.22 0.65 0.64 0.65 0.64 0.65 0.65 Table 2. Quantitative results of our baseline method on VidChatBench benchmark. Audio means generating audio jointly. Spatial means the set-by-set prediction spatial transformer. Noise means the addition of noise in training the autoregressive audio-visual generator. 6. Experimental Results and Analysis 6.1. The VidChatBench Benchmark. The audio-visual dyadic interactive virtual human generation involves audio-visual understanding, response generation, and the synthesis of corresponding audio and video It requires models to possess both understandcontent. ing and generative capabilities, making it significantly more challenging than traditional generation task. To effectively evaluate the quality and appropriateness of the generated content, we constructed the VidChatBench benchmark, consisting of 500 representative input-output pairs with unseen speaker IDs, accompanied by set of tailored evaluation metrics. Specifically, we assess the model performance along the following five dimensions: (1) Video Quality. To assess the visual fidelity of the generated videos compared to the ground truth (GT), we adopt several widely used metrics in the video and audio domains, including FID, FVD, PSNR, and SSIM. (2) Identity Preservation. The generated video is expected to maintain consistent identity with the reference image throughout the sequence without temporal degradation. We employ ArcFace to extract facial features frame-by-frame from the video and compute the cosine distance between these features and the reference image. The average distance across all frames is used (3) Dialogue Coheras the identity preservation score. ence. The generated content should exhibit semantic relevance and appropriateness with respect to the input. For each evaluation sample in our test set, we construct 5 candidate responses of varying quality, generated and ranked by multimodal large language model [5]. score is then assigned to each candidate according to its rank, using the ordered set [0.2, 0.4, 0.6, 0.8, 1.0]. During testing, we obtain the ASR transcription of the generated audio and compute its CLIP [34] distance to each candidate response. The final dialogue coherence score is the score of the closestmatching candidate. (4) Audio-Visual Consistency. Synchronization between audio and lip movements is critical for the perceived realism of the generated video. We evaluate this alignment using the SyncNet confidence score, where higher score indicates better synchronization between the generated audio and lip motion. (5) Emotional Alignment. In human interaction, the generated portrait should exhibit emotionally appropriate and temporally coherent expressions. We utilize Deep3DFaceRecon [16] to extract 64-dimensional expression features from both the generated and GT videos. The FID of these expression features measures the emotional alignment between the generated and GT videos. (6) Speaker Identity Preservation. Following [9], we compute the timbre similarity (SIM-o) for evaluating the timbre of generated audio. The timbre of our generated audio is controlled by the reference audio of scenarios. The dataset does not currently capture the complex dynamics of multi-party conversations, such as group turn-taking. Extending the data collection to include these multi-person interactions is valuable for future research. the CosyVoice decoder. 6.2. Qualitative and Quantitative Results Our primary quantitative evaluations and ablation studies are presented in Table 2, conducted on VidChatBench. We assess our model under two distinct protocols: The Conditioned Setting, where generation is conditioned on textual inputs, specifically the ground-truth Automatic Speech Recognition (ASR) transcript and descriptive caption. The Dyadic Setting, where the model generates response directly from the audiovisual input of an initiator, thereby simulating natural dyadic interaction. The results reveal several key insights. First, the dyadic setting significantly outperforms the conditioned setting, which we attribute to the richer, fine-grained information preserved in direct audiovisual inputs compared to abstracted textual information. Second, our joint audiovisual generation approach successfully maintains high video quality, demonstrating that incorporating audio as an additional condition does not degrade visual fidelity. Furthermore, our ablation studies validate two crucial components: (1) the spatial transformer, which yields substantial improvements in visual metrics by refining frame-level visual tokens, and (2) the training noise injection strategy, which effectively mitigates error accumulation in the autoregressive process, enhance overall video generation quality. 7. Conclusion We introduce the first large-scale high-quality dataset SpeakerVid-5M, which is designed for audio-visual dyadic interactive virtual human generation task as well as VidChatBench benchmark to evaluate performance of trained models. In addition, we provide baseline dyadic interactive method trained on SpeakerVid-5M. We categorize SpeakerVid-5M into large-scale pretraining subset and curated high-quality subsets for SFT. Experiments demonstrate the effectiveness of our proposed dataset and the highquality Supervised Fine-Tuning (SFT) data. 8. Limitation and Future Work While SpeakerVid-5M represents significant contribution to the field of interactive virtual humans, we still discuss the current scope of this work and highlight several promising avenues for further development. First, our baseline model is presented as preliminary benchmark to validate the SpeakerVid-5M dataset. Its performance was constrained by limited computational resources and does not represent the state-of-the-art. We anticipate that future work using more advanced architectures and large-scale training will unlock the datasets full potential and yield higher-fidelity results. Second, the scope of interaction within SpeakerVid5M is focused on single-person and two-person (dyadic) SpeakerVid-5M: Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "9. Implementation Details During both training and inference, we standardize the frame rate to 8 FPS and resize all video frames to 480768 resolution. The VAE compresses the input with temporal stride of 4 and spatial stride of 8. We treat each 44 feature patch as token embedding, resulting in 360 tokens per frame in the latent space. The spatial transformer further refines each latent frame into 1440 tokens. For the audio modality, each chunk contains 12 audio tokens. To initiate generation, we introduce learnable special tokens as startof-generation embeddings for both the audio and visual tokens. The Qwen2.5-Omni Thinker is kept frozen throughout the training process, while all other components remain trainable, resulting in total of 0.8 billion trainable parameters. The learning rate is set to 1e-4, with warm-up and decay strategies applied. The visual pretraining and joint audio-visual training are conducted over 15 days on 128 NVIDIA L40S GPUs, with video clips in 3-7 seconds. The fine-tuning stage is carried out on 32 NVIDIA A800 GPUs over 5 days, using clips ranging from 3 to 14 seconds. 10. Annotation File Usage In this section, we provide detailed explanation of the annotation files in our SpeakerVid-5M dataset to promote the application. The basic annotation file serves as central repository for essential metadata pertaining to each clip. This includes: Source video ID and URL: Enabling direct access to the original video content. Clip timestamps: The precise start and end times of the clip within the source video. Human bounding box: Spatial localization of the primary subject within the clips frames. Video resolution: The dimensions of the video content. SyncNet confidence score: metric assessing the lip-sync quality. DOVER score: quality score of overall video clip quality. Clarity score: An indicator of visual clarity. Speaker ID: Unique identification based on audio for the speaker present in the clip. These comprehensive annotations are designed to empower users to directly retrieve the original videos from YouTube and efficiently extract high-quality single-person audiovisual clips using straightforward FFmpeg commands. Furthermore, we provide open-source code that facilitates the intelligent linking of these individual clips, thereby enabling the convenient construction of two-person dialogues and more complex multi-turn dialogue datasets. The score Annotation. The score annotation provides quantitative clarity metrics for salient regions within each video, namely the left hand, right hand, and face. For each region, we compute Laplacian-based scores on frame-by-frame basis. These are provided as both absolute scores, which are comparable across the entire dataset, and relative scores, normalized within each clip. Furthermore, the frame-wise absolute scores are aggregated to produce single, holistic clip-level score. This hierarchical annotation is designed for two primary use cases: the fine-grained, frame-level scores serve as dynamic conditioning signals for generative models, enabling more precise control over motion fidelity. In contrast, the clip-level scores offer an effective mechanism for dataset curation, allowing researchers to easily filter for high-clarity training samples to enhance model robustness. The Caption Annotation. The Caption annotation file provides rich set of clip-level semantic labels, automatically generated using large multimodal model (qwen2.5 vl). These annotations offer structured, multifaceted description of each videos content, enabling detailed analysis and control. The specific labels are as follows: Video Quality: Clarity: binary label indicating the absence or presence of significant motion blur or artifacts. Camera Dynamics: Classification of camera motion (e.g., static, panning, zooming) and shot framing (e.g., close-up, medium shot). Subject & Scene Composition: Subject Count: The number of individuals detected in the frame. Framing View: label indicating if the subject is framed in full-body or upper-body view. Head Pose: The estimated orientation of the subjects head (e.g., frontal, side view). Scene Entities: list of recognized objects and persons present. Behavioral & Action Details: Speech Activity: binary label indicating active speech. Motion Status: binary label classifying the primary subject as either static or in motion. Movement Intensity: categorical rating of the subjects motion intensity (e.g., low, medium, high). Holistic Action Summary: high-level textual description of the subjects overall actions. Fine-grained Action Description: detailed textual account of specific, nuanced actions performed by the subject. Facial Expression Summary: textual description of the subjects primary facial expression. Collectively, these annotations furnish structured representation of the video content. This enables both finegrained conditioning for generative tasks and provides robust framework for the objective evaluation of synthesized visual attributes and behaviors. The Scene Annotation. The Scene annotation provides the temporal metadata required for managing and assembling video clips from the original source footage. This annotation is generated through two-stage process. First, an initial scene detection algorithm segments the source videos into coherent shots, recording their start/end timestamps and frame indices. Subsequently, these segments are further partitioned to enforce maximum duration of 14 seconds per clip. The final annotation file catalogs each processed clip with unique identifier and its precise temporal boundaries (start and end times). This structured metadata is crucial for downstream applications, enabling the seamless concatenation of clips for long-form video synthesis and providing flexible framework for tasks involving multi-turn dialogue or extended narrative generation. The Speaker Annotation. Speaker annotation is performed on the original audio of video recordings to provide detailed speaker diarization results. This comprehensive annotation process involves several key components. Initially, raw diarization results are generated, capturing the precise start and end times for each detected speaker turn, along with their respective assigned speaker IDs. These results provide foundational temporal map of all spoken segments. For applications focused on dyadic dialogue scenarios, the raw results undergo filtering process. This step specifically identifies and prioritizes the two primary speakers (labeled as Speaker and Speaker B). critical criterion for this filtering is that these two speakers must collectively account for at least 80% of the total speech duration within the video. Furthermore, in the context of dyadic data filtering, only the conversational exchanges occurring exclusively between Speaker and Speaker are retained. This ensures the selection of highly relevant dialogue for subsequent model training and analysis. The final stage yields cleaned and refined list of speaker IDs and their corresponding start and end times for each segment. This provides clear and accurate speaker attribution of the video, ensuring high-quality data for downstream tasks. The ASR Annotation. ASR annotation is performed on unified single-person audiovisual clips, which are meticulously derived from praevia video and audio processing. For example, in dual-person co-present scenes, the segmentation process spatially divides the video into two distinct regions, each corresponding to an individual. Concurrently, the associated audio is temporally segmented to align with the speech of each respective person. To achieve these single-person audiovisual clipswhere the video prominently features only one person and the audio contains solely that persons speechwe integrate YOLO for visual analysis with speaker diarization for audio segmentation. Upon obtaining these refined clips, automatic speech recognition is then applied to their corresponding audio segments. The ASR output encompasses several key components. Transcribed speech text: The verbatim textual representation of the spoken content. Confidence score: metric indicating the reliability and accuracy of the transcription. Speech compression ratio: The ratio between the original speech duration and its compressed form, relevant for efficiency analysis. No-speech probability: The likelihood that given audio segment contains no discernible speech. Language information: Identification of the language spoken within the clip. 11. Visualization of Pretrain and Finetune"
        },
        {
            "title": "Model",
            "content": "As illustrated in Figure 7, the model resulting from our initial pre-training phase exhibits pronounced motion blur, particularly in the face and hands. These artifacts are significantly exacerbated during rapid movements, degrading perceptual quality. To address this, we curated high-quality subset by filtering the training data based on facial and hand clarity. The figure demonstrates that fine-tuning on this refined data yields substantial improvements in visual fidelity, markedly reducing blur and enhancing detail in regions critical for human perception. To enable this data refinement, we compute blur scores for both the face and hands in each frame using Laplacian-based method. Recognizing their broader utility, we release these scores as part of our public dataset annotations. The value of such explicit quality signals is substantiated by prior work; for instance, CyberHost demonstrated that conditioning model on hand blur scores can significantly enhance the clarity and fidelity of synthesized hand motions. 12. Prompt Used in Annotation Tables 3 and 4 present the caption prompts used for QwenVL-2.5, which serve as effective guidance for generating high-quality responses during captioning. 13. Failed Case and Analysis (1) Fine-grained Hand Dynamics: Prevailing talking-head datasets are predominantly constrained to rudimentary hand motions, such as resting, crossing, or waving. Consequently, the synthesis of more complex and expressive actions, including intricate gesturing or human-object interactions, remains formidable challenge in digital human generation. This difficulty is compounded by the fact that Figure 7. Impact of finetuning on generation quality. comparison between the model after pretraining phase and after finetuning on our high-quality subset. textual descriptions typically lack the requisite granularity to supervise these fine motor skills, thereby hindering models ability to learn rich hand dynamics. To address this limitation, we provide comprehensive annotations for Figure 8. Visualization of multi-turn dialogue and listening scenarios. Top (Multi-turn Dialogue): We showcase sequence of conversational turns between an initiator and responder. By preserving temporal context, our dataset facilitates the training of models capable of coherent, long-form conversations. Bottom (Listening Case): speaker is paired with non-speaking listener. each video clip, encompassing hand keypoints, bounding boxes, and novel hand-blur score. These annotations furnish precise supervisory signals, enabling more effective control and rigorous evaluation of synthesized hand motion quality. (2) Synthesis of Occluded or Out-of-Frame Body Parts: core challenge in this domain is the synthesis of body parts that are occluded or absent in the reference image, for instance, rendering newly raised arm or animating eyes from closed to an open state. This task necessitates models endowed with robust generative priors and comprehensive understanding of human anatomy and plausible motion. The challenge is often exacerbated by data bias; many existing datasets are heavily skewed towards front-facing, tightly-cropped portraits, which lack diverse viewpoints and full-body context. To counteract this, our dataset incorporates wide array of perspectives, including full-body, upper-body, and profile views. This provides more balanced distribution of human body configurations, thereby empowering models to generalize beyond the visible input and mitigate in-painting artifacts. (3) Temporal Coherence in Long-Form Video GenerFigure 9. Diverse examples of dyadic pairs from SpeakerVid-5M. This figure highlights the diversity of subjects, environments, and interaction styles captured in our dataset. ation: Generating temporally coherent, long-form video presents two primary obstacles. First, the scarcity of highquality, long-duration training data fundamentally limits model performance. Our dataset addresses this by providing large corpus of high-fidelity clips (514 seconds) that are specifically designed for seamless concatenation into extended sequences (e.g., 20 seconds to 3 minutes). Second, autoregressive models are prone to error accumulation, which degrades generation quality over time. To mitigate this, we introduce simple yet effective noise injec- # Your Role: Video Annotation Expert Table 3. Prompt Used in Video Annotation ## Objective: As professional annotator, your task is to evaluate and label video content based on clarity, camera motion, human presence and activity, and semantic understanding. Your annotations must be structured and follow predefined JSON format. ## Annotation Guidelines For each input video, assess and annotate the following attributes: 1. Visual Clarity: Assess whether the primary subject in the video is clearly visible and distinguishable. Options: \"Yes\" or \"No\". 2. Camera Motion: Identify the type(s) of camera movement observed. Choose one or more from the following: [\"Static Camera\", \"Dynamic Shooting\", \"Still Shot\", \"Left-to-Right Pan\", \"Right-to-Left Pan\", \"Zoom In\", \"Zoom Out\"]. 3. Human Motion Presence: Determine whether the person in the video exhibits obvious and recognizable physical movement. Options: \"Yes\" or \"No\". 4. Motion Intensity Level: Rate the level of physical activity on scale from 1 (very minimal motion) to 5 (extremely intense movement). Aim to differentiate clearly across the full range rather than concentrating ratings in the middle. 5. Entity List: List all visually prominent entities in the video in descending order of saliency, e.g., [\"man\", \"teddy bear\", \"river\", \"traffic sign\", \"apple\"]. 6. Speech Presence: Identify whether the person is clearly speaking in the video. Options: \"Yes\" or \"No\". 7. Observed Actions: List the specific actions shown in the video, such as [\"running\", \"dancing\", \"eating\", \"talking\", \"singing\", \"presenting\"]. 8. Number of People: Count the number of distinct people visible in the video. 9. Upper-Body Only: Indicate whether only the upper half of the person is visible (if the legs or lower body appear, select No). Options: \"Yes\" or \"No\". 10. Facing Direction: Specify the subjects orientation relative to the camera. Options: \"Front\", \"Side\", or \"Back\". ## Output Format: Return the annotation results in the following JSON structure: 1. Clarity: \"Yes\" 2. Camera Motion: [\"Zoom In\", \"Left-to-Right Pan\"] 3. Human Motion Presence: \"Yes\" 4. Motion Intensity Level: [3] 5. Entity List: [\"man\", \"dog\", \"apple\"] 6. Speech Presence: \"Yes\" 7. Observed Actions: [\"talking\"] 8. Number of People: 1 9. Upper-Body Only: \"Yes\" 10. Facing Direction: \"Side\" Note: Please ignore any subtitles or on-screen text when performing the annotation. Focus solely on the videos visual and auditory content. tion strategy during training. This technique enhances the models temporal stability, ensuring consistent quality over extended generative horizons."
        },
        {
            "title": "References",
            "content": "[1] Veo3. 3 [2] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, Oriol Vinyals, and Andrew Zisserman. Deep audio-visual speech recognition. IEEE transactions on pattern analysis and machine intelligence, 44(12):87178727, 2018. 3 [3] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. Lrs3-ted: large-scale dataset for visual speech recognition. arXiv preprint arXiv:1809.00496, 2018. 3 [4] Tenglong Ao. Body of her: preliminary study on endto-end humanoid agent. arXiv preprint arXiv:2408.02879, 2024. 2, 3 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint # Your Role: Human Motion Expert Table 4. Prompt Used in Video Motion Intensity Level ## Annotation Guidelines: As human motion expert, assess the intensity of movement in the video by focusing on the amplitude and frequency of body movements: 1. Movement Amplitude: Whether the person frequently makes large body movements like arm swings, body turns, etc. 2. Movement Frequency: Whether the person repeats similar movements frequently, such as nodding their head, gesturing, etc. Score range from 1 to 5. 1. The person is nearly stationary, with only minimal head or small hand movements. 2. The person makes occasional small gestures or minor body adjustments. 3. The person uses moderate gestures with occasional body adjustments, such as slight forward leans. 4. The person uses frequent gestures, with larger body movements and more frequent body shifts. 5. The persons movements are frequent and large, with extensive use of hand gestures and body shifts, indicating high intensity. # Your Role: Audience Member ## Annotation Guidelines: As an audience member, assess the movement intensity of the speaker by focusing on their emotional expression and the resulting body movements: 1. Emotional Fluctuations: Whether the speaker shows large emotional fluctuations in their speech, and whether it is accompanied by body language. 2. Audience Reactions: Whether the speaker adjusts their body in response to audience reactions (e.g., applause, nodding). Score range from 1 to 5. 1. The speaker speaks in calm, even tone with minimal body movement. 2. The speaker has slight emotional fluctuations, with occasional small gestures or head movements. 3. The speaker shows moderate emotional fluctuations and occasional body movements like hand gestures or body shifts. 4. The speaker shows significant emotional fluctuations, with frequent body movements, gestures, and emotional intensity. 5. The speaker displays intense emotional fluctuations, with frequent and large gestures and body movements, indicating high intensity. # Your Role: Labeling Expert ## Annotation Guidelines: As labeling expert, assess the intensity of movement by considering the body language and interaction frequency: 1. Gesture Usage: Whether the person frequently uses hand gestures or body movements to emphasize their speech. 2. Interaction Frequency: Whether the person interacts frequently with others, especially through body language responses (e.g., nodding, smiling). Score range from 1 to 5. 1. The person makes little to no gestures or body movements and interacts minimally. 2. The person occasionally uses gestures or makes small body movements, with limited interaction. 3. The person frequently uses gestures and makes moderate body adjustments, with moderate interaction with others. 4. The person uses hand gestures frequently, with larger body movements and frequent interaction. 5. The person has frequent and strong body movements, with highly frequent interactions and large gestures, indicating high intensity. arXiv:2502.13923, 2025. 4, 9 [6] Brandon Castellano. PySceneDetect. https://github. com/Breakthrough/PySceneDetect. 3 [7] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In Proceedbenchmark for human activity understanding. ings of the ieee conference on computer vision and pattern recognition, pages 961970, 2015. [8] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Soleymani. Magicdance: Realistic human dance video generation with motions & facial expressions transfer. CoRR, 2023. 6 [9] Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, and Xie Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. Figure 10. Analysis of failure cases. These include generating facial artifacts or unnatural distortions and struggling with severe motion blur during rapid head or hand movements. arXiv preprint arXiv:2410.06885, 2024. 9 [10] Yi Chen, Sen Liang, Zixiang Zhou, Ziyao Huang, Yifeng Ma, Junshu Tang, Qin Lin, Yuan Zhou, and Qinglin Lu. Hunyuanvideo-avatar: High-fidelity audio-driven huarXiv preprint man animation for multiple characters. arXiv:2505.20156, 2025. 2, 3 [11] Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, et al. 3d-speaker-toolkit: An open source toolkit for multi-modal speaker verification and diarization. 2025. [12] Joon Son Chung and Andrew Zisserman. Lip reading in the In Computer VisionACCV 2016: 13th Asian Conwild. ference on Computer Vision, Taipei, Taiwan, November 2024, 2016, Revised Selected Papers, Part II 13, pages 87103. Springer, 2017. 3 [13] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. arXiv preprint Voxceleb2: Deep speaker recognition. arXiv:1806.05622, 2018. 3, 6 [14] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 8 [15] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. [16] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 00, 2019. 9 [17] Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024. 8 [18] Glenn Jocher and the Ultralytics Team. Ultralytics YOLOv8: https : / / Cutting-edge object detection models. github . com / ultralytics / ultralytics, 2023. Accessed: 2025-06-27. 3 [19] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas Breuel, Gal Chechik, and Yale Song. Acav100m: Automatic curation of large-scale datasets for audio-visual the video representation learning. In Proceedings of IEEE/CVF International Conference on Computer Vision, pages 1027410284, 2021. [20] Chunyu Li, Chao Zhang, Weikai Xu, Jinghui Xie, Weiguo Feng, Bingyue Peng, and Weiwei Xing. Latentsync: Audio conditioned latent diffusion models for lip sync. arXiv preprint arXiv:2412.09262, 2024. 2 [21] Hui Li, Mingwang Xu, Yun Zhan, Shan Mu, Jiaye Li, Kaihui Cheng, Yuxuan Chen, Tan Chen, Mao Ye, Jingdong Wang, et al. Openhumanvid: large-scale high-quality dataset for enhancing human-centric video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 77527762, 2025. 3, 5, 6 [22] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 8 [23] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 8 [24] Gaojie Lin, Jianwen Jiang, Chao Liang, Tianyun Zhong, Jiaqi Yang, Zerong Zheng, and Yanbo Zheng. Cyberhost: one-stage diffusion framework for audio-driven talking body generation. In The Thirteenth International Conference on Learning Representations, 2025. 4 [25] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 2, [26] Chetwin Low and Weimin Wang. Talkingmachines: Realtime audio-driven facetime-style video via autoregressive diffusion models. arXiv preprint arXiv:2506.03099, 2025. 2, 3 [27] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactor-m1: Holistic, expressive and robust human image animation with hybrid guidance. arXiv preprint arXiv:2504.01724, 2025. 2, 3 [28] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: large-scale speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017. 3, 6 [29] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: In ProModeling non-deterministic dyadic facial motion. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2039520405, 2022. 3 [30] Evonne Ng, Sanjay Subramanian, Dan Klein, Angjoo Kanazawa, Trevor Darrell, and Shiry Ginosar. Can language models learn to listen? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10083 10093, 2023. 3 [31] Ziqiao Peng, Jiwen Liu, Haoxian Zhang, Xiaoqiang Liu, Songlin Tang, Pengfei Wan, Di Zhang, Hongyan Liu, and Jun He. Omnisync: Towards universal lip synchronization via diffusion transformers. arXiv preprint arXiv:2505.21448, 2025. 2 [32] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the 28th ACM international conference on multimedia, pages 484492, 2020. [33] Jinwei Qi, Chaonan Ji, Sheng Xu, Peng Zhang, Bang Zhang, and Liefeng Bo. Chatanyone: Stylized real-time portrait video generation with hierarchical motion diffusion model. arXiv preprint arXiv:2503.21144, 2025. 2, 3 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 9 [35] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In Internarecognition via large-scale weak supervision. tional conference on machine learning, pages 2849228518. PMLR, 2023. 4 [36] Akshay Raina and Vipul Arora. Syncnet: Using causal convolutions and correlating objective for time delay estimation in audio signals. arXiv preprint arXiv:2203.14639, 2022. 3 [37] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+ d: large scale dataset for 3d human activity analIn Proceedings of the IEEE conference on computer ysis. vision and pattern recognition, pages 10101019, 2016. 6 [38] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 6 [39] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2024. 2, 3 [40] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. [41] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1797917989, 2023. 2 [42] Duomin Wang, Bin Dai, Yu Deng, and Baoyuan Wang. Disentangling planning, driving and rendering for photorealistic avatar agents. In Computer Vision ECCV 2024 Workshops, pages 137147, Cham, 2025. Springer Nature Switzerland. 2, 3 [43] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. Mead: large-scale audio-visual dataset for emotional In European conference on comtalking-face generation. puter vision, pages 700717. Springer, 2020. 6 [44] Mengchao Wang, Qiang Wang, Fan Jiang, Yaqi Fan, Yunpeng Zhang, Yonggang Qi, Kun Zhao, and Mu Xu. Fantasytalking: Realistic talking portrait generation via coherent benchmark dataset and baseline. In European conference on computer vision, pages 124142. Springer, 2022. 3 [57] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebvhq: large-scale video facial attributes dataset. In European conference on computer vision, pages 650667. Springer, 2022. 6 [58] Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Hu, Shuang Liang, and Zhipeng Ge. Infp: Audio-driven inIn Proteractive head generation in dyadic conversations. ceedings of the Computer Vision and Pattern Recognition Conference, pages 1066710677, 2025. 2, motion synthesis. arXiv preprint arXiv:2504.04842, 2025. 2, 3 [45] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencIn Proceedings of the IEEE/CVF conference on coming. puter vision and pattern recognition, pages 1003910049, 2021. 3 [46] Cong Wei, Bo Sun, Haoyu Ma, Ji Hou, Felix Juefei-Xu, Zecheng He, Xiaoliang Dai, Luxin Zhang, Kunpeng Li, Tingbo Hou, et al. Mocha: Towards movie-grade talking character synthesis. arXiv preprint arXiv:2503.23307, 2025. 2, 3 [47] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. 5 [48] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, arXiv preprint et al. Qwen2. 5-omni technical report. arXiv:2503.20215, 2025. 8 [49] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2024. [50] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 4 [51] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. 4 [52] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial In Proceedings of the IEEE/CVF Context-video dataset. ference on Computer Vision and Pattern Recognition, pages 1480514814, 2023. 3, 6 [53] Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, and Baoyuan Wang. Talking head generation with probabilistic audio-to-visual diffusion priors. 2023 ieee. In CVF International Conference on Computer Vision (ICCV), pages 76117621, 2022. 2 [54] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with In Proceedings of high-resolution audio-visual dataset. the IEEE/CVF conference on computer vision and pattern recognition, pages 36613670, 2021. 3 [55] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy, Xiaogang Wang, and Ziwei Liu. Pose-controllable talking face generation by implicitly modularized audio-visual representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41764186, 2021. [56] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, and Tao Mei. Responsive listening head generation:"
        }
    ],
    "affiliations": [
        "StepFun",
        "The Hong Kong University of Science and Technology",
        "Tsinghua University"
    ]
}