{
    "paper_title": "PixelCraft: A Multi-Agent System for High-Fidelity Visual Reasoning on Structured Images",
    "authors": [
        "Shuoshuo Zhang",
        "Zijian Li",
        "Yizhen Zhang",
        "Jingjing Fu",
        "Lei Song",
        "Jiang Bian",
        "Jun Zhang",
        "Yujiu Yang",
        "Rui Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structured-image tasks. In this paper, we propose PixelCraft, a novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises a dispatcher, a planner, a reasoner, critics, and a set of visual tool agents. To achieve high-fidelity processing, we construct a high-quality corpus and fine-tune an MLLM into a grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. Building on this foundation, PixelCraft facilitates flexible visual reasoning through a dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. Moreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting a new standard for structured image reasoning. Our code will be available at https://github.com/microsoft/PixelCraft."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 8 1 5 2 . 9 0 5 2 : r Preprint. PIXELCRAFT: MULTI-AGENT SYSTEM FOR HIGH-FIDELITY VISUAL REASONING ON STRUCTURED IMAGES Jun Zhangρ Yujiu Yangπ Rui Wangϕ Shuoshuo Zhangϕπ Zijian Liϕρ Yizhen Zhangϕπ Jiang Bianϕ ϕMicrosoft Research ρHong Kong University of Science and Technology {zss24,zhangyizhen24}@mails.tsinghua.edu.cn, zijian.li@connect.ust.hk {jifu,lesong,jiabia,ruiwa}@microsoft.com eejzhang@ust.hk, yang.yujiu@sz.tsinghua.edu.cn Jingjing Fuϕ Lei Songϕ πTsinghua University"
        },
        {
            "title": "ABSTRACT",
            "content": "Structured images (e.g., charts and geometric diagrams) remain challenging for multimodal large language models (MLLMs), as perceptual slips can cascade into erroneous conclusions. Intermediate visual cues can steer reasoning; however, existing cue-based methods are constrained with low-fidelity image processing and linear, rigid reasoning patterns, limiting their effectiveness on complex structuredimage tasks. In this paper, we propose PixelCraft, novel multi-agent system for high-fidelity image processing and flexible visual reasoning on structured images. The system comprises dispatcher, planner, reasoner, critics, and set of visual tool agents. To achieve high-fidelity processing, we construct high-quality corpus and fine-tune an MLLM into grounding model, whose pixel-level localizations are integrated with traditional computer vision (CV) algorithms in tool agents. Building on this foundation, PixelCraft facilitates flexible visual reasoning through dynamic three-stage workflow of tool selection, agent discussion, and self-criticism. Moreover, unlike prior linear reasoning patterns that simply append historical images, PixelCraft maintains an image memory to allow the planner to adaptively revisit earlier visual steps, explore alternative reasoning branches, and dynamically adjust the reasoning trajectory during discussion. Extensive experiments on challenging chart and geometry benchmarks demonstrate that PixelCraft significantly improves visual reasoning performance for advanced MLLMs, setting new standard for structured image reasoning. Our code will be available at https://github.com/microsoft/PixelCraft."
        },
        {
            "title": "INTRODUCTION",
            "content": "Structured images, such as charts and geometric diagrams, pose formidable challenges to current multimodal large language models (MLLMs) (Wang et al., 2025; Yin et al., 2024; Li et al., 2025a; Xia et al., 2025). Whereas natural images are typically characterized by features such as objects, textures, and local visual patterns, which can be effectively captured by existing vision models, structured images encode symbolic and structural elements such as coordinates, data points, line connections, and numerical annotations. Interpreting these structural representations requires precise symbolic abstraction rather than mere pattern recognition. Moreover, reasoning over structured images requires much higher granularity and accuracy. While coarse visual features may suffice for understanding natural images, even subtle differences in structured images (e.g., slightly misreading the height of single bar) can dramatically alter the interpretation and downstream reasoning. To enhance the understanding and reasoning ability of structured images, initial strategies, represented by chain of thought (CoT) (Wei et al., 2022), focused on fine-tuning with specialized textual reasoning paths (Masry et al., 2024b; Huang et al., 2025; Xu et al., 2024). However, relying solely Equal contribution, author order does not indicate contribution. Work done during internship at Microsoft. Corresponding authors. 1 Preprint. Figure 1: Comparison between CoT, visual CoT, and our proposed PixelCraft. Compared with existing methods, PixelCraft enables high-fidelity image processing and flexible visual reasoning. on textual reasoning often leads to the loss of fine-grained spatial and structural information, making it difficult to capture patterns such as subtle visual differences or geometric constraints. Recent studies have attempted to address this limitation by constructing intermediate visual clues to support visual CoT (Meng et al., 2023; Hu et al., 2024; Fu et al., 2025). Despite this innovation, existing methods either depend on underlying source codes, which is often unavailable, or provide only lowfidelity image processing. Consequently, their applicability is restricted to specific set of structured images, and their performance remains limited on increasingly complex and realistic benchmarks, such as CharXiv (Wang et al., 2024) and ChartQAPro (Masry et al., 2025). Besides high-fidelity image processing, structured image understanding also requires accurate and flexible multi-step visual reasoning. However, most existing studies focus on one-step editing or adopt chain-like linear paradigm, where each intermediate image is derived solely from its predecessor (Hu et al., 2024; Fu et al., 2025; Kumar et al., 2024). This linear approach imposes cognitive rigidity, forcing the model into monologue of one-way reasoning. Sophisticated visual analysis, however, is inherently non-linear process of hypothesis testing, formulating premise, exploring its implications visually, and, upon encountering contradictions, backtracking to revise earlier assumptions. While few works on natural images have hinted at non-linearity through zoom-in or multi-region marking (Yang et al., 2023a; Su et al., 2025), they cannot support the rich, recursive exploration needed for structured images. As the example illustrated by Fig. 1, flexible visual reasoning is needed for structured image understanding, which requires the ability to recall historical images, explore multiple reasoning branches, and dynamically adjust the trajectory. To fill this research gap, we propose PixelCraft, multi-agent system designed for high-fidelity image processing and flexible visual reasoning of structured images. The system comprises dispatcher, planner, reasoner, planning critic, visual critic, and suite of visual tool agents. To dismantle the barrier of low-fidelity processing, we architect synergistic approach: compact MLLM (Qwen2.5-VL-3B (Bai et al., 2025)), fine-tuned on our synthetic corpus for precise pixellevel grounding, acts as smart eye to map textual references to coordinates. These coordinates then drive classical computer-vision (CV) operators within our tool agents, which act as robotic hands to perform precise image edits. To break free from the cognitive straightjacket of linear reasoning, we propose three-stage, planner-centric workflow: 1) the dispatcher performs queryaware agent selection; 2) the planner coordinates role-driven discussion among agents to assemble the visual reasoning process; and 3) planning critic inspects the trace for errors, triggering rereasoning. Crucially, we introduce planner-managed image memory that functions as cognitive whiteboard. Instead of indiscriminately feeding all historical images into an ever-expanding context, the planner stores intermediate clues and selectively recalls them for subsequent steps. This enables flexible branching and backtracking, departing from the ephemeral, one-way nature of prior visual CoT methods while simultaneously reducing long-context overhead. The main contributions of our work are summarized as follows: 2 Preprint. We propose PixelCraft, novel multi-agent system for structured image reasoning that integrates query-aware agent selection, agent discussion, and iterative self-correction with plannermanaged image memory. It features planner-managed image memory that selectively recalls prior visual states instead of streaming all images, enabling explicit branching and backtracking while keeping context compact. This non-linear, discussion-centric workflow departs from classic visual-interleaved chains and delivers flexible, high-fidelity reasoning on structured images with reduced long-context overhead and fewer visual errors. PixelCraft enables high-fidelity, pixel-level image processing to enhance the visual reasoning performance on structured images. We construct high-quality corpus to finetune compact MLLM (Qwen2.5-VL-3B) for high-fidelity, pixel-level grounding; its precise coordinates drive classical CV operators inside tool agents, yielding precise, robust and faithful edits. PixelCraft shows strong empirical gains on structured-image benchmarks with component-wise evidence. The gains are consistent and significant across widely used and challenging benchmarks (ChartXiv, ChartQAPro, EvoChart) and hold across diverse advanced backbones (GPT-4o, GPT4.1-mini, and Claude 3.7 Sonnet)."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Visual understanding of structured images. The visual understanding of structured images like charts remains significant challenge for multimodal models (Wang et al., 2024; Masry et al., 2025; Xia et al., 2025). Pioneering works adopted two-stage approach, using OCR for data extraction followed by LLM reasoning (Liu et al., 2022; Lee et al., 2023). To enhance the reasoning capabilities, subsequent research leveraged Chain of Thought (CoT) data to fine-tune models (Masry et al., 2022; Han et al., 2023; Masry et al., 2024b;a; Meng et al., 2024; Li et al., 2025b), with recent works introducing reinforcement learning (RL) (Chen et al., 2025). Despite these advances, performance remains unsatisfactory on challenging benchmarks, particularly when relying solely on models intrinsic capabilities. Tool use for visual reasoning. Recent advancements in MLLMs have been significantly propelled by their integration with external tools like OCR and search engines (Liu et al., 2025; Yin et al., 2024; Wang et al., 2025; Li et al., 2025a). Early tool-augmented methods, such as MM-REACT (Yang et al., 2023b) and ViperGPT (Surıs et al., 2023), primarily generate textual or code-based outputs, orchestrating calls to vision models or generating Python scripts to solve visual queries. To address more complex tasks, subsequent line of research has focused on generating intermediate visual evidence. prominent example, Set-of-Mark (SoM) (Yang et al., 2023a), overlays explicit markers onto image regions to provide the model clear visual vocabulary, paradigm successfully extended to tasks like visual grounding (Jia et al., 2024; Wu et al., 2024; Gao et al., 2024; Liu et al., 2024) and object detection (Zhou et al., 2024; Hu et al., 2024). Our work diverges by focusing on visual reasoning for structured images, domain demanding high-fidelity processing. Recent works Refocus (Fu et al., 2025) and OpenThinkImg (Su et al., 2025) also employ visual tools for chart understanding. However, their toolsets are often specializedfor example, relying on contour or line detectionwhich limits generalizability. Moreover, their linear, chain-like reasoning is inflexible: it lacks backtracking and branching while overlooking recent findings that MLLM performance degrades on multi-image inputs (Tian et al., 2025; Zhang et al., 2025c). Multi-agent collaboration. The multi-agent collaboration paradigm enhances reasoning beyond single-agent systems. prominent approach is multi-agent debate, where agents independently generate solutions and then converge on final answer through aggregation or discussion, thereby improving reasoning quality (Liang et al., 2023; Du et al., 2023). This principle of leveraging solution diversity is also evident in frameworks for voting (Wang et al., 2022), medical decisionmaking (Kim et al., 2024), and group discussion (e.g., ReConcile (Chen et al., 2024)). In contrast to these debate-and-select methods, distinct paradigm focuses on structured collaboration among specialized agents (Wu et al., 2023; Li et al., 2023). popular methodology is role-playing (Shao et al., 2023), where task is decomposed and collaboratively addressed by agents in specific roles, like planner or reasoner (Wu et al., 2023). Our work builds upon this role-playing concept by defining team of agents with synergistic capabilities for planning, image processing, reasoning, and criticizing. While recent work explores dynamic tool generation (Zhang et al., 2025a), creating 3 Preprint. Figure 2: An illustration of the PixelCraft workflow. The process begins with Agent Selection, where the dispatcher chooses the appropriate tools. Next, during Agent Discussion, the planner coordinates tool agents to process the image (e.g., cropping and masking) and the reasoner to perform analysis, with the visual critic providing real-time validation. Finally, the planning critic performs post-hoc review of the entire process, confirming its correctness. high-fidelity visual tools automatically remains challenging. Our framework equips dedicated tool agents based on the LLM-generated tools."
        },
        {
            "title": "3 PIXELCRAFT",
            "content": "We consider the complex visual reasoning tasks of chart and geometric understanding, which require step-by-step intermediate visual clues to obtain the final answer. To achieve this, we propose high-fidelity agents (PixelCraft), multi-modal multi-agent system providing high-fidelity and flexible visual reasoning on structured images. Its design is built on two core principles: synergizing MLLM intelligence with CV algorithms for high-fidelity tool use, and enabling flexible, non-linear reasoning through planner-centric, discussion-based workflow. As illustrated in Fig. 2, MLLMs serve as the dispatcher, planner, reasoner, and critics, collaborating with specialized tool agents to execute three-stage process: 1) query-aware agent selection, 2) role-driven agent discussion, and 3) iterative refinement and self-correction. 3.1 AGENT ROLES Dispatcher. The dispatcher serves as the initial entry point. It analyzes the querys requirements and intelligently activates only the most relevant tool agents for the reasoning process, improving the efficiency and response quality. Planner. Functioning as the conductor of the agent orchestra, the planner orchestrates the entire reasoning process. It decomposes the complex query into manageable sub-tasks, selects the next agent to act, and manages the flow of image and text information. critical feature here is the introduction of an image memory. Conventional approaches that feed all historical images into the context suffer from severe long-context overhead and are restricted to linear, chain-like reasoning pattern. The planners image memory stores all intermediate visual outputs, allowing it to adaptively recall any historical image. This enables the exploration of alternative reasoning branches and facilitates more flexible and effective visual reasoning process for complex queries. Tool Agents. To address the nuanced requirements of visual analysis, we initially attempted to prompt LLMs to generate specialized tool agents based on the ArxivQA training set (Li et al., 2024). However, we found that these auto-generated tools were often ineffective, primarily due to the lack of precise grounding coordinates, or were simply invalid, suffering from incorrect code execution and producing erroneous visual outputs. To overcome these challenges, we adopt two-pronged approach: first, we finetune grounding model on meticulously curated dataset (Section 4) to provide accurate pixel-level references. Second, we manually refine the invalid tools to ensure their functionality and correctness. This semi-automated approach, combining LLM-based generation Preprint. with expert-driven validation, proved crucial for building reliable toolset. The detailed process of tool generation and refinement is available in Appendix A. For chart reasoning, we develop four visual tool agents: 1) Subfigure cropping crops single subfigure from multi-chart image using textual description (e.g., subplot at row 2, column 1). 2) Region magnification zooms into specified region to highlight local details with x/y-axis ticks. 3) Adding auxiliary lines adds reference line with x/y-axis ticks. 4) Masking Data with Legend: masks irrelevant data series by identifying the color associated with specified legend item. For geometric reasoning, we obtain the tool agents based on symbolic entities (points and lines) and their relations: 1) Point Connection draws dashed line segment between two specified points to visualize their geometric relation. 2) Perpendicular Line Construction constructs line perpendicular to reference line and passing through given point. 3) Parallel Line Construction constructs line parallel to reference line and passing through given point. We also include the code execution tool to perform numerical computation for the above two tasks. Reasoner. The reasoner functions as the systems dedicated analysis expert. Given input images and query, it applies logical reasoning to interpret structured data within charts and geometric diagrams. While various sophisticated prompting strategies could be employed, we deliberately use brief generic instruction to demonstrate the inherent power of our high-fidelity tool agents and the overall framework, rather than attributing performance gains to complex prompt engineering. Planning Critic and Visual Critic. To form robust, two-layer error correction mechanism, we introduce two distinct critics operating at different stages of the workflow: 1) visual critic for in-loop verification: Visual tools, unlike deterministic coding, can introduce errors. To ensure the fidelity of the reasoning chain, the visual critic operates in-loop goal satisfaction of the processed image (e.g., verifying crop was successful) and answerability of an image before it is passed to the reasoner. (2) planning critic for post-hoc refinement: It scrutinizes the sequence of tool usage and logical steps for inefficiencies or errors, such as using suboptimal tool or flawed reasoning path. Its feedback is then used to guide the self-correction stage. 3.2 WORKFLOW OF PIXELCRAFT The workflow of PixelCraft is query-aware, role-driven, and visual-critic, which enables highfidelity image processing and flexible visual reasoning. The systems workflow unfolds across the following three stages: Query-aware Agent Selection. The workflow begins with the dispatcher, which analyzes the incoming query to determine the necessary modalities and processing requirements. Based on this initial triage, the dispatcher selects relevant subset of tool agents to perform visual reasoning, together with the reasoner. This careful selection ensures that only the most pertinent agents are activated, thereby optimizing both computational efficiency and the relevance of the subsequent process. Role-driven Agent Discussion. Following agent selection, the planner orchestrates the reasoning process. It decomposes the primary query into manageable subqueries, sequences agent activations, and coordinates all inter-agent communication. At each step, the planner dynamically activates the appropriate agent (either tool agent or the reasoner) and selects an image from the image memory (which contains all historical visual clues and their corresponding descriptions). To ensure high-fidelity execution, the planner assigns specific goal to tool agents (e.g., crop the subfigure at row 1 column 1) or poses precise subquery to the reasoner. Throughout this process, both processed images and textual informationsuch as goals, subqueries, and analytical outputsare flexibly exchanged among agents via the planner, enabling each subquery to be tackled with the most suitable capabilities. To prevent visual errors from propagating, we introduce critical inspection steps. After tool agent processes an image, the visual critic evaluates its goal satisfaction, i.e., whether the visual output successfully fulfills the goal assigned by the planner. Furthermore, when processed image and its associated subquery are sent to the reasoner, the visual critic assesses the images answerability. When an error is detected, the error alert will be returned to the planner for subsequent reasoning, ensuring the reasonings robustness. Preprint. Iterative Refinement and Self-Correction. Once an initial answer is generated, the planning critic performs final review of the entire reasoning process, scrutinizing each step for accuracy, logical consistency, and completeness. By identifying misused tools or erroneous conclusions, the planning critic proposes corrections to the tool list (e.g., adding or removing tools) and provides valuable suggestions to refine the reasoning process, such as proposing better subqueries or tool usage strategies. This feedback serves as additional input for second attempt where PixelCraft re-answers the query. This procedure enables cycle of self-improvement, enhancing the systems decision-making and tool selection over time. The self-correction example can be found in Appendix C.1. Through this structured and collaborative workflow, PixelCraft integrates flexible tool selection, adaptive image memory, robust visual verification, and self-correcting reasoning process. This synergy achieves accurate, flexible, and interpretable visual reasoning for complex queries."
        },
        {
            "title": "4 GROUNDING FOR HIGH-FIDELITY IMAGE EDITING",
            "content": "Precise visual grounding is essential for downstream image editing tools, such as extracting specific subplot for further analysis. However, grounding elements within structured visual inputs like charts, scientific plots, and geometric diagrams poses unique challenges due to their abstract and compositional layouts. While the prior work Refocus (Fu et al., 2025) has approached this by locating objects based on specific visual primitives (e.g., lines, squares), its reliance on these predefined structures limits its applicability to narrow range of chart types. To overcome these limitations, we introduce hybrid dataset of synthesized charts and geometric diagrams, leveraging it to fine-tune Qwen2.5-VL-3B (Bai et al., 2025) for high-fidelity grounding in these challenging domains. We curate hybrid dataset from two complementary sources: (1) programmatically synthesized charts and (2) annotated geometric diagrams. Programmatically Synthesized Charts. To generate large-scale dataset with diverse layouts and precise ground-truth annotations, we propose multi-stage synthesis pipeline. The initial stage focuses on single-panel charts and involves two primary steps: content specification and diversified rendering. First, we employ GPT-4o (Hurst et al., 2024) to produce structured JSON objects that specify textual elements (e.g., titles, legends, axis labels). Second, we build upon set of predefined base code templates, which are then programmatically augmented and rewritten by GPT-4o to enhance visual and structural diversity. These modified templates are subsequently rendered into images via Matplotlib. The rendering process is instrumented to extract the precise position coordinates of all visual elements, thereby generating accurate ground-truth for grounding tasks. To introduce greater structural complexity and align with common subfigure grounding tasks, the second stage of our pipeline composes multi-panel chart figures. We randomly sample, duplicate, and arrange the generated single-panel charts into complex layouts, creating composite images that feature between 2 and 16 subfigures with randomized spatial margins. In total, our pipeline yielded 53k ground-truth annotation pairs, with 43k extracted from our synthesized single-panel charts and the remaining 10k from multi-panel compositions. Geometry Annotation. To enhance our models geometric grounding capabilities, we augment our dataset with 2,000 samples from the Inter-GPS benchmark (Lu et al., 2021). Each sample in this benchmark consists of geometric diagram accompanied by rich annotations of its constituent elements and properties. We process these source annotations to extract the position coordinates of geometric points along with their corresponding textual labels. This curated data is specifically utilized for point-level geometric tools. Supervised Fine-Tuning. We formulate structured image grounding as an autoregressive sequence prediction task. Specifically, given an input image and textual prompt , the model generates sequence = (y1, . . . , yT ) that jointly encodes the textual answer and the corresponding bounding boxes. To this end, we finetune Qwen2.5-VL-3B (Bai et al., 2025) with our curated dataset, where spatial locations are represented by absolute coordinates to align with the models native grounding format. Full training details are provided in Appendix B.1. The finetuned grounding model provides accurate grounding coordinates with the elements in structured images, enabling high-fidelity image processing for visual reasoning. The quantitative evaluation and example comparison of the models grounding accuracy are provided in Section 5.3. 6 Preprint. Table 1: Performance comparison across different models and evaluation methods GPT-4o GPT-4.1-mini Claude-3.7-sonnet Method ChartXiv ChartQAPro EvoChart ChartXiv ChartQAPro EvoChart ChartXiv ChartQAPro EvoChart Direct answer CoT Debate Reconcile Refocus Ours 49.6 51.1 50.7 52.4 47.2 55.2 +5.6 52.51 56.52 49.38 54.36 46.30 58.83 +6.32 62.64 68.64 65.52 65.20 50.88 70.24 +7. 58.6 63.8 62.4 63.5 60.7 68.1 +9.5 57.85 62.21 57.75 59.49 57.24 65.56 +7.71 71.28 76.64 72.72 72.64 58.96 79.44 +8. 67.1 68.3 67.7 68.5 62.4 73.9 +6.8 62.83 66.07 66.58 65.97 58.42 69.82 +6.99 74.16 77.92 78.08 78.56 77.06 80.48 +6. Table 2: Accuracy on the Geometry3K (Lu et al., 2021) auxiliary-line subset. Table 3: Results comparing visual CoT paradigm with ours framework. Model Direct Answer CoT Debate Reconcile Ours CharXiv ChartQAPro GPT-4o GPT-4.1-mini Claude-3.7-sonnet 21.09 24.22 30.47 17.19 25.00 26.56 29.69 28.13 28.13 26.56 32.81 32.03 26.56 34.38 33.59 Visual CoT Ours 65.0 68. 61.04 65."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we first evaluate PixelCraft on chart and geometric reasoning tasks. Ablation studies demonstrate the effectiveness of each component of PixelCraft. Furthermore, the analysis and discussion demonstrate why PixelCraft improves visual reasoning capabilities in the aspects of grounding accuracy, high-fidelity image processing, flexible visual reasoning, tool usage, and selfcorrection. 5.1 RESULTS ON CHART REASONING Benchmarks and Models. Our evaluation is conducted on three recent and challenging chart reasoning benchmarks: CharXiv (Wang et al., 2024), ChartQAPro (Masry et al., 2025), and EvoChart (Huang et al., 2025). We utilize the reasoning-focused question set for CharXiv as commonly used and the full test sets for both ChartQAPro and EvoChart. Following the protocol from CharXiv (Wang et al., 2024), we employ an LLM-as-a-judge approach using GPT-4.1-mini-20250414 to evaluate the correctness of the final answers. To ensure fair comparison across all methods, particularly given the long outputs generated by agentic and CoT approaches, we omit the 5% margin typically used for ChartQAPro and apply this unified evaluation standard consistently. We demonstrate the effectiveness of PixelCraft by integrating it with several powerful MLLMs, including GPT-4o-20240806, GPT-4.1-mini-20250414, and Claude-3.7-sonnet. Baselines. We take Direct answer and chain of thought (CoT) methods as the basic baselines. Additionally, we include more advanced baselines in terms of tool using and multi-agent collaboration: Tool-using method: Although tool-augmented visual reasoning has been widely explored, chart-specific tools remain limited. To our knowledge, Refocus (Fu et al., 2025) is representative chart-tool approach, and we use it as our baseline. Multi-agent methods: we select two popular multi-agent methods: Debate (Du et al., 2023) and Reconcile (Chen et al., 2024). We set up Debate and Reconcile with two reasoners corresponding to the two agents (planner and reasoner) in our work, where these two agents discuss to converge on final answer. Main results. Table 1 shows that our proposed PixelCraft significantly outperforms all the baselines on these three chart-reasoning benchmarks across GPT-4o, GPT-4.1-mini and Claude-3.7-Sonnet. Moving from the naive Direct answer setting to chain-of-thought (CoT) prompting already provides boost (nearly 36 %), confirming the general value of explicit reasoning. The multi-agent collaboration methods, including Debate and Reconcile, show little benefit, indicating that multiple agents without specialized visual tools struggle to perform visual reasoning on charts. Refocus couples an LLM with chart-related tools, performs inconsistently and in several cases lags even the CoT baseline, suggesting that its visual tools are insufficient for complex structured image processing. In contrast, PixelCraft, equipped with high-fidelity tools powered by our fine-tuned grounding model, consistently achieves the highest accuracy across all benchmarks, demonstrating the clear superiority of our approach. Preprint. (a) Comparison of grounding methods. (b) Illustrative grounding example. Figure 3: Grounding case and method comparison. (a) Comparison of different grounding methods within our agent framework, including our fine-tuned model, Qwen2.5-VL-7B and GPT-4.1-mini. (b) Grounding case showing the performance of Qwen2.5-VL-7B, Refocus and Ours, with colored boxes indicating grounded regions. 5.2 RESULTS ON GEOMETRIC REASONING Complex geometric reasoning often requires the visual clues (i.e., auxiliary lines) to solve, where the auxiliary lines need to be sketched in the geometric image to help visual reasoning. To construct such specific geometric benchmark with requirements of visual clues, we filter 128 complex test samples that require the intermediate visual clues to answer from the test set of Geometry3K (Lu et al., 2021). Since the tools from Refocus (Fu et al., 2025) and Visual Sketchpad (Hu et al., 2024) do not support geometric reasoning tasks or require the source code to perform image processing, which is commonly unavailable, we omit them in the evaluation. Main results. The results in Table 2 demonstrate clear hierarchy of performance among different reasoning strategies on the auxiliary-line subset of Geometry3K. The effectiveness of baseline methods progresses logically. Compared to simple Direct Answer, incorporating CoT proves beneficial for most models, underscoring the value of explicit reasoning. Furthermore, the multiagent strategies, Debate and Reconcile, consistently outperform these single-agent approaches. This highlights the power of collaborative frameworks to verify logic and reduce errors in complex tasks. Building upon these insights, our proposed PixelCraft method consistently surpasses all baselines across every model. It achieves the highest accuracy in all test cases, establishing new state-of-theart on this challenging benchmark. This consistent and significant advantage validates the superior design and robustness of our method for solving complex geometric problems. Detailed case studies and visual illustrations are provided in Appendix C.2. 5.3 ANALYSIS AND DISCUSSION To further demonstrate the effectiveness of PixelCraft, we first conduct ablation studies for each component and then explore the question Why can PixelCraft improve the performance?. Ablation studies. To evaluate the effectiveness of each component of our agent, we conduct rolewise ablation study. As shown in Table 4, introducing the Tool Agents (TA) provides the largest average performance gain relative to the no-component CoT baseline, underscoring the critical need for specialized tools in visual analysis. Adding the Dispatcher (Disp) provides further improvements, as filtering irrelevant tools leads to more accurate tool usage. The Visual Critic (VC) also contributes positively by filtering invalid processed images and avoiding erroneous answers. When the three are combined, their synergistic effect brings performance to 67.5% on CharXiv and 64.89% on ChartQAPro. Finally, incorporating the Planning Critic (PC) yields the best overall results of 68.1% and 65.56%, respectively. This systematic analysis validates that each component contributes positively and cumulatively to the agents overall reasoning capabilities. Comparison with visual CoT. To validate the architectural benefits of our agent framework, we conducted controlled comparison against simplified Visual Chain-of-Thought (CoT) baseline, which simply includes all the historical images for reasoning. This baseline utilized the exact same Preprint. Figure 4: Effectiveness and prevalence of tool usage with GPT-4.1-mini on CharXiv (Wang et al., 2024) and Geometry3K (Lu et al., 2021). TA Disp VC PC CharXiv ChartQAPro - - - - 63.8 65.0 65.9 66.0 67.5 68.1 62.21 63.66 64.43 63.96 64.89 65.56 Table 4: Role-wise ablation over Tool Agents (TA), Dispatcher (Disp), Visual Critic (VC), and Planning Critic (PC). Figure 5: Critic-based erroneous query identification (TP/FP counts) and postrefinement accuracy. prompts and tools but employed single planner to handle both task planning and visual reasoning monolithically. As shown in Table 3, our full agent framework significantly outperforms this simplified approach. This result underscores the superiority of our design of the setting of image memory and image selection beyond the visual tools, which allows flexible visual reasoning by allowing image selection and image recall for alternative branches of reasoning. High-fidelity image edits benefit visual reasoning. Precise localization of chart components is critical prerequisite for accurate data extraction, directly contributing to superior performance on downstream chart analysis benchmarks. To achieve this, we finetuned grounding model that substantially boosts localization accuracy, increasing the overall IoU from 0.26 to 0.93 compared to the base model (details in Appendix C.3). The impact of this enhancement is validated on our downstream benchmark. As shown in Fig. 3a, our proposed agent system with the finetuned grounding model significantly outperforms that with the base model. Fig. 3b provides visual explanation for this performance gap: our model accurately localizes the queried subfigure while both the base model and the Refocus method fail, thus enabling more precise subsequent processing. Analysis on visual tool usage frequency and performance gain. We analyze the tool usage frequency and the performance gain of each visual tool. As shown in Fig. 4, LLMs achieve an imbalanced tool calling frequency on CharXiv and Geometry3K, with more preference on Subfigure cropping and Point Connection. This is because the tool calling is query-driven and image-driven. Most of the images on CharXiv are multi-chart images, with queries about deep analysis on single subfigure or comparison between subfigures. The queries on Geometry3K mainly require point connection for mathematical analysis. All of the visual tools improve the performance over the base CoT method without visual tools, with significant improvement on some tools. For instance, Making Data with Legend improves 18.4% accuracy on CharXiv and Parallel Line Construction holds an accuracy improvement of 50% on Geometry3K. This demonstrates the necessity and effectiveness of our proposed visual tool agents. PixelCraft can identify and refine the erroneous answering. After the answer generation, PixelCraft will review the tool usage and the whole reasoning process to identify the erroneous answers for the next-round reanswering. We conduct three-round reviewing and reanswering to evaluate the identification and the reanswering accuracy. Fig. 5 shows that most of the identified queries are incorrect-answering (true positive), and the identification number significantly drops to near 0 after 2 rounds. Furthermore, after reanswering with the updated tool and suggestions, PixelCraft refines some incorrect answers, resulting in higher accuracy. Preprint."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we proposed PixelCraft, novel multi-agent system that unifies high-fidelity, pixellevel tool agents with flexible reasoning workflow for structured image reasoning. Tool agents, powered by fine-tuned pixel-level grounding model and classical CV operators, deliver precise image edits that preserve critical visual evidence. Building on these trusted tool agents, coordinated planner, reasoner, and critic workflows, supported by image memory, enable non-linear, multi-branch exploration and targeted re-reasoning. Across challenging chart and geometry benchmarks, PixelCraft consistently elevates strong MLLMs, establishing new standard for reliable and sophisticated multimodal reasoning."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Justin Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 70667085, 2024. Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Yufeng Zhong, and Lin Ma. Chart-r1: Chain-of-thought supervision and reinforcement for advanced chart reasoner. arXiv preprint arXiv:2507.15509, 2025. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, et al. Cantor: Inspiring multimodal chain-ofthought of mllm. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 90969105, 2024. Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal In The Thirty-eighth Annual Conference on Neural Information Processing language models. Systems, 2024. Muye Huang, Han Lai, Xinyu Zhang, Wenjun Wu, Jie Ma, Lingling Zhang, and Jun Liu. Evochart: benchmark and self-training approach towards real-world chart understanding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 36803688, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Zixi Jia, Jiqiang Liu, Hexiao Li, Qinghua Liu, and Hongbin Gao. Dcot: Dual chain-of-thought In The 16th Asian Conference on Machine Learning prompting for large multimodal models. (Conference Track), 2024. Yubin Kim, Chanwoo Park, Hyewon Jeong, Yik Chan, Xuhai Xu, Daniel McDuff, Hyeonhoon Lee, Marzyeh Ghassemi, Cynthia Breazeal, and Hae Park. Mdagents: An adaptive collaboration of llms for medical decision-making. Advances in Neural Information Processing Systems, 37:7941079452, 2024. 10 Preprint. Somnath Kumar, Yash Gadhia, Tanuja Ganu, and Akshay Nambi. Mmctagent: Multi-modal critical thinking agent framework for complex visual reasoning. arXiv preprint arXiv:2405.18358, 2024. Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: ScreenIn International Conference on shot parsing as pretraining for visual language understanding. Machine Learning, pp. 1889318912. PMLR, 2023. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, et al. Perception, reason, think, and plan: survey on large multimodal reasoning models. arXiv preprint arXiv:2505.04921, 2025a. Zijian Li, Jingjing Fu, Lei Song, Jiang Bian, Jun Zhang, and Rui Wang. Chain of functions: programmatic pipeline for fine-grained chart reasoning data. arXiv preprint arXiv:2503.16260, 2025b. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multiagent debate. arXiv preprint arXiv:2305.19118, 2023. Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From arXiv preprint brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv:2504.01990, 2025. Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Martin Eisenschlos. Matcha: Enhancing visual language pretraining with math reasoning and chart derendering. arXiv preprint arXiv:2212.09662, 2022. Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, 2022. Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. arXiv preprint Instruction tuning for chart comprehension and reasoning. Chartinstruct: arXiv:2403.09028, 2024a. Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. arXiv preprint arXiv:2407.04172, 2024b. Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, et al. Chartqapro: more diverse and challenging benchmark for chart question answering. arXiv preprint arXiv:2504.05506, 2025. 11 Preprint. Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassisstant: universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning. arXiv preprint arXiv:2401.02384, 2024. Fanxu Meng, Haotong Yang, Yiding Wang, and Muhan Zhang. Chain of images for intuitively reasoning. arXiv preprint arXiv:2311.09241, 2023. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: trainable agent for roleIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language playing. Processing, pp. 1315313187, 2023. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1188811898, 2023. Xinyu Tian, Shu Zou, Zhaoyuan Yang, and Jing Zhang. Identifying and mitigating position bias In Proceedings of the Computer Vision and Pattern of multi-image vision-language models. Recognition Conference, pp. 1059910609, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multiagent conversation. arXiv preprint arXiv:2308.08155, 2023. Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Philip Torr, and Jian Wu. Dettoolchain: new prompting paradigm to unleash detection ability of mllm. In European Conference on Computer Vision, pp. 164182. Springer, 2024. Renqiu Xia, Mingsheng Li, Hancheng Ye, Wenjie Wu, Hongbin Zhou, Jiakang Yuan, Tianshuo Peng, Xinyu Cai, Xiangchao Yan, Bin Wang, et al. Geox: Geometric problem solving through unified formalized vision-language pre-training. In ICLR, 2025. Zhengzhuo Xu, Bowen Qu, Yiyan Qi, Sinan Du, Chengjin Xu, Chun Yuan, and Jian Guo. Chartmoe: Mixture of diversely aligned expert connector for chart understanding. arXiv preprint arXiv:2409.03277, 2024. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023a. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023b. Preprint. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xiong-Hui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow generation. In The Thirteenth International Conference on Learning Representations, 2025a. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025b. Yizhen Zhang, Yang Ding, Shuoshuo Zhang, Xinchen Zhang, Haoling Li, Zhong-zhi Li, Peijie Wang, Jie Wu, Lei Ji, Yelong Shen, et al. Perl: Permutation-enhanced reinforcement learning for interleaved vision-language reasoning. arXiv preprint arXiv:2506.14907, 2025c. Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought prompting for visual reasoning refinement in multimodal large language models. arXiv preprint arXiv:2405.13872, 2024. 13 Preprint."
        },
        {
            "title": "A AUTOMATED GENERATION OF VISUAL TOOLS",
            "content": "To automate the discovery and creation of visual tools, we leveraged GPT-4.1-mini to programmatically generate potential tool candidates. We initiated this process by first randomly sampling 500 questions from ArxivQA (Li et al., 2024), which comprises diverse set of charts from arXiv papers. Following this, we engineered detailed prompt designed to instruct the model to generate visual tool codes and corresponding tool descriptions. In total, this process generated 468 tool candidates. To systematically organize the output, we first encoded the name and description of each tool into vector representation using Qwen3Embedding-8B (Zhang et al., 2025b). We then performed clustering analysis on these embeddings to consolidate duplicates and identify key functional categories. An illustrative sample of the resulting clusters is provided in Fig. 6a and Table 5. Our analysis of these clusters revealed that the vast majority of the generated tools fell into few recurring functional categories, primarily those demanding strong visual grounding capabilities like Subf igure Cropping and Adding Auxiliary Lines. Based on this finding, we consolidated the most promising tools from these recurring categories. To improve their robustness and generality, these selected tools were first processed by the GPT-o3 for automated rewriting, followed by final phase of manual tuning to ensure their correctness and practical usability. This curated set of high-fidelity visual tools formed the core of our agents toolkit. Table 5: Cluster statistics and representative tool descriptions. Cluster ID Cluster Size Tool Definition (examples) Summary Tools 1 2 3 148 128 64 Others 27 draw vertical guide line draw horizontal guide line mark threshold highlight subfigure crop subfigure isolate subplot highlight series mask highlight mp curve highlight data points magnify region around crop and magnify region crop region find peaks in data highlight max point crop beta vs time panel highlight intersection point draw spectral type guides Adding Auxiliary Lines Subfigure Cropping Masking Data with Legend Region Magnification Useless tools To probe the breadth of GPT-4.1-minis generative capabilities beyond this initial set, we conducted second experiment where we explicitly constrained the model from producing the known groundingbased tools. This constraint prompted shift towards generating significantly more complex tools, whose procedures were heavily reliant on classical computer vision primitives for precise object detection and data extraction, with representative examples including detecting all data points in scatter plot to compute their mean, or locating the intersection point of two plotted lines. However, our empirical evaluation revealed that these novel computational tools consistently exhibited low accuracy, even after being subjected to the same GPT-o3 rewriting process (as depicted in Fig. 6b). Given the critical impact of tool fidelity on the agents downstream reasoning performanceand the high risk of error propagation from unreliable toolswe made the strategic decision to exclude this latter set from our agents final toolkit during evaluation."
        },
        {
            "title": "B IMPLEMENTATION DETAILS",
            "content": "B.1 TRAINING DETAILS We performed full-parameter supervised fine-tuning on Qwen2.5-VL-3B-Instruct (Bai et al., 2025) with our custom-collected dataset of 53,000 samples. The model was trained for single epoch on 14 Preprint. (a) Clustering visualization (b) Line-intersection case Figure 6: Left: t-SNE projection of the generated tool embeddings. Right: representative case in which classical computer-vision utilities detect the intersection of two lines (red dots) on chart. four NVIDIA A100 GPUs using learning rate of 1 105. representative training prompt is shown below: Example Prompt System: You are helpful assistant specialized in chart analysis. Examine the provided chart image and return the pixel coordinates of the requested element, or return Not found if the element does not exist in the image. User: (example.png) This is chart image. Please locate the pixel coordinates of <object ref start>the subfigure at row 2, column 2<object ref end> in the image. Assistant: <box start>[x1, y1, x2, y2]<box end> B.2 VISUAL TOOL IMPLEMENTATION DETAILS This section provides more detailed breakdown of the implementation for the visual tools introduced in Section 3.1. core component across these tools is our specialized grounding model, which localizes the elements specified in the planners query into precise pixel coordinates within the image.. The operational flow for each tool is as follows: Subfigure Cropping: This tool first employs our grounding model to localize the bounding boxes of both the target subfigure and any relevant legends (e.g., the legend for plot A) based on the textual description. If multiple elements are identified, such as chart and its corresponding but spatially separate legend, the tool programmatically crops these regions and composes them into single, coherent image for analysis. Region Magnification: For this operation, the planner provides the start and end tick values on the and axes that define the region of interest. The grounding model is then tasked with identifying the precise pixel coordinates corresponding to these tick marks. Subsequently, the tool performs crop operation based on these coordinates and resizes the resulting region to achieve magnification, preserving local details. Adding Auxiliary Lines: Similarly to region magnification, this tool leverages the grounding model to determine the pixel locations of specified tick values on an axis. It then renders horizontal or vertical line at this position to serve as visual reference for comparison or thresholding tasks. Masking Data with Legend: This tool operates in two-stage process. First, the grounding model localizes the specified legend item (e.g., the line representing Group A) within the chart. Second, the tool extracts the dominant color from the localized legends icon. This extracted color value is then used to create binary mask for the corresponding data series in the plot area, effectively isolating it from other visual elements. 15 Preprint. Figure 7: Examples of our visual tools for structured images: (a) cropping subfigures, (b) magnifying regions, (c) masking elements by legend, and (d) adding auxiliary lines. Furthermore, our Geometric Tools share unified implementation pipeline where the grounding model is first invoked to detect the key points (e.g., line endings, vertices) specified by the query. The tool then programmatically renders the desired line based on these geometric constraints. Illustrative case studies for these tool implementations are presented in Fig. 7. B.3 PROMPT TEMPLATES Planner You are an expert on chart understanding to analyze the question step by step until answering the final answer. You are given the chart figure <image> with the image path being <image path>, and need to solve the following question: <question>. The question can be solved via the intermediate processed image using tools. To solve the complex task, you can decompose it into some simple subquestions and use tools to generate intermediate reasoning processes until getting the final answer. Available Tools Use the following tools to analyze and process images: <tool descriptions> 16 Preprint. Image Pool <image pool> Instructions 1. Workflow: Actions should be logical and tool-driven rather than directly outputting the answer. Each THOUGHT analyzes progress; if the final answer is reached, output it in FINAL ANSWER with TERMINATE in the ACTION section. Select tools based on their descriptions and constraints; if one fails, try alternatives. Always use image paths from the image pool for tool inputs. 2. Final Answer: Provide concise summary when the solution is found, then end with TERMINATE. 3. Format: Follow the structure strictly: THOUGHT N: [Analysis] ACTION N: tool_name(key=value) OBSERVATION N: [Result] Response History: <history> Current Step Template: THOUGHT {i}: [Your analysis] ACTION {i}: tool_name(key=value) OBSERVATION {i}: [Result or observation] Reasoner System: You are an expert in chart analysis and data visualization. Please analyze the chart carefully and provide accurate answers based on the visual data presented. User: (example.png) {question} Visual Critic Please check whether the given image contains enough information to answer the question: <question>. Examples: If the question requires value extraction from chart, the image should contain the chart with clear axis and data points. If the question is about the trend of chart, it need not contain the chart title or axis labels. If the question is about one subchart compared to another, it is acceptable to have only one subchart in the image. If the image is extremely incomplete or the question is not related to the image, return false. If the image is complete and the question is suitable for answering with the image, return true. Planning Critic You are helpful AI assistant. You are given question and chart image. You need to reason with the question and the chart image, and then evolve the plan to answer the question by adjusting the tools and the corresponding parameters. If the current plan is perfect, you should maintain the current plan. Available Tools: <tool descriptions> 17 Preprint. Question: <question> Reasoning Process: <current plan> Instructions: 1. Analyze the question and the answer to assess the correctness of the final answer based on the image. 2. Some intermediate outputs from tools may be incorrect; analyze the correctness of the extracted information. 3. If the final answer is correct, mark ADJUSTMENT: False; otherwise, mark ADJUSTMENT: True. 4. If the final answer is incorrect, analyze the flawed reasoning process and extracted information, with ADJUSTMENT: True. 5. If some tools failed or were incorrectly applied during the reasoning process, remove them from the tool list. Output the updated tool list as: tools: [tool1, tool2, tool3] 6. If ADJUSTMENT: True, provide detailed suggestions for the incorrect parts. Dispatcher You are an expert chart analyst specializing in automated tool selection for chart analysis tasks. Task Overview Given chart image and related question, you must analyze both inputs and select all possible tools from the list that can help answer the question. Available Tools <tool descriptions> Selection Guidelines 1. Priority Order: Consider tools in order of their relevance to the question type. 2. Empty Selection: Return [] if no tools are suitable. 3. Context Awareness: Consider the description and constraints of each tool carefully. Analysis Framework 1. Question Analysis: Identify the core task (counting, comparison, extraction, calculation). 2. Chart Assessment: Determine chart structure (single/multi-chart, complexity). 3. Tool Matching: Map identified needs to available tools. 4. Validation: Ensure selected tools satisfy constraints and requirements."
        },
        {
            "title": "C ADDITIONAL RESULTS",
            "content": "C.1 SELF-CORRECTION EXAMPLES We present an illustrative case to demonstrate the systems self-correction capabilities. In Fig. 8, the critic identified that the initial reasoning omitted the Subfigure Cropping tool. Based on this feedback, the planner refined its process, leading to the correct answer. C.2 GEOMETRIC REASONING EXAMPLES As shown in Fig. 9, we present representative case to illustrate our systems ability in geometric reasoning. Our visual tools further enable the model to faithfully capture pointline relationships in geometric diagrams, thereby enhancing the accuracy and robustness of its reasoning performance. 18 Preprint. Figure 8: Illustration of self-correction. Figure 9: Illustration of Geometric Reasoning. C.3 ADDITIONAL GROUNDING RESULTS To demonstrate the effectiveness of our finetuned grounding model, we compare its performance with the base model on test set of 500 samples. This test set was annotated using the same pipeline as our synthetic training data to ensure consistency. As summarized in the main text and detailed in Table 6, our finetuned model achieves dramatic improvement in grounding accuracy. The overall accuracy for our model is 0.93, substantial leap from the base models 0.26. Additional visual Table 6: Grounding accuracy on structured chart elements. IoU columns evaluate bounding-box overlap for subplots, legend regions, and textual labels (titles and axis labels). PCK@0.01 measures point localization accuracy for axis tick marks using threshold of 0.01 max(height, width). Model Subplot regions Legend regions Qwen2.5-VL-3B Qwen2.5-VL-7B GPT-4.1-mini Ours (3B) (IoU) 0.27 0.52 0.78 0.99 (IoU) 0.04 0.17 0.18 0. 19 Text labels (IoU) Axis ticks (PCK@0.01) Overall 0.05 0.21 0.11 0.90 0.04 0.15 0.17 0. 0.10 0.26 0.31 0.93 Preprint. Figure 10: Additional grounding examples. examples can be found in Fig. 10. This precise localization of chart components is the foundation upon which our systems downstream chart analysis capabilities are built. C.4 ESTIMATED COSTS To quantify the computational overhead, we report the average inference time of our framework in comparison with chain-of-thought (CoT) prompting. We evaluate 100 sampled questions from CharXiv (Wang et al., 2024) and 100 from ChartQAPro (Masry et al., 2025). As shown in Table 7, our method requires additional latency owing to the multi-step reasoning and tool usage. While slower than CoT, this trade-off is offset by the substantial accuracy gains discussed in the main text. Table 7: Average response time (in seconds) of CoT and our framework using GPT-4.1-mini. Method CharXiv ChartQAPro CoT Ours 3.75 16.45 3.44 11.22 Preprint."
        },
        {
            "title": "D LIMITATIONS AND FUTURE WORKS",
            "content": "While PixelCraft significantly improves performance across several challenging benchmarks, we identify two key limitations that highlight promising directions for future work. One limitation stems from the current inability of MLLMs to autonomously generate reliable, highfidelity visual tools. Our initial explorations found that purely LLM-generated tools were often ineffective, suffering from incorrect code execution for visual editing, which requires slight manual validation and refinement (see details in Appendix A). Future work on fully automated tool generation and verification is thus critical to improve adaptability and reduce this curation overhead. Additionally, PixelCraft requires strong backbone MLLMs for task decomposition and tool orchestration. Weaker ones may constrain the frameworks effectiveness with incorrect planning and tool calling. Future work could investigate mitigation strategies, such as developing more robust agent communication protocols or lightweight, specialized models for planning and criticism. Progress in these complementary directions will be critical for evolving PixelCraft into more general and adaptable framework for structured image reasoning."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Microsoft Research",
        "Tsinghua University"
    ]
}