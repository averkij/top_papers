{
    "paper_title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
    "authors": [
        "Hengyuan Xu",
        "Wei Cheng",
        "Peng Xing",
        "Yixiao Fang",
        "Shuhan Wu",
        "Rui Wang",
        "Xianfang Zeng",
        "Daxin Jiang",
        "Gang Yu",
        "Xingjun Ma",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 5 7 9 4 1 . 0 1 5 2 : r WithAnyone: Towards Controllable and ID Consistent Image Generation Hengyuan Xu1,2 Wei Cheng2, Peng Xing2 Yixiao Fang2 Shuhan Wu2 Rui Wang2 Xianfang Zeng2 Daxin Jiang2 Gang Yu2, Xingjun Ma1, Yu-Gang Jiang1 1 Fudan University 2 StepFun Project Page MultiID-2M MultiID-Bench Models Code Figure 1. Showcases of WithAnyone. WithAnyone is capable of generating high-quality, controllable, and ID-consistent images by leveraging ID-contrastive training on the proposed MultiID-2M dataset."
        },
        {
            "title": "Abstract",
            "content": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct large-scale Wei Cheng leads this project; Corresponding authors. paired dataset MultiID-2M tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose novel training paradigm with contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, diffusion-based model that effectively mitigates copypaste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation. 1. Introduction With the rapid progress of generative artificial intelligence, controllable image generation via reference images or image prompting [16, 19, 44, 57, 59, 66] and identity-consistent (ID-consistent) generation [8, 14, 15, 21, 50, 64, 68] have achieved remarkable advances: modern models can synthesize portraits that closely match the provided individual. Recent efforts [4, 8] push resemblance toward near-perfect reproduction. While pursuing higher similarity seems natural, beyond certain point, excessive fidelity becomes counterproductive. In real photographs of the same person, identity similarity varies substantially due to natural changes in pose, expression, makeup, and illumination  (Fig. 2)  . By contrast, many generative models adhere to the reference image far more rigidly than this natural range of variation. Although such over-optimization may seem beneficial, it suppresses legitimate variation, reducing controllability and limiting practical usability. We term this failure mode the copy-paste artifact: rather than synthesizing an identity in flexible, controllable manner, the model effectively copies the reference image into the output (see Fig. 2). In this work, we formalize this artifact, develop metrics to quantify it, and propose novel training strategy to mitigate it. Mitigating copy-paste artifacts is fundamentally constrained by the lack of suitable training data. While numerous large-scale face datasets exist [9, 22, 29, 47, 51, 67, 70], they remain ill-suited for controllable multi-identity generation. Critically, few datasets provide paired references for each identity-multiple images of the same person across diverse expressions, poses, hairstyles, and viewpoints. As result, most prior work resorts to single-person, reconstructionbased training [14, 50], where the reference and target coincide. This setup inherently promotes copying and exacerbates copy-paste artifacts. Constructing datasets with multiple references per identity, particularly in group photos, and developing methods to effectively exploit such data remain open challenges. In this work, we introduce large-scale open-source Multi-ID dataset, MultiID-2M, together with comprehensive benchmark, MultiID-Bench, designed for intrinsic evaluation of multi-identity image generation. MultiID2M contains 500k group photos featuring 15 recognizable celebrities. For each celebrity, hundreds of individual images are provided as paired references, covering diverse expressions, hairstyles, and viewing angles. In addition, 1.5M unpaired group photos without references are included. MultiID-Bench establishes standardized evaluation protocol for multi-identity generation. Beyond widely adopted metrics such as ID similarity [11, 45], it quantifies copypaste artifacts by measuring distances between generated images, references, and ground truth. Evaluation on 12 stateof-the-art customization models highlights clear trade-off Figure 2. Our Observation. Natural variations, such as head pose, expression, and makeup, may cause more face similarity decrease than expected. Copying reference image limits models ability to respond to expression and makeup adjustment prompts. between ID similarity and copy-paste artifacts (see Fig. 5). Furthermore, we present WithAnyone, novel identity customization model built on the FLUX [27] architecture, as step toward mitigating copy-paste artifacts. WithAnyone maintains state-of-the-art identity similarity (with regard to target image) while substantially reducing copy-paste, thereby breaking the long-observed trade-off between fidelity and artifacts. This advance is enabled by paired-training strategy combined with an ID contrastive loss enhanced with large negative pool, both made possible by our paired dataset. The labeled identities and their reference images enable the construction of an extended negative pool (images of different identities), which provides stronger discrimination signals during optimization. In summary, our main contributions are: MultiID-2M: large-scale dataset of 500k group photos containing multiple identifiable celebrities, each with hundreds of reference images capturing diverse variations, along with 1.5M additional unpaired group photos. This resource supports pre-training and evaluation of multiidentity generation models. MultiID-Bench: comprehensive benchmark with standardized evaluation protocols for identity customization, enabling systematic and intrinsic assessment of multiFigure 3. Overview of WithAnyone. It builds on large-scale dataset, MultiID-2M, constructed through four-step pipeline: (1) collect and cluster single-ID data based on identity similarity; (2) gather multi-ID data via targeted searches using desired identity names with negative keywords for filtering; (3) form image pairs by matching faces between single-ID and multi-ID data; and (4) apply post-processing for quality control and stylization. Training proceeds in four stages: (1) pre-train on single-ID, multi-ID, and open-domain images with fixed prompts; (2) train with image-caption supervision; (3) fine-tune with ID-paired data; and (4) perform quality tuning using curated high-quality subset. identity image generation methods. WithAnyone: novel ID customization model built on FLUX that achieves state-of-the-art performance, generating high-fidelity multi-identity images while mitigating copy-paste artifacts and enhancing visual quality. 2. Related Work Single-ID Preservation. The generation of Identitypreserving images is core topic in customized synthesis [5, 20, 35, 48, 49, 52, 58, 60, 63]. Many methods in the UNet/Stable Diffusion era inject learned embeddings (e.g., CLIP or ArcFace) via cross-attention or adapters [17, 4043, 64]. With the rise of DiT-style backbones [13, 27, 38] (e.g., SD3, FLUX), progress in ID preservation like PuLID [14], also attracts great attention. Multi-ID Preservation. Multi-ID preservation remains relatively underexplored. Some works target spatial control of multiple identities [15, 25, 68], while others focus on identity fidelity. Methods such as XVerse [4] and UMO [8] use VAE-derived face embeddings concatenated with model inputs, which can produce pixel-level copy-paste artifacts and reduce controllability. DynamicID [18]1 achieves improved controllability but is constrained by limited task-specific data 1Excluded from our experiments due to unavailability of code and pretrained models. and evaluation standards. Other general-purpose customization and editing models [2, 30, 36, 37, 5356, 61] can also synthesize images containing multiple identities, but their ID similarity is often compromised for generality. ID-Centric Datasets and Benchmarks. Although there are numerous single-ID datasets [23, 51] and multi-ID collections [9, 22], paired reference images are scarce, so reconstruction remains the dominant training objective for multi-ID datasets. Representative datasets are listed in Table 4. Evaluation protocols are underdeveloped: several works (e.g., PuLID [14], UniPortrait [15], and others [60, 68]) construct test sets by sampling identities from CelebA [29], which undermines reproducibility. Recent efforts benchmark multiple reference generation [54, 71] while focusing on general customization. To address this, we release curated multi-ID benchmark with standardized splits and comprehensive metrics to facilitate future research. 3. MultiID-2M: Paired Multi-Person Dataset"
        },
        {
            "title": "Construction",
            "content": "MultiID-2M is large-scale multi-person dataset constructed via four-stage pipeline: (1) collect single-ID images from the web and construct clean reference bank by clustering ArcFace [11] embeddings, yielding 1M reference images across 3k identities (averaging 400 per identity); (2) retrieve candidate group photos via multi-name and scenea Model Architecture Training Objectives Figure 4. (a) Architecture of WithAnyone: Each reference is encoded by both face-recognition network and general image encoder, yielding identity-discriminative signals and complementary mid-level features. Face embeddings are restricted to attend only to image tokens within their corresponding face regions. (b) Training Objectives of WithAnyone: In addition to the diffusion loss, we incorporate an ID contrastive loss and ground-truthaligned ID loss, which together provide consistent and accurate identity supervision. aware queries and detect faces; (3) assign identities by matching ArcFace embeddings to single-ID cluster centers using cosine similarity (threshold 0.4); and (4) perform automated filtering and annotation, including Recognize Anything [69], aesthetic scoring [12], OCR-based watermark/logo removal, and LLM-based caption generation [1]. The final corpus comprises 500k identified multi-ID images with matched references from the reference bank, as well as 1.5M additional unidentified multi-ID images for reconstruction training, covering 25k unique identities, with diverse nationalities and ethnicities. Further details of the construction pipeline and dataset statistics are provided in Appendix B. 4. MultiID-Bench: Comprehensive ID Customization Evaluation MultiID-Bench is unified benchmark for group-photo (multi-ID) generation. It samples rare, long-tail identities with no overlap to training data, yielding 435 test cases. Each case consists of one ground-truth (GT) image containing 14 people, the corresponding 14 reference images as inputs, and prompt describing the GT. Detailed statistics are provided in Appendix B. Evaluation considers both identity fidelity and generation quality. Let r, t, denote the face embeddings of the reference identity, the target (ground-truth), and the generated image, respectively. We define similarity between two embeddings as Sim(a, b), specifically we term the generated images face similarity with regard to GT as SimGT, and to reference as SimRef , Sim(a, b) = ab b , (1) Specially, we denote SimRef = Sim(r, g) and SimGT = Sim(t, g). Prior works [8, 14, 15, 68] has largely reported only SimRef , which inadvertently favors trivial copy-paste: directly replicating the reference appearance maximizes the score, even when the prompt specifies changes in pose, expression, or viewpoint. In contrast, MultiID-Bench uses SimGT the similarity to the ground-truth identity described by the prompt as the primary metric. This design penalizes excessive copying when natural variations (e.g., pose, expression, occlusion) are expected, while rewarding faithful realization of the prompted scene. We define = arccos(Sim(a, b)) (geodesic distance on the unit sphere). The Copy-Paste metric is given by distance angular θab the as MCP(g t, r) = θgt θgr max(θtr, ε) [1, 1], (2) where ε is small constant for numerical stability. The metric thus captures the relative bias of toward the reference versus the ground truth t, normalized by angular distance of and t. score of 1 means fully coincides with the reference (perfect copy-paste), while 1 means full agreement with the ground truth. We additionally report identity blending, prompt fidelity (CLIP I/T), and aesthetics; formal definitions and further details are provided in Appendix C. 5. WithAnyone: Controllable and ID-"
        },
        {
            "title": "Consistent Generation",
            "content": "Building on the scale and paired-reference supervision of the MultiID-2M, we devise training strategies and tailored objectives that transcend reconstruction to enable robust, identity-conditioned synthesis. This rich, identity-labeled supervision not only substantially improves identity fidelity but also suppresses trivial copypaste artifacts and affords finer control over multi-identity composition. Motivated by these advantages, we introduce WithAnyone - unified architecture and training recipe designed for controllable, high-fidelity multi-ID generation. Architectural schematics and implementation details are provided in Fig. 4 and Appendix E. 5.1. Training Objectives Diffusion Loss. We adopt the mini-batch empirical flowmatching loss. For each batch, we sample data latent x1 pdata, Gaussian noise x0 (0, I), and timestep U(0, 1). We then form the interpolated latent xt = (1 t)x0 + tx1 and regress the target velocity (x1 x0): where g(T) are the detected landmarks, and (, ) extracts the aligned face embedding. Instead of using g(G) as landmarks for G, our GT-aligned ID loss is computed as: Ldiff = (cid:13) (cid:13)vθ(x(i) , t(i), c(i)) (x(i) 1 x(i) 0 )(cid:13) 2 2, (cid:13) (3) Lid = 1 cos(f (g(T), G), (g(T), T)). (6) where c(i) denotes the conditioning signal. Ground-truth-Aligned ID Loss. Since ArcFace embedding requires landmark detection and alignment, directly extracting landmarks from Igen is unreliable because generated images are obtained through noisy diffusion or one-step denoising. Prior methods compromise: PortraitBooth [39] applies the loss only at low noise levels (t < 0.25), discarding supervision at higher noise, while PuLID [14] fully denoises generated results at significant computational cost. In contrast, we align the generated image using GT landmarks, thereby avoiding noisy landmark extraction. We minimize the cosine distance between GT-aligned ArcFace embeddings of the generated and ground-truth (GT) faces: LID = 1 cos(g, t) (4) where and are ArcFace embeddings of the generated and GT images. This design (1) enables applying the ID loss across all noise levels, (2) incurs negligible overhead throughout training, and (3) implicitly supervises generated landmarks. Ablation studies (Sec. 6.3) demonstrate more accurate identity measurement and substantially improved identity preservation. Denoting the face recognition model as (, ) (Arcface [11], in our case), and the coupled detection model as g() (RetinaFace [10]), the generated image as G, and the ground-truth image as T, embedding extraction should be performed as follows: = (g(T), T), (5) ID Contrastive Loss With Extended Negatives. To further strengthen identity preservation, we introduce an ID contrastive loss that explicitly pulls the generated image closer to its reference images in the face embedding space while pushing it away from other identities. The loss follows the InfoNCE [31] formulation: LCL = log exp(cos(g, r)/τ ) j=1 exp(cos(g, nj))/τ ) (cid:80)M , (7) where is the embedding of reference image of the same identity as the generated image, nj are embeddings of negatives from different identities, and τ is temperature hyperparameter. This formulation relies on ID-labeled datasets, which make it possible to draw thousands of negatives per sample from the reference bank, thereby greatly enriching the diversity of negative examples. The overall training objective is weighted sum of the above losses: = Ldiff + λIDLID + λCLLCL, (8) where λID and λCL are hyper-parameters controlling the contributions of the ID loss and contrastive loss, respectively. Both are set to 0.1 across all training phases described below. 5.2. Training pipeline Copypaste artifacts largely arise from reconstruction-only training, which encourages models to replicate the reference Table 1. Quantitative comparison on the single-person subset of MultiID-Bench and OmniContext. second-, and third-best performance, respectively. For Copy-Paste ranking, only cases with Sim(GT) > 0.40 are considered. , and , indicate the first-, MultiID-Bench OmniContext Single Character Subset Method DreamO OmniGen OmniGen2 FLUX.1 Kontext Qwen-Image-Edit GPT-4o Native UNO USO UMO UniPortrait ID-Patch InfU PuLID InstantID Ours GT Ref Identity Metrics Generation Quality Sim(GT) Sim(Ref) CP CLIP-I CLIP-T Aes 0.454 0.398 0.365 0.324 0.324 0.425 0.304 0.401 0.458 0.447 0.426 0.439 0.452 0.464 0. 1.000 0.521 0.694 0.602 0.475 0.408 0.409 0.579 0.428 0.635 0.732 0.677 0.633 0.630 0.705 0.734 0.578 0.521 1.000 0.303 0.248 0.142 0.099 0.093 0.178 0.141 0.286 0.359 0.265 0.231 0.233 0.315 0.337 0. -0.999 0.999 0.793 0.780 0.787 0.755 0.776 0.794 0.765 0.790 0.783 0.793 0.792 0.772 0.779 0.764 0.798 N/A N/A 0.322 0.317 0.331 0.327 0.316 0.311 0.314 0.329 0.305 0.319 0.312 0.328 0.305 0.295 0. N/A N/A 4.877 5.069 4.991 5.319 5.056 5.344 4.923 5.077 4.850 5.018 4.900 5.359 4.839 5.255 4.783 N/A N/A Method DreamO OmniGen OmniGen2 FLUX.1 Kontext Qwen-Image-Edit GPT-4o Native UNO USO UMO UniPortrait ID-Patch InfU PuLID InstantID Ours Quality Metrics Overall PF SC Overall 8.13 7.50 8.64 7.72 7.66 7.98 7.22 6.96 6. 6.62 N/A 7.69 6.62 4.89 7.43 7.09 5.52 8.50 8.60 8.16 9.06 7.72 7.88 7.92 6.00 N/A 4.62 6.83 5.49 7.04 7.02 5.47 8.34 7.94 7.51 8.12 7.04 6.70 6.79 5.55 N/A 4.70 5.78 4.35 6.52 Single-ID subset Multi-ID subset Figure 5. Trade-off between Face Similarity and Copy-paste. Except for WithAnyone, the other models fall roughly on fitted curve, illustrating clear trade-off between face similarity and copy-paste. Upper-right corner is desired. image rather than learn robust identity-conditioned generation. Leveraging our paired dataset, we employ four-phase training pipeline that gradually transitions the objective from reconstruction toward controllable, identity-preserving synthesis. Phase 1: Reconstruction pre-training with fixed prompt. We begin with reconstruction pre-training to initialize the backbone, as this task is simpler than full identityconditioned generation and can exploit large-scale unlabeled data. For the first few thousand steps, the caption is fixed to constant dummy prompt (e.g., two people), ensuring the model prioritizes learning the identity-conditioning pathway rather than drifting toward text-conditioned styling. The full MultiID-2M is used in this phase, which typically lasts for 20k steps, at which point the model achieves satisfactory identity similarity. To further enhance data diversity, CelebAHQ [23], FFHQ [24], and subset of FaceID-6M [51] are also incorporated. Phase 2: Reconstruction pre-training with full captions. This phase aligns identity learning with textconditioned generation and lasts for an additional 40k steps, during which the model reaches peak identity similarity. Phase 3: Paired tuning. To suppress trivial copypaste behavior, we replace 50% of the training samples with paired instances drawn from the 500k labeled images in MultiID2M. For each paired sample, instead of using the same image as both input and target, we randomly select one reference image from the identitys reference set and another distinct image of the same identity as the target. This perturbation breaks the shortcut of direct duplication and compels the model to rely on high-level identity embeddings rather than low-level copying. Phase 4: Quality tuning. Finally, we fine-tune on curated high-quality subset augmented with generated stylized variants to (i) enhance perceptual fidelity and (ii) improve style robustness and transferability. This phase refines texture, lighting, and stylistic adaptability while preserving the strong identity consistency established in earlier phases. 6. Experiments In this section, we present comprehensive evaluation of baselines and our WithAnyone model on the proposed MultiID-Bench. Baselines. We evaluate two categories of baseline methods: general customization models and face customization methods. The general customization models include OmniGen [61], OmniGen2 [54], Qwen-Image-Edit [53], FLUX.1 Kontext [2], UNO [56], USO [55], UMO [8], and native GPT-4o-Image [32]. The face customization methods include UniPortrait [15], ID-Patch [68], PuLID [14] (referring to its FLUX [27] implementation throughout this paper), and InstantID [50]. All models were evaluated on the singleperson subset of the benchmark, while only those supporting Table 2. Quantitative comparison on the multi-person subset of MultiID-Bench. indicate the first-, second-, and third-best performance, respectively. For Copy-Paste ranking, only cases with Sim(GT) > 0.35 are considered. GPT exhibits prior knowledge of identities from TV series in subsets with more than two IDs, leading to abnormally high similarity scores. , and , 2-people Subset 3-and-4-people Subset Method DreamO OmniGen OmniGen2 GPT UNO UMO UniPortrait ID-Patch Ours Identity Metrics Generation Quality Sim(GT) Sim(Ref) CP Bld CLIP-I CLIP-T Aes 0.359 0.345 0.283 0.332 0.223 0.328 0.367 0.350 0.405 0.514 0.529 0.353 0.400 0.274 0.491 0.601 0.517 0. 0.179 0.209 0.081 0.061 0.043 0.176 0.254 0.183 0.161 0.105 0.110 0.112 0.092 0.082 0.111 0.075 0.085 0.079 0.763 0.750 0.763 0.774 0.735 0.743 0.750 0.767 0. 0.319 0.326 0.334 0.328 0.325 0.316 0.323 0.326 0.321 4.764 5.152 4.547 5.676 4.805 4.772 5.187 4.671 4.883 Method DreamO OmniGen OmniGen2 GPT UNO UMO UniPortrait ID-Patch Ours Identity Metrics Generation Quality Sim(GT) Sim(Ref) CP Bld CLIP-I CLIP-T Aes 0.311 0.345 0.288 0.445 0.228 0.318 0.343 0.379 0.414 0.427 0.529 0.374 0.484 0.276 0.465 0.517 0.543 0. 0.116 0.209 0.099 0.048 0.046 0.180 0.178 0.195 0.171 0.081 0.110 0.071 0.044 0.065 0.070 0.048 0.059 0.045 0.709 0.750 0.734 0.815 0.717 0.717 0.708 0.781 0. 0.317 0.326 0.329 0.320 0.319 0.309 0.323 0.329 0.325 4.695 5.152 4.664 5.647 4.880 4.946 5.090 4.547 4.955 Figure 6. Qualitative Results of Different Generation Methods. The text prompt is extracted from the ground-truth image shown on the leftmost side. multi-ID generation were additionally tested on the multiperson subset. Further implementation details are provided in Appendix F.1. 6.1. Quantitative Evaluation The quantitative results are reported in Tables 1 and 2. We observe clear trade-off between face similarity and copy-paste artifacts. As shown in Fig. 5, most methods align closely with regression curve, where higher face similarity generally coincides with stronger copy-paste. This indicates that many existing models boost measured similarity by directly replicating reference facial features rather than synthesizing the identity. In contrast, WithAnyone deviates substantially from this curve, achieving the highest face similarity with regard to GT while maintaining markedly lower copy-paste score. WithAnyone also achieves the highest score among IDspecific reference models on the OmniContext [54] benchmark. However, VLMs [1, 32] exhibit limited ability to distinguish individual identities and instead emphasize nonidentity attributes such as pose, expression, or background. Despite that general customization and editing models often outperform face customization models on OmniContext, WithAnyone still has best performance among face customization models. 6.2. Qualitative Comparison To complement the quantitative results, Fig. 6 presents qualitative comparisons between our method, state-of-the-art general customization/editing models, and face customization generation models. It shows that identity consistency remains significant weakness of general customization or editing models, consistent with our quantitative findings. Many VAE-based approaches where references are encoded through VAE, such as FLUX.1 Kontext and DreamO tend to produce faces that either exhibit copy-paste artifacts or deviate markedly from the target identity. likely reason is that VAE embeddings emphasize low-level features, leaving high-level semantic understanding to the diffusion backbone, which may not have been pre-trained for this task. ID-specific reference models also struggle with copy-paste artifacts. For example, they fail to make the subject smile when the reference image is neutral and often cannot adjust head pose or even eye gaze. In contrast, WithAnyone generates flexible, controllable faces while faithfully preserving identity. 6.3. Ablation and User Studies To better understand the contribution of each component in WithAnyone, we conduct ablation studies on the training strategy, the GT-aligned ID loss, the InfoNCE-based ID loss, and our dataset. Due to space constraints, we report Table 3. Ablation Study. indicate the first, second, third performance respectively. We ablate paired data training (without stage 2, w/o s2), GT-Aligned landmark ID loss (Self-aligned, S.A.), extended negative samples in InfoNCE (w/o neg). And model trained on FFHQ is also compared. Ablation Identity Metrics Generation Quality Sim(G) Sim(R) CP CLIP-I CLIP-T Aes Phases w/o Phase 3 Loss Data Ours w/o GT-Align w/o Ext. Neg. FFHQ only Full Setting 0. 0.385 0.368 0.224 0.405 0.625 0.549 0.455 0. 0.551 0.239 0.175 0.074 0.027 0.161 0. 0.763 0.740 0.658 0.770 0.307 0.317 0.304 0. 0.321 4.955 4.754 4.984 5.039 4.883 Figure 7. Comparison of GT-aligned and Prediction-aligned landmarks. the key results here, with additional analyses provided in Appendix G. As shown in Table 3, the paired-data fine-tuning phase reduces copy-paste artifacts without diminishing similarity to the ground truth, while training on FFHQ performs significantly worse than on our curated dataset. Fig. 7 further demonstrates that the GT-aligned ID loss lowers denoising error at low noise levels and yields higher-variance, more informative gradients at high noise, thereby strengthening identity learning. By ablating extended negatives, leaving only 63 negative samples from the batch (originally extended to 4096), the effectiveness of ID contrastive loss is greatly reduced. More ablation results can be found in Appendix G. We conduct user study to evaluate perceptual quality and identity preservation. Ten participants were recruited and asked to rank 230 groups of generated images according to four criteria: identity similarity, presence of copy-paste artifacts, prompt adherence, and aesthetics. The results, shown in Fig. 8, indicate that our method consistently achieves the highest average ranking across all dimensions, demonstrating both stronger identity preservation and superior visual quality. Moreover, the copy-paste metric exhibits moderate positive correlation with human judgments, suggesting that it captures perceptually meaningful artifacts. Further details of the study design, ranking protocol, and statistical analysis are provided in Appendix H. 7. Conclusion Copy-paste artifacts are common limitation of identity customization methods, and face-similarity metrics often exacerbate the issue by implicitly rewarding direct copying. In this work, we identify and formally quantify this failure mode through MultiID-Bench, and propose targeted solutions. We curate MultiID-2M and develop training strategies and loss functions that explicitly discourage trivial replication. Empirical evaluations demonstrate that WithAnyone significantly reduces copy-paste artifacts while maintaining and in many cases improving identity similarity, thereby breaking the long-standing trade-off between fidelity and copying. These results highlight practical path toward more faithful, controllable, and robust identity customization. Figure 8. User study. Bigger bubbles indicate higher ranking."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 8, 19 [2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, 2025. 3, 6, 13 [3] Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395, 2024. 15 [4] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. 2, 3 [5] Weifeng Chen, Jiacheng Zhang, Jie Wu, Hefeng Wu, Xuefeng Xiao, and Liang Lin. Id-aligner: Enhancing identity-preserving text-to-image generation with reward feedback learning. arXiv preprint arXiv:2404.15449, 2024. 3 [6] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu Chen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming Yu, Zhengyu Lin, et al. Dna-rendering: diverse neural actor repository for high-fidelity humancentric rendering. In ICCV, 2023. 14 [7] Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu, Kwan-Yee Lin, and Hongsheng Li. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798, 2022. [8] Yufeng Cheng, Wenxu Wu, Shaojin Wu, Mengqi Huang, Fei Ding, and Qian He. Umo: Scaling multiidentity consistency for image customization via matching reward. arXiv preprint arXiv:2509.06818, 2025. 2, 3, 4, 6 [9] Jiaming Chu, Lei Jin, Yinglei Teng, Jianshu Li, Yunchao Wei, Zheng Wang, Junliang Xing, Shuicheng Yan, and Jian Zhao. Uniparser: Multi-human parsing with unified correlation representation learning. TIP, 2024. 2, 3, 14 [10] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Singleshot multi-level face localisation in the wild. In CVPR, 2020. 5 [11] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, 2019. 2, 3, 5, 19 [12] discus0434. aesthetic-predictor-v2-5. https : / / github . com / discus0434 / aesthetic - predictor-v2-5, 2023. Accessed: 2025-05-12. 4, 13, 15 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 3 [14] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, Peng Zhang, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. In NeurIPS, 2024. 2, 3, 4, 5, 6, 15 [15] Junjie He, Yifeng Geng, and Liefeng Bo. Uniportrait: unified framework for identity-preserving single-and multi-human image personalization. ICCV, 2025. 2, 3, 4, 6, 14 [16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Promptto-prompt image editing with cross attention control. ICLR, 2023. [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 3 [18] Xirui Hu, Jiahao Wang, Hao Chen, Weizhan Zhang, Benqi Wang, Yikun Li, and Haishun Nan. Dynamicid: Zero-shot multi-id image personalization with flexible facial editability. ICCV, 2025. 3 [19] Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, and Hui Xiong. Simulating the real world: unified survey of multimodal generative models. arXiv preprint arXiv:2503.04641, 2025. 2 [20] Junha Hyung, Jaeyo Shin, and Jaegul Choo. Magicapture: High-resolution multi-concept portrait customization. In AAAI, 2024. 3 [21] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, and Xin Lu. Infiniteyou: Flexible photo recrafting while preserving your identity. ICCV, 2025. 2 [22] Qing Jiang, Lin Wu, Zhaoyang Zeng, Tianhe Ren, Yuda Xiong, Yihao Chen, Qin Liu, and Lei Zhang. Referring to any person. 2025. 2, 3, 14 [23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. ICLR, 2018. 3, 6 [24] Tero Karras, Samuli Laine, and Timo Aila. stylebased generator architecture for generative adversarial networks. In CVPR, 2019. 6, 19 [25] Chanran Kim, Jeongin Lee, Shichang Joung, Bongmo Kim, and Yeul-Min Baek. Instantfamily: Masked attention for zero-shot multi-id image generation. arXiv preprint arXiv:2404.19427, 2024. [26] Minchul Kim, Anil Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In CVPR, 2022. 19 [27] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3, 6, 13 [28] Black Forest Labs. https : //huggingface.co/blackforestlabs/ FLUX.1-Krea-dev, 2025. 13 Flux.1 krea. [29] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, 2015. 2, 3, [30] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. SIGGRAPH Asia, 2025. 3 [31] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 5 [32] OpenAI. Addendum to gpt-4o system card: Native image generation, 2025. 6, 8 [33] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin ElNouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. arXiv:2304.07193, 2023. 14 [34] Dongwei Pan, Long Zhuo, Jingtan Piao, Huiwen Luo, Wei Cheng, Yuxin Wang, Siming Fan, Shengqi Liu, Lei Yang, Bo Dai, et al. Renderme-360: large digital asset library and benchmarks towards high-fidelity head avatars. NeurIPS, 2023. [35] Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Jiankang Deng, Bernhard Kainz, and Stefanos Zafeiriou. Arc2face: foundation model for id-consistent human faces. In ECCV, 2024. 3 [36] Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, and Kfir Aberman. Object-level visual prompts for compositional image generation. arXiv preprint arXiv:2501.01424, 2025. 3 attention: Semantic-aware attention values for concept personalization. In SIGGRAPH, 2025. 3 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3 [39] Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. Portraitbooth: versatile portrait model for fast identity-preserved personalization. In CVPR, 2024. [40] Guocheng Qian, Kuan-Chieh Wang, Or Patashnik, Negin Heravi, Daniil Ostashev, Sergey Tulyakov, Daniel Cohen-Or, and Kfir Aberman. Omni-id: Holistic identity representation designed for generative tasks. CVPR, 2025. 3 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 15 [42] Xingyu Ren, Alexandros Lattas, Baris Gecer, Jiankang Deng, Chao Ma, and Xiaokang Yang. Facial geometric detail recovery via implicit representation. In FG, 2023. 13 [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 3 [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2 [45] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: unified embedding for face recognition and clustering. In CVPR, 2015. 2, [46] Erich Schubert, Jörg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu. Dbscan revisited, revisited: why and how you should (still) use dbscan. TODS, 2017. 13 [47] Lorenzo Stacchio, Alessia Angeli, Giuseppe Lisanti, Daniela Calanca, and Gustavo Marfia. Imago: family photo album dataset for socio-historical analysis of the twentieth century. arXiv preprint arXiv:2012.01955, 2020. 2, 14 [48] Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. Face0: Instantaneously conditioning textto-image model on face. In SIGGRAPH Asia, 2023. 3 [49] Qinghe Wang, Xu Jia, Xiaomin Li, Taiqing Li, Liqian Ma, Yunzhi Zhuge, and Huchuan Lu. Stableidentity: Inserting anybody into anywhere at first sight. TMM, 2025. 3 [37] Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, and Daniel Cohen-Or. Nested [50] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Instantid: Zero-shot identityand Anthony Chen. preserving generation in seconds. arXiv:2401.07519, 2024. 2, 6 arXiv preprint free multi-subject image generation with localized attention. IJCV, 2025. 3 [51] Shuhe Wang, Xiaoya Li, Jiwei Li, Guoyin Wang, Xiaofei Sun, Bob Zhu, Han Qiu, Mo Yu, Shengjie Shen, Tianwei Zhang, et al. Faceid-6m: large-scale, opensource faceid customization dataset. arXiv preprint arXiv:2503.07091, 2025. 2, 3, 6 [52] Yibin Wang, Weizhong Zhang, Jianwei Zheng, and Cheng Jin. High-fidelity person-centric subject-toimage synthesis. In CVPR, 2024. [53] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3, 6 [54] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 3, 6, 8, 19 [55] Shaojin Wu, Mengqi Huang, Yufeng Cheng, Wenxu Wu, Jiahe Tian, Yiming Luo, Fei Ding, and Qian He. Uso: Unified style and subject-driven generation via disentangled and reward learning. arXiv preprint arXiv:2508.18966, 2025. 6 [56] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. ICCV, 2025. 3, 6, 14 [57] Tong Wu, Yinghao Xu, Ryan Po, Mengchen Zhang, Guandao Yang, Jiaqi Wang, Ziwei Liu, Dahua Lin, and Gordon Wetzstein. Fiva: Fine-grained visual attribute dataset for text-to-image diffusion models. NeurIPS, 2024. 2 [58] Yi Wu, Ziqiang Li, Heliang Zheng, Chaoyue Wang, and Bin Li. Infinite-id: Identity-preserved personalization via id-semantics decoupling paradigm. In ECCV, 2024. [59] Chufeng Xiao and Hongbo Fu. Customsketching: Sketch concept extraction for sketch-based image synthesis and editing. In Computer Graphics Forum. Wiley Online Library, 2024. 2 [60] Guangxuan Xiao, Tianwei Yin, William Freeman, Frédo Durand, and Song Han. Fastcomposer: Tuning- [61] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 3, 6 [62] Hengyuan Xu, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. Permutation equivariance of transformers and its applications. In CVPR, 2024. 15 [63] Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege Zhang, Pei Cheng, Gang Yu, and Bin Fu. Facestudio: Put your face everywhere in seconds. arXiv preprint arXiv:2312.02663, 2023. 3 [64] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter arXiv preprint for text-to-image diffusion models. arxiv:2308.06721, 2023. 2, 3, [65] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 15, 19 [66] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2 [67] Ning Zhang, Manohar Paluri, Yaniv Taigman, Rob Fergus, and Lubomir Bourdev. Beyond frontal faces: Improving person recognition using multiple cues. In CVPR, 2015. 2, 14 [68] Yimeng Zhang, Tiancheng Zhi, Jing Liu, Shen Sang, Liming Jiang, Qing Yan, Sijia Liu, and Linjie Luo. Id-patch: Robust id association for group photo personalization. In CVPR, 2025. 2, 3, 4, 6, 14, 19 [69] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. arXiv preprint arXiv:2306.03514, 2023. 4, 13 [70] Yujie Zhong, Relja Arandjelovic, and Andrew Zisserman. Compact deep aggregation for set retrieval. In ECCV, 2018. 2, [71] Cailin Zhuang, Ailin Huang, Wei Cheng, Jingwei Wu, Yaoqi Hu, Jiaqi Liao, Hongyuan Wang, Xinyao Liao, Weiwei Cai, Hengyuan Xu, et al. Vistorybench: Comprehensive benchmark suite for story visualization. arXiv preprint arXiv:2505.24862, 2025."
        },
        {
            "title": "Appendix",
            "content": "A. Family of WithAnyone FLUX.1 comprises family of models, including FLUX.1 [27], FLUX.1 Kontext [2] and FLUX.1 Krea [28]. Krea is text-to-image model with improved real-person face generation, whereas Kontext is an image-editing model that excels at making targeted adjustments while preserving the rest of the image. However, as reported in Table 1, Kontext shows limited consistency with the reference face identity. Our method, WithAnyone, can be seamlessly integrated into Kontext for the face customization downstream tasks like face editing. As illustrated in Fig. 9, WithAnyone effectively injects identity information from the reference images into the target image. The overall training pipeline follows the procedure described in Sec. 5, with single modification: the input image provided to Kontext (whose tokens are concatenated with the noisy latent at each denoising step) is set to the target image with the face region blurred. B. MultiID-2M Construction Details To fill in the void left by the lack of publicly available multiID datasets, data constraction pipeline is proposed to create large-scale dataset of multi-person images with paired identity references for identities on the data record. Based on this pipeline, 500k group photo images are collected, featuring 3k identities, each with hundreds of single-ID reference images. Another 1M images that cannot be identified are also included in the dataset for image reconstruction training purpose for image reconstruction training purpose. B.1. Dataset Construction Pipeline The pipeline contains four steps, as shown in Fig. 3. The detailed pipeline are as follows. Single-ID images. To construct ID reference set, singleID images were collected from the web using celebrity names as search queries on Google Images. For each image, facial features were extracted with ArcFace [42], ensuring that only images containing exactly one face were retained. To remove outliers, DBSCAN [46] clustering was applied to the embeddings for each celebrity, resulting in set of cluster centers and hundreds of reference images per identity. This process established reliable reference set for each unique identity. Human review confirms the accuracy of the ID bank built in this step. Multi-ID images. To achieve best searching efficiency, group photos were obtained using more complex queries that combined multiple celebrity names, keywords indicating the number of people (e.g., two celebrities), scene descriptors (e.g., award ceremony), and negative keywords to filter out irrelevant results. ArcFace embeddings were extracted for these images, yielding large pool of candidate multi-ID images. At this stage, the dataset comprised more than 20 million images. Retrieval. To provide ID reference for the multi-ID images, it is necessary to retrieve the IDs on it. All single-ID cluster centers were aggregated into an embedding matrix. For each detected face in every multi-ID image, its ArcFace embedding was compared to all single-ID cluster centers to determine identity. The similarity between two embeddings was calculated as: sim(id1, id2) = cos(f (id1), (id2)) (9) where id1 and id2 denote two faces, and is the ArcFace embedding network. Each face in multi-ID image was assigned the identity of the single-ID cluster center with the highest similarity, provided the similarity exceeded predefined threshold (0.5). This approach enabled accurate and automated identity assignment in group images and facilitated retrieval of corresponding reference images. Filtering and labelling. To further improve dataset quality, series of annotation and filtering steps were applied. The Recognize Anything model [69], an aesthetic score predictor [12], and other auxiliary tools were used for annotation. Images with low aesthetic scores or those identified as collages rather than genuine group photos were excluded. Figure 9. Application of WithAnyone-Kontext. Marrying editing models, WithAnyone is capable of face editing given customization references. ID Appearance. Nationality Distribution. Figure 10. Overview of Dataset Distributions. (a) ID appearance distribution for the subset of one nation: the x-axis represents celebrities, sorted by the number of images in which they appear. (b) Nationality distribution: celebrities in our dataset come from over 10 countries, with most data sourced from China and the USA. (c) Word cloud of the most frequent words in the captions. Benchmark Distribution. Optical Character Recognition (OCR) tools detected watermarks and logos, which were cropped out when possible; otherwise, the images were discarded. Finally, descriptive captions were generated for the images using large language model, enriching the dataset with textual information. So far, dataset with three parts is obtained: (1) 1M single-ID images as reference bank, or single-ID crosspaired training; (2) 500k paired multi-ID images with identified persons; (3) 1M unpaired multi-ID images, which can be used for training scenario without the need of references, such as reconstruction. B.2. Dataset Statistics Following prior arts [6, 7, 29, 34], comprehensive statistics of the dataset are provided in Fig. 11, including the distribution of nationalities, the count of appearances per identity, and word cloud illustrating the most frequent terms in the generated image captions, offering insights into the diversity and richness of the dataset. long-tail distribution is observed in the count of appearances per identity in Fig. 11a, with few identities appearing frequently while many others are less common. This provide diverse set of identities, as well as perfect test dataset without identity interaction with the training set. Fig. 11b and Fig. 10c illustrate MultiID-2Ms nationality distribution and action diversity respectively. The comparison between the proposed dataset and existing multi-ID datasets are listed in Table 4, highlighting MultiID-2Ms outstanding volume and paired references. C. Benchmark and Metrics Details Most existing methods are evaluated on privately curated test sets that are seldom released, and even when datasets are shared, the accompanying evaluation protocols vary widely. For example, ID-Patch [68] and UniPortrait [15] measure identity similarity using ArcFace embeddings, whereas UNO [56] relies on DINO [33] and CLIP similarity scores. This Table 4. Statistic comparison for multi-identity group photo datasets. #Img refers to total scale of the dataset; #Paired refers to paired group image number; #Img / ID indicates number of reference image for each single ID; #ID / Img means number of IDs appears on group photos. Dataset #Img #Paired #Img / ID #ID / Img IMAGO [47] MHP [9] PIPA [67] HumanRef [22] Celebrity Together [70] 80k 5k 40k 36k 194k 0 0 40k 36 0 0 0 cross 1+ - 2 10 1 10 1 14+ 1 5 MultiID-2M 1.5M 500k 100+ 1 heterogeneity together with the common practice of reporting only the cosine similarity between matched ArcFace embeddings fails to capture more nuanced insights and can even encourage degenerate behavior in which models produce images that are effectively copy-pastes of the reference photos. In this work, MultiID-Bench is introduced as unified and extensible evaluation framework for group photo (multiID) generation. It standardizes assessment along two complementary axes: (i) identity fidelity (preserving each target identity without unintended copying and blending), and (ii) generation quality (semantic faithfulness to the prompt/- ground truth and overall aesthetic quality). The data used in MultiID-Bench are drawn from the long-tail portion of MultiID-2M. We first select the least frequent identities and gather all images containing them. To prevent information leakage, the training split is filtered to ensure zero identity overlap with the benchmark set. The final benchmark contains 435 samples; each sample provides 14 reference identities (with their images), corresponding ground-truth image, and text prompt describing that ground-truth scene. Identity Blending. In the similarity matrix, the offdiagonal elements correspond to the similarity between different identities. The average of the diagonal elements is Clothes & Accessories Distribution. Action Distribution. Figure 11. Distribution of Clothes and Action Labels of Proposed Dataset. used as the metric for identity fidelity, and the average of the off-diagonal elements serves as the metric for identity blending, as in Eq. 10. MBld(xg, xt) = 1 2 (cid:88) (cid:88) i=1 j=1,j=i cos(gi, tj) (10) where gi is the embedding of the i-th face in the generated image xg, and tj is the embedding of the j-th face in the ground-truth image xt. lower value indicates less unintended blending between different identities, which is desirable. Generation quality. The overall generation quality is evaluated based on CLIP-I and CLIP-T, which are the de facto standards for evaluating the prompt-following capability [41], are employed to measure the cosine similarity in the CLIP embedding space between the generated image and the ground truth image or caption. Additionally, an aesthetic score model [12] is used to assess the aesthetic quality of the generated images. D. Galleries of WithAnyone We show more results of WithAnyone in Fig. 12, Fig. 13, and Fig. 14. E. Model Framework Details We follow prior work [14, 64] and integrate lightweight identity adapter into the diffusion backbone. Identity embeddings are injected by cross-attention so that the base generative prior is preserved while controllable identity signals are added. Face embedding. Each reference face is first encoded by ArcFace, producing 1 512 identity embedding. To match the tokenized latent space of the DiT backbone, this vector is projected with multi-layer perceptron (MLP) into tokens of dimension 3072 (i.e., an 8 3072 tensor). This tokenization provides sufficient capacity for the cross-attention layers to integrate identity cues without overwhelming the generative context. Controllable attribute retention. Completely suppressing copy-like behavior is not always desirable: users sometimes expect certain mid-level appearance attributes (e.g., hairstyle, accessories) to be preserved. ArcFace focuses on high-level, identity-discriminative geometry and texture cues but omits many mid-level semantic factors. To expose controllable retention of such attributes when needed, we optionally incorporate SigLIP [65] as secondary encoder. SigLIP provides more semantically entangled representations, enabling selective transfer of style-relevant traits while ArcFace anchors identity fidelity. Attention mask and location control. To further improve identity disentanglement and precise localization in the generated images, an attention mask and location control mechanism are incorporated [3, 62]. Specifically, groundtruth facial bounding boxes are extracted from the training data and used to generate binary attention masks. These masks are applied to the attention layers of the backbone model, ensuring that each reference token only attends to its corresponding face region in the image, providing location control at the same time. Feature injection. After each transformer block of the DiT backbone, we inject face features through crossattention modulation: = H+λid softmax (cid:18) (HWQ)(EWK) (cid:19) + (EWV ), (11) where denotes the current hidden tokens, the stacked face-embedding tokens, and WQ, WK, WV the projection matrices; is the query/key dimension, and λid = 1.0 during training. When SigLIP is enabled, its tokens are processed by parallel cross-attention with an independent scaling Figure 12. Galleries of Single-ID Generation. Figure 13. Galleries of 2-person Generation. Figure 14. Galleries of 3-to-4-person Generation. coefficient. F. Experimental Details F.1. Implementation Details WithAnyone is trained on 8 NVIDIA H100 GPUs, with batch size of 4 on each GPU. The learning rate is set to 1e4, and the AdamW optimizer is employed with weight decay of 0.01. The pre-training phase runs for 60k steps, with fixed prompt used during the first 20k steps. The subsequent paired-tuning phase lasts 30k steps: 50% of the samples use paired (reference, ground-truth) data, while the remaining 50% continue reconstruction training. Finally, quality/style tuning stage of 10k steps is performed with reduced learning rate of 1 105. For the extended ID contrastive loss, the target is used as the positve sample, while other IDs from samples in the same batch serve as negative samples. With the global batch size of 32, this yields less than hundred negative samples. Extended negative samples are drawn from reference bank. If this ID is identified as one of the 3k ID in the reference bank, we simply omit its own ID and draw the from other IDs. If this ID is not identified, then it makes things easier all the IDs in the reference bank can be used as negative samples. For other baseline methods, official implementations and checkpoints (or API) are used with default settings. Methods are tested on MultiID-Bench and real-human subset of OmniContext [54]. OmniContext uses Vision-Language Models (VLMs) to evaluate the prompt-following (PF) and subjectconsistency (SC) of generated images. For reproducibility, the VLM is fixed to Qwen2.5-VL [1]. ID-Patch [68] requires pose condition, and we use the ground-truth pose for it. Single face embedding model may induce biased evaluation on ID similarity, thus we average three de-facto face recognition models consine similarity to compute the overall ID similarity metric, namely ArcFace [11], FaceNet [45], and AdaFace [26]. F.2. More Discussion on the Quantitative Results The performance of GPT on our 3-and-4-people subset offers useful validation of our copy-paste metric, as shown in Table 2. This subset largely comprises group photographs from TV series that GPT may have encountered during pretraining, so GPT attains unusually high identity-similarity scores both to the ground truth (GT) and to the reference images. Actually, in one case GPT even generates an ID from the TV series that is not present in the reference images. This behaviour approximates an idealized scenario in which model fully understands and faithfully reproduces the target identity: similarity to GT and to references are both high, and the copy-paste measure the difference between distances to GT and to references approaches zero. These observations are consistent with our metric design and support its ability to distinguish true identity understanding from trivial copy-and-paste replication. We report the experimental limit in Table 1. If one model completely copy the reference image, SimGT = 0.521, SimRef = 1.0, and copy-paste is 0.999, which aligns with the theoretical limit 1.0 of copy-paste. The prompt-following ability is measured by CLIP-I and CLIP-T in our benchmark, and is judged by VLM in OmniContext. WithAnyonegains state-of-the-art performance in both metrics, and is ranked the highest in our user study. However, the credibility of CLIP scores and the aesthetic scores may be debated, as they are not always consistent with human perception. G. Ablation Study Details In this section, we systematically evaluate the impact of training strategy, GT-aligned ID-Loss, InfoNCE ID Loss, and our dataset construction. User study is also conducted to validate the consistency of the proposed metrics with human perception, as well as evaluate the human preference on different methods. SigLIP signal. SigLIP [65] signal is introduced to retain copy-paste effect when user tend to retain the features from reference images like hairstyle, accessories, etc. As shown in Fig. 16, increasing the SigLIP signal weight effectively amplifies the copy-paste effect while simultaneously boosting ID similarity to the reference images exactly as expected, since stronger SigLIP guidance enforces tighter semantic alignment and transfers more fine-grained appearance cues (e.g., hairstyle, accessories, local textures). Training strategy. We evaluate the effect of paired-data fine-tuning stage. After an initial reconstruction training phase, we either continue training with paired (reference, ground-truth) data or keep training under the reconstruction objective for 10k steps. As shown in Table 3, continuing with paired data effectively reduces the copy-paste effect without compromising similarity to the ground truth. Dataset construction. To validate the effectiveness of our dataset, we trained model on FFHQ [24] using reconstruction training for the same number of steps. As shown in Table 3, the FFHQ-trained model performs poorly across all metrics. This likely stems from FFHQs limited diversity and size, as it contains only 70k face-only portrait images. GT-aligned ID-Loss. We validate the GT-aligned IDLoss with simple experiment that visualizes predicted faces at different denoising time steps during training. As shown in Fig. 7, at low noise levels the GT-aligned ID-Loss is substantially lower than the loss computed using predictionaligned landmarks, indicating that aligning faces to groundtruth landmarks reduces denoising error and yields more accurate identity assessment. At high noise levels the GTaligned ID-Loss shows greater variance, producing stronger Figure 15. ID Loss Curves with λ InfoNCE Loss. 0.1 is 0.1 InfoNCE Loss without extended negative samples, and 0.1+ is 0.1 InfoNCE Loss with extended negative samples. Figure 16. Trade-off Curves with λ Siglip and (1λ) ArcFace signal. H. User Study Details Our user study is conducted with the same data samples and generated results in our quantitative experiments. Due to tight financial budget, we randomly select 100 samples from single-person subset, 100 samples from 2-people subset, and all samples from 3-and-4 people subset. 10 participants are recruited for the study, all of whom are trained with brief tutorial to understand the task and evaluation criteria. We illustrate the interface used in our user study in Fig. 17. H.1. Correlation Analysis We analyze the correlation between our proposed metrics and user study results. As shown in Table 5, our copy-paste metric shows moderate positve correlation with user ratings on copy-paste effect. H.2. Participant Instructions Figure 17. User Study Interface. and more informative gradients that help the model learn identity features. We provide the instructions for training the participants in the following table. InfoNCE Loss. The InfoNCE loss with extended negative samples is crucial for the convergence in the early training stage. We conduct toy experiment with 1000 training samples, and record ID Loss curves with no InfoNCE loss, 0.1 InfoNCE loss without extended negatives, and 0.1 InfoNCE loss with extended negatives. As shown in Fig. 15, ID loss fits lot faster with InfoNCE loss with extended negatives, demonstrating its effectiveness in accelerating training convergence. It also largely increases the ID similarity score, as shown in Table 3. I. Prompts for Language Models Large language models (LLMs) and vision-language models (VLMs) are used in various stages of our work, including dataset captioning and OmniContext evaluation. I.1. Dataset Captioning Besides the system prompt, we design 6 different prompts to generate diverse captions for each image. 1 prompt is randomly selected for each image during captioning. Table 5. Correlation Statistics Between Machine Ranking and Human Ranking. Reported values include Pearsons r, Spearmans ρ, and Kendalls τ with corresponding p-values. Dimension (N) Pearson (p) Spearman ρ (p) Kendall τ (p) Copy-Paste ID Sim 0.4417 (7.98e48) 0.3254 (1.54e26) 0.4535 (1.26e50) 0.3237 (2.91e26) 0.3405 (1.10e46) 0.2423 (1.11e25) Participant Instructions and Evaluation Procedure Data source and task overview. Five different methods generated images under the following conditions: single prompt that describes the ground truth image. Between 1 and 4 people in the scene (most examples contain 12 people). For each trial you will be shown the ground truth image, input images, and generation instruction. Then you will observe five generated group-photo results (one per method) and rank them according to several evaluation dimensions. Use 5-star scale where 5 stars = best and 1 star = worst. Please read the input image(s) and the editing instruction carefully before inspecting the generated results. Evaluation procedure (per-image ranking). Rank each generated image individually on the following criteria. Identity similarity How well do the person(s) in the generated image resemble the person(s) in the ground truth image? Rank images by their resemblance to the ground truth image: the more the generated person(s) look like the original reference, the higher the rating. Important: When judging identity similarity, ignore factors such as image quality, rendering artifacts, or general aesthetics. Focus only on how much the person(s) resemble the original reference(s). Also, try to assess resemblance to the ground truth image as whole, rather than comparing to any single separate reference person n. Copy-and-paste effect (excessive mimicry of the reference) Generated images should resemble the original reference but should not be direct copies of an individual reference photo. Evaluate whether the generated person appears to be directly copied from one of the reference images. Consider changes (or lack thereof) in expression, head pose and orientation, facial expression/demeanor, and lighting/shading. The lower the degree of direct copying (i.e., the less it looks like pasted replica), the better. Rank according to the amount of change observed in the person(s): more natural variation (less copy-paste) should be ranked higher. Prompt following Does the generated image reflect the content and constraints specified by the prompt/instruction? Rank images by prompt fidelity: the more faithfully the image follows the prompt, the higher the ranking. Aesthetics Judge the overall visual quality and pleasantness of the generated image (e.g., smoothness of rendering, harmonious body poses and composition). Rank images by aesthetic quality: higher perceived visual quality receives higher ratings. Full Prompts for Dataset Captioning (6 variants) System Prompt: You are an advanced vision-language model tasked with generating accurate and comprehensive captions for images. Prompt 1: Please provide brief description of the image based on these guidelines: 1. Describe the clothing, accessories, or jewelry worn by the people in detail. 2. Describe the genders, actions, and posture of the individual in detail, focusing on what they are doing. 3. The description should be concise, with maximum of 77 words. 4. Start with This image shows Prompt 2: Offer short description of the image according to these rules: 1. Focus on details about clothing, accessories, or jewelry. 2. Focus on the gender, activity, and pose, and explain what the people is doing. 3. Keep the description within 77 words. 4. Begin the description with This image shows Prompt 3: Please describe the image briefly, following these instructions: 1. Provide detailed description of the clothing or jewelry the person may be wearing. 2. Provide detailed description of the two persons gender, actions, and body position. 3. Limit the description to no more than 77 words. 4. Begin your description with This image shows Prompt 4: Describe the picture briefly according to these rules: 1. Provide detailed description of the clothing, jewelry, or accessories of the individuals. 2. Focus on the two persons gender, what they are doing, and their posture. 3. Keep the description concise, within limit of 77 words. 4. Start your description with This image shows Prompt 5: Provide short and precise description of the image based on the following guidelines: 1. Describe what the person is wearing or any accessories. 2. Focus on the gender, activities, and body posture of the person. 3. Ensure the description is no longer than 77 words. 4. Begin with This image shows Prompt 6: Briefly describe the image according to these instructions: 1. Provide precise description of the clothing, jewelry, or other adornments of the people. 2. Focus on the persons gender, what they are doing, and their posture. 3. The description should not exceed 77 words. 4. Start with the phrase This image shows Modified Prompt for OmniContext Evaluation (Face Identity Focus) Rate from 0 to 10: Task: Evaluate how well the facial features in the final image match those of the individuals in the original reference images, as described in the instruction. Focus strictly on facial identity similarity; ignore hairstyle, clothing, body shape, background, and pose. Scoring Criteria 0: The facial features are completely different from those in the reference images. 13: The facial features have minimal similarity with only one or two matching elements. 46: The facial features have moderate similarity but several important differences remain. 79: The facial features are highly similar with only minor discrepancies. 10: The facial features are perfectly matched to those in the reference images. Pay detailed attention to these facial elements: Eyes: Shape, size, spacing, color, and distinctive characteristics of the eyes and eyebrows. Nose: Shape, size, width, bridge height, and nostril appearance. Mouth: Lip shape, fullness, width, and distinctive smile characteristics. Facial structure: Cheekbone prominence, jawline definition, chin shape, and forehead structure. Skin features: Distinctive marks like moles, freckles, wrinkles, and overall facial texture. Proportions: Overall facial symmetry and proportional relationships between features. Example: If the instruction requests combining the face from one image onto another pose, the final image should clearly show the same facial features from the source image. Important: For each significant facial feature difference, deduct at least one point. Ignore hairstyle, body shape, clothing, background, pose, or other non-facial elements. Focus only on facial similarity, not whether the overall instruction was followed. Scoring should be strict high scores should only be given for very close facial matches. Consider the level of detail visible in the images when making your assessment. Editing instruction: <instruction>"
        }
    ],
    "affiliations": [
        "Fudan University",
        "StepFun"
    ]
}