{
    "paper_title": "How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads",
    "authors": [
        "Ingeol Baek",
        "Hwan Chang",
        "Sunghyun Ryu",
        "Hwanhee Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite significant advancements in Large Vision Language Models (LVLMs), a gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, a large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide a deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images."
        },
        {
            "title": "Start",
            "content": "How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads Ingeol Baek1, Hwan Chang1, Sunghyun Ryu2, Hwanhee Lee1 1Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea 2Department of Computer Engineering, Sejong University, Seoul, Korea {ingeolbaek, hwanchang, hwanheelee}@cau.ac.kr ryusunghyun1002@sju.ac.kr 5 2 0 2 1 2 ] . [ 1 5 6 8 5 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Despite significant advancements in Large Vision Language Models (LVLMs), gap remains, particularly regarding their interpretability and how they locate and interpret textual information within images. In this paper, we explore various LVLMs to identify the specific heads responsible for recognizing text from images, which we term the Optical Character Recognition Head (OCR Head). Our findings regarding these heads are as follows: (1) Less Sparse: Unlike previous retrieval heads, large number of heads are activated to extract textual information from images. (2) Qualitatively Distinct: OCR heads possess properties that differ significantly from general retrieval heads, exhibiting low similarity in their characteristics. (3) Statically Activated: The frequency of activation for these heads closely aligns with their OCR scores. We validate our findings in downstream tasks by applying Chain-of-Thought (CoT) to both OCR and conventional retrieval heads and by masking these heads. We also demonstrate that redistributing sink-token values within the OCR heads improves performance. These insights provide deeper understanding of the internal mechanisms LVLMs employ in processing embedded textual information in images."
        },
        {
            "title": "Introduction",
            "content": "Large Vision-Language Models (LVLMs) (Liu et al., 2023; Li et al., 2023; Dong et al., 2024; Liu et al., 2024a) extend the capabilities of Large Language Models (LLMs) (Zhao et al., 2023; Achiam et al., 2023; Grattafiori et al., 2024) into multimodal domains. Typically, an LVLM integrates pretrained vision encoder with pretrained LLM decoder via an adapter, enabling it to generate coherent text grounded in both image and text inputs. The power of LVLMs lies in their deep understanding of image-text relationships, which is eviCorresponding author. 1 Figure 1: Visualization of image attention maps from InternVL2-8B. and denote the layer and attention head of the LVLM, respectively. dent in tasks like VQA (Antol et al., 2015; Goyal et al., 2017) and particularly those relying on text embedded in images, such as OCR-VQA. LVLMs have demonstrated impressive performance on these OCR-centric tasks (Mathew et al., 2021; Huang et al., 2024; Reddy et al., 2024). Despite these advancements, there is limited interpretability research investigating the internal mechanisms driving this performance, specifically how LVLMs handle textual information within images. Recent interpretability work has identified retrieval heads in LLMsattention units that copy relevant tokens from long textual contexts to support longcontext factuality (Wu et al., 2024b)and visual grounding heads that localize image regions in response to textual queries (Xiao et al., 2024; Wu et al., 2024a; Kang et al., 2025b). However, these studies do not address how LVLMs internally locate and extract embedded text within images. We hypothesize that LVLMs contain distinct class of attention headsOptical Character Recognition (OCR) headsthat operate independently of copy-paste retrieval heads. While retrieval heads attend to text tokens in the input sequence and replicate them in the output, OCR heads should selectively attend to visual patches corresponding to characters or words and directly guide text extraction from images. To test this hypothesis, we convert the Passkey and Needle-ina-Haystack (NIAH) (Kamradt, 2023) benchmarks into multi-image QA setup requiring model to answer by copying text from rendered image patches. By analyzing attention distributions during answer generation (see Figure 1), we observe that certain heads consistently concentrate on the ground-truth text regions, supporting the existence of specialized OCR heads. Based on these observations, we formulate two core research questions: RQ1. How can we identify the OCR heads? RQ2. How do OCR heads differ from existing retrieval heads? To answer these questions, we introduce an algorithm for detecting OCR heads in multi-image LVLMs. Our experiments reveal three key properties that distinguish OCR heads from retrieval heads: (1) Less Sparsity: OCR heads activate more densely across instances compared to the 35% active rate of conventional retrieval heads. (2) Qualitative Distinctiveness: OCR heads exhibit low overlap with retrieval heads, indicating they form an independent functional group. (3) Static Activation Patterns: OCR head activations remain consistent across diverse contexts, unlike contextsensitive retrieval heads. To validate the specialized role of OCR heads, we evaluate their behavior in downstream tasks via CoT (Wei et al., 2022; Li et al., 2025) prompting, attention masking, and attention redistribution. These studies not only confirm the mechanistic role of OCR heads in reading embedded text but also demonstrate that manipulating their attention distributions can improve OCR-VQA performance. Our findings fill critical gap in LVLM interpretability and provide actionable insights for enhancing multimodal reasoning and reducing hallucination in OCR-centric applications. We summarize our contributions as follows: We propose scoring-based method for automatically identifying OCR heads in LVLMs. We demonstrate that OCR heads exhibit reduced sparsity, qualitative distinctiveness, and static activation patterns across diverse contexts. Through CoT prompting, targeted head masking, and strategic sink-token redistribution, we confirm OCR heads specialized role in text extraction from images."
        },
        {
            "title": "2.1 Retrieval Heads",
            "content": "Retrieval heads are identified as specific type of attention head primarily responsible for identifying and copying relevant information from the input context to the generated output. (Wu et al., 2024b) These heads are understood to play crucial role in the models ability to retrieve factual information, particularly from long input sequences. They are mechanistically linked to the process of extracting and repeating input tokens."
        },
        {
            "title": "2.2 Retrieval Score",
            "content": "To quantitatively identify which attention heads are responsible for copying relevant tokens from the input, retrieval score has been defined. (Wu et al., 2024b) This score serves to measure how consistently specific attention head engages in this copy-paste behavior. Given question q, an answer k, and context x, the model addresses NIAH tasklocating within based on q. Let denote attention head, the generated token, and Rx attention score; then two conditions must be satisfied: (1) the generated token is part of the target sentence k, and (2) the input token xj with the highest attention aligns exactly with w, formally xj = w, = argmax(a). Under these conditions, the retrieval score for head is calculated as: Retrieval Score for head = gh k (1) where gh represents the set of all correctly identified (copied-and-pasted) tokens by head h. Intuitively, this retrieval score measures the tokenlevel recall capability of specific head. Retrieval score is computed over many NIAH instances, and heads achieving score above 0.1 are classified as retrieval heads."
        },
        {
            "title": "Identifying Heads for Text Recognition",
            "content": "In this section, we describe our method for identifying attention heads within LVLMs that are primarily responsible for retrieving textual information from images. We term these specialized heads as Optical Character Recognition Heads (OCR heads). We adapt the concept of the retrieval score (Wu et al., 2024b), originally used to quantify copy-paste behavior from text contexts 2 Figure 2: The figure illustrates the image input format designed to identify tokens responsible for copy-pasting text within the image. Tokens corresponding to the patch-to-bounding box intersection ratio serve as evidence patch tokens. The retrieval score computation utilizes attention between generated tokens and evidence patch tokens. in LLMs, to the multimodal domain of LVLMs processing image inputs. Our adapted metric, the OCR score, measures how consistently head retrieves text from image regions. Heads with higher OCR scores are more frequently involved in copying token information from image patches corresponding to text."
        },
        {
            "title": "3.1 Constructing Image Passkey and Needle",
            "content": "in Haystack Dataset To identify heads involved in image text retrieval, we generate an image-based passkey and NIAH dataset. The original task embeds specific phrasefor example, the passkey is [number]within long text context and asks question requiring the model to retrieve the number. We convert this text-based task into an imagebased one by rendering the text context into multiimages. To convert text data into image data, we render the text content into images, breaking lines and moving to the next image based on preset rules for length and line count. We then generate bounding boxes around the two parts of the ground-truth answer-the passkey numbers and the needle sentence-within each image using rulebased approach. This involves detecting the digits of the answer text and calculating their coordinates based on text width and line position. This process yields separate bounding boxes x_min, y_min, x_max, and y_max for each character of the correct answer. As shown in the bounding box of Figure 2, this process generates images with associated bounding box information for the answer. Note that while bounding boxes are used for analysis like calculating intersection with patches, they are not provided as input to the model during inference."
        },
        {
            "title": "3.2 Patch Token Preprocessing",
            "content": "As in Figure 2, we process each image into sequence of patch tokens by the visual encoder. The encoder scans the image from top-left to bottomright, generating one token representation for each non-overlapping patch of size . If an image has dimensions and patch size , it yields w/N h/N tokens. For example, 294 196 image with 14 14 patches results in 294/14 196/14 = 294 tokens. When processing multiple images, each resized to fixed dimension, the total number of patch tokens is w/N h/N I, where is the number of images. These patch tokens form the visual part of the models input sequence."
        },
        {
            "title": "3.3 Evidence Patch Tokens",
            "content": "the bounding box positions of the We adjust ground truth answer based on any resizing applied to the original images. For example, if an image is resized from 588392 to 294196, the bounding box coordinates are scaled by 0.5. After scaling, we calculate the Intersection over Union IoU between each answer bounding box and each image patch token. Figure 2s Patch-to-Bounding Box Intersection Ratio illustrates this overlap. Patch tokens with an IoU greater than 0.1 with any ground truth answer bounding box are designated as Evidence Patch Tokens. These tokens represent the image regions containing the text we expect the model to retrieve. Patch tokens with IoU below 0.1 are discarded from this set. 3 Figure 3: Proportion of OCR Heads and Retrieval Heads (RH) identified in the InternVL2 and Qwen2-VL models."
        },
        {
            "title": "3.5 Detecting OCR Head",
            "content": "Following the concept of retrieval score introduced in (Wu et al., 2024b) to identify copy-paste behavior from text contexts, we define the OCR score to quantify this behavior when retrieving text from images. The original retrieval score measures when heads highest attention aligns with an input token that is part of the correct generated output, specifically from text context. Our OCR score adapts this concept for image input by focusing on attention to Evidence Patch Tokens. Let be the question, be the correct answer text, and be generated token. Let be the set of Evidence Patch Tokens identified through the Patch-to-Bounding Box Intersection Ratio. We assign the OCR score to head when the following two conditions hold for generated token w: 1. The generated token must be part of the correct answer text k: k. 2. The patch token xj in the input sequence that receives the highest attention for must belong to the set of Evidence Patch Tokens E: xj E, where = arg max(a). The OCR score for head is then calculated using the same token-level recall formula as the original retrieval score: OCR Score for head = gh k (2) where gh is the set of generated tokens for which head met the two conditions above, i.e., correctly \"copied\" token by attending to an Evidence Patch Token. This score measures the tokenlevel recall capability of specific head for retrieving text from image regions identified as evidence. We calculate the OCR score for each attention head across diverse subset of the image passkey and NIAH dataset. This dataset includes instances with varying \"context lengths\" simulated using 2 to 12 images. All hyper-parameter settings are kept identical to those previously used for configuring retrieval heads. We analyze 1,200 sampled instances at fixed ratio. To classify head as an OCR head, we require it to achieve an OCR score greater than 0.1 in at least 10% of these instances, i.e., in 120 or more cases. For any head meeting this frequency criterion, we then compute its average OCR score across all 1,200 instances. If this average score also exceeds 0.1, that head is classified as an OCR head."
        },
        {
            "title": "4 How OCR Heads Differ in Their\nOperational Characteristics from\nRetrieval Heads",
            "content": "In this section, we discuss three key properties that distinguish the identified OCR heads from conventional retrieval heads: 1) reduced sparsity, 2) qualitative distinctiveness, and 3) static activation. For the experiments, we use the Qwen2-VL (Wang et al., 2024) and InternVL2 (Cai et al., 2024) models to compare the characteristics of the OCR head and the retrieval head."
        },
        {
            "title": "Specialization",
            "content": "Figure 3 demonstrates that the OCR head is less sparse than the retrieval head. In the case of the retrieval head, when text-based passkey and NIAH datasets are used as input, only 36% of the heads 4 Figure 4: Visualization comparing the OCR Score for OCR Heads and Retrieval Score for Retrieval Heads. OCR Score and Retrieval Score measure token-level recall, how many correct tokens were copied, while Activation Frequency indicates how often head activates on at least one token above threshold."
        },
        {
            "title": "4.2 Do OCR Head and Retrieval Heads Exist",
            "content": "Qwen2-VL-7B Instruction 0 0-0.1 0.1-0.3 0.3-0.5 0.5-1.0 0.1 0.507 0.196 0.104 0.083 0.076 0."
        },
        {
            "title": "Similarity",
            "content": "InternVL2-8B Instruction 0 0-0.1 0.1-0.3 0.3-0.5 0.5-1.0 0.1 0.593 0.093 0.000 0.000 0.000 0.133 Table 1: Jaccard similarity of the average retrieval scores between OCR Heads and Retrieval Heads. that activate during passkey retrieval are involved, as stated in Wu et al. (2024b)s paper (we consider heads with retrieval score above 0.1 as actively participating in retrieval). We observe similar results in models with more than 7 billion parameters. Consistently, when image-based inputs containing text are used for the OCR head, we find that 91.3% more heads are involved on average compared to when text-based datasets are used. On the other hand, for heads that show low-frequency retrieval scores between 0.01 and 0.1, i.e., those affected by copy-paste, the retrieval head involves 192% more heads than the OCR head. Based on these experimental results, we conclude that the OCR head activates stronger heads more clearly, and its involvement in low-frequency retrieval is smaller, making it less sparse and more specialized. as Distinct Entities? Building on the previous experiment, one might ask whether the heads involved in retrieval heads or OCR heads are distributed similarly. To investigate this, we check how commonly the heads are distributed based on similarity. We measure the similarity between heads using the Jaccard similarity coefficient, as shown in the formula below: J(A, B) = AB AB = AB A+BAB (3) In Table 1, for the Qwen2-VL-7b model, the Jaccard similarity for heads with retrieval score above 0.1 is 0.347, which is relatively low. For the InternVL-8b model, we observe an even lower value of 0.133. This shows that there is low similarity between the retrieval heads and the OCR heads, confirming that they are clearly distinct."
        },
        {
            "title": "Score",
            "content": "Heads serve multiple roles beyond single task. We conduct experiments to assess how much attention the retrieval heads focus on tasks like copypaste, comparing OCR heads and retrieval heads in terms of their sensitivity. Sensitivity refers to how head activates in specific contexts; highly sensitive head activates in only one context and not in others. For instance, if passkey context is \"the passkey is 1234,\" head with high sensitivity would activate strongly for \"1\" but not for \"234.\" To evaluate this, we compare activation frequency with the average retrieval score, as in previous 5 Figure 5: In the OCR-VQA task, we calculate the OCR score and Retrieval score based on the CoT prompting. L5_H19 refers to Layer 5, Head 19. The blue and green lines indicate the token positions with the highest attention. Qwen2-VL-7B"
        },
        {
            "title": "Method",
            "content": "Top10 OCR Head"
        },
        {
            "title": "Avg OCR Score\nAvg Retrieval Score",
            "content": "Top10 Retrieval Head"
        },
        {
            "title": "Score",
            "content": "0.4004 0.0123 0.2929 0.4705 Table 2: The average scores of the top 10 OCR and Retrieval heads when CoT prompting is applied in the OCR-VQA task. studies (Wu et al., 2024b). Activation frequency measures how often activation occurs for any token, giving score of 1 if any token activates. If activation occurs in just one token, the average retrieval score would be 1/4. Therefore, the difference between frequency and average retrieval score indicates whether head performs roles beyond retrieval in the context. In Figure 4, we observe significant difference in the frequency and average score for the retrieval head, suggesting that it plays additional roles outside of retrieval. This is what we refer to as \"dynamically activated.\" On the other hand, the OCR head shows little difference, indicating that these heads, specialized for OCR tasks, are \"statically activated.\""
        },
        {
            "title": "5 Unveiling the Functional Roles of OCR\nHeads in Downstream Applications",
            "content": "al., To compare the impact of OCR heads and retrieval heads on downstream tasks, we conduct three complementary analyses: CoT prompting, attention head masking, and sink token redistribution. We also use the OCR-VQA (Mishra et 2019), Multi-Page DocVQA (Tito et al., 2023), DocVQA (Mathew et al., 2021), NQ (Kwiatkowski et al., 2019), and Hotthe potQA (Yang et al., 2018) datasets for downstream tasks. Furthermore, the evaluation metric for all the datasets in the downstream tasks is the F1 score. 5.1 Impact of OCR Head for Reasoning in OCR-VQA We investigate the contribution of OCR and retrieval heads to the reasoning capabilities of LVLMs within the OCR-VQA task using CoT prompting. As illustrated in Figure 5, CoT prompting enables the generation of intermediate reasoning steps before producing the final answer based on the input of the image. The blue and green lines represent the positions where maximum attention occurs in the tokens. We calculate the OCR score between the image tokens and the reasoning part using the method in Section 3, and calculate the retrieval score by copying the answer from the reasoning and answer sections, as described in Section 2. In Table 2, we measure the scores based on the top 10 heads with the highest average scores for each method. The OCR head shows high OCR score, while the retrieval head not only shows high retrieval score but also high OCR score. The fact that the OCR head only shows high OCR score, as discussed in Section 4.1, suggests that it is more specialized. On the other hand, the retrieval head consistently shows high scores because it is dynamically activated as we discuss in Section 4.3. This means that the retrieval head is context-sensitive and performs multiple roles. Based on these observations, we highlight the significant influence of the OCR head when performing CoT reasoning on text information in LVLMs."
        },
        {
            "title": "Masking on Task Performance",
            "content": "To understand the functional contributions of OCR heads and retrieval heads across multiple downstream tasks, we perform systematic head-masking analysis. Specifically, we evaluate how LVLM performance degrades when we mask the top-scoring OCR heads, retrieval heads (RH), or an equal number of randomly selected heads. We apply this intervention to two VQA tasksDocVQA, which requires copying spe6 Masked Heads DocVQA MP-DocVQA NQ HotpotQA Qwen2-VL-7B OCR RH Random OCR RH Random OCR RH Random OCR RH Random Baseline 76.3 69.1 58.0 49. 5 10 20 74.1 73.9 59.7 76.1 74.6 67.1 75.2 0.3 74.2 0.2 75.1 0.4 68.1 66.7 47.7 69.4 67.6 62. 67.9 0.2 66.6 0.4 67.7 1.7 57.7 59.0 56.8 57.2 53.8 36.4 57.6 0.6 56.4 0.2 57.3 0.5 51.2 49.7 47.5 47.8 45.3 37. 49.5 0.1 47.9 1.2 48.9 0.4 Intern VL2-8B OCR RH Random OCR RH Random OCR RH Random OCR RH Random Baseline 39.1 35.0 58. 35.8 5 10 20 36.6 30.0 24.2 39.7 39.7 38.2 39.2 0.2 38.8 0.2 38.3 0.6 32.8 25.1 20. 35.5 35.7 36.1 35.1 0.6 34.7 0.4 34.6 0.3 60.3 59.6 56.6 56.3 55.8 58.9 55.9 0.3 57.8 0.3 57.6 0.2 33.5 33.5 34. 32.8 28.2 25.7 34.6 0.1 29.2 0.3 33.5 1.2 Table 3: Performance on VQA and QA datasets when masking the OCR head, the retrieval head (RH), or randomly selected head (Random: mean variance across 5 selections). DocVQA MP-DocVQA Qwen2-VL-7B InternVL2-8B"
        },
        {
            "title": "Baseline\nOCR\nRH",
            "content": "76.3 76.6 (+0.3) 76.2 (-0.1) 39.1 40.0 (+0.9) 39.3 (+0.2) 69.1 69.7 (+0.6) 69.4 (+0.3) 35.0 35.8 (+0.8) 34.7 (-0.3) Table 4: Performance results after redistributing the attention sink token value in the OCR head and retrieval head (RH) across MP-DocVQA and DocVQA tasks. cific text from single image, and Multi-Page DocVQA, which involves reasoning over multiple imagesand two QA tasks, Natural Questions (NQ) and HotpotQA, where we include the evidence passage and randomly sample 05 negative passages with shuffled ordering. We mask the top 5, 10, and 20 heads of each type (OCR, RH, and random). As shown in Table 3, masking OCR heads causes pronounced decline in VQA performance: with 20 heads masked, DocVQA and MP-DocVQA F1 scores fall to 59.7% and 47.7% (nearly 20% drop), whereas masking RH yields much smaller declines of 9.2% and 7.3%, respectively. By contrast, on the QA tasks, masking RH leads to substantial performance decrease. These results confirm that visiontext VQA tasks depend critically on OCR heads, while retrieval heads play more pivotal role in text-based QA."
        },
        {
            "title": "5.3 Enhancing OCR Head Performance via\nSink Token Value Redistribution",
            "content": "In this experiment, we investigate the potential for enhancing downstream task performance by strategically redistributing the attention values associated with the sink token. An attention sink token (Kang et al., 2025a) refers to token in the input sequence that receives disproportionately high attention scores despite its lack of semantic relevance. Although it carries little meaningful information, its attention value diverts focus from more important tokens in the sequence. Recent research (Yu et al., 2024; Kang et al., 2025a; Gu et al., 2025) suggests that optimizing the attention score distribution and reducing the impact of such attention sinks can enhance performance, especially in tasks requiring more accurate and meaningful attention allocation. Based on this, we redistribute the sink token value proportionally to the attention scores of other tokens. This process can be expressed as: ˆAl h[T, t] = Al h[T, t] + β h[T,t] Al i{1, ,T } Al h[T,i] (cid:80) (4) Here, and refer to the layer and head, respectively, and represents the attention score. refers to the total number of tokens in the attention score, and is the original value of the sink token. β is hyperparameter that controls how much we want to remove excess attention scores from the attention sink. Following prior studies (Yu et al., 2024), we set β = 0.4. First, we compute the attention score ratios for all tokens, excluding the 0th sink token. The computed ratios are then multiplied by to calculate the redistributed attention scores for each token, which are added to the original attention score to obtain ˆA. This new ˆA is used for subsequent operations, and after inference, the F1 score is measured. The heads selected for redistribution are the top 4 heads with the highest average OCR or retrieval scores. Table 4 shows that redistributing the sink token among OCR heads boosts performance on DocVQA and MP-DocVQA for both QwenVL and InternVL models. In contrast, redistributing Figure 6: Character-wise activation of top-5 OCR heads; visually similar shapes co-activate common heads. among retrieval heads lowers performance in some cases and yields slight gains in others."
        },
        {
            "title": "Recognition via OCR Heads",
            "content": "We investigate whether each characterfrom 0 to 9 and to ztends to activate specific attention head. To do this, we replaced the original multidigit pass keys in our dataset with single characters (09, az), rendered the prompts as images, and then applied the OCR scoring procedure described in Section 3. Figure 6 shows the activation heatmaps along with the top-5 heads per character. Characters with similar shapes consistently activate the same OCR headsfor instance, 1 and share four heads (L3H8, L2H4, L14H8, and L17H24), and 9, g, and share three. This consistent co-activation demonstrates that OCR heads mechanistically encode visual similarity to guide text extraction. We show that the behavior of OCR heads can directly elucidate the mechanism by which LVLMs perform optical character recognition on embedded text. The complete set of character-wise activation heatmaps for all tokens is presented in Appendix A."
        },
        {
            "title": "7 Related Work",
            "content": "Attention Head Analysis in LVLM Interpretability research in LVLMs has explored how attention integrates visual and textual information. Prior work identifies various head types: image-centric heads (Kang et al., 2025a) focus on semantically meaningful features, and localization heads (Kang et al., 2025b) enable grounding without explicit training. Amplifying visual attention has also been proposed to reduce hallucinations (Liu et al., 2024b), while entropy-based approaches identify visual heads (Bi et al., 2024). These studies illuminate visual-text alignment but do not address attention heads specialized in recognizing and retrieving embedded text, which is central to OCR tasks. In-Context Identification Heads Transformerbased models utilize attention heads that focus on different aspects of input data to extract meaningful information (Zheng et al., 2024). In-Context Identification heads selectively attend to and aggregate structural, syntactic, and semantic cues within the prompt, writing these distilled signals back into the residual stream to guide subsequent generation. For example, the Previous Head (Olsson et al., 2022) captures positional relationships, while the Duplicate Head (Wang et al., 2022) identifies repeating content, becoming more active with increased repetition. In contrast, the Rare Words Head (Voita et al., 2019) targets lowfrequency tokens. Retrieval Heads (Wu et al., 2024b) precisely pinpoint the positions of specific tokens. These specialized heads are typically identified through replacement-based methods such as ablation (Yu and Ananiadou, 2024; Jin et al., 2024) or patching (Merullo et al., 2023; Todd et al., 2023), or scoring-based methods (Jin et al., 2024; Crosbie and Shutova, 2024). In this study, we propose the OCR Score, scoring-based approach for efficiently detecting attention heads with unique capabilities."
        },
        {
            "title": "8 Conclusion",
            "content": "This study uncovers OCR heads, specialized LVLM attention heads that directly read text in images. Unlike dynamically activated retrieval heads, OCR heads are less sparse, functionally distinct, and statically activated. CoT prompting, headmasking, and sink-token redistribution confirm their vital role. These mechanisms offer new insight into LVLM parsing of embedded text, paving the way for principled model behavior."
        },
        {
            "title": "Limitations",
            "content": "We validate the OCR head properties through five empirical experiments. However, many opensource LVLMs support only single image input or rely on thumbnail-style training settings. Some models also produce image tokens that mismatch with patch counts due to encoder variations. These constraints limit the diversity of models we evaluate. Additionally, due to computational constraints, we conduct experiments on the passkey and NIAH dataset only up to 8k samples."
        },
        {
            "title": "Ethics Statement",
            "content": "This study involves open-source LVLMs for QA tasks, and some models may generate inappropriate outputs. Also, the VQA and QA datasets used in our experiments may also include potentially harmful or biased content."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was supported by Institute for Information & Communications Technology Planning & Evaluation (IITP) through the Korea government (MSIT) under Grant No. 2021-0-01341 (Artificial Intelligence Graduate School Program (Chung-Ang University))."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433. Jing Bi, Junjia Guo, Yunlong Tang, Lianggong Bruce Wen, Zhang Liu, and Chenliang Xu. 2024. Unveiling visual perception in language models: An arXiv preprint attention head analysis approach. arXiv:2412.18108. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. Internlm2 technical report. arXiv preprint arXiv:2403.17297. Joy Crosbie and Ekaterina Shutova. 2024. Induction heads as an essential mechanism for pattern arXiv preprint matching in in-context learning. arXiv:2407.07011. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. CoRR, abs/2401.16420. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, and Min Lin. 2025. When attention sink emerges in language models: An empirical view. In The Thirteenth International Conference on Learning Representations. Kung-Hsiang Huang, Hou Pong Chan, Yi Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, and Heng Ji. 2024. From pixels to insights: survey on automatic chart understanding in the era of large foundation models. IEEE Transactions on Knowledge and Data Engineering. Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, and Jun Zhao. 2024. Cutting off the head ends the conflict: mechanism for interpreting and mitigating knowledge conflicts in language models. arXiv preprint arXiv:2402.18154. Gregory Kamradt. 2023. Needle in haystack - pressure testing llms. Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. 2025a. See what you are told: Visual attention sink in large multimodal models. In The Thirteenth International Conference on Learning Representations. Seil Kang, Junhyeok Kim, and Jinyeong Kim, Seong Jae Hwang. 2025b. Your large visionlanguage model only needs few attention heads for visual grounding. arXiv preprint arXiv:2503.06287. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. 9 Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large lanIn Proceedings of the 40th Interguage models. national Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR. Yunxin Li, Zhenyu Liu, Zitao Li, Xuanyu Zhang, Zhenran Xu, Xinyu Chen, Haoyuan Shi, Shenyuan Jiang, Xintong Wang, Jifang Wang, et al. 2025. Perception, reason, think, and plan: survey on large multimodal reasoning models. arXiv preprint arXiv:2505.04921. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024a. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2629626306. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In NeurIPS. Shi Liu, Kecheng Zheng, and Wei Chen. 2024b. Paying more attention to image: training-free method for alleviating hallucination in lvlms. In European Conference on Computer Vision, pages 125140. Springer. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2023. Circuit component reuse across tasks in arXiv preprint transformer arXiv:2310.08744. language models. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. 2019. Ocr-vqa: Visual question answering by reading text in images. In ICDAR. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022. In-context learning and induction heads. arXiv preprint arXiv:2209.11895. Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024. Docfinqa: long-context arXiv preprint financial arXiv:2401.06915. reasoning dataset. Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023. Hierarchical multimodal transformPattern Recognition, ers for multipage docvqa. 144:109834. Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron Wallace, and David Bau. 2023. Function vectors in large language models. arXiv preprint arXiv:2310.15213. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multihead self-attention: Specialized heads do the heavy arXiv preprint lifting, the rest can be pruned. arXiv:1905.09418. Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2022. Interpretability in the wild: circuit for indirect obarXiv preprint ject identification in gpt-2 small. arXiv:2211.00593. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Junda Wu, Zhehao Zhang, Yu Xia, Xintong Li, Zhaoyang Xia, Aaron Chang, Tong Yu, Sungchul Kim, Ryan Rossi, Ruiyi Zhang, et al. 2024a. Visual prompting in multimodal large language models: survey. arXiv preprint arXiv:2409.15310. Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu. 2024b. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574. Linhui Xiao, Xiaoshan Yang, Xiangyuan Lan, Yaowei Towards arXiv preprint Wang, and Changsheng Xu. 2024. visual grounding: survey. arXiv:2412.20206. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Zeping Yu and Sophia Ananiadou. 2024. How do large language models learn in-context? query and key matrices of in-context heads are two towers for metric learning. arXiv preprint arXiv:2402.02872. Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, and Yingyan (Celine) Lin. 2024. Unveiling and harnessing hidden attention sinks: enhancing large language models without training through attention calibration. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. 10 Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Zifan Zheng, Yezhaohui Wang, Yuxin Huang, Shichao Song, Mingchuan Yang, Bo Tang, Feiyu Xiong, and Zhiyu Li. 2024. Attention heads of large arXiv preprint language models: survey. arXiv:2409.03752. Character-wise Analysis We investigate whether each characterfrom 0 to 9 and to ztends to activate specific attention head. To do this, we replaced the original multi-digit pass keys in our dataset with single characters (09, az), rendered the prompts as images, and then applied the OCR scoring procedure described above. As shown in Figure 7, we found that visually similar characters share common high-activation heads among their top 5. For instance, for the characters 1 and i, four heads (L14H8, L2H4, L17H24, and L3H8) appear jointly in their top 5. Similarly, for 9, g, and q, the heads L3H8, L14H8, and L19H20 cooccur among the top 5 activations. This suggests that when visually similar shapes enter the ViT encoder, they produce similar representations that in turn trigger the same decoder heads in the LLM. In Figure 6, we use the Qwen2-VL-7B model; the horizontal axis denotes the head index, and the vertical axis denotes the layer index."
        },
        {
            "title": "B CoT Prompt",
            "content": "Lets think about it step by step. The response must be valid JSON object with two fields: \"thinking\" and \"answer\". The \"answer\" field should provide the final answer, starting with \"Answer:\". Here is the question: \"Question\": questions[n] Respond in the following JSON format: { \"thinking\": \"Thinking: ...\", \"answer\": \"Answer: ...\" } Table 5: CoT Prompt. In Section 4.3, we use Chain-of-Thought (CoT) prompting to simultaneously measure the OCR score and the retrieval score. To obtain structured responses, we instruct the model to generate reasoning and answers in JSON format, as illustrated in Table 5. Specifically, we extract the OCR score from the input image tokens and reasoning component, and compute the retrieval score from the reasoning and answer components. Figure 7: Heatmap visualization of top5 OCR scores for each character from 0 to 9 and to z."
        }
    ],
    "affiliations": [
        "Department of Artificial Intelligence, Chung-Ang University, Seoul, Korea",
        "Department of Computer Engineering, Sejong University, Seoul, Korea"
    ]
}