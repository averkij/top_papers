{
    "paper_title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning",
    "authors": [
        "Wenqiao Zhu",
        "Ji Liu",
        "Rongjuncheng Zhang",
        "Haipang Wu",
        "Yulun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \\TheName{} in terms of robustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code is available at https://github.com/WNQzhu/CARFT."
        },
        {
            "title": "Start",
            "content": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning Wenqiao Zhu1, 2, , Ji Liu1, Rongjuncheng Zhang1, Haipang Wu1, Yulun Zhang2 1HiThink Research, 2Shanghai Jiao Tong University 5 2 0 2 1 2 ] . [ 1 8 6 8 5 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reasoning capability plays significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chainof-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., CARFT, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the finetuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of CARFT in terms of robustness, performance (up to 10.15%), and efficiency (up to 30.62%). Code is available at https://github.com/WNQzhu/CARFT."
        },
        {
            "title": "Introduction",
            "content": "The reasoning capability of Large Language Models (LLMs) stands as critical component, drivContact to: zhuwnq@outlook.com ing an extensive array of potential applications, which span mathematical problem (Wang et al., 2024; Luo et al., 2023), financial analysis (Yang et al., 2023; Zhang et al., 2023a), and medical applications (Singhal et al., 2022), etc. The advent of reasoning LLMs, e.g., OpenAI o1 (Jaech et al., 2024), OpenAI o3 (OpenAI, 2025), LlamaNemotron (Bercovich et al., 2025), Claude 3.7 (Anthropic, 2024), and DeepSeek R1 (DeepSeekAI, 2025), has significantly heightened the interest in exploring the reasoning capabilities of LLMs across both academic and industrial sectors. Additionally, given the straightforward verification of answers, the task of solving mathematical problems has emerged as pivotal domain in the study of LLM reasoning capacities. One of the conventional strategies for augmenting the reasoning capabilities of LLMs is Supervised Fine-Tuning (SFT). SFT entails fine-tuning LLMs with training samples that incorporate annotated Chain-of-Thought (CoT) (Wei et al., 2022). In training dataset train, each training sample is structured as tuple (x, c, y), where represents the input question, represents the annotated CoT, and denotes the correct ground truth answer. CoT in the training sample is generally written or labeled by experienced experts or highend LLMs, which is highly valuable for the finetuning of LLMs. SFT-based reasoning enhancement approaches only exploit single annotated CoT for each question within the training dataset. However, multiple CoTs (Zhang et al., 2023b) exist for each question. Hence, conducting SFT with only single annotated CoT in the training dataset may limit the generalization capability of LLMs. To address the limitations of SFT-based methods, Reinforcement Learning (RL)-based fine-tuning approaches emerge (Luong et al., 2024; Shao et al., 2024; Liu et al., 2025). prominent and stateof-the-art RL-based fine-tuning approach is ReFT Method A-CoT SG-CoT SFT PPO-like (e.g., ReFT) CARFT Table 1: An overview of whether methods employ Annotated-CoT (A-CoT) or Self-Generated CoT (SGCoT). (Luong et al., 2024), which incorporates online RL approach, i.e., Proximal Policy Optimization (PPO) (Schulman et al., 2017), to dynamically sample CoTs at each training step. This mechanism enables ReFT to leverage multiple CoTs, thereby improving the generalization capability of LLMs. Despite their effectiveness, two major limitations still exist with the existing RL-based fine-tuning approaches, which hinders the advancement of LLMs. First, existing RL-based approaches solely rely on on-policy sampled CoTs without considering the highly valued annotated CoTs while enhancing reasoning performance. Due to the reward hacking problem (Skalse et al., 2022), such sampled CoTs may not be valid or correct, potentially degrading model performance. Second, existing approaches suffer from unstable training. The inherent exploratory nature of RL can lead to model collapse, i.e., phenomenon where the behavior of LLMs significantly deteriorates during training. This instability can severely impact the performance of LLMs and result in undesirable outputs. To address these challenges, we propose novel Contrastive learning with Annotated CoT-based Reinforced Fine-Tuning approach, i.e., CARFT, which effectively leverages the valuable annotated CoTs in the training dataset while sampling other potential CoTs so as to achieve superb performance. CARFT begins with learning unified representation for each CoT, encompassing both high-quality annotated CoTs and on-policy sampled CoTs. Based on this representation, we design contrastive signals to improve both the reasoning performance and the stability of the finetuning process. Specifically, we propose exploiting masked loss, e.g., InfoNCE (Chen et al., 2020), to utilize the unified representation to generate the contrastive signal. This contrastive signal serves as guiding mechanism for the on-policy CoT sampling process, helping to stabilize the fine-tuning of LLMs while maximizing the utilization of information from the annotated CoT. Table 1 and Figure 1 illustrate the working characteristic of CARFT. In addition, we propose embedding-enhanced partial reward to further improve the performance. The key contributions of this paper are summarized as follows: We propose novel contrastive learning-based framework with an original contrastive signal construction method that fully exploits annotated CoTs to improve both the performance and the stability in the fine-tuning of LLMs. We design an embedding-enhanced partial reward so as to further improve the stability in the reinforced fine-tuning process and to achieve superb performance of LLMs. We conduct extensive experiments and thorough ablation studies to demonstrate the effectiveness of CARFT compared with three baseline approaches, two foundation models, and two datasets. Extensive experimental results demonstrate that CARFT significantly outperformances baselines in terms of effectiveness (up to 10.15%) and robustness."
        },
        {
            "title": "2 Related Work",
            "content": "Recent years have witnessed widespread application in Natural Language Processing (NLP), particularly in the domains of preference optimization (Stiennon et al., 2020; Rafailov et al., 2023; Gheshlaghi Azar et al., 2024; Zhu et al., 2025) and reasoning (Luong et al., 2024; Shao et al., 2024; Yu et al., 2025; Liu et al., 2025). These methods typically follow standard three-stage pipeline: (1) SFT, (2) reward modeling, and (3) RL-based optimization. key distinction among these approaches lies in how the reward signal is obtained. In preference optimization, reward models are learned from human feedback, while in mathematical reasoning tasks, rule-based methods are typically exploited to construct reward signals, as ground-truth answers can be explicitly verified. Within the context of preference optimization, Direct Preference Optimization (DPO) (Rafailov et al., 2023) has emerged as an effective algorithm that avoids the need for explicit reward model training. However, due to its offline nature (Feng et al., 2024), DPO may struggle to explore diverse CoTs (Luong et al., 2024). As result, on-policy approaches, e.g., GRPO (Shao et al., 2024), DAPO (Yu et al., 2025), Dr.GRPO (Liu et al., 2025), and ReFT (Luong et al., 2024), are Figure 1: Comparison between SFT, ReFT, and CARFT on the exploration of CoT. generally employed to better explore such diversity in reasoning. On-policy approaches utilize multiple rollouts to estimate the Generalized Advantage Estimation (GAE). DAPO and Dr.GRPO are both improved variants of GRPO. Specifically, DAPO is designed for long-CoT scenarios and introduces four key techniques: higher clipping, dynamic sampling, token-level policy gradient loss, and overlong reward shaping. On the other hand, Dr.GRPO improves upon GRPO by eliminating the bias present in the original method. While these approaches are effective, they come with the trade-off of increased computational complexity. In contrast, ReFT (Luong et al., 2024) utilizes only single on-policy sample per step, making it significantly computationally efficient. Despite their strengths, these approaches rely solely on on-policy sampling, ignoring potentially valuable annotated CoTs already present in the training data. Moreover, model collapse occurs frequently within the reinforced fine-tuning process with the existing approaches."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first present the preliminary of reinforced LLM fine-tuning. Then, we detail CARFT, including the contrastive learning-based framework with an original contrastive signal construction method and an embedding enhanced partial reward method. 3.1 Preliminary of Reinforced Fine-Tuning Reinforced fine-tuning incorporates feedback signals derived from either learned reward model or predefined rules to guide the training of LLMs. Given an input prompt and the corresponding response produced by LLM, the objective is to maximize the expected cumulative reward, which is formally formulated as: max πθ xD,yπθ(yx) [r(x, y)] βDKL [πθ(y πref(y x) x)] , (1) where r(x, y) denotes the reward function, θ represents the parameters of the LLM, πθ refers to the learnable target policy, πref corresponds to reference policy, i.e., typically the initially pre-trained LLM, exploited to stabilize training, and β denotes the coefficient of KL-divergence, which encourages the updated policy to stay close to the original distribution. commonly used algorithm in this setting is PPO (Schulman et al., 2017), which employs GAE (Schulman et al., 2015) for stable gradient updates. In particular, the advantage at time step is calculated as: Lt (cid:88) ˆAt = (γλ)lσt+l, (2) l=0 where represents the maximum length of token sequence, σt is the Temporal Difference (TD) residual at timestep t, λ (0, 1] serves as the [0, 1] controls the GAE discount factor, and γ discounting of TD residuals. σt is formulated as: σt = Vϕ(st) + rtotal(st, at, st+1) + γVϕ(st), where st denotes the state at time step t, and at represents the action at time step t, γ is similar as defined in Formula 2, rtotal( ) calculates the total ) estimates the state value from reward, and Vϕ( given state with ϕ referring to vector of policy parameters. Actions correspond to individual tokens selected from the vocabulary. Figure 2: The framework CARFT is composed of two sequential stages: (i) supervised fine-tuning (SFT), followed by (ii) contrastive feedback. The total reward at the token level includes both the external reward signal and an internal regularization based on the KullbackLeibler (KL) divergence between the current and reference policies, as defined as follows: rtotal(st, at, st+1) = r(st, at) + βDKL [πθ( st) πref( st)] , outside of the interval [1 - ϵ, 1 + ϵ] (see details in (Schulman et al., 2017)). Then, the value loss is defined as: (cid:104) max (cid:16) value(ϕ) = (cid:13) (cid:13) (cid:13) (cid:13) clip 1 2 (cid:16) ˆRt Vϕ(st) Vϕ(st), ˆAt 2, ˆRt ϵ, ˆAt + ϵ 2(cid:19)(cid:21) (cid:17)(cid:13) (cid:13) (cid:13) , where πθ represents the sampling actions policy with policy parameters ϕ, and πref corresponds to the sampling actions with reference policy. Given GAE ˆAt and state value Vϕ(st), we can estimate the reward as defined in Formula 3: where ϵ is similar as defined in 4. Finally, the overall reinforcement learning loss combines both the policy loss and the value loss as defined in Formula 5: ˆRt = ˆAt + Vϕ(st). (3) RL = policy + α value, (5) Under the PPO framework, the policy and value loss functions are separately formulated to ensure stable and effective fine-tuning process. The policy loss is defined in Formula 4. policy(θ) = (cid:20) min (cid:18) πθ(at πold θ (at st) , 1 st) st) st) ˆAt, (cid:19) ϵ, 1 + ϵ clip (cid:18) πθ(at πold θ (at (cid:19)(cid:21) , ˆAt (4) θ where πold corresponds to the sampling process before update, ϵ represents hyperparameter controlling the clipping range and preventing excessively large policy updates, and ˆAt modifies the surclip rogate objective by clipping the probability ratio, which removes the incentive for moving πθ(atst) πold θ (atst) (cid:16) πθ(atst) πold θ (atst) ϵ, 1 + ϵ , 1 (cid:17) where α balances the relative importance of the policy and value losses within the reinforced finetuning process. 3.2 CARFT While existing approaches either overemphasizes the annotated CoT (for SFT) or face challenges in achieving stable reinforced fine-tuning while ignoring annotated CoT (for existing RL-based approaches), we propose novel contrastive learningbased approach, i.e., CARFT, to properly levarage the annotated CoTs so as to address this issue. In this section, we first present the overall workflow of CARFT. Then, we explain the CoT embeddings. Afterward, we propose masked contrastive signal construction approach in CARFT. Finally, we explain novel embedding enhanced partial reward method. 3.2.1 Workflow As shown in Figure 2, the overall workflow of CARFT consists of two sequential stages: the SFT stage and the reinforced fine-tuning stage. SFT We assume that each training sample in the training dataset is triplet (x, c, y), where denotes the input question, represents the annotated CoT, and is the ground-truth answer. We carry out SFT with few epochs to improve the instruction-following ability of the LLM. Contrastive Feedback Let c1 and c2 denote two distinct CoTs corresponding to the same input question x1, derived either from training examples or on-policy sampling, i.e., (x1, c1, y1) and (x1, c2, y1). We assume that y1 is the valid answer to x1. Since c1 and c2 pertain to the same input x1, we posit the existence of conditional distribution h1) and p1(c h1), where h1 denotes latent varic2 able associated with x1. Given that c1 and c2 are sampled from the same distribution, there should ) under which the exist similarity metric m( distance between c1 and c2 is smaller than the distance between c1 and any ci drawn from different hi), with high probability: distribution pi,i=1(c h1) such that both c1 p1(c p1(c , m(c1, c2) m(c1, ci), for ci pi,i=1(c hi). This insight provides two key advantages when incorporated as an unsupervised signal. First, it enables us to exploit the annotated CoTs in the training data in the reinforced fine-tuning process of LLMs. Second, it offers guiding signal for CoT generation, helping to stabilize the reinforced finetuning process and to mitigate the risk of model collapse. 3.2.2 Chain-of-Thought Embeddings Given CoT of length L, represented as: = [a1, a2, , aL] , we denote the corresponding token embeddings and state values as: = [H1, H2, , HL] and Vϕ = [Vϕ(1), Vϕ(2), , Vϕ(L)] , respectively. To obtain compact representation of the entire CoT, we compute weighted sum of the token embeddings using the softmax-normalized state values, as defined in Formula 6. (cid:88) = Softmax(Vϕ) H, (6) where denotes element-wise multiplication between the state values and the corresponding embedding vectors. In practice, in order to reduce memory consumption, we first project each embedding Hi into lower-dimensional space using simple singlelayer MultiLayer Perceptron (MLP), denoted by proj( ). The projected embeddings are then exploited in place of the original ones: = [proj(H1), proj(H2), , proj(HL)] . 3.2.3 Masked Contrastive Signal Construction In this section, we design two types of contrastive signals for reinforced fine-tuning, i.e., positive and negative. We denote the signal related to CoT that results in correct answer by positive signal, and that results in wrong answer by negative signal. xi, crollout Positive Signal Given batch of training samples xi, cannotated 1 with presenting the batch , yi} { size, we conduct LLM self-generation to generB ate batch of rollout CoTs, i.e., 1 . , yi} By employing the CoT embedding module, we could get embeddings of the annotated CoTs eannotated 1 exploiting { } the approach presented in Section 3.2.2. We construct contrastive feedback with InfoNCE (Chen et al., 2020) to provide the positive contrastive signal as defined in Formula 7. 1 and rollout CoT erollout { } { , (cid:88) i= log Lc1 = /τ ) , erollout , erollout eannotated eannotated exp( M1 (cid:80)B j=1 exp( /τ ) (7) where M1 represents binary mask, in which each element takes the value 1 if the corresponding CoT leads to correct answer, and 0 otherwise. The notation denotes the inner product. , Negative Signal We devise scheme to utilize the signal within the negative CoT as well. We denote the annotated CoTs and the associated negative CoTs by cannotated , respectively. Initially, we calculate the Longest Common Subsequence (LCS) of cannotated . Subsequently, based on the LCS and the parts of the sequence that exclude the LCS, we construct four and crollout and crollout i , erollout i,LCS , i,LCS i,exc , respectively. Then, the negative conembeddings, denoted eannotated and erollout trastive signal is formulated in Formula 8. , eannotated i,exc i= log (cid:88) Lc2 = exp( (cid:80)B j=1 exp( erollout i,LCS , eannotated /τ ) i,exc erollout i,LCS , erollout j,exc M2 /τ ) (8) M2 represents binary mask, in which each where element takes the value 1 if the corresponding CoT leads to wrong answer, and 0 otherwise. Optimization We optimize the following reinforcement learning loss to learn the policy: = RL + {Lc1 or Lc2} (9) where balances the relative importance of the PPO and contrastive losses during the reinforced fine-tuning process (see detailed algorithms in Appendix). 3.2.4 Embedding-enhanced Partial Reward In order to further improve the stability and the performance of the contrastive signal, we propose an embedding-enhanced partial reward method. ReFT (Luong et al., 2024) assigns partial reward r(x, y) = 0.1 to the CoT when it is negative CoT, from which numerical answer can be extracted. Unlike the partial reward in ReFT, we introduce fine-grained partial reward by leveraging our unified CoT Embedding, which provides tool to measure CoT similarity. r(x, y) = eannotated, erollout 0.1 + 0.2. (10) The inner product eannotated, erollout ranges from 1 to 1, leading to partial reward range of [0.1, 0.3]. When the CoTs are dissimilar, the in1, resulting in reward ner product approaches close to 0.1; when they are similar, the reward approaches 0.3. This strategy encourages wellbehaved CoT generation. By assigning differentiated rewards to negative CoTs, the embeddingenhanced partial reward method further improve the stability of the reinforced fine-tuning process and the final performance of LLMs."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present the experimental results. First, we present our experiment setup. Then, we demonstrate the evaluation of CARFT compared with SFT, ReFT, and Dr.GRPO. Afterward, we present an ablation study. Method SVAMP GSM8K #Train Samples #Test Samples 3076 1000 7465 , Table 2: Statics of the train and test datasets. 4.1 Experimental Setup We conduct experiments on two publicly available datasets: SVAMP (Patel et al., 2021) and GSM8K (Cobbe et al., 2021). Table 2 presents the key statistics of SVAMP and GSM8K. For the reasoning process, we leverage the CoT annotations from (Luong et al., 2024), which were generated based on few-shot prompting (Wei et al., 2022; Gao et al., 2023) with GPT-3.5-turbo (OpenAI, 2023). Our experiments are conducted based on two foundation models: CodeLlama-7B (Rozière et al., 2023) and Qwen2.5-7B-Instruct (Team, 2024). We evaluate CARFT in comparison with three baseline approaches: SFT, ReFT (Luong et al., 2024), and Dr.GRPO (Liu et al., 2025). ReFT is state-ofthe-art RL approach for LLM fine-tuning. As an advanced extension of GRPO (Shao et al., 2024), Dr.GRPO demonstrates excellent performance on R1-like (DeepSeek-AI, 2025) tasks. See setup details in Appendix. 4.2 Evaluation of CARFT As illustrated in Table 3, CARFT significantly outperforms SFT and ReFT across different models (up to 10.15% on average). With the SVAMP dataset, CARFT yields substantial accuracy enhancements compared with SFT. Precisely, the accuracy escalates from 62.3% to 64.8% and from 86.9% to 88.0%, with absolute increments of 2.5% and 1.1% for CodeLlama and Qwen2.5-Instruct, respectively. Moreover, with the GSM8K dataset, CARFT showcases remarkable improvements as well. The accuracy climbs from 43.82% to 50.95% and from 80.67% to 84.31%, corresponding to absolute boosts of 7.13% and 3.64% for CodeLlama and Qwen2.5-Instruct, respectively. Table 3 further reveals that ReFT can outperform the SFT baseline once the training process stabilizes. Nevertheless, the performance of ReFT remains inferior to CARFT. In addition, the experimental results demonstrate that ReFT is plagued by the model collapse issue, which significantly undermines its effectiveness. Furthermore, we find that when model collapse occurs, the performance Figure 5: Accuracy of CARFT with positive signal and negative signal, based on the SVAMP dataset and with the CodeLlama-7B as the backbone model. Figure 3: Accuracy curves of various methods on SVAMP dataset and Qwen2.5-7B-Instruct backbone. Figure 4: Accuracy curves of various methods on SVAMP dataset and CodeLlama-7B backbone. of ReFT lags far behind that of SFT (up to 14.56%) and CARFT (up to 18.2%). Furthermore, Figures 3 depicts the accuracy curves of different approaches on the SVAMP dataset, with Qwen2.5-7B-Instruct serving as the backbone model. These results indicate that ReFT undergoes model collapse after undergoing finetuning for just one epoch. In addition, we find that when using Qwen2.5-7B-Instruct, SFT is prone to an unstable tuning process, as evidenced by the decline in its accuracy as the fine-tuning process advances. In contrast, CARFT exhibits remarkable stability and superb performance throughout the entire training process. This outstanding performance can be attributed to our contrastive feedback mechanism as presented in Section 3.2.3, which offers reference signals for the generation of CoTs. As demonstrated in Figure 4, CARFT consistently outperforms ReFT during the fine-tuning Figure 6: Accuracy Curve of CARFT with positive signal and negative signal, based on the SVAMP dataset and with the CodeLlama-7B as the backbone model. process and converges rapidly, swiftly reaching peak accuracy values. Nevertheless, the figure also suggests that CARFT is potentially vulnerable to unstable fine-tuning process. See additional experimental results in Appendix. As shown in Table 4, CARFT with embeddingenhanced partial reward enabled significantly outperforms all baseline approaches in terms of both accuracy (up to 0.5% compared with Dr.GRPO and 1.7% compared with ReFT) and efficiency (up to 30.62% compared with Dr.GRPO). Interestingly, Dr.GRPO also surpasses ReFT in terms of performance metrics (1.2%), which is accompanied with considerable increase in computational time. Specifically, Dr.GRPO relies on significant computing resources due to the generation of larger number of CoTs. 4.3 Ablation Study Positive Signal versus Negative Signal We conduct an ablation study to show the impact of positive and negative contrastive signals. As shown in Figure 5, CARFT outperformances ReFT with both Method Size SVAMP GSM8K Average CodeLlama + SFT CodeLlama + ReFT (Luong et al., 2024) CodeLlama + CARFT Qwen2.5-Instruct + SFT Qwen2.5-Instruct + ReFT (Luong et al., 2024) Qwen2.5-Instruct + CARFT 7B 7B 7B 7B 7B 7B 62.3% 43.82 % 53.06% 62.5% 50.27% 56.39% 64.8% 50.95% 57.88% 86.9% 80.67% 83.79% 85.9% 66.11% 76.01% 88.0% 84.31% 86.16% Table 3: Evaluation Accuracy of Various Methods on the SVAMP and GSM8K Datasets. Method Accuracy Time Cost(hours) ReFT Dr.GRPO CARFT 62.5% 63.7% 64.2% 14.12 24.49 16.99 Table 4: Evaluation Accuracy of Various Methods on the SVAMP Datasets, based on CodeLlama-7B. Figure 7: Accuracy of CARFT with different c, based on the SVAMP dataset and with the CodeLlama-7B as the backbone model. positive (2.30% higher) and negative contrastive (0.4% higher) signals. Notably, the positive signal demonstrates more pronounced performance gain (1.9% higher) compared to its negative counterpart. Due to its excellent performance, we employ the positive signal in our experiments. Robustness To assess the robustness of the proposed method, we perform an ablation study on the contrastive loss coefficient c. As illustrated in Figure 7, by systematically varying the value of 103, within the range from 5 we observe that CARFT consistently outperforms the SFT and ReFT baseline across all tested values. This consistent superiority in performance strongly validates the robustness of CARFT and demonstrates its resilience to changes in the contrastive loss coefficient. 104 to 1.5 Stability To further enhance the stability of the reinforced fine-tuning process, we propose an Figure 8: Accuracy Curve of CARFT Partial Rewards, based on the SVAMP dataset and with the CodeLlama7B as the backbone model. embedding-enhanced partial reward method, as described in Section 3.2.4. As shown in Figure 8, this approach effectively improves training stability. The tuning process of CARFT using the embedding-enhanced partial reward method achieves final accuracy that is 0.5% higher than that of the baseline without the method. Moreover, CARFT with this enhancement exhibits more stable accuracy improvement curve. CARFT with embedding-enhanced partial reward enabled achieves peak accuracy of 64.2%, which also corresponds to significant improvements over ReFT, with gains of up to 1.7%."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we propose novel contrastive learning-based framework with annotated CoTs, i.e., CARFT, to enhance the reasoning capabilities of LLMs. We propose generating contrastive signals from both positive and negative CoTs while incorporating annotated CoTs. In order to further improve the stability of the reinforced fine-tuning process, we propose novel embedding-enhanced partial reward method. Extensive experimental results demonstrate significant advantages of CARFT in terms of performance (up to 10.15%) and efficiency (up to 30.62%). In addition, CARFT corresponds to better stability during reinforced finetuning compared with existing approaches."
        },
        {
            "title": "Limitations",
            "content": "CARFT requires additional computational overhead to compute the embeddings of CoTs, to achieve excellent. As result, it consumes longer computational time compared to the ReFT and SFT. However, CARFT needs less computational time than Dr.GRPO as CARFT needs less on-policy sampled CoTs. In addition, CARFT is designed to exploit centralized annotated CoT dataset. The annotated datasets may be stored in multiple data centers or devices, which may hinder the application of CARFT with decentralized data. In this work, we have restricted the context to fewer than 1024 tokens; we plan to explore improving reasoning in long-context scenarios (Zhu et al., 2024)."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. Claude 3.7. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, and 1 others. 2025. Llama-nemotron: Efficient reasoning models. arXiv preprint arXiv:2505.00949. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML20. JMLR.org. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR). Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems (NeurIPS). DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. 2024. Towards analyzing and understanding the limitations of dpo: theoretical perspective. arXiv preprint arXiv:2404.04626. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: program-aided language models. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. 2024. general theoretical paradigm to understand learning from human preferences. In AISTATS. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, and 1 others. 2024. Openai o1 system card. arXiv preprint arXiv:2412.16720. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Hoi. 2022. CodeRL: Mastering code generation through pretrained models and deep reinforcement learning. In Advances in Neural Information Processing Systems. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. 2025. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language modarXiv preprint els via reinforced evol-instruct. arXiv:2308.09583. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. 2024. Reft: Reasoning with reinforced fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. OpenAI. 2023. Gpt-3.5 turbo fine-tuning and api updates. OpenAI. 2025. Introducing openai o3 and o4-mini. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094, Online. Association for Computational Linguistics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In NeurIPS. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 20. IEEE Press. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. KDD 20, page 35053506, New York, NY, USA. Association for Computing Machinery. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, and 7 others. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and P. Abbeel. 2015. Highdimensional continuous control using generalized advantage estimation. CoRR, abs/1506.02438. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera Arcas, Dale Webster, and 11 others. 2022. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138. Joar Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger. 2022. Defining and charIn Proceedings of the acterizing reward hacking. 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In NeurIPS. Qwen Team. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif Rasul, Alvaro Bartolome, Alexander M. Rush, and Thomas Wolf. The Alignment Handbook. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. 2020. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2024. Mathcoder: Seamless code integration in LLMs for enhanced mathematical reasoning. In The Twelfth International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023. Fingpt: Open-source financial large language models. FinLLM Symposium at IJCAI 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, and 16 others. 2025. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476. Boyu Zhang, Hongyang Yang, tianyu Zhou, Ali Babar, and Xiao-Yang Liu. 2023a. Enhancing financial sentiment analysis via retrieval augmented large language models. ACM International Conference on AI in Finance (ICAIF). Mengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi Feng, and Andrew Lan. 2023b. Interpretable math word problem solution generation via step-by-step planning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68586877, Toronto, Canada. Association for Computational Linguistics. Wenqiao Zhu, Ji Liu, Lulu Wang, Jun Wu, and Yulun Zhang. 2025. Sgdpo: Self-guided direct preference optimization for language model alignment. In Findings of the Association for Computational Linguistics: ACL 2025. Wenqiao Zhu, Chao Xu, Lulu Wang, and Jun Wu. 2024. Psc: Extending context window of large language models via phase shift calibration. In EMNLP."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Experimental Settings All experiments are conducted on an ensemble of 8 H100-80GB GPUs. Given that reinforced fine-tuning for reasoning tasks is inherently timeintensive, we utilize FlashAttention (Dao et al., 2022; Dao, 2024) and DeepSpeed Zero stage 3 (Rajbhandari et al., 2020; Rasley et al., 2020) to expedite the fine-tuning process. These technologies enable us to scale up the batch size, thereby enhancing computational efficiency. Additionally, we utilize the HuggingFace Alignment Handbook (Tunstall et al.) and the TRL library (von Werra et al., 2020) as methodological guides to streamline the fine-tuning implementation. To ensure consistency and comparability across experiments, we adopt structured hyperparameter configuration strategy. During the warmup phase, we initialize training with batch size of 64 and learning rate of 1e-5. This learning rate is then adjusted to 3e-7 during the reinforcement fine-tuning stage to stabilize the optimization process. We maintain batch size of 64 for all models on the SVAMP dataset. On GSM8K, we tailor the batch size to each models computational characteristics: 64 for Qwen2.5-7B-Instruct and 96 for CodeLlama-7B, balancing memory efficiency and training throughput. In reinforcement learning components, we set the KL divergence coefficient to 0.05 to regulate policy updates and employ temperature parameter (τ ) of 0.2 and = 1e 3 in the contrastive learning loss to control embedding similarity. We set the dimension of the projected embedding to 64. For PPO optimization, we configure λ = 1, γ = 0.95, α = 5, ϵ = 0.2, and = 2. For Dr.GRPO, to ensure fair comparison, we set the parameter in Dr.GRPO to 2, which matches the maximum number of CoTs in CARFT at each step. We set the reward r(x, y) to 1 if the answer is correct, and 0 otherwise. We also adopt partial reward scheme (Le et al., 2022), setting the reward to 0.1 in cases where numerical answer can be extracted but is incorrect. Training epoch limits are determined based on empirical convergence behavior. For the SFT baseline, we cap training at 60 epochs due to its tendency to be unstable; beyond this point, additional epochs yield diminishing returns. To ensure fair comparison across methods, for ReFT, CARFT, and Dr.GRPO, we fine-tune the base model for 4 epochs and select the best checkpoint for reinBatch Size Accuracy 64 96 51.48% 50.95% Table 5: CARFT with different batch size, based on CodeLlama-7B model and GSM8K dataset. Figure 9: Accuracy curves of various methods on GSM8K dataset and Qwen2.5-7B-Instruct backbone. forced fine-tuning. All of these approaches are then trained for 70 epochs, allowing sufficient iterations for convergence while maintaining experimental rigor. A.2 More Experiments Batch Size In our experiments, we utilized FlashAttention (Dao et al., 2022; Dao, 2024) and DeepSpeed Zero stage 3 (Rajbhandari et al., 2020; Rasley et al., 2020) to accelerate the fine-tuning process with large batch size. To evaluate how batch size affects model performance, we conducted systematic ablation study. As shown in Table 5, increasing the batch size can degrade the performance of large language models (LLMs). Specifically, enlarging the batch size from 64 to 96 led to drop in accuracy from 51.48% to 50.95%. This suggests that reducing the batch size may be viable strategy for achieving better performance. It is worth emphasizing that all experiments were carried out with consistent batch size configurations to ensure fair and valid comparison. Explains of the Accuracy Curve Figure 9 indicates that ReFT also suffers from model collapse, which yields poor results. CARFT shows strong stability across the whole fine-tuning process and outperforms both SFT and ReFT significantly. Figure 10: Accuracy curves of various methods on GSM8k dataset and CodeLlama-7B backbone. Figure 12: RL loss curve for CARFT with the GSM8K dataset and CodeLlama-7B serving as the backbone model. Figure 11: Accuracy Curve of CARFT with different c, based on the SVAMP dataset and with the CodeLlama7B as the backbone model. Figure 10 presents the accuracy of various methods on the GSM8K dataset using the CodeLlama7B model as the backbone. We observed that further training did not lead to performance improvements in the SFT (Supervised Fine-Tuning) phase, so we terminated the training early. The figure also demonstrates that CARFT outperforms ReFT with higher convergency accuracy. Figure 11 illustrates the accuracy curves of CARFT under different values of parameter c. It can be observed that CARFT attains the optimal performance when = 1e 3. Loss We present the RL (Reinforcement Learning) and contrastive learning loss curves for CARFT and ReFT models in Figures 12, 13, 14, 15, 16, and 17. These results are based on the GSM8K dataset and utilize the CodeLlama-7B and Figure 13: Constrastive loss curve for CARFT with the GSM8K dataset and CodeLlama-7B serving as the backbone model. Qwen2.5-7B-Instruct base models. We make the following observations: (1) As shown in Figure 12 and Figure 14, when using CodeLlama-7B as the base model, CARFT and ReFT exhibit similar loss curves. (2) In contrast, when Qwen2.5-7BInstruct is used as the base model, the loss curves of CARFT and ReFT differ significantly. In particular, ReFT displays fluctuating pattern, suggesting instability during fine-tuning. (3) Furthermore, as seen in Figure 13 and Figure 16, the contrastive loss varies across models, indicating differences in the CoT embedding spaces learned by each model. A.3 CARFT with Positive Signal We describe the CARFT framework with positive signal in Algorithm 1. For each pair (c, ˆc), where denotes the annotated CoT and ˆc represents the Algorithm 1: CARFT with Positive Signal Input :Tuples of (question, CoT, answer): train = of updates per RL step:U , Initial policy: π0 θ . (x, c, { } , Number of RL steps: , Number Output :Final Policy: πθ 1 to do 1 for x, c, ˆc πθ EXTRACT(ˆc) ˆy eannotated c, Compute σt, ˆAt, ˆRt, for 1 to do ˆerollout M1 4 5 6 7 8 θ, ϕ end 9 10 end 11 return πθ // Sample training data from train // On-policy CoT sampling train // Extract answer ˆc // Construct CoT Embeddings OPTIMIZATION_STEP( ) // Equation 9 Algorithm 2: CARFT with Negative Signal :Tuples of (question, CoT, answer): train = of updates per RL step:U , Initial policy: π0 θ . Input (x, c, { } , Number of RL steps: , Number Output :Final Policy: πθ 1 to do 1 for 2 x, c, ˆc πθ EXTRACT(ˆc) ˆy eannotated , eannotated exc LCS Compute σt, ˆAt, ˆRt, for 1 to do θ, ϕ 3 4 6 7 8 M2 end 9 10 end 11 return πθ // Sample training data from train // On-policy CoT sampling train // Extract answer LCS , ˆerollout ˆerollout ˆc exc c, // Construct CoT Embeddings OPTIMIZATION_STEP( ) // Equation Figure 14: RL loss curve for ReFT with the GSM8K dataset and CodeLlama-7B serving as the backbone model. Figure 16: Contrastive loss curve for CARFT with the GSM8K dataset and Qwen2.5-7B-Instruct serving as the backbone model. Figure 15: RL loss curve for CARFT with the GSM8K dataset and Qwen2.5-7B-Instruct serving as the backbone model. Figure 17: RL loss curve for ReFT with the GSM8K dataset and Qwen2.5-7B-Instruct serving as the backbone model. on-policy sampled CoT, we construct the corresponding embeddings for each CoT, resulting in eannotated and ˆerollout, respectively. Which is then utilized to guide the fine-tuning steps. A.4 CARFT with Negative Signal The masked InfoNCE loss then leverages erollout LCS , to provide feedback for train- , and ˆerollout exc eannotated exc ing. We describe the CARFT framework with negative signal in Algorithm 2. For each pair (c, ˆc), where denotes the annotated CoT and ˆc represents the onpolicy sampled CoT, we first compute the longest common subsequence (LCS) between the two sequences. Using the LCS tokens, we construct corresponding LCS embeddings for both sequences, and ˆerollout resulting in eannotated LCS , respectively. The remaining tokensthat is, those not included in the LCSare used to generate two additional emand ˆerollout beddings: eannotated LCS . exc exc"
        }
    ],
    "affiliations": [
        "HiThink Research",
        "Shanghai Jiao Tong University"
    ]
}