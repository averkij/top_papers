{
    "paper_title": "Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning",
    "authors": [
        "Zhenwen Liang",
        "Sidi Lu",
        "Wenhao Yu",
        "Kishan Panaganti",
        "Yujun Zhou",
        "Haitao Mi",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 7 8 6 5 1 . 2 1 5 2 : r Technical Report Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning Zhenwen Liang1, Sidi Lu1, Wenhao Yu1, Kishan Panaganti1, Yujun Zhou1,2, Haitao Mi1, Dong Yu1 1Tencent AI Lab, 2University of Notre Dame Correspondence to: zhenwzliang@global.tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface-level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, gradient-guided reinforcement learning framework in which exploration is driven not by external heuristics but by the models own first-order update geometry. For each response, G2RL constructs sequence-level feature from the models final-layer sensitivityobtainable at negligible cost from standard forward passand measures how each trajectory would reshape the policy by comparing these features within sampled group. Trajectories that introduce novel gradient directions receive bounded multiplicative reward scaler, while redundant or off-manifold updates are deemphasized, yielding self-referential exploration signal that is naturally aligned with PPO-style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUPRO) on Qwen3-base 1.7B/4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy-based GRPO and external-embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonaland often opposinggradient directions while maintaining semantic coherence, revealing that policys own update space provides far more faithful and effective basis for guiding exploration in LLM RL."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has become central mechanism for improving the reasoning and decision-making abilities of large language models (LLMs), extending their capabilities beyond supervised finetuning (Christiano et al., 2017; Ouyang et al., 2022; Rafailov et al., 2023). Yet, despite this progress, exploration in LLM RL remains fundamentally underdeveloped. Current exploration strategiesentropy bonuses, outcome rarity, or external semantic comparatorsare all driven by signals extrinsic to the model. They encourage the policy to sample more widely, but they do so without regard for the models own update structure. As result, exploration often becomes diffuse, misaligned, or fragile under sparse reward signals, especially when supervision is binary or verifiable (Sutton et al., 1998; Auer et al., 2002; Kakade & Langford, 2002). Entropy increases randomness but is oblivious to whether two responses induce meaningfully different directions. External similarity models based on semantic embeddings provide sequence-level contrast but operate in representation spaces that differ from the models internal geometry. trajectory may appear novel semantically yet offer no new gradient information for the policy; conversely, trajectories essential for improving the models reasoning may be penalized for superficial semantic similarity. In short, existing methods guide exploration through lenses external to the policy, producing an enduring mismatch between exploration signals and the optimization dynamics that actually govern learning. 1 Technical Report Figure 1: Comparison of the characteristics of GRPO, EVOL-RL, and G2RL. The dispersion of points indicates semantic variety, while arrows represent gradient directions. Only G2RL explicitly encourages exploration aligned with the models intrinsic update geometry. This motivates more principled question: Can an LLM learn to explore by examining how each trajectory would reshape its own parameters, rather than relying on external proxies? Such an approach would shift exploration from being externally imposed to being self-guided: trajectories are preferred when they meaningfully expand the policys own update directions, and discouraged when they contribute redundant or uninformative gradients. We propose G2RL, Gradient-guided reinforcement learning framework that realizes this idea. Instead of encouraging diversity in the output space, G2RL encourages exploration directly in the policys update geometry. For each generated response, we construct sequence-level feature derived from the models own first-order sensitivity at the final layeravailable at negligible cost from standard forward pass. This feature summarizes how the trajectory would steer the models output distribution through its gradients. By comparing these features within group of candidates, we obtain policy-intrinsic exploration score: responses that introduce new gradient directions are upweighted through bounded reward scaler, whereas responses that merely reinforce existing update directions receive less emphasis. This gradient-guided mechanism offers three conceptual advantages. First, it grounds exploration in the same geometry that governs optimization, eliminating the semanticoptimization mismatch inherent in external comparators. Second, it provides self-referential criterion: the model explores what it stands to learn from, not what appears diverse to an auxiliary encoder. Third, the construction integrates seamlessly into GRPO (Shao et al., 2024), requiring no extra backward passes. We evaluate G2RL across math and general reasoning benchmarks and multiple LLM scales, finding consistent improvements in pass@1, maj@16, and pass@k. Beyond raw accuracy, G2RL produces richer reasoning trajectories and more meaningful gradient dispersion, supporting the central hypothesis that exploration should be guided not by external heuristics but by the policys own update geometry. Contributions. We introduce G2RL, gradient-guided RL method that defines exploration through the models own first-order update geometry, avoiding reliance on entropy or external semantic encoders. We propose bounded, groupwise reward-scaling mechanism that emphasizes trajectories offering novel gradient directions while preserving optimization stability. Experiments on math and general reasoning tasks demonstrate that G2RL systematically improves single-sample accuracy and multi-sample coverage, achieving healthier and more structurally meaningful exploration dynamics."
        },
        {
            "title": "2 Method",
            "content": "We introduce G2RL, gradient-guided reinforcement learning method for large language models (LLMs) that augments group-relative policy optimization (GRPO) with an exploration signal 2 Technical Report computed in the policys own update space. Rather than rewarding diversity in output space, G2RL adjusts the contribution of each trajectory according to how it reshapes the models gradient directions. This section formalizes the setting, recalls GRPO, derives the gradient features, defines groupwise gradient-guided exploration score, and integrates it into stable PPO-style objective with KL control. All symbols are summarized as they appear. 2.1 Preliminaries and Group-Relative Policy Optimization Let be set of prompts and the response space. For , response is the token sequence = (y1, . . . , yL) Y. An autoregressive LLM with parameters θ defines pθ(y x) = t=1 pθ(yt x, y<t) . (1) We generate, for each prompt x, group of candidate responses {y(i)}m i=1 from fixed behavior policy πθold (autoregressive or nucleus sampling). Each response receives base scalar reward r(i) R, e.g., verifiable pass/fail or task-specific score. GRPO (Shao et al., 2024) dispenses with learned critic and estimates groupwise advantages by standardizing rewards within the group: = 1 i=1 r(i), sr = (cid:115) 1 i=1 (cid:0)r(i) r(cid:1)2 + ε, A(i) = r(i) sr , (2) with ε > 0 for numerical stability. Let ρ(i) = (cid:16) πθ πθold (cid:17) y(i) (cid:0)y(i) x(cid:1) , clip(u; 1 ϵ, 1 + ϵ) = min(cid:8)max{u, 1 ϵ}, 1 + ϵ(cid:9), (3) and let DKL(πθ( x) πref( x)) be per-prompt KL penalty to reference policy (e.g., the SFT model). The GRPO objective is JGRPO(θ) = x,{y(i)}π θold (cid:34) 1 i=1 (cid:16) min ρ(i) A(i), clip(cid:0)ρ(i); 1 ϵ, 1 + ϵ(cid:1)A(i)(cid:17) (cid:35) β DKL(πθπref) . (4) The clipping term stabilizes policy updates, while the KL regularizer prevents drift from the reference policy. G2RL modifies only the way advantages are constructed, leaving PPO-style clipping and KL control unchanged. 2.2 Gradient Features: Policy-Referential Sensitivity Let ht Rdh denote the final-layer hidden state at time t, and let the LM head be linear: zt = Wht + RV, pt = softmax (cid:17) , (cid:16) zt (5) where RVdh , RV, vocabulary size V, and temperature > 0. Writing e(yt) {0, 1}V for the one-hot of the realized token, the standard score-function identity yields the exact token-level gradient with respect to ht: log pθ(yt x, y<t) ht = (cid:16) 1 e(yt) pt (cid:17) Rdh . Define the token residual rt := e(yt) pt and the token feature ϕt := Wrt Rdh . 3 (6) (7) Technical Report Up to the constant 1/T, ϕt is the exact first-order sensitivity of the log-likelihood to perturbations at ht. Aggregating over the response produces sequence-level feature Φ(x, y) = t=1 αt ϕt, αt = αt s=1 αs + ε , (8) with default uniform weights αt 1 and masking for non-response tokens if applicable. Interpretation. By (5)(6), any infinitesimal perturbation δht induces δ log pθ(yt ) ϕt/T, δht, so the sequence feature Φ(x, y) summarizes, to first order, how the response (x, y) would steer the models output distribution through its final-layer representation. As detailed in Appendix A.1, the full parameter gradient along trajectory factors through this feature: for every layer there exists trajectory-dependent linear operator Lk(x, y) such that θk ℓ(x, y) = 1 Lk(x, y) Φ(x, y). Thus, all upstream updates lie in linear images of the same sequence feature Φ(x, y), and angular relations between responses in Φ-space are propagatedup to layerwise linear transformsto the actual optimization directions. We use these features not because the last layer dominates all upstream effects, but because it forms the unique first-order sensitivity bottleneck through which trajectory-specific information must pass, and it is available from the forward pass without any extra backpropagation. This makes cosine geometry on Φ principled and computationally cheap proxy for comparing how different trajectories guide the policys updates. 2.3 Groupwise Gradient-Guided Exploration Score Given group {Φ(i)}m similarities: i=1, we first unit-normalize the sequence features and define pairwise cosine ˆΦ(i) = Φ(i) Φ(i)2 + ε , Sij = (cid:10) ˆΦ(i), ˆΦ(j)(cid:11) [1, 1]. Next, we construct reward-weighted coefficients wij = exp(cid:0)r(j)(cid:1) 1{j = i} k=i exp(cid:0)r(k)(cid:1) + ε , which form probability distribution over the other candidates in the group: wij 0 and j=i wij = 1. Higher-reward responses therefore act as more important reference directions when we assess the contribution of y(i). Using these ingredients, we define bounded, scale-invariant gradient-guided exploration score: ν(i) = (cid:115) (cid:16) max (cid:17) wij S2 ij, 0 [0, 1]. 1 j=i (11) ij measures how well the direction ˆΦ(i) can be explained by Intuitively, the term j=i wij S2 weighted combination of the other responses gradient directions. If ˆΦ(i) is almost parallel to several high-reward peers, the weighted squared cosine similarities S2 ij are large and their sum approaches 1; ij is small and ν(i) is close to 0, indicating that y(i) contributes little new in that case, 1 j=i wijS2 information to the update geometry. Conversely, if ˆΦ(i) is nearly orthogonal to most high-reward ij are small, the weighted sum is far below 1, and ν(i) is close to 1. Thus ν(i) can responses, all S2 be read as the remaining component of the update direction for y(i) that is not captured by the dominant, high-reward directions in the group. Because we work with unit vectors in (9), this score is invariant to any common rescaling of the underlying features Φ(i). 4 (9) (10) Technical Report 2.4 Gradient-Guided Reward Shaping We convert the exploration score into multiplicative reward factor that preserves optimization stability. Let ν(i) denote bounded, monotone transformation of ν(i) (e.g., minmax normalization within the group). We define (cid:16) ν(i), r(i)(cid:17) = 1 + λ (cid:16) r(i)(cid:17) ν(i), λ [0, λmax], and the gradient-guided reward is r(i) = r(i) (cid:16) ν(i), r(i)(cid:17) . (12) (13) This formulation induces an asymmetric effect. For correct responses (r(i) = 1), high exploration score boosts the reward, prioritizing trajectories that follow successful yet geometrically distinct update directions over redundant repetitions. For incorrect responses (r(i) = 1), high exploration score amplifies the penalty (r(i) < 1), while low exploration score (high similarity to correct peers) mitigates the penalty (r(i) > 1). An incorrect response with low exploration score has its gradient feature Φ(i) aligned with the subspace of correct solutions, suggesting near-miss whose update direction is still informative; by penalizing these less, the policy is encouraged to stay within plausible reasoning manifold. Conversely, an incorrect response with high exploration score is nearly orthogonal to successful trajectories, indicating off-manifold or hallucinated behavior that should be suppressed. 2.5 Practical Reward Rescaling In all our experiments the base rewards are binary, r(i) {1, 1}, indicating incorrect vs. correct responses. The gradient-guided factor in (12) modifies these rewards but we keep the overall scale tightly controlled. Concretely, we instantiate the mapping (ν(i), r(i)) so that it is bounded and monotone in the normalized exploration score, and we clip the shaped reward in (13) to fixed interval: r(i) clip(cid:0)r(i); c, c(cid:1), = 3. Since r(i) {1, 1}, this guarantees that the effective reward magnitude seen by the policy always satisfies r(i) 3, so the gradient-guided term can at most moderately upor down-weight individual samples. This keeps the advantage scale stable while still allowing the exploration signal to reshape the relative weighting of candidates within each group. 2.6 Computation and Implementation Token features without extra backprop. Equation (6) depends only on quantities from the forward pass. Computing ϕt = Wrt can be implemented as ϕt = e(yt) pt = W(:,yt) Evpt (14) i.e., column gather and matrixvector product with pt. Aggregating ϕt over yields Φ(x, y) with negligible overhead relative to the softmax and log-prob computations already performed. In practice, G2RL is therefore drop-in modification of GRPO that replaces groupwise advantages by gradient-guided ones while leaving the rest of the RL pipeline unchanged. (cid:2) W(:,v) (cid:3),"
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate the proposed G2RL on two Qwen3 base models (1.7B and 4B). Our gradient-guided exploration term is applied as reward shaping within the groupwise standardization. We use the MATH training set yielding 7.5k training problems (Hendrycks et al., 2021). rule-based verifier provides both the RL reward signal during training and the correctness oracle at evaluation time. 5 Technical Report 3.1 Benchmarks and Metrics We report results on AIME24, AIME25, MATH500, AMC, and GPQA. For each prompt we report pass@1, maj@16 and pass@16. pass@k is the fraction of prompts for which at least one of the samples is verified correct. maj@16 is the majority-vote accuracy over 16 samples: we select the most frequent final answer string among the 16 and verify that single prediction (ties broken uniformly). All numbers are percentages; bold indicates the best within each block. 3.2 Main Results Table 1: Main results on MATH500, AMC, AIME24, AIME25 with Qwen3-1.7B-Base and Qwen3-4BBase backbone. MATH500 AMC AIME24 AIME25 Model pass@1 maj@16 pass@16 pass@1 maj@16 pass@16 pass@1 maj@16 pass@16 pass@1 maj@16 pass@16 Qwen3-1.7B-Base GRPO Entropy Bonus EVOL-RL G2RL 63.5 64.3 64.3 66.2 Qwen3-4B-Base GRPO Entropy Bonus EVOL-RL G2RL (Ours) 76.9 79.0 80.0 80.8 73.2 74.1 73.7 76. 81.6 87.2 87.7 87.8 86.6 86.7 86.9 88.7 90.8 93.2 93.5 93.6 31.2 32.2 32.2 33.9 47.9 50.5 50.9 52.3 42.0 42.3 43.9 44. 56.2 63.7 62.0 63.8 68.1 65.7 66.2 68.5 75.1 79.5 81.9 82.0 7.5 9.6 8.4 10.1 12.4 17.8 19.4 19.9 14.8 17.2 15.8 17. 18.2 25.4 28.2 28.7 24.4 23.3 27.3 28.0 31.1 40.0 42.4 43.8 4.6 4.6 5.3 7.5 10.0 16.1 17.5 20.1 6.8 6.9 7.9 11. 16.2 24.4 23.9 29.0 22.2 24.5 21.6 24.8 32.5 41.5 39.8 45.0 On 1.7B, the gradient-guided variant G2RL improves both single-try quality and multi-sample coverage across all datasets, indicating that exploration in the models own update space leads to more effective use of the sampling budget. The gains are largest on AIME25, where pass@1 rises to 7.5 and maj@16 to 11.4, indicating that G2RL encourages useful optimization-space exploration rather than mere noise. On MATH500, improvements are consistent through pass@16 (88.7 vs. 86.9 for the strongest baseline). The only case where baseline slightly edges out G2RL is AMC pass@16 (68.1 vs. 68.5), while G2RL leads on the remaining metrics for that dataset. The 4B results amplify these trends. On AIME25, pass@1 improves from 17.5 (best baseline) to 20.1 and maj@16 from 23.9 to 29.0. On MATH500, gains are smaller but consistent across all metrics, reaching pass@16 = 93.6. On AMC, G2RL maintains the best pass@1, maj@16, and pass@16, indicating stronger sample efficiency and coverage at scale. Across both model sizes, G2RL improves pass@1 on every dataset, showing that gradient-guided exploration shifts probability mass toward higher-quality solutions. Gains in pass@16 demonstrate better coverage of distinct correct modes, especially on challenging splits like AIME25. 3.3 Analysis on General Reasoning Tasks We assess generalization on two broad-coverage reasoning benchmarks using the 4B model. On GPQA, which is four-option multiple-choice, sampling benefits are pronounced: pass@k rises with k. Our method improves single-try quality and consensus (pass@1 = 38.7, maj@16 = 44.0) and achieves the best pass@16 = 89.2; at pass@32 the task is near-saturated and EVOL-RL is marginally higher (93.7 vs. 93.5). On the larger and more diverse MMLUpro, we report pass@1 only, our approach attains higher micro-average (58.47) than EVOL-RL (57.17), entropy bonus (57.14), and GRPO (56.15). These results indicate that the gradient-guided exploration signal improves generalization beyond math-style settings, raising both single-try accuracy and useful coverage without auxiliary encoders. 6 Technical Report Table 2: General reasoning results on the backbone of Qwen3-4B-Base. Left: GPQA (multiple-choice) with full sampling metrics. Right: MMLUpro micro-average pass@1. All numbers are percentages; best in bold. GPQA (4B) Method GRPO Entropy Bonus EVOL-RL G2RL (Ours) pass@1 maj@16 pass@16 pass@32 85.2 92.6 93.7 93.5 37.2 37.8 37.4 38.7 81.2 88.6 88.9 89.2 39.2 42.8 42.1 44.0 MMLUpro (4B, pass@1) Method GRPO Entropy Bonus EVOL-RL G2RL (Ours) Micro Avg. 56.15 57.14 57.17 58.47 3.4 Training dynamics Key Finding 1 G2RL achieves the fastest and highest gains in accuracy and response length while keeping entropy moderate; entropy bonuses mainly inflate entropy and token count, and EVOL-RL shows healthy but ultimately weaker improvements due to its externally defined exploration signal. Figure 2: Training dynamics on AIME25: mean@8 accuracy, average response length, and entropy for GRPO, Entropy Bonus, EVOL-RL, and G2RL. To understand how different exploration mechanisms influence learning, we compare training dynamics for three methods: Entropy Bonus, EVOL-RL, and G2RL. We track AIME25 mean@8 accuracy, average response length, and output entropy over training steps (Figure 2). Accuracy and response length. G2RL shows the steepest and most stable improvement in mean@8: its accuracy curve rises quickly and plateaus at the highest level among all methods. EVOL-RL improves more smoothly than entropy bonus, but still converges below G2RL. In parallel, G2RL drives rapid early increase in average response length, indicating that the model quickly learns to produce richer, more structured reasoning rather than merely recycling short patterns. For Entropy Bonus, length is more volatile and less predictive of accuracy. Entropy as noisy proxy for exploration. Entropy Bonus unsurprisingly produces the largest increase in output entropy, but this growth is only weakly coupled to accuracy: entropy continues to rise even when accuracy saturates, suggesting that many additional tokens are uninformative for solving the task. G2RL, in contrast, yields moderate but aligned entropy increase: entropy rises together with both accuracy and response length, reflecting exploration that contributes to useful reasoning. EVOL-RL behaves in between these extremes. 7 Technical Report Why G2RL outperforms EVOL-RL. Both EVOL-RL and G2RL maintain healthy curves where accuracy and length co-evolve without obvious instability. The key difference lies in how exploration signal is defined. EVOL-RL relies on an external encoder whose similarity geometry is only loosely tied to the current policy, and thus cannot perfectly adapt to its evolving representation. G2RL instead measures exploration pressure directly in the policys gradient feature space, keeping the exploration signal aligned with the actual update directions and yielding faster learning and higher final accuracy. 3.5 Analysis of Exploration Geometry Key Finding 2 G2RL alters the optimization landscape, increasing the ratio of opposing gradient directions (negative similarity) by nearly 5 compared to GRPO. Crucially, we observe distinct misalignment between semantic-space and gradient-space geometry: external embeddings fail to capture the structural orthogonality that drives efficient exploration. Figure 3: Exploration-geometry analysis on AIME25. Left: Distribution of pairwise gradient-space cosine similarity. G2RL significantly shifts the distribution toward zero. Middle: Distribution of semantic-space cosine similarity. G2RL maintains high semantic coherence despite much more diverse gradient geometry. Right: Ratio of negative similarity pairs. G2RL generates nearly 5 more orthogonal or opposing optimization directions than vanilla GRPO. To investigate the underlying mechanism of our method, we conducted controlled analysis of the exploration geometry induced by different training strategies in both the policys native gradient space and an external semantic space. We sampled 8 responses for each of 30 randomly selected problems from the AIME25 validation set using three models: vanilla GRPO, EVOL-RL, and G2RL. We measured pairwise similarity using two metrics: Gradient Geometry (based on the policys own update features Φ) and Semantic Similarity (based on an external embedding model). Gradient Geometry and Orthogonality. As shown in Figure 3 (Left), G2RL shifts the distribution of pairwise gradient similarities significantly toward zero. While vanilla GRPO responses exhibit high collinearity (mean cosine similarity 0.208), G2RL reduces this to 0.064, indicating much broader coverage of the optimization space. Most notably, we analyze the Negative Similarity Ratio (Figure 3, Right), which tracks response pairs pointing in opposing directions. Vanilla GRPO produces only 5.91% negative pairs. In contrast, G2RL boosts this to 28.09%. This confirms that our gradient-guided exploration mechanism successfully drives the policy toward structurally distinct reasoning paths that offer complementary gradient information, rather than merely rephrasing similar solutions. 8 Technical Report Misalignment in Semantic Space. The results in semantic space (Figure 3, Middle) reveal critical insight. Vanilla GRPO actually yields lower semantic similarity (0.738) than G2RL (0.769). naive interpretation might suggest GRPO is \"more diverse.\" However, given GRPOs inferior performance, this likely reflects incoherent or off-manifold exploration. G2RL maintains higher semantic consistency (staying on-topic and coherent) while simultaneously maximizing gradient orthogonality. This discrepancy shows that external semantic embeddings are an unreliable proxy for RL exploration: they may penalize subtle but update-relevant reasoning variations that are valuable in the policys own gradient space."
        },
        {
            "title": "4 Discussion: The Geometry of Efficient Exploration",
            "content": "Breaking Collinearity via Orthogonal Gradients. Standard GRPO treats all correct responses uniformly, assigning them identical positive advantages. Our analysis of gradient geometry reveals structural flaw in this approach: under vanilla GRPO, successful trajectories exhibit high gradient collinearity (mean similarity 0.21). This implies that nominally diverse samples are often redundant in optimization space, pushing parameters along the same dominant direction and accelerating mode collapse. G2RL fundamentally alters this dynamic. By explicitly guiding exploration in the policys own sensitivity space, it does not merely spread samples out in output space; it encourages them to be functionally orthogonal in update space. Our experiments show 5 increase in negative similarity pairs compared to the baseline, suggesting that G2RL actively selects trajectories that provide complementary gradient informationeffectively counterbalancing over-represented directions of the dominant mode and keeping the optimization landscape flat enough to permit continued, stable exploration. Semantic vs. Optimization Geometry. critical insight from our comparison with EVOL-RL is the misalignment between human-intelligible semantic variation and optimizer-relevant structural variation. Intuitively, one might expect lower semantic similarity to correlate with better exploration. However, our results show the opposite: G2RL maintains higher semantic consistency (0.77) than vanilla GRPO (0.74) while achieving drastically lower gradient similarity. This indicates that external embedding models are deceptive proxies for RL: they may reward surface-level changes (phrasing, irrelevant tangents) that contribute little to learning, or penalize subtle but high-value reasoning shifts that matter for optimization. G2RL succeeds because it operates directly in the policys intrinsic gradient geometry: it encourages variations that maximize the geometric difference in the parameter update step, regardless of whether those variations appear semantically distinct to an external encoder. In doing so, it effectively decouples useful exploration from random noise in output space. Credit Assignment within Correctness Classes. Finally, G2RL addresses subtle credit assignment ambiguity in sparse-reward settings. In binary-reward math tasks, the optimizer cannot distinguish between fragile, pattern-matched solution and robust, principled one if both happen to be correct. By modulating the reward with gradient-guided weights, G2RL implements dynamic re-weighting scheme: it down-weights the easy, repetitive paths whose gradient directions are already crowded, and amplifies the signal from rarer trajectories that open new update directions. This allows the model to continue extracting learning signal from correct answers long after standard policy gradient would have saturated on single mode, and it does so by leveraging the models own update geometry as the reference for what constitutes informative exploration."
        },
        {
            "title": "5 Related Work",
            "content": "Exploration and diversity in reinforcement learning. long line of work encourages exploration by maximizing entropy, which improves robustness and prevents premature convergence by discouraging early overcommitment to narrow modes (Haarnoja et al., 2018). Qualitydiversity (QD) methods such as MAP-Elites extend this idea by simultaneously maintaining performance and 9 Technical Report behavioral diversity, yielding repertoires of high-quality yet distinct solutions (Mouret & Clune, 2015; Pugh et al., 2016). In unsupervised RL, DIAYN maximizes mutual information between latent skills and states to acquire set of diverse policies without external rewards (Eysenbach et al., 2018). Taken together, these approaches demonstrate that structured exploration is not merely auxiliary but central to stable learning and broad generalization. However, they typically reason about exploration in terms of entropy or behavior space, rather than in terms of the geometry of parameter updates. Exploration-aware RL formulations for LLMs. Recent work on LLM reinforcement learning makes exploration explicit in the training objective. Song et al. (2025) diagnose diversity collapse under majority-style training and propose outcome-based exploration, which assigns bonuses to rare outcomes (historically or within-batch) to recover coverage without sacrificing accuracy. DARLING jointly optimizes learned diversity signal with task reward, showing gains on both non-verifiable and competition-math settings (Li et al., 2025). Label-free formulations also emphasize exploration to avert collapse: EVOL-RL couples majority-based selection for stability with novelty-aware rewards for exploration, embodying variationselection principle (Zhou et al., 2025), while RESTRAIN turns spurious majority signals into penalties for overconfident, low-consistency rollouts, enabling self-driven RL that maintains healthy variability without gold labels (Yu et al., 2025). Our work, G2RL, is aligned with this trajectory in that it modifies the RL objective to reshape exploration, but it differs in how the exploration signal is defined. Instead of relying on entropy or external embedding spaces to measure diversity, G2RL computes policy-referential similarity in the models own gradient feature space, avoiding auxiliary encoders and aligning the exploration signal directly with the policys update geometry."
        },
        {
            "title": "6 Conclusion",
            "content": "G2RL offers principled and practical framework for guiding exploration in large language models by anchoring it to the policys own update geometry rather than to entropy or external semantic encoders. By deriving sequence-level representation from last-layer gradient sensitivity and integrating it as bounded, reward-coupled weighting within GRPO, the method selectively amplifies correct trajectories that introduce new optimization directions and suppresses incoherent ones, breaking gradient collinearity without compromising training stability. Across math and general reasoning benchmarks and two Qwen3-base model scales, this simple modification yields consistent gains in pass@1, maj@16, and pass@k, produces healthier training dynamics, and increases the prevalence of genuinely orthogonal gradient pairs by five-foldwhile maintaining coherent, on-topic outputs. These results support broader perspective: efficient exploration in LLM reinforcement learning is achieved not by inflating entropy or superficial semantic dispersion, but by shaping the geometry of the optimization landscape through gradient-guided, policy-intrinsic signals."
        },
        {
            "title": "References",
            "content": "Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235256, 2002. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without reward function. arXiv preprint arXiv:1802.06070, 2018. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pp. 18611870. Pmlr, 2018. 10 Technical Report Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the nineteenth international conference on machine learning, pp. 267274, 2002. Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, and Tianlu Wang. Jointly reinforcing diversity and quality in language model generations. arXiv preprint arXiv:2509.02534, 2025. Jean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909, 2015. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Justin Pugh, Lisa Soros, and Kenneth Stanley. Quality diversity: new frontier for evolutionary computation. Frontiers in Robotics and AI, 3:40, 2016. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning. arXiv preprint arXiv:2509.06941, 2025. Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Zhaoning Yu, Will Su, Leitian Tao, Haozhu Wang, Aashu Singh, Hanchao Yu, Jianyu Wang, Hongyang Gao, Weizhe Yuan, Jason Weston, et al. Restrain: From spurious votes to signals self-driven rl with self-penalization. arXiv preprint arXiv:2510.02172, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Yujun Zhou, Zhenwen Liang, Haolin Liu, Wenhao Yu, Kishan Panaganti, Linfeng Song, Dian Yu, Xiangliang Zhang, Haitao Mi, and Dong Yu. Evolving language models without labels: Majority drives selection, novelty promotes variation. arXiv preprint arXiv:2509.15194, 2025. 11 Technical Report"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Theoretical Motivation: Diversity in the Optimization Landscape A.1.1 First-Order Sensitivity at the Last Layer For generated trajectory (x, y), the contribution of token to the log-likelihood gradient in last-layer space is ht ℓt = 1 W(cid:0)e(yt) pt (cid:1) =: 1 ϕt, (15) where pt = softmax(Wht/T). Collecting all token-level gradients yields the sequence-level sensitivity G(h1:L) := (cid:0)h ℓ, . . . , hL ℓ(cid:1) = 1 (ϕ1, . . . , ϕL). A.1.2 Layerwise Factorization of Backpropagation The network computation graph is During backpropagation, the parameters are fixed. Define the Jacobians hk+1 = fk+1(hk; θk), = 0, . . . , 1. (h) k+1 := hk+1 hk , (θ) k+1 := hk+1 θk . Applying the chain rule repeatedly gives the per-token upstream gradient θk ℓt = (cid:0)J (θ) k+1 (cid:1)(cid:16) (h) k+2 (cid:17) (cid:17) (cid:16) (h) hL ℓt. Equation (19) can be organized by defining the cumulative transition operator TkL := (h) k+2 (h) k+3 (h) (with TLL = I), so that θk ℓt = (cid:0)J (θ) k+1 (cid:1)T kLhL ℓt. Substituting (15) yields the explicit upstream factorization: θk ℓt = 1 (cid:0)J (cid:124) (θ) k+ (cid:1)T (cid:123)(cid:122) Lk(x,y) kL (cid:125) ϕt. Here Lk(x, y) := (cid:17) (cid:16) (θ) k+1 kL is trajectory-dependent linear operator determined entirely by the forward pass activations. Equation (22) exhibits complete structural decomposition: θk ℓt Im(cid:0)Lk(x, y)(cid:1) ϕt. (16) (17) (18) (19) (20) (21) (22) (23) (24) Two structural sources of diversity. From (24), diversity in parameter updates across responses can arise only from: (i) Variation in ϕt (ii) Variation in Lk(x, y) (differences in last-layer sensitivity to tokens), (differences in intermediate activations and Jacobians). (25) (26) 12 Technical Report Why shaping Φ is principled. For response y, define sequence-level feature such as Φ(x, y) = t=1 αtϕt, so that the full upstream gradient satisfies θk ℓ(x, y) = 1 Lk(x, y) Φ(x, y). Thus all optimization signals factor through the same structural pipeline: Φ(x, y) Lk(x, y) θk ℓ(x, y). (27) (28) Shaping the geometry of Φ therefore shapes the geometry of the entire family of upstream gradients in (28). In particular, promoting angular dispersion among Φ(x, y) encourages the resulting updates to explore broader subspace of the parameter space, without making assumptions on the Jacobians because Φ is the unique quantity through which all upstream gradients must factor linearly. A.2 Training Configuration. We conduct our experiments on two recent open-source base models: Qwen3-1.7B-Base and Qwen34B-Base. Our training process is implemented using the GRPO algorithm. To ensure that the model has sufficient capacity for complex, multi-step reasoning, we set the maximum response length to 8k and 12k tokens during generation for 1.7B and 4B models, respectively. To guide the models reasoning process, we utilize the system prompt from SimpleRL-Zoo (Zeng et al., 2025): Please reason step by step, and put your final answer within boxed{}. System Prompt Our training hyper-parameters are: Table 3: General hyper-parameters for RL training. Hyperparameter Value Train Batch Size PPO Mini-Batch Size PPO Micro-Batch Size Rollouts Generation Temperature Validation Temperature Learning Rate Use KL Loss KL Loss Coefficient 16 1 (effective size of 32) 2 16 1.0 0.8 5e-7 True 0."
        }
    ],
    "affiliations": [
        "Tencent AI Lab",
        "University of Notre Dame"
    ]
}