{
    "paper_title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening",
    "authors": [
        "Liuhan Chen",
        "Xiaodong Cun",
        "Xiaoyu Li",
        "Xianyi He",
        "Shenghai Yuan",
        "Jie Chen",
        "Ying Shan",
        "Li Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify a critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably can't make the end frame have a strong enough impact on the intermediate content like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose a novel framework, termed Sci-Fi, which applies a stronger injection for the constraint of a smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on a well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 0 2 1 2 . 5 0 5 2 : r Sci-Fi: Symmetric Constraint for Frame Inbetweening Liuhan Chen1 Xiaodong Cun2 Xiaoyu Li3 Xianyi He1,4 Shenghai Yuan1,4 Jie Chen1 Ying Shan3 Li Yuan1 1 Shenzhen Graduate School, Peking University 2 GVC Lab, Great Bay University 3 ARC Lab, Tencent PCG 4 Rabbitpre Intelligence https://github.com/GVCLab/Sci-Fi Figure 1: Some challenging examples of our Sci-Fi for frame inbetweening. Due to symmetric start-end-frame constraints, our Sci-Fi can produce harmonious inbetweening in complex scenarios, containing large and complicated motions of vehicles, people, animals, and cartoon characters."
        },
        {
            "title": "Abstract",
            "content": "Frame inbetweening aims to synthesize intermediate video sequences conditioned on the given start and end frames. Current state-of-the-art methods mainly extend large-scale pre-trained Image-to-Video Diffusion Models (I2V-DMs) by incorporating end-frame constraints via directly fine-tuning or omitting training. We identify critical limitation in their design: Their injections of the end-frame constraint usually utilize the same mechanism that originally imposed the start-frame (single image) constraint. However, since the original I2V-DMs are adequately trained for the start-frame condition in advance, naively introducing the end-frame constraint by the same mechanism with much less (even zero) specialized training probably cant make the end frame have strong enough impact on the intermediate content Corresponding authors Preprint. Under review. like the start frame. This asymmetric control strength of the two frames over the intermediate content likely leads to inconsistent motion or appearance collapse in generated frames. To efficiently achieve symmetric constraints of start and end frames, we propose novel framework, termed Sci-Fi, which applies stronger injection for the constraint of smaller training scale. Specifically, it deals with the start-frame constraint as before, while introducing the end-frame constraint by an improved mechanism. The new mechanism is based on well-designed lightweight module, named EF-Net, which encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. This makes the end-frame constraint as strong as the start-frame constraint, enabling our Sci-Fi to produce more harmonious transitions in various scenarios. Extensive experiments prove the superiority of our Sci-Fi compared with other baselines."
        },
        {
            "title": "Introduction",
            "content": "Humans can naturally imagine the intermediate motions from the given sparse frames, even with larger differences. This imagination is significant in film production [14], animation creation [511], and video content editing [1214]. Due to the ability to save expensive human labor, methods that predict the intermediate dynamic content automatically [1519] are widely researched in the computer vision area, especially in the emerging community of generative artificial intelligence. Traditional methods for frame inbetweening (interpolation) [1, 2028] mainly train neural network from scratch to estimate intermediate optical flow, so they might only work for simple rigid motions with narrow gaps of input frames. Since large pre-trained image [2935] and video [24, 3650] diffusion models [51, 52] have general generative priors, recent works utilize these pre-trained image-to-video diffusion models (I2V-DMs) for this task, which enables larger dynamics and various predictions of the intermediate content. For example, some work combines pre-trained I2V-DM [41] with temporally bidirectional sample strategy [5355], introducing the end-frame constraint by treating it as the start frame to control the temporally flipped video, with low (or even no) training consumption. Others directly use relatively higher computational consumption and data (still much lower than that for training base I2V-DMs [2, 3]) to finetune I2V-DMs [2, 56] by injecting the end-frame constraint as that of the start frame, without significant modification of network structure. In these methods, the mechanisms for injecting the end-frame constraint are the same as those for injecting the start-frame constraint in the corresponding I2V-DMs. However, the original large-scale I2V-pretraining is only for the start-frame constraint, making its training much more sufficient than that of the end-frame constraint. This causes the end-frame constraint to be inherently weaker than the start-frame constraint. As shown in Fig. 2 (a), due to weaker end-frame constraint, the motions of the intermediate frames generated by the method are closer to the I2V dynamics of the start frame instead of the expected ground truth. This leads to distorted predicted path with large gap from the real path, resulting in inconsistent motion and appearance collapse in generated content. To achieve symmetric start-end-frame constraints without large-scale specialized training, its necessary to differentiate the injection mechanisms for these two constraints based on training scales, i.e., the constraint with smaller training scale should have stronger injection mechanism. To simplify the framework design, we can maintain the processing of the start frame while focusing on developing an improved injection mechanism for the end-frame constraint. The new mechanism should enable the generation of the intermediate frames to be strongly influenced by the end frame, which is similar to controllable image [5769] and video [7088] generation at the task level: They all enforce the output to be constrained by additional conditions. Furthermore, the most efficient network design for this task is to turn the control signals into pixel-wise or frame-wise features, then explicitly inject them into the base models by directly adding or cross-attention [89]. Hence, following their paradigms to inject the end frame constraint may also bring success to our task. Inspired by the above finding, we present Sci-Fi, novel framework that efficiently achieves the symmetric start-end-frame constraints for frame inbetweening. Specifically, it processes the start frame as before while injecting the end frame by utilizing an improved mechanism. To obtain the improvement, we carefully design lightweight module, termed EF-Net. It efficiently encodes only the end frame and expands it into temporally adaptive frame-wise features explicitly injected into the I2V-DM. This enables the end frame to powerfully affect the intermediate content, making the Figure 2: Comparison between current I2V-DM-based methods and our Sci-Fi. (a) In current I2V-DM-based methods, the end-frame constraint is weaker than the start-frame constraint due to the same injection mechanism but smaller training scale. Consequently, inbetweening dynamics are heavily influenced by the start frames I2V prior. For instance, rider motions in generated frames deviate from ground truth, instead resembling base model I2V results, causing distorted predicted path with collapsed content. (b) Our Sci-Fi maintains start frame processing while enhancing endframe constraint injection. This achieves symmetric start-end-frame constraints with small training, yielding fine predicted path close to the real one with smoother inbetweening. end-frame constraint as strong as the start-frame constraint. As shown in Fig. 2 (b), given an input pair, owing to symmetric start-end-frame constraints, our Sci-Fi can generate intermediate frames with smooth dynamics. This brings fine predicted path with small gap from the real path and more harmonious transitions of the generated content. Furthermore, as shown in Fig. 1, our Sci-Fi can produce harmonious inbetweening containing complex dynamics in various scenarios. Our main contributions are: (1) We propose novel framework for frame inbetweening, termed Sci-Fi, that efficiently achieves symmetric start-end-frame constraints by handling the start frame as before and applying an improved injection mechanism for the end-frame constraint. (2) To develop the new mechanism, we propose lightweight module, termed EF-Net, which efficiently encodes only the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. (3) Extensive experiments prove the superiority of our method compared with other baselines."
        },
        {
            "title": "2 Related Work",
            "content": "Traditional Video Frame Inbetweening. Frame inbetweening is widely researched computer vision task that generates the intermediate frames based on the start and end frames. Most traditional interpolation methods first train network to estimate the optical flows between the start and end frames [1, 25, 90]. Then, they synthesize the middle frames by utilizing predicted optical flow to warp or splat [26, 27]. However, due to the limitations of optical flow in temporal modeling [28, 9193], they usually fail to produce acceptable results when facing large motion or complex scenarios. Besides, given an input pair, they cant provide multiple feasible results due to deterministic predictions. Diffusion Model for Generative Frame Inbetweening. Recently, diffusion models have achieved state-of-the-art performance in generating high-quality images [2935, 94] and videos [24, 36 50, 95103]. Several studies try to exploit the general generation priors of diffusion models [51, 52] for generative frame inbetweening [1519]. Currently, methods that utilize large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs) [2, 3, 41] achieve the best performance. They leverage I2V-DMs by proposing temporally bidirectional sample strategy [5355], or directly finetuning I2V-DMs without significant modification of network structure. In these methods, the mechanisms for injecting the constraints of the start and end frames are the same, but the training for the former is 3 much more extensive than that of the latter. This results in the end-frame constraint being inherently weaker than the start frame, causing degraded motion and appearance in the intermediate content. Controllable Image and Video Generation. To improve the controllability and stability of image and video generation, some methods [5788, 104106] incorporate additional control signals, such as depth [59, 68, 70], trajectory [12, 14], 3D point cloud [107110], match lines and human pose [13], and optical flow [111], to the base diffusion models. The most efficient way to impose this control is to turn the control signals into pixel-wise or frame-wise features, then explicitly inject them into the network [89]. For frame inbetweening, the frame-wise conditions are difficult to obtain. Thus, most of them are derived from interpolation [1214, 107] between the conditions from the start and end frames or estimation from traditional model [111]. These frame-wise conditions probably conflict with the real-world motion, leading to the degradation of the generated content."
        },
        {
            "title": "3 Method",
            "content": "In Sec. 3.1, we first review the background of I2V-DMs, which are also the basis of our method. Then, theoretical proof that the start and end frame constraints are asymmetric in current I2V-DM-based methods is provided in Sec. 3.2. After that, in Sec. 3.3 and Sec. 3.4, we introduce our proposed framework, named Sci-Fi, and the network design of its core module, termed EF-Net, respectively."
        },
        {
            "title": "3.1 Preliminaries: Image-to-Video Diffusion Model",
            "content": "Image-to-Video Diffusion Model (I2V-DM) is mainly finetuned from the Text-to-Video Diffusion Model (T2V-DM) to enable generating video conditioned on the start frame. When training, based on the pre-trained T2V-DM, given an -frame video RF 3HW , I2V-DM first uses variational autoencoder (VAE) [112117] to transform into the latent representation Rf chw. Then, random Gaussian noise ϵ is added to z, based on the noise schedule αt and σt at time t: zt = αtz + σtϵ, ϵ (0, I). (1) The obtained zt is used to train denoiser Dθ with parameter θ, which denoises based on time t, image condition ci (and optional other conditions co), by minimizing the following loss: = Ezt,t,ϵ,ci,co (cid:104) Dθ(zt; t, ci, co) yt(t, ϵ, zt)2 2 (cid:105) , (2) where yt(t, ϵ, zt) is the target for the output of the network at time t, determined by specific network prediction, such as original noise-prediction [51] or v-prediction [118]. For inference, we use the denoiser Dθ to recover the latent representation from pure Gaussian noise ZT (0, I) by iteratively denoising times. Specifically, at time t, Dθ will provide prediction ˆyt based on zt (obtained from the previous denoising step), t, ci, and co. Then, it produces cleaner noised latent zt1 with less noise, by specific mathematical calculation [3, 41], expressed as: After times denoising, we use the same VAE to decode z0 and obtain the generated video. zt1 = C(zt, ˆyt = Dθ(zt; t, ci, co), t). (3)"
        },
        {
            "title": "3.2 Asymmetric Constraint Strength in Current Frame Inbetweening",
            "content": "We first denote the injection mechanism for the single image ci in the original I2V-DMs as Mci. For current frame inbetweening methods based on I2V-DMs, we use Mcs and Mce to denote the constraint injection mechanisms for the start frame cs and end frame ce, respectively. These three constraints and their injection mechanisms satisfy: Mci = Mcs = Mce. We use li, ls, and le to denote their training scales. In practice, the original I2V-DMs are adequately pre-trained for the single image condition in advance, while the training steps for frame inbetweening are much fewer (or even zero). Therefore, the training scales satisfy: (4) For constraint c, we express its strength on the models output as λc, which is determined by the injection mechanism and the training scale l, expressed as λc = H(M, l). Intuitively, with the li ls = le. (5) 4 Figure 3: Our proposed framework Sci-Fi and its core module EF-Net (a) Our Sci-Fi efficiently achieves symmetric start-end-frame constraints by handling the start frame as before and applying an improved injection mechanism for the end-frame constraint. (b) This new mechanism is based on lightweight module, EF-Net, which efficiently encodes the end frame and expands it into temporally adaptive frame-wise features injected into the I2V-DM. same injection mechanism, is monotonically increasing function of the training scale. Hence, according to Eq. 4 and 5, we can derive that: λci = H(Mci, li) > λcs = H(Mcs, ls) = λce = H(Mce , le). (6) Given that the original I2V-DMs are only controlled by the start frame (cs = ci), the final constraint strength of the start frame after further finetuning, denoted as ˆλcs, satisfies: ˆλcs (λci + λcs ) > λce. (7) This indicates that in the current I2V-DM-based methods, the end-frame constraint is weaker than the start-frame constraint due to the same constraint injection mechanism but smaller training scale."
        },
        {
            "title": "3.3 Proposal of Sci-Fi",
            "content": "According to Sec. 3.2, to achieve symmetric start-end-frame constraints without increasing specialized training (maintaining Eq. 5 unchanged), an effective way is to break the equation of Eq. 4 by stronger injection mechanism for the end-frame constraint. Based on this, as shown in Fig. 3 (a), we propose our novel framework, termed Sci-Fi. We choose CogVideoX-5B-I2V [3, 56] as our base I2V-DM. The mechanism in this base model for injecting the single image (start-frame) constraint ci at time is only to temporally zero-pad ci, then concatenate it with the noised latent zt in the channel dimension. For our Sci-Fi, we first keep this processing for the start frame, denoted as: cs = ci. (8) Then, for the end-frame condition, besides the above processing, we utilize novel proposed lightweight module (detailed in Sec. 3.4), denoted as EF-Net, to additionally encode and inject this constraint into the base I2V model, expressed as: Mcs = Mci = {Concat([ci, 0], zt)}, Mce = {Concat([0, ce], zt); EF-Net(ce)}. (9) Furthermore, we can jointly express the injection mechanism of the start-end-frame constraints as: Mcs,ce = {Concat([cs, 0, ce], zt); EF-Net(ce)}. (10) Since the concatenation of the start frame cs and the end frame ce is on the same axis of the same network layer, we can view this as the same injection mechanism. Thus, it satisfies: Mcs Mce. (11) Intuitively, when the training scale is fixed, monotonically increases as the injection mechanism becomes more powerful. For our Sci-Fi, since the training scales cs and ce still satisfy Eq. 5, while additionally introducing EF-Net to process ce, the constraint strengths λcs and λce follow: λcs = H(Mcs, ls) < λce = H(Mce , le). (12) With proper design of the EF-Net (introduced in Sec. 3.4), we can further obtain greater λce that offsets the λci , brought from large-scale pre-training specialized for the start frame, expressed as: ˆλcs (λci + λcs ) λce λcs. (13) According to Eq. 13, in our Sci-Fi, the constraint strength of the end frame is on par with the start frame. This symmetry largely improves the quality of the generative intermediate frames."
        },
        {
            "title": "3.4 Network Design of EF-Net",
            "content": "To strongly yet efficiently incorporate the end-frame constraint into the I2V-DM, we propose our EFNet inspired by the successful network design paradigm in controllable image and video generation, such as ControlNet [59], T2I-Adapter [68], and ControlNeXt [69]. However, these previous modules for adding control to the base model can only handle pixel-wise or frame-wise conditions. In contrast, we turn single image (the end frame) into temporally adaptive frame-wise features before injecting them into the I2V-DM. The network structure of EF-Net is shown in Fig. 3 (b). Specifically, it takes only the end-frame ce as input and transforms it into serial features RLD (1 ) by series of transformer blocks Bj (1 ), expressed as: = Bj(F j1), (14) where 0 is obtained by patchifying the ce into tokens. The and are the number of tokens and the token dimension, respectively. To turn into frame-wise features, we first utilize linear projection Pj that predicts token-wise temporal coefficients. These coefficients can temporally expand by times (the frame number of zt) by an outer product operation, which can be expressed as: ˆF = Pj(F j) j, (15) where Pj(F j) RLf and ˆF R(Lf )D. To make ˆF more temporally adaptive, we should incorporate dynamic information of the whole video sequence into this end-frame feature. Actually, the temporal information already explicitly exists in the latent zt, which represents the video content, though interfered with noise. Hence, to incorporate zt, we patchify then concatenate it with ˆF j. This composite is sent into non-linear MLP to produce the final frame-wise feature j, expressed as: = MLP(Concat( ˆF j, Patchify(zt))). (16) After that, we directly add to the features output by the first blocks of the I2V-DM: Aj+1 = BI2V (Aj) + j, (17) where BI2V denotes the j-th block of the I2V-DM and Aj is its input. To ensure the lightness of EF-Net, we set = 4, which is much smaller than the number of blocks in I2V-DM (N = 42). Transformer blocks Bj (1 ) in our EF-Net can vary with different network structures. In this work, we utilize the DiT block in CogVideoX-5B-I2V for convenience. We consider this enough to impose strong end-frame constraint since the end-frame features have explicitly influenced the intermediate features of the whole video in the base I2V-DM by frame-wise condition injection."
        },
        {
            "title": "4 Experiments",
            "content": "The organization of this section is as follows: In Sec. 4.1, we introduce our experimental settings. Then, we compare our Sci-Fi with other state-of-the-art methods in Sec. 4.2. Besides, we show the generalization ability of our Sci-Fi to the cartoon frame inbetweening in Sec. 4.3. Finally, further ablation experiments for our method are presented in Sec. 4.4."
        },
        {
            "title": "4.1 Experimental Setting",
            "content": "Datasets. We obtain the training data by collecting the real-world videos from the publicly available creative material platform, iStock2. For testing, we follow most related works on generative frame inbetweening [13, 54, 55, 111]. Specifically, we curate 119 and 100 video clips for evaluation from the DAVIS dataset [119] and Pexels3, respectively. The video clips in the two test datasets cover various scenarios, including human action, animal motion, vehicle movement, and natural scenes. Implementation Details. We train our Sci-Fi model with small resource consumption. Specifically, the training process requires only 6k iterations, with total batch size of 4. The AdamW optimizer [120] is used to update the parameters of both EF-Net and the entire base model (CogVideoX5B-I2V) simultaneously. We employ cosine annealing learning rate with an initial value of 3e-5. The number of inference steps for Sci-Fi is 50, matching the officially recommended setting. 2https://www.istockphoto.com/ 3https://www.pexels.com/ 6 Table 1: Quantitative Comparison between our Sci-Fi and some advanced methods on DAVIS and Pexels datasets. The best and second-best scores for each metric are bolded and underlined."
        },
        {
            "title": "Pexels",
            "content": "LPIPS FID FVD VBench LPIPS FID FVD VBench FILM [1] EMA-VFI [27] DynamiCrafter [2] MoG [111] TRF [53] GI [54] ViBiDSampler [55] FCVG [13] CogVideoX-FT [56] 0.2313 0.3697 0.3146 0.2497 0.3217 0.2463 0.2724 0.2467 0.2349 37.47 74.02 43.11 32.76 45.56 31.64 33.78 29.24 26.46 739.07 882.44 586.21 534.82 549.77 555.60 480.85 531.00 449."
        },
        {
            "title": "Ours",
            "content": "0.2096 22.30 382.03 0.8162 0.7342 0.7461 0.7855 0.7810 0.7929 0.7832 0.8000 0.8104 0.8240 0.2254 0.3270 0.3188 0.2779 0.3439 0.2511 0.2931 0.2440 0. 36.37 63.08 43.96 40.64 46.48 32.51 36.94 30.04 27.96 734.43 804.39 522.26 656.42 607.12 650.59 528.19 607.25 526.42 0.2246 24.50 501.70 0.8261 0.7671 0.7646 0.8028 0.7863 0.8140 0.7867 0.8166 0. 0.8373 Figure 4: Visual Comparison between our Sci-Fi and some state-of-the-art methods. Our Sci-Fi greatly improves the visual quality of intermediate content in diverse scenarios. Evaluation Metrics. Following most related works [13, 5355, 111], we adopt LPIPS [121] and FID [122] to assess the individual frame quality. Additionally, we utilize FVD [123] and recently proposed benchmark, VBench [124], to measure the overall quality of the videos. Since VBench evaluates videos across multiple dimensions, following [54, 111], we use the officially provided weights to calculate the final weighted average scores. More details can be found in Appendix A.1."
        },
        {
            "title": "4.2 Comparison with Other Baselines",
            "content": "Baselines. We quantitatively compare our Sci-Fi with nine baselines for frame inbetweening. Specifically, FILM [1] and EMA-VFI [27] are widely used optical-flow-based methods. Dynamicrafter [2] is obtained by directly finetuning its base I2V-DM, and MoG [111] is its improved version by incorporating frame-wise optical-flow guidance. TRF [53], GI [54], and ViBiDSampler [55] are all based on the temporally bidirectional sample strategy. Besides utilizing this strategy, FCVG [13] further adds frame-wise conditions of matched lines and human poses into the base I2V-DM. CogVideoX-FT [56] is an open-source project that directly finetunes CogVideoX-5B-I2V [3] with 10 times more training 7 Figure 5: Results of user study. Each pie chart illustrates the proportion of videos output by each method that are selected by participants in specific evaluation dimension. Table 2: Inference Times of I2V-DM based methods. Our Sci-Fi utilizes the same base I2V-DM as CogVideoX-FT with enhanced end-frame injection, but only increases very little inference time. Video Size 16512 251024576 49720 480 Methods DynamiCrafter [2] MoG [111] TRF [53] GI [54] ViBiDSampler [55] FCVG [13] CogVideoX-FT [56] Ours Step Time (s) 50 21.09 50 23.95 50 192.68 50 616.52 25 102.36 25 115. 50 234.89 50 237.82 overhead than ours. Typical baselines, including FILM [1] (based on optical flow), GI [54] and FCVG [13] (based on temporally bidirectional sample strategy), as well as CogVideoX-FT [56] (based on directly fine-tuning), are selected for further qualitative comparison and user study. Quantitative Comparison. Since the intermediate content generated by some methods and our Sci-Fi is controlled by the constraints of the start and end frames, as well as the text prompts, for fair quantitative comparison, we utilize widely used vision language model, Qwen2.5-VL-7B [125], to automatically predict the text prompts only based on the start and end frames, detailed in the Appendix A.2. As shown in Tab. 1, our Sci-Fi achieves the highest scores across all metrics on the DAVIS and Pexels datasets. For example, on the two datasets, the VBench scores of our Sci-Fi are much higher than those of the CogVideoX-FT, which utilizes the same base I2V-DM as ours. Although these quantitative metrics do not fully align with human intuitive perception, to some degree, they still prove the significance of symmetric start-end-frame constraints for frame inbetweening, which are absent in other generation-based methods, while existing in our Sci-Fi. Qualitative Comparison. The superiority of our method is further reflected in Fig. 4. When start and end frames have large gap, other methods produce inharmonious intermediate content containing inconsistent motions or collapsed appearances. But our Sci-Fi can still provide smooth transitions. For example, as shown in the first two columns of Fig. 4, the intermediate frames generated by other methods all contain inappropriate movement trajectories of the humans or distorted content. On the contrary, our Sci-Fi achieves much better result with harmonious dynamics. User Study. In addition, we conduct user study to measure human preference for model outputs across three dimensions: motion quality, content fidelity, and overall attraction. Specifically, we utilize Sci-Fi and four other methods to produce corresponding results for 30 start-end-frame pairs. Then, for videos with the same start and end frames, participants separately selected the best ones based on the three dimensions, respectively. total of 32 participants took part in this experiment, providing 2,880 ratings. As illustrated in Fig. 5, for each evaluated dimension, the proportion of our Sci-Fi being selected exceeds three-quarters, demonstrating strong human preference for the outputs produced by our method. More details of the user study are provided in Appendix A.3. Inference Efficiency. Besides, we test the inference times of our Sci-Fi and other I2V-DM-based methods. The denoising steps for other methods follow their official implementations. As shown in Tab. 2, methods based on different I2V-DM will produce videos of different sizes. Although our Sci-Fi utilizes the same base I2V-DM as CogVideoX-FT, it only increases very little inference time, indicating the efficiency of our proposed method."
        },
        {
            "title": "4.3 Generalization to Cartoon Frame Inbetweening",
            "content": "Frame inbetweening is also significant technology for cartoon creation [511]. Although our Sci-Fi is trained on real-world data, it demonstrates strong generalization ability to cartoon frame 8 Figure 6: Visual results of our Sci-Fi for cartoon frame inbetweening. The generated high-quality cartoon videos demonstrate strong generalization ability of our method. Table 3: Results of ablation experiments on the DAVIS dataset. (a) Different module designs of EF-Net. (b) Scaling the frame-wise features of EF-Net. Variants LPIPS FID FVD VBench Strength LPIPS FID FVD VBench w/o EF-Net EF-Net (w/o zt) EF-Net (w Ej) Ours 0.2270 0.2188 0.2148 0.2096 25.18 22.81 24.89 22.30 422.35 400.37 375.89 382.03 0.8096 0.8168 0.8151 0. = 0.5 = 1.0 = 1.5 = 2.0 0.2167 0.2096 0.2147 0.2204 23.72 22.30 23.69 24.36 399.69 382.03 430.21 447.31 0.8206 0.8240 0.8243 0.8248 inbetweening. As shown in Fig. 6, our Sci-Fi can still obtain visually high-quality results even for challenging scenarios, such as complex actions of cartoon characters or animals, and large vehicle or camera movements. More quantitative and qualitative comparisons between our Sci-Fi and other state-of-the-art methods for cartoon frame inbetweening are provided in Appendix A.4."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Module Design of EF-Net. We train three other model variants with the same setting introduced in Sec. 4.1 and compare them with our method on the DAVIS dataset: (1) w/o EF-Net, i.e., we directly remove the entire EF-Net. (2) EF-Net (w/o zt), i.e., we remove the incorporation of the noised latent zt (Eq. 16 and 17) in our EF-Net. (3) EF-Net (w Ej), i.e., we consider adding learnable temporal position embedding to temporally vary frame-wise features after the outer product operation in Eq. 15. According to Tab. 3 (a), removing the entire EF-Net leads to the poorest results, indicating the significance of using the additional module for enhancing end-frame constraint injection. Compared with the second and third variants, except for slightly lower FVD than that of the third variant, our method obtains the best results on the other three metrics. This indicates the effectiveness of the noise latent incorporation and omitting the temporal position embedding in our EF-Nets module design. Scaling Frame-wise Features. For some modules used in controllable generation, such as ControlNet [59] and ControlNeXt [69], constraint strength can be manually adjusted by utilizing factor to scale the pixel-wise or frame-wise features before adding them to the base model. We also explore this scaling for the frame-wise features produced by our EF-Net. According to the results on the DAVIS dataset, shown in Tab. 3, naive scaling may degrade the performance of our Sci-Fi. Thus, maintaining = 1.0 aligned with the training setting is probably the first choice in most scenarios."
        },
        {
            "title": "5 Conclusion and Limitations",
            "content": "In this paper, we proposed novel framework for frame inbetweening, termed Sci-Fi, to achieve symmetric start-end-frame constraints with small training consumption by handling the start frame as before and applying an improved injection mechanism for the end-frame constraint. We developed this new mechanism by proposing lightweight module, named EF-Net, which can efficiently encode 9 only the end frame and expand it into temporally adaptive frame-wise features injected into the I2V-DM. Extensive experiments proved the superiority of our method compared to other baselines. Limitations Our methods performance is limited by the generation capability of its base model (CogVideoX-5B-I2V). As shown in Appendix B, maintaining consistent dynamics and appearances remains challenging in scenarios involving fast or large-scale human body movements and the motion of small objects. potential approach for improvement is to scale up the model at the cost of much more computation, exemplified by the recently proposed industry model Wan2.1-FLF2V-14B [4]. Comparisons between our method and Wan2.1-FLF2V-14B [4] are also presented in Appendix B."
        },
        {
            "title": "References",
            "content": "[1] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and Brian Curless. Film: Frame interpolation for large motion. In European Conference on Computer Vision, pages 250266. Springer, 2022. [2] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. [3] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2025. [4] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [5] Li Siyao, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris Metaxas, Chen Change Loy, and Ziwei Liu. Deep animation video interpolation in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 65876595, 2021. [6] Li Siyao, Tianpei Gu, Weiye Xiao, Henghui Ding, Ziwei Liu, and Chen Change Loy. Deep geometrized cartoon line inbetweening. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 72917300, 2023. [7] Shuhong Chen and Matthias Zwicker. Improving the perceptual quality of 2d animation interpolation. In European Conference on Computer Vision, pages 271287. Springer, 2022. [8] Jiaming Shen, Kun Hu, Wei Bao, Chang Wen Chen, and Zhiyong Wang. Bridging the gap: Sketch-aware interpolation network for high-quality animation sketch inbetweening. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1028710295, 2024. [9] Zhihang Zhong, Gurunandan Krishnan, Xiao Sun, Yu Qiao, Sizhuo Ma, and Jian Wang. Clearer frames, In European Conference on anytime: Resolving velocity ambiguity in video frame interpolation. Computer Vision, pages 346363. Springer, 2024. [10] Jinbo Xing, Hanyuan Liu, Menghan Xia, Yong Zhang, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Tooncrafter: Generative cartoon interpolation. ACM Transactions on Graphics (TOG), 43(6):111, 2024. [11] Tianyi Zhu, Wei Shang, and Dongwei Ren. Thin-plate spline-based interpolation for animation line inbetweening. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1099511003, 2025. [12] Wen Wang, Qiuyu Wang, Kecheng Zheng, Hao Ouyang, Zhekai Chen, Biao Gong, Hao Chen, Yujun Shen, and Chunhua Shen. Framer: Interactive frame interpolation. arXiv preprint arXiv:2410.18978, 2024. [13] Tianyi Zhu, Dongwei Ren, Qilong Wang, Xiaohe Wu, and Wangmeng Zuo. Generative inbetweening through frame-wise conditions-driven video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [14] Maham Tanveer, Yang Zhou, Simon Niklaus, Ali Mahdavi Amiri, Hao Zhang, Krishna Kumar Singh, and Nanxuan Zhao. Motionbridge: Dynamic video inbetweening with flexible controls. arXiv preprint arXiv:2412.13190, 2024. 10 [15] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion for prediction, generation, and interpolation. Advances in neural information processing systems, 35:23371 23385, 2022. [16] Liao Shen, Tianqi Liu, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, and Zhiguo Cao. Dreammover: In European Leveraging the prior of diffusion models for image interpolation with large motion. Conference on Computer Vision, pages 336353. Springer, 2024. [17] Zhilin Huang, Yijie Yu, Ling Yang, Chujun Qin, Bing Zheng, Xiawu Zheng, Zikun Zhou, Yaowei Wang, and Wenming Yang. Motion-aware latent diffusion models for video frame interpolation. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 10431052, 2024. [18] Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 14721480, 2024. [19] Zihao Zhang, Haoran Chen, Haoyu Zhao, Guansong Lu, Yanwei Fu, Hang Xu, and Zuxuan Wu. Eden: In Proceedings of the Enhanced diffusion for high-quality large-motion video frame interpolation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [20] Simon Niklaus, Long Mai, and Feng Liu. Video frame interpolation via adaptive separable convolution. In Proceedings of the IEEE international conference on computer vision, pages 261270, 2017. [21] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, and Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 90009008, 2018. [22] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depthaware video frame interpolation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37033712, 2019. [23] Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54375446, 2020. [24] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim. Xvfi: extreme video frame interpolation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1448914498, 2021. [25] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate flow estimation for video frame interpolation. In European Conference on Computer Vision, pages 624642. Springer, 2022. [26] Zhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. Amt: Allpairs multi-field transforms for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98019810, 2023. [27] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang. Extracting motion and appearance via inter-frame attention for efficient video frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 56825692, 2023. [28] Chunxu Liu, Guozhen Zhang, Rui Zhao, and Limin Wang. Sparse global matching for video frame interpolation with large motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1912519134, 2024. [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [30] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2266922679, 2023. [31] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [33] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [34] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. [35] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [36] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. [37] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [38] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36:75947611, 2023. [39] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [40] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. [41] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [42] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. [43] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64416451, 2024. [44] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73107320, 2024. [45] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66356645, 2024. [46] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [47] Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. [48] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [49] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [50] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [51] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [53] Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael Black, and Xuaner Zhang. Explorative inbetweening of time and space. In European Conference on Computer Vision, pages 378395. Springer, 2024. [54] Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steven Seitz. Generative inbetweening: Adapting image-to-video models for keyframe interpolation. In The Thirteenth International Conference on Learning Representations, 2025. [55] Serin Yang, Taesung Kwon, and Jong Chul Ye. Vibidsampler: Enhancing video interpolation using bidirectional diffusion sampler. In The Thirteenth International Conference on Learning Representations, 2025. [56] Zhengcong Fei. Cogvideox-interpolation. https://github.com/feizc/CogvideX-Interpolation, 2025. [57] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [58] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative and controllable image synthesis with composable conditions. In International Conference on Machine Learning, pages 1375313773. PMLR, 2023. [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [60] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. Advances in Neural Information Processing Systems, 36:4296142992, 2023. [61] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:1112711150, 2023. [62] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [63] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [64] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88398849, 2024. [65] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79327942, 2024. [66] Xiaoyu Liu, Yuxiang Wei, Ming Liu, Xianhui Lin, Peiran Ren, Xuansong Xie, and Wangmeng Zuo. Smartcontrol: Enhancing controlnet for handling rough visual conditions. In European Conference on Computer Vision, pages 117. Springer, 2024. 13 [67] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency feedback: Project page: liming-ai. github. io/controlnet_plus_plus. In European Conference on Computer Vision, pages 129147. Springer, 2024. [68] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 42964304, 2024. [69] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [70] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 73467356, 2023. [71] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video diffusion models with motion prior and reward feedback learning. arXiv preprint arXiv:2305.13840, 2023. [72] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023. [73] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. [74] Jiwen Yu, Xiaodong Cun, Chenyang Qi, Yong Zhang, Xintao Wang, Ying Shan, and Jian Zhang. Animatezero: Video diffusion models are zero-shot image animators. arXiv preprint arXiv:2312.03793, 2023. [75] David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. Moonshot: Towards controllable video generation and editing with multimodal conditions. arXiv preprint arXiv:2401.01827, 2024. [76] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [77] Tian Xia, Xuweiyi Chen, and Sihan Xu. Unictrl: Improving the spatiotemporal consistency of text-tovideo diffusion models via training-free unified attention control. Transactions on Machine Learning Research, 2024. [78] Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. Uniedit: unified tuning-free framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185, 2024. [79] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [80] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. [81] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. [82] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. [83] Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang. Easycontrol: Transfer controlnet to video diffusion for controllable generation and interpolation. arXiv preprint arXiv:2408.13005, 2024. [84] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for video diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. [85] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. In The Thirteenth International Conference on Learning Representations, 2025. [86] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. arXiv preprint arXiv:2503.07027, 2025. [87] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. Cameractrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. [88] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identity-preserving text-to-video generation by frequency decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [89] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [90] Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang. Ifrnet: Intermediate feature refine network for efficient frame interpolation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19691978, 2022. [91] Mengshun Hu, Kui Jiang, Zhihang Zhong, Zheng Wang, and Yinqiang Zheng. Iq-vfi: implicit quadratic In Proceedings of the IEEE/CVF Conference on motion estimation for video frame interpolation. Computer Vision and Pattern Recognition, pages 64106419, 2024. [92] Guangyang Wu, Xin Tao, Changlin Li, Wenyi Wang, Xiaohong Liu, and Qingqing Zheng. Perceptionoriented video frame interpolation via asymmetric blending. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27532762, 2024. [93] Yuhan Liu, Yongjian Deng, Hao Chen, and Zhen Yang. Video frame interpolation via direct synthesis with the event-based reference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84778487, 2024. [94] Junsong Chen, Simian Luo, and Enze Xie. Pixart-δ: Fast and controllable image generation with latent consistency models. In ICML 2024 Workshop on Theoretical Foundations of Foundation Models, 2024. [95] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations, 2023. [96] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1845618466, 2023. [97] Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional imageto-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1844418455, 2023. [98] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [99] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. [100] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. In International Conference on Machine Learning, pages 2510525124. PMLR, 2024. 15 [101] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. [102] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [103] Yuwei Guo, Ceyuan Yang, Ziyan Yang, Zhibei Ma, Zhijie Lin, Zhenheng Yang, Dahua Lin, and Lu Jiang. Long context tuning for video generation. arXiv preprint arXiv:2503.10589, 2025. [104] Liuhan Chen, Yirou Wang, and Yongyong Chen. End-to-end xy separation for single image blind deblurring. In Proceedings of the 31st ACM International Conference on Multimedia, pages 12731282, 2023. [105] Shaodong Wang, Yunyang Ge, Liuhan Chen, Haiyang Zhou, Qian Wang, Xinhua Cheng, and Li Yuan. Prompt2poster: Automatically artistic chinese poster creation from prompt only. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1071610724, 2024. [106] Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, Ser-Nam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024. [107] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [108] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [109] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. In The Thirteenth International Conference on Learning Representations, 2025. [110] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model. arXiv preprint arXiv:2408.16767, 2024. [111] Guozhen Zhang, Yuhan Zhu, Yutao Cui, Xiaotong Zhao, Kai Ma, and Limin Wang. Motion-aware generative frame interpolation. arXiv preprint arXiv:2501.03699, 2025. [112] Diederik Kingma and Max Welling. Auto-encoding variational {Bayes}. In Int. Conf. on Learning Representations, 2014. [113] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [114] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. In 12th International Conference on Learning Representations, ICLR 2024, 2024. [115] Sijie Zhao, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Muyao Niu, Xiaoyu Li, Wenbo Hu, and Ying Shan. Cv-vae: compatible video vae for latent generative video models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [116] Liuhan Chen, Zongjian Li, Bin Lin, Bin Zhu, Qian Wang, Shenghai Yuan, Xing Zhou, Xinhua Cheng, and Li Yuan. Od-vae: An omni-dimensional video compressor for improving latent video diffusion model. arXiv preprint arXiv:2409.01199, 2024. [117] Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. Wf-vae: Enhancing video vae by wavelet-driven energy flow for latent video diffusion model. arXiv preprint arXiv:2411.17459, 2024. [118] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. [119] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 16 [120] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [121] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [122] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [123] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. ICLR Workshop, 2019. [124] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [125] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025."
        },
        {
            "title": "A More Details and Results of Experiments",
            "content": "In this section, we first provide the calculation details of the evaluation metrics in Appendix A.1. Then, in Appendix A.2, we introduce how to utilize the vision language model to automatically predict the text description based only on the start and end frames, when models require text prompt input besides the constraints of the two frames. In addition, we also give more details about our user study in Appendix A.3. After that, in Appendix A.4, we present the quantitative and qualitative comparison of cartoon frame inbetweening between our Sci-Fi and other state-of-the-art methods. Besides, in the supplementary materials, all video examples in the main text and the appendix are integrated into video demo file named video_demos.mp4. A.1 Calculation Details of Evaluation Metrics Table 4: VBench dimensional scores of our Sci-Fi and other baselines on the DAVIS dataset. Temporal Average Flickering Background Aesthetic Quality Consistency Consistency Motion Smoothness Imaging Quality Methods Subject Score FILM [1] EMA-VFI [27] DynamiCrafter [2] MoG [111] TRF [53] GI [54] ViBiDSampler [55] FCVG [13] CogVideoX-FT [56] Ours 0.9301 0.8496 0.8681 0.9147 0.9078 0.9334 0.9126 0.9269 0.9280 0.9388 0.9294 0.8892 0.9100 0.9332 0.9181 0.9240 0.9141 0.9262 0.9387 0.9450 0.5147 0.4160 0.5084 0.5116 0.4938 0.5157 0.5101 0.5139 0.5327 0. 0.6732 0.3918 0.6752 0.6508 0.6137 0.6560 0.6404 0.6405 0.6692 0.6757 0.9896 0.9906 0.9439 0.9722 0.9784 0.9807 0.9750 0.9829 0.9834 0.9897 0.9679 0.9802 0.9083 0.9359 0.9490 0.9347 0.9422 0.9558 0.9539 0. 0.8162 0.7342 0.7461 0.7855 0.7810 0.7929 0.7832 0.8000 0.8104 0.8240 Table 5: VBench dimensional scores of our Sci-Fi and other baselines on the Pexels dataset. Temporal Average Flickering Background Aesthetic Quality Consistency Consistency Motion Smoothness Imaging Quality Methods Subject Score FILM [1] EMA-VFI [27] DynamiCrafter [2] MoG [111] TRF [53] GI [54] ViBiDSampler [55] FCVG [13] CogVideoX-FT [56] Ours 0.9356 0.8791 0.8853 0.9218 0.9122 0.9423 0.9092 0.9285 0.9295 0.9368 0.9380 0.9155 0.9209 0.9436 0.9239 0.9383 0.9203 0.9347 0.9419 0.9494 0.5291 0.4626 0.5274 0.5299 0.5121 0.5386 0.5233 0.5366 0.5492 0. 0.6855 0.4593 0.6736 0.6739 0.6306 0.6845 0.6499 0.6682 0.6856 0.6982 0.9906 0.9921 0.9518 0.9742 0.9749 0.9821 0.9727 0.9856 0.9851 0.9919 0.9720 0.9831 0.9201 0.9484 0.9473 0.9499 0.9428 0.9656 0.9618 0. 0.8261 0.7671 0.7646 0.8028 0.7863 0.8140 0.7867 0.8166 0.8214 0.8373 In practice, the frame numbers of the videos generated by different methods vary. Therefore, to fairly conduct quantitative comparisons between our Sci-Fi and other baselines, we follow most related works [13, 54, 55], which curate ground truth video clips containing 25 frames. For LPIPS, when the frame number of videos generated by method differs from the ground truth (25 frames), we first uniformly extract 25 frames from those generated videos before calculating. For FID and FVD, following [88, 101], before calculating, we uniformly extract 16 frames from both the ground truth and generated video clips. For VBench, following [111], we evaluate the generated videos across six dimensions: subject consistency, background consistency, aesthetic quality, imaging quality, motion smoothness, and temporal flickering. In Tab. 4 and Tab. 5, we provide the detailed dimensional scores of our Sci-Fi and other baselines on the DAVIS and Pexels datasets, respectively, which are the basis of the VBench scores in Tab. 1. After calculating the dimensional scores, we obtain final weighted average score by using the officially provided weights [124], expressed as: sf = 6 (cid:88) j= wj sj smax smax smin . 18 (18) Table 6: Quantitative comparison about different types of text prompts."
        },
        {
            "title": "Prompt Types",
            "content": "LPIPS D-P U-P VLM-P VLM-R-P 0.2266 0.2168 0.2096 0.2064 FID 27.26 23.63 22.30 22.95 FVD 567.54 477.87 382.03 380.12 VBench 0.8129 0.8169 0.8240 0.8233 Figure 7: Visual Comparison between different types of text prompts. Utilizing VLM to predict or refine text prompts can enhance the performance of our Sci-Fi. Here, sf is the final score, sj(1 6) are the dimensional scores, wj is the weight on the j-th dimension, and smax are also the officially provided dimensional normalization factors. , smin A.2 Utilizing the Vision Language Model for Frame Inbetweening Since the intermediate content generated by some methods and our Sci-Fi is controlled by the constraints of the start and end frames, as well as the text prompts, for fair quantitative comparison, we utilize widely used vision language model (VLM), Qwen2.5-VL-7B [125], to automatically predict the text prompts only based on the start and end frames. Specifically, we send each start-endframe pair and text instruction, This is the first and last frame of video clip. Describe this video in one continuous paragraph., to Qwen2.5-VL-7B. It will produce detailed text prompt for this frame pair. We conduct an ablation study to explore the effect of the text prompt on the output video quality. Besides the above VLM-predicted prompt, denoted as VLM-P, we consider three other types of text prompts: (1) Default prompt, denoted as D-P, i.e., we use the prompt, \"This is clear, smooth and high-quality video.\", for every input pair. (2) User-provided short prompt, denoted as U-P, i.e., we mimic the habits of actual users who provide brief text description for an input frame pair, such as \"A man is wearing grayish-white helmet and riding motorcycle on the road.\". (3) LLM-refined prompt, denoted as VLM-R-P, i.e., we send each start-end-frame pair and the corresponding U-P, as well as text instruction, This is the first and last frame of video clip. U-P is short description of it. Please describe this video in one continuous paragraph in detail., to Qwen2.5-VL-7B. We then utilize the models output as the final text prompt. The quantitative results on the DAVIS dataset are provided in Tab. 6. They demonstrate that utilizing the vision language model to predict long and detailed prompts will improve the performance of our methods. Using VLM-P and VLM-R-P achieves much better scores on all metrics compared to U-P and D-P. The visual comparison in Fig. 7 further proves the superiority of leveraging VLM. Without detailed prompt as condition, some objects in the video vanish, leading to appearance collapse. We believe that utilizing VLM to predict or refine text prompts can bring performance gains, since prompts handled by VLM are more aligned with the training used text prompts than the default prompt or user-provided short prompts. 19 Table 7: The detailed vote counts of the five methods."
        },
        {
            "title": "Methods",
            "content": "FILM [1] GI [54] FCVG [13] CogVideoX-FT [56] Ours Total"
        },
        {
            "title": "Motion Quality\nContent Fidelity\nOverall Attraction",
            "content": "15 9 9 81 72 67 9 11 14 118 94 88 737 774 782 960 960 Table 8: Quantitative Comparison between our Sci-Fi and some advanced methods on cartoon frame inbetweening. The best and second-best scores for each metric are bolded and underlined."
        },
        {
            "title": "Methods",
            "content": "LPIPS FILM [1] EMA-VFI [27] DynamiCrafter [2] ToonCrafter [10] MoG [111] TRF [53] GI [54] ViBiDSampler [55] FCVG [13] CogVideoX-FT [56]"
        },
        {
            "title": "Ours",
            "content": "0.1962 0.2503 0.3397 0.3223 0.3225 0.3308 0.2284 0.2643 0.2524 0.2350 0.1959 FID 44.90 80.10 64.81 60.44 58.87 61.33 45.67 49.78 48.05 40.72 37.39 FVD 691.57 784.98 527.41 495.23 491.72 571.17 542.42 560.53 562.63 466.50 439.63 VBench 0.8390 0.7731 0.7906 0.8115 0.8089 0.7920 0.8308 0.8030 0.8260 0.8230 0.8403 A.3 Details of User Study For our user study, the full text of the instruction given to participants is: \"Hello! Thank you for participating in this survey. We will present 30 groups of video clips. For each group, the start and end frames are identical (pre-determined images), while the intermediate content is generated by five different AI frame inbetweening models, respectively. (AI frame inbetweening refers to the process where model generates intermediate transition content based on given initial and final frames.). Your task is to evaluate each group of videos based on the following three dimensions: (1) Motion quality, i.e., whether the movement of objects and scenes is natural and smooth, without abrupt motion or sudden content jumps. (2) Content fidelity, i.e., whether the shape and form of objects and scenes match their real-world appearance, without unreasonable deformations or chaotic distortions. (3) Overall attraction, i.e., comprehensive visual quality and personal preference. For each dimension, please select one video that you think performs best. Your feedback is extremely important to us!\". The detailed vote counts of the five methods are shown in Tab. 7. A.4 Comparison on Cartoon Frame Inbetweening For quantitative comparison, we collect 100 cartoon video clips from the internet, including Japanese, American, and Chinese animations. According to the results shown in Tab. 8, our Sci-Fi achieves the best score across all metrics. For example, our Sci-Fis FID and FVD are lower than those of the second-best method, CogVideoX-FT, which utilizes the same base I2V-DM as ours. The superiority of our method for cartoon frame inbetweening is further reflected in Fig. 8. For example, as shown in the middle two columns of Fig. 8, the dynamics and appearances of the ship in the intermediate frames generated by other methods are all chaotic. On the contrary, our Sci-Fi achieves much better result, containing harmonious intermediate transition with consistent dynamics and appearances. Failed Cases and Comparison with Wan2.1-FLF2V-14B Our methods performance is limited by the generation capability of its base model (CogVideoX5B-I2V). It struggles to maintain consistent dynamics and appearances in scenarios involving fast or large-scale human body movements and the motion of small objects. As shown in Fig. 9, the body of the person is inconsistent, and the ball suddenly disappears. potential approach for improvement is to scale up the model, exemplified by the recently proposed industry model Wan2.1-FLF2V-14B [4], Figure 8: Visual comparison between our Sci-Fi and some advanced methods on cartoon frame inbetweening. Our Sci-Fi achieves better visual quality when generalized to cartoon data. Figure 9: Visual comparison of Wan2.1-FLF2V-14B [4], CogVideoX-FT [56], and our Sci-Fi. Wan2.1-FLF2V-14B improves the quality of generated videos at the cost of much more computation. which was developed concurrently with our work. According to the visual comparison in Fig. 9, the larger model, Wan2.1-FLF2V-14B [4], can bring better visual results and reduce the distortion in intermediate content. However, as shown in Tab. 9, scaling up the model will lead to much greater inference consumption. For frame inbetweening, designing efficient methods that work well in various scenarios remains challenging, and is worth researching in the community. 21 Table 9: Inference Times of Wan2.1-FLF2V-14B [4], CogVideoX-FT [56] and our Sci-Fi. Due to larger model size, Wan2.1-FLF2V-14B has much greater inference consumption. Video Size"
        },
        {
            "title": "Methods",
            "content": "Time (s)"
        },
        {
            "title": "Step",
            "content": "Wan2.1-FLF2V-14B [4] CogVideoX-FT [56] Ours 81 1280 720 49 720 480 49 720 480 50 50 50 3829.85 234.89 237.82 14B 5B 5B"
        },
        {
            "title": "C Instruction of Code and Data",
            "content": "In the supplementary material, we zip the training, inference, and evaluation codes to file named code.zip. Besides, we also provide some examples of our training and testing data, placing them into folder named data_examples. Complete datasets will be released after the acceptance of the work."
        },
        {
            "title": "D Societal impacts",
            "content": "Our proposed Sci-Fi can efficiently obtain high-quality results for frame inbetweening. This novel technology may bring about potential societal implications. On the one hand, providing more visually harmonious intermediate content benefits fields such as film production and animation creation. On the other hand, it also raises ethical and safety concerns. The ease of generating high-quality intermediate content could lead to surge in the production of harmful or misleading content, such as deepfakes, potentially exacerbating misinformation and privacy invasion issues. We condemn the misuse of generative AI that harms individuals or spreads misinformation."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "GVC Lab, Great Bay University",
        "Rabbitpre Intelligence",
        "Shenzhen Graduate School, Peking University"
    ]
}