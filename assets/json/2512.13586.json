{
    "paper_title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
    "authors": [
        "Jia-Nan Li",
        "Jian Guan",
        "Wei Wu",
        "Chongxuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 8 5 3 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "REFUSION: DIFFUSION LARGE LANGUAGE MODEL WITH PARALLEL AUTOREGRESSIVE DECODING Jia-Nan Li1, 2 Jian Guan2 Wei Wu2 Chongxuan Li1 1Gaoling School of Artificial Intelligence, Renmin University of China {lijianan, chongxuanli}@ruc.edu.cn {jianguanthu, wuwei19850318}@gmail.com 2Ant Group Code: https://github.com/ML-GSAI/ReFusion Model: https://huggingface.co/GSAI-ML/ReFusion"
        },
        {
            "title": "ABSTRACT",
            "content": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce REFUSION, novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to higher slot level, where each slot is fixed-length, contiguous sub-sequence. This is achieved through an iterative plan-and-infill decoding process: diffusion-based planning step first identifies set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with unified causal framework and reduces the learning complexity from the token combination space to manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that REFUSION not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18 speedup on average, but also bridges the performance gap to strong ARMs while maintaining 2.33 average speedup."
        },
        {
            "title": "INTRODUCTION",
            "content": "While autoregressive models (ARMs) (Grattafiori et al., 2024; Yang et al., 2025; Jaech et al., 2024) have achieved impressive progress across wide range of tasks (Chen et al., 2021; Wei et al., 2022; Lightman et al., 2023; Li et al., 2024), their inference throughput is fundamentally limited by sequential, left-to-right decoding process that precludes parallelization (Chen et al., 2023; Cai et al., 2024; Zhang et al., 2025). In contrast, masked diffusion models (MDMs) (Nie et al., 2025; Ye et al., 2025) operate via an iterative denoising process with no fixed generation order. This flexibility yields two significant advantages. First, it permits parallel decoding by assuming conditional independence among target tokens: their joint probability, given the context, is assumed to be the product of their individual marginal probabilities (Li et al., 2023). Second, it offers the potential for the model to discover better generation orders than the rigid left-to-right trajectory (Kim et al., 2025). Despite these theoretical advantages, existing MDMs often suffer from two issues: (1) Architectural bottlenecks negate efficiency gains from parallelism. The flexibility of generation orders requires bidirectional attention in MDMs (Vaswani et al., 2017; Devlin et al., 2019), an architectural choice fundamentally incompatible with Key-Value (KV) caching used in ARMs (Radford et al., 2018). That is, each decoding iteration forces full re-computation of the KV states of the entire context, introducing significant latency and making MDMs significantly slower than ARMs (Feng et al., 2025). (2) Intractable training complexity hinders coherent parallel generation. MDMs typically decode multiple tokens with high marginal probabilities in parallel (Nie et al., 2025). However, the Equal contribution. Corresponding authors: Wei Wu and Chongxuan Li."
        },
        {
            "title": "Preprint",
            "content": "conditional independence assumption frequently fails for these tokens, particularly for nearby tokens, leading to severe incoherence (Huang et al., 2022; Luxembourg et al., 2025; Gwak et al., 2025). For example, in context where both at once and right now are valid, an MDM might decode spurious output right once by independently sampling tokens with high individual marginal probabilities but low joint probability. We attribute this failure to an immense learning challenge: modeling data distribution over an exponential space of possible token combinations is far more demanding than the fixed sequential dependency of ARMs. Consequently, current MDMs often remain undertrained for reliably identifying conditionally independent tokens. To address these challenges, we introduce REFUSION, masked diffusion large language model that elevates the parallel decoding process from the traditional token level to higher slot level, progressively denoising slots in parallel from fully masked sequence. Specifically, we partition the initially masked sequence into fixed-length, consecutive sub-sequences, i.e., slots, and unfold each decoding iteration into two synergistic steps: first, diffusion-based planning step identifies set of weakly dependent slots, and second, an autoregressive infilling step decodes these slots in parallel (Figure 2). This design is guided by key finding from our pilot study (4.1): inter-token dependency is highly localized, decaying significantly with distance. Therefore, serializing adjacent tokens within slot directly mitigates the violation of conditional independence for these strongly-coupled tokens. Furthermore, such simple design also uniquely achieves two critical benefits simultaneously: (1) By repositioning newly generated slots (Sahoo et al., 2025) to precede masked ones for the next decoding iteration, REFUSION naturally accommodates causal attention that allows the KV states of all previously generated tokens to be seamlessly reused; And (2) it reduces the learning complexity from an intractable token combination space to substantially more manageable slot permutation space. Appendix A.1 compares REFUSION and existing MDMs in detail. REFUSIONs training process mirrors its inference dynamics. For each training sequence, we randomly mask several slots, permute the clean slots, and reorder the input so that clean slots precede masked ones. The model is then optimized with hybrid objective that cultivates its dual capabilities: an autoregressive loss on the permuted clean slots for sequential generation, and denoising loss on the masked slots for context-aware parallel reconstruction. Unlike traditional MDMs which learn only from masked positions, this hybrid objective uses every token for supervision, boosting data efficiency. Our extensive experiments on seven benchmarks spanning math, code generation, and general-purpose understanding and reasoning demonstrate that REFUSION decisively establishes new state-of-the-art for MDMs. Compared to LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025), REFUSION achieves an average performance gain of 34% while being over 18 faster in throughput (tokens/sec). More strikingly, REFUSION consistently challenges and often surpasses strong ARMs. For instance, it outperforms Qwen3-8B (Yang et al., 2025) on GSM8K (Cobbe et al., 2021) and MBPP (Austin et al., 2021) by 3.68 absolute points while being 2.33 faster on average. This dual advantage in both performance and speed is further illustrated in Figure 1, where REFUSION (the red point) pushes the performance-efficiency boundary significantly towards the top-right quadrant. It significantly outperforms both the Qwen3-series (the blue line) and prior MDM-based methods, which are situated Figure 1: Performance-throughput trade-off on MBPP. We plot pass@1 (%) against throughput (tokens/sec), with both metrics calculated relative to the Qwen3-8B baseline at the origin. The REFUSION left-to-right ablation forces serial decoding using the REFUSION model."
        },
        {
            "title": "Preprint",
            "content": "in the bottom-left or bottom-right regions, indicating that they lag behind in throughput, performance, or both dimensions. Furthermore, our controlled experiments confirm that these gains are driven by our architectural and training innovations, rather than initialization or data advantages. Our contributions are summarized as follows: I. We propose REFUSION, generative model integrating inter-slot parallel decoding with intra-slot autoregressive decoding, combining the strengths of autoregREssive and difFUSION-based modeling. II. To the best of our knowledge, REFUSION is the first MDM that achieves full KV cache reuse of every decoded token, while maintaining global generation flexibility and tractable training complexity. III. Extensive experiments on seven diverse benchmarks show that REFUSION not only overwhelmingly surpasses all prior MDMs in both performance and speed, but also bridges the performance gap to ARMs while maintaining the efficiency advantage."
        },
        {
            "title": "2 RELATED WORK",
            "content": "MDMs promise to outperform traditional ARMs by offering faster inference through parallel decoding and potentially superior solutions via flexible generation orders (Kim et al., 2025). Recent MDMs such as LLaDA (Nie et al., 2025), the first open-source MDM trained from scratch, and Dream (Ye et al., 2025), initialized from an ARM, have delivered performance on par with ARMs of equivalent scale across diverse tasks, establishing MDMs as viable research direction. Architectural Designs for Efficient MDMs. Standard MDMs reliance on bidirectional attention precludes the use of KV caching. Recent work alleviates this bottleneck through three main strategies. The first strategy approximates KV cache reuse while retaining bidirectional attention. For instance, dLLM-Cache (Liu et al., 2025) reuses slow-changing KV states, while sparse-dLLM (Song et al., 2025) dynamically prunes non-critical KV states. The second strategy mixes bidirectional attention and causal attention. Models like BD3-LMs (Arriola et al., 2025) and Fast-dLLM (Wu et al., 2025a) partition the sequence into consecutive blocks, enforcing left-to-right order between blocks to enable KV cache reuse, while retaining parallel, bidirectional generation within each block. D2F (Wang et al., 2025) further parallelizes the generation of succeeding blocks, although performance is limited by the lack of inter-block lookahead attention. While sharing the concept of grouped unit, REFUSIONs slot operates fundamentally differently from block in these approaches. In the block-based design, fixed left-to-right inter-block schedule sacrifices any-order flexibility, while intra-block bidirectional attention sacrifices KV caching and risks incoherence. In contrast, REFUSION enables both global any-order generation and full KV cache reuse with unified causal framework. The final strategy leverages only causal attention, enabling an exact KV cache. Eso-LMs (Sahoo et al., 2025), for instance, dynamically reposition newly generated tokens ahead of masked ones at each step to facilitate caching. However, this strategy introduces an intractable learning objective at token-level permutation space, which hinders training and leads to significant performance drops. Decoding Strategies in MDMs. crucial aspect of MDM inference is the strategy used to select which tokens to decode in parallel at each step. Existing approaches generally fall into two categories. The first class leverages confidence heuristics derived from the models own distribution, such as top token probability (Nie et al., 2025), low entropy (Ben-Hamu et al., 2025), and probability margins between top candidates (Kim et al., 2025). Some methods further refine these heuristics with position-aware weights and frequency-based calibration (Huang et al., 2025). While simple, these methods rely on the often-unreliable assumption that the models confidence scores are perfectly calibrated (Wu et al., 2025a). The second class employs external models for verification, e.g., using small ARM to validate and extend the longest acceptable prefix (Hu et al., 2025; Israel et al., 2025), or using dedicated reward models to guide generation (Gwak et al., 2025). Although effective, these approaches introduce the overhead of maintaining and querying separate model. Unlike these methods, REFUSION adopts unified inference framework that benefits from the parallel efficiency of MDMs without sacrificing the quality assurance of ARMs, all within single architecture."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of REFUSION. Left (Inference): An iterative slot-level plan-and-infill loop. diffusion step plans and drafts slots, followed by parallel autoregressive verify-and-predict infilling. Reordering generated slots before masked ones enables full KV cache reuse, while position IDs correspond to their ground-truth indices, invariant to reordering. Right (Training): Mirrors inference, optimizing hybrid objective of autoregressive loss (LARM) on permuted clean slots and denoising loss (LMDM) on masked slots."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Autoregressive Models. ARMs are prominent class of generative models that factorize the joint probability of sequence = (x1, . . . , xL) by enforcing strict left-to-right conditional dependency using causal attention mask. This structure leads to next-token prediction objective, where the model parameters θ are optimized by minimizing the negative log-likelihood: (cid:80)L i=2 log Pθ(xi x<i). During inference, generation is an inherently sequential process requiring forward passes to produce sequence of length , resulting in latency that scales with the sequence length. Masked Diffusion Models. MDMs represent another class of generative models, operate on mask-and-denoise principle. During training, each sample x0 = (x1 0 ) is corrupted to xt by masking each token with special token [MASK] under probability (0, 1). The model learns to reconstruct the original context by minimizing the objective: 1 = (cid:1). MDM inference proceeds by progressively generating tokens from fully [MASK]) log Pθ masked sequence. It requires fewer forward passes than an ARM thanks to parallel decoding, but each pass is drastically more expensive due to its incompatibility with KV caching. 0, , xL i=1 1(xi 0 xt 0, (cid:0)xi (cid:80)L"
        },
        {
            "title": "4 METHODOLOGY",
            "content": "Traditional MDMs allow flexible token-level decoding process during inference. We elevate this concept to operate on slots, i.e., fixed-length, non-overlapping sequence of continuous tokens, denoising them in parallel. Consequently, our approach yields two critical benefits: it enables full KV cache reuse by arranging newly generated slots before masked ones with causal framework, and it substantially reduces training complexity from the token-level combination space to more manageable slot-level permutation space. To support non-sequential generation, we build REFUSION upon standard causal architecture with key modification: it accepts an explicit, non-contiguous list of position IDs. By applying RoPE (Su et al., 2021) to these absolute position IDs, the model can correctly compute relative distances and attend to all logical predecessors. Figure 2 illustrates the inference and training process. 4.1 LOCALITY OF INTER-TOKEN DEPENDENCY cornerstone of REFUSION is grouping contiguous tokens into slots for serial generation. This design is motivated by the critical insight that the conditional independence assumption is most prone to failure for nearby tokens, frequently leading to semantic incoherence (Luxembourg et al., 2025)."
        },
        {
            "title": "Preprint",
            "content": "To formalize this insight and guide our design, we conduct pilot study to quantitatively investigate how dependency strength between two tokens correlates with their relative distance. 0 and xj 0 influences the models prediction of xi Formally, we define the dependency strength between two tokens, xi 0, in given context xt, as the degree to which the presence of xj 0. In practice, we approximate this measurement in pilot study on the GSM8K test set (Cobbe et al., 2021). For corrupted sequence xt, we first reveal the ground-truth token xj 0 at randomly selected masked position j, and then quantify the influence of this reveal on the prediction at any other masked position through the Jensen-Shannon (JS) divergence (Manning & Schutze, 1999) between the distributions before and after this reveal, i.e., p(xi 0). higher divergence implies stronger dependency, with zero divergence indicating conditional independence. Using both LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025), we plot the averaged JS divergence against the relative distance between positions and in Figure 3. The average JS divergence consistently decays as the relative distance increases, and this decay is more rapid in denser contexts (i.e., lower masking ratios t). This result directly motivates us to define slot as contiguous token sequence, thereby grouping strongly correlated tokens for serial decoding within slot, in contrast to prior block-based methods that decode nearby tokens within block in parallel (Arriola et al., 2025). 0xt) and p(xi 0xt, xj 4.2 SYNERGISTIC DECODING ALGORITHM AT INFERENCE Armed with the quantitative evidence that inter-token dependency is highly localized, we design the inference algorithm to explicitly leverage this property. The process iteratively generates final response r0 from prompt p0, starting with an initial masked sequence r1. This sequence is partitioned into consecutive slots of [MASK] tokens each. Each iteration comprises two synergistic steps: (1) diffusionbased slot planning to identify slots that are strongly dependent on the context but weakly interdependent; and (2) autoregressive slot infilling to decode them in parallel. , in generation order) with the masked slots ( Smasked Step I: Diffusion-based Slot Planning. The first Figure 3: The locality of inter-token depenstep leverages the models MDM capability to plan dency in MDMs, with the sign on the x-axis the next decoding slots. At timestep t, which is dedenoting the direction from the revealed token fined as the ratio of remaining masked slots, we construct the input St by concatenating already-decoded (positive for rightward, negative for leftward). slots ( Sclean , in their original positional order). This ordering naturally enables KV cache reuse. The model then computes certainty score for each masked slot based on its predictive distribution. While various heuristics exist for certainty score, we adopt simple yet effective one: the probability of the most likely token at the slots first position. Finally, batch of slots with scores exceeding threshold τslot is selected for subsequent infilling. This strategy identifies slots that are strongly constrained by the existing context and weakly interdependent (e.g., distinct function definitions in code generation), making them suitable to parallelize. Furthermore, to accelerate the subsequent autoregressive generation, we adopt strategy from speculative decoding (Leviathan et al., 2023): for each selected slot, we generate corresponding draft slot by sampling token from its distribution at every position, yielding the draft slots Sdraft . Step II: Autoregressive Slot Infilling. The second step efficiently verifies the draft slots Sdraft and completes them using the models autoregressive capability, while employing speculative decoding strategy powered by the models MDM capability to accelerate the process. (1) Global Verification. We first concatenate all draft slots into single sequence in their original positional order. The model performs single forward pass to compute the conditional probability for each token, conditioned on the prompt and the already-decoded slots. We identify the longest prefix of this concatenated sequence where all token probabilities exceed τtoken. If this prefix spans one or more complete slots, we accept these slots wholesale and immediately proceed to the next planning iteration, thereby bypassing costly suffix completion. (2) Parallel Iterative Completion. If global"
        },
        {
            "title": "Preprint",
            "content": "verification fails to yield complete slots, we revert to processing the draft slots independently via parallel iterative completion process. In each iteration, we execute two steps: (i) Verification: we independently identify the longest valid prefix for each slot where conditional probabilities exceed τtoken; (ii) Prediction: we retain this prefix, re-mask the remaining suffix, and predict the masked tokens conditioned on the context and the prefix using the models MDM capability. This cycle repeats until all slots are fully completed. Finally, the newly completed slots are appended to the sequence of decoded slots. Their KV caches are directly concatenated for future iterations, valid approximation as the lack of inter-slot conditioning during parallel generation has minimal impact on final performance (see 5.5). The plan-and-infill cycle continues until all slots are filled, at which point the final response is constructed by restoring the original slot order. The decoding process is formalized in Appendix A.2."
        },
        {
            "title": "4.3 TRAINING OF REFUSION",
            "content": "The training procedure for REFUSION is carefully designed to mirror the dynamics of our two-step decoding algorithm. This requires data construction strategy that simulates the non-sequential, partially-decoded states encountered during generation, and hybrid training objective that jointly optimizes the models planning and infilling capabilities. Training Data Construction. To simulate the partially decoded states encountered during iterative generation, we introduce three-step strategy to construct training data from each prompt-response 0 , . . . , SK pair (p0, r0). The response r0 is first partitioned into sequence of slots, S0 = [S1 0 ], each of size k. Then, corrupted version St is constructed given masking ratio (0, 1) as follows: (1) Random slot masking. Analogous to token-level masking in traditional MDMs, we randomly select and mask tK slots from the original sequence S0. Each selected slot is replaced with block of [MASK] tokens. (2) Permutation of clean slots. Since the generation order of slots is dynamically determined, the model must learn to process context in any arbitrary permutation. To achieve this, we randomly permute the unmasked (clean) slots to form Sclean , while keeping the original relative positions of the masked slots to form Smasked . (3) Reorder. The final training instance St is assembled by concatenating the permuted clean slots followed by the masked slots. Hybrid Training Objective. To empower our model with the dual capabilities of global planning and local decoding, we propose hybrid training objective that learns from every token in the sequence. This approach also provides significant benefit of data efficiency, which contrasts with traditional MDMs where clean tokens only serve as context and provide no direct supervision. On one hand, the clean slots Sclean are trained with standard ARM loss for next token prediction: LARM = E(p0,r0)D tU (0,1) 1 (k 1) Sclean Sclean (cid:88) (cid:88) i=1 j= (cid:16) log Pθ vi,j p0, Sclean t,<(i,j) (cid:17) , where vi,j is the j-th token in the i-th clean slot, Sclean t,<(i,j) is the prefix of the token in Sclean . On the other hand, the masked slots Smasked are trained with an MDM objective for denoising: LMDM = E(p0,r0)D tU (0,1) 1 Smasked Smasked (cid:88) (cid:88) i=1 j=1 log Pθ(vi,j 0 p0, Sclean , Smasked t,(i,j)) , where vi,j 0 i-th slot of Smasked is the ground-truth token from the original response corresponding to the j-th token in the . The final training objective is summation of the two losses, balanced by λ: = LARM + λLMDM. (3) We initialize our model Pθ with an off-the-shelf ARM backbone, strategy validated by prior work (Gong et al., 2025; Ye et al., 2025). Crucially, all tokens retain their original positional indices from r0 throughout the process. This allows the model to maintain awareness of the relative positions among all tokens, ensuring sequence coherence despite the shuffled input order. 6 (1) (2)"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Zero-shot performance and throughput (TPS) comparison on multiple benchmarks. Each model displays accuracy/pass@1 (top row) and throughput (TPS, bottom row). Within the MDM category, we highlight the best performance results in bold and underline the second best. An italic score in the ARM category signifies that it surpasses the best-performing MDM. Model MMLU-Pro ARC-C GSM8K MATH GPQA HumanEval MBPP Avg. Llama-3-8B-Instruct Qwen3-8B LLaDA-8B-Instruct LLaDA w/ Fast-dLLM LLaDA w/ D2F Dream-7B-Instruct Dream w/ Fast-dLLM Dream w/ D2F REFUSION Autoregressive Model 82.76 44.12 90.36 42.78 75.13 42.81 81.96 31.20 25.48 19.73 83.28 30.11 Masked Diffusion Model 85.58 0.03 82.85 0.86 84.13 3. 88.31 0.06 86.86 1.42 85.92 59.08 89.76 32.46 76.35 27.35 76.27 73.07 39.04 82.59 76.42 20.30 75.82 61.49 41.62 79.20 84.91 81.24 38.78 23.93 38.58 52.23 23.68 59. 46.60 18.99 36.76 58.24 29.98 84.14 54.22 81.77 29.46 42.00 39.06 30.43 32.37 1.99 28.35 17.54 31.25 23.84 30.36 1.81 31.25 22.96 30.13 49.96 35.49 64. 35.23 32.07 67.25 31.42 35.80 18.21 35.02 39.81 22.84 44.54 40.05 15.98 40.36 47.18 23.82 56.43 45.94 52.74 46.34 42.26 87.80 30.95 45.12 12.42 37.80 62.52 36.59 96. 56.71 3.51 56.10 49.73 50.00 69.15 78.66 103.90 53.00 41.68 63.80 30.07 25.60 2.97 24.80 37.19 35.20 53.85 50.40 1.23 10.60 19.55 51.60 65.59 68.20 92. 49.63 37.81 73.36 32.42 48.51 12.41 46.24 40.46 38.96 52.13 55.55 8.84 48.25 37.22 44.72 66.22 65.31 72."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUP Implementation Details. We initialize REFUSION from the Qwen3-8B checkpoint (Yang et al., 2025) and fine-tune it for 4 epochs on diverse 3.7M-sample dataset (1.22B-tokens) covering mathematics, coding, and general instruction-following. For inference, we employ semi-autoregressive remasking strategy (Nie et al., 2025): the output sequence is partitioned into blocks of size b, which are decoded serially. Within each block, our plan-and-infill algorithm from 4.2 is applied. Detailed implementation and hyperparameter specifics are provided in Appendix B.1 and B.2, respectively. Evaluation Benchmarks and Metrics. To comprehensively evaluate REFUSION, we test its performance on diverse benchmarks spanning: (1) General-purpose understanding and reasoning: MMLU-Pro (Wang et al., 2024) and ARC-C (Clark et al., 2018); (2) Mathematical and scientific problem-solving: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), and GPQA (Rein et al., 2024); (3) Code generation: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). We use pass@1 for code generation and accuracy for the others. We further assess inference throughput using tokens decoded per second (TPS) with single A100 GPU and batch size of 1. Baselines. We evaluate REFUSION against three categories of baselines1: (1) ARMs: Llama-3-8BInstruct (AI@Meta, 2024) and Qwen3-8B (Yang et al., 2025). (2) MDMs: LLaDA-8B-Instruct (Nie et al., 2025), and Dream-7B-Instruct (Ye et al., 2025). (3) State-of-the-art MDM acceleration methods: Fast-dLLM (Wu et al., 2025b) and D2F (Wang et al., 2025). We implement the baseline methods based on their official hyperparameters2. 1We omit BD3-LMs due to its significantly smaller scale (0.2B parameters). However, its decoding strategy is shared by other baselines like LLaDA and Fast-dLLM, whose performance serves as proxy. Nevertheless, we provide comparison with retrained BD3-LMs under unified settings in 5.3. 2Appendix C.1 provides comparisons against broader set of models."
        },
        {
            "title": "5.2 MAIN RESULTS",
            "content": "The main results in Table 1 highlight two key findings: (1) REFUSION dominates all MDM baselines. REFUSION consistently outperforms all MDM baselines in both performance and throughput (TPS) across all seven benchmarks, often by substantial margin. For instance, on HumanEval, it achieves 78.66% pass@1, surpassing the next-best MDM (Dream-7B-Instruct) by nearly 22 absolute points. While acceleration methods like Fast-dLLM and D2F improve throughput at significant performance cost, they still fall short of REFUSIONs efficiency. Notably, on MBPP, REFUSION reaches 92.09 TPS, which is 1.4 faster than the next-fastest MDM (Dream w/ D2F). REFUSION thus delivers both state-of-the-art performance and superior efficiency, establishing new frontier for MDMs3. (2) REFUSION challenges strong ARMs. More remarkably, REFUSION challenges and often surpasses strong ARMs. It delivers an average speedup of 2.33 over Qwen3-8B across all tasks while exhibiting superior performance on several benchmarks. For instance, on GSM8K and MBPP, it outperforms Qwen3-8B by 3.68 absolute points. This demonstrates that our non-autoregressive approach can break the long-standing trade-off between the speed of MDMs and the quality of ARMs (Feng et al., 2025). 5.3 CONTROLLED COMPARISON To isolate the benefits of REFUSION from data or backbone advantages, we adopted controlled approach: retraining reproducible baselines using unified settings, while retraining REFUSION to match the configurations of non-open-source baselines. Table 2: Controlled comparison of models initialized from Qwen3-8B and trained on 120K subset. Model MMLU-Pro ARC-C GSM8K MATH GPQA HumanEval MBPP Avg. Qwen3-8B (Retrained) LLaDA (Retrained) BD3-LMs (Retrained) REFUSION (Retrained) 52.71 31.32 5.12 0.26 26.11 16.82 42.14 40.56 88.65 30.46 70.65 0.03 78.24 2.19 84.81 29.01 87.57 31.03 13.42 1.18 83.55 16.35 80.74 53. 67.84 30.73 1.38 0.20 47.32 16.90 51.78 77.42 33.26 26.70 0.00 0.15 29.02 2.82 31.70 46.65 53.66 30.73 4.27 0.66 59.15 15.59 70.12 58.40 59.20 30.16 6.20 0.42 46.80 14.09 58.20 67.35 63.27 30.16 14.43 0.41 52.88 12.11 59.93 53.34 Reproducible Baselines. We conduct controlled comparison using 120K data subset randomly sampled from the full 3.7M dataset due to resource constraints. Specifically, we fine-tune Qwen3-8B, LLaDA, BD3-LMs4, and REFUSION for 10 epochs using their respective original objectives, with all models initialized from Qwen3-8B. This setup ensures that observed differences are attributable solely to the algorithm design. Appendix C.3 discusses the scaling properties of REFUSION regarding data size. Results in Table 2 confirm the architectural superiority of REFUSION. LLaDA suffers catastrophic performance collapse, and BD3-LMs lags behind REFUSION in both performance and speed. We note that the already highly-optimized Qwen3-8B baseline understandably degrades when retrained on our smaller, open-source dataset. However, under this controlled setting (with data advantages eliminated), REFUSION still outperforms it by 16 points on HumanEval while being 1.9 faster. This result robustly validates that REFUSIONs architectural innovations are the primary driver of its success, enabling effective learning even from limited data where standard MDMs fail. Non-Open-Sourced Baseline. Furthermore, we conduct controlled comparison with Dream7B-Instruct. Since its training code and data processing details are not open-sourced, we cannot retrain it on Qwen3-8B. Instead, we train REFUSION variant on Dreams original Qwen2.5-7B backbone. It is crucial to note the significant disparity in training resources: Dream benefits from massive pre-training (580B tokens, 146.5M samples) followed by SFT (1.8M samples), whereas our REFUSION variant is exclusively fine-tuned (3.7M samples) without any pre-training. Despite this disadvantage, Table 3 shows that REFUSION still achieves 2.23% average performance gain and massive 11.05 speedup over Dream. REFUSION significantly excels on reasoning and coding tasks (GSM8K, HumanEval, MBPP). Its lower performance on knowledge-intensive tasks 3Appendix C.2 further analyzes the trade-off frontiers of different models. 4The block size is set to 8, which is the minimum block size used in the REFUSION experiments."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Impact of key hyperparameters on MBPP (0-shot). The plots show the change in pass@1 (%) and throughput (TPS) of REFUSION relative to Qwen3-8B (horizontal dashed lines at zero). When one parameter is varied, others are held at their default values (τslot = 0.9, τtoken = 0.3, = 32). Yellow shaded regions highlight the sweet spot where REFUSION surpasses the baseline in both metrics. (MMLU-Pro, ARC-C) is expected, as it skips the pre-training stage that Dream utilizes for knowledge injection. These results collectively confirm that REFUSIONs architectural advantages are robust across different base models and training setups. Table 3: Controlled comparison with Dream-7B-Instruct on its native Qwen2.5-7B backbone. Model MMLU-Pro ARC-C GSM8K MATH GPQA HumanEval MBPP Avg. Dream-7B-Instruct REFUSION (Retrained) 40.05 15.98 35.25 76.02 88.31 0.06 83.11 53.38 76.42 20.30 80.21 107.83 46.60 18.99 46.36 139.55 30.36 1.81 29.02 102.29 56.71 3.51 68.90 106. 50.40 1.23 61.60 98.04 55.55 8.84 57.78 97.71 5.4 ANALYSIS OF HYPERPARAMETERS We examine the key hyperparameters governing the performance-efficiency trade-off in REFUSION: the slot selection threshold τslot, the token acceptance threshold τtoken, and the slot size k. The threshold τslot controls the confidence for slot selection (planning), and τtoken governs the confidence for draft acceptance (infilling), while defines the granularity of the generation unit. An analysis of other hyperparameters is shown in Appendix C.4. As illustrated in Figure 4, these hyperparameters create predictable trade-off. (1) Slot selection threshold τslot: Increasing τslot improves performance due to higher token reliability. However, throughput (TPS) exhibits non-monotonic trend. Although higher threshold reduces slot parallelism, processing fewer slots mitigates synchronization overhead during the Parallel Iterative Completion phase, which can potentially boost throughput. (2) Token acceptance threshold τtoken: Increasing τtoken reduces the number of accepted draft tokens per step, thereby lowering throughput. Performance also follows non-monotonic trend. While higher thresholds enforce stricter verification, excessive strictness causes frequent truncation and regeneration. This can trap the model in local optima and degrade final performance. (3) Slot size k: larger slot size enhances local coherence and allows more draft tokens to be accepted wholesale during verification. This leads to simultaneous gains in both performance and speed. Collectively, these analyses reveal robust and wide sweet spot, highlighted by the yellow shaded regions in Figure 4, where REFUSION consistently surpasses the Qwen3-8B baseline in both performance and throughput (TPS). This superior operating zone corresponds to slot selection threshold τslot [0.5, 1.0], token acceptance threshold τtoken [0.1, 0.9], and slot size {8, 32}. 5.5 ABLATION ON KV CACHE REUSE To maximize efficiency, REFUSION directly concatenates the KV caches of parallel-generated slots, bypassing costly forward pass that would otherwise be needed to contextualize them. To quantify the impact of this approximation, we conduct an ablation study comparing our default model against variant, REFUSION w/ KV Re-computation, which performs this extra forward pass to ensure full contextualization at the cost of speed. As shown in Table 4, our default approach is consistently 1.161.33 faster across all benchmarks. Surprisingly, this significant speedup comes at virtually no cost to performance; in fact, accuracy"
        },
        {
            "title": "Preprint",
            "content": "remains stable and even slightly improves on several benchmarks. We hypothesize this counterintuitive benefit arises from form of implicit regularization: by avoiding over-conditioning on potentially flawed parallel drafts, our method mitigates error propagation. This result validates our KV cache reuse strategy not merely as speed-accuracy trade-off, but as design choice that simultaneously enhances efficiency and robustness. Table 4: Ablation regarding our KV cache reuse mechanism. We compare our default REFUSION, which efficiently reuses KV caches by concatenating them after parallel generation, against variant (w/ KV Re-computation) that recomputes caches for full contextualization at higher cost. Model MMLU-Pro ARC-C GSM8K MATH GPQA HumanEval MBPP REFUSION w/ KV Re-computation REFUSION"
        },
        {
            "title": "5.6 CASE STUDY",
            "content": "45.56 42.80 45.94 52.74 89.76 28.03 89.76 32.46 84.38 69.42 84.91 81.24 54.18 69.20 54.22 81.77 35.49 48.51 35.49 64.11 77.44 78.00 78.66 103. 68.20 74.45 68.20 92.09 Figure 5 provides qualitative understanding of how REFUSION solves programming problem from the MBPP benchmark, highlighting two key capabilities: (1) High degree of parallelism. The model frequently generates multiple slots concurrently. For instance, at iteration 8, it simultaneously generates four separate slots, demonstrating its ability to exploit parallel decoding opportunities. Simultaneously, within each slot, the model leverages speculative decoding to accept four tokens in single pass, significantly accelerating the generation process. (2) Non-linear generation order. The generation process is markedly non-linear. For example, the model constructs the central for loop structure (iteration 5) before initializing local variable sum = 1 (iteration 6). This ability to plan and execute in parallel, non-monotonic fashion allows REFUSION to construct complex, structured code in manner that is both efficient and conceptually closer to human problem-solving. Appendix D.1 shows the results of baseline models on the same problem. Furthermore, to facilitate understanding, we provide detailed visualization of the decoding process in Appendix D.2. Figure 5: case study of REFUSION generating Python function for an MBPP problem (k = 4, τslot = 0.6, τtoken = 0.3, = 16). The number in the top-left corner of each slot indicates the generation order, while the color intensity within each slot represents the generation time (darker indicates earlier)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present REFUSION, novel generative model that synergizes the strengths of diffusion-based planning and autoregressive infilling to address the long-standing efficiency and coherence challenges in traditional MDMs. This unique design enables full KV cache reuse within flexible, any-order generation framework, while making the training objective tractable by simplifying the combinatorial complexity of the generation space. Extensive evaluations across seven benchmarks show that REFUSION establishes new state of the art for MDMs. More strikingly, it bridges the performance gap to strong ARMs, often outperforming them while being significantly faster. Our work demonstrates that by structuring the parallel generation process, it is possible to achieve the throughput potential of MDMs without sacrificing generation quality. Future directions include further scaling of the model and data size, as well as leveraging reinforcement learning to optimize the models planning policy for complex, multi-step reasoning tasks."
        },
        {
            "title": "REFERENCES",
            "content": "AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martın Blazquez, Guilherme Penedo, Lewis Tunstall, Andres Marafioti, Hynek Kydlıˇcek, Agustın Piqueres Lajarın, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clementine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https://arxiv.org/abs/2502.02737. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://arxiv.org/abs/2503.09573. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, 2019. Guhao Feng, Yihan Geng, Jian Guan, Wei Wu, Liwei Wang, and Di He. Theoretical benefit and limitation of diffusion language model. arXiv preprint arXiv:2502.09622, 2025. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. Scaling diffusion language models via adaptation from autoregressive models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=j1tSLYKwg8. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024."
        },
        {
            "title": "Preprint",
            "content": "Daehoon Gwak, Minseo Jung, Junwoo Park, Minho Park, ChaeHun Park, Junha Hyung, and Jaegul Choo. Reward-weighted sampling: Enhancing non-autoregressive characteristics in masked diffusion llms. arXiv preprint arXiv:2509.00707, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed Abdelfattah, Jae-sun Seo, Zhiru Zhang, and Udit Gupta. Accelerating diffusion language model inference via efficient kv caching and guided diffusion. arXiv preprint arXiv:2505.21467, 2025. Fei Huang, Tianhua Tao, Hao Zhou, Lei Li, and Minlie Huang. On the learning of non-autoregressive transformers. In International conference on machine learning, pp. 93569376. PMLR, 2022. Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, and Tong Xiao. Pc-sampler: Position-aware calibration of decoding bias in masked diffusion models. arXiv preprint arXiv:2508.13021, 2025. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models. 2024. URL https://arxiv.org/pdf/2411.04905. Daniel Israel, Guy Van den Broeck, and Aditya Grover. Accelerating diffusion llms via adaptive parallel decoding. arXiv preprint arXiv:2506.00413, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Jaeyeon Kim, Kulin Shah, Vasilis Kontonis, Sham Kakade, and Sitan Chen. Train for the worst, plan for the best: Understanding token ordering in masked diffusions. arXiv preprint arXiv:2502.06768, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. 2024. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org, 2023. Jia-Nan Li, Quan Tu, Cunli Mao, Zhengtao Yu, Ji-Rong Wen, and Rui Yan. Streamingdialogue: Prolonged dialogue learning via long context compression with minimal losses. Advances in Neural Information Processing Systems, 37:8607486101, 2024. Yifan Li, Kun Zhou, Wayne Xin Zhao, and Ji-Rong Wen. Diffusion models for non-autoregressive text generation: survey. arXiv preprint arXiv:2303.06574, 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. Omer Luxembourg, Haim Permuter, and Eliya Nachmani. Plan for speeddilated scheduling for masked diffusion language models. arXiv preprint arXiv:2506.19037, 2025."
        },
        {
            "title": "Preprint",
            "content": "Christopher Manning and Hinrich Schutze. Foundations of statistical natural language processing. MIT press, 1999. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding with unsupervised learning. 2018. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, and Arash Vahdat. Esoteric language models. arXiv preprint arXiv:2506.01928, 2025. Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction. arXiv preprint arXiv:2508.02558, 2025. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2021. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multitask language understanding benchmark. Advances in Neural Information Processing Systems, 37: 9526695290, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025a. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, 2025b. URL https://arxiv.org/abs/2505.22618."
        },
        {
            "title": "Preprint",
            "content": "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip Yu, et al. survey on parallel text generation: From parallel decoding to diffusion language models. arXiv preprint arXiv:2508.08712, 2025."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Comparison between REFUSION and existing MDMs. denotes the generation length and denotes the block or slot size. Model Generation Scope Attention Mechanism Generation Order Full KV Cache Reuse LLaDA Full Sequence Bidirectional Any-Order BD3-LMs REFUSION Intra-block Bidirectional Any-Order Inter-block Intra-slot Inter-slot Causal Causal Causal Left-to-Right Left-to-Right Any-Order Number of Distinct Masking Patterns (cid:1) 2L (cid:80)L (cid:0)L l= 2k (cid:0)L/k (cid:80)L/k i=1 (( (cid:1) i! = ( )! 1 )! 1 2L for large k)"
        },
        {
            "title": "A METHODOLOGICAL DETAILS",
            "content": "A.1 COMPARISON BETWEEN REFUSION AND REPRESENTATIVE MDMS Table 5 provides detailed, side-by-side comparison of the architectural and methodological designs of REFUSION against two representative MDMs, LLaDA (Nie et al., 2025) and BD3-LMs (Arriola et al., 2025). This comparison highlights how REFUSION uniquely addresses the fundamental trade-offs between generation flexibility, computational efficiency, and learning complexity. (1) LLaDA, as conventional MDM, operates on the entire sequence with bidirectional attention mechanism. This grants it maximum flexibility, allowing for fully unconstrained, any-order generation process. However, this design choice incurs two significant penalties. First, the bidirectional attention is fundamentally incompatible with KV caching, resulting in substantial computational overhead at each decoding step. Second, it must learn dependencies across an exponential space of possible masking patterns. For sequence of length L, any given training or inference state is defined by subset of tokens that remain masked. Since each of the positions can be either masked or unmasked, the model must, in principle, handle any of the 2L possible subsets of visible context5. This combinatorial space of approximately 2L distinct masking patterns presents an intractable objective, as the model may not be sufficiently trained on the specific patterns encountered during inference, leading to incoherent parallel generation. (2) BD3-LMs attempts to mitigate these issues with hybrid, block-based approach. It enforces rigid, left-to-right generation order between blocks, which enables KV cache reuse across block boundaries. However, within each block, it retains bidirectional attention and any-order token generation. This design makes critical compromise. It sacrifices global generation flexibility for discovering optimal generation strategies, which is key theoretical advantage of MDMs. Furthermore, it still faces the challenges of token-level incoherence and the inability to utilize KV caching for intra-block decoding. (3) REFUSION introduces more elegant and unified solution. Generation is structured at the slot level. Within each slot (intra-slot), generation is autoregressive (left-to-right) under causal attention mask, directly addressing the strong local dependencies between adjacent tokens. Between slots (inter-slot), the model retains the flexibility of any-order generation, enabling it to discover better, non-linear generation paths than the left-to-right order. Crucially, by reordering generated slots to always precede masked ones in the input sequence, REFUSION enables full KV cache reuse for every decoded token, feature unique among these models. This design simultaneously achieves two critical goals: it combines global generation flexibility with universal computational efficiency, and it drastically reduces the learning complexity from an exponential token-level permutation space to far more manageable slot-level one (( )! 1). For typical sequence length of = 4, 096, slot size of just = 8 is sufficient to ensure ( )! 1 < 2L. In summary, while prior models are forced to trade flexibility for efficiency or vice versa, REFUSIONs innovative slot-based framework is the only approach that concurrently offers global any-order generation, full KV cache reuse, and tractable training objective. 5Notably, due to the bidirectional attention, the model is invariant to the order in which clean tokens are revealed. Therefore, the learning complexity is not permutations (L!)."
        },
        {
            "title": "Preprint",
            "content": "A."
        },
        {
            "title": "INFERENCE FORMALIZATION",
            "content": "In this section, we formalize the two-step decoding iteration as follows. Crucially, standard causal attention is consistently applied throughout the decoding process, and the position IDs for all tokens always correspond to their indices in the correct ground-truth sequence, remaining invariant regardless of this repositioning. By using these consistent position IDs with RoPE (Su et al., 2021), the model correctly perceives relative positions even when the input buffer is reordered. , in generation order) with the remaining masked slots ( Smasked Step I: Diffusion-based Slot Planning. The first step leverages the models MDM capability to plan the next decoding slots. At timestep (defined as the ratio of remaining masked slots), we construct the input St for enabling KV cache by concatenating already-decoded clean slots ( Sclean , in their original positional order). The planning process then generates draft for all masked slots. This draft serves dual purpose: providing basis for scoring each slot for planning, and acting as speculative guess for the subsequent infilling stage (Leviathan et al., 2023). Specifically, for each position in the i-th slot of Smasked is sampled from the models marginal distribution, conditioned on the leading context: , draft token di,j di,j Pθ( p0, Sclean (4) t,(i,j) denote the tokens before the position of the target token. This yields draft version }. We then quantify the models certainty score of i-th as the models predicted probability of its first token di,1 where Smasked of the masked slots, denoted as Sdraft slot Si = { di,j in Smasked t,(i,j)), , Smasked : t C( Si t) = Pθ( di,1 (5) The model then selects batch of slots with scores exceeding threshold τslot for subsequent infilling. If no slot meets this criterion, the single slot with the globally highest score is selected instead. This strategy identifies slots that are strongly constrained by the existing context and weakly interdependent (e.g., distinct function definitions in code generation), making them suitable to parallelize. t,(i,1)). p0, Sclean , Smasked Step II: Autoregressive Slot Infilling. The second step verifies and completes the selected draft slots by leveraging the models autoregressive modeling capability. To accelerate infilling, we employ speculative decoding strategy powered by the models MDM capability. To achieve this, we first concatenate the slots in their original left-to-right order. The model then calculates the conditional probability of each token, conditioned on all preceding tokens within the newly formed sequence: p0, Sclean p0, Sclean Next, we verify the draft by identifying the longest prefix of the concatenated sequence, with length l, where every tokens probability exceeds the threshold τtoken. If the prefix is long enough to form at least one full slot (i.e., k), we accept the first l/k slots and immediately begin new planning-infilling iteration, bypassing the costly suffix completion. t,(i,1)), t,<(i,j)). , Smasked , Sdraft Pθ( di,1 Pθ( di,j if = 1 if > 1 P( di,j ) = (cid:40) (6) Otherwise, if the accepted prefix fails to cover at least one complete slot (i.e., < k), we revert to processing the draft slots independently via Parallel Iterative Completion process. We independently refine the draft Si j=1 for each slot in parallel, where denotes the set of indices for the selected slots. The completion process iterates through two steps until all selected slots are fully completed. In each iteration m: = { di,j }k (cid:110) η l(m) = max (i) Verification: For each slot I, we identify the longest valid prefix length l(m) where the conditional probability of every token in the prefix exceeds τtoken, conditioned on the prompt, the already-decoded clean slots, and the preceding tokens within slot i: (cid:12) (cid:12) η, Pθ( di,j (cid:12) where di,<j denotes the prefix within the i-th slot. (ii) Prediction: We retain the valid prefix di,1:l(m) masked tokens by sampling from the models distribution: di,j , Smasked Pθ( p0, Sclean t, l(m) , re-mask the remaining suffix, and predict the p0, Sclean (l(m) , di,1:l(m) ) > τtoken , di,<j , k]. (cid:111) ), (7) (8) , t t i <j"
        },
        {
            "title": "Preprint",
            "content": "Table 6: Hyperparameter settings for different tasks. Benchmark Generation Length Slot Selection Threshold τslot Token Acceptance Threshold τtoken Slot Size Block Size MMLU-Pro ARC-C GSM8K MATH GPQA HumanEval MBPP 512 512 512 512 512 512 0.9 0.8 0.9 0.8 0.8 0.9 0.9 0.4 0.1 0.4 0.6 0.2 0.4 0.3 16 8 32 32 8 32 32 128 8 128 64 16 128 128 Table 7: Performance comparison of different models on HumanEval and MBPP. Model # Total Params # Activated Params System Optimization Throughput (TPS) HumanEval (# Shots) MBPP (# Shots) Nova Micro GPT 4o Mini Gemini 2.0 Flash Lite Mercury Coder Mini Mercury Coder Small Gemini Diffusion Seed Diffusion LLaDA-MoE REFUSION - - - - - - - 7B 8B Autoregressive Model Masked Diffusion Model - - - - - - - 1B 8B 148 59 201 1,109 737 1,479 1,600 98 79.30 (0) 88.00 (0) 90.00 (0) 88.00 (0) 90.00 (0) 89.60 (0) 82.80 (0) 62.20 (0) 78.66 (0) 65.40 (3) 74.60 (3) 75.00 (3) 77.10 (3) 76.60 (3) 76.00 (3) 79.40 (3) 67.45 (3) 68.20 (0) After infilling each selected slot, the completed slots are moved from Smasked . For the subsequent iteration, the KV caches from these parallel-generated slots are concatenated. While this parallel generation forgoes inter-slot conditioning, we observe in our experiments that this has minimal impact on final performance (see 5.5). This planning-infilling iteration repeats with an updated timestep until no masks remain (t = 0), at which point the final response r0 is formed by sorting Sclean back into its original sequence order. to Sclean t"
        },
        {
            "title": "B EXPERIMENTAL DETAILS",
            "content": "B.1 IMPLEMENTATION DETAILS Our training data comprises 3.7M samples from MAmmoTH (Yue et al., 2023), OpenMathInstruct2 (Toshniwal et al., 2024), OpenCoder (Huang et al., 2024), SmolLM 2 (Allal et al., 2025), and Tulu 3 (Lambert et al., 2024). For OpenMathInstruct-2, we use its 1M open-source version and remove questions longer than 1,024 tokens as instructed. We use global batch size of 512, maximum sequence length of 4,096, and learning rate of 2e-5. The training is conducted on 16 nodes, each with 8 H20 GPUs, and is accelerated using DeepSpeed ZeRO-2 (Rajbhandari et al., 2020) and Flash-attention-2 (Dao, 2023). The total training cost is approximately 10.68K H20 GPU-hours. We set λ in Eq. 3 to 1. For each training sample, we randomly select slot size from {4, 8, 16, 32}. Existing MDMs decode sequences to predetermined length. Even when an end-of-sequence (EOS) token appears early, the model still expends decoding time on all tokens with higher position IDs. To address this issue, we introduce mechanism for efficient variable-length generation. Specifically, during training, we pad shorter sequences in mini-batch with padding tokens and exclude these tokens from the loss computation. During inference, upon generating an EOS token, we dynamically truncate the target length to that tokens position. This prevents the decoding of any tokens with higher position ID, thereby reducing redundant computation. B.2 HYPERPARAMETER SETTING During the REFUSION inference process, four hyperparameters can be adjusted: the slot selection threshold τslot, the token acceptance threshold τtoken, the slot size k, and the block size b. Table 6 shows the specific settings used in our evaluation."
        },
        {
            "title": "C EXPERIMENT RESULTS",
            "content": "C.1 COMPARISON WITH CLOSED-SOURCE AND DIVERSE ARCHITECTURES To comprehensively demonstrate our advantages, we further compare REFUSION against closedsource and diverse ARM and MDM architectures. As shown in Table 7, REFUSION outperforms several highly optimized ARMs in either speed or performance. Specifically, it achieves 1.66 the throughput of GPT-4o Mini and surpasses Nova Micro by 2.8 points on MBPP. Furthermore, REFUSION yields results approaching those of larger and more powerful closed-source MDMs. C.2 TRADE-OFF FRONTIER ANALYSIS To further distinguish REFUSION from prior MDMs, we investigate the trade-off frontiers of different models. Figure 6 shows that both LLaDA and Dream suffer sharp performance decline as parallelism (tokens generated per forward pass, TPF) increases6, indicating failure to uphold the conditional independence assumption when selecting tokens for parallel decoding. In contrast, REFUSIONs curve is substantially flatter, validating that its training and decoding strategies can more reliably identify conditionally independent tokens. Figure 6: Pass@1 on MBPP for REFUSION and baseline MDMs over TPF. C.3 SCALING WITH DATA SIZE To understand the scaling properties of our model, we investigate the impact of training data size on REFUSIONs performance and efficiency. To this end, we collect additional data of potentially lower quality to expand our training set to 14M samples. Figure 7 illustrates the results of this analysis on GSM8K and MBPP, where we train REFUSION for one epoch on datasets of varying sizes (from 120K to 14M samples) and evaluate it using the same hyperparameters as in Table 6. Figure 7: Scaling properties of REFUSION on GSM8K and MBPP. We plot performance (Accuracy/Pass@1, %) and inference throughput (TPS) as function of training data size. The results reveal clear and positive scaling trend for both key metrics. Specifically, throughput (TPS, dashed lines) improves generally consistently as the training data size increases. For instance, on MBPP, throughput rises from approximately 51 TPS with 120K samples to over 81 TPS with 14M samples. This indicates that as the model is exposed to more diverse data, its internal generation process becomes more efficient, likely due to higher confidence and thus higher acceptance rate of its parallel drafts, leading to fewer decoding iterations. Interestingly, the performance scaling (solid lines) is not strictly monotonic, common phenomenon when training with fixed epoch count. On GSM8K, accuracy peaks at 2M samples before slightly decreasing at 3.7M. This behavior highlights trade-off between data breadth and training depth under constrained computational budget: with fixed one-epoch schedule, training on larger dataset potentially leads to under-convergence relative to the datasets complexity. Nevertheless, the consistent rise in throughput coupled with the substantial performance uplift from the 120K baseline suggests that with an increased computational budget (i.e., more training epochs on the larger datasets), performance would likely continue to improve, further unlocking the full potential of our approach. 6We use TPF here, rather than TPS, to isolate the algorithmic trade-off from any system-level overheads."
        },
        {
            "title": "Preprint",
            "content": "C.4 ANALYSIS OF BLOCK SIZE Our inference strategy is compatible with semi-autoregressive remasking (Nie et al., 2025). Specifically, during inference, the target sequence is partitioned into consecutive blocks of size b. These blocks are decoded sequentially, while our synergistic decoding algorithm is applied to each block as single unit. Notably, the constraint must be satisfied, where is the size of slot, the fundamental unit for parallel decoding in our method. Figure 8 illustrates the impact of block size on our methods performance and throughput (TPS). The figure reveals that both metrics exhibit non-monotonic trends as increases. The trend in performance arises from trade-off between the planning horizon and modeling complexity. Specifically, while larger allows the model to cover longer semantic units in single plan to enhance coherence, it simultaneously imposes greater challenge in generating larger, more complex blocks in an arbitrary order. Similarly, the non-monotonic trend in throughput (TPS) is attributable to computational overhead. Although larger provides more opportunities for parallelism, it forces the model to process longer sequence containing many padded (i.e., yet-to-be-generated) positions. This significantly increases the latency of each decoding step, which eventually diminishes and then reverses the throughput gains observed with larger block sizes. Although performance fluctuates slightly with larger block sizes, REFUSIONs pass@1 decreases by only approximately 0.2% relative to its peak as increases from 32 to 512. This robustness to block size highlights the models ability to leverage strong diffusion-based planning to select the most appropriate slots for decoding across wide range. Collectively, these analyses reveal robust and wide sweet spot, highlighted by the yellow shaded regions in Figures 4 and 8, where REFUSION consistently surpasses the Qwen3-8B baseline in both performance and throughput (TPS). This superior operating zone corresponds to block size [32, 128]."
        },
        {
            "title": "D CASE STUDY",
            "content": "D.1 CODE GENERATION CASE STUDY Figure 8: Relative change in REFUSIONs pass@1 (%) and throughput (tokens/sec) compared to Qwen3-8B (horizontal dashed lines at zero) as block size varies. The yellow shaded region highlights the range of where REFUSION surpasses Qwen3-8B. Table 8 showcases responses to the same MBPP problem as Figure 5 from different models. REFUSIONs ability to globally plan the overall structure via diffusion-based process while locally infilling details autoregressively results in better-structured, high-quality code, demonstrating its superiority over existing MDMs. D.2 STEP-BY-STEP VISUALIZATION OF INFERENCE In order to facilitate the understanding of our inference method, we show step-by-step decoding process in Figure 9. Specifically, our inference process progressively generates the response through an iterative plan-and-infill mechanism. The model maintains standard token-wise causal attention throughout the entire process. Each decoding iteration operates as two-stage cycle: First, the planning stage predicts drafts for all masked slots in parallel and selects subset of high-quality slots based on confidence scores. Second, the infilling stage treats the selected slots as batch and completes them autoregressively to ensure local coherence. To enable full KV cache reuse, the newly decoded slots are physically moved to the front of the remaining masked slots after infilling. Crucially, while the tokens physical position may change due to this repositioning, their position IDs remain invariant, always corresponding to their indices in the correct ground-truth sequence. By utilizing these consistent position IDs with RoPE (Su et al., 2021), the model accurately perceives the relative positions of all tokens, even when the input buffer is reordered."
        },
        {
            "title": "Preprint",
            "content": "Table 8: Model responses to the same MBPP problem as Figure 5. Problem You are an expert Python programmer. Your task is to write single Python function to solve the problem described below, and here is your task: Write function to sum all amicable numbers from 1 to specified number. Your code should pass these tests: assert amicable_numbers_sum(999)==504 assert amicable_numbers_sum(9999)==31626 assert amicable_numbers_sum(99)==0 LLaDA-8B-Instruct def amicable_numbers_sum(limit): def is_perfect_square(n): root = int(n**0.5) return root * root == def sum_divisors(n): total = 0 for in range(1, n): if % == 0: total += return total def is_amicable(a, b): return sum_divisors(a) == and sum_divisors(b) == Dream-7B-Instruct amicable = set() for in range(1, limit + 1): if is_perfect_square(i): amicable.add(i) for in range(i + 1, limit + 1): if is_amicable(i, j): amicable.add(j) return sum(amicable) def amicable_numbers_sum(n): def is_amicable(num): divisors = 1 for in range(2, num): if num % == 0: divisors += if != num // i: divisors += num // return divisors == num total = 0 for in range(2, n): if is_amicable(i): total += return total REFUSION def amicable_numbers_sum(n): def sum_divisors(num): sum = 1 for in range(2, int(num**0.5) + 1): if num % == 0: sum += if != num // i: return sum sum += num // amicable_sum = 0 for in range(2, + 1): sum_i = sum_divisors(i) if sum_i != and sum_divisors(sum_i) == i: amicable_sum += return amicable_sum"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Visualization of the REFUSION inference mechanism."
        },
        {
            "title": "E LIMITATIONS",
            "content": "A primary limitation of our current framework is the immutability of generated slots. Once the tokens within slot are generated via diffusion-based planning and autoregressive infilling, they"
        },
        {
            "title": "Preprint",
            "content": "are considered final and cannot be remasked or refined in future iterations. This design choice, while simplifying the process, precludes the model from correcting potential errors made within completed slot. promising direction for future work would be to introduce re-masking mechanism at the sub-slot level. For instance, after infilling slot, the model could verify the generated tokens and preserve only high-confidence prefix, while re-masking the lower-quality suffix. This would allow for iterative refinement but would necessitate more complex inference logic, potentially involving dynamic adjustments of slot sizes to handle these newly masked, smaller segments. Developing an efficient strategy for such dynamic, fine-grained refinement remains key challenge for future research."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}