{
    "paper_title": "AMO Sampler: Enhancing Text Rendering with Overshooting",
    "authors": [
        "Xixi Hu",
        "Keyang Xu",
        "Bo Liu",
        "Qiang Liu",
        "Hongliang Fei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost."
        },
        {
            "title": "Start",
            "content": "AMO Sampler: Enhancing Text Rendering with Overshooting Xixi Hu1,2*, Keyang Xu1*, Bo Liu2, Qiang Liu2and Hongliang Fei1 1Google, 2University of Texas at Austin {hxixi,bliu,lqiang}@cs.utexas.edu; {keyangxu,hongliangfei}@google.com 4 2 0 2 8 2 ] . [ 1 5 1 4 9 1 . 1 1 4 2 : r Figure 1. Improved Text Rendering. (a)-(d) illustrate four common text rendering mistakes in text-to-image generations. Compared to the standard Euler sampler (purple), our Attention Modulated Overshooting sampler (AMO) (yellow) produces accurate and complete text without additional training, and remains as computationally efficient as the Euler sampler. Abstract Achieving precise alignment between textual instructions and generated images in text-to-image generation is significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation Equal contribution. Qiang Liu and Hongliang Fei jointly advised this work. (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively control the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost. 1. Introduction Recent advances in diffusion models [18, 19, 3133] have enabled high-quality image and video generation. Text-toimage generation, where neural networks create images from natural language prompts, has emerged as transformative application of AI. Despite significant progress, key challenge remains in precisely aligning generated images with given text instructions, especially in text rendering tasks, where models often struggle to display specific text accurately. This misalignment results in errors like misspellings or incorrect wording (see Figure 1 for examples), limiting the models utility in fields like graphic design, advertising, and assistive technologies [7, 30]. Although fine-tuning with curated text data can improve text rendering [4, 5], it requires additional data collection and computationally expensive retraining, making it impractical for many applications. Furthermore, such fine-tuning may inadvertently compromise the models overall imagegeneration capabilities. In this work, we investigate methods to enhance text rendering quality. We focus on rectified flow (RF) [19] models, which have emerged as compelling alternative to conventional diffusion models due to their conceptual simplicity, ease of implementation, and improved generation quality [7]. Specifically, we introduce lightweight, training-free sampling approach that significantly improves text rendering accuracy in generated images. We introduce novel and straightforward stochastic sampling approach on top of RF models, named the Overshooting sampler, that iteratively adds noise to the Euler sampler while preserving the marginal distribution. In particular, the Overshooting sampler alternates between over-simulating the learned ordinary differential equation (ODE) and reintroducing the noise (See Section 3). As we will show in Section 3.1, Overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from the successive applications of the Euler sampler, therefore enhancing the text generation quality. The overshooting strength is controlled by hyperparameter > 0, corresponding to the magnitude of the Langevin step. When is large, the Langevin term becomes inaccurate and can introduce error itself. To mitigate this problem, we propose targeted use of the Overshooting sampler for text rendering, by adaptively controlling its strength on different patches of the image according to their attention scores with the text content in the prompt. We name the combined approach as Attention Modulated Overshooting sampler (AMO). We validate the AMO sampler on state-of-the-art RFbased text-to-image models, including SD3, Flux, and AuraFlow. Our experiments demonstrate significant improvement in text rendering accuracy, with correct text generation rates increasing by 32.3% on SD3 and 35.9% on Flux, without compromising overall image quality. Noise ODE Overshooting (Equation (1)) Zt Zs ˆZo Image Time Noise Compensation (Equation (2)) Figure 2. Visualization of the Overshooting Sampler. Given Zt at time t, we first over-simulate the learned ODE to ˆZo, and then add noise and return to Zs. The noise is carefully selected such that Zs matches Xss marginal distribution. 2. Background on Rectified Flow This section provides brief introduction to Rectified Flow (RF) [19]. RF seeks to learn mapping from an easy-tosample initial distribution X0 π0, which we assume to be the standard Gaussian (0, I), to target data distribution X1 π1. This is achieved by learning velocity field that minimizes the following objective: (cid:90) 1 min E(X0,X1)π0,π1 (cid:20)(cid:13) (cid:13)v(Xt, t) Xt (cid:13) 2(cid:21) (cid:13) (cid:13) (cid:13) dt. In RF, Xt is defined as time-differentiable interpolation between X0 and X1, i.e., Xt = tX1 + (1 t)X0 and Xt = X1 X0. Once is learned, it induces an ordinary differential equation (ODE): dt Zt = v(Zt, t), [0, 1], Z0 = X0. It can be shown that Zt and Xt share the same marginal law [19] if is learned well, and therefore simulating this ODE with Z0 = X0 results in Z1 being samples from the target distribution π1. In practice, this ODE can be discretized using the Euler method by selecting time steps t0 = 0 < t1 < < tN = 1, and iteratively updating: Z0 π0. Ztk+1 = Ztk + (tk+1 tk)v( Ztk , tk), We use to differentiate the discretized ODE trajectory from its ideal continuous time limit Z. Note the entire process is deterministic once Z0 is chosen. 3. Attention Modulated Overshooting Sampler In this section, we derive the Overshooting sampler that adds stochastic noise to the Euler sampler while preserving the marginal distribution (Section 3.1). Then we illustrate this is equivalent to adding Langevin dynamics term at each Euler step (Section 3.2). Importantly, the extra Langevin term can help correct the compounding error from successive Euler steps. When the overshooting strength is high, the stochastic sampler can introduce artifacts. To mitigate this problem, we propose straightforward attention modulation method that adaptively controls the strength of the overshooting for each image patch based on its attention score with the text content in the prompt (Section 3.3). 2 Algorithm 1 Attention-Modulated Overshoot Sampling for Rectified Flow. 1: procedure OVERSHOOTINGSAMPLER(v, {ti}N 2: Initialize Z0 (cid:0)0, I(cid:1) for {0, . . . , 1} do i=0, R+) 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: Calculate velocity vi = v( Zti; ti); get the cross attention mask mi Overshooted ODE update: ˆZo = Zti + (o ti) vi, with = min(cid:0)ti+1 + c(ti+1 ti)mi, 1(cid:1). Backward update by adding noise: Zti+1 ˆZo + bξi, where ξi (cid:0)0, I(cid:1), and = and = (cid:112)(1 s)2 (a(1 o))2. return ZtN 3.1. Stochastic Sampling via Overshooting This section provides derivation of stochastic sampling method, Overshooting sampler, for RF-trained models. The main idea is to overshoot the forward Euler step and subsequently compensate with backward noise injection. In the limit of small step sizes, this process converges to stochastic differential equation (SDE), and we will show rigorously in the subsequent section that the resulting SDE ensures the marginal preserving property according to the Fokker-Planck equation. Following our notation in the previous section, let Z0 = X0 π0 be sample from the initial noise distribution, and assume we have obtained Zt at time t, and want to get Zs for the next time point = + ϵ, where ϵ > 0 is the step size when denoising. Note that for standard Euler sampler, Zs is obtained from Zt + ϵv( Zt, t). In comparison, to introduce stochastic noise, we propose the overshooting sampler which consists of the following two steps (See Figure 2): 1. ODE Overshooting First, we temporarily advance Zt to ˆZo where = s+cϵ (with > 0) denotes an overshooting time point that is larger than s. Specifically, we conduct the following the forward Euler step: ˆZo = Zt + v( Zt, t)(o t) = Zt + (1 + c)ϵv( Zt, t). (1) We use ˆZo to emphasize that it is reached via overshooting. 2. Noise Compensation Next, we want to revert from time to time by noising ˆZo. Assume we achieve this by computing Zs = ˆZo + bξ, ξ (0, I). (2) Then, the goal is to determine the coefficients and here. Law If we assume the overshooting step is accurate, then ˆZo Law= oX1 + (1 o)X0, where Law= denotes equality in Zo distribution. Therefore, Zs Law= a(oX1 + (1 o)X0) + bξ = aoX1 + (cid:112)a2(1 o)2 + b2ξ, (3) where ξ (0, I) and the last line is derived as both ξ and X0 are i.i.d Gaussian noises. On the other hand, we Law= sX1 + (1 s)Xs. Hence, by matching the know that Zs coefficients, we get (cid:114) = , = (1 s)2 s2 (1 o) . (4) o2 Recall that = t+ϵ and = t+(1+c)ϵ, as ϵ 0, the above process (steps 1 and 2) will approach the following limiting stochastic different equation (SDE) (See Appendix A.1 for the derivation): (cid:18) dZt = (1 + c)v(Zt, t) (cid:19) (cid:114) Zt dt + 2(1 t) dWt, (5) where Wt denotes the Brownian motion. 3.2. Overshooting Euler + Langevin Dynamics Equation (5) can also be derived directly from the FokkerPlanck equation following similar ideas from [33]. Let dZt = ft(Zt)dt + σtdWt be SDE where σt 0 is diffusion coefficient independent of Xt. Denote by ρt the density of Zt. According to the Fokker-Planck equation: ρt = (cid:0)ftρt (cid:1) + 2(cid:0)σ2 ρt (cid:1), = (cid:0)(ft log ρt)ρt (6) (cid:1). 1 2 σ2 2 This is true in almost all contemporary diffusion/flow models. 3 The last line follows ρt/ρt = log ρt. Now, let ft(Zt) = v(Zt, t) + σ2 2 log ρt(Zt), one can check that for any function σt (according to Equation (6)), the SDE (cid:18) dZt = v(Zt, t) + σ2 2 (cid:19) log ρt(Zt) dt + σtdWt, (7) Remark It is worth noting that the above equivalence (between Equation (5) and Overshooting) only holds in the limit of infinitely small step sizes. Compared to applying the Euler discretization of the SDE in Equation (5), we empirically found that the Overshooting sampler tends to be more stable and yields better text rendering on real images (See Table 1). shares the same marginal law at any time as the ODE 20 Steps 50 Steps 100 Steps dZt = v(Zt, t)dt. Discretize SDE in Equation (5) Overshooting 76.0 % 71.0 % 51.5 % 68.5 % 81.5 % 81.5 % This is because the σ2 2 log ρt cancels out in Equation (6) and it reduces to the continuity equation ρt = (vtρt) for the original ODE. Note that for RF models, log ρt(x) = (tv(x, t)x)/(1t). Hence, by choosing σ2 = 2(1t)c/t, one exactly recovers Equation (5). The full derivation is provided in Appendix A.2. Langevin Dynamics Corrects Marginals Note that Equation (7) can be equivalently viewed as the ODE combined with one step of Langevin dynamics: dZt = v(Zt, t)dt + αt log ρt(Zt)dt + (cid:18) 2αtdWt (cid:124) (cid:123)(cid:122) Langevin Dynamics (8) (cid:19) , (cid:125) where αt = σ2 /2 is the step size for the Langevin dynamics. As Zt comes from successive Euler steps, it does not necessarily follow ρt of Zt. In this case, the Langevin term in Equation (8) helps move Zt towards the desired marginal ρt (see e.g., [14]). In Figure 3, we visualize this correction effect on 2D toy problem. Table 1. Text Rendering Accuracy between Discretize SDE in Equation (5) and Overshooting. We conduct human study on the text rendering accuracy and compare discretizing SDE in Equation (5) and Overshooting. We compare the two samplers for 20, 50, and 100 inference steps, and found that with fewer steps, the Overshooting sampler demonstrates more significant improvement over applying the discretized SDE. This is because when ϵ (the step size) is large, the Euler discretization becomes inaccurate and unstable. 3.3. Attention Modulation In practice, while increasing can enhance text rendering quality, it may also introduce artifacts (see Figure 6 for examples). This is because, with larger values of c, the single-step Langevin correction in Equation (8) becomes less accurate. To address this issue, we propose dynamically adjusting the overshooting strength for different image patches based on their attention scores to the text content in the prompt. Simply put, this approach increases overshooting for areas related to the text while applying less to the rest of the image. More concretely, assume the image consists of patches, where and denote the height and width dimensions of the 2D image tokens (e.g., 64 64 in our experiment). Let ximage h,w be the (h, w)-th image patch token. Let {xtext }n i=1 denote the set of tokens within the language ini struction prompt for generating text and is the total number of text-related tokens (e.g., so {xtext} can be the Tokyo Halloween Night in the prompt poster design with title text of Tokyo Halloween Night). Then, we construct mask Rhw in the following way: mh,w = (cid:88) i=1 exp(Q(xtext h,w exp(Q(xtext )K(ximage h,w )) )K(ximage h,w ) (cid:80) , (9) Figure 3. Euler versus Overshooting on toy dataset. The noise (π0) and data (π1) distributions are shown as blue and light-purple dots. Top: The samples from Euler deviate from π1. Overshooting sampler helps correct the marginal. As increases, the correction effect is stronger, but it also introduces smoothing artifacts. Bottom: Starting with Zt (t = 0.5) from the Euler sampler, if we apply 5 times of (Overshooting - Euler), i.e., the Langevin dynamics part in Equation (8)), the samples align better with Z0.5. where and denote the query and key vectors in attention. We then average the attention map over different layers and heads and rescale its values between 0 and 1. After that, we apply the resulting attention map, m, to give different image patches different amounts of overshooting. Specifically, assume [0, 1]hw, where oh,w = + cϵmh,w, we have ˆZo = Zt +v( Zt, t)(ot) = Zt +ϵ(1+cm)v( Zt, t), (10) where denotes element-wise product. The noise compensation step in Equation (4) is similarly adapted, where a, b, are replaced by other vector counterparts: (cid:114) = , = (1 s)2 s2 (1 o)2 . (11) o2 In the above, all operations are elementwise. Both and have dimensions Rhw. The entire AMO sampling process is provided in Algorithm 1. 4. Related Work This section gives an overview of diffusion and flow-based generative models, followed by discussion on deterministic and stochastic sampling techniques, and recent advances in enhancing text rendering in text-to-image generation. Diffusion and Flow Models. Diffusion models [12, 32, 33] have emerged as powerful generative frameworks capable of producing high-fidelity data, including images, videos, audio, and point clouds [2, 3, 6, 13, 26, 30]. These models add noise to data in forward process, then learn to reverse this noise to generate new samples, thereby modeling the data distribution through progressive denoising process. Recently, Rectified Flow (RF) [1, 10, 18, 21]also known as Flow Matching, InterFlow, and IADBhas been proposed as novel approach that leverages an ordinary differential equation (ODE) with deterministic sampling during inference. RF simplifies the diffusion and denoising process, offering computational efficiency while maintaining highquality generation, and positioning itself as compelling alternative to traditional diffusion models. The rectified flow model has proven successful in various applications, including image generation[7], sound generation [9, 16] and video generation [27]. Deterministic and Stochastic Sampling Methods. Sampling strategies in generative modeling are crucial as they influence the quality, diversity, and efficiency of generated samples. Deterministic sampling methods, such as those based on ODE solvers [14, 23, 31, 33], provide computational efficiency and stability but may lead to poorer output quality [14]. On the other hand, stochastic sampling methods introduce randomness into the sampling process, offering an alternative approach with added variability in intermediate steps. Meng et al. [25] introduced an N-step stochastic sampling method for distilled diffusion models, where noise is added at intermediate steps to achieve efficient sampling with as few as 2-4 steps. This approach provides an alternative to deterministic sampling, allowing the model to produce high-quality samples. Karras et al. [14] proposed hybrid stochastic sampling technique that combines deterministic ODE steps with noise injection. In their method, noise is temporarily added at each step to improve sampling quality, followed by an ODE backward step to maintain the correct 5 distribution. This hybrid approach results in better output quality compared to purely deterministic sampling methods. However, these existing methods are specifically designed for diffusion models. In contrast, we propose stochastic sampler for rectified flow, providing an alternative solution to the traditional Euler method. Enhancing Text Rendering in T2I Generation. Accurate text rendering in text-to-image (T2I) generation models remains significant challenge, as models often struggle to produce text within images that precisely matches the prompts, leading to incoherent or incorrect textual content. Classifier-Free Guidance (CFG)[11] can alleviate this issue by adjusting the influence of the text prompt during sampling, effectively balancing prompt guidance and the diversity of the generated content. Scaling or enlarging the text encoder has been shown to benefit text rendering[26]. Additionally, using T5 text encoder significantly improves text rendering performance [7, 30]. Specialized fine-tuning approaches have also been explored, where pretrained text-to-image models are adapted with architectures designed specifically for text rendering tasks [4, 5, 20, 22, 24, 29, 34, 35]. These methods enhance the models ability to generate accurate textual content but typically require extensive retraining or fine-tuning, which can be computationally intensive. In contrast, our work introduces training-free approach that enhances text rendering during inference. By incorporating stochastic sampling and an attention mechanism into the Rectified Flow framework, we improve text rendering quality without modifying the underlying model or incurring additional training costs. 5. Experiment We conduct experiments with several open-source text-toimage models based on Rectified Flow, including Stable Diffusion 3 (medium) [7], Flux (dev) [15], and AuraFlow [8]. Image generation was performed using the NVIDIA A40 GPU during inference. To ensure high-quality visual assessment, all output images were generated at resolution of 1024x1024 pixels. Detailed model configurations and hyperparameter settings can be found in the Appendix A.3. Evaluation Metrics. We use combination of automated and human evaluations to assess the performance of our models. For automated evaluation, we adopt benchmarks including DrawTextCreative [20], ChineseDrawText [24] and TMDBEval500 [5], which comprises total of 893 prompts drawn from various data sources. To assess the correctness of rendered text, we compute OCR Accuracy and OCR F-measure using pre-trained Mask TextSpotter v3 model [17]. We evaluate the samples visual-textual alignment using CLIP Score, specifically CLIP L/14 [28], and also compute FID for overall visual quality between the CLIP image features and validation set images. Figure 4. Comparison of text rendering quality between Euler and our stochastic sampling method across three different text-to-image models: (a) Flux, (b) Stable Diffusion 3 (SD3), and (c) AuraFlow. All results are generated using the same random seed for consistent comparison. Within each pair of images, the left column corresponds to the Euler sampler, while the right column displays the results from our method. Our approach consistently generates clearer and more legible text that closely matches the provided prompts. Additional examples are provided in the Appendix. However, automated OCR tools, such as those in MARIOEval [4], showed limitations in accurately detecting text from the generated images. This is partly because general-purpose text-to-image models can produce diverse and artistic fonts that humans can readily understand, but OCR models cannot recognize accurately. It is also worth noting that OCR accuracy can be negatively impacted by extraneous content in images, such as posters, hurting recalls but not affecting the overall human perception. This discrepancy is especially pronounced for general-purpose text-to-image models (see Appendix A.4). To address these limitations, we conduct human evaluation by manually assessing the correctness of text rendering on samples covering 100 prompts. Further details are provided in the Appendix A.3. 5.1. Comparison with Euler Sampler sacrificing overall visual quality. Quantitative Results As shown in Table 2, AMO achieves 82.5% accuracy on Flux model in human evaluation, notably surpassing the 74% accuracy of the standard Euler sampler. In addition, AMO improves the OCR metrics across all three text-to-image models, demonstrating substantial enhancement in the models ability to render text accurately. Furthermore, compared to the standard Euler method, AMO yields better FID scores, indicating superior overall image quality. As shown in Figure 5, we evaluated the performance of AMO using 20, 50, and 100 steps. Our results demonstrate that AMO consistently outperforms the deterministic sampler across all step counts, with better performance improvement in the low-step scenarios. In this section, we compare AMO against the Euler sampler both quantitatively and qualitatively. We found that AMO significantly outperforms Euler on text rendering without Qualitative Results. Figure 4 presents visual comparison between AMO and the standard Euler sampling applied to Flux, Stable Diffusion 3, and AuraFlow. Images generated 6 FID () CLIP () OCR-A () OCR-F () CR () SD3 (Euler) SD3 (AMO) Flux (Euler) Flux (AMO) AuraFlow (Euler) AuraFlow (AMO) 81.1 80.9 111.8 108.9 128.4 117.5 0.319 0.317 0.299 0. 0.299 0.298 0.256 0.279 0.313 0.381 0.075 0.082 0.473 0.482 0.458 0. 0.238 0.258 32.5 % 43.0 % 74.0 % 82.5 % 1.0 % 3.0 % Table 2. Quantitative evaluation of AMO against the Euler sampler. OCR-A and OCR-F are short for OCR (Accuracy) and OCR (F-Measure). CR is the correction rate from human evaluation. that Overshooting without attention modulation can result in an over-smoothing effect, making the generated samples lose high-frequency details (See the parrot feather and smoke). This confirms the necessity of attention modulation. (O, N, A) (, , ) (, , ) (, , ) (, , ) FID () CLIP () OCR-A () OCR-F () CR () 111.8 367.8 109.5 109.0 0.299 0.126 0.304 0.301 0.313 0.030 0.368 0.381 0.458 0.000 0.503 0. 74.0 % 0.0 % 81.5 % 82.5 % Table 3. Ablation of each component in AMO on Flux. Figure 5. The comparison of Euler sampler and AMO across different sampling steps (20, 50, and 100 steps). AMO consistently outperforms the deterministic sampler on text rendering performance across all step sizes. by AMO exhibit clear and legible text that closely aligns with the given prompts. In contrast, the Euler method frequently produces misaligned and misspelled text. 5.2. Ablation Studies In this section, we conduct detailed ablation study to analyze the impact of different components in AMO on text rendering accuracy and image quality using the Flux model. Impact of Components in AMO. The AMO sampler essentially consists of three parts: ODE overshooting (O), noise compensation (N), and attention modulation (A). We ablate their individual contribution, by comparing AMO without all of them (, , ) (i.e., the Euler sampler), with only overshooting (, , ), without attention modulation (, , ) and the full AMO. Results are summarized in Table 3. It is observed that both overshooting and noise compensation are crucial for achieving accurate text rendering and high image quality. Notably, introducing overshooting alone results in 0% correct rate, as the marginal law is not preserved. The full AMO results in similar performance to the Overshooting sampler (AMO without attention modulation), we think this is because metrics like OCR-F do not capture the image generation quality well. Therefore, we provide visualization of samples from the Overshooting sampler against those from AMO in Figure 6. We observe Figure 6. Image Quality for Euler, Overshooting, and AMO. Please zoom in for details. Bottom: both Overshooting (AMO without attention modulation) and AMO render the correct texts, while Euler renders misspelled texts. Top: Looking at the parrots feather or the smoke behind the saxophone, Euler generates highfidelity high-frequency details while the Overshooting sampler over-smooths the image (fewer details). AMO preserves the details from the Euler, with attention modulation. In addition, we conduct 5 Steps Overshooting, meaning that we use = c/5 but apply (Overshoot - Euler) 5 times (i.e., the Langevin step in Equation (8)) followed by 1 Euler step in the end at each time t. We see that with smaller but more local Langevin steps the smoothing effect also goes away, but in practice, this requires more model evaluations. Impact of Overshooting Strength. We further examine the effect of c, which governs the maximum overshooting strength per step. Results are shown in Figure 7. Generally, 7 increasing enhances text rendering accuracy, with performance plateauing at 2 and occasionally declining for very large values of c. Consequently, we set = 2 as the default in practice. To illustrate the degradation in image quality caused by high c, we direct readers to Appendix A.5.2 for examples. Figure 7. Impact of overshooting strength on Text Rendering Performance. This figure illustrates how varying the overshooting strength parameter in AMO affects the Flux models text rendering performance. Larger tends to achieve higher text rendering quality. We observe that 2 is usually good and use = 2 as the default value. 5.3. Comparison with Finetuned T2I models In this section, we evaluate both text rendering capability and overall quality for two image generation model families for text rendering: 1) General-purpose T2I models trained on extensive image datasets using rectified flow, such as Stable Diffusion 3, Flux and AuraFlow; and 2) Task-specific T2I models explicitly trained on datasets of ground-truth written text, such as GlyphControl [22] and TextDiffuser [5]. Figure 8. Results of human evaluation comparing text rendering quality and overall image quality across five methods. Participants viewed five images, each generated by one of the methods, and were asked to vote for: (1) the models with the best text rendering quality (multiple-choice), and (2) the model with the best overall image quality (single-choice). The chart shows the number of votes received by each method for both criteria. Specifically, we conducted human evaluation study to assess image generation and text rendering quality across 8 different methods. The study involved 304 cases (selected randomly from DrawTextCreative, ChineseDrawText, and TMDBEval500) and 15 participants, with each case presenting two questions: (1) \"Which of the following images exhibits the highest text rendering quality? (multiple-choice)\" and (2) \"Which of the following images demonstrates the best overall image quality? (single-choice)\". The results are summarized in Figure 8. As shown, AMO achieves text rendering accuracy on par with the GraphControl model, which is specifically trained for this task, while delivering superior overall image quality due to the advanced capabilities of the Flux model. Crucially, since AMO is training-free methodunlike approaches such as TextDiffuser or GlyphControlit can be effortlessly applied to any existing model without requiring further training or risking potential image quality loss from fine-tuning. Additionally, when compared to Overshooting (AMO without attention modulation), AMO demonstrates clear advantage: while both yield similar text rendering quality, the Overshooting sampler diminishes image quality relative to the Euler sampler, whereas AMO maintains parity with Euler. This underscores the importance of attention modulation for optimal performance. For additional qualitative comparisons, please refer to Appendix B. 6. Conclusion and Limitation Accurate text rendering has been critical challenge in textto-image generation. Existing solutions, such as specialized fine-tuning or scaling up the text encoder, often require modifications to the training process, which can be computationally expensive and time-consuming. This work introduces training-free method, the Overshooting sampler sampler, that enhances text rendering by strategically incorporating curated randomness into the sampling process. Importantly, Overshooting sampler significantly improves text rendering accuracy with almost no overhead compared to the vanilla Euler sampler. In particular, Overshooting sampler alternates between overshooting the learned ODE and re-introducing noise, while ensuring the marginal laws are well-preserved. In addition, we introduce an attention modulation that quantitatively controls the degree of overshooting, concentrating on text regions within the image while minimizing interference with other areas. We validate AMO on popular open-source text-to-image flow models, including Stable Diffusion 3, Flux, and AuraFlow. Results demonstrate that AMO consistently outperforms baseline methods in text rendering accuracy without degrading the image quality of the pretrained model. One limitation of this work is the lack of systematic evaluation of overshootings impact on specific aesthetics or its applicability beyond image generation. Future work could explore extending AMO to other domains."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 5 [2] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. 5 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 5 [4] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power arXiv preprint of language models for text rendering. arXiv:2311.16465, 2023. 2, 5, 6 [5] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36, 2024. 2, 5, 8 [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 5 [8] Hugging Face. Aura Flow Pipeline Documentation, 2023. Huggingface. 5 [9] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and JunarXiv preprint Flux that plays music. shi Huang. arXiv:2409.00587, 2024. 5 [10] Eric Heitz, Laurent Belcour, and Thomas Chambon. Iterative α-(de) blending: minimalist deterministic diffusion model. In ACM SIGGRAPH 2023 Conference Proceedings, pages 18, 2023. 5 [11] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 5 [14] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35:2656526577, 2022. 4, 5 [15] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2023. GitHub repository. 5 [16] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi Adi, Jay Mahadeokar, et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36, 2024. 5 [17] Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, and Xiang Bai. Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part XI 16, pages 706722. Springer, 2020. 5, [18] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 2, 5 [19] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport. arXiv preprint arXiv:2209.14577, 2022. 2 [20] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware arXiv preprint models improve visual text rendering. arXiv:2212.10562, 2022. 5 [21] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2022. 5 [22] Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Ji Li, and Yuhui Yuan. Glyph-byt5-v2: strong aesthetic baseline for accurate multilingual visual text rendering. arXiv preprint arXiv:2406.10208, 2024. 5, 8 [23] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. [24] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Seamlessly rendering text with intricate spatial structures in text-to-image generation. arXiv preprint arXiv:2303.17870, 2023. 5 [25] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 5 [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 5 [27] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 5 [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 9 [29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3, 2022. 5 [30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2, [31] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 2, 5 [32] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 5 [33] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2, 3, 5 [34] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. 5 [35] Yiming Zhao and Zhouhui Lian. Udifftext: unified framework for high-quality text synthesis in arbitrary images via character-aware diffusion models. arXiv preprint arXiv:2312.04884, 2023. 5 A. Appendix A.1. The SDE Limit of the Overshooting Sampler In this section, we derive the asymptotic limit of the overshooting samplers update as stochastic differential equation (SDE) by considering the infinitesimal step size ϵ 0 in the definitions of and o. Recall that where is constant parameter. Combining the update equations (see Equation (1) and Equation (2)), we obtain = + ϵ, = + cϵ = + (1 + c)ϵ, Zs = ˆZo + bξ = Zt + a(o t)v( Zt, t) + bξ = Zt + (a 1) Zt + a(o t)v( Zt, t) (cid:125) (cid:124) (cid:123)(cid:122) Drift , + bξ (cid:124)(cid:123)(cid:122)(cid:125) Diffusion"
        },
        {
            "title": "We aim to express the update in the form",
            "content": "Zt+ϵ Zt + vadj( Zt, t)ϵ + σt ϵξt, which corresponds to the EulerMaruyama discretization of the SDE dZt = vadj(Zt, t)dt + σtdWt, with Wt denoting standard Wiener process. To this end, we perform first-order Taylor expansion assuming ϵ 0: First, we compute 1: where we use the approximation for small ϵ. Next, we compute a(o t): 1 = 1 = = cϵ cϵ , a(o t) = (o t) = + ϵ + (1 + c)ϵ (1 + c)ϵ (1 + c)ϵ, Now, we compute b2: b2 = (1 s)2 s2 (cid:32)(cid:18) 1 (cid:19)2 = s2 (cid:19)2 (cid:18) 1 (cid:18) 1 (cid:19)2(cid:33) where (x) = (cid:0) 1x (cid:1)2 . Using first-order Taylor expansion of (x) around = s, we have = s2 (f (s) (o)) , (o) (s) + (s)(o s) = (s) + (s)cϵ = 2 1 cϵ, Combining the above results, the update equation becomes Zt+ϵ Zt + vadj( Zt, t)ϵ + σt ϵξt, where the adjusted velocity is vadj( Zt, t) = (cid:19) Zt + (cid:18) 1 ϵ Zt + (1 + c)v( Zt, t), (cid:18) a(o t) ϵ = (cid:19) v( Zt, t) 11 (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) Thus, the limit SDE is dZt = vadj(Zt, t)dt + σtdWt = (cid:16) (1 + c)v(Zt, t) (cid:114) (cid:17) Zt dt + 2c(1 t) dWt. (22) This provides the SDE limit of the overshooting sampler as ϵ 0. A.2. Stochastic Sampler by Fokker Planck Equation As mentioned in Section 3.2, according to the Fokker-Planck Equation, for an ODE dZt = v(Zt, t)dt, we can construct family of SDEs that share the same marginal law as the ODE at all t: (cid:18) dZt = v(Zt, t) + σ2 (cid:19) log ρt(Zt) dt + σtdWt. Now, we only need to figure out log ρt(Zt) and then we can find the corresponding σ2 that matches with the limiting SDE of the overshooting algorithm. To this end, we present the next two lemmas before presenting the equivalence. Lemma A.1. Assume random variables = + Z, where and are independent, then log ρX (x) = E[y log ρY (Y ) = x] = E[z log ρZ(Z) = x], where ρZ and ρY are the density functions of and , respectively. Proof. xρX (x) ρX (x) (cid:82) z ρX,Z(x, z)dz ρX (x) log ρX (x) = = = = ρZ(z)xρY (x z)dz ρX (x) xρY (x z) ρY (x z) ρZ(z)ρY (x z) ρX (x) dz (cid:82) (cid:90) (cid:90) = log ρY (x z) ρZ(z)ρY (x z) ρX (x) = E[x log ρY (X Z) = x] = E[y log ρY (Y ) = x]. dz // and are independent Lemma A.2. Given the linear interpolation in Rectified Flow Xt = tX1 + (1 t)X0, where X0 (0, I), we have Proof. As X0 and X1 are independent since X0 is the standard multivariant Gaussian and X1 is the data distribution, take = tX1 and = (1 t)X0. According to Lemma A.1, we have log ρt(x) = tv(x, t) 1 . (23) log ρt(x) = E[z log ρZ(Z) Xt = x] 1 (1 t)2 E[Z Xt = x] = = E[X0 Xt = x] 1 1 1 1 tv(x, t) 1 = = // (0, (1 t)2I) // = (1 t)X0 // E[X1 X0 Xt] = v(Xt, t). E[t(X1 X0) Xt Xt = x] // Xt = tX1 + (1 t)X0 Plugging in Equation (23) to the SDE, we have (cid:18) dZt = v(Zt, t) + σ2 2 tv(Zt, t) Zt 1 (cid:19) dt + σtdWt. If we choose σ2 = 2c 1t , then we get (cid:18) dZt = v(Zt, t) + 1 (cid:18) = (1 + c)v(Zt, t) which matches Equation (5) exactly. A.3. Experiment Details (cid:19) (cid:114) dt + 2c 1 dWt tv(Zt, t) Zt 1 (cid:19) (cid:114) Zt dt + 2c 1 dWt, (24) Model Configurations and Hyperparameter Settings. The hyperparameter settings for the Flux (FLUX.1-dev), Stable Diffusion 3 Medium, and AuraFlow models are summarized in Table 4. Unless stated otherwise, all experiments are conducted with default inference step count of 100. Hyperparameters FLUX.1-dev Stable Diffusion 3 Medium AuraFlow Image size CFG scale Model Precision Overshooting Strength 1024 1024 3.5 BFloat16 2.0 1024 1024 7.0 Float32 1.0 1024 1024 3.5 Float16 1.0 Table 4. Hyperparameter settings for our experiments. Human Evaluation Setup. We conducted human evaluations to assess text rendering quality and image fidelity. The details of the human evaluation setup are as follows: Text Rendering Evaluation: The evaluation includes total of 100 prompts, each consisting of 58 words, which are provided in the supplementary material (prompts_human_eval.txt). Participants are presented with text prompt and an image generated by one of the models. They are tasked with assessing the correctness of the rendered text in the image. Each image is evaluated by at least two participants. In total, this evaluation involved 72 unique participants. Comparative Evaluation of Text and Image Quality: To compare text rendering quality and overall image quality, 100 samples were selected. These include 25 prompts each from the DrawTextCreative, ChineseDrawText, and TMDBEval500 benchmarks, as well as the primary human evaluation prompts. This evaluation was conducted with 15 participants. This comprehensive evaluation ensures robust assessment of the models ability to generate high-quality images and accurately render text. A.4. Problem in evaluating OCR While OCR tools provide an automatic method for assessing the correctness of rendered text in images, our experiments reveal limitations in existing OCR systems when evaluating state-of-the-art text-to-image models such as FLUX. Specifically, we employed Mask TextSpotter v3 [17] and found that it struggles to accurately detect and recognize text generated by FLUX. As illustrated in Figure 9, Mask TextSpotter performs better when evaluating models like TextDiffuser and GlyphControl, which tend to generate text with simpler layouts and standard fonts. These characteristics align more closely with the training data of the OCR model, making detection easier. In contrast, FLUX-generated text exhibits greater stylistic flexibility and diversity, posing significant challenges for existing OCR tools despite the text being rendered correctly. We provide examples in Figure 9, highlighting the OCR performance disparity. The detected text boxes and predictions are shown in red. These results underscore the need for improved OCR systems capable of handling the creative and flexible text styles generated by advanced text-to-image models. 13 Figure 9. Examples of OCR model performance. Detected text boxes and prediction results are shown in red. The OCR model fails to detect text generated by the FLUX model effectively, even though the text is rendered correctly. Figure 10. Image Quality for Euler, Overshooting, and AMO. Please zoom in for finer details. The Overshooting method shown here employs one-step overshooting strategy, ensuring the overall computational cost remains comparable across all three methods. The overshooting approach results in cartoonish, over-smoothed outputs that lack high-frequency details. In contrast, Euler and AMO generate images that resemble real-world visuals more closely. 14 A.5. Additional Qualitative Results A.5.1 Additional Results on Image Quality for Euler, Overshooting, and AMO We present additional results in Figure 10 to further illustrate our findings. These results confirm that overshooting (without attention modulation) tends to produce an over-smoothing effect, leading to generated samples lacking high-frequency details. Figure 11. Samples generated by varying c. As increases, the images gradually lose complexity and fine details due to over-smoothing. For moderate values of c, such as = 2, the results achieve balance between accurate text rendering and visual quality. A.5.2 Quantative Results on AMO with Different Overshooting Strength In the experiment section, we demonstrated that increasing the overshooting strength improves text rendering accuracy, with performance plateauing at 2 and occasionally declining for very large values of c. Here, we provide visual examples for 15 varying values of c, as shown in Figure 11. We observe that as increases significantly, the generated images tend to exhibit simpler structures and fewer details. This behavior is expected because the attention modulation applies soft overshooting strategy, where excessively large introduces over-smoothing artifacts. However, these artifacts are significantly mitigated compared to results generated without attention modulation. A.5.3 Additional Samples on Comparison between Euler and AMO We provide more results in Figure 12. showcasing the differences between the Euler sampler and our AMO method. Figure 12. Comparison of text rendering quality between Euler and AMO. Results are presented across three different text-to-image models: Flux, Stable Diffusion 3, and AuraFlow. All images are generated using the same random seed. In each pair of images, the left column shows the results from the Euler sampler, while the right column displays results generated by our AMO method. AMO consistently produces clearer and more legible text that aligns more closely with the given prompts, demonstrating its superiority in text rendering quality. 16 B. Additional Results on Comparison with Finetuned Text-to-Image Models We present sample images from the human evaluation study comparing TextDiffuser, GlyphControl, Euler, Overshooting, and AMO. These examples are shown in Figure 13. During the human evaluation, participants were presented with five images generated by the respective methods and asked to answer two questions: Question 1: Which of the following images exhibits the highest text rendering quality? (Multiple-choice) Question 2: Which of the following images demonstrates the best overall image quality? (Single-choice) Figure 13. Comparison of samples generated by different methods, including TextDiffuser, GlyphControl, Euler, Overshooting, and AMO. During the human evaluation, participants were shown five images for comparison."
        }
    ],
    "affiliations": [
        "Google",
        "University of Texas at Austin"
    ]
}