{
    "paper_title": "DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing",
    "authors": [
        "Dianyi Wang",
        "Ruihang Li",
        "Feng Han",
        "Chaofan Ma",
        "Wei Song",
        "Siyuan Wang",
        "Yibin Wang",
        "Yi Xin",
        "Hongjian Liu",
        "Zhixiong Zhang",
        "Shengyuan Ding",
        "Tianhang Wang",
        "Zhenglin Cheng",
        "Tao Lin",
        "Cheng Jin",
        "Kaicheng Yu",
        "Jingjing Chen",
        "Wenjie Wang",
        "Zhongyu Wei",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, a lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), a deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable 'think tokens' to provide the generative backbone with structured, reasoning-rich guidance. We further design a data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on a high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages a mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only ~50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research."
        },
        {
            "title": "Start",
            "content": "DeepGen 1.0: Lightweight Unified Multimodal Model for Advancing Image Generation and Editing Dianyi Wang1,2*, Ruihang Li1,3* Feng Han1,2*, Chaofan Ma4*, Wei Song1,5,6*, Siyuan Wang8*, Yibin Wang1,2*, Yi Xin1,7, Hongjian Liu3, Zhixiong Zhang1,4, Shengyuan Ding1,2, Tianhang Wang1,5, Zhenglin Cheng1,5,6, Tao Lin6, Cheng Jin2, Kaicheng Yu6, Jingjing Chen2, Wenjie Wang3, Zhongyu Wei1,2, Jiaqi Wang1 1Shanghai Innovation Institute, 2Fudan University, 3University of Science and Technology of China, 4Shanghai Jiao Tong University, 5Zhejiang University, 6Westlake University, 7Nanjing University, 8University of Southern California *Core Contributors, Project Leaders 6 2 0 2 2 1 ] . [ 1 5 0 2 2 1 . 2 0 6 2 : r Figure 1 Overview of DeepGen 1.0s visual generation and editing abilities, including reasoning-intensive scenarios."
        },
        {
            "title": "Abstract",
            "content": "Current unified multimodal models for image generation and editing typically rely on massive parameter scales (e.g., >10B), entailing prohibitive training costs and deployment footprints. In this work, we present DeepGen 1.0, lightweight 5B unified model that achieves comprehensive capabilities competitive with or surpassing much larger counterparts. To overcome the limitations of compact models in semantic understanding and fine-grained control, we introduce Stacked Channel Bridging (SCB), deep alignment framework that extracts hierarchical features from multiple VLM layers and fuses them with learnable think tokens to provide the generative backbone with structured, reasoning-rich guidance. We further design data-centric training strategy spanning three progressive stages: (1) Alignment Pre-training on large-scale image-text pairs and editing triplets to synchronize VLM and DiT representations, (2) Joint Supervised Fine-tuning on high-quality mixture of generation, editing, and reasoning tasks to foster omni-capabilities, and (3) Reinforcement Learning with MR-GRPO, which leverages mixture of reward functions and supervision signals, resulting in substantial gains in generation quality and alignment with human preferences, while maintaining stable training progress and avoiding visual artifacts. Despite being trained on only 50M samples, DeepGen 1.0 achieves leading performance across diverse benchmarks, surpassing the 80B HunyuanImage by 28% on WISE and the 27B Qwen-Image-Edit by 37% on UniREditBench. By open-sourcing our training code, weights, and datasets, we provide an efficient, high-performance alternative to democratize unified multimodal research. GitHub: https://github.com/DeepGenTeam/DeepGen HuggingFace: https://huggingface.co/DeepGenTeam/DeepGen-1.0 Datasets: https://huggingface.co/datasets/DeepGenTeam/DeepGen-1."
        },
        {
            "title": "Introduction",
            "content": "Advancing image generation and editing to handle increasingly complex instructions requires models that go beyond mere pixel synthesis to possess deep semantic understanding. To meet this demand, promising paradigm has emerged that integrates the comprehensive capabilities of vision-language models (VLMs) with the generative power of diffusion models, aiming to achieve semantically accurate generation and precise editing. Closed-source systems such as GPT-Image-1 [1] and Nano Banana [2] have validated this potential. In the open-source domain, recent wave of models, including BAGEL [3], HunyuanImage 3.0 [4], Qwen-Image [5], and LongCat-Image [6], has actively explored this direction to elevate generative performance through unified understanding. These advancements underscore the transformative impact of unified models in redefining the boundaries of visual generation. Despite this rapid progress, current high-performing unified models remain prohibitively expensive. Models such as Qwen-Image (27B), HunyuanImage 3.0 (80B), BAGEL (14B), and Emu3.5 (34B) all demand billions of training samples and massive computational resources. Many further require separate generation and editing models, doubling the total parameter count, e.g., pushing deployment footprints to total of 54B for Qwen-Image & Qwen-Image-Edit and 26B for LongCat-Image & LongCat-Image-Edit. While the need for lightweight alternatives is clear, existing small-scale unified models [7, 8, 9] have consistently underperformed across diverse tasks, thereby reinforcing common perception: compact models lack the capacity for comprehensive multimodal generation and editing. Interestingly, closer examination of recent benchmarks challenges this view: performance does not scale monotonically with model size. For example, as shown in Fig. 2, Lumina-DiMOO (8B) achieves generation score of 86.04 on DPG-Bench, surpassing the larger BAGEL (14B, 85.10). Similar patterns are observed across other benchmarks and evaluation dimensions (Table 1, 2, 3, 4, and 5). This indicates that, for unified multimodal models, larger scale alone does not necessarily guarantee stronger performance. Motivated by this observation, we argue that lightweight model, when empowered by synergistic architecture design and data-centric training strategies, can achieve comprehensive capabilities competitive with or even surpassing 2 Figure 2 Model performance comparison on image generation and editing benchmarks. Bubble size is proportional to model parameter count. Dashed outer rings indicate models with unreported parameter counts. Higher scores correspond to better performance. much larger counterparts. To substantiate this, we present DeepGen 1.0, compact framework with total of 5B parameters (3B VLM and 2B DiT) that integrates general generation, reasoning generation, text rendering, general editing, and reasoning editing within single model. Despite its compact size, DeepGen 1.0 achieves results competitive with or exceeding models 3 to 16 its size, as highlighted in Fig. 2. For instance, in general instruction following DPG-Bench, DeepGen 1.0 attains 87.90, eclipsing massive baselines like HunyuanImage 3.0 (86.10). Moving to reasoning-intensive tasks, it achieves 0.73 on WISE, outperforming the 80B HunyuanImage 3.0 (0.57) by remarkable 28% margin. Furthermore, on the editing front, it dominates the UniREditBench with 77.5, surpassing the dedicated 27B Qwen-Image-Edit (56.5) by over 37%. Across the board, DeepGen 1.0 demonstrates that intelligent design can triumph over raw scale. Remarkably, the entire training requires only 50M samples across simple three-stage pipeline, compared to 1.2B samples for LongCat-Image and 5B for HunyuanImage 3.0. To support these comprehensive capabilities within compact 5B budget, we introduce specialized architecture that maximizes VLM-DiT synergy. DeepGen 1.0 employs 3B VLM [10] as the understanding and reasoning backbone and 2B DiT [11] as the generative backbone. To align these two modules, we propose Stacked Channel Bridging (SCB). SCB first extracts hidden states from six uniformly distributed VLM layers (spanning low, mid, and high levels) to capture hierarchical features from visual and text inputs. To further enhance reasoning, we inject learnable think tokens that act as an implicit chain of thoughts. These multi-source features are then channel-wise concatenated and fused via lightweight connector into dense multimodal conditional sequence. Unlike prior methods that rely on the final VLM layer [5, 12] or use average pooling [13] that blurs fine-grained details, this design fully preserves both fine-grained visual details and high-level semantics, while providing the DiT with structured, reasoning-rich guidance. To fully unlock the potential of of DeepGen 1.0s compact architecture, we design data-centric training strategy tailored for tight VLM-DiT integration in the low-parameter regime. This strategy emphasizes simplicity and data efficiency across three progressively stages. First, in Alignment Pre-training, we optimize only the connector and learnable think tokens to align VLM representations with the DiTs latent space, utilizing large-scale image-text pairs and editing triplets. Second, during Joint Supervised Fine-tuning (SFT), we unfreeze the DiT and apply LoRA to the VLM for end-to-end optimization. We curate high-quality data mixture by integrating general generation and editing data, reasoning-based generation and editing data, and text-rendering data to foster omni-capabilities while preserving the VLMs inherent knowledge. Finally, 3 Figure 3 Overview of DeepGen 1.0 architecture. DeepGen 1.0 adopts unified VLM-DiT paradigm with dual-branch visual encoding strategy: ViT encoder captures high-level semantics for the VLM, while VAE encoder extracts compressed latent features for the DiT. Multimodal conditions derived from the VLM, together with reference-image VAE latents, are concatenated with the target images noise tokens to form single DiT input sequence, enabling self-attention over both conditioning and generation signals. Stacked channel bridging (SCB) performs deep feature fusion between the VLM and DiT to strengthen generation and editing, while DiT positional encodings explicitly distinguish reference tokens from target tokens. Icons shown at the right of each block indicate whether the corresponding module is frozen or trainable during the Pre-Training, SFT, and RL stages, respectively. we employ Reinforcement Learning (RL) to further align the model with human preferences. We adopt our novel MR-GRPO, with mixture of rewards and supervision signals, enhancing it with decoupled advantage normalization [14] to better preserve multi-reward granularity. To prevent capability degradation during RL, we introduce an auxiliary supervised diffusion loss, ensuring the model retains the broad capabilities acquired during the joint supervised fine-tuning stage. Our contributions are summarized as follows: We present DeepGen 1.0, compact 5B unified model that integrates general generation, reasoning, text rendering, and editing within single framework. Despite its small size, it achieves performance competitive with or surpassing models up to 16 larger (e.g., 80B), demonstrating that massive scaling is not the sole path to high-performance multimodal generation. We propose Stacked Channel Bridging (SCB), lightweight alignment module that fuses multi-layer VLM features via channel concatenation and shallow connector. Augmented with learnable think tokens, SCB enables deep semantic transfer from the VLM to the DiT while preserving fine-grained visual details, offering superior alternative to standard final-layer or average-pooling approaches. We design data-centric training strategy spanning three progressive stages: (1) alignment pre-training on large-scale pairs and triplets, (2) joint SFT on high-quality mixture of generation, reasoning, editing, and text rendering tasks, and (3) we propose MR-GRPO for RL alignment with auxiliary supervision and mixture of rewards , enabling stable preference optimization without capability degradation. We conduct comprehensive evaluations across diverse benchmarks, demonstrating leading performance among open-source models in reasoning-based generation and editing, while maintaining competitive general generation quality. 4 Figure 4 Overview of our training data for broad omni-capabilities and comprehensive evaluation across benchmarks. We publicly release the DeepGen 1.0 framework, including model weights, training and evaluation code, and key data components. By providing an efficient and high-performance alternative to resourceintensive large models, we aim to democratize unified multimodal research and empower broader community exploration."
        },
        {
            "title": "2 Model Architecture",
            "content": "DeepGen 1.0 follows VLM-DiT architecture as shown in Fig 3, where the VLM offers strong multimodal understanding with well cross-modal alignment and rich world knowledge to capture complex multimodal priors from both textual and visual inputs. The DiT serves as high-fidelity generation decoder guided by multimodal conditional inputs extracted from the VLM. We utilize Qwen-2.5-VL (3B) [10] as our pretrained VLM and SD3.5-Medium (2B) as our DiT, initialized from [11] with joint generation and editing capability. Feature alignment is achieved via streamlined connector module, which instantiates SigLIP visual encoder [15] followed by six transformer layers [16]. This compact design maintains total model size of approximately 5B parameters, striking an optimal balance between performance and computational efficiency. Stacked Channel Bridging (SCB) Prior unified multimodal models [5, 6, 12, 17] typically take the final-layer (or penultimate-layer) hidden states of VLM, transform them through connector, and use them as multimodal conditional input to the DiT. This design has two key limitations. First, the final VLM layers are heavily biased toward high-level semantic abstraction, often discarding fine-grained visual details that are critical for DiT modeling [18]. Second, relying on single layer makes the conditional signal vulnerable to layer-specific representation biases, which can hinder stable alignment and effective fusion between the VLM and DiT. An alternative line of work [3, 19, 20] performs deep fusion by introducing shared attention between the VLM and DiT at every layer. However, this approach substantially increases parameter scale and optimization complexity, making efficient and reliable training challenging. Subsequent works [13] aggregate hidden states from multiple VLM layers using average pooling. To more effectively and efficiently aggregate features from multiple VLM layers while preserving fine-grained information and enhancing reasoning, we propose the Stacked Channel Bridging (SCB) framework. SCB operates through three integrated steps: - Think Token Injection. While standard VLM representations provide rich interleaved multimodal signals [7, 21], explicit reasoning tokens can further act as implicit Chains of Thought (CoT). To strengthen the models reasoning capability, we first inject fixed set of learnable think tokens into the VLM input sequence. These tokens interact with textual and visual inputs across all layers via self-attention, progressively summarizing hidden representations and effectively extracting knowledge encoded in the VLM. 5 Table 1 Comparison of different models across general image generation and editing benchmarks. Top-1/2/3 results within each column excluding closed-source models are marked with gold, silver, and bronze icons. Model Params General T2I Generation GenEval DPGBench UniGenBench General Editing ImgEdit GEdit-EN Nano Banana GPT-Image-1 Seedream 4.0 FLUX.1 Kontext [Pro] Janus-Pro Show-o2 BLIP3-o MetaQuery-XL OmniGen2 UniWorld v1 BAGEL FLUX.1 [Dev] X-Omni 7B 7B 7B + 1.4B 7B+ 1.6B 3B + 4B 7B + 12B 14B 12B 7B + 12B Lumina-DiMOO 8B Mammoth2 8B + 3B + 2B LongCat-Image LongCat-Image-Edit Hunyuan-Image 3.0 Z-Image-Turbo Qwen-Image Qwen-Image-Edit [2509] GLM-Image DeepGen 1.0 (SFT) DeepGen 1.0 (RL) 7B + 6B 7B + 6B 80B 4B + 6B 7B + 20B 7B + 20B 9B + 7B 3B + 2B 3B + 2B Closed-source Models 0.75 0.84 0.84 85.23 85.20 88.25 Open-source Models 0.80 0.76 0.84 0.80 0.80 0.80 0.82 0.82 0.83 0.88 0. 0.87 0.72 0.84 0.87 0.86 0. 84.20 86.14 81.60 82.05 83.57 81.38 85.10 83.84 87.65 86.04 87.20 86.80 86.10 85. 88.32 84.78 87.05 87.90 87.45 92.77 87.30 75.84 61.61 62.73 59.87 63.09 63.11 61.53 69. 53.77 71.12 71.40 78. 74.18 75.74 4.35 4.20 4.18 4.00 3.43 3.26 3.20 4.06 4.50 4.35 4.09 4.14 7.54 7.53 7.68 6.56 6.41 4.85 6.52 6.60 7.60 7.54 7. 7.17 - Layer Selection. With the think tokens injected, we select multiple VLM hidden states to fuse, balancing performance and computational efficiency. Instead of relying on single layer, and following [22] which suggests that sparsely and uniformly distributed layers within VLMs provide effective representations for visual information, we select six hidden states sampled uniformly across the low-, mid-, and high-level layers. This ensures the capture of varying-granularity visual features and semantics, alongside the reasoning information embedded in the think token positions. - Feature Fusion. Finally, we integrate the selected multi-layer hidden states, which now encode both multimodal features and think token representations. Given set of selected VLM hidden states [ğ‘¥1, . . . , ğ‘¥ğ‘›] â„ğ¿ğ‘‘ where ğ‘› denotes the number of selected layers and ğ¿ is the sequence length (including think tokens), we first stack them along the channel dimension. This concatenated feature tensor in dimension ğ‘‘ is then projected to match the DiT input width using lightweight two-layer MLP. The aligned features are then fed into Transformer-encoder-based connector to deeply fuse information across layers, producing the final robust conditional input ğ‘ â„ğ¿ğ‘‘DiT: ğ‘ = Encoder(MLP(Concatch(ğ‘¥1, . . . , ğ‘¥ğ‘›))). (1)"
        },
        {
            "title": "3.1 Stage 1: Alignment Pre-Training",
            "content": "In the initial stage, we focus on establishing alignment between the VLM and the DiT. To achieve this, we train only the connector and 128 learnable think tokens while keeping all other model parameters frozen. 6 Table 2 Evaluation of reasoning-based text-to-image generation involving world knowledge on the WISE [23] benchmark. \"*\" denotes generation with textual CoT reasoning. Model Params Cultural Time Space Biology Physics Chemistry Overall GPT-Image-1 Seedream 4.0 Janus-Pro FLUX.1 [Dev] MetaQuery-XL BLIP3-o UniWorld-V1 OmniGen2 BAGEL* NextFlow-RL STAR Hunyuan-Image 3.0 Qwen-Image LongCat-Image 7B 12B 7B+ 1.6B 7B + 1.4B 7B + 12B 3B + 4B 14B 7B + 18B 7B 80B 7B + 20B 7B + 6B DeepGen 1.0 (SFT) DeepGen 1.0 (RL) 3B + 2B 3B + 2B 0.81 0.78 0.30 0.48 0.56 0.53 0.42 0.76 0.63 0.61 0.58 0.62 0. 0.70 0.72 Closed-source Models 0.71 0.73 0.89 0.85 Open-source Models 0.37 0.58 0.55 0.55 0.52 0.69 0.63 0.67 0.57 0.63 0.61 0.71 0.81 0.49 0.62 0.62 0.73 0.64 0.75 0.77 0.61 0.70 0.77 0. 0.82 0.70 0.83 0.79 0.36 0.42 0.49 0.45 0.43 0.65 0.58 0.74 0.56 0.57 0.66 0. 0.67 0.79 0.84 0.42 0.51 0.63 0.59 0.50 0.75 0.67 0.69 0.63 0.75 0.72 0.79 0. 0.74 0.67 0.26 0.35 0.41 0.41 0.34 0.58 0.39 0.66 0.31 0.40 0.49 0.65 0.66 0.80 0. 0.35 0.50 0.55 0.62 0.55 0.47 0.70 0.62 0.66 0.57 0.62 0.65 0.72 0.73 This phase utilizes general text-to-image generation and image editing tasks. Specifically, the model is trained for 200,000 iterations with the data details listed in Table 8. All images are generated at fixed resolution of 512 512. We utilize learning rate of 1 104 with 20,000 warm-up steps. For complete list of hyperparameters, please refer to Table 9 in Appendix A."
        },
        {
            "title": "3.2 Stage 2: Joint Supervised Fine-Tuning",
            "content": "In the second stage, we unfreeze the entire model and conduct joint VLM-DiT training, aiming to strengthen instruction-following capability and image synthesis quality with improved visual fidelity, semantic alignment, and knowledge-aware reasoning. To mitigate potential degradation of the VLMs multimodal comprehension during joint optimization, we apply LoRA [24] for efficient fine-tuning of the VLM. We train the model on diverse and high-quality mixture of tasks designed to foster omni abilities, including general text-to-image generation and editing, reasoning-based generation and editing, and text rendering. We perform supervised fine-tuning for 400,000 iterations on the multi-task dataset detailed in table 8. Images are trained at fixed resolution of 512512 while preserving the original aspect ratio via dynamic resizing. The model is optimized with learning rate of 5 105 with 20,000 warm-up steps. Detailed LoRA configurations and hyperparameters are provided in Table 9 of Appendix A. DeepGen 1.0 follows VLM-DiT architecture as shown in Fig 3, where the VLM offers strong multimodal understanding with well cross-modal alignment and rich world knowledge to capture complex multimodal priors from both textual and visual inputs. The DiT serves as high-fidelity generation decoder gudided by multimodal conditional inputs extracted from the VLM. We utilize Qwen-2.5-VL (3B) [10] as our pretrained VLM and SD3.5-Medium (2B) as our DiT, initialized from [11] with joint generationediting capability. Feature alignment is achieved via streamlined connector module, which instantiates SigLIP visual encoder [15] followed by six transformer layers [16]. This compact design maintains total model size of approximately 5B parameters, striking an optimal balance between performance and computational efficiency."
        },
        {
            "title": "3.3 Stage 3: Reinforcement Learning",
            "content": "To further improve generation quality and alignment with human preferences, we apply reinforcement learning after supervised fine-tuning. We propose the MR-GRPO framework, variant of Pref-GRPO [27], 7 Table 3 Evaluation of reasoning-based text-to-image generation with the philosophical framework on the T2ICoREBench [25] benchmark through Qwen3-VL-32B-Thinking [26]. \"*\" denotes generation with textual CoT reasoning. Model Params R-LR R-BR R-HR R-PR R-GR R-AR R-CR R-RR Overall Nano Banana GPT-Image-1 Seedream 4.0 Janus-Pro FLUX.1 [Dev] Show-o2 BLIP3-o OmniGen2 BAGEL* Hunyuan-Image 3.0 Qwen-Image Z-Image-Turbo 7B 12B 7B 7B + 1.4B 3B + 4B 14B 80B 7B + 20B 4B + 6B LongCat-Image 7B + 6B DeepGen 1.0 (SFT) 3B + 2B DeepGen 1.0 (RL) 3B + 2B 65.4 61.6 79.2 27.2 26.3 30.2 18.4 26.8 28.6 41.6 42.2 37.8 41. 38.8 38.5 Closed-source Models 59.7 52.0 51.4 57.2 58.1 52.9 88.3 89.9 89. Open-source Models 15.9 18.0 21.3 16.0 19.2 22.2 27.4 29.5 24.8 32.2 28.7 29. 28.0 25.9 29.4 19.0 32.9 24.8 42.3 40.0 37.8 38.4 40.2 41.2 25.4 66.8 59.7 44.6 64.1 66.2 76. 78.6 75.6 78.3 79.1 79.5 83.5 76.7 88.6 7.3 38.0 40.4 45.0 37.5 55.8 52. 47.9 46.0 72.6 51.5 51.9 84.1 82.4 80.1 30.8 59.7 54.7 51.1 56.5 59.5 52. 55.2 59.4 66.3 65.7 66.9 67.5 67.7 70.8 8.8 35.7 32.8 36.8 37.9 42.6 55. 59.0 49.6 55.8 42.0 45.6 58.7 47.5 42.8 4.6 18.1 13.1 12.3 13.6 29.3 20. 18.4 18.6 32.6 19.8 19.6 70.5 67.0 69.4 18.5 36.1 35.2 30.4 36.1 41.1 46. 46.3 43.7 52.2 45.7 46.5 which extends Group Relative Policy Optimization (GRPO) [28] to flow matching models by performing on-policy stochastic sampling and evaluating each generated image with mixture of pointwise and pairwise reward models. We further introduce novel auxiliary supervised diffusion loss that complements KL regularization to mitigate capability degradation during prolonged RL training. In addition, we validate and adopt two concurrent improvements into our pipeline: (1) noise-preserving stochastic sampling strategy [29] that produces cleaner samples and more accurate reward signals, and (2) decoupled advantage normalization scheme [14] that better preserves multi-reward signal granularity. Concretely, given text condition â„, the flow model samples group of ğº images {ğ‘¥ğ‘– denoising trajectories {ğ‘¥ğ‘– ğ‘‡ normalize each reward independently within each group before aggregation, following [14]: . For multi-reward optimization with reward functions {ğ‘…ğ‘˜}ğ¾ and the corresponding , we , . . . , ğ‘¥ğ‘– 0}ğº ğ‘–=1 0}ğº ğ‘–=1 , ğ‘¥ğ‘– ğ‘˜=1 ğ‘‡1 ğ´ğ‘– ğ‘˜ = ğ‘…ğ‘˜(ğ‘¥ğ‘– 0, â„) mean({ğ‘…ğ‘˜(ğ‘¥ ğ‘— 0, â„)}ğº ğ‘—= std({ğ‘…ğ‘˜(ğ‘¥ ğ‘— 0, â„)}ğº ğ‘—=1 ) ) , (2) and obtain the final advantage Ë†ğ´ğ‘– via weighted aggregation (cid:205)ğ‘˜ ğ‘¤ğ‘˜ ğ´ğ‘– across the training batch. The training objective is: ğ‘˜ followed by batch-wise normalization â„’GRPO(ğœƒ) = ğ”¼â„ğ’Ÿ (cid:34) 1 ğº ğº (cid:213) ğ‘–= 1 ğ‘‡ ğ‘‡1 (cid:213) (cid:16) ğ‘¡=0 (cid:16) min ğ‘¡ (ğœƒ) Ë†ğ´ğ‘– , clip(ğ‘Ÿ ğ‘– ğ‘Ÿ ğ‘– ğ‘¡ (ğœƒ), 1ğœ–, 1+ğœ–) Ë†ğ´ğ‘– (cid:17) ğ›½ ğ·KL(ğœ‹ğœƒğœ‹ref) (cid:35) (cid:17) , (3) ğ‘¡ (ğœƒ) = ğ‘ğœƒ(ğ‘¥ğ‘– where ğ‘Ÿ ğ‘– ğ‘¡ , â„) is the per-step importance ratio. We use 3 complementary reward functions to jointly optimize visual quality, text rendering accuracy, and semantic alignment; details on the reward design, stochastic sampler, and training configuration are deferred to Appendix B. ğ‘¡ , â„)(cid:14)ğ‘ğœƒold(ğ‘¥ğ‘– ğ‘¥ğ‘– ğ‘¡Î”ğ‘¡ ğ‘¡Î”ğ‘¡ ğ‘¥ğ‘– The KL-divergence regularization is computed in velocity space: ğ·KL(ğœ‹ğœƒğœ‹ref) = Ë†ğ‘£ğœƒ(ğ‘¥ğ‘¡ , ğ‘¡) Ë†ğ‘£ref(ğ‘¥ğ‘¡ , ğ‘¡)2. (4) While the KL penalty constrains the policy from drifting too far from the reference model, we observe that it alone is insufficient to prevent capability degradation as RL training scales beyond 1000 steps: the Figure 5 UniGenBench evaluation curves during RL training over 1,500 steps. The left axis shows the overall score and the right axis shows the text generation sub-score. Both metrics improve steadily throughout training, with the overall score rising from 0.747 to 0.756 and the text score increasing from 0.25 to 0.34, demonstrating that RL simultaneously enhances text rendering fidelity and general generation quality. model exhibits notable performance drop on tasks requiring complex instruction comprehension, such as reasoning-based generation. We attribute this to the fact that KL regularization only penalizes divergence from the reference policy without providing positive guidance toward high-quality generation. To this end, we introduce an auxiliary supervised diffusion loss â„’SFT computed on our high-quality SFT dataset, which continuously anchors the model to its supervised fine-tuning distribution. The overall training objective is: â„’total = (1 ğœ†) â„’GRPO + ğœ† â„’SFT, (5) where â„’SFT is the standard flow matching loss and ğœ† is small mixing coefficient. This formulation allows the model to optimize for reward signals via GRPO while retaining the generation capabilities acquired during supervised fine-tuning."
        },
        {
            "title": "4 Data",
            "content": "The overall composition of our training data is illustrated in Fig. 4. It combines real-world, synthetic, and carefully curated open-source datasets, covering broad spectrum of tasks including general generation and editing, reasoning-based generation and editing, text rendering, and application-oriented scenarios. General Generation Our pre-training corpus is sourced from several publicly available imagetext pair datasets, including text-to-image-2M [30], LAION-Aesthetic-6M [31], Megalith-10M [32], RedCaps-5M [33], and CC-12M [34]. For high-quality instruction fine-tuning, we curate mixture of open instruction-following datasets, including BLIP-3o (60k samples) [7], ShareGPT-4o-Image (45k samples) [35], Echo-4o-Image (100k samples) [36], and OpenGPT4o-Image (40k samples) [37]. These are combined with 10M in-house real samples spanning both longand short-form prompts (ratio 3:1). In addition, we synthesize approximately 50k high-clarity photorealistic images paired with fine-grained prompts using Nano Banana, further enriching detailed image generation covering both Chinese and English. General Editing For general image editing, we collect image-instruction-image triplets from variety of open-source datasets, including NHR-Edit [38] (720k samples), GPT-Image-Edit (1.5M samples) [39], ShareGPT-4o-Image-Edit set (50k samples) [35], OpenGPT4o-Image-Edit set (40k samples) [37], Nano-bananaconsist (150k samples) [40], Pico-Banana (250k samples) [41], X2I2 [12](1.6M samples) and Uniworld-Edit set [17](1.2M samples) together with 1.1M in-house editing samples covering both Chinese and English. Reasoning-based Generation and Editing We utilize reasoning generation and editing datasets (150k and 100k samples, respectively) from UniReason [42], covering five major knowledge domains: cultural commonsense, natural science, spatial, temporal and logical reasoning. Text Rendering and Application-oriented Scenarios To strengthen text rendering, we curate captions from documentand infographic-centric multimodal QA datasets [45]. Gemini 2.5 Pro [46] is used to stochastically compose diverse rendering attributes, e.g., font styles, layouts, and color schemes, and combine them with an Table 4 Evaluation of reasoning-based editing involving world knowledge on the RISE [43] and UniREditBench [44]. \"*\" denotes generation with textual CoT reasoning. Model Params Temporal Causal Spatial Logical Overall Real World Game World Overall RISE UniREditBench Nano Banana GPT-Image-1 Seedream 4.0 FLUX-Kontext-Pro FLUX.1-Kontext [Dev] OmniGen2 Lumina-DiMOO BAGEL* Qwen-Image edit [2509] 12B 3B + 4B 8B 14B 7B + 20B DeepGen 1.0 (SFT) DeepGen 1.0 (RL) 3B + 2B 3B + 2B 25.9 34.1 12.9 2.3 5.9 4.7 15.3 12.9 Closed-source Models 47.8 32.2 12.2 37.0 37.0 11.0 18.8 10.6 7.1 Open-source Models 5.5 17.8 10. 18.9 14.4 13.0 21.0 17.0 14. 13.0 1.2 1.2 2.4 4.7 2. 32.8 28.9 10.8 5.8 11.9 8.9 13.3 10. 75.2 81.0 66.2 45.0 53.7 51.4 56.8 71.0 74.3 73. 60.4 62.1 45.4 46.5 33.1 45.6 45.1 41.9 80.7 78. 68.3 73.4 55.8 45.8 43.4 48.5 51.0 56.5 77.5 75. open-source prompt set tailored for text rendering from [47]. Corresponding images are synthesized using Qwen-Image, resulting in 500k text-rendering samples. We further extend the corpus to application-oriented scenarios such as Chinese poetry generation and poster design, contributing an extra 60k samples. The detailed dataset usage in each stage is provided in Table 8 of Appendix A."
        },
        {
            "title": "5.1 Evaluation Setup\nGeneral Generation We assess general text-to-image generation using GenEval [48] to measure fundamental\nsemantic alignment, and DPG-Bench [49] to assess long-prompt instruction following. In addition, we adopt\nUniGenBench [27] for a comprehensive and fine-grained evaluation of general generation capability, covering\nten major categories (e.g., attribute binding, style control, and text rendering).\nReasoning Generation We evaluate world-knowledge reasoning-based generation on WISE [23], which\ncontains 1,000 prompts spanning cultural knowledge, natural science, and spatialâ€“temporal understanding. In\naddition, we adopt the T2I-CoREBench reasoning set [25], which covers eight reasoning categoriesâ€”Logical\n(R-LR), Behavioral (R-BR), Hypothetical (R-HR), Procedural (R-PR), Generalization (R-GR), Analogical (R-AR),\nCommonsense (R-CR), and Reconstructive (R-RR)â€”to assess reasoning generation under a structured,\nphilosophy-inspired taxonomy.",
            "content": "General Editing We evaluate general image editing on ImgEdit [50] and GEdit-EN [51]. These benchmarks assess core editing competencies, including instruction following, editing consistency and output quality. Reasoning Editing We evaluate world-knowledge reasoning-based image editing using UniREditBench [44] with 2,700 meticulously curated samples covering both realand game-world scenarios, and RISE [52] with 327 samples across temporal, causal, spatial, and logical dimensions. Text Rendering We evaluate text rendering performance on CVTG-2K [53], which focuses on English text generation across diverse real-world scenarios, including street scenes, advertisements, and memes."
        },
        {
            "title": "5.2 Model Performance",
            "content": "We compare DeepGen 1.0 against broad set of strong baselines, covering both closed-source and open-source models. Closed-source systems include GPT-Image-1 [54], the Nano Banana family (i.e., Gemini-2.5-FlashImage [2]), Seedream 4.0 [55], and FLUX.1 Kontext [Pro] [56]. Open-source baselines span advanced generation-only models such as FLUX.1 [Dev] [56] and Z-Image-Turbo [57], as well as state-of-the-art 10 Table 5 Evaluation of text rendering on the CVTG-2K [53]. Model Params Word Accuracy Closed-source Models NED CLIPScore Nano Banana Pro GPT-Image-1 Seedream 4.0 0.7788 0.8569 0.8451 Open-source Models FLUX.1 [dev] Z-Image-Turbo Hunyuan-Image 3.0 Qwen-Image LongCat-Image GLM-Image DeepGen 1.0 (SFT) DeepGen 1.0 (RL) 12B 4B + 6B 80B 7B + 20B 7B + 6B 9B + 7B 3B + 2B 3B + 2B 0.4965 0.8585 0.7650 0.8288 0. 0.9116 0.6605 0.7533 0.8754 0.9478 0.9224 0.6879 0. 0.8765 0.9116 0.9361 0.9557 0.8426 0.8936 0.7372 0.7982 0. 0.7401 0.8048 0.8121 0.8017 0.7859 0.7877 0. 0.8278 unified multimodal models supporting both multimodal understanding and image synthesis. These include autoregressive unified models (e.g., Janus-Pro [58]) and discrete diffusion-based approaches (e.g., Lumina-DiMOO [59]). Most unified models follow the VLMDiT paradigm, connecting VLMs with diffusion transformers via explicit connectors. Representative examples include BLIP-3o [7] and MetaQuery-XL [21], which use fixed set of learnable tokens to convey multimodal conditions to the DiT, as well as UniWorld-V1 [17], OmniGen2 [12], the Qwen-Image series [5], and LongCat-Image [6], which condition the DiT on single-layer VLM hidden states. In contrast, deep-fusion methods tightly couple VLMs and DiTs through shared attention within unified backbone, as exemplified by Hunyuan-Image-3.0 [4], BAGEL [3], and Show-o2 [60]. We further include models that autoregressively predicts discrete image tokens as conditions for subsequent DiT refinement, such as X-Omni [61], GLM-Image [62], NextFlow-RL [63], STAR [64], and Mammoth2 [13]. Notably, our DeepGen 1.0 remains highly lightweight, with only approximately 5B parameters, whereas most competing unified multimodal models operate at 7B parameters or more."
        },
        {
            "title": "5.2.1 Performance of General Generation and Editing",
            "content": "As shown in Table 1, DeepGen 1.0 achieves strong performanceefficiency trade-off. With only 5B parameters (3B+2B), it consistently matches or surpasses substantially larger unified multimodal baselines across wide range of general generation and editing benchmarks, ranking among the top three in all evaluated settings. Notably, DeepGen 1.0 unifies high-quality generation and editing within single model, rather than relying on separate specialized models. General Generation On GenEval [48], DeepGen 1.0 achieves 0.87, matching leading models such as QwenImage [5] and LongCat-Image [6] while using significantly fewer parameters and no external LLM-based prompt rewriting. On DPGBench [49], it scores 87.90, ranking second and demonstrating strong long-horizon instruction following ability. On the more comprehensive UniGenBench, DeepGen 1.0 achieves 75.74, again ranking second and outperforming many larger open-source baselines, including LongCat-Image [6], Z-Image-Turbo [57], and Hunyuan-Image 3.0 [4]. Despite using approximately 4 fewer parameters, it approaches open-source state-of-the-art performance. Overall, these results demonstrate DeepGen 1.0s robust semantic alignment, strong long-horizon instruction following for long prompts, and comprehensive fine-grained generation capabilities. General Editing On ImgEdit [50] and GEdit-EN [51], DeepGen 1.0 remains highly competitive, ranking third under RL. It outperforms strong unified baselines such as Mammoth2, BAGEL, and OmniGen2, while approaching the performance of larger, edit-specialized models (e.g., Qwen-Image-Edit and LongCat-ImageEdit). Across both generation and editing, RL consistently yields further performance gains. As the RL curve on UniGenBench visualized in Fig 5, RL simultaneously enhances the models general capabilities and text 11 Table 6 Ablation study of DeepGen 1.0 architecture. GenEval DPGBench GEdit-EN WISE RISE DeepGen 1.0 Settings w/o SCB w/o Think Tokens w/o Activate VLM 0.86 0.86 0.87 0. 87.05 85.55 86.35 86.74 7.12 6.75 7.02 6.93 0.72 0.70 0.68 0.71 13.3 12.6 11.7 12.9 rendering performance."
        },
        {
            "title": "5.2.2 Performance of Reasoning-based Generation and Editing",
            "content": "While maintaining strong general capabilities, DeepGen 1.0 exhibits advanced reasoning performance under compact 5B (3B+2B) parameter budget across both reasoning-based generation and editing benchmarks. Results for world-knowledge reasoning-based generation on WISE [23], T2I-CoREBench [25], and worldknowledge-grounded editing on RISE [52] and UniREditBench [44] are shown in Table 2, 3, and 4, respectively. Reasoning-based Generation On WISE, DeepGen 1.0 achieves the best performance (0.73) among open-source models, outperforming strong baselines such as BAGEL [3] (relying on explicit CoT for reasoning), LongCatImage [6], and STAR [64], while further narrowing the gap to closed-source systems (e.g., GPT-Image-1 [1] and Seedream 4.0 [55]). Improvements are consistent across diverse knowledge domains including cultural, temporal, spatial, and natural scientific reasoning, demonstrating DeepGen 1.0s effective use of world knowledge during generation. On T2I-CoREBench, DeepGen 1.0 attains 46.5, ranking among the top opensource models and matching or slightly surpassing substantially larger baselines such as Qwen-Image [5], Hunyuan-Image 3.0 [4], and Z-Image-Turbo [57]. This indicates broad coverage across diverse reasoning types, including logical, procedural, analogical, commonsense, and reconstructive reasoning. Reasoning-based Editing DeepGen 1.0 also demonstrates strong reasoning-based editing capability. On RISE, it achieves leading overall score 13.3 (ranked 1st) with SFT and remaining competitive under RL. On UniREditBench, it achieves 77.5 (SFT) and 75.7 (RL), significantly outperforming other open-source baselines and even exceeding the closed-source GPT-Image-1 overall. These results highlight DeepGen 1.0s robust world-knowledge-grounded editing across both real-world and game-world scenarios."
        },
        {
            "title": "5.2.3 Performance of Text Rendering",
            "content": "As shown in Table 5, DeepGen 1.0 exhibits strong text-rendering performance with only 5B parameters. RL training substantially improves Word Accuracy from 0.6605 to 0.7533, significantly enhancing character-level correctness and legibility. Meanwhile, DeepGen 1.0 preserves the highest CLIPScore (0.8278) among opensource models, indicating that improved textual fidelity does not compromise overall semantic alignment. These results validate that our RL stage effectively enhances precise text synthesis while maintaining strong instruction-level consistency."
        },
        {
            "title": "5.3.1 Architecture Design",
            "content": "We conduct ablation studies to quantify the contribution of key architectural components in DeepGen 1.0, by respectively implementing without applying: (1) stacked channel bridging, (2) think tokens, and (3) VLM activation. Results across benchmarks are shown in Table 6. Effect of SCB. Removing Stacked Channel Bridging (w/o SCB) consistently degrades performance across all benchmarks: DPGBench drops from 87.05 to 85.55, GEdit from 7.12 to 6.75, WISE from 0.72 to 0.70, and RISE from 13.3 to 12.6. This verifies that SCB effectively aggregates multiple-layer VLM features and mitigates information loss compared to single-layer conditioning, thereby providing higher-quality multimodal signals to the DiT for both generation and editing. Effect of Think Tokens. Removing the learnable think tokens (w/o Think Tokens) leads to the most pronounced regression on reasoning-intensive benchmarks: WISE decreases from 0.72 to 0.68 and RISE from 13.3 to 11.7. This suggests that think tokens serve as an implicit reasoning buffer that distills knowledge 12 Figure 6 Evaluation curves during training for ablation variants on UniGenBench. (a) Overall score showing the importance of auxiliary SFT loss for training stability. Without it, performance degrades after 300 steps and falls well below the starting point. (b) Text generation score demonstrating that all methods improve text rendering, but removing the SFT loss results in slower and less stable progress. from VLM representations, strengthening world-knowledge-driven generation and editing beyond what hidden-state conditioning alone. Effect of Activating the VLM. Disabling VLM activation (w/o Activate VLM) also harms performance (e.g., GenEval 0.85, GEdit 6.93, WISE 0.71, RISE 12.9), indicating that modest VLM fine-tuning improves alignment with the DiT and downstream tasks, yielding more robust generation, editing, and reasoning."
        },
        {
            "title": "5.3.2 RL Settings",
            "content": "To validate the contribution of each setting in our MR-GRPO framework, we conduct ablation studies by removing: (1) the auxiliary SFT loss, (2) the KL divergence regularization, and (3) the reward-wise advantage normalization. All variants are trained for 1,000 steps under identical configurations and evaluated on UniGenBench. Effect of Auxiliary SFT Loss. The auxiliary SFT loss is critical for maintaining generation quality during extended RL training. As shown in Figure 6(a), removing this loss leads to performance degradation after approximately 300 steps, eventually dropping well below the initial checkpoint by the end of training. Figure 6(b) further shows that text rendering improvement is also slower and more erratic without the SFT loss, lagging behind the baseline throughout most of training. This indicates that KL regularization alone is insufficient to anchor the model to its supervised fine-tuning distribution, and the SFT loss provides essential positive guidance that prevents capability drift and stabilizes learning across all objectives. Effect of KL Regularization. Removing KL regularization leads to lower UniGenBench overall score (75.07 vs. 75.69) and noticeable drop on DPGBench (87.32 vs. 87.75), as shown in Table 7. Figure 6(a) further reveals that the w/o KL variant lags behind the baseline throughout training, indicating that unconstrained policy updates can lead to forgetting of capabilities acquired during supervised fine-tuning. The combination of KL regularization and auxiliary SFT loss provides complementary constraints: KL penalizes divergence from the reference policy, while SFT loss provides positive guidance toward high-quality generation. Effect of Reward-wise Normalization. Normalizing advantages independently for each reward before aggregation stabilizes multi-reward optimization. As shown in Figure 6(a), replacing reward-wise normalization with joint normalization across all rewards yields comparable performance in the early stages but leads to growing gap after approximately 600 steps, with the final performance falling notably short of the baseline. Table 7 further shows significant drop in text generation score (32.18 vs. 35.06), suggesting that high-variance rewards can dominate the policy updates and impede progress on specific objectives when normalization is not applied per reward. 13 Table 7 Ablation study of RL training settings. All variants are trained for 1,000 steps and evaluated on generation (GenEval, DPGBench & UniGenBench) and editing (GEdit-EN). We individually remove the auxiliary SFT loss, velocity KL regularization and reward-wise advantage normalization from the full configuration. GenEval DPGBench GEdit-EN UniGenBench (Text) UniGenBench (Overall) DeepGen 1.0 (RL) w/o Auxiliary SFT Loss w/o Velocity KL w/o Reward-wise Norm 0.87 0.87 0.87 0.86 (-0.01) 87.75 87.40 (-0.35) 87.32 (-0.43) 87.73 (-0.02) 7.05 6.99 (-0.06) 7.02 (-0.03) 7.02 (-0.03) 35.06 33.33 (-1.73) 32.47 (-2.59) 32.18 (-2.88) 75.69 74.33 (-1.36) 75.07 (-0.62) 75.27 (-0.42)"
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present DeepGen 1.0, lightweight yet powerful unified multimodal model that seamlessly integrates image generation and editing within compact 5B parameter framework. By synergizing deep VLM-DiT alignment architecture with progressive, data-centric training strategy, we demonstrate that comprehensive omni-capabilities, spanning generation, reasoning, and editing, can be achieved without relying on massive parameter scaling or excessive computational resources. Extensive evaluations highlight that DeepGen 1.0 not only outperforms existing open-source models of similar size but also rivals substantially larger systems (e.g., 80B parameters), particularly in reasoning-intensive and instruction-following tasks. Beyond technical contributions, DeepGen 1.0 offers broader implications for sustainable AI. By decoupling high-quality generation from massive computational resources, it paves the way for accessible research on consumer-grade hardware. By open-sourcing DeepGen 1.0, we hope it serves as foundational step toward democratizing unified multimodal intelligence and inspiring new efficient architectures."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Introducing 4o image generation. introducing-4o-image-generation/, March 2025. OpenAI blog post. https://openai.com/index/ [2] Google. Introducing Gemini 2.5 Flash Image, our state-of-the-art image model. https://developers. googleblog.com/introducing-gemini-2-5-flash-image/, August 2025. [3] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining, 2025. URL https://arxiv.org/abs/2505.14683. [4] Siyu Cao, Hangting Chen, Peng Chen, YÄ³i Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, Tiankai Hang, Duojun Huang, Jie Jiang, Zhengkai Jiang, WeÄ³ie Kong, Changlin Li, Donghao Li, Junzhe Li, Xin Li, Yang Li, Zhenxi Li, Zhimin Li, Jiaxin Lin, Linus, Lucaz Liu, Shu Liu, Songtao Liu, Yu Liu, Yuhong Liu, Yanxin Long, Fanbin Lu, Qinglin Lu, Yuyang Peng, Yuanbo Peng, Xiangwei Shen, Yixuan Shi, Jiale Tao, Yangyu Tao, Qi Tian, Pengfei Wan, Chunyu Wang, Kai Wang, Lei Wang, Linqing Wang, Lucas Wang, Qixun Wang, Weiyan Wang, Hao Wen, Bing Wu, Jianbing Wu, Yue Wu, Senhao Xie, Fang Yang, Miles Yang, Xiaofeng Yang, Xuan Yang, Zhantao Yang, Jingmiao Yu, Zheng Yuan, Chao Zhang, Jian-Wei Zhang, Peizhen Zhang, Shi-Xue Zhang, Tao Zhang, Weigang Zhang, Yepeng Zhang, Yingfang Zhang, Zihao Zhang, ZÄ³ian Zhang, Penghao Zhao, Zhiyuan Zhao, Xuefei Zhe, Jianchen Zhu, and Zhao Zhong. Hunyuanimage 3.0 technical report, 2025. URL https://arxiv.org/abs/2509.23951. [5] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. URL https://arxiv.org/abs/2508.02324. [6] Meituan LongCat Team, Hanghang Ma, Haoxian Tan, Jiale Huang, Junqiang Wu, Jun-Yan He, Lishuai Gao, Songlin Xiao, Xiaoming Wei, Xiaoqi Ma, Xunliang Cai, Yayong Guan, and Jie Hu. Longcat-image technical report, 2025. URL https://arxiv.org/abs/2512.07584. [7] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, Le Xue, Caiming Xiong, and Ran Xu. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset, 2025. URL https://arxiv.org/abs/2505.09568. 14 [8] Jinheng Xie, WeÄ³ia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, ZhÄ³ie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2025. URL https://arxiv.org/abs/2408.12528. [9] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2025. URL https: //arxiv.org/abs/2411.07975. [10] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, ShÄ³ie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. [11] Hongyang Wei, Baixin Xu, Hongbo Liu, Size Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, Chuanxin Tang, Zidong Wang, Yichen Wei, Liang Hu, Boyi Jiang, Wei Li, Ying He, Yang Liu, Xuchen Song, Yangguang Li, and Yahui Zhou. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model, 2026. URL https://arxiv.org/abs/2509.04548. [12] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation, 2025. URL https://arxiv.org/abs/2506.18871. [13] Tao Shen, Xin Wan, Taicai Chen, Rui Zhang, Junwen Pan, Dawei Lu, Fanding Lei, Zhilin Lu, Yunfei Yang, Chen Cheng, Qi She, Chang Liu, and Zhenbang Sun. Mammothmoda2: unified ar-diffusion framework for multimodal understanding and generation, 2025. URL https://arxiv.org/abs/2511.18262. [14] Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, YuChiang Frank Wang, Kwang-Ting Cheng, et al. Gdpo: Group reward-decoupled normalization policy optimization for multi-reward rl optimization. arXiv preprint arXiv:2601.05242, 2026. [15] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. URL https://arxiv.org/abs/2303.15343. [16] Size Wu, Zhonghua Wu, Zerui Gong, Qingyi Tao, Sheng Jin, Qinyue Li, Wei Li, and Chen Change Loy. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025. [17] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, Yatian Pang, and Li Yuan. Uniworld-v1: High-resolution semantic encoders for unified visual understanding and generation, 2025. URL https://arxiv.org/abs/2506.03147. [18] Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, and Ajinkya Kale. Unifusion: Vision-language model as unified encoder in image generation, 2025. URL https://arxiv.org/abs/2510.12789. [19] Zeyu Wang, Zilong Chen, Chenhui Gou, Feng Li, Chaorui Deng, Deyao Zhu, Kunchang Li, Weihao Yu, Haoqin Tu, Haoqi Fan, and Cihang Xie. Lightfusion: light-weighted, double fusion framework for unified multimodal understanding and generation, 2025. URL https://arxiv.org/abs/2510.22946. [20] WeÄ³ia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation, 2025. URL https://arxiv.org/abs/2412. 15188. [21] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [22] Siyuan Wang, Dianyi Wang, Chengxing Zhou, Zejun Li, Zhihao Fan, Xuan-Jing Huang, and Zhongyu Wei. Activating distributed visual region within llms for efficient and effective vision-language training and inference. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3071530727, 2025. [23] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, and Li Yuan. Wise: world knowledge-informed semantic evaluation for text-to-image generation, 2025. URL https://arxiv.org/abs/2503.07265. 15 [24] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. [25] Ouxiang Li, Yuan Wang, Xinting Hu, HuÄ³uan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Xiaojuan Qi, and Fuli Feng. Easier painting than thinking: Can text-to-image models set the stage, but not direct the play?, 2025. URL https://arxiv.org/abs/2509.03516. [26] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. URL https://arxiv.org/abs/2511.21631. [27] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for stable text-to-image reinforcement learning, 2025. URL https://arxiv.org/abs/2508.20751. [28] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [29] Feng Wang and Zihao Yu. Coefficients-preserving sampling for reinforcement learning with flow matching. arXiv preprint arXiv:2509.05952, 2025. [30] Jacky He and contributors. text-to-image-2M: high-quality, diverse textimage training dataset. https:// huggingface.co/datasets/jackyhate/text-to-image-2M, 2024. Hugging Face dataset. [31] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. URL https://arxiv.org/abs/2210.08402. [32] Ollin Matsubara and Draw Things AI Team. Megalith-10M: dataset of 10 million public-domain photographs. https://huggingface.co/datasets/madebyollin/megalith-10m, 2024. Hugging Face dataset. CC0/FlickrCommons images; Florence-2 captions available in the megalith-10m-florence2 variant. [33] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: web-curated image-text data created by the people, for the people, 2021. URL https://arxiv.org/abs/2111.11431. [34] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts, 2021. URL https://arxiv.org/abs/2102.08981. [35] Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation, 2025. URL https: //arxiv.org/abs/2506.18095. [36] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, and WeÄ³ia Li. Echo-4o: Harnessing the power of gpt-4o synthetic images for improved image generation, 2025. URL https://arxiv.org/abs/2508.09987. [37] Zhihong Chen, Xuehai Bai, Yang Shi, Chaoyou Fu, Huanyu Zhang, Haotian Wang, Xiaoyan Sun, Zhang Zhang, Liang Wang, Yuanxing Zhang, Pengfei Wan, and Yi-Fan Zhang. Opengpt-4o-image: comprehensive dataset for advanced image generation and editing, 2025. URL https://arxiv.org/abs/2509.24900. [38] Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, and Aleksandr Gordeev. Nohumansrequired: Autonomous high-quality image editing triplet mining, 2025. URL https://arxiv.org/abs/2507.14119. [39] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit-1.5m: million-scale, gpt-generated image dataset, 2025. URL https://arxiv.org/abs/2507.21033. [40] Nano-banana-150k. https://github.com/yejy53/Nano-banana-150k, 2024. GitHub repository. 16 [41] Yusu Qian, Eli Bocek-Rivele, Liangchen Song, Jialing Tong, Yinfei Yang, Jiasen Lu, Wenze Hu, and Zhe Gan. Pico-banana-400k: large-scale dataset for text-guided image editing, 2025. URL https://arxiv.org/abs/ 2510.19808. [42] Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang, Siyuan Wang, Zhongyu Wei, et al. Unireason 1.0: unified reasoning framework for world knowledge aligned image generation and editing. arXiv preprint arXiv:2602.02437, 2026. [43] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, and Haodong Duan. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing, 2025. URL https://arxiv.org/abs/2504.02826. [44] Feng Han, Yibin Wang, Chenglin Li, Zheming Liang, Dianyi Wang, Yang Jiao, Zhipeng Wei, Chao Gong, Cheng Jin, Jingjing Chen, and Jiaqi Wang. Unireditbench: unified reasoning-based image editing benchmark, 2025. URL https://arxiv.org/abs/2511.01295. [45] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Didi Zhu, Chunsheng Wu, Huajie Tan, Chunyuan Li, Jing Yang, Jie Yu, Xiyao Wang, Bin Qin, Yumeng Wang, Zizhen Yan, Ziyong Feng, Ziwei Liu, Bo Li, and Jiankang Deng. Llava-onevision-1.5: Fully open framework for democratized multimodal training, 2025. URL https://arxiv.org/abs/2509.23661. [46] Google. Gemini 2.5 pro. https://deepmind.google/models/gemini/pro/, 2025. [47] Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, and Hongsheng Li. Flux-reason-6m & prism-bench: million-scale text-to-image reasoning dataset and comprehensive benchmark, 2025. URL https://arxiv.org/abs/2509.09680. [48] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/2310.11513. [49] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. URL https://arxiv.org/abs/2403.05135. [50] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark, 2025. URL https://arxiv.org/abs/2505.20275. [51] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing, 2025. URL https://arxiv.org/abs/2504.17761. [52] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models, 2025. URL https://arxiv.org/abs/2505.16707. [53] Nikai Du, Zhennan Chen, Shan Gao, Zhizhou Chen, Xi Chen, Zhengkai Jiang, Jian Yang, and Ying Tai. Textcrafter: Accurately rendering multiple texts in complex visual scenes, 2025. URL https://arxiv.org/abs/2503.23461. [54] OpenAI. Gpt-image-1, 2025. URL https://openai.com/index/introducing-4o-image-generation/. Accessed: 2025. [55] ByteDance. Seedream 4.0, 2025. URL https://seed.bytedance.com/en/seedream4_0. Accessed: 2025-08. [56] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. Preprint. [57] Image Team, Huanqia Cai, Sihan Cao, Ruoyi Du, Peng Gao, Steven Hoi, Zhaohui Hou, ShÄ³ie Huang, Dengyang Jiang, Xin Jin, Liangchen Li, Zhen Li, Zhong-Yu Li, David Liu, Dongyang Liu, Junhan Shi, Qilong Wu, Feng Yu, Chi Zhang, Shifeng Zhang, and Shilin Zhou. Z-image: An efficient image generation foundation model with single-stream diffusion transformer, 2025. URL https://arxiv.org/abs/2511.22699. [58] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling, 2025. URL https: //arxiv.org/abs/2501.17811. 17 [59] Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, Jinbin Bai, Qian Yu, Dengyang Jiang, Yuandong Pu, Haoxing Chen, Le Zhuo, Junjun He, Gen Luo, Tianbin Li, Ming Hu, Jin Ye, Shenglong Ye, Bo Zhang, Chang Xu, Wenhai Wang, Hongsheng Li, Guangtao Zhai, Tianfan Xue, Bin Fu, Xiaohong Liu, Yu Qiao, and Yihao Liu. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding, 2025. URL https://arxiv.org/abs/2510.06308. [60] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models, 2025. URL https://arxiv.org/abs/2506.15564. [61] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. [62] Z.ai Team. Glm-image: Auto-regressive for dense-knowledge and high-fidelity image generation, jan 2026. URL https://z.ai/blog/glm-image. [63] Huichao Zhang, Liao Qu, Yiheng Liu, Hang Chen, Yangyang Song, Yongsheng Dong, Shikun Sun, Xian Li, Xu Wang, Yi Jiang, Hu Ye, Bo Chen, Yiming Gao, Peng Liu, Akide Liu, Zhipeng Yang, Qili Deng, Linjie Xing, Jiyang Liu, Zhao Wang, Yang Zhou, Mingcong Liu, Yi Zhang, Qian He, Xiwei Hu, Zhongqi Qi, Jie Shao, Zhiye Fu, Shuai Wang, Fangmin Chen, Xuezhi Chai, Zhihua Wu, Yitong Wang, Zehuan Yuan, Daniel K. Du, and Xinglong Wu. Nextflow: Unified sequential modeling activates multimodal understanding and generation, 2026. URL https://arxiv.org/abs/2601.02204. [64] Jie Qin, Jiancheng Huang, Limeng Qiao, and Lin Ma. Star: Stacked autoregressive scheme for unified multimodal learning, 2025. URL https://arxiv.org/abs/2512.13752. [65] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. [66] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. [67] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. [68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021."
        },
        {
            "title": "Appendix",
            "content": "A Pre-Training & SFT Details Table 8 and 9 provide the details of dataset usage and hyperparameter configurations at each stage, respectively. Table 8 The data details used in Pre-Training and Supervised Fine-Tuning stages. \"\" denotes covering both Chinese and English prompts. Stage Task Data source Pre-Training General Generation General Editing General Generation Supervised Fine-Tuning General Editing Reasoning Generation Reasoning Editing Text Rendering text-to-image-2M [30], LAION-Aesthetic-6M [31], Megalith-10M [32], RedCaps-5M [33], CC-12M [34] NHR-Edit [38], GPT-Image-Edit [39], ShareGPT-4o-Image-Edit [35], OpenGPT4o-Image-Edit [37], Nano-banana-consist [40], Pico-banana [41], X2I2 [12], UniWorld-Edit set [17], in-house editing data BLIP-3o [7], ShareGPT-4o-Image [35], Echo-4o-Image [36], OpenGPT4o-Image [37], Self-Banana-50K, in-house generation data NHR-Edit [38], GPT-Image-Edit [39], ShareGPT-4o-Image-Edit [35], OpenGPT4o-Image-Edit [37], Nano-banana-consist [40], Pico-banana [41], X2I2 [12], UniWorld-Edit set [17], in-house editing data UniReason-T2I set [42] UniReason-Edit set [42] General text rendering, poster design, Chinese poem Size 35M 6.6M 11M 6.6M 150K 100K 560K Table 9 Detailed Hyperparameters and Configurations of the Pre-Training and Supervised Fine-Tuning. Hyperparameters Learning Rate LR Scheduler Weight Decay Gradient Norm Clip Optimizer warmup ratio Batch Size Training GPUs Gen. Resolution Arbitrary Resolution Trainable Param LoRA Rank LoRA ğ›¼ LoRA Dropout Stage-I (Pre-Training) 1.0 104 Cosine 0.05 1.0 AdamW 0.01 512 64H200 Stage-II (Supervised Fine-Tuning) 5.0 105 Cosine 0.05 1.0 AdamW 0.01 768 64H200 512 SCB connector - - - 512 SCB connector, DiT, LoRA in VLM 64 128 0."
        },
        {
            "title": "B Reinforcement Learning Details",
            "content": "Noise-Preserving Stochastic Sampling. When sampling trajectories, the deterministic flow-matching ODE ğ‘‘ğ‘¥ğ‘¡ = Ë†ğ‘£ğœƒ(ğ‘¥ğ‘¡ , ğ‘¡) ğ‘‘ğ‘¡ is unsuitable for the exploration required by reinforcement learning. Prior works [65, 66] convert it into stochastic differential equation (SDE) to introduce randomness. However, the standard Flow-SDE formulation injects noise that exceeds the schedulers expected noise level at each timestep, degrading sample quality and producing inaccurate reward signals. We instead adopt noise-preserving 19 stochastic sampling strategy [29] that ensures the noise level remains consistent with the flow matching scheduler at every timestep: ğ‘¥ğ‘¡Î”ğ‘¡ = (cid:0)1 (ğ‘¡ Î”ğ‘¡)(cid:1) Ë†ğ‘¥0 + (ğ‘¡ Î”ğ‘¡) cos(cid:0) ğœ‚ğœ‹ 2 (cid:1) Ë†ğ‘¥1 + (ğ‘¡ Î”ğ‘¡) sin(cid:0) ğœ‚ğœ‹ (cid:1) ğœ–, (6) where Ë†ğ‘¥0 = ğ‘¥ğ‘¡ ğ‘¡ Ë†ğ‘£ğœƒ and Ë†ğ‘¥1 = ğ‘¥ğ‘¡ + (1ğ‘¡) Ë†ğ‘£ğœƒ are the predicted clean sample and noise respectively, ğœ– ğ’© (0, ğ¼) is freshly sampled Gaussian noise, and ğœ‚ [0, 1] controls the stochasticity strength. The log-probability for computing importance ratios is simplified as [29]: log ğ‘ğœƒ(ğ‘¥ğ‘¡Î”ğ‘¡ ğ‘¥ğ‘¡) = ğ‘¥ğ‘¡Î”ğ‘¡ ğœ‡ğœƒ(ğ‘¥ğ‘¡ , ğ‘¡)2, (7) where ğœ‡ğœƒ(ğ‘¥ğ‘¡ , ğ‘¡) = (1(ğ‘¡ Î”ğ‘¡)) Ë†ğ‘¥0 + (ğ‘¡ Î”ğ‘¡) cos(cid:0) ğœ‚ğœ‹ (cid:1) Ë†ğ‘¥1 is the deterministic component of the sampling step. 2 This formulation removes the variance normalization term present in the standard log-probability, avoiding numerical instability at small noise levels. Reward Functions. We employ three reward functions to provide complementary training signals. (1) VLM-based pairwise preference reward [27] that evaluates image-text alignment and visual quality by comparing all generated images within each group and computing per-sample win rates as reward scores. (2) An OCR reward [67] that measures text rendering accuracy by detecting rendered text in the generated image and comparing it against the target text specified in the prompt. (3) CLIP similarity score [68] that captures overall semantic consistency between the generated image and the text condition. Each prompt category is assigned different reward composition: text-rendering prompts are weighted toward the OCR reward, while general text-to-image prompts prioritize the preference reward. The detailed reward weights are provided in Table 11. Training Details. The RL training prompts are drawn from two categories: general text-to-image prompts and text-rendering prompts. The auxiliary SFT data is sampled from an independent curated corpus of high-quality image-text pairs covering both general generation and text rendering. Dataset details are provided below. We train with group size of ğº = 8, generating images at 512 512 resolution using 50 denoising steps. The model is optimized with learning rate of 2 106 for 1,500 steps. The complete set of hyperparameters is listed in Table 10. Hyperparameters. Table 10 summarizes the full set of hyperparameters used for RL training. Table 10 Hyperparameters for reinforcement learning training. Hyperparameter Value Group size ğº Image resolution Denoising steps SDE stochasticity ğœ‚ Timestep fraction Learning rate Total training steps KL coefficient ğ›½ Clip range ğœ– SFT auxiliary coefficient ğœ† SFT auxiliary frequency Global batch size DeepSpeed stage Precision 8 512 512 50 1.0 0.6 2 106 1,500 5 107 1 104 1 104 Every step 256 ZeRO-2 BF16 Reward Weights. Table 11 shows the per-category reward weight configuration. Text-rendering prompts are weighted toward the OCR reward to directly optimize text accuracy, while general text-to-image prompts rely primarily on the VLM-based preference reward for holistic quality assessment. 20 Table 11 Reward weight configuration by prompt category. Prompt Category Preference CLIP Sim OCR Text rendering General T2I 0.2 0.7 0.1 0.3 0.7 RL Training Prompts. The RL training prompts consist of two categories with proportional sampling. Text-rendering prompts (sample weight 3.0) are drawn from UniGenBench text data, Qwen-Image text rendering captions, and curated text rendering prompts. General text-to-image prompts (sample weight 1.0) are sourced from UniGenBench general data, BLIP3-o captions, ShareGPT-4o image descriptions, and CoREBench prompts. Auxiliary SFT Data. The auxiliary supervised data for computing â„’SFT is drawn from an independent corpus of high-quality image-text pairs. This corpus includes general text-to-image pairs (from BLIP3-o, ShareGPT-4o, Echo-4o, OpenGPT-4o, GenEval, and Self-Banana-50K collections) with sample weight 1.0, and text rendering pairs with sample weight 3.0 to match the emphasis on text rendering in the RL prompts."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanjing University",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "University of Science and Technology of China",
        "University of Southern California",
        "Westlake University",
        "Zhejiang University"
    ]
}