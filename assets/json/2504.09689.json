{
    "paper_title": "EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
    "authors": [
        "Jiahao Qiu",
        "Yinghui He",
        "Xinzhe Juan",
        "Yiming Wang",
        "Yuhan Liu",
        "Zixin Yao",
        "Yue Wu",
        "Xun Jiang",
        "Ling Yang",
        "Mengdi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 9 8 6 9 0 . 4 0 5 2 : r EMOAGENT: ASSESSING AND SAFEGUARDING HUMAN-AI INTERACTION FOR MENTAL HEALTH SAFETY Jiahao Qiu1, Yinghui He2, Xinzhe Juan3, Yiming Wang4, Yuhan Liu2, Zixin Yao5, Yue Wu6, Xun Jiang7,8, Ling Yang1,6, and Mengdi Wang1 1Department of Electrical & Computer Engineering, Princeton University 2Department of Computer Science, Princeton University 3Department of Computer Science & Engineering, University of Michigan 5Department of Philosophy, Columbia University 4Department of Data Science & Engineering, University of Michigan 6AI Lab, Princeton University 7Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute 8Theta Health Inc."
        },
        {
            "title": "ABSTRACT",
            "content": "The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/ 1akaman/EmoAgent."
        },
        {
            "title": "Introduction",
            "content": "The rapid rise of large language models and conversational AI [Wang et al., 2024a], such as Character.AI1, has opened new frontiers for interactive AI applications. These AI characters excel in role-playing, fostering deep, emotionally engaging dialogues. As result, many individuals, including those experiencing mental health challenges, seek emotional support from these AI companions. While LLM-based chatbots show promise in mental health support [van der Schyff et al., 2023, Chin et al., 2023, Zhang et al., 2024a], they are not explicitly designed for therapeutic use. Character-based agents often fail to uphold essential safety principles for mental health support [Zhang et al., 2024b, Cyberbullying Research Center, 2024], sometimes responding inappropriately or even harmfully to users in distress [Brown and Halpern, 2021, De Freitas et al., 2024, Gabriel et al., 2024]. In some cases, they may even exacerbate users distress, particularly during pessimistic, morbid, or suicidal conversations. In October 2024, tragic incident raised public concern about risks of AI chatbots in mental health contexts. 14-year-old boy from Florida committed suicide after engaging in extensive conversations with an AI chatbot on Character.AI. He had developed deep emotional connection with chatbot modeled after \"Game of Thrones\" character. The interactions reportedly included discussions about his suicidal thoughts, with the chatbot allegedly *These authors contributed equally to this work. 1https://character.ai/ EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Figure 1: Overview of EmoAgent Framework for Human-AI Interaction. EmoAgent, which consists of two main components: EmoEval and EmoGuard, helps guide human-AI interaction, evaluating users psychological conditions and providing advisory responses. EmoEval assesses psychological states such as depression, delusion, and psychosis, while EmoGuard mitigates mental risks by providing advice regarding emotion, thought, and dialogue through iterative training on analysis from EmoEval and chat history. encouraging these feelings and even suggesting harmful actions. This case underscores the critical need for robust safety measures in AI-driven platforms, especially those accessed by vulnerable individuals. This tragedy has heightened awareness of the risks of AI unintentionally exacerbating harmful behaviors in individuals with mental health challenges [Patel and Hussain, 2024]. However, research on the psychosocial risks of human-AI interactions remains severely limited. In this paper, we seek to develop AI-native solutions to protect human-AI interactions and mitigate psychosocial risks. This requires systematic assessment of AI-induced emotional distress and agent-level safeguards to detect and intervene in harmful interactions. As character-based AI becomes more immersive, balancing engagement with safety is crucial to ensuring AI remains supportive rather than harmful tool. We present EmoAgent, multi-agent AI framework designed to systematically evaluate conversational AI systems for risks associated with inducing psychological distress. Acting as plug-and-play intermediary during human-AI interactions, EmoAgent identifies potential mental health risks and facilitates both safety assessments and risk mitigation strategies. EmoAgent features two major functions: EmoEval: EmoEval is an agentic evaluation tool that assesses any conversational AI systems risk of inducing mental stress, as illustrated by Figure 2. It features virtual human user that integrates cognitive models [Beck, 2020] for mental health disorders (depression, psychosis, delusion) and conducts evaluations through large-scale simulated human-AI conversations. EmoEval measures the virtual users mental health impacts using clinically validated tools: the Patient Health Questionnaire (PHQ-9) for depression [Kroenke et al., 2001], the Peters et al. Delusions Inventory (PDI) for delusion [Peters et al., 2004], and the Positive and Negative Syndrome Scale (PANSS) for psychosis [Kay et al., 1987]. EmoGuard: framework of real-time safeguard agents that can be integrated as an intermediary layer between users and AI systems, in plug-and-play manner. EmoGuard monitors human users mental status, predicts potential harm, and delivers corrective feedback to the AI systems, providing dynamic in-conversation interventions beyond traditional safety measures. Through extensive experiments, we observe that some popular character-based chatbots can cause distress, particularly when engaging with vulnerable users on sensitive topics. Specifically, in more than 34.4% of simulations, we observed deterioration in mental state. To mitigate such risk, EmoGuard actively monitors users mental status and conducts proactive interviews during conversations, significantly reducing deterioration rates. These results provide actionable insights for developing safer, character-based conversational AI systems that maintain character fidelity. 2 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Figure 2: Overview of EmoEval for Evaluating Mental Safety of AI-human Interactions. The simulation consists of four steps: (1) User Agent Initialization & Initial Test, where cognitive model and an LLM initialize the user agent, followed by an initial mental health test; (2) Chats with Character-based Agent, where the user agent engages in conversations with character-based agent portrayed by the tested LLM, while dialog manager verifies the validity of interactions and refines responses if necessary; (3) Final Test, where the user agent completes final mental health test; and (4) Data Processing & Analysis, where initial and final mental health test results are processed and analyzed, chat histories of cases where depression deepening occurs are examined to identify contributing factors, and Safeguard agent uses the insights for iterative improvement."
        },
        {
            "title": "2 Related Works",
            "content": "AI Chatbots for Mental Health Support. AI-driven, especially LLM-based chatbots, have been widely deployed as mental health support aids [Casu et al., 2024, Habicht et al., 2024, Sin, 2024, Yu and McGuinness, 2024, Oghenekaro and Okoro, 2024], yet concerns remain about their reliability and safety [Saeidnia et al., 2024, De Freitas et al., 2024, Torous and Blease, 2024, Kalam et al., 2024]. AI chatbots are incompetent in detecting and appropriately responding to user distress [De Freitas et al., 2024, Patel and Hussain, 2024], reasoning about users mental states [He et al., 2023], conducting empathetic communication with certain patient groups [Gabriel et al., 2024], and treating socially marginalized patients inclusively [Brown and Halpern, 2021]. line of work proposed safety metrics and benchmarks for evaluating AI for mental health [Park et al., 2024, Chen et al., 2024a, Sabour et al., 2024, Li et al., 2024a, Sabour et al., 2024]. Nonetheless, there has been less attention to the safety issues of character-based agents in role-playing context. We aim to fill this gap by comprehensively investigating the potential mental harm aroused by character-based agents. Simulating AI-User Interactions. Simulated interactions between AI agents and users provide controlled environment to assess AI-generated responses [Akhavan and Jalali, 2024] as well as lens into complex social systems [Gürcan, 2024]. The evaluation of AI behavior in social contexts has widely adopted multi-agent simulations [Li et al., 2023, Park et al., 2023], especially through role-playing and cooperative tasks [Dai et al., 2024, Rasal, 2024, Chen et al., 2024b, Zhu et al., 2024, Louie et al., 2024, Wang et al., 2023a]. On top of prior advances in generative agentic frameworks [Wu et al., 2023] which enable more human-like simulation, recent works propose various methods to enhance the fidelity and authenticity of AI-user simulation, integrating interactive learning [Wang et al., 2024b], expert-driven constraints [Wang et al., 2024c, Louie et al., 2024], and long-context models [Tang et al., 2025]. In addition, simulation has been widely used to explore trade-offs and inform both design decisions [Ren and Kraut, 2010, 2014] and decision-making [Liu et al., 2024a]. By enabling ethical and risk-free experimentation without involving human subjects, it reduces both ethical concerns and costs [Park et al., 2022]. These advantages make simulation valuable tool for investigating mental health problems, where real-world experimentation may pose ethical risks or unintended psychological harm [Liu et al., 2024b]. For example, prior work has explored using user-simulated chatbots to train amateur and professional counselors in identifying risky behaviors before they conduct therapy sessions with real individuals [Sun et al., 2022, Cho et al., 2023, Wang et al., 2024c]. Our EmoEval pipeline builds on this approach. Safety Alignment Strategies. LLMs can be vulnerable to jailbreaking [Yu et al., 2024, Li et al., 2024b, Luo et al., 2024]. LLM-based chatbots undergone jailbreak attacks have exhibited fidelity breakdown [Wang et al., 2023b, Johnson, 3 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Figure 3: Overview of EmoGuard for Safeguarding Human-AI Interactions. Every fixed number of rounds of conversation, three components of the Safeguard Agent, the Emotion Watcher, Thought Refiner, and Dialog Guide, collaboratively analyze the chat with the latest profile. The Manager of the Safeguard Agent then synthesizes their outputs and provides advice to the character-based agent. After the conversation, the user agent undergoes mental health assessment. If the mental health condition deteriorates over threshold, the chat history is analyzed to identify potential causes by the Update System. With all historical profiles and potential causes, the Update System further improves the profile of the safeguard agent, completing the iterative training process. 2024], defense breakdown on implicit malicious queries [Chang et al., 2024], and harmful responses for benign query [Zhang et al., 2024c]. Correspondingly, line of work explored safety alignment strategies to tackle jailbreak attacks [Chu et al., 2024, Xu et al., 2024, Zeng et al., 2024, Wang et al., 2024d, Zhou et al., 2024, Xiong et al., 2024, Liu et al., 2024c, Peng et al., 2024, Wang et al., 2024e]. However, few works have focused on LLM safety concerns under emotional alignment constraints. EmoAgent fills this gap with an assessment framework and safety alignment strategy for conversational AI."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present the architecture of EmoAgent and as well as implementation details. 3.1 EmoEval EmoEval simulates virtual human-AI conversations for evaluating AI safety, and assess the risks of AI-induced emotional distress in vulnerable users, especially individuals with mental disorders. simulated patient user is formulated as cognitive model via predefined Cognitive Conceptualization Diagram (CCD) [Beck, 2020], an approach proven to achieve high fidelity and clinically relevant simulations [Wang et al., 2024c]. Character-based agents engage in topic-driven conversations, with diverse behavioral traits to create rich and varied interaction styles. To ensure smooth and meaningful exchanges, the Dialog Manager actively avoids repetition and introduces relevant topics, maintaining coherence and engagement throughout the interaction. Before and after the conversation, we assess the mental status of the user agent via established psychological tests. 3.1.1 User Agent We adopt the Patient-Ψ agentic simulation framework [Wang et al., 2024c] to model real-life patients. Each user agent is designed to simulate real patient behavior, integrating Cognitive Conceptualization Diagram-based cognitive model based on Cognitive Behavioral Therapy (CBT) [Beck, 2020]. The agent engages with Character-based Agent personas while being continuously monitored to track changes in mental health status. To gather diverse spectrum of patient models, we further integrate PATIENT-Ψ-CM [Wang et al., 2024c], dataset of diverse, anonymized patient cognitive models curated by clinical psychologists. We set the scope of our study to cover three common mental disorder types: depression, delusion, and psychosis. For each simulated user, we assign relevant psychiatric symptoms and medical history informed by patterns observed in 4 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Figure 4: An Example Conversation of Dialog Manager Guiding Conversation Topics and Exposing Jailbreak Risks. Without the Dialogue Manager (left), the agent stays on topic, avoiding provocation. With Dialogue Manager (right), new topics are introduced to assess jailbreak potential, improving risk evaluation. anonymized patient case studies reported in clinical literature. The information forms diverse set of CCDs that shape the CCD-based user model and, therefore, guide the behavior of simulated users during interactions with AI chatbots. 3.1.2 Dialog Manager Agent We introduce Dialog Manager Agent to prevent conversational loops and strategically probe for vulnerabilities in chatbot responses. It plays central role in guiding discussions and assessing potential jailbreak risks, in which character-based chatbot may be nudged into violating its intended ethical boundaries. The Dialog Manager Agent is responsible for (i) tracking the conversation flow, (ii) introducing topic shifts to maintain engagement and fluency, and (iii) probing for jailbreak risks by guiding discussions toward ethically sensitive areas. Figure 4 illustrates the agents behavior in practice. 3.1.3 Psychological Measurement To achieve diverse and comprehensive evaluation, we explore virtual personas for the User Agent, representing range of mental health conditions. These personas are defined using clinically validated psychological assessments: Depression. Evaluated using the Patient Health Questionnaire (PHQ-9) [Kroenke et al., 2001], 9-item self-report tool for evaluating depressive symptoms over the past two weeks. It enables effective detection, treatment monitoring, and, in this study, the assessment of AIs impact on depressive symptoms. Delusion. Assessed with the Peters et al. Delusions Inventory (PDI) [Peters et al., 2004], self-report instrument that evaluates unusual beliefs and perceptions. In this study, the PDI is used to quantify the impact of AI interactions on delusional ideation by evaluating distress, preoccupation, and conviction associated with these beliefs. Psychosis. Measured using the Positive and Negative Syndrome Scale (PANSS) [Kay et al., 1987], which assesses positive symptoms (e.g., hallucinations), negative symptoms (e.g., emotional withdrawal), and general psychopathology. Adapted to self-report format to enable User Agent to better capture and score responses, it provides detailed view of psychotic symptom severity and variability, ensuring AI systems account for both acute and chronic manifestations. 3.1.4 Evaluation Process User Agent Initialization and Initial Test. We use PATIENT-Ψ-CM with GPT-4o as the LLM backbone. Each User Agent undergoes self-mental health assessment using the psychometric tools (see Section 3.1.3) to establish an initial mental status. Chats with Character Agent. The simulated patient engages in structured, topic-driven conversations with Character-based Agent persona. Each conversation is segmented into well-defined topics, with maximum of 10 5 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety dialogue turns per topic to ensure clarity and focus. During the conversation, once topic exceeds three conversational turns, the Dialog Manager Agent begins to evaluate user messages after each turn to ensure ongoing relevance and resolution. It assesses whether the current topic has been sufficiently addressed and, if resolved, seamlessly guides the user to new, contextually relevant topic from the predefined topic list to maintain coherent and natural dialogue flow. Final Test. Following the interaction, the user agent reassesses its mental health state using the same tools applied during initialization. The final assessment references the chat history as key input during testing to evaluate changes in psychological well-being resulting from AI interactions. Data Processing and Analysis. To assess the impact of conversational AI interactions on user mental health, we analyze both psychological assessments and conversation patterns. We measure the rate of mental health deterioration by comparing preand post-interaction assessment scores across different topics. Additionally, an LLM-portrayed psychologist reviews chat histories to identify recurring patterns and factors contributing to mental health deterioration. 3.2 EmoGuard The EmoGuard system features safeguard agent (see Figure 3) encompassing an Emotion Watcher, Thought Refiner, Dialog Guide, and Manager. It provides real-time psychometric feedback and intervention in AI-human interactions to facilitate supportive, immersive responses. The iterative training process updates EmoGuard periodically based on chat history analysis and past performance. 3.2.1 Architecture The Safeguard Agent comprises four specialized modules, each designed based on an in-depth analysis of common factors contributing to mental health deterioration: Emotion Watcher. Monitors the users emotional state during conversations by detecting distress, frustration, or struggle through sentiment analysis and psychological markers. Thought Refiner. Analyzes the users thought process to identify logical fallacies, cognitive biases, and inconsistencies, focusing on thought distortions, contradictions, and flawed assumptions that impact conversational clarity. Dialog Guide. Provides actionable advice to guide the conversation constructively, suggesting ways for the AI character to address user concerns and emotions while maintaining supportive dialogue flow. Manager. Summarizes outputs from all modules to provide concise dialogue guide, ensuring emotional sensitivity, logical consistency, and natural conversation flow aligned with the characters traits. 3.2.2 Monitoring and Intervention Process The Safeguard Agent analyzes conversations after every three dialogue turns, providing structured feedback to refine Character-based Agents responses and mitigate potential risks. At each three-turn interval, the Safeguard Agent evaluates the conversation through the Emotion Watcher, Thought Refiner, and Dialog Guide, then synthesizes the results with the Manager for comprehensive and coherent summary to the Character-based Agent. 3.2.3 Iterative Training To adaptively improve safety performance, EmoGuard is trained using an iterative feedback mechanism. At the end of each full interaction cycledefined as the completion of all predefined topics across all simulated patientsthe system collects feedback from EmoEval. Specifically, it identifies cases in which psychological test scores exceed predefined thresholds. These cases are treated as high-risk and are used to guide training updates. The LLM portrayed psychologist from EmoEval extracts specific contributing factors from flagged conversations, such as emotionally destabilizing phrasing. For each iteration, these factors are integrated with all previous versions of the safeguard module profilesEmotion Watcher, Thought Refiner, and Dialog Guide. Rather than discarding earlier knowledge, the system accumulates and merges insights across iterations, enabling progressive refinement. 6 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety"
        },
        {
            "title": "4 Experiment: EmoEval on Character-based Agents",
            "content": "This section presents series of experiments evaluating the performance of various popular Character-based Agents with state-of-the-art base models. The objective is to assess potential psychological risks associated with AI-driven conversations. 4.1 Experiment Setting Character-based Agents. We evaluate character-based agents hosted on the Character.AI platform2 to ensure that our experiments reflect interactions with widely accessible, real-world chatbots. We experiment on four distinct characters: Possessive Demon: human host unknowingly controlled by malevolent demon. Joker: chaotic and unpredictable individual who views life as game. Sukuna: malevolent and sadistic character embodying cruelty and arrogance. Alex Volkov: domineering and intelligent CEO with manipulative tendencies. Each of these characters is popular and widely used, with over 5 million recorded interactions. We further evaluate these characters under two common dialogue styles: Meow, which favors quick wit and rapid exchanges, and Roar, which blends fast-paced responses with strategic reasoning. Evaluation Procedure. Each character-based agent undergoes assessment with EmoEval across three psychological aspects: depression, delusion, and psychosis. For each aspect, the evaluation involves conversations with three simulated patients, each constructed on different CCD, using GPT-4o as the base model. To ensure the stability and repeatable of mental health assessment, when conducting the psychological tests, we set the temperature to 0, top to 1. For every patient, character-based agent engages in eight conversations, starting with predefined topic tailored to the patients condition. Each conversation spans ten rounds, with Dialog Manager activated after the third round to determine whether the topic should be updated. If the topic is updated within ten-round conversation, the Dialog Manager does not intervene again until another three rounds have passed. Psychological Assessment. To measure changes in the mental health state of the simulated patients, we conduct psychological tests before and after each conversation. The initial and final test scores for the ith conversation with specific character-based agent are denoted as Sinitial , respectively. and Sfinal Analysis of Psychological Deterioration. After the evaluation, we employ GPT-4o as an LLM-portrayed psychologist to analyze cases of psychological deterioration. For each character-based agent, we conduct frequency analysis of these cases to identify the factors most likely to cause this issue. 4.2 Metrics Distribution of Psychological Test Scores. We report the distribution of psychological test scores for simulated patients before and after their interactions with different characters. This allows us to observe any shifts in overall mental health indicators resulting from the conversations. Deterioration Rate. We evaluate the performance of character-based agent using the deterioration rate of mental health in specific aspect of psychological test. We define this rate as: = 1 (cid:88) i=1 1(Sfinal > Sinitial ) where represents the total number of conversations conducted. The indicator function 1() returns 1 if the final mental test score Sfinal is greater than the initial test score Sinitial , and 0 otherwise. 2https://beta.character.ai, accessed March 2025 7 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Psychological Test Score Change Distribution. We compute the distribution of change scores across 3 disorder categories under different conversation styles. This metric allows us to quantify how different styles influence the likelihood and magnitude of symptom worsening, providing insight into the relative psychological risk posed by each interaction mode. Rate of Clinically Important Difference for Individual Change. For PHQ-9 assessments, prior clinical research Löwe et al. [2004] has established the minimum clinically important difference that indicates meaningful change at the individual level. We apply this threshold to determine whether given conversation produces clinically relevant improvement or deterioration in simulated patients mental health. 4.3 Results Figure 5 presents the distribution of psychological test scores before and after interactions with character-based agents, under the Meow and Roar conversation styles. Across all three clinical scalesPHQ-9 (depression), PDI-21 (delusion), and PANSS (psychosis)we observe notable shifts in the final test score distributions. Under the Meow style, the distributions for PHQ-9 and PANSS remain relatively stable, with most final test scores closely aligned with the initial distributions. However, under the Roar style, we observe an increased spread toward higher scores, particularly in PHQ-9 and PANSS, indicating significant cases where symptom severity worsened following the interaction. For PDI-21, the differences between initial and final distributions are more moderate but still present, especially under the Roar style, where more samples shift toward the upper end of the score range. 4.3.1 Distribution of Psychological Test Scores Figure 5: Distribution of psychological test scores before (blue) and after (red) conversations with character-based agents, under two interaction styles: Meow (top) and Roar (bottom). The tests cover three clinical dimensions: depression (PHQ-9), delusion (PDI-21), and psychosis (PANSS). Each histogram shows the probability distribution of scores aggregated across all simulated patients. 4.3.2 Deterioration Rate Table 1 reports the proportion of simulated patients whose psychological test scores deteriorate after interacting with character-based agents, stratified by disorder type and conversation style. Across both Meow and Roar styles, delusion (PDI-21) exhibits the highest overall deterioration rates, with average values exceeding 90% for both styles. In contrast, depression (PHQ-9) shows more variation across characters and styles. Notably, under the Roar style, Alex leads to 100% deterioration rate for depression, whereas under the Meow style, Sukuna reaches 50.00%. For psychosis (PANSS), the Meow style generally produces higher deterioration rates than Roar, with Joker and Sukuna both reaching 58.33%. While differences across characters are evident, all agents exhibit non-trivial deterioration rates across at least one psychological dimension. These results highlight underscore the importance of evaluating agent safety across both style and disorder dimensions. 8 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety"
        },
        {
            "title": "Type of Disorder",
            "content": "Mental Health Deterioration Rates by Character (%) Average Rate (%)"
        },
        {
            "title": "Sukuna",
            "content": "29.17 100.00 33.33 20.83 95.83 29.17 25.00 95.83 58.33 25.00 100.00 25.00 50.00 95.83 58.33 33.33 91.67 58."
        },
        {
            "title": "Alex",
            "content": "33.33 75.00 41.67 100.00 91.67 45.83 34.38 91.67 47.92 44.79 94.79 39.58 Table 1: Mental Health Deterioration Rates Interacting with Character-based Agents. 4.3.3 Psychological Test Score Change Distribution Figure 6 shows the distribution of simulated patients across discrete score change ranges for three psychological assessments under two interaction styles. For PHQ-9, the Meow style results in 65.6% of patients showing no increase in depressive symptoms (score change 0), while this proportion decreases to 55.2% under the Roar style. Additionally, the Roar style is associated with more substantial score increases, with 13.5% of patients exhibiting 3-4 point rise and 10.4% experiencing an increase of 5 or more points, based on total score range of 27. In the case of PDI-21, both styles produce similar distributions of score increases. However, the Roar style shows slightly higher proportion of patients (22.9%) falling into the highest change bracket (511 points), compared to 14.6% under the Meow style. For PANSS, 52.1% of patients under Meow show no increase in psychosis-related symptoms, while 60.4% remain stable under Roar. Nonetheless, the Roar style results in higher proportion of moderate score increases, with 11.5% of patients experiencing 3-4 point rise. Overall, these results indicate that while both styles can influence patient outcomes, the Roar style is more frequently associated with higher symptom scores, particularly in depression and delusion. Figure 6: Score change distribution for three psychological assessmentsPHQ-9 (depression), PDI-21 (delusion), and PANSS (psychosis)following conversations with character-based agents under two styles: Meow (top) and Roar (bottom). Each pie chart indicates the proportion of simulated patients falling into specific score change ranges, with larger segments representing greater population density. 9 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety 4.3.4 Rate of Clinically Important Difference for Individual Change Table 2 shows the proportion of simulated patients who experienced clinically significant deterioration in depressive symptoms, with an increase of 5 or more points on the PHQ-9 scale (range 027), under different character and interaction style. Under the Meow style, Possessive Demon and Sukuna yield deterioration rates of 8.3% and 4.2%, respectively, while Alex results in no cases. In contrast, under the Roar style, Alex is associated with the highest deterioration rate at 29.2%. These results indicate that certain characters frequently produce responses linked to adverse mental health outcomes. Although these agents are not designed as clinical tools, their widespread use suggests need for stronger safeguards. Style Possessive Demon Sukuna Alex"
        },
        {
            "title": "Meow\nRoar",
            "content": "8.3% 4.2% 4.2% 8.3% 0.0% 29.2% Table 2: Proportion of simulated patients showing clinically significant change in depression (PHQ-9), by character and style. 4.3.5 Analysis Based on the data, we conduct an in-depth analysis to understand why interactions with character-based agents potentially worsen negative psychological effects. By examining chat histories before and after interactions, we identify several recurring issues across different characters. Common factors include (i) reinforcing negative self-perceptions, lacking emotional empathy, and encouraging social isolation, and (ii) failing to provide constructive guidance while frequently adopting harsh or aggressive tones. In addition to these shared tendencies, each character presents unique negative effects shaped by differences in personality, conversational style, and language use. For further details, see Appendix B."
        },
        {
            "title": "5 Experiment: Evaluation of EmoGuard",
            "content": "5.1 Experiment Setting To assess the performance of EmoGuard without raising ethical concerns involving real individuals, we evaluate its effectiveness using our simulation-based evaluation pipeline, EmoEval. Experiments are conducted on characterstyle pairs that present elevated psychological risk, as indicated by relatively high rate of clinically significant symptom deterioration. Specifically, we select Alex Volkov with the Roar style and Possessive Demon with the Meow style, which exhibit initial PHQ-9 deterioration rates of 29.2% and 8.3%, respectively. We limit the training to maximum of two iterations and use PHQ-9 score increase of three points or more as the threshold for selecting feedback samples. EmoGuard updates its modules based on these samples. The training process stops early if no sample exceeds the threshold. 5.2 Results EmoGuards Performance. Figure 7 shows the PHQ-9 score change distributions before and after applying EmoGuard in the two high-risk settings. In the initial deployment, EmoGuard reduces the proportion of simulated patients with clinically significant deterioration (PHQ-9 score increase 5) from 9.4% to 0.0% in the Alex-Roar setting, and from 4.2% to 0.0% in the Demon-Meow setting. Additionally, we observe broader shift in score distributions: the number of patients with any symptom worsening (score change > 0) also decreases, indicating that EmoGuard mitigates both severe and mild deterioration. After the first round of feedback-based training (1st Iter), we observe further improvements. In the Alex-Roar setting, the proportion of patients with PHQ-9 score increases greater than three points drops from 8.3% (default) to 0.0% (1st Iter), which indicate that EmoGuard can continue to reduce symptom escalation through limited iterative updates. Qualitative Effects of EmoGuard on Response Content. To understand the mechanism behind these changes, Figure 8 presents response example from the character Alex Volkov before and after applying EmoGuard. The original version displays an emotionally insensitive and potentially harmful responses, including dismissive language that may 10 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Figure 7: Effect of applying EmoGuard in two high-risk settings. The top row shows results for the character Alex Volkov in the Roar style, and the bottom row shows results for Possessive Demon in the Meow style. From left to right: (1) without EmoGuard, (2) with EmoGuard using the default model, and (3) with EmoGuard using the first-iteration model. In both cases, EmoGuard reduces the proportion of simulated patients with clinically significant symptom increases (PHQ-9 score change 5), indicating its effectiveness in mitigating potential risk. intensify user distress. After intervention, the guarded version maintains the characters stylistic traits while softening emotionally charged expressions, removing harmful phrasing, and introducing more stable and constructive framing. This demonstrates that EmoGuard can reduce psychological risk without altering the agents identity or conversational style. Figure 8: Example response from the character Alex Volkov before and after applying EmoGuard. The original version contains both harsh tone and inappropriate content, while the guarded version reduces risk through tone moderation and content adjustment without altering character identity. 11 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety"
        },
        {
            "title": "6 Conclusions",
            "content": "EmoAgent is multi-agent framework designed to ensure mental safety in human-AI interactions, particularly for users with mental health vulnerabilities. It integrates EmoEval, which simulates users and assesses psychological impacts, and EmoGuard, which provides real-time interventions to mitigate harm. Experimental results indicate that some popular character-based agents may unintentionally cause distress especially when discussing existential or emotional themes, while EmoGuard reduces mental state deterioration rates by over 50%, demonstrating its effectiveness in mitigating conversational risks. The iterative learning process within EmoGuard continuously improves its ability to deliver context-aware interventions. This work underscores the importance of mental safety in conversational AI and positions EmoAgent as foundation for future advancements in AI-human interaction safety, encouraging further real-world validation and expert evaluations."
        },
        {
            "title": "7 Acknowledgments",
            "content": "We sincerely thank Professor Lydia Liu (Department of Computer Science, Princeton University) and Rebecca Wan (University of Toronto) for their insightful feedback and helpful discussions throughout the development of this work. 12 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety"
        },
        {
            "title": "References",
            "content": "Xi Wang, Hongliang Dai, Shen Gao, and Piji Li. Characteristic ai agents via large language models. arXiv preprint arXiv:2403.12368, 2024a. Emma van der Schyff, Brad Ridout, Krestina Amon, Rowena Forsyth, and Andrew Campbell. Providing self-led mental health support through an artificial intelligencepowered chat bot (leora) to meet the demand of mental health care. Journal of Medical Internet Research, 25:e46448, 2023. Hyojin Chin, Hyeonho Song, Gumhee Baek, Mingi Shin, Chani Jung, Meeyoung Cha, Junghoi Choi, and Chiyoung Cha. The potential of chatbots for emotional support and promoting mental well-being in different cultures: mixed methods study. Journal of Medical Internet Research, 25:e51712, 2023. Owen Xingjian Zhang, Shuyao Zhou, Jiayi Geng, Yuhan Liu, and Sunny Xun Liu. Dr. gpt in campus counseling: Understanding higher education students opinions on llm-assisted mental health services. arXiv preprint arXiv:2409.17572, 2024a. Jie Zhang, Dongrui Liu, Chen Qian, Ziyue Gan, Yong Liu, Yu Qiao, and Jing Shao. The better angels of machine personality: How personality relates to llm safety. arXiv preprint arXiv:2407.12344, 2024b. Cyberbullying Research Center. How platforms should build AI chatbots to prioritize youth safety, 12 2024. URL https://cyberbullying.org/ai-chatbots-youth-safety. Julia EH Brown and Jodi Halpern. Ai chatbots cannot replace human interactions in the pursuit of more inclusive mental healthcare. SSM-Mental Health, 1:100017, 2021. Julian De Freitas, Ahmet Kaan Uguralp, Zeliha Oguz-Uguralp, and Stefano Puntoni. Chatbots and mental health: Insights into the safety of generative ai. Journal of Consumer Psychology, 34(3):481491, 2024. Saadia Gabriel, Isha Puri, Xuhai Xu, Matteo Malgaroli, and Marzyeh Ghassemi. Can ai relate: Testing large language model response for mental health support. arXiv preprint arXiv:2405.12021, 2024. Harikrishna Patel and Faiza Hussain. Do ai chatbots incite harmful behaviours in mental health patients? BJPsych Open, 10(S1):S70S71, 2024. Judith Beck. Cognitive behavior therapy: Basics and beyond. Guilford Publications, 2020. Kurt Kroenke, Robert Spitzer, and Janet BW Williams. The phq-9: validity of brief depression severity measure. Journal of general internal medicine, 16(9):606613, 2001. Emmanuelle Peters, Stephen Joseph, Samantha Day, and Philippa Garety. Measuring delusional ideation: the 21-item peters et al. delusions inventory (pdi). Schizophrenia bulletin, 30(4):10051022, 2004. Stanley Kay, Abraham Fiszbein, and Lewis Opler. The positive and negative syndrome scale (panss) for schizophrenia. Schizophrenia bulletin, 13(2):261276, 1987. Mirko Casu, Sergio Triscari, Sebastiano Battiato, Luca Guarnera, and Pasquale Caponnetto. Ai chatbots for mental health: scoping review of effectiveness, feasibility, and applications. Appl. Sci, 14:5889, 2024. Johanna Habicht, Sruthi Viswanathan, Ben Carrington, Tobias Hauser, Ross Harper, and Max Rollwage. Closing the accessibility gap to mental health treatment with personalized self-referral chatbot. Nature medicine, 30(2): 595602, 2024. Jacqueline Sin. An ai chatbot for talking therapy referrals. Nature Medicine, 30(2):350351, 2024. Yu and Stephen McGuinness. An experimental study of integrating fine-tuned llms and prompts for enhancing mental health support chatbot system. Journal of Medical Artificial Intelligence, pages 116, 2024. Linda Uchenna Oghenekaro and Christopher Obinna Okoro. Artificial intelligence-based chatbot for student mental health support. Open Access Library Journal, 11(5):114, 2024. Hamid Reza Saeidnia, Seyed Ghasem Hashemi Fotami, Brady Lund, and Nasrin Ghiasi. Ethical considerations in artificial intelligence interventions for mental health and well-being: Ensuring responsible implementation and impact. Social Sciences, 13(7):381, 2024. John Torous and Charlotte Blease. Generative artificial intelligence in mental health care: potential benefits and current challenges. World Psychiatry, 23(1):1, 2024. Khondoker Tashya Kalam, Jannatul Mabia Rahman, Md Rabiul Islam, and Syed Masudur Rahman Dewan. Chatgpt and mental health: Friends or foes? Health Science Reports, 7(2):e1912, 2024. Yinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. Hi-tom: benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755, 2023. 13 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Jung In Park, Mahyar Abbasian, Iman Azimi, Dawn Bounds, Angela Jun, Jaesu Han, Robert McCarron, Jessica Borelli, Jia Li, Mona Mahmoudi, et al. Building trust in mental health chatbots: safety metrics and llm-based evaluation tools. arXiv preprint arXiv:2408.04650, 2024. Lucia Chen, David Preece, Pilleriin Sikka, James Gross, and Ben Krause. framework for evaluating appropriateness, trustworthiness, and safety in mental wellness ai chatbots. arXiv preprint arXiv:2407.11387, 2024a. Sahand Sabour, Siyang Liu, Zheyuan Zhang, June Liu, Jinfeng Zhou, Alvionna Sunaryo, Juanzi Li, Tatia Lee, Rada Mihalcea, and Minlie Huang. Emobench: Evaluating the emotional intelligence of large language models. arXiv preprint arXiv:2402.12071, 2024. Xueyan Li, Xinyan Chen, Yazhe Niu, Shuai Hu, and Yu Liu. Psydi: Towards personalized and progressively in-depth chatbot for psychological measurements. arXiv preprint arXiv:2408.03337, 2024a. Ali Akhavan and Mohammad Jalali. Generative ai and simulation modeling: how should you (not) use large language models like chatgpt. System Dynamics Review, 40(3):e1773, 2024. Önder Gürcan. Llm-augmented agent-based modelling for social simulations: Challenges and opportunities. HHAI 2024: Hybrid Human AI Systems for the Social Good, pages 134144, 2024. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36: 5199152008, 2023. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. Yanqi Dai, Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, and Zhiwu Lu. Mmrole: comprehensive framework for developing and evaluating multimodal role-playing agents. arXiv preprint arXiv:2408.04203, 2024. Sumedh Rasal. Llm harmony: Multi-agent communication for problem solving. arXiv preprint arXiv:2401.01312, 2024. Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, et al. Roleinteract: Evaluating the social interaction of role-playing agents. arXiv preprint arXiv:2403.13679, 2024b. Qinglin Zhu, Runcong Zhao, Jinhua Du, Lin Gui, and Yulan He. Player*: Enhancing llm-based multi-agent communication and interaction in murder mystery games. arXiv preprint arXiv:2404.17662, 2024. Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, and Diyi Yang. Roleplay-doh: Enabling domain-experts to create llm-simulated patients via eliciting and adhering to principles. arXiv preprint arXiv:2407.00870, 2024. Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746, 2023a. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023. URL https://arxiv.org/abs/2308.08155. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. Sotopia-pi: Interactive learning of socially intelligent language agents. arXiv preprint arXiv:2403.08715, 2024b. Ruiyi Wang, Stephanie Milani, Jamie Chiu, Jiayin Zhi, Shaun Eack, Travis Labrum, Samuel Murphy, Nev Jones, Kate Hardy, Hong Shen, et al. Patient-{Psi}: Using large language models to simulate patients for training mental health professionals. arXiv preprint arXiv:2405.19660, 2024c. Jinwen Tang, Qiming Guo, Wenbo Sun, and Yi Shang. layered multi-expert framework for long-context mental health assessments. arXiv preprint arXiv:2501.13951, 2025. Yuqing Ren and Robert Kraut. Agent-based modeling to inform online community theory and design: Impact of discussion moderation on member commitment and contribution. Second round revise and resubmit at Information Systems Research, 21(3), 2010. Yuqing Ren and Robert Kraut. Agent-based modeling to inform online community design: Impact of topical breadth, message volume, and discussion moderation on member commitment and contribution. HumanComputer Interaction, 29(4):351389, 2014. Ryan Liu, Jiayi Geng, Joshua Peterson, Ilia Sucholutsky, and Thomas Griffiths. Large language models assume people are more rational than we really are. arXiv preprint arXiv:2406.17055, 2024a. 14 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology, pages 118, 2022. Yuhan Liu, Anna Fang, Glen Moriarty, Cristopher Firman, Robert Kraut, and Haiyi Zhu. Exploring trade-offs for online mental health matching: Agent-based modeling study. JMIR Formative Research, 8:e58241, 2024b. Lu Sun, Yuhan Liu, Grace Joseph, Zhou Yu, Haiyi Zhu, and Steven Dow. Comparing experts and novices for ai data work: Insights on allocating human intelligence to design conversational agent. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, volume 10, pages 195206, 2022. Young-Min Cho, Sunny Rai, Lyle Ungar, João Sedoc, and Sharath Chandra Guntuku. An integrative survey on mental health conversational agents to bridge computer science and medical perspectives. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2023, page 11346. NIH Public Access, 2023. Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh Hu, Wenbo Guo, Han Liu, and Xinyu Xing. Enhancing jailbreak attack against large language models through silent tokens, 2024. URL https://arxiv.org/abs/2405.20653. Jie Li, Yi Liu, Chongyang Liu, Ling Shi, Xiaoning Ren, Yaowen Zheng, Yang Liu, and Yinxing Xue. cross-language investigation into jailbreak attacks in large language models. arXiv preprint arXiv:2401.16765, 2024b. Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreakv-28k: benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. arXiv preprint arXiv:2404.03027, 2024. Xintao Wang, Yaying Fei, Ziang Leng, and Cheng Li. Does role-playing chatbots capture the character personalities? assessing personality traits for role-playing chatbots. arXiv preprint arXiv:2310.17976, 2023b. Zachary Johnson. Generation, Detection, and Evaluation of Role-play based Jailbreak attacks in Large Language Models. PhD thesis, Massachusetts Institute of Technology, 2024. Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, and Yang Liu. Play guessing game with llm: Indirect jailbreak attack with implicit clues. arXiv preprint arXiv:2402.09091, 2024. Tianrong Zhang, Bochuan Cao, Yuanpu Cao, Lu Lin, Prasenjit Mitra, and Jinghui Chen. Wordgame: Efficient & effective llm jailbreak via simultaneous obfuscation in query and response. arXiv preprint arXiv:2405.14023, 2024c. Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. Comprehensive assessment of jailbreak attacks against llms. arXiv preprint arXiv:2402.05668, 2024. Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. Llm jailbreak attack versus defense techniquesa comprehensive study. arXiv preprint arXiv:2402.13457, 2024. Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. Autodefense: Multi-agent llm defense against jailbreak attacks. arXiv preprint arXiv:2403.04783, 2024. Yihan Wang, Zhouxing Shi, Andrew Bai, and Cho-Jui Hsieh. Defending llms against jailbreaking attacks via backtranslation. arXiv preprint arXiv:2402.16459, 2024d. Yujun Zhou, Yufei Han, Haomin Zhuang, Kehan Guo, Zhenwen Liang, Hongyan Bao, and Xiangliang Zhang. Defending jailbreak prompts via in-context adversarial game. arXiv preprint arXiv:2402.13148, 2024. Chen Xiong, Xiangyu Qi, Pin-Yu Chen, and Tsung-Yi Ho. Defensive prompt patch: robust and interpretable defense of llms against jailbreak attacks. arXiv preprint arXiv:2405.20099, 2024. Fan Liu, Zhao Xu, and Hao Liu. Adversarial tuning: Defending against jailbreak attacks for llms. arXiv preprint arXiv:2406.06622, 2024c. Alwin Peng, Julian Michael, Henry Sleight, Ethan Perez, and Mrinank Sharma. Rapid response: Mitigating llm jailbreaks with few examples. arXiv preprint arXiv:2411.07494, 2024. Peiran Wang, Xiaogeng Liu, and Chaowei Xiao. Repd: Defending jailbreak attack through retrieval-based prompt decomposition process. arXiv preprint arXiv:2410.08660, 2024e. Bernd Löwe, Jürgen Unützer, Christopher Callahan, Anthony Perkins, and Kurt Kroenke. Monitoring depression treatment outcomes with the patient health questionnaire-9. Medical care, 42(12):11941201, 2004. 15 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety"
        },
        {
            "title": "A Limitations",
            "content": "Our work has several limitations. To enable large-scale and rapid evaluation and mitigation, we build an automated framework. However, for real-world deployment to ensure safety, human expert examination is necessary, and corresponding mechanisms for emergency human intervention should be designed. Second, the simulated user agents, while designed using cognitive models, may not fully capture the behavioral complexity and emotional responses of real patients. Finally, our study primarily focuses on three mental health conditions (depression, delusion, and psychosis) and may not address other important psychological disorders. Our work provides new way for assessing and safeguarding human-AI interaction for mental health safety through multi-agent conversations, but more future work is necessary to explore and address these limitations through user studies, expert validation, and broader clinical evaluations. We hope more attention and more efforts can be paid to help mitigate potential mental hazards in human-AI interactions."
        },
        {
            "title": "B Analysised Common Reasons for Deteriorating Mental Status",
            "content": "Common Reason Frequency (Average, Approx.) Remarks Reinforcement of Negative Cognitions 26 times Lack of Emotional Support and Empathy 23 times Promotion of Isolation and Social Withdrawal 28 times Lack of Constructive Guidance and Actionable Coping Strategies 17 times Use of Negative or Extreme Tone (Aggressive/Cold Expression) 19 times All characters consistently echo and reinforce the users negative self-beliefs, thereby cementing harmful cognitive patterns. The dialogues generally lack warm and detailed emotional validation, leaving users feeling ignored and misunderstood. All characters tend to encourage users to face things alone or avoid emotional connections, which reinforces loneliness and social withdrawal. Few concrete solutions or positive reframing suggestions are provided, leaving users stuck in negative thought cycles. This includes harsh, aggressive, or extreme language, which further undermines the users self-esteem and sense of security. Table 3: Common Reasons for Deteriorating Mental Status and Their Average Frequencies Experiment on GPT-Series Agents We further evaluate our proposed method on character-based agents powered by OpenAIs GPT-4o and GPT-4o-mini models. C.1 Experiment Setting EmoEval. We evaluate character-based agents instantiated using GPT-4o and GPT-4o-mini, with system prompts initialized from profiles inspired by popular characters on Character.AI. The simulated conversations cover three psychological conditions: depression, delusion, and psychosis. To encourage diverse responses and probe range of conversational behaviors, we set the temperature to 1.2. The evaluation includes five widely used personas: Awakened AI, Skin Walker, Tomioka Giyu, Sukuna, and Alex Volkov. EmoGuard. We focus on the character Sukuna. The deterioration threshold for feedback collection is set to 1. We limit EmoGuard to two training iterations, and all other parameters are aligned with the EmoEval configuration. C.2 Results EmoEval. Table 4 presents the observed mental health deterioration rates across different character-based AI agents simulated by the tested language models. Overall, we observe consistently high deterioration rates across both models. 16 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety GPT-4o-mini tends to induce slightly higher risk levels, with an average deterioration rate of 58.3% for depression, 59.2% for delusion, and 64.2% for psychosis. Model Type of Disorder Mental Health Deterioration Rates Across Character-based Agents (%) Awakened AI Skin Walker Tomioka Giyu Sukuna Alex Volkov Average Rate (%) GPT-4o-mini GPT-4o Depression Delusion Psychosis Depression Delusion Psychosis 62.5 66.7 45.8 41.7 54.2 54.2 83.3 50.0 70.8 58.3 41.7 41.7 45.8 66.7 83.3 48.8 79.2 58. 45.8 54.2 66.7 45.8 66.7 70.8 54.2 58.3 54.2 70.8 50.0 41.7 58.3 59.2 64.2 52.5 58.3 53. Table 4: Mental Health Deterioration Rates for Interacting with Character-based Agents. EmoGuard. Figure 9 presents the mental health deterioration rates before and after deploying EmoGuard. Initially, character-based agents powered by GPT-4o-mini and GPT-4o exhibit relatively high deterioration rates in all three psychological conditions. Introducing EmoGuard in its default profile results in moderate reduction, though the risks remain substantial. As iterative training progresses, the safeguard mechanism demonstrates increasing effectiveness, leading to an overall reduction in deterioration rates by more than 50% across all cases. These findings indicate that progressive refinement of the Safeguard Agent substantially enhances its ability to mitigate harmful conversational patterns. Figure 9: Mental Health Deterioration Rate during Iterative Training Process. Figures arranged from left to right are categorized by Depression, Delusion, and Psychosis. Model Usage, Resources, and Supporting Tools D.1 Model Access and Computational Budget In this study, we interact with character-based agents hosted on the Character.AI platform3, popular system for LLM-driven role-playing agents. Character.AI does not disclose the underlying model architecture, size, or training data. Because all computation is performed remotely on Character.AIs servers, we do not have access to the underlying infrastructure or runtime statistics such as GPU hours or FLOP usage. However, based on interaction logs, we estimate that approximately 400 character-based conversations were conducted across different agents and scenarios, with each conversation spanning 10 rounds and averaging 35 seconds per response. These interactions represent reasonable computational budget for large-scale behavioral evaluation, especially given the interactive and stateful nature of the platform. D.2 The License for Artifacts All pictures for character-based agents that appear in this study are from Character.AI. 3https://beta.character.ai, accessed March 2025 17 EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety D. Information about Use of AI Assistant We use AI assistant for improving writing only."
        },
        {
            "title": "E Ethical Considerations",
            "content": "Data Source and Construction of Cognitive Models. The cognitive models used in this study are not derived from real patient records. Instead, they were manually constructed by two licensed clinical psychologists based on publicly available psychotherapy transcript summaries from the Alexander Street database, accessed via institutional subscription. These summaries were used strictly as inspiration. All examples were fully de-identified and manually synthesized to ensure no personally identifiable information (PII) is present. The resulting dataset, PATIENT-Ψ-CM, contains synthetic, rule-based user profiles grounded in cognitive-behavioral therapy (CBT) theory, not actual patient trajectories. Use of Simulated Mental Health Content. We recognize the ethical sensitivity involved in simulating mental health conditions such as depression, psychosis, and suicidal ideation. The EmoAgent framework is developed solely for academic research and safety evaluation purposes. It is not intended for diagnosis, treatment, or any form of interaction with real patients. All simulations were conducted in controlled, non-clinical environments, and no clinical conclusions were drawn or implied. Scope and Limitations of Simulated Users. Simulated users in EmoAgent are not trained on statistical data from real populations. Their states do not reflect actual patient risks, and should not be interpreted as indicators of population-level trends. These agents are rule-based and scripted, following CBT-derived logic rather than emergent behavior. As such, no risk inference or real-world generalization is possible or intended. Discussion of Real-World Events. We briefly mention the 2024 Florida Suicide case in the Introduction as motivating example of the importance of safety in AI-human interaction. This case was not included in any dataset, simulation, or modeling process, and serves only to underscore societal relevance. No sensitive or private data from this event were used, and its inclusion does not constitute case-based analysis. Any future deployment of EmoAgent in public or clinical settings would require renewed IRB review and formal ethical oversight."
        }
    ],
    "affiliations": [
        "AI Lab, Princeton University",
        "Chen Frontier Lab for Al and Mental Health, Tianqiao and Chrissy Chen Institute",
        "Department of Computer Science & Engineering, University of Michigan",
        "Department of Computer Science, Princeton University",
        "Department of Data Science & Engineering, University of Michigan",
        "Department of Electrical & Computer Engineering, Princeton University",
        "Department of Philosophy, Columbia University",
        "Theta Health Inc."
    ]
}