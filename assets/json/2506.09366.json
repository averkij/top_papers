{
    "paper_title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending",
    "authors": [
        "Yuxuan Kuang",
        "Haoran Geng",
        "Amine Elhafsi",
        "Tan-Dzung Do",
        "Pieter Abbeel",
        "Jitendra Malik",
        "Marco Pavone",
        "Yue Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and loco-manipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, a novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, a parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by a set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 6 3 9 0 . 6 0 5 2 : r SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending Yuxuan Kuang1,2,3 Haoran Geng4 Amine Elhafsi2 Tan-Dzung Do3 Pieter Abbeel4 Jitendra Malik4 Marco Pavone2 Yue Wang1 1University of Southern California 2Stanford University 4University of California, Berkeley 3Peking University Equal contributions Figure 1: SkillBlender performs versatile autonomous humanoid loco-manipulation tasks within different embodiments and environments, given only one or two intuitive reward terms."
        },
        {
            "title": "Abstract",
            "content": "Humanoid robots hold significant potential in accomplishing daily tasks across diverse environments thanks to their flexibility and human-like morphology. Recent works have made significant progress in humanoid whole-body control and locomanipulation leveraging optimal control or reinforcement learning. However, these methods require tedious task-specific tuning for each task to achieve satisfactory behaviors, limiting their versatility and scalability to diverse tasks in daily scenarios. To that end, we introduce SkillBlender, novel hierarchical reinforcement learning framework for versatile humanoid loco-manipulation. SkillBlender first pretrains goal-conditioned task-agnostic primitive skills, and then dynamically blends these skills to accomplish complex loco-manipulation tasks with minimal task-specific reward engineering. We also introduce SkillBench, parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight challenging loco-manipulation tasks, accompanied by set of scientific evaluation metrics balancing accuracy and feasibility. Extensive simulated experiments show that our method significantly outperforms all baselines, while naturally regularizing behaviors to avoid reward hacking, resulting in more accurate and feasible movements for diverse loco-manipulation tasks in our daily scenarios. Our code and benchmark will be open-sourced to the community to facilitate future research. Project page: https://usc-gvl.github.io/SkillBlender-web/."
        },
        {
            "title": "Introduction",
            "content": "Humanoid robots hold significant potential to be seamlessly deployed in our daily lives to accomplish everyday tasks across diverse environments due to their flexibility and human-like morphology. This alignment is crucial since our environments, tasks, and tools are designed around human Preprint. Under review. capabilities [41]. Specifically, we want humanoids to perform versatile loco-manipulation tasks in our daily lives autonomously, instead of executing pre-programmed motions only. However, humanoid loco-manipulation remains extremely challenging due to the high-dimensional nature of their observation and action spaces, as well as the complex dynamics inherent in bipedal systems [21]. Previous optimal control-based works focused on building dynamic models for model predictive control (MPC) [9, 24], which have made great progress on humanoid control. On the other hand, recent model-free reinforcement learning (RL) methods [11, 5, 51, 8, 15, 14] have also made significant strides in agile humanoid whole-body control, benefiting from highly parallel simulation training [34, 40] that largely improves sample efficiency. However, these methods fall short of making humanoid robots versatile being able to perform diverse tasks in scalable way. For instance, optimal control-based methods often require building and tuning accurate dynamic models and complex cost functions tailored to every single task, along with time-intensive optimization, limiting their scalability across different tasks. Regarding RLbased methods, first, most of them are mainly designed for expressive tasks like motion mimicking or teleoperation, lacking the ability to perform versatile loco-manipulation tasks autonomously. Moreover, to successfully learn task and avoid reward hacking, RL-based methods often require labor-intensive reward shaping to balance terms like task success, orientation, upper body pose, gait, contact, curiosity, etc. [11, 5, 51, 45] for each single task, limiting their versatility and possibility for infinite task variations in our daily lives. Therefore, its crucial to find painless and scalable way for humanoids to learn versatile loco-manipulation capabilities without extensive task-specific tuning. To that end, we draw inspiration from human motor skill development, where fundamental capabilities like walking and reaching are acquired first and later combined for more complex tasks [39], enabling sophisticated whole-body coordination. By leveraging these skill priors, we humans can perform versatile tasks in our daily lives without learning them from scratch. To mimic such mechanisms, we propose SkillBlender, novel Hierarchical Reinforcement Learning (HRL) framework for versatile humanoid whole-body loco-manipulation, leveraging pretrain-then-blend paradigm with minimal reward engineering. We first pretrain set of goal-conditioned primitive skills that are task-agnostic, reusable, and physically interpretable. high-level controller then learns to synthesize subgoals and per-joint weight vectors to blend these low-level skills. Unlike prior HRL methods [36, 49, 27], our approach proposed unique vectorized weighting mechanism to blend different skills, enabling more flexible and accurate humanoid actions. This hierarchical structure not only simplifies the search space when training the high-level controller but also reduces the need for extensive reward engineering, requiring only one or two reward terms per task. This enables our methods versatility, generality, and scalability to diverse loco-manipulation tasks in our daily scenarios. Beyond our algorithmic contributions, we recognize the critical role of simulation benchmarks in humanoid learning [10]. Many previous benchmarks either do not support fully parallel simulation [41, 1, 44] or lack whole-body coordination [6]. More importantly, they overlook the humanoids motion feasibility, which encourages reward hacking [13] to maximize task returns, leading to unnatural or unrealistic behaviors if without careful reward engineering [41, 31]. This underscores the need for parallel, comprehensive, and scientifically grounded benchmark to systematically evaluate humanoid loco-manipulation, balancing task accuracy and motion feasibility. As such, we also introduce SkillBench, parallel, cross-embodiment, and diverse benchmark for humanoid whole-body loco-manipulation. SkillBench supports three distinct humanoid morphologies, four primitive skills, and eight challenging loco-manipulation tasks. Unlike previous benchmarks that only assess success via task return [44, 1, 41], our evaluation framework incorporates metrics from two complementary dimensions: (1) the accuracy metric to measure task completion success and (2) set of feasibility metrics to assess the naturalness and realism of humanoid motion. Our extensive experiments on our simulated benchmark show that our SkillBlender significantly outperforms existing baselines in both accuracy and feasibility, producing more natural and feasible behaviors with minimal task-related rewards. Our code and benchmark will be open-sourced to promote future research. In summary, the key contributions of our work are three-fold: We propose SkillBlender, pretrain-then-blend framework for versatile humanoid whole-body loco-manipulation. With our unique skill blending strategy, SkillBlender produces more accurate and feasible behaviors for diverse loco-manipulation tasks with minimal reward engineering. We introduce SkillBench, parallel, cross-embodiment, and diverse benchmark for humanoid whole-body loco-manipulation for comprehensive evaluation. Our benchmark includes two complementary metrics that measure both the accuracy and feasibility of humanoid motions. 2 We provide and will open-source set of structured, broadly useful, reusable, and task-agnostic humanoid primitive skills and diverse simulated task environments, as well as models and pretrained checkpoints to facilitate future open humanoid research."
        },
        {
            "title": "2 Related Works",
            "content": "Humanoid Whole-Body Control Humanoid whole-body control and loco-manipulation remains extremely difficult due to its high dimensionality and unstable bipedal nature. To tackle this problem, previous non-learning-based methods focused on building dynamic models for MPC [9, 24]. However, these methods require tuning accurate dynamic models and complex cost functions for each task, along with time-consuming optimization. Recent times witnessed significant progress on learning-based methods leveraging model-free reinforcement learning for humanoid locomotion [38, 11, 54], motion tracking [5, 8, 15, 14, 16, 22, 17, 3, 30, 50], loco-manipulation [51], and other tasks [20, 18, 53], due to RLs robustness against model mismatch and uncertainties, as well as capabilities of real-time agile motions on legged robots [28, 35]. However, most of them only focused on locomotion or motion mimicking, lacking the abilities to autonomously perform versatile loco-manipulation tasks in scalable manner. Moreover, these RL-based methods require lots of tedious reward tuning on orientation, gait, contact, curiosity, etc., on each setting [45], limiting their versatility and possibility for infinite task variations in our daily lives. Compared to these works, our SkillBlender overcomes the need for tedious reward engineering, generally requiring up to two reward terms for each task to train robust and natural policies for versatile humanoid loco-manipulation without any motion priors, which is scalable to diverse autonomous loco-manipulation tasks. Hierarchical Reinforcement Learning Hierarchical Reinforcement Learning (HRL) strategies have been used in many works to handle the complex temporal dependencies of long-horizon tasks, which are challenging for conventional RL [43, 2, 19, 29, 7]. HRL has also seen frequent application in quadruped loco-manipulation [49, 27, 23, 4, 52] and physics-based animation [42, 36, 46, 37, 32, 33, 47]. Recently [41, 13] have also shown promising results of HRL on humanoid whole-body control to boost policy learning and avoid reward hacking. However, those methods only consider single low-level policy instead of multiple reusable skills which are more structural for complex whole-body loco-manipulation tasks. Compared to MCP [36] or ASE [37] which consider multiple skills, our methods low-level skills are goal-conditioned and physically interpretable, which are specialized and generally useful, allowing them to be reused and blended effectively. Additionally, our SkillBlender proposed unique skill blending strategy with vectorized weighting, allowing more flexible, accurate, and feasible movements. Humanoid Learning Benchmarks Due to the sheer complexity of humanoid robots, it is essential to build benchmarks for humanoid whole-body loco-manipulation, especially simulated benchmarks due to the expense and danger of humanoid hardware. Many previous benchmarks either focus exclusively on locomotion [44], consider loco-manipulation to limited extent [1], or are geared towards animation with virtual animation characters [44]. They do not support parallel simulation either, which is extremely important for RL training. Recently, HumanoidBench [41] began benchmarking loco-manipulation on actual robot models; however, its parallelization capabilities remain limited, supporting only small number of parallel environments if not disabling most collisions. BiGym [6] leverages Unitree H1 robot to benchmark variety of bimanual manipulation tasks; however, it uses floating base for all the demonstrations that fall out of whole-body loco-manipulation. Recent Mimicking-Bench [31] is mainly used for human motion tracking purposes with limited embodiments and tasks. Moreover, all these previous benchmarks lack scientific metrics to evaluate motion feasibility and naturalness. In this work, we attempt to address the aforementioned limitations by introducing our massively parallel, cross-embodiment, and diverse SkillBench to systematically benchmark humanoid whole-body loco-manipulation algorithms with scientific evaluations."
        },
        {
            "title": "3 SkillBlender",
            "content": "Our SkillBlender draws inspiration from human growth and development: infants lift and turn their heads before they can turn over, and move their limbs (arms and legs) before grasping an object [39]. As humans, we first learn individual primitive motor skills when we grow up. These primitive skills are generalized, reflexive, and task-agnostic so that they are not tied to specific tasks and can be 3 Figure 2: Overview of SkillBlender. We first pretrain goal-conditioned primitive expert skills that are task-agnostic, reusable, and physically interpretable, and then reuse and blend these skills to achieve complex whole-body loco-manipulation tasks given only one or two task-specific reward terms. used in various daily tasks, and they also have physical goals like walking to one specific location, reaching specific target, etc. When we encounter new task, like dancing, that requires whole-body coordination, we can then reuse and blend these skills to finish the task. Inspired by this, we propose our method SkillBlender, which first pretrains set of goal-conditioned primitive skills, that are task-agnostic, reusable, and physically interpretable, and then dynamically blends these skills given new high-level task, requiring minimal reward engineering. In this section, we will describe our problem formulation in Sec. 3.1, low-level primitive skill learning in Sec. 3.2, and high-level skill blending in Sec. 3.3. 3.1 Problem Formulation We formulate our humanoid whole-body loco-manipulation policy learning problem as goalconditioned Markov Decision Process (MDP) = S, A, , R, γ, of state S, action A, transition function , reward R, discount factor γ, and task goal G. The objective is to maximize the expected return [(cid:80) γtrt] by finding an optimal policy π(atgt, st), where the subscript indexes the time step. In our hierarchical pipeline, we label the i-th low-level primitive skill as πi responsible for outputting humanoid actions ai state st. We label the high-level controller as πH ({gi weight vector tgi L(ai t, st), which is conditioned on the skills subgoal gi and the current and t}, {W for each low-level skill conditioned on the high-level goal gt, and state st. }gt, st), which outputs subgoal gi 3.2 Learning Individual Low-Level Primitive skills At the core of our framework is the ability to reuse and blend goal-conditioned primitive skills for new tasks, requiring only minimal task-specific reward tuning with only one or two reward terms. To achieve this, we first pretrain set of low-level primitive skills as whole-body policies, denoted as πi L, using goal-conditioned reinforcement learning. More specifically, each low-level skill policy πi tgi L(ai as input. The state st consists of the humanoids proprioceptive information, such as joint positions and velocities, and is shared across all policies. In contrast, the subgoal gi encodes task-specific information and varies depending on the task. The Rd, represents the target joint positions for the humanoids entire body, output of each policy, ai which are then converted to torques using proportional-derivative (PD) controller. t, st) receives state st and subgoal gi The low-level primitive policies are trained with dense rewards, incorporating task-relevant goalmatching rewards, regularization rewards, gait rewards, and other auxiliary objectives. While reward tuning is necessary to train these expert skills, the resulting policies are modular and reusable, allowing for seamless integration into high-level tasks with minimal additional reward engineering, and we anticipate that future humanoid manufacturers may directly provide such pretrained skills as standardized capabilities. In this work, we focus on four broadly useful, task-agnostic primitive skills, though our approach can, in principle, accommodate an arbitrarily large skill library. Below, we provide detailed description of these four primitive skills: 4 Walking. The humanoid is required to walk robustly in response to commanded velocity, enabling locomotion and basic mobility. The goal input consists of the desired velocity on the XY plane and yaw axis. Reaching. The humanoid remains stationary while reaching target 3D points in its surroundings using both wrists, supporting its manipulation capabilities. The goal input is the relative distance between the humanoids wrist positions and their respective targets. Squatting. The humanoid squats down or stands up to reach specified root height, facilitating adaptability to different workspaces. The goal input is the relative height between the humanoids root and the target height. Stepping. The humanoid steps onto sampled ground points, enabling tasks involving foot-based, non-prehensile manipulation. Similar to Reaching, the goal input is the relative distance between the humanoids feet and their respective targets on the floor. 3.3 Reusing and Blending Skills for High-Level Loco-Manipulation Tasks Once the primitive skills are constructed, they can then be dynamically blended for novel tasks involving complex whole-body loco-manipulation, guided solely by task-specific rewards. In this blending process, all selected primitive skills are simultaneously activated, and their actions are weighted to accomplish challenging tasks beyond the capability of single primitive policy. Unlike prior multi-expert approaches [49, 36, 27], our approach proposed unique vectorized weighting mechanism to blend different skills, enabling more flexible and accurate skill blending. After the low-level primitive skills are constructed, given high-level task specification, we first employ skill selector to choose subset of skills to blend. For example, task requiring the humanoid to reach distant points would primarily rely on Walking and Reaching, while Squatting and Stepping would be less relevant, so only Walking and Reaching would be selected for blending. This selection process could be performed manually or by using foundation models leveraging their commonsense reasoning capabilities [48, 26, 25], as demonstrated in Fig. 5. After selecting the relevant skills, we train high-level controller πH that takes the current state and task-specific goal as input and outputs both the subgoals for the selected low-level goal-conditioned skills and the corresponding per-joint weight vectors used for blending their outputs. The final action is computed as weighted sum of the actions from these low-level policies, as illustrated in Fig. 2. Specifically, let the selected low-level primitive skills be {πj L}, given the task goal gt and state st, πH network first outputs the raw subgoals {gj , . . . , gk } and the raw weight vectors { t [0, 1]d is continuous and matches the dimensionality of the action space. The raw subgoals are then clamped to avoid exaggerated values, producing the processed subgoals {gj Then, we add Softmax layer to the raw weight vectors { of non-linearity in order to avoid direct linear combination that leads to reward hacking: } as input for the low-level goal-conditioned skills. , . . . , t } on the joint level as layer }, where each t , . . . , L, . . . , πk , . . . , gk t [m] = t [m] i=j (cid:80)k [m] (1) [m] is the weight scalar of the m-th joint (element) of the n-th skills weight vector. We where verified the essentiality of the Softmax layer to provide non-linearity in Sec. 5.3. After the high-level controller πH generates the final subgoals {gj vectors {W }, each low-level skill πi current state st as input to produce the action ai all the actions as: } and per-joint weight then concatenates its assigned subgoal gi with the t. The final action at is then computed by weighting , . . . , , . . . , gk at = (cid:88) i=j ai (2) where denotes the Hadamard (element-wise) product. During training, only the high-level controller is updated, while the low-level skills remain frozen. Notably, the blending process requires only one or two task-specific reward terms, significantly reducing the effort required for reward shaping. Figure 3: Our SkillBench is parallel, cross-embodiment, and diverse simulated benchmark containing three embodiments, four primitive skills, and eight loco-manipulation tasks."
        },
        {
            "title": "4 SkillBench",
            "content": "To facilitate standardized humanoid learning research, we propose new benchmark SkillBench, which is parallel, cross-embodiment, and diverse. We implement SkillBench in NVIDIA Isaac Gym [34] with PhysX physics engine, benefiting from its highly-parallelized simulation. SkillBench encompasses 3 distinct humanoid embodiments, 4 primitive skills, and 8 complex loco-manipulation tasks. In this section, we describe the details of SkillBenchs simulated environments. 4.1 Observation and Action We support various kinds of observations, which include (1) proprioception, including the humanoids joint angles, joint velocities, last actions, base linear and angular velocities, and base Euler angles (projected gravity); (2) task-related states, such as the positions and rotations of objects in the scene; and (3) ego-centric vision, including RGB, depth (point clouds), and segmentation masks. In this work, we first consider state-based policies, using only proprioception and task-related states. We also include preliminary investigation on vision-based RL in Sec. G.3. Following previous works [15, 14, 8, 51], the action at Rd is the target joint positions of the whole-body joints on the humanoid, which is subsequently converted into torques using PD control. 4.2 Embodiments and Morphologies As shown in Fig.3, our SkillBench is designed for cross-embodiment compatibility, supporting three distinct humanoid models. We specifically choose Unitree humanoids due to their widespread adoption in recent years. In our SkillBench, we support the 19-DoF Unitree H1, 21-DoF Unitree G1, and 21-DoF Unitree H1-2. Their morphology details can be found in Sec. F. In SkillBench, we fix all wrist and hand DoFs, as well as all torso DoFs except for the yaw axis, to simplify whole-body loco-manipulation. These DoFs can be enabled by users if needed. Under this configuration, H1 and H1-2 share similar overall sizes and shapes, while G1 and H1-2 share similar morphologies with 21 DoFs. This design aims to support future research on cross-embodiment humanoid learning by providing standardized yet extensible platform. 4.3 Low-Level Skills and High-Level Tasks Our SkillBench provides 4 primitive skills: Walking, Reaching, Squatting, and Stepping. These are generally applicable, broadly useful, and task-agnostic skills. For high-level tasks, we designed 8 6 complex loco-manipulation tasks that require whole-body coordination and are categorized into three difficulty levels: Easy, Medium, and Hard, based on task horizon and contact richness. Easy tasks focus on short-horizon interactions with minimal contact: FarReach: Reach two distant 3D points using both hands. ButtonPress: Press wall-mounted button with the left wrist while keeping the right arms pose. CabinetClose: Close an open cabinet in front of the humanoid. Medium tasks remain short-horizon but involve increased contact with objects and the environment: FootballShoot: Shoot football towards goal position. BoxPush: Push box on table to target position. PackageLift: Lift package to specified height. Hard tasks involve rich contact dynamics and require multi-stage, long-horizon coordination: BoxTransfer: Transfer box from one table to target location on another table. PackageCarry: Carry package to distant location. Skill and task visualizations are shown in Fig. 3. Since G1 is shorter and smaller, objects and goal positions are scaled accordingly in G1 environments to ensure reachability. For further details, including task descriptions, task-specific goals, success checkers, reward functions, and additional parameters, please refer to Sec. in the appendix. 4.4 Evaluation Metrics As stated in Sec. 1, scientific and comprehensive evaluation system is crucial for humanoid learning, rather than relying solely on task return comparisons. Therefore, our benchmark incorporates two types of metrics: the accuracy metric for assessing task success and set of feasibility metrics to evaluate motion feasibility. For the accuracy metric, we use Error () to quantify the average deviation between the current and goal states. For example, in FarReach, Error is defined as the L1 distance between the humanoids current wrist positions and the target positions. Across all tasks, Error is measured in meters (m). We also set success threshold with regard to Error for each task. For feasibility metrics, we have: Tilt (), the average root pitch and roll angles, measured in radians (rad); Root Height (), measured in meters (m); Average Joint Torque τ (), measured in Newton-meters (N m); Average Joint Power (), measured in Watts (W). These feasibility metrics capture both stability (via Tilt and h) and action intensity (via τ and ), providing comprehensive assessment of the overall feasibility of humanoid behaviors."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup On the H1 embodiment of SkillBench, we compare our method against two vanilla RL baselines learning from scratch: (1) model-free PPO [40] and (2) model-based DreamerV3 [12]. We also compare against three hierarchical baselines: (1) the HumanoidBench baseline (HB) which uses two-hand reaching policy as the low-level policy and then trains task-specific high-level controller, (2) Sequential HRL which trains high-level policy selector that decides which low-level skill to activate at each timestep and thereby sequence them, and (3) MCP [36] which leverages multiplicative compositional policies that synthesize scalar weights to average the low-level skills. We also compare our method with PPO [40] on the G1 and H1-2 embodiments in Sec. G.1. All methods are trained with the same reward functions, which are designed to be straightforward and incorporate only one or two intuitive task-specific terms (e.g., the distance between the current hand positions and target positions). We perform 20 rollouts per task for evaluation. To compute Error, we measure the state deviation between the final and goal states for each rollout, reporting both the mean and standard deviation. Feasibility metrics are first averaged over the duration of each rollout and subsequently averaged across all rollouts to assess motion feasibility throughout the task. Additional implementation details are provided in Sec. E.2. 7 Task Metrics PPO [40] DreamerV3 [12] HB [41] Sequential MCP [36] Ours Error 0.0160.008 0.0330. 0.2470.121 0.0450.031 0.0210.012 FarReach 0.796 0.800 Tilt 0.242 0.312 N/A 0.013 0.018 0.045 0.936 0.969 0. τ 23.4 25.0 16.9 14.1 13.5 36.9 37.5 39.6 20.5 20.2 Error 0.0190.011 0.0270.005 0.3470.178 0.1320.061 0.0050.003 0.0090.007 ButtonPress Tilt 0.471 0.511 0.453 0.007 0.016 0. 0.868 0.839 0.877 0.918 0.910 0.848 τ 31.7 33.3 30.7 18.7 13.9 16.8 39.8 45.7 93.7 49.4 19.2 20.3 Error 0.0000.000 0.0020.002 0.0510.218 0.0520.055 0.0010.004 0.0000.000 CabinetClose Tilt 0.333 0.313 0.097 0.013 0.061 0. 0.886 0.878 0.802 0.899 0.916 0.903 τ 37.2 37.1 22.7 18.9 15.0 13.6 65.4 65.0 29.4 39.9 21.0 16.0 Table 1: Quantitative comparison between our method and baseline methods on H1-Easy tasks. Task Metrics PPO [40] DreamerV3 [12] HB [41] Sequential MCP [36] Ours FootballShoot BoxPush Error 1.7730.244 1.7990.270 1.6840.187 1.8020.217 1.6040.263 1.1090.285 Tilt 0.245 0.240 0.093 0.025 0.054 0.131 0.650 0.830 0.849 0.979 0.888 0.843 τ 48.0 48.8 23.8 12.0 20.0 26.1 54.5 55.0 45.4 22.6 42.8 92. Error 0.1840.207 0.1740.236 0.1250.039 0.1120.047 0.0370.039 0.0090.007 Tilt 0.581 0.560 0.119 0.003 0.020 0.064 0.832 0.838 0.803 0.986 0.884 0.884 τ 51.7 49.3 17.9 8.7 12.6 15.0 91.7 89.9 14.9 2.8 5.9 9.9 Error 0.0260.018 0.1320.054 0.5710.193 0.6180.226 0.4850.116 0.0240. PackageLift Tilt 0.635 0.154 0.551 0.161 0.561 1.143 0.953 0.021 0.832 0.032 0.717 0.062 τ 32.2 33.5 39.2 10.9 13.7 21.0 32.2 43.8 68.2 10.7 9.1 15.1 Table 2: Quantitative comparison between our method and baseline methods on H1-Medium tasks. Task Metrics PPO [40] DreamerV3 [12] HB [41] Sequential MCP [36] Ours Error 0.4330.059 0.4590.157 0.4590.047 0.4580.045 0.4210.026 0.0070. BoxTransfer Tilt 0.675 0.160 0.666 0.164 0.762 0.131 0.984 0.028 0.894 0.034 0.884 0.055 τ 46.8 40.1 25.6 12.2 15.3 16.7 37.6 35.9 23.0 5.8 7.8 17.0 Error 0.0200.008 0.1590.039 0.4430.120 0.4280.110 0.1520.030 0.0130.008 PackageCarry Tilt 0.331 0.330 0.336 0.008 0.032 0. 0.727 0.756 0.838 0.943 0.871 0.787 τ 44.5 42.3 18.3 10.5 18.0 21.3 53.0 46.4 21.7 8.0 28.4 28.9 Table 3: Quantitative comparison between our method and baseline methods on H1-Hard tasks. 5.2 Results and Analysis We show our main results of different difficulty levels in Table 1, 2, and 3. Cells highlighted in red indicate that the mean error exceeds the success threshold, signifying failure to learn successful policy, while green denotes successful policies. We bold the best metric of all successful policies and underline the second-best. Our method significantly outperforms all baselines across most tasks and metrics, highlighting its clear advantages with respect to task success and motion feasibility. Qualitative comparisons in Fig. 4 further show that our method produces more accurate, natural, and feasible behaviors compared to baseline approaches. We first compare our framework with vanilla RL algorithms that learn these tasks from scratch. Although vanilla PPO [40] and DreamerV3 [12] can succeed in Easy tasks, they struggle with most Medium and Hard tasks. Moreover, vanilla RL exhibits severe reward hacking on these easy tasks with extreme motions, as shown in their poor feasibility metrics and qualitative results. In contrast, our method shows more accurate, natural, and feasible behaviors on all tasks due to our more structured exploration and more flexible skill blending. We also compare our method with ones with different hierarchy designs. Compared to the HumanoidBench baseline (HB) [41], our method shows consistently better performance, due to our better low-level representations that not only decouple different humanoid functionalities but also provide natural regularization that mitigates reward hacking. Compared with Sequential HRL adopted in many previous works [29, 23, 4], we found that in humanoid learning, this paradigm leads to far worse performance than skill blending, failing to learn all the possible tasks. This occurs because, for humanoids, each primitive skill controls specific body range, making it more effective to activate multiple skills simultaneously for whole-body coordination. For example, when carrying box, both Walking and Reaching should be activated at the same time to hold the box while moving. Compared to MCP [36] which has comparable or slightly better performance to ours on certain easy tasks, it struggles with harder tasks, barely successfully learning them except BoxPush, emphasizing the superiority of our vectorized skill blending mechanism that fosters learning complex tasks. Our analysis suggests that the strength of our framework stems from the structural priors from the low-level primitive skills that provide extra robustness and regularization, effectively reducing the RL search space, improving sample efficiency, and mitigating reward hacking. Our method simplifies the problem of task-specific policy optimization by constraining the search space to combinations of high-quality, pretrained primitives, which inherently regularizes behaviors to minimize reward hacking. As such, SkillBlender not only significantly outperforms all baselines in task success but also yields more feasible and well-behaved humanoid motion. 8 Figure 4: Qualitative comparison between different methods. Our SkillBlender not only achieves higher task accuracy, but also avoids reward hacking and yields more natural and feasible movements. Task Metrics w/o Walking w/o Reaching w/o Softmax HumanPlus [8] ExBody [5] Ours Error 0.4080.223 0.1720.061 0.0320.023 0.0240.008 0.0490.030 0.0210.012 FarReach Tilt 0.835 0.051 0.995 0.039 0.821 0.129 0.915 0.207 0.980 0.051 0.045 0.955 BoxPush PackageCarry τ 16.6 12.9 22.3 19.1 13.8 13.5 9.3 27.7 29.4 34.2 20.8 20. Error 0.0320.028 0.0650.093 0.0940.048 0.0150.008 0.0210.013 0.0090.007 Tilt 0.021 0.039 0.209 0.228 0.036 0.064 0.874 0.932 0.796 0.884 0.877 0.884 τ 16.8 13.2 50.6 21.5 16.2 15.0 10.1 20.5 44.9 14.3 11.8 9.9 Error 0.3830.100 0.3620.077 0.0460.018 0.0230.014 0.4130.072 0.0130. Tilt 0.030 0.033 0.163 0.258 0.093 0.043 0.628 0.836 0.603 0.742 0.784 0.787 τ 18.8 12.8 39.9 27.3 13.0 21.3 14.4 6.1 50.3 35.6 9.4 28.9 Table 4: Ablation studies on tasks of different difficulty levels. Our method shows consistently better performance, validating the effectiveness of our framework design. 5.3 Ablation Studies To further analyze our framework design, we conduct ablation studies on various components to highlight the importance of each element in our method and validate different hierarchy designs. We evaluate on three tasksFarReach, BoxPush, and PackageCarryrepresenting different difficulty levels, using the H1 robot. The results for the task Error across different ablation methods are shown in Table 4. As illustrated, removing either Walking or Reaching leads to significant performance degradation due to restricted search space, highlighting the essential role of basic primitive skills. We also verified the effectiveness of the Softmax layer on the raw weights produced by the highlevel controller network, as in Eq. 1. As shown in the results, removing the Softmax layer leads to significantly worse performance, especially in feasibility metrics. This is in line with the non-linearity provided by the Softmax layer, which effectively produces weight constraints, reduces reward hacking, and generates more natural and feasible movements. We also experimented with using human motion trackers as low-level policies [13]. We train HumanPlus [8] and ExBody [5] using the aligned state st same as our settings, and train high-level controller to output the trackers input goal for each task. As shown in Table 4, both trackers underperform our SkillBlender, demonstrating the superiority of our skill blending hierarchy. HumanPlus [8] tracks whole-body motions, which can be seen as regularized version of PPO. However, this regularization also leads to the policys inability to achieve higher task success. Moreover, it does not fully resolve the reward hacking issue as shown in Fig. 4. In ExBody [5], humanoid control is split into two body parts, limiting the high-level controllers exploration, making it difficult to learn complex tasks. In contrast, our method is more accurate, thanks to its more structured and versatile action space derived from different primitive skills and their dynamic blending."
        },
        {
            "title": "6 Conclusions, Limitations, and Future Works",
            "content": "Conclusions In this paper, we introduced SkillBlender, novel pretrain-then-blend framework for versatile humanoid whole-body loco-manipulation. At the core of SkillBlender is to pretrain primitive skills and dynamically blend them for complex loco-manipulation tasks with minimal reward engineering. We also proposed new benchmark, SkillBench, which is parallel, cross-embodiment, and diverse, to benchmark humanoid whole-body loco-manipulation scientifically. Extensive simulated experiments demonstrate the effectiveness of our framework, showcasing SkillBlenders capabilities of performing complex and challenging whole-body loco-manipulation tasks accurately and naturally. We hope our method and benchmark can benefit future open research on humanoid learning. 9 Limitations and Future Works Despite compelling results, our work has certain limitations that can be further improved in future works. First, our study primarily focuses on whole-body locomanipulation using the humanoids forearms, without incorporating specific end-effectors such as parallel grippers or dexterous hands. Additionally, we have not yet deployed our high-level task policies in real-world scenarios, due to the reliance on state-based observations and the sim2real gap. As part of future work, we plan to explore more effective real2sim physics alignment techniques (e.g., [17]) to enable agile and robust humanoid movements in the real world. We also hope that future research in related domains such as design and development of simulation-friendly humanoid hardware and advanced vision-based reinforcement learning algorithms will help address these challenges and further advance the field of humanoid learning."
        },
        {
            "title": "Acknowledgments",
            "content": "We express our sincere gratitude to Guanya Shi, Hongsuk Choi, Jialiang Zhang, Jilong Wang, Luis A. Pabon for fruitful discussions. We also thank Unitree for their hardware support. The USC Geometry, Vision, and Learning Lab acknowledges generous supports from Toyota Research Institute, Dolby, Google DeepMind, Capital One, Nvidia, and Qualcomm. Yue Wang is also supported by Powell Research Award. Pieter Abbeel holds concurrent appointments as professor at UC Berkeley and as an Amazon Scholar. This paper describes work performed at UC Berkeley and is not associated with Amazon."
        },
        {
            "title": "References",
            "content": "[1] Firas Al-Hafez, Guoping Zhao, Jan Peters, and Davide Tateo. Locomujoco: comprehensive imitation learning benchmark for locomotion. arXiv preprint arXiv:2311.02496, 2023. [2] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017. [3] Qingwei Ben, Feiyu Jia, Jia Zeng, Junting Dong, Dahua Lin, and Jiangmiao Pang. Homie: Humanoid loco-manipulation with isomorphic exoskeleton cockpit. arXiv preprint arXiv:2502.13013, 2025. [4] Xuxin Cheng, Ashish Kumar, and Deepak Pathak. Legs as manipulator: Pushing quadrupedal agility beyond locomotion. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 51065112. IEEE, 2023. [5] Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, and Xiaolong Wang. Expressive whole-body control for humanoid robots. arXiv preprint arXiv:2402.16796, 2024. [6] Nikita Chernyadev, Nicholas Backshall, Xiao Ma, Yunfan Lu, Younggyo Seo, and Stephen James. Bigym: demo-driven mobile bi-manual manipulation benchmark. arXiv preprint arXiv:2407.07788, 2024. [7] Alejandro Escontrela, Xue Bin Peng, Wenhao Yu, Tingnan Zhang, Atil Iscen, Ken Goldberg, and Pieter Abbeel. Adversarial motion priors make good substitutes for complex reward functions. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2532. IEEE, 2022. [8] Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, and Chelsea Finn. Humanplus: Humanoid shadowing and imitation from humans. arXiv preprint arXiv:2406.10454, 2024. [9] Ahmad Gazar, Majid Khadiv, Andrea Del Prete, and Ludovic Righetti. Stochastic and robust mpc for bipedal locomotion: comparative study on robustness and performance. In 2020 IEEE-RAS 20th International Conference on Humanoid Robots (Humanoids), pages 6168. IEEE, 2021. [10] Haoran Geng, Feishi Wang, Songlin Wei, Yuyang Li, Bangjun Wang, Boshi An, Charlie Tianyue Cheng, Haozhe Lou, Peihao Li, Yen-Jen Wang, et al. Roboverse: Towards unified platform, dataset and benchmark for scalable and generalizable robot learning. arXiv preprint arXiv:2504.18904, 2025. 10 [11] Xinyang Gu, Yen-Jen Wang, Xiang Zhu, Chengming Shi, Yanjiang Guo, Yichen Liu, and Jianyu Chen. Advancing humanoid locomotion: Mastering challenging terrains with denoising world model learning. arXiv preprint arXiv:2408.14472, 2024. [12] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. [13] Nicklas Hansen, Jyothir SV, Vlad Sobal, Yann LeCun, Xiaolong Wang, and Hao Su. Hierarchical world models as visual whole-body humanoid controllers. arXiv preprint arXiv:2405.18418, 2024. [14] Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Omnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning. arXiv preprint arXiv:2406.08858, 2024. [15] Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learning human-to-humanoid real-time whole-body teleoperation. arXiv preprint arXiv:2403.04436, 2024. [16] Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, et al. Hover: Versatile neural whole-body controller for humanoid robots. arXiv preprint arXiv:2410.21229, 2024. [17] Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, et al. Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. arXiv preprint arXiv:2502.01143, 2025. [18] Xialin He, Runpei Dong, Zixuan Chen, and Saurabh Gupta. Learning getting-up policies for real-world humanoid robots. arXiv preprint arXiv:2502.12152, 2025. [19] Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller, and David Silver. Learning and transfer of modulated locomotor controllers. arXiv preprint arXiv:1610.05182, 2016. [20] Tao Huang, Junli Ren, Huayi Wang, Zirui Wang, Qingwei Ben, Muning Wen, Xiao Chen, Jianan Li, and Jiangmiao Pang. Learning humanoid standing-up control across diverse postures. arXiv preprint arXiv:2502.08378, 2025. [21] Yildirim Hurmuzlu, Frank Génot, and Bernard Brogliato. Modeling, stability and control of biped robotsa general framework. Automatica, 40(10):16471664, 2004. [22] Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, and Xiaolong Wang. Exbody2: Advanced expressive humanoid whole-body control. arXiv preprint arXiv:2412.13196, 2024. [23] Yandong Ji, Zhongyu Li, Yinan Sun, Xue Bin Peng, Sergey Levine, Glen Berseth, and Koushil Sreenath. Hierarchical reinforcement learning for precise soccer shooting skills using quadrupedal robot. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 14791486. IEEE, 2022. [24] Charles Khazoom, Seungwoo Hong, Matthew Chignoli, Elijah Stanger-Jones, and Sangbae Kim. Tailoring solution accuracy for fast whole-body model predictive control of legged robots. IEEE Robotics and Automation Letters, 2024. [25] Yuxuan Kuang, Hai Lin, and Meng Jiang. Openfmnav: Towards open-set zero-shot object navigation via vision-language foundation models. arXiv preprint arXiv:2402.10670, 2024. [26] Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, and Yue Wang. Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. arXiv preprint arXiv:2407.04689, 2024. [27] Niranjan Kumar, Irfan Essa, and Sehoon Ha. Cascaded compositional residual learning for complex interactive behaviors. IEEE Robotics and Automation Letters, 8(8):46014608, 2023. [28] Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science robotics, 5(47):eabc5986, 2020. [29] Chengshu Li, Fei Xia, Roberto Martin-Martin, and Silvio Savarese. Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators. In Conference on Robot Learning, pages 603616. PMLR, 2020. [30] Jialong Li, Xuxin Cheng, Tianshu Huang, Shiqi Yang, Ri-Zhao Qiu, and Xiaolong Wang. Amo: Adaptive motion optimization for hyper-dexterous humanoid whole-body control, 2025. URL https://arxiv.org/abs/2505.03738. [31] Yun Liu, Bowen Yang, Licheng Zhong, He Wang, and Li Yi. Mimicking-bench: benchmark for generalizable humanoid-scene interaction learning via human mimicking. arXiv preprint arXiv:2412.17730, 2024. [32] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1089510904, 2023. [33] Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris Kitani, and Weipeng Xu. Universal humanoid motion representations for physics-based control. arXiv preprint arXiv:2310.04582, 2023. [34] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning, 2021. [35] Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning robust perceptive locomotion for quadrupedal robots in the wild. Science robotics, 7(62):eabk2822, 2022. [36] Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. Mcp: Learning composable hierarchical control with multiplicative compositional policies. Advances in neural information processing systems, 32, 2019. [37] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. ACM Transactions On Graphics (TOG), 41(4):117, 2022. [38] Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, and Jitendra Malik. Humanoid locomotion as next token prediction. arXiv preprint arXiv:2402.19469, 2024. [39] Novella Ruffin. Human growth and development-a matter of principles. 2009. [40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [41] Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. arXiv preprint arXiv:2403.10506, 2024. [42] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito. Neural state machine for characterscene interactions. ACM Transactions on Graphics, 38(6):178, 2019. [43] Richard Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2): 181211, 1999. [44] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. 12 [45] Bart van Marum, Aayam Shrestha, Helei Duan, Pranay Dugar, Jeremy Dao, and Alan Fern. Revisiting reward design and evaluation for robust humanoid standing and walking. arXiv preprint arXiv:2404.19173, 2024. [46] Tingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler. Unicon: Universal neural controller for physics-based character motion. arXiv preprint arXiv:2011.15119, 2020. [47] Yinhuai Wang, Qihan Zhao, Runyi Yu, Ailing Zeng, Jing Lin, Zhengyi Luo, Hok Wai Tsui, Jiwen Yu, Xiu Li, Qifeng Chen, et al. Skillmimic: Learning reusable basketball skills from demonstrations. arXiv preprint arXiv:2408.15270, 2024. [48] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [49] Yang, Yuan, Zhu, Yu, and Li. Multi-expert learning of adaptive legged locomotion. sci, 2020. [50] Yanjie Ze, Zixuan Chen, JoÃG, Pedro AraÃšjo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, arXiv preprint and Karen Liu. Twist: Teleoperated whole-body imitation system. arXiv:2505.02833, 2025. [51] Chong Zhang, Wenli Xiao, Tairan He, and Guanya Shi. Wococo: Learning whole-body humanoid control with sequential contacts. arXiv preprint arXiv:2406.06005, 2024. [52] Jiazhao Zhang, Nandiraju Gireesh, Jilong Wang, Xiaomeng Fang, Chaoyi Xu, Weiguang Chen, Liu Dai, and He Wang. Gamma: Graspability-aware mobile manipulation policy learning based on online grasping pose fusion. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 13991405. IEEE, 2024. [53] Ziwen Zhuang and Hang Zhao. Embrace collisions: Humanoid shadowing for deployable contact-agnostics motions. arXiv preprint arXiv:2502.01465, 2025. [54] Ziwen Zhuang, Shenzhe Yao, and Hang Zhao. Humanoid parkour learning. arXiv preprint arXiv:2406.10759, 2024."
        },
        {
            "title": "A Observation Space",
            "content": "For state-based policies, the observation space for the actor (goal gt and state st) comprises 3d+6+N dimensions (d is the robots DoF), in which st comprises 3d + 6 dimensions (joint angles, joint velocities, last actions, base angular velocity, and base Euler angles or projected gravity), and gt comprises of dimensions based on the task specification. Observation space for vision-based policies is discussed in Sec. G.3."
        },
        {
            "title": "B Action Space",
            "content": "The action space for each low-level primitive skill πi (or baseline methods like PPO [40], DreamerV3 [12]) is Rd. And for the high-level controller πH that blends low-level primitive skills {πj L}, the action dimension dH is: L, . . . , πk dH = (cid:88) i=j (Ni + d) (3) comprising the subgoal gi RNi and vector weight [0, 1]d for each primitive skill πi L. Low-Level Primitive Skill Specifications C.1 Walking Objective. Walk in given velocity command. Goal input. Linear velocity command on xy axes and angular velocity command on the yaw axis. C.2 Reaching Objective. Reach two 3D target points using its two wrists while standing still. Goal input. The relative distances between the humanoids wrists and respective target points. C.3 Squatting Objective. Squat down or stand up to reach target root height. Goal input. The relative root height between the current root height and the target height. C.4 Stepping Objective. Step its two feet on sampled points on the ground. Goal input. The relative distances between the humanoids feet and respective target points. High-Level Loco-Manipulation Task Specifications In this section, we take Unitree H1 as representative platform to describe our high-level task specifications, including the task objective, goal input, skills, success threshold, and reward function. The reward functions are kept simple and intuitive, consisting of only one or two terms, thereby requiring minimal reward engineering. D.1 FarReach Objective. Reach two distant 3D points using both hands. Goal input. The relative distances between the humanoids wrists and respective target points. Skills. Walking + Reaching Success threshold. 0.05m. Reward. The reward function is defined as: R(s, a) = 5e4pwr ˆpwr (4) D.2 ButtonPress Objective. Press wall-mounted button with the left wrist while keeping the right arms pose. Goal input. The relative distances between the humanoids left wrist and the button. Skills. Walking + Reaching Success threshold. 0.05m. Reward. The reward function is defined as: R(s, a) = 5e4pwrpbn + 0.5e4qra (5) D.3 CabinetClose Objective. Close an open cabinet in front of the humanoid. Goal input. (1) The articulation angles for the articulated cabinets two doors; (2) The relative distances between the humanoids wrists and the cabinet. Skills. Walking + Reaching Success threshold. 0.01m. Reward. The reward function is defined as: R(s, a) = 5e4pwrparti + 5e4qarti (6) D.4 FootballShoot Objective. Shoot football towards goal position. Goal input. (1) The relative distance between the ball and the goal; (2) The relative distance between the humanoids torso and the ball. Skills. Walking + Stepping Success threshold. 1.5m. Reward. The reward function is defined as: R(s, a) = e4pxy torsopxy oriball + 5epballpgoal (7) D.5 BoxPush Objective. Push box on table to target position. Goal input. (1) The relative distance between the box and the target; (2) The relative distance between the humanoids wrists and the box. Skills. Walking + Reaching Success threshold. 0.05m. Reward. The reward function is defined as: 15 R(s, a) = 5e4pbox ˆpbox + 5e4pwrpbox (8) D.6 PackageLift Objective. Lift package to specified height. Goal input. (1) The relative distance between the package and the target; (2) The relative distance between the humanoids wrists and the package. Skills. Reaching + Squatting Success threshold. 0.1m. Reward. The reward function is defined as: R(s, a) = 5e4hpkg ˆhpkg + 5e4pwrppkg (9) D.7 BoxTransfer Objective. Transfer box from one table to target location on another table. Goal input. (1) The relative distance between the box and the target; (2) The relative distance between the humanoids wrists and the box. Skills. Walking + Reaching Success threshold. 0.05m. Reward. The reward function is defined as: R(s, a) = 5e4pbox ˆpbox + e4pwrpbox (10) D.8 PackageCarry Objective. Carry package to distant location. Goal input. (1) The relative distance between the package and the target; (2) The relative distance between the humanoids wrists and the package. Skills. Walking + Reaching + Squatting Success threshold. 0.1m. Reward. The reward function is defined as: R(s, a) = 5e4ppkg ˆppkg + 5e4pwrppkg (11)"
        },
        {
            "title": "E Implementation Details",
            "content": "E.1 Skill Selector In this work, the skills are manually selected to blend. However, given the task and skill specifications and descriptions, we can leverage an LLM to perform common-sense reasoning [48] to select relevant skills automatically. In Fig. 5 we show an example of GPT-4o selecting skills for the FarReach task. E.2 Baseline Implementations We use the standard PPO [40] and DreamerV3 [12] implementations for our baselines. And for HumanoidBench baseline (HB) [41], we freeze the PPO-trained FarReach policy as the low-level two-hand reaching policy and then train high-level controller, which is also implemented in PPO. Note that for HB, FarReach is not applicable (N/A) since there is no high-level controller. 16 Figure 5: An example of GPT-4o reasoning to perform skill selection on the FarReach task. For all model-free RL methods, we use 4096 parallel environments during training. Due to training stability and speed constraints, we set the number of parallel environments to 16 for the model-based DreamerV3 [12] baseline. E.3 Neural Network Architechtures All state-based policy networks are implemented as end-to-end MLPs. Actors are all MLPs with [512, 256, 128] hidden units, and critics are all MLPs with [768, 256, 128] hidden units. Critics can also access privileged observations such as base linear velocity, body mass, contact mask, etc. For visual RL, the image encoders are implemented as vanilla CNNs. E.4 Training Details For all goal-conditioned model-free RL methods in this work, we employ Proximal Policy Optimization (PPO) [40] to optimize the policy. For PPO training, we set the entropy coefficient to 0.001, learning rate to 1e-5, and number of mini-batches to 4. In addition, we put γ = 0.994 and λ = 0.9. E.5 Compute Resources We use NVIDIA RTX 4090/A6000/A100 GPU for training our low-level skills and high-level controllers. All the training and inference are done on single GPU. RAM required is less than 24GB in the training stage, and less than 12GB in the inference stage. For easy tasks like FarReach or ButtonPress it typically takes 12-24 hours to finish training, while harder tasks take longer, typically 24-72 hours."
        },
        {
            "title": "F Morphology Details",
            "content": "As shown in Fig. 3, our SkillBench is designed for cross-embodiment compatibility, supporting three distinct humanoid models. We specifically choose Unitree humanoids due to their widespread adoption in recent years. The supported models include: Unitree H1. The most widely used humanoid embodiment in prior works [15, 14, 8, 51]. It stands approximately 1.7 meters tall and features 19 degrees of freedom (DoFs), including two 3-DoF shoulders, two 1-DoF elbows, 1-DoF yaw joint in the torso, two 3-DoF hips, two 1-DoF knees, and two 1-DoF pitch ankle joints. Unitree G1. smaller humanoid model, standing around 1.2 meters tall. It features an additional roll DoF on each ankle, increasing the total DoF count to 21. Unitree H1-2. Morphologically similar to G1, with 2-DoF ankles and total of 21 DoFs, but comparable in size and shape to H1. 17 Task Metrics PPO [40] Ours Error 0.0190.009 0.0230.018 FarReach Tilt 0.754 0.220 0.214 0.715 τ 13.3 11.0 32.5 30. Error 0.0140.007 0.0320.083 ButtonPress Tilt 0.573 0.930 0.647 0.196 CabinetClose τ 20.9 13.2 56.5 38.2 Error 0.6220.469 0.0000. Tilt 0.552 0.234 0.674 0.647 τ 25.8 12.6 63.6 22.6 Table 5: Quantitative comparison between our method and baseline methods on G1-Easy tasks. Task Metrics PPO [40] Ours FootballShoot Error 1.7330.236 1.4760.276 Tilt 0.309 0.190 0.756 0.664 τ 20.8 17.7 79.6 70. Error 0.0750.128 0.0390.096 BoxPush Tilt 0.462 0.176 0.657 0.615 τ 25.1 15.4 65.3 54. Error 0.2260.070 0.0740.078 PackageLift Tilt 0.557 0.545 0.773 0.346 τ 25.8 20.4 36.3 57.6 Table 6: Quantitative comparison between our method and baseline methods on G1-Medium tasks. Task Metrics PPO [40] Ours Error 0.4890.047 0.3400.021 BoxTransfer Tilt 0.676 0.538 0.496 0.304 τ 26.7 15.9 37.5 50.9 Error 0.2910.055 0.0580.069 PackageCarry Tilt 1.088 0. 0.313 0.324 τ 29.7 15.0 102.5 47.4 Table 7: Quantitative comparison between our method and baseline methods on G1-Hard tasks. Task Metrics PPO [40] Ours Error 0.0130.005 0.0490. FarReach Tilt 0.914 0.309 0.919 0.077 τ 32.0 15.2 37.6 18.5 Error 0.0270.017 0.0230.024 ButtonPress Tilt 0.728 0.392 0.879 0.090 CabinetClose τ 26.9 18.9 31.2 26.5 Error 0.0010.003 0.0000.000 Tilt 0.339 0.121 0.779 0.897 τ 45.7 20. 53.7 24.6 Table 8: Quantitative comparison between our method and baseline methods on H1-2-Easy tasks. Task Metrics PPO [40] Ours FootballShoot BoxPush Error 1.5690.244 1.4000. Tilt 0.232 0.095 0.796 0.871 τ 31.3 27.0 129.2 80.4 Error 0.0400.027 0.0500.094 Tilt 0.311 0. 0.862 0.884 τ 38.6 17.1 34.1 11.6 Error 0.4700.118 0.5570.189 PackageLift Tilt 0.434 1.272 0.686 0.079 τ 48.9 16. 66.9 18.9 Table 9: Quantitative comparison between our method and baseline methods on H1-2-Medium tasks. Task Metrics PPO [40] Ours Error 0.3970.024 0.3150.046 BoxTransfer Tilt 0.962 0.393 0.829 0.072 PackageCarry τ 32.0 17.4 27.9 13.6 Error 0.3730.061 0.0510.019 Tilt 0.802 0.038 0.594 0.732 τ 35.8 23. 43.8 34.3 Table 10: Quantitative comparison between our method and baseline methods on H1-2-Hard tasks."
        },
        {
            "title": "G Additional Experiments",
            "content": "G.1 Results on G1 and H1-2 Embodiments We also analyze the performance of different humanoid embodiments across various tasks. We compare our method with PPO [40] on the G1 and H1-2 embodiments. Quantitative results are shown in Table. 5, 6, 7, and 8, 9, 10, and we illustrate qualitative results in Fig. 6. As shown in the results, our method outperforms PPO by large margin, producing more accurate and natural movements, demonstrating our frameworks superiority across multiple humanoid embodiments. Comparing H1 results with G1 and H1-2, we also observe that G1 and H1-2 generally exhibit higher task errors than H1. This discrepancy is primarily due to their increased degrees of freedom (DoFs), particularly the additional ankle roll DoFs, which introduce greater instability. These results suggest that even small increase in foot articulation can significantly increase task difficulty. G.2 Skill Blending Decomposition To provide more intuitive understanding of our framework, especially our proposed skill blending mechanism, Fig. 7 visualizes the whole-body per-joint weights at different stages of FarReach, ButtonPress, and BoxPush, all of which blend Walking and Reaching. This visualization highlights the spatial-temporal decomposition of our skill blending, where the two skills interleave rather than one skill dominating the overall motion. For example, in FarReach, we observe clear spatial decomposition: Walking primarily influences the lower body, while Reaching governs upper-body 18 Figure 6: Qualitative results on G1 and H1-2 embodiments. Our method produces more accurate and natural movements, validating our frameworks superiority across multiple embodiments. Figure 7: Visualization of whole-body per-joint weights at different stages of three different tasks. More blue means more Reaching, and more green means more Walking. Figure 8: Ego-centric visual observations, including RGB, depth (point cloud), and segmentation masks. movements. Similarly, in ButtonPress, the contribution of Reaching progressively increases throughout the task, particularly as the left wrist approaches the button. G.3 Vision-Based Policy Learning Although our framework SkillBlender is currently state-based, we have also conducted some preliminary research on vision-based RL, given its high potential in real-world applications. As shown in Fig. 8, our SkillBench supports ego-centric visual observations, including RGB images, depth images (point clouds), and segmentation masks. In this work, we trained vision-based policies using ego-centric RGB images, using PPO [40], DreamerV3 [12], and our SkillBlender, on the H1 BoxPush task. Due to the rendering and training speed, we set the number of parallel environments to 4 and 64, and the image resolution to 64 48. In Table 11, we present the maximum task mean reward for different methods across various parallel environment settings. Our method outperforms PPO and DreamerV3 in both the 4 and 64 environment 19 # Environments PPO [40] DreamerV3 [12] Ours 4 21.53 25.42 28.14 64 32.34 34.00 37.90 Table 11: Maximum task mean reward () on vision-based H1 BoxPush task. Figure 9: Demonstrations of our primitive skill sim2real deployment. We control the humanoid to perform periodical Reaching and Squatting. configurations, demonstrating its effectiveness in both state-based and vision-based settings. However, it is important to note that due to the challenges in visual RL and limited parallel environments, none of these methods were able to successfully complete the task, which is why we focus on comparing task mean rewards. This highlights the importance of highly parallelized simulations. We hope that future advancements in efficient and high-quality parallel rendering will further support ego-centric vision-based humanoid learning. Real-World Skill Deployment We utilize Unitree H1 humanoid robot to deploy our simulation-trained policies in the real world as sanity check on sim2real transfer. As shown in Fig. 9, we successfully deployed our primitive skills in the real world and controlled them with goal conditions. For robust sim2real transfer, we leverage larger domain randomization and incorporate projected gravity input. Video results can be found in the supplementary material, where we control the primitive skills to perform various task-agnostic periodical movements. In future works, we aim to distill our state-based high-level task policies into vision-based policies and directly deploy them to the real world."
        },
        {
            "title": "I Common Failure Cases",
            "content": "We observe several common failure modes in both our method and baseline methods. For primitive skills, despite applying domain randomization during simulation training, policies can struggle when faced with out-of-distribution states such as highly unusual initial poses leading to failure to initiate motion properly. This issue is further exacerbated in real-world settings, where sensor noise is more significant. For high-level tasks in simulation, particularly Hard tasks with longer horizons (e.g. BoxTransfer), we occasionally observe that the humanoid prematurely ceases exploration upon receiving relatively high intermediate reward. This results in suboptimal behaviors where the humanoid stays in local minima without completing the full task."
        },
        {
            "title": "J Broader Impacts",
            "content": "This work has the potential for several positive societal impacts, including applications in elder care, assistance in hazardous or inaccessible environments, and improved autonomy in service robotics. However, potential negative consequences include job displacement and increased overreliance on robotic systems. We encourage responsible development and deployment of such technologies and hope our contributions ultimately lead to broad societal benefit."
        }
    ],
    "affiliations": [
        "Peking University",
        "Stanford University",
        "University of California, Berkeley",
        "University of Southern California"
    ]
}