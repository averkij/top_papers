{
    "paper_title": "Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs",
    "authors": [
        "Haochen Wang",
        "Yuhao Wang",
        "Tao Zhang",
        "Yikang Zhou",
        "Yanwei Li",
        "Jiacong Wang",
        "Ye Tian",
        "Jiahao Meng",
        "Zilong Huang",
        "Guangcan Mai",
        "Anran Wang",
        "Yunhai Tong",
        "Zhuochen Wang",
        "Xiangtai Li",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been a promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehen- sive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides a more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos."
        },
        {
            "title": "Start",
            "content": "Grasp Any Region: Towards Precise, Contextual"
        },
        {
            "title": "Pixel Understanding for Multimodal LLMs",
            "content": "Haochen Wang1,2*, Yuhao Wang3* , Tao Zhang4 , Yikang Zhou4, Yanwei Li5, Jiacong Wang2, Ye Tian3, Jiahao Meng3, Zilong Huang5, Guangcan Mai5, Anran Wang5, Yunhai Tong3 , Zhuochen Wang5, Xiangtai Li5, Zhaoxiang Zhang1,2 1NLPR, MAIS, CASIA 2UCAS 3PKU 4WHU 5ByteDance Equal Contribution Corresponding Authors https://github.com/Haochen-Wang409/Grasp-Any-Region https://huggingface.co/HaochenWang/GAR-1B"
        },
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) excel at holistic understanding, they struggle in capturing the dense world with complex scenes, requiring fine-grained analysis of intricate details and object inter-relationships. Region-level MLLMs have been promising step. However, previous attempts are generally optimized to understand given regions in isolation, neglecting crucial global contexts. To address this, we introduce Grasp Any Region (GAR) for comprehensive region-level visual understanding. Empowered by an effective RoI-aligned feature replay technique, GAR supports (1) precise perception by leveraging necessary global contexts, and (2) modeling interactions between multiple prompts. Together, it then naturally achieves (3) advanced compositional reasoning to answer specific free-form questions about any region, shifting the paradigm from passive description to active dialogue. Moreover, we construct GAR-Bench, which not only provides more accurate evaluation of single-region comprehension, but also, more importantly, measures interactions and complex reasoning across multiple regions. Extensive experiments have demonstrated that GAR-1B not only maintains the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5 on DLC-Bench, but also excels at modeling relationships between multiple prompts with advanced comprehension capabilities, even surpassing InternVL3-78B on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong capabilities can be easily transferred to videos. Date: October 22, 2025 Correspondence: {wanghaochen2022, zhaoxiang.zhang}@ia.ac.cn, xiangtai94@gmail.com 5 2 0 2 1 2 ] . [ 1 6 7 8 8 1 . 0 1 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "The ambition of Multimodal Large Language Models (MLLMs) is to endow machines with human-like abilities to perceive, interpret, and reason about the dense visual world [21, 22, 60]. To date, renowned state-of-the-art models [1, 10, 3133, 50, 52] have made remarkable strides, excelling in answering general questions about an entire image. However, this global-level perception struggles with the dense understanding of cluttered environments, intricate object details, and the complex interplay between multiple entities. 1 Figure 2 Illustration of our GAR, which is superior at leveraging necessary global context to (a) generate precise captions, where green is correct and red means wrong, (b) model complex interactions among multiple prompts, and perform reasoning such as (c) recognizing non-entities. Colors of <Prompt0>, <Prompt1>, and <Prompt2> correspond to masks with respective colors. Images are sampled from [39], [23], and [30] for (a), (b), and (c), respectively. To address the limitation of global perception, several previous works [3, 22, 25, 60, 64] argue for paradigm shift to region-level MLLMs. Specifically, they equip MLLMs with promptable and finegrained interactions to achieve targeted region-level understanding, using boxes [3, 64] or masks [22, 60]. This mechanism transforms the model from passive observer of the entire scene into an active participant capable of deep, localized analysis. Conventional region MLLMs [22, 25] mainly focus on the ability to generate descriptive caption for single region, and thus model architectures are generally optimized to understand given region in isolation. This design often neglects crucial global context, e.g., misidentifying frog-shaped slipper as real frog in Figure 2a. Alternatives [58, 60] that employ pooled local features suffer from insufficient details. Therefore, unified framework that can simultaneously resolve these issues to facilitate more sophisticated and interactive capabilities remains significant area for investigation. To this end, we propose Grasp Any Region (GAR) for comprehensive region understanding. As shown in Figure 2, key features include: Figure 1 Performance comparison. GAR achieves strong performances not only on region-level understanding, but also on general multimodal benchmarks. (1) Precise Perception. Thanks to the leverage of necessary global contexts, GAR achieves more precise perception of given regions, which is the fundamental capability for region MLLMs. As shown in Figure 2a by aggregating information from the broader, unmasked scene, our GAR manages to generate much more accurate descriptions than previous crop-based approaches [22]. (2) Interactions between Multiple Prompts. Our framework moves beyond the prevailing single-prompt paradigm, which treats every region of interest as an isolated entity. As illustrated in Figures 2b and 2c, GAR manages to model relationships between an arbitrary number of prompts. (3) Advanced Compositional Reasoning Capabilities. Empowered with the aforementioned features, GAR is naturally equipped with advanced compositional reasoning capabilities, allowing it to answer any specific free-form questions, e.g., recognizing non-entities shown in Figure 2c. To achieve these capabilities, effectively encoding global contexts becomes equally crucial as local detailed features. To this end, we propose an RoI-aligned feature replay technique. Specifically, GAR first encodes the full, uncropped image (together with the mask prompt) with AnyRes [27]. Subsequently, RoI-Align [16] 2 is employed to gather relevant features directly from the global feature map. Those gathered features are inherently context-aware, providing sufficient local details while maintaining global information simultaneously. Please refer to Figure 3 for the detailed pipeline. Furthermore, we introduce GAR-Bench, which not only provides more accurate evaluation of single-region comprehension by constructing multiple-choice questions, but also, more importantly, measures interaction and complex reasoning across multiple regions. It includes test cases that require model to aggregate information from multiple visual regions to arrive at correct conclusion, thereby quantifying the ability to interpret the whole scene rather than independent parts. Empirically, shown in Figure 1, our GAR-1B not only outperforms DAM-3B [22] and PAM-3B [25] on detailed captioning benchmarks [22, 24, 58], but also excels in general multimodal benchmarks [4, 43, 51, 53]. Interestingly, it even outperforms large-scale models like InternVL3-78B [66] on GAR-Bench, demonstrating its advanced comprehension capability in modeling interactions between multiple prompts. More importantly, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong comprehension capabilities can be easily transferred to videos. We hope our work inspires the community to develop MLLMs that can perceive and understand the dense visual world more effectively."
        },
        {
            "title": "2 Related Works",
            "content": "Multimodal Large Language Models (MLLMs). Typical MLLMs [1, 19, 19, 20, 26, 27, 42, 46, 47, 52, 56, 57, 66] project visual features extracted from pre-trained visual encoders [35, 62] to LLM for understanding multimodal contents. However, these models usually lack precise localization capabilities [22, 25] and struggle to understand specific regions. One potential solution is to think with images [33, 45, 48]. But these agentic models require complex multi-turn conversations, while we mainly focus on precise perception within single-turn dialogue. Region-Level MLLMs. Different from conventional image-level comprehension, localized understanding requires MLLMs to capture regional attributes. Previous methods either utilize visual markers [54], bounding boxes [3, 38, 58, 64], or segmentation masks [22, 60], to represent regions-of-interests within an image. We simply regard masks as visual prompts, since masks have less ambiguity than other representations. However, previous approaches only support single visual prompt, and often neglect global context. GAR is designed for modeling the relationship between an arbitrary number of visual prompts while effectively maintaining crucial global context. Benchmarks for Region-Level Understanding. Typical region-level benchmarks only evaluate the caption quality for single prompt using conventional language-based captioning metrics [14, 38, 58, 60, 65], model-based similarities [2, 60], and LLM-Judged accuracies without the need for reference captions [22]. GAR-Bench is to systematically evaluate the comprehension capabilities with multiple visual prompts. It contains caption protocol to measure the correctness of descriptions for the relation between visual prompts, and VQA protocol to evaluate both the basic understanding capability for specific regions, e.g., color and shape, and advanced compositional reasoning abilities for multiple regions."
        },
        {
            "title": "3 Grasp Any Region",
            "content": "We start from the task formulation in 3.1. Subsequently, we introduce our model architecture and training data pipeline in 3.2 and 3.3, respectively. Finally, we introduce our benchmark designs in 3.4 to systematically evaluate region-level comprehension capabilities."
        },
        {
            "title": "3.1 Task Formulation",
            "content": "The task of grasping any region is hierarchical challenge from basic perception to complex, compositional reasoning about specific visual regions. Specifically, given an image RHW 3, where indicates the , where Mi {0, 1}HW , the objective resolution, and set of binary visual prompts, e.g., masks {Mi}N is to generate precise text response that demonstrates multi-layered comprehension of the scene, e.g., detailed attributes description and relational caption, based on the given text instruction : i=1 = RegionModel (cid:0)I, {Mi}N i=1, (cid:1) . (1) Figure 3 Illustration of our GAR. It leverages single-pass visual encoder to create holistic feature map of the entire scene, thus preserving global context. Simultaneously, an RoI-Aligned Feature Replay mechanism extracts high-fidelity features for specific objects of interest. Both the global context features and the detailed local features are then fed into an LLM to accurately infer complex relationships and interactions between multiple objects. Specifically, this task is structured in three ascending levels of capability: (1) Generating detailed descriptions for single region is the foundation, e.g., describe <Prompt1> in detail, where <Prompt1> actually denotes binary mask and is specified by the user. It requires the model to accurately perceive and articulate the fine-grained attributes contained strictly within the boundaries of given prompt. (2) The next stage requires understanding the given region with the necessary global contexts. This moves beyond isolated analysis, requesting to aggregate information from the broader, unmasked scene. This capability is critical for advanced reasoning tasks such as position identification (i.e., locating an object as the second from the left in the third row) and non-entity recognition (e.g., correctly identifying reflection in mirror versus physical object), where the prompt itself is insufficient for correct interpretation. (3) Finally, the task culminates in the ability to perceive, understand, and describe the relationship between multiple regions. This assesses the capacity for true compositional reasoning by requiring it to articulate the spatial, functional, or interactive connections between different prompts."
        },
        {
            "title": "3.2 Model Architecture",
            "content": "The task definition above requires overcoming the contextual blindness inherent in models that analyze prompted regions in isolation. As established, this myopic focus can lead to fundamental reasoning errors, such as misidentifying frog-shaped slipper as real frog because the surrounding bedroom context is ignored. Therefore, our architectural design of Grasp Any Region (GAR) is guided by central principle: to achieve fine-grained understanding of the prompted region while simultaneously preserving and leveraging the global context of the entire scene. Illustrated in Figure 3, we introduce two new components into the architecture: (1) simple yet effective prompt encoding scheme, and (2) novel RoI-aligned feature replay technique. Prompt Encoding and Integration. To integrate spatial guidance into the vision backbone, we introduce lightweight prompt encoding mechanism similar to [22] and [40]. The input binary mask, which specifies the region(s) of interest, is first processed by simple convolutional block [18] to produce mask embedding. This zero-initialized [63] mask embedding is then added to ViTs [12] patch embeddings. RoI-aligned Feature Replay. To simultaneously provide sufficient local details and maintain necessary global 4 Figure 4 Illustration of our training data pipeline, which mainly includes two rounds of captioning and judging. Specifically, (1) starting from using the seed dataset to train seed captioner, we first construct 456K fine-grained descriptions. Subsequently, (2) we utilize both datasets to obtain fine-grained captioner, and leverage the annotations of the Panoptic Scene Graph (PSG) dataset [55] to provide sufficient relation-aware captions and question-answering pairs. Finally, our GAR models are trained with all three parts. context, we introduce the RoI-aligned feature replay technique. Specifically, our model processes the full, uncropped image (with the encoded mask prompt) with AnyRes [27], producing global feature map that is rich in contextual information. Based on the input mask, we then derive corresponding bounding box for the region of interest and employ RoI-Align [16] to gather the relevant feature vectors directly from the global feature map. Because the features are extracted from feature map that was computed over the entire image, they are inherently context-aware, which elegantly avoids the pitfalls of local-only processing in [22]. At the same time, it provides the subsequent language model with sufficiently detailed, high-resolution representation of the prompted region, enabling it to perform fine-grained understanding. This replay of context-rich features allows GAR to simultaneously zoom in on detail without losing sight of the bigger picture. Ablations of this design can be found in Table 8, where we demonstrate that this design is capable of both (1) providing sufficient local details and (2) preserving global contexts."
        },
        {
            "title": "3.3 Training Data Pipeline\nTo enhance model capabilities from basic object recognition with single region to complex relational reasoning\nwith multiple regions, we design a multi-stage process to generate a large-scale, high-quality dataset, as\nillustrated in Figure 4. Ablations of each round can be found in Table 10. Prompts for each stage can be\nfound in § G.",
            "content": "Round 1: Enhance Recognition Capability. Initially, we start from the Describe Anything-1.5M dataset [22]. However, we observe deficiencies in its fine-grained recognition capability, limiting the quality of generated captions for more complex scenarios. To address this, we integrated images and masks provided by [40], which is subset of ImageNet-21K [11], an extremely fine-grained classification dataset and renowned for its detailed and extensive category labels. We employ the seed captioner to generate descriptions and then utilize an LLM to validate these generated captions against the ground-truth categories, resulting in refined fine-grained dataset of 456K samples. We utilize both datasets to train fine-grained captioner. Round 2: Supporting Multiple Prompts. To further enable understanding multiple prompts, we incorporated the Panoptic Scene Graph (PSG) dataset [55], which is rich in relational information. We first query the finegrained captioner to generate detailed description for each region. Subsequently, we regard Qwen2.5-72B [41] as the LLM-Merger, together with the original annotations provided by the PSG dataset [55], to generate: (1) 144K rich object descriptions that explicitly integrate relational context, (2) 144K question-answering pairs designed to probe the understanding of complex relationships, and (3) 126K multiple-choice questions. We construct relation dataset with 414K samples in total during this stage."
        },
        {
            "title": "3.4 GAR-Bench\nFinally, we introduce GAR-Bench, a comprehensive benchmark suite designed to systematically evaluate\nthe region-level comprehension capabilities of MLLMs beyond simply describing a single region. Specifically,\nit is structured into two primary components: a multi-prompt captioning task (GAR-Bench-Cap) and a\nmultifaceted visual question answering task (GAR-Bench-VQA). The captioning component is designed to\nassess a model’s ability to describe the complex relationships and interactions between multiple visual prompts\nin a cohesive narrative. The VQA component further dissects a model’s understanding into two key areas: (1)\nits ability to perceive basic attributes for a given prompt, and (2) its capacity for advanced, region-centric\ncompositional reasoning that requires synthesizing information from the prompt and its surrounding context.",
            "content": "GAR-Bench-Cap goes beyond isolated object descriptions and measures the ability to perform compositional scene understanding. In this task, model is provided with an image and two or more distinct visual prompts. It contains two sub-tasks: (1) simply describe the relationship, and (2) generate detailed captions including necessary relationships. For the simple protocol, models are directly asked with what is the relationship between <Prompt1> and <Prompt2> and are required to answer the question simply. For the detailed protocol, for instance, <Prompt1> highlights person and <Prompt2> is bike, the model is not evaluated on its ability to describe each independently, but rather on its capacity to generate an accurate description of their relation like, <Prompt1> is riding <Prompt2>. The models need to perform spatial reasoning, action recognition, and semantic integration across disparate image regions, thereby quantifying its ability to interpret scene as cohesive whole rather than collection of independent parts. GAR-Bench-VQA is designed to shift the evaluation from static description to dynamic, interactive dialogue. This task assesses the ability to answer specific questions about one or more prompted regions, directly measuring its comprehension rather than its descriptive fluency. To provide comprehensive and multi-faceted evaluation of the reasoning abilities, we divide it into two distinct but complementary sub-tasks: perception and reasoning. Perception evaluates the models foundational ability to recognize basic visual attributes of single object, serving as litmus test for its core visual acuity. This task quantifies the ability to perceive the foundational details. Specifically, for given visual prompt, the model is asked targeted questions about its intrinsic visual properties, specifically focusing on color, shape, material, and texture/pattern. Reasoning is designed to probe higher-order cognitive abilities. This component challenges the model to synthesize information from local prompts, global context, and the relationships between multiple prompts to arrive at logical conclusions. It is composed of several sub-tasks, each targeting unique and challenging aspect of visual reasoning: Position evaluates the models grasp of spatial arrangement and ordinal logic within global context. model is presented with mask on single object within larger group and asked to identify its precise position in complex, grid-like structure. Answering correctly requires the model to not only recognize the masked object but also to process the entire scene structure. Non-Entity Recognition is designed to test this specific capability by requiring the model to leverage sufficient global context. For instance, the given prompt might highlight reflection in mirror, the shadow of person, face depicted on television screen, and so on. The model is then queried to determine if the prompted region corresponds to physical entity. Success in this task demonstrates that the model is performing sophisticated context-aware reasoning rather than simple pattern matching on the masked pixels alone. Relation measures the capacity for complex compositional reasoning across multiple prompts. In this challenging setup, the model is presented with several visual prompts and must deduce the intricate spatial or logical relationship between them. key challenge is the inclusion of redundant prompts. To arrive at the correct answer, the model must ignore the potentially distracting information. It requires the model to build mental scene graph, which is essential for comprehending complex object assemblies and interactions in cluttered, real-world environments. For more benchmark details, including the annotation pipeline and statistics, please refer to B.1 and B.2. 6 Table 1 Comparison on GAR-Bench-VQA. indicates this subtask evaluates the interaction between multiple visual prompts. means evaluated with the thinking mode. Our GAR-1B even outperforms InternVL3-78B [66]. Moreover, GAR-8B surpasses private state-of-the-art non-thinking model GPT-4o [31]. Method Overall Perception (198) Reasoning (226) Color (69) Shape (64) Texture (29) Material (36) Position (64) Non-Entity (61) Relation (101) Private General MLLMs 65.3 70.3 68.8 48.3 55.2 58. 52.8 63.9 66.7 Public General MLLMs 25.0 40.6 53.1 50.0 21.9 37.5 40.6 54.7 34.5 44.8 41.4 65.5 48.3 58.6 51.7 58.6 Region MLLMs 45.3 25.0 39.1 3.1 46.9 54. 29.6 44.8 41.4 6.9 69.0 75.9 30.6 27.8 30.6 33.3 38.9 41.7 55.6 61.1 30.6 38.9 36.1 5.6 47.2 52.8 34.8 58.0 62.3 29.0 39.1 46.4 46.4 30.4 36.2 39.1 44.9 39.1 33.3 55.1 2.9 55.1 59. 53.5 61.3 64.2 34.4 41.7 50.9 52.8 35.1 38.9 46.5 50.5 34.3 37.5 38.2 2.4 50.6 59.9 57.8 54.7 64.1 43.8 59.4 71.9 68.8 48.4 51.6 60.9 53.1 54.7 60.9 31.3 1.6 21.9 48. 60.2 49.2 64.9 26.2 36.1 36.1 44.3 26.2 27.9 36.1 47.5 21.3 34.3 36.1 1.6 62.3 60.7 61.4 71.3 70.3 44.6 40.6 58.4 57.4 38.6 33.6 47.5 45.5 21.8 32.7 31.7 0.0 56.4 68. GPT-4o [31] o3 [33] Gemini-2.5-Pro [10] Qwen2.5-VL-3B [1] Qwen2.5-VL-7B [1] Qwen2.5-VL-32B [1] Qwen2.5-VL-72B [1] InternVL3-2B [66] InternVL3-8B [66] InternVL3-38B [66] InternVL3-78B [66] Sa2VA-8B [59] VP-SPHINX-13B [24] DAM-3B [22] PAM-3B [25] GAR-1B GAR-8B"
        },
        {
            "title": "4 Experiments",
            "content": "Owing to page limitations, we only present the key properties in this section. For implementation details, comparative baselines, and ablation studies, please refer to C. Advanced comprehension requires precisely modeling complex relationships between multiple prompts. To evaluate this capability, we conducted comprehensive comparison on our GAR-Bench-VQA. As demonstrated in Table 1, GAR-8B achieves an impressive overall score of 54.5, surpassing even the powerful, private, stateof-the-art non-thinking model, GPT-4o [31]. Furthermore, the efficiency and effectiveness of our approach are highlighted by GAR-1B. Despite its significantly smaller size, it scores 50.6 overall, outperforming large-scale public models like InternVL3-78B [66]. This advantage is particularly evident in fine-grained perception tasks, where GAR-1B and GAR-8B achieve Texture scores of 69.0 and 75.9, respectively. Detailed localized captioning requires generating detailed descriptions for given regions with multiple sentences. We benchmark our GAR models on series of challenging datasets, and the results consistently demonstrate their state-of-the-art capabilities. As shown in Table 2, on our GAR-Bench-Cap, GAR-1B and GAR-8B achieve the highest overall scores of 57.5 and 62.2, respectively, even exceeding that of powerful private models like Gemini-2.5-Pro [10]. This superiority is further confirmed on the DLC-Bench [22] in Table 3, where GAR-1B and GAR-8B again outperform top models like DAM-3B using either LLaMA3.1 [13] or GPT-4o [31] as the judge. The zero-shot performance of our models on Ferret-Bench [58] and MDVP-Bench [24], detailed in Table 4, is particularly noteworthy. On both benchmarks, our GAR emerges as the top-performing model across every single category. Specifically on MDVP-Bench, our models show commanding lead, with GAR-8B achieving score of 178.6 on natural images, result that is substantially higher than any competitor. Collectively, these comprehensive evaluations across multiple benchmarks unequivocally establish GAR as the new state-of-the-art for producing rich, accurate, and detailed localized captions. 7 Table 2 Comparison of localized relational captioning on our GAR-Bench-Cap. We utilize GPT-4o [31] with cropped images and masks to judge the answer. Method Overall (204) Simple (97) Detailed (107) Private General MLLMs GPT-4o [31] o3 [33] Gemini-2.5-Pro [10] Public General MLLMs Qwen2.5-VL-3B [1] Qwen2.5-VL-7B [1] Qwen2.5-VL-32B [1] InternVL3-2B [66] InternVL3-8B [66] InternVL3-38B [66] Region MLLMs DAM-3B [22] PAM-3B [25] VP-SPHINX-13B [24] Sa2VA-8B [59] GAR-1B GAR-8B 51.5 56.9 59.3 22.5 32.4 36.8 29.4 33.8 45.1 13.1 21.1 32.3 45.6 57.5 62.2 39.2 37.1 51.6 9.3 12.4 17.5 14.4 11.3 29.9 17.5 3.1 27.8 46.4 56.7 66. 62.6 74.8 66.4 34.6 50.5 54.3 43.0 54.2 58.9 10.3 39.3 39.3 44.9 63.6 64.5 Table 3 Comparison on detailed localized captioning on DLC-Bench [22]. indicates using GPT-4o [31] with extra cropped images as judge, otherwise performing textonly judging, where discussions can be found in F. means our evaluation with the official checkpoint. Method Avg. Pos. Neg. Private General MLLMs Gemini-2.5-Pro [10] GPT-4o [31] o1 [32] 55.8 61.5 62.5 Region MLLMs GPT4RoI-7B [64] Shikra-7B [3] Ferret-7B [58] RegionGPT-7B [14] VP-SPHINX-13B [24] DAM-3B [22] GAR-1B GAR-8B DAM-3B [22] GAR-1B GAR-8B 26.3 22.2 22.4 27.2 22.5 64.5 67.9 67.4 72.6 77.1 77.0 36.5 43.4 46.3 6.5 2.7 6.4 13.0 11.7 47.2 48.9 50.2 61.8 66. 68.0 75.2 79.6 78.8 46.2 41.8 38.4 41.4 33.2 81.8 87.0 84.6 83.4 88.0 86.0 Table 4 Zero-shot results on region-level detailed image captioning on Ferret-Bench [58] and and MDVP-Bench [24]. We adopt SAM [17] to produce masks conditioned on bounding boxes for MDVP-Bench [24]. All results are our reproduction using the official checkpoint, as the original judger GPT-4V is no longer available, and we take GPT-4o as the judge. Method Osprey-7B [60] PAM-3B [25] DAM-3B [22] GAR-1B GAR-8B Ferret-Bench MDVP-Bench (Box Caption) Refer. Desc. Natural OCR Multi-Panel Sceenshot 52.2 55.0 56.0 64. 107.7 71.4 87.0 152.6 178.6 99.4 94.3 127.7 149.6 149.1 70.0 86.8 79.4 103.7 117.2 81.3 84.5 76.4 115.3 123.0 Table 5 Results of category-level image recognition on LVIS [15] and PACO [36] following Osprey [60]. Method LVIS PACO Sim. IoU Sim. IoU GPT4RoI-7B [64] 51.3 12.0 48.0 12.1 63.8 36.6 58.7 26.0 Ferret-7B [58] 65.2 38.2 73.1 52.7 Osprey-7B [60] 89.0 77.7 84.2 73.2 DAM-8B [22] 88.6 78.3 87.4 74.9 PAM-3B [25] 91.0 68.2 93.2 72.4 GAR-1B 93.6 88.7 95.5 91.8 GAR-8B Open-class category-level image recognition requires the model to recognize the category of the object and part entities. We evaluate this capability in Table 5. Our GAR-8B demonstrates significant leap in performance, establishing new state-of-the-art. It consistently outperforms all prior methods across every metric, achieving top scores of 93.6 semantic similarity and 88.7 semantic IoU on LVIS [15], and 95.5 semantic similarity and 91.8 semantic IoU on PACO [36]. This indicates its superior ability in both semantic understanding and precise localization. These results demonstrate the effectiveness of GAR for complex recognition tasks, showcasing its robust performance in identifying diverse range of object categories. Extension to videos is straightforward. Similar to [22], we simply extend our GAR models to videos and evaluate them on VideoRefer-BenchD [61] and VideoRefer-BenchQ [61] in Table 6 and Table 7, respectively. We uniformly sample 16 frames to represent video. Our GAR-8B surpasses DAM-8B [22] under the zero-shot setting. More importantly, as demonstrated in Table 7, our our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B, demonstrating its strong comprehension capabilities can be easily transferred to videos. However, as our models are actually trained with images, they get reasonably low scores on temporally related tasks, e.g., temporal description (TD) in Table 6 and future predictions in Table 7. 8 Table 6 Zero-shot comparison of detailed localized video captioning on VideoRefer-BenchD [61]. For single-frame, we select the target frame and apply AnyRes with max_num_tiles=16. For multi-frame, we uniformly sample 16 frames and turn off AnyRes. Only zero-shot methods are listed here. Method General MLLMs LLaVA-OneVison-7B [20] Qwen2-VL-7B [49] InternVL2-26B [6] GPT-4o Region MLLMs Elysium-7B [44] Ferret-7B [58] Osprey-7B [60] Artemis-7B [34] DAM-8B [22] GAR-1B GAR-8B Single-Frame Multi-Frame Avg. SC AD TD HD Avg. SC AD TD HD 2.12 2.39 2.84 2. 1.57 2.18 2.34 2.72 2.75 2.62 2.97 3.55 3.34 2.35 3.08 3.19 4.41 4.41 1.58 2.24 2.99 2.96 0.30 2.01 2.16 2.98 2.96 2.19 2.03 2.57 3. 0.02 1.54 1.54 1.09 1.58 2.07 2.31 2.25 2.50 3.59 2.14 2.45 2.40 2.45 2.48 2.55 3.20 3.25 2.23 2.41 2.26 3.34 2.83 3.44 3.09 3.30 4.08 4. 3.20 3.30 3.42 4.45 4.38 4.53 1.94 2.54 3.35 3.31 2.38 2.66 1.34 3.30 3.01 3.25 2.50 2.22 3.08 3.11 1.97 2.10 1.39 3.03 1.61 2.57 2.41 2.12 2.28 2. 1.38 1.58 2.90 2.58 2.30 3.42 Table 7 Zero-shot comparison of detailed video understanding on VideoRefer-BenchQ [61]. indicates trained on in-domain VideoRefer-700k with regard to VideoRefer-Bench. Notably, our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B [61], demonstrating that its strong capabilities can be easily transferred to videos. Method General MLLMs InternVL2-26B [6] Qwen2-VL-7B [49] LLaVA-OneVision-7B [20] GPT-4o [31] Region MLLMs Osprey-7B [60] Ferret-7B [58] VideoRefer-7B [61] GAR-1B GAR-8B Overall (1000) Basic Questions (235) Sequential Questions (256) Relationship Questions (252) Reasoning Questions (143) Future Predictions (114) 65.0 66.0 67.4 71. 39.9 48.8 71.9 69.9 72.0 58.5 62.0 58.7 62.3 45.9 35.2 75.4 75.0 77.2 63.5 69.6 62.9 74.5 47.1 44.7 68.6 69.9 71.0 53.4 54.9 64.7 66. 30.0 41.9 59.3 59.7 61.7 88.0 87.3 87.4 88.0 48.6 70.4 89.4 83.2 86.6 78.9 74.6 76.3 73.7 23.7 74. 78.1 63.7 68.1 Qualitative Results. We provide qualitative comparisons between our GAR-8B with DAM-3B [22] on detailed localized captioning on DLC-Bench [22] in Figure 5. As demonstrated in the figure, our GAR-8B is more capable of generating precise descriptions, especially when the category of the given prompt can be determined only when understanding sufficient global contexts. More comparisons can be found in D."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces Grasp Any Region (GAR), family of MLLMs for region understanding, and GARBench, systematic evaluation framework that not only provides more accurate evaluation of single-region comprehension, but also for multi-prompt interaction and advanced compositional reasoning. On detailed captioning benchmarks [22, 24, 58], GAR demonstrates superior performance over DAM [22]. More importantly, our GAR achieves advanced comprehension capability in modeling interactions between multiple prompts. Specifically, on GAR-Bench-VQA, GAR-1B even surpasses InternVL3-78B [66]. On VideoRefer-BenchQ [61], our zero-shot GAR-8B even outperforms in-domain VideoRefer-7B [61]. We hope our work inspires the community to develop MLLMs that can perceive, interrogate, and understand the dense visual world more 9 Figure 5 Qualitative comparisons on DLC-Bench [22], where green indicates correct descriptions and red means errors. We compare our GAR-8B with DAM-3B [22]. Thanks to the encoded global contexts, our GAR-8B produces much more accurate descriptions. effectively."
        },
        {
            "title": "Ethics Statement",
            "content": "Our research is grounded in ethical practices, with particular attention paid to the responsible use of data. All datasets employed in this study are publicly available and well-established within the computer vision community. Specifically, our training data includes ImageNet-21K [11] and the PSG [55] dataset (with image sources from COCO [23], while our benchmarking was conducted on FSC-147 [37], RGBD-Mirror [30], and SA-1B [17]. Our use of this data is in accordance with their provided licenses and intended academic purpose."
        },
        {
            "title": "Reproducibility Statement",
            "content": "We are committed to ensuring the reproducibility of the research presented in this paper. To this end, comprehensive implementation details for our models and experiments are provided in C, including the training procedures and all hyperparameters used. Furthermore, upon acceptance of this paper, all source code, datasets, and trained model checkpoints will be made publicly available."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S-H Gary Chan, and Hongyang Zhang. Revisiting referring expression comprehension evaluation in the era of large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 513524, 2025. 10 [3] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [4] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, arXiv preprint Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. [5] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1332013331, 2024. [6] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [7] Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. [8] XTuner Contributors. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/xtuner, 2023. [9] Google DeepMind. Gemini-2.5-flash. https://deepmind.google/models/gemini/flash/, 2025. [10] Google DeepMind. Gemini-2.5-pro. https://deepmind.google/models/gemini/pro/, 2025. [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 248255. Ieee, 2009. [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1379613806, 2024. [15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 53565364, 2019. [16] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 29612969, 2017. [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer In Proceedings of the IEEE/CVF Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. International Conference on Computer Vision (ICCV), pages 40154026, 2023. [18] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4): 541551, 1989. [19] Weixian Lei, Jiacong Wang, Haochen Wang, Xiangtai Li, Jun Hao Liew, Jiashi Feng, and Zilong Huang. The scalability of simplicity: Empirical analysis of vision-language learning with single transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 11 [21] Xiangtai Li, Tao Zhang, Yanwei Li, Haobo Yuan, Shihao Chen, Yikang Zhou, Jiahao Meng, Yueyi Sun, Shilin Xu, Lu Qi, et al. Denseworld-1m: Towards detailed dense grounded caption in the real world. arXiv preprint arXiv:2506.24102, 2025. [22] Long Lian, Yifan Ding, Yunhao Ge, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, et al. Describe anything: Detailed localized image and video captioning. arXiv preprint arXiv:2504.16072, 2025. [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740755, 2014. [24] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. In International Conference on Learning Representations (ICLR), 2025. [25] Weifeng Lin, Xinyu Wei, Ruichuan An, Tianhe Ren, Tingwei Chen, Renrui Zhang, Ziyu Guo, Wentao Zhang, Lei Zhang, and Hongsheng Li. Perceive anything: Recognize, explain, caption, and segment anything in images and videos. arXiv preprint arXiv:2506.05302, 2025. [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems (NeurIPS), 36:3489234916, 2023. [27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. https://llava-vl.github.io/blog/2024-01-30-llava-next/, 2024. [28] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [30] Haiyang Mei, Bo Dong, Wen Dong, Pieter Peers, Xin Yang, Qiang Zhang, and Xiaopeng Wei. Depth-aware mirror segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 30443053, 2021. [31] OpenAI. Openai-gpt-4o. https://openai.com/index/gpt-4o-system-card/, 2024. [32] OpenAI. Openai-o1. https://openai.com/o1/, 2024. [33] OpenAI. Openai-o3. https://openai.com/index/introducing-o3-and-o4-mini/, 2025. [34] Jihao Qiu, Yuan Zhang, Xi Tang, Lingxi Xie, Tianren Ma, Pengyu Yan, David Doermann, Qixiang Ye, and Yunjie Tian. Artemis: Towards referential understanding in complex videos. Advances in Neural Information Processing Systems (NeurIPS), 37:114321114347, 2024. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 87488763, 2021. [36] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 71417151, 2023. [37] Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai. Learning to count everything. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 33943403, 2021. [38] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1300913018, 2024. [39] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 84308439, 2019. 12 [40] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-clip: clip model focusing on wherever you want. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1301913029, 2024. [41] Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [42] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems (NeurIPS), 37:8731087356, 2024. [43] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95689578, 2024. [44] Han Wang, Yongjie Ye, Yanjie Wang, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level perception in videos via mllm. In European Conference on Computer Vision (ECCV), pages 166185. Springer, 2024. [45] Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, and Zhaoxiang Zhang. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999, 2025. [46] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2025. [47] Haochen Wang, Anlin Zheng, Yucheng Zhao, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Zhaoxiang Zhang. Reconstructive visual instruction tuning. In International Conference on Learning Representations (ICLR), 2025. [48] Jiacong Wang, Zijiang Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025. [49] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [50] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [51] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1308413094, 2024. [52] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [53] xAI. Grok. 2024. [54] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [55] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic scene graph generation. In European Conference on Computer Vision (ECCV), pages 178196. Springer, 2022. [56] Ruofeng Yang, Bo Jiang, Cheng Chen, Baoxiang Wang, Shuai Li, et al. Few-shot diffusion models escape the curse of dimensionality. Advances in Neural Information Processing Systems (NeurIPS), 37:6852868558, 2024. [57] Ruofeng Yang, Zhijie Wang, Bo Jiang, and Shuai Li. Leveraging drift to improve sample complexity of variance exploding diffusion models. Advances in Neural Information Processing Systems (NeurIPS), 37:107662107702, 2024. [58] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 13 [59] Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025. [60] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2820228211, 2024. [61] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, Yueting Zhuang, et al. Videorefer suite: Advancing spatial-temporal object understanding with video llm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1897018980, 2025. [62] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1197511986, 2023. [63] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 38363847, 2023. [64] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. In European Conference on Computer Vision (ECCV), pages 5270. Springer, 2024. [65] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. Advances in Neural Information Processing Systems (NeurIPS), 37:7173771767, 2024. [66] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Overview",
            "content": "Here is the table of contents of this appendix: In B, we introduce details of our GAR-Bench, including the annotation pipeline and statistics. In C, we provide more implementation details as well as experimental results. Detailed ablations of each component can be found in this section. In D, we provide qualitative results on both detailed image captioning and understanding, and localized video captioning and understanding. In E, we discuss potential limitations and analyze failure cases. In F, we discuss some underlying issues towards the evaluation protocols of DLC-Bench [22]. In G, we provide all prompts we utilized to construct our dataset. Finally in H, we discuss the use of LLMs in preparing this paper. Details of GAR-Bench B.1 Annotation Pipeline The construction of GAR-Bench follows rigorous, semi-automated pipeline designed to generate high-quality, diverse, and challenging data. This process combines the strengths of advanced foundation models for initial data generation with the nuanced judgment of team of 8 MLLM experts for curation, annotation, and quality control. Image Selection. To ensure the relevance and challenge of our sub-tasks, we begin by carefully curating source images from existing datasets known to contain specific visual patterns. For the relation tasks, we source images from the Panoptic Scene Graph (PSG) dataset [55], which is rich in complex scene graphs and explicit object relationships, providing natural foundation for multi-prompt interaction queries. For the non-entity recognition task, we utilize the RGBD-Mirror dataset [30], as it specifically contains scenes with mirrors and reflections, allowing us to create unambiguous test cases for distinguishing real objects from illusory ones. For the position task, we select images from the FSC-147 dataset [37], which features images with numerous countable objects often arranged in grid-like patterns, making it ideal for evaluating spatial and ordinal reasoning. Other images are from SA-1B [17]. Mask Labeling. Following image selection, we generate high-quality segmentation masks for all potential objects of interest. This stage is similar to [21], which decomposes complex scenes into different objects, while not containing numerous meaningless, trivial objects like those in the SA-1B [17] dataset. Object Selection and Annotation. With high-quality pool of object masks generated, the annotation team performs the critical tasks of selection and annotation. The experts first reviewed the masks, selecting only those with high segmentation quality that are also qualified for the target sub-task. Concurrently, they are responsible for annotating the ground-truth information required for the benchmark. Specifically, for the reasoning protocol of GAR-Bench-VQA, they meticulously annotate the correct answers for relation, ordering, and entity status. For GAR-Bench-Cap, they annotate the ground-truth captions describing the interactions between the selected masked objects. Automated Attribute Generation. For the perception protocol of GAR-Bench-VQA, we leverage the advanced capabilities of Gemini-2.5-Pro [10]. For each selected and verified object mask, we prompt the model to generate list of its basic perceptual attributes, including its primary color, shape, material, and any discernible texture or pattern. 15 Figure 6 Statistics of our GAR-Bench. We (a) slightly prioritize reasoning over perception, and build challenging questions through (b) multiple visual prompts (even have 2 questions with 7 prompts and 9 prompts) and (c) small areas of each prompt with an average of 4.4%. Table 8 Ablations across different model architectures with PerceptionLM-1B. indicates using GPT-4o [31] with extra cropped images as the judge, instead of text-only judging. Our proposed RoI-aligned feature replay strategy effectively preserves necessary global contexts. We also report the average latency (ms) to generate the first token and the maximum number of tokens for ViT [12]. By default, we set max_num_tiles=16 for AnyRes [27], resulting in maximum of 17 crops in total for one global image."
        },
        {
            "title": "Local",
            "content": "GAR-Bench DLC-Bench"
        },
        {
            "title": "Inference Speed",
            "content": "Caption VQA Avg. Pos. Neg. Latency # ViT Tokens 1 2 3 image + mask 4 image + mask image + mask image + mask + cross-attention image + mask RoI-aligned feature replay 20.1 19.1 28.4 57.5 69.3 60.2 78.4 37.8 68.8 57.3 80.3 40.0 36.6 77.4 70.1 84.8 50.6 77.1 66.2 88. 36.1 57.1 93.1 87.7 256 4,608 4,608 4,352 Quality Control and Formatting. The raw, annotated data then underwent meticulous, multi-stage quality control process. First, human experts review all machine-generated attributes from the previous step to verify their factual correctness and filter out any ambiguous or inaccurate labels. Following this verification, the experts transform the raw annotations into the final benchmark formats. For all VQA tasks, they rewrite the question-answer pairs into standardized multiple-choice format, ensuring consistent and objective evaluation. For the captioning task, the ground-truth data was structured for compatibility with LLM-as-a-Judge evaluation protocols similar to [22]. Difficulty Filtering. As final quality assurance measure, we implement difficulty filtering process to ensure the benchmark remains challenging for even the most advanced models. Specifically, any question answered correctly by all four state-of-the-art non-thinking MLLMs, i.e., Qwen2.5-VL-72B [1], InternVL3-78B [66], GPT-4o [31], and Gemini-2.5-Flash [9], was excluded from the final benchmark. B.2 Statistics Distribution of Each Discipline. As demonstrated in Figure 6a, GAR-Bench slightly prioritizes advanced reasoning (53%) over basic perception (47%) with relatively balanced distribution. In addition, it prioritizes complex relational reasoning with multiple prompts in the relation protocol. Distribution of Number of Prompts. As illustrated in Figure 6b, our GAR-Bench even contains 2 questions with 7 prompts and 9 prompts, respectively, leading to an advanced requirement of modeling complex relationships between multiple visual prompts. Distribution of Areas of Prompts. We compute the relative area of each visual prompt in Figure 6c, where the majority of prompts in GAR-Bench are extremely small, with sharp peak near 0.0. The mean area across 16 Table 9 Ablations across different model architectures with different base models. indicates using GPT-4o [31] with extra cropped images as the judge, instead of text-only judging. Our proposed RoI-aligned feature replay strategy effectively preserves necessary global contexts. Global Local GAR-Bench DLC-Bench Caption VQA Avg. Pos. Neg. Base Model: Qwen2.5-VL-3B [1] 5 6 7 image + mask 8 image + mask image + mask image + mask + cross-attention image + mask RoI-aligned feature replay Base Model: InternVL3-2B [66] 9 10 11 image + mask 12 image + mask image + mask image + mask + cross-attention image + mask RoI-aligned feature replay 24.5 27.9 34.3 41.2 24.6 29.4 32.8 43.1 30.7 30.0 32.1 40.8 33.0 31.8 36.1 44.6 52.2 55.7 62.1 69.2 65.6 68.8 70.3 73. 38.0 46.8 50.7 58.1 48.5 56.7 61.6 63.8 66.4 64.6 73.5 80.3 82.6 80.9 79.0 82.2 Table 10 Ablations on each component of our data with 1B model size. indicates using GPT-4o [31] with extra cropped images as the judge, instead of text-only judging. Each component of our data plays significant role. Data GAR-Bench DLC-Bench 1 Seed Dataset-1.5M 2 1 + Fine-Grained Dataset-456K 3 2 + Relation Dataset-414K 13.8 14.2 57.5 41.5 44.1 50.6 74.4 77.5 77. Caption VQA Avg. Pos. 63.0 67.6 66.2 Neg. 85.8 87.4 88.0 Table 11 Performance on general multimodal benchmarks [4, 43, 51, 53], where we set mask = 1 for evaluation. Method V* MMVP RealWorldQA MMStar DAM-3B [22] PAM-3B [25] GAR-8B 45.0 1.4 59.2 60.7 4.3 78.0 54.3 1.7 58.7 39.7 2.7 43.9 all questions is 4.4%. This distribution highlights the importance of addressing small-scale and fine-grained understanding."
        },
        {
            "title": "C More Experiments",
            "content": "Implementation Details. We adopt PerceptionLM series [7] as our base model, as it demonstrates strong perception capabilities among several open-source MLLMs. We perform supervised fine-tuning of the model on our GAR-2.5M using Xtuner [8] with the AdamW optimizer [29] with global batch size of 64 and learning rate of 1e-5 with cosine decay [28]. Comparison Baselines. We mainly compare our GAR with both general MLLMs, including state-of-the-art private models [10, 31, 33], and representative public models [1, 26, 66], and region-level MLLMs, including GLaMM [38], GPT4RoI [64], Osprey [60], Shikra [3], Ferret [58], RegionGPT [14], OMG-LLaVA [65], VPSPHINX [24], Sa2VA [59], DAM [22], and PAM [25]. We transform masks to boxes for box-level MLLMs, e.g., [3, 25, 58, 64], as our GAR-Bench provides segmentation masks by default. On video benchmarks, we further compare with LLaVA-OneVision [20], Qwen2-VL [49], InternVL2 [6], Elysium [44], Artemis [34], and VideoRefer [61]. Ablations on Architecture Designs. We first elaborate on our key architecture design, i.e., RoI-aligned feature replay in Table 8. Other baselines include: 1 only local images, 2 DAM-like architectures [22] which preserves context via zero-initialized gated cross-attention, 3 simply cropping local images as supplement 17 of global images, and 4 our RoI-aligned feature replay design. As demonstrated in Table 8, both 1, 2, and 3 struggle at modeling multi-prompt relations, leading to poor results on GAR-Bench, although 3 is superior at precise description on DLC-Bench [22]. However, our proposed RoI-aligned feature replay strategy effectively preserves necessary global contexts while achieving competitive performances on DLC-Bench. In Table 9, we further extend our ablations on model architectures to more base models, including Qwen2.5VL-3B [1] and InternVL3-2B [66]. As demonstrated in the table, our proposed RoI-aligned feature replay consistently brings significant improvements over different base models. Ablations on Data Pipeline. We study the effectiveness of our data in Table 10. Starting from the seed dataset, i.e., Describe-Anything-1.5M [22], we first add our Fine-Grained Dataset-456K, and then add our Relation Dataset-414K. By introducing our Fine-Grained Dataset-456K, our model is able to produce more accurate recognition, leading to an improvement of +3.1 on DLC-Bench [22]. By further combining our proposed Relation Dataset-414K, the model is finally equipped with compositional reasoning capabilities with multiple prompts at this time, resulting in significant improvements on our GAR-Bench. Performances on General Multimodal Benchmarks. We compare our GAR-8B with other region-level models, i.e., DAM-3B [22] and PAM-3B [25], on general vision-centric multimodal benchmarks, including V* [51], MMVP [43], RealWorldQA [53], and MMStar [4]. As illustrated in Table 11, our GAR-8B outperforms them by large margin."
        },
        {
            "title": "D Qualitative Results",
            "content": "D.1 Qualitative Results on GAR-Bench Relation of GAR-Bench-VQA. In Figure 7, we provide qualitative comparisons on the relation protocol of our GAR-Bench-VQA, including two failure cases (the last row). As demonstrated in the figure, GAR-8B manages to not only effectively model relationships but also leverage crucial local details for choosing the best answer. For instance, in the right example of the middle row, the person (<Prompt0>) is actually not reading the book (<Prompt1>), since she is looking at the camera. Our GAR-8B manages to recognize such details and thus select <Prompt0> is holding <Prompt1> instead of reading, while both Gemini-2.5-Pro [10] and o3 [33] fail. However, as illustrated in the last two examples in Figure 7, current models still sometimes struggle to understand complex relationships with more than two objects. Constructing such complicated training data and keeping the correctness of relation annotations could be potential solution. Non-Entity Recognition of GAR-Bench-VQA. In Figure 8, we provide qualitative comparisons on the non-entity recognition protocol of our GAR-Bench-VQA, including two failure cases (the last row). As demonstrated in the figure, GAR-8B is able to correctly recognize objects in the mirror without any depth prior, thanks to its encoded global contexts. However, as demonstrated in the right case in the last row, current models still struggle to distinguish whether the reflection actually comes from the mirror (<Prompt2>) or other reflective surfaces (<Prompt0> and <Prompt1>). D.2 Qualitative Results on VideoRefer-Bench Detailed Localized Video Captioning. In Figure 9, we provide qualitative results of extending GAR-8B to generate detailed video descriptions on VideoRefer-BenchD [61]. In most cases, where videos usually remain static, GAR-8B manages to generate detailed, specific, and precise descriptions. However, as demonstrated in the last example, GAR-8B fails to capture detailed temporal differences among frames, leading to low score on temporal description. This is because our GAR models are actually trained with only images and lack fine-grained temporal comprehension capabilities. Detailed Video Understanding. In Figure 10, we provide qualitative results of extending GAR-8B to detailed video understanding on VideoRefer-BenchQ. GAR-8B is capable of understanding basic motions under zero-shot setting, e.g., the sequential question, the relation question, and the reasoning question. However, on the future prediction protocol, GAR-8B sometimes fails to choose correctly with significant motion changes."
        },
        {
            "title": "E Limitation and Failure Cases",
            "content": "One potential limitation is that our GAR is limited to static images. Although it can be successfully extended to video and even achieves competitive results compared with video models (please refer to Tables 6 and 7 for detailed experimental results), it sometimes fails when input videos contain significant motion changes. Specifically, as demonstrated in the failure cases in Figures 9 and 10, GAR-8B is superior at comprehending and describing static videos, and is also capable of understanding basic motions. However, with significant motion changes, GAR-8B sometimes fails. Carefully collecting video training data is potential solution. Discussion on DLC-Bench Our analysis in Figure 11 reveals significant weakness in the original judger of DLC-Bench [22], which relies on text-only LLM, i.e., LLaMA3.1-8B [13], for automated scoring. Specifically, fundamental flaw in the original DLC-Bench [22] evaluation lies in its assumption that semantic categories can be accurately adjudicated within the abstract confines of language space alone. However, Figure 11 demonstrates that this text-only approach is inherently unreliable due to the ambiguity of linguistic labels without visual contexts. We argue that the image is the only ground truth capable of resolving this ambiguity. Therefore, we provide the image for valid evaluation. To truly assess models descriptive power, the judge must be multimodal, capable of grounding the generated caption in the visual reality it purports to describe."
        },
        {
            "title": "G Prompt Templates",
            "content": "We provide all of our prompts utilized in building our data in Figures 12, 13, 14, and 15."
        },
        {
            "title": "H Use of LLMs",
            "content": "In preparing this paper, LLMs are utilized as general-purpose assistive tool. Specifically, the use of LLMs is strictly limited to proofreading the author-written text for grammatical errors, spelling corrections, and improvements to language clarity. This application is consistent with the use of conventional grammar-checking software and did not extend to research ideation, data analysis, or the generation of any substantive content. 19 Figure 7 Qualitative comparisons on the relation protocol of our GAR-Bench-VQA, including two failure cases (bottom). Notably, in the right case of the middle row, the person (<Prompt0>) is actually not reading the book (<Prompt1>), since she is looking at the camera. Our GAR-8B manages to recognize such details while both Gemini-2.5-Pro [10] and OpenAI-o3 [33] fail. From the last two cases, we can tell that models are still struggling with understanding complex relationships with more than two objects. All images are sampled from [39]. 20 Figure 8 Qualitative comparisons on the non-entity recognition protocol of our GAR-Bench-VQA, including two failure cases (bottom). From the last two cases, we can tell that models are sometimes still struggling with recognizing non-entities, especially distinguishing reflection from the mirror (<Prompt2>) and other surfaces (<Prompt0> and <Prompt1>). Figure 9 Qualitative results of detailed video captioning on VideoRefer-BenchD [61], including one failure case with low temporal description score. Videos are sampled from [5]. 22 Figure 10 Qualitative results of detailed video understanding on VideoRefer-BenchQ [61], including one failure case in the future prediction protocol. 23 Figure 11 Incorrect text-only judging results using LLaMA3.1-8B [13] on DLC-Bench [22]. The model is required to judge whether the description is consistent with the ground-truth category name. We illustrate both correct and wrong results. Providing extra cropped images and masks to GPT-4o [31] effectively eliminates this issue. Figure 12 Prompt for judging the description and the ground-truth category. Figure 13 Prompt for generating relation-aware caption. 25 Figure 14 Prompt for generating question-answering pairs. 26 Figure 15 Prompt for generating multiple-choice questions."
        }
    ],
    "affiliations": [
        "ByteDance",
        "NLPR, MAIS, CASIA",
        "PKU",
        "UCAS",
        "WHU"
    ]
}