{
    "paper_title": "Evaluating, Synthesizing, and Enhancing for Customer Support Conversation",
    "authors": [
        "Jie Zhu",
        "Huaixia Dou",
        "Junhui Li",
        "Lifan Guo",
        "Feng Chen",
        "Chi Zhang",
        "Fang Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin."
        },
        {
            "title": "Start",
            "content": "Evaluating, Synthesizing, and Enhancing for Customer Support Conversation Jie Zhu1,2*, Huaixia Dou1,2*, Junhui Li1, Lifan Guo2, Feng Chen2, Chi Zhang2, Fang Kong1 1School of Computer Science and Technology, Soochow University 2Qwen DianJin Team, Alibaba Cloud Computing 5 2 0 2 6 ] . [ 1 3 2 4 4 0 . 8 0 5 2 : r Abstract Effective customer support requires not only accurate problem-solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and realworld service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service supporters to respond using well-defined support strategies. We propose structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customeragent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be publicly available at https: //github.com/aliyun/qwen-dianjin."
        },
        {
            "title": "Introduction",
            "content": "Customer support aims to help users resolve productor service-related issues through effective and context-aware communication. While large-scale dialogue systems have received growing attention in recent years (Budzianowski et al. 2018; Rashkin et al. 2019; Peskov et al. 2019; Liu et al. 2021), customer support conversations remain underexplored in the NLP community,1 largely due to the scarcity of publicly available benchmarks and the sensitive, domainspecific nature of support interactions. Meanwhile, large language models (LLMs) like GPT-3 (Brown et al. 2020) have shown impressive capabilities in open-domain dialogue generation (Zhang et al. 2020b; Bae et al. 2022; Thoppilan et al. 2022), but their ability to generate realistic and effective customer support conversations remains underexamined. *These authors contributed equally Corresponding Author 1This paper uses dialogue and conversation interchangeably. Figure 1: An example dialogue between service supporter (left) and customer (right), showing support strategies (noted in parentheses) used by the supporter. The conversation is organized into five stages of the proposed CSC framework, shown in colored boxes. Customer support guidelines from COPC, the internationally recognized standard for customer experience management,2 emphasize that high-quality support often depends on structured communication strategies, similar to those used in emotional support settings (Liu et al. 2021). As shown in Figure 1,3 the service supporter begins with the Greeting strategy to initiate the conversation and explore the customers issue. Upon recognizing the customers negative emotion, the supporter adopts the Emotional Management strategy, expressing empathy and understanding to alleviate the customers distress. After understanding the problem, the supporter employs the Information Delivery strategy to provide clear and actionable guidance, followed by the Feedback Request strategy to check for further concerns. Finally, the conversation ends with the Appreciation and Closure 2https://www.copc.com/copc-standards/ 3The example is translated into English for better readability. strategy, ensuring respectful and reassuring followed by Feedback Request to check for further concerns. Despite the importance of effective customer support, research on real-time customer service conversations remains limited, mainly due to lack of task-specific design and high-quality annotated data. Many studies (Xu et al. 2017; Oraby et al. 2017; Cui et al. 2017; Mesquita, Martins, and Almeida 2022) rely on asynchronous, exchanges (e.g., Twitter), where interactions span minutes to days. These settings differ significantly from the immediate, uninterrupted nature of real-time support. Moreover, effective support requires not only resolving issues but also showing empathy and emotional support. Yet, most task-oriented dialogue datasets (Wen et al. 2017; Budzianowski et al. 2018; Peskov et al. 2019; Gung et al. 2023) lack the intentional use of supportive strategies like emotional management or empathetic closure, which are vital for high-quality customer service. To address this gap, we introduce the task of customer support conversation (CSC), to facilitate the training of customer service supporters. The goal is to help them respond with appropriate strategies that combine accurate solutions with empathetic communication. Based on COPC standards and inspired by the emotional support framework (Liu et al. 2021), we develop CSC-specific framework with five dialogue stages and twelve support strategies. Using it, we build CSConv, high-quality dataset adapted from real service interactions and refined for structured strategy use. To address the lack of high-quality training data, we also develop role-playing approach that creates RoleCS, synthetic dataset of strategy-rich conversations by assigning LLMs distinct roles aligned with the CSC framework. Fine-tuning on RoleCS significantly boosts LLMs ability to generate strategy-aligned and effective responses on CSConv."
        },
        {
            "title": "2.1 Task-Oriented Conversation Datasets.",
            "content": "The goal of CSConv, which is to generate conversations based on real spoken customer service interactions, differs significantly from previous task-oriented dialogue datasets. Many earlier synthetic datasets have been created using the Wizard of Oz (WOZ) framework (Kelley 1984), where one person acts as the system and another as the user. Wen et al. (2017) introduce crowdsourced version of the WOZ framework to collect domain-specific dialogue data more efficiently. Other task-oriented dialogue datasets include Frames (El Asri et al. 2017), MultiWOZ (Budzianowski et al. 2018), MultiDoGO (Peskov et al. 2019), EMPATHETICDIALOGUES (Rashkin et al. 2019), Taskmaster-1 (Byrne et al. 2019), ESConv (Liu et al. 2021), and NATCS (Gung et al. 2023), each of which explores different domains and annotation schemes for developing conversational agents. Among them, ESConv is closely related to our work, as both it and CSConv aim to enhance service quality using structured support strategies. However, their construction methods differ significantly: ESConv uses crowdsourced WOZ setup, while CSConv rewrites real-world service dialogues using high-performing LLMs."
        },
        {
            "title": "3.1 CSC Framework\nThe CSC framework organizes the customer support process\ninto five stages, each with recommended strategies designed\nto enhance the quality and effectiveness of interaction.",
            "content": "Stages. Building on the three core stages of supportive communication from Hill (2019) and the COPC practical guidelines, we work with domain experts to refine and extend this structure for customer support. The resulting CSC framework defines five stages: Connecting (greeting and rapport building), Identifying (understanding the customers issue and emotional state), Exploring (discussing and evaluating potential solutions), Resolving (delivering and confirming resolution), and Maintaining (closing the interaction while preserving the customer relationship). Figure 2 illustrates the flow across these stages. Importantly, these stages are not rigid steps but modular components that can appear in various orders or combinations depending on the nature of the conversation. For example, even when no solution is reached, supporter may still provide empathy (Exploring), acknowledge service limitations (Resolving), and close the conversation professionally (Maintaining). This flexible structure enables consistent analysis and strategy modeling across both successful and challenging customer support scenarios. Strategies. In parallel, we work with domain experts to define twelve actionable support strategies aligned with the five stages: Greeting (GT), Identity Verification (IV), Emotional Management (EM), Restatement or Paraphrasing (RP), Problem Refinement (PR), Providing Suggestions (PS), Information Delivery (ID), Resolution Implementation (RI), Feedback Request (FR), Appreciation and Closure (AC), Relationship Continuation (RC), and Others. Figure 2: Overview of the CSC frameworks five stages, each paired with recommended support strategies (see Table 9 in Appendix A). The typical flow is: 1 Connecting 2 Identifying 3 Exploring 4 Resolving 5 Maintaining (black arrows), but it can be adjusted based on the specifics of each conversation (dashed arrows). These strategies provide practical guidance for managing both task-related issues and emotional support throughout the customer interaction. For the details of stages and strategies, please refer to Appendix A."
        },
        {
            "title": "3.2 Dataset Construction\nWe construct the CSConv dataset using Chinese customer\nservice dialogues collected from our pre-sales and after-\nsales customer service centers. Prior to our access, all con-\nversations were professionally transcribed, manually cor-\nrected, and fully de-identified to ensure privacy protection\nand integrity. While these raw conversations authentically\nreflect real-world customer interactions, they often lack con-\nsistent structure, making it difficult to systematically an-\nnotate support strategies according to the COPC-informed\nCSC framework. To address this, we employ an LLM to\nrewrite the conversations while preserving the original se-\nmantics and user intent. This controlled rewriting aligns con-\nversations with the defined dialogue stages and strategies,\nenabling more consistent and interpretable annotations with-\nout sacrificing the complexity of real user queries.",
            "content": "Importantly, the goal of constructing CSConv is not to evaluate real-time chatbot performance, but to facilitate the training of customer service supporters by helping them learn to respond using appropriate strategies guided by the COPC framework. In total, we collect 690K conversations spanning eight in-domain topics. To ensure quality, we use four-stage pipeline to guide data selection and refinement. 1. Pre-filtering: We first apply rule-based filtering to remove low-quality conversations. These rules exclude conversations that are too short or too long, contain overly lengthy utterances, show an imbalance between customer and agent turns, or have high proportion of ineffective customer responses. Additionally, we use an LLM to exclude conversations with offensive or unprofessional content. 2. Sampling and Rewriting: Up to 500 filtered conversations per topic are sampled and rewritten by an LLM to align with the CSC framework. During this process, the LLM analyzes the original scenario and generates new conversation that preserves the core issue while improving clarity, structure, and emotional engagement. For each agent turn, the LLM selects an appropriate support strategy based on conversation context, occasionally using Others to maintain conversational naturalness. Customer responses are also refined for coherent interaction. 3. Post-filtering: After rewriting, second round of rulebased and LLM-based checks verifies structure (e.g., strategy coverage, speaker alternation, and length) and filters out conversations lacking coherence, empathy, or strategic alignment. 4. Manually annotation: Finally, experts certified in COPC review the remaining conversations, evaluating the support agents responses for realism, empathy, and adherence to the CSC framework. This results in curated evaluation set of 1,855 high-quality conversations. Appendix provides details on filtering rules, prompt templates, and annotation guidelines. For rewriting, we use DeepSeek-R1 (Guo et al. 2025) instead of GPT-4o (OpenAI 2024), as the latter tends to produce shorter, less emotionally rich dialogues. See Appendix B.5 for comparison."
        },
        {
            "title": "3.3 Dataset Analysis\nOverview Statistics. Table 1 compares\nstatistics of\nCSConv before and after rewriting, covering 1,855 con-\nversations. Rewritten dialogues are longer, averaging 27.27\nversus 19.06 utterances, with supporter responses increas-\ning from 41.16 to 48.72 words and customer responses de-\ncreasing from 21.60 to 17.17. Notably, strategy use (exclud-\ning Others) rises from 55.28% to 97.82%,4 indicating more\ndeliberate and guided support. These shifts reflect the goal\nof rewriting, which is to facilitate the training of customer\nservice supporters in applying appropriate strategies aligned\nwith the COPC framework.",
            "content": "Topic Distribution. Table 2 presents the distribution of conversations across eight topic. Each topic, excluding Others, accounts for roughly 11% to 16% of the dataset. shown in Figure 3, Strategy Distribution. As the most commonly used strategies are Information Delivery (14.9%), Emotional Management (11.9%), and Provide Suggestions (10.0%). This highlights the dual importance of delivering clear information and managing customer emotions in effective support conversations. Additional statistics on strategy transitions and unique strategy distributions are provided in Appendix C."
        },
        {
            "title": "3.4 CSC Task Definition\nWe denote a customer support conversation as D =\n{(Pi, Ti, Ui)}N\ni=1, consisting of N turns exchanged between\na supporter S and a customer C. For each turn, Pi ∈ {S, C}",
            "content": "4We use Qwen2.5-72B-Instruct to assign support strategy to each supporter response in the dialogues prior to rewriting. - - Total Supporter Customer Conversations Utterances Avg. Utterance Number Avg. Utterance Length Utterances Avg. Utterance Number Avg. Utterance Length Strategy Use (w/o Others) Utterances Avg. Utterance Number Avg. Utterance Length Number Original Rewritten 1,855 35,350 19.06 31.48 17,862 9.63 41.16 55.28% 17,488 9.43 21.60 1,855 50,587 27.27 33.27 25,810 13.91 48.72 97.82% 24,777 13.36 17.17 Table 1: Statistics of CSConv before and after rewriting. Topic Account and Transaction Management Product Consultation Technical Support and Online Services Complaints and Dispute Resolution Marketing and Promotion Activities Risk Management and Security Financial Consulting and Planning Others Overall Num Proportion 265 242 295 263 211 266 263 50 1, 14.3% 13.0% 15.9% 14.2% 11.4% 14.3% 14.2% 2.7% 100.0% Table 2: Distribution of topics in CSConv. denotes the speaker, Ui is the utterance text, and Ti represents the response strategy used. Strategies are selected from predefined set G, and are assigned only to the supporters turns. That is, if Pi = C, then Ti = NULL. Following the task of emotional support conversation (Liu et al. 2021; Ye et al. 2025), we define the CSC task as generating the supporters response. Specifically, at turn k, where Pk = S, the model receives the conversation history Xk = {(Pi, Ti, Ui)}k1 i=1 as input. The CSC task is then divided into two subtasks:5 1. Strategy Prediction: Predict the appropriate support strategy Tk based on the conversation history Xk. 2. Response Generation: Generate response Uk conditioned on both the predicted strategy Tk and the conversation history Xk, ensuring that the output is consistent with both the customers needs and the strategic intent."
        },
        {
            "title": "4 Synthetic Conversation Generation with",
            "content": "Role-Playing Agents Fine-tuning LLMs has shown effective for improving performance in task-specific dialogue applications. However, building high-quality, multi-turn customer support datasets remains challenging due to the need for domain expertise, contextual coherence, and dialogue diversity. This limits both the scalability and scenario coverage. While recent studies use LLMs for dataset augmentation via rewriting or imitation, they often yield limited variation in dialogue Figure 3: Strategy proportion of CSConv. flow and support strategy. To address this, inspired by Ye et al. (2025), we adopt multi-role role-playing framework to generate synthetic customer support dialogues that are diverse, coherent, and representative of real-world scenarios."
        },
        {
            "title": "4.1 Role Playing Conversation Generation\nAs shown in Figure 4, our role-playing framework involves\nfive roles: Planner, Supporter Assistant, Supporter, Cus-\ntomer Assistant, and Customer. Each role serves a distinct\npurpose. The Planner defines the dialogue scenario and sets\nthe Customer’s communication goal. The main interaction\noccurs between the Supporter and the Customer, simulating\na real customer service exchange. Meanwhile, their respec-\ntive Assistants, the Supporter Assistant and the Customer\nAssistant, offer strategic guidance to help them fulfill their\nroles more effectively. We use deepseek-r1 (Guo et al.\n2025) to simulate all roles, with detailed prompts available\nin Appendix E. We describe each role below.",
            "content": "Planner Before the conversation begins, the Planner selects topic from predefined list of customer topics in Table 2 (excluding Others). It then samples customer profile from character pool that contains diverse customer personas. Using the selected topic and customer profile o, the Planner prompts an LLM to generate detailed service scenario along with corresponding communication goal g, denoted as (g, e) = (o, e). This setup ensures that each dialogue is grounded in realistic context. Supporter Assistant The Supporter Assistant recommends an appropriate support strategy from the predefined strategy set G, based on the current dialogue history hs and the scenario e. The recommendation is generated by querying the LLM M, i.e., = M(hs, G, e). This strategic guidance helps the Supporter stay aligned with the customers needs and maintain coherent throughout the conversation. 5We further investigate how strategy prediction contributes to improving response generation in Section 6.3. Supporter Guided by the support strategy suggested by the Supporter Assistant, the Supporter generates contextuFigure 4: Illustration of synthetic conversation generation using role-playing agents. ally appropriate response rs. This response is produced by the LLM M, conditioned on the dialogue history hs, the strategy and the scenario e, i.e., rs = M(hs, t, e). Customer Assistant The Customer Assistant guides the conversation by generating the next direction d, ensuring alignment with the customers communication goal g. To do so, it prompts the LLM using the current dialogue history hc, the customers goal g, and the scenario e. This helps the customer maintain coherent and goal-oriented behavior throughout the conversation. Formally, the direction is generated as = M(hc, g, e). Customer The Customer generates response rc based on the current dialogue history hc, the intended conversational direction d, the character profile o, and the scenario e. This ensures that the response is both contextually appropriate and consistent with the customers persona. Formally, the generation process is denoted as: rc = M(hc, d, o, e). Diversity in conversations relies heavily on the richness of customer personas (Wang et al. 2025). To support this, we construct rich character profile pool that captures wide spectrum of customer backgrounds, behaviors, and communication styles, thereby enhancing the realism and variability of the simulated interactions. Character Profile Pool. We design comprehensive character profile template specifically for customer personas, incorporating attributes such as demographics, financial status, and communication preference. To populate this template, we use Qwen2.5-72B-Instruct to automatically extract and complete profile information from 15,980 realworld customer service dialogues. To reduce redundancy and ensure profile diversity, each structured profile is converted into free-text description, and pairwise cosine similarity is computed using Qwens text-embedding-v2. Profiles exceeding similarity threshold of 0.85 are considered redundant and removed. Following this filtering process, we retain 1,948 distinct customer profiles in our final pool. Additional details, including the profile template and construction prompts, are provided in Appendix D."
        },
        {
            "title": "5 Experimentation\nTest-Time Prompting. For both subtasks outlined in Sec-\ntion 3.4, we adopt a unified single-prompt method. Given the",
            "content": "Model Size B-2 B-4 R-L BS BR ACC B-2 B-4 R-L BS BR ACC GPT-4o DeepSeek-R1 DeepSeek-V3 LLaMA3.1-Instruct + RoleCS LLaMA3.1-Instruct + RoleCS Qwen2.5-Instruct + RoleCS Qwen2.5-Instruct + RoleCS - 671B 671B 8B 8B 70B 70B 7B 7B 72B 72B Evaluation with reference context Evaluation with generated context 8.13 11.67 11.57 4.28 11.06 6.85 11.73 6.73 11.23 8.61 12.15 2.97 5.09 4.95 1.44 4.77 2.38 5.11 2.29 4.80 3.23 5.32 4.12 8.44 7.04 3.62 6.93 4.10 7.64 3.80 7.39 5.41 7.97 64.26 66.57 66.09 58.68 66.21 63.14 66.62 62.75 66.35 64.63 66. 51.27 52.09 53.10 38.84 52.13 47.53 53.69 48.21 52.14 52.49 54.49 42.58 39.78 41.99 17.16 42.15 38.78 42.79 18.78 42.96 37.22 43.29 6.41 8.41 8.43 2.62 8.80 5.57 9.62 5.11 8.61 6.28 9.38 2.07 3.11 3.33 0.89 3.72 1.76 4.00 1.53 3.51 2.00 3.90 2.22 6.27 4.14 1.31 4.52 2.59 4.89 2.43 4.13 3.59 4.35 62.95 64.80 64.13 55.06 64.57 62.24 65.15 61.62 64.42 63.14 64. 51.48 52.01 53.09 35.97 49.85 46.83 50.91 47.43 48.80 50.55 53.65 36.29 35.23 36.54 13.75 35.73 30.36 39.44 19.39 30.52 30.93 36.02 Table 3: Performance comparison on CSConv. The best and second best results are in bold and underlined, respectively. task, we report strategy prediction accuracy (ACC)."
        },
        {
            "title": "5.2 Experimental Results\nTable 3 presents the main results on CSConv under two\nevaluation settings: (1) evaluation with reference context,\nwhich all LLMs are evaluated using the same gold history\nfor fair comparison, and (2) evaluation with generated con-\ntext, which assesses performance when relying on model-\ngenerated histories, thus reflecting the ability to maintain\ncoherence and relevance without ground truth context. From\nthe results, we have the following observations:",
            "content": "Larger models tend to perform better among nonfinetuned LLMs. Additionally, Chinese-centric models like Qwen and DeepSeek outperform more general models such as LLaMA and GPT, indicating that alignment with language and cultural context benefits CSC performance. RoleCS proves highly effective, as fine-tuning on it significantly improve performance across all metrics. In particular, Qwen2.5-Instruct-72B, after fine-tuning, matches or surpasses DeepSeek-R1, strong baseline for Chinese-language tasks. Evaluation with generated context shows similar performance trends as with reference context, but with lower absolute scores. This drop reflects the difficulty of maintaining consistency and quality in multi-turn conversations when relying on model-generated dialogue history, echoing findings in prior work such as Ye et al. (2025)."
        },
        {
            "title": "6 Discussion\nWe conduct further analysis under the reference context set-\nting to examine the effectiveness of our role-playing ap-\nproach in synthetic conversation generation.",
            "content": "Impact of Role-Playing on Data Quality"
        },
        {
            "title": "6.1\nTo evaluate our\nRoleCS with two baselines:",
            "content": "role-playing framework, we compare Baseline 1: Conversations are generated via in-context learning without any role-playing. Baseline 2: Role-playing is applied, but without the Supporter Assistant agent. Figure 5: Word overlap between cumulative customer utterances and profile (Aligned vs. Random). conversation history, the LLM, whether fine-tuned or not, is prompted once to first identify the appropriate support strategy and then generate the corresponding response. The full prompt is provided in the Appendix I."
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "CSC 2024), DeepSeek-V3 GPT-4o (Guo et several widely including evaluate task, DeepSeek-R1 (Liu Models. We used LLMs (Opeon the al. nAI 2024), 2025), Qwen-2.5-7B/72B-Instruct (Yang et al. 2024a), and LLaMA-3.1-8B/70B-Instruct (Grattafiori et al. 2024). To evaluate the impact of the proposed RoleCS, we fine-tune the Qwen and LLaMA models using 137,406 fine-tuning instances extracted from RoleCS, formatted consistently with our single-prompt in test-time. Fine-tuning and inference configurations are provided in Appendix J. al. et Metrics. Following prior work (Liu et al. 2021; Ye et al. 2025), we adopt diverse set of metrics to assess the response quality. These include BLEU-n (B-n) (Papineni et al. 2002) and ROUGE-L (R-L) (Lin 2004) for measuring lexical overlap, BERTScore (BS) (Zhang et al. 2020a) and BLEURT (BR) (Sellam, Das, and Parikh 2020) for semantic similarity. In addition, to evaluate alignment with the intended support strategy, which is core aspect of the CSC SFT Data Baseline 1 Baseline 2 RoleCS B-2 8.51 9.40 11. B-4 3.09 3.26 4.80 R-L 5.35 6.98 7.39 BS 64.35 65.22 66.35 BR 46.70 50.18 52.14 ACC 33.09 36.17 42.96 Strategy Vanilla Predict Oracle B-4 3.11 3.23 3.80 BS 64.43 64.63 66.34 ACC - 37.22 100.00 Table 4: Performance comparison of fine-tuning on different datasets. Table 5: Performance comparison across different support strategy variants. Model DeepSeek-R1 DeepSeek-V3 LLaMA3.1-Instruct LLaMA3.1-Instruct Qwen2.5-Instruct Qwen2.5-Instruct Size GPT-4o Qwen-Plus Human 3.55 671B 3.36 671B 2.93 8B 70B 3.58 3.10 7B 3.79 72B 89.70 89.54 88.68 89.76 88.76 89.98 90.89 90.54 90.34 91.02 90.46 91.04 Figure 6: Performance comparison under different synthetic conversation dataset sizes. All three datasets are generated using Deepseek-R1 with equal conversation counts. For fairness, we fine-tune Qwen2.5-7b-Instruct on each. As shown in Table 4, role-playing improves performance over in-context learning (Baseline 2 vs. Baseline 1), and adding the Supporter Assistant further boosts results (RoleCS vs. Baseline 2). These results highlight the value of strategy-rich training data and the importance of high-quality dataset construction."
        },
        {
            "title": "6.3\nWe evaluate the effect of incorporating support strategies\nusing three variants of Qwen2.5-72B-Instruct with-\nout fine-tuning: (1) Vanilla, where the model generates a re-\nsponse without any support strategy guidance; (2) Predict,\nour default setup, where the model first predicts the strat-\negy and then generates a response; and (3) Oracle, where\nthe ground-truth strategy is provided in the prompt.",
            "content": "As shown in Table 5, the Predict variant slightly outperforms the Vanilla, indicating that even simple strategy prediction improves response quality. The Oracle variant performs best, highlighting that more accurate strategy prediction can further enhance the performance of CSC task. Table 6: Model performance evaluated by GPT-4o, QwenPlus, and human judges. denotes fine-tuned models. Agreement GPT-4o-Judge & Human Annotators Human Annotators Kappa 0.658 0.628 Table 7: Fleiss Kappa scores."
        },
        {
            "title": "6.4 Evaluation with LLMs and Human as Judges\nTo mitigate potential bias (e.g., GPT-4o favoring its own\noutputs), we use both GPT-4o and Qwen-Plus6 to evaluate\nresponse quality across six dimensions: accuracy, helpful-\nness, understanding, coherence, informativeness, and em-\npathy, each scored on a 0–100 scale. We report the av-\nerage overall scores from both GPT-4o-Judge and Qwen-\nPlus-Judge, using the evaluation prompt in Appendix K. As\nshown in Table 6, both judges reveal consistent performance\npatterns. Notably, fine-tuned Qwen2.5-Instruct-72B\nand LLaMA3.1-Instruct-70B outperform other mod-\nels, even exceeding DeepSeek-R1 in overall quality.",
            "content": "For human evaluation, we randomly select 100 conversations and have them independently rated by three professional annotators on 1-5 Likert scale (Joshi et al. 2015) across the same six dimensions. Table 6 shows the average overall scores, which align with trends from the LLM-based evaluations. Table 7 reports Fleiss Kappa (Fleiss 1971) scores among annotators and between GPT-4o-Judge and the annotators. The results show strong inter-rater agreement and confirm that GPT-4o-Judge aligns well with human judgment."
        },
        {
            "title": "7 Conclusion\nThis paper addresses the challenges of customer support\nconversations (CSCs) in the NLP field by introducing\nCSConv, a high-quality dataset grounded in support strate-\ngies, and a role-playing framework for generating realistic,\ngoal-driven dialogues. Our approach significantly enhances\nLLMs’ ability to generate coherent, context-aware, and em-\npathetic responses in customer service scenarios. We believe",
            "content": "6https://help.aliyun.com/zh/model-studio/models that our contributions will inspire further research in this field, promoting advancements in empathetic and effective customer support technologies. References Bae, S.; Kwak, D.; Kim, S.; Ham, D.; Kang, S.; Lee, S.-W.; and Park, W. 2022. Building Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models. In Proceedings of NAACL-HLT, 21282150. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language models In Proceedings of NeurIPS, 1877 are few-shot learners. 1901. Budzianowski, P.; Wen, T.-H.; Tseng, B.-H.; Casanueva, I.; Ultes, S.; Ramadan, O.; and Gaˇsic, M. 2018. MultiWOZ - Large-Scale Multi-Domain Wizard-of-Oz Dataset for TaskOriented Dialogue Modelling. In Proceedings of EMNLP, 50165026. Byrne, B.; Krishnamoorthi, K.; Sankar, C.; Neelakantan, A.; Goodrich, B.; Duckworth, D.; Yavuz, S.; Dubey, A.; Kim, K.-Y.; and Cedilnik, A. 2019. Taskmaster-1: Toward Realistic and Diverse Dialog Dataset. In Proceedings of EMNLPIJCNLP, 45164525. Cui, L.; Huang, S.; Wei, F.; Tan, C.; Duan, C.; and Zhou, M. 2017. SuperAgent: Customer Service Chatbot for In Proceedings of ACL, System E-commerce Websites. Demonstrations, 97102. El Asri, L.; Schulz, H.; Sharma, S.; Zumer, J.; Harris, J.; Fine, E.; Mehrotra, R.; and Suleman, K. 2017. Frames: corpus for adding memory to goal-oriented dialogue systems. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, 207219. Fleiss, J. L. 1971. Measuring Nominal Scale Agreement among Many Raters. Psychological Bulletin, 76(5): 378 382. Grattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; and Others. 2024. The Llama 3 Herd of Models. CoRR, abs/2407.21783. Gung, J.; Moeng, E.; Rose, W.; Gupta, A.; Zhang, Y.; and Mansour, S. 2023. NatCS: Eliciting Natural Customer Support Dialogues. In Findings of ACL, 96529677. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. CoRR, abs/2501.12948. Hill, C. E. 2019. Helping Skills: Facilitating Exploration, Insight, and Action. American Psychological Association, 5th edition. Hu, E. J.; yelong shen; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In Proceedings of ICLR. CoRR, GPT-4o System Card. Joshi, A.; Kale, S.; Chandel, S.; and Pal, D. K. 2015. Likert scale: Explored and explained. British journal of applied science & technology, 7(4): 396. Kelley, J. F. 1984. An iterative design methodology for user-friendly natural language office information applications. ACM Trans. Inf. Syst., 2(1): 2641. Li, J.; Galley, M.; Brockett, C.; Gao, J.; and Dolan, B. 2016. Diversity-Promoting Objective Function for Neural Conversation Models. In Proceedings of NAACL-HLT, 110119. Lin, C.-Y. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, 7481. Liu, A.; Feng, B.; Xue, B.; Wang, B.; Wu, B.; et al. 2024. DeepSeek-V3 Technical Report. CoRR, abs/2412.19437. Liu, S.; Zheng, C.; Demasi, O.; Sabour, S.; Li, Y.; Yu, Z.; Jiang, Y.; and Huang, M. 2021. Towards Emotional Support In Proceedings of ACL-IJCNLP, 3469 Dialog Systems. 3483. Mesquita, T.; Martins, B.; and Almeida, M. 2022. Dense Template Retrieval for Customer Support. In Proceedings of COLING, 11061115. OpenAI. 2024. abs/2410.21276. Oraby, S.; Gundecha, P.; Mahmud, J.; Bhuiyan, M.; and Akkiraju, R. 2017. How May Help You?: Modeling Twitter Customer ServiceConversations Using Fine-Grained Dialogue Acts. In Proceedings of IUI, 343355. Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, 311318. Peskov, D.; Clarke, N.; Krone, J.; Fodor, B.; Zhang, Y.; Youssef, A.; and Diab, M. 2019. Multi-Domain GoalOriented Dialogues (MultiDoGO): Strategies toward Curating and Annotating Large Scale Dialogue Data. In Proceedings of EMNLP-IJCNLP, 45264536. Rashkin, H.; Smith, E. M.; Li, M.; and Boureau, Y.-L. 2019. Towards Empathetic Open-domain Conversation Models: New Benchmark and Dataset. In Proceedings of ACL, 5370 5381. Salton, G.; Wong, A.; and Yang, C. S. 1975. Vector Space Model for Automatic Indexing. Communications of the ACM, 18(11): 613620. Sellam, T.; Das, D.; and Parikh, A. 2020. BLEURT: Learning Robust Metrics for Text Generation. In Proceedings of ACL, 78817892. Thoppilan, R.; Freitas, D. D.; Hall, J.; Shazeer, N.; Kulshreshtha, A.; and Others. 2022. LaMDA: Language Models for Dialog Applications. CoRR, abs/2201.08239. Wang, X.; Zhang, H.; Ge, T.; Yu, W.; Yu, D.; and Yu, D. 2025. OpenCharacter: Training Customizable RolePlaying LLMs with Large-Scale Synthetic Personas. CoRR, abs/2501.15427. Wen, T.-H.; Vandyke, D.; Mrkˇsic, N.; Gaˇsic, M.; RojasBarahona, L. M.; Su, P.-H.; Ultes, S.; and Young, S. 2017. Network-based End-to-End Trainable Task-oriented Dialogue System. In Proceedings of EACL, 438449. Wu, W.; Wu, H.; Jiang, L.; Liu, X.; Zhao, H.; and Zhang, M. 2024. From Role-Play to Drama-Interaction: An LLM Solution. In Findings of ACL, 32713290. Xu, A.; Liu, Z.; Guo, Y.; Sinha, V.; and Akkiraju, R. 2017. New Chatbot for Customer Service on Social Media. In Proceedings of CHI, 35063510. Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; et al. 2024a. Qwen2.5 Technical Report. CoRR, abs/2412.15115. Yang, B.; Liu, D.; Xiao, C.; Zhao, K.; Tang, C.; Li, C.; Yuan, L.; Yang, G.; Huang, L.; and Lin, C. 2024b. Crafting Customisable Characters with LLMs: Introducing SimsChat, Persona-Driven Role-Playing Agent Framework. CoRR, abs/2406.17962. Ye, J.; Xiang, L.; Zhang, Y.; and Zong, C. 2025. SweetieChat: Strategy-Enhanced Role-playing Framework for In Diverse Scenarios Handling Emotional Support Agent. Proceedings of COLING, 46464669. Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2020a. BERTScore: Evaluating Text Generation with BERT. In Proceedings of ICLR. Zhang, Y.; Sun, S.; Galley, M.; Chen, Y.-C.; Brockett, C.; Gao, X.; Gao, J.; Liu, J.; and Dolan, B. 2020b. DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation. In Proceedings of ACL: System Demonstrations, 270278. Zheng, Y.; Zhang, R.; Zhang, J.; Ye, Y.; and Luo, Z. 2024. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models. In Proceedings of ACL: System Demonstrations, 400410."
        },
        {
            "title": "Appendix",
            "content": "A Stages and Strategies in CSC Framework Table 8 presents the five stages in the CSC framework while Table 9 shows the twelve strategies in the framework. More Details of Data Construction In the following, we present more details to the prefiltering, rewriting and post-filtering in data construction. Then, we compare written conversations by DeepSeek-R1 and GPT-4o. B.1 Rules and Prompt for Pre-filtering To ensure the quality and suitability of the initial customer support conversations, we apply set of rule-based prefiltering rules prior to rewriting. These rules help filter out conversations that are structurally inadequate or contain limited user engagement: Rule 1: Dialogue Length Constraint. Each conversation must contain more than 6 but fewer than 60 utterances to ensure sufficient interaction. Rule 2: Utterance Length Constraint. No single utterance should exceed 500 characters. Rule 3: Speaker Balance. The number of utterances from the service supporter must not exceed twice the number of customer utterances, promoting balanced participation. Rule 4: Customer Utterance Effectiveness. At least 70% of customer utterances must be considered effective, where an utterance is defined as effective if it contains more than 3 characters, ensuring active and meaningful user input. In we utilize LLM an addition, (Qwen2.5-72B-Instruct) to automatically assess whether conversation is of low quality. conversation is flagged as low quality if it contains any of the following: (1) explicitly offensive, abusive, or inappropriate language, or (2) clear signs of unprofessional behavior from the service supporter, such as impatience, indifference, or disrespect. Figure 7 shows the prompt used for this evaluation. B.2 Prompt for Rewriting Figure 8 shows the prompt for rewriting. B.3 Rules and Prompt for Post-filtering After the conversations are rewritten, we apply series of post-filtering rules to ensure quality and structural consistency: Rule 1: Minimum Utterance Requirement. Each conversation must contain at least 10 utterances and at most 50 utterance to ensure sufficient conversational depth. Rule 2: System Message Removal. Any systemgenerated messages (e.g., (System Auto-Push)) at the end of conversations are removed to maintain textual clarity. Rule 3: Strategy Presence Check. The support strategies Greeting, Identity Verification, and Appreciation and Closure must each appear at least once in the dialogue to reflect complete and well-structured interaction. Rule 4: Speaker Alternation Constraint. The customer and service supporter must alternate speaking in the conversation. After applying above rules, we further assess the overall quality of the rewritten conversations using an LLM (Qwen2.5-72B-Instruct). The model is prompted to evaluate each dialogue and classify it as either high or low quality based on coherence, naturalness, and adherence to support strategies. Conversations deemed to be of low quality are subsequently discarded. The prompt used for this evaluation is shown in Figure 9. B.4 Annotation Guideline Figure 10 shows the annotation guidelines for customer support conversations, designed by domain experts. B.5 Example of Rewritten Conversations by Deepseek-R1 and GPT-4o Figure 11 shows conversation before rewriting, while Figure 12 and Figure 13 compare the rewritten conversations by GPT-4o and DeepSeek-R1. More Statistics of CSConv Strategy Transition. To analyze the distrubition of support strategies across different phases of customer support conversations, we divide each conversation into six equal intervals, denoted as I1, ,I6, representing normalized progression from start to end. For each interval Ii, we calculate the proportion of each strategy as the ratio of its frequency to the total number of strategies employed within that interval. Figure 14 shows the distribution. At the beginning of conversations, the Greeting (GT) strategy is most prevalent, reflecting typical protocol for initiating customer interactions. As the conversation progresses, strategies such as Information Delivery (ID) and Provide Suggestions (PS) become more frequent, indicating shift toward problem exploration and resolution. Toward the end of the interaction, Appreciation and Closure (AC) emerges as the dominant strategy, signaling the completion of the support process. Interestingly, the usage of Emotional Management (EM) remains relatively stable across all phases, underscoring the consistent need for emotional engagement throughout the conversation. To analyze strategy transitions, we identify the most frequent 2-hop and 3-hop patterns in the conversations. The results are presented in Table 10. These patterns highlight common strategy flows such as Greeting (GT) Identity Verification (IV) and Information Delivery (ID) Providing Suggestions (PS), reflecting structured and consistent progression in customer service interactions. The observed transitions align well with standard service protocols, emphasizing coherent dialogue management. Stage Description Connecting Identifying Exploring Resolving Maintaining Greeting and establishing connection Understanding and identifying problems Seeking solution Resolving and confirming Ending and maintaining relationship Table 8: Five stages in the CSC framework. Strategy Greeting (GT) Identity Verification (IV) Emotional Management (EM) Restatement or Paraphrasing (RP) Problem Refinement (PR) Providing Suggestions (PS) Information Delivery (ID) Resolution Implementation (RI) Feedback Request (FR) Appreciation and Closure (AC) Relationship Continuation (RC) Others Stages Description Utilize friendly and professional language to greet customers, creating warm communication atmosphere. Ensure the accuracy and security of the service by asking for the customers basic information. Express understanding and care for the customers feelings to help alleviate negative emotions. Restate the customers issue to ensure accurate understanding. Employ detailed inquiries to fully and accurately comprehend customer needs. Offer professional advice or action steps based on the customers issue. Clearly explain relevant company policies, processes, or steps to help customers understand the basis of solutions. Execute the agreed-upon solution, ensuring all steps are followed as planned, and update the customer on the progress. Seek customer feedback after the issue has been addressed to gauge their satisfaction and identify potential areas for improvement. End the conversation positively, ensuring the customer feels valued and laying solid foundation for future interactions. Guide customers towards future service or product updates, ensuring they understand how to continue receiving support and service in the future, thereby establishing bridge for further interaction. Situations that do not belong to the above eleven strategies. Table 9: Twelve strategies in the CSC framework. The cells of lightblue , lightgreen , lightpurple , lightyellow , and lightpink represent the Connecting, Identifying, Exploring, Resolving, and Maintaining stages, repsectively. - 2-Hop 3-Hop Pattern GT IV IV RP ID PS FR AC EM ID GT IV RP IV RP EM RC FR AC ID PS RI EM ID PS Proportion 5.69% 4.84% 4.55% 4.22% 3.59% 4.85% 2.57% 2.42% 1.91% 1.77% Table 10: Top-5 2-hop and 3-hop strategy patterns. ent strategy types, demonstrating the broad applicability and flexibility of the predefined strategy set. This variety reflects the dynamic nature of real-world customer service interactions and enhances the overall richness of the dataset."
        },
        {
            "title": "D Details of Character Profile Construction",
            "content": "Figure 16 illustrates an example of profiles, which are constructed using the prompt presented in Figure 17 (In Chinese) and Figure 18 (In English). Figure 19 shows the prompt for profile-to-text conversion."
        },
        {
            "title": "Conversation Dataset",
            "content": "Distribution of Strategy Types. To examine the diversity of support strategies, we analyze the number of distinct strategy types used within individual conversations, as shown in Figure 15. Most conversations employ more than ten differFigures 20, 21, 22, 23, and 24 illustrate the prompts used by Planner, Supporter Assistant, Supporter, Customer Assistant, and Customer, respectively, during the synthetic dialogue generation process. These prompts guide each roles Figure 7: Prompt template for pre-filtering. - Total Supporter Customer - Dialogues Utterances Avg. Utterance Number Avg. Utterance Length Utterances Avg. Utterance Number Avg. Utterance Length Utterances Avg. Utterance Number Avg. Utterance Length Number 11,232 263,580 23.47 57.14 137,406 12.23 66.98 126,174 11.23 46.43 Table 11: Statistics of the RoleCS dataset. behavior to ensure coherence, strategy alignment, and contextual consistency throughout the conversation. Filtering Rules for Synthetic Dialogues We first discard dialogues with fewer than 10 or more than 50 utterances, then use the prompt from Figure 25 to filter out low-quality dialogues. Synthetic Dataset Analysis Similar to the analysis of the CSConv dataset, Table 11 summarizes the statistics of the RoleCS dataset, while Figure 26 shows strategy distribution across conversation progress, and Figure 27 illustrates the variety of strategy Dataset CSConv RoleCS Distinct-1 Distinct-2 Distinct-3 19.27 22.35 46.07 55.31 1.09 0.95 Table 12: Distinct-n scores of 1,000 conversations from CSConv and RoleCS. types within dialogues. Diversity Analysis We examine the diversity of the CSConv and RoleCS datasets from two perspectives: lexical diversity and semantic diversity. To assess lexical diversity, we adopt Distinct-n (Li et al. 2016) metric. For fair comparison, we randomly select 1,000 dialogues from each dataset, removing speaker and strategy information, and utilizing Qwen2.5-7B tokenizer for tokenization. Regarding semantic diversity, we calculate cosine similarity between pairs of distinct dialogues using TF-IDF features (Salton, Wong, and Yang 1975). Table 12 and Figure 28 present the results, respectively."
        },
        {
            "title": "Generating Response",
            "content": "Figure 29 presents the prompt for predicting strategy and generating response on the CSC task. Details of Fine-tuning and Inference For fine-tuning, we use the LLaMA-Factory framework (Zheng et al. 2024), applying LoRA (Hu et al. 2022) with rank of 8 and scaling factor of 16. All fine-tunings are conducted on 4 NVIDIA A100 80GB GPUs, with batch size of 4 and gradient accumulation over 2 steps. The learning rate is set to 3e-5, and fine-tuning is run for 3 epochs to mitigate the risk of overfitting. For inference, we use the checkpoint from the final epoch and configure the decoding parameters with top-p value of 0.7 and temperature of 0.95. Prompt for LLM as Judge Figure 30 shows the evaluation prompt used when employing GPT-4o and Qwen-Plus as judges. Figure 8: Prompt template for rewriting. Figure 9: Prompt template for post-filtering. Figure 10: Annotation guideline for evaluating the quality of customer support conversations. Figure 11: An example from real-world customer service conversations. Figure 12: Rewritten dialogue by GPT-4o. Figure 13: Rewritten dialogue by DeepSeek-R1. Figure 14: Strategies distribution at different conversation progress on CSConv dataset. Figure 15: Distribution of strategy types across different dialogues on CSConv. Figure 16: Profile example. Figure 17: Prompt template for completing profile based on real-world customer service conversation (In Chinese). Figure 18: Prompt template for completing profile based on real-world customer service conversation (In English). Figure 19: Prompt template for transforming structured profile into descriptive text. Figure 20: Prompt template used by planner agent. Figure 21: Prompt template used by supporter assistant agent. Figure 22: Prompt template used by supporter agent. Figure 23: Prompt template used by customer assistant agent. Figure 24: Prompt template used by customer agent. Figure 25: Prompt template used in filtering synthetic conversations. Figure 26: Strategies distribution at different conversation progress in the RoleCS dataset. Figure 27: Number of strategy types in dialogue within the RoleCS dataset. Figure 28: Cosine similarity statistics between pairs of distinct dialogues using TF-IDF vectors. Figure 29: Prompt template for predicting support strategy and generating response on the CSC task. Figure 30: Prompt template used by LLM as judge."
        }
    ],
    "affiliations": [
        "Qwen DianJin Team, Alibaba Cloud Computing",
        "School of Computer Science and Technology, Soochow University"
    ]
}