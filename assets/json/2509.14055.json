{
    "paper_title": "Wan-Animate: Unified Character Animation and Replacement with Holistic Replication",
    "authors": [
        "Gang Cheng",
        "Xin Gao",
        "Li Hu",
        "Siqi Hu",
        "Mingyang Huang",
        "Chaonan Ji",
        "Ju Li",
        "Dechao Meng",
        "Jinwei Qi",
        "Penchong Qiao",
        "Zhen Shen",
        "Yafei Song",
        "Ke Sun",
        "Linrui Tian",
        "Feng Wang",
        "Guangyuan Wang",
        "Qi Wang",
        "Zhongjian Wang",
        "Jiayu Xiao",
        "Sheng Xu",
        "Bang Zhang",
        "Peng Zhang",
        "Xindi Zhang",
        "Zhe Zhang",
        "Jingren Zhou",
        "Lian Zhuo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 5 0 4 1 . 9 0 5 2 : r WAN-ANIMATE: UNIFIED CHARACTER ANIMATION AND REPLACEMENT WITH HOLISTIC REPLICATION HumanAIGC Team Tongyi Lab, Alibaba"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Wan-Animate, unified framework for character animation and replacement. Given character image and reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scenes lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the characters appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code. Project Page: https://humanaigc.github.io/wan-animate/"
        },
        {
            "title": "INTRODUCTION",
            "content": "Character image animation is significant research area that has achieved remarkable progress, driven by advancements in visual generative techniques, particularly the application of diffusion models. This technology holds substantial potential value, with wide-ranging applications in filmmaking, advertising, and the creation of digital avatars. Previous works, such as Hu (2024); Zhu et al. (2025); Tu et al. (2025), have thoroughly explored diffusion-based architectures for character image animation, introducing significant enhancements in consistency and controllability. Furthermore, some studies Hu et al. (2025); Men et al. (2024); Xu et al. (2024c); Wang et al. (2025a) have incorporated environmental information, extending the generative capabilities to more versatile tasks like character replacement and human-object interaction synthesis. More recently, following the introduction of Sora Brooks et al. (2024) by OpenAI, video generation leveraging the Diffusion Transformer Peebles & Xie (2023) architecture has undergone explosive development. The emergence of numerous open-source works Kong et al. (2025); Wan et al. (2025) has catalyzed parallel advancements in related sub-tasks. Consequently, DiT-based character image animation Gan et al. (2025); Zhou et al. (2025); Luo et al. (2025) has also garnered considerable research interest. By capitalizing on the capabilities of pre-trained video foundation models, the realism and temporal coherence of the generated character videos have been substantially improved. However, critical gap remains: no existing framework provides holistic solution for high-fidelity character animation that unifies the control of motion, expression, and environment interaction. Within the open-source domain, existing works on character image animation exhibit significant shortcomings in both performance and completeness. The majority of open-source contributions, such as Zhu et al. (2025); Zhang et al. (2024); Tu et al. (2025); Wang et al. (2024), are designed based on UNet-based foundation models (e.g., SD Rombach et al. (2022), SVD Blattmann et al. (2023)), and their results lag considerably behind the current state-of-the-art. While some DiT-based open-source projects Zhou et al. (2025); Wang et al. (2025b) focus on motion control, they fall short 1 Figure 1: Given character image and reference video, Wan-Animate supports two core functionalities. In the first, which we term Animation, it reenacts the motion and expression of the character in the reference video to animate the static source image. In the second, termed Replacement, it substitutes the character in the reference video with the source identity, ensuring seamless integration with the environment. in holistically replicating expressive facial dynamics in conjunction with body movements. Furthermore, there is scarcity of dedicated open-source methods for integrating character animation with environmental contexts (i.e., character replacement). Although some video generation methods Jiang et al. (2025) can approximate this functionality, they typically suffer from issues with consistency and usability. These unresolved challenges collectively hinder the continuous development and innovation of character image animation within the open-source community. To address the aforementioned issues, this report introduces unified framework for Character Animation and Replacement, termed Wan-Animate, which achieves holistic replication with highfidelity results. As illustrate in Figure 1, given character image and reference video, Wan-Animate can accurately replicate the facial expressions and body movements from the reference to animate the character, generating realistic character video. Concurrently, Wan-Animate supports character replacement, enabling the animated character to be integrated into the reference video to replace the original character. This process also replicates the videos lighting and color tone, achieving seamless fusion of the character and the environment. Methodologically, Wan-Animate is built upon the Wan-I2V model as its foundation and is enhanced through post-training with additional control conditions. Compared to Wan-I2V, Wan-Animate features modified input definition tailored for the demands of character animation. We employ modified input paradigm to differentiate between reference conditions and regions designated for generation, which in turn guides the injection of corresponding latents. This design unifies reference image injection, temporal frame guidance, and the mode selection between full-frame generation and character replacement into common symbolic representation. Crucially, this approach preserves the original input structure of Wan-I2V, thereby minimizing distributional shift during post-training. To achieve holistic character control, we decouple the control signals into body motion and facial expressions. For body motion, we adopt skeleton-based representation to balance accuracy and generality. As this signal is spatially aligned, it is injected by being added to the initial noise latents. For expression replication, we directly use the original face images from the reference video as the driving signal to preserve maximum detail. These face images are encoded into latent vectors to disentangle expression information from identity attributes. These latents are then temporally compressed to align with the video latents and are injected into the model via cross-attention. This joint control strategy demonstrates high robustness and precision. When performing character replacement, we develop an auxiliary Relighting LoRA to enhance the consistency between character and 2 environment. After the base model ensures the consistent transfer of the characters appearance, this module applies appropriate environmental lighting and color tones, resulting in more seamless integration of the replaced character into the video. We are committed to releasing the entire Wan-Animate framework to the public, encompassing the model weights and the complete pipeline. Experimental results demonstrate that Wan-Animate possesses excellent and versatile capabilities. It not only animates characters to generate expressive videos but also generalizes well to wide range of humanoid characters, demonstrating strong robustness across various scenarios such as portraits, half-body, and full-body shots. Furthermore, it exhibits competitive advantage in quality even when compared to several closed-source commercial products. We hope that the release of Wan-Animate will make significant contribution to accelerating the development of character image animation. We also aim to empower developers of all levels with access to high-caliber model, enabling them to build diverse applications, inspire novel product paradigms, and ultimately facilitate the technologys real-world deployment."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Character Image Animation. Earlier works in image animation primarily focused on warpingbased feature representations and GAN-based architectures Siarohin et al. (2019); Zhao & Zhang (2022); Siarohin et al. (2021). In contrast, recent approaches Hu (2024); Xu et al. (2024b); Chang et al. (2023); Zhu et al. (2025); Tu et al. (2025) have shifted to designing architectures based on diffusion models Ho et al. (2020), which has led to significant improvements in performance. For instance, Animate Anyone Hu (2024) utilizes ReferenceNet structure to inject the characters appearance features, achieving excellent results in consistency preservation. For temporal modeling, it employs temporal layers inspired by AnimateDiff Guo et al. (2023), which are embedded within the Stable Diffusion Rombach et al. (2022) architecture. This design paradigm has been adopted by many subsequent methods Zhu et al. (2025); Tu et al. (2025). However, as foundation model primarily for image generation, Stable Diffusion lacks inherent temporal knowledge. To address this limitation, some works Wang et al. (2024); Zhang et al. (2024) have started building upon video foundation models. Since these models already possess knowledge of inter-frame consistency and continuity from their pre-training, the resulting image animation architectures can be more streamlined. More recently, with the dramatic improvement in video generation capabilities brought by DiT-based models Kong et al. (2025); Wan et al. (2025); Yang et al. (2024), their application to image animation Zhou et al. (2025); Luo et al. (2025) has led to substantial enhancements in the realism and temporal continuity of generated characters. Correspondingly, our Wan-Animate is also built upon the open-source model Wan2.1, fully leveraging its robust pre-trained knowledge to ensure high-quality visual generation from the outset. Beyond the generation of the character itself, some works also investigate the fusion of the animated character with its surrounding environment or with objects it interacts with. For instance, Men et al. (2024) takes an additional background image as input, enabling the generated character video to feature specified scene. Another approach, proposed by Hu et al. (2025), utilizes masking mechanism to differentiate between the background environment and interactive objects, which results in generated characters that are highly compatible with the environment. Conceptually, these methods can be adapted for the task of video character replacement. Furthermore, related line of research focuses on generating videos of human-object interactions Xu et al. (2024c); Wang et al. (2025a). These methods aim to generate not only moving character but also co-moving object that is animated in physically consistent and synchronized manner. Facial Animation. The field of facial animation Wang et al. (2022); Guo et al. (2024) has also benefited significantly from the application of diffusion models, achieving remarkable advancements. Drawing inspiration from pose-guided human animation, some early works Wei et al. (2024); Ma et al. (2024); Zhang et al. (2024); Tu et al. (2025) employed facial landmarks as control signals to generate expressions, with their architectural designs closely following the human animation paradigm. However, compared to body poses, facial landmarks can lose fine-grained details during extraction, which compromises the expressiveness of the resulting animations and makes it difficult to synthesize subtle expressions. Moreover, in cross-identity application scenarios, dense facial landmarks demand high precision in signal retargeting, posing significant challenge when driving diverse character identities. More recently, some methods Xu et al. (2024a); Drobyshev et al. (2024); 3 Figure 2: Overview of Wan-Animate, which is built upon Wan-I2V. We modify its input formulation to unify reference image input, temporal frame guidance, and environmental information (for dual-mode compatibility) under common symbolic representation. For body motion control, we use skeleton signals that are merged via spatial alignment. For facial expression control, we leverage implicit features extracted from face images as the driving signal. Additionally, for character replacement, we train an auxiliary Relighting LoRA to enhance the characters integration with the new environment. Ki et al. (2024); Xie et al. (2024); Zhao et al. (2025); Xu et al. (2025); Luo et al. (2025) have begun to move away from manually defined motion signals, instead using the raw source images to extract implicit representations as control inputs. This approach has led to substantial improvements in both expressiveness and generality. Video Generation. Since the release of Sora Brooks et al. (2024) by OpenAI, DiT-based Peebles & Xie (2023) approaches have gradually supplanted UNet-based ones, becoming the mainstream technical route in video generation research. The adoption of pure Transformer architecture allows model parameters to be scaled up significantly. Coupled with the expansion of training data, this has resulted in qualitative leap in video generation quality. To fuse multi-modal features within Transformer, videos are tokenized into discrete sequences. Specifically, 3D VAE Kingma & Welling (2014) is first used to compress the video in both spatial and temporal dimensions, drastically reducing feature length. The compressed representation is then discretized through patchifying process. Most recent DiT-based video generation models Kong et al. (2025); Wan et al. (2025); Yang et al. (2024) follow this pipeline. As the technology has matured, growing number of foundation models for video generation have been open-sourced, with HunyuanVideo and Wan being particularly representative works. These have spurred surge of follow-up research and applications in video generation. Consequently, the field of character image animation has also benefited from the advancements in video foundation models, with its performance gradually aligning with the capabilities of general video generation."
        },
        {
            "title": "3 MODEL DESIGN AND ARCHITECTURE",
            "content": "3.1 TASK DEFINITION Wan-Animate features two core functionalities: Animation Mode and Replacement Mode. In Animation Mode, the character from source image is animated according to the motion of the character in reference video, while the background from the source image is preserved. This process is analogous to an Image-to-Video (I2V) synthesis task. In Replacement Mode, the character from the source image is driven by the same reference motion but is then integrated into the environment of the reference video. This effectively replaces the original subject, task that corresponds to Videoto-Video (V2V) translation. The common objective of both modes is to accurately replicate the motion and facial expressions from the reference character. The key distinction lies in the source of the final videos background: in Animation Mode, it is derived from the source image, whereas in Replacement Mode, it is inherited from the reference video. Wan-Animate unifies both modes within single, jointly trained model, with the exception of the Relighting LoRA which is specific to the Replacement Mode. By making minor adjustments to the input format, the model can generate outputs in the desired mode. The overall architecture is shown in Figure 2. 4 3."
        },
        {
            "title": "INPUTS FORMULATION",
            "content": "Wan-Animate leverages Wan-I2V as its foundational architecture. The input to Wan-I2V consists of three components: noise latent, conditional latent, and binary mask. Since the I2V task is defined as generating video from given image as the first frame, the conditional latent is constructed by concatenating the given image with zero-filled frames along the temporal dimension. The binary mask, which shares the same spatial and temporal dimensions as the conditional latent, uses value of 1 to denote preserved frames and 0 for frames to be generated. For I2V, only the mask for the first frame is set to 1. However, character image animation imposes different requirements on the input paradigm. Firstly, unlike the I2V setup where the image serves as the starting frame, our task requires character image to act as consistent appearance reference. The content of the generated video is dictated by driving signals, not initiated from the character image itself. Secondly, to enable animation of arbitrary length, the generation of subsequent segments must be conditioned on the final frame(s) of the preceding segment. This provides temporal guidance and ensures continuity, facilitating the synthesis of long videos. Third, we aim to unify the Animation Mode and Replacement Mode into single model through compatible representation, thereby reducing redundant training efforts. Therefore, to accommodate these unique demands, Wan-Animate introduces modified input paradigm based on the original Wan-I2V formulation. Reference Formulation. Given reference character image, we first encode it into dedicated reference latent using the Wan-VAE. To fully leverage the inter-frame consistency capabilities pretrained in the Wan model, the reference latent is concatenated with the conditional latents along the temporal dimension (with the binary mask set to 1). This concatenation serves as the primary mechanism for injecting the characters appearance. To accommodate the temporal guidance required for long video synthesis, we randomly select the first few latents from the target sequence to serve as temporal latents. For these selected latents, their corresponding ground-truth values are used as the condition latents, and the associated binary mask is set to 1 across the entire frame. This enables the model to generate temporally coherent videos guided by these temporal frames. We employ probabilistic training strategy where temporal latents are used only with certain probability. This approach ensures the model learns to balance its generative capabilities across different conditional inputs. Notably, the denoising process generates complete output sequence, including the portions for the references. The resulting frames that correspond to these references are subsequently discarded. Environment Formulation. In Animation Mode, the conditional frames corresponding to the target frames are zero-filled, and their associated binary mask is set entirely to 0. Consequently, WanAnimate generates the character video while preserving the background from the given reference image, process analogous to the standard I2V mode. In Replacement Mode, we first segment the character from the reference video. Following the mask formulation strategy from Hu et al. (2025), we then generate environment images by zeroing out the segmented subject region. This environment image serves as the content for the condition frames. Correspondingly, the binary mask is set to 1 for the environment regions and 0 for the subject region. As result, Wan-Animate only generates content within the mask-zeroed areas, thereby preserving the original background of the reference video. In summary, the input paradigm of Wan-Animate, while adapted for new tasks, fundamentally inherits the core philosophy of Wan-I2V. This design elegantly accommodates the diverse conditional requirements of character animation and supports dual generative modes. This adaptability allows the model to be fine-tuned rapidly and effectively during post-training, leading to strong empirical results. 3.3 CONTROL SIGNALS Body Control. Prior research has demonstrated the effectiveness of spatially-aligned signals for guiding human video generation. In terms of current technical approaches, there are two primary types of body control signals: 2D skeleton-based representations and rendered images from 3D SMPL Loper et al. (2023). The skeleton-based approach offers better generality, particularly for non-humanoid characters with unconventional shapes, demonstrating greater robustness. However, it faces challenges in representing complex motions due to spatial ambiguity and is susceptible to issues like missing or erroneous keypoints. Conversely, SMPL, as 3D signal, provides more 5 Figure 3: Face images are encoded into frame-wise implicit latents, which are then temporally aligned with the DiT latents. These features are injected via cross-attention mechanism that operates within each corresponding temporal segment. accurate representation of inter-limb relationships in complex poses but may lack precision for extremity positions and has poor capture capability for non-human characters. Additionally, rendered SMPL images contain the characters shape information. This can cause the model to rely on the shape cues embedded within the motion signal, which complicates the learning of identity consistency, especially if the SMPL shape is not accurate. After careful consideration, we adopt the skeleton-based representation for body control, as it better caters to the majority of mainstream use cases. In our implementation, the skeleton for the character in the target frames is extracted using VitPose Xu et al. (2022) to generate pose frames. In our design of Body Adapter, these pose frames are compressed by Wan-VAE to align spatially and temporally with the target latents. We use projection layer to patchify the pose latents and add them to the patchified noise latents. Crucially, the reference latent is not injected with pose information. This design choice serves to temporally differentiate the reference latent from the target latents. Face Control. straightforward approach would be to use facial landmarks as spatially-aligned signal for driving facial animation, similar to body control. However, this method suffers from loss of fine-grained detail during landmark extraction, making it difficult to fully replicate the expressiveness of the character from the reference video. Moreover, as dense signals, facial landmarks demand high precision; otherwise, they can severely compromise identity consistency, especially in cross-identity scenarios involving significant facial shape disparities. In contrast, we avoid manually defined facial signals and instead use the raw facial image directly as the driving input. During training, we leverage the characters skeletal information to locate and crop the facial region from the driving image. Since our training is self-supervised, it is crucial to disentangle identity information from expression information when extracting facial features. This prevents the model from using identity cues to guide generation, which could lead to identity leakage. We employ two primary strategies to address this challenge: 1) We spatially compress the facial image into 1D latent, which reduces the storage of low-level, identity-specific information. 2) During training, we apply suite of data augmentations to the facial image, including scaling, color jittering, and random noise. This introduces deliberate discrepancies between the augmented face and the target face, discouraging the model from overfitting to identity features. Architecturally, in Face Adapter, we adopt an encoder structure identical to that of Wang et al. (2022) to extract features from each face image. We also employ Linear Motion Decomposition to orthogonalize these features, which facilitates better disentanglement of expression information. Input face images are resized to 512 512, and each frame is compressed into latent vector. As shown in Figure 3, we use stack of 1D causal convolutional layers to temporally downsample the face latents, aligning their sequence length with that of the noise latents. The aligned face latents are then injected into dedicated Face Blocks within the Transformer. Feature fusion is achieved via temporally-aligned cross-attention mechanism, where the attention computation is confined to the corresponding set of tokens at each timestep. To reduce the computational load, we opt to inject face information only into specific layers of the DiT network. Empirically, we perform this injection every 5 layers in the 40-layer Wan-14B model, resulting in total of 8 injection layers. Figure 4: Examples of data augmentation using IC-Light."
        },
        {
            "title": "3.4 RELIGHTING LORA",
            "content": "Preserving the characters appearance is crucial feature in character image animation. However, when performing character replacement, challenge arises because the character and the environment originate from different sources. Strictly maintaining appearance consistency can lead to mismatch between the animated characters lighting and color tone and those of the new environment, which compromises the realism of the final result. Therefore, for Replacement Mode, we introduce an auxiliary Relighting LoRA Hu et al. (2022). This module allows for further adjustments to the characters lighting and color tone during replacement, enabling it to adapt to the new environment. The Relighting LoRA is applied exclusively to the self-attention and cross-attention layers within the DiT blocks. To train this LoRA, we construct specific data pairs. For reference image sampled from video clip, we first segment and crop the character from the original image. We then use IC-Light Zhang et al. (2025) to synthesize the character onto new, random background. As illustrated in Figure 4, leveraging IC-Lights capabilities, the characters lighting and color tone are influenced by the new background, creating discrepancy with the original video sequence. This newly synthesized image is then used as the reference, allowing the Relighting LoRA to learn the ability to perform lighting and color adjustments. When augmented with the Relighting LoRA, Wan-Animate can produce better environmental fusion for the replaced character while simultaneously preserving its identity. 3.5 TRAINING The training process of Wan-Animate is divided into the following stages: Body Control Training. We first focus exclusively on training the model for Animation Mode. In this stage, conditioning is limited to the body control signal, with no facial signal injection. The goal is for the model to quickly learn our modified input paradigm (i.e., the specific configurations for the reference image and temporal images) and to master the alignment with the body control signal. Face Control Training. Next, we introduce facial signal injection. Building upon the model from Stage 1, we integrate the Face Adapter and Face Block modules. To accelerate training, we initialize portion of their parameters using the pre-trained encoder weights from Wang et al. (2022). This stage primarily utilizes portrait data, as facial motion is the dominant dynamic in such videos, allowing for focused learning of expression-driven animation. We also use facial landmarks to identify head, eye, and mouth regions, applying higher loss weight to these areas to enhance their fidelity. Joint Control Training. Here, we combine the Face Adapter and Face Block modules from Stage 2 with the main model trained in Stage 1, and perform joint control training on the full dataset. Our experiments show that the standalone face module already possesses strong expression-driving capabilities, enabling the full model to converge rapidly. Joint Mode Training. In this stage, we adapt the training data to include formats for both Animation Mode and Replacement Mode. Given the models established animation capabilities and the compatibility of our input formulation with Wan-I2Vs pre-training, this transition is remarkably smooth. Relighting LoRA Training. Finally, we exclusively train the relighting capability for the Replacement Mode by applying the Relighting LoRA. The detailed methodology for this stage is described in Section 3.4. 7 3."
        },
        {
            "title": "INFERENCE",
            "content": "Pose Retargeting. During inference, the characters in the provided image and reference video often have different identities. Due to disparities in bone proportions and relative size, for Animation Mode, we perform pose retargeting on the skeletons extracted from the reference video. This involves calculating the length ratio of each corresponding limb between the two characters and adjusting the target poses bone lengths to match the character in the source image. Additionally, the pose is translated to align with the characters position in the image. The reference point for this translation is determined by the framing of the shot (e.g., feet for full-body and neck for halfbody or portraits). We will open-source simplified version our retargeting pipeline. Since we use 2D skeleton, the characters posture can affect the accuracy of the calculated bone lengths. To mitigate this, we provide an auxiliary solution. Specifically, we use the image editing model Wu et al. (2025); Labs et al. (2025) to edit the characters in both the reference and driving images into standard T-pose. The scaling factors are then calculated based on the bone lengths from these edited T-pose images. In most scenarios, this approach leads to more accurate retargeting. In Replacement Mode, given that the character may have specific interactions with the environment, we aim to avoid disrupting these relationships. Therefore, we do not recommend using pose retargeting during character replacement. This, however, introduces limitation for certain use cases, such as replacing characters with significant body shape differences, which may result in some deformation. Long Video. For long video generation, we adopt an iterative generation approach. Specifically, for the first segment, we concatenate only the reference latent and the noise latents. After generating the video result for this segment, we select its last few frames to serve as the temporal guidance for the subsequent segment. The generation of all subsequent segments then involves concatenation of the reference latent, the temporal latents, and the new noise latents. Based on practical usage, we typically use one or two latents as temporal guidance, corresponding to 1 or 5 frames, respectively. After the denoising process is complete for each segment, we discard the portions corresponding to the reference latent and the temporal guidance latents. The remaining generated content is then concatenated to form the final long video."
        },
        {
            "title": "IMPLEMENTATION",
            "content": "4.1 DATA CONSTRUCTION We collected large dataset of human-centric videos, covering activities such as speaking, facial expressions, and body movements. We implemented quality measures Wu et al. (2023); Xu et al. (2023); Schuhmann (2022) similar to those required for general video generation. To ensure identity consistency during training, we verified that each video clip features only single, consistent character. We extracted skeleton information for each character, which serves dual purpose: first, as the motion signal annotation, and second, as criterion for filtering videos based on character behavior. For the character replacement task, we use the annotated skeletons to track the character and then extract the corresponding character masks using SAM2 Ravi et al. (2024). Additionally, we used the QwenVL2.5-72B Bai et al. (2025) model to generate textual descriptions for each video to support the post-training requirements of Wan. While Wan-Animate supports degree of textual control, the motion signal is the dominant control factor, making text control non-core feature. In practice, we recommend using default text prompt. 4.2 PARALLEL STRATEGY Our training process involves loading multiple models: DiT, T5 Raffel et al. (2020), VAE, and CLIP Radford et al. (2021). For the memory-intensive models, DiT and T5, we employ Fully Sharded Data Parallelism (FSDP) Zhao et al. (2023) to reduce the per-GPU memory footprint. The remaining models are trained using standard Data Parallelism (DP). For the DiT model specifically, we also utilize Context Parallelism scheme, which combines RingAttention and Ulysses Fang & Zhao (2024) to enable parallel training. This approach further reduces memory consumption and accelerates training speed. For the frame-wise facial feature extraction within the Face Adapter, we parallelize the computation within each Ulysses group by treating the facial frames from single video clip as batch and processing them concurrently."
        },
        {
            "title": "Method",
            "content": "SSIM LPIPS FVD"
        },
        {
            "title": "Method",
            "content": "SSIM LPIPS FVD Moore-AA Champ MicmicMotion Unianimate StableAnimator Wan-Animate 0.761 0.749 0.742 0.787 0.794 0.813 0.288 0.297 0.307 0.271 0.265 0.227 170.07 177.64 184.71 155.03 147.92 118. LivePortrait AniPortrait Emoji X-portrait2 SkyReel-A1 Wan-Animate 0.811 0.791 0.803 0.825 0.821 0.834 0.231 0.252 0.244 0.212 0.231 0.205 118.67 135.08 127.95 98.03 101.45 94.65 Table 1: Quantitative comparisons."
        },
        {
            "title": "4.3 DETAILS",
            "content": "Wan-Animate supports arbitrary output resolutions. In Animation Mode, the output aspect ratio conforms to that of the input character image. In Replacement Mode, it conforms to the reference videos aspect ratio. The final inference resolution is determined based on the total number of video tokens after patchify. For example, we first calculate target token count based on standard resolution like 1280 720. Then, for given aspect ratio, we select the resolution that yields token count closest to this target. Each inference segment consists of 78 frames. One frame is statically reserved for the character image. Of the remaining 77 frames, for any segment other than the first, 1 or 5 frames are used as temporal reference frames, sourced from the end of the preceding segment. To maintain high inference efficiency, classifier-free guidance (CFG) is disabled by default. However, in scenarios where finer control over facial expression is desired, CFG can be optionally enabled for the face conditioning input to adjust the reenactment effect."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 QUANTITATIVE EVALUATION We conducted quantitative comparison with several mainstream open-source character animation frameworks. To facilitate more comprehensive evaluation, we established our own benchmark for quantitative assessment. The test dataset contains videos of human subjects in various scenarios, featuring different character scales and actions. For the evaluation, we adopted self-reconstruction task: the first frame of video is used as the reference image, and the model then reconstructs the video using motion signals from the subsequent frames. We employed several widely-used quantitative metrics, including SSIM Wang et al. (2004), LPIPS Zhang et al. (2018), and FVD Unterthiner et al. (2018). Additionally, we partitioned subset containing only portraits from our test data to conduct separate quantitative comparison against specialized facial animation methods. The comparison results are presented in Table 6. Most of the existing open-source frameworks are built upon earlier UNet-based foundation models, which results in certain shortcomings in generation quality, particularly regarding human realism, local details, and temporal smoothness. While recent opensource works based on DiT have improved the performance baseline, they are often limited in their comprehensiveness (e.g., body-driven models lack effective expression reenactment, expressiondriven models do not include the body, and support for diverse character types and scales is lacking). In comparison, Wan-Animate performs better than these current open-source works, standing as the most comprehensive and highest-performing open-source model to date. 5.2 HUMAN EVALUATION Currently, the solutions that most closely resemble Wan-Animate in terms of both functionality and performance are primarily closed-source: Runways Act-two Runway (2025) and Bytedances DreamActor-M1 Luo et al. (2025). Compared to existing open-source alternatives, these two proprietary solutions represent the state-of-the-art in character animation in the industry. We compare Wan-Animate with these two methods to demonstrate its superiority. Since conventional quantitative reconstruction metrics may not accurately reflect perceptual differences when the results are of high quality, we employ cross-ID animation setup and conduct user study for this comparison. Each data pair in our evaluation set consists of driving video and different character image. After generating the results, we invited 20 participants for subjective evaluation. Specifically, we presented 9 Figure 5: Human evaluation with current SOTA. Figure 6: Qualitative comparison for Animation Mode. two generated videos side-by-side in an anonymous fashion (one from Wan-Animate, one from competing method) and asked participants to choose their preferred result. Their preference was based on comprehensive consideration of video generation quality, overall identity consistency, motion accuracy, and expression accuracy. The results of the user study are shown in Figure 5, which clearly indicates that Wan-Animate achieved superior outcome. We believe that the opensourcing of Wan-Animate will raise the performance baseline for open-source models in this domain, contributing to the application and long-term development of this technology. 10 Figure 7: Qualitative comparison for Replacement Mode. 5.3 QUALITATIVE EVALUATION In this section, we present visual comparison of our results. Animation Mode. We compare Wan-Animate with Animate Anyone, VACE, Runway Act-two, and Dreamactor-M1. As can be seen in Figure 6: due to the limitations of its base model, Animate Anyone exhibits significantly lower generation quality. VACE, being general-purpose controllable video generation model, shows instability in character animation tasks. Runway Act-two struggles significantly with capturing relatively complex motions. DreamActor-M1 tends to have slightly lower quality in local details and overall image fidelity. In comparison, Wan-Animate demonstrates more comprehensive and stable performance overall. Replacement Mode. We compare Wan-Animate with Animate Anyone 2 and VACE. As shown in Figure 7: Animate Anyone 2 also suffers from insufficient generation quality, again likely due to its base model. VACE has issues with identity consistency. Furthermore, its general-purpose nature makes it highly dependent on parameter tuning, resulting in higher barrier to entry. In contrast, Wan-Animate is much more user-friendly and performs better in character replacement. 5.4 ABLATION STUDY Ablation Study on Face Adapter Training. Our training scheme employs progressive pipeline: we first train for body control, then for facial expressions, and finally train them jointly. This process involves specific data usage and training techniques at each stage. This scheme is highly beneficial for the convergence of the face adapter. To validate its effectiveness, we conduct an ablation study. The baseline for comparison involves training the entire control module jointly on all data from the start. The results are shown in Figure 8. We observe that in the baseline experiment, the expression driving is inaccurate, and the model struggles to converge properly. We believe this is because body motion is more complex; learning to align the body first facilitates the subsequent learning of expressions. Furthermore, since the face generally occupies small portion of the frame in typical data, training the expression module on portrait data, where the face is prominent, significantly accelerates its convergence. Effect of Relighting LoRA. In Replacement Mode, we train the Relighting LoRA on specifically constructed data to achieve better integration of the character with the environment in terms of lighting and color tone. We conducted an ablation study to verify its effect. Figure 9 shows comparison of the results with and without the Relighting LoRA. As can be seen, without the LoRA, the characFigure 8: Ablation study on Face Adapter Training. Figure 9: Ablation study of Relighting LoRA. ters lighting and color tone in the generated video maintain strong consistency with the reference image. However, this can appear incongruous when integrated into the new environment. Therefore, the Relighting LoRA adds degree of flexible adaptability on top of the strong consistency requirement of the character animation task. With the Relighting LoRA, the fusion of the character and the environment becomes more realistic and harmonious. Critically, this is achieved without breaking the characters perceptual identity. 5.5 MORE QUALITATIVE RESULTS In Figure 10, we showcase variety of results generated by Wan-Animate, demonstrating its wide range of potential applications. Performance Reenactment: Wan-Animate allows specified person to precisely replicate the performance of character in source video, enabling the recreation of classic performance scenes. Cross-Style Transfer: The model can robustly transfer real persons performance to various types of characters, which is highly beneficial for filmmaking and animation. Complex Motion Synthesis: Wan-Animate can replicate dance routines and other special actions, facilitating content creation for short-form entertainment videos. Dynamic Camera Movement: The model can generate character actions that include camera movements, showing its value in adFigure 10: Qualitative Results for various applications. vertisement production. Character Replacement: Furthermore, Wan-Animates robust character replacement capability facilitates applications such as re-imagining scenes from films and TV series or editing characters in commercial photography and advertising."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces Wan-Animate, state-of-the-art method for character animation and replacement. Wan-Animate supports two core functionalities: Character Animation: Given reference video and character image, it drives the character image with the motion from the video to generate new animation. Character Replacement: Given reference video and character image, it replaces the character in the video with the new one. We design modified input paradigm that unifies these diverse input forms, making the training process more efficient. Wan-Animate achieves precise reenactment of both facial expressions and body motions. For signal injection, we disentangle motion and expression. Motion signals are integrated with the input noise latents via spatially-aligned fusion, while expression signals are injected via attention using implicit features extracted from the facial image. Furthermore, for character replacement, we have designed an auxiliary LoRA module that enables the model to better achieve lighting and color tone consistency between the character and the new environment. The performance of Wan-Animate surpasses that of current open-source and closed-source algorithms. We will open-source Wan-Animate to contribute to the further iteration and application of this technology."
        },
        {
            "title": "7 CONTRIBUTORS",
            "content": "All contributors are listed in alphabetical order by their last names. Gang Cheng, Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, Ju Li, Dechao Meng, Jinwei Qi, Penchong Qiao, Zhen Shen, Yafei Song, Ke Sun, Linrui Tian, Feng Wang, Guangyuan Wang, Qi Wang, Zhongjian Wang, Jiayu Xiao, Sheng Xu, Bang Zhang, Peng Zhang, Xindi Zhang, Zhe Zhang, Jingren Zhou, Lian Zhuo"
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning, 2023. Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pantic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84988507, 2024. Jiarui Fang and Shangchun Zhao. Usp: unified sequence parallelism approach for long context generative ai, 2024. URL https://arxiv.org/abs/2405.07719. Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, and Jianke Zhu. Humandit: Pose-guided diffusion transformer for long-form human motion video generation. arXiv preprint arXiv:2502.04847, 2025. Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81538163, 2024. Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image animation with environment affordance. arXiv preprint arXiv:2502.06145, 2025. 14 Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. Taekyung Ki, Dongchan Min, and Gyeongsu Chae. Float: Generative motion latent flow matching for audio-driven talking portrait. arXiv preprint arXiv:2412.01064, 2024. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/ abs/1312.6114. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: arXiv preprint Flow matching for in-context image generation and editing in latent space. arXiv:2506.15742, 2025. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: In Seminal Graphics Papers: Pushing the Boundaries, skinned multi-person linear model. Volume 2, pp. 851866. 2023. Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactor-m1: Holistic, expressive and robust human image animation with hybrid guidance. arXiv preprint arXiv:2504.01724, 2025. Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pp. 112, 2024. Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Mimo: Controllable character video synthesis with spatial decomposed modeling. arXiv preprint arXiv:2409.16160, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Runway. Creating with act-two, 2025. URL https://help.runwayml.com/hc/en-us/ articles/42311337895827-Creating-with-Act-Two."
        },
        {
            "title": "Christoph",
            "content": "Schuhmann. improved-aesthetic-predictor. https://github.com/ christophschuhmann/improved-aesthetic-predictor, 2022. Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1365313662, 2021. Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. StaIn Proceedings of the bleanimator: High-quality identity-preserving human image animation. Computer Vision and Pattern Recognition Conference, pp. 2109621106, 2025. Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Lizhen Wang, Zhurong Xia, Tianshu Hu, Pengrui Wang, Pengfei Wang, Zerong Zheng, and Ming Zhou. Dreamactor-h1: High-fidelity human-product demonstration video generation via motiondesigned diffusion transformers. arXiv preprint arXiv:2506.10568, 2025a. Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. arXiv preprint arXiv:2406.01188, 2024. Xiang Wang, Shiwei Zhang, Longxiang Tang, Yingya Zhang, Changxin Gao, Yuehuan Wang, and Nong Sang. Unianimate-dit: Human image animation with large-scale video diffusion transformer. arXiv preprint arXiv:2504.11289, 2025b. Yaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Latent image animator: Learning to animate images via latent space navigation. arXiv preprint arXiv:2203.09043, 2022. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In International Conference on Computer Vision (ICCV), 2023. You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, pp. 111, 2024. 16 Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2024a. Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation, 2022. URL https://arxiv.org/abs/2204.12484. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation In Proceedings of the IEEE/CVF Conference on Computer Vision and using diffusion model. Pattern Recognition, pp. 14811490, 2024b. Ziyi Xu, Ziyao Huang, Juan Cao, Yong Zhang, Xiaodong Cun, Qing Shuai, Yuchen Wang, Linchao Bao, Jintao Li, and Fan Tang. Anchorcrafter: Animate cyberanchors saling your products via human-object interacting video generation. arXiv preprint arXiv:2411.17383, 2024c. Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1590915919, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 36573666, 2022. Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, Linjie Luo, Jinli Suo, and Yebin Liu. X-nemo: Expressive neural motion reenactment via disentangled latent attention. arXiv preprint arXiv:2507.23143, 2025. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel, 2023. URL https://arxiv.org/abs/2304. 11277. Jingkai Zhou, Yifan Wu, Shikai Li, Min Wei, Chao Fan, Weihua Chen, Wei Jiang, and Fan Wang. Realisdance-dit: Simple yet strong baseline towards controllable character animation in the wild. arXiv preprint arXiv:2504.14977, 2025. Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pp. 145162. Springer, 2025."
        }
    ],
    "affiliations": [
        "HumanAIGC Team Tongyi Lab, Alibaba"
    ]
}