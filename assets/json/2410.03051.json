{
    "paper_title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark",
    "authors": [
        "Wenhao Chai",
        "Enxin Song",
        "Yilun Du",
        "Chenlin Meng",
        "Vashisht Madhavan",
        "Omer Bar-Tal",
        "Jeng-Neng Hwang",
        "Saining Xie",
        "Christopher D. Manning"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video detailed captioning is a key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AuroraCap, a video captioner based on a large multimodal model. We follow the simplest architecture design without additional parameters for temporal modeling. To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of input visual tokens. Surprisingly, we found that this strategy results in little performance loss. AuroraCap shows superior performance on various video and image captioning benchmarks, for example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include simple descriptions, consisting of a few dozen words, which limits research in this field. Therefore, we develop VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. In addition, we propose a new LLM-assisted metric VDCscore for bettering evaluation, which adopts a divide-and-conquer strategy to transform long caption evaluation into multiple short question-answer pairs. With the help of human Elo ranking, our experiments show that this benchmark better correlates with human judgments of video detailed captioning quality."
        },
        {
            "title": "Start",
            "content": "Aurora Series: AuroraCap AURORACAP: EFFICIENT, PERFORMANT VIDEO DETAILED CAPTIONING AND NEW BENCHMARK Wenhao Chai 1,2 Enxin Song Yilun Du 4 Chenlin Meng 2,3 Vashisht Madhavan 2 Omer Bar-Tal 2 Saining Xie 5 Christopher D. Manning 3 Jeng-Neng Hwang 1 1 University of Washington 4 Harvard University 2 Pika Lab 5 New York University 3 Stanford University 4 2 0 2 4 ] . [ 1 1 5 0 3 0 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Video detailed captioning is key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AURORACAP, video captioner based on large multimodal model. We follow the simplest architecture design without additional parameters for temporal modeling. To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of input visual tokens. Surprisingly, we found that this strategy results in little performance loss. AURORACAP shows superior performance on various video and image captioning benchmarks, for example, obtaining CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include simple descriptions, consisting of few dozen words, which limits research in this field. Therefore, we develop VDC, video detailed captioning benchmark with over one thousand carefully annotated structured captions. In addition, we propose new LLM-assisted metric VDCSCORE for bettering evaluation, which adopts divide-and-conquer strategy to transform long caption evaluation into multiple short question-answer pairs. With the help of human Elo ranking, our experiments show that this benchmark better correlates with human judgments of video detailed captioning quality. Code, docs, weight, benchmark and training data are all avaliable at website."
        },
        {
            "title": "INTRODUCTION",
            "content": "The task of video detailed captioning involves generating comprehensive and coherent textual descriptions of video content, capturing not only the primary actions and objects but also intricate details, contextual nuances, and temporal dynamics. It has emerged as critical area of research in computer vision and natural language processing, with significant implications for the fields of robotics (Yang et al., 2023b; Du et al., 2024), ego-centric perception (Grauman et al., 2022; 2023), embodied agents (Zhao et al., 2023; 2024c;d; Deng et al., 2023; 2024; Zhao et al., 2024b), and video editing (Chai et al., 2023) and generation (Bar-Tal et al., 2024). The challenges of video detailed captioning compared to past problems include the limited detailed caption data for training and evaluation, and also the lack of good evaluation metric. Before the emergence of Large Language Models (LLMs), previous models could only generate very short and rough descriptions of videos (Wang et al., 2022; Xu et al., 2023; Yan et al., 2022; Yang et al., 2023a). Although these models have been trained on web-scale video-text datasets (e.g., HowTo100M (Miech et al., 2019) and VideoCC3M (Nagrani et al., 2022)), their capabilities remain limited due to their smaller scale and the lack of world knowledge possessed by LLMs. Recently, researchers start to build more powerful large multimodal models (LMMs) upon pretrained LLMs (e.g., LLaVA (Liu et al., 2024b), InstructBlip (Dai et al., 2024b), InternVL (Chen et al., 2023b)). These models typically use intermediate components (e.g., Q-Former (Li et al., 2023a) or an MLP) to connect the pre-trained vision transformer (ViT) (Dosovitskiy et al., 2020) and the LLM. Expanding from image-based LMMs to video-based LMMs is natural progression, as videos can be viewed as sequences of frames. While most LMMs start with loading the pre-trained weights from image models and are further fine-tuned with video-text data, we find that LLaVA-like models can be 1 Aurora Series: AuroraCap Table 1: Benchmark comparison for video captioning task. Ave. Length indicates the average number of words per caption. Compared to the existing benchmarks, VDC has the longest captions. Dataset Theme # Video # Clip # Caption # Word # Vocab. Ave. Length MSVD (Chen & Dolan, 2011) MSR-VTT (Xu et al., 2016) ActivityNet (Krishna et al., 2017a) S-MiT (Monfort et al., 2021) M-VAD (Torabi et al., 2015) MPII-MD (Rohrbach et al., 2013) Youcook2 (Zhou et al., 2018) Charades (Sigurdsson et al., 2016) VATEX (Wang et al., 2019) Open Movie Cooking Human 1,970 7,180 20,000 515,912 92 94 2,000 9,848 41, 1,970 10,000 100,000 515,912 48,986 68,337 15,400 10,000 41,300 70,028 200,000 100,000 515,912 55,905 68,375 15,400 27,380 413,000 607,339 1,856,523 1,340,000 5,618,064 519,933 653,467 121,418 607,339 4994,768 13,010 29,316 15,564 50,570 18,269 24,549 2,583 13,000 44,103 8.67 9.28 13.40 10.89 9.30 9.56 7.88 22.18 12.09 VDC (ours) Open 1,027 1,027 1,027 515,441 20, 500.91 easily adapted to video one without any additional parameters but only with high-quality video-text instruction data for fine-tuning. However, naive treatment of videos as series of image frames can result in significant computational overhead and may cause generalization of the length problem (Wang et al., 2024b). To address these concerns, more specifically, to reduce the number of visual tokens, Video-LLaMA (Li et al., 2023f) adapts the video Q-former, MovieChat (Song et al., 2023) uses memory bank, LLaMA-VID (Li et al., 2023g) simply uses global pooling, and FastV (Chen et al., 2024b) drops visual tokens by attention rank within LLM layers. In this paper, we present AURORACAP, adapting simple yet efficient method called Token Merging (Bolya et al., 2022), which is proved to be effective in image and video classification and editing tasks (Li et al., 2023e). To be specific, we gradually combine similar tokens in transformer layer using bipartite soft matching algorithm to reduce the number of visual tokens. Following this pattern, our experiments show that we can use only 10% to 20% visual tokens compared to the original tokens generated by ViT with marginal performance drop in various benchmarks. With this technique, it is easier to support higher-resolution and longer video sequence inputs for training and inference. We present results on several widely used benchmarks, but find that existing video understanding benchmarks are either question-answer-based (Song et al., 2023; Chen & Dolan, 2011; Caba Heilbron et al., 2015; Xu et al., 2016; Xiao et al., 2021; Fu et al., 2024; Wu et al., 2024), which cannot demonstrate detailed descriptive abilities, or they provide descriptions that are too short, with only few words (Xu et al., 2016; Caba Heilbron et al., 2015) as shown in Table 1. Some large-scale datasets focus on specific domains such as ego-centric (Grauman et al., 2023) or contain low-quality videos and annotations (Bain et al., 2021). Therefore, we construct the VDC (Video Detailed Captions) benchmark, which contains over one thousand high-quality video-caption pairs spanning wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. We utilize GPT-4o as our recaption assistant with hierarchical prompt design. To preserve as much information as possible from the videos and maintain temporal consistency, we implement dense-frame extraction strategy. Using the dense frames as input, despite description of the whole video, we also generate high-quality captions from different aspects, including objective facts, backgrounds, camera angles and movements. Manual quality inspection is employed to ensure the quality of the video captions. While existing video-caption datasets (Chen et al., 2024d; Ju et al., 2024; Li et al., 2024b) offer structured captions, VDC is the first benchmark focused on detailed video captioning, providing significantly longer and more detailed captions than others as shown in the Table 1 and Section B. We also introduce novel evaluation metric specifically designed for detailed captioning task. Traditional metrics like METEOR (Banerjee & Lavie, 2005), CIDEr (Vedantam et al., 2015), and BLEU (Papineni et al., 2002), designed for machine translation or short captions, fail to evaluate detailed captions which contain rich semantic information. On the other side, an LLM-based evaluation metric is commonly used in visual question answering benchmarks (Maaz et al., 2023; Wu et al., 2024), especially for those generated by VLMs (Song et al., 2024; 2023). However, we observe that the LLM-based evaluation metric still struggles to differentiate the quality of detailed captions and tends to give lower scores. To address these challenges, we propose VDCSCORE, novel captioning evaluation metric that leverages the reliability of large language models (LLMs) by Aurora Series: AuroraCap evaluating short visual question-answer pairs. We first decompose the ground-truth caption into set of concise question-answer pairs using the LLM, then generate corresponding responses from the predicted caption. Finally, the LLM is used to assess the accuracy of each response to provide an overall score. In particular, our paper makes the following contributions: In Section 2, we illustrate how we can reduce the number of tokens used for image or video input before injecting into LLM with marginal performance drop. Using these insights, we propose AURORACAP, which is shown to be the state-of-the-art video captioning model. In Section 3, we present VDC, the first benchmark for detailed video captioning, featuring over one thousand videos with significantly longer and more detailed captions. We comprehensively evaluate proprietary and open-source models using our proposed VDCSCORE metric."
        },
        {
            "title": "2.1 ARCHITECTURE",
            "content": "Large multimodal model. To effectively leverage the capabilities of both the pre-trained LLM and visual model, which is typically CLIP (Radford et al., 2021) or DINO (Oquab et al., 2023), LLaVA adapt simple multi-layer perceptron (MLP) as projection layer to connect each patch tokens of image features into the word embedding space. The original LLaVA model is trained by two-stage instruction-tuning procedure, which first pretraining projection layer for feature alignment and then finetuning end-to-end while freeze the visual encoder. Recent works like Prismatic VLMs (Karamcheti et al., 2024) and Idefics2 (Laurencon et al., 2024) further explore the design space of LLaVA architecture. We adapt some conclusion from those works for training the model. Token merging. To increase the throughput of existing ViT models, Token Merging (Bolya et al., 2022) is proposed to gradually combines similar tokens in transformer to reduce the number of tokens passing through ViT models. Token Merging has been proven to be effective on image and video classification tasks even without the need for training. Token Merging is applied between the attention and MLP within each transformer block as: 1. Alternatively partition the tokens into two sets and of roughly equal size. 2. For each token in set A, calculate the token similarity with each token in set based on cosine similarity of the Key features in attention block. 3. Use bipartite soft matching and then select the most similar pairs. 4. Merge the tokens using weighted average, record the token size. 5. Concatenate the two sets and back together again. Once the tokens have been merged, they actually carry the features for more than one input patch. Therefore, the merged tokens will have less effect in softmax attention as = softmax (cid:18) QK (cid:19) + log (1) where is the number of patches the token represents after token merging. We conduct frame-wise token merging in AURORACAP, the visualization of token merging can be found in Appendix C. 2.2 TRAINING RECIPE Building upon the exploration in works like Prismatic VLMs (Karamcheti et al., 2024), Idefics2 (Laurencon et al., 2024), and InternVL (Chen et al., 2023b), we further adopt threestage training strategy, which can be noted as Pretraining stage, Vision stage and Language stage. The training data used in each stage are shown in Table E3, Table E4 and Table E5. More training details including hyper-parameters selection and data preprocessing operation can be found in Appendix E. Pretraining stage. Similar to LLaVA, we first align visual features from the vision encoder ViT with the word embedding space of LLMs. To achieve this, we freeze the pretrained ViT and LLM, training solely the vision-language connector. Consistent with LLaVA-1.5 (Li et al., 2024a), we employ two-layer MLP as the projection layer and pretrain on 1.3M image-caption pairs. To optimize performance, we explore various combinations of the pre-trained ViT and LLM in Appendix D. 3 Aurora Series: AuroraCap Table 2: Comparison of AURORACAP with LLM-based SoTA methods on image captioning benchmarks under zero-shot setting. The number in the upper right corner indicates the number of shots. Scores with bold indicate the best performance under zero-shot setting. Model LLaVA-1.5-7B (Liu et al., 2023) LLaVA-1.5-13B LLaVA-1.6-7B (Liu et al., 2024a) LLaVA-1.6-13B MiniCPM-V-3B (Hu et al., 2023) DeCap (Li et al., 2023d) Flamingo-80B (Alayrac et al., 2022) Chameleon-34B (Team, 2024) GPT-4V Gemini-1.5 Pro AURORACAP-7B 74.9 79.4 68.4 66.6 66.8 56.7 67.2 74.72 55.38 82.24 88. Flickr (31,784) B@1 B@4 71.7 73.6 69.6 65.2 68.0 75.6 28.4 30.2 26.6 24.2 25.1 21.2 32.8 26.1 26.6 23.2 22.2 27.2 21.8 26.7 52.8 53.9 50.3 48.8 51.0 55.4 105.5 109.2 88.4 88.1 89.9 42.7 111.4 NoCaps (4,500) B@1 B@4 82.6 84.2 73.8 68.7 79.1 85.6 40.2 42.4 34.8 34.0 33.2 44. 30.3 30.6 25.9 25.4 29.7 29.9 59.4 60.3 54.6 54.9 55.8 60.6 110.3 115.6 99.9 101.8 94.2 91.2 84.3 120.22 78.58 99.82 120.8 COCO-Cap (5,000) B@1 B@4 73.0 74.6 67.7 62.2 69.8 78.0 29.7 31.5 28.4 27.5 23.9 24.7 35.3 29.2 29.4 25.5 24.6 28.3 25.0 28.6 55.5 56.5 52.4 52.1 52.3 57. Vision stage. Unlike LLaVA, we next unfreeze the pretrained ViT while freezing the LLM during vision stage and train with the public data among various computer vision tasks (e.g., captioning, object identification, classification, reasoning, VQA, and etc.) to get better generalization (Huh et al., 2024). The motivation for doing this is that CLIP ViT usually performs poorly in aspects such as Orientation and Direction, Positional and Relational Context, Quantity and Count (Tong et al., 2024b). However, since the most of the collected datasets lack high-quality and detailed corresponding language descriptions, the labels often consist of only few words or short phrase when converted to text. Therefore, unfreezing the language model at this stage is risky, as it may lead to degradation in the performance of the language model. Language stage. Finally, we conduct end-to-end training, which means all the components are trainable, with the most high-quality public data during language stage training. We mix all the data, including images and videos, captions and instructions, into each mini-batch for training. To improve video captioning performance, we duplicate the video captioning datasets twice. We remove all the video training data for training image-based AURORACAP as well for image captioning task. 2.3 EVALUATION In this section we evaluate AURORACAP on various tasks including image captioning, video captioning, and video question answering. Appendix show detailed evaluation settings. Image Captioning. We evaluate AURORACAP using CIDEr (C), BELU-4 (B@4), BELU-1 (B@1), METEOR (M), and ROUGE-L (R) metric on Flickr (Plummer et al., 2015), NoCaps (Agrawal et al., 2019), and COCO-Cap (Lin et al., 2014) benchmarks and compare it with LLM-based state-of-the-art methods as shown in Table 2. We show the performance of image based AURORACAP under zero-shot settings. Notice that these benchmarks all contain short captions consisting of single sentence, so they only partially reflect the models performance. The performance mentioned in the rest of this paper refers to video-based AURORACAP. Video Captioning. Although the current video captioning benchmarks are only contains onesentence captions, to compare with prior work, we similarly evaluate on these benchmarks. We evaluate AURORACAP using CIDer (C), BELU-4 (B@4), BELU-1 (B@1), METEOR (M), and ROUGE-L (R) metric on MSR-VTT (Xu et al., 2016), VATEX (Wang et al., 2019) and compare it with other methods as shown in Table 3. Video Question Answering. We evaluate AURORACAP on MSVD-QA (Xu et al., 2017), ActivityNet-QA (Yu et al., 2019), MSRVTT-QA (Xu et al., 2017), and iVQA (Yang et al., 2021) for video question answering tasks as shown in Table 4. Although AURORACAP is primarily captioning model, it achieves competitive performance in some VQA datasets (ANet, iVQA). For others (MSVD, MSR-VTT) performance is more modest, but still not bad. In some failure cases observed in the model, we found that prompting the model to generate comprehensive caption for the video input can lead to outputs that include the correct answer. This phenomenon may be attributed to disruption in the models instruction-following capabilities during the visual-based training period. We regard this as promising avenue for future research. 4 Aurora Series: AuroraCap Table 3: Comparison of AURORACAP with SoTA methods on existing video captioning benchmarks under zero-shot setting. Model ZeroCap (Tewel et al., 2022) DeCap (Li et al., 2023d) PaLI-3 (Chen et al., 2022) Ma et al. (Ma et al., 2024) LLaVA-7B (Liu et al., 2024b) Video-LLaMA (Zhang et al., 2023a) AURORACAP-7B MSR-VTT (1,000) B@1 B@4 58.6 2.9 14.7 3.5 4.9 21. 16.3 20.4 17.3 16.8 23.9 9.6 18.6 21.3 22.1 16.9 2.3 33.1 35.4 28.7 49.5 18.7 23.9 3.8 33.8 VATEX (1,000) B@1 B@4 57.1 13.1 2.8 4.3 18.4 15.3 14.1 16.3 19. 23.5 21.8 40.8 Table 4: Comparison of AURORACAP with SoTA methods on video question answering and classification benchmarks under zero-shot setting. The pretrained llm size is 7B for all the listed models. Model Just Ask (Yang et al., 2021) FrozenBiLM (Yang et al., 2022) Video-LLaMA (Zhang et al., 2023a) VideoChat (Li et al., 2023b) Video-ChatGPT (Maaz et al., 2023) LLaMA-VID (Li et al., 2023g) Video-LLaVA (Lin et al., 2023a) FreeVA (Wu, 2024) LLaVA-NeXT-Video (Liu et al., 2024a) MovieChat (Song et al., 2023) MovieChat+ (Song et al., 2024) AURORACAP-7B ANet MSVD Acc 24.7 12.4 26.5 35.2 47.4 45.3 51.2 53.5 45.7 48.1 61. Score 1.1 2.2 2.7 3.3 3.3 3.5 3.2 3.4 3.4 3.8 Acc 32.2 51.6 56.3 64.9 69.7 70.7 73.8 75.2 76.5 62. Score 2.5 2.8 3.3 3.7 3.9 4.1 3.8 3.9 3.6 MSR-VTT Acc Score iVQA Acc 16.8 29.6 45.0 49.3 57.7 59.2 60.0 52.7 53.9 43.5 1.8 2.5 2.8 3.2 3.5 3.5 2.6 2.7 2.9 12.2 26.8 55."
        },
        {
            "title": "3 VDC: A VIDEO DETAILED CAPTIONING BENCHMARK",
            "content": "3.1 BENCHMARK DATASET CURATION 3.1.1 VIDEO COLLECTION AND PROCESSING To ensure the reliability of the benchmark, it is crucial to maintain high video quality, balanced data distribution, and content complexity. Panda-70M (Chen et al., 2024e) offers high-resolution, opendomain YouTube video dataset with diverse one-minute clips across wildlife, cooking, sports, news, TV shows, gaming, and 3D rendering, ideal for studying complex real-world scenarios. Additionally, large volume of aesthetically appealing videos from user-uploaded platforms like Mixkit (Mixkit, 2024), Pixabay (Pixabay, 2024), and Pexels (Pexels, 2024) provides scenic views and visually pleasing human activities with minimal transitions and simpler events. Ego4D (Grauman et al., 2022) complements the video source by focusing on ego-centric human activities and auto-driving scenarios, ensuring comprehensive coverage of real-world scenes. To mitigate content homogeneity among these candidate videos and maintain diversity in the final dataset, inspired by ShareGPT4Video (Chen et al., 2024d), we building VDC upon those various video sources. Note that the videos used in VDC construction are not included in the training data of AURORACAP. To ensure balanced data distribution, we allocate equal proportions of videos from Panda-70M (Chen et al., 2024e), Ego4D (Grauman et al., 2022), Mixkit (Mixkit, 2024), Pixabay (Pixabay, 2024), and Pexels (Pexels, 2024). We first split the video into clips and apply dense frame extraction. 3.1.2 STRUCTURED DETAILED CAPTIONS CONSTRUCTION PIPELINE We believe that comprehensive detailed video caption benchmark should encompass various aspects, including main objects, camera movements, and background. However, most existing benchmarks (Li et al., 2024b; Chen et al., 2024d) provide only single caption for the entire video with less structured Aurora Series: AuroraCap Table 5: The video source distribution of the proposed VDC benchamrk including diverse settings such as natural landscapes, human activities, and animal activities. Videos from different sources have similar proportion in VDC, reducing the data bias. Video Source # Sample Proportion Duration (sec.) Ave. Length (sec.) Ave. # Keyframe Panda-70M (Chen et al., 2024e) Ego4D (Grauman et al., 2022) Mixkit (Mixkit, 2024) Pixabay (Pixabay, 2024) Pexels (Pexels, 2024) 229 196 197 199 208 22.25% 19.05% 19.14% 19.34% 20.21% Total 1,027 5,714 10,935 3,261 4,748 4, 29,001 24.95 55.79 16.55 23.86 20.88 28.18 7.18 19.46 6.58 8.99 7.99 10.43 Figure 1: Video length statistics. Figure 2: Distribution of structured caption length. details. Therefore, we develop structured detailed captions construction pipeline to generate extra detailed descriptions from various perspectives, significantly extending the length and enhancing the richness compared to previous benchmarks. Following (Ju et al., 2024), the structured captions in VDC encompass not only short and detailed captions but also three additional categories: (1) main object caption, offering comprehensive analysis of the primary subjects actions, attributes, interactions, and movements across frames, including variations in posture, expression, and speed; (2) background caption, providing detailed descriptions of the background, such as objects, location, weather, time, and dynamic elements; and (3) camera caption, which details the camera work, including shot types, angles, movements, transitions, and special effects. To generate detailed, fine-grained, and accurate captions, we leverage GPT-4o to produce video descriptions. We utilize the dense video frames to obtain captions. We observed that generating all captions in single conversation round often introduces hallucinations in the detailed captions. To address this, we design hierarchical prompt strategy to efficiently obtain accurate structured captions and detailed captions in two conversation rounds: (1) structured captions generation and (2) detailed captions integration. In the first round, the prompt briefly introduces the differences between structured captions and uses the dense video frames as input to generate the short caption, main object caption, background caption, camera caption, and the detailed caption. In the second round, the generated captions serve as the reference. The second-round prompt guides GPT-4o to enhance the detailed caption based on the initial captions, ensuring consistency without introducing new entities or relations, and producing vivid, engaging, and informative description. The whole prompt template for the structured detailed captions construction pipeline can be found in Appendix H. Finally, we conduct manual review to correct captions with hallucinations and supplement omitted visual elements. The refined detail structured captions are then used as the ground truth for evaluation. 3.1.3 COMPARISON ON NUMERICAL STATISTICS Based on the hierarchical scheme, VDC can capture rich variety of details of the video and reduce hallucinations. The visual representation in Figure 1 demonstrates the video duration distribution of VDC. Over 87% of the videos exhibit duration ranging from 10K to 12K frames, while 1% of videos extending beyond 60 seconds. Only 13% of videos have duration less than 10 seconds. As illustrated in Table 1, the average length of detailed descriptions in VDC is significantly longer than in previous benchmarks. Figure 2 shows the length distribution of structured captions in VDC, with detailed captions averaging over 500 words. Appendix present more statistics. Aurora Series: AuroraCap Figure 3: Evaluation pipeline with VDCSCORE. Like when humans take reading comprehension tests, we transform the matching between two paragraphs into set of question-answer pairings. We first generate some question-answer pairs based on the ground truth captions, then derive corresponding answers one by one from the generated captions, and finally perform matching. The process is automatically evaluated with the LLM involvement in each step. 3.2 EVALUATION METRIC DESIGN AND LEADERBOARD 3.2.1 VDCSCORE: EVALUATING DETAILED CAPTIONS WITH LLMS Evaluating video captions requires not only assessing the quality of the captions but also flexibly evaluating the alignment between the video and the caption. While metrics such as BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), and ROUGE-L (Lin, 2004) have been employed for caption evaluation, these metrics are predominantly designed for short captions and rely heavily on word-level frequency-based alignment. Given the advanced semantic understanding capabilities of large language models (LLMs), Video-ChatGPT (Li et al., 2023b) proposes using LLM as an evaluation assistant to directly judge the correctness of the whole predicted captions and assign scores. However, as demonstrated in Table 6, our experiments indicate that when dealing with detailed captions, the direct application of LLM struggles to accurately distinguish the correctness of various predicted captions, fails to effectively evaluate the precision of detailed descriptions, and exhibits tendency to assign disproportionately lower scores. Therefore, we introduce VDCSCORE, novel quantitative metric that utilizes LLMs to evaluate the similarity between predicted and ground-truth detailed captions through divide-and-conquer approach. The core idea of VDCSCORE is to decompose long detailed captions into multiple short questionanswering pairs, avergae the evaluation of each pair as the final result. We elaborate the design of VDCSCORE in the following parts: (1) ground-truth question-answer pairs extraction, (2) responsed answers generation and (3) answers matching. As illustrated in Figure 3, we first employ GPT-4o to generate question-answer pairs from the detailed ground-truth caption. To ensure that the generated question-answer pairs capture as much information as possible from the original caption and facilitate accurate evaluation by LLMs, we constrain the number of generated pairs and impose specific guidelines: the questions must be open-ended, and the answers should be concise, and directly relevant to the questions. For fair comparison and to mitigate potential variability arising from generating different question-answer pairs for the same caption, we pre-generate standardized set of question-answer pairs for all captions in VDC, as depicted in Figure 3. The used prompt templates used along with additional examples, are provided in Appendix I. VDCSCORE subsequently analyzes the predicted captions by leveraging ground-truth questionanswer pairs. We prompt GPT-4o to read the detailed predicted captions and generate answers based solely on these captions. To mitigate biases arising from discrepancies in the length between ground-truth and predicted answers, we also impose constraints ensuring that responses are limited to concise sentences or phrases. Consequently, for each pair of ground-truth and predicted captions, we obtain set of <question, correct answer, predicted answer> triplets. Following Video-ChatGPT (Li et al., 2023b), we then ask GPT-4o to output two scores for each triplet: one 7 Aurora Series: AuroraCap Figure 4: Comparison between various models with different number of visual tokens input on VDC. For Gemini-1.5-Pro, we only report the performance. We manage the number of visual tokens by managing token merging for AURORACAP, and manage the number of frames for others. AURORACAP achieves much better VDCSCORE than all other models given certain compression in the number of visual tokens kept and indeed approaches the performance of Gemini-1.5-Pro. Table 6: Comparison of AURORACAP with LLM-based baseline methods on VDCSCORE and other evaluation metrics under zero-shot setting. For each evaluation metric, we report the average value of the five structured captions in VDC. Note that VDD, CIDEr, and BELU are only the average of background and main object caption, since the values of the others are closed to zero. Model VDCSCORE Score Acc VDD Acc Score B@1 B@ Elo Gemini-1.5 Pro (Reid et al., 2024) LLaMA-VID (Li et al., 2023f) Video-ChatGPT-7B (Maaz et al., 2023) MovieChat-7B (Song et al., 2023) VILA-7B (Lin et al., 2023b) Video-LLaVA-7B (Lin et al., 2023a) LLaVA-1.5-7B (Liu et al., 2023) LongVA-7B (Zhang et al., 2024b) LLaVA-1.5-13B (Liu et al., 2023) LLaVA-NeXT-V7B (Zhang et al., 2024d) LLaVA-1.6-7B (Liu et al., 2024a) LLaVA-1.6-13B (Liu et al., 2024a) ShareGPT4Video-8B (Chen et al., 2024d) LLaVA-OV-7B (Li et al., 2024a) InternVL-2-8B (Chen et al., 2023b) AURORACAP-7B 41.73 30.86 31.12 31.92 32.61 32.80 33.98 34.50 34.78 35.46 35.70 35.85 36.17 37.45 37.72 38.21 2.15 1.62 1.62 1.64 1.70 1.72 1.76 1.79 1.80 1.85 1.85 1.85 1.85 1.94 1.96 1. 49.68 4.63 8.57 10.24 16.27 14.14 26.71 32.65 28.26 25.62 40.16 34.55 36.44 41.83 48.99 48.33 3.07 1.63 1.84 1.86 2.02 2.00 2.33 2.69 2.36 2.34 2.69 2.51 1.85 2.70 3.03 2. 5.97 1.48 2.92 5.14 8.20 4.43 6.63 4.83 3.90 2.66 3.09 5.55 1.02 4.09 5.59 9.51 29.72 17.74 17.31 14.33 19.13 17.20 29.80 18.75 20.43 20.18 17.36 29.23 12.61 28.34 15.75 30. 2.63 1.46 2.19 3.17 2.11 2.32 2.54 2.16 2.02 2.33 1.59 2.50 0.79 2.84 2.48 4.06 21.21 20.19 1, 8.07 11.57 13.60 5.62 10.36 22.79 13.43 26.37 28.17 24.23 20.26 8.33 23.98 10.76 17.47 16.96 14.98 16.63 17.53 20.36 14.84 17.87 17.51 17.08 19.96 16.31 19.59 17.63 859 944 890 1,073 1,007 825 969 943 1,022 846 728 1,102 1,155 1,081 19.09 21.58 1, for answer correctness and another for answer quality. The final accuracy and score are calculated by averaging the correctness score and quality score respectively. When using two same captions as input, VDCSCORE returns an accuracy of 100%, demonstrating the feasibility and reliability. 3.2.2 BENCHMARKING VIDEO DETAILED CAPTIONING To our knowledge, no standard evaluation benchmarks have been established for detailed video captioning. To advance this field, we assess several baselines on our proposed VDC. As illustrated in Table 6, we present quantitative comparison between our AURORACAP with existing stateof-the-art LMMs. We compare the VDCSCORE with both rule-based and model-based caption metrics with AURORACAP performing well. BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015), ROUGE-L (Lin, 2004) and METEOR (Banerjee & Lavie, 2005) are included as representative 8 Aurora Series: AuroraCap Table 7: Comparison of AURORACAP with LLM-based baseline methods on VDCSCORE under zeroshot structured captions setting. We consider the VDCSCORE of detailed captions, short captions, background captions, main object captions and camera captions. We also test the vision-blind case suggesting by (Chen et al., 2024c; Tong et al., 2024a). Model Vicuna-v1.5-7B (Chiang et al., 2023) Llama-3.1-8B (Dubey et al., 2024) Camera Acc / Score 21.68 / 1.12 17.83 / 1.00 Short Acc / Score 23.06 / 1.17 17.90 / 1. Background Main Object Acc / Score Acc / Score 22.02 / 1.15 19.52 / 1.10 22.64 / 1.16 19.57 / 1.10 Detailed Acc / Score 23.09 / 1.20 20.10 / 1.22 Gemini-1.5 Pro (Reid et al., 2024) 38.68 / 2.05 35.71 / 1.85 43.84 / 2.23 47.32 / 2.41 43.11 / 2.22 LLaMA-VID (Li et al., 2023f) Video-ChatGPT-7B (Maaz et al., 2023) MovieChat-7B (Song et al., 2023) VILA-7B (Lin et al., 2023b) Video-LLaVA-7B (Lin et al., 2023a) LLaVA-1.5-7B (Liu et al., 2023) LongVA-7B (Zhang et al., 2024b) LLaVA-1.5-13B (Liu et al., 2023) LLaVA-NeXT-V7B (Zhang et al., 2024d) LLaVA-1.6-7B (Liu et al., 2024a) LLaVA-1.6-13B (Liu et al., 2024a) ShareGPT4Video-8B (Chen et al., 2024d) LLaVA-OV-7B (Li et al., 2024a) InternVL-2-8B (Chen et al., 2023b) 39.47 / 2.10 37.46 / 2.00 37.25 / 1.98 34.33 / 1.83 37.48 / 1.97 38.38 / 2.04 35.32 / 1.90 38.97 / 2.07 39.73 / 2.10 36.50 / 1.93 35.61 / 1.86 33.28 / 1.76 37.82 / 2.02 39.08 / 2.11 29.92 / 1.56 29.36 / 1.56 32.55 / 1.59 30.40 / 1.55 30.67 / 1.63 28.61 / 1.51 31.94 / 1.63 30.89 / 1.60 30.63 / 1.60 31.91 / 1.65 31.90 / 1.66 39.08 / 1.94 32.58 / 1.70 33.02 / 1.74 28.01 / 1.45 33.68 / 1.70 28.99 / 1.54 35.15 / 1.80 32.50 / 1.70 34.86 / 1.79 36.39 / 1.85 34.79 / 1.78 36.54 / 1.88 37.58 / 1.92 38.90 / 1.99 35.77 / 1.81 37.43 / 1.92 37.47 / 1.89 31.24 / 1.59 30.47 / 1.60 31.97 / 1.64 33.38 / 1.72 36.01 / 1.85 34.62 / 1.76 40.95 / 2.11 36.27 / 1.84 36.54 / 1.88 36.03 / 1.85 36.65 / 1.87 37.12 / 1.89 38.21 / 1.96 44.16 / 2.22 25.67 / 1.38 24.61 / 1.26 28.82 / 1.46 29.78 / 1.58 27.36 / 1.43 33.43 / 1.73 27.91 / 1.48 33.00 / 1.74 33.84 / 1.77 36.47 / 1.89 36.18 / 1.89 35.62 / 1.84 41.20 / 2.13 34.89 / 1.82 AURORACAP-7B 43.50 / 2.27 32.07 / 1.68 35.92 / 1.84 39.02 / 1.97 41.30 / 2.15 Table 8: Comparison of AURORACAP with LLM-based baseline methods on VDD (Liu et al., 2024a) under zero-shot structured captions setting. We consider the VDD (Liu et al., 2024a) of detailed captions, short captions, background captions, main object captions and camera captions. Model Camera Acc / Score Short Acc / Score Background Main Object Acc / Score Acc / Score Detailed Acc / Score Gemini-1.5 Pro (Reid et al., 2024) 18.89 / 2.115 16.91 / 1.572 57.73 / 3.263 41.64 / 2.886 2.581 / 0.330 LLaMA-VID (Li et al., 2023f) Video-ChatGPT-7B (Maaz et al., 2023) VILA-7B (Lin et al., 2023b) Video-LLaVA-7B (Lin et al., 2023a) LLaVA-1.5-7B (Liu et al., 2023) LongVA-7B (Zhang et al., 2024b) LLaVA-1.5-13B (Liu et al., 2023) LLaVA-NeXT-V7B (Zhang et al., 2024d) LLaVA-1.6-7B (Liu et al., 2024a) LLaVA-1.6-13B (Liu et al., 2024a) ShareGPT4Video-8B (Chen et al., 2024d) LLaVA-OV-7B (Li et al., 2024a) InternVL-2-8B (Chen et al., 2023b) 28.28 / 2.513 16.00 / 2.175 4.005 / 1.751 20.00 / 2.336 26.88 / 2.515 17.00 / 2.204 32.65 / 2.662 29.81 / 2.645 21.11 / 2.229 21.56 / 2.199 33.28 / 1.768 17.11 / 2.086 29.00 / 2.545 1.034 / 1.042 4.173 / 1.032 2.087 / 0.233 3.193 / 1.064 1.222 / 0.793 1.016 / 0.794 2.836 / 0.922 1.913 / 0.957 8.696 / 1.146 9.798 / 1.206 4.908 / 0.986 11.15 / 1.277 4.041 / 1.079 6.198 / 1.895 14.14 / 2.273 22.45 / 2.385 17.17 / 2.253 36.50 / 2.725 50.00 / 3.203 37.55 / 2.749 33.23 / 2.692 53.31 / 3.105 39.37 / 2.692 35.77 / 1.813 55.82 / 3.149 63.64 / 3.446 3.063 / 1.366 3.001 / 1.423 10.10 / 1.672 11.11 / 1.765 16.92 / 1.937 15.31 / 2.196 18.98 / 1.978 18.02 / 1.999 27.01 / 2.286 29.73 / 2.329 37.12 / 1.899 27.84 / 2.258 34.34 / 2.627 2.046 / 0.304 1.038 / 0.192 1.015 / 0.262 3.130 / 0.316 0.694 / 0.276 0.002 / 0.247 0.688 / 0.275 0.887 / 0.279 1.282 / 0.279 1.287 / 0.271 3.213 / 0.752 1.372 / 0.249 3.032 / 0.394 AURORACAP-7B 49.40 / 3.141 3.313 / 0.886 59.52 / 3.261 37.14 / 2.533 1.275 / 0.295 rule-based metrics. For model-based metrics, we also consider Video Detailed Description (Li et al., 2023b) (VDD), which employs ChatGPT as an evaluation assistant to compare full captions. Table 7 presents VDCSCORE performance across various sections of structured captions within VDC. Following (Chen et al., 2024c), we also incorporate vision-blind baselines. Furthermore, Figure 4 illustrates schematic diagram of the performance and efficiency of different video training models. Since the comparison of model inference time under different architectures, models, deployment frameworks, and output lengths is unfair, so we used the number of visual tokens as representation of efficiency. AURORACAP achieves superior performance in video detailed captioning while utilizing significantly fewer visual tokens than other models, fully highlighting the efficiency of AURORACAP. We also show additional experimental results with VDD metric as shown in Table 8. We also perform human study using Elo ranking to supplement our evaluation and provide more intuitive assessment of AURORACAPs performance. As depicted in Figure 5, VDCSCORE shows the better correlation with human evaluation results than VDD and ROUGE metric. Ablation study on token merging ratio. As core strategy of AURORACAP, token merging plays significant role in reducing the number of visual tokens. We further study how the video detailed 9 Aurora Series: AuroraCap Figure 5: Pearson correlation analysis among three evaluation metrics VDCSCORE, ROUGEL (Lin, 2004), VDD (Liu et al., 2024a), and human Elo rankings for video models. VDD reflects the score for detailed captions. VDCSCORE demonstrates the highest consistency with expert judgments, thereby reinforcing the reliability. The detailed settings are provided in the Appendix L. Figure 6: Ablation study of token merging on VDC. We found that token merging significantly reduces the number of tokens while maintaining minimal performance drop, and even showing improvement in some aspects. We highlight the token merging ratio when achieving 90% and 80% performance with the dash line and filled area. captioning capability is influenced by token merge ratio. We define the performance percentage as the proportion between the highest and lowest values on the entire performance curve. As shown in Figure 6, most models maintain satisfactory performance (> 80%) even with only 0.2 of visual token kept ratio. Since AURORACAP focuses on spatial visual token merging, the temporal features introduce additional complexity to explore the token merging laws, resulting in the optimal performance may occurs at middle level of visual token kept ratio."
        },
        {
            "title": "4 CONCLUSION",
            "content": "In this paper, we first introduce AURORACAP, efficient video detailed captioner based on large multimodal model. By leveraging the token merging strategy, we significantly reduce the computational overhead without compromising performance. We also present VDC, novel video detailed captioning benchmark designed to evaluate comprehensive and coherent textual descriptions of video content. For better evaluating, We propose VDCSCORE , new LLM-assisted metric with divideand-conquer strategy. Our extensive evaluation on various video and image captioning benchmarks demonstrated that AURORACAP achieves competitive results, even outperforming state-of-the-art models in some tasks. We also conduct thorough ablation studies to validate the effectiveness of token merging and other aspects of our model. We found that the current model performs poorly in terms of the trade-off between performance and the scale of input tokens. Additionally, there is still room for improvement in camera handling and detailed captioning. We hope that VDC can bring new insights to the video detailed captioning task. 10 Aurora Series: AuroraCap"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We acknowledge Pika Lab for the computing resources support. We are also thankful to Xtuner 1, lmms-eval 2, and SGLang 3 for their well-developed and user-friendly codebase. We would like to thank Enxin Song for her contribution to the benchmark processing. We would like to thank Professor Yilun Du, Professor Saining Xie, and Professor Christopher D. Manning for their valuable discussions and revisions of this paper."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This research on video captioning utilizes publicly available datasets, ensuring that all data complies with privacy regulations. We acknowledge the potential biases that can arise in automatic caption generation, particularly concerning gender, race, or other characteristics. We have taken measures to evaluate and minimize such biases, while remaining committed to further improvements. Additionally, we recognize the potential risks of misuse, such as generating misleading captions, and have checked the training dataset with safeguards against such applications."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have made several efforts to ensure the reproducibility of our work. All the key implementation details, including the architecture of our model, the training procedures, and hyperparameter settings, are described in supplementary meterial Section E. The introduction of the used evaluation benchmarks and settings are in Section F. The introduction of the used evaluation benchmarks and settings are in Section F. Prompt template of VDC generation is in Section H. Question-answer pairs generation prompt template of VDCSCORE is in Section I. And calculation details of Elo ranking is in Section L. We have also outlined any hardware configurations and computation requirement used for our experiments in Figure D6 to further support reproducibility. 1GitHub Repository: https://github.com/InternLM/xtuner 2GitHub Repository: https://github.com/EvolvingLMMs-Lab/lmms-eval 3GitHub Repository: https://github.com/sgl-project/sglang 11 Aurora Series: AuroraCap"
        },
        {
            "title": "REFERENCES",
            "content": "Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 89488957, 2019. 4, 30, 33 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 4 Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part 14, pp. 382398. Springer, 2016. 23 Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3948, 2016. 32 Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 17281738, 2021. 2 Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pp. 6572, 2005. 2, Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 1 Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 23 Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 42914301, 2019. 32 Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In The Eleventh International Conference on Learning Representations, 2022. 2, 3 Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In Proceedings of the ieee conference on computer vision and pattern recognition, pp. 961970, 2015. 2, 23 Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee. Matryoshka multimodal models. arXiv preprint arXiv:2405.17430, 2024. 23 Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2304023050, 2023. 1 David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pp. 190200, 2011. 2, 23 Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024a. 32 12 Aurora Series: AuroraCap Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. arXiv preprint arXiv:2403.06764, 2024b. 2 Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023a. 32 Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024c. 9 Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024d. 2, 5, 8, 9, 28, 32 Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1332013331, 2024e. 5, 6 Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. Xin Chen, Yu Zhu, Hua Zhou, Liang Diao, and Dongyan Wang. Chinesefoodnet: large-scale image dataset for chinese food recognition. arXiv preprint arXiv:1705.02743, 2017. 32 Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023b. 1, 3, 8, 9 Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 23 Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. 9 Dawei Dai, YuTang Li, YingGe Liu, Mingming Jia, Zhang YuanHui, and Guoyin Wang. 15m multimodal facial image-text dataset, 2024a. URL https://arxiv.org/abs/2407.08515. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Instructblip: Towards general-purpose visionBoyang Li, Pascale Fung, and Steven Hoi. language models with instruction tuning. Advances in Neural Information Processing Systems, 36, 2024b. 1 Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 326335, 2017. 32 Jie Deng, Wenhao Chai, Jianshu Guo, Qixuan Huang, Wenhao Hu, Jenq-Neng Hwang, and Gaoang Wang. Citygen: Infinite and controllable 3d city layout generation. arXiv preprint arXiv:2312.01508, 2023. 1 Jie Deng, Wenhao Chai, Junsheng Huang, Zhonghan Zhao, Qixuan Huang, Mingyan Gao, Jianshu Guo, Shengyu Hao, Wenhao Hu, Jenq-Neng Hwang, et al. Citycraft: real crafter for 3d city generation. arXiv preprint arXiv:2406.04983, 2024. 1 13 Aurora Series: AuroraCap Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. 1 Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36, 2024. 1 Zhengfang Duanmu, Wentao Liu, Zhongling Wang, and Zhou Wang. Quantifying visual image quality: bayesian view. Annual Review of Vision Science, 7:437464, 2021. 32 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 9 Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. Multi30k: Multilingual englishgerman image descriptions. arXiv preprint arXiv:1605.00459, 2016. 32 Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2 Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to machine? dataset and methods for multilingual image question. Advances in neural information processing systems, 28, 2015. 32 Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Imageinwords: Onoe, Andrew Bunner, Ranjay Krishna, Jason Baldridge, and Radu Soricut. Unlocking hyper-detailed image descriptions. arXiv preprint arXiv:2405.02793, 2024. 24 Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. 30, 32, Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1899519012, 2022. 1, 5, 6, 24 Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives. arXiv preprint arXiv:2311.18259, 2023. 1, 2 Mingfei Han, Xiaojun Chang, Heng Wang, and Linjie Yang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. arXiv preprint arXiv:2312.10300, 2023. 24 Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan Liu, and Maosong Sun. Large multilingual models pivot zero-shot multimodal learning across languages. arXiv preprint arXiv:2308.12038, 2023. 4 Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies, pp. 12331239, 2016. 32 Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. 27, 30, Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. arXiv preprint arXiv:2405.07987, 2024. 4 14 Aurora Series: AuroraCap Vladimir Iashin and Esa Rahtu. better use of audio-visual cues: Dense video captioning with bi-modal transformer. arXiv preprint arXiv:2005.08271, 2020. Jitesh Jain, Jianwei Yang, and Humphrey Shi. Vcoder: Versatile vision encoders for multimodal large language models. arXiv preprint arXiv:2312.14233, 2023. 32 Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1370013710, 2024. 23 Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 29012910, 2017. 32 Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. arXiv preprint arXiv:2407.06358, 2024. 2, 6, 32, 36 Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. arXiv preprint arXiv:2402.07865, 2024. Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz. e-vil: dataset and benchmark for natural language explanations in vision-language tasks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 12441254, 2021. 32 Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. 26 Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. hierarchical approach for generating descriptive image paragraphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 317325, 2017. 32 Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pp. 706715, 2017a. 2 Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017b. 26 Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 3 Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herve Le Borgne, Romaric Besancon, Jose Moreno, and Jesus Lovon Melgarejo. Viquae, dataset for knowledge-based visual question answering about named entities. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 31083120, 2022. 32 Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. 3, 8, 9, 32 Boyi Li, Ligeng Zhu, Ran Tian, Shuhan Tan, Yuxiao Chen, Yao Lu, Yin Cui, Sushant Veer, Max Ehrlich, Jonah Philion, et al. Wolf: Captioning everything with world summarization framework. arXiv preprint arXiv:2407.18908, 2024b. 2, 5 15 Aurora Series: AuroraCap Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023a. 1 Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, and Chuang Gan. Flexattention for efficient high-resolution vision-language models. arXiv preprint arXiv:2407.20228, 2024c. 23 KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023b. 5, 7, 9, 23 Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023c. 32 Wei Li, Linchao Zhu, Longyin Wen, and Yi Yang. Decap: Decoding clip latents for zero-shot captioning via text-only training. arXiv preprint arXiv:2303.03032, 2023d. 4, Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Ling-Yu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. arXiv preprint arXiv:2407.08303, 2024d. 32 Xirong Li, Weiyu Lan, Jianfeng Dong, and Hailong Liu. Adding chinese captions to images. In Proceedings of the 2016 ACM on international conference on multimedia retrieval, pp. 271275, 2016. 32 Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. Cococn for cross-lingual image tagging, captioning, and retrieval. IEEE Transactions on Multimedia, 21(9):23472360, 2019. 32 Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging for zero-shot video editing. arXiv preprint arXiv:2312.10656, 2023e. 2 Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023f. 2, 8, 9, 23 Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023g. 2, 5 Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023h. 33 Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a. 5, 8, 9, 23 Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. 7, 8, 10, 23 Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023b. 8, 9, 23 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. 4, 26, 30, 32, 33 Feng Liu, Tao Xiang, Timothy Hospedales, Wankou Yang, and Changyin Sun. ivqa: Inverse visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 86118619, 2018. 31, 34 Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 4, 8, 9, 16 Aurora Series: AuroraCap Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. 4, 5, 8, 9, 10, 23, 32 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. 1, 5, 23, 32 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 33 Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 32 Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-llama: Reliable video narrator via equal distance to visual tokens. arXiv preprint arXiv:2312.08870, 2023. 23 Yunchuan Ma, Laiyun Qing, Guorong Li, Yuankai Qi, Quan Sheng, and Qingming Huang. Retrieval enhanced zero-shot video captioning. arXiv preprint arXiv:2405.07046, 2024. Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 2, 5, 8, 9, 23 Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pp. 31953204, 2019. 32 Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 33 Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 22002209, 2021. 32, 33 Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 26302640, 2019. 1 Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pp. 947952. IEEE, 2019. 26, Mixkit. Free stock video clips, motion graphics, & music tracks, 2024. URL https://mixkit. co/. Accessed: 2024-08-24. 5, 6 Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass, and Aude Oliva. Spoken moments: Learning joint audio-visual representations from video descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1487114881, 2021. 2 Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid. Learning audio-video modalities from image captions. In European Conference on Computer Vision, pp. 407426. Springer, 2022. 1 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2023. 3 Aurora Series: AuroraCap Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. 2, 7, 8, 23 Pexels. Free imageds and video clips, 2024. URL https://www.pexels.com/zh-cn/. Accessed: 2024-08-24. 5, 6 Pixabay. Stunning free images & royalty-free stock, 2024. URL https://pixabay.com/. Accessed: 2024-08-24. 5, Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pp. 26412649, 2015. 4, 33 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. 3 Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, and Tom Goldstein. Cinepile: long video question answering dataset and benchmark. In Synthetic Data for Computer Vision Workshop@ CVPR 2024, 2024. 23 Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 8, 9 Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, and Lu Hou. Testa: Temporal-spatial token aggregation for long-form video-language understanding. arXiv preprint arXiv:2310.19060, 2023. 23 Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele. Translating video content to natural language descriptions. In Proceedings of the IEEE international conference on computer vision, pp. 433440, 2013. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 32 Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pp. 146162. Springer, 2022. 32 Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 23 Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 742758. Springer, 2020. 32, 33 Gunnar Sigurdsson, Gul Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pp. 510526. Springer, 2016. 2 Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. 18 Aurora Series: AuroraCap Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 2, 5, 8, 9, 23 Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang. Moviechat+: arXiv preprint long video question answering. Question-aware sparse memory for arXiv:2404.17176, 2024. 2, 5, Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 217223, 2017. 32 Reuben Tan, Ximeng Sun, Ping Hu, Jui-hsien Wang, Hanieh Deilamsalehy, Bryan Plummer, Bryan Russell, and Kate Saenko. Koala: Key frame-conditioned long video-llm. arXiv preprint arXiv:2404.04346, 2024. 23 Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1387813888, 2021. 32 Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 4 Qwen team. Qwen2-vl. 2024. Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1791817928, 2022. 5 Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 52385248, 2022. 32 Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi Tang, Yuan Zhang, Jianbin Jiao, Qi Tian, and Qixiang Ye. Chatterbox: Multi-round multimodal referring and grounding. arXiv preprint arXiv:2401.13307, 2024. 32 Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024a. 9, 32 Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024b. 4 Atousa Torabi, Christopher Pal, Hugo Larochelle, and Aaron Courville. Using descriptive video services to create large data source for video annotation research. arXiv preprint arXiv:1503.01070, 2015. Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575, 2015. 2, 7, 8, 23, 27 Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. 27, 32 Han Wang, Yanjie Wang, Yongjie Ye, Yuxiang Nie, and Can Huang. Elysium: Exploring object-level perception in videos via mllm. arXiv preprint arXiv:2403.16558, 2024a. 23 Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022. 1 Aurora Series: AuroraCap Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui, Qi Zhang, Xuanjing Huang, and Xiaoling Wang. Length generalization of causal transformers without position encoding. arXiv preprint arXiv:2404.12224, 2024b. 2 Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023. 32 Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 45814591, 2019. 2, 4, 24, 27, 31, 33, 34, 39 Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv:2407.15754, 2024. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. arXiv preprint arXiv:2312.14135, 2023. 32 Wenhao Wu. Freeva: Offline mllm as training-free video assistant. arXiv e-prints, pp. arXiv2405, 2024. 5, 23 Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of questionanswering to explaining temporal actions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 97779786, 2021. 2 Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pp. 16451653, 2017. 4, 31, 33, Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al. mplug-2: modularized multi-modal foundation model across text, image and video. In International Conference on Machine Learning, pp. 3872838748. PMLR, 2023. 1 Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 52885296, 2016. 2, 4, 24, 27, 31, 33, 34, 39 Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024a. 23 Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841, 2024b. 27, 28 Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 50365045, 2022. 24 Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu. Videococa: Video-text modeling with zero-shot transfer from contrastive captioners. arXiv preprint arXiv:2212.04979, 2022. Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 16861697, 2021. 4, 5, 33 Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems, 35:124141, 2022. 5 20 Aurora Series: AuroraCap Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1071410726, 2023a. 1, 23 Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023b. Barry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. End-to-end multimodal fact-checking and explanation generation: challenging dataset and models. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 27332743, 2023. 32 Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 30 Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 91279134, 2019. 4, 27, 31, 33, 34 Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. 30, 33 Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67206731, 2019. 32 Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023a. 5, 23 Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024a. 23 Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024b. URL https://arxiv.org/abs/2406.16852. 8, 9 Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023b. 23 Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024c. 23, 32 Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2019. Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024d. URL https://llava-vl.github.io/blog/2024-04-30-llava-next-video/. 8, 9 Yue Zhao, Long Zhao, Xingyi Zhou, Jialin Wu, Chun-Te Chu, Hui Miao, Florian Schroff, Hartwig Adam, Ting Liu, Boqing Gong, et al. Distilling vision-language models on millions of videos. arXiv preprint arXiv:2401.06129, 2024a. 23 21 Aurora Series: AuroraCap Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, JenqNeng Hwang, and Gaoang Wang. See and think: Embodied agent in virtual environment. arXiv preprint arXiv:2311.15209, 2023. 1 Zhonghan Zhao, Wenhao Chai, Xuan Wang, Ke Ma, Kewei Chen, Dongxu Guo, Tian Ye, Yanting Zhang, Hongwei Wang, and Gaoang Wang. Steve series: Step-by-step construction of agent systems in minecraft. Computer Vision and Pattern Recognition Workshop, 2024b. Zhonghan Zhao, Kewei Chen, Dongxu Guo, Wenhao Chai, Tian Ye, Yanting Zhang, and Gaoang Wang. Hierarchical auto-organizing system for open-ended multi-agent navigation. arXiv preprint arXiv:2403.08282, 2024c. 1 Zhonghan Zhao, Ke Ma, Wenhao Chai, Xuan Wang, Kewei Chen, Dongxu Guo, Yanting Zhang, Hongwei Wang, and Gaoang Wang. Do we really need complex agent system? distill embodied agent into single model. arXiv preprint arXiv:2404.04619, 2024d. 1 Yinhe Zheng, Guanyi Chen, Xin Liu, and Jian Sun. Mmchat: Multi-modal chat dataset on social media. arXiv preprint arXiv:2108.07154, 2021. 32 Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. 2, 23 Xingyi Zhou, Anurag Arnab, Shyamal Buch, Shen Yan, Austin Myers, Xuehan Xiong, Arsha Nagrani, and Cordelia Schmid. Streaming dense video captioning. arXiv preprint arXiv:2404.01297, 2024. 22 Aurora Series: AuroraCap"
        },
        {
            "title": "Supplementary Material",
            "content": "The supplementary material is structured as follows: Iiterature review about the related works in Section A. Benchmark comparison between VDC and other video captioning benchmarks in Section B. The calculation and the visualization examples of token merging in Section C. More ablation studies for AURORACAP in Section D. The training settings for AURORACAP in Section E. The introduction of the used evaluation benchmarks and settings in Section F. More limitations of AURORACAP at current stage in Section G. Prompt template of VDC generation in Section H. Question-answer pairs generation prompt template of VDCSCORE in Section I. More statistics information of VDC in Section J. More statistics information of VDCSCORE in Section K. Calculation details of Elo ranking in Section L. Case studies among several baselines in Section M."
        },
        {
            "title": "A RELATED WORKS",
            "content": "Video Captioning The general goal of video captioning is understanding video and describing it with natural language. Unlike image captioning, video captioning requires the description also on the temporal dimension (e.g., human action, camera and object movement). The current datasets including videos in the domain of human activities (Caba Heilbron et al., 2015), cooking scene (Zhou et al., 2018; Iashin & Rahtu, 2020), movie (Song et al., 2023), and open domain (Chen & Dolan, 2011). In Table 1, we show the summary of current video captioning benchmarks. Most of them are with short caption, which is not suitable for evaluating video detailed captioning task. There are several widely used metrics to evaluate the correctness of generated caption, such as BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), SPICE (Anderson et al., 2016), and BERTScore (Zhang et al., 2019). Dense Video Captioning (Yang et al., 2023a; Zhou et al., 2024) is also captioning task but further requires localizing the events temporally. In this paper, we focus on the detailed description of short video clip, where there are no scene changes or camera switches. Large Multimodal Models for Video With the develop of LLMs and LMMs (Maaz et al., 2023; Zhang et al., 2023b; Li et al., 2023f; Song et al., 2023; Zhang et al., 2023a; Song et al., 2024; Zhao et al., 2024a; Zhou et al., 2024; team, 2024; Jin et al., 2024; Beyer et al., 2024; Shang et al., 2024; Cai et al., 2024; Cheng et al., 2024; Li et al., 2024c), many recent works have explored adapting them into video understanding field (e.g., Video-LLaMA (Li et al., 2023f), Video-LLaVA (Lin et al., 2023a), VideoChat (Li et al., 2023b), Vista-LLaMA (Ma et al., 2023), LLaVA-Hound (Zhang et al., 2024c), Koala (Tan et al., 2024), Elysium (Wang et al., 2024a), and MovieChat (Song et al., 2023; 2024)). Thanks to this flexible design, the models can combine pretrained knowledge with minimal trainable parameters. Instead of requiring large-scale training, using only small amount of high-quality training data can even achieve better results. Most of the existing models use additional parameters for temporal modeling. There are also some interesting observations that we can actually build advancing LMMs without additional parameters for temporal modeling (Xu et al., 2024a; Lin et al., 2023b; Liu et al., 2024a; Zhang et al., 2024a; Ren et al., 2023) or even without further training with video-text data (Wu, 2024). Recent works (e.g., FreeVA (Wu, 2024), LLaVA-Next (Liu et al., 2024a), VLIA (Lin et al., 2023b), and PLLaVA (Xu et al., 2024a)) also find that the vanilla LLaVA-like model pretrained on high-quality image instruction data can also be strong video understanding model. FreeVA further observe that using existing video instruction tuning data like Video-ChatGPT 100K (Maaz et al., 2023) to tune LMMs may not necessarily lead to improvements. As the concurrent work, we also observe this phenomenon and proceeded to develop video detailed captioning baseline training based on the LLaVA architecture. As for the benchmark , inspired by LLaVA (Liu et al., 2024b), Video-ChatGPT (Maaz et al., 2023) introduces 100K video clips with text instructions with the first vLMMs benchmark evaluation system powered by LLMs. MovieChat-1K (Song et al., 2023) and CinePile (Rawal et al., 2024) 23 Aurora Series: AuroraCap are question-answering based benchmark for long-form video understanding. Shot2Story20K (Han et al., 2023) comprises videos with 2 to 8 shots each sourced for our dataset from the public video benchmark HDvila100M (Xue et al., 2022). However, currently there is no video benchmark available to evaluate video detailed captioning tasks like IIW (Garg et al., 2024) did in image captioning field. In this paper, our work fills this gap."
        },
        {
            "title": "B BENCHMARK COMPARISON",
            "content": "We compare our proposed VDCwith some examples from several video captioning benchmarks as shown in Figure B1, Figure B2, and Figure B3. The corresponding captions are shown as followings: Figure B1: Video example of MSR-VTT (Xu et al., 2016) benchmark. Figure B2: Video example of VATEX (Wang et al., 2019) benchmark. Figure B3: Video example of VDC benchmark sampled from Ego4D (Grauman et al., 2022). Benchmark Caption (for VDC we show only the detailed caption) MSR-VTT B1 (4 words) Teams are playing soccer. VATEX B2 (13 words) woman instructs and demonstrates how to remove the insides of pumpkin. VDC B3 (618 words) The video opens with an intimate close-up of surface adorned with vibrant green moss and intricate lichen, initially evoking the serene beauty of natural landscape. This organic imagery quickly transitions, revealing that the mossy surface is actually part of motorcycle or vehicles engine compartment, creating striking contrast between the lush textures of nature and the cold, hard lines of mechanical components. As the camera angle shifts, the viewer is drawn deeper into the engine compartment, where the interplay of moss overgrowth on various machinery introduces fascinating blend of organic life and industrial elements, highlighting the unexpected coexistence of nature and technology. The perspective then zooms in, accentuating the rich details of the mossy growth, which clings tenaciously to the metallic surfaces, while dark cavity beneath hints at the complexity of the machinery. The reflective metallic surfaces glint in the light, further enhancing the visual contrast and inviting the viewer to explore this unique juxtaposition. Suddenly, the scene shifts dramatically, with rapid camera motion creating vibrant blur of colors and shapes, transforming the previously detailed views into chaotic whirlwind, suggesting swift movement through the intricate landscape of the engine compartment. As the motion blur begins to dissipate, the viewer is presented with clearer image of lightcolored, textured surface, where blurred mechanical components can be discerned, indicating deceleration in movement. The camera stabilizes, revealing rough-textured floor or ground that suggests an indoor or industrial environment, characterized by sense of organized chaos. The scene transitions to detailed examination of cluttered workspace filled with tangled wires, casings, and components in variety of colors, emphasizing the disorganized state of electronic or mechanical internals, possibly during maintenance or repair process. 24 Aurora Series: AuroraCap The perspective shifts once more, showcasing darker, textured surfaces juxtaposed against lighter insulating materials, with hidden metallic elements peeking through, suggesting another angle within this same cluttered interior space. human hand enters the frame, reaching out to interact with the components, signaling an active workspace filled with purpose. As the scene expands, additional hands join the fray, actively manipulating various objects within the crowded environment, signifying an ongoing task or collaborative effort amidst the complex array of components and materials. The atmosphere is imbued with sense of urgency and engagement, as the camera captures the dynamic interactions of the individuals working together. The camera work remains fluid and dynamic, featuring mix of close-up shots that highlight the intricate details of the components and wider angles that provide context to the bustling environment. The slightly shaky nature of the shots adds layer of realism and immersion, drawing the viewer into the heart of the action. The low light conditions create moody ambiance, with shadows dancing across the surfaces, enhancing the visual depth and interest of the scene. Overall, the video encapsulates vivid portrayal of the intersection between nature and machinery, as well as the collaborative spirit of those engaged in the intricate task of maintenance and repair within this unique setting."
        },
        {
            "title": "C TOKEN MERGING VISUALIZATION",
            "content": "Figure C4: Token merging visualization. From top to bottom, the image IDs are COCO:COCOtrain2014-000000247906, VG:2331508, SA-1B:sa-393200, OCR-VQA:71789111. From left to right, the number of tokens representing the images are 490, 154, 18, and 6. The number of tokens merged in each layer of the transformer in ViT is calculated by the following formula: = (1 ratio) (W H/P 2)/L (2) where , is the width and the height of the input image, is the patch size, is the number of layers in the transformer, ratio is the proportion of tokens to be kept. The merging is evenly distributed across all layers of the transformer. Aurora Series: AuroraCap We show token merging visualization examples from COCO (Lin et al., 2014), VG (Krishna et al., 2017b), SA-1B (Kirillov et al., 2023), and OCR-VQA (Mishra et al., 2019) in Figure C4, since we only conduct frame-wise operation. Features extracted by CLIP ViT-H model 4. We conduct image captioning for the examples given final token numbers by 7B AURORACAP as following. We remove line breaks in generated caption for better reading experience. # Token Describe this image in detail. (Figure C4 Row 1) 490 In the heart of winter wonderland, skier takes flight, suspended in mid-air against the backdrop of clear blue sky. The skier, clad in vibrant red and white jacket, is captured in moment of pure exhilaration. The skiers body is angled towards the left side of the image, with their skis crossed in an impressive display of skill and control. The skiers helmet, stark contrast to the surrounding snow, is adorned with pink and white design, adding touch of color to the otherwise monochrome landscape. The skiers goggles, reflecting the bright sunlight, are testament to the intensity of the sport. Beneath the skier, the snow-covered mountain slopes stretch out, dotted with trees that stand as silent spectators to the skiers daring feat. The image is freeze-frame of action and adventure, encapsulating the thrill and beauty of winter sports. 154 In the midst of clear blue sky, skier is captured in moment of thrilling action. The skier, clad in vibrant red and white jacket, is suspended in mid-air, performing an impressive trick. The skiers body is parallel to the ground, creating sense of motion and excitement. The skiers skis, striking combination of red and white, are crossed in front of them, adding to the dynamic nature of the image. The skiers helmet, necessary safety measure, is white and contrasts with the colorful jacket. The backdrop to this action-packed scene is breathtaking snowy mountain landscape. The mountains peak is visible in the distance, adding depth to the image. The overall composition of the image suggests high-speed descent down the mountain, with the skier skillfully navigating the terrain. This image encapsulates the exhilarating sport of skiing, showcasing not only the skill of the skier but also the stunning beauty of the mountain landscape. 18 In the heart of snowy mountain, daring skier takes flight. Dressed in vibrant red and white jacket, black pants, and protective white helmet, the skier is captured mid-air, performing an impressive trick. The skiers body is parallel to the ground, arms outstretched, as if embracing the thrill of the moment. The skiers skis, striking combination of red and white, are pointed upwards towards the clear blue sky. The skiers poles, held firmly in their hands, add to the dynamic nature of the scene. The backdrop to this exhilarating moment is breathtaking snowy mountain landscape. The mountains peak, blanketed in snow, stretches out into the distance. The clear blue sky above is devoid of clouds, adding to the serene beauty of the scene. This image captures not just moment, but story of adventure, thrill, and the sheer joy of skiing. 6 The image captures thrilling moment of snowboarder in mid-air, performing an impressive trick. The snowboarder, clad in vibrant red and white jacket, black pants, and protective helmet, is the central figure in this dynamic scene. The snowboarder is holding onto rope with one hand, suggesting that they are being pulled up the mountain by snowmobile, common practice in snowboarding to gain speed and momentum. The rope extends from the top left corner of the image, creating sense of movement and direction. The backdrop of the image is breathtaking snowy mountain landscape, complete with trees and other natural elements. The snowboarder is positioned in the center of the image, drawing the viewers attention to their daring stunt. The image is testament to the exhilarating sport of snowboarding and the skill and courage of the athletes who participate in it. Its freeze-frame of an adrenaline-fueled moment, encapsulating the thrill and excitement of snowboarding. # Token What is the number in the picture? (Figure C4 Row 2) 490 The number in the picture is 45. It is displayed on the back of the jersey that the person is wearing. 154 The number in the picture is 45. 18 The number in the picture is 45. It is displayed on the back of the baseball jersey worn by the player in the image. 6 The number in the picture is 45. It is displayed in large, bold font on the jersey of the baseball player in the image. # Token What is the relative position of the white and red cars in the picture? Which one in the left? (Figure C4 Row 3) 490 The white car is positioned to the left of the red car in the image. The white car is closer to the foreground and is stopped at the traffic light, while the red car is further back, also stopped at the same traffic light. The relative position of the cars is such that the white car is in the left lane and the red car is in the right lane, both facing the same direction of traffic. 4Huggingface Model: https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-378 26 Aurora Series: AuroraCap 154 The white car is positioned to the left of the red car in the image. The red car is further back on the road, indicating it is behind the white car. This is determined by the perspective of the image, where the white car is closer to the foreground and the red car is further away, both in the same lane and moving in the same direction. 18 The white car is on the left side of the image, and the red car is on the right side. The white car is closer to the foreground, while the red car is further away, positioned behind the white car. 6 The white car is on the left side of the image, while the red car is on the right side. The white car is positioned closer to the foreground, while the red car is further back, near the intersection."
        },
        {
            "title": "D ABLATION STUDIES",
            "content": "In this section, we conduct the ablation studies in several aspects including token merging, slowfast inference, efficiency, training strategy, and backbone selection. Token merging. As core strategy of AURORACAP, token merging plays significant role in reducing the number of visual tokens. We conduct extensive ablation studies to explore the impact of the token kept ratio Rvtk in terms of performance across multiple tasks including image captioning, visual question answering, video captioning, and video question answering as shown in Figure D7 and Figure D8. We define the performance percentage as the proportion between the highest and lowest values on the entire performance curve. We identify the minimum retention thresholds for achieving 90% and 80% performance. As shown in Figure D7, while the performance of AURORACAP generally declines with fewer visual tokens across most benchmarks, it remains relatively stable at higher retention levels. Most models maintain satisfactory performance (> 80%) even with only 0.4 of Rvtk, highlighting the efficiency of our approach. Visual token retention thresholds vary by task complexity, with more visually demanding tasks needing higher retention of visual tokens. For instance, CIDEr (Vedantam et al., 2015) on COCO-Cap (Veit et al., 2016) maintains over 90% performance with an Rvtk of 0.3, whereas accuracy on GQA (Hudson & Manning, 2019) drops to 90% when the Rvtk is reduced to 0.8. Unlike image understanding, the optimal performance across most video understanding benchmarks occurs at relatively low Rvtk as depicted in Figure D8. And for MSR-VTT (Xu et al., 2016), VATEX (Wang et al., 2019), and ActivityNet-QA (Yu et al., 2019), even achieve better results at extremely low Rvtk (< 0.1). It indicates that comparing to image, video input have higher redundancy. Note that AURORACAP focuses on spatial visual token merging, while the temporal features introduce additional complexity to explore the token merging laws. Appendix shows more calculation details and the visualization results of token merging. Slowfast inference. Inspired by Slowfast-LLaVA (Xu et al., 2024b), we explore whether combining frames with low and high Rvtk can enhance performance. In practice, we dont conduct token merging in the first frame and concatenate them with the merged tokens from subsequent frames. We apply this strategy to both video captioning and video question answering tasks, comparing performance with and without the inclusion of full first-frame visual tokens. As illustrated in Table D1, slowfast inference brings marginal performance improvement in video question answering tasks or even drop in video captioning tasks but with more computing cost. Therefore, by default, we dont using slowfast inference for video detailed captioning. We also present the performance curve with and without the inclusion of full first-frame visual tokens as the visual token kept ratio varies during inference across multiple video understanding tasks. As illustrated in Figure D9 and Figure D10, despite the inclusion of full first-frame visual tokens, slowfast inference does not consistently result in significantly positive effects on performance. In some cases, the incorporation of full first-frame visual tokens even worsens performance degradation as the kept ratio decreases, particularly in video captioning tasks. Efficiency. To assess the inference speed, we utilize the inference time per video question-answering pair in seconds (TPV) as an evaluative metric. SGLang is an accelerated serving framework for LLMs and multimodal LLMs. We consider four settings including with or without token merging and SGLang. Figure D5 indicates the minimum TPV achievable in each settings across seven video understanding datasets. Reducing the visual tokens and using SGLang result in excellent inference times per video question-answering pair while all the datasets with short video and question inputs. In contrast, maintaining full visual tokens or omitting the use of SGLang results in comparatively Aurora Series: AuroraCap Table D1: Ablation on slowfast inference for AURORACAP-7B. We present the average performance among different token merging ratio on various video understanding benchmarks. We show that slowfast inference brings marginal performance improvement in video question answering tasks or even drop in video captioning tasks but with more computing cost. Setting w/o slowfast w/ slowfast 26.72 26.18 -0. B@1 53.01 51.68 -1.33 MSR-VTT B@4 C 17.58 17.00 -0.58 21.25 21.20 -0.05 46.78 46.16 -0.62 28.03 28.07 +0.04 B@1 52.28 52.16 -0. VATEX B@4 ANet MSVD MSRVTT Acc Acc Acc 15.22 15.12 -0.10 16.95 16.95 -0.01 38.30 38.27 -0.03 58.55 59.66 +1.11 56.45 55.65 -0.80 37.26 38.22 +0. Figure D5: Comparison between different inference settings: A: Rvtk = 1.0, without SGLang, B: Rvtk = 0.1, without SGLang, C: Rvtk = 1.0, with SGLang, D: Rvtk = 0.1, with SGLang. The number indicates the maximum inference time in seconds for each benchmark. slower performance, demonstrating the superior inference efficiency of AURORACAP. For each input, we process the video at resolution of 378 378 and sample 8 frames using single H100 GPU. Training strategy. Alternative training strategies for the language stage of AURORACAP are less frequently explored, which is the primary focus of this section. For fair comparison, we use the same training datasets across all settings and maintain consistent hyper-parameters. The following training settings are explored: Setting A: End-to-end training and set Rvtk to 1.0. Setting B: Following Slowfast-LLaVA (Xu et al., 2024b), we retain full visual tokens in the first frame and concatenate with merged tokens using Rvtk of 0.1. Setting C: End-to-end training and set Rvtk to 0.1. Setting D: Most videos in the training data have no more than 8 key frames, while subset (mainly from ShareGPT4Video (Chen et al., 2024d)) contains significantly more. We first exclude this subset from end-to-end training and then use it to train solely the LLM, enhancing its ability to handle multi-frame inputs. Setting E: Training solely the LLM match the performance of Setting set Rvtk at 0.1. We implement these training strategies, track training costs in H100 hours, and evaluate across various video understanding tasks. As shown in Figure D6, while training with an Rvtk of 1.0 improves performance, it significantly increases training time. Surprisingly, mixing lower and higher visual token ratios during training offers no significant advantage. Training only the LLM under the two settings results in performance drop, indicating that enhancing long video understanding still requires collaboration with the finetuning the visual encoder. Therefore, we choose Setting as the final training strategy. Backbone selection. We use the the training loss among the last ten iterations in original LLaVA alignment pretraining stage to guidance the ViT and LLM backbones selection as shown in Table D2. 28 Aurora Series: AuroraCap Figure D6: Comparison between different training strategy in Language stage. We take Accuracy for Question-Answering tasks and CIDEr for captioning tasks as the evaluation metric and present the performance percentage. We choose Setting as the final training strategy as shown with *. The number shows the maximum value for each benchmark. Table D2: Final training loss during pretraining stage with original LLaVA pretraining data. ViT ViT Size LLM LLM Size Loss facebook/dinov2-giant openai/clip-vit-large-patch14-336 openai/clip-vit-large-patch14-336 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k facebook/dinov2-giant laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-bigG-14-laion2B-39B-b160k openai/clip-vit-large-patch14-336 apple/DFN5B-CLIP-ViT-H-14-378 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-bigG-14-laion2B-39B-b160k openai/clip-vit-large-patch14-336 laion/CLIP-ViT-bigG-14-laion2B-39B-b160k laion/CLIP-ViT-bigG-14-laion2B-39B-b160k apple/DFN5B-CLIP-ViT-H-141,136M microsoft/phi-2 428M Qwen/Qwen1.5-0.5B-Chat 428M microsoft/phi-2 1,845M microsoft/phi-2 1,136M lmsys/vicuna-13b-v1.5 1,845M internlm/internlm2-chat-7b 1,845M internlm/internlm2-chat-20b 1,845M deepseek-ai/deepseek-llm-67b-chat 1,845M mistralai/Mistral-7B-Instruct-v0.1 428M mistralai/Mixtral-8x7B-Instruct-v0.1 632M lmsys/vicuna-13b-v1.5-16k 1,845M lmsys/vicuna-7b-v1.5-16k 1,845M mistralai/Mixtral-8x7B-Instruct-v0.1 428M lmsys/vicuna-7b-v1.5 1,845M meta-llama/Llama-2-13b-chat-hf 1,845M lmsys/vicuna-13b-v1. 632M lmsys/vicuna-7b-v1.5-16k 2.7B 0.5B 2.7B 2.7B 13B 7B 20B 67B 7B 8x7B 13B 7B 8x7B 7B 13B 13B 7B 3.3021 3.1001 2.8067 2.7124 2.3895 2.3437 2.2745 2.1572 2.1569 2.0815 2.0443 2.0365 1.9889 1.9762 1.9708 1.9412 1."
        },
        {
            "title": "E DETAILED TRAINING SETTINGS",
            "content": "We use CLIP ViT-H 5, Vicuna-1.5-7B 6 as the initialization of AURORACAP-7B. Training hyperparameters for both stages are shown in Table E6. For visual data preprocessing, we resize each image or video frame to the short side of 378 while keeping original aspect. Instead of doing center crop, we conduct bilinear position embedding interpolation for each training sample. For video data, we extract frames at 2 FPS uniformly. For token merging, we use constant schedule for each blocks."
        },
        {
            "title": "F EVALUATION BENCHMARKS AND SETTINGS",
            "content": "We list all the hyper-parameters and prompt used for evaluation as shown in Table F7. For our proposed VDC, we show the settings as following with the max number of the tokens of 1,024: Type Prompt used for evaluation Camera Describe any camera zooms, pans, or angle changes. Background Summarize the background setting of the video based on these frames. Main Object Describe the main subject, including their attributes and movements throughout the video. Detail Imagine the video from these frames and describe it in detail. 5Huggingface Model: https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-378 6Huggingface Model: https://huggingface.co/lmsys/vicuna-7b-v1.5-16k 29 Aurora Series: AuroraCap Figure D7: Ablation study of token merging on image captioning on Flickr (Young et al., 2014), NoCaps (Agrawal et al., 2019), COCO-Cap (Lin et al., 2014), visual question answering in GQA (Hudson & Manning, 2019), MMMU (Yue et al., 2023), VQAv2 (Goyal et al., 2017). We found that token merging significantly reduces the number of tokens while maintaining minimal performance drop, and even showing improvement in some tasks. We highlight the token merging ratio when achieving 90% and 80% performance with the dash line and filled area. 30 Aurora Series: AuroraCap Figure D8: Ablation study of token merging on video captioning on MSRVTT (Xu et al., 2016), VATEX (Wang et al., 2019), video question answering on MSRVTT-QA (Xu et al., 2016), ActivityNetQA (Yu et al., 2019), MSVD-QA (Xu et al., 2017), iVQA (Liu et al., 2018). We found that token merging significantly reduces the number of tokens while maintaining minimal performance drop, and even showing improvement in some tasks. We highlight the token merging ratio when achieving 90% and 80% performance with the dash line and filled area. 31 Aurora Series: AuroraCap Table E3: Summary of datasets used for training AURORACAP in Pretraining stage. Task # Sample Dataset Image Captioning 1.3M LAION-CC-SBU-595K (Liu et al., 2024b), ShareGPT4V (Chen et al., 2023a), ALLaVA-Caption-LAION-4V (Chen et al., 2024a), ALLaVA-Caption-VFLAN4V (Chen et al., 2024a), DenseFusion (Li et al., 2024d) Table E4: Summary of datasets used for training AURORACAP in Vision stage. For classification, Reasoning, VQA, and Generation tasks, we adopt the dataset processed by M3IT (Li et al., 2023c) to fit the training objective of language models. Task # Sample Dataset Captioning Object-centric 1,925K 438K Classification 238K Reasoning 100K VQA 3,518K Generation Chinese Total 145K 193K ShareGPT4V-PT (Chen et al., 2023a), TextCaps (Sidorov et al., 2020), ImageParagraph-Captioning (Krause et al., 2017) COST (Jain et al., 2023), ChatterBox (Tian et al., 2024), V* (Wu & Xie, 2023) COCO-GOI (Lin et al., 2014), COCO-Text (Veit et al., 2016), ImageNet (Russakovsky et al., 2015), COCO-ITM (Lin et al., 2014), e-SNLI-VE (Kayser et al., 2021), Mocheg (Yao et al., 2023), IQA (Duanmu et al., 2021) CLEVR (Johnson et al., 2017), NLVR (Suhr et al., 2017), VCR (Zellers et al., 2019), VisualMRC (Tanaka et al., 2021), Winoground (Thrush et al., 2022) VQA v2 (Goyal et al., 2017), Shapes VQA (Andreas et al., 2016), DocVQA (Mathew et al., 2021), OK-VQA (Marino et al., 2019), Text-VQA (Singh et al., 2019), OCR-VQA (Mishra et al., 2019), A-OKVQA (Schwenk et al., 2022), ScienceQA (Lu et al., 2022) ST-VQA (Biten et al., 2019), ViQuAE (Lerner et al., 2022), LLaVAOneVision (Li et al., 2024a) Visual Storytelling (Huang et al., 2016), Visual Dialog (Das et al., 2017), Multi30k (Elliott et al., 2016) COCO-Caption CN (Li et al., 2019), Flickr-8k-Caption CN (Li et al., 2016), multimodal Chat (Zheng et al., 2021), FM-IQA (Gao et al., 2015), ChineseFoodNet (Chen et al., 2017) 6.6M For all datasets, we uniformly sample without duplication. Table E5: Summary of datasets used for training AURORACAP in Language stage. Task # Sample Dataset Image Captioning 1,779K Video Captionin 1,659K Image Instruction 9,742K Video Instruction Language-only 268k 143K ShareGPT4V (Chen et al., 2023a), ALLaVA-Caption-LAION-4V (Chen et al., 2024a), ALLaVA-Caption-VFLAN-4V (Chen et al., 2024a), DenseFusion (Li et al., 2024d), FaceCaption (Dai et al., 2024a) MiraData (Ju et al., 2024), LLaVA-Hound (Zhang et al., 2024c), ShareGPT4Video (Chen et al., 2024d), Private Data LLaVA-Mix-665K (Liu et al., 2023), LVIS-Instruct4V (Wang et al., 2023), ALLaVA-Instruct-LAION-4V (Chen et al., 2024a), ALLaVA-Instruct-VFLAN4V (Chen et al., 2024a), Cambrian (Tong et al., 2024a), M4-Instruct (Liu et al., 2024a) LLaVA-Hound (Zhang et al., 2024c), ShareGPT4Video (Chen et al., 2024d) Evol-Intruct-GPT4-Turbo-143K (Chen et al., 2024a) Total 15.0M We duplicate video captioning and instruction datasets twice."
        },
        {
            "title": "G LIMITATIONS",
            "content": "We evaluate AURORACAP on various visual question answering benchmarks as shown in Table G8. Since the performance of the VQA task heavily depends on the performance of the LLM, we chose the same LLM for fair comparison. Also, due the the limitation of OCR-related samples in training dataset, AURORACAP does not perform well in OCR at current stage as shown in Table G9. 32 Aurora Series: AuroraCap Table E6: Training hyper-parameters for AURORACAP. Hyper-parameters Pretraining stage Vision stage Language stage trainable parameters warmup schedule warmup start factor warmup ratio learning rate schedule optimizer optimizer hyper-parameters weight decay max norm epoch peak learning rate total equivalent batch size token keep proportion MLP ViT + MLP ViT + MLP + LLM linear 1e-5 0.03 cosine decay AdamW (Loshchilov & Hutter, 2017) 1, 2 = (0.9, 0.999) 0.1 1 1 1e-4 6,144 100% 2e-5 768 10% 2e-4 512 100% Table F7: Evaluation settings summary for each benchmarks. For all benchmarks we set temperature, top p, number of beams to 0, 0, 1 respectively. Benchmark # Sample # Tokens Prompt Flickr (Plummer et al., 2015) NoCaps (Agrawal et al., 2019) COCO-Cap (Lin et al., 2014) ChartQA (Masry et al., 2022) DocVQA (Mathew et al., 2021) TextCaps (Sidorov et al., 2020) GQA (Hudson & Manning, 2019) POPE (Li et al., 2023h) MMMU (Yue et al., 2023) VQAv2 (Goyal et al., 2017) MSR-VTT (Xu et al., 2016) VATEX (Wang et al., 2019) MSVD-QA (Xu et al., 2017) ActivityNet-QA (Yu et al., 2019) MSRVTT-QA (Xu et al., 2017) iVQA (Yang et al., 2021) 31,784 4,500 5,000 2,500 5,349 3,166 12,578 9, 900 214,354 1,000 4,478 1,161 8,000 6,513 6,000 64 64 64 16 32 64 16 128 16 16 64 64 64 64 64 64 Provide one-sentence caption for the provided image. Provide one-sentence caption for the provided image. Provide one-sentence caption for the provided image. Answer the question with single word. Answer the question using single word or phrase. Provide one-sentence caption for the provided image. Answer the question using single word or phrase. Answer the question using single word or phrase. Answer with the option letter from the given choices directly. Answer the question using single word or phrase. Answer the question using single word or phrase. Provide one-sentence caption for the provided video. Provide brief single-sentence caption for the last video below. Answer the question using single word or phrase. Answer the question using single word or phrase. Answer the question using single word or phrase. Answer the question using single word or phrase. Table G8: Comparison AURORACAP with LLM-based SoTA methods on visual question answering benchmarks under zero-shot setting. Model LLM LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.6-7B LLaVA-1.6-13B Vicuna-1.5-7B Vicuna-1.5-13B Vicuna-1.5-7B Vicuna-1.5-13B AURORACAP-7B Vicuna-1.5-7B MMMU (900) GQA (12,578) POPE (9,000) VQAv2 (214,354) Acc 35.30 34.80 35.10 35.90 36.11 Acc 61.97 63.24 64.23 65.36 59.72 F1 85.87 85.92 86.40 86.26 83.31 Acc 76.64 78.26 80.06 80.92 75.85 Table G9: Limitation in terms of OCR capability compared with LLaVA models. Appendix shows the introduction and metrics of each benchmark. Model LLM LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.6-7B LLaVA-1.6-13B Vicuna-1.5-7B Vicuna-1.5-13B Vicuna-1.5-7B Vicuna-1.5-13B AURORACAP-7B Vicuna-1.5-7B ChartQA (2,500) DocVQA (5,349) TextCaps (3,166) Acc Acc Acc 28.08 30.29 74.35 77. 34.60 98.15 103.92 71.79 67.39 93.33 18.24 18.20 54.84 62.20 25.88 Aurora Series: AuroraCap Figure D9: Comparison between performance with and without the inclusion of full first-frame visual tokens during inference on video captioning on MSRVTT (Xu et al., 2016), VATEX (Wang et al., 2019), video question answering on MSRVTT-QA (Xu et al., 2016), ActivityNet-QA (Yu et al., 2019), MSVD-QA (Xu et al., 2017), iVQA (Liu et al., 2018). As the visual token kept ratio varies, the blue curve indicates performance with slowfast inference, while the red curve represents performance without slowfast inference. 34 Aurora Series: AuroraCap Figure D10: Comparison between performance with and without the inclusion of full first-frame visual tokens during inference on VDC. As the visual token kept ratio varies, the blue curve indicates performance with slowfast inference, while the red curve represents performance without slowfast inference. Aurora Series: AuroraCap"
        },
        {
            "title": "H VDC GENERATION PROMPT TEMPLATE",
            "content": "Following (Ju et al., 2024), we utilize LLM to generate structured detailed captions. Given an input video, LLM return five detailed captions, including camera caption, short caption, background caption, main object caption and detailed caption for the entire video guided by our designed prompt template. The complete prompt is shown as followings: Type Prompt SYSTEM You are describing the video. Please provide detailed captions of the video from different aspects. User Please provide detailed and comprehensive captions for the following content: 1. Short Caption: Summarize the video in one detailed sentence, capturing key actions and the overall mood. 2. Background Caption: Provide detailed description of the background, including objects, location, weather, time, and any dynamic elements such as movements in the environment. 3. Main Object Caption: Give thorough description of the main subjects actions, attributes, interactions, and movements throughout the video frames, including changes in posture, expression, or speed. 4. Camera Caption: Describe the camera work in detail, including shot types, angles, movements, transitions, and any special effects used to enhance the video. 5. Reference Caption: Generate detailed dense caption for the video that is at least 300 words long. The caption should capture all visible actions, environmental details, and the overall emotional atmosphere in depth. Describe in detail the interactions between the main subjects and their environment, including subtle nuances of their movements or expressions. Elaborate on the sounds, textures, and other sensory experiences depicted in the video. Discuss the camera techniques used extensively, including shot types, angles, movements, and transitions. Highlight the mood and tone of the video throughout, creating rich narrative that connects viewers emotionally to the scene. Include comprehensive descriptions of background elements that add context and depth, such as weather conditions, time of day, and cultural or historical settings. Make sure to provide vivid portrayal that is engaging, informative, and rich enough for AI to re-generate the video content. No need to provide summary content. Do not describe each frame individually. Avoid using phrases like first frame. The description should be rich enough for AI to re-generate the video. Please generate the response as Python dictionary string with keys like short caption. DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. These are the frames from the video: [VIDEO KEYFRAME INPUT] GPT-4o Short Caption: In sunny outdoor setting, two workers engage in collaborative effort to prepare gravel base for construction project, showcasing teamwork and focus. Camera Caption: The camera captures the scene from low angle, providing unique perspective that emphasizes the workers actions and the surrounding environment. The shots are primarily medium close-ups, focusing on the workers upper bodies and hands as they interact. The camera occasionally shifts to wider angle, revealing more of the workspace and background elements. There are smooth transitions between shots, maintaining steady flow that enhances the viewers engagement with the unfolding narrative. The use of slightly fish-eye lens effect adds depth to the scene, making the environment feel more immersive. Background Caption: The scene is set in residential area with well-maintained lawn and brick wall partially constructed in the background. The ground is covered with white plastic sheet, and scattered gravel is visible. few wooden planks lie on the ground, and tools are scattered around, including black water bottle and power tool. Main Object Caption: The main subjects are two workers, one wearing gray sweatshirt and gloves, and the other dressed in casual work attire. They are actively engaged in conversation, with the first worker gesturing with his hands to emphasize points while explaining the task at hand. His posture is relaxed yet focused, indicating collaborative atmosphere. The second worker listens attentively, occasionally nodding and responding with gestures, showcasing dynamic interaction. Both workers exhibit sense of camaraderie and professionalism as they prepare for the next steps in their project. Reference Caption: The video unfolds . . . (300 words) . . . through them. User The video has been describe from the following aspects:1. short caption: short caption, 2. background caption: background caption, 3. main object caption: main object caption, 4. camera caption: camera caption, 5.Reference Caption: reference caption Please generate detailed dense caption for the video that is pretty long. You should expand the reference caption upon the information provided in the short caption, background caption, main object caption, and camera caption. Ensure that the detailed caption does not introduce any new entities or relationships that were not mentioned in the previous captions. Make sure to provide vivid portrayal that is engaging, informative, and rich enough for AI to re-generate the video content. Avoid using phrases like first frame, short caption, background caption, main object caption, and camera caption. The description should be rich enough for AI to re-generate the video. GPT-4o Detailed Caption: The video opens with . . . (500 words) . . . literature that surrounds them. User [Manually check and refine] 36 Aurora Series: AuroraCap QUESTION-ANSWER PAIRS GENERATION PROMPT TEMPLATE OF VDCSCORE To decompose the ground-truth structured detailed captions in VDC, we utilize GPT-4o as the LLM assistant to generate numerous short question-answer pairs for subsequent evaluation. The full prompt and example cases are presented as followings: Type Prompt SYSTEM You are an intelligent chatbot designed for generating 20 question-answer pairs given detailed description of video or image. You are describing the video. Heres how you can accomplish the task: INSTRUCTIONS: - Cover the main objects and actions in the video or image. - The questions should be open-ended and start with What, Who, Where, When, Why, How, etc. - The answer should be short sentence or phrase. - Generate 20 question-answer pairs. User Please generate 20 question-answer pairs given detailed description of video or image: detailed description: The camera work is dynamic, featuring mix of close-up shots and wider angles that provide context to the environment. The shots are slightly shaky, adding sense of realism and immersion to the viewer. Transitions between shots are smooth, with the camera often panning or tilting to follow the subjects movements. The use of low light creates moody atmosphere, with shadows enhancing the visual interest of the scene . . . Please generate the response in the form of Python list of tuple with the question and the corresponding answer. DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python list of tuple. For example, your response should look like this: [(the question 1, the answer 1), (the question 2, the answer 2), . . . ]. GPT-4o [question: What is the style of camera work in the video?, answer: dynamic, question: What is the effect of shaky shots in the video?, answer: realism and immersion, question: What type of transitions are used in the video?, answer: smooth, question: Why is the moody atmosphere created in the video?, answer: through low light, question: What type of camera movement is achieved through panning and tilting?, answer: following the subjects movements, question: What is the effect of the camera movement on the viewer?, answer: sense of realism and immersion, . . . ] User [Manually check and refine]"
        },
        {
            "title": "J MORE STATISTICS INFORMATION OF VDC",
            "content": "Figure J11 indicates the word distribution of the structured captions in VDC. Table J10 illustrates the distribution of the question categories we provided for VDCSCORE calculation. (a) Camera Caption (b) Short Caption (c) Background Caption (d) Main Object Caption (e) Detailed Caption (f) Total Figure J11: Word cloud of different structured captions in VDC, showing the diversity. 37 Aurora Series: AuroraCap Table J10: Distribution of the question categories in VDCSCORE. Type Background Camera Short Main object Detailed Environment & Scene Character & Object Intent & Outcomes Action Attributes & Relationship 3,464 6,272 4,585 2,861 944 2,543 3,779 5,105 1,198 334 4,401 4,834 5,067 1,857 1, 3,097 6,035 4,474 3,192 1,478 3,549 6,440 4,640 1,932 1,"
        },
        {
            "title": "K MORE STATISTICS INFORMATION OF VDCSCORE",
            "content": "We generate total of 96,902 question-answer pairs for VDC, with an average of 18.87 pairs per detailed caption. As depicted in Figure K12, each section of the structured captions includes similar number of question-answer pairs. Additionally, Figure K13 presents the distribution of question types generated for VDC. To enhance the evaluation of detailed captions, we configure all questions as open-ended. Environment & Scene encompasses inquiries about location, environment, atmosphere, scene, and time. Character & Object focuses on entities within the caption, such as people, animals, plants, and objects, while Attribute & Relation examines their attributes and interrelations. Intent & Outcomes addresses deeper interpretative questions regarding methods, purposes, reasons, and outcomes. We further analyze the distribution of generated question types within structured captions. Main object captions predominantly feature Character & Object questions, whereas Environment & Scene questions are more prominent in background captions, and Camera questions constitute larger proportion in camera. Figure K12: Question-answer pairs proportaion in structured captions. Figure K13: Distribution of question type in VDCSCORE."
        },
        {
            "title": "L CALCULATION OF ELO RANKING",
            "content": "In this section, we present the methodology for evaluating and ranking AURORACAPand various models using the Elo rating system as shown in Figure L14. The parameters used in the simulation are summarized in Table L11. Table L11: Elo parameter setting. Parameter Number initial Elo mean Elo standard deviation base of logarithm scaling factor K-factor minimum Elo rating number of simulated matches 1,000 300 10 400 32 700 2, 38 Aurora Series: AuroraCap Figure L14: GUI screen for Elo ranking."
        },
        {
            "title": "M CASE STUDY",
            "content": "We perform an extensive case study of AURORACAP on variety of videos for video detailed captioning. As shown as followings, AURORACAP is capable of providing excellent detailed captions regarding the camera motion, background and main object with less hallucination. Figure M15: Video example of MSR-VTT (Xu et al., 2016) benchmark. Figure M16: Video example of VATEX (Wang et al., 2019) benchmark. Figure M17: Video example. 39 Aurora Series: AuroraCap Figure M18: Video example. Figure M19: Video example. Figure M20: Video example. Figure M21: Video example. Figure M22: Video example. Example Caption Figure M15 [short caption] man with mustache and woman with ponytail are sitting at table. Figure M16 [short caption] man plays the drums in recording studio. Figure M17 [detailed caption] The video depicts serene beach scene where young woman stands on the sandy shore, gazing out towards the ocean. She is wearing black beanie and pink jacket, adding pop of color to the otherwise muted scene. The beach, vast expanse of sand, stretches out in front of her, meeting the ocean at the horizon. The ocean, vast body of water, is visible in the background. The beach is bathed in soft, diffused light, creating dreamy atmosphere. The girls gaze is directed towards the horizon, suggesting sense of wonder or contemplation. The image is slightly blurred, adding dreamy quality to the scene. The womans position on the beach, coupled with the gentle waves of the ocean, suggests moment of contemplation or admiration. The relative positions of the objects suggest peaceful day at the beach, with the girl possibly enjoying the serene view of the ocean. The colors are mostly muted, with the girls pink jacket standing out against the sandy beach and the blue ocean. The blurred background and the out-of-focus elements, such as the ocean and the sky, contribute to the sense of tranquility and focus on the woman. There is no text present in the video, and the colors are muted, with the exception of the pink jacket, which stands out against the more subdued tones of the surroundings. 40 Aurora Series: AuroraCap Figure M18 [detailed caption] The video features man with beard and long hair, lying on the ground with his head resting on tree trunk. He is wearing colorful shirt with mix of orange and green patterns. The mans face is partially obscured by the tree trunk, but his eyes are visible, looking upwards. He appears to be smiling or laughing, with his mouth open. The mans arms are crossed over his chest, suggesting relaxed or playful posture. The background is blurred, but it appears to be natural outdoor setting, possibly forest or park. In the background, another person is partially visible, their presence subtly hinted at by hand reaching out from the left side of the frame. The colors in the video are warm, with the mans shirt standing out against the muted background. The mans hair and beard are dark, and he has light complexion. The overall mood of the video is lighthearted and carefree. There are no visible texts or other objects in the video, and the relative positions of the objects remain constant with the man in the foreground and the second person in the background. Figure M19 [background caption] The background features race track with visible tire marks and barriers, surrounded by grassy areas and few scattered trees. The track is set in rural or semi-rural location, with hills in the distance and cloudy sky overhead, suggesting overcast weather conditions. The track itself appears well-maintained with smooth surface, designed for highspeed racing. The weather, indicated by the cloudy sky, contributes to the overall ambiance of the scene, enhancing the sense of cool, possibly early morning or late afternoon setting. Figure M20 [camera caption] The camera maintains steady, frontal view throughout the video, capturing the womans expressions and the intricate details of the bookshelves. The camera starts at the right edge of the bookshelf, moving across the shelf, and ends at the left edge of the bookshelf. The framing is consistent, focusing on the woman and the bookshelves, with the camera positioned at medium distance to the subject. This movement provides comprehensive view of the bookshelf, showcasing the variety of books and their arrangement on the shelves. The camera occasionally pans to reveal the depth of the library, showcasing the rows of books and the inviting atmosphere. The use of natural light enhances the visual appeal, creating warm and inviting tone throughout the video. Figure M21 [camera caption] The view shot remains relatively static, focusing on the children playing in the backyard. The camera angle is at eye level, capturing the scene from distance that allows both children to be visible. There is minimal camera movement, maintaining steady focus on the children and their activities. The sequence of video frames suggests continuous moment of play without significant changes in shooting angles or camera movement. Figure M22 [main object caption] The main subject in the video, black car, is seen driving down street that appears to be in state of disarray. The car moves steadily forward, navigating around obstacles such as blue car parked on the side of the road. The cars movement is smooth and continuous, suggesting it is either in motion or has just come to stop. The environment around the car is chaotic, with debris scattered across the road and signs of destruction, indicating recent event or disaster. The cars position remains central in the frame, with the camera angle focused on it from slightly elevated perspective, possibly from vehicle or structure above."
        }
    ],
    "affiliations": [
        "Harvard University",
        "New York University",
        "Pika Lab",
        "Stanford University",
        "University of Washington"
    ]
}