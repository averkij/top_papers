{
    "paper_title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "authors": [
        "Junjie Wang",
        "Bin Chen",
        "Yulin Li",
        "Bin Kang",
        "Yichi Chen",
        "Zhuotao Tian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at \\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}."
        },
        {
            "title": "Start",
            "content": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception Junjie Wang1 Bin Chen2,3,* Yulin Li1 Bin Kang3 Yichi Chen3 Zhuotao Tian1,* 1School of Computer Science and Technology, HIT, Shenzhen 2International Research Institute for Artificial Intelligence, HIT, Shenzhen 3University of Chinese Academy of Sciences 5 2 0 2 ] . [ 1 0 1 4 4 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIPs image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, novel framework that enhances CLIP by decoupling the self-attention module to obtain content and context features respectively. The content features are aligned with image crop representations to improve local discriminability, while context features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at https://github.com/xiaomoguhz/DeCLIP. 1. Introduction In the era of deep learning, dense prediction tasks like object detection [44, 55] and image segmentation [12, 57] have rapidly advanced and are widely used. However, traditional methods [7, 40, 91] recognize only fixed set of predefined categories. This restriction hinders the practical application of these methods in real-world settings, where the range of visual concepts is virtually boundless. Consequently, increasing attention has been drawn to open-vocabulary methods [14, 67, 70, 82], which aim to detect and segment objects from any category using textual descriptions. *Corresponding authors Figure 1. DeCLIP outperforms previous state-of-the-art models on broad range of open-vocabulary dense prediction benchmarks. Building on the success of Vision-Language Models (VLMs) [13, 41, 52, 61] pre-trained on image-text pairs, such as CLIP [52], researchers have started leveraging these models for open-vocabulary dense prediction tasks. Among these [8, 65, 6769, 84], transfer-learning approaches [11, 30, 37, 65, 68, 78] have shown outstanding performance. These methods utilize the image encoder of VLM as feature extractor and exclusively train lightweight task-specific components. Whereas using VLMs as feature extractors offers significant advantages due to their comprehensive pre-training, directly applying these image-level models to dense prediction often leads to domain shift issues [68, 70]. What hinders CLIP in dense perception? To assess VLMs constraints in dense perception, we analyze CLIPs attention maps across various layers (Figure 3(a)). Our experiments reveal that CLIPs [CLS] token may interfere Figure 2. Quantitative and qualitative comparisons between our method and CLIP. (a) Performance comparisons of open-vocabulary dense predictions on COCO [43]. (b) Attention map comparisons, with the anchor image token marked in red. with the correlations among other image tokens, leading to suboptimal performance in dense prediction tasks. Specifically, we have observed that in deeper layers (behind the 9th layer), the [CLS] token shifts focus away from primary objects within the image and attends highly to certain background tokens, as highlighted by the bright spots in the first row of Figure 3(a). Moreover, image tokens (rows 2 and 3, Figure 3(a)) exhibit similar behavior with the [CLS] token, showing high attention to certain background tokens regardless of their positions. This observation sheds light on why CLIP struggles in dense prediction tasks: its image tokens fail to aggregate information from spatially or semantically related regions, resulting in dense features that lack local discriminability and spatial consistency. As shown in Figure 2(a), directly using CLIP features on the COCO dataset yields relatively inferior performance in open-vocabulary region classification and semantic segmentation. To tackle this, an intuitive approach is to enhance CLIPs local representations through fine-tuning. However, balancing the optimizations of both local feature spatial correlations and vision-language semantic alignment within unified architecture becomes is it feasible to disentangle new challenge. Therefore, CLIPs features and apply separate guiding constraints to obtain diverse features within unified architecture? Our solution. To address these challenges, we propose DeCLIP, general unsupervised fine-tuning method aimed at enhancing both the discriminability and spatial consistency of CLIPs local features. The core idea is to decouple the self-attention module of CLIP and learn from different teacher models separately. Specifically, DeCLIP decouples the features in the selfattention module into content and context components. The content features, responsible for local discriminability, are fine-tuned by aligning pooled region features with their corresponding image crop [CLS] representations. Meanwhile, the context features, responsible for spatial consistency, are learned from the feature correlations generated by Vision Foundation Models (VFMs). This decoupled distillation design effectively mitigates optimization conflicts, improving the generalization ability when applying CLIP to downstream open-vocabulary dense prediction tasks. As shown in Figure 2, DeCLIP significantly outperforms CLIP in local discriminability and spatial consistency. To summarize, our contributions are as follows: We analyze CLIP and find that its limitation in openvocabulary dense prediction arises from image tokens failing to aggregate information from spatially or semantically related regions. To address this issue, we propose DeCLIP, simple yet effective unsupervised fine-tuning framework, to enhance the discriminability and spatial consistency of CLIPs local features via decoupled feature enhancement strategy. Extensive experiments demonstrate that DeCLIP can be decently applied to mainstream open-vocabulary dense prediction tasks, including object detection and semantic segmentation. As illustrated in Figure 1, DeCLIP outperforms state-of-the-art methods across broad range of benchmarks, achieving superior performance metrics in all evaluated task domains. 2. Background and Motivation In the following, we provide concise overview of foundational concepts pertinent to this study in Section 2.1, and highlight important findings in Section 2.2, which offer valuable insights for motivating the proposed approach. 2.1. Preliminaries Contrastive Language-Image Pre-training (CLIP) [52] is built upon two encoders, one for images and one for text. The visual encoder of CLIP can be CNN series [27, 45] or ViT [19], and the text encoder is Transformer [62]. This paper focuses on the CLIP model with the ViT architecture, Figure 3. Visualization of attention maps across different encoding layers of CLIP and VFM. The attention weights are calculated at low resolution, then averaged across different heads, and finally upsampled to the original image resolution for visualization. The anchor image token is marked in red. We observe the occurrence of the proxy token phenomenon in CLIP, but not in VFM. Furthermore, when the position of the anchor image token is shifted, VFM shows better correlation for image tokens with the same semantics. Figure 4. Pre-fine-tuning methods for adapting CLIP to dense prediction tasks. Existing work considers establishing region-text alignment through cost-effective methods via: (a) using images as pseudo regions or (b) using self-distillation on image patches. The former regards the entire image as region, which results in loss of details. The latter uses self-distillation on the image patches thereby gaining more fine-grained information, but still fails to apply to pixel-level image segmentation. (c) Unlike prior approaches, we use VFM to guide the spatial consistency of CLIPs features, and decouple CLIPs features for distillation separately to avoid optimization conflicts. which adopts the [CLS] token to represent the overall features of an image. CLIP learns vision-language alignment by maximizing the cosine similarity between the [CLS] token and text features of matched image-text pairs, and minimizing the similarity for unmatched pairs. Dense feature extraction with CLIP. ViT-based CLIP consists of series of stacked attention blocks. For example, the ViT-B version of CLIP includes 12 attention block layers. Let = {x0, x1, , xhw} denotes the input to the last attention block, where xi R1D. The computation within this attention block can be expressed as: = Projq(X), = Projk(X), = Projv(X), = + Proj (Attnqk V) , = + FFN(Y), (1) (2) (3) where Q, K, and represent the query, key, and value respectively; Proj denotes projection layembeddings, (cid:16) (cid:17) QK/ ers; Attnqk = SoftMax represents the selfattention process, with denoting the dimension of each attention head. FFN denotes feed-forward network. For simplicity, normalization operations are omitted. After passing through the final attention block, Z[0] represents the global [CLS] token. The remaining image patch embeddings Z[1 : w] can be reshaped to obtain dense feature representations Xdense RCHW 1. Adapting CLIP to dense prediction tasks. Several studies have attempted to alleviate the domain shift issue in applying CLIP to dense prediction tasks via fine-tuning strategies. These approaches fall into two main categories: Joint fine-tuning. These methods fine-tune CLIP while training task-specific components [14, 30, 31, 39, 42, 72, 77]. For instance, CAT-Seg [14] proposes an attention fine-tuning strategy based on ViT CLIP, which general1The final V-L projection layer is omitted here for brevity. izes well to unseen categories. MAFT [30] leverages attention bias to fine-tune CLIP for mask classification. Pre-fine-tuning. These methods directly fine-tune CLIP using cost-efficient techniques [49, 6870, 85], which are more closely aligned with the approach proposed in this paper. As illustrated in Figure 4(a), CLIM [69] employs mosaic augmentation technique to stitch multiple images into single image, enabling each sub-image to serve as pseudo-region for region-text contrastive learning. CLIPSelf [68] enhances CLIPs region classification accuracy by maximizing cosine similarity between its region representations and the corresponding image crop representations, as illustrated in Figure 4(b). 2.2. Key Observations Despite the promising results of the two categories of finetuned methods in Section 2.1, they continue to exhibit certain limitations. Joint fine-tuning methods are typically specific to tasks or models and heavily rely on labor-intensive annotations of dense prediction tasks. On the other hand, pre-fine-tuning methods demonstrate broader applicability. However, their region-level fine-tuning technique remains limited in image segmentation tasks that require pixel-level details. To tackle this issue, we investigate the feasibility of incorporating pixel-level details into CLIPs pre-finetuning, enabling it to better align with open-vocabulary dense prediction tasks. In the following, we start by analyzing CLIPs attention maps across various layers. The proxy token phenomenon. As shown in Figure 3(a), we found that in CLIPs shallow layer, the attention weights of CLIPs [CLS] token are widely distributed across the image (i.e., layer 6). However, in the deeper layers, the [CLS] token shifts its focus away from primary objects in the image and attends to specific tokens, as highlighted by the bright spots within the image background. Additionally, we found that image tokens (rows 2 and 3) exhibit similar behavior to the [CLS] token, showing high attention to certain tokens in the background, regardless of their position. These background tokens may serve as proxies for the [CLS] token. This suggests that these tokens aggregate essential information from other image tokens, enabling the [CLS] token to form an approximate global view by summarizing content from them, thereby facilitating image classification. However, these proxy tokens negatively affect the feature correlations between image tokens. As illustrated in Figure 3(a), when we shift the position of the anchor image token (from the bird to the branch), we observe that the new image token still pays high attention to the proxy tokens. This results in lack of correlation between image patches that share the same semantics, which is detrimental to dense prediction tasks. VFMs exhibit better dense correlations. Considering the inherent constraints that impede CLIPs efficacy in dense Table 1. Performance of different distillation schemes. Distillation Type Region Classification (mAcc) Semantic Segmentation (mIoU) COCO (Thing) COCO (Stuff) Context59 CityScape Self Distillation [68] Self+VFM Distillation [36] Self+VFM+Decouple 69.5 65.6 (-3.9) 75.0 (+5.5) 44.6 41.3 (-3.3) 51.8 (+7.2) 29.4 32.4 (+3.0) 35.3 (+5.9) 25.6 28.7 (+3.1) 32.3 (+6.7) perception tasks, we instead observe that VFMs such as the DINO series [5, 51], trained in self-supervised manner, and the SAM series [36, 54], trained on large-scale segmentation data, are capable of extracting features with strong spatial consistency, as shown in Figure 3(b). In particular, the attention map of VFMs does not exhibit the proxy token phenomenon observed in CLIP. Furthermore, when we change the position of the anchor image token, the VFM shows better correlation for image tokens with the same semantics. Therefore, we consider whether VFMs can be incorporated into the pre-fine-tuning process to further improve the feature correlations of CLIP. However, this straightforward approach fails to achieve satisfactory results. Conducting VFM distillation2 and selfdistillation3 simultaneously results in reduced region classification performance, as shown in Table 1 (row 2). We hypothesize that this observation stems from the fact that spatial feature correlation and vision-language alignment have different optimization focuses, and optimizing them simultaneously within single model results in trade-offs. 3. Method Through the above analysis, we found that CLIP underperforms in dense prediction tasks since its image tokens fail to effectively aggregate information from semantically related regions. Observations of VFMs attention maps inspired us to incorporate them into CLIPs pre-fine-tuning process. Considering the optimization conflict between feature correlations and visual-language alignment, we applied decoupled feature enhancement strategy to CLIP. In this section, we introduce DeCLIP, an unsupervised fine-tuning framework for adapting CLIP to dense prediction tasks. We first explain how to decouple CLIPs selfattention mechanism into content and context components in Sec.3.1, then describe how these components learn from different teacher models in Sec.3.2 by distillation. 3.1. Decoupled Attention The unsuccessful attempts to simultaneously perform selfdistillation and VFM distillation on Xdense (Table 1, row 2) prompted us to explore the feasibility of decoupled distillation. In the following, we propose decoupling CLIPs 2VFM distillation indicates aligning the feature self-correlations between CLIPs Xdense and that of the VFM. 3Self-distillation refers to aligning region features from Xdense with their corresponding [CLS] representation. Figure 5. Illustration of the DeCLIP framework. We decouple CLIPs final attention module into context and content features for distillation, avoiding optimization conflicts between feature correlations and visual-language alignment. CLIP itself serves as the teacher for content features to improve region classification accuracy. VFM serves as the teacher for context features to enhance spatial consistency. self-attention module to obtain content and context features, and separately optimize the local discriminability and spatial consistency abilities, as illustrated in Figure 4(c). Rethinking the self-attention. As described in Sec.2.1, in CLIPs last attention block, the features are weighted and summed under the guidance of the attention map (Attnqk) derived from and K, which define spatial or semantic relationships among image tokens. Studies [38, 59, 63, 71] have shown that CLIPs dense features Xdense can be directly used for semantic segmentation by per-pixel classification, indicating that each pixel of Xdense contains independent semantic information. Inspired by this, we regard and as anchors for improving spatial consistency, and Xdense as an anchor for enhancing local discriminability. Additionally, recent training-free OVS studies [38, 63] have further promoted us to decouple CLIPs self-attention followed by distillation. They modify CLIPs attention block from Attnqk to Attnqq and remove the residual connections, simplifying the optimization of local feature consistency by focusing on alone. Based on our rethinking of CLIPs self-attention and inspired by these methods, we propose decoupling CLIPs last attention block to obtain content and context features for distillation as follows: Xcontext = Projq(X), = Projv(X), Xcontent = Proj (Attncontext V) , Attncontext = SoftMax (cid:16) XcontextX context/ (4) (5) (6) (cid:17) . Specifically, is aggregated based on the attention map (Attncontext) generated from Xcontext. Xcontext determines which image tokens are semantically or spatially related. Xcontent carries the semantic information of each image token in the visual-language space. By decoupling the features in this manner, we can apply different guidance constraints to Xcontext and Xcontent to obtain diverse feature representations in unified architecture without interference. As observed in Sec. 2.2, VFM exhibits strong correlation for image tokens with the same semantics, thus we leverage it as guidance for Xcontext to improve CLIPs local feature spatial consistency. Meanwhile, we employ the selfdistillation technique as guidance for Xcontent to enhance the visual-language alignment of CLIPs region feature. As demonstrated in Table 1 (row 3), this decoupled optimization significantly improves the local discriminability and spatial consistency of CLIPs features, leading to simultaneous enhancements in both region classification accuracy and semantic segmentation performance. 3.2. DeCLIP The previous section presents method for obtaining the decoupled context and content features from CLIP. In this section, we elaborate on how the decoupled features Xcontent and Xcontext learn from their respective teacher models to enhance CLIPs performance on open-vocabulary dense prediction tasks. Content feature distillation. As shown in Figure 5, the first teacher model in DeCLIP is itself, which is known as self-distillation [9, 49, 50, 68]. we employ an image patching method to align the region representations of the student models feature map with the corresponding image crop representations (i.e., [CLS] token) of the teacher model. Specifically, the input image is first divided into sub1, 2, . . . , regions. Subsequently, these sub-regions are cropped from the original image, resulting in set of sub-images = {I k}. The student model takes the image as input and outputs the content feature Xcontent RCHW and the context feature Xcontext RDHW , as mentioned in Eq.(6). Here, represents the dimension of the CLIP visual encoder, and represents the shared dimension of the vision-language modality. Then, the student model uses RoI Align [28] to pool region features from Xcontent based on the cropping coordinates of S, resulting in region feature set Fs = {f k }, where R1C. 2 , . . . , 1 , Meanwhile, the teacher model takes the sub-image set as input and outputs series of [CLS] tokens corresponding to the cropped sub-images, resulting in [CLS] token set R1C. Finally, we use Ft = {f cosine similarity loss to align the [CLS] tokens from Ft with the region features from Fs as follows: k}, where 2, . . . , 1, Lcontent = 1 (cid:88) (cid:18) 1 i=1 s i i (cid:19) . (7) 1, 1 , 2, . . . , 2 , . . . , The intuition behind this distillation branch is that, for objects within an image, classifying them using image crops (i.e., [CLS] token) achieves higher accuracy than using region features [68]. This is because CLIP is pre-trained on image-text pairs using contrastive learning, as mentioned in Sec.2.1. Therefore, the distillation learning of Xcontent enhances the discriminability of CLIPs region features, i.e., Fs = {f }, by mimicking the [CLS] tokens obtained from the image crops, i.e., Ft = {f k}. However, as previously discussed in Sec.2.2, the regionlevel fine-tuning remains limited in image segmentation that requires pixel-wise scene understanding. Context feature distillation. As discussed in Sec.2.2, VFMs do not exhibit CLIPs proxy token issue and better correlate semantically related image tokens, which may be conducive to the fine-grained local perception. Therefore, we distilled these correlations into CLIPs Xcontext features. As illustrated in Figure 5, the same image is input into the VFM to obtain its dense feature representations dense RDHW . To ensure consistency in the numXVFM ber of image tokens after patch embedding, different input resolutions are typically used for the VFM and the student CLIP. To transfer VFMs correlations between image tokens to CLIP, an intermediary is required to represent the correlation volume between two image tokens. Cosine similarity is used in our method, specifically as follows: rij = xi xj xi xj . (8) Here, xi R1D and xj R1D represent the i-th and j-th image patch tokens. rij denotes the correlation volume between patch tokens xi and xj. We use the L2 loss to align the discrepancy in the correlation volume between the image tokens of XVFM dense and Xcontext, specifically as follows: Lcontext ="
        },
        {
            "title": "1\nHW",
            "content": "H (cid:88) (cid:88) i=1 j=1 (cid:13) (cid:13)rVFM ij rCLIP ij (cid:13) (cid:13)2 , (9) ij ij and rCLIP where rVFM denote the correlation volume between xi and xj for VFM and CLIP, respectively. Finally, the entire distillation learning process of DeCLIP can be expressed as follows: Ltotal = Lcontent + λLcontext, (10) where λ4 represents the loss scaling hyperparameter. 4. Experiments 4.1. Datasets and Evaluation We conducted extensive evaluations across multiple openvocabulary dense prediction benchmarks, encompassing object detection, semantic segmentation, and segmentation based on VLM features. Due to space limitations, detailed descriptions of the datasets, evaluation metrics, and implementation specifics are provided in the Appendix. 4.2. Benchmark Results Open-Vocabulary Detection. Table 2 presents DeCLIPs performance on OV-COCO and OV-LVIS benchmarks. On OV-COCO, DeCLIP improves the F-ViT [68] baseline by 3.5 and 1.9 mAP, and the OV-DQUO [65] baseline by 6.9 and 2.7 mAP on novel classes. On OV-LVIS, it achieves gains of 1.5 and 2.3 mAP with F-ViT, as well as 1.3 and 2.2 mAP with OV-DQUO on rare classes. Cross-dataset evaluations of F-ViT+DeCLIP trained on OV-LVIS  (Table 3)  further confirm DeCLIPs superiority over existing methods. Open-Vocabulary Semantic Segmentation. Table 4 displays the performance of the CAT-Seg [14] model using DeCLIP as the backbone across various open-vocabulary semantic segmentation benchmarks. The results show that DeCLIP significantly enhances segmentation performance on all datasets. Notably, even with the ViT-B/16 version of DeCLIP, CAT-Seg nearly surpasses all existing SOTA methods that utilize substantially larger encoders like ConvNeXt-L. When employing the ViT-L/14 version of DeCLIP, the model achieves new SOTA results in openvocabulary semantic segmentation tasks. Open-Vocabulary Semantic Segmentation Based on VLM Features. Following existing methods [38, 59, 63], in this experiment, we assign each pixel in the feature map the category with which it has the highest cosine similarity. The low-resolution prediction result is up-sampled to the 4The sensitivity analysis is in the appendix. Figure 6. Comparisons between DeCLIP and existing methods in terms of open-vocabulary region classification ability at different resolutions on the COCO panoptic dataset. Table 2. Comparison with state-of-the-art open-vocabulary object detection methods. Caption supervision indicates that the method learns from extra image-text pairs, while CLIP supervision refers to transferring knowledge from CLIP. : DETR-based detectors [4]. (a) OV-COCO benchmark (b) OV-LVIS benchmark Method Supervision Backbone ViLD [24] Detic [89] OV-DETR [81] BARON-KD [67] SAS-Det [84] OV-DQUO [65] RegionCLIP [85] CORA [70] OV-DQUO [65] RO-ViT [34] CFM-ViT [33] CLIPSelf [68] CLIPSelf [68] CLIP Caption CLIP CLIP CLIP CLIP Captions CLIP CLIP CLIP CLIP CLIP CLIP CLIP F-ViT [68]+DeCLIP F-ViT [68]+DeCLIP CLIP OV-DQUO+DeCLIP CLIP OV-DQUO+DeCLIP CLIP RN50 RN50 RN50 RN50 RN50 RN50 RN50x4 RN50x4 RN50x ViT-L/16 ViT-L/16 ViT-B/16 ViT-L/14 ViT-B/16 ViT-L/14 ViT-B/16 ViT-L/14 APNovel 50 27.6 27.8 29.4 34.0 37.4 39.2 39.3 41.7 45.6 33.0 34.1 37.6 44.3 41.1 (+3.5) 46.2 (+1.9) 46.1 (+6.9) 48.3 (+2.7) Method Supervision Backbone ViLD [24] OV-DETR [81] BARON-KD [67] RegionCLIP [85] OV-SAM [79] CORA+ [70] F-VLM [37] CLIP CLIP CLIP Caption CLIP Caption CLIP CLIP CLIP Caption CLIP CLIP CLIP CLIPSelf [68] OV-DQUO [65] Detic [89] RO-ViT [34] CLIPSelf [68] OV-DQUO [65] CLIP F-ViT [68]+DeCLIP F-ViT [68]+DeCLIP CLIP OV-DQUO+DeCLIP CLIP OV-DQUO+DeCLIP CLIP RN50 RN50 RN50 RN50x4 RN50x16 RN50x4 RN50x ViT-B/16 ViT-B/16 Swin-B ViT-H/16 ViT-L/14 ViT-L/14 ViT-B/16 ViT-L/14 ViT-B/16 ViT-L/14 mAPr 16.3 17.4 22.6 22.0 24.0 28.1 32.8 25.3 29.7 33.8 34.1 34.9 39.3 26.8 (+1.5) 37.2 (+2.3) 31.0 (+1.3) 41.5 (+2.2) Table 3. Transfer evaluation of the LVIS-trained detector on COCO and Objects365 datasets. Method Supervised Baseline [24] ViLD [24] DetPro [20] BARON [67] F-VLM [37] CoDet [47] RO-ViT [35] CLIPSelf [68] DeCLIP COCO Objects365 [58] AP AP50 AP75 50.9 67.6 46.5 AP AP50 AP75 28.0 38.6 25.6 36.6 34.9 36.2 37.9 39.1 - 40.5 41.0 55.6 53.8 55.7 61.6 57.0 - 63.8 64. 39.6 37.4 39.1 41.2 42.3 - 44.3 44.8 11.8 12.1 13.6 16.2 14.2 17.7 19.5 20.0 18.0 18.8 21.0 27.4 20.5 27.4 31.3 32. 12.6 12.9 14.5 17.5 15.3 19.1 20.7 21.2 original resolution to obtain the final segmentation map. As shown in Table 5, DeCLIP outperforms all existing methods in terms of average mIoU across eight benchmarks, highlighting the effectiveness of our approach in improving the discriminability and spatial consistency of VLM features. Open-Vocabulary Region Classification. We assess the region classification performance of DeCLIP, RegionCLIP [85], and CLIPSelf [68] at various resolutions on the COCO-Panoptic validation set. Using RoI Align [28] and Mask Pooling, we extract local features from the feature maps based on annotated bounding boxes and masks, assigning categories based on maximum cosine similarity. As illustrated in Figure 6, the Top-1 mean accuracy (mAcc) results demonstrate that DeCLIP consistently surpasses existing methods in region recognition across all resolutions. 4.3. Ablation Study The impact of VFMs. We analyzed the impact of various VFM configurations on DeCLIP performance. As shown in Table 6, DeCLIP distilled from DINO [5] performs moderately in segmentation but trails SAM [36, 54] and DINOv2 [51] in region classification. DeCLIP distilled from SAM excels in region classification but shows lower segmentation performance compared to DINO. DINOv2 achieves balance in both region classification and segmentation. Table 4. Results on open-vocabulary semantic segmentation. indicates results re-experimented by CAT-Seg [14]. Method ZegFormer [17] ZSseg [75] OVSeg [42] SAN [76] ODISE [74] MAFT [30] FC-CLIP [78] FrozenSeg [11] CAT-Seg [14] CAT-Seg [14] Backbone Training Set ADE Context459 ADE150 Context59 VOC20 VOC21 COCO-Stuff COCO-Stuff COCO-Stuff COCO-Stuff COCO-Panoptic ViT-B/16 ViT-B/16 ViT-L/14 ViT-L/14 ViT-L/14 ConvNeXt-L COCO-Stuff ConvNeXt-L COCO-Panoptic ConvNeXt-L COCO-Panoptic ViT-B/16 ViT-L/14 COCO-Stuff COCO-Stuff 5.6 7.0 9.0 13.7 11.1 13.1 14.8 14.8 12.0 16.0 10.4 - 12.4 17.1 14.5 17.0 18.2 19.7 19.0 23.8 18.0 20.5 29.6 33.3 29.9 34.4 34.1 34.4 31.8 37.9 45.5 47.7 55.7 60.2 57.3 57.5 58.4 - 57.5 63. 89.5 88.4 94.5 95.5 - 93.0 95.4 - 94.6 97.0 65.5 - - - 84.6 - 81.8 82.5 77.3 82.5 CAT-Seg+DeCLIP ViT-B/16 CAT-Seg+DeCLIP ViT-L/14 COCO-Stuff COCO-Stuff 15.3 (+3.3) 17.6 (+1.6) 21.4 (+2.4) 25.9 (+2.1) 36.3 (+4.5) 40.7 (+2.8) 60.6 (+3.1) 63.9 (+0.6) 96.6 (+2.0) 97.7 (+0.7) 81.3 (+4.0) 83.9 (+1.4) Table 5. Results on open-vocabulary semantic segmentation based on VLM features. Method CLIP [52] MaskCLIP [87] GroupViT [73] ReCo [60] TCL [6] OVSeg [42] SCLIP [63] ClearCLIP [38] CLIP-DINOiser [71] DeCLIP (ours) With background category Without background category VOC21 Context60 COCO-Obj VOC20 CityScape Context59 ADE COCO-Stf 18.8 43.4 52.3 25.1 51.2 53.8 59.1 51.8 62.1 59.7 9.9 23.2 18.7 19.9 24.3 20.4 30.4 32.6 32.4 35. 8.1 20.6 27.5 15.7 30.4 25.1 30.5 33.0 34.8 36.4 49.4 74.9 79.7 57.7 77.5 - 80.4 80.9 80.9 85. 6.5 24.9 18.5 21.6 23.5 - 32.2 30.0 31.7 32.8 11.1 26.4 23.4 22.3 30.3 - 34.2 35.9 35.9 39. 3.1 11.9 10.4 11.2 14.9 5.6 16.1 16.7 20.0 21.9 5.7 16.7 15.3 14.8 19.6 - 22.4 23.9 24.6 25. Avg 14.1 30.3 30.7 23.5 33.9 - 38.2 38.1 40.3 41.9 Table 6. Ablation studies on the impact of different VFMs on open-vocabulary region classification and segmentation. VFMs Arch DINO [5] ViT-B/8 DINO [5] ViT-B/16 SAM [36] ViT-B/16 ViT-L/16 SAM [36] DINOv2 [51] ViT-B/14 DINOv2 [51] ViT-L/14 Region Classification (mAcc) Semantic Segmentation (mIoU) COCO (Thing) COCO (Stuff) Context59 COCO-Stf ADE 68.4 67.6 75.0 76.8 77.2 77. 49.4 47.4 51.8 52.6 52.5 53.1 37.3 38.1 35.3 37.7 39.2 38.0 23.2 23.7 22.0 23.0 25.3 24.1 19.5 20.4 18.5 20.0 21.9 21.3 ciated with the anchor image token. Moreover, this experiment reveals why DeCLIP distilled from DINOv2 works best: SAM lacks semantic association ability, while DINO focus indiscriminately on all primary objects in the image. 5. Conclusion This paper analyzes the limitations of CLIP in dense prediction tasks from the perspective of its attention map. We observed that CLIPs [CLS] token negatively affects the attention map of image tokens. To address this issue, we proposed DeCLIP, decoupled feature enhancement strategy. Extensive experiment results on open-vocabulary that Dedense prediction benchmarks demonstrate CLIP outperforms state-of-the-art methods, achieving excellent performance across all evaluated task domains. Figure 7. Qualitative comparisons of attention maps between VFMs and DeCLIP. The anchor image token is marked in red. Qualitative results. Figure 7 presents the visual comparison of attention maps between DINO, SAM, DINOv2, and DeCLIP. Experimental results show that DeCLIP effectively focuses on regions spatially or semantically asso-"
        },
        {
            "title": "References",
            "content": "[1] Benedikt Alkin, Lukas Miklautz, Sepp Hochreiter, and Johannes Brandstetter. Mim-refiner: contrastive learning boost from intermediate pre-trained representations. arXiv preprint arXiv:2402.10093, 2024. 21 [2] Xiang An, Kaicheng Yang, Xiangzi Dai, Ziyong Feng, and Jiankang Deng. Multi-label cluster discrimination for viIn European Conference on sual representation learning. Computer Vision, pages 428444. Springer, 2025. 21 [3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Cocostuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12091218, 2018. 17, 20 [4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. Endto-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 7, 20 [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. EmergIn ing properties in self-supervised vision transformers. Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 4, 7, 8, 21 [6] Junbum Cha, Jonghwan Mun, and Byungseok Roh. Learning to generate text-grounded mask for open-world semantic segmentation from only image-text pairs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1116511174, 2023. [7] Fangyi Chen, Han Zhang, Kai Hu, Yu-Kai Huang, Chenchen Zhu, and Marios Savvides. Enhanced training of querybased object detection via selective query recollection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 23756 23765, 2023. 1 [8] Fangyi Chen, Han Zhang, Zhantao Yang, Hao Chen, Kai Hu, and Marios Savvides. Rtgen: Generating region-text pairs for open-vocabulary object detection. arXiv preprint arXiv:2405.19854, 2024. 1, 17 [9] Jun Chen, Deyao Zhu, Guocheng Qian, Bernard Ghanem, Zhicheng Yan, Chenchen Zhu, Fanyi Xiao, Sean Chang Culatana, and Mohamed Elhoseiny. Exploring open-vocabulary semantic segmentation from clip vision encoder distillation only. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 699710, 2023. 5 [10] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96409649, 2021. 21 [11] Xi Chen, Haosen Yang, Sheng Jin, Xiatian Zhu, and Hongxun Yao. Frozenseg: Harmonizing frozen foundation models for open-vocabulary segmentation. arXiv preprint arXiv:2409.03525, 2024. 1, 8, 21 [12] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention In mask transformer for universal image segmentation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. 1 [13] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible In scaling laws for contrastive language-image learning. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. 1 [14] Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. Catseg: Cost aggregation for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4113 4123, 2024. 1, 3, 6, 8, 15, 20, 21 [15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset In for semantic urban scene understanding. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. 17, 20 [16] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. arXiv preprint arXiv:2309.16588, 2023. 13, 21 [17] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1158311592, 2022. 8, 20, [18] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1158311592, 2022. 20 [19] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 2, 13 [20] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for open-vocabulary object detection with vision-language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1408414093, 2022. 7, 17 [21] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303338, 2010. 20 [22] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: visual representation for neon genesis. Image and Vision Computing, 149:105171, 2024. 15 [23] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In European Conference on Computer Vision, pages 540557. Springer, 2022. [24] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021. 7, 17, 21 [25] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: In dataset for large vocabulary instance segmentation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. 17, 18 [26] Kunyang Han, Yong Liu, Jun Hao Liew, Henghui Ding, Jiajun Liu, Yitong Wang, Yansong Tang, Yujiu Yang, Jiashi Feng, Yao Zhao, et al. Global knowledge calibration for fast open-vocabulary segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 797807, 2023. 20 [27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 2, 13 [28] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. 6, [29] Joonhyun Jeong, Geondo Park, Jayeon Yoo, Hyungsik Jung, Proxydet: Synthesizing proxy novel and Heesu Kim. classes via classwise mixup for open-vocabulary object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 24622470, 2024. 17 [30] Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, and Humphrey Shi. Learning mask-aware clip representations for zero-shot segmentation. Advances in Neural Information Processing Systems, 36:3563135653, 2023. 1, 3, 4, 8, 21 [31] Siyu Jiao, Hongguang Zhu, Jiannan Huang, Yao Zhao, Yunchao Wei, and Humphrey Shi. Collaborative vision-text representation optimizing for open-vocabulary segmentation. In European Conference on Computer Vision, pages 399416. Springer, 2025. 3, 21 [32] Laurynas Karazija, Iro Laina, Andrea Vedaldi, and Christian Rupprecht. Diffusion models for zero-shot open-vocabulary segmentation. arXiv preprint arXiv:2306.09316, 2023. 20, 21 [33] Dahun Kim, Anelia Angelova, and Weicheng Kuo. Contrastive feature masking open-vocabulary vision transformer. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1555615566, 2023. 7, 17, 20 [34] Dahun Kim, Anelia Angelova, and Weicheng Kuo. Regionaware pretraining for open-vocabulary object detection with vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1114411154, 2023. 7, 17 [35] Dahun Kim, Anelia Angelova, and Weicheng Kuo. Regionaware pretraining for open-vocabulary object detection with In Proceedings of the IEEE/CVF vision transformers. conference on computer vision and pattern recognition, pages 1114411154, 2023. 7, 17, [36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 4, 7, 8, 21 tion upon frozen vision and language models. arXiv preprint arXiv:2209.15639, 2022. 1, 7, 17, 20 [38] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Clearclip: Decomposing clip representations for dense vision-language inference. arXiv preprint arXiv:2407.12442, 2024. 5, 6, 8, 15, 17, 18 [39] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546, 2022. 3, 20, 21 [40] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel Ni, and Heung-Yeung Shum. Mask dino: Towards unified transformer-based framework for object detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30413050, 2023. 1 [41] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23390 23400, 2023. [42] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation In Proceedings of the IEEE/CVF with mask-adapted clip. Conference on Computer Vision and Pattern Recognition, pages 70617070, 2023. 3, 8, 20, 21 [43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2, 17, 18, 20 [44] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. DAB-DETR: Dynamic anchor boxes are better queries for DETR. In International Conference on Learning Representations, 2022. 1 [45] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 2, 13 [46] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 20 [47] Chuofan Ma, Yi Jiang, Xin Wen, Zehuan Yuan, and Xiaojuan Qi. Codet: Co-occurrence guided region-word alignment for open-vocabulary object detection. Advances in Neural Information Processing Systems, 36, 2024. 7, [48] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 891898, 2014. 17, 20 [37] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. F-vlm: Open-vocabulary object detec- [49] Jishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip HS Torr, and Ser-Nam Lim. Open vocabulary semantic segmentation with patch aligned contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1941319423, 2023. 4, 5, 21 [50] Muhammad Ferjad Naeem, Yongqin Xian, Xiaohua Zhai, Lukas Hoyer, Luc Van Gool, and Federico Tombari. Silc: Improving vision language pretraining with self-distillation. In European Conference on Computer Vision, pages 3855. Springer, 2025. 5 [51] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4, 7, 8, 20, 21 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 87488763. PMLR, 2021. 1, 2, 8, 15, 16, 20, 21 [53] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative vision foundation In Proceedings of model reduce all domains into one. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1249012500, 2024. [54] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 4, 7, 21 [55] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. 1, 20 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 17, 19 [57] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 1 [58] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. 7, 17, 18, 19 [59] Tong Shao, Zhuotao Tian, Hang Zhao, and Jingyong Su. Explore the potential of clip for training-free open vocabulary semantic segmentation. In European Conference on Computer Vision, pages 139156. Springer, 2025. 5, 6, 13 [60] Gyungin Shin, Weidi Xie, and Samuel Albanie. Reco: Retrieve and co-segment for zero-shot transfer. Advances in Neural Information Processing Systems, 35:3375433767, 2022. 8 [61] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023. 1, 15, 20, 21 [62] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 2 [63] Feng Wang, Jieru Mei, and Alan Yuille. Sclip: Rethinking self-attention for dense vision-language inference. arXiv preprint arXiv:2312.01597, 2023. 5, 6, 8, 17, 18 [64] Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi Pouransari. Sam-clip: Merging vision foundation models towards semantic and spatial understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36353647, 2024. 21 [65] Junjie Wang, Bin Chen, Bin Kang, Yulin Li, YiChi Chen, Ov-dquo: OpenWeizhi Xian, and Huifeng Chang. vocabulary detr with denoising text query training and open-world unknown objects supervision. arXiv preprint arXiv:2405.17913, 2024. 1, 6, 7, 15, 17, 20, [66] Luting Wang, Yi Liu, Penghui Du, Zihan Ding, Yue Liao, Qiaosong Qi, Biaolong Chen, and Si Liu. Object-aware distillation pyramid for open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1118611196, 2023. 20, 21 In Proceedings of [67] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for openthe vocabulary object detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1525415264, 2023. 1, 7, 17, 20, 21 [68] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, and Chen Change Loy. CLIPSelf: Vision transformer distills itself for open-vocabulary dense prediction. In The Twelfth International Conference on Learning Representations, 2024. 1, 4, 5, 6, 7, 15, 16, 17, 18, 20, 21 [69] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Wentao Liu, and Chen Change Loy. Clim: Contrastive languageimage mosaic for region representation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6117 6125, 2024. 1, 4, 17, 21 [70] Xiaoshi Wu, Feng Zhu, Rui Zhao, and Hongsheng Li. Cora: Adapting clip for open-vocabulary detection with region In Proceedings of prompting and anchor pre-matching. the IEEE/CVF conference on computer vision and pattern recognition, pages 70317040, 2023. 1, 4, 7, 17, 18, 20, 21 [71] Monika Wysoczanska, Oriane Simeoni, Michael Ramamonjisoa, Andrei Bursuc, Tomasz Trzcinski, and Patrick Perez. Clip-dinoiser: Teaching clip few dino tricks. arXiv preprint arXiv:2312.12359, 2023. 5, 8 [72] Bin Xie, Jiale Cao, Jin Xie, Fahad Shahbaz Khan, and Yanwei Pang. Sed: simple encoder-decoder for openIn Proceedings of the vocabulary semantic segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34263436, 2024. 3, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1393813947, 2024. 1, 7, 17, 20, 21 [85] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Regionbased language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1679316803, 2022. 4, 7, 16, 17, 20, 21 [86] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127:302321, 2019. 17, 20 [87] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European Conference on Computer Vision, pages 696712. Springer, 2022. 8, 17, 18 [88] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training arXiv preprint arXiv:2111.07832, with online tokenizer. 2021. 21 [89] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand In European classes using image-level supervision. Conference on Computer Vision, pages 350368. Springer, 2022. 7, 17, 20 [90] Chaoyang Zhu and Long Chen. survey on openvocabulary detection and segmentation: Past, present, and future. arXiv preprint arXiv:2307.09220, 2023. [91] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. 1 [73] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1813418144, 2022. 8, 20 [74] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. 8 [75] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. simple baseline for openvocabulary semantic segmentation with pre-trained visionIn European Conference on Computer language model. Vision, pages 736753. Springer, 2022. 8 [76] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-vocabulary semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2945 2954, 2023. 8 [77] Xin Xu, Tianyi Xiong, Zheng Ding, and Zhuowen Tu. Masqclip for open-vocabulary universal image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 887898, 2023. 3, 21 [78] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip. Advances in Neural Information Processing Systems, 36, 2024. 1, 8, [79] Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, and Chen Change Loy. Open-vocabulary sam: Segment and recognize twenty-thousand classes interactively. In ECCV, 2024. 7, 21 [80] Nir Zabari and Yedid Hoshen. Open-vocabulary semantic segmentation using test-time distillation. In European Conference on Computer Vision, pages 5672. Springer, 2022. 20 [81] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr with conditional In European Conference on Computer Vision, matching. pages 106122. Springer, 2022. 7, 17, 20 [82] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and ShihFu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439314402, 2021. 1, 18 [83] Heng Zhang, Qiuyu Zhao, Linyu Zheng, Hao Zeng, Zhiwei Ge, Tianhao Li, and Sulong Xu. Exploring regionword alignment in built-in detector for open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16975 16984, 2024. 17 [84] Shiyu Zhao, Samuel Schulter, Long Zhao, Zhixing Zhang, Vijay Kumar G, Yumin Suh, Manmohan Chandraker, and Dimitris N. Metaxas. Taming self-training for openthe vocabulary object detection. In Proceedings of DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception"
        },
        {
            "title": "Overview",
            "content": "This material provides supplementary details to the main paper, including the following sections: (6) Details of Proxy Token Phenomenon (7) Additional Experiments (7.1) Ablation Studies (7.2) sanity Checks (7.3) Further Details on Benchmark Results (8) Additional Qualitative Analysis (8.1) Analyses of Feature Correlations (8.2) Comparison of Semantic Segmentation Results (8.3) Comparison of Attention Maps (9) Details of Experimental Settings (9.1) Datasets and Evaluation Protocols (9.2) Implementation Details (10) Related Work (10.1) Open-Vocabulary Dense Prediction (10.2) Transferring VLMs to Dense Prediction Tasks (10.3) Vision Foundation Models 6. Details of Proxy Token Phenomenon This section primarily supplements the details of the proxy token phenomenon observed in CLIP, offering deeper insights into the rationale behind our proposed DeCLIP. Observation. As stated in the main paper, ViT-based [19] CLIP utilizes the [CLS] token to represent the overall features of an image and performs image-text contrastive learning accordingly. Therefore, it is commonly believed that the [CLS] token comprehensively attends to all image tokens during the forward pass to obtain global view, thereby enhancing the image classification process. Unexpectedly, the [CLS] token ceased to focus on the primary object in the image starting from the 7th layer and instead redirected its attention to several image tokens in the background as shown in the first row of Figure 8. These specific image tokens continued to receive significant attention from the [CLS] token in the following encoding layers. similar pattern was observed in the attention maps of CLIPs image tokens. As shown in the second row of Figure 8, we first randomly selected an image token located on the primary object in the image as the anchor image token, and then visualized its attention maps across different encoder layers. The experimental results show that the attention of the anchor image token in layers 1-6 is primarily distributed over the object it belongs to. However, after the 7th layer, which is when the [CLS] token shifted its attention to several specific image tokens in the background, the anchor image token also began to focus on these specific image tokens. Moreover, as illustrated in the third row of Figure 8, when the position of the anchor image token is shifted, the new anchor image token continues to exhibit high attention towards these specific tokens. This demonstrates that this phenomenon is not limited to particular image token but is instead widespread across the image tokens in CLIP. Analysis. One possible explanation for this phenomenon could be the redundancy present in image data. Images inherently carry higher information load than text, encompassing substantial background details that are unrelated to image classification tasks. These specific background tokens may serve as proxies for the [CLS] token. This suggests that these tokens aggregate essential information from other image tokens, enabling the [CLS] token to form an approximate global view by summarizing content from them, thereby facilitating image classification. This perspective is also supported by recent studies [16, 59]. In over decade of CNN [27, 45] development, no studies have reported similar phenomena. Therefore, we speculate that the second reason for this phenomenon may stem from the ViT architecture [19]. The classic ResNet [27] architecture consists of four stages, in which the feature resolution is halved and the number of channels is doubled at each stage. This is process of learning sparse features, where redundant image details are progressively discarded, and feature semantics are continually enhanced. However, CLIP with ViT architecture lacks this process. After patch embedding, the size and the number of channels in the feature map remain unchanged. As result, the model spontaneously generates proxy tokens to mimic the process of learning sparse features, akin to CNN. Effects. As discussed above, the proxy token phenomenon allows ViT CLIP to learn sparse features, which facilitate the extraction of key information from images, enhance image-text contrastive learning and reduce the optimization burden. However, this phenomenon causes the image tokens in CLIP to indiscriminately focus on the proxy tokens in the background, rather than on the regions that are spatially or semantically related to them. Consequently, this leads to CLIPs dense features to lack local discriminability and spatial consistency, affecting its performance in openFigure 8. Visualization of the proxy token phenomenon in the attention maps of the CLIP visual encoder. Specifically, the input image resolution is 224*224. We extract the attention weights from each attention block of CLIP and average them across the multihead dimension (after Softmax), yielding attention maps R197197. M[0, 1:] R1196 represents the attention map from the [CLS] token to other image tokens (first row). M[1:197, 1:197] R196196 represents the attention map between each image token and all image tokens. We randomly select specific image tokens attention map (the second and third rows, indicated by the red dots) for visualization, each with dimensions of 1*196. We reshape them to 1*14*14 and apply bilinear upsampling to 1*224*224 for better visualization. Table 7. Ablation study on types of XContext. Table 8. Ablation study on number of fine-tuning layers. XContext Q + Region Classification (mAcc) Semantic Segmentation (mIoU) COCO (Thing) COCO (Stuff) PASCAL Context59 ADE Fine-tuning Layers Region Classification (mAcc) Semantic Segmentation (mIoU) COCO (Thing) COCO (Stuff) PASCAL Context59 ADE 77.2 76.5 77.3 52.5 51.0 53.8 38.7 39.4 39.2 21.8 21.6 21.9 3 6 9 12 62.7 67.1 70.7 72. 47.0 47.8 50.5 51.3 38.0 39.0 39.0 38.7 21.8 22.3 22.1 21.8 vocabulary dense prediction tasks. 7. Additional Experiments 7.1. Ablation Studies In this section, we conduct thorough ablation study on DeCLIP, encompassing the examination of various Xcontext implementations, the variation in the number of fine-tuning layers, the impact of the hyperparameter λ in the loss function, and the influence of the distillation baseline. Except for the region classification experiment in Table 7, which was conducted at resolution of 10241024, the region classification performance in all other experiments was assessed at resolution of 560560. Additionally, the semantic segmentation performance of all ablation experiments was assessed at resolution of 336336. Types of Context. Since there are various implementations of Xcontext, including Q, K, and + K, we performed an ablation study on their performance in dense prediction tasks, including region classification (mAcc) and semantic segmentation (mIoU), as shown in Table 7. Specifically, implementing Xcontext based on means that the last attention block of CLIP leverages to compute the attention weight. Additionally, implementing Xcontext based on + involves first computing the attention weights of and separately, and then summing them. The experimental results indicate that the performance differences among the three implementations are minimal, while the and exhibits slightly better performance in dense prediction tasks. Number of fine-tuning layers. We performed an ablation study to examine the relationship between the number of fine-tuning attention blocks and dense prediction performance. The experiment was conducted on the ViT-B version of CLIP, which comprises total of 12 attention blocks. we experiment with updating the last 3, 6, 9, and 12 attention blocks. As shown in Table 8, we observed that as the number of fine-tuning layers increased, the performance of region classification continuously improved, reaching its peak at 12 layers. However, the performance of semantic segmentation peaked at 6 layers, and as the number of layers increased further, the performance slightly declined. In practice, to balance the performance of both tasks, we chose to fine-tune all attention blocks in the implementation of DeCLIP. Sensitivity Analysis of λ. In DeCLIP, we employ hyperparameter λ to balance the weight between Lcontent and Lcontext. We performed an ablation study to examine the Table 9. Ablation Study on EVA-CLIP for open-vocabulary semantic segmentation Method Backbone Training Set ADE847 Context459 ADE150 Context59 VOC20 VOC21 CAT-Seg+CLIP [52] CAT-Seg+CLIP [52] ViT-B/16 ViT-L/14 CAT-Seg+EVA-CLIP [61] ViT-B/16 CAT-Seg+EVA-CLIP [61] ViT-L/14 CAT-Seg+DeCLIP CAT-Seg+DeCLIP ViT-B/16 ViT-L/ COCO-Stuff COCO-Stuff COCO-Stuff COCO-Stuff COCO-Stuff COCO-Stuff 12.0 16.0 11.9 14.2 15.3 17. 19.0 23.8 17.6 21.3 21.4 25.9 31.8 37.9 30.4 34.8 36.3 40. 57.5 63.3 52.3 56.2 60.6 63.9 94.6 97.0 94.2 95.8 96.6 97. 77.3 82.5 74.2 80.1 81.3 83.9 Table 10. Ablation Study on EVA-CLIP for open-vocabulary semantic segmentation based on VLM features. Method With background category Without background category VOC21 Context60 COCO-Obj VOC20 CityScape Context59 ADE COCO-Stf CLIP [52] EVA-CLIP [61] ClearCLIP [38] EVA-ClearCLIP DeCLIP 18.8 23.4 51.8 47.0 59.7 9.9 12.8 32.6 29.7 35.3 8.1 15.3 33.0 30.2 36. 49.4 55.9 80.9 78.3 85.0 6.5 12.8 30.0 26.3 32.8 11.1 13.9 35.9 29.4 39.2 3.1 7.7 16.7 16.7 21.9 5.7 9.7 23.9 20.4 25.3 Avg 14.1 18.9 38.1 34.7 41.9 Table 11. Sentitivity Analysis of hyperparameter λ. Table 12. Comparison of different distillation baselines. λ 0.1 0.2 0.25 0.3 Region Classification (mAcc) Semantic Segmentation (mIoU) COCO (Thing) COCO (Stuff) PASCAL Context59 ADE Source Region Classification (mAcc) Semantic Segmentation (mIoU) COCO (Thing) COCO (Stuff) PASCAL Context59 72.4 72.4 72.2 71.9 50.6 51.0 51.3 51.4 37.9 38.4 38.7 38. 21.3 21.7 21.8 21.7 OpenAI EVA-CLIP 65.0 72.2 38.8 51.3 36.2 38.7 ADE 18.6 21.8 relationship between the hyperparameter λ and dense prediction performance. The experimental results demonstrate that our method exhibits strong robustness, and the dense prediction performance of DeCLIP does not fluctuate drastically with changes in λ. Furthermore, the results indicate that λ = 0.25 strikes good balance between region classification capability and image segmentation performance. Distillation Baseline. In our experiments, we used EVACLIP [61] as the baseline for DeCLIP, as we found that it demonstrated improved performance after distillation, as shown in Table 12. This can be attributed to two main factors: (1) EVA-CLIP uses the EVA02 [22] model for initializing the visual encoder. EVA02 was trained using Masked Image Modeling (MIM), thereby enhancing its compatibility with Vision Foundation Models (VFMs). (2) EVA-CLIPs [CLS] token exhibits superior zero-shot classification capability compared to OpenAIs model [68]. In Sec. 7.2, we conducted comprehensive sanity checks to verify whether the performance improvement of DeCLIP in dense prediction tasks is due to the use of EVA-CLIP. 7.2. sanity Checks To eliminate potential biases that EVA-CLIP [61] might introduce, we conducted additional sanity check experiments. Specifically, we first apply vanilla EVA-CLIP as the backbone network in the CAT-Seg [14] model and compare its performance with DeCLIP in the Open-Vocabulary Semantic segmentation (OVSS) task, as shown in Table 9. Furthermore, we re-implemented ClearCLIP [38] based on EVA-CLIP and named it EVA-ClearCLIP. Then, we compared the performance between EVA-CLIP, EVAClearCLIP, and DeCLIP in the OVSS based on VLM features task, as shown in Table 10. We did not conduct further open-vocabulary detection experiments because the baseline detectors, OV-DQUO [65] and F-ViT [68], have already used EVA-CLIP as the backbone network in their respective studies. OVSS. As shown in Table 9, experimental results demonstrate that directly applying EVA-CLIP to CAT-Seg performs worse than OpenAIs model. In contrast, DeCLIP significantly improves CAT-Segs performance across all semantic segmentation benchmarks. OVSS based on VLM feautures. As shown in Table 10, experimental results indicate that EVA-CLIP performs slightly better than CLIP in this task, while EVAClearCLIP underperforms in comparison to ClearCLIP. However, both EVA-CLIP and EVA-ClearCLIP fall significantly short of DeCLIPs average performance of 41.9 across the eight benchmarks. Figure 9. Qualitative comparison of feature correlations between DeCLIP and existing pre-fine-tuning approaches [68, 85]. Specifically, the input image resolution is 336*336. We extract the output features from each attention block of CLIP, where each feature R441D. Then, we compute the feature correlations FC R441441 between the image tokens within using cosine similarity. We randomly select specific image tokens feature correlation (indicated by the red dots) and upsample it to resolution of 336*336 for visualization. Based on the results of the aforementioned experiments, we conclude that the performance improvement of DeCLIP is not attributable to the introduction of EVA-CLIP, but is instead due to the superiority of the decoupled feature enhancement strategy. 7.3. Further Details on Benchmark Results We present detailed results for the OV-COCO, OV-LVIS, and cross-dataset benchmarks to provide comprehensive comparison of the open-vocabulary object detection task, as shown in Tables 13 and 14. 8. Additional Qualitative Analysis This section further presents qualitative experimental analysis of our proposed DeCLIP method in comparison to existing methods, including feature correlation analysis, semantic segmentation results, and attention map comparisons, thereby providing more comprehensive demonstration of the superiority of DeCLIPs decoupled feature enhancement strategy. 8.1. Analyses of Feature Correlations We have analyzed CLIP and found that its limitation in open-vocabulary dense prediction arises from image tokens failing to aggregate information from spatially or semantically related regions. Figure 9 presents comparison of feature correlations among CLIP [52], DeCLIP, and existing pre-finetuning methods [68, 85] at each vision encoder layer. This experiment provide insight into how the output features of each layer in CLIPs visual encoder changed after fine-tuning. In this experiment, we randomly select an image token from the primary object within the image (i.e., the bird) as the anchor and visualize the cosine similarity between the anchor and the other image tokens. The experimental results indicate that the impact of various finetuning methods on the correlation of CLIPs output features becomes noticeable starting from the 6th encoder layer. CLIP vs. existing pre-fine-tuning methods. Rows 1, 2, and 3 of Figure 9 exhibit the changes in feature correlations of CLIP after region-level fine-tuning [68, 85]. The experimental results indicate that region-level fine-tuning enhances the feature correlations of the anchor image token to start converging towards the object it belongs to (rows 2 and 3), rather than being randomly scattered across the image (row 1). This change is highly effective for open-vocabulary object detection tasks. As relevant features become more focused, region features exhibit enhanced discriminative power in the visual-language space when extracting the objects region features from the image for recognition. However, these methods remain constrained in image segmentation tasks that demand pixel-level precision. As shown in the feature correlation results in rows 2 and 3 of Figure 9, most of the pixels surrounding the bird will be misclassified Table 13. Detailed comparison on OV-COCO and OV-LVIS benchmarks. Caption supervision indicates that the method learns from extra image-text pairs, while CLIP supervision refers to transferring knowledge from CLIP. : Detection Transformer based detectors. (a) OV-COCO benchmark [43] (b) OV-LVIS benchmark [25] Supervision Backbone APNovel 50 APBase 50 AP50 Method Supervision Backbone mAPr mAPc mAPf mAP Method ViLD [24] Detic [89] OV-DETR [81] ProxyDet [29] RegionCLIP [85] RTGen [8] BARON-KD [67] CLIM [69] SAS-Det [84] RegionCLIP [85] CORA [70] OV-DQUO [65] RO-ViT [34] CFM-ViT [33] F-ViT [68] BIND [83] F-ViT [68] CLIP Caption CLIP Caption Caption Caption CLIP CLIP CLIP Captions CLIP CLIP CLIP CLIP CLIP CLIP CLIP CLIP F-ViT+DeCLIP CLIP F-ViT+DeCLIP OV-DQUO+DeCLIP CLIP OV-DQUO+DeCLIP CLIP RN50 RN50 RN50 RN50 RN50 RN50 RN50 RN50 RN50 RN50x4 RN50x4 RN50x ViT-L/16 ViT-L/16 ViT-B/16 ViT-L/16 ViT-L/14 ViT-B/16 ViT-L/14 ViT-B/16 ViT-L/14 27.6 27.8 29.4 30.4 31.4 33.6 34.0 36.9 37.4 39.3 41.7 45.6 33.0 34.1 37.6 41.5 44.3 41.1 46.2 46.1 48.3 59.5 51.1 61.0 52.6 57.1 51.7 60.4 - 58.5 61.6 44.5 - - - 54.9 58.3 64.1 57.8 65.2 56.3 60.0 51.2 45.0 52.7 46.8 50.4 46.9 53.5 - 53.0 55.7 43.8 - 47.7 46.0 50.4 54.8 59.0 53.5 60.3 53.6 56.9 ViLD [24] OV-DETR [81] BARON-KD [67] RegionCLIP [85] CORA+ [70] SAS-Det [84] CLIM [69] F-VLM [37] F-ViT [68] RTGen [8] BIND [83] Detic [89] CFM-ViT [33] RO-ViT [34] F-ViT [68] ProxyDet [29] CoDet [47] OV-DQUO [65] CLIP CLIP CLIP Caption Caption CLIP CLIP CLIP CLIP Caption CLIP Caption CLIP CLIP CLIP Caption Caption CLIP F-ViT+DeCLIP CLIP CLIP F-ViT+DeCLIP OV-DQUO+DeCLIP CLIP OV-DQUO+DeCLIP CLIP RN50 RN50 RN50 RN50x4 RN50x4 RN50x4 RN50x64 RN50x64 ViT-B/16 Swin-B ViT-L/16 Swin-B ViT-L/14 ViT-H/16 ViT-L/14 Swin-B ViT-L/14 ViT-L/ ViT-B/16 ViT-L/14 ViT-B/16 ViT-L/14 16.6 17.4 22.6 22.0 28.1 29.1 32.3 32.8 25.3 30.2 32.5 33.8 33.9 34.1 34.9 36.7 37.0 39.3 26.8 37.2 31.0 41.5 24.6 25.0 27.6 32.1 - 32.4 - - 21.8 39.9 33.4 - - - 34.6 - 46.3 - 22.4 35.2 - - 30.3 32.5 29.8 36.9 - 36.8 - - 29.1 41.3 35.3 - - - 35.6 - 46.3 - 29.8 36.5 - - 25.5 26.6 27.6 32.3 - 33.5 - 34.9 25.2 38.8 33.2 47.0 36.6 35.1 35.1 41.5 44.7 - 26.0 36.0 27.7 34.6 Table 14. Detailed comparison of transferring LVIS-trained detectors to the COCO and Objects365 datasets. 8.2. Comparison of Semantic Segmentation Results Method Supervised Baseline [24] COCO [43] AP AP50 AP75 50.9 67.6 46.5 Objects365 [58] AP AP50 AP75 APs APm APl 25.6 38.6 28.0 - - - ViLD [24] DetPro [20] BARON [67] F-VLM [37] CoDet [47] RO-ViT [35] CLIPSelf [68] DeCLIP 36.6 34.9 36.2 37.9 39.1 - 40.5 41. 55.6 53.8 55.7 59.6 57.0 - 63.8 64.6 39.6 37.4 39.1 41.2 42.3 - 44.3 44.8 11.8 12.1 13.6 16.2 14.2 17.7 19.5 20. 18.0 18.8 21.0 25.3 20.5 27.4 31.3 32.2 12.6 12.9 14.5 17.5 15.3 19.1 20.7 21.2 - 4.5 5.0 - - - 9.7 10. - 11.5 13.1 - - - 23.2 24.4 - 18.6 20.7 - - - 35.5 36.7 as bird rather than to be background. CLIP vs. DeCLIP. Rows 1 and 4 of Figure 9 exhibit the changes in feature correlations of CLIP after decoupled feature enhancement strategy. The experimental results indicate that DeCLIP enhances the feature correlations of the anchor image token to closely align with the object it represents, in clear contrast with other existing pre-fine-tuning approaches (row 2 and 3). This experiment reveals why DeCLIP is better suited for image segmentation tasks than existing methods. Additionally, the experiment demonstrates DeCLIPs also superiority over current pre-finetuning approaches in region classification tasks. As shown in the feature correlation map of DeCLIPs 12th layer, the image regions corresponding to the same object as the anchor image token display strong red color, indicating very high feature correlation strength in these regions, thereby enhancing the discriminative power of region features within the visual-language space. 10 shows qualitative of Figure comparison MaskCLIP [87], SCLIP [63], ClearCLIP [38], and our proposed DeCLIP across the Context59 [48], COCOStuff [3], Cityscapes [15], and ADE20K [86] datasets. We observe that, compared to other methods, DeCLIP consistently produces higher-quality and more precise segmentation maps. Specifically, benefiting from content feature distillation, which improves the discriminability of local features, DeCLIP successfully recognizes trees, people, and curbs in the images, as shown in columns 1, 5, and 6 of Figure 10, whereas other models fail. Furthermore, our observation indicates that the distillation of context features improves the spatial consistency of DeCLIPs local features, leading to smoother and less noisy segmentation results compared to other models, as demonstrated in columns 2, 3, 4, and 7 of Figure 10. This demonstrates the superiority of our decoupled feature enhancement strategy. 8.3. Comparison of Attention Maps Figure 11 offers detailed comparison of attention maps between CLIP and our proposed DeCLIP approach. As DeCLIP involves unsupervised fine-tuning, we conducted tests using diverse cross-domain image styles to thoroughly assess its generalization capability. Specifically, we utilized generative models [56] to generate test images in various styles such as ink painting, watercolor, sketch, animation, and oil painting, which are depicted on the left side of Figure 11. These cross-domain test images were not part of the fine-tuning dataset for DeCLIP (i.e., COCO2017 [43]). Figure 10. Qualitative comparison of the open-vocabulary semantic segmentation results between DeCLIP and existing approaches [38, 63, 87]. In addition, we performed detailed comparison of attention maps between CLIP and DeCLIP on in-domain images. Specifically, we selected subset of images from the Object365 [58] validation set for testing, with the results shown on the right-hand side of Figure 11. During the testing phase, we first resized the images to 336336 pixels and then fed them into the model to extract features. Subsequently, we randomly selected an anchor image token and visualized its attention map in the 12th attention block, as indicated by the red dots on the test images in Figure 11. For details on the calculation process of the attention map, please refer to Figure 8. As depicted in Figure 11 , due to the proxy token phenomenon, the heatmap generated by the anchor image token in vanilla CLIP frequently lacks semantic consistency with its corresponding object. In contrast, despite being fine-tuned only on the natural scene dataset COCO, DeCLIP demonstrates significant semantic relevance for both in-domain and cross-domain test images. Moreover, benefiting from context feature distillation, DeCLIPs semantic correlations demonstrate remarkably fine granularity, effectively outlining the boundaries of each object semantically associated with the anchor image token. 9. Details of Experimental Settings In this section, we present further details and configurations utilized in our experiments. 9.1. Datasets and Evaluation Protocols Open-Vocabulary Detection. Following established settings [68, 70, 82], we evaluated our model on the OV-COCO [43], OV-LVIS [25], COCO, and Object365 [58] datasets. The OV-COCO dataset includes 48 base categories and 17 novel categories. The training set contains only base categories, totaling 107,761 images, while the validation set comprises 4,836 images featuring both base and novel categories. We report the mean Average Precision (mAP) at an Intersection over Union (IoU) threshold of 0.5 for novel categories. The OV-LVIS dataset consists of 1,203 categories. Its training set includes only 461 common and 405 frequent categories, totaling 100,170 images. The validation set contains 19,809 images with common, frequent, and rare categories. We report the mAP for rare categories at IoU thresholds ranging from 0.5 to 0.95. Additionally, we provide cross-dataset evaluation results on the COCO and Object365 validation sets for models trained on OV-LVIS to assess generalization across domains. Figure 11. Comprehensive comparison of attention maps between CLIP and DeCLIP. The left side presents images of various styles generated by generative models [56]. The images presented on the right-hand side comes from subset of images in the Object365 [58] validation set. Anchor image token marked in red. Open-Vocabulary Semantic Segmentation. In line with prior studies [14], we trained our model on the COCOStuff dataset [3], which comprises 118,000 images with dense annotations across 171 categories. We then evaluated the model on the ADE20K [86], PASCAL VOC [21], and PASCAL-Context [48] datasets. ADE20K [86] includes 20,000 training images and 2,000 validation images, with two category sets: A-150 (150 common categories) and A847 (847 categories) [18]. PASCAL-Context consists of 5,000 training and validation images, with category sets PC59 (59 categories) and PC-459 (459 categories). The PASCAL VOC dataset includes 1,500 images for training and validation, featuring category sets PAS-20 (20 categories) and PAS-21 (20 object categories plus one background class). We used mean Intersection over Union (mIoU) as the evaluation metric in all experiments. Open-Vocabulary Semantic Segmentation Based on VLM Features. To further evaluate DeCLIP, we assessed it on six commonly used semantic segmentation benchmarks: PASCAL VOC 2012 [21], PASCAL Context [48], Cityscapes [15], ADE20K [86], COCO-Stuff [43], and COCO-Object [3]. For datasets including background category, we refer to them as VOC21 and Context60; those without background category are termed VOC20 and Context59. Consistent with previous experiments, we used mIoU as the evaluation metric across these benchmarks. 9.2. Implementation Details DeCLIP. DeCLIP was trained on training set images from the COCO2017 [43] dataset using 8 GPUs, each with batch size of 2, for 6 epochs (about 44 min/epoch on 84090 GPUs). The AdamW [46] optimizer with learning rate of 1e5 and weight decay of 0.1 was employed during the training process. During the content feature distillation process, the image is divided into blocks, where = n, and and are randomly sampled from the range [1, 6]. After cropping image blocks from the original image, the patches are resized to resolution of 224224 and subsequently fed into the teacher model to generate the corresponding [CLS] tokens for content feature distillation. Unless stated otherwise, our experiments were conducted using EVA-CLIP [61]. In the process of context feature distillation, given the distinct image preprocessing methods with varying means and standard deviations used by CLIP and VFM during pretraining, we incorporated the corresponding parameters during the distillation process. Additionally, to address the potential variation in patch sizes between CLIP and VFM (e.g., CLIP uses 16-patch size while DINOV2 uses 14patch size), we adjusted the image resolutions to maintain consistency in the number of image tokens. For example, we set the resolution of CLIP to 1024 and that of DINOV2 to 896, ensuring both models possess 4096 image tokens. The weight λ for context feature distillation is established at 0.25. Unless specified otherwise, our default VFM is DINOv2 [51]. Open-vocabulary detection. In the open-vocabulary detection experiment, DeCLIP was evaluated in two model baselines: F-ViT [68] and OV-DQUO [65]. These baselines are constructed based on transfer learning principles, utilizing the image encoder of CLIP for feature extraction while maintaining the backbone network frozen during training and only training the task-specific components. The two baseline models utilize distinct detector architectures: F-ViT employs the traditional Faster R-CNN [55] architecture, whereas OV-DQUO utilizes the modern Detection Transformer [4] architecture. This enables thorough assessment of the efficacy of our proposed approach. We maintained the default training strategies and hyperparameter configurations from the original studies for both baseline models to uphold experiment fairness. The only modification was to the temperature parameter when integrating DeCLIP for object detection. For F-ViT, the temperature was set to 45 for the OV-COCO benchmark and 90 for the OV-LVIS benchmark. In OV-DQUO, the temperature was set to 50 for both the OV-COCO and OV-LVIS benchmarks. Open-Vocabulary Semantic Segmentation. In the openvocabulary semantic segmentation experiments, we applied DeCLIP to the CAT-Seg [14] baseline. For all experiments, we adhered to the default training and inference settings of vanilla CAT-Seg, replacing only the image encoder with DeCLIP. Open-Vocabulary Semantic Segmentation Based on VLM Features. During inference, we resized the shorter side of images to 448 pixels and employed sliding window strategy with window size of 336336 and stride of 112112. For all datasets, we generate textual descriptions by utilizing the standard ImageNet prompts [52] in conjunction with their respective class names. No post-processing steps were applied. 10. Related Work 10.1. Open-Vocabulary Dense Prediction Open-vocabulary dense prediction aims to detect and segment visual concepts from novel categories using textual descriptions, extending beyond the base categories on which the model was trained. According to recent surveys [90], methods in this field can be broadly classified into four categories: knowledge distillation-based [26, 66, 67, 81], pseudo-labeling [65, 80, 84, 85, 89], region-aware training [23, 33, 35, 70, 73], and transfer learning-based approaches [17, 32, 37, 39, 42, 65, 68]. dense prediction stems from the inability of image tokens to effectively aggregate information from spatially or semantically related regions. To address this, we propose integrating VFMs into the pre-fine-tuning process and decoupling features for distillation, thereby improving the discriminability and spatial consistency of CLIPs local features. 10.3. Vision Foundation Models Vision foundation models, including the Self-Supervised Representation Learning (SSL) series [1, 2, 5, 10, 51, 88] and the SAM series [36, 54], which are trained on largescale segmentation data, demonstrate the ability to extract features that exhibit strong spatial consistency. SSL is key area in computer vision that focuses on learning meaningful visual features without manual annotations [1, 2, 5, 10, 51, 88]. Vision models trained through SSL can extract image features with excellent spatial understanding. For example, the DINO series [5, 51] can identify similar semantic regions across different images and segment main objects without explicit supervision. Another prominent vision foundation model is SAM [36, 54], which demonstrates similarly outstanding spatial understanding. Trained on the extensive SA-1B segmentation dataset, SAM can accurately capture and segment objects regions in images based on prompts. Recently, some studies have explored the combination of CLIP with VFM, such as SAM-CLIP [64], OV-SAM [79], and FrozenSeg [11], with the goal of integrating SAMs powerful image segmentation capabilities and CLIPs zeroshot semantic perception capabilities. AM-RADIO [53] trains unified vision model through multi-teacher distillation from multiple foundational vision models such as CLIP, DINOv2, and SAM. However, SAM-CLIP, OVSAM, and FrozenSeg focus on integrating CLIP into SAM rather than enhancing CLIP itself as DeCLIP does. AMRADIO does not support OVSS, as confirmed by its authors in Github issues (No. 81, 55, and 42). Another study that solves similar problems to DeCLIP is ViT-Register [16]. However, unlike DeCLIP, ViT-Register [16] does not solve the dense perception deficiency arising from CLIPs imagetext alignment. Knowledge distillation-based methods, such as ViLD [24], BARON [67], and OADP [66], propose various distillation frameworks to transfer the generalized classification knowledge of VLMs [52, 61] into dense prediction models. Pseudo-labeling methods like RegionCLIP [85] and SAS-Det [84] enhance region-text alignment by generating pseudo-labels for image-text pairs using VLMs or selftraining techniques. Region-Aware Training methods, exemplified by CORA [70], improve the object classification accuracy of CLIP by learning region prompts. Transfer Learning-Based methods [14, 17, 3032, 42, 65, 68, 77, 78] utilize the image encoder of VLM as feature extractor and exclusively train lightweight task-specific components. These methods have become mainstream in open-vocabulary dense prediction due to their broad applicability. While leveraging VLMs as feature extractors offers significant advantages due to their comprehensive pre-training, directly applying these image-level models to dense prediction tasks often results in domain shift issues [68, 70], thereby limiting their performance. In this paper, we integrate DeCLIP into transfer learning-based object detection baselines F-ViT and OV-DQUO, as well as the image segmentation baseline CATSeg, to enhance their performance in open-vocabulary dense prediction tasks. 10.2. Transferring VLMs to Dense Prediction Tasks As VLMs [52, 61] were initially trained on image-text pairs, the direct application of these image-level models to dense prediction tasks, which require region-level or pixel-level semantic understanding, results in significant performance degradation. Several studies have attempted to address this limitation through fine-tuning strategies. These approaches can be broadly categorized into joint fine-tuning and prefine-tuning approaches. Joint fine-tuning methods fine-tune CLIP while training task-specific components [14, 30, 31, 39, 42, 72, 77]. For instance, CAT-Seg [14] proposes an attention fine-tuning strategy based on ViT CLIP, which generalizes well to unseen categories. MAFT [30] leverages attention bias to finetune CLIP for mask classification. Pre-fine-tuning methods directly fine-tune CLIP using cost-efficient techniques [49, 6870, 85]. For instance, CLIM [69] employs mosaic augmentation technique to stitch multiple images into single image, enabling each sub-image to serve as pseudo-region for region-text contrastive learning. CLIPSelf [68] enhances CLIPs region classification accuracy by maximizing cosine similarity between its region representations and the corresponding image crop representations. Despite the promising results of the two categories of fine-tuned methods, they continue to exhibit certain limitations. In contrast to these studies, we conduct an analysis of CLIP and identify that its limitation in open-vocabulary"
        }
    ],
    "affiliations": [
        "International Research Institute for Artificial Intelligence, HIT, Shenzhen",
        "School of Computer Science and Technology, HIT, Shenzhen",
        "University of Chinese Academy of Sciences"
    ]
}