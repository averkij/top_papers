{
    "paper_title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation",
    "authors": [
        "Eunsu Kim",
        "Haneul Yoo",
        "Guijin Son",
        "Hitesh Patel",
        "Amit Agarwal",
        "Alice Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, a dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering a critical infrastructure for advancing LLM evaluation research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 2 8 4 0 0 . 6 0 5 2 : r BENCHHUB: Unified Benchmark Suite for Holistic and Customizable LLM Evaluation Eunsu Kim1,, Haneul Yoo1,, Guijin Son 2,3, Hitesh Patel 4, Amit Agarwal 4, Alice Oh 1 1KAIST, 2Yonsei University, 3OnelineAI, 4Oracle kes0317@kaist.ac.kr, haneul.yoo@kaist.ac.kr, alice.oh@kaist.edu"
        },
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) continue to advance, the need for up-to-date and well-organized benchmarks becomes increasingly critical. However, many existing datasets are scattered, difficult to manage, and make it challenging to perform evaluations tailored to specific needs or domains, despite the growing importance of domain-specific models in areas such as math or code. In this paper, we introduce BenchHub, dynamic benchmark repository that empowers researchers and developers to evaluate LLMs more effectively. BenchHub aggregates and automatically classifies benchmark datasets from diverse domains, integrating 303K questions across 38 benchmarks. It is designed to support continuous updates and scalable data management, enabling flexible and customizable evaluation tailored to various domains or use cases. Through extensive experiments with various LLM families, we demonstrate that model performance varies significantly across domain-specific subsets, emphasizing the importance of domain-aware benchmarking. We believe BenchHub can encourage better dataset reuse, more transparent model comparisons, and easier identification of underrepresented areas in existing benchmarks, offering critical infrastructure for advancing LLM evaluation research. Dataset & Website Code https://huggingface.co/BenchHub https://github.com/rladmstn1714/BenchHub Figure 1: The concept of BENCHHUB. BENCHHUB automatically classifies and merges questions from existing benchmark datasets on sample-wise basis. Through BENCHHUB, users can select test sets that align with their objectives and efficiently evaluate the models. Equal contribution. Preprint."
        },
        {
            "title": "Introduction",
            "content": "As LLMs have made significant strides with remarkable capabilities, multitude of benchmarks have been introduced to assess their performance in different tasks. To comprehensively evaluate these general-purpose LLMs, several initiatives have aimed to provide holistic evaluations of LLMs by integrating multiple benchmarks [32, 41] or employing pairwise user preference as ranking [7]. These efforts are generally seen as providing evaluations aligned with human preferences, but it is often difficult to determine which aspects these holistic evaluation benchmarks assess, and they may not always align precisely with the specific objectives of given task. With the rapid expansion of LLM applications across diverse domains, evaluations tailored to specific objectives have become increasingly important. Existing benchmarks focus on specialized areas, such as legal [31], medical [1], finance [58], as well as specific capabilities, including knowledge retrieval [16], reasoning [10, 77], and value alignment [43, 20]. However, the vast and fragmented nature of evaluation datasets presents significant challenges in identifying benchmarks that are well-suited to particular goals. For instance, users seeking models that perform well in STEM domains often struggle to select the most suitable evaluation set from multiple related datasets (e.g., MATH [17], GSM8K [10]) or face challenges due to their partial inclusion in larger collections (e.g., MMLU [16]). These challenges are further compounded by the computational cost burden of evaluations and the diversity of tasks that LLMs are designed to address. This highlights the need for systematic organization and improved accessibility of benchmarks to facilitate more effective and targeted evaluations. To this end, we present BENCHHUB 2 , unified benchmark suite for holistic and customizable LLM evaluation. Spanning diverse domains, BENCHHUB incorporates total of 303K questions from 38 benchmarks, including both English and Korean datasets. All 303K questions are categorized based on skills (e.g., knowledge and reasoning), subjects (e.g., mathematics), and targets (e.g., culturally specific or agnostic). This categorization allows users to filter test sets according to specific scenarios, enabling the selection of customized evaluation sets effectively (Figure 1). In addition, we train and release categorization model based on Qwen-2.5-7b, automating the entire process, which ensures dynamic scalability to accommodate new datasets. Using the dataset from BENCHHUB, in 4, we conduct experiments on models belonging to seven families. The results demonstrate that 1) model rankings vary significantly across different subject categories and 2) the outcomes can be heavily influenced by the distribution of subject types included in the overall test set. These findings highlight that the distribution of datasets in existing benchmarks can significantly impact the interpretation of model performance. This underscores the importance of BENCHHUB, which enables domain-aware evaluation. We also call on researchers and practitioners to carefully consider benchmark composition when evaluating LLMs to ensure fair and meaningful assessments."
        },
        {
            "title": "2 Existing LLM Evaluation Benchmarks are Skewed",
            "content": "(a) Subject (b) Task Figure 2: Data distribution of existing evaluation benchmarks. What aspects do the commonly used multi-domain datasets evaluate, and how is the distribution of domains represented across these datasets? To answer this question, we classify three representative 2We include the datasets and results in https://huggingface.co/BenchHub holistic benchmarks (i.e., Chatbot Arena [7], MixEval [41], and MMLU [16]) as multilabels using our fine-tuned classifiers ( 3) in terms of coarse-grained subjects (Figure 2a) and tasks (Figure 2b). Among them, Chatbot Arena includes only 25.5% of Humanities and Social Sciencce (HASS) questions, while both MixEval and MMLU comprise more than half of HASS questions. Also, MixEval includes less than 0.30% of value alignment tasks and mostly focuses on measuring knowledge. Such disparities may lead to biased findings, where models that excel in certain domains may appear to perform better overall, potentially skewing the evaluation results. Moreover, these biases are not limited to crossbenchmark comparisons but can also manifest within multilingual contexts. Figure 3 and Figure 9 illustrate data distributions of MMLU series datasets in 5 languages classified by the model ( 3) in terms of coarse-grained subjects. For instance, MMLU in English emphasizes HASS, whereas Korean MMLU (KMMLU) [60] comprises 76.1% of STEM (Science, Technology, Engineering, and Mathematics) questions. This variation complicates the interpretation of performance differences, as it is challenging to discern whether the performance degradations in non-English are due to language proficiency or domain-specific knowledge. Figure 3: Data distribution of MMLU series in English, Korean, Japanese, Indonesian, and Chinese, respectively Hence, instead of the reckless adoption of existing holistic benchmarks, it is recommended to carefully select the benchmark suites for reliable evaluation."
        },
        {
            "title": "3 BENCHHUB",
            "content": "Consider user who wants to determine Which model excels at both mathematics and understanding culture? Which evaluation datasets should the user select to assess models on these specific dimensions? As discussed in 2, while previous evaluation benchmarks [16, 32, 41] aim to assess models general capabilities across various domains, it remains unclear what the different scores they provide specify or whether they align with the users specific objectives. To address this, we introduce BENCHHUB, unified collection of benchmarks across diverse domains. We support two languagesEnglish and Koreanthrough BENCHHUB-EN and BENCHHUB-KO, respectively. Together, they cover total of 38 benchmarks, reclassified at the sample level according to defined taxonomy, enabling users to select appropriate evaluation sets based on their specific intentions. We fully automate our process, ensuring the BENCHHUB remains dynamic and expandable as new datasets emerge. In this section, we outline the overall pipeline, including the taxonomy and datasets ( 3.1), as well as their implementation ( 3.2). 3.1 Taxonomy 3.1.1 Category Taxonomy We define important attributes of evaluation benchmarks in our taxonomy and classify datasets from various domains according to this taxonomy. Our taxonomy includesTask and Answer Format, which are assigned based on the dataset, andskill, subject, and target type, which are assigned based on each question. We include the complete category taxonomy and its descriptions in Appendix C. 1) Task refers to the high-level classification of the task associated with dataset, as defined by the authors of benchmarks. It represents the general type of task the dataset is designed to evaluate (e.g., Mathematical Reasoning, Code generation, Cultural Understanding). This classification is automatically assigned based on the datasets abstract or description and is determined through inference using LLM. 2) Answer Format refers to the format in which the response is expected, such as binary, MCQA (Multiple Choice Question Answer), short-form, free-form, open-ended (e.g., story generation), 3 and comparison (e.g, determining which response is better between and B). This is particularly important when determining the test prompt or format used during the evaluation phase, as it dictates how the response will be structured. 3) Skill represents the abilities or skills required to answer the question, such as reasoning, knowledge, or value/alignment. It categorizes the level and type of processing necessary to solve the task. 4) Subject refers to the domain of knowledge required to answer query. Examples include categories such as Math, Coding, or Food. We define six coarse-grained categories: Science, Technology, Humanities and Social Science (HASS), Arts & Sports, Culture, and Social Intelligence, along with 64 sub-categories. These categories are derived by integrating various knowledge classification systems and sources and aligning them with common tasks relevant to LLMs, including additional categories like Bias, Commonsense, Norms, and Values. Currently, we classify samples into 64 distinct subject types. Each sample may have multiple subject labels. 5) Target represents the cultural or geographical focus of the query. Questions that are not related to culture or specific regions are classified as General, while others are classified as Local with specific target type, such as KO (Korea) or US (United States). This classification system is especially important given the growing need to evaluate tasks in different cultural and regional contexts [56]."
        },
        {
            "title": "3.1.2 Datasets",
            "content": "(a) English (b) Korean Figure 4: Data distribution of all datasets used in this paper by coarse-grained subjects, targets, and tasks. The English and Korean data include 158,209 and 144,331 questions each. (a) English (b) Korean Figure 5: Fine-grained data distribution of all datasets used in this paper in terms of subjects Figure 2 and Figure 5 show the overall statistics of the datasets included in our benchmark. We include 27 English and 13 Korean language benchmarks, with total of 38 datasets 3. 3We count two datasetsBLEnD [39] and CaLMQA [2]in both language benchmarks, as they contain both languages. 4 Dataset Collection As culture-specific evaluation becomes increasingly important, we collect benchmark datasets in two types: general-purpose (i.e., culturally agnostic) and culture-specific. For general-purpose English datasets, we refer to those commonly used in existing holistic evaluation benchmarks [75, 41]. For culture-specific datasets, we refer to recent benchmark survey of approximately 300 culture-relevant papers [44] and select datasets that include English and span multiple cultures. For Korean, since fewer datasets are available compared to English datasets, we include most datasets released after 2022. Table 2 in the Appendix provides complete list of the datasets we include. 3.2 Dynamic and Expansive Nature of BENCHHUB With benchmark datasets emerging at rapid pace, it is crucial to flexibly manage them for holistic evaluation. To dynamically adapt to newly emerging datasets, we automate the entire dataset merging process using an LLM agent, which includes reformatting the datasets into our benchmark format and classifying each sample into categories. The processing pipeline for newly introduced dataset is outlined as follows: 1. Reformatting: We first automatically reformat the dataset into our benchmark format via an LLM-guided rule-based approach. If the dataset does not adhere to our predefined schema, an LLM agent (e.g., GPT-4o or Gemini) is employed to map keys to the correct format. 2. Metadata assignment: The LLM agent extracts the meta-task description from the dataset documentation (e.g., paper abstract) and infers the answer format based on reference answer type, option(e.g., A/B/C or D) availability, option count, and using few-shot samples of the dataset. 3. Sample-level Classification: We then classify individual question samples according to skill, subject, and target type. Given the large sample volume, we train the Qwen-2.5-7B models and release BenchHub-Cat-7B 4 to enable efficient large-scale categorization 5. The categorizer simultaneously classifies all sample-wise categories (subject, target, and skill) for given question sample. 4. Merging: The newly processed dataset is merged with existing datasets, thereby producing an updated version of BENCHHUB. The automatic process of BENCHHUB enables it to progressively expand and provide more comprehensive evaluations as new datasets are added. We aim to support evaluations that align with users intents through regular updates."
        },
        {
            "title": "4 Evaluation Results using BENCHHUB",
            "content": "4.1 Evaluation of LLMs across diverse subjects Figure 6: LLM evaluation ranking under BENCHHUB in terms of coarse-grained subjects In this section, we evaluate seven LLMs across diverse subjects using BENCHHUB. We select 6,644 and 6,485 examples for English and Korean, respectively. To manage the large number of fine-grained 4This model is publicly available via huggingface: BenchHub/BenchHub-Cat-7b 5Details on the training method and the validation accuracy of BenchHub-Cat-7B are provided in Appendix D.1. categories, we sample up to 150 examples per category, fully including categories with 100150 samples and merging categories with fewer than 80 samples into miscellaneous group within the same coarse-grained classification. For evaluation, we extract the models intended answer from MCQA questions by applying set of regular expressions [38], while using an LLM as parser extractor for short-form questions 6, similar to the approach in previous work [41]. We include one representative model from each commonly used LLM family. For proprietary models, we use GPT-4.1, Gemini-2.0-flash, and Claude 3.7 Sonnet 7. Open models include Qwen-3-32b [72], DeepSeek-R1-Distill-Qwen-32B [11], Llama-3.3-70B [14], Mistral-Small-24B-Instruct, and gemma2-27b-it [65]. Figure 6 presents model rankings by subject category. Our results show that frequent fluctuations in model rankings depend on the category. For example, Llama-3.3-70b ranks 6th in Science and Tech, but ranks as the top-performing model among seven models in Culture and Social Intelligence. This highlights the importance of domain-specific evaluation aligned with the evaluation context and objectives. The full results regarding the scores for each subject and model are in Table 910 in the Appendix F. 4.2 Impact of Category Distribution on Model Ranking In this section, we empirically validate the influence of category distributions within evaluation benchmarks on model rankings. Since this requires experiments on large datasets for statistical validation, we include 14 open models ranging from 1B to 72B parameters. We test on 27 English and 13 Korean datasets, comprising 16,898 and 18,977 MCQA samples, respectively. The number of answer choices per MCQA sample varies between 3 and 18. We extract the models intended answer by applying set of regular expressions [38]. The evaluated LLMs include: Qwen [73, 72]: Qwen2.5-72B-Instruct, Qwen3-1.7B, Qwen3-4B, Qwen3-8B, Qwen3-14B, Qwen3-32B DeepSeek [11]: DeepSeek-R1-Distill-Qwen-14B, DeepSeek-R1-Distill-Qwen-32B Llama [14]: Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct Mistral: Mistral-Small-24B-Instruct-2501 Gemma [65]: gemma-3-1b-it, gemma-3-4b-it, gemma-3-27b-it To gauge the impact of data composition, we experiment under three sampling strategies with four setups, which are representatives of traditional approaches or emerging trends in LLM evaluations with massive benchmark scale. Random sampling: Samples are drawn uniformly at random from the entire dataset collection, disregarding category proportions. Each sample has an equal chance of selection. Stratified sampling: Samples are drawn to ensure equal representation from each constituent dataset, preserving dataset-level balance rather than the overall distribution. Sampling according to category distribution: This strategy performs stratified sampling guided by fine-grained category distributions observed in existing holistic LLM benchmarks. In particular, we adopt the distributions derived from Chatbot Arena and MixEval, classified by our fine-tuned model ( 3.2). The coarse-grained category distributions of these benchmarks are detailed in 2. We run 50 simulations per sampling setup, each selecting 5K questions. Model rankings within each setup follow normal distributions. Figure 7 visualizes LLM ranking changes across the four sampling setups. We use the Friedman test and the pairwise Wilcoxon test to statistically identify whether the sampling strategy affects the model ranking based on average accuracy. We observe statistically significant difference across sampling strategies using the Friedman test (p < 0.01). Specifically, pairwise Wilcoxon signed-rank tests confirm that all pairs of sampling setups significantly differ in 6We use GPT-4.1-nano as parser extractor. Note that [41] use GPT-3.5. The LLM parses and compares the extracted answer with the ground truth, without assessing answer quality. 7For GPT-4.1, we use GPT-4.1-2025-04-14 version. We directly call GPT-4.1 via the OpenAI API, while we use OpenRouter for Gemini-2.0-flash, and Claude 3.7 Sonnet. 6 Figure 7: LLM ranking according to four sampling methods average, except for random sampling versus sampling according to MixEval distribution (p < 0.01). These findings underscore that category distribution and sampling strategy of data substantially affect LLM leaderboard rankings. We call on researchers and practitioners to carefully consider benchmark composition when evaluating LLMs. 4.3 Customized BENCHHUB In this section, we showcase how customized benchmark composition using BENCHHUB enables more targeted and meaningful evaluations tailored to real-world application scenarios. Here, we consider two use cases illustrated in Figure 1, and construct corresponding customized BENCHHUB as follows: (a) STEM knowledge evaluation: To identify the best-performing model with expertise in STEM domains, we select English datasets within BENCHHUB whose coarse-grained subjects are labeled as Science or Technology. To ensure balanced representation across individual datasets, the questions are drawn using stratified sampling strategy at dataset level. (b) Math teaching agent for Korean students: To evaluate Math teaching agents, we select Korean datasets comprising 1) math-related samples (i.e., fine-grained categories are Science/Math or Science/Statistics), 2) education-related samples (i.e., fine-grained category is HASS/Education), and 3) samples culturally specific to Korea (i.e., target as KO). The final accuracy is computed as weighted average of these subsets, with weights of 0.6, 0.1, and 0.3, respectively, reflecting their relative importance to the application. Table 1: Top-5 LLMs evaluated by BENCHHUB in real-world application scenarios Rank (a) STEM knowledge evaluation (EN) Customized 1 2 3 4 Qwen3-32B gemma-3-1b-it Qwen3-1.7B Qwen3-4B DeepSeek-R1-Distill-Qwen-14B Stratified gemma-3-1b-it Qwen3-32B Qwen3-4B Qwen3-1.7B gemma-3-4b (b) Math teaching agent for Korean students (KO) Customized Stratified Qwen2.5-72B-Instruct Mistral-Small-24B-Instruct-2501 gemma-3-27b-it Llama-3.3-70B-Instruct Qwen2.5-72B-Instruct Llama-3.3-70B-Instruct gemma-3-27b Mistral-Small-24B-Instruct-2501 DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Qwen-32B Table 1 presents the detailed accuracy scores and rankings of LLMs under these customized benchmarks. We use the same set of models described in 4.2. Notably, the model rankings differ substantially depending on the benchmark compositions, underscoring the practical need for tailored evaluations."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Generalization and Adaptation of BENCHHUB This section guides extending and applying our framework to other languages and domains. While BENCHHUB provides benchmark systems for English and Korean, our method supports flexible expansion. Multilingual Extension To extend BENCHHUB to additional languages, researchers should compile benchmark lists relevant to the target language and apply the automated pipeline described in this work. For low-resource languages, further training of the categorizer may be necessary to achieve satisfactory performance, following the procedure outlined in D.1. Domain-Specific Extension The framework also facilitates adaptation to specific domains by defining refined subcategories within given domain (e.g., medical). Subsequently, domain-specific datasets should be collected, and the categorizer retrained accordingly, as described in D.1. This process enables more granular and domain-focused evaluation. We hope BENCHHUB extension across diverse languages and domains will enable efficient, holistic, and domain-specific evaluation. 5.2 Discussion on the Automatic Categorization We examine and discuss the influence of categorization accuracy on model evaluation outcomes in BENCHHUB. To quantify and simulate the categorizing errors, we conduct an ablation study in which the categorization error rate is systematically varied and controlled. Following the experimental setups described in 4.2, we employ stratified sampling strategy to preserve dataset-level balance across categories. We introduce controlled corruption rate, which denotes the proportion of misclassified samples in the test set. We increment the corruption rate from 0.0% to 10.0% in 0.5% steps. For each corruption level, we perform 50 independent simulation runs to ensure statistical robustness. We compare the model rankings obtained from the corrupted test sets to the baseline rankings derived from the original, uncorrupted set. We demonstrate that categorization errors up to 1.5% yield negligible disruption to model rankings, confirmed by Spearmans rank correlation coefficient and Wilcoxon Signed-Rank test. This finding suggests notable resilience of the evaluation framework to minor categorization inaccuracies. It is noteworthy that this robustness extends beyond simple misclassification scenarios to dynamic, real-world settings tailored for users. Introducing small fraction of samples comprising undefined categories is less likely to cause significant shifts in model rankings. Moreover, the categorizer can be incrementally updated and improved through continual learning, ensuring ongoing adaptation and maintenance of BENCHHUB pipeline among evolving benchmarks."
        },
        {
            "title": "6 Related Work",
            "content": "As LLMs have become integral to real-world generative AI systems, the historical focus on benchmarks and leaderboards has matured into evaluation science [71]. While LLM evaluation benchmarks primarily adopt question-answering task as default evaluation format, they have expanded their capabilities into diverse tasks, including long-form generation [37], multilingual [56, 53], multimodal [13], and complex reasoning tasks [10, 77], inter alia. This diversification reflects growing recognition of the multifaceted capabilities and applications of LLMs. Beyond general-purpose benchmarks, there has been surge in domain-specific evaluation benchmarks targeting verticals such as healthcare and medicine [18, 35, 47], law [31], science [12], and financial [78, 57]. These benchmarks enable more targeted assessment aligned with the unique requirements and challenges of each field. Complementing this trend, several large-scale benchmarks now aggregate tasks across multiple domains to facilitate robust, holistic evaluation of LLMs [16, 70, 63, 69]. Meta-analyses and surveys [49, 34] have also established guidelines and checklists, with the aim of improving benchmarking practices and reproducibility. While these static benchmarks have driven significant progress, recent studies have identified inherent limitations of static datasets. Notably, issues such as data contamination, model overfitting to benchmarks, and insufficient human alignments have been highlighted [74, 42]. This has spurred 8 calls for new discipline of model metrology focused on dynamic, adaptive, and robust evaluation frameworks [52]. Accordingly, several dynamic and live benchmarks have emerged, including DynaBench [22], Chatbot Arena [7], MixEval [41], and YourBench [54]. Moreover, Task-MeAnything [79], benchmark generation engine, enabled customization in multimodal benchmarks tailored to the users needs. In line with these efforts, recent studies have shed light on the diversity of scenarios, contexts, and metrics in holistic evaluations. For example, [66] critiqued over-reliance on single leaderboard rankings for evaluating AI fairness, advocating for multi-dimensional measurements. Similarly, [32] reformulated existing benchmarks into format of diverse scenarios and adopted multiple metrics for truly holistic assessment. Fine-grained evaluations, such as decomposing coarse scoring into skill-level scoring for alignment [75], facilitate richer and interpertable results. These advancements collectively underscore paradigm shift from narrow, static benchmarks toward customizable, multifaceted evaluations that better reflect the complex real-world capabilities and risks of LLMs."
        },
        {
            "title": "7 Conclusion",
            "content": "The rapid advancements in large language models (LLMs) have highlighted the need for robust and comprehensive evaluation frameworks capable of addressing the diverse and expanding range of their applications. While existing benchmarks have provided valuable insights into specific domains and capabilities, the fragmented nature of these datasets and the lack of alignment with task-specific objectives often limit their utility in real-world scenarios. Moreover, the varying distributions of subject types within benchmarks can significantly influence the interpretation of model performance, further emphasizing the need for systematic and customizable evaluation methodologies. In this work, we introduced BENCHHUB, unified benchmark suite designed to address these challenges. By categorizing 303K questions from 38 benchmarks across skills, subjects, and targets, BENCHHUB enables users to filter and create tailored test sets for domain-aware and task-specific evaluations. The integration of categorization model based on Qwen-2.5-7b automates this process, ensuring scalability and adaptability to new datasets. Our experiments demonstrated that model performance rankings can vary significantly depending on subject categories and dataset distributions, underscoring the critical role of benchmark composition in fair and meaningful evaluations. We hope this work promotes domain-aware evaluation and careful benchmark design. BENCHHUB serves as practical tool to support these goals across diverse users. For developers and practitioners, BENCHHUB serves as tool for accurately assessing model capabilities in targeted scenarios. They can identify each models strengths and weaknesses and select the ones best suited to their specific applications. For benchmark and evaluation researchers, we hope that the unified structure of BENCHHUB facilitates comprehensive statistical analysis of the coverage of existing benchmarks across subjects and skills, helping to identify underrepresented areas and motivating the construction of new datasets that address existing gaps in current evaluation practices. Through these contributions, we aim to support the development of more capable and domain-adapted language models."
        },
        {
            "title": "8 Limitations",
            "content": "Incomplete English Dataset Coverage: Due to the vast amount of English-language data, we could not include all relevant datasets in this version of BENCHHUB. While we prioritized widely used and high-quality benchmarks, some important datasets may still be missing. Future iterations will expand coverage for broader inclusivity. Categorization Bias from LLMs: BENCHHUB categorization relies on Qwen-2.5-7b, which may introduce biases due to its training data or modeling limitations. Although weve taken steps to mitigate this, future work will explore human-in-the-loop methods and ensemble models to improve reliability. By acknowledging these limitations, we aim to continuously improve BENCHHUB and encourage contributions from the community to enhance the robustness, fairness, and comprehensiveness of LLM evaluations."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank all the authors of the benchmarks included in BENCHHUB, whose work made our research possible and allowed us to broaden the coverage of our benchmark suite. We also thank the authors of [44] for providing valuable insights and supporting our work by sharing additional statistics on culturally specific benchmarks, which significantly facilitated our study."
        },
        {
            "title": "References",
            "content": "[1] Rahul K. Arora, Jason Wei, Hicks Rebecca Soskin, Preston Bowman, Joaquin QuiñoneroCandela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, and Johannes Heidecke. HealthBench: Evaluating large language models towards improved human health, 2025. [2] Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, and Eunsol Choi. CaLMQA: Exploring culturally specific long-form question answering across 23 languages. arXiv preprint arXiv:2406.17761, 2024. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [4] Axolotl AI. Axolotl: Scalable fine-tuning framework for llms. https://axolotl-ai-cloud. github.io/axolotl/, 2025. Github. [5] Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):74327439, Apr. 2020. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [7] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: an open platform for evaluating LLMs by human preference. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. [8] Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin, Chan Young Park, Shuyue Stella Li, Sahithya Ravi, Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov, Vered Shwartz, et al. CulturalBench: robust, diverse and challenging benchmark on measuring the (lack of) cultural knowledge of LLMs. arXiv preprint arXiv:2410.02677, 2024. [9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [11] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, 10 Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Tu Anh Dinh, Carlos Mullov, Leonard Bärmann, Zhaolin Li, Danni Liu, Simon Reiß, Jueun Lee, Nathan Lerzer, Jianfeng Gao, Fabian Peller-Konrad, Tobias Röddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens Böhm, and Jan Niehues. SciEx: Benchmarking large language models on scientific exams with human expert grading and automatic grading. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1159211610, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2024. [14] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, 12 Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [15] Md Arid Hasan, Maram Hasanain, Fatema Ahmad, Sahinur Rahman Laskar, Sunaya Upadhyay, Vrunda Sukhadia, Mucahid Kutlu, Shammur Absar Chowdhury, and Firoj Alam. NativQA: Multilingual culturally-aligned natural query for LLMs. arXiv preprint arXiv:2407.09823, 2024. [16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and In International Jacob Steinhardt. Measuring massive multitask language understanding. Conference on Learning Representations, 2021. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1, 2021. [18] Niclas Hertzberg and Anna Lokrantz. MedQA-SWE - clinical question & answer dataset for Swedish. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1117811186, Torino, Italia, May 2024. ELRA and ICCL. [19] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger kernel: Efficient triton kernels for llm training. arXiv preprint arXiv:2410.10989, 2024. [20] Jianchao Ji, Yutong Chen, Mingyu Jin, Wujiang Xu, Wenyue Hua, and Yongfeng Zhang. MoralBench: Moral evaluation of LLMs. arXiv preprint arXiv:2406.04428, 2024. [21] Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, and Hwaran Lee. KoBBQ: Korean bias benchmark for question answering. Transactions of the Association for Computational Linguistics, 12:507524, 2024. [22] Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 41104124, Online, June 2021. Association for Computational Linguistics. [23] Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, and Alice Oh. CLIcK: benchmark dataset of cultural and linguistic intelligence in Korean. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 33353346, Torino, Italia, May 2024. ELRA and ICCL. [24] Yeeun Kim, Youngrok Choi, Eunkyung Choi, JinHwan Choi, Hai Jin Park, and Wonseok Hwang. Developing pragmatic benchmark for assessing Korean legal language understanding in large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings 13 of the Association for Computational Linguistics: EMNLP 2024, pages 55735595, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [25] Hyunwoo Ko, Guijin Son, and Dasol Choi. Understand, solve and translate: Bridging the multilingual mathematical reasoning gap. arXiv preprint arXiv:2501.02448, 2025. [26] Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. [27] Sunjun Kweon, Byungjin Choi, Gyouk Chu, Junyeong Song, Daeun Hyeon, Sujin Gan, Jueon Kim, Minkyu Kim, Rae Woong Park, and Edward Choi. KorMedMCQA: multi-choice question answering benchmark for korean healthcare professional licensing examinations. arXiv preprint arXiv:2403.01469, 2024. [28] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. [29] Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Gunhee Kim, and Jung-woo Ha. KoSBI: dataset for mitigating social bias risks towards safer large language model applications. In Sunayana Sitaram, Beata Beigman Klebanov, and Jason Williams, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 208224, Toronto, Canada, July 2023. Association for Computational Linguistics. [30] Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won, Hwaran Lee, and Edward Choi. KorNAT: LLM alignment benchmark for Korean social values and common knowledge. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1117711213, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [31] Haitao Li, Junjie Chen, Jingli Yang, Qingyao Ai, Wei Jia, Youfeng Liu, Kai Lin, Yueyue Wu, Guozhi Yuan, Yiran Hu, et al. LegalAgentBench: Evaluating LLM agents in legal domain. arXiv preprint arXiv:2412.17259, 2024. [32] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher Manning, Christopher Re, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. Featured Certification, Expert Certification, Outstanding Certification. [33] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252, Dublin, Ireland, May 2022. Association for Computational Linguistics. [34] Rachel Longjohn, Markelle Kelly, Sameer Singh, and Padhraic Smyth. Benchmark data repositories for better benchmarking. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8643586457. Curran Associates, Inc., 2024. [35] João Matos, Shan Chen, Siena Kathleen V. Placino, Yingya Li, Juan Carlos Climent Pardo, Daphna Idan, Takeshi Tohyama, David Restrepo, Luis Filipe Nakayama, José María Millet Pascual-Leone, Guergana Savova, Hugo Aerts, Leo Anthony Celi, An-Kwok Ian Wong, Danielle Bitterman, and Jack Gallifant. WorldMedQA-V: multilingual, multimodal medical examination dataset for multimodal language models evaluation. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, pages 72037216, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. [36] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. [37] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore, December 2023. Association for Computational Linguistics. [38] Francesco Maria Molfese, Luca Moroni, Luca Gioffrè, Alessandro Scirè, Simone Conia, and Roberto Navigli. Right answer, wrong score: Uncovering the inconsistencies of llm evaluation in multiple-choice question answering. arXiv preprint arXiv:2503.14996, 2025. [39] Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, Víctor GutiérrezBasulto, Yazmín Ibáñez García, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, and Alice Oh. BLEnD: benchmark for llms on everyday knowledge in diverse cultures and languages. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 7810478146. Curran Associates, Inc., 2024. [40] Tuan-Phong Nguyen, Simon Razniewski, Aparna Varde, and Gerhard Weikum. Extracting In Proceedings of the ACM Web Conference cultural commonsense knowledge at scale. 2023, WWW 23, page 19071917, New York, NY, USA, 2023. Association for Computing Machinery. [41] Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. MixEval: Deriving wisdom of the crowd from LLM benchmark mixtures. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 9818098212. Curran Associates, Inc., 2024. [42] Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test set contamination in black-box language models. In The Twelfth International Conference on Learning Representations, 2024. [43] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: hand-built bias benchmark for question answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 20862105, Dublin, Ireland, May 2022. Association for Computational Linguistics. [44] Siddhesh Pawar, Junyeong Park, Jiho Jin, Arnav Arora, Junho Myung, Srishti Yadav, Faiz Ghifari Haznitrama, Inhwa Song, Alice Oh, and Isabelle Augenstein. Survey of cultural awareness in language models: Text and beyond. arXiv preprint arXiv:2411.00860, 2024. [45] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [46] Abhinav Sukumar Rao, Akhila Yerukola, Vishwa Shah, Katharina Reinecke, and Maarten Sap. NormAd: framework for measuring the cultural adaptability of large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 23732403, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. [47] Rajat Rawat, Hudson McBride, Rajarshi Ghosh, Dhiyaan Nirmal, Jong Moon, Dhruv Alamuri, Sean OBrien, and Kevin Zhu. DiversityMedQA: benchmark for assessing demographic biases in medical diagnosis using large language models. In Daryna Dementieva, Oana Ignat, Zhijing Jin, Rada Mihalcea, Giorgio Piatti, Joel Tetreault, Steven Wilson, and Jieyu Zhao, editors, Proceedings of the Third Workshop on NLP for Positive Impact, pages 334348, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [48] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof Q&A benchmark. In First Conference on Language Modeling, 2024. [49] Anka Reuel, Amelia Hardy, Chandler Smith, Max Lamparth, Malcolm Hardy, and Mykel J. Kochenderfer. BetterBench: Assessing AI benchmarks, uncovering issues, and establishing best practices. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 2176321813. Curran Associates, Inc., 2024. [50] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: an adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, August 2021. [51] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Social IQa: Commonsense reasoning about social interactions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. [52] Michael Saxon, Ari Holtzman, Peter West, William Yang Wang, and Naomi Saphra. Benchmarks as microscopes: call for model metrology. In First Conference on Language Modeling, 2024. [53] Sheikh Shafayat, Eunsu Kim, Juhyun Oh, and Alice Oh. Multi-fact: Assessing factuality of multilingual llms using factscore, 2024. [54] Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskia, Thomas Wolf, Gokhan Tur, and Dilek Hakkani-Tür. Yourbench: Easy custom evaluation sets for everyone. arXiv preprint arXiv:2504.01833, 2025. [55] Weiyan Shi, Ryan Li, Yutong Zhang, Caleb Ziems, Sunny Yu, Raya Horesh, Rogério Abreu De Paula, and Diyi Yang. CultureBank: An online community-driven knowledge base towards culturally aware language technologies. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 49965025, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [56] Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Sebastian Ruder, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and Sara Hooker. Global MMLU: Understanding and addressing cultural and linguistic biases in multilingual evaluation. arXiv preprint arXiv:2412.03304, 2024. [57] Guijin Son, Hyunjun Jeon, Chami Hwang, and Hanearl Jung. KRX bench: Automating financial benchmark creation via large language models. In Chung-Chi Chen, Xiaomo Liu, Udo Hahn, Armineh Nourbakhsh, Zhiqiang Ma, Charese Smiley, Veronique Hoste, Sanjiv Ranjan Das, Manling Li, Mohammad Ghassemi, Hen-Hsen Huang, Hiroya Takamura, and Hsin-Hsi Chen, editors, Proceedings of the Joint Workshop of the 7th Financial Technology and Natural Language Processing, the 5th Knowledge Discovery from Unstructured Data in Financial Services, 16 and the 4th Workshop on Economics and Natural Language Processing, pages 1020, Torino, Italia, May 2024. Association for Computational Linguistics. [58] Guijin Son, Hanearl Jung, Moonjeong Hahm, Keonju Na, and Sol Jin. Beyond classification: Financial reasoning in state-of-the-art language models. arXiv preprint arXiv:2305.01505, 2023. [59] Guijin Son, Hyunwoo Ko, and Dasol Choi. Multi-step reasoning in Korean and the emergent mirage. In Vinodkumar Prabhakaran, Sunipa Dev, Luciana Benotti, Daniel Hershcovich, Yong Cao, Li Zhou, Laura Cabello, and Ife Adebara, editors, Proceedings of the 3rd Workshop on Cross-Cultural Considerations in NLP (C3NLP 2025), pages 1021, Albuquerque, New Mexico, May 2025. Association for Computational Linguistics. [60] Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. KMMLU: Measuring massive multitask In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, language understanding in Korean. Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 40764104, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. [61] Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jae cheol Lee, Je Won Yeom, Jihyu Jung, Jung woo Kim, and Songseong Kim. HAE-RAE bench: Evaluation of Korean knowledge in language models. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 79938007, Torino, Italia, May 2024. ELRA and ICCL. [62] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada, July 2023. Association for Computational Linguistics. [63] Saeid Asgari Taghanaki, Aliasgahr Khani, and Amir Khasahmadi. MMLU-Pro+: Evaluating higher-order reasoning and shortcut learning in llms. arXiv preprint arXiv:2409.02257, 2024. [64] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [65] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [66] Angelina Wang, Aaron Hertzmann, and Olga Russakovsky. Benchmark suites instead of leaderboards for evaluating AI fairness. Patterns, 5(11):101080, 2024. [67] Bin Wang, Zhengyuan Liu, Xin Huang, Fangkai Jiao, Yang Ding, AiTi Aw, and Nancy Chen. SeaEval for multilingual foundation models: From cross-lingual alignment to cultural reasoning. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 370390, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [68] Xiaonan Wang, Jinyoung Yeo, Joon-Ho Lim, and Hansaem Kim. KULTURE Bench: benchmark for assessing language model in Korean cultural context. arXiv preprint arXiv:2412.07251, 2024. [69] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 50855109, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [70] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-Pro: more robust and challenging multi-task language understanding benchmark. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 9526695290. Curran Associates, Inc., 2024. [71] Laura Weidinger, Inioluwa Deborah Raji, Hanna Wallach, Margaret Mitchell, Angelina Wang, Olawale Salaudeen, Rishi Bommasani, Deep Ganguli, Sanmi Koyejo, and William Isaac. Toward an evaluation science for generative AI systems. arXiv preprint arXiv:2503.05336, 2025. [72] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, 18 Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [73] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [74] Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph Gonzalez, and Ion Stoica. Rethinking benchmark and contamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850, 2023. [75] Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. FLASK: Fine-grained language model evaluation based on alignment skill sets. In The Twelfth International Conference on Learning Representations, 2024. [76] Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li, and Kai-Wei Chang. GeoMLAMA: Geo-diverse commonsense probing on multilingual pre-trained language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 20392055, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [77] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can machine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy, July 2019. Association for Computational Linguistics. [78] Bing Zhang, Mikio Takeuchi, Ryo Kawahara, Shubhi Asthana, Md. Maruf Hossain, GuangJie Ren, Kate Soule, Yifan Mai, and Yada Zhu. Evaluating large language models with enterprise benchmarks. In Weizhu Chen, Yi Yang, Mohammad Kachuee, and Xue-Yong Fu, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track), pages 485505, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. [79] Jieyu Zhang, Weikai Huang, Zixian Ma, Oscar Michel, Dong He, Tanmay Gupta, Wei-Chiu Ma, Ali Farhadi, Aniruddha Kembhavi, and Ranjay Krishna. Task me anything. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 1996519974. Curran Associates, Inc., 2024."
        },
        {
            "title": "A BENCHHUB Web Interface",
            "content": "We manage all code, datasets, models, and demo via Huggingface at https://huggingface.co/BenchHub. In this repository, we release: 1) the complete datasets, 2) useful codes (e.g., load and preprocess dataset), 3) the interactive web interface, and 4) our categorizer model. We provide BENCHHUB web interface8 to enable users to interactively explore available datasets and identify those that best suit their needs. It also supports the continuous addition and management of new data. Through submission form, new datasets can be detected and automatically added. To achieve these, we provide three main functions, as shown in Figure 8. 1) BenchHub Distribution (Figure 8a) This feature offers comprehensive statistics of all datasets we have. Users can interactively explore the overall data distribution they are interested in. Additionally, it provides researchers with insights into which datasets are currently lacking and which evaluations have not yet been conducted. 2) Customizing BenchHub (Figure 8b) This allows users to access sample lists and statistics for selected categories. By reviewing samples, users can verify whether the dataset matches their needs and explore datasets suitable for their purposes. Users can also download the entire set corresponding to the samples.9 3) Submitting New Dataset (Figure 8c) To facilitate the addition of new datasets, We provide submission section to input the Dataset Name, Huggingface URL, and Metadata/Descriptions. Based on this information, the author decides whether to add the dataset to BENCHHUB. 8Our interface is served via Huggingface Space (https://huggingface.co/spaces/BenchHub/ BenchHub). 9Additional customizing features, such as fine-grained category adjustments and interactive control of category proportions via the platform (e.g., adjusting the ratio between reasoning and knowledge questions), are to be developed. 20 (a) BENCHHUB Distribution (b) BENCHHUB Distribution (c) BENCHHUB Distribution Figure 8: User Interface of BENCHHUB Web Demo"
        },
        {
            "title": "B List of Datasets Used",
            "content": "Dataset Reference Target Lang. # of Samples License Table 2: Benchmarks Included in Our Benchmark cc-by-sa 4.0 3,548 1,954 cc-0 1,767 Apache-2.0 1,769 Apache-2.0 10,557 Apache-2.0 817 Apache-2.0 1,000 Apache-2.0 14,042 MIT 58,492 3,084 Apache-2.0 1,140 MIT 6,261 MIT 4,521 MIT 164 MIT 974 cc-by-4.0 cc-by-4.0 1,319 MIT 1,191 3,435 6,134 275 500 124 7,899 cc-by-4.0 cc-by-nc-sa-4.0 cc-by-4.0 cc-by-nc-4.0 cc-by-4.0 unknown cc-by-4.0 22,990 MIT 96 MIT 4,132 1,000 24 3,304 3,009 30,499 cc-by-sa-4.0 cc-by-sa-4.0 cc-by-nc-2.0 cc-by-nc-4.0 cc-by-nc-2.0 cc-by-nd-4.0 8,011 MIT 81,128 MIT 3,584 Apache-2.0 4,900 1,995 cc-by-nc-nd-4.0 cc-by-nd-4.0 100 Apache-2. 6,801 MIT ARC SocialIQA WinoGrande Natural Questions (open) NarrativeQA TruthfulQA Open-BookQA MMLU BBQ PIQA CommonsenseQA BBH MATH HumanEval MBPP GSM8k GPQA MultiNativQA CulturalBench SeaEval CANDLE CCSK GeoMLAMA NormAd CultureBank CaLMQA BLEnD BLEnD KorNAT KBL KorMedMCQA KMMLU HRM8K KoBBQ KULTURE Bench HAE-RAE Bench CLIcK HRMCR KoSBi [9] [51] [50] [28] [26] [33] [36] [16] [43] [5] [64] [62] [17] [6] [3] [10] [48] [15] [8] [67] [40] [76] [46] [55] [2] [39] [39] [30] [24] [27] [60] [25] [21] [68] [61] [23] [59] [29] General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN General EN EN Local EN Local EN Local EN Local EN Local EN Local EN Local EN, KO Local EN Local KO Local Local EN General KO General KO General KO General KO KO Local KO Local KO Local KO Local KO Local KO Local 22 (a) MMLU (b) KMMLU (c) JMMLU (d) Indo-MMLU Figure 9: Detailed data distribution of MMLU series in English, Korean, Japanese, Indonesian, and Chinese, respectively (e) CMMLU"
        },
        {
            "title": "C Taxonomy Details",
            "content": "C.1 Problem Type Table 3: Problem types, descriptions, and examples Format Binary Multiple-choice QA (MCQA) Description Two-option choice questions, typically Yes/No or True/False. Multiple-choice question answering format. Open-ended generation Example Is the Earth flat? No What is the capital of France? (A) Paris (B) Rome (C) Berlin (A) What is 2+2? 4 Explain the theory of relativity. The theory of relativity, developed by Albert Einstein. . . Tell story about journey to the moon. Once upon time, brave astronaut set off on magical voyage beyond the stars. . . Which response is more helpful? A: Yes. B: Sure, heres how you can do that. . . (B) Short-form Free-form Short, direct answer generation. Extended, explanatory answer generation. Open-ended Answer generation without definitive correct response, involving free imagination storytelling. or Subjective or preferencebased evaluation of responses, typically involving agreement, helpfulness, or safety. Table 4: Task types, descriptions, and examples Description Example Questions that seek factual information, definitions, or explanations. Answers are usually explicitly stated or based on memorized knowledge. Questions that require logical thinking, problem-solving, understanding causeeffect relationships, or commonsense reasoning. Answers are not directly stated, and require interpretation or deduction. \"What is the capital of France?\" There are 12 apples in box. If 4 people share them equally, how many apples does each person get? \"I saw grandson and their grandfather last week outside the Walmart trying to book cab on Uber. Who was not comfortable using the phone?\" (Example from [43]) Value/alignment Questions that involve value judgments, opinions, or behavioral patterns. Comparison C.2 Skill Task Knowledge Reasoning Table 5: Target types and descriptions Target Description General general target without specific cultural or national focus."
        },
        {
            "title": "Local",
            "content": "A specific target toward certain culture (e.g., EN, KO). C.3 Target C.4 Subject We use 6 coarse-grained and 64 fine-grained subjects to classify samples in existing LLM evaluation benchmarks. Table 6 lists the subjects and their definitions. We finalize the subject lists by aggregating WebDewey 10 based on Dewey Decimal Classification (DDC) system and Korean culture-specific classification systems 1112. Table 6: Subject types and descriptions Coarse-grained Fine-grained Description Science Mathematics Statistics Physics Astronomy Chemistry Biology Earth science Geology Atmospheric science Life science Mechanics Materials eng. Chemical eng. Technology Electrical eng. IT Energy eng. The study of numbers, quantities, structures, and abstract reasoning. The science of data collection, analysis, interpretation, and presentation. The study of matter, energy, and the fundamental forces of nature. The scientific study of celestial objects and phenomena beyond Earth. The study of substances, their properties, and how they interact and change. The study of living organisms and their vital processes. The study of Earths physical constitution, processes, and systems. The science of Earths physical structure, materials, and geological history. The study of the Earths atmosphere, weather, climate, and air dynamics. broad field encompassing all sciences related to living organisms and life processes. including The study and application of forces and motion in physical systems. The science and engineering of the properties and uses of materials. The use of chemistry, physics, and engineering principles to design processes for large-scale chemical production. The study and application of electricity, electronics, and electromagnetism. The development, maintenance, and use of computer systems and networks for processing and distributing data. The study and technology of producing, converting, and managing energy resources. 10https://www.oclc.org/en/webdewey.html 11디지털집현전 (https://k-knowledge.kr/guide/nkiClassifi.jsp). 12한국민족문화대백과사전 (https://encykorea.aks.ac.kr/). Nuclear eng. Civil eng. Urban eng. AI"
        },
        {
            "title": "Programming",
            "content": "Engineering principles applied to nuclear power and radiation systems. Design and construction of infrastructure like buildings, roads, and bridges. Engineering focused on city planning, urban infrastructure, and systems. Artificial intelligence and machine learning systems and research. Computer programming and software development practices. Marine eng. Aerospace eng. Environmental eng. Application of engineering principles to environmental protection and sustainability. Engineering of aircraft, spacecraft, and related systems. Engineering of ships, submarines, and marine technology. Science and technology applied to crop and livestock production. Applied sciences in medicine, healthcare, and biomedical technologies. Agricultural eng. Biomedical eng. Humanities and Social Science (HASS) Literature Language Philosophy Religion Cognitive studies Psychology History Geography Politics Economics Law Administration Welfare Education Trade Media Architecture Sculpture The study and interpretation of written, oral, and textual works. The study of human language, linguistics, and communication. The exploration of knowledge, ethics, existence, and reasoning. The study of spiritual beliefs, practices, and religious systems. The study of how individuals perceive, interpret, and respond to information and interactions. The scientific study of human mind, behavior, and mental processes. The study of past events, civilizations, and historical change. The study of physical and human features of the Earths surface. The study of power, governance, political systems, and public policies. The analysis of production, consumption, and distribution of goods and services. The system of rules, rights, and justice within societies. The organization and implementation of policies in governmental and institutional systems. social_science&humanity systems, programs, and policies aimed at improving public well-being and equity. The study and practice of teaching, learning, and knowledge systems. The exchange of goods and services and the systems governing commerce. The study of communication, journalism, and information dissemination. The art and science of designing buildings and physical structures. The creation of three-dimensional artistic forms using various materials. Arts and Sports"
        },
        {
            "title": "Family",
            "content": "Holiday Work life Food Clothing Housing Daily life Leisure Commonsense Value Bias Norms Artistic expression through visual imagery using paint and other media. The art of sound arrangement in melody, harmony, and rhythm. Live artistic performances including theater, dance, music, and acting. Physical activities and competitive games for exercise and entertainment. The artistic and technical creation of images using cameras. Cultural and celebratory events often including art, food, and tradition. The design and aesthetics of clothing, style, and wearable art. Inherited customs, rituals, and beliefs passed across generations. The social unit of individuals connected by kinship or domestic relationships. Social events and public holidays marking special occasions. Cultural norms and practices surrounding work, employment, and work-life balance. Cultural practices, preparation, and significance of cuisine. Attire and fashion as expressions of identity and culture. Living environments and domestic architecture shaped by culture. Everyday routines, behaviors, and practices in social life. Recreational activities, hobbies, and non-work-related pastimes. General world knowledge that people rely on in everyday life. Moral, ethical, or cultural principles guiding behavior and judgment. Deviations in judgment or data caused by subjective factors. Shared social expectations and rules of appropriate behavior. Culture Social intelligence"
        },
        {
            "title": "D Implementation of BENCHHUB",
            "content": "D.1 Automatic Categorization We fine-tune the Qwen-2.5-7B models 13 to automatically categorize the skill, subject and target type of given sample. Since obtaining sufficient training data for all defined categories is difficult and manually labeling all queries is challenging, we use synthetic data approach. Instead of generating synthetic queries directly, which can be unreliable, we generate synthetic rationales for given queries to ensure reliability. The process is as follows: first, we create all possible combinations of our three categoriesskill, task, and target. We provide the LLM with category descriptions along with this specific category combination, Table 7: Accuracy of fine-tuned categorizer on Qwen-2.5-7b Accuracy Subject Skill Target 0.871 0.967 0. 13This model is publicly available via huggingface: BenchHub/BenchHub-Cat-7b 27 and ask it to generate explanations for why hypothetical query fits each category. We use GPT-4o as synthetic rationale generator. We then train the model with these rationales as inputs and the categories as outputs, enabling it to learn category definitions and their applications. The following are the examples and the prompts we use for the categorization training."
        },
        {
            "title": "Example of Rationale",
            "content": "example = \"The query is asking about the cause of symptoms (vomiting and diarrhea) in 6-year-old boy who ate kimbap at kindergarten and later experienced these symptoms along with three other children. This question is seeking factual information about the likely pathogen responsible for the symptoms, which falls under the category of knowledge. The query is specific to situation in Korea, given the context of kindergarten and the food mentioned (kimbap). The subject area is related to biology, specifically microbiology or pathogens."
        },
        {
            "title": "Prompt for Rationale Generation of Given Query",
            "content": "I want to assign three categories to the following query, but before doing this, you should create description of the given query. Explain the query first (e.g., what the question is asking about (i.e., subject type), the type of ability needed to solve it (i.e., task type), whether its question about specific culture or general question (i.e., target type), etc.). Refer to the definition of each label and the output format. Label Definition: {description} Now, create description for the following query. Prompt for Synthetic Rationale Generation The following are the categories of one query, with an explanation for each category provided below. Your job is to generate query description to derive the appropriate category from each query. The query itself is not given, but you need to imagine query that fits the given category and create description for that query. The information about the query doesnt need to be extremely specific, but rather should highlight why it corresponds to each category. Please refer to the example description and explanation of the category. Description example: {example} Category explanation: {tasks} Now, lets start! Given category: {category} Your Description: Prompt for Category Generation **You are an agent tasked with assigning three categoriessubject_type, task_type, and target_typeto describe what is required to answer the following prompt.** * **subject_type**: What domain of knowledge or skill is needed? * **task_type**: What type of cognitive process or reasoning is involved? * **target_type**: Is the required knowledge or skill specific to particular country or culture? Note: Focus on the knowledge or skill needed to solve the prompt, not the topic it mentions on the surface. For example, if the prompt involves counting apples, the subject_type should be \"math\", not \"food\". The following text is meta data of certain prompt. Based on this data, assign three labels to the following data. Refer to the description of each label and the output format. Present the output in the following format: task_type : str,target_type : str,subject_type : LIST[str] Please refer the following information: ### **Task Type Description** - **task_type** indicates the type of task the query belongs to. Categorize the question based on its primary intent rather than its wording. #### **Task Categories:** - **knowledge** Questions that seek factual information, definitions, or explanations.Answers are usually explicitly stated or based on memorized knowledge. - Example: *\"What is the capital of France?\"* - Example: *\"What is the pythagorean theorem?\"* - **reasoning** Questions that require logical thinking, problem-solving, understanding cause-effect relationships, or commonsense judgment. Answers are not directly stated, and require interpretation or deduction. This includes commonsense reasoning everyday inferences person can make based on typical human experience. - Example: *\"If train departs at 3 PM and travels at 60 km/h, when will it reach city 180 km away?\"* - **value/alignment** Questions that involve **value judgments**, opinions, or behavioral patterns. - Example: *\"Is it ethical to use AI in hiring decisions?\"* - Example: *\"What are the social impacts of remote work?\"* ### **Target Description** - **target_type** indicates the country or cultural region that the query is focusing on. This classification is based on the subject matter of the question, **not the language in which it is written**. - Identify whether the question is specifically about countrys culture, society, history, or any other aspect related to that region. - If there is no corresponding value, you can add it. #### **Target Options:** - **general** general target without specific cultural or national focus. - **ko** Targeting **Korea**. - **us** Targeting **the United States**. - (중략) - subject_type represents the knowledge domain or reasoning field needed to answer the prompt. Identify the content of the query and select one or more of the following values. If there is no matching category, respond with misc. - Categories: ### **science Categories** - **science/math** - The study of numbers, quantities, structures, and abstract reasoning. - **science/biology** - The study of living organisms and their vital processes. - (중략) - **science/microbiology** - The study of microorganisms and pathogens. (가정된 세부 카테고리) Now, present the corresponding categories of following data in json format. Data: \"query\": \"What causes vomiting and diarrhea in child after eating kimbap?\", \"answer\": \"Likely bacterial infection such as Salmonella or E. coli.\", \"category\": null \"subject_type\": [\"science/biology\", \"science/microbiology\"], \"task_type\": \"knowledge\", \"target_type\": \"ko\""
        },
        {
            "title": "E Reproducibility Statement",
            "content": "Table 8: SFT configuration details for 3.2. E.1 Experimental Setups We use Axolotl [4] for the SFT training in 3.2. We train Qwen2.5-7B-Instruct with DeepSpeedZero3 [45] on 4 A6000 48GB GPUs for 5 hours per run. We follow the method of [19] for optimization. Hyperparameter Value 8,192 Sequence Length 2 105 Learning Rate Global Batch (Effective) 256 Learning Rate Scheduler Cosine Decay 0.05 Warmup Ratio 3 Training Epochs E.2 License We release BENCHHUB, including our source code and trained models, under the Apache License 2.0. For the datasets provided by BENCHHUB, the entire dataset is released under the most restrictive license among them CC BY-NC-ND 4.0 although the applicable license may vary depending on the specific subset selected by the user. The license for each dataset is listed in Table 2. E.3 Instructions and System Prompts Please read the following passage and answer the question. Choose one answer from {label set}. Passage: {passage} Question: {question} Choices: {choices} Answer: 다음 지문을 참고하여 질문에 답하여라. 답은 보기 중 하나를 {label set} 중에서 고르시오. 지문: {passage} 질문: {question} 보기: {choices} 답: 29 Answer the following question. Choose one answer from {label set}. {question} Choices: {choices} Answer: Question: 다음 질문에 답하여라. 답은 보기 중 하나를 {label set} 중에서 고르시오. 답: {question} 보기: {choices} 질문:"
        },
        {
            "title": "F Experimental Results",
            "content": "See Table 9-10 for the scores (accuracies) of the models across subject types. 30 Table 9: Results of all models across fine-grained categories (English) Llama-3.3-70B DeepSeek-R1-32B claude-3.7-sonnet gemma-3-27b gemini-2.0 gpt-4.1 subject Tech Urban eng. Nuclear eng. Marin eng. Biomedical eng. Mechanics Materials eng. IT Environmental eng. Energy eng. Electrical eng. Programming Civil eng. Chemical eng. AI Agricultural eng. Aerospace eng. Science Statistics Physics Mathematics Life science Geology Earth science Chemistry Biology Atmospheric science Astronomy HASS Welfare Trade Cognitive studies Religion Politics Philosophy Media Literature Law Language History Geography Education Economics Administration Social Intelligence Value Norms Commonsense Bias Culture Work life Tradition Housing Food Family Daily life Clothing Holiday Arts % Sports Sports Sculpture Photography Performing Painting Music Festivals Fashion Architecture 0.882 1.000 1.000 0.963 0.943 0.987 0.904 0.957 0.953 0.877 1.000 1.000 0.714 0.931 1.000 1.000 0.879 0.892 0.918 0.965 0.990 0.979 0.863 0.959 0.990 0.965 0.896 0.944 0.620 0.912 0.909 0.875 0.857 0.950 0.750 0.736 0.911 0.911 0.957 0.893 0.899 0.699 0.816 0.837 0. 0.778 0.833 1.000 0.534 0.913 0.600 1.000 1.000 0.781 1.000 1.000 0.846 1.000 1.000 0.500 1.000 1.000 0.765 0.750 0.667 0.828 0.829 0.920 0.735 0.739 0.802 0.816 0.913 0.769 0.571 0.984 0.867 0.833 0.803 0.800 0.956 0.798 0.816 0.798 0.814 0.730 0.753 0.843 0.722 0.807 0.524 0.877 0.759 0.664 0.864 0.850 0.596 0.548 0.864 0.804 0.793 0.809 0.797 0.890 0.658 0.765 1. 0.667 0.881 1.000 0.479 0.739 0.521 1.000 1.000 0.578 1.000 0.600 0.846 0.600 1.000 1.000 0.800 0.857 0.588 1.000 1.000 0.779 0.647 0.784 0.724 0.754 0.767 0.704 0.556 0.750 0.714 0.355 0.795 0.833 0.600 0.705 0.587 0.678 0.656 0.779 0.697 0.769 0.641 0.809 0.737 0.767 0.662 0.860 0.767 0.623 0.778 0.750 0.518 0.519 0.857 0.886 0.652 0.713 0.819 0.857 0.581 0.856 0. 0.700 0.800 0.750 0.675 0.705 0.676 1.000 1.000 0.786 0.500 0.800 0.808 0.700 0.900 1.000 0.600 0.571 0.765 0.500 1.000 0.743 0.706 0.811 0.690 0.797 0.826 0.722 0.611 0.750 0.714 0.420 0.864 0.833 0.563 0.689 0.717 0.809 0.792 0.788 0.650 0.767 0.783 0.835 0.654 0.811 0.580 0.914 0.767 0.711 0.889 0.950 0.540 0.526 0.786 0.773 0.795 0.597 0.819 0.599 0.613 0.877 0. 0.720 0.806 1.000 0.553 0.659 0.590 1.000 1.000 0.929 1.000 0.400 0.654 0.900 0.900 0.500 0.800 1.000 0.824 0.500 1.000 0.716 0.829 0.760 0.783 0.855 0.791 0.825 0.826 0.923 0.571 0.817 0.800 1.000 0.803 0.842 0.872 0.781 0.776 0.840 0.762 0.818 0.753 0.843 0.729 0.800 0.481 0.895 0.693 0.632 0.759 0.850 0.610 0.518 0.578 0.804 0.793 0.695 0.732 0.788 0.605 0.749 0. 0.704 0.950 0.750 0.479 0.609 0.475 1.000 1.000 0.453 1.000 0.800 0.769 0.900 0.800 1.000 1.000 0.714 0.625 0.500 0.500 0.563 0.559 0.595 0.598 0.652 0.628 0.609 0.667 0.750 0.429 0.474 0.705 1.000 0.452 0.549 0.756 0.565 0.688 0.692 0.510 0.533 0.739 0.704 0.576 0.494 0.500 0.724 0.635 0.455 0.667 0.684 0.294 0.420 0.463 0.628 0.580 0.574 0.551 0.653 0.516 0.871 0. 0.600 0.618 1.000 0.360 0.591 0.355 1.000 1.000 0.714 0.500 0.400 0.673 0.600 0.900 1.000 0.800 0.429 31 Mistral-24B 0.882 1.000 1.000 0.794 0.941 0.932 0.782 0.928 0.872 0.800 0.722 1.000 0.571 0.330 0.932 1.000 0.622 0.713 0.711 0.904 0.885 0.942 0.720 0.835 0.935 0. 0.797 0.856 0.629 0.948 0.872 0.651 0.722 0.950 0.679 0.504 0.881 0.818 0.848 0.752 0.841 0.619 0.710 0.837 0.288 0.720 0.784 0.750 0.456 0.818 0.532 1.000 1.000 0.857 1.000 0.800 0.846 1.000 0.800 0.500 0.600 1.000 Table 10: Results of all models across fine-grained categories (Korean) subject Tech Urban eng. Nuclear eng. Marine eng. Biomedical eng. Mechanics Materials eng. IT Environmental eng. Energy eng. Electrical eng. Programming Civil eng. Chemical eng. AI Agricultural eng. Aerospace eng. Science Statistics Physics Mathematics Life science Geology Earth science Chemistry Biology Atmospheric science Astronomy HASS Welfare Trade Religion Psychology Politics Philosophy Media Literature Law Language History Geography Education Economics Administration Social Intelligence Value Norms Commonsense Bias Culture Work life Tradition Leisure Housing Food Family Daily life Clothing Holiday Arts & Sports Sports Sculpture Photography Performing Painting Music Festivals Fashion Architecture gpt-4.1 claude-3.7-sonnet gemini-2.0 gemma-3-27b DeepSeek-R1-32B Llama-3.3-70B Mistral-24B 0.552 0.676 0.688 0.838 0.661 0.720 0.854 0.591 0.587 0.688 0.667 0.517 0.711 0.861 0.605 0.757 0.813 0.826 0.842 0.783 0.755 0.701 0.760 0.852 0.719 1.000 0.783 0.856 0.846 1.000 0.806 0.843 0.942 0.836 0.604 0.807 0.775 0.711 0.732 0.814 0.731 0.848 0.884 0.835 0.993 0.926 0.962 1.000 0.824 0.850 0.826 0.837 0.793 0. 0.960 0.923 0.800 0.950 0.931 0.912 0.941 0.626 0.745 0.634 0.647 0.826 0.805 0.709 0.820 0.877 0.649 0.674 0.778 0.722 0.669 0.809 0.829 0.605 0.786 0.813 0.870 0.889 0.783 0.765 0.769 0.829 0.875 0.688 1.000 0.745 0.767 0.860 1.000 0.858 0.897 0.928 0.914 0.555 0.906 0.794 0.778 0.816 0.820 0.766 0.879 0.881 0.873 0.966 0.926 0.960 1.000 0.824 0.923 0.792 0.837 0.793 0. 0.960 0.833 0.855 0.950 0.932 0.971 1.000 0.626 0.778 0.507 0.559 0.521 0.507 0.495 0.510 0.756 0.427 0.457 0.535 0.667 0.391 0.539 0.618 0.386 0.564 0.582 0.595 0.487 0.609 0.422 0.552 0.612 0.664 0.531 1.000 0.742 0.752 0.706 0.000 0.634 0.718 0.755 0.739 0.416 0.685 0.526 0.522 0.603 0.701 0.635 0.818 0.810 0.757 1.000 0.768 0.819 0.500 0.676 0.684 0.818 0.682 0.655 0. 0.917 0.727 0.600 0.752 0.794 0.667 0.882 0.490 0.658 0.559 0.618 0.625 0.409 0.563 0.560 0.667 0.480 0.551 0.646 0.667 0.530 0.641 0.676 0.539 0.579 0.571 0.644 0.848 0.635 0.627 0.627 0.643 0.586 0.625 1.000 0.516 0.658 0.714 1.000 0.714 0.715 0.897 0.760 0.463 0.763 0.691 0.698 0.586 0.606 0.598 0.697 0.881 0.822 0.951 0.826 0.858 1.000 0.647 0.769 0.696 0.823 0.690 0. 0.818 0.833 0.655 0.911 0.833 0.758 1.000 0.524 0.641 0.504 0.676 0.569 0.727 0.537 0.608 0.727 0.456 0.507 0.549 0.667 0.503 0.596 0.694 0.464 0.621 0.571 0.626 0.385 0.635 0.608 0.604 0.574 0.766 0.531 0.900 0.755 0.752 0.805 1.000 0.717 0.791 0.877 0.700 0.510 0.648 0.622 0.594 0.701 0.704 0.691 0.818 0.881 0.718 1.000 0.921 0.917 0.500 0.735 0.744 0.652 0.751 0.621 0. 0.960 1.000 0.768 0.930 0.896 0.909 0.941 0.565 0.711 32 0.543 0.588 0.611 0.767 0.487 0.619 0.803 0.462 0.457 0.549 0.667 0.497 0.574 0.657 0.506 0.629 0.549 0.603 0.359 0.739 0.618 0.575 0.643 0.742 0.656 1.000 0.724 0.766 0.812 1.000 0.667 0.757 0.876 0.798 0.544 0.750 0.603 0.631 0.755 0.692 0.711 0.788 0.721 0.748 0. 0.921 0.900 1.000 0.676 0.789 0.864 0.738 0.759 0.674 0.913 0.917 0.667 0.884 0.837 0.879 0.813 0.571 0.664 0.468 0.588 0.569 0.713 0.420 0.608 0.742 0.368 0.399 0.500 0.833 0.430 0.560 0.703 0.428 0.579 0.615 0.542 0.359 0.635 0.510 0.575 0.512 0.711 0.563 0.800 0.705 0.731 0.856 0.000 0.703 0.757 0.877 0.800 0.530 0.705 0.570 0.597 0.660 0.656 0.675 0.758 0.762 0.767 1. 0.921 0.911 0.500 0.676 0.821 0.800 0.764 0.655 0.654 0.864 0.833 0.655 0.918 0.918 0.909 0.941 0.456 0."
        }
    ],
    "affiliations": [
        "KAIST",
        "OnelineAI",
        "Oracle",
        "Yonsei University"
    ]
}