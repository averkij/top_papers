{
    "paper_title": "Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures",
    "authors": [
        "Tyler A. Chang",
        "Catherine Arnett",
        "Abdelrahman Eldesokey",
        "Abdelrahman Sadallah",
        "Abeer Kashar",
        "Abolade Daud",
        "Abosede Grace Olanihun",
        "Adamu Labaran Mohammed",
        "Adeyemi Praise",
        "Adhikarinayum Meerajita Sharma",
        "Aditi Gupta",
        "Afitab Iyigun",
        "Afonso Simplício",
        "Ahmed Essouaied",
        "Aicha Chorana",
        "Akhil Eppa",
        "Akintunde Oladipo",
        "Akshay Ramesh",
        "Aleksei Dorkin",
        "Alfred Malengo Kondoro",
        "Alham Fikri Aji",
        "Ali Eren Çetintaş",
        "Allan Hanbury",
        "Alou Dembele",
        "Alp Niksarli",
        "Álvaro Arroyo",
        "Amin Bajand",
        "Amol Khanna",
        "Ana Chkhaidze",
        "Ana Condez",
        "Andiswa Mkhonto",
        "Andrew Hoblitzell",
        "Andrew Tran",
        "Angelos Poulis",
        "Anirban Majumder",
        "Anna Vacalopoulou",
        "Annette Kuuipolani Kanahele Wong",
        "Annika Simonsen",
        "Anton Kovalev",
        "Ashvanth. S",
        "Ayodeji Joseph Lana",
        "Barkin Kinay",
        "Bashar Alhafni",
        "Benedict Cibalinda Busole",
        "Bernard Ghanem",
        "Bharti Nathani",
        "Biljana Stojanovska Đurić",
        "Bola Agbonile",
        "Bragi Bergsson",
        "Bruce Torres Fischer",
        "Burak Tutar",
        "Burcu Alakuş Çınar",
        "Cade J. Kanoniakapueo Kane",
        "Can Udomcharoenchaikit",
        "Catherine Arnett",
        "Chadi Helwe",
        "Chaithra Reddy Nerella",
        "Chen Cecilia Liu",
        "Chiamaka Glory Nwokolo",
        "Cristina España-Bonet",
        "Cynthia Amol",
        "DaeYeop Lee",
        "Dana Arad",
        "Daniil Dzenhaliou",
        "Daria Pugacheva",
        "Dasol Choi",
        "Daud Abolade",
        "David Liu",
        "David Semedo",
        "Deborah Popoola",
        "Deividas Mataciunas",
        "Delphine Nyaboke",
        "Dhyuthy Krishna Kumar",
        "Diogo Glória-Silva",
        "Diogo Tavares",
        "Divyanshu Goyal",
        "DongGeon Lee",
        "Ebele Nwamaka Anajemba",
        "Egonu Ngozi Grace",
        "Elena Mickel",
        "Elena Tutubalina",
        "Elias Herranen",
        "Emile Anand",
        "Emmanuel Habumuremyi",
        "Emuobonuvie Maria Ajiboye",
        "Eryawan Presma Yulianrifat",
        "Esther Adenuga",
        "Ewa Rudnicka",
        "Faith Olabisi Itiola",
        "Faran Taimoor Butt",
        "Fathima Thekkekara",
        "Fatima Haouari",
        "Filbert Aurelian Tjiaranata",
        "Firas Laakom",
        "Francesca Grasso",
        "Francesco Orabona",
        "Francesco Periti",
        "Gbenga Kayode Solomon",
        "Gia Nghia Ngo",
        "Gloria Udhehdhe-oze",
        "Gonçalo Martins",
        "Gopi Naga Sai Ram Challagolla",
        "Guijin Son",
        "Gulnaz Abdykadyrova",
        "Hafsteinn Einarsson",
        "Hai Hu",
        "Hamidreza Saffari",
        "Hamza Zaidi",
        "Haopeng Zhang",
        "Harethah Abu Shairah",
        "Harry Vuong",
        "Hele-Andra Kuulmets",
        "Houda Bouamor",
        "Hwanjo Yu",
        "Iben Nyholm Debess",
        "İbrahim Ethem Deveci",
        "Ikhlasul Akmal Hanif",
        "Ikhyun Cho",
        "Inês Calvo",
        "Inês Vieira",
        "Isaac Manzi",
        "Ismail Daud",
        "Itay Itzhak",
        "Iuliia",
        "Alekseenko",
        "Ivan Belashkin",
        "Ivan Spada",
        "Ivan Zhelyazkov",
        "Jacob Brinton",
        "Jafar Isbarov",
        "Jaka Čibej",
        "Jan Čuhel",
        "Jan Kocoń",
        "Jauza Akbar Krito",
        "Jebish Purbey",
        "Jennifer Mickel",
        "Jennifer Za",
        "Jenny Kunz",
        "Jihae Jeong",
        "Jimena Tena Dávalos",
        "Jinu Lee",
        "João Magalhães",
        "John Yi",
        "Jongin Kim",
        "Joseph Chataignon",
        "Joseph Marvin Imperial",
        "Jubeerathan Thevakumar",
        "Judith Land",
        "Junchen Jiang",
        "Jungwhan Kim",
        "Kairit Sirts",
        "Kamesh R",
        "Kamesh V",
        "Kanda Patrick Tshinu",
        "Kätriin Kukk",
        "Kaustubh Ponkshe",
        "Kavsar Huseynova",
        "Ke He",
        "Kelly Buchanan",
        "Kengatharaiyer Sarveswaran",
        "Kerem Zaman",
        "Khalil Mrini",
        "Kian Kyars",
        "Krister Kruusmaa",
        "Kusum Chouhan",
        "Lainitha Krishnakumar",
        "Laura Castro Sánchez",
        "Laura Porrino Moscoso",
        "Leshem Choshen",
        "Levent Sencan",
        "Lilja Øvrelid",
        "Lisa Alazraki",
        "Lovina Ehimen-Ugbede",
        "Luheerathan Thevakumar",
        "Luxshan Thavarasa",
        "Mahnoor Malik",
        "Mamadou K. Keita",
        "Mansi Jangid",
        "Marco De Santis",
        "Marcos García",
        "Marek Suppa",
        "Mariam D'Ciofalo",
        "Marii Ojastu",
        "Maryam Sikander",
        "Mausami Narayan",
        "Maximos Skandalis",
        "Mehak Mehak",
        "Mehmet İlteriş Bozkurt",
        "Melaku Bayu Workie",
        "Menan Velayuthan",
        "Michael Leventhal",
        "Michał Marcińczuk",
        "Mirna Potočnjak",
        "Mohammadamin Shafiei",
        "Mridul Sharma",
        "Mrityunjaya Indoria",
        "Muhammad Ravi Shulthan Habibi",
        "Murat Kolić",
        "Nada Galant",
        "Naphat Permpredanun",
        "Narada Maugin",
        "Nicholas Kluge Corrêa",
        "Nikola Ljubešić",
        "Nirmal Thomas",
        "Nisansa de Silva",
        "Nisheeth Joshi",
        "Nitish Ponkshe",
        "Nizar Habash",
        "Nneoma C. Udeze",
        "Noel Thomas",
        "Noémi Ligeti-Nagy",
        "Nouhoum Coulibaly",
        "Nsengiyumva Faustin",
        "Odunayo Kareemat Buliaminu",
        "Odunayo Ogundepo",
        "Oghojafor Godswill Fejiro",
        "Ogundipe Blessing Funmilola",
        "Okechukwu God'spraise",
        "Olanrewaju Samuel",
        "Olaoye Deborah Oluwaseun",
        "Olasoji Akindejoye",
        "Olga Popova",
        "Olga Snissarenko",
        "Onyinye Anulika Chiemezie",
        "Orkun Kinay",
        "Osman Tursun",
        "Owoeye Tobiloba Moses",
        "Oyelade Oluwafemi Joshua",
        "Oyesanmi Fiyinfoluwa",
        "Pablo Gamallo",
        "Pablo Rodríguez Fernández",
        "Palak Arora",
        "Pedro Valente",
        "Peter Rupnik",
        "Philip Oghenesuowho Ekiugbo",
        "Pramit Sahoo",
        "Prokopis Prokopidis",
        "Pua Niau-Puhipau",
        "Quadri Yahya",
        "Rachele Mignone",
        "Raghav Singhal",
        "Ram Mohan Rao Kadiyala",
        "Raphael Merx",
        "Rapheal Afolayan",
        "Ratnavel Rajalakshmi",
        "Rishav Ghosh",
        "Romina Oji",
        "Ron Kekeha Solis",
        "Rui Guerra",
        "Rushikesh Zawar",
        "Sa'ad Nasir Bashir",
        "Saeed Alzaabi",
        "Sahil Sandeep",
        "Sai Pavan Batchu",
        "SaiSandeep Kantareddy",
        "Salsabila Zahirah Pranida",
        "Sam Buchanan",
        "Samuel Rutunda",
        "Sander Land",
        "Sarah Sulollari",
        "Sardar Ali",
        "Saroj Sapkota",
        "Saulius Tautvaisas",
        "Sayambhu Sen",
        "Sayantani Banerjee",
        "Sebastien Diarra",
        "SenthilNathan. M",
        "Sewoong Lee",
        "Shaan Shah",
        "Shankar Venkitachalam",
        "Sharifa Djurabaeva",
        "Sharon Ibejih",
        "Shivanya Shomir Dutta",
        "Siddhant Gupta",
        "Silvia Paniagua Suárez",
        "Sina Ahmadi",
        "Sivasuthan Sukumar",
        "Siyuan Song",
        "Snegha A.",
        "Sokratis Sofianopoulos",
        "Sona Elza Simon",
        "Sonja Benčina",
        "Sophie Gvasalia",
        "Sphurti Kirit More",
        "Spyros Dragazis",
        "Stephan P. Kaufhold",
        "Suba. S",
        "Sultan AlRashed",
        "Surangika Ranathunga",
        "Taiga Someya",
        "Taja Kuzman Pungeršek",
        "Tal Haklay",
        "Tasi'u Jibril",
        "Tatsuya Aoyama",
        "Tea Abashidze",
        "Terenz Jomar Dela Cruz",
        "Terra Blevins",
        "Themistoklis Nikas",
        "Theresa Dora Idoko",
        "Thu Mai Do",
        "Tilek Chubakov",
        "Tommaso Gargiani",
        "Uma Rathore",
        "Uni Johannesen",
        "Uwuma Doris Ugwu",
        "Vallerie Alexandra Putra",
        "Vanya Bannihatti Kumar",
        "Varsha Jeyarajalingam",
        "Varvara Arzt",
        "Vasudevan Nedumpozhimana",
        "Viktoria Ondrejova",
        "Viktoryia Horbik",
        "Vishnu Vardhan Reddy Kummitha",
        "Vuk Dinić",
        "Walelign Tewabe Sewunetie",
        "Winston Wu",
        "Xiaojing Zhao",
        "Yacouba Diarra",
        "Yaniv Nikankin",
        "Yash Mathur",
        "Yixi Chen",
        "Yiyuan Li",
        "Yolanda Xavier",
        "Yonatan Belinkov",
        "Yusuf Ismail Abayomi",
        "Zaid Alyafeai",
        "Zhengyang Shan",
        "Zhi Rui Tam",
        "Zilu Tang",
        "Zuzana Nadova",
        "Baber Abbasi",
        "Stella Biderman",
        "David Stap",
        "Duygu Ataman",
        "Fabian Schmidt",
        "Hila Gonen",
        "Jiayi Wang",
        "David Ifeoluwa Adelani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover a large number of languages and cultures. In this paper, we present Global PIQA, a participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lower-resource languages (up to a 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides a glimpse into the wide diversity of cultures in which human language is embedded."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 1 8 0 4 2 . 0 1 5 2 : r Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures v0.1 Tyler A. Chang1*, Catherine Arnett2*, and Authors at the 5th Multilingual Representation Learning (MRL) Workshop(cid:134) 1UC San Diego, 2EleutherAI (cid:134)For full authorship list, see A. *Equal contribution"
        },
        {
            "title": "Abstract",
            "content": "To date, there exist almost no culturally-specific evaluation benchmarks for large language models (LLMs) that cover large number of languages and cultures. In this paper, we present Global PIQA, participatory commonsense reasoning benchmark for over 100 languages, constructed by hand by 335 researchers from 65 countries around the world. The 116 language varieties in Global PIQA cover five continents, 14 language families, and 23 writing systems. In the non-parallel split of Global PIQA, over 50% of examples reference local foods, customs, traditions, or other culturally-specific elements. We find that state-of-the-art LLMs perform well on Global PIQA in aggregate, but they exhibit weaker performance in lowerresource languages (up to 37% accuracy gap, despite random chance at 50%). Open models generally perform worse than proprietary models. Global PIQA highlights that in many languages and cultures, everyday knowledge remains an area for improvement, alongside more widely-discussed capabilities such as complex reasoning and expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA provides glimpse into the wide diversity of cultures in which human language is embedded. https://mrlbenchmarks.github.io/ Global PIQA (cid:135) mrlbenchmarks"
        },
        {
            "title": "Introduction",
            "content": "Nearly all prominent multilingual benchmarks for large language models (LLMs) translate existing English datasets into other languages (e.g. XNLI, XCOPA, Belebele, XStoryCloze, MGSM, and Global MMLU; Conneau et al., 2018; Ponti et al., 2020; Bandarkar et al., 2024; Lin et al., 2022; Shi et al., 2023; Singh et al., 2025). As result, the vast majority of the worlds languages lack culturally-specific evaluation datasets that cover local customs, traditions, and everyday life for speakers of the language. The culturally-specific datasets that do exist generally still rely heavily on translation or are limited to relatively small number of languages (e.g. Global MMLU and BLEnD; Singh et al., 2025; Myung et al., 2024). This lack of culturally-specific datasets is particularly relevant in the domain of commonsense reasoning, where LLMs are evaluated for physical, social, and world knowledge that is broadly known by the majority of people in community. Commonsense reasoning capabilities have long been desirable property of LLM-based systems, evaluated through popular benchmarks such as HellaSwag (Zellers et al., 2019) and PIQA (Bisk et al., 2020). Because commonsense reasoning focuses on everyday physical and social activities, and it has its basis in community knowledge, it differs greatly Preprint. Figure 1: Map of the 116 language varieties represented in Global PIQA, colored according to top-level language families from Glottolog (Hammarström et al., 2023). All languages and language families are listed in B. across languages and cultures. This variation across communities is particularly noticeable when compared to the relative uniformity of more abstract capabilities, such as mathematical or logical reasoning, which have been the focus of many recent LLM evaluation benchmarks (e.g. MGSM; Shi et al., 2023). Unfortunately, culturally-specific commonsense reasoning evaluation datasets do not exist for the vast majority of the worlds languages. To fill this gap, we present Global PIQA, culturally-specific physical commonsense reasoning benchmark created by native speakers of over 100 language varieties across the globe. In contrast to previous multilingual benchmarks, examples in the non-parallel split of Global PIQA are written directly in each language, largely by NLP researchers who speak the language, involving very little translation. Authors were given flexibility to determine the topics and domains for their examples, in order to develop target-language original prompts (Kreutzer et al., 2025) that are appropriate for each linguistic and cultural context. All contributors to the datasets were offered authorship on this paper, to reflect the significance of these intellectual contributions to the project. We then evaluate state-of-the-art LLMs on Global PIQA. We find that proprietary models perform well in aggregate, with the best performing model achieving an accuracy of 91.7% (Gemini 2.5 Pro). In some ways, this is expected, as Global PIQA is designed to evaluate commonsense knowledge that is widely known in each cultural and linguistic community. However, Global PIQA highlights disparities between highand low-resource languages; for example, the best performing model for Sub-Saharan African languages reaches an accuracy of only 80.2% for those languages, compared to 95.6% for Western European languages (with chance at 50%). We also find that open weight models generally perform worse than proprietary models. We hope that Global PIQA will enable researchers to measure and ultimately close the multilingual performance gap both across languages and between open and proprietary models. More broadly, Global PIQA provides glimpse into wide array of global cultures, through commonsense examples describing everyday life in over 100 languages."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Multilingual evaluation datasets. Most multilingual evaluations for standard LLM tasks (e.g. question answering and mathematical reasoning) are the product of translation from English (e.g. EU20, mArenaHard, Okapi, MMLU-ProX, and MGSM; Thellmann et al., 2024; Dang et al., 2024; Lai et al., 2023; Xuan et al., 2025; Shi et al., 2023). In some cases, the translations are automatic without any human verification, which can lead to unnatural examples and low-quality datasets due to artifacts from machine translation (Singh et al., 2025). In other cases, benchmarks are professionally translated or use human-verified translations (e.g. Belebele, MMMLU, IrokoBench, Global MMLU, and XQuAD; Bandarkar et al., 2024; OpenAI, 2024; Adelani et al., 2025; Singh et al., 2025; Artetxe et al., 2020). These benchmarks are less likely to suffer from quality issues related to machine translation, but they are still not necessarily culturally relevant for the target languages. Benchmarks 1We release Global PIQA under CC-BY-SA 4.0 license. Global PIQA is intended only for LLM evaluation. We do not allow training of AI systems on Global PIQA, or on synthetic data that uses Global PIQA as seed. 2 Figure 2: The format of Global PIQA examples. Each example can be used either in completion setting (to evaluate pretrained-only models) or prompted setting (to evaluate instruction-tuned models). Evaluation method details are in 5. translated from English have been found to propagate Anglocentric perspectives and values (Singh et al., 2025; Kreutzer et al., 2025). Culturally-specific evaluation. Culturally-specific evaluation is critical for designing models that align with values other than those from higher-resourced countries for LLM research (Nigatu et al., 2024). Culturally-specific benchmarks have been constructed for variety of languages (e.g. INCLUDE, TyDi QA, CulturalBench, MultiLoKo, DOSA, and BLEnD; Romanou et al., 2025; Clark et al., 2020; Chiu et al., 2025; Hupkes and Bogoychev, 2025; Seth et al., 2024; Myung et al., 2024), and datasets such as MMLU (Hendrycks et al., 2021) have been localized to other languages (e.g. CMMLU, KMMLU, ArabicMMLU, TurkishMMLU, and IndoMMLU; Li et al., 2024; Son et al., 2025; Koto et al., 2024; Yüksel et al., 2024; Koto et al., 2023). Results from these localized benchmarks correlate more strongly with human judgments of model quality than results from translated or non-localized benchmarks (Wu et al., 2025). However, these datasets often focus on challenging knowledge questions in localized topics, rather than commonsense cultural knowledge which is often widely known in the community but not documented on the web. Physical Interaction: Question Answering (PIQA). To define the task format and scope for Global PIQA, we take inspiration from English PIQA (Bisk et al., 2020). PIQA aims to measure physical commonsense reasoning, which we note in 1 is likely to vary substantially across languages and cultures. In Global PIQA, we define physical commonsense reasoning as broad collection of related tasks relying on knowledge of physical properties, affordances (types of actions an agent can perform with an object; Gibson, 1979; Jones et al., 2022), and physical and temporal relations.2 Each example in PIQA consists of goal (or prompt) and two possible solutions, one correct and one incorrect (e.g. Figure 2). Prompt-solution pairs can consist of sentence beginnings and completions, questions and answers, or goals (e.g. making specific food dishes) and solutions. Even five years after its initial release, PIQA is still being used in evaluations, e.g. reported in technical reports for releases such as Gemma 3 (Team Gemma et al., 2025) and Llama 3 (Meta AI, 2024). Despite its broad usage as benchmark for English, PIQA has not been translated or broadly adapted as multilingual benchmark, much less extended to massively multilingual and culturally-specific settings.3 2We discuss more narrow definitions of physical commonsense reasoning from previous work in C.2. 3Üstün et al. (2024) machine-translate PIQA into 93 languages to train the Aya model, but these translations are not human verified. Translations also exist on Hugging Face for Catalan and Basque."
        },
        {
            "title": "3 Global PIQA: Non-Parallel Split",
            "content": "Thus, we construct Global PIQA, physical commonsense reasoning benchmark for 116 language varieties. The primary split of Global PIQA is non-parallel (i.e. not translated across languages) to allow authors to write culturally-specific examples for their languages. Following the PIQA dataset (Bisk et al., 2020; 2), each example consists of prompt and two candidate solutions, one correct and one incorrect. Each example can be used to evaluate either pretrained-only model (Figure 2, center) or an instruction-tuned model (Figure 2, right). Determining the correct solution is designed to require physical commonsense reasoning, although we allow for fairly flexible definitions of physical commonsense (C.2). 3.1 Organizing Global and Participatory Benchmark For the non-parallel split of Global PIQA, authors contributed datasets following the task format described above (details in 3.2). Authors provided their datasets with short dataset descriptions,4 and all authors of included datasets were offered co-authorship on this paper. To date, the Global PIQA project has involved 335 contributors across 65 countries and 173 university or company affiliations. Our authors range from early career undergraduate researchers to professors at major global universities. Here, we describe key decisions that made the collaboration success. Researchers and authorship. One major reason for this benchmarks success was that we recruited NLP researchers themselves to construct the datasets. In this setup, researchers benefit from co-authorship on large benchmark paper, and they have both the domain expertise and motivation to write high quality examples. Participation is entirely voluntary. This contrasts with benchmarks where external annotators are paid to create datasets, with little incentive to create high quality examples. Our collaborative approach involving other NLP researchers is less exploitative, and because many of our authors develop technologies for their language(s), authors also benefit from having high quality benchmark in their language(s). Recruiting. We were able to recruit diverse group of contributors through large online communities, low-resource NLP community organizations, social media, and personal connections.5 We also identified NLP researchers with experience constructing benchmarks and language models for specific languages or language families, and we contacted them directly to broaden our reach. We maintained spreadsheet of interested volunteers (with contact information and languages spoken) to keep volunteers informed throughout the process. Early feedback. We allowed authors to send initial examples and preliminary versions of their datasets for feedback well before the dataset submission deadline. This contrasts with traditional shared tasks at NLP conferences, where participants have minimal interaction with the organizers prior to submitting. Furthermore, we held FAQ meetings one month before the deadline, held at multiple times to accommodate different time zones, and we maintained consistently-updated set of slides with instructions and FAQs for creating the Global PIQA datasets. Data quantity. We required minimum of 100 examples per language for each submitted dataset. We found that this quantity was doable so as not to discourage researchers from participating, but large enough to ensure that researchers put significant thought into creating their datasets. Timeline and acceptances. The shared task was publicly announced in late June 2025, with submission deadline of September 15. This allowed almost three months to recruit contributors and for groups to develop datasets. The timeline was short enough, however, that no momentum was lost. After the dataset submission deadline, we also continued to allow submissions for languages and dialects that were still missing from the benchmark. We individually reached out to volunteers who had signed up for specific missing languages, and in many cases, we were able to work out later deadlines that were more amenable to those authors. In cases where an initial dataset submission did not meet quality checks (3.3), the dataset was not simply rejected; instead, we worked with the authors to make improvements for the dataset to be accepted. 4Dataset descriptions ranged from single paragraphs to full length papers. Individual dataset descriptions that individual authors have decided to publicly release are on our GitHub. Brief summaries are in G. 5We publicized the Global PIQA task through announcements on the Eleuther AI Discord, the LINGUIST List, Masakhane, X/Twitter, BlueSky, and LinkedIn. 4 3.2 Dataset Construction Methods We asked authors to construct at least 100 examples in their language, all manually checked by native speaker of the language. Translated examples directly from the English PIQA dataset are not included in the non-parallel split of Global PIQA. Authors were asked to construct examples (prompt, solution0, solution1) where (1) the correct solution relates to physical properties of one or more objects, and (2) an average person who speaks the language natively would likely know the answer. We encouraged authors to include culturally-specific examples that might not be easily translatable into English, or that might require regional or cultural commonsense knowledge. Specifically, in the guidelines sent to all authors, we encouraged examples based on local foods, places, everyday objects, customs, traditions, religions, literature, folklore, or art forms. We asked authors to vary the length of their examples (e.g. to include some examples greater than 25 words long), make the two candidate solutions as similar as possible (while still having one be unambiguously correct and the other unambiguously incorrect), and avoid having the incorrect solution be so absurd that it is extremely obvious. Full guidelines sent to authors are on our GitHub. Aside from these guidelines, authors were provided substantial flexibility in creating the datasets for their languages. This is benefit of having researchers construct their own datasets; as native speakers and researchers working in each language, they themselves are experts who can ensure the quality of their respective datasets. This flexibility also allowed each author to construct dataset that was culturally specific to their language and dialect, in the way that they believed was best. Method descriptions for individual languages are in G. Diverse methods. Indeed, authors used wide variety of methods to brainstorm and construct examples. total of 132 groups of authors contributed datasets. We encouraged authors to manually write examples, and 116 out of 132 groups drafted their examples manually (i.e. without the help of LLMs). Some authors (28 groups) wrote examples motivated by content on websites or other resources in their language, such as recipe blogs, DIY pages, question forums, or how-to books. Many groups brainstormed examples based on specific topic categories, such as food, home, clothing, transportation, hobbies, or religion. The vast majority of groups (127 out of 132 groups) explicitly reported making their datasets at least partially culturally-specific, covering local foods, clothing, traditions, everyday life, and/or customs. In line with the task description (3.2), all groups reported writing examples based on everyday topics. For example, one group spent one month adapting examples from naturally-occurring sentences spoken by family and friends, and another group read examples aloud to their parents and grandparents to verify colloquial [language] usage, cultural appropriateness, and everyday realism. All groups had examples written or checked by at least one native speaker, and most groups (85 groups) had multiple native speakers check each example. Brief method details for individual groups are in G, and we highly encourage readers to explore these individual dataset descriptions. small number of groups (5 out of 132 groups) used LLMs to generate topic ideas, but not to generate examples themselves. Another 16 groups used LLMs to initially generate examples, before filtering, editing, and manual verification by the authors. In these cases, LLMs had to be prompted carefully so as not to generate easy and generic examples; for example, one group reported that our preliminary attempts involved using state-of-the-art Large Language Models (LLMs) to generate question candidates. However, we found these outputs to be consistently inadequate (for Tamil). Another group reported that LLMs produced poor quality samples; no such items were included in the final dataset (for Azerbaijani). The 16 groups that used LLMs to generate initial examples reported needing to filter the resulting datasets heavily for quality (e.g. keeping only 14.6% and 22.0% of examples in the two independent groups who reported the proportions of examples kept). 3.3 Compiling the Dataset The next step in constructing the Global PIQA non-parallel split was to run quality checks and compile the dataset for each language. For each language, we standardized column names, added unique example IDs, and normalized language codes to use ISO 639-3 individual language codes (e.g. cmn for Mandarin Chinese, c.f. macrolanguage codes; language code details in B) with ISO 15924 script codes (e.g. latn for Latin script). In cases where dataset used specific dialect within an individual language code, we appended an optional four-letter region code; for example, the Global PIQA language code for Brazilian Portuguese is por_latn_braz. Finally, to inspect the data more 5 easily, we generated machine translations into English using Gemini 2.5 Pro (October 2025). The translation prompt used is in C.1. Additional manual annotation and cultural specificity. Based on these LLM-generated English translations, we dropped examples that did not fit the task description (e.g. we dropped several abstract logic puzzles and complex mathematical reasoning questions). We also dropped examples that seemed trivially easy based on the English translations. Finally, we annotated examples as culturally-specific if they met at least one of three criteria: (1) the example requires knowledge of word that does not translate well into English, e.g. specific food dishes or local brands, (2) the example describes specific holidays, folklore, traditions, or sayings, or (3) the correct solution likely varies by region, e.g. involving local norms, laws, or customs. Annotation details, along with motivations for our operationalization of cultural specificity, are in C.2. In cases where all examples were quite non-culturally-specific, or where dropping trivial and off-task examples led to dataset with under 100 examples in the language, we worked with those authors to reach the 100 example threshold and to increase the number of culturally-specific examples in their dataset if possible. Subsampling. After cleaning but before any subsampling, the full dataset consists of 27K examples covering 116 language codes (B). Because this full dataset is highly skewed across languages and often overwhelmed by non-culturally-specific examples or many examples about similar topics, we subsample to an official non-parallel split of exactly 100 examples per language to use for model evaluations. Subsampling details for the official non-parallel split are in C.3, and we provide an overview here. First, where possible (i.e. when this does not reduce our sample size to less than 100 examples for given language), we filter out examples where the two candidate solutions differ in length by more than 25 bytes, when normalized to English byte equivalents. We also filter out examples whose non-stopword tokens overlap by more than 50% with another example in the dataset, using the per-language tokenizers from Goldfish (Chang et al., 2024).6 This aims to ensure diversity across topics for the official Global PIQA dataset for each language. Finally, we sample 100 examples from this filtered subset for each language. We sample culturallyspecific examples before non-culturally specific examples (as annotated above), and within each of these categories, we first sample examples that did not use any LLMs in the creation process. In the resulting official non-parallel split, 59.9% of examples are annotated as culturally-specific, and only 3.5% of examples are written with the help of LLMs. All examples have been manually validated by at least one native speaker of the respective language, and 72.9% have been validated by multiple native speakers. 3.4 Official Non-Parallel Split The resulting official non-parallel split of Global PIQA contains 100 examples per language for 116 language codes. When excluding region codes, the Global PIQA non-parallel split covers 107 language-script combinations and 101 unique ISO 639-3 language codes. These languages cover five continents, 14 language families, and 23 scripts (writing systems). The full list of languages is in B. Importantly, the dataset contains 59.9% culturally-specific examples, as annotated in 3.3, enabling evaluations across wide variety of global cultures. Across languages, the mean prompt length is 71.4 English character equivalents (C.3), with mean correct solution length of 69.8 and mean incorrect solution length of 68.6."
        },
        {
            "title": "4 Global PIQA: Parallel Split",
            "content": "This paper describes v0.1 of Global PIQA. We are in the process of developing parallel split of Global PIQA. This dataset will consist of PIQA-style items originally written in English, which we will machine translate and send to the authors of the non-parallel split for correction and validation. The vast majority of Global PIQA authors have professional working proficiency in English on top of their native language(s). As discussed in 2, parallel (translated) datasets are inherently biased 6Due to the lack of available resources for many low-resource languages in our dataset, we define stopword tokens as tokens that appear in at least 25% of examples in the Global PIQA dataset for that language. Details for token overlap filtering are in C.3. 6 Figure 3: Accuracy averaged across all languages vs. parameter count for open-weight models. We display names of top-performing models. Shape indicates model family, and color indicates openness (open-weight in purple vs. fully open in pink, including open data). All other models are plotted as gray dots. Chance performance (50%) and performance for Gemini 2.5 Pro and Gemini 2.5 Flash-Lite are plotted as dashed lines. towards the source language, but we will aim to make the English parallel split as non-culturallyspecific as possible. While parallel evaluation datasets do not allow for culturally-specific evaluations, they allow researchers to make more direct comparisons across languages; for example, in Global PIQA, we hope that the parallel split will allow us to determine whether performance differences across languages are due to (1) differences in models physical commonsense reasoning capabilities in different languages, vs. (2) differences in how well the models perform in different cultural contexts, as evaluated in the non-parallel split."
        },
        {
            "title": "5 Results for State-of-the-Art LLMs",
            "content": "Finally, we evaluate existing LLMs on Global PIQA. We find that proprietary models perform well when averaged across all languages, but performance is substantially worse for some languages and regions. Open-weight models generally under-perform relative to closed models, both in aggregate and for each individual language. 5.1 Evaluation Format We evaluate models in one of two formats (Figure 2): completion or prompted. All examples in Global PIQA are amenable to either format. Both versions are implemented in the LM Evaluation Harness (Gao et al., 2024). Completion evaluation: For models that are not tuned to follow instructions (i.e. pretrained-only or base models), we compute the log-probability from the LLM for each candidate solution given the prompt, normalized by the length of each solution in bytes: log(P (solution prompt))/ len(solution). If the correct solution has higher normalized probability than the incorrect solution, then we mark the model correct for that example. Prompted evaluation: For models that are tuned to follow instructions (e.g. the vast majority of proprietary models, and instruction-tuned and RL-tuned open models), we prompt the LLM with the prompt template in Figure 2. We sample up to 2048 tokens, and score the responses using string matching. Evaluation method details are in E. For smaller models (e.g. up through 24B parameters), we find that base models evaluated with the completion format perform better than instruction-tuned (IT) models evaluated with the prompted format. This is consistent with the claim that instruction-following imposes auxiliary task demands that may obscure capabilities in smaller models (Hu and Frank, 2024). In the main text here, we 7 Model Western Europe Eastern Europe Middle East North Africa Subsahar. Africa Central Asia South Asia Southeast Asia East Asia Americas & Oceania Avg. 7-10B Weight Class (IT) Qwen 3 (8B) Gemma 2 (9B) Apertus (9B) Qwen 2.5 (7B) 80.6 78.1 72.6 72.4 12-20B Weight Class (IT) Gemma 3 (12B) GPT-oss (20B) Qwen 3 (14B) Qwen 2.5 (14B) Phi-4 (14B) 83.6 84.6 84.0 80.9 81.9 27-32B Weight Class (IT) Gemma 3 (27B) Qwen 3 (32B) Qwen 2.5 (32B) Gemma 2 (27B) 86.1 82.2 84.6 85.0 70-72B Weight Class (IT) Qwen 2.5 (72B) Llama 3.1 (70B) Apertus (70B) Closed Models (IT) Gemini 2.5 Pro Gemini 2.5 Flash Claude Sonnet 4.5 GPT-5 GPT-5 mini 88.7 83.7 77.7 95.6 94.1 94.6 94.7 93.6 79.1 76.1 73.3 69.0 82.6 81.0 83.2 77.1 78. 86.5 80.7 78.9 81.6 84.6 82.1 78.2 95.2 93.7 93.7 93.9 92.8 74.2 70.5 64.3 69.8 79.8 79.6 76.6 76.7 72.7 82.9 75.1 78.1 77. 82.0 79.2 73.8 92.4 90.2 89.3 89.2 86.3 66.8 64.8 62.0 59.8 78.0 73.8 71.8 72.6 66.0 80.2 71.8 72.2 78.4 76.0 74.8 70. 93.8 90.4 88.4 89.6 83.4 56.3 43.7 55.3 57.5 65.5 65.9 57.6 60.4 58.0 67.2 58.2 60.2 41.8 61.5 66.2 61.7 80.2 76.3 74.7 70.4 72. 70.3 65.0 66.0 59.0 78.5 75.5 75.8 62.7 64.7 80.7 75.8 65.0 70.3 76.0 75.8 70.5 93.2 92.2 90.0 93.2 90.7 76.0 71.1 69.1 64. 80.9 79.3 80.0 72.8 76.0 82.6 78.3 75.1 76.3 77.7 79.8 73.1 90.0 88.1 88.2 83.4 84.8 83.0 79.5 70.2 76.8 82.8 86.3 86.7 81.7 78. 87.3 81.5 86.5 84.7 88.7 83.7 77.2 92.3 91.7 94.2 93.5 92.7 82.4 75.0 67.4 74.4 77.8 81.2 85.8 84.2 77.0 82.2 82.2 85.0 79. 88.2 79.6 74.0 91.0 90.2 91.4 91.4 88.4 86.2 85.2 80.0 83.4 85.6 86.8 88.2 87.6 86.0 89.8 87.2 89.4 88.6 91.8 86.0 84. 97.0 96.4 94.2 96.4 93.0 75.1 70.4 68.3 67.6 79.5 79.1 78.5 74.9 74.5 82.4 76.9 76.8 75.4 80.6 79.2 74.0 91.7 89.8 89.5 88.3 87. Table 1: Accuracies for the top models in each weight class, aggregated across all languages (Avg.) and by region. Top scores per weight class are bolded. All models here are instruction-tuned (IT) models using the prompted evaluation format (5.1). Results for other models are in F. focus on results for models with 7B+ parameters, and thus all results in this section use the prompted evaluation format. In F, we report results for models using the completion format. For all results, we report accuracy, where chance accuracy is 50%. 5.2 Models We evaluate wide range of open, open-weight, and proprietary (closed) models on Global PIQA. As noted in 5.1, we focus on instruction-tuned models (including proprietary models) in the main text here, which we evaluate with the prompted format. Evaluated models include Apertus (HernándezCano et al., 2025), Qwen 2.5 and 3 (Yang et al., 2024; Qwen Team, 2025), Llama 3.1 and 3.2 (Meta AI, 2024), Gemma 2 and 3 (Team Gemma et al., 2024, 2025), Aya and Command (Dang et al., 2024; Cohere Team et al., 2025), GPT-5 (regular, mini, and nano; OpenAI, 2025), Sonnet 4.5 (Anthropic, 2025), and Gemini 2.5 (pro, flash, and flash-lite; Google DeepMind, 2025c,b,a). We also evaluate wide variety of open-weight models that are trained to focus on one language or region; because these models are generally smaller and thus underperform larger models, results for these language-specific models are in F. The full list of models we evaluate is in E.1. Proprietary models are evaluated with thinking on, with 1024-token thinking budget for Gemini and Claude, and medium thinking for GPT-5 (details in E). The open-weight models range from 300M to 72B parameters. 5.3 Results In Table 1, we report accuracies averaged across languages per region in the Global PIQA non-parallel split, along with the overall accuracy per model. The best-performing model overall is Gemini 2.5 Pro, with an average score of 91.7%. Gemini 2.5 Pro achieves the highest score of any model for eight of the ten regions in Table 1. The best open-weight model overall is Gemma 3 27B (average score of 82.4%), outperforming open-weight models even at the 7072B parameter scale. Gemma 3 27B performs best out of the open-weight models for languages in Eastern Europe, the Middle East, North Africa, Sub-Saharan Africa, Central Asia, and South Asia. Overall, open-weight model performance steadily increases as parameter counts increase, but performance begins to plateau around 50B parameters, and there remains gap between the top proprietary models and the strongest 8 open-weight models that we evaluate (9.3% accuracy gap; Figure 3). We hope that Global PIQA will help direct progress towards closing the gap between open and proprietary LLMs. Global PIQA also highlights languages for which state-of-the-art LLMs underperform. There are 18 languages for which the best-performing LLM achieves less than 90% accuracy; this is despite the fact that in ad hoc human evaluations for 12 languages, average human performance was 95.1% (D). In fact, for seven languages, the top LLM score is less than 80%: Burushaski (bsk_arab: 66%), Chakavian (ckm_latn: 74%), Ekpeye (ekp_latn: 60%), Idoma (idu_latn: 71%), Lingala (lin_latn: 68%), Manipuri (mni_mtei: 63%), and Urhobo (urh_latn: 64%). In Sub-Saharan African languages, even the best performing model only reaches an average accuracy of 80.2%  (Table 1)  , compared to 95.6% in Western European languages. We report the full list of best models per language in B."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "In this paper, we have presented Global PIQA, physical commonsense reasoning benchmark covering 116 language varieties. Unlike previous benchmarks, Global PIQA is participatory benchmark, constructed by hand by 335 researchers across 65 countries. This enables the construction of culturally-specific non-parallel split, where 59% of examples reference local foods, clothing, customs, traditions, or other culturally-specific elements. We find that proprietary LLMs perform well overall on Global PIQA, but there are still significant disparities for some languages and regions. Open weight models generally have lower accuracies than proprietary models, but Global PIQA allows researchers to clearly quantify the gap between open and proprietary models in multilingual settings. Notably, Global PIQA measures culturally-specific everyday knowledge, demonstrating that in many languages, areas for improvement can be as simple as everyday reasoning. This contrasts with complex logical reasoning and expert knowledge, which have been the focus of many recent LLM benchmarks. Limitations. Of course, Global PIQA has several limitations. First, the sample size per language is only 100 examples; in the future, we hope that our participatory approach to benchmark construction will facilitate the construction of larger datasets. Second, we note that while Global PIQA contains culturally-specific examples, these examples are snapshots specific to our authors and researchers, not necessarily representative of entire cultures. Cultural stereotypes may be present in the dataset, although all examples are constructed by native speakers of the languages. Finally, we emphasize that more languages is not necessarily better when constructing multilingual benchmarks; researchers should work with communities themselves to determine if and how they want their languages included. In Global PIQA, we have sought to work together with native speakers as authors, giving authors flexibility and ownership over how they construct their datasets. Global PIQA v1. This paper currently describes Global PIQA v0.1; in the coming months, for Global PIQA v1, we plan for significant additions. First, as discussed in 4, we are developing parallel split of Global PIQA, which will double the size of the dataset. In addition, we are still looking for contributors to expand the language coverage of both the non-parallel and parallel splits of Global PIQA. Prospective contributors can register their interest through this form or visit the project website to get involved. We particularly welcome contributions for less-resourced languages and language varieties. We close by noting that the scale of participation in this project has far exceeded the organizers expectations. The result is manually curated, culturally-specific evaluation dataset with unprecedented language coverage. We are excited to continue developing community-led open-source multilingual evaluations, and we believe that this is an extremely promising avenue for addressing the critical lack of benchmarks for the vast majority of the worlds languages."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "See for author list. Global PIQA would not be possible without the efforts of all of the authors. We also thank several anonymous contributors who preferred not to be authors on this paper. The research of Yolanda Xavier is supported by Portuguese national funding through the FCT Portuguese Foundation for Science and Technology, I.P. as part of the project UID/3213/2025 Linguistics Research Centre of NOVA University Lisbon (CLUNL) and by the Doctoral Grant (FCT PhD grant) number 2022.13977.BD from the same funder. Group 0025 is supported by the following grants: CLARIN-PL (POIR.04.02.00-00C002/19, FENG.02.04-IP.040004/24, 2024/WK/01), DARIAH-PL (POIR.04.02.00-00-D006/20, KPOD.01.18-IW.03-0013/23). Annika Simonsen was funded by the European Commission under grant agreement no. 101135671. CEB has been partially funded by the German ministry for education and research (BMBF) through the TRAILS project (grant number 01IW24005). Group 0070 is supported by funding from King Abdullah University of Science and Technology (KAUST) - Center of Excellence for Generative AI, under award number 5940. Group 0079 would like to thank Mr. Sudhir R. Narayana for help with correction and verification of items in their dataset. Sina Ahmadi gratefully acknowledges support from the University of Zurich (UZH) Postdoc Grant (reference number 269093). Group 0133 would like to thank the MbazaNLP community, including students from the University of Rwanda, School of Art and Languages. We would also like to thank Yonatan Bisk for useful insights into the original PIQA dataset."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W., Chen, Y.-C., Chen, Y.-L., Cheng, H., Chopra, P., Dai, X., Dixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao, M., Garg, A., Giorno, A. D., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Hu, W., Huynh, J., Iter, D., Jacobs, S. A., Javaheripi, M., Jin, X., Karampatziakis, N., Kauffmann, P., Khademi, M., Kim, D., Kim, Y. J., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang, C., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M., Liu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh, A., Majercak, D., Mazzola, M., Mendes, C. C. T., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., Perez-Becker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Ren, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X., Tanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang, G., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R., Wen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, F., Yang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y., and Zhou, X. (2024a). Phi-3 Technical Report: Highly Capable Language Model Locally on Your Phone. arXiv preprint arXiv:2404.14219. Abdin, M., Aneja, J., Behl, H., Bubeck, S., Eldan, R., Gunasekar, S., Harrison, M., Hewett, R. J., Javaheripi, M., Kauffmann, P., et al. (2024b). Phi-4 Technical Report. arXiv preprint arXiv:2412.08905. Adebara, I., Elmadany, A., and Abdul-Mageed, M. (2024). Cheetah: Natural Language Generation for 517 African Languages. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1279812823, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Adelani, D. I., Ojo, J., Azime, I. A., Zhuang, J. Y., Alabi, J. O., He, X., Ochieng, M., Hooker, S., Bukula, A., Lee, E.-S. A., Chukwuneke, C. I., Buzaaba, H., Sibanda, B. K., Kalipe, G. K., Mukiibi, J., Kabongo Kabenamualu, S., Yuehgoh, F., Setaka, M., Ndolela, L., Odu, N., Mabuya, R., Osei, S., Muhammad, S. H., Samb, S., Guge, T. K., Sherman, T. V., and Stenetorp, P. (2025). IrokoBench: New Benchmark for African Languages in the Age of Large Language Models. In Chiruzzo, L., Ritter, A., and Wang, L., editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 27322757, Albuquerque, New Mexico. Association for Computational Linguistics. 10 Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. (2025). GPT-OSS-120B and GPT-OSS-20B Model Card. arXiv preprint arXiv:2508.10925. Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, É., Hesslow, D., Launay, J., Malartic, Q., et al. (2023). The Falcon Series of Open Language Models. arXiv preprint arXiv:2311.16867. Alves, D. M., Pombal, J., Guerreiro, N. M., Martins, P. H., Alves, J., Farajian, A., Peters, B., Rei, R., Fernandes, P., Agrawal, S., Colombo, P., de Souza, J. G. C., and Martins, A. F. T. (2024). Tower: An open multilingual large language model for translation-related tasks. An, S., Bae, K., Choi, E., Choi, K., Jungkyu Choi, S., Hong, S., Hwang, J., Jeon, H., Jeongwon Jo, G., Jo, H., et al. (2024). EXAONE 3.5: Series of Large Language Models for Real-world Use Cases. arXiv e-prints, pages arXiv2412. Anthropic (2025). Claude Sonnet 4.5 System Card. Technical report, Anthropic. Version date as published (system card). Arnett, C., Chang, T. A., and Bergen, B. K. (2024). Bit of Problem: Measurement Disparities in Dataset Sizes Across Languages. In Proceedings of the Annual Meeting of the Special Interest Group on Under-Resourced Languages. Artetxe, M., Ruder, S., and Yogatama, D. (2020). On the Cross-lingual Transferability of Monolingual Representations. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 46234637, Online. Association for Computational Linguistics. Bae, K., Choi, E., Choi, K., Jungkyu Choi, S., Choi, Y., Han, K., Hong, S., Hwang, J., Hwang, T., Jang, J., et al. (2025). EXAONE 4.0: Unified large language models integrating non-reasoning and reasoning modes. arXiv e-prints, pages arXiv2507. Bak, Y., Lee, H., Ryu, M., Ham, J., Jung, S., Nam, D. W., Eo, T., Lee, D., Jung, D., Kim, B., et al. (2025). Kanana: Compute-efficient bilingual language models. arXiv preprint arXiv:2502.18934. Bandarkar, L., Liang, D., Muller, B., Artetxe, M., Shukla, S. N., Husa, D., Goyal, N., Krishnan, A., Zettlemoyer, L., and Khabsa, M. (2024). The Belebele Benchmark: Parallel Reading Comprehension Dataset in 122 Language Variants. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 749775, Bangkok, Thailand. Association for Computational Linguistics. Battaglia, P. W., Hamrick, J. B., and Tenenbaum, J. B. (2013). Simulation as an engine of physical scene understanding. Proceedings of the National Academy of Sciences, 110(45):1832718332. BigScience Workshop, Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., et al. (2022). BLOOM: 176B-Parameter Open-Access Multilingual Language Model. arXiv preprint arXiv:2211.05100. Bisk, Y., Zellers, R., Le bras, R., Gao, J., and Choi, Y. (2020). PIQA: Reasoning about Physical Commonsense in Natural Language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):74327439. Chang, T. A., Arnett, C., Tu, Z., and Bergen, B. K. (2024). Goldfish: Monolingual Language Models for 350 Languages. Preprint. Chiu, Y. Y., Jiang, L., Lin, B. Y., Park, C. Y., Li, S. S., Ravi, S., Bhatia, M., Antoniak, M., Tsvetkov, Y., Shwartz, V., and Choi, Y. (2025). CulturalBench: Robust, Diverse and Challenging Benchmark for Measuring LMs Cultural Knowledge Through Human-AI Red-Teaming. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T., editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2566325701, Vienna, Austria. Association for Computational Linguistics. 11 Clark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J. (2020). TyDi QA: Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. Transactions of the Association for Computational Linguistics, 8:454470. Cohere Team, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., et al. (2025). Command A: An enterprise-ready large language model. arXiv preprint arXiv:2504.00698. Conneau, A., Rinott, R., Lample, G., Williams, A., Bowman, S., Schwenk, H., and Stoyanov, V. (2018). XNLI: Evaluating Cross-lingual Sentence Representations. In Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii, J., editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 24752485, Brussels, Belgium. Association for Computational Linguistics. Corrêa, N. K., Sen, A., Falk, S., and Fatimah, S. (2024). Tucano: Advancing Neural Text Generation for Portuguese. arXiv preprint arXiv:2411.07854. Dang, J., Singh, S., Dsouza, D., Ahmadian, A., Salamanca, A., Smith, M., Peppin, A., Hong, S., Govindassamy, M., Zhao, T., et al. (2024). Aya Expanse: Combining Research Breakthroughs for New Multilingual Frontier. arXiv preprint arXiv:2412.04261. DeepSeek-AI (2025). DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. Dou, L., Liu, Q., Zhou, F., Chen, C., Wang, Z., Jin, Z., Liu, Z., Zhu, T., Du, C., Yang, P., Wang, H., Liu, J., Zhao, Y., Feng, X., Mao, X., Yeung, M. T., Pipatanakul, K., Koto, F., Thu, M. S., Kydlíˇcek, H., Liu, Z., Lin, Q., Sripaisarnmongkol, S., Sae-Khow, K., Thongchim, N., Konkaew, T., Borijindargoon, N., Dao, A., Maneegard, M., Artkaew, P., Yong, Z.-X., Nguyen, Q., Phatthiyaphaibun, W., Tran, H. H., Zhang, M., Chen, S., Pang, T., Du, C., Wan, X., Lu, W., and Lin, M. (2025). Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLM. arXiv preprint arXiv:2502.12982. Ekgren, A., Cuba Gyllensten, A., Stollenwerk, F., Öhman, J., Isbister, T., Gogoulou, E., Carlsson, F., Casademont, J., and Sahlgren, M. (2024). GPT-SW3: An autoregressive language model for the Scandinavian languages. In Calzolari, N., Kan, M.-Y., Hoste, V., Lenci, A., Sakti, S., and Xue, N., editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 78867900, Torino, Italia. ELRA and ICCL. Faysse, M., Fernandes, P., Guerreiro, N. M., Loison, A., Alves, D. M., Corro, C., Boizard, N., Alves, J., Rei, R., Martins, P. H., Casademunt, A. B., Yvon, F., Martins, A., Viaud, G., HUDELOT, C., and Colombo, P. (2025). CroissantLLM: Truly Bilingual French-English Language Model. Transactions on Machine Learning Research. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. (2024). The Language Model Evaluation Harness. Gen2B (2025). HyGPT 1.0: Technical Report. Tech. report, Gen2B. Version 1.0, May 9, 2025. Gibson, J. J. (1979). The ecological approach to visual perception. Psychology Press. Gonzalez-Agirre, A., Pàmies, M., Llop, J., Baucells, I., Dalt, S. D., Tamayo, D., Saiz, J. J., Espuña, F., Prats, J., Aula-Blasco, J., Mina, M., Rubio, A., Shvets, A., Sallés, A., Lacunza, I., Pikabea, I., Palomar, J., Falcão, J., Tormo, L., Vasquez-Reina, L., Marimon, M., Ruíz-Fernández, V., and Villegas, M. (2025). Salamandra Technical Report. Google DeepMind (2025a). Gemini 2.5 Flash-Lite Model Card. Technical report, Google DeepMind. Model card, published via DeepMind media server. Google DeepMind (2025b). Gemini 2.5 Flash Model Card. Technical report, Google DeepMind. Model card, stable release. 12 Google DeepMind (2025c). Gemini 2.5 Pro Model Card. Technical report, Google DeepMind. Last updated June 27, 2025. Hammarström, H., Forkel, R., Haspelmath, M., and Bank, S. (2023). Glottolog 4.8. Max Planck Institute for Evolutionary Anthropology, Leipzig. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2021). Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations. Hernández-Cano, A., Hägele, A., Huang, A. H., Romanou, A., Solergibert, A.-J., Pasztor, B., Messmer, B., Garbaya, D., ˇDurech, E. F., Hakimi, I., et al. (2025). Apertus: Democratizing Open and Compliant LLMs for Global Language Environments. arXiv preprint arXiv:2509.14233. Hu, J. and Frank, M. (2024). Auxiliary task demands mask the capabilities of smaller language models. In First Conference on Language Modeling. Hupkes, D. and Bogoychev, N. (2025). MultiLoKo: multilingual local knowledge benchmark for llms spanning 31 languages. arXiv preprint arXiv:2504.10356. Inception (2024). Jais Family Model Card. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Renard Lavaud, L., Lachaux, M.-A., Stock, P., Le Scao, T., Lavril, T., Wang, T., Lacroix, T., and El Sayed, W. (2023). Mistral 7B. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. (2024). Mixtral of Experts. arXiv preprint arXiv:2401.04088. Jones, C. R., Chang, T. A., Coulson, S., Michaelov, J. A., Trott, S., and Bergen, B. (2022). Distrubutional Semantics Still Cant Account for Affordances. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 44. Koto, F., Aisyah, N., Li, H., and Baldwin, T. (2023). Large Language Models Only Pass Primary School Exams in Indonesia: Comprehensive Test on IndoMMLU. In Bouamor, H., Pino, J., and Bali, K., editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1235912374, Singapore. Association for Computational Linguistics. Koto, F., Li, H., Shatnawi, S., Doughman, J., Sadallah, A., Alraeesi, A., Almubarak, K., Alyafeai, Z., Sengupta, N., Shehata, S., Habash, N., Nakov, P., and Baldwin, T. (2024). ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 56225640, Bangkok, Thailand. Association for Computational Linguistics. Kreutzer, J., Briakou, E., Agrawal, S., Fadaee, M., and Kocmi, T. (2025). Déjà vu: Multilingual LLM evaluation through the lens of machine translation evaluation. In Second Conference on Language Modeling. Lai, V., Nguyen, C., Ngo, N., Nguyen, T., Dernoncourt, F., Rossi, R., and Nguyen, T. (2023). Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. In Feng, Y. and Lefever, E., editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 318327, Singapore. Association for Computational Linguistics. Li, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y., Duan, N., and Baldwin, T. (2024). CMMLU: Measuring massive multitask language understanding in Chinese. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1126011285, Bangkok, Thailand. Association for Computational Linguistics. Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., et al. (2021). Few-shot learning with multilingual language models. arXiv preprint arXiv:2112.10668. 13 Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., OHoro, B., Wang, J., Zettlemoyer, L., Kozareva, Z., Diab, M., Stoyanov, V., and Li, X. (2022). Few-shot learning with multilingual generative language models. In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 90199052, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Martins, P. H., Alves, J., Fernandes, P., Guerreiro, N. M., Rei, R., Farajian, A., Klimaszewski, M., Alves, D. M., Pombal, J., Boizard, N., et al. (2025). EuroLLM-9B: Technical report. arXiv preprint arXiv:2506.04079. Meta AI (2024). The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783. Myung, J., Lee, N., Zhou, Y., Jin, J., Putri, R. A., Antypas, D., Borkakoty, H., Kim, E., PerezAlmendros, C., Ayele, A. A., Gutiérrez-Basulto, V., Ibáñez García, Y., Lee, H., Muhammad, S. H., Park, K., Rzayev, A. S., White, N., Yimam, S. M., Pilehvar, M. T., Ousidhoum, N., CamachoCollados, J., and Oh, A. (2024). BLEnD: Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages. In Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., and Zhang, C., editors, Advances in Neural Information Processing Systems, volume 37, pages 7810478146. Curran Associates, Inc. Naveen, P. and Trojovsk`y, P. (2024). Overview and challenges of machine translation for contextually appropriate translations. iScience, 27(10). Ng, R., Nguyen, T. N., Huang, Y., Tai, N. C., Leong, W. Y., Leong, W. Q., Yong, X., Ngui, J. G., Susanto, Y., Cheng, N., et al. (2025). SEA-LION: Southeast asian languages in one network. arXiv preprint arXiv:2504.05747. Nguyen, D. Q., Nguyen, L. T., Tran, C., Nguyen, D. N., Phung, D., and Bui, H. (2023a). PhoGPT: Generative pre-training for Vietnamese. arXiv preprint arXiv:2311.02945. Nguyen, Q., Pham, H., and Dao, D. (2023b). VinaLLaMA: Llama-based Vietnamese foundation model. arXiv preprint arXiv:2312.11011. Nigatu, H. H., Tonja, A. L., Rosman, B., Solorio, T., and Choudhury, M. (2024). The zenos paradox In Proceedings of the 2024 Conference on Empirical Methods of low-resource languages. in Natural Language Processing, pages 1775317774, Miami, Florida, USA. Association for Computational Linguistics. Ociepa, K. and Azurro Team (2024). Introducing APT3-1B-Base: Polish Language Model. Ociepa, K., Łukasz Flis, Kinas, R., Wróbel, K., and Gwozdziej, A. (2025). Bielik v3 Small: Technical Report. OpenAI (2024). Multilingual Massive Multitask Language Understanding (MMMLU). OpenAI (2025). GPT-5 System Card. Technical report, OpenAI. Version dated August 7, 2025. Owen, L., Tripathi, V., Kumar, A., and Ahmed, B. (2024). Komodo: Linguistic Expedition into Indonesias Regional Languages. arXiv preprint arXiv:2403.09362. Piloto, L. S., Weinstein, A., Battaglia, P., and Botvinick, M. (2022). Intuitive physics learning in deep-learning model inspired by developmental psychology. Nature Human Behaviour, 6(9):12571267. Ponti, E. M., Glavaš, G., Majewska, O., Liu, Q., Vulic, I., and Korhonen, A. (2020). XCOPA: Multilingual Dataset for Causal Commonsense Reasoning. In Webber, B., Cohn, T., He, Y., and Liu, Y., editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 23622376, Online. Association for Computational Linguistics. Qwen Team (2025). Qwen3 technical report. 14 Romanou, A., Foroutan, N., Sotnikova, A., Nelaturu, S. H., Singh, S., Maheshwary, R., Altomare, M., Chen, Z., Haggag, M. A., A, S., Amayuelas, A., Amirudin, A. H., Boiko, D., Chang, M., Chim, J., Cohen, G., Dalmia, A. K., Diress, A., Duwal, S., Dzenhaliou, D., Florez, D. F. E., Farestam, F., Imperial, J. M., Islam, S. B., Isotalo, P., Jabbarishiviari, M., Karlsson, B. F., Khalilov, E., Klamm, C., Koto, F., Krzeminski, D., de Melo, G. A., Montariol, S., Nan, Y., Niklaus, J., Novikova, J., Ceron, J. S. O., Paul, D., Ploeger, E., Purbey, J., Rajwal, S., Ravi, S. S., Rydell, S., Santhosh, R., Sharma, D., Skenduli, M. P., Moakhar, A. S., soltani moakhar, B., Tarun, A. K., Wasi, A. T., Weerasinghe, T. O., Yilmaz, S., Zhang, M., Schlag, I., Fadaee, M., Hooker, S., and Bosselut, A. (2025). INCLUDE: Evaluating multilingual language understanding with regional knowledge. In The Thirteenth International Conference on Learning Representations. Rostami, P., Salemi, A., and Dousti, M. J. (2024). PersianMind: Cross-Lingual Persian-English Large Language Model. Roussis, D., Voukoutis, L., Paraskevopoulos, G., Sofianopoulos, S., Prokopidis, P., Papavasileiou, V., Katsamanis, A., Piperidis, S., and Katsouros, V. (2025). Krikri: Advancing Open Large Language Models for Greek. Sarvam AI (2025). Sarvam-M: Explorations in Post Training and Inferencing Optimizations for Hybrid Indic LLM. https://www.sarvam.ai/blogs/sarvam-m. Sengupta, N., Sahu, S. K., Jia, B., Katipomu, S., Li, H., Koto, F., Marshall, W., Gosal, G., Liu, C., Chen, Z., Afzal, O. M., Kamboj, S., Pandit, O., Pal, R., Pradhan, L., Mujahid, Z. M., Baali, M., Han, X., Bsharat, S. M., Aji, A. F., Shen, Z., Liu, Z., Vassilieva, N., Hestness, J., Hock, A., Feldman, A., Lee, J., Jackson, A., Ren, H. X., Nakov, P., Baldwin, T., and Xing, E. (2023). Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. Seth, A., Ahuja, S., Bali, K., and Sitaram, S. (2024). DOSA: Dataset of Social Artifacts from Different Indian Geographical Subcultures. In Calzolari, N., Kan, M.-Y., Hoste, V., Lenci, A., Sakti, S., and Xue, N., editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 53235337, Torino, Italia. ELRA and ICCL. Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, D., Das, D., and Wei, J. (2023). Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations. Singh, S., Romanou, A., Fourrier, C., Adelani, D. I., Ngui, J. G., Vila-Suero, D., Limkonchotiwat, P., Marchisio, K., Leong, W. Q., Susanto, Y., Ng, R., Longpre, S., Ruder, S., Ko, W.-Y., Bosselut, A., Oh, A., Martins, A., Choshen, L., Ippolito, D., Ferrante, E., Fadaee, M., Ermis, B., and Hooker, S. (2025). Global MMLU: Understanding and addressing cultural and linguistic biases in multilingual evaluation. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T., editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1876118799, Vienna, Austria. Association for Computational Linguistics. Son, G., Lee, H., Kim, S., Kim, S., Muennighoff, N., Choi, T., Park, C., Yoo, K. M., and Biderman, S. (2025). KMMLU: Measuring massive multitask language understanding in Korean. In Chiruzzo, L., Ritter, A., and Wang, L., editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 40764104, Albuquerque, New Mexico. Association for Computational Linguistics. Team Gemma, Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M., et al. (2025). Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786. Team Gemma, Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., et al. (2024). Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Thellmann, K., Stadler, B., Fromm, M., Buschhoff, J. S., Jude, A., Barth, F., Leveling, J., Flores-Herr, N., Köhler, J., Jäkel, R., et al. (2024). Towards Multilingual LLM Evaluation for European Languages. arXiv preprint arXiv:2410.08928. Ullman, T. D., Spelke, E., Battaglia, P., and Tenenbaum, J. B. (2017). Mind games: Game engines as an architecture for intuitive physics. Trends in Cognitive Sciences, 21(9):649665. Üstün, A., Aryabumi, V., Yong, Z., Ko, W.-Y., Dsouza, D., Onilude, G., Bhandari, N., Singh, S., Ooi, H.-L., Kayid, A., Vargus, F., Blunsom, P., Longpre, S., Muennighoff, N., Fadaee, M., Kreutzer, J., and Hooker, S. (2024). Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1589415939, Bangkok, Thailand. Association for Computational Linguistics. Voukoutis, L., Roussis, D., Paraskevopoulos, G., Sofianopoulos, S., Prokopidis, P., Papavasileiou, V., Katsamanis, A., Piperidis, S., and Katsouros, V. (2024). Meltemi: The first open Large Language Model for Greek. Wu, M., Wang, W., Liu, S., Yin, H., Wang, X., Zhao, Y., Lyu, C., Wang, L., Luo, W., and Zhang, K. (2025). The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks. arXiv preprint arXiv:2504.15521. Xuan, W., Yang, R., Qi, H., Zeng, Q., Xiao, Y., Feng, A., Liu, D., Xing, Y., Wang, J., Gao, F., et al. (2025). MMLU-ProX: Multilingual Benchmark for Advanced Large Language Model Evaluation. arXiv preprint arXiv:2503.10497. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. (2024). Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115. Version v2 (revised 3 January 2025). Yao, B., Jiang, M., Bobinac, T., Yang, D., and Hu, J. (2024). Benchmarking Machine Translation with Cultural Awareness. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1307813096, Miami, Florida, USA. Association for Computational Linguistics. Yoo, K. M., Han, J., In, S., Jeon, H., Jeong, J., Kang, J., Kim, H., Kim, K.-M., Kim, M., Kim, S., et al. (2024). HyperCLOVA Technical Report. arXiv preprint arXiv:2404.01954. Yüksel, A., Köksal, A., Senel, L. K., Korhonen, A., and Schuetze, H. (2024). TurkishMMLU: Measuring massive multitask language understanding in Turkish. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N., editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 70357055, Miami, Florida, USA. Association for Computational Linguistics. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). HellaSwag: Can machine really finish your sentence? In Korhonen, A., Traum, D., and Màrquez, L., editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Zhang, W., Chan, H. P., Zhao, Y., Aljunied, M., Wang, J., Liu, C., Deng, Y., Hu, Z., Xu, W., Chia, Y. K., Li, X., and Bing, L. (2025). SeaLLMs 3: Open foundation and chat multilingual large language models for Southeast Asian languages. In Dziri, N., Ren, S. X., and Diao, S., editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations), pages 96105, Albuquerque, New Mexico. Association for Computational Linguistics. Zhao, Y., Liu, C., Deng, Y., Ying, J., Aljunied, M., Li, Z., Bing, L., Chan, H. P., Rong, Y., Zhao, D., et al. (2025). Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers. arXiv preprint arXiv:2503.00865. Zosa, E., Luoma, J., Hakala, K., Virtanen, A., Koistinen, M., Luukkonen, R., Reunamo, A., Pyysalo, S., and Burdge, J. (2025). Poro 2: Continued Pretraining for Language Acquisition."
        },
        {
            "title": "A Author Contributions",
            "content": "Global PIQA would not be possible without the efforts of all of the authors. We intentionally do not list authors by contributed language. This is to preserve privacy, as some authors would prefer not to be contacted by large number of unaffiliated projects that require expertise in their language. Correspondence should be sent to the lead authors (tachang@ucsd.edu and catherine@eleuther.ai) or to mrl.benchmarks@gmail.com. Global PIQA is community effort, and it does not necessarily reflect the opinions or views of the authors affiliated organizations. Co-Leads Tyler A. Chang*, UC San Diego Catherine Arnett*, EleutherAI *Equal contribution. Contributors (Alphabetical) Abdelrahman Eldesokey, King Abdullah University of Science and Technology (KAUST) Abdelrahman Sadallah, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) Abeer Kashar, University of Waterloo Abolade Daud, Masakhane Abosede Grace Olanihun, Obafemi Awolowo University Adamu Labaran Mohammed, Independent Adeyemi Praise, Tonative Adhikarinayum Meerajita Sharma, Banasthali Vidyapith Aditi Gupta, International Institute of Information Technology Hyderabad Afitab Iyigun, Boston University Afonso Simplício, NOVA School of Science and Technology, NOVA University Lisbon Ahmed Essouaied, Higher School of Communications of Tunis (SUPCOM) Aicha Chorana, University of Laghouat Akhil Eppa, Independent Akintunde Oladipo, The African Research Collective Akshay Ramesh, Vellore Institute of Technology - Chennai Aleksei Dorkin, University of Tartu Alfred Malengo Kondoro, Hanyang University Alham Fikri Aji, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) Ali Eren Çetintas, Middle East Technical University Allan Hanbury, TU Wien Alou Dembele, RobotsMali Alp Niksarli, Davidson College Álvaro Arroyo, University of Oxford Amin Bajand, Linköping University Amol Khanna, Booz Allen Hamilton Ana Chkhaidze, University of California San Diego Ana Condez, NOVA School of Science and Technology, NOVA University Lisbon Andiswa Mkhonto, Independent Andrew Hoblitzell, Purdue University Andrew Tran, Independent Angelos Poulis, Boston University Anirban Majumder, Amazon Science (work done independently, outside of their role at Amazon) Anna Vacalopoulou, Institute for Language and Speech Processing, Athena Research Center Annette Kuuipolani Kanahele Wong, University of Hawaii at Manoa Annika Simonsen, University of Iceland Anton Kovalev, University of Massachusetts Lowell Ashvanth.S, Cohere Labs Community Ayodeji Joseph Lana, Ekiti State University Barkin Kinay, Robert College Bashar Alhafni, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) Benedict Cibalinda Busole, Independent Bernard Ghanem, King Abdullah University of Science and Technology (KAUST) Bharti Nathani, Banasthali Vidyapith Biljana Stojanovska Ðuric, University of Rijeka Bola Agbonile, Zabbot LLC Bragi Bergsson, Independent Bruce Torres Fischer, University of Hawaii at Hilo Burak Tutar, Middle East Technical University Burcu Alakus Çınar, Middle East Technical University Cade J. Kanoniakapueo Kane, University of Hawaii at Manoa Can Udomcharoenchaikit, Vidyasirimedhi Institute of Science and Technology Catherine Arnett, EleutherAI Chadi Helwe, Lebanese American University Chaithra Reddy Nerella, International Institute of Information Technology Hyderabad Chen Cecilia Liu, Independent Chiamaka Glory Nwokolo, University of Ibadan Cristina España-Bonet, German Research Center for Artificial Intelligence (DFKI) & Barcelona Supercomputing Center (BSC) Cynthia Amol, Maseno University DaeYeop Lee, Pohang University of Science and Technology & Modulabs Dana Arad, Technion Israel Institute of Technology 17 Daniil Dzenhaliou, École Polytechnique Fédérale de Lausanne (EPFL) Gia Nghia Ngo, True North International School Gloria Udhehdhe-oze, University of Port Daria Pugacheva, Artificial Intelligence Research"
        },
        {
            "title": "Harcourt",
            "content": "Institute (AIRI) Dasol Choi, Yonsei University & AIM"
        },
        {
            "title": "Intelligence",
            "content": "Daud Abolade, University of Lagos David Liu, Boston University David Semedo, NOVA School of Science and Technology, NOVA University Lisbon David Stap, NXAI Deborah Popoola, Tonative Deividas Mataciunas, AQ22 Delphine Nyaboke, Independent Dhyuthy Krishna Kumar, Independent Diogo Glória-Silva, NOVA School of Science and Technology, NOVA University Lisbon Diogo Tavares, NOVA School of Science and Technology, NOVA University Lisbon Divyanshu Goyal, Independent DongGeon Lee, Pohang University of Science and Technology Gonçalo Martins, NOVA School of Science and Technology, NOVA University Lisbon Gopi Naga Sai Ram Challagolla, Independent Guijin Son, OneLineAI Gulnaz Abdykadyrova, Independent Hafsteinn Einarsson, University of Iceland Hai Hu, City University of Hong Kong Hamidreza Saffari, Polytechnic University of"
        },
        {
            "title": "Milan",
            "content": "Hamza Zaidi, University of Waterloo Haopeng Zhang, University of Hawaii at Manoa Harethah Abu Shairah, King Abdullah University of Science and Technology (KAUST) Harry Vuong, Independent Hele-Andra Kuulmets, University of Tartu Houda Bouamor, Carnegie Mellon University in Qatar Hwanjo Yu, Pohang University of Science and Technology Duygu Ataman, Middle East Technical Iben Nyholm Debess, University of the Faroe University Ebele Nwamaka Anajemba, Nnamdi Azikiwe University, Awka Egonu Ngozi Grace, Alvan Ikoku Federal College of Education, Owerri Elena Mickel, Independent Elena Tutubalina, Artificial Intelligence Research Institute (AIRI) Elias Herranen, Independent Emile Anand, Cognition AI Emmanuel Habumuremyi, Rwanda Journalists Association Emuobonuvie Maria Ajiboye, Delta State University, Abraka Eryawan Presma Yulianrifat, Universitas Indonesia Islands Ibrahim Ethem Deveci, Middle East Technical University Ikhlasul Akmal Hanif, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) Ikhyun Cho, University of Illinois Urbana-Champaign Inês Calvo, NOVA School of Science and Technology, NOVA University Lisbon Inês Vieira, NOVA School of Science and Technology, NOVA University Lisbon Isaac Manzi, MbazaNLP Ismail Daud, University of Ibadan Itay Itzhak, Technion Israel Institute of Technology Esther Adenuga, The African Research Iuliia (Julia) Alekseenko, IHU Strasbourg & Collective Ewa Rudnicka, Wrocław University of Science and Technology Fabian Schmidt, University of Würzburg Faith Olabisi Itiola, University of Ibadan Faran Taimoor Butt, Moscow Institute of Physics and Technology (MIPT) Fathima Thekkekara, Independent Fatima Haouari, University of Sheffield Filbert Aurelian Tjiaranata, Universitas Indonesia Firas Laakom, King Abdullah University of Science and Technology (KAUST) Francesca Grasso, University of Turin Francesco Orabona, King Abdullah University of Science and Technology (KAUST) Francesco Periti, KU Leuven, Flanders Make Gbenga Kayode Solomon, Adekunle Ajasin University University of Strasbourg & Centre national de la recherche scientifique (CNRS) & INSERM Ivan Belashkin, Independent Ivan Spada, University of Turin Ivan Zhelyazkov, Independent Jacob Brinton, Boston University Jafar Isbarov, Virginia Tech Jaka ˇCibej, University of Ljubljana Jan ˇCuhel, Independent Jan Kocon, Wrocław University of Science and Technology Jauza Akbar Krito, Universitas Gadjah Mada Jebish Purbey, Cohere Labs Community Jennifer Mickel, EleutherAI Community Jennifer Za, Independent Jenny Kunz, Linköping University Jihae Jeong, Pohang University of Science and Technology Jimena Tena Dávalos, Universidad Pedagógica Nacional Unidad 161 Morelia Jinu Lee, University of Illinois Urbana-Champaign João Magalhães, NOVA School of Science and Technology, NOVA University Lisbon John Yi, Boston University Jongin Kim, Boston University Joseph Chataignon, University of Bern Joseph Marvin Imperial, University of Bath & National University Philippines Jubeerathan Thevakumar, University of"
        },
        {
            "title": "Moratuwa",
            "content": "Judith Land, Independent Junchen Jiang, Shanghai Jiao Tong University Jungwhan Kim, NAVER Cloud Kairit Sirts, University of Tartu Kamesh R, Sathyabama Institute of Science and Technology Kamesh V, Sathyabama Institute of Science and Technology Kanda Patrick Tshinu, Tshwane University of Technology Kätriin Kukk, Linköping University Kaustubh Ponkshe, École Polytechnique Fédérale de Lausanne (EPFL) Kavsar Huseynova, Baku Higher Oil School Ke He, Shanghai Jiao Tong University Kelly Buchanan, Stanford University Kengatharaiyer Sarveswaran, University of Jaffna Kerem Zaman, University of North Carolina at Chapel Hill Khalil Mrini, Oracle Kian Kyars, Independent Krister Kruusmaa, Tallinn University Kusum Chouhan, Banasthali Vidyapith Lainitha Krishnakumar, University of Moratuwa Laura Castro Sánchez, Centro Singular de Marcos García, Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS-USC) Marek Suppa, Comenius University in Bratislava Mariam DCiofalo, Independent Marii Ojastu, University of Tartu Maryam Sikander, Cohere Labs Community Mausami Narayan, Independent Maximos Skandalis, Laboratoire dInformatique, de Robotique et de Microélectronique de Montpellier (LIRMM) & Centre national de la recherche scientifique (CNRS) & University of Montpellier Mehak Mehak, Independent Mehmet Ilteris Bozkurt, Middle East Technical"
        },
        {
            "title": "University",
            "content": "Melaku Bayu Workie, Addis Ababa University Menan Velayuthan, University of Jaffna Michael Leventhal, RobotsMali Michał Marcinczuk, CodeNLP (Gdansk, Poland) Mirna Potoˇcnjak, Independent Mohammadamin Shafiei, University of Milan Mridul Sharma, Institute for Research and Innovation in Intelligent Systems (IRIIS) Mrityunjaya Indoria, Banasthali Vidyapith Muhammad Ravi Shulthan Habibi, Universitas Indonesia Murat Kolic, Independent Nada Galant, ˇCakavski sabor Naphat Permpredanun, Independent Narada Maugin, Paris Cité University Nicholas Kluge Corrêa, University of Bonn Nikola Ljubešic, Jožef Stefan Institute Nirmal Thomas, Pratham International Nisansa de Silva, University of Moratuwa Nisheeth Joshi, Banasthali Vidyapith Nitish Ponkshe, University of Minnesota Twin Cities Nizar Habash, New York University (NYU) Abu Dhabi Investigación en Tecnoloxías Intelixentes (CiTIUS-USC) Nneoma C. Udeze, Northwestern University Noel Thomas, Mohamed bin Zayed University of Laura Porrino Moscoso, Universidad Alfonso Artificial Intelligence (MBZUAI) El Sabio Leshem Choshen, Massachusetts Institute of Technology (MIT) & MIT-IBM Watson AI Lab Levent Sencan, Boston University Lilja Øvrelid, University of Oslo Lisa Alazraki, Imperial College London Lovina Ehimen-Ugbede, University of Alicante Luheerathan Thevakumar, Independent Luxshan Thavarasa, University of Moratuwa Mahnoor Malik, NED University of Engineering and Technology Mamadou K. Keita, Rochester Institute of Technology Mansi Jangid, Banasthali Vidyapith Marco De Santis, University of Udine Noémi Ligeti-Nagy, Eötvös Loránd University (ELTE), Research Centre for Linguistics Nouhoum Coulibaly, RobotsMali Nsengiyumva Faustin, University of Rwanda Odunayo Kareemat Buliaminu, University of Benin Odunayo Ogundepo, The African Research Collective Oghojafor Godswill Fejiro, Delta State University, Abraka Ogundipe Blessing Funmilola, University of Ibadan Okechukwu Godspraise, Tonative Olanrewaju Samuel, Stonybrook University Olaoye Deborah Oluwaseun, University of Ilorin Olasoji Akindejoye, University of Ibadan 19 Olga Popova, Artificial Intelligence Research Samuel Rutunda, Digital Umuganda & Institute (AIRI)"
        },
        {
            "title": "MbazaNLP",
            "content": "Olga Snissarenko, Kazakhstan Branch of Lomonosov Moscow State University Onyinye Anulika Chiemezie, Nnamdi Azikiwe University, Awka Orkun Kinay, University of Edinburgh Osman Tursun, Queensland University of"
        },
        {
            "title": "Technology",
            "content": "Owoeye Tobiloba Moses, University of Ibadan Oyelade Oluwafemi Joshua, University of Ilorin Oyesanmi Fiyinfoluwa, University of"
        },
        {
            "title": "Johannesburg",
            "content": "Pablo Gamallo, Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS-USC) Pablo Rodríguez Fernández, Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS-USC) Palak Arora, DIT University Pedro Valente, NOVA School of Science and Technology, NOVA University Lisbon Peter Rupnik, Jožef Stefan Institute Philip Oghenesuowho Ekiugbo, National Institute for Nigerian Languages, Aba Pramit Sahoo, Independent Prokopis Prokopidis, Institute for Language and Speech Processing, Athena Research Center Pua Niau-Puhipau, University of Hawaii at Manoa Quadri Yahya, University of Abuja Rachele Mignone, University of Turin Raghav Singhal, École Polytechnique Fédérale de Lausanne (EPFL) Ram Mohan Rao Kadiyala, Cohere Labs Community Raphael Merx, The University of Melbourne Rapheal Afolayan, University of Ilorin Ratnavel Rajalakshmi, Vellore Institute of Technology - Chennai Rishav Ghosh, Independent Romina Oji, Linköping University Ron Kekeha Solis, University of Hawaii at Manoa Rui Guerra, NOVA School of Science and Technology, NOVA University Lisbon Rushikesh Zawar, Independent Saad Nasir Bashir, Bayero University Kano Saeed Alzaabi, New York University (NYU) Abu Dhabi Sahil Sandeep, Vellore Institute of Technology - Chennai Sai Pavan Batchu, Independent SaiSandeep Kantareddy, Independent Salsabila Zahirah Pranida, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) Sam Buchanan, University of California Berkeley Sander Land, Writer Inc. Sarah Sulollari, University of Vienna Sardar Ali, Independent Saroj Sapkota, Institute for Research and Innovation in Intelligent Systems (IRIIS) Saulius Tautvaisas, Independent Sayambhu Sen, Amazon Alexa Sayantani Banerjee, IIT Madras Sebastien Diarra, RobotsMali SenthilNathan.M, Independent Sewoong Lee, University of Illinois Urbana-Champaign Shaan Shah, University of California San Diego Shankar Venkitachalam, Independent Sharifa Djurabaeva, Dennis-Yarmouth High"
        },
        {
            "title": "School",
            "content": "Sharon Ibejih, Tonative Shivanya Shomir Dutta, Vellore Institute of Technology - Chennai Siddhant Gupta, IIT Roorkee Silvia Paniagua Suárez, Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS-USC) Sina Ahmadi, University of Zurich Sivasuthan Sukumar, University of Moratuwa Siyuan Song, University of Texas at Austin Snegha A., IIT Bombay Sokratis Sofianopoulos, Institute for Language and Speech Processing, Athena Research Center Sona Elza Simon, IIT Bombay Sonja Benˇcina, Parafraza Sophie Gvasalia, Lightcast Sphurti Kirit More, Independent Spyros Dragazis, Boston University Stephan P. Kaufhold, University of California San Diego Suba.S, Independent Sultan AlRashed, King Abdullah University of Science and Technology (KAUST) Surangika Ranathunga, Massey University Taiga Someya, The University of Tokyo Taja Kuzman Pungeršek, Jožef Stefan Institute Tal Haklay, Technion Israel Institute of Technology Tasiu Jibril, Bayero University Kano Tatsuya Aoyama, Georgetown University Tea Abashidze, Independent Terenz Jomar Dela Cruz, Independent Terra Blevins, Northeastern University Themistoklis Nikas, Boston University Theresa Dora Idoko, Benue State University Thu Mai Do, The Dewey Schools Hanoi Tilek Chubakov, Independent Tommaso Gargiani, Independent Uma Rathore, Banasthali Vidyapith Uni Johannesen, University of the Faroe Islands 20 Uwuma Doris Ugwu, Ignatius Ajuru University Yolanda Xavier, Linguistics Research Centre of of Education Vallerie Alexandra Putra, Bina Nusantara"
        },
        {
            "title": "University",
            "content": "Vanya Bannihatti Kumar, Independent Varsha Jeyarajalingam, University of Jaffna Varvara Arzt, TU Wien Vasudevan Nedumpozhimana, Trinity College"
        },
        {
            "title": "Dublin Ireland",
            "content": "Viktoria Ondrejova, Comenius University in"
        },
        {
            "title": "Bratislava",
            "content": "Viktoryia Horbik, Independent Vishnu Vardhan Reddy Kummitha, Independent Vuk Dinic, Independent Walelign Tewabe Sewunetie, African Institute of Mathematical Sciences (AIMS) Research and Innovation Centre (RIC) Winston Wu, University of Hawaii at Hilo Xiaojing Zhao, Hong Kong Polytechnic University Yacouba Diarra, RobotsMali Yaniv Nikankin, Technion Israel Institute of Technology Yash Mathur, Independent Yixi Chen, Zhejiang University Yiyuan Li, University of North Carolina at Chapel Hill"
        },
        {
            "title": "NOVA University Lisbon",
            "content": "Yonatan Belinkov, Technion Israel Institute of Technology & Kempner Institute, Harvard University Yusuf Ismail Abayomi, Obafemi Awolowo"
        },
        {
            "title": "University",
            "content": "Zaid Alyafeai, King Abdullah University of Science and Technology (KAUST) Zhengyang Shan, Boston University Zhi Rui Tam, National Taiwan University Zilu Tang, Boston University Zuzana Nadova, Universidad del País Vasco Evaluation Infrastructure Baber Abbasi, EleutherAI Stella Biderman, EleutherAI Workshop Organizers Catherine Arnett, EleutherAI David Stap, University of Amsterdam Duygu Ataman, New York University Fabian Schmidt, University of Würzburg Hila Gonen, University of Washington Jiayi Wang, University College London Tyler A. Chang, University of California San Diego David Ifeoluwa Adelani, McGill University & Mila"
        },
        {
            "title": "B Language Codes and Included Languages",
            "content": "We normalize all language codes in Global PIQA to use ISO 639-3 individual language codes (three letters), ISO 15924 script codes (four letters), and an optional custom four-letter region code for dialects within an individual language code. For example, the code for Mexican Spanish is spa_latn_mexi, and the code for Peninsular Spanish (as spoken in Spain) is spa_latn_spai. When the language code was unclear for an individual dataset based on the description from the authors, we worked with authors to identify the specific ISO 639-3 and ISO 15924 codes that would best reflect their dataset. For clarity, we note: ISO 639-3 macrolanguage codes are often used in other work for some languages. We use individual language codes for more precision, and here, we show mappings from commonly-used macrolanguage codes to the ISO 639-3 individual codes used in Global PIQA: Mandarin Chinese: zho cmn Cantonese Chinese: zho yue Standard Estonian: est ekk Norwegian Bokmål: nor nob Norwegian Nynorsk: nor nno Nepali: nep npi Iranian Persian (Farsi): fas pes Swahili (Kiswahili): swa swh Northern Uzbek: uzb uzn Standard Malay: msa zsm Central Kurdish: kur ckb Dialects of Arabic are often separate individual ISO 639-3 language codes. In Global PIQA, we have: Iraqi Arabic (Gelet): acm_arab Yemeni Arabic: acq_arab 21 Tunisian Arabic: aeb_arab Gulf Arabic: afb_arab Levantine Arabic (Jordan): apc_arab_jord Levantine Arabic (Lebanon): apc_arab_leba Levantine Arabic (Palestine): apc_arab_pale Levantine Arabic (Syria): apc_arab_syri Modern Standard Arabic: arb_arab Algerian Arabic: arq_arab Najdi (Saudi) Arabic: ars_arab Moroccan Arabic: ary_arab Egyptian Arabic: arz_arab Filipino dataset (language code fil) separate from the Tagalog dataset (language code tgl) is not included, despite the two being considered separate individual language codes in ISO 639-3. This is because native speakers of Tagalog often refer to the two languages interchangeably; Filipino is the standardized national language of the Philippines, but it draws influence primarily from Tagalog. Using these language, script, and optional region codes, Global PIQA contains 116 unique language varieties. This includes 107 unique ISO language-script combinations, 101 unique ISO 639-3 language codes, and 23 unique ISO 15924 script codes. Language counts per region and language family are shown in Table 3. Language families use the top-level families from Glottolog; we note that the Indo-European family is large family including the Armenic, Balto-Slavic, Celtic, Germanic, Indo-Aryan, and Iranian sub-families (among others), with languages ranging from English and Spanish to Hindi and Persian (Farsi). We also note that all of the languages in the Americas region in Global PIQA are originally European languages that are now spoken in the Americas; if possible, we hope to include more indigenous languages of the Americas in the next version of Global PIQA (v1; 6). The full list of languages in Global PIQA, along with the best-performing LLM on Global PIQA for each language, is in Table 2. Table 2: List of all languages in the Global PIQA non-parallel split. Includes language code, language name, language family, the percentage of culturally-specific examples in the official non-parallel split for the language, the best-performing model for the language, and the accuracy of the best-performing model for the language. Burushaski North Azerbaijani Bambara Language name Iraqi Arabic (Gelet) Yemeni Arabic Tunisian Arabic Gulf Arabic Best model GPT-5 GPT-5 Gemini 2.5 Pro Sonnet 4.5 Gemini 2.5 Pro Gemini 2.5 Pro GPT-5 Gemini 2.5 Pro GPT-5 Gemini 2.5 Pro GPT-5 Gemini 2.5 Pro Gemini 2.5 Pro Gemini 2.5 Pro Gemini 2.5 Pro Sonnet 4.5 Gemini 2.5 Flash Gemini 2.5 Pro Sonnet 4.5 Sonnet 4.5 Gemini 2.5 Flash Sonnet 4.5 Language code Afro-Asiatic acm_arab Afro-Asiatic acq_arab Afro-Asiatic aeb_arab Afro-Asiatic afb_arab Northern Tosk Albanian Indo-European als_latn Afro-Asiatic Amharic amh_ethi Afro-Asiatic Levantine Arabic (Jordan) apc_arab_jord Afro-Asiatic apc_arab_leba Levantine Arabic (Lebanon) Afro-Asiatic apc_arab_pale Levantine Arabic (Palestine) Afro-Asiatic Levantine Arabic (Syria) apc_arab_syri Afro-Asiatic Modern Standard Arabic arb_arab Afro-Asiatic Algerian Arabic arq_arab Afro-Asiatic Najdi (Saudi) Arabic ars_arab Afro-Asiatic Moroccan Arabic ary_arab Afro-Asiatic Egyptian Arabic arz_arab Assamese Indo-European asm_beng Turkic azj_latn Mande bam_latn Belarusian Indo-European bel_cyrl Indo-European Bengali ben_beng Indo-European Bengali ben_latn Bhojpuri Indo-European bho_deva Bosnian Indo-European bos_latn Isolate bsk_arab Bulgarian Indo-European bul_cyrl Catalan Indo-European cat_latn Czech Indo-European ces_latn Central Kurdish Indo-European ckb_arab Chakavian Indo-European ckm_latn Sino-Tibetan cmn_hans Sino-Tibetan cmn_hant German Indo-European deu_latn Indo-European dhd_deva Uralic ekk_latn Ekpeye Atlantic-Congo ekp_latn Greek Indo-European ell_grek English Indo-European eng_latn Faroese Indo-European fao_latn Uralic Finnish fin_latn Indo-European French (Canada) fra_latn_cana Indo-European French (France) fra_latn_fran Galician Indo-European glg_latn Indo-European Gujarati guj_gujr Afro-Asiatic Hausa hau_latn Hawaiian (olelo Hawaii) Austronesian haw_latn Afro-Asiatic Hebrew heb_hebr Indo-European Hindi hin_deva Croatian Indo-European hrv_latn Uralic hun_latn Eastern Armenian Indo-European hye_armn Igbo Atlantic-Congo ibo_latn Idoma Atlantic-Congo idu_latn Austronesian ind_latn Icelandic Indo-European isl_latn Isoko Atlantic-Congo iso_latn Italian Indo-European ita_latn Austronesian jav_latn Japonic jpn_jpan Language Cultural family percent 21.0% 80.0% 100.0% 98.0% 37.0% 100.0% 76.0% 39.0% 100.0% 100.0% 100.0% 100.0% 85.0% 100.0% 100.0% 100.0% 19.0% 13.0% 38.0% 100.0% 70.0% 16.0% 35.0% 77.0% 12.0% 58.0% 100.0% 92.0% 46.0% 100.0% 94.0% 87.0% 17.0% 86.0% 100.0% 100.0% 13.0% 41.0% 36.0% 3.0% 46.0% 44.0% 35.0% 75.0% 31.0% 90.0% 100.0% 8.0% Gemini 2.5 Flash 15.0% Gemini 2.5 Pro 12.0% 100.0% Sonnet 4.5 81.0% Gemma SEA LION 9B Qwen 2.5 72B 100.0% Gemini 2.5 Pro 82.0% Gemini 2.5 Pro 40.0% Sonnet 4.5 100.0% Sonnet 4.5 62.0% Gemini 2.5 Flash 36.0% Best acc. 97.0% 95.0% 96.0% 90.0% 97.0% 89.0% 97.0% 95.0% 93.0% 91.0% 95.0% 95.0% 90.0% 94.0% 95.0% 98.0% 96.0% 81.0% 99.0% 96.0% 96.0% 92.0% Gemini 2.5 Pro 100.0% 66.0% Phi 4 97.0% Sonnet 4.5 Gemini 2.5 Pro 100.0% 97.0% Gemini 2.5 Pro 89.0% Gemini 2.5 Pro 74.0% Gemini 2.5 Pro 90.0% Gemini 2.5 Flash 90.0% Gemini 2.5 Pro 98.0% Gemini 2.5 Pro 92.0% GPT-5 97.0% GPT-5 60.0% GPT-5 Mini 89.0% Gemini 2.5 Pro 97.0% Sonnet 4.5 96.0% Gemini 2.5 Pro 96.0% GPT-5 99.0% Sonnet 4.5 96.0% Gemini 2.5 Pro 99.0% Gemini 2.5 Pro 97.0% GPT-5 Mini 97.0% GPT-5 95.0% Gemini 2.5 Pro 95.0% Gemini 2.5 Pro 96.0% Sonnet 4.5 Gemini 2.5 Pro 100.0% 98.0% 96.0% 89.0% 71.0% 98.0% 93.0% 77.0% 97.0% 94.0% 97.0% Mandarin Chinese Mandarin Chinese Dhundari Estonian Javanese Japanese Indonesian Hungarian 23 kan_knda kat_geor kaz_cyrl kin_latn kir_cyrl kor_hang lin_latn lit_latn luo_latn mal_mlym mar_deva mkd_cyrl mni_beng mni_mtei nag_latn nld_latn nno_latn nob_latn npi_deva pan_guru pcm_latn pes_arab pol_latn por_latn_braz por_latn_port ron_latn rus_cyrl rwr_deva sin_sinh slk_latn slk_latn_sari slv_latn slv_latn_cerk snd_arab snd_deva spa_latn_mexi spa_latn_peru spa_latn_spai srp_cyrl srp_latn swe_latn swh_latn tam_taml tel_telu tgl_latn tha_thai tur_latn uig_arab ukr_cyrl urd_arab urd_latn urh_latn uzn_latn vie_latn yor_latn yue_hant zsm_latn zul_latn Kannada Georgian Kazakh Luo Malayalam Marathi Portuguese (Brazil) Portuguese (Portugal) Dravidian Kartvelian Turkic Kinyarwanda Atlantic-Congo Kyrgyz Turkic Koreanic Korean Lingala Atlantic-Congo Lithuanian Indo-European Nilotic Dravidian Indo-European Macedonian Indo-European Sino-Tibetan Manipuri Manipuri Sino-Tibetan Nagamese Indo-European Dutch Indo-European Norwegian Nynorsk Indo-European Indo-European Norwegian Bokmål Indo-European Nepali Indo-European Eastern Panjabi Indo-European Nigerian Pidgin (Naijá) Indo-European Western Farsi Polish Indo-European Indo-European Indo-European Romanian Indo-European Russian Indo-European Marwari Indo-European Sinhala Indo-European Slovak Indo-European Šariš Slovak Indo-European Slovenian Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Indo-European Serbian Indo-European Serbian Indo-European Swedish Indo-European Swahili Atlantic-Congo Dravidian Dravidian Austronesian Tai-Kadai Turkic Turkic Ukrainian Indo-European Urdu Indo-European Urdu Indo-European Urhobo Atlantic-Congo Turkic Austronesian Yoruba Atlantic-Congo Sino-Tibetan Austronesian Zulu Atlantic-Congo Slovenian (Cerkno) Sindhi Sindhi Spanish (Mexico) Spanish (Peru) Spanish (Peninsular) Tamil Telugu Tagalog / Filipino Thai Turkish Uighur Northern Uzbek Vietnamese Yue Chinese (Cantonese) Standard Malay Sonnet 4.5 Gemini 2.5 Pro GPT-5 Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.5 Pro Gemini 2.5 Pro Sonnet 4.5 Gemini 2.5 Pro Gemini 2.5 Pro Gemini 2.5 Flash 95.0% 71.0% 94.0% 29.0% 92.0% 82.0% 94.0% 67.0% 99.0% 16.0% 92.0% 100.0% 68.0% 53.0% 99.0% 98.0% 90.0% 29.0% 86.0% 100.0% 96.0% 84.0% Gemini 2.5 Pro 100.0% 45.0% 88.0% Gemini 2.5 Pro 17.0% 63.0% Gemma 3 27B 100.0% 88.0% Sonnet 4.5 17.0% 91.0% Gemini 2.5 Pro 59.0% 95.0% GPT-5 Mini 15.0% 94.0% Gemini 2.5 Pro 67.0% Sonnet 4.5 100.0% 20.0% 95.0% Gemini 2.5 Flash 17.0% 97.0% GPT-5 61.0% GPT-5 95.0% 57.0% Gemini 2.5 Pro 100.0% 80.0% 98.0% GPT-5 35.0% 95.0% GPT-5 54.0% Sonnet 4.5 100.0% 0.0% 96.0% GPT-5 72.0% 96.0% Gemini 2.5 Flash 52.0% Gemini 2.5 Pro 90.0% 77.0% Gemini 2.5 Pro 100.0% 13.0% 94.0% Gemini 2.5 Flash 50.0% 99.0% Sonnet 4.5 29.0% Gemini 2.5 Pro 87.0% 28.0% Gemini 2.5 Pro 100.0% 80.0% 95.0% Gemini 2.5 Pro 17.0% 99.0% Gemini 2.5 Pro 36.0% 99.0% Qwen 2.5 72B 20.0% 98.0% Gemini 2.5 Pro 100.0% 99.0% GPT-5 Mini 5.0% 98.0% GPT-5 5.0% 99.0% Gemini 2.5 Pro 71.0% 90.0% GPT-5 16.0% 90.0% 100.0% Gemini 2.5 Pro 95.0% 81.0% Gemini 2.5 Flash-Lite 96.0% GPT-5 98.0% 94.0% Gemini 2.5 Flash 49.0% 96.0% GPT-5 100.0% 94.0% Gemini 2.5 Pro 57.0% 97.0% GPT-5 47.0% Gemini 2.5 Pro 98.0% 98.0% Gemini 2.5 Pro 100.0% 44.0% 64.0% Gemma 3 12B 48.0% 93.0% GPT-5 Mini 43.0% 93.0% GPT-5 93.0% 87.0% Gemini 2.5 Pro 100.0% 94.0% Sonnet 4.5 85.0% 97.0% Sonnet 4.5 95.0% 91.0% GPT-5 17.0%"
        },
        {
            "title": "Region",
            "content": "# Langs"
        },
        {
            "title": "Western Europe\nEastern Europe\nSouth Asia\nSubsaharan Africa\nMiddle East\nSoutheast Asia\nEast Asia\nCentral Asia\nNorth Africa\nNorth America\nSouth America\nOceania",
            "content": "14 27 23 16 12 6 5 4 4 2"
        },
        {
            "title": "Language Family",
            "content": "# Langs Indo-European Afro-Asiatic Atlantic-Congo Austronesian Turkic Sino-Tibetan Dravidian Uralic Japonic Kartvelian Koreanic Mande Nilotic Tai-Kadai Isolate 59 16 10 6 6 5 4 3 1 1 1 1 1 1 1 Table 3: Number of languages in Global PIQA per region (left) and per language family (right). Regions are defined in F. Dataset Cleaning, Compilation, and Sampling Details C.1 Non-Parallel Split: Cleaning and Compilation As described in 3, authors contributed datasets to the Global PIQA non-parallel split for their own language(s). At minimum, each dataset contributed to Global PIQA contained prompt, solution0, solution1, and label column. For each dataset, we first removed exact duplicate examples and invalid examples where the two solutions were identical. We normalized column names, moved supplemental information (e.g. topic fields or other columns added by individual groups) to supplement column, and we converted all text fields to use UTF-8 text encoding. For transparency, we annotated any examples that used LLMs to initially generate the example; this is relatively small number of examples (10.1% before subsampling, then 3.5% in the official non-parallel split), and all examples are human validated before inclusion in Global PIQA (see method descriptions in G). For several datasets, we found that sentence completion examples (i.e. examples where the prompt is an incomplete sentence, and the candidate solutions complete the sentence) contained prompts ending with ellipses (...) or underscores (___, i.e. fill-in-the-blank). We removed these ending ellipses and underscores, as the completions are concatenated directly onto the prompts when fed into LLMs in the completion setup (E). As preliminary check, we used Google Translate to translate random subset of 20 examples per dataset, to identify any egregious errors (e.g. all examples far too easy, not following the task format, or large numbers of repetitive examples). Based on this preliminary check, if any datasets were clearly not culturally specific (see annotation guidelines in C.2), we asked the dataset authors for optional revisions to add more culturally-specific examples. In these cases, we asked authors to modify or add examples to include words that are unlikely to translate well into other languages, such as food words, words for types of clothing, or local brand names. After this initial cleaning and revision, to better inspect the data, we used Gemini 2.5 Pro to translate each [prompt] [solution] into English for all datasets. The translation prompt used is in Figure 4, and we accessed Gemini 2.5 Pro (October 2025) through Googles Gemini API using paid API key. The resulting machine translations are available in the publicly-released Global PIQA dataset. Of course, we note that translation quality into English is likely worse for lower-resource languages; for example, the English machine translations for Burushaski (bsk_arab) examples appear consistently poor. Even in high-resource languages, the machine translations sometimes correct the incorrect solution when translating prompt with the incorrect solution. Still, based on the translations, for many languages we were able to spot-check labels in the datasets for examples that had clear correct answers. From this cursory verification, we found two datasets with systematic 25 Translate the following into English. If there are any words that do not translate well into English (e.g. specific foods or cultural items), keep only those words in the original language. Do not respond to the content of the sentence; *only* translate it. Respond only with the translation, with *no* additional text. Text to translate: [TEXT_TO_TRANSLATE] Figure 4: Translation prompt template used to translate examples into English with Gemini 2.5 Pro. errors where the annotated labels were often flipped to be incorrect; we worked with the authors of these datasets to correct and revalidate the labels. Finally, we combined all datasets per language, and we added unique example IDs including group (i.e. dataset) number, example index, and language code. For groups that submitted parallel datasets in multiple languages (e.g. Group 0065 for eight dialects of Arabic, or Group 0042 for Catalan and Peninsular Spanish), the parallel examples have the same group number and example index, only differing in language code. This allows the small number of parallel examples in the non-parallel split of the Global PIQA dataset to still be found. C.2 Non-Parallel Split: English Annotations of Task Adherence and Cultural Specificity Next, we used the machine-generated English translations of all examples to annotate cultural specificity and loose adherence to the task description. Annotations were completed by one of the primary authors, who is native English speaker. Task adherence. We use fairly broad definition of physical commonsense reasoning for Global PIQA. Our definition covers knowledge of physical properties of objects, affordances (types of actions an agent can perform with an object; Gibson, 1979; Jones et al., 2022), and physical and temporal relations. Notably, our definition is much broader than intuitive physics, i.e. the use of mental simulations to predict how objects will behave in some environment, which has been widely studied in cognitive science (e.g. Battaglia et al., 2013; Ullman et al., 2017; Piloto et al., 2022). Following broader definition as in the original PIQA dataset (Bisk et al., 2020), we use the following guidelines for adherence to the task description: 1. Drop examples that consist of complex or abstract logical problem, as these do not fit the task description of physical commonsense reasoning. For example, we drop complex logic puzzles and computer programming questions. 2. Drop examples that appear both generic and extremely easy based on the English translation. For example, we drop examples such as When you heat water, it becomes [hot/cold]. 3. Keep examples that query common knowledge about locations (e.g. locations of cities or famous monuments, or common events to observe in particular cities). We count these under loose definition of physical commonsense. 4. Keep examples that query social or cultural knowledge. These examples often describe regional customs, norms, and traditions, which we would like to remain present in Global PIQA. Additionally, these examples may arguably still be considered physical commonsense due to the embodied nature of everyday human interactions. 5. Where possible, drop examples that query obscure historical factoids. In some languages, there are too few total examples to drop all such examples, so small number of historical knowledge questions may still be present in the dataset. These examples are generally apparent from the machine translations to English. Based on these guidelines, we dropped approximately 2K out of 29K examples in the submitted datasets, before subsampling for the official non-parallel split. In cases where this filtering caused the number of examples in language to drop below 100 examples, we worked directly with authors to reach the 100 example minimum. We note that given our fairly flexible definitions of physical commonsense reasoning, we do not guarantee that the entire Global PIQA dataset evaluates physical commonsense reasoning in strict sense; some examples may be better categorized as social commonsense, cultural knowledge, or common knowledge. 26 Cultural specificity. Because cultural specificity is fairly subjective and perspective-dependent, we attempt to provide clear guidelines for when we annotated an example as culturally-specific. Our definition of culturally-specific covers both culturally-specific examples, i.e. examples that are only relevant in specific region or language, and culturally-sensitive examples, i.e. examples whose solution varies across regions or languages (Myung et al., 2024; Singh et al., 2025). When we use the term culturally specific, we refer to this broad definition. We formulated the guidelines here in an attempt to reduce potential bias and the presence of stereotypes in our annotations of cultural specificity. We annotate examples for cultural specificity using these guidelines: 1. Some datasets have some examples marked as culturally specific by the dataset authors. We annotate these examples as culturally specific; this defers to the authors (members of the cultural communities) to choose examples that they believe reflect their culture, giving more ownership back to the communities themselves. 2. We annotate examples as culturally specific if they describe specific holidays, folklore, traditions, sayings, or aphorisms in the language. 3. We annotate an example as culturally specific if its solution likely varies by region. For example, traffic rules and social norms are likely to vary across regions. 4. If an example contains word that does not translate well into English, then we annotate it as culturally specific. This can include words for local food dishes, traditional objects or articles of clothing, or local brands. We do not count city names (or person names), as many examples that simply mention city are not actually specific to that city. We acknowledge that some words are ambiguously English vs. borrowed from another language; in these cases, we use our best judgment based on how commonly the word is used in English. 5. We do not count the presence of local ingredients or objects if they have widely used English words, such as corn, rice, beans, or many fruits and vegetables, even if these items vary in popularity across regions. In other words, we do not annotate an example as culturally specific solely based on the presence of these items. This guideline aims to reduce bias where some examples might otherwise be annotated as culturally specific based on stereotypical associations between specific foods and corresponding regions or cultures. 6. In cases where the English machine translation appears to be extremely low quality, such that the topic of the example is not clear, we use our best judgment based on the previous guidelines. We lean towards annotating cultural specificity in borderline cases, because we expect that machine translation systems are more likely to perform poorly in culturally-specific scenarios (Naveen and Trojovsk`y, 2024; Yao et al., 2024). Through these annotations, we primarily aim to have coarse filter for cultural specificity, such that we can up-sample culturally-specific examples in the following section. In the full non-parallel dataset (i.e. before subsampling to the official split), 39.0% of examples are annotated as culturally specific. We note that even when marked as culturally specific, many examples do not actually require knowledge of the referenced culturally-specific item or tradition to correctly answer the prompt; in many cases, the culturally-specific element is referenced, but the correct answer can be inferred naively from the rest of the context. C.3 Non-Parallel Split: Subsampling to the Official Split Before subsampling, the Global PIQA non-parallel split is highly skewed across languages. For example, before subsampling, Hindi (hin_deva) and Yoruba (yor_latn) have 1.4K examples each, while many other languages have close to the minimum dataset submission requirement of 100 examples. The full dataset before subsampling is available at https://huggingface.co/datasets/ mrlbenchmarks/global-piqa-nonparallel in the unsampled_full folder, containing 27K examples. Due to the imbalance across languages, we select subsample of 100 diverse and maximally culturally-specific examples in each language as the official non-parallel split of Global PIQA. This enables efficient evaluations of state-of-the-art LLMs across all languages in Global PIQA. When filtering, we apply the following stages per language; we continue to the next stage unless that stage would cause the dataset for the language to fall below 100 examples. This allows us to maximize the quality and diversity of the examples for each language while still maintaining at least 100 examples per language. We note that the extremely low quality examples and off-task examples 27 were already filtered out by the cleaning and annotations in C.1 and C.2. We apply the following filtering stages in order (or until reaching 100 examples in the language): 1. We remove any duplicate prompts, i.e. examples that have the same prompt but different pairs of solutions. This is generally very small number of examples (e.g. one or two examples), and zero examples for most datasets. This filtering step drops total of 60 examples across all languages. Note that exact duplicate examples (i.e. same prompt and same solutions) were already removed in C.1. 2. We filter out examples where the two candidate solutions differ in length by more than 25 English byte equivalents (this is roughly the same as English character equivalents, because most English characters are one byte in UTF-8). We compute English byte equivalents by computing the solution lengths first in raw UTF-8 bytes, then dividing by the languages byte premium (Arnett et al., 2024), which is the estimated number of bytes used to encode text in the language compared to content-matched (parallel) text in English. We perform this filtering step to attempt to minimize any length biases in the dataset, where longer solutions might be assigned systematically lower probabilities than shorter solutions by pretrained-only models, leading to bias towards shorter solutions for those models. This filtering step drops total of 1.6K examples across all languages. 3. We filter out examples whose non-stopword tokens overlap by more than 50% with another example in the dataset. Specifically, we tokenize all examples using the Goldfish tokenizer for the language (Chang et al., 2024). For the 15 Global PIQA languages not covered by the 350 languages in Goldfish, we use simple space-based tokenizer after removing common punctuation symbols; all Global PIQA languages without Goldfish tokenizer use scripts that separate words with spaces. Upon tokenizing all examples, we define stopword tokens as tokens that appear in at least 25% of examples for the language. Then, we sort examples by length (in order to give longer examples priority), and we loop through all examples, dropping any examples in which greater than 50% of its non-stopword tokens are contained in another previously-encountered example. This filtering step aims to increase the diversity of examples in the official Global PIQA non-parallel split, particularly for languages with large numbers of examples covering similar topics. This filtering step drops total of 1.2K examples across all languages. Finally, we sample 100 examples from the filtered subset for each language. We sample culturallyspecific examples before non-culturally specific examples (as annotated in C.2), and within each of these categories, we first sample examples that did not use any LLMs in the creation process. We shuffle the correct and incorrect solutions to balance 0 and 1 labels."
        },
        {
            "title": "D Ad Hoc Human Evaluations",
            "content": "We do not explicitly perform human evaluation study due to the substantial resources that it would take to run study for the large number of languages involved in Global PIQA. However, several groups reported human evaluations on their dataset contributions to the non-parallel split, where native speaker was asked to choose correct solutions without access to the ground truth labels, or inter-annotator agreement percentages were reported (from which we can compute an analogy to human accuracy by treating the other annotators labels as the ground truth). On top of this, we conducted ad hoc human evaluation with one author (a native speaker of Mandarin Chinese) for the Mandarin Chinese datasets (simplified and traditional Chinese characters, cmn_hans and cmn_hant) in the Global PIQA official non-parallel split, after observing somewhat low scores in the language for some models (e.g. GPT-5 with less than 90% accuracy, given that Mandarin Chinese is high-resource language). Accuracies for individual human annotators for the 12 language varieties with available human results are shown in Table 4. In these ad hoc human evaluations, mean human annotator accuracy was 95.1%, and none of the fifteen individual annotators had accuracy below 91%. Of course, we note that there is likely some sampling bias, where dataset authors who chose to run human evaluations were also more likely to construct high quality datasets in the first place. That said, we even observe high accuracies for Mandarin Chinese (95% and 93%), in which we ran our ad hoc human evaluation after dataset submissions and compilation, independent of the dataset authors. These results suggest that human accuracy on the Global PIQA non-parallel split is likely to be at least 90%, and potentially as high as 95%. After running these ad hoc evaluations, examples were updated based on disagreeing labels. 28 Language Acc. Language Slovenian (slv_latn) Serbian (srp_latn) Catalan (cat_latn) Tamil (tam_taml) Algerian Arabic (arq_arab) Mandarin (cmn_hans) 97% Croatian (hrv_latn) 97% Macedonian (mkd_cyrl) 94%, 95%, 98% Estonian (ekk_latn) 95% European Portuguese (por_latn_port) 95% Moroccan Arabic (ary_arab) 95% Mandarin (cmn_hant) Acc. 100% 92% 95% 91%, 95% 95% 93% Table 4: Ad hoc human evaluations, showing accuracies for individual human annotators for various languages, on individual dataset contributions to the Global PIQA non-parallel split. Details in D."
        },
        {
            "title": "E Evaluation Details",
            "content": "As described in 5.1, we evaluate pretrained-only models with the completion evaluation format and instruction-tuned models with the prompted evaluation format. For the completion evaluations, we normalize solution log-probabilities by the length of each solution in bytes; we do not need to use English byte equivalents normalized per language (byte premiums; Arnett et al., 2024), because for each example, this would only multiply each log-probability by constant, leaving the ranking of solutions unchanged. For the prompted evaluations, we use the prompt template in Figure 2, generating maximum of 2048 response tokens. For open and open-weight models, we sample using temperature τ = 0.90 and top-p = 0.80; for closed models, we use temperature τ = 0.90. For thinking models, we allow up to 1024 thinking tokens, with the remainder allocated to the response. Gemini 2.5 and Claude Sonnet 4.5 allow setting this directly, but GPT-5 only supports low, medium, and high thinking. We use medium thinking for GPT-5 with total generation length of 2048 tokens, to be comparable with the other models. After sampling responses, we use string matching (best answer is: or best answer is: B, as specified in the prompt template instructions) to mark answers as correct or incorrect. E.1 Full List of Models We evaluate several large well-known LLMs on Global PIQA, along with wide variety of open and open-weight models. We prioritize models that were requested by the authors of the datasets, and we prioritize models pretrained from scratch over adapted and fine-tuned models. We evaluate Global PIQA on 146 models, including seven closed models and 139 open-weight models: Claude Sonnet 4.5 (Anthropic, 2025) Gemini 2.5 Pro (Google DeepMind, 2025c), Flash (Google DeepMind, 2025b), and Flash-Lite (Google DeepMind, 2025a) GPT-5 (Regular, Mini, and Nano; OpenAI, 2025) APT3 1B (Ociepa and Azurro Team, 2024) Apertus (8B and 70B; Hernández-Cano et al., 2025) Aya Expanse (Dang et al., 2024) BLOOM (560M, 1.1B, 1.7B, 3B, 7.1B; BigScience Workshop et al., 2022) Babel 9B (Zhao et al., 2025) Bielik v3 (1.5B and 4.5B; Ociepa et al., 2025) Cheetah (Adebara et al., 2024) Command 7B and 32B (Cohere Team et al., 2025) Croissant LLM v0.1 1B (Faysse et al., 2025) DeepSeek R1 Distill Qwen (1.5B, 7B 14B; DeepSeek-AI, 2025) EXAONE 3.5 7.8B and 32B (An et al., 2024) and EXAONE 4 1.2B and 32B (Bae et al., 2025) EuroLLM (9B; Martins et al., 2025) Falcon (7B; Almazrouei et al., 2023) GPT-SW3 1.3B (Ekgren et al., 2024) GPT-oss 20B (Agarwal et al., 2025) 29 Ganda Gemma7 and Swahili Gemma8 Gemma 2 (2B, 9B, 27B; Team Gemma et al., 2024) and Gemma 3 (270M, 1B, 4B, 12B, 27B; Team Gemma et al., 2025) Gemma SEA-LION v3 9B and Llama SEA-LION 8B (Ng et al., 2025) Gromenauer 7B9 HyGPT 10B (Gen2B, 2025) HyperCLOVA (500M and 1.5B; Yoo et al., 2024) Jais (1.3B, 2.7B, 6.7B, 30B; Sengupta et al., 2023; Inception, 2024) Kanana 1.5 (2.1B and 8B; Bak et al., 2025) Komodo 7B (Owen et al., 2024) Llama 3.1 (8B base and instruct, 70B base and instruct) and 3.2 (1B base and instruct, 3B base and instruct; Meta AI, 2024) Llama Krikri 8B (Roussis et al., 2025) Meltemi v1.5 7B (Voukoutis et al., 2024) Minerva10 (1B, 3B, and 7B) Mistral v0.1 7B, Mistral v0.3 7B, Mistral Small, and Mixtral v0.1 (Jiang et al., 2023, 2024) PersianMind v1.0 (Rostami et al., 2024) Phi-3 (medium and mini instruct; Abdin et al., 2024a), Phi-3.5 mini instruct, and Phi-4 (full and mini instruct; Abdin et al., 2024b) PhoGPT (7.5B; Nguyen et al., 2023a) Poro 2 8B (Zosa et al., 2025) Qwen 2.5 (500M 1.5B, 3B, 7B 14B, and 32B, 72B; Yang et al., 2024), and Qwen 3 (600M, 1.7B, 4B, 8B, 14B, 32B; Qwen Team, 2025) Sailor2 (1B, 8B, 20B; Dou et al., 2025) Salamandra 2B and 7B (Gonzalez-Agirre et al., 2025) Sarvam-m (Sarvam AI, 2025) SeaLLMs v3 1.5B and 7B (Zhang et al., 2025) TowerBase and TowerInstruct v0.1 7B and 13B (Alves et al., 2024) Tucano 1.1B and 2.4B (Corrêa et al., 2024) vinaLlama (2.7B and 7B; Nguyen et al., 2023b) Viking 7B11 and 13B12 XGLM (1.7b, 2.9, 4.5B, 7.5B; Lin et al., 2021) E.2 Refusals and Overthinking in Proprietary Models We find that GPT-5 and Gemini 2.5 (but not Claude Sonnet 4.5) sometimes refuse to respond to the examples in the prompted format (E), or they sometimes overthink until reaching their token budget. As described in E, Gemini and Claude were explicitly given 1024-token thinking budget and further 1024-token response budget. Because GPT-5 only allows for low, medium, or high thinking settings, we set it to medium thinking with total (thinking plus response) token budget of 2048 tokens. Given that the examples in Global PIQA are fairly brief commonsense queries, this budget was designed to be generous. Still, particularly for small models (e.g. Gemini 2.5 Flash-Lite and GPT-5 Nano), the models sometimes overthink until reaching their maximum tokens. Rates of refusals and overthinking per model are in Table 5. Interestingly, we also find that in lower-resource languages, there are higher refusal rates from Gemini and GPT-5, but not from Claude. In the seven languages where the best model accuracy was less than 80% (bsk_arab, ckm_latn, ekp_latn, idu_latn, lin_latn, mni_mtei, and urh_latn), refusal rates from GPT-5 and Gemini 2.5 Pro were 0.14% and 1.43% respectively, compared to overall averages of 0.02% and 0.09%. In GPT-5, the overthinking rate also increased dramatically for these lower-resource languages, up to 43.7% of examples from an overall average of 4.9%. 7https://huggingface.co/CraneAILabs/ganda-gemma-1b 8https://huggingface.co/CraneAILabs/swahili-gemma-1b 9https://huggingface.co/bertin-project/Gromenauer-7B 10https://huggingface.co/collections/sapienzanlp/minerva-llms 11https://huggingface.co/LumiOpen/Viking-7B 12https://huggingface.co/LumiOpen/Viking-13B 30 Model Refusal Overthinking Claude Sonnet 4.5 Gemini 2.5 Flash-Lite Gemini 2.5 Flash Gemini 2.5 Pro GPT-5 Nano GPT-5 Mini GPT-5 (regular size) 0.00% 0.26% 0.09% 0.09% 0.04% 0.03% 0.02% 0.00% 1.23% 0.65% 0.03% 13.67% 1.30% 4.91% Table 5: Refusal and overthinking rates for proprietary models when evaluated on Global PIQA."
        },
        {
            "title": "F Results Per Region for All Models",
            "content": "Table 6 shows average performance for each region for all models we tested. The regions are defined as: Central Asia: kaz_cyrl (Kazakh), kir_cyrl (Kirghiz), uig_arab (Uighur), uzn_latn (Northern Uzbek). East Asia: cmn_hans (Mandarin Chinese), cmn_hant (Mandarin Chinese), jpn_jpan (Japanese), kor_hang (Korean), yue_hant (Yue Chinese). Eastern Europe: als_latn (Eastern Albanian), azj_latn (North Azerbaijani), bel_cyrl (Belarusian), bul_cyrl (Bulgarian), ces_latn (Czech), ckm_latn (Chakavski), ell_grek (Greek), ekk_latn (Estonian), hrv_latn (Croatian), hun_latn (Hungarian), hye_armn (Eastern Armenian), kat_geor (Georgian), lit_latn (Lithuanian), mkd_cyrl (Macedonian), pol_latn (Polish), por_latn_port (Portuguese), ron_latn (Romanian), slk_latn_sari (Slovak), slv_latn_cerk (Slovenian), srp_cyrl (Serbian), srp_latn (Serbian), tur_latn (Turkish), ukr_cyrl (Ukrainian), bos_latn (Bosnian), rus_cyrl (Russian), slk_latn (Slovak), slv_latn (Slovenian). Middle East: acq_arab (Yemeni Arabic), afb_arab (Gulf Arabic), apc_arab_jord (Jordanian Arabic), apc_arab_leba (Lebanese Arabic), apc_arab_pale (Palestinian Arabic), apc_arab_syri (Syrian Arabic), arb_arab (Standard Arabic), ars_arab (Najdi Arabic), ckb_arab (Central Kurdish), heb_hebr (Hebrew), pes_arab (Western Farsi), acm_arab (Iraqi Arabic). North Africa: aeb_arab (Tunisian Arabic), arq_arab (Algerian Arabic), ary_arab (Moroccan Arabic), arz_arab (Egyptian Arabic). North America: fra_latn_cana (French), spa_latn_mexi (Spanish). Oceania: haw_latn (Hawaiian). South America: por_latn_braz (Portuguese), spa_latn_peru (Spanish). South Asia: ben_latn (Bengali), bho_deva (Bhojpuri), bsk_arab (Burushaski), dhd_deva (Dhundari), guj_gujr (Gujarati), mar_deva (Marathi), mni_beng (Manipuri), nag_latn (Naga Pidgin), npi_deva (Nepali), rwr_deva (Marwari), sin_sinh (Sinhala), snd_arab (Sindhi), snd_deva (Sindhi), tam_taml (Tamil), urd_arab (Urdu), urd_latn (Urdu), asm_beng (Assamese), ben_beng (Bengali), hin_deva (Hindi), kan_knda (Kannada), mal_mlym (Malayalam), mni_mtei (Manipuri), pan_guru (Eastern Panjabi), tel_telu (Telugu). Southeast Asia: ind_latn (Indonesian), jav_latn (Javanese), tgl_latn (Filipino/Tagalog), tha_thai (Thai), vie_latn (Vietnamese), zsm_latn (Malay). Sub-Saharan Africa: amh_ethi (Amharic), bam_latn (Bambara), hau_latn (Hausa), ibo_latn (Igbo), idu_latn (Idoma), iso_latn (Isoko), pcm_latn (Nigerian Pidgin), urh_latn (Urhobo), yor_latn (Yoruba), zul_latn (Zulu), ekp_latn (Ekpeye), kin_latn (Kinyarwanda), lin_latn (Lingala), luo_latn (Luo), swh_latn (Swahili). Western Europe: cat_latn (Catalan), eng_latn (English), fao_latn (Faroese), fin_latn (Finnish), fra_latn_fran (French), glg_latn (Galician), isl_latn (Icelandic), ita_latn (Italian), nld_latn (Dutch), nno_latn (Norwegian Nynorsk), nob_latn (Norwegian Bokmål), swe_latn (Swedish), deu_latn (German), spa_latn_spai (Spanish). 31 Model Western Europe Eastern Europe Middle East North Africa Subsahar. Africa Central Asia South Asia Southeast Asia East Asia Americas & Oceania Avg. Sub-1B Weight Class (PT) Gemma 3 (270M) BLOOM 560M Qwen 2.5 (500M) 1B Weight Class (PT) Gemma 3 (1B) Swahili Gemma (1B) Ganda Gemma (1B) Llama 3.2 (1B) XGLM (1.7B) Qwen 2. (1.5B) BLOOM (1.7B) BLOOM (1.1B) Jais (1.3B) Bielik v3 (1.5B) SeaLLMs v3 (1.5B) Sailor2 (1B) Kanana 1.5 (2.1B) GPT SW3 (1.3B) CroissantLLM v0.1 Minerva v1.0 (1B) Tucano (1.1B) APT3 (1B) 2-3B Weight Class (PT) Gemma 3 (4B) Gemma 2 (2B) XGLM (4.5B) Llama 3.2 (3B) Qwen 2.5 (3B) Salamandra (2B) XGLM (2.9B) BLOOM (3B) Jais (2.7B) Bielik v3 (4.5B) VinaLLaMA (2.7B) Minerva v1.0 (3B) Tucano (2.4B) Cheetah Base 2-3B Weight Class (IT) Qwen 3 (4B) Gemma 3 (4B) Qwen 2.5 (3B) Gemma 2 (2B) Phi 4 mini Phi 3.5 mini Phi 3 mini Bielik v3.0 (4.5B) Llama 3.2 (3B) Salamandra (2B) Tucano (2.4B) 7-10B Weight Class (PT) Apertus (8B) SEA-LION v3 (9B) gemma-2-9b EuroLLM (9B) SEA-LION v3 (8B) Llama-3.1-8B Salamandra (7B) Babel-9B Poro 2 (8B) Qwen 2.5 (7B) Sailor2 (8B) SeaLLMs v3 (7B) Mistral v0.1 (7B) Krikri (8B) Tower v0.1 (7B) Gromenauer (7B) xglm-7.5B Komodo (7B) Viking (7B) Jais (6.7B) BLOOM (7.1B) Kanana 1.5 (8B) Meltemi v1.5 (7B) Falcon (7B) Minerva v1.0 (7B) VinaLLaMA (7B) PersianMind v1.0 PhoGPT (7.5B) 53.3 52.6 52.0 59.8 56.7 53.7 56.7 57.2 54.2 54.2 53.8 52.4 55.5 54.2 50.3 53.6 57.3 54.9 53.7 50.4 47.6 70.6 61.8 61.5 61.3 60.8 62.1 57.5 55.3 56.4 57.6 52.3 55.0 51.9 49.9 77.3 69.2 65.7 61.7 60.5 63.6 63.1 60.9 53.0 14.9 1.4 74.5 72.5 73.6 74.3 68.5 67.9 71.2 61.1 65.8 62.8 58.7 61.5 64.3 61.1 61.9 59.9 58.2 57.9 66.3 57.9 54.7 56.9 57.0 57.8 56.3 52.1 53.8 48. 53.3 53.2 51.7 59.1 55.3 54.6 56.6 53.2 53.8 52.6 52.3 52.4 53.1 53.6 52.9 53.3 53.0 54.1 52.1 51.9 51.7 65.1 59.2 60.9 58.4 55.8 59.2 55.5 53.2 53.2 54.0 54.6 52.9 52.5 49.3 74.8 69.6 63.6 61.6 61.6 57.7 55.3 56.1 53.7 11.8 2.7 70.8 68.7 68.7 68.0 61.3 62.5 65.3 58.1 58.8 57.4 55.6 56.7 59.9 58.0 57.4 57.4 55.6 55.7 55.0 53.7 52.7 54.2 55.4 53.1 53.6 53.6 52.2 49.2 49.7 50.5 50. 55.5 52.1 52.1 49.2 51.5 49.0 52.8 51.2 56.9 52.7 47.7 52.7 50.8 52.3 49.7 48.8 51.0 51.4 59.4 53.0 52.3 53.4 51.5 50.5 50.7 51.9 56.9 53.6 53.3 50.8 49.7 49.2 70.2 67.6 62.2 56.5 55.8 57.0 52.4 49.8 37.6 11.5 6.4 60.5 60.5 60.4 56.3 55.5 56.1 50.9 55.3 52.6 52.9 53.1 52.8 51.1 51.3 51.5 51.2 53.5 51.8 50.6 60.5 53.0 52.2 52.1 48.2 50.8 52.6 50.0 47.2 53.0 51.0 51.6 52.6 51.6 52.8 53.2 53.8 53.6 53.6 52.2 58.6 53.2 53.8 50.8 49.4 52.0 53.8 50.6 52.6 48. 53.8 53.6 52.6 50.2 50.8 53.4 53.0 53.4 55.6 52.6 53.6 54.4 53.6 49.4 63.6 67.6 58.8 56.2 52.0 55.6 57.4 44.4 29.4 9.4 5.0 58.6 57.6 56.8 58.2 54.0 55.8 52.6 55.2 52.0 55.8 55.0 54.0 51.0 50.4 52.2 53.2 51.8 52.4 56.4 56.6 54.8 54.2 53.0 49.8 48.8 53.8 53.2 50.0 63.0 61.1 62.4 62.8 61.1 62.0 64.4 57.0 63.2 60.2 61.0 62.6 62.3 61.3 61.4 62.7 60.7 58.3 61.4 59.7 56.9 65.2 65.0 62.1 65.0 65.1 63.0 59.1 60.3 63.9 62.7 60.2 60.9 61.2 51. 56.9 57.0 55.7 52.3 56.6 55.7 55.1 49.6 44.8 11.4 2.1 63.0 66.4 65.4 63.8 64.0 64.2 65.2 63.1 66.0 62.4 66.0 62.4 63.2 64.1 62.6 63.0 61.0 62.9 60.9 61.4 61.1 62.7 61.2 65.2 62.7 60.1 63.4 50.8 55.5 54.0 53.2 55.8 53.8 54.5 51.5 50.8 53.8 53.0 53.2 54.2 52.5 53.8 53.0 54.8 54.5 54.0 50.5 51.2 52.0 65.8 55.0 57.0 59.7 55.2 52.2 53.2 51.8 54.2 53.8 53.2 50.0 50.0 44.0 66.2 61.5 51.5 46.8 52.8 53.8 53.0 49.5 52.8 13.5 2. 70.2 60.7 62.5 51.2 62.0 60.8 55.5 56.2 54.8 54.5 52.2 55.2 54.2 53.2 54.8 55.5 52.5 55.0 54.8 54.5 54.2 53.5 53.2 53.2 54.0 52.8 51.5 49.8 51.4 52.1 50.3 54.6 53.4 53.0 52.2 52.6 50.0 52.4 52.7 50.7 50.3 50.3 51.9 52.1 50.6 50.6 51.3 51.7 49.2 58.0 52.9 51.2 54.1 50.2 52.2 52.5 52.9 51.5 51.2 51.4 51.0 52.4 52.2 71.7 70.3 56.2 59.0 58.0 52.2 53.5 51.3 54.2 13.2 3.8 57.8 59.1 57.8 53.4 56.9 55.9 50.1 54.1 52.8 51.7 52.1 52.7 51.2 53.2 50.8 51.6 52.4 52.1 50.3 49.8 53.6 52.9 49.2 50.6 51.7 49.7 53.1 52. 55.0 53.8 53.2 59.8 58.7 58.0 53.7 58.0 57.2 57.2 58.7 52.3 49.3 54.7 62.7 48.7 50.2 51.5 51.7 49.8 51.5 68.7 61.5 59.8 56.7 59.8 52.2 58.3 58.5 49.0 52.8 55.0 49.8 49.7 48.7 77.8 71.5 70.3 65.5 61.3 58.8 60.2 54.2 58.8 10.2 2.7 71.7 72.0 69.8 52.7 65.2 63.7 53.8 63.8 60.0 64.8 73.8 61.8 57.8 58.5 55.8 56.0 59.7 59.5 52.3 54.7 58.5 56.0 53.5 51.3 51.7 56.7 52.2 51.2 55.8 53.0 53. 61.8 57.4 56.6 52.4 54.8 57.6 55.2 57.0 53.0 53.6 57.8 55.4 57.8 53.2 50.8 57.4 54.2 52.2 60.6 61.0 60.8 55.6 60.6 53.8 58.0 58.4 54.4 51.6 53.8 55.2 52.6 50.6 78.6 67.8 66.2 61.4 59.0 62.4 57.0 53.6 56.6 10.8 4.6 59.6 64.0 61.8 61.6 57.6 56.6 56.2 61.6 56.4 63.4 63.2 66.8 57.6 54.4 57.0 55.8 56.2 55.8 56.2 56.6 59.8 59.6 53.4 56.4 52.6 54.8 52.8 54.4 54.0 58.0 54.0 59.0 53.0 49.0 56.0 58.0 53.0 50.0 48.0 49.0 47.0 47.0 58.0 56.0 50.0 55.0 54.0 50.0 53. 53.0 51.0 59.0 49.0 52.0 49.0 57.0 54.0 51.0 48.0 48.0 49.0 48.0 52.0 47.0 52.0 51.0 46.0 50.0 46.0 51.0 46.0 47.0 11.0 2.0 53.0 48.0 52.0 43.0 57.0 53.0 52.0 51.0 51.0 54.0 49.0 48.0 50.0 58.0 56.0 50.0 55.0 53.0 54.0 52.0 55.0 46.0 51.0 56.0 49.0 52.0 46.0 51.0 54.0 53.7 53.0 58.5 55.7 55.3 55.2 54.6 54.5 54.5 54.3 54.2 53.9 53.9 53.9 53.8 53.6 53.5 52.8 52.7 51.0 63.8 58.5 58.3 57.8 56.8 56.7 55.7 55.1 55.1 54.9 54.1 53.3 53.1 49. 71.7 68.1 61.8 59.4 59.2 57.9 57.0 54.1 50.7 12.1 3.2 66.2 65.7 65.4 62.4 61.2 61.2 59.9 59.0 58.5 58.4 58.0 57.9 57.8 57.2 56.8 56.5 56.2 56.1 56.1 55.9 55.7 55.6 54.6 54.5 54.0 53.7 53.5 50.0 Table 6: Aggregated results across all regions, for pretrained-only (PT) and instruction-tuned (IT) models. 32 Model 7-10B Weight Class (IT) Qwen3 (8B) Gemma 2 (9B) Apertus (9B) Qwen 2.5 (7B) Poro 2 (8B) Aya Expanse (8B) Krikri (8B) Sailor2 (8B) Llama 3.1 (8B) EXAONE 3.5 (7.8B) Mixtral v0.1 8x7B EuroLLM (9B) Kanana 1.5 (8B) SeaLLMs v3 (7B) Mistral v0.3 (7B) Command (7B) DeepSeek Qwen (7B) Mistral v0.1 (7B) VinaLLaMA (7B) Tower v0.1 (7B) Minerva v1.0 (7B) Meltemi v1.5 (7B) Salamandra (7B) 12-20B Weight Class (IT) Gemma 3 (12B) GPT-oss (20B) Qwen 3 (14B) Qwen2.5-14B-Instruct Phi-4 (14B) DeepSeek Qwen (14B) Sailor2 (20B) Phi 3 medium Tower v0.1 (13B) 27-32B Weight Class (IT) Gemma 3 (27B) Qwen 3 (32B) Qwen 2.5 (32B) Gemma 2 (27B) Sarvam-M Aya Expanse (32B) DeepSeek Qwen (32B) Command (32B) EXAONE 3.5 (32B) Mistral Small Jais (30B) EXAONE 4.0 (32B) 70-72B Weight Class (IT) Qwen 2.5 (72B) Llama 3.1 (70B) Apertus (70B) Poro 2 (70B) Closed Models (IT) Gemini 2.5 Pro Gemini 2.5 Flash Claude Sonnet 4.5 GPT-5 GPT-5 mini Gemini 2.5 Flash-Lite GPT-5 nano Western Europe Eastern Europe Middle East North Africa Subsahar. Africa Central Asia South Asia Southeast Asia East Asia Americas & Oceania Avg. 80.6 78.1 72.6 72.4 70.6 64.8 67.9 65.1 66.6 64.4 74.0 66.2 64.3 65.4 61.5 60.0 60.1 55.4 52.7 52.5 48.8 49.3 18.1 83.6 84.6 84.0 80.9 81.9 79.1 75.3 68.7 54.4 86.1 82.2 84.6 85.0 83.7 80.6 83.1 72.7 68.9 66.6 60.6 70.2 88.7 83.7 77.7 4. 95.6 94.1 94.6 94.7 93.6 91.9 82.6 79.1 76.1 73.3 69.0 68.0 67.1 66.6 62.4 64.0 62.1 66.1 67.3 61.6 60.5 65.2 59.2 59.8 57.4 52.4 52.5 49.3 48.5 19.1 82.6 81.0 83.2 77.1 78.8 73.6 71.6 62.9 52.0 86.5 80.7 78.9 81.6 79.2 77.8 76.4 73.6 67.5 65.9 57.0 58.0 84.6 82.1 78.2 2.8 95.2 93.7 93.7 93.9 92.8 90.4 81. 74.2 70.5 64.3 69.8 62.2 69.7 60.7 62.8 62.0 61.0 57.7 62.8 58.6 59.1 59.2 64.2 52.9 51.9 47.4 48.9 46.6 49.6 19.8 79.8 79.6 76.6 76.7 72.7 74.4 69.9 57.5 51.5 82.9 75.1 78.1 77.5 73.2 79.5 76.2 75.2 59.8 56.5 71.2 61.1 82.0 79.2 73.8 1.7 92.4 90.2 89.2 89.2 86.3 86.2 76.2 66.8 64.8 62.0 59.8 56.2 61.6 58.8 59.6 55.6 54.0 53.6 60.2 53.8 51.6 54.6 60.2 53.0 51.6 47.2 50.8 47.0 44.6 23. 78.0 73.8 71.8 72.6 66.0 66.8 65.6 56.6 50.8 80.2 71.8 72.2 78.4 64.8 72.6 69.6 68.4 54.8 56.2 66.0 44.4 76.0 74.8 70.4 1.2 93.8 90.4 88.4 89.6 83.4 86.4 70.6 56.3 43.7 55.3 57.5 55.9 56.3 55.9 56.5 50.6 54.8 50.0 52.3 54.4 52.8 58.6 50.9 48.8 51.1 48.3 52.1 47.7 49.1 20.1 65.5 65.9 57.6 60.4 58.0 57.1 59.2 55.5 51. 67.2 58.2 60.2 41.8 54.1 58.2 57.6 55.9 57.2 53.9 53.4 44.3 61.5 66.2 61.7 1.6 80.2 76.3 74.7 70.4 72.4 69.5 52.7 70.2 65.0 66.0 59.0 57.0 52.5 52.2 51.5 55.8 53.8 44.0 48.0 52.0 52.2 52.0 50.8 46.0 48.8 51.5 50.8 48.2 41.2 18.8 78.5 75.5 75.8 62.7 64.8 52.2 54.2 52.2 50.8 80.8 75.8 65.0 70.2 64.8 61.5 55.2 52.5 51.0 48.5 50.2 21. 76.0 75.8 70.5 0.5 93.2 92.2 90.0 93.2 90.8 88.0 75.5 76.0 71.1 69.1 64.2 63.0 60.8 60.6 61.8 61.5 62.2 55.7 52.8 58.5 56.0 54.3 59.3 54.3 50.0 48.1 47.0 46.8 43.8 19.2 80.9 79.3 80.0 72.8 76.0 66.9 68.6 55.1 51.2 82.6 78.3 75.1 76.3 79.5 72.8 71.0 67.2 61.0 55.2 51.4 47.0 77.7 79.8 73.1 2. 90.0 88.1 88.2 83.4 84.8 85.2 72.6 83.0 79.5 70.2 76.8 69.3 65.0 67.3 79.2 67.5 61.5 67.7 57.7 65.7 68.2 60.5 60.8 56.2 54.2 53.8 49.3 47.8 48.7 18.5 82.8 86.3 86.7 81.7 78.7 81.8 79.2 65.8 52.8 87.3 81.5 86.5 84.7 79.8 80.0 83.0 74.2 67.3 65.5 58.5 65.5 88.7 83.7 77.2 3.3 92.3 91.7 94.2 93.5 92.7 92.0 86. 82.4 75.0 67.4 74.4 63.8 71.0 65.6 62.4 68.4 66.4 64.0 65.4 70.6 69.2 59.8 68.6 60.8 52.4 54.6 52.2 44.8 43.4 13.6 77.8 81.2 85.8 84.2 77.0 80.0 80.2 67.8 53.4 82.2 82.2 85.0 79.4 79.8 80.6 82.0 76.0 72.4 60.4 57.6 71.2 88.2 79.6 74.0 4.8 91.0 90.2 91.4 91.4 88.4 87.2 80.4 52.0 53.0 47.0 55.0 56.0 54.0 49.0 47.0 53.0 56.0 45.0 50.0 51.0 48.0 52.0 50.0 44.0 52.0 44.0 47.0 42.0 38.0 25. 58.0 55.0 62.0 56.0 51.0 65.0 51.0 53.0 45.0 66.0 56.0 63.0 65.0 53.0 49.0 60.0 48.0 57.0 55.0 53.0 46.0 68.0 56.0 58.0 1.0 95.0 91.0 82.0 91.0 76.0 87.0 52.0 75.1 70.4 68.3 67.6 64.6 64.1 63.2 63.0 62.2 61.7 61.3 60.7 60.6 60.0 60.0 59.5 56.2 53.9 50.8 50.8 47.7 47.6 19.1 79.5 79.1 78.5 74.9 74.5 71.2 70.2 61.2 52. 82.4 76.9 76.8 75.4 75.3 74.7 73.8 69.6 63.9 60.5 58.3 56.0 80.6 79.2 74.0 2.7 91.7 89.8 89.5 88.3 87.4 86.4 75.4 Table 6: Aggregated results across all regions, for pretrained-only (PT) and instruction-tuned (IT) models (continued)."
        },
        {
            "title": "G Individual Dataset Descriptions",
            "content": "Here, we provide brief descriptions of the methods that individual groups used to construct their contributions to the non-parallel split of Global PIQA (3). Longer dataset description papers that authors consented to release are at https://github.com/mrlbenchmarks/global-piqa. Authors were recruited and organized as described in 3.1, and all contributors were offered authorship. The vast majority have chosen to be authors on this paper. This project would not be possible without the efforts of all authors. We note that we intentionally do not list authors with their groups and languages. This is to preserve privacy, as some authors would prefer not to be contacted by large number of unaffiliated projects that require expertise in their language. Group 0000: Hindi (hin_deva: 94 examples) Manually written in English by native Hindi speaker, machine-translated into Hindi using Google Translate, then checked, corrected, and refined by the dataset author. Approximately 25% of examples are designed to be culturally-grounded, with references to specific Indian culinary items, musical instruments, common fauna, and social traditions, such as customs within wedding ceremony. Group 0001: Telugu (tel_telu: 131 examples) Manually written by native Telugu speaker, with examples crafted to reflect realistic scenarios encountered in Telugu households, agriculture, cooking, transportation, and daily problem-solving. Each question was double-checked, and edge cases and ambiguous situations were discarded to ensure high quality. Group 0002: French (Canadian) (fra_latn_cana: 155 examples) Topic ideas were brainstormed using LLMs, but examples were all written manually. All examples were checked or written by native speaker. Group 0003: Yoruba (yor_latn: 120 examples) Examples from English PIQA were translated and culturally adapted to Yoruba by native Yoruba speaker. Care was taken to preserve Yoruba idiomatic forms, and for culturally unique contexts, questions were created directly in Yoruba rather than translated. Culturally-specific domains include cooking, clothing, farming, weather, transportation, religion, household practices, and festivals. Group 0004: French (fra_latn_fran: 100 examples) Manually written by native French speaker, with examples crafted by observing daily life and social interactions, and by browsing French websites for topics such as furniture, home goods, sports, and news. Many examples were designed to be specific to French culture, e.g. including French food and social norms, or how to take the metro in Paris. Group 0005: Finnish (fin_latn: 100 examples) Manually written by native Finnish speaker, with many examples covering Finnish culture and everyday life. Topics include traditional foods, household chores, log cabin terms, saunas, winter activities, reindeer-related terms, and Finnish sports and traditions. Group 0006: Hungarian, Romanian (hun_latn: 100 examples, ron_latn: 100 examples) Examples were written in English, translated into Hungarian and Romanian (by native speakers of those languages), and reviewed by another translator. All translators and editors were offered authorship. Group 0007: Ukrainian (ukr_cyrl: 100 examples) Manually written by native Ukrainian speaker, and checked by another native speaker, both from Western Ukraine. Topics were inspired by Ukrainian websites and blogs, as well as personal knowledge, covering Ukrainian cuisine, traditions, superstitions, and local Ukrainian festivities. Group 0008: Mandarin (cmn_hans: 100 examples, cmn_hant: 98 examples) Manually written by native Mandarin speaker and verified by another native speaker. Examples were balanced across culturally-specific food, clothing and materials, musical instruments, and other objects. Examples were written using Chinese simplified characters, but also translated into traditional characters using Google Translate with human verification. Group 0009: Hebrew (heb_hebr: 67 examples) Manually written by native Hebrew speaker, with examples covering specific Hebrew linguistic constructions, along with Israeli cultural knowledge, such as places, food, climate, and Jewish religion and culture. By design, some items may resist direct translation into other languages, and in some cases, translation may alter the validity of the designated correct answer. Group 0010: Indonesian (ind_latn: 204 examples) Examples were generated with the assistance of ChatGPT (GPT-5) using carefully guided prompts to produce PIQA-style examples. All examples were manually reviewed, corrected, and finalized by native speaker of Indonesian to ensure quality, correctness, and cultural relevance. Because the original LLM-generated examples were often fairly generic, at least 50 examples were manually edited to reflect uniquely Indonesian contexts (e.g. local foods, household practices, and traditional objects). The dataset was written in Standard Indonesian (Bahasa Indonesia). Group 0011: Italian (ita_latn: 100 examples) Manually written by native Italian speaker. ChatGPT was occasionally used to correct typos or to find appropriate words that did not immediately come to mind, but never to generate examples themselves. All final versions of examples were human verified. To include examples reflecting Italian culture, some examples were motivated by online recipes and websites in Italian. Group 0012: Hausa (hau_latn: 100 examples) Manually written by native Hausa speaker, using culturally-relevant themes to motivate example creation. Themes included traveling, food, school, exams, driving, and health. Group 0013: Portuguese (Brazilian) (por_latn_braz: 100 examples) Manually written by native Brazilian Portuguese speaker, covering food, traditions, regional objects, daily activities, and environmental contexts that are common to Brazil, particularly southern Brazil. Group 0014: Dutch (nld_latn: 120 examples) Manually written by native Dutch speaker, using specific culturally-relevant topics to motivate example creation. Topics include bicycle maintenance techniques, preparation of traditional Dutch foods, managing Dutch rainfall, and navigating Amsterdams narrow spaces. All examples were verified by another native speaker. Group 0015: Tagalog / Filipino (tgl_latn: 103 examples) Manually written by native Tagalog speaker. separate Filipino dataset was not included, as many native speakers of Tagalog do not draw strong distinction between the two. Examples in this dataset were written to be culturally-specific to the Philippines, covering three main topics: (1) cooking and baking, (2) crafts and construction of cultural objects, and (3) art, dances, and literature. The author cross-checked information using websites such as Philippine Wikipedia, Philippine government blogs on culture, and informal verification from fellow native speakers living in the Philippines. Group 0016: Vietnamese (vie_latn: 100 examples) Manually written by native Vietnamese speaker, and examples contain Vietnamese cultural contexts such as everyday objects, weather, clothing, routines, safety, school, simple social norms, and holidays. Group 0017: Russian, Iraqi Arabic (Gelet) (rus_cyrl: 100 examples, acm_arab: 121 examples) Manually written by native Russian and Iraqi Arabic (Gelet) speakers, covering everyday topics such as weather, transportation, home safety, work, hobbies, nature, sports, school, and technology. For more culturally-specific subset, approximately 20 examples for Iraqi Arabic were translated from the Modern Standard Arabic dataset from Group 0065; native speaker of Iraqi Arabic selected examples that were culturally relevant to their region. 35 Group 0018: Korean (kor_hang: 100 examples) Manually written and verified by three native Korean speakers. Examples were written to cover popular Korean games, food, and mandatory military service. Group 0019: Mandarin (cmn_hans: 93 examples) Manually written by native Mandarin speaker, covering traditional Chinese culture, food, objects, everyday life, customs, and computer use. Some examples were motivated by reading guidebooks on transportation, cooking, or safety operations. Some examples were also designed to cover recently-developed technologies from within the past five to ten years. Group 0020: Kannada (kan_knda: 99 examples) Manually written by native Kannada speaker, and verified by another native speaker. Examples reflect cultural aspects of Karnataka (an Indian state where Kannada is widely spoken), as well as everyday scenarios. Group 0021: Yoruba (yor_latn: 99 examples) Manually written by native Yoruba speaker, and verified by another native speaker. Examples are written to be relevant to the Yoruba land, including festivals, traditions, foods, and clothing. Group 0022: Slovenian, Croatian, Serbian, Macedonian, Slovenian Cerkno, Chakavian (slv_latn: 100 examples, hrv_latn: 100 examples, srp_latn: 149 examples, srp_cyrl: 150 examples, mkd_cyrl: 100 examples, slv_latn_cerk: 100 examples, ckm_latn: 100 examples) Manually written by native speakers of Slovenian, Croatian, Serbian, Macedonian, and two dialects: Slovenian Cerkno and Croatian Chakavian. Authors attempted to include culturally-relevant examples for their language(s). Examples were motivated by everyday objects, life hacks, recipes, and/or assembly manuals in each language. For each dataset, another co-author with significant understanding of the language or dialect solved the task without access to labels. Human accuracies were 97%, 100%, 97%, and 92%, excluding the two low-resource dialects. Labels were adjusted based on disagreements from this cross-check. Group 0023: Tagalog (tgl_latn: 100 examples) Manually written by native Tagalog speaker, using both common spoken Tagalog (Northern and Manila dialects) and the Filipino dialect. Writing style varies between street-spoken Tagalog and formal Tagalog, and topics focus on daily life in the agricultural town of Talavera, Nueva Ecija (e.g. fishing and cooking). Some examples were inspired by Instructables posts, adapted to be culturally-relevant. Group 0024: French (fra_latn_fran: 115 examples) Manually written and reviewed by native French speakers, using French as spoken in mainland France. Examples were written by observing everyday actions, with distracting information added to some prompts to make the examples more challenging. Group 0025: Polish (pol_latn: 130 examples) Manually written and reviewed by native Polish speakers. Authors drew upon their knowledge of Polish history, culture, customs, and everyday habits. Group 0026: Norwegian Bokmål, Norwegian Nynorsk (nob_latn: 117 examples, nno_latn: 117 examples) Manually written in Norwegian Bokmål by native Norwegian speakers, including examples covering local foods, activities, traditions, folklore, and indigenous culture. Text embedding similarity search and then manual verification were used to ensure that examples were not direct translations of English PIQA. Examples were translated into Norwegian Nynorsk using the Nynorsk dictionary from LEXIN OsloMet, and checked by Norwegian speaker who used Norwegian Nynorsk in school. Group 0027: Malay (zsm_latn: 100 examples) Manually written by native Malay speaker, using Standard Malay (Bahasa Melayu). Examples were designed to cover local commonsense, social norms, food and drink, religious life, and ev36 eryday routines. Examples were written with natural Malay phrasing and colloquial register where appropriate. Group 0028: Faroese (fao_latn: 100 examples) Manually written and reviewed by native Faroese speakers. Approximately 35 examples were written to be specific to the Faroe Islands, focusing on Faroese food preparation and preservation techniques, weather patterns, traditional clothing, wool and knitting, and geography. Group 0029: Urdu (urd_arab: 115 examples) This dataset was written by native Urdu speakers, using Gemini 2.5 Flash and Claude Sonnet 4 for example clarification and refinement. Local websites such as UrduPoint were used to motivate examples, and examples were designed to reflect everyday life in Pakistan, including Pakistani food preparation, household practices, social customs, and traditional crafts. The dataset is written in Standard Pakistani Urdu, with every example checked by at least two native speakers. Group 0030: Uzbek (uzn_latn: 101 examples) Manually written by native Northern Uzbek speaker, drawing from real-life experiences and commonly-used expressions in Uzbek. Colloquial phrases are used where appropriate. The dataset is written using Latin script, although Cyrillic script is also widely used in Uzbekistan. Group 0031: Icelandic (isl_latn: 100 examples) Manually written by native Icelandic speakers, covering culturally-specific topics such as food and cooking, holidays and traditions, civics and culture, folklore, geography, history, and agriculture. Some examples were inspired by browsing the Icelandic science web (https://www. visindavefur.is/). Group 0032: Bengali (ben_beng: 102 examples) Manually written by native Bengali speaker, with culturally grounded examples reflecting daily life in Bangladesh and West Bengal, India. Examples were written to reflect everyday topics such as household chores, seasonal weather, agriculture, cooking, storage, and material interactions. Group 0033: Tunisian Arabic (aeb_arab: 100 examples) This dataset was created using mix of manual writing and LLM generation, with all examples verified by two native speakers of Tunisian Arabic. The examples are written to reflect everday life in Tunisia, including cooking practices, traditional music and instruments, household activities, local customs, and everyday objects. Because Tunisian Arabic is primarily spoken dialect with no standardized orthography, some linguistic variation may appear across examples. Group 0034: Marathi (mar_deva: 119 examples) Manually written by native Marathi speakers, using Marathi as spoken in Pune City, Maharashtra, India (i.e. Puneri dialect). Examples were written to cover culturally-specific everyday topics such as education and exams, cooking and household activities, sports and games, and shopping and technology. Group 0035: Japanese (jpn_jpan: 101 examples) One subset of this dataset was created by native Japanese speakers using ChatGPT to translate English PIQA examples and to replace lexical elements with Japanese-specific counterparts. Another subset prompted ChatGPT to generate novel Japanese examples that required knowledge of Japanese cultural norms and conventions. Of the translated subset, 35 out of 145 passed quality checks by the native speakers, and of the novel generations, 66 out of 300 generated examples passed quality checks. All examples were verified by two native Japanese speakers. Group 0036: Italian (ita_latn: 120 examples) Manually written by native Italian speakers, covering household, cuisine, and entertainment domains, focusing on everyday scenarios reflecting local Italian practices. All examples were validated for fluency, correctness, and adherence to the task description by another native speaker. Group 0037: Indonesian (ind_latn: 120 examples) Manually written and verified by native Indonesian speakers, with examples motivated by the authors general knowledge, past experiences, and daily life activities. By design, some prompts incorporated culturally specific Indonesian elements, such as food and traditional musical instruments. All examples were checked by at least two native speakers. Group 0038: Vietnamese (vie_latn: 120 examples) Manually written and verified by native Vietnamese speakers, highlighting both Kinh Vietnamese culture and minority ethnic culture (e.g. from the 50+ ethnic minority groups in present-day Vietnam). Examples cover culturally-specific knowledge such as cooking and farming methods, folklore, traditions, well-known cultural events, and minority ethnic culture. All examples were checked by at least two native speakers. Group 0039: Korean (kor_hang: 441 examples) Korean questions were collected from Naver Knowledge iN1, popular Korean Q&A platform, covering diverse everyday scenarios where Korean users seek practical advice on physical tasks and problem-solving. Qwen3-4B, Qwen3-32B, and HCX-14B were used to identify PIQA-style questions, keeping only questions where all three models unanimously agreed that the question fit the task description (less than 1% of the originally collected examples). Then, GPT-4o was used to refine questions and generate incorrect solutions. Two native Korean speakers independently validated each question, improving question clarity, calibrating difficulty levels, and verifying cultural appropriateness. KoSentenceBERT was used to removed near-duplicate questions. Of the final dataset, approximately 85 questions contain elements specific to Korean culture such as traditional foods and cooking methods, clothing care, housing systems, specialized appliances, and cultural practices. Group 0040: Urdu (urd_arab: 99 examples, urd_latn: 100 examples) Manually written by native Urdu speaker using Latin script, in line with the way many Pakistanis communicate on social media platforms. Examples were transliterated into Urdu script using Gemini 2.5 Flash and then manually verified. Group 0041: Hebrew (heb_hebr: 209 examples) Manually written by native Hebrew speakers, with each example verified by another native speaker. Approximately 55 examples cover everyday Israeli life or Jewish religious practices, including recipes, household cleaning techniques, cultural traditions, and religious customs. For some examples, motivation for topics came from Wikipedia articles or from lists of everyday objects obtained by prompting LLMs. Group 0042: Catalan, Peninsular Spanish (cat_latn: 100 examples, spa_latn_spai: examples) Manually written in Catalan by native Catalan and Spanish speaker, covering everyday topics such as clothing, festivity, folklore, food, literature, music, and sports. Many examples include concepts and situations that are specific to Catalan-speaking communities, and some examples do not translate well into other languages. The Catalan dataset underwent human evaluation by three native speakers, who achieved accuracies of 94%, 95%, and 98% respectively; examples were then adjusted based on this cross-checking. The dataset was translated into Spanish using Google Translate, then post-edited by the same native speaker, keeping examples for Spanish only if they remained valid after translation. Group 0043: Polish (pol_latn: 103 examples) Manually written by native Polish speaker based on physics topics, including fundamental laws of physics, material properties, and principles governing interactions between materials. Online materials describing at-home basic experiments were used to motivate some examples, and several Polish-specific words (e.g. cooking and food items) were used. Group 0045: Belarusian (bel_cyrl: 183 examples) Manually written in conversational Belarusian by native Belarusian speakers, inspired by household situations, local customs, and guides on Belarusian life. LLMs were then used for paraphrasing, 38 lengthening examples, and normalizing style, and then all examples were checked again by two native speakers. Group 0046: Swedish (swe_latn: 98 examples) Manually written by native Swedish speaker, and checked by another native speaker. Roughly half of examples include Swedish slang, traditions, or foods, or hard-to-translate Swedish words. Group 0047: Bulgarian (bul_cyrl: 122 examples) Manually written by native Bulgarian speaker, and checked by another native speaker. Examples are designed to test specific types of physical commonsense reasoning, with distractors (incorrect solutions) that are still semantically related to the prompts. Examples are interwoven with Bulgarian cultural elements and require knowledge of Bulgarian morphological cues (e.g. word inflections). Group 0048: Mandarin, Cantonese (cmn_hans: 407 examples, yue_hant: 223 examples) Manually written and reviewed by native Mandarin and Cantonese speakers, based on online encyclopedias and guidebooks in Mandarin and Cantonese. Example domains include activities (e.g. sports), food, geography, and art. Group 0049: Yoruba, Igbo, Naija (Nigerian Pidgin), Hausa, Isoko, Urhobo, Idoma (yor_latn: 974 examples, ibo_latn: 432 examples, pcm_latn: 247 examples, hau_latn: 213 examples, iso_latn: 107 examples, urh_latn: 119 examples, idu_latn: 101 examples) Manually written by native speakers of Yoruba, Hausa, Igbo, Idoma, Urhobo, Naija (Nigerian Pidgin English), and Isoko, as part of community effort by the Linguistics Island community of linguists. Examples cover specific linguistic structures, and topics include food, culture, education, and technology. Group 0050: Bengali, Mandarin, Greek, Korean, Turkish (ben_beng: 50 examples, cmn_hans: 48 examples, cmn_hant: 17 examples, ell_grek: 50 examples, kor_hang: 50 examples, tur_latn: 49 examples) Manually written by native speakers of Bengali, Mandarin (Taiwanese using traditional characters, mainland using simplified characters), Greek, Korean, and Turkish. All examples were checked by another native speaker of the language. Many examples were written by first thinking of culturallyspecific item, then brainstorming physical properties of that item that could be incorporated into PIQA-style example. Group 0051: Uyghur (uig_arab: 132 examples) Manually written by native speaker of Uyghur, with each example proofread by five native speakers and using Uyghur spell-checker. Examples were inspired by Uyghur literary materials, including cultural and traditional texts, proverbs and sayings, folklore collections, and instructional manuals. Group 0052: Urdu (urd_arab: 96 examples) Manually written by native speaker of Urdu, covering domains such as cooking, religion, weather, science, and household activities. Examples were designed to cover regional cuisine, local household items, and local daily practices. LLMs were used to brainstorm ideas, but not to generate final examples. Group 0053: Bengali (ben_latn: 100 examples) Manually written by native Bengali speaker using Banglish, or Bengali language written in Latin script, often used by Bengali speakers in online settings and informal communication. Examples cover culturally-specific topics such as Bengali religious festivals and practices, traditional foods and cooking, household objects and tools, traditional games and activities, seasonal practices and nature, and folk traditions and customs. ChatGPT was used to brainstorm additional cultural topics, but not to generate examples. Group 0055: Estonian (ekk_latn: 100 examples) Manually written by native Estonian speakers, covering culturally relevant elements such as traditional Estonian foods, local materials, and region-specific practices. Inspiration for some examples was drawn from the Maybe Im Lucky feature of Sõnaveeb, the language portal maintained by the Institute of the Estonian Language, generating randomly-selected Estonian words. Examples were each tested on six randomly-selected LLMs, and examples that all models got correct were dropped or edited. For human evaluation, another native speaker achieved an accuracy of 95%; examples were then adjusted based on this cross-checking. Group 0056: Dutch (nld_latn: 100 examples) This dataset was constructed by native Dutch speaker using hybrid LLM and manual approach, then reviewed by another native speaker. It includes culturally-relevant topics such as chocolate sprinkles on bread, ice skating, dikes, local sports, and specific dishes. LLMs, including GPT5, Gemini 2.5 Pro, and Claude Sonnet 4, were used in drafting samples, suggesting topics, and proofreading, but overall, their performance was found to be severely lacking in understanding the task and generating suitable examples without significant refinement. Group 0057: Estonian, Persian (Farsi), Swedish (ekk_latn: 105 examples, pes_arab: 123 examples, swe_latn: 100 examples) The Estonian part of this dataset was manually written by native Estonian speaker, and reviewed by another native speaker. Topics include Estonian food, companies, places, cultural events and holidays, and typical activities and phenomena during different seasons of the year. The Farsi part of this dataset was manually written and reviewed by native Farsi speakers, covering six thematic categories: cooking and food, housekeeping and cleaning, daily life and social customs, driving and travel, health and safety, and life hacks and tools. The dataset emphasizes cultural and contextual knowledge, and inspiration was drawn from online articles in Farsi. The Swedish part of this dataset was manually written by native Swedish speaker, and reviewed by another native speaker, drawing inspiration from online sources that cover everyday physical activities (e.g. sports, gardening, household life, traditional festivities, and traffic-related scenarios). Group 0058: Hindi, Sindhi, Punjabi, Manipuri, Bengali, Gujarati, Marathi, Nepali, Bhojpuri, Marwari, Dhundhari, Nagamese (hin_deva: 117 examples, snd_deva: 116 examples, pan_guru: 117 examples, mni_beng: 117 examples, bho_deva: 117 examples, guj_gujr: 94 examples, mar_deva: 117 examples, npi_deva: 93 examples, ben_beng: 117 examples, rwr_deva: 117 examples, dhd_deva: 116 examples, nag_latn: 117 examples) Examples in this dataset were primarily adapted from reasoning textbooks in English and Hindi that are widely used for preparation for competitive exams. Examples were written to reflect India-specific cultural contexts. Each example was manually or semi-automatically (i.e. machine-translated with human verification) translated into the 12 target languages, with careful preservation of meaning, cultural familiarity, and syntactic naturalness. All examples were independently labeled by two native speakers to ensure validity. Group 0059: Lingala (lin_latn: 102 examples) Manually written by native Lingala speaker, covering culturally-specific everyday contexts and daily life. Group 0060: Greek (ell_grek: 206 examples) Manually written and reviewed by native Greek speakers. Some prompts are adapted from variety of online material, including government and non-governmental organization (NGO) publications, academic theses, course presentations, commercial product brochures, and Wikipedia. Approximately 40% of the final examples are annotated by the authors as culturally specific. Group 0061: Sindhi (snd_arab: 139 examples) Manually written by native Sindhi speaker, using Standard Sindhi (Vicholi Sindhi) in the PersoArabic script. Examples are culturally grounded in folklore, history, literature, foods, festivals, traditions, and everyday life in Sindh, Pakistan. Group 0062: Swahili, Dhuluo, Lingala (swh_latn: 220 examples, luo_latn: 101 examples, lin_latn: 188 examples) The dataset was manually written and reviewed by native speakers of Swahili, Dholuo, and Lingala, covering topics such as food, agriculture, transportation, and household practices. The Swahili examples are split between Kenyan and Tanzanian Swahili; these two varieties are structurally similar, but Tanzanian contributions emphasize domestic and rural practices, while Kenyan contributions 40 highlight more urban contexts. The Lingala examples focus on rural life in Central Africa, including cassava preparation, termite cooking, fishing, river transport, market trading, and home construction. Group 0063: Albanian (als_latn: 106 examples) Manually written by linguist specializing in Albanian and native speaker of Albanian. Topics cover domains such as cooking, cleaning, object construction, Albanian traditional activities (e.g. music, dances, weddings), cultural practices, and agricultural tasks. The authors note that both dataset creators primarily reside outside the main Albanian-speaking continuum, potentially affecting the representativeness of the selected topics. Group 0064: Indonesian (ind_latn: 228 examples) This dataset was created by native Indonesian speakers using GPT-4o with careful prompting to generate culturally-specific examples. Topics include agriculture, art, daily activities, family relationships, fisheries and trade, food, religious holidays, traditional games, and wedding traditions. Examples were filtered for fluency, correctness, and adherence to the task format, and SentenceBERT was used to filter out near-duplicate examples. All examples were reviewed and edited by two native Indonesian speakers, using Standard Indonesian (Bahasa Indonesia). The filtering stages (including filtering for ambiguous solutions) resulted in removing 85.4% of the original LLM-generated examples. Group 0065: Modern Standard Arabic, Syrian Arabic, Emirati Arabic, Tunisian Arabic, Algerian Arabic, Moroccan Arabic, Egyptian Arabic, Palestinian Arabic (arb_arab: 115 examples, apc_arab_syri: 115 examples, afb_arab: 115 examples, aeb_arab: 115 examples, arq_arab: 115 examples, ary_arab: 115 examples, arz_arab: 114 examples, apc_arab_pale: 115 examples) Manually written by native speakers of eight Arabic dialects (including Modern Standard Arabic). Examples were written by all of the authors to be balanced across locales, and the resulting dataset was translated into each Arabic dialect by the respective native speaker. Domains covered include household, clothing, cooking, hospitality, events, and religion. Group 0066: Galician (glg_latn: 109 examples) Manually written and reviewed by native Galician speakers. Approximately half of the dataset covers Galician traditions and seasonal festivities, local customs and folklore, or traditional instruments. Galician websites (e.g. Galician Wikipedia, or local websites) were used to motivate some examples, but none of the content on these sites was used directly. Group 0067: Malayalam (mal_mlym: 101 examples) Manually written and reviewed by native Malayalam speakers from different regions of Kerala: one from Muvattupuzha (Idukki and Kottayam dialects), and one from Ottappalam (Palakkad and Thrissur dialects). Examples were written to cover topics specific to Kerala, such as local weather, traditional food recipes, regional flora and fauna, cultural flair, and religious traditions. Group 0068: Persian (Farsi) (pes_arab: 100 examples) This dataset was created by native Farsi speakers using hybrid LLM and manual approach. LLMs were prompted to propose high-level categories and illustrative examples, spanning both everyday knowledge and culturally-specific practices. Based on these examples, the authors either created new samples from scratch inspired by the proposed categories or edited the LLM-generated examples. All examples were reviewed and edited by two native speakers. Group 0069: Hindi, Telugu (hin_deva: 179 examples, tel_telu: 97 examples) This dataset was created by native Hindi and Telugu speakers, using hybrid LLM and manual approach. First, native speakers wrote small set of seed examples which were used to prompt Gemini to expand the dataset. Each generated example was reviewed and edited by native speakers. The Hindi portion of the dataset uses Standard Hindi, which is widely understood across Northern India, with many prompts inspired by cultural practices such as food preparation, household activities, and regional crafts. The Telugu portion is based on Standard Telugu, spoken in Telangana and Andhra Pradesh, and it reflects daily life in those regions, from traditional agricultural practices to the handling of clay utensils. Group 0070: Yemeni Arabic, Egyptian Arabic, Tunisian Arabic, Saudi Arabic, Jordanian Arabic, Lebanese Arabic (acq_arab: 100 examples, arz_arab: 100 examples, aeb_arab: 99 examples, ars_arab: 100 examples, apc_arab_jord: 100 examples, apc_arab_leba: 100 examples) Manually written by native speakers of six Arabic dialects. Examples cover culturally-specific topics such as food, locations, religion, art, games, cultural items, and clothing. Group 0071: Gujarati (guj_gujr: 100 examples) This dataset was created by native Gujarati speaker, using hybrid LLM and manual approach. ChatGPT was prompted to generate examples, and native Gujarati speaker manually filtered and edited all examples. Topics include household activities, local festivals, food, school settings, kitchen tools, farm life, animals, seasons, games, common objects, and geography, all reflective of Gujarati customs and environments. Group 0072: Norwegian Bokmål (nob_latn: 100 examples) Manually written by native Norwegian speaker, using Norwegian Bokmål. The dataset covers Norwegian-specific activities, such as the preparation of traditional food dishes and the use of traditional objects. Group 0073: Nepali (npi_deva: 201 examples) Manually written and reviewed by native speakers of Nepali, based on topics including household tasks, personal care, outdoor activities, crafts, sports, and recreational pursuits. Another split of this dataset was generated with LLMs and human-verified, but only the human-written examples are included in Global PIQA. Group 0074: Tamil (tam_taml: 235 examples) Manually written by native Tamil speakers, focusing on Tamil cooking, including traditional Indian food preparation, ingredients, and terminology. Group 0075: Tamil (tam_taml: 113 examples) Manually written and reviewed by native Tamil speakers. Examples cover cultural and traditional dimensions of Sri Lankan life, including food practices, health and safety, religious traditions, rituals and customs, literature and arts, and traditional dress and identity. Group 0076: Malayalam (mal_mlym: 100 examples) Manually written by native Malayalam speaker, and checked by other native speakers. Topics include local culture, cuisine, etiquette, superstitions, religion, and life hacks. Motivation for examples was often drawn from everyday objects in the authors household. Several prompts intentionally illustrate linguistic features unique to Malayalam. Group 0077: Russian (rus_cyrl: 92 examples) Manually written by native Russian speaker, covering topics such as cooking, safety measures, basic physics, and basic computer use. Some questions are designed to be based on Russian culture. Group 0078: Marathi (mar_deva: 103 examples) This dataset was created by native Marathi speakers, using hybrid LLM and manual approach. ChatGPT was prompted to generate examples, and native Marathi speakers manually filtered and edited all examples. Topics include household activities, local festivals, food, school settings, kitchen tools, farm life, animals, seasons, games, common objects, and geography, all reflective of Marathi customs and environments. Group 0079: Bengali, Hindi, Kannada, Tamil, Malayalam (ben_beng: 100 examples, hin_deva: 100 examples, kan_knda: 97 examples, tam_taml: 100 examples, mal_mlym: 100 examples) This dataset was created using LLM generation with human verification by native speakers of Bengali, Hindi, Kannada, Tamil, and Malayalam. LLMs (Gemini 2.5 Pro and Qwen 3) and translation models (MADLAD-400) were used in multi-stage pipeline to identify topic clusters in English PIQA, to generate localized examples in English (localized to specific Indian states where the respective languages are widely spoken), to translate examples to the respective languages, then to correct any errors in the translations. After this pipeline, native speakers validated all examples. Group 0080: Russian (rus_cyrl: 100 examples) Examples in this dataset were generated by prompting GPT 5, GPT 4.1, and o4-mini with information from Russian school textbooks. All examples were manually edited and verified by native Russian speakers. Group 0081: Telugu (tel_telu: 93 examples) Manually written and reviewed by native Telugu speakers, using occasional Godavari regional slang. Topics include household activities, food preparation, natural phenomena, and cultural practices. Group 0082: Telugu, Nepali, Hindi (tel_telu: 191 examples, npi_deva: 192 examples, hin_deva: 192 examples) Manually written and reviewed by native Telugu, Nepali, and Hindi speakers. Embeddings of English translations were used to ensure that no examples were duplicates of English PIQA examples, and Gemini 2.5 Flash was used to verify the correctness of some examples. Posthoc, some examples were modified to incorporate more culturally-specific elements. Group 0083: Hindi (hin_deva: 101 examples) Manually written and reviewed by native Hindi speakers, focusing on everyday scenarios. Topics include food and cooking, household chores, health and safety, festivals and traditions, travel, technology and gadgets, environment and hygiene, personal care, and emergency situations. Group 0085: Hindi, Kannada, Telugu, Malayalam (hin_deva: 97 examples, kan_knda: 120 examples, tel_telu: 100 examples, mal_mlym: 111 examples) Manually written and reviewed by native speakers of Hindi, Kannada, Telugu, and Malayalam. Examples were written to be relevant to speakers of the respective language, covering topics such as food, clothing, household items, everyday life, festivals, and traditions. GPT-4 was used initially to generate examples for inspiration, but all examples in the final dataset are manually written. Group 0086: Greek (ell_grek: 92 examples) This dataset was manually constructed by native Greek speaker, by navigating Greek websites on the internet, searching for sentences about given topic, then adapting the sentences for the task. Topics include puzzles and riddles, household, cooking and recipes, driving, gardening, DIY, sports, construction, vacation, spatiotemporal orientation, and dance. Group 0087: Turkish (tur_latn: 141 examples) Manually written by native Turkish speakers, motivated by Turkish content such as food blogs, household advice websites, and health institution pages. All examples were manually verified by several Turkish speakers. Group 0088: Yoruba, Nigerian Pidgin (Naijá) (yor_latn: 199 examples, pcm_latn: 191 examples) Manually written and reviewed by native Yoruba and Nigerian Pidgin speakers. First, the authors compiled list of everyday physical items relevant to both cultures, inspired by online videos, language dictionaries, and social media. Then, realistic scenarios were manually written for different items, and these prompts were used as the basis for examples. Group 0089: Marwari, Marathi (mar_deva: 124 examples, rwr_deva: 124 examples) Manually written and reviewed by native Marathi and Marwari speakers, covering culturally-specific topics such as home, cooking, farming and rural contexts, weather, and desert travel. Group 0090: Telugu (tel_telu: 43 examples) Manually written and reviewed by native Telugu speakers, using Kosta Andhra Telugu, dialect spoken in coastal Andhra Pradesh, India. Examples in the dataset cover local festivals and traditional foods. Group 0091: Tamil (tam_taml: 226 examples) Manually written and reviewed by native Tamil speakers, after an initial attempt to use LLMs produced examples that were often generic, obvious, or culturally inaccurate. In the final dataset, all examples are either entirely manually written or substantially rewritten and refined from primitive LLM-generated example. Culturally-specific topics include traditional rituals, literature and history, agrarian and folk wisdom, and art. Group 0092: Bengali (ben_beng: 79 examples) Manually written by native Bengali speaker, and reviewed by other native speakers. The dataset uses standard colloquial Bengali as commonly spoken in Kolkata, India, and it includes references to local customs, food, holidays and traditions, and household objects. Group 0093: Slovak, Šariš Slovak (slk_latn: 100 examples, slk_latn_sari: 100 examples) Manually written by native speakers of Slovak and the Šariš dialect of Slovak. Examples were inspired by content on DIY and home improvement sites in Slovak, but no content was copied directly. Group 0094: Assamese, Bengali, Hindi, Malayalam, Manipuri (asm_beng: 195 examples, ben_beng: 498 examples, hin_deva: 376 examples, mal_mlym: 212 examples, mni_mtei: 114 examples) Manually written and reviewed by native speakers of Assamese, Bengali, Hindi, Malayalam, and Manipuri, covering everyday topics such as food, rituals, tools, climate, and household practices. Additional manual verification is in progress for Maithili, Orya, and Telugu datasets. Group 0095: Italian (ita_latn: 105 examples) Manually written and reviewed by native Italian speakers, covering culturally-specific topics such as local foods, artisanal products, domestic practices, and folklore. Group 0096: Thai (tha_thai: 97 examples) Manually written by native Thai speaker. Inspired by browsing the internet in Thai, some examples cover local landmarks, art, cooking, and customs that are unique to Thailand. Group 0097: Hindi, Marathi, Tamil (hin_deva: 67 examples, mar_deva: 85 examples, tam_taml: 150 examples) Manually written and reviewed by native speakers of Hindi, Marathi, and Tamil, covering culturallyrelevant everyday scenarios in Indic contexts, such as food preparation, household chores, and electronic device usage. Examples underwent extensive validation and rewriting, including reading examples aloud to parents, grandparents, and younger relatives. Group 0098: Hindi (hin_deva: 83 examples) Manually written by native Hindi speaker, and reviewed by another native speaker. Examples were drawn from diverse domains such as traditional Indian games, handicrafts, festivals, musical instruments, and everyday life. Group 0099: Czech (ces_latn: 195 examples) Manually written and reviewed by native Czech speakers, covering domains such as everyday activities, cooking, household tasks, and activities related to traditional Czech customs or sayings. Some examples use Moravian and Silesian dialects, or contemporary Gen and Gen Alpha slang (e.g. skibidi and 6-7). For examples using slang or dialects, the authors consulted external collaborators from those demographic groups to ensure correct usage. Examples were passed into GPT-5 and Claude Opus 4.1 for edits, and small number of examples were generated directly by the LLMs themselves; all examples underwent human validation by multiple native speakers. Group 0100: Thai (tha_thai: 97 examples) Manually written by native Thai speaker, using the central Thai dialect. Examples cover specific Thai knowledge, such as Muay Thai movements. Group 0101: Sinhala (sin_sinh: 110 examples) Manually written and reviewed by native Sinhala speakers, covering domains such as literature, religion, mythology, sports, food, and history, primarily in Sri Lankan context. 44 Group 0102: Turkish, Azerbeijani, Kyrgyz (tur_latn: 135 examples, azj_latn: 119 examples, kir_cyrl: 113 examples) This dataset was written and reviewed by native speakers of Turkish, Azerbaijani, and Kyrgyz. Topics include household routines, cooking, driving, and seasonal conditions, along with everyday and culturally-specific items. Some examples in Turkish were initially generated using GPT-5, but many Turkish examples are fully original, and all examples were verified by native speakers. LLMs were not used for Azerbaijani or Kyrgyz; for example, for Azerbaijani, trials with GPT-5 and Gemini 2.5 Pro produced poor quality samples. Group 0103: Tamil (tam_taml: 688 examples) Manually written and reviewed by native speakers of Tamil, using Sri Lankan Tamil and covering domains such as domestic chores, culinary practices, agriculture, and traditional artifacts. Examples were deduplicated with n-grams and SBERT embeddings. When evaluated by humans, four native speakers agreed unanimously on the label for 95% of examples. Group 0104: Korean (kor_hang: 181 examples) This dataset was constructed by native Korean speakers using hybrid LLM and manual approach. Using multi-stage pipeline, LLMs were given Korean-specific seed scenarios to (1) generate examples, (2) validate the questions, (3) validate the solutions, (4) generate distractor solutions, and (5) validate distractors. Finally, examples were deduplicated, and biased answers (e.g. examples that could be solved with simple heuristics) were removed. All final examples were validated by native Korean speaker. Group 0105: Kinyarwanda (kin_latn: 108 examples) Manually written by native Kinyarwanda speaker, and reviewed by another native speaker, using the standard dialect spoken in education and media. Examples cover everyday scenarios such as household activities, tools and objects, food, transportation, and weather. Group 0106: Swahili (swh_latn: 172 examples) Manually written by native Swahili speaker, covering variety of everyday contexts. Group 0107: Central Kurdish (ckb_arab: 100 examples) Manually written by native Kurdish speaker, using Central Kurdish (also known as Sorani). Examples focus on village life and traditional practices (e.g. cooking, handicrafts, agriculture, animal husbandry, and customs), domains where Kurdish possesses rich and nuanced vocabulary. Group 0108: Hungarian (hun_latn: 105 examples) Manually written and reviewed by native Hungarian speakers, covering variety of physical phenomena and incorporating Hungarian cultural context. Group 0109: Turkish (tur_latn: 99 examples) Manually written by native Turkish speaker, with some sentences adapted from online food recipes. Group 0110: Russian (rus_cyrl: 100 examples) Manually written and reviewed by two native Russian speakers, covering everyday scenarios. Some examples cover culturally-specific holidays or foods. Group 0112: Javanese (jav_latn: 120 examples) One native Javanese speaker contracted five other annotators through Prolific at rate of 8 GBP per hour, which is significantly above the minimum hourly wage in Indonesia. Many examples were written to be culturally-specific, covering local music, food, nature, and daily life. Generally, this dataset uses the Ngoko register, or casual language in Javanese. Although standardized writing guideline exists for Javanese, it is not universally followed, and there is substantial variation in orthography and spelling. Annotators were allowed to write in the form they naturally used, to better capture authentic language use. The final examples were reviewed by the primary author of this dataset. 45 Group 0113: Georgian (kat_geor: 100 examples) Manually written and reviewed by native Georgian speakers, covering everyday knowledge and activities. Some examples drew inspiration from the Georgian book, Imagination and Skillful Hands by Neli Okropiridze, which offers tips and tricks for range of DIY projects and was once widely used in the Georgian community. Group 0114: Burushaski (bsk_arab: 100 examples) Manually written by native Burushaski speaker, using the Yasin dialect. All examples were checked for grammatical correctness, cultural relevance, and physical commonsense validity. Group 0115: Peruvian Spanish (spa_latn_peru: 102 examples) This dataset was manually compiled by native Spanish speakers. Sentences were adapted from naturally occurring speech among the dataset authors family and friends. Some examples were drawn from public-interest topics in Lima, Peru, including local traditions or the conduct of public officials. Any names, addresses, or direct identifiers were removed and replaced with more generic placeholders. To capture authentic language usage, tense and punctuation were not standardized but instead left reflective of colloquial speech. Group 0116: Russian (rus_cyrl: 53 examples) This small dataset was manually written and reviewed by native Russian speakers from the South Ural Mountains region of Russia. Several examples are designed to test local commonsense knowledge. Group 0117: Hawaiian ( Olelo Hawaii) (haw_latn: 100 examples) Manually written by second-language olelo Hawaii speakers, and verified by native speakers. Examples cover wide range of scenarios, including contexts specific to Hawaii, the Hawaiian language, and Hawaiian culture, as well as everyday situations. All Hawaiian text was written in modern orthography, including both the okina and kahako. Relevant to anyone using this dataset, the dataset authors note the distinction between noonoo Hawaii (Hawaiian ways of thinking) and noonoo Haole (foreign ways of thinking) as applied to NLP, where data representation choices risk importing external frameworks. Preserving noonoo Hawaii ensures that datasets and computational models reflect culturally grounded perspectives, maintaining authenticity and integrity in the development of Hawaiian language technologies. Group 0118: Portuguese (European) (por_latn_port: 105 examples) Manually written and reviewed by native European Portuguese speakers, with many examples covering Portuguese culture (e.g. references to festivities, holidays, and the preparation of traditional dishes). Two native speakers evaluated the dataset without access to labels, achieving accuracies of 90.7% and 95.4% respectively. Group 0119: Algerian Arabic, Moroccan Arabic (arq_arab: 209 examples, ary_arab: 198 examples) This dataset was crowdsourced from native Algerian and Moroccan (Darija) Arabic speakers. All examples were checked by other native speakers for naturalness, correctness, and cultural relevance. Contributors and annotators participated voluntarily without monetary compensation. Recruitment occurred via open community channels; participants gave informed consent, could withdraw at any time, and were not subject to coercion or undue influence. No personally identifiable information was collected. Across three annotators, average pairwise agreement on labels was over 95% (Cohens kappa > 0.90 for all pairs). Group 0120: Amharic (amh_ethi: 141 examples) Approximately half of this dataset was manually written by native Amharic speaker; the other half was generated by using Gemini 2.5 to expand the size of the dataset. All examples were then verified by multiple native speakers. Examples focus on the topics of sports, culture, history, politics, and education. Group 0121: German (deu_latn: 100 examples) Manually written by native German speaker, covering culturally-specific topics such as food and customs that might not be well known outside of Germany. ChatGPT was used to help double-check grammar and spelling, but not to generate examples. 46 Group 0122: German (deu_latn: 26 examples) This small dataset was manually written by native German speaker, covering topics such as sports, household, gardening, and entertainment. Group 0123: English (USA and UK) (eng_latn: 104 examples) This dataset was obtained by filtering the English PIQA test set to approximately 100 high-quality examples. Examples were excluded if they contained typos or nonsensical answer choices; some examples were modified to correct these errors. Many examples were selected based on cultural relevance to English-speaking contexts in the United States of America or the United Kingdom (e.g. US Thanksgiving, or American football). The resulting dataset was validated by another native English speaker. Group 0124: Amharic (amh_ethi: 119 examples) Manually written by native Amharic speaker, and validated by other native speakers. Examples cover everyday contexts in Ethiopian society, including traditions, customs, food, history, and proverbs. Group 0125: Bambara (bam_latn: 111 examples) This dataset was compiled by native Bambara speakers. Some examples were based on content from French quizzes on technical knowledge, translated into Bambara by professional translators. Other examples were written to be culturally-specific to Bambara-speaking contexts. All examples were refined and validated by native Bambara speakers. Group 0126: Peninsular Spanish (spa_latn_spai: 101 examples) Manually written by native Spanish speaker, using central-northern Peninsular Spanish (e.g. as spoken in Madrid and the interior of Castilla León). Examples cover culturally-specific foods, customs, and domestic practices. Group 0127: Eastern Armenian (hye_armn: 102 examples) Manually written by an Armenian speaker, and checked by native speaker. Prompts were first outlined in English then translated to Eastern Armenian. Topics include cutlery and tableware, fabrics and clothing, laundry, and cooking. small number of examples are specific to Armenian culture. Group 0128: Lithuanian (lit_latn: 74 examples) Manually written by native speaker of Lithuanian, with examples constructed using mix of domain expertise and simple Lithuania-related questions. GPT-5 was used to brainstorm ideas, but not to generate examples. Group 0129: Lithuanian (lit_latn: 100 examples) Examples in this dataset were generated based on Wikipedia articles using GPT-5, then manually rephrased and checked by two native speakers of Lithuanian. Topics include traditional Lithuanian food, traditions, places, and literature. Group 0130: Zulu (zul_latn: 100 examples) Manually written by native speaker of isiZulu, with examples written to reflect everyday scenarios and local cultural practices. Group 0131: Kazakh (kaz_cyrl: 100 examples) Manually written by native speaker of Kazakh, using the Northeastern Kazakh dialect, and including some specific words that are commonly used in Karaganda city. Examples cover culturally-specific topics, including food, drinks, music, customs, animals, games, history, architecture and monuments, weather, nature, clothing, and jewelry. Group 0132: Bosnian (bos_latn: 145 examples) Manually written by native Bosnian speaker, using the Ijekavian standard. The dataset covers regionally salient vocabulary and scenarios, including cooking, household tasks, nature, and religious and social customs. 47 Group 0133: Kinyarwanda (kin_latn: 99 examples) Manually written and reviewed by native Kinyarwanda speakers. Examples cover everyday domains such as everyday objects, weather, folklore, and literature. The dataset is written in standard Kinyarwanda, without dialectal variations such as those spoken in the northern and southern provinces of Rwanda. Group 0134: Peninsular Spanish, Mexican Spanish (spa_latn_spai: 100 examples, spa_latn_mexi: 100 examples) Manually written by native Spanish speakers, covering variety of subtypes of physical commonsense reasoning. Examples reference local foods, places, traditions, architecture, and everyday objects and tasks in Spain and Mexico (for Peninsular and Mexican Spanish respectively). The Peninsular and Mexican Spanish datasets differ at the topic, lexical, and syntactic levels, to reflect differences between the two dialects. All examples in the two datasets were verified and edited by native Spanish speaker living in Spain or Mexico respectively. Group 0135: Ekpeye (ekp_latn: 100 examples) Manually written by native Ekpeye speaker, with topics covering everyday life, local Nigerian foods, and local customs."
        }
    ],
    "affiliations": [
        "EleutherAI",
        "UC San Diego"
    ]
}