{
    "paper_title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts",
    "authors": [
        "Haoyuan Wu",
        "Haoxing Chen",
        "Xiaodong Chen",
        "Zhanchao Zhou",
        "Tieyuan Chen",
        "Yihong Zhuang",
        "Guoshan Lu",
        "Zenan Huang",
        "Junbo Zhao",
        "Lin Liu",
        "Zhenzhong Lan",
        "Bei Yu",
        "Jianguo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size."
        },
        {
            "title": "Start",
            "content": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts Haoyuan Wu1,2, Haoxing Chen1, Xiaodong Chen1,3, Zhanchao Zhou1,4,6, Tieyuan Chen1,5, Yihong Zhuang1, Guoshan Lu1, Zenan Huang1, Junbo Zhao1,4, Lin Liu1, Zhenzhong Lan1,6, Bei Yu2, Jianguo Li1 1Inclusion AI 2The Chinese University of Hong Kong 3Renmin University of China 4Zhejiang University 5Shanghai Jiao Tong University 6Westlake University"
        },
        {
            "title": "Abstract",
            "content": "The Mixture of Experts (MoE) architecture is cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of uniform size, activating fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.143.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size. 5 2 0 2 1 1 ] . [ 1 5 8 7 7 0 . 8 0 5 2 : r Figure 1: Benchmark performance of GroveMoE-Inst and its counterparts. Our GroveMoE-Inst achieves performance comparable to open-source SOTA LLMs of similar or even larger scales. Core Contributors. Corresponding Authors. https://github.com/inclusionAI/GroveMoE"
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in Large Language Models (LLMs) have spurred the adoption of the Mixture of Experts (MoE) architecture (Jiang et al., 2024; Yang et al., 2025; Liu et al., 2024; Sun et al., 2024; Google DeepMind, 2025; Huo et al., 2025) considering its great model capacity. The MoE model operates by dynamically routing input tokens to relevant subset of multiple experts, enhancing computational efficiency and scalability. However, key limitation of conventional MoE models is their reliance on homogeneous experts, which activates fixed number of parameters irrespective of input token complexity. Given that token complexity varies, computational resources should ideally be allocated dynamically with more resources for complex tokens and fewer for simple ones (Huang et al., 2024). The current rigidity precludes such fine-grained control over computation. Inspiration from the big.LITTLE CPU architecture (Greenhalgh, 2011), which uses heterogeneous cores to manage computational load efficiently. Analogously, employing experts of varying sizes within MoE models could enable dynamic resource allocation based on computational demand. Drawing inspiration from the structure of trees, we introduce Grove MoE, novel architecture that leverages parallel adjugate experts to expand model capacity efficiently. Grove reflects our architectural design, where experts are organized into disjoint groups. Similar to how branches share common trunk within tree cluster, experts within Grove group share single adjugate expert. If multiple activated experts belong to the same group, their shared adjugate expert is computed only once before being added to each experts output. This mechanism enables form of dynamic computation allocation, allowing Grove MoE to expand model capacity with high computational efficiency. Crucially, the Grove MoE architecture is compatible with existing routing mechanisms, eliminating the need for complex, manually designed routing strategies (Huang et al., 2024; Wang et al., 2024b) to manage expert activation counts. Moreover, we apply loss-free expert loading balance strategy during mid-training (Liu et al., 2024; Su, 2025). Furthermore, the upcycling strategy (Komatsuzaki et al., 2023; Nakamura et al., 2025) has been widely used for upcycling dense models, which offer larger model capacity. Recent studies (Baykal et al., 2023; Wu et al., 2024; Chen et al., 2025) have also shown that expanding model capabilities through parallel computation, especially by reusing existing weights, is an effective strategy comparable to direct parameter scaling. Consequently, we develop our GroveMoE architecture based on pre-trained MoE models through mid-training (Meta-AI, 2025; Yang et al., 2025) and post-training stages. We summarize our main contributions as follows: We introduce the Grove MoE architecture, which features new mechanism of dynamic computation allocation, allowing for parameter expansion while maintaining manageable computational costs. We develop GroveMoE-Base and GroveMoE-Inst, open-source models that dynamically activate 3.143.28B parameters, upcycled based on Qwen3-30B-A3B-Base through mid-training and post-training. We conduct empirical evaluations showing that our GroveMoE models achieve performance comparable to open-source SOTA LLMs of similar or even larger scales across various tasks."
        },
        {
            "title": "2 Related Works",
            "content": "big.LITTLE Architecture. The big.LITTLE CPU architecture (Greenhalgh, 2011) offers compelling model for computational efficiency by integrating high-performance big cores and energy-efficient LITTLE cores within single processor, dynamically routing tasks to the appropriate core type. In contrast, traditional MoE architectures (Yang et al., 2025; Liu et al., 2024) typically employ homogeneous experts of uniform size, analogous to processor with only one type of core, which leads to suboptimal efficiency. Drawing inspiration from the big.LITTLE architecture, we propose an MoE architecture where experts vary in computational capacity and the experts are dynamically activated. In this paper, we introduce the Grove MoE architecture, which materializes this concept through novel adjugate experts and dynamic activation mechanism. MoE Architecture with Dynamic Activation. Prior research has explored dynamic activation of expert counts in MoE models to mitigate the ineffectiveness of fixed top-k routing for modeling targets of different complexity. Naive approaches (Jin et al., 2024; Zeng et al., 2024) indirectly vary the active expert count by including blank or constant experts in the routing pool. DynMoE (Guo et al., 2024) enables top-any gating mechanism to choose any number of experts. ReMoE (Wang et al., 2024b) activates experts with positive scores via the ReLU function. Both DynMoE and ReMoE face the challenge of needing explicit mechanisms to manage the upper bound on the number of activated experts so as to avoid potential high computation As the GroveMoE architecture is inspired by the big.LITTLE CPU design, the name Grove also honors Andy Grove, legendary figure in the semi-conductor industry. 2 overhead. Moreover, their relatively complex routing strategies are incompatible with the well-established top-k routing mechanisms, which brings potential issues in practice. In contrast, our GroveMoE layer intrinsically achieves dynamic activation by assigning adjugate experts to separate groups. This approach guarantees controllable activation count and thus manageable computation overhead. It requires no specialized router modifications, and the excellent compatibility makes it widely applicable. Upcycling Strategy. The performance of LLMs is intrinsically related to the model capacity. Various upcycling methods (Komatsuzaki et al., 2023; Nakamura et al., 2025) have been proposed to upcycle dense models, leveraging knowledge from pre-training models. This paper develops GroveMoE-Base and GroveMoE-Inst models by applying the upcycle strategy to the MoE model Qwen3-30B-A3B-Base (Yang et al., 2025)."
        },
        {
            "title": "3.1 Traditional MoE Layer\ni=1 and a router R. Let x ∈ Rd represent the feature of an input token,\nAn MoE layer comprises n experts {Ei}n\nwhere d denotes the feature dimension. The routing scores are calculated as ρ = R(x) ∈ Rn and the output\nof the i-th expert is ei = Ei(x) ∈ Rd. The final output of the MoE layer for each token is a weighted sum of\nthe outputs from a selected subset of experts:",
            "content": "y = ρiei, iarg topk(ρ) (1) where arg topk() selects the indices of the top routing scores."
        },
        {
            "title": "3.2 Grove MoE with Adjugate Experts\nExpanding model capacity during mid-training is a promising strategy, as it preserves existing knowledge\nwhile providing additional resources to acquire complex skills. However, under the upcycling strategy,\ndirectly duplicating each expert’s parameters would disturb the original distribution of ρ. Specifically, when\nexperts are duplicated, k must be scaled proportionally by the same scale factor. However, k should be\ncontrolled to avoid introducing significant activation parameters. Alternatively, extending the parameters of\neach expert Ei maintains the original ρ distribution and offers a viable solution.\nThe AltUp architecture (Baykal et al., 2023) demonstrates that introducing parallel blocks can increase model\ncapacity without incurring substantial computational overhead. Moreover, scaling parallel computation\nby reusing existing parameters has proven effective for enhancing model capability, with similar effects as\nscaling parameters (Wu et al., 2024; Chen et al., 2025).",
            "content": "Inspiredly, we introduce Grove MoE, novel architecture that leverages parallel adjugnate experts for efficient model capacity expansion. In the Grove MoE layer, we divide experts, {Ei}n i=1, into disjoint groups, where divides n, and each group contains n/g experts. The groups {Gj}g (cid:26) j=1 can be defined as follows: (cid:27) (cid:23) Gj = Ei + 1 = , for = 1, 2, . . . , g, (cid:22) 1 n/g where denotes the floor function. Motivated by big.LITTLE architecture (Greenhalgh, 2011), we introduce an adjugate expert Aj for each group Gj. Notably, the capacity of the adjugate expert Aj can differ from that of the expert Ei. The modified output ei is then computed as: ei = Ei(x) + λAj(x), where = (cid:23) (cid:22) 1 n/g + 1. Here, λ is the scaling factor for the adjugate expert. The final output of the Grove MoE layer is: = ρi ei = iarg topk(ρ) iarg topk(ρ) ρi(Ei(x) + λAj(x)), where = (cid:23) (cid:22) 1 n/g + 1. The key advantage of the Grove MoE architecture is dynamic computation allocation. To illustrate, consider experts Er and Es where . According to Equation (4): = (cid:107) (cid:107) (cid:106) r1 n/g (cid:106) s1 n/g ρr er + ρs es = ρr(Er(x) + λAj(x)) + ρs(Es(x) + λAj(x)) = ρrEr(x) + ρsEs(x) + (ρr + ρs)λAj(x), where = (cid:23) (cid:22) 1 n/g + 1. (5) 3 (2) (3) (4) Figure 2: Comparison between the traditional MoE layer and our Grove MoE layer with dynamic computation allocation. For clarity, we configure n/g = 2 and = 4 for the Grove MoE layer. For the adjugate expert Aj, the routing weight ρA = (ρr + ρs)λ should be restricted to be no more than the weight of experts Er and Es, especially for the upcycling mid-training case. Generally, we restrict λ 1.0/(n/g) = g/n. For instance, for MoE model with n=128 experts and g=64 groups, we restrict λ 0.5. As shown in Figure 2, if multiple activated experts belong to the same group, their adjugate expert Aj is computed only once before being added to each experts output, scaled by the sum of their routing weights. According to Equation (5), the number of activated adjugate expert Aj ranges from to for each Grove MoE layer, where denotes the ceiling function. This mechanism enables form of dynamic computation allocation, allowing Grove MoE to expand model capacity with high computational efficiency, which aligns with the design concept of the big.LITTLE architecture (Greenhalgh, 2011). (cid:108) n/g (cid:109)"
        },
        {
            "title": "3.3 Experts Loading Balance\nIn MoE models, workload imbalance among experts can induce routing collapse and reduce computational\nefficiency. To better balance load distribution while maintaining model performance, we employ an auxiliary-\nloss-free load balancing strategy (Liu et al., 2024). Specifically, we introduce a dynamic expert-wise bias term\nb to adjust routing scores ρ. The gating mechanism in Equation (4) is therefore modified as:",
            "content": "y = iarg topk(ρ+b) ρi ei, (6) where is updated via sign gradient descent to minimize imbalance. Formally, we define = E(f ) as the current load distribution across experts under the bias b, where = [ f1, f2, , fn] and fi denotes the assignment probability for each token: fi = (cid:26)1/k, 0, arg topk(ρ + b), otherwise. (7) The uniform load distribution is = [ 1 , 1 , , ]. To optimize load balancing (Su, 2025), is updated as: α (cid:115) 1 i= (Fi Qi)2 . (8) Specifically, Equation (8) uses RMSNorm to normalize the imbalance (F Q) for better workload balance. To address the sensitivity of the parameter α in Equation (8), resulting from its coupling with the Sigmoid router (Liu et al., 2024; Su, 2025), we decouple the routing calculation. The final output is computed as: iarg topk(ρ(σ)+b) (h) ei, = (9) ρ 4 Table 1: Architecture exploration on different expert group settings. The highest and second-best scores are shown in bold and underlined, respectively. Expert Groups - - = 64; = 128 = 32; = 256 = 16; = 512 Architecture # Total Params # Activated Params # Avg. Activation # Training Tokens MoE MoE 30B 30B 3B 3B 3B 3B 50B 0B Grove MoE 33B 3.14B-3.28B 3.26B 50B Grove MoE 33B 3.14B-3.57B 3.51B 50B Grove MoE 33B 3.14B-4.11B 3.93B 50B MMLU CMMLU SuperGPQA GSM8K MATH GPQA-Diamond HumanEval+ MBPP+ Average 81.58 80.63 36.10 89.39 59.75 39.39 83.54 71.96 82.56 86.54 35.93 89.54 65.90 38. 83.54 74.34 67.79 69.59 82.57 86.47 36.22 90.07 66.52 41.41 85.37 75. 70.54 82.12 85.54 36.09 90.83 66.60 39.39 84.75 75.13 70.06 80.23 85.57 36. 90.67 66.64 44.95 84.15 74.07 70.30 Table 2: Architecture exploration on different scaling factors. The highest and second-best scores are shown in bold and underlined, respectively. Scaling Factor Architecture # Total Params # Training Tokens MMLU CMMLU SuperGPQA GSM8K MATH GPQA-Diamond HumanEval+ MBPP+ Average - MoE 30B 0B 81.58 80.63 36.10 89.39 59.75 39.39 83.54 71.96 67.79 - MoE 30B 50B 82.56 86.54 35.93 89.54 65.90 38.38 83.54 74.34 69.59 λ = 0.20 λ = 0. λ = 0.05 Grove MoE 33B 50B Grove MoE 33B 50B Grove MoE 33B 50B 79.69 86.33 33.99 90.14 66.62 43. 85.37 75.93 70.19 82.57 86.47 36.22 90.07 66.52 41.41 85.37 75.66 70. 82.62 86.55 36.32 90.83 65.86 43.94 84.76 75.13 70.75 where ρ(h) is the output of Softmax router and ρ(σ) is the output of Sigmoid router: exi j=1 exj and ρ This decoupled approach enables us to use constant value of α = 0.001, as used in Liu et al. (2024). 1 1 + exi (σ) = (h) = ρ . (10)"
        },
        {
            "title": "3.4 Reuse of Pre-Trained Weights\nBuilding on the concept of upcycling strategy (Komatsuzaki et al., 2023), we leverage pre-trained weights\nfrom MoE models. During initialization of our Grove MoE architecture, each expert Ei is derived from a\npre-trained MoE layer. To ensure structural coherence, other components, such as the normalization and\nattention layers, are directly copied from the pre-trained transformer block. Additionally, the down-projection\nblocks of newly inserted modules {Aj}g\nj=1 follow a\nnormal distribution with a standard deviation of 0.006 (Liu et al., 2024).",
            "content": "j=1 are zero-initialized. The remaining weights in {Aj}g"
        },
        {
            "title": "4.1 Mid-Training Data\nThe mid-training stage is designed to target specific proficiencies, such as reasoning and code generation.\nThe corpus utilized for this stage is a diverse collection of textual and non-textual data, encompassing sources",
            "content": "5 Figure 3: The group routing distribution across three configurations with different group numbers g. As the number of groups decreases, the average number of adjugate experts activations decreases. including web content, books, academic papers, social media, encyclopedias, mathematics, and programming code. In total, this high-quality corpus consists of approximately 400 billion tokens."
        },
        {
            "title": "4.2 Evaluation Benchmarks\nOur comprehensive evaluation of the base models assesses five core capabilities: general knowledge, scientific\nknowledge, reasoning, mathematics, and coding. The evaluation is conducted using 13 distinct benchmarks:",
            "content": "General Tasks: MMLU (Hendrycks et al., 2020)(5-shot), MMLU-Pro (Wang et al., 2024a)(5-shot, CoT), CMMLU (Li et al., 2023)(5-shot), SuperGPQA (Du et al., 2025)(5-shot), C-Eval (Huang et al., 2023)(5-shot), and BBH (Suzgun et al., 2022)(3-shot, CoT). Math & STEM Tasks: GSM8K (Cobbe et al., 2021)(4-shot, CoT), MATH (Hendrycks et al., 2021)(4shot, CoT), and GPQA-Diamond (Rein et al., 2024)(5-shot). Coding Tasks: HumanEval+ (Liu et al., 2023)(0-shot), MBPP+ (Liu et al., 2023)(0-shot), MultiPLE (Cassano et al., 2023)(0-shot)(Python, C++, Java, PHP, TypeScript, C#, Bash, JavaScript), and CRUX-O (Gu et al., 2024)(1-shot, CoT)."
        },
        {
            "title": "4.3 Architecture Exploration\nWe explored several key design choices for the model architecture, focusing on the configuration of the expert\ngroups and the scaling factor in our Grove MoE architecture. The architecture of the shared parallel experts\n{Aj}g\nj=1 adopts the design established in Qwen3 MoE architecture (Yang et al., 2025). Architectural explo-\nration is conducted using 50B tokens sampled from the mid-training dataset, with evaluations performed\nacross diverse task types. The exploration is based on the Qwen3-30B-A3B-Base model (Yang et al., 2025),\nusing direct mid-training without upcycling as the baseline.",
            "content": "Expert Groups. As detailed in Table 1, we evaluate the impact of expert group configurations on model performance while maintaining approximately 33B total parameters. Three configurations are compared: (1) = 64, = 128; (2) = 32, = 256; and (3) = 16, = 512, where denotes the intermediate dimension of {Aj}g j=1. For general knowledge, language understanding, and code generation, configuration with = 64, = 128 achieves optimal performance. Meanwhile, configuration with = 16, = 512 yields superior results for complex mathematical reasoning and STEM tasks. Notably, three configurations outperform the baseline in terms of average performance, demonstrating the effectiveness of our Grove MoE architecture for expanding model capacity. Scaling Factor. Table 2 evaluates the influence of the expert output scaling factor λ. For general knowledge and language understanding, λ = 0.05 achieves peak performance. For mathematics and STEM tasks, both λ = 0.05 and λ = 0.20 excel, while λ = 0.20 is optimal for code generation. In general, Grove MoE with smaller scaling factor outperforms the baseline across multiple tasks. Group Routing Analysis. To analyze the group routing distribution, we sample 1 million tokens from the mid-training dataset. For an LLM with = 128 experts, Figure 3 illustrates the distribution across three configurations. With large number of groups (g = 64), expert activation is broadly distributed, with most experts assigned to 78 groups. In contrast, configurations with fewer groups (g = 32 and = 16) exhibit highly consolidated expert activation. This consolidation directly impacts computational efficiency. With 6 Table 3: Comparison among GroveMoE-Base and other strong open-source baselines. The highest and second-best scores are shown in bold and underlined, respectively. Mistral-Small-3.1 BaseGemma3-27B Base Qwen2.5-32B Base Qwen3-30B-A3B Base Llama4-Scout Base GroveMoE Base Architecture # Total Params # Activated Params General Tasks MMLU MMLU-Pro CMMLU SuperGPQA BBH C-Eval Math & STEM Tasks GSM8K MATH GPQA-Diamond Coding Tasks HumanEval+ MBPP+ MultiPL-E CRUX-O Dense 24B 24B Dense 27B 27B Dense 32B 32B 81.65 55.67 74.85 30.47 83.46 72.81 85.90 43.90 39.90 60.98 71.16 27.32 50. 79.89 52.97 70.17 30.15 79.19 70.00 82.71 49.80 36.36 57.32 69.84 48.20 60.12 83.50 59.04 88.17 35.80 84.30 86.96 90.45 60.42 41.41 78.05 73.81 52.57 67. MoE 30B 3B 81.58 59.58 80.63 36.10 81.58 87.82 89.39 59.75 39.39 83.54 71.96 61.76 67.20 MoE 109B 17B 79.08 57.32 76.05 27.54 82.60 74. 86.43 51.34 37.54 64.63 69.84 48.53 59.54 Grove MoE 33B 3.14B-3.28B 82.86 59.06 86.75 38.74 82.09 87.84 90.83 64.82 41.92 85.98 76.19 60.38 70. the = 64 configuration, the average number of activated parameters is 3.26B, reducing computation by approximately 5%. As the number of groups decreases, the computational savings increase. For the = 16 configuration, the savings reach approximately 20%."
        },
        {
            "title": "4.5 Mid-Training Evaluation\nFor the base model baselines, we compare our GroveMoE-Base models with leading open-source base models,\nincluding Mistral-Small-3.1-Base-2503 (Mistral AI, 2025), Gemma3-27B-Base (Gemma et al., 2025), Qwen2.5-\n32B-Base (Yang et al., 2024), Qwen3-30B-A3B-Base (Yang et al., 2025), and Llama4-Scout-Base (Meta-AI, 2025).\nAll models are evaluated using the same evaluation pipeline and the widely used evaluation settings (Yang\net al., 2024; 2025) to ensure fair comparison.",
            "content": "As depicted in Table 3, our GroveMoE-Base model, built on the Grove MoE architecture, achieves strong balance of performance and efficiency. Our Grove MoE architecture enables operation with fewer activated parameters than dense models and achieves greater parameter efficiency than other MoE baselines. GroveMoE-Base excels at complex reasoning, surpassing all competing baselines in Math & STEM and coding benchmarks to achieve the highest average scores. In addition to these advanced reasoning capabilities, GroveMoE-Base is highly competitive in general tasks. It matches the performance of Qwen2.5-32B-Base, dense model with similar total parameter count, while maintaining its advantage in activation efficiency. Notably, GroveMoE-Base is developed based on Qwen3-30B-A3B-Base. As shown in Table 3, the Grove MoE architecture facilitates an efficient expansion of model capacity with only minor additional computational cost. The Grove MoE architecture allows the model to preserve foundational knowledge while providing dedicated resources to master new, complex skills. 7 Table 4: Comparison among GroveMoE-Inst and other strong open-source non-reasoning baselines. The highest and second-best scores are shown in bold and underlined, respectively. Mistral-Small-3.2 Instruct-2506 Gemma3-27B IT Qwen3-32B Non-Thinking Qwen3-30B-A3B Non-Thinking Llama4-Scout Dense 24B 24B Dense 27B 27B Dense 32B 32B 80.29 68.11 74.02 37.53 85.51 86.02 72.01 58.24 82.52 83.87 84.18 86.50 33.40 36.88 28.12 49.94 61.89 81.94 73.54 69.49 25.90 32.25 78.21 75.97 67.10 65.82 35.63 85.79 87.81 67.31 53.63 86.14 89. 85.82 87.80 33.30 29.58 23.12 45.33 59.85 78.81 73.83 65.50 26.75 30.86 75.31 82.93 68.25 84.63 43.04 85.45 84.02 87.53 63.64 85.27 90.49 85.26 87.40 31.80 27.71 22.92 53.60 59.52 82.93 72.75 68.62 31.44 28.57 75. MoE 30B 3B 80.12 63.30 83.13 40.50 82.55 86.38 85.95 65.27 84.55 88.33 84.68 88.70 33.70 28.33 21.67 51.71 60.26 84.15 75.16 66.04 28.89 29.43 73.69 MoE 109B 17B 81.88 64.92 76.12 42.02 77.37 88.26 74.69 62.31 85.57 73.49 81.46 82.60 25.78 28.60 10.00 55.56 56.11 79.88 70.37 45.00 25.45 32.04 45.41 GroveMoE Inst Grove MoE 33B 3.14B-3.28B 88.04 72.78 86.66 47.69 88.42 88.84 87.60 82.19 86.54 92.01 90.56 94.60 43.50 54.58 44.38 61.30 71.22 90.24 78.31 74.53 33.38 34.60 76.11 Architecture # Total Params # Activated Params General Tasks MMLU MMLU-Pro CMMLU SuperGPQA BBH DROP C-Eval AGIEval Alignment Tasks IFEval Arena-Hard Math & STEM Tasks MATH MATH-500 Omni-MATH AIME24 AIME25 GPQA-Diamond OlympiadBench Coding & Agent Tasks HumanEval+ MBPP+ MultiPL-E LiveCodeBench v5 LiveCodeBench v6 BFCL v3 (Live)"
        },
        {
            "title": "5.1 Supervised Fine-Tuning\nFollowing the mid-training stage, the GroveMoE-Base model undergoes supervised fine-tuning (SFT). This\nstage is crucially dependent on the training data. Given the scarcity and high annotation cost of human-\ngenerated data, synthetic data has become increasingly important. Our SFT dataset is constructed from a\nseed set of 1–2 million instances, comprising human annotations and open-source materials. This initial\ndataset is then substantially expanded through a data synthesis pipeline.",
            "content": "Our data synthesis process begins by generating novel prompts using methods inspired by Magpie-style approaches (Xu et al., 2024) and OSS-Instruct (Wei et al., 2023). We then apply rejection sampling (Grattafiori et al., 2024) to produce candidate responses using various LLMs (Yang et al., 2025; Google DeepMind, 2025; Hurst et al., 2024). To ensure high data quality, we employ multi-stage filtering process. Initially, rule-based filters are applied to reasoning-intensive data, such as code, mathematics, and logic problems. Subsequently, all data types undergo final assessment by an LLM-based evaluator, which uses detailed rubric to verify the response quality and relevance. This rigorous data curation process yields robust dataset for SFT."
        },
        {
            "title": "5.2 Evaluation Benchmarks\nTo comprehensively evaluate the quality of instruction-tuned models, we evaluate LLMs on a series of\npost-training benchmarks. The post-training benchmarks can be categorized into several dimensions:",
            "content": "8 Figure 4: Evaluation results of SFT on various benchmarks. indicates the performance improvement of SFT trained with GroveMoE-Base over Qwen3-30B-A3B-Base. General Tasks: For general language understanding tasks, we utilize various benchmarks including MMLU (Hendrycks et al., 2020), MMLU-Pro (Wang et al., 2024a), CMMLU (Li et al., 2023), SuperGPQA (Du et al., 2025), BBH (Suzgun et al., 2022), DROP (Dua et al., 2019), C-Eval (Huang et al., 2023), and AGIEval (Zhong et al., 2023). Alignment Tasks: To evaluate how well the model aligns with human preferences, we employ suite of specialized benchmarks. For instruction-following performance, we report the average prompt-level and instruction-level strict accuracy of IFEval (Zhou et al., 2023). To assess alignment with human preferences on general topics, we utilize Arena-Hard (Li et al., 2024). Math & STEM Tasks: For evaluating mathematical reasoning skills, we employ MATH (Hendrycks et al., 2021), MATH-500 (Lightman et al., 2023), Omni-MATH (Gao et al., 2024), AIME24 (AIME, 2025), and AIME25 (AIME, 2025). For STEM tasks, we utilize GPQA-Diamond (Rein et al., 2024) and OlympiadBench (He et al., 2024) as evaluation benchmarks. For AIME problems, we sample 16 times for each question and take the average accuracy as the final score. For GPQA-Diamond, we sample 8 times for each query and report the average accuracy. Coding & Agent Tasks: To test the LLMs proficiency in coding and agent-based tasks, we use HumanEval+ (Liu et al., 2023), MBPP+ (Liu et al., 2023), MultiPL-E (Cassano et al., 2023), LiveCodeBench (Jain et al., 2024) (v5, 2024.10-2025.02 and v6, 2025.02-2025.05), and BFCL v3 (Live) (Yan et al., 2024). For BFCL v3 (Live), all models are evaluated using the prompt format. Notably, we configure the maximum output length to 16K to avoid overly lengthy output for non-reasoning LLMs during the evaluation process (Yang et al., 2025)."
        },
        {
            "title": "5.4 Post-Training Evaluation\nWe compare our GroveMoE-Inst with leading open-source LLMs, including Mistral-Small-3.2-Instruct-\n2506 (Mistral AI, 2025), Gemma3-27B-IT (Gemma et al., 2025), Qwen3-32B (Yang et al., 2025), Qwen3-30B-\nA3B (Yang et al., 2025), and Llama4-Scout (Meta-AI, 2025).",
            "content": "As shown in Table 4, GroveMoE-Inst establishes excellent performance across comprehensive set of benchmarks, maintaining high parameter efficiency. In general and alignment tasks, the model consistently outperforms its counterparts, securing the highest scores on all benchmarks. The superiority of our GroveMoE-Inst is particularly pronounced in mathematics and STEM, where GroveMoE-Inst ranks first across all listed benchmarks, highlighting its powerful reasoning capabilities. Furthermore, GroveMoE-Inst 9 demonstrates exceptional performance in coding and agent-based tasks. It surpasses other baselines on most coding & agent benchmarks, which underscores its advanced skills in code generation and problem-solving."
        },
        {
            "title": "5.5 Effectiveness of GroveMoE-Base\nTo evaluate the effectiveness of our GroveMoE-Base model, we applied the same post-training strategy to a\ncomparable model, Qwen3-30B-A3B-Base (Yang et al., 2025), for a direct comparison.",
            "content": "As shown in Figure 4, the instruction-tuned model derived from GroveMoE-Base consistently outperforms its Qwen3-30B-A3B-Base counterpart across the vast majority of tasks, highlighting its strong potential as foundation model. Specifically, the GroveMoE-Base model achieves higher scores on nearly all general and alignment benchmarks. The advantage is further pronounced in specialized domains, where it significantly outperforms the Qwen model on most mathematics and STEM benchmarks. Furthermore, its superiority extends to code generation and agent-based tasks, where it secures stronger results on key benchmarks. In summary, these results demonstrate that GroveMoE-Base is more powerful foundation model. Its larger model capacity enables fine-tuned derivatives to achieve superior performance across wide spectrum of domains, including general knowledge, mathematics, and coding."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces GroveMoE models, efficient and open-source LLMs built upon the Grove MoE architecture, which incorporates novel mechanism for dynamic computation allocation. The Grove MoE architecture improves computational efficiency by dividing experts into groups, each with an adjugate expert. This design ensures that shared computations are performed only once per group, even when multiple experts are activated. GroveMoE-Base and GroveMoE-Inst are 33B-parameter models developed based on the Qwen3-30B-A3B-Base model using our Grove MoE architecture during the mid-training and post-training stage. It dynamically activates 3.143.28B parameters per token. Empirical evaluations demonstrate that our GroveMoE models, including Base and Inst models, achieve performance comparable to SOTA open-source models of similar or even larger sizes, thereby validating the effectiveness of the Grove MoE architecture."
        },
        {
            "title": "Limitations",
            "content": "Although our Grove MoE architecture provides solid foundation, two primary limitations constrain its current potential and guide our future research. The first limitation stems from scarcity of long-CoT data within the mid-training corpus. This data deficiency curtails the models capacity for advanced reasoning, creating capability gap compared to instruction-tuned LLMs that possess stronger foundational reasoning abilities, such as Qwen3-30B-A3B-2507 (Yang et al., 2025), etc. The second limitation is the exclusive reliance on rejection sampling for model refinement, without the integration of RL techniques. While rejection sampling has been effective, we anticipate that incorporating RL methods will significantly enhance the models overall capabilities. This remains key objective for future development. References AIME. AIME Problems and Solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php/ AIME Problems and Solutions. Cenk Baykal, Dylan Cutler, Nishanth Dikkala, Nikhil Ghosh, Rina Panigrahy, and Xin Wang. Alternating Updates for Efficient Transformers. In Annual Conference on Neural Information Processing Systems (NIPS), 2023. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, et al. MultiPL-E: Scalable and Polyglot Approach to Benchmarking Neural Code Generation. IEEE Transactions on Software Engineering, 49(7):36753691, 2023. Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, and Zhongxin Liu. Parallel Scaling Law for Language Models. arXiv preprint arXiv:2505.10475, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training Verifiers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168, 2021. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. SuperGPQA: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: Reading Comprehension Benchmark Requiring Discrete Reasoning over Paragraphs. arXiv preprint arXiv:1903.00161, 2019. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-MATH: Universal Olympiad Level Mathematic Benchmark for Large Language Models. arXiv preprint arXiv:2410.07985, 2024. Gemma, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 Technical Teport. arXiv preprint arXiv:2503.19786, 2025. Google DeepMind. Gemini2.5 Pro. https://deepmind.google/technologies/gemini/pro/, 2025. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783, 2024. Peter Greenhalgh. big.LITTLE Processing with ARL Cortex-A15 & Cortex-A7. ARM White paper, 17, 2011. Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CruxEval: Benchmark for Code Reasoning, Understanding and Execution. arXiv preprint arXiv:2401.03065, 2024. Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, and Tao Lin. Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models. arXiv preprint arXiv:2405.14297, 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring Massive Multitask Language Understanding. arXiv preprint arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving with the Math Dataset. arXiv preprint arXiv:2103.03874, 2021. Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin, Kun Xu, Liwei Chen, Songfang Huang, and Yansong Feng. Harder Tasks Need More Experts: Dynamic Routing in MoE Models. arXiv preprint arXiv:2403.07652, 2024. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-Eval: Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models. Advances in Neural Information Processing Systems, 36:6299163010, 2023. Bi Huo, Bin Tu, Cheng Qin, Da Zheng, Debing Zhang, Dongjie Zhang, En Li, Fu Guo, Jian Yao, Jie Lou, et al. dots.llm1 Technical Report. arXiv preprint arXiv:2506.05767, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o System Card. arXiv preprint arXiv:2410.21276, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code. arXiv preprint arXiv:2403.07974, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of Experts. arXiv preprint arXiv:2401.04088, 2024. Peng Jin, Bo Zhu, Li Yuan, and Shuicheng Yan. MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts. arXiv preprint arXiv:2410.07348, 2024. Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse Upcycling: Training mixture-of-experts from dense checkpoints. In International Conference on Learning Representations (ICLR), 2023. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring Massive Multitask Language Understanding in Chinese. arXiv preprint arXiv:2306.09212, 2023. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and Benchbuilder Pipeline. arXiv preprint arXiv:2406.11939, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. In The Twelfth International Conference on Learning Representations, 2023. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. DeepSeek-V3 Technical Report. arXiv preprint arXiv:2412.19437, 2024. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101, 2017. Meta-AI. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation, 2025. URL https://ai.meta.com/blog/llama-4-multimodal-intelligence/. Mistral AI. Mistral-Small-3.1. https://mistral.ai/news/mistral-small-3-1, 2025. Taishi Nakamura, Takuya Akiba, Kazuki Fujii, Yusuke Oda, Rio Yokota, and Jun Suzuki. Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-Initialization. arXiv preprint arXiv:2502.19261, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: Graduate-Level Google-Proof Q&Q Benchmark. In First Conference on Language Modeling, 2024. sgl-project. SGLang. https://github.com/sgl-project/sglang, 2025. Jianlin Su. MoE Travels 3, 2025. URL https://kexue.fm/archives/10757. Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al. Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent. arXiv preprint arXiv:2411.02265, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging Big-Bench Tasks and Whether Chainof-Thought Can Solve Them. arXiv preprint arXiv:2210.09261, 2022. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: More Robust and Challenging Multi-Task Language Understanding Benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. 12 Ziteng Wang, Jun Zhu, and Jianfei Chen. ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing. arXiv preprint arXiv:2412.14711, 2024b. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering Code Generation with OSS-Instruct. arXiv preprint arXiv:2312.02120, 2023. Haoyuan Wu, Haisheng Zheng, Zhuolun He, and Bei Yu. Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks. In Empirical Methods in Natural Language Processing (EMNLP), 2024. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing. arXiv preprint arXiv:2406.08464, 2024. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley Function Calling Leaderboard. https://gorilla.cs.berkeley.edu/blogs/8 berkeley function calling leaderboard.html, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 Technical Report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388, 2025. Zihao Zeng, Yibo Miao, Hongcheng Gao, Hao Zhang, and Zhijie Deng. AdaMoE: Token-Adaptive Routing with Null Experts for Mixture-of-Experts Language Models. arXiv preprint arXiv:2406.13233, 2024. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: Human-Centric Benchmark for Evaluating Foundation Models. arXiv preprint arXiv:2304.06364, 2023. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-Following Evaluation for Large Language Models. arXiv preprint arXiv:2311.07911, 2023."
        }
    ],
    "affiliations": [
        "Inclusion AI",
        "Renmin University of China",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Westlake University",
        "Zhejiang University"
    ]
}