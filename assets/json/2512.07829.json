{
    "paper_title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation",
    "authors": [
        "Yuan Gao",
        "Chen Chen",
        "Tianrong Chen",
        "Jiatao Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 9 2 8 7 0 . 2 1 5 2 : r One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation Yuan Gao, Chen Chen, Tianrong Chen, Jiatao Gu Apple Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representationseither by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generationfriendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), simple-yet-effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and second that takes the reconstructed features as input for image generation. FAE is genericit can be instantiated with variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative familiesdiffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256256, our diffusion model with CFG attains nearstate-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning. Correspondence: Yuan Gao (yga65@apple.com), Jiatao Gu (jgu32@apple.com) Date: December 9,"
        },
        {
            "title": "Introduction",
            "content": "In past years, diffusion models (Ho et al., 2020; Saharia et al., 2021; Rombach et al., 2022a) have significantly advanced the quality and flexibility of visual generation, making them the dominant framework for producing high-resolution images and videos. key recent change driving this progress is the integration of powerful pretrained visual representations, typically obtained from large-scale self-supervised learning frameworks based on masked image prediction (Oquab et al., 2023; Tschannen et al., 2025). Such frameworksexemplified by models like REPA (Yu et al., 2024b) and VA-VAE (Yao et al., 2025) utilize the rich semantic and structural information from the large models trained on unlabeled data. When incorporated into the diffusion pipelineeither within the denoising process or in variational autoencoders (VAEs)these representations substantially improve both training efficiency and generative fidelity. Despite this progress, adapting pre-trained visual representations to generative models remains challenging due to the inherent mismatches between understanding-oriented representations and generation-friendly latent spaces. Self-supervised models, in order to build up hard task with the unlabelled data, masking and prediction tasks are used, not only in the image area but also text and audio. To capture the complicated distribution of different possibilities of the masked regions, especially to simulate the distribution with simple embedding multiplication and softmax function, large latent dimension is required. In contrast, generation models such as diffusion models and normalizing flow models, are often formulated as denoising processes, evolving noisy input toward clean signal through an iterative refinement process. In diffusion models, for 1 Figure 1 Comparison between standard VAE (Rombach et al., 2022b), VA-VAE (Yao et al., 2025), RAE (Zheng et al., 2025) and our proposed FAE. The number shows the channel dimension of the generative modeling space. instance, the input is perturbed by Gaussian noise and repeatedly denoised through multiple timesteps. To ensure trajectory stability throughout this process, the hidden representations must simultaneously encode the information of both the noised input and its clean predicted target. When the latent dimension is large, this dual encoding becomes more resource demanding, and the diffusion dynamics become sensitive to noise-level scheduling, often leading to instability or slower convergence. Therefore, generative models favor compact, low-dimensional latent spaces, which make the denoising trajectory smoother, reduce the training burden, and preserve generative fidelity under limited model capacity. This discrepancy often results in inefficiencies and necessitates complex architectural modifications when integrating pre-trained representations. In this paper, we revisit this problem from different perspective and ask: Is it truly necessary to preserve the high-dimensional structure of pre-trained visual representations when just zero-shot adapting them? In fact, although self-supervised models are trained on masked prediction tasks, the adaptation only involves unmasked inputs where the need for modeling diverse distribution is diminished. Instead, the goal is to leverage the rich semantics and spatial information from the pre-train features. Building on this insight, we introduce FAE (Feature Auto-Encoder), simple yet effective framework that compresses the pre-train embedding into compact, generation-friendly space. We employ only single attention layer followed by linear projection to map the embeddings into continuous low-dimensional code, and use lightweight decoder to reconstruct the original features. During experiments, we observed that the adaptation task is substantially weaker than the original self-supervised pre-training task; as result, overly complex adapting frameworks tend to lose information from the pre-trained embeddings. Empirically maintaining closer distance between the compressed code and the original embedding leads to higher reconstruction quality. Therefore, we adopt minimal design, using only single attention layer as the encoder to remove redundant global information shared across patch embeddings. We validate our method by integrating it into existing diffusion frameworks, including SiT (Ma et al., 2024) and LightningDiT (Yao et al., 2025) and normaling-flow based models (e.g., STARFlow (Gu et al., 2025a)). On ImageNet 256256 generation, with CFG, diffusion model using FAE attains nearstate-of-the-art FID of 1.29 in 800 epochs and reaches an FID of 1.70 within only 80 training epochs. Without CFG, diffusion model using FAE achieves state-of-the-art FID of 1.48 in 800 epochs and reaches an FID of 2.08 with only 80 epochs, highlighting both its generation quality and its learning efficiency."
        },
        {
            "title": "2 Related Work",
            "content": "Visual Representation Learning Self-supervised learning (SSL) has become cornerstone for learning general visual representations without manual labels. Early contrastive frameworks such as MoCo (He et al., 2020) and SimCLR (Chen et al., 2020) maximized agreement between augmented views of the same image, while later non-contrastive approaches such as BYOL (Grill et al., 2020) and DINO (Caron et al., 2021) demonstrated that strong representations can emerge even without negative pairs. These methods trained large Vision Transformers (ViTs) (Dosovitskiy et al., 2021b) to learn globally coherent, semantically rich feature spaces that transfer effectively to downstream tasks. More recent works have explored parameter-efficient ways to adapt such pretrained ViTs. Adapter-based and prompt-tuning methodsincluding AdaptFormer (Chen et al., 2022a) and Visual Prompt Tuning (VPT) (Jia et al., 2022)insert lightweight modules or learnable tokens into frozen backbones to tailor them to new domains with minimal fine-tuning overhead. However, most SSL-based adaptation studies focus on discriminative objectivesclassification, segmentation, or retrievalrather than generative modeling. Our work differs in that it repurposes pretrained self-supervised ViT for visual generation, showing that single attention layer suffices to bridge the gap between discriminative pretraining and generative diffusion training. Visual Generative Models Diffusion models (Ho et al., 2020; Song et al., 2021b) have emerged as dominant paradigm for image generation, achieving remarkable realism and diversity. LDM (Rombach et al., 2022b) further improved efficiency by performing denoising in compressed latent space learned by VAE. Subsequent work has explored various architectural improvements to transformers, including DiT (Peebles and Xie, 2023b) and SiT (Zhang et al., 2023), which are designed for scalability and enhanced global context modeling. More recently, normalizing flow based models such as TARFlow (Zhai et al., 2024) and STARFlows (Gu et al., 2025a,b) have emerged new type of generative models that become promising alternatives drastically different from standard diffusion models on visual generation. Representation Alignment Aligning generative model representations with pretrained visual encoders has proven effective for stabilizing training and enhancing sample quality. REPA (Yu et al., 2024b) proposes to align noisy hidden features of diffusion transformers with clean image embeddings from pretrained ViT, substantially accelerating convergence and improving fidelity. Follow-up work such as REPA-E(Yang et al., 2024) extends this idea by enforcing semantic consistency between latent tokens and image features throughout training. Recent analyses have emphasized the inherent tension between reconstruction quality and generation stability, underscoring the need to reconcile these competing objectives. VA-VAE (Xu et al., 2024) highlights that high-dimensional latent spaces may favor reconstruction but hinder generative convergence, motivating strategies that align latent encoders with pretrained vision models. Along this line, concurrent to our work, several studies have explored directly using pre-trained embeddings as tokenizer inputs to improve generation quality, including (Bi et al., 2025) and (Gui et al., 2025). Contemporary work RAE (Zheng et al., 2025) directly adopts pre-trained embeddings as the diffusion latent space. While this avoids explicit alignment, it demands significant architectural changes to the generator (e.g., wider channels, additional heads) to accommodate high-dimensional feature maps. Our work complements these findings by reusing pretrained ViTs directly as the generative backbone rather than modifying the latent space, and demonstrates that lightweight adaptation via single attention layer can yield stable and high-quality diffusion generation. Where prior methods typically rely on external alignment losses or auxiliary projection heads to bridge discriminative and generative representations, our method internalizes the alignment process: by inserting single-layer attention adapter into pretrained ViT, we enable the models own attention mechanisms to reconcile discriminative and generative objectives. This design unifies feature reuse and representation alignment within single, minimal architectural modification, reducing both training cost and overall system complexity."
        },
        {
            "title": "3 Motivation",
            "content": "A core challenge in adapting pre-trained visual representations for generative models is the inherent dimensional and functional mismatch between self-supervised understanding models and generation models. Representation encoders (He et al., 2021; Oquab et al., 2023; Tschannen et al., 2025; Radford et al., 2021), 3 whose performance typically improves with higher-dimensional feature spaces, naturally favor large embeddings. For example, Dino-V2-G (Oquab et al., 2023) has dimension of 1, 536. This has also become the de-facto choice in MLLMs (Liu et al., 2023; Bai et al., 2023). However, such high-dimensional representations are poorly suited for many generative models (Rombach et al., 2022a; Peebles and Xie, 2023a; Gu et al., 2025a) operating in latent space. Unlike representation learning tasks, which only require encoding semantic information, generation tasks must accurately recover fine-scale details from noisy inputs, making the denoising process highly sensitive to the dimensionality and structure of the latent space, making it harder for the model to preserve and refine the injected noise, leading to instability and reduced sample quality. In normal cases, generation works in much lower dimensional space, ranging from 4 64. Existing methods tackle this mismatch from two main directions: (1) Feature alignment. Methods such as REPA (Yu et al., 2024b) and VA-VAE (Yao et al., 2025) attempt to align the features of an external representation encoder with the generative model, either inside the generator or within VAE. This typically requires carefully designed alignment losses and additional training stages. However, because the generator or VAE architecture is substantially different from the pre-trained encoder, such alignment inevitably discards information that is not immediately useful for generation, limiting the benefit of using the original representations. (2) Direct modeling. More recently, RAE (Zheng et al., 2025) directly adopts pre-trained embeddings as the diffusion latent space. While this avoids explicit alignment, it demands significant architectural changes to the generator (e.g., wider channels, additional heads) to accommodate high-dimensional feature maps. As result, the model design becomes tightly coupled to the embedding dimensionality, making it difficult to scale or transfer across different encoders. This motivates us to seek design that simultaneously (1) keeps generation in low-dimensional latent space, so that existing generative architectures can be reused without substantial ad-hoc modifications; and (2) remains as close as possible to the representation-encoder feature space, so that we fully inherit the strengths of pre-trained features and can ideally recover their semantics from the learned latents, rather than relying on lossy alignment losses. To this end, we introduce FAE, new Feature Auto-Encoding approach in which lightweight encoder compresses high-dimensional representation features into compact latent space tailored for generation. In the following, we formalize this design and describe the architecture and training objectives of FAE in detail."
        },
        {
            "title": "4 Method",
            "content": "In this section, we introduce FAE, simple-yet-effective frame to bridge visual representation learning and generative models using feature-level autoencoders."
        },
        {
            "title": "4.1 Single-Attention Encoder",
            "content": "We first train feature encoder to compress frozen pre-trained embeddings into low-dimensional latent space. To preserve as much information as possible from the large pre-trained model, we deliberately use minimal encoder: single self-attention layer followed by linear projection that maps pre-trained patch embeddings to compact latents z. This design reduces the parameter count and keeps the mapping close to the original feature space (see Figure 7 in Appendix). Importantly, the adaptation objective (feature reconstruction) is substantially weaker than the original selfsupervised pre-training task (e.g., masked region prediction). Over-parameterized encoders tend to overfit this easier objective, effectively re-encoding features for reconstruction and discarding information that is not directly supervised. Empirically, we find that keeping the encoder shallow and the latents close to the pretrained embeddings leads to higher reconstruction quality and better downstream understanding. Notably, the self-attention layer is crucial: it operates across patch embeddings to remove redundant global information and redistribute capacity, whereas purely linear projection acts independently on each feature dimension and cannot adaptively de-redundantize patch-wise information. Our ablation in Table 5 confirms these observations: the single-attention encoder outperforms purely linear encoder and yields significantly better reconstruction quality than deep Transformer encoder."
        },
        {
            "title": "4.2 Double Decoder",
            "content": "A central design in FAE is to separate feature reconstruction from image synthesis. Given compressed latents z, we first reconstruct the original representation space, and only then decode pixels from the reconstructed features. This double-decoder design lets us preserve the semantics of the frozen encoder while giving the pixel decoder the flexibility needed for high-fidelity image generation. Feature Decoder. Starting from the compact latent z, 6-layer Transformer feature decoder reconstructs the original embedding ˆx. Each layer uses the same hidden dimension as the corresponding pre-trained backbone (e.g., DINOv2) so that the reconstructed features live in compatible representation space. We employ Rotary Positional Embedding (RoPE) (Su et al., 2024), RMSNorm (Zhang and Sennrich, 2019), and SwiGLU activations (Shazeer, 2020), which we find empirically improve stability and reconstruction quality. The feature decoder is trained with standard VAE objective consisting of an ℓ2 reconstruction term and KL regularization term: LVAE = ˆx x2 2 + β KL(cid:0)q(z x) p(z)(cid:1). (4.1) This encourages to remain close to simple prior while retaining enough information for accurate recovery of the pre-trained embeddings. Compare to prior work like VA-VAE (Yao et al., 2025) and Wang and He (2025), our loss is quite simple, its just L2 Loss. This make our reconstructed results can be easily zero-shot adapted into downstream task trained on original pre-train embedding, which we will show the results in the following subsection. In practice, we observe that high-quality feature reconstruction is strong predictor of downstream generative and understanding performance. Pixel Decoder. On top of the reconstructed features ˆx, we attach ViT-Lbased pixel decoder (Dosovitskiy et al., 2021a) that maps them to RGB images. Conceptually, the feature decoder restores the language of the pre-trained encoder, and the pixel decoder learns to translate that language into pixels. By letting the pixel decoder operate on rich, semantically meaningful embeddings rather than raw latents, we simplify the generation problem and improve visual fidelity. Following prior work (Yu et al., 2024a), we train the pixel decoder with combination of adversarial, perceptual, and reconstruction losses: Lpix = λGANLGAN + λpercLperc + λrecLrec. (4.2) Here, the adversarial term LGAN encourages realistic textures and global coherence, the perceptual loss Lperc aligns high-level features with the ground-truth images, and the reconstruction term Lrec preserves low-level details. Together with the feature decoder, this yields compact latent space that remains semantically faithful to the pre-trained encoder while supporting high-quality image synthesis. In the first stage, we inject We train the pixel decoder in two stages, entirely in the embedding space. Gaussian noise into the frozen pre-trained embeddings and train the decoder to directly reconstruct images ϵ (cid:0)0, σ2I(cid:1), where we set σ = 0.4 for DINOv2 and scale it according from these noisy features: = x+ϵ, to the norm of the pre-trained embeddings. This produces Gaussian embedding decoder that is robust to moderate perturbations in the representation space. Pixel Fine-Tuning. Once the first stage converges, we fine-tune the same pixel decoder on the reconstructed embeddings ˆx produced by the feature decoder. Because the decoder only operates on embedding space, the Gaussian embedding decoder can be reused across different variants of FAE without architectural changes. Remarkably, even without fine-tuning on ˆx, the Gaussian embedding decoder already achieves strong generation quality , indicating that our compressed latent space preserves the majority of the information in the original pre-trained embeddings. We visualize the overall training stages in Figure 2. The detailed parameters and pixel decoder reconstruction quality are available in the Appendix."
        },
        {
            "title": "4.3 Generative Model Training",
            "content": "Once the latent space is ready, we in parallel train generative models directly on the low-dimensional, compact latents z. During this stage, only the frozen backbone encoder and the single-layer feature encoder are used to map images into latent space, making generator training both memoryand compute-efficient. This decoupling turns the latent space into modular interface: as long as model can predict or transform z, it can be used as generator without any change to its core architecture. In this work, we instantiate two 5 Figure 2 An illustration of Training Stages of FAE. Stage Ia and Ib can be trained independently. representative models on top of the same FAE latent space: SiT (Ma et al., 2024), diffusion model, and STARFlow (Gu et al., 2025a), normalizing flow. For both, we adopt the default parameterizations and training configurations from their original works, simply replacing their native latent representation with our learned z, without additional architectural tricks or alignment losses."
        },
        {
            "title": "4.4 Semantic Preservation in the Latent Space",
            "content": "A key property of FAE is that it can be directly applied to variety of pre-trained visual representations (e.g., DINOv2 (Oquab et al., 2023), SigLIP (Tschannen et al., 2025)) without architectural changes, while largely preserving their understanding capabilities, thanks to an explicit feature-decoder reconstruction objective that encourages the latent space to stay semantically close to the original embeddings rather than collapsing into purely generative code. We verify this by examining patch-wise similarity structure (see Figure 8 and Figure 9 in Appendix). After passing through FAE, patches that are close in the original representation space remain close in our latent space, indicating that FAE largely preserves the relational geometry of the pre-trained features. Moreover, our latents retain the cross-image patchmatching behavior characteristic of DINOv2: semantically corresponding regions across different images (e.g., players hand, an animals head) are still reliably matched using cosine similarity in the FAEs latent space (see Figure 3 and also Figure 10 Figure 11 in Appendix). This suggests that FAE preserves fine-grained, part-level semantics rather than only coarse global information. Figure 3 Matching across images. We match patch-level FAE features between images from different images that share similar semantic information. This exhibits the ability of our model to understand relations between similar parts of different objects. 6 Model SigLIP2 FAE (SigLIP2) ViT g-opt/16 g-opt/ Res COCO TI COCO IT 256 256 73.10% 72.94% 55.45% 55.79% Table 1 Text & Image Retrieval on COCO dataset. Method Training Epochs #Params Generation w/o CFG Generation w/ CFG gFID sFID IS Pre. Rec. gFID sFID IS Pre. Rec. MaskGIT (Chang et al., 2022) LlamaGen (Sun et al., 2024) VAR (Tian et al., 2024) MagViT-v2 (Yu et al., 2023) MAR (Li et al., 2024) MaskDiT (Zheng et al., 2023) DiT (Peebles and Xie, 2023a) SiT (Ma et al., 2024) FasterDiT (Yao et al., 2024) MDT (Gao et al., 2023a) MDTv2 (Gao et al., 2023b) REPA (Yu et al., 2024b) VA-VAE RAE (DiT-XL) RAE (DiTDH-XL) FAE FAE w/ Timestep Shift 555 300 350 1080 800 1600 1400 1400 400 1300 1080 800 64 800 800 80 800 64 80 800 64 80 800 AutoRegressive (AR) 227M 3.1B 2.0B 307M 945M 6.18 9.38 3.65 2.35 8.24 182.1 112.9 200.5 227.8 Latent Diffusion Models 675M 675M 675M 675M 675M 675M 675M 675M 675M 676M 839M 839M 675M 675M 675M 675M 675M 675M 5.69 9.62 8.61 7.91 6.23 5.90 5.14 2.17 1.87 2.16 1.51 2.55 2.39 1.58 2.34 2.08 1.48 10.34 6.85 6.32 5.45 5.23 4.22 4.36 4.37 4.38 4.38 4.38 4.20 4.24 177.9 121.5 131.7 131.3 143.0 130.2 205.6 209.7 214.8 242.9 189.9 192.8 223.7 206.6 207.6 239.8 0.80 0.69 0. 0.74 0.67 0.68 0.67 0.71 0.76 0.77 0.80 0.82 0.79 0.82 0.82 0.80 0.83 0.82 0.81 0.51 0.67 0.62 0.60 0.67 0.67 0.69 0.65 0.62 0.65 0.63 0.59 0.63 0.58 0.59 0. 0.58 0.59 0.63 2.18 1.80 1.78 1.55 2.28 2.27 2.06 2.03 1.79 1.58 1.42 2.11 1.35 1.41 1.13 2.01 1.92 1.41 1.87 1.70 1.29 5.97 5.67 4.60 4.50 4.63 4.57 4.52 4.70 4.16 4.15 4.39 4.35 4.34 4.39 4.33 4.32 263.3 365.4 319.4 303.7 276.6 278.2 270.3 264.0 283.0 314.7 305.7 252.3 295.3 309.4 262.6 250.3 249.6 274. 241.1 243.8 268.0 0.81 0.83 0.81 0.80 0.83 0.82 0.81 0.81 0.79 0.80 0.81 0.79 0.80 0.78 0.83 0.83 0.81 0.82 0.82 0.80 0.58 0.57 0. 0.61 0.57 0.59 0.60 0.61 0.65 0.65 0.58 0.65 0.63 0.67 0.59 0.59 0.63 0.59 0.61 0.64 Table 2 Class-conditional Image Generation Performance on ImageNet 256256."
        },
        {
            "title": "5 Experiments",
            "content": "We evaluate our proposed method on two standard generation benchmarks: class-conditional image generation on ImageNet-1K (Deng et al., 2009) and text-to-image generation trained on CC12M (Changpinyo et al., 2021) and evaluated on MS-COCO (Lin et al., 2015). Our experiments show that the proposed approach substantially accelerates training convergence while improving overall generation quality. To further demonstrate the generality of our framework, we also apply it to the STARFlow (Gu et al., 2025a) training paradigm and observe consistent improvements. In addition, we investigate whether the learned latent representations preserve strong semantic understanding capabilities by performing zero-shot adaptation on common downstream tasks, including ImageNet-1K linear probing and MS-COCO imagetext retrieval."
        },
        {
            "title": "5.1 Class-conditional Image Generation",
            "content": "Implementation details. We processed images from ImageNet into resolution of 256256 following ADM (Dhariwal and Nichol, 2021). Then each image is then encoded into compressed vector of shape 161632 using FAE. For pre-train embedding, we explored DinoV2 (Oquab et al., 2023), we use batch size of 1024 for training VAE. For latent diffusion model, we explored SiT following Ma et al. (2024)s setting and LightningDit following Yao et al. (2025)s setting, we use the XL model size from these papers. To ensure fair comparison with DiTs and SiTs, we consistently use batch size of 512 during training. Evaluation and results. For generation without CFG (Ho and Salimans, 2022), we use SDE (Song et al., 2021a) and runs 250 steps. We achieve an FID score of 2.08 in 80 epochs and an FID score of 1.48 in 800 epochs. For generation with CFG, we use ODE and runs 250 steps. We achieve an FID score of 1.70 in 80 epochs, and an FID score of 1.29 7 Model Res ImageNet top-1 DinoV2-S/14 distilled DinoV2-B/14 distilled DinoV2-L/14 distilled DinoV2-g/14 FAE (DinoV2-g/14) 224 224 224 224 224 80.80% 84.40% 86.50% 87.00% 86.17% Table 3 ImageNet Linear Probing top-1 accuracy comparison for FAE and different DinoV2 variants (all at 224 resolution). Figure 4 FID Score Converging Curve in 800 epochs. Detailed CFG and timesteps shifts are available in the appendix. Experiments on ImageNet demonstrate that our method can achieves state-of-the-art (SOTA) performance on image generation without CFG. And the CFG results also got improved comparing to VA-VAE. We also provide some examples for the class conditioned generation in Figure 5. Detailed hyper-parameter settings are provided in Appendix Section D. Figure 5 Random samples of ImageNet 256x256 and Text-to-Images using diffusion models."
        },
        {
            "title": "5.2 Text-to-Image Generation",
            "content": "Implementation details. For pre-train embedding, we reused the DinoV2 Encoder from Imagenet and train an extra Siglip2 (Tschannen et al., 2025) Encoder on ImageNet. For training latent diffusion, we only use CC12M dataset with 256x256 resolution and follow the data processing from starflow (Gu et al., 2025a)s setting. We consistently use batch size of 256 during training. For text tokenzier, we use t5-xl. We compare our model based on DinoV2 and Siglip2 embeddings. We also add SD-VAE as baseline. Evaluation and results. We evaluate FAE on MS-COCO (Lin et al., 2015) following the data preprocessing protocol of U-ViT (Bao et al., 2023). For sampling without using CFG (Ho and Salimans, 2022), we use an SDE sampler (Song et al., 2021a) with 250 steps and obtain an FID of 7.47 after 400 training epochs. When using CFG, we switch to an ODE sampler with 250 steps and further improve the FID to 6.90 at 400 epochs. Notably, these nearstate-of-the-art results are achieved using only CC12M for pre-training, i.e., with significantly fewer images than typical web-scale text-to-image models. We also provide qualitative text-to-image examples in Figure 5, and more examples in Appendix Figure 13. These samples are generated at 384 384 resolution using SigLIP2-FAE backbone and an MMDiT decoder with 2B parameters. The model produces visually coherent images that follow short text prompts, indicating good textimage alignment. 8 Model FID Type Training datasets #Params DALL-E (Ramesh et al., 2021) CogView (Ding et al., 2021) LAFITE (Zhou et al., 2021) GLIDE (Nichol et al., 2021) Make-A-Scene (Gafni et al., 2022) DALL-E 2 (Ramesh et al., 2022) Imagen (Saharia et al., 2022) Parti (Yu et al., 2022) Re-Imagen (Chen et al., 2022b) GAN Diffusion DALL-E dataset (250M) Internal dataset (30M) CC3M (3M) DALL-E dataset (250M) 28 Autoregressive 27.1 Autoregressive 26.94 12.24 11.84 Autoregressive Union datasets (without MS-COCO) (35M) 10.39 DALL-E dataset (250M) Internal dataset (460M) + LAION (400M) 7.27 7.23 Autoregressive LAION (400M) + FIT (400M) + JFT (4B) 6.88 KNN-ImageText (50M) Diffusion Diffusion Diffusion SDVAE+T5 (w/o CFG) 21. Diffusion FAE (SigLIPV2)+T5 (w/o CFG) FAE (SigLIPV2)+T5 (w/ CFG) FAE (DinoV2)+T5 (w/o CFG) FAE (DinoV2)+T5 (w/ CFG) 7.57 7.11 7.47 6.90 Diffusion Diffusion Diffusion Diffusion CC12M CC12M CC12M CC12M CC12M 12B 4B 75M + 151M (TE) 3.5B + 1.5B (SR) 4B 4.5B + 700M (SR) 2B + 4.6B (TE) + 600M (SR) 20B + 630M (AE) 2.5B + 750M (SR) 604M 604M+ 514M (FAE) 604M+ 514M (FAE) 604M+ 514M (FAE) 604M+ 514M (FAE) Table 4 FID results of different models on MS-COCO validation (256 256). All the models are trained on external dataset and zero-shot evaluated on MS-COCO using 30K example. Overall, experiments on COCO show that our method attains competitive, near-SOTA image generation quality while using substantially less training data. Detailed hyper-parameter settings are provided in the Appendix Section D."
        },
        {
            "title": "5.3 Latent Normalizing Flows",
            "content": "We further validate the universality of FAE by training different family of latent generative model. Specifically, we instantiate STARFlow (Gu et al., 2025a) an end-to-end latent normalizing flow generator that maps noise directly to FAEs latents, using the default configuration of 1.4B parameters and standard SD-VAE baseline. The original STARFlow uses patch size 1 for SD-VAE latents, yielding sequences that are 4 longer than those of FAE; for fair comparison, we instead use patch size 2 for SD-VAE, resulting in sequence length of 256 equivalent to FAE. As shown in Figure 6, the SD-VAE baseline attains FID of 4.51 under 400 training epoch, whereas the FAE-based variant (DinoV2-g/14) achieves FID of 2.67 and converges substantially faster for both guided and unguided scenarios. Additional visual results are provided in the Appendix. (a) Results without CFG. (b) Results with CFG. Figure 6 Comparison of STARFlow (Gu et al., 2025a) with SDVAE and the proposed FAE under the same settings."
        },
        {
            "title": "5.4 Image Understanding",
            "content": "We further validate the reconstructed embedding quality by zero-shot adapting it to the downstream tasks. Linear Probing. We evaluate the DINOv2 (Oquab et al., 2023) linear probing using existing layer and weights from the origin model. We directly passed the FAE reconstructed embedding to the layer. For data preprossing, we follow DINOv2. Espeically, we use the register version of FAE (The generation results are 9 Model Linear Probing CFG 64 Epochs Single Attention Linear 6-Layer Transformer Direct Predict DinoV2 86.17% 85.74% w/o w/ w/o w/ w/o w/ w/o w/ 2.98 3.03 3.31 15.37 160 Epochs 2.27 1.79 320 Epochs 1.98 1.61 2.38 1.92 2.47 1. 12.99 17.85 2.07 1.76 2.13 1.65 12.72 16.53 Table 5 Ablation results comparing different encoder structure. similar to the one we listed in the main experiments, which is based on the non-register one.) Results are shown in Table Text-Image Retrieval. We evaluate the MS-COCO text-image dataset on COCO2014, following Siglip2s original setting. We compute the text embedding using SigLip2 (Tschannen et al., 2025) text encoder. And compute the image embedding using image encoder and pass it to FAE and use the reconstructed embedding. Then we compute the cosine similarity between the pair and find the Top-1. Results are shown in Table 1."
        },
        {
            "title": "6 Ablation Study",
            "content": "In this section, we conduct several ablation experiments to examine the impact of each component in our single-layer adaptation framework. For CFG-guidance, we grid search the CFG in 0.1 level and report the best results. All the CFG are enabled from t=0.7 to t=0.0 in the ablation experiments. For pixel decoder, we use decoder fine-tuned with 6 layer transformer reconstructed DinoV2 embedding. Detailed experiment settings and results for each ablation study are available in Section in Appendix. FAE Model Structure. We evaluate the different FAE structure and architectural design in this subsection. We show that the shallower networks yield better generation quality and faster convergence. Our single attention layer model outperforms both linear and 4/6 layer transformers in FID, while achieving comparable embedding-reconstruction similarity to deeper transformersand substantially higher similarity than the linear baseline. This suggests that compact attention design not only accelerates optimization but also preserves semantic representation quality, potentially benefiting downstream zero-shot probing. Results are available in Table 5 LDM Model Structure. We further analyze the model structure changes in the latent diffusion model. Starting from the base SiT architecture, we sequentially integrate SwiGLU, ROPE, and RMSNorm, resulting in structure equivalent to LightningDiT. Our conclusion is consistent with the Lightning paper, each component contributes positively to both convergence speed and overall generation quality, with the largest gains observed when all three are combined. See Table 6 Model SiT SiT + SwiGLU SiT + SwiGLU + ROPE SiT + SwiGLU + ROPE + RMSNorm CFG 64 Epochs w/o w/ 2.98 160 Epochs 2.27 1.79 320 Epochs 1.98 1. w/o w/ w/o w/ w/o w/ 3.02 2.86 2.74 2.26 1.75 2.182 1.78 2.15 1.71 1.97 1.60 1.89 1.63 1.86 1. Table 6 Ablation results comparing LDM model structure 10 Token Dimension. We test VAE latent dimensions of 32, 48, and 64. After fine-tuning from the same decoder trained on Gaussian noise, the 64-dim model shows lower rFID than 48-dim and 32-dim variants However, the final generation results indicate that the 32-dim setting achieves the best FID scores and IS scores, as well as the fastest convergence. Notably, when time-shift is enabled (see below), the performance gap between different token dimensions narrows substantially. Results are available in Table 7 Model 32-dim 48-dim 64-dim CFG 64 Epochs w/o w/ 2.67 160 Epochs 2.02 1.70 320 Epochs 1.76 1.52 w/o w/ w/o w/ 2.73 2.86 2.10 1.73 2.25 1.76 1.88 1. 1.99 1.64 Table 7 Ablation results comparing different latent dimension. Time Shift. Finally, we ablate the time-shift parameter in the diffusion process. By introducing timeshift, loss weighting and diffusion trajectory changes, thus effectively bridge the quality differences across VAE latent dimensions and significantly accelerate convergence. With time-shift, our model reaches state-ofthe-art convergence speed and generation quality within only 64 epochs. Results are available in appendix Table"
        },
        {
            "title": "7 Conclusion",
            "content": "We presented FAE, simple yet powerful framework for adapting high-quality self-supervised visual repIn contrast to prior approaches that rely on complex objectives or resentations for generative modeling. substantial architectural modifications to diffusion models, FAE uses an extremely simple design: single attention layer paired with two lightweight decoders. Across class-conditional and text-to-image benchmarks, FAE demonstrates strong and efficient performance. On ImageNet 256256, our diffusion model with CFG achieves nearstate-of-the-art FID of 1.29 with 800 training epochs and 1.70 with only 80 epochs. Without CFG, FAE attains state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), highlighting both its high sample quality and fast learning behavior. Despite these promising results, FAE still has limitations. Because the encoder is trained without an explicit image reconstruction loss, the rFID and tokenizer fidelity lag behind methods such as VA-VAE that directly optimize reconstruction quality. Overall, FAE provides simple and general mechanism for leveraging pretrained vision encoders in generative modeling, offering compelling balance between architectural minimalism, adaptability, and performance."
        },
        {
            "title": "References",
            "content": "Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models, 2023. Tianci Bi, Xiaoyi Zhang, Yan Lu, and Nanning Zheng. Vision foundation models can be good tokenizers for latent diffusion models, 2025. Mathilde Caron et al. Emerging properties in self-supervised vision transformers. In IEEE International Conference on Computer Vision, 2021. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 11 Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts, 2021. Shaoxiang Chen et al. Adaptformer: Adapting vision transformers for scalable visual recognition. In Advances in Neural Information Processing Systems, 2022a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, 2020. Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. ArXiv preprint, 2022b. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. In Advances in Neural Information Processing Systems, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021a. Alexey Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021b. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. ArXiv preprint, 2022. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2316423173, 2023a. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023b. Jean-Bastien Grill et al. Bootstrap your own latent: new approach to self-supervised learning. In NeurIPS, 2020. Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Miguel Angel Bautista, Josh Susskind, and Shuangfei Zhai. Starflow: Scaling latent normalizing flows for highresolution image synthesis. arXiv preprint arXiv:2506.06276, 2025a. Jiatao Gu, Ying Shen, Tianrong Chen, Laurent Dinh, Yuyang Wang, Miguel Angel Bautista, David Berthelot, Josh Susskind, and Shuangfei Zhai. Starflow-v: End-to-end video generative modeling with normalizing flow. arXiv preprint arXiv:2511.20462, 2025b. Ming Gui, Johannes Schusterbauer, Timy Phan, Felix Krause, Josh Susskind, Miguel Angel Bautista, and Björn Ommer. Adapting self-supervised representations as latent space for efficient generation, 2025. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. IEEE TPAMI, 2020. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv:2111.06377, 2021. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Menglin Jia et al. Visual prompt tuning. In European Conference on Computer Vision, 2022. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. 12 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: arXiv preprint Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv:2401.08740, 2024. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. ArXiv preprint, 2021. Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023a. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE Conference on Computer Vision and Pattern Recognition, 2023b. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Proceedings of the 38th International Conference on Machine Learning, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. ArXiv preprint, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022a. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition, 2022b. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. ArXiv preprint, 2021. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv preprint, 2022. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, 2021b. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas 13 Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization, 2025. Mingyu Xu et al. Reconstruction vs. generation: Taming the optimization dilemma in latent diffusion models. IEEE Conference on Computer Vision and Pattern Recognition, 2024. Zeyu Yang et al. Representation entanglement for generation. In IEEE Conference on Computer Vision and Pattern Recognition, 2024. Jingfeng Yao, Wang Cheng, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. arXiv preprint arXiv:2410.10356, 2024. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models, 2025. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. ArXiv preprint, 2022. Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation, 2024a. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024b. Shuangfei Zhai, Ruixiang Zhang, Preetum Nakkiran, David Berthelot, Jiatao Gu, Huangjie Zheng, Tianrong Chen, Miguel Angel Bautista, Navdeep Jaitly, and Josh Susskind. Normalizing flows are capable generative models. arXiv preprint arXiv:2412.06329, 2024. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Kai Zhang et al. Sit: Generative diffusion models with sequential feature transformations. In IEEE International Conference on Computer Vision, 2023. Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Lafite: Towards language-free training for text-to-image generation. ArXiv preprint, 2021."
        },
        {
            "title": "A FAE Encoder Structure",
            "content": "We merge the consecutive linear layers in the attention module and use larger per-head dimension for the encoder. The structure of our encoder is shown in Figure 7. Figure 7 Modified Attention"
        },
        {
            "title": "B Ablation on FlowMatching Timesteps Shift",
            "content": "Our ablation experiment demonstrates that applying Timesteps Shift accelerates convergence, mitigates the discrepancy across different latent token dimensions, and provides modest improvement in the final FID score."
        },
        {
            "title": "Model",
            "content": "32-dim, ts=0.7 32-dim, ts=0.5 32-dim, ts=0.3 48-dim, ts=0.5 48-dim, ts=0.3 64-dim, ts=0. CFG 64 Epochs w/o w/ 2."
        },
        {
            "title": "320 Epochs\n1.6836\n1.5131",
            "content": "w/o w/ w/o w/ w/o w/ w/o w/ w/o w/ 2.3233 2.3220 2.4329 2.3599 2.4398 1.8501 1.6735 1.8800 1. 1.9546 1.6797 1.9105 1.6694 1.9549 1.7563 1.7086 1.5682 1.7743 1.6227 1.6952 1. 1.6911 1.5423 1.7581 1.5402 Table 8 Ablation results comparing different timesteps shift across different token dimension."
        },
        {
            "title": "C rFID",
            "content": "Because the encoder training is disentangled with the image reconstruction loss, our rFID and tokenizer reconstruction fidelity lag behind methods such as VA-VAE that directly optimize reconstruction quality. SD-VAE VA-VAE FAE 32-dim FAE 64-dim rFID 0.73 0.28 0. 0.66 Table 9 Reconstruction rFID comparison."
        },
        {
            "title": "D FAE Hyper Parameters",
            "content": "Category Field Encoder Decoder Pixel Decoder 16161536 2562563 1024 24 4 64 16 305. 1M 512 AdamW 1e-4 Cosine 1000 (0.9,0.999) Architecture Optimization Interpolants Input dim. Output dim. Hidden dim. Num. layers MLP Ratio Dim. per head Num. heads Total Params (M) Training iters Batch size Optimizer Peak LR LR Scheduler Warmup (β1, β2) αt σt wt Training objective Sampler Sampling steps Guidance 16161536 161664 6144 1 256 24 38.17 161632 16161536 1536 6 4 64 24 170.43 1M 1024 AdamW 1e-4 Cosine 1000 (0.9, 0.999) LDM 161632 161632 1152 28 4 72 16 675. 2M 512 AdamW 1e-4 Constant (0.9,0.999) MMDiT 161632 161632 1024 16 4 64 16 603.46 1M 512 AdamW 1e-4 Constant (0.9,0.999) MMDiT 384x384 242464 242464 1536 24 4 64 24 2017. 1M 512 AdamW 1e-4 Constant (0.9,0.999) 1-t σt v-prediction Euler-Maruyama (w/o CFG) Euler (w/ CFG) 250 0.9 (t=10.9) 2.5 (t=0.70) 1-t σt v-prediction Euler-Maruyama (w/o CFG) Euler (w/ CFG) 250 1-t σt v-prediction Euler-Maruyama (w/o CFG) Euler (w/ CFG) 250 1.5 (t=0.90) 1.5 (t=0.90) The MMDiT 384 384 and its FAE encoder are only used for generating high quality examples provided in the paper. The three-partitioned CFG are only used for generating Main Results. The main results uses timesteps shift=0.4. All ablation experiments use single cfg scale for = 0.7 0.0, and the cfg scale is grid searched in 0.1 fineness. The ablation experiments for FAE Model Structure are using SiT. The linear probing results are got from separate encoder trained with DinoV2 register version with same parameters. And the ablation experiments for Token Dimension and Time Shift are using LightningDiT."
        },
        {
            "title": "E Patch Embedding Similarity Maps",
            "content": "We compare the similarity between different patches inside single images. For each triplet of visualizations, the first image shows the similarity map computed from the DINOv2 embeddings, while the second shows the corresponding similarity map derived from our FAE latents. The third image displays the original image patch used as the query. The selected query patches are highlighted with red rectangle, and darker colors in the similarity maps indicate higher similarity values. Figure 8 Similarity of photo of cat. 4 Figure 9 Similarity of photo of impala."
        },
        {
            "title": "F Matching most Similar patch pair across two images",
            "content": "Our latents retain the cross-image patchmatching behavior characteristic of DINOv2: semantically corresponding regions across different images are still reliably matched using cosine similarity in the latent space. This suggests that FAE preserves fine-grained, part-level semantics rather than only coarse global information. We first identify animal-related patches using K-Means clustering. From these, we randomly select patches in the first image and match each one to the patch in the second image with the highest cosine similarity. For each example, 16 patch pairs are selected and visualized. Figure 10 Matching most similar patch pair from two photo of bird. Figure 11 Matching most similar patch pair from two photo of elephant. 6 Text-to-Image Prompts The prompts for the text to image examples in Figure 5 are: \"a wooden bench under large oak tree with warm sunlight streaming through branches and fallen leaves scattered below\", \"a panoramic mountain ridge under soft morning clouds\", \"a wooden arrow sign reading north trail pointing into the woods\", \"an alpine lake surrounded by steep cliffs, water perfectly still except for faint circular ripples, floating pollen creating tiny shimmering patterns on the surface\", \"a window sticker reading welcome\", \"a snow leopard walking across snowy slope, faint pawprints trailing behind\", \"a winter woodland with heavy snow draped asymmetrically across branches, faint animal tracks weaving between tree shadows under pale blue light\", \"a sticky note attached to monitor reading finish draft by 5 pm\", \"a small shop window sign reading local goods\", \"a tortoise lumbering through sunlit shrubs, shell etched with age patterns\", \"a wooden produce crate stamped farm fresh positioned in sunlit garden shed beside tools\", \"a rocky mountain meadow scattered with boulders and wildflowers under bright daylight\""
        },
        {
            "title": "H STARFlow Examples",
            "content": "Figure 12 Random samples of ImageNet 256x256 generated by STarFlow model. 8 Extra Examples from Siglip2 MMDiT 384 384 Model Figure 13 Random samples of Siglip2 MMDiT 384 384 Model. 9 The prompts for the text to image examples are: \"a foggy bridge spans calm river reflecting muted city lights.\", \"a forest bridge plank etched: cross slow. water murmurs beneath the boards.\", \"a hiking lodge interior has carved plaque: rest; the mountains will wait. snow drifts past the windows.\", \"a small café interior glows softly as candles flicker on wooden tables.\", \"a quiet backyard garden with warm sun patterns filtering through leaves.\", \"a hillside dotted with tiny cottages beneath lavender evening sky.\", \"a bus stop poster saying: keep going. golden morning light warms the street.\", \"a seaside cabin porch sign saying: breathe deeply. waves pulse against the rocks below.\", \"a lantern-lit alleyway glowing softly between old brick walls.\", \"a quiet library alcove features framed message: seek answers, but also seek the calm between them. warm lamplight glows against tall wooden shelves.\", \"a kitchen window frames sun-washed herbs, bowls, and warm shelves.\", \"an orchard bench plaque reading: savor sweet. blossoms float in warm breezes.\", \"a windy hillside covered in tall dry grass, each stalk catching light differently, distant sheep forming small irregular white clusters\", \"a river winds beside sleepy village, reflecting pale morning skies and drifting willow branches.\", \"a stormy coastline where winds whip through rugged rocks and dark water.\", \"a quiet european street lined with stone buildings glows under early dawn light as café chairs sit empty on cobblestones.\", \"a quiet university quad filled with shaded benches and tree-lined paths.\", \"a calm river flows beside small town, reflecting the pale sky while fishermen prepare their nets and willow branches trail across the waters surface.\", \"a serene lake mirrors the surrounding pines and layered mountain ridges during early dawn.\", \"a remote mountain lake reflects drifting clouds and jagged peaks.\", Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions."
        }
    ],
    "affiliations": [
        "Apple"
    ]
}