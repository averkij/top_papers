{
    "paper_title": "DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps",
    "authors": [
        "Hongjie Zhu",
        "Zeyu Zhang",
        "Guansong Pang",
        "Xu Wang",
        "Shimin Wen",
        "Yu Bai",
        "Daji Ergu",
        "Ying Cai",
        "Yang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, a novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose a hybrid-feature alignment module in DOEI that combines RGB values, embedding-guided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers state-of-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchmarks, including PASCAL VOC (+3.6%, +1.5%, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github.com/AIGeeksGroup/DOEI."
        },
        {
            "title": "Start",
            "content": "DOEI: Dual Optimization of Embedding Information for Attention-Enhanced Class Activation Maps Hongjie Zhu1, Zeyu Zhang2, Guansong Pang3, Xu Wang4, Shimin Wen1, Yu Bai1, Daji Ergu1, Ying Cai1, Yang Zhao5 1Southwest Minzu University 2The Australian National University 3Singapore Management University 4Sichuan University 5La Trobe University 5 2 0 2 1 2 ] . [ 1 5 8 8 5 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Weakly supervised semantic segmentation (WSSS) typically utilizes limited semantic annotations to obtain initial Class Activation Maps (CAMs). However, due to the inadequate coupling between class activation responses and semantic information in high-dimensional space, the CAM is prone to object co-occurrence or under-activation, resulting in inferior recognition accuracy. To tackle this issue, we propose DOEI, Dual Optimization of Embedding Information, novel approach that reconstructs embedding representations through semantic-aware attention weight matrices to optimize the expression capability of embedding information. Specifically, DOEI amplifies tokens with high confidence and suppresses those with low confidence during the class-to-patch interaction. This alignment of activation responses with semantic information strengthens the propagation and decoupling of target features, enabling the generated embeddings to more accurately represent target features in high-level semantic space. In addition, we propose hybrid-feature alignment module in DOEI that combines RGB values, embeddingguided features, and self-attention weights to increase the reliability of candidate tokens. Comprehensive experiments show that DOEI is an effective plug-and-play module that empowers stateof-the-art visual transformer-based WSSS models to significantly improve the quality of CAMs and segmentation performance on popular benchincluding PASCAl VOC (+3.6%, +1.5%, marks, +1.2% mIoU) and MS COCO (+1.2%, +1.6% mIoU). Code will be available at https://github. com/AIGeeksGroup/DOEI."
        },
        {
            "title": "1 Introduction\nSemantic segmentation [Ge et al., 2024; Zhang et al., 2025;\nWu et al., 2023; Zhang et al., 2023] aims to assign each\npixel in an image a specific category label [Li et al., 2022b;\nZhang et al., 2024b; Tan et al., 2024b; Tan et al., 2024a].",
            "content": "Corresponding author (caiying@swun.edu.cn). Project lead. Figure 1: Visualization comparison between the baseline (MCTformer [Xu et al., 2022]) and our method. The baseline model results exhibit background noise (object co-occurrence) and less precise localization (under-activation), as highlighted by red arrows and white bounding boxes. In contrast, our proposed method effectively reduces background activation and enhances focus on target regions, as highlighted by yellow arrows. Traditional methods often rely on extensive and precise pixellevel annotations to promote network performance. However, obtaining such annotations is notoriously time-consuming and resource-intensive [Cheng et al., 2023]. Consequently, researchers have increasingly adopted alternative forms of weak supervision, such as scribbles [Vernaza and Chandraker, 2017], bounding boxes [Lee et al., 2021b], point annotations [Bearman et al., 2016], and image-level labels [Lee et al., 2021a], to achieve pixel-level segmentation. This paper explores techniques based on image-level labels, which are particularly advantageous due to their ease of collection from internet sources and minimal annotation costs. Current image-level WSSS techniques commonly include the following steps: (1) generating CAMs [Zhou et al., 2016; Wang et al., 2020b; Xu et al., 2022] for specific categories leveraging classification network to locate objects roughly; (2) refining them into pseudo-mask annotations [Ahn and into the embedding process to strengthen the influence of embeddings with reliable semantic information during information interaction. This enables class-specific feature learning at each layer. Additionally, by mitigating the occurrence of false positives through reducing the influence of unreliable embeddings, the final embedding retains the maximal useful information about the target object, thereby amplifying the models discriminative capability in generating CAMs. Furthermore, to improve the accuracy of feature representations in the multi-dimensional space during optimization, we propose hybrid-feature alignment module that integrates RGB information of the original image with the embeddings cosine similarity features. The incorporation of this module aims to address the limitations of embedding representation capability in low-dimensional space. By employing this strategy, we further refine the models comprehension and representation of image semantics, leading to higher-quality object localization maps. Our main contributions are summarized as follows: We propose novel mechanism, namely Dual Optimization of Embedding Information (DOEI), which is plugand-play and can be applied to each layer encoder of the ViT. This mechanism effectively boosts useful information in the embedding while suppressing irrelevant information, achieving rich representation of image features and intra-class diversity, thereby improving the accuracy of CAMs and reducing activation noise. We introduce novel feature alignment module as complementary optimization to DOEI. This module integrates the RGB values of the original image, the spatial features of embeddings, and self-attention scores, making the candidate tokens more meaningful, thereby facilitating the accurate representation of semantic structures and the effective transmission of embedding information. We incorporate the proposed mechanism into ViT-based models, with experiments demonstrating that this mechanism advances baseline model performance across different datasets without adding additional learnable parameters. It effectively prevents incorrect target localization and significantly improves the integrity of target recognition."
        },
        {
            "title": "2.1 Weakly Supervised Semantic Segmentation",
            "content": "In WSSS, full pixel-level segmentation relies on limited supervisory signals. The development of WSSS has significantly alleviated the dependency on large amounts of pixellevel labels in traditional semantic segmentation models. Prevailing approaches mainly use the CAM technique introduced by Zhou et al. [2016] as an initial step in identifying target object locations. The CAM combines the global average pooling layer with the classification layer to efficiently consolidate feature information for each pixel, generating an activation map that is aligned with the semantic representation of specific category and serving as key component of Figure 2: (a) The image and query targets (). (b) The self-attention maps in the Transformer block capture semantic-level relationships at various granularities. The high-activation regions learned by each layer not only provide critical information that subsequent layers may miss but also generate CAMs that often focus on different regions. This broadens the coverage of target feature areas, effectively mitigating the tendency of activation maps to focus excessively on local salient regions. Kwak, 2018]; (3) applying these pseudo-mask annotations and the original images to train semantic segmentation network. Acquiring high-quality CAMs is vital for the subsequent processes [Cheng et al., 2023]. However, existing WSSS methods often suffer from inaccurate CAMs, such as the mistaken activation of non-target objects (object co-occurrence) and incomplete activation of target objects (under-activation), due to the limited semantic depth of image-level labels. These challenges hinder accurate localization and segmentation, especially in multi-target scenes. As shown in Figure 1, this phenomenon occurs primarily due to the failure to fully capture complex interactions between deep structures and features while establishing strong relationship between activation responses and image semantics. Prior knowledge of an objects category can offer insights into its holistic features when interacting with the original image. Although this information may appear as simple words or numbers in low-dimensional space, it can be articulated in more comprehensive and intricate manner in highdimensional space. Existing works predominantly emphasize extracting feature information for classification from input images, neglecting the critical role of high-dimensional semantic space in generating accurate CAMs [Wang et al., 2020b]. This oversight typically results in CAMs that either cover only portion of objects distinctive features or erroneously include non-target objects. As application scenarios for WSSS tasks grow more complex, limitations in the models ability to fully recognize contours and accurately locate target objects have become increasingly apparent. Our observations reveal that, in WSSS tasks, the cascaded encoders in ViT [Dosovitskiy et al., 2021]-based classification or representation learning models facilitate hierarchical and longrange information modeling for class-specific activations, as illustrated in Figure 2. Inspired by this, we incorporate feedback from attention mechanisms across cascaded encoders Figure 3: An overview of the proposed method. The RGB image undergoes transformation into class tokens and patch tokens via the embedding layer. The dot-product attention mechanism is then employed to compute the attention score matrix and generate output tokens. To further refine the attention score matrix, the cosine similarity between the RGB values of the original image and the tokens is used to adjust the distribution of attention weights. Furthermore, the CPDO and PPDO methods are customized to amplify high-confidence information and suppress the influence of low-confidence information. Finally, the optimized tokens are incorporated into the original tokens as residuals, producing refined output tokens for subsequent computations in the encoder. WSSS. However, CAMs typically display only the salient regions of the target object, hindering the capture of complete location information. Therefore, directly adopting CAM as full pixel-level segmentation labels is constrained by these shortcomings. Most of the research focuses on generating more precise and comprehensive CAMs and refining pseudolabels to augment the fidelity and precision of segmentation results. By optimizing the CAM generation process and incorporating richer semantic information, researchers aim to mitigate issues of insufficient or overly concentrated activation areas, paving the way for more accurate and versatile WSSS methodologies."
        },
        {
            "title": "2.2 Generating High-quality CAMs",
            "content": "Convolutional Neural Networks (CNNs) [Qi et al., 2025] and Vision Transformers (ViTs) [Wu et al., 2024; Ji et al., 2024; Zhang et al., 2024a] are two popular approaches for generating CAMs. CNN-based methods often face challenges with localized activation due to the limited receptive field and reliance on local features. To address this, strategies such as random occlusion [Kumar Singh and Jae Lee, 2017] (e.g., hide-and-seek) and adversarial training [Kweon et al., 2023] have been employed to expand activation areas. Techniques like dilated convolutions [Huang et al., 2018] and multi-layer feature integration [Li et al., 2022a] further elevate segmentation accuracy by capturing more contextual information. Additionally, specialized loss functions, such as contrastive loss [Zhu et al., 2024] and SEC loss [Wu et al., 2022], and supplementary data, including saliency maps and videos [Wang et al., 2018], have been used to improve object localization.An alternative method for generating CAMs is to utilize the self-attention weight matrix in ViT [Dosovitskiy et al., 2021]. For example, Gao et al. [2021] combine semantic-aware annotations with semantically unrelated attention maps, providing feasible approach for object localization by utilizing the semantic and localization information extracted by ViT. Ru et al. [2022] refine the initial pseudolabels for segmentation by learning robust semantic affinities with the aid of multi-head attention mechanism. Xu et al. [2023] transform simple class labels into high-dimensional semantic information through Contrastive Language-Image Pretraining (CLIP) to guide the ViT, forming multimodal representation of text and images, thus generating more precise object localization maps."
        },
        {
            "title": "2.3 Refinement of CAMs",
            "content": "At present, existing refinement methods primarily focus on the second stage of multi-stage models. For instance, Ahn et al. [2018] train AffinityNet by leveraging reliable foreground and background activation maps to predict affinities between neighboring pixels, which are then used as metrics for the random walk algorithm, thereby expanding the CAM. The IRN method [Ahn et al., 2019] further refines CAM by estimating object boundary information through the semantic affinities between the original pixels in the image. Wang et al. [2020a] propose method that refines CAM by leveraging high-confidence pixels from segmentation results as inputs to pairwise affinity network. Xu et al. [2021] highlight that affinities in saliency maps and segmented representations more effectively reflect the diversity in CAM representations. For WSSS tasks, such refinement is crucial for improving the final segmentation performance and helps generate more accurate and reliable pixel-level pseudo-labels."
        },
        {
            "title": "3 Methodology\n3.1 Preliminaries\nFor ViT-based multi-classification networks, an input image\nis first split into N × N patches through a convolutional layer\nor a fully connected layer, which are then converted into N 2\npatch tokens (cid:8)tn ∈ R1×D, n = 1, 2, . . . , N 2(cid:9), where D rep-\nresents the embedding dimension of each token. Inspired by\nMCTformer [Xu et al., 2022], among these patch tokens, C\nclass tokens t∗ ∈ RC×D is concatenated, where C repre-\nsents the number of classes, and t∗ is a learnable parameter\ninitialized randomly. After adding positional encoding, these\ntokens Γ ∈ R(N 2+C)×D are fed into L cascaded encoders\nof the standard Transformer modules, where Γ represents the\ncombination of tn and t∗. Specifically, tokens Γ are projected\nby employing three learnable parameter matrices W Q, W K,\nW V , which map them into query Q ∈ R(N 2+C)×dq , key\nK ∈ R(N 2+C)×dk , and value V ∈ R(N 2+C)×dv . Here, dq\n= dk, denotes the feature dimension for the query and key,\nwhile dv indicates the feature dimension of the value. Sub-\nsequently, the scaled dot-product attention mechanism is ap-\nplied to compute the attention matrix A between the query\nand key:",
            "content": "Si = ΓiW , {Q, K, }, and Ai = QiK dk (1) where {1, 2, . . . , L} denotes the i-th transformer block. The softmax function is applied to matrix Ai, which is then multiplied by the key Vi, after which residual connection is added, followed by Layer Normalization and input into the MLP to produce output Xi. Xi = MLP(LN(softmax(Ai)Vi + Xi1)) (2) where Xi denotes the output of the i-th transformer block. When = L, class tokens Γcls RCD are extracted from Xi and averaged across channels to yield category scores ycls RC. Subsequently, these category scores are matched with image-level ground truth labels via multi-label soft margin loss (M LSM ), and the loss is calculated to enable supervised learning: Lcls = MLSM(ycls, y) = 1 (cid:88) i=1 yi log σ(yi cls) (3) + (1 yi) log(1 σ(yi cls))"
        },
        {
            "title": "3.2 Overall Pipeline\nInspired by the modeling of long-range dependencies at dif-\nferent levels by the encoder layers in the standard ViT, we ex-\nplore whether multi-scale inter-object coupling interactions\ncan be utilized to optimize input embeddings during forward\npropagation, thereby extending the semantic diversity of an-\nchor class activation features in the high-level semantic space.",
            "content": "The overall architecture of DOEI is shown in Figure 3. We employ TS-CAM [Gao et al., 2021], TransCAM [Li et al., 2023], and MCTformer [Xu et al., 2022] as baseline models to assess the effectiveness of the proposed method. In each baseline network, we adopt standard multi-stage WSSS process: 1) Generating Class Activation Maps (CAMs); 2) Refining the CAMs; 3) Training the segmentation network. Generating CAMs. We generate CAMs by training multi-class classifier on the baseline network. Specifically, the feature map output Rhwd by the baseline network is weighted and summed with the weight matrix of the classifiers final layer to extract the corresponding classspecific feature map c, followed by normalization. Finally, the CAM is generated by calculating the contrast threshold β (0 < β < 1) relative to the background."
        },
        {
            "title": "M c",
            "content": "tmp = ReLu (cid:33) i,cF (cid:32) (cid:88) i= i,j = (cid:40) arg max(M i,j,:), 0, if max(M i,j,: if max(M i,j,: tmp) β, tmp) β (4) (5) where ReLu is nonlinear activation function used to filter out negative values in tmp. Min-Max normalization is applied to scale tmp to [0, 1]. The pixel (i, j) represents specific location in the feature map. Refining CAMs. We employed the same method as the baseline network, utilizing the AffinityNet [Ahn and Kwak, 2018] introduced by Ahn et al. to refine the initial seeds. AffinityNet learns reliable affinities between pixels from the initial localization map as supervisory signal and predicts an affinity matrix to enable stable expansion of seed regions, thereby generating pseudo-mask labels. Training Segmentation Network. Building on previous mainstream research [Zhang et al., 2021] and the baseline network configuration, we selected DeepLabv1 [Chen, 2014] with ResNet38 [Wu et al., 2019] backbone as the segmentation network. The segmentation network uses pseudo-mask labels as supervisory signals to perform regression prediction for each pixel. The loss function is given as: Lseg = 1 N (cid:88) (cid:88) i=1 j=1 ˆyj log (cid:32) (cid:33) exp(zj ) k=1 exp(zk ) (cid:80)C (6) where represents the total number of pixels in the image; denotes the index of each pixel, ranging from 1 to ;and represents the class index, ranging from 1 to C. ˆyj denotes the ground truth label for pixel i, otherwise 0 (i.e., one-hot encoding).zj denotes the logit value of the network output for pixel belonging to class j."
        },
        {
            "title": "3.3 Dual Optimization of Embedding Information\nGiven the importance of high-quality CAMs for WSSS, this\npaper focuses on CAMs generation. Previous research has\nmainly focused on extracting key classification features from\nthe image, frequently overlooking the importance of guid-\ning information generated through the interaction between",
            "content": "semantic content and image features. Therefore, we propose dual optimization mechanism for embedding information (DOEI) in ViT to fully utilize the interaction between semantic content and image features. The proposed DOEI consists of two independent modules: Class-wise Progressive Decoupling Optimization (PPDO) and Patch-wise Progressive Decoupling Optimization (CPDO), which are used to optimize patch-to-class and class-to-class interactions, respectively. p2c, where Si,x Patch-wise Progressive Decoupling Optimization. Traditional methods typically generate weight matrix Ai through the dot-product self-attention mechanism of class tokens and patch tokens in the i-th layer, and multiply it by the key generated from the embedding of the input in the (i 1)- th layer to obtain the input embedding for the (i + 1)-th layer. p2c = Ai[1 : C, + 1 : + 2] The Si RN 2C represents the attention score of patch-to-class, and p2c = Ai[C + 1 : + 2, 1 : + 1] RN 2C represents Si,y the attention score of class-to-patch, with and used here to denote the two different forms of Si p2c. These scores reflects the degree of relevance of the image information in the patch to that class. In other words, the Sp2c indicates the most likely class affiliation for that patch. Unreliable Sp2c (e.g., when patch lacks relevant positional information for the class) can result in the creation of numerous false-positive pixels in the CAM during subsequent embedding transmission, causing incorrect class activation. Furthermore, assigning greater weight to high-confidence Sp2c can amplify their influence in subsequent embeddings, thereby guiding the model to focus more on specific categories. We partitioned all patch-to-class attention scores Sp2c into Patch Candidate Confidence Score (P Ccs) and Patch Candidate Non-confidence Scores (P Cns): = Sp2c = Sx p2c + (Sy p2c)T (7)"
        },
        {
            "title": "X p",
            "content": "i = Xi[m :, :] + Xi[n : m, :] Ai[P Ccs] AFp2c + Xi[m :, :] Ai[P Cns] SFp2c (11) where the Augment Factor (AFp2c) and Suppression Factor (SFp2c) are controllable hyper-parameters. Class-wise Progressive Decoupling Optimization. Class-to-class attention scores capture the similarity among In multi-object, complex scenes, where the categories. model relies solely on image-level annotations, it often struggles to accurately capture target location information, making it difficult to align these locations with semantic data. To address this challenge, CPDO is introduced to establish complementary similarities between categories. Specifically, the location information of one category can be supplemented by the semantic information of other similar categories, thereby enhancing the models ability to perceive target locations and reinforcing the correspondence between semantic content and spatial positions. Similarity, we partitioned all class-to-class attention scores Sc2c into Class Candidate Confidence Score CCcs and Class Candidate Non-confidence Score CCus:Similarly, we divide all class-to-class attention scores Sc2c into candidate confidence scores CCcs and non-confidence scores CCus,then apply enhancement and suppression operations to obtain the optimized embedding vectors : = Sc2c = Sx c2c + (Sy c2c)T (12) Ccs = (cid:91) i= {ai,j TopIndices (Ci,j, t)} (13) Ccs = (cid:91) i=1 {ai,j TopIndices (Pi,j, t)} (8) Cns = (cid:91) i=1 {bi,j BottomIndices(Ci,j, 1 t)} (14) = (L i) STc2c (15) Cns = (cid:91) i=1 {bi,j BottomIndices(Pi,j, 1 t)} (9) denote and as the dimensions of P, where indicates the number of classes and indicates the number of patches. is calculated by the following formula: = (L i) STp2c (10) where {1, 2, . . . , L} represents the l-th transformer block and represents the total number of transformer layers. The selective threhold (STp2c) represents hyperparameter used to control the division threshold. We consider that each layer of the self-attention mechanism learns image information at different scales. Therefore, we employed progressive strategy that selects different numbers of confidence and nonconfidence scores based on the features of each layer. We apply enhancement and suppression operations to Ccs and Cns, respectively, calculate the optimized embedding vectors, and combine them with the original embedding vectors with residual connections: = Xi[n : m, :] + Xi[n : m, :] Ai[CCcs] AFc2c + Xi[n : m, :] Ai[CCns] SFc2c (16) Finally, and are concatenated to form new embedding vector Xi+1, which is then fed as input to the next layer of the self-attention mechanism."
        },
        {
            "title": "3.4 Hybrid Feature Alignment Module\nThe self-attention scores, which fully represent image fea-\nture information, provide a reliable foundation for embed-\nding optimization in DOEI. Relying solely on the attention\nweight matrix generated by the ViT encoder as a source of\npotential information for embedding optimization is insuffi-\ncient to fully capture the high-dimensional features of objects\nanchored in the image. Therefore, to comprehensively cap-\nture the latent semantic and structural features within the im-\nage, the process of selecting feature information needs to be\nre-evaluated to encompass a broader range of rich content.",
            "content": "This paper introduces novel feature information construction method that effectively extracts key information from images by integrating self-attention weights, raw RGB values, and features from the ViT patch embedding layer. The specific construction method is as follows: = (1 α) Wattn + α drgb(x) demb(X) (17) here, Wattn represents the self-attention weight matrix generated by the ViT model. Denote drgb(x) and demb(X) as the normalized pure RGB features of the original image and the cosine similarity based on Transformer embedding features, respectively."
        },
        {
            "title": "4.1 Datasets and Evaluation Protocol",
            "content": "The datasets employed in the experiments PASCAL VOC 2012 [Everingham et al., 2010] and MS COCO 2014 [Lin et al., 2014] along with detailed description of the evaluation metrics, are provided in Supplementary Section 1. 4."
        },
        {
            "title": "Implementation Details",
            "content": "We integrate the proposed method in three baseline networks (i.e., MCTformer [Xu et al., 2022], TS-CAM [Gao et al., 2021], TransCAM [Li et al., 2023]). All classification networks for generating CAMs use the parameter settings of the original baseline. Due to differences in network architecture, the Patch-wise Progressive Decoupling Optimization (PPDO) module is only applied in TransCAM and TSCAM. We draw on previous work [Xu et al., 2022] and apply PSA [Ahn and Kwak, 2018] to the generated initial CAMs to obtain pseudo-mask labels (Mask). The final semantic segmentation network uses DeepLabV1 [Chen, 2014] based on ResNet38 [Wu et al., 2019], with parameter settings consistent with the compared baseline methods. In testing, we evaluate the model performance with inputs at multiple scales (0.5, 0.75, 1.0, 1.25, 1.5) and apply DenseCRFs [Chen, 2014] post-processing to the output results."
        },
        {
            "title": "4.3 Main Results",
            "content": "Pascal VOC. In Table 1, we demonstrate the improvement in seed object localization maps after incorporating the DOEI mechanism into baseline methods, and compare these results with those of classical WSSS methods. As shown in Table 1, the baseline models with DOEI achieved improvements of 6.8%, 1.4%, and 3.8% on seed, and enhancements of 4.7%, 1.1%, and 3.7% on pseudo-mask labels (mask) after PSA processing. Additionally, Table 2 presents the performance of segmentation models trained with pseudo-labels generated by the baseline models incorporating DOEI on the Pascal VOC validation and test sets. The results show that the proposed mechanism significantly improved the baseline models, with performance increases of 4.1%, 0.9%, and 0.9%, respectively. Method AdvCAM [2021a] CDA [2021] CPN [2021] ReCAM [2022b] CLIMS [2022] FPR [2023] SFC [2024] Seed 55.6 55.4 57.4 54.8 56.6 60.3 64.7 Mask 69.9 63.4 67.8 69.7 70.5 - 73.7 TS-CAM [2021] DOEI+TS-CAM TransCAM [2023] DOEI+TransCAM MCTformer [2022] DOEI+MCTformer 29.9 36.7 6.8 64.9 66.3 1.4 61.7 65.5 3.8 41.4 45.5 4.1 70.2 71.3 1.1 69.1 72.2 3.7 Table 1: Evaluation of the initial seed (Seed) and its corresponding pseudo segmentation ground-truth mask (Mask) is conducted through mIoU (%) on the PASCAL VOC train set. denotes our reproduced result. Method SEAM [2020b] AuxSegNet [2021] EPS [2021c] ECS-Net [2021] AdvCAM [2021a] CDA [2021] Spatial-BCE [2022] ReCAM [2022b] SIPE [2022a] CLIMS [2022] LPCAM [2023] CLIP-ES [2023] TS-CAM [2021] DOEI+TS-CAM TransCAM [2023] DOEI+TransCAM MCTformer [2022] DOEI+MCTformer Backbone ResNet101 ResNet38 ResNet101 ResNet38 ResNet101 ResNet38 ResNet38 ResNet101 ResNet101 ResNet101 ResNet101 ResNet101 ResNet38 ResNet38 ResNet38 ResNet38 ResNet38 ResNet38 Val 64.5 69.0 71.0 64.3 68.1 66.1 70.0 69.5 68.8 70.4 71.8 73.8 Test 65.7 68.6 71.8 65.3 68.0 66.8 71.3 69.6 69.7 70.0 72.1 73.9 43.2 46.8 3.6 69.3 70.8 1.5 70.2 71.4 1.2 42.9 47.0 4.1 69.6 70.5 0.9 70.1 71.0 0.9 Table 2: Performance comparison of WSSS methods in terms of mIoU (%) on the PASCAL VOC val and test split through different segmentation backbones. : Our reimplemented results by means of official code. Note that TS-CAM doesnt provide official evaluation results on PASCAL VOC dataset, the results of TS-CAM () are derived by us. MS COCO. Table 3 reports the results of our DOEI compared to previous methods on MS COCO val split. MS COCO is complex dataset containing 80 classification categories, covering wide range of objects from everyday life. Our method achieved performance improvements of 1.2% and 1.6% over the baseline model, respectively."
        },
        {
            "title": "4.4 Ablation Study and Parameter Analysis",
            "content": "We perform ablation and parameter experiments on the PASCAL VOC train set to assess the effectiveness of the proposed method. All experiments are conducted via MCTformer [Xu et al., 2022], with DeiT-S serving as the backbone network. Method CONTA [2020] AuxSegNet [2021] CDA [2021] EPS [2021c] ReCAM [2022b] SIPE [2022a] TransCAM [2023] DOEI+TransCAM MCTformer [2022] DOEI+MCTformer Backbone ResNet38 ResNet38 ResNet38 ResNet101 ResNet38 ResNet101 ResNet38 ResNet38 ResNet38 ResNet38 Val 32.8 33.9 33.2 35.7 42.9 43.6 43.5 44.7 1.2 42.0 43.6 1.6 Table 3: Performance comparison of WSSS methods in terms of mIoU (%) on the MS COCO val set. Note that TransCAM doesnt provide official evaluation results on MS COCO dataset, the results of TransCAM () are implemented by us. The Effectiveness of the Components. Table 4 presents the performance improvements contributed by each component of the Dual Optimization of Embedding Information (DOEI) mechanism and the Hybrid-Feature Alignment Introducing either Patch-wise Progressive (HFA) module. Decoupling Optimization (PPDO) or Class-wise Progressive Decoupling Optimization (CPDO) individually increases the models seed accuracy from 61.7% to 64.2% and 64.3%, respectively. Combining PPDO and CPDO to form the DOEI mechanism yields significant performance improvement, achieving an mIoU of 65.1%. Further integrating DOEI with HFA results in an even greater performance boost, with an mIoU of 65.5%. These results highlight the DOEI methods ability to enhance the global representation of target objects while effectively suppressing interference from non-target objects. Additionally, the findings confirm the effectiveness of the HFA module in addressing the limitations of relying solely on attention scores as features, thereby enhancing overall model performance. Baseline CPDO PPDO HFA mIoU 61.7 63.3 63.2 64.3 64.6 64.9 65.5 Table 4: Performance improvements from different optimization mechanisms in the mIoU (%) evaluation metric on the PASCAL VOC train set. Hyper-parameters in PPDO and CPDO. Figure 4 shows the results of analyzing each of the following parameters individually, while keeping the other hyper-parameters fixed: selection threshold, amplification factor, and suppress-factor for both patch-to-class and class-to-class (AFp2c, SFp2c and STc2c, AFc2c, STp2c). These hyper-parameters are used to generate the optimal CAM configuration. The experimental results show that when these parameter values fall within certain range, the mIoU achieves better values. As shown in Figure 4(a) and (c), by amplifying the patch information most likely to belong to specific class (as learned by the model), the model improves its attention to that class, expanding the activation range; conversely, by suppressing the information least likely to belong to that class, the model reduces its focus on irrelevant regions, effectively preventing false activations. Additionally, class-to-class attention scores reflect the similarity of different semantic features. By leveraging the commonalities of similar semantics within the image, the model can selectively activate complementary semantic information, even in the absence of explicit class guidance. When an appropriately selected value for STc2c, AFc2c and SFc2c are used, the models embedding information can be optimized toward the correct semantic direction, thereby improving its performance and accuracy, as shown in Figure 4(b) and (d). Additional Parameter Analysis. To verify the effectiveness of the proposed method, we also conduct experiments to analyze the parameters of the Hybrid Feature Alignment weight α and the selection of embedding optimization layers. When the α value falls within specific range, the mIoU value significantly improves. Furthermore, as the number of optimization layers increases, the mIoU value reaches its maximum when the layer count is at its maximum, achieving optimal performance. The detailed experimental results are provided in Supplementary Section 2. Figure 4: The impact of different hyper-parameter values in the CPDO and PPDO modules on mIoU."
        },
        {
            "title": "4.5 Qualitative Analysis",
            "content": "We provide qualitative comparison results of the baseline network (i.e., TS-CAM, TransCAM and MCTformer) with DOEI on representative examples from the PASCAL VOC and MS COCO datasets in Supplementary Figure 2. These comparisons provide clear visual demonstration of the substantial improvements achieved by our method, particularly in enhancing the target localization accuracy of the baseline models. By refining embedded information, DOEI surpasses baseline approaches, demonstrating its ability to deliver more precise localization across diverse, challenging datasets."
        },
        {
            "title": "5 Conclusion\nIn this paper, we investigate the optimization of input em-\nbeddings by coupling class tokens and patch tokens across\nmultiple scales of self-attention, thus enriching the diversity\nof anchored class activation features. We propose a Dual\nOptimization of Embedding Information mechanism (DOEI)\nthat integrates seamlessly with the ViT. DOEI leverages cou-\npled attention within the self-attention mechanism to am-\nplify the semantic information of specific categories and sup-\npress noise during forward propagation, effectively recon-\nstructing the semantic representation of embeddings. Addi-\ntionally, we construct hybrid feature representations by com-\nbining the RGB values of the image, embedding features, and\nself-attention scores, thereby enhancing the reliability of can-\ndidate tokens within the DOEI mechanism. Experimental re-\nsults show that the baseline models with DOEI successfully\nalleviate over-activation and under-activation issues on the\nPascal VOC and MS COCO datasets, greatly improving the\nquality of class activation maps and boosting semantic seg-\nmentation performance.",
            "content": "Ethical Statement There are no ethical issues. References [Ahn and Kwak, 2018] Jiwoon Ahn and Suha Kwak. Learning pixel-level semantic affinity with image-level superIn vision for weakly supervised semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49814990, 2018. [Ahn et al., 2019] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly supervised learning of instance segmenIn Proceedings of the tation with inter-pixel relations. IEEE/CVF conference on computer vision and pattern recognition, pages 22092218, 2019. [Bearman et al., 2016] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. Whats the point: Semantic segmentation with point supervision. In European conference on computer vision, pages 549565. Springer, 2016. [Caesar et al., 2018] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 12091218, 2018. [Chen and Sun, 2023] Zhaozheng Chen and Qianru Sun. Extracting class activation maps from non-discriminative features as well. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3135 3144, 2023. [Chen et al., 2022a] Qi Chen, Lingxiao Yang, Jian-Huang Lai, and Xiaohua Xie. Self-supervised image-specific prototype exploration for weakly supervised semantic segIn Proceedings of the IEEE/CVF conference mentation. on computer vision and pattern recognition, pages 4288 4298, 2022. [Chen et al., 2022b] Zhaozheng Chen, Tan Wang, Xiongwei Wu, Xian-Sheng Hua, Hanwang Zhang, and Qianru Sun. Class re-activation maps for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 969 978, 2022. [Chen et al., 2023] Liyi Chen, Chenyang Lei, Ruihuang Li, Shuai Li, Zhaoxiang Zhang, and Lei Zhang. Fpr: False positive rectification for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11081118, 2023. [Chen, 2014] Liang-Chieh Chen. Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062, 2014. [Cheng et al., 2023] Zesen Cheng, Pengchong Qiao, Kehan Li, Siheng Li, Pengxu Wei, Xiangyang Ji, Li Yuan, Chang Liu, and Jie Chen. Out-of-candidate rectification for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2367323684, 2023. [Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. [Everingham et al., 2010] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303338, 2010. [Gao et al., 2021] Wei Gao, Fang Wan, Xingjia Pan, Zhiliang Peng, Qi Tian, Zhenjun Han, Bolei Zhou, and Qixiang Ye. Ts-cam: Token semantic coupled attention map for weakly supervised object localization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 28862895, 2021. [Ge et al., 2024] Jinchao Ge, Zeyu Zhang, Minh Hieu Phan, Bowen Zhang, Akide Liu, and Yang Zhao. Esa: Annotation-efficient active learning for semantic segmentation. arXiv preprint arXiv:2408.13491, 2024. [Huang et al., 2018] Zilong Huang, Xinggang Wang, Jiasi Wang, Wenyu Liu, and Jingdong Wang. Weaklysupervised semantic segmentation network with deep seeded region growing. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 70147023, 2018. [Ji et al., 2024] Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, and Simon Lucey. Sine activated low-rank matrices for parameter efficient learning. arXiv e-prints, pages arXiv2403, 2024. [Kumar Singh and Jae Lee, 2017] Krishna Kumar Singh and Yong Jae Lee. Hide-and-seek: Forcing network to be meticulous for weakly-supervised object and action localization. In Proceedings of the IEEE international conference on computer vision, pages 35243533, 2017. [Kweon et al., 2023] Hyeokjun Kweon, Sung-Hoon Yoon, and Kuk-Jin Yoon. Weakly supervised semantic segmentation via adversarial learning of classifier and reconstructor. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11329 11339, 2023. [Lee et al., 2021a] Jungbeom Lee, Eunji Kim, and Sungroh Yoon. Anti-adversarially manipulated attributions for weakly and semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 40714080, 2021. [Lee et al., 2021b] Jungbeom Lee, Jihun Yi, Chaehun Shin, and Sungroh Yoon. Bbam: Bounding box attribution map for weakly supervised semantic and instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26432652, 2021. [Lee et al., 2021c] Seungho Lee, Minhyun Lee, Jongwuk Lee, and Hyunjung Shim. Railroad is not train: Saliency as pseudo-pixel supervision for weakly supervised semanIn Proceedings of the IEEE/CVF contic segmentation. ference on computer vision and pattern recognition, pages 54955505, 2021. [Li et al., 2022a] Jinlong Li, Zequn Jie, Xu Wang, Yu Zhou, Xiaolin Wei, and Lin Ma. Weakly supervised semantic segmentation via progressive patch learning. IEEE Transactions on multimedia, 25:16861699, 2022. [Li et al., 2022b] Ruihuang Li, Shuai Li, Chenhang He, Yabin Zhang, Xu Jia, and Lei Zhang. Class-balanced pixel-level self-labeling for domain adaptive semantic segIn Proceedings of the IEEE/CVF conference mentation. on computer vision and pattern recognition, pages 11593 11603, 2022. [Li et al., 2023] Ruiwen Li, Zheda Mai, Zhibo Zhang, Jongseong Jang, and Scott Sanner. Transcam: Transformer attention-based cam refinement for weakly supervised semantic segmentation. Journal of Visual Communication and Image Representation, 92:103800, 2023. [Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. [Lin et al., 2023] Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li, Binbin Lin, Haifeng Liu, and Xiaofei He. Clip is also an efficient segmenter: text-driven approach for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1530515314, 2023. [Qi et al., 2025] Xuyin Qi, Zeyu Zhang, Huazhan Zheng, Mingxi Chen, Numan Kutaiba, Ruth Lim, Cherie Chiang, Zi En Tham, Xuan Ren, Wenxin Zhang, et al. Medconv: Convolutions beat transformers on long-tailed bone density prediction. arXiv preprint arXiv:2502.00631, 2025. [Ru et al., 2022] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: End-to-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1684616855, 2022. [Su et al., 2021] Yukun Su, Ruizhou Sun, Guosheng Lin, and Qingyao Wu. Context decoupling augmentation for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 70047014, 2021. [Sun et al., 2021] Kunyang Sun, Haoqing Shi, Zhengming Zhang, and Yongming Huang. Ecs-net: Improving weakly supervised semantic segmentation by using connections In Proceedings of the between class activation maps. IEEE/CVF international conference on computer vision, pages 72837292, 2021. [Tan et al., 2024a] Shengbo Tan, Rundong Xue, Shipeng Luo, Zeyu Zhang, Xinran Wang, Lei Zhang, Daji Ergu, Zhang Yi, Yang Zhao, and Ying Cai. Segkan: Highresolution medical image segmentation with long-distance dependencies. arXiv preprint arXiv:2412.19990, 2024. [Tan et al., 2024b] Shengbo Tan, Zeyu Zhang, Ying Cai, Daji Ergu, Lin Wu, Binbin Hu, Pengzhang Yu, and Yang Zhao. Segstitch: Multidimensional transformer for robust and efficient medical imaging segmentation. arXiv preprint arXiv:2408.00496, 2024. [Vernaza and Chandraker, 2017] Paul Vernaza and Manmohan Chandraker. Learning random-walk label propagation for weakly-supervised semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 71587166, 2017. [Wang et al., 2018] Xiang Wang, Shaodi You, Xi Li, and Huimin Ma. Weakly-supervised semantic segmentation by iteratively mining common object features. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13541362, 2018. [Wang et al., 2020a] Xiang Wang, Sifei Liu, Huimin Ma, and Ming-Hsuan Yang. Weakly-supervised semantic segmentation by iterative affinity learning. International Journal of Computer Vision, 128:17361749, 2020. [Wang et al., 2020b] Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen. Self-supervised equivariant attention mechanism for weakly supervised semantic In Proceedings of the IEEE/CVF confersegmentation. ence on computer vision and pattern recognition, pages 1227512284, 2020. [Wu et al., 2019] Zifeng Wu, Chunhua Shen, and Anton Van Den Hengel. Wider or deeper: Revisiting the resnet model for visual recognition. Pattern recognition, 90:119133, 2019. [Wu et al., 2022] Tong Wu, Guangyu Gao, Junshi Huang, Xiaolin Wei, Xiaoming Wei, and Chi Harold Liu. Adaptive spatial-bce loss for weakly supervised semantic segmentation. In European Conference on Computer Vision, pages 199216. Springer, 2022. [Zhang et al., 2024b] Zeyu Zhang, Xuyin Qi, Bowen Zhang, Biao Wu, Hien Le, Bora Jeong, Zhibin Liao, Yunxiang Liu, Johan Verjans, Minh-Son To, et al. Segreg: Segmenting oars by registering mr images and ct annotations. In 2024 IEEE International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2024. [Zhang et al., 2025] Ruicheng Zhang, Haowei Guo, Zeyu Zhang, Puxin Yan, and Shen Zhao. Gamed-snake: Gradient-aware adaptive momentum evolution deep snake arXiv preprint model for multi-organ segmentation. arXiv:2501.12844, 2025. [Zhao et al., 2024] Xinqiao Zhao, Feilong Tang, Xiaoyang Wang, and Jimin Xiao. Sfc: Shared feature calibration in weakly supervised semantic segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 75257533, 2024. [Zhou et al., 2016] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29212929, 2016. [Zhu et al., 2024] Lianghui Zhu, Xinggang Wang, Jiapei Feng, Tianheng Cheng, Yingyue Li, Bo Jiang, Dingwen Zhang, and Junwei Han. Weakclip: Adapting clip for weakly-supervised semantic segmentation. International Journal of Computer Vision, pages 121, 2024. [Wu et al., 2023] Biao Wu, Yutong Xie, Zeyu Zhang, Jinchao Ge, Kaspar Yaxley, Suzan Bahadir, Qi Wu, Yifan Liu, and Minh-Son To. Bhsd: 3d multi-class brain hemorrhage segmentation dataset. In International Workshop on Machine Learning in Medical Imaging, pages 147156. Springer, 2023. [Wu et al., 2024] Biao Wu, Yutong Xie, Zeyu Zhang, Minh Hieu Phan, Qi Chen, Ling Chen, and Qi Wu. Xlip: Cross-modal attention masked modelling for arXiv preprint medical arXiv:2407.19546, 2024. language-image pre-training. [Xie et al., 2022] Jinheng Xie, Xianxu Hou, Kai Ye, and Linlin Shen. Clims: Cross language image matching for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44834492, 2022. [Xu et al., 2021] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, Ferdous Sohel, and Dan Xu. Leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 69846993, 2021. [Xu et al., 2022] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu. Multi-class token transformer for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 43104319, 2022. [Xu et al., 2023] Lian Xu, Wanli Ouyang, Mohammed Bennamoun, Farid Boussaid, and Dan Xu. Learning multimodal class-specific tokens for weakly supervised dense In Proceedings of the IEEE/CVF object localization. Conference on Computer Vision and Pattern Recognition, pages 1959619605, 2023. [Zhang et al., 2020] Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, and Qianru Sun. Causal intervention for weakly-supervised semantic segmentation. Advances in Neural Information Processing Systems, 33:655666, 2020. [Zhang et al., 2021] Fei Zhang, Chaochen Gu, Chenyue Zhang, and Yuchao Dai. Complementary patch for weakly supervised semantic segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 72427251, 2021. [Zhang et al., 2023] Zeyu Zhang, Bowen Zhang, Abhiram Hiwase, Christen Barras, Feng Chen, Biao Wu, Adam James Wells, Daniel Ellis, Benjamin Reddi, Andrew William Burgan, Minh-Son To, Ian Reid, and Richard Hartley. Thin-thick adapter: Segmenting thin scans using thick annotations. OpenReview, 2023. [Zhang et al., 2024a] Zeyu Zhang, Xuyin Qi, Mingxi Chen, Guangxi Li, Ryan Pham, Ayub Qassim, Ella Berry, Zhibin Liao, Owen Siggs, Robert Mclaughlin, et al. Jointvit: Modeling oxygen saturation levels with joint supervision In Annual Conference on Medion long-tailed octa. cal Image Understanding and Analysis, pages 158172. Springer, 2024. DOEI: Dual Optimization of Embedding Information With Attention-Enhanced Class Activation Maps - Supplementary Materials -"
        },
        {
            "title": "1 Datasets and Evaluation protocol",
            "content": "We assess the effectiveness of the proposed method by evaluating its performance on the widely used datasets, PASCAL VOC 2012 [Everingham et al., 2010] and MS COCO 2014 [Lin et al., 2014]. PASCAL VOC is divided into training (train), validation (val), and test sets, containing 1,464, 1,449, and 1,456 images, respectively, covering 20 target categories and one background category. To maintain consistency with the general protocol followed in previous studies [Xu et al., 2022; Cheng et al., 2023], we used the augmented training set containing 10,582 images to train the classification network. MS COCO includes 80 target categories and one background category, with approximately 82,000 training images and 40,000 validation images. Following [Lee et al., 2021c], we retained only images containing target classes and extracted the ground-truth labels from COCO stuff [Caesar et al., 2018]. To ensure fair comparison of experimental results with previous studies [Xu et al., 2022; Cheng et al., 2023], we followed the same calculation approach and used mean Intersection-over-Union (mIoU) as the evaluation metric, which is suitable for assessing the train and val sets of two datasets. For the PASCAL VOC test set, results are provided by the official online evaluation server. Figure 1: Impact of varying numbers of embedding optimization layers on the mIoU, FP, and FN of the final CAM."
        },
        {
            "title": "2 Parameter Analysis",
            "content": "Hybrid Feature Alignment weight α. To highlight the impact of weight values α on experimental results, all other hyper-parameters were fixed to the configuration that produced the best CAM quality. As shown in Figure 1(a), the graph illustrates the effect of varying weights on mIoU values. Additionally, to visually demonstrate the improvement brought by the hybrid feature alignment weight α, we performed comparative heatmap visualization of the fusion strategy. As shown in Figure 1, panel (b) displays the heatmap visualizations of self-attention weights for specific category, both without and with the introduction of hybrid feature alignment. Experimental results show that when the weight is set to 0.35, mIoU reaches its maximum value, improving by 0.7% compared to the case without hybrid feature alignment. This demonstrates that the introduction of hybrid feature alignment module allows the model to effectively address the limitations of the ViT self-attention mechanism in capturing image features. The fused features offer more comprehensive representation of information across various dimensional spaces in the image, thereby fully validating the effectiveness of the feature fusion strategy. Selection of Embedding Optimization Layers. To showcase the improvement in CAM generation from embedding optimization and identify the optimal number of embedding optimization layers, we optimized embeddings at different numbers of cascaded encoders and compared the resulting improvements in the target localization map quality. We use mIoU to measure the accuracy of CAM and introduce the false positive (FP) and false negative (FN) metrics to jointly assess the severity of over-activation and under-activation in the model. As shown in Figure 3, we set different numbers of embedding optimization layers across layers 1 to 12 (the transformer encoder in DeiT-S consists of 12 layers) and calculated the results separately for each configuration, where refers to refers to the first layers near the input end. The results indicate that embedding optimization at every layer most effectively suppresses over-activation or under-activation. In contrast, without embedding optimization, the FP and FN values are the highest, and the mIoU value is the lowest. This suggests that, with the help of the embedding optimization mechanism, the model can appropriately extend to the entire target area while suppressing extension to non-target regions."
        },
        {
            "title": "3 Qualitative Analysis\nFigure 2 illustrates a clear comparison of the results obtained\nby applying our proposed method versus those of the baseline\nmodel. On the PASCAL VOC dataset, the quality of class\nactivation maps (CAMs) generated after optimizing with our\ninformation embedding technique is significantly superior to\nthat of the original baseline model. Notably, even on large-\nscale and highly complex datasets such as MS COCO, our\ninnovative method demonstrates outstanding and consistent\nperformance. This is largely attributed to its ability to ef-",
            "content": "Figure 2: Visualization of the CAMs of input images generated by different methods. (a)(e) Input; (b)(f) Baseline; (c)(g) Dual Optimization of Embedding Information (DOEI); (d)(h) GT; Figure 3: Impact of varying numbers of embedding optimization layers on the mIoU, FP, and FN of the final CAM. fectively extract, refine, and optimize the intrinsic knowledge learned by the model, enabling more precise representation of the target features and progressively achieving enhanced overall performance."
        }
    ],
    "affiliations": [
        "La Trobe University",
        "Sichuan University",
        "Singapore Management University",
        "Southwest Minzu University",
        "The Australian National University"
    ]
}