{
    "paper_title": "On Relation-Specific Neurons in Large Language Models",
    "authors": [
        "Yihong Liu",
        "Runsheng Chen",
        "Lea Hirlimann",
        "Ahmad Dawar Hakimi",
        "Mingyang Wang",
        "Amir Hossein Kargaran",
        "Sascha Rothe",
        "François Yvon",
        "Hinrich Schütze"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as a combination of relations and entities, it remains unclear whether some neurons focus on a relation itself -- independent of any entity. We hypothesize such neurons detect a relation in the input text and guide generation involving such a relation. To investigate this, we study the Llama-2 family on a chosen set of relations with a statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation $r$ on the LLM's ability to handle (1) facts whose relation is $r$ and (2) facts whose relation is a different relation $r' \\neq r$. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation-specific neurons. $\\textbf{(i) Neuron cumulativity.}$ The neurons for $r$ present a cumulative effect so that deactivating a larger portion of them results in the degradation of more facts in $r$. $\\textbf{(ii) Neuron versatility.}$ Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons transfer across languages. $\\textbf{(iii) Neuron interference.}$ Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at https://github.com/cisnlp/relation-specific-neurons."
        },
        {
            "title": "Start",
            "content": "On Relation-Specific Neurons in Large Language Models Yihong Liu1,2,*, Runsheng Chen3,*, Lea Hirlimann1, Ahmad Dawar Hakimi1,2, Mingyang Wang1,2,4, Amir Hossein Kargaran1,2, Sascha Rothe5,, François Yvon6,, and Hinrich Schütze1,2, 1Center for Information and Language Processing, LMU Munich 2Munich Center for Machine Learning (MCML) 3Technical University of Munich 4Bosch Center for Artificial Intelligence 5Google DeepMind, Zürich, Switzerland 6Sorbonne Université, CNRS, ISIR, France {yihong, hirlimann}@cis.lmu.de runsheng.chen@tum.de"
        },
        {
            "title": "Abstract",
            "content": "In large language models (LLMs), certain neurons can store distinct pieces of knowledge learned during pretraining. While knowledge typically appears as combination of relations and entities, it remains unclear whether some neurons focus on relation itself independent of any entity. We hypothesize such neurons detect relation in the input text and guide generation involving such relation. To investigate this, we study the Llama-2 family on chosen set of relations with statistics-based method. Our experiments demonstrate the existence of relation-specific neurons. We measure the effect of selectively deactivating candidate neurons specific to relation on the LLMs ability to handle (1) facts whose relation is and (2) facts whose relation is different relation = r. With respect to their capacity for encoding relation information, we give evidence for the following three properties of relation- (i) Neuron cumulativity. specific neurons. The neurons for present cumulative effect so that deactivating larger portion of them results in the degradation of more facts in r. (ii) Neuron versatility. Neurons can be shared across multiple closely related as well as less related relations. Some relation neurons trans- (iii) Neuron interferfer across languages. ence. Deactivating neurons specific to one relation can improve LLM generation performance for facts of other relations. We will make our code publicly available at https://github. com/cisnlp/relation-specific-neurons."
        },
        {
            "title": "Introduction",
            "content": "Large text corpora like Wikipedia contain abundant factual knowledge. LLMs, pretrained on such corpora, can function as knowledge bases that retrieve information and generate text involving factual content (Petroni et al., 2019; Jiang et al., 2020). Recent *Equal contribution. Equal advising. studies suggest that some knowledge is parameterized by LLMs (Dai et al., 2022; Geva et al., 2023), especially within the feed-forward layers of the Transformer architecture (Vaswani et al., 2017), which act as key-value memory (Geva et al., 2021). Factual knowledge is often expressed as relational fact in triple form: subject, relation, and object, e.g., (NVIDIA, company_ceo, Jensen Huang). However, it remains unclear whether each fact is stored and processed separately through knowledge neurons (Dai et al., 2022), or if there exist relationspecific neurons that focus on the relation itself and guide generating the object once the subject and relation of triple have been detected. In this work, we examine the existence of relation-specific neurons in decoder-only LLMs. Our study focuses on the Llama-2 family (7B and 13B) (Touvron et al., 2023) and examines factual knowledge grouped into 12 types of relations. To pinpoint relation-specific neurons for these relations, we adopt the neuron identification method proposed by Cuadros et al. (2022), which identifies the neurons that are uniquely activated in one group of sentences (positive examples) while not in another (negative examples). Kojima et al. (2024) successfully applied this method to uncover language-specific neurons. Following this line of work, we construct zero-shot prompts featuring specific relation for the positive examples and prompts with other relations for the negative examples. Neurons whose activation patterns are positively correlated with positive examples are regarded as relation-specific neurons. To understand the impact of these neurons, we perform question-answering on new held-out prompts. These prompts for each relation share the same relation as the positive examples used for identification but have no entity overlap; this disentangles the effects of entities and relations. For each relation, we compare performance between 5 2 0 F 4 2 ] . [ 1 5 5 3 7 1 . 2 0 5 2 : r the original model and the model in which the neurons for that relation are deactivated intra-relation results. We also study how deactivating the neurons for one relation influences performance on other relations inter-relation results. Our experiments reveal several key properties of these neurons: Neuron cumulativity. These neurons present cumulative effect so that deactivating larger portion of them results in the degradation of more facts, suggesting LLM distributes relational knowledge across neurons in manner that can vary significantly from fact to fact. This property aligns with the evidence of the existence of redundant or self-repair neurons (Dalvi et al., 2020; McGrath et al., 2023; He et al., 2024). Our analysis suggests the frequency of fact in the pretraining data is associated with its sensitivity to given subset. Neuron versatility. Because the total number of neurons is finite while the number of possible relations is vast, some neurons strongly associate with multiple relations. Surprisingly, these relations need not be closely linked two less related relations can share group of neurons, leading to performance drops in both relations if those neurons are deactivated. Such neurons also generalize across languages. This property aligns with recent findings showcasing that some neurons are shared across languages and tasks (Wang et al., 2024a; Tang et al., 2024; Kojima et al., 2024). Neuron interference. Some relation-specific neurons appear to confuse the model when it processes other relations. Deactivating such neurons can yield improved performance on these other relations. This property aligns with broader evidence that sub-networks or circuits within LLMs may serve several different functional roles (Wang et al., 2023a; Bayazit et al., 2024; Mondorf et al., 2024)."
        },
        {
            "title": "2.1 Dataset Manipulation",
            "content": "We use the factual knowledge dataset from Hernandez et al. (2024) for this research, which contains 25 relations. Each relation has different number of facts. Each fact can be represented as subjectrelation-object triple (s, ri, o). We only consider relations that have more than 300 facts to ensure the reliability of our findings. This results in 12 relations. We refer to the set of triples for relation ri as Dri. We then perform the following steps for each relation ri to construct the data used to identify its relation-specific neurons. Step 1: Creating Evaluation Data. For each triple set Dri, we randomly select 50 triples as held-out set for evaluation. (cf. 2.3). We refer to the selected triples as Deva (for evaluation) and the ri remaining triples as Ddet ri (for detection). Step 2: Formulating Prompts. For each triple (s, ri, o) in Ddet ri , we create prompts concerning the subject and the relation ri using the templates provided by Hernandez et al. (2024). For example, we construct zero-shot prompt The CEO of NVIDIA is? Answer: for the triple (NVIDIA, company_CEO, Jensen Huang) with an expected answer Jensen Huang. We also create prompts for Deva in the same way. We ri ri and eva refer to the resulting prompt sets as det . ri Step 3: Validating Prompts. We hypothesize that the model will leverage relation-specific neurons to generate the correct answer, i.e., the object. Therefore, such neurons should be fired in those prompts for which the model answers correctly. We feed the prompt to the model and set the maximum length of the generation to be 2. We then check if the generated tokens are the same as the object (first two tokens): if they are the same, we regard the output as being correct. We exclude prompts that the model answers wrongly from det ri ."
        },
        {
            "title": "2.2 Relation-Specific Neuron Identification",
            "content": "Following (Cuadros et al., 2022), we identify the relation-specific neurons using statistical association measures. This method assigns score for each neuron, representing its level of expertise in distinguishing specific relation from others. Defining Neurons. neural network, or specifically Transformer (Vaswani et al., 2017), consists of many weight matrices. For single weight matrix Rd1d2, we define neuron as column, mapping representation from Rd1 to R. We assign unique index to each neuron and investigate its output value. We only consider the neurons in feed-forward networks (FFNs), i.e., neurons in up_proj, gate_proj, and down_proj, since previous studies have shown that knowledge is mostly stored there (Dai et al., 2022). Grouping Prompts. For each relation ri, we collect positive and negative examples. Specifically, we regard det ri as positive examples and randomly sample 4 det ri prompts from the prompt sets of other relations as negative examples.1 For simplic1We also sample negative examples with different seeds in our preliminary experiments. The neurons for each relation show little change, suggesting the stability of the neurons. ri and 0 otherwise. ri ri and ity, the positive and negative examples selected for relation ri are referred to as + ri respectively. The final data used to detect the relation-specific neurons for relation ri is then Eri = + ri . For each individual example ej ri, we assign binary ri to it: 1 if ej label bj ri + ri + Neuron Output Values. Let om,j,n be the output value of neuron for the n-th token in ej ri when feeding the example to the model. Following Kojima et al. (2024), we average the outputs over tokens to form the final output value of neuron for the entire example ej n=1 om,j,n , ri where is the number of effective tokens in ej ri the output values of [PAD] tokens are regarded as noise and therefore ignored in the computation. ri = 1 ri: om,j (cid:80)T Computing Experts. The level of expertise of each neuron for relation ri is computed by formulating classification task. Specifically, we regard the output value om,j as the prediction score with ri ej ri as input and bj ri as its ground-truth label. In this way, for an individual neuron m, we have the Eri , bj following data: {om,j ri} j=1 . We then measure ri this neurons performance by setting all output values as classification thresholds and comparing the predictions with the ground truth labels. Average precision (AP ) is used as the metric (the area under the precision-recall curve). By doing this, we obtain AP ri for all , allowing us to rank them by their level of expertise in differentiating relation ri from others. The top neurons are regarded as relation-specific neurons in descending order."
        },
        {
            "title": "2.3 Controlled Generation",
            "content": "For each relation ri, we want to investigate the impact of the identified top-k relation-specific neurons. Therefore, we control text generation by overriding the output values with 0 during inference, aiming to deactivate or suppress these neurons. Specifically, we feed eva , the prompts from ri the held-out evaluation prompt set for relation ri, into the model. During inference, we simply set the output values of all top-k relation-specific neurons for ri to constant 0 and let the model generate two tokens, regarded as the models answer to the question. The answer is then compared to the first two tokens of the correct answer, i.e., the object."
        },
        {
            "title": "Model",
            "content": "#Layers #Neurons (FFNs) #Neurons (total) Llama-2-7B Llama-2-13B 32 40 835,584 1,310,720 1,359,872 2,129,920 Table 1: LLama-2 model statistics tioned in 2.2, we consider the neurons in FFNs which account for more than half of neurons in both 7B and 13B, as shown in Table 1. We also report our preliminary results when considering other types of neurons (e.g., up_proj) in E."
        },
        {
            "title": "3.2 Datasets",
            "content": "We manipulate the relational knowledge datasets from Hernandez et al. (2024) using the procedure described in 2.1. Recall that we cover 12 relations in our experiments. Prompt sets det (for ri neuron identification) and eva (for evaluation) are ri constructed for each relation. det ri varies for different relations.2 eva is constructed by randomly ri selecting 50 triples for each relation. Since these 50 triples are not used when creating det ri , this setup ensures no subject overlap between det ri and eva for the same relation ri. In addition, ri we ensure minimal subject overlap across relations (mostly 0 between det rj ). The only exception is between person_mother and person_father, which share lot of subjects. detailed entity overlap analysis is presented in B. ri and det"
        },
        {
            "title": "4 Results and Discussion",
            "content": "We apply our identification method to both 7B and 13B models for all 12 relations. We regard the top 3,000 neurons with the highest AP values as the relation-specific neurons, as for this threshold, we achieve good coverage of relation-specific neurons with set of neurons that is not too large. We discuss the impact of this meta-parameter in 5.1. 4.1 Identified Relation-Specific Neurons Distribution Across Layers. We display the distribution of relation-specific neurons across layers in the 7B model in Figure 2 (see for the 13B model). Most neurons are located in the models middle layers. Such distribution differs from language-specific neurons, which are mostly located in the first and last few layers (Kojima et al., 2024). We hypothesize that relational knowledge requires more than surface-level information that is We consider the 7B and 13B models from the LLama-2 family (Touvron et al., 2023). As men2P det ri can be different for 7B and 13B models because the number of prompts excluded in the validating prompt step (described in 2.1) can be different. Figure 1: Intra-relation results. The left (resp. right) figure displays the results of held-out evaluation prompt set eva ri (resp. identification prompt set det ri ). We report the performance of the original model (without any deactivation), e.g., 7b-original, the model with 3,000 random neurons deactivated (averaged over 10 seeds), e.g., 7b-random, and the model with relation-specific neurons deactivated, e.g., 7b-relation. Figure 2: Distribution of relation-specific neurons across layers. Most are located in the middle layers. mainly encoded and processed in the first and last few layers. Therefore, these relation-specific neurons naturally emerge in the middle layers, where the model has integrated enough lexical and syntactic signals to model and process the relation. This finding is consistent with several studies that show that mapping vectors with certain functions can be extracted from the middle layers (Merullo et al., 2024; Hernandez et al., 2024; Todd et al., 2024). Overlap Across Relations. We display the overlap of relation-specific neurons across relations for the 7B model in Figure 3 ( 13B is in C). We see that person_mother and person_father share large share of neurons, possibly due to the large overlap between their subjects, (see in B). However, even though there is almost no subject overlap between any other relations, many relations still share some neurons with others. For instance, person_occupation and person_sport_position share 297 neurons, possibly because they are similar relations sport Extensive neuis roughly also an occupation. ron overlap can also be observed when two relations are mapping from the same type of subjects, e.g., company_ceo and company_hq, or mapping to the same type of objects, e.g., company_ceo and person_father. However, we show in 4.2.2 that high neuron overlap does not necessarily imply high level of mutual interference. Figure 3: Overlap of the relation-specific neurons across 12 relations. For example, 2053 indicates the number of neurons shared between the 3,000 identified neurons for person_father and the 3,000 for person_mother."
        },
        {
            "title": "4.2 Controlled Generation",
            "content": "For each relation, we set the output values of its identified 3,000 relation-specific neurons to 0, and observe how the deactivation impacts the relation itself and other relations in terms of accuracy. 4.2.1 Intra-Relation Results In addition to intra-relation results, i.e., deactivating the 3,000 identified relation-specific neurons for relation and evaluating the same relation, we also create baseline by randomly deactivating 3,000 neurons in the model. We report the results of the original models, the results of the baseline, and the intra-relation results in Figure 1. We can observe clear performance drop on the identification prompt set det ri when comparing the accuracy of the original model and the model whose relation-specific neurons are deactivated.3 On the other hand, the model with randomly 3,000 3For some relations, e.g., product_company. We show in 5.1 that the drop can become noticeable when we deactivate more than 3,000 neurons. the drop is moderate, Figure 4: Inter-relation results for the 7B model (left) and the 13B model (right). The number in cell (ri, rj) indicates the accuracy drop of relation ri when deactivating the relation-specific neurons of relation rj. deactivated neurons does not show much difference compared with the original model, indicating the 3,000 relation neurons are closely associated with the facts included in det ri . On the evaluation set eva , we also observe notable accuracy drop ri across models for most relations. Since eva ri and det ri do not share any entities, the accuracy drop can only be attributed to the fact that deactivating 3,000 neurons affects the relation itself the common characteristic between eva ri . Therefore, we argue that relation-specific neurons do exist in LLMs. These neurons are entity-irrelevant, rather, they focus on specific relation. ri and det On the other hand, the accuracy does not drop to 0 for any relation (except landmark_country in the 13B model) when its identified relationspecific neurons are deactivated. This indicates these 3,000 neurons do not equally influence all facts that belong to certain relation. We therefore have the neuron cumulativity hypothesis: The relation-specific neurons are associated with different facts belonging to the concerned relation. These neurons present cumulative effect so that deactivating small number of them (in this case, 3,000) results in the degradation of some (not all) facts, whereas more facts are affected when larger portion is disabled. This disparity highlights that LLMs do not uniformly encode all facts that belong to given relation, but rather distribute relational knowledge across neurons in manner that can vary significantly from fact to fact. We validate this by demonstrating that the accuracy further drops by deactivating more neurons in 5.1. We also show that the sensitivity of fact to given population of neurons may correlate with how frequently the fact appears in the pretraining data in 5.3. 4.2.2 Inter-Relation Results To understand the effect of how these neurons influence the models ability to answer prompts across multiple relations, we use accuracy drop as metaccoriginal ric: acc_dropri,rj accoriginal is the accuracy of the original model for ri ri and accdeactivated-rj eva is the accuracy for eva ri when the relation-specific neurons of rj are deactivated. Results are displayed in Figure 4. deactivated-rj ri acc ri accoriginal ri , where = ri When we compare the 7B and 13B models, no consistent pattern emerges across relations. This indicates that, though being trained on the same data, differences in model size and parameter initialization appear to substantially change the functionality of neurons. In particular, most relations in the 13B model are less influenced when neurons of other relations are deactivated than in the 7B model except in the following cases: deactivating neurons of landmark_country strongly affects several other relations concerning the notion of location; person_mother and person_occupation are sensitive to the deactivation of neurons of other relations. Despite these divergences, we propose two hypotheses that hold across both models. Neuron versatility. We observe that deactivating relation-specific neurons for one relation can strongly affect not only that relation but also others, whether they are closely or only loosely related. For example, disabling person_pro_sport neurons has large effect on person_sport_position (but not vice versa) in both 7B and 13B models, likely because the model first needs to understand sport before inferring position. Similarly, deactivating person_father neurons reduces accuracy on person_mother, as Figure 5: Influence of deactivating different numbers of relation-specific neurons for each relation. The variation of accuracy on the relation itself and the average accuracy on other relations is shown. Increasing the number clearly affects the relation itself, but the effect on other relations is obvious only until 3,000 or 10,000 neurons. both share the concept of an immediate parental relationship. Even more loosely related relations can exhibit clear accuracy drop: deactivating star_constellation neurons affects landmark_continent in both models, possibly because both involve the abstract notion of location. Neuron interference. Deactivating the relationspecific neurons of one relation can sometimes improve the accuracy for other relations phenomenon more pronounced in the 7B model, likely because its smaller parameter space is less capable of isolating different relations. In the 7B model, several relations frequently benefit from this effect: for instance, person_mother improves when neurons from 5 out of 11 other relations mostly less related ones are deactivated. This effect is also observed for closely related relations: disabling company_ceo neurons offers small accuracy boost to company_hq for both models, while deactivating landmark_country neurons benefits landmark_continent in the 7B model. Interestingly, the 13B model shows the opposite effect for landmark_continent when disabling landmark_country, implying that country information can help predict continent for the larger model. These findings indicate that neuron interference happens across model sizes, but its specific patterns vary potentially due to factors such as parameter initialization, pretraining data order, or other hyperparameters."
        },
        {
            "title": "5 Further Analysis",
            "content": "5."
        },
        {
            "title": "Influence of the Numbers of Neurons",
            "content": "In this section, we investigate the effect of varying the number of relation-specific neurons on the 7B Relation #total #affected company_ceo company_hq landmark_continent landmark_country person_father person_mother person_occupation person_plays_instrument person_pro_sport person_sport_position product_company star_constellation 11 5 6 6 6 7 8 3 8 20 9 2 0 2 0 2 5 1 0 0 11 2 0 Table 2: Case study. Out of the prompts (#total) where the model answers correctly when deactivating 1,000 but not when deactivating 3,000 neurons, few of them are affected when deactivating the difference (2,000). model (see for 13B). Specifically, we consider ten values: 10, 50, 200, 500, 1,000, 3,000, 10,000, 20,000, and 50,000. When deactivating varying numbers of neurons for relation, we report the variation of accuracy for that relation and the average accuracy for all other relations in Figure 5. The variation in individual relations is in Figure 16. Relation-specific neurons of different relations present varying degree of cumulative effect. Although the accuracy for most relations drops substantially once 3,000 or 10,000 neurons are deactivated, some relations are far more sensitive to smaller-scale deactivation. For example, disabling only 50 neurons reduces the accuracy from 80% to 50% for star_constellation, and similar decline occurs for person_plays_instrument. We hypothesize that this sensitivity reflects how many distinct objects each relation maps to: if relation has fewer unique objects, fewer neurons may be critical. By contrast, relations like person_occupation display lower sensitivity, retaining about 20% accuracy even after 50,000 neurons are deactivated."
        },
        {
            "title": "5.2 Are These Neurons Multilingual?",
            "content": "Recent studies suggest that some neurons encoding factual knowledge or handling specific tasks are multilingual (Stanczak et al., 2022; Zhang et al., 2024; Wang et al., 2024a). natural question is whether relation-specific neurons identified solely via English prompts also function across languages. To explore this, we translate eva to 5 ri languages: German (deu), Spanish (esp), French (fra), Chinese (zho), and Japanese (jpn) (see for details). We then deactivate the previously identified 3,000 neurons in the 7B model and measure the effect on these languages, as shown in Figure 6. Although the models accuracy is generally lower in non-English languages, it still achieves decent results for most relations (except for jpn and zho). Once the neurons for given relation are deactivated, the accuracy drops across nearly all languages supporting our neuron versatility hypothesis. Our findings align with recent explanations that LLMs tend to translate the input text from any language into English for task solving in the middle layers based on shared representation space (Wendler et al., 2024; Dumas et al., 2024; Zhao et al., 2024). As result, deactivating English neurons naturally disrupts this shared space, impairing the models capability to generalize across languages for the affected relation."
        },
        {
            "title": "5.3 Fact Frequencies vs. Neuron Cumulativity",
            "content": "We now examine our neuron cumulativity hypothesis by asking: why do some facts show higher sensitivity to given set of relation neurons than others? We hypothesize that the frequency of fact in the pretraining data can be key factor, as more frequent facts may be memorized more robustly and thus remain less sensitive to deactivation. Because the pretraining data for Llama 2 is not publicly available, we approximate it using Dolma (Soldaini et al., 2024), 3 trillion-token opensource corpus. For each relation, we split the facts into two groups: (a) resilient facts, for which the 7B (or 13B) model correctly predicts the object both before and after deactivating 3,000 relationspecific neurons. (b) sensitive facts, for which the model is correct before but not after these neurons are deactivated.4 We then count how many documents in Dolma contain both the subject and 4We do not consider other numbers of relation-specific neurons because (1) if #neurons < 3,000, there are not enough facts whose predictions change, and (2) if #neurons > 3,000, facts belonging to other relations will also be influenced lot. Figure 6: Accuracy on 12 relations across 6 languages. The upper bars (resp. lower bars) show the accuracy before (resp. after) the deactivation of 3,000 relationsspecific neurons. Even though these neurons are identified using English, they usually influence other languages, indicating multilinguality of these neurons. Validation of the cumulative effect. However, it remains unclear whether the further accuracy drop between two thresholds in Figure 5 is driven by the newly deactivated neurons or the cumulative effect of all deactivated neurons. To further investigate our neuron cumulativity hypothesis, we conduct case study using 1,000 and 3,000 deactivated neurons. Specifically, we identify prompts from eva ri where the model answers correctly with 1,000 deactivated neurons but fails when 3,000 are deactivated. We then deactivate only the 2,000 additional neurons and measure the number of affected prompts, as shown in Table 2. The results support the cumulative effect for most relations: the degradation of more facts primarily stems from the collective deactivation of all 3,000 neurons. Deactivating relation-specific neurons has marginal effect on other relations until certain thresholds are reached. Typically, these thresholds occur around 3,000 or 10,000, below which the accuracy on other relations remains stable supporting the choice of 3,000 neurons. Once more neurons are deactivated, other relations also deteriorate, consistent with our neuron versatility hypothesis. This effect is clearer among closely related relations (e.g., person_pro_sport and person_sport_position), as displayed in Figure 16. Even deactivating up to 50,000 neurons seldom reduces other relations to near-zero accuracy, suggesting high degree of relation-specificity. One exception is company_hq, for which disabling 50,000 neurons causes all relations accuracies to approach zero possibly because some of these neurons underlie more general generation capabilities of the model (Sun et al., 2024; Yu et al., 2024). Figure 7: Relative difference between the average fact frequencies of the group (a) resilient facts and (b) sensitive facts for each relation in 7B (top) and 13B (bottom) models. Resilient facts generally appear more often than sensitive facts in most relations in the pertaining data. object of each fact, calling this the fact frequency.5 Finally, we compute the average frequency for resilient and sensitive facts in each relation ri, denoted respectively as group(a) ri and group(b) ri . ri group(a) group(b) ri group(b) ri for Relative difference: diffri = each relation ri is reported in Figure 7. We find that resilient facts generally appear more often in Dolma than sensitive facts, with only 3 exceptions in the 7B model and 2 exceptions in the 13B model (note that landmark_country is omitted for the 13B model because no facts fall into group (a)). We evaluate this difference with the Wilcoxon SignedRank Test (Woolson, 2005) and obtain p-values of respectively 0.11 and 0.03 for the 7B and the 13B models.6 These results show that there is difference (statistically significant in the 13B model at the 5% level) between the two groups, supporting our hypothesis that more frequent facts are generally less sensitive to the deactivation of given set of relation-specific neurons."
        },
        {
            "title": "5.4 Relations vs. Concepts",
            "content": "We saw in Figure 3 that the storage of relations is generally well separated, but that there are exceptions. We can view relation as relating two concepts, e.g., company_ceo relates instances of the subject concept company to instances of the object concept CEO. From this perspective, the exceptions in Figure 3, i.e., cases where 5We use ElasticSearch API from WIMBD (Elazar et al., 2024) that allows for counting and searching in large corpora. 6We use nonparametric test because the difference across relations does not follow Gaussian distribution. Figure 8: Overlap between the top 3000 neurons of relations and concepts in the 13B model. relation r1 overlaps with relation r2, are generally cases where the concepts of r1 and r2 are the same or overlap. For example, company_ceo and company_hq have the same subject concept. To further explore this hypothesis empirically, we again use the method applied in 2 to relations, but now use it for concepts; that is we identify sets of concept-specific neurons. We group the LRE dataset triples by subject concept, resulting in 11 different concepts. We create set of triples with novel relations such as can and has a, balanced across positive and negative samples. This ensures that the models completion for prompt like (Lincoln has a) depends on the concept instance (Lincoln), not on the relation (has a). Figure 8 shows the overlap between relation neurons and concept neurons for 13B. Most of the cells with large counts support our hypothesis that the overlaps we observe are rooted in relations being representationally associated with their subject and object concepts. Clear examples include company_ceo and its subject concept company; company_hq and its object concept city (assuming that hq is subcategory of city); and landmark_continent and its subject concept landmark. There is little overlap of person with relations like person_mother, potentially because person is more general and semantically unspecific concept than the others. Note that several concepts do not match specific relation, e.g., superhero, and therefore are not strongly associated with any relation. Recall that we picked the concepts according to the availability of annotated data in LRE. However, most identified neurons are only concept neurons or only relation neurons, suggesting that relational and conceptual representations are largely separate."
        },
        {
            "title": "6 Conclusion",
            "content": "This work highlights the existence of relationspecific neurons in LLMs neurons that focus on relations rather than entities. Our experiments show these neurons primarily reside in the middle layers and can be shared across multiple relations. Through systematic deactivation, we reveal their influence on both the targeted and other relations, leading to three key hypotheses: neuron cumulativity (deactivating larger portion of relationspecific neurons results in the degradation of more facts belonging to the concerning relation), neuron versatility (neurons are shared across relations and languages), and neuron interference (neurons from one relation can disrupt the processing of another). These findings shed new light on how LLMs handle relational facts at the neuron level, contributing to the interpretability of LLMs."
        },
        {
            "title": "Limitations",
            "content": "While our findings provide valuable insights, several limitations remain and offer opportunities for future research. First, this work focuses on factual knowledge grouped into 12 relations. Although this selection does not diminish the validity of our findings and hypotheses, it represents relatively narrow set of relations. Future work can explore broader range of relations and analyze how relation-specific neurons behave across more diverse set of relations. Second, our multilingual analysis includes only five languages. While these languages demonstrate neuron versatility, they do not fully capture linguistic diversity. Future research could investigate additional languages, particularly low-resource ones, to determine whether relation-specific neurons exhibit similar relational functionality across these languages. Lastly, we observe that more frequent facts tend to be more robust to the deactivation of relation-specific neurons in both the 7B and 13B models. Fact frequency is approximated using the Dolma corpus (Soldaini et al., 2024) in this study. However, LLama-2 models may incorporate larger and more diverse pretraining dataset, potentially leading to some discrepancies between these approximated fact frequencies and their actual frequencies."
        },
        {
            "title": "Acknowledgments",
            "content": "We appreciate suggestions and comments from other members of CIS, LMU Munich. We want to thank Lixi Lius suggestions for figure design."
        },
        {
            "title": "References",
            "content": "Omer Antverg and Yonatan Belinkov. 2022. On the pitfalls of analyzing individual neurons in language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James R. Glass. 2019. Identifying and controlling important neurons in neural machine translation. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Deniz Bayazit, Negar Foroutan, Zeming Chen, Gail Weiss, and Antoine Bosselut. 2024. Discovering knowledge-critical subnetworks in pretrained language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 65496583, Miami, Florida, USA. Association for Computational Linguistics. Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language models can explain neurons in language models. Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloff. 2022. Self-conditioning pre-trained language models. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 44554473. PMLR. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493 8502, Dublin, Ireland. Association for Computational Linguistics. Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James R. Glass. 2019. What is one grain of sand in the desert? analyzing individual neurons in deep NLP models. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 63096317. AAAI Press. This research was supported by DFG (grant SCHU 2246/14-1). We gratefully acknowledge support from Google through generous research grant. Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. 2020. Analyzing redundancy in pretrained transformer models. In Proceedings of the"
        },
        {
            "title": "2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4908–4926,\nOnline. Association for Computational Linguistics.",
            "content": "Clément Dumas, Veniamin Veselovsky, Giovanni Monea, Robert West, and Chris Wendler. 2024. How do llamas process multilingual text? latent exploration through activation patching. In ICML 2024 Workshop on Mechanistic Interpretability. Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and Yonatan Belinkov. 2020. Analyzing individual neuIn Proceedrons in pre-trained language models. ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 48654880, Online. Association for Computational Linguistics. Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hannaneh Hajishirzi, Noah A. Smith, and Jesse Dodge. 2024. Whats in my big data? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, and 14 others. 2022. Softmax linear units. Transformer Circuits Thread. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac HatfieldDodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, and 6 others. 2021. mathematical framework for transformer circuits. Transformer Circuits Thread. Amit Elhelo and Mor Geva. 2024. Inferring functionality of attention heads from their parameters. Preprint, arXiv:2412.11965. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. 2023. Dissecting recall of factual associations in auto-regressive language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1221612235, Singapore. Association for Computational Linguistics. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are keyvalue memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 54845495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi Sun, Will Hathaway, Neel Nanda, and Dimitris Bertsimas. 2024. Universal neurons in GPT2 language models. Trans. Mach. Learn. Res., 2024. Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsimas. 2023. Finding neurons in haystack: Case studies with sparse probing. Trans. Mach. Learn. Res., 2023. Shwai He, Guoheng Sun, Zheyu Shen, and Ang Li. 2024. What matters in transformers? not all attention is needed. Preprint, arXiv:2406.15786. Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. 2024. Linearity of relation decoding in transformer language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423438. Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka, and Yutaka Matsuo. 2024. On the multilingual ability of decoder-based pre-trained language models: Finding and controlling language-specific neurons. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 69196971, Mexico City, Mexico. Association for Computational Linguistics. János Kramár, Tom Lieberum, Rohin Shah, and Neel Nanda. 2024. Atp*: An efficient and scalable method for localizing llm behaviour to components. Preprint, arXiv:2403.00745. Tom Lieberum, Matthew Rahtz, János Kramár, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. 2023. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. Preprint, arXiv:2307.09458. Weize Liu, Yinlong Xu, Hongxia Xu, Jintai Chen, Xuming Hu, and Jian Wu. 2024. Unraveling babel: Exploring multilingual activation patterns of LLMs and their applications. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1185511881, Miami, Florida, USA. Association for Computational Linguistics. Ang Lv, Yuhan Chen, Kaiyi Zhang, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, and Rui Yan. 2024. Interpreting key mechanisms of factual recall in transformer-based language models. Preprint, arXiv:2403.19521. Thomas McGrath, Matthew Rahtz, Janos Kramar, Vladimir Mikulik, and Shane Legg. 2023. The hydra effect: Emergent self-repair in language model computations. Preprint, arXiv:2307.15771. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. 2023. Massediting memory in transformer. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2024. Language models implement simple Word2Vec-style vector arithmetic. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 50305047, Mexico City, Mexico. Association for Computational Linguistics. Philipp Mondorf, Sondre Wold, and Barbara Plank. 2024. Circuit compositions: Exploring modular structures in transformer-based language models. Preprint, arXiv:2410.01434. Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. 2020. Zoom in: An introduction to circuits. Distill. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, and 7 others. 2022. Incontext learning and induction heads. Transformer Circuits Thread. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowlIn Proceedings of the 2019 Conferedge bases? ence on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 24632473, Hong Kong, China. Association for Computational Linguistics. Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. 2024. practical review of mechanistic interpretability for transformer-based language models. Preprint, arXiv:2407.02646. Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. 2022. Neuron-level interpretation of deep NLP models: survey. Transactions of the Association for Computational Linguistics, 10:12851303. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, and 17 others. 2024. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1572515788, Bangkok, Thailand. Association for Computational Linguistics. Ran Song, Shizhu He, Shuting Jiang, Yantuan Xian, Shengxiang Gao, Kang Liu, and Zhengtao Yu. 2024. Does large language model contain task-specific neurons? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 71017113, Miami, Florida, USA. Association for Computational Linguistics. Karolina Stanczak, Edoardo Ponti, Lucas Torroba Hennigen, Ryan Cotterell, and Isabelle Augenstein. 2022. Same neurons, different languages: Probing morphosyntax in multilingual pre-trained models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 15891598, Seattle, United States. Association for Computational Linguistics. Mingjie Sun, Xinlei Chen, J. Zico Kolter, and Zhuang Liu. 2024. Massive activations in large language models. Preprint, arXiv:2402.17762. Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu Wei, and Ji-Rong Wen. 2024. Language-specific neurons: The key to multilingual capabilities in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 57015715, Bangkok, Thailand. Association for Computational Linguistics. Eric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron Mueller, Byron C. Wallace, and David Bau. 2024. Function vectors in large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 others. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart M. Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2023a. Interpretability in the wild: circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. 2023b. Interpretability in the wild: circuit for indirect object identification in GPT-2 small. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Weixuan Wang, Barry Haddow, Minghao Wu, Wei Peng, and Alexandra Birch. 2024a. Sharing matters: Analysing neurons across languages and tasks in llms. Preprint, arXiv:2406.09265. Yifei Wang, Yuheng Chen, Wanting Wen, Yu Sheng, Linjing Li, and Daniel Dajun Zeng. 2024b. Unveiling factual recall behaviors of large language models through knowledge neurons. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 73887402, Miami, Florida, USA. Association for Computational Linguistics. Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. 2024. Do llamas work in English? on the latent language of multilingual transformers. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1536615394, Bangkok, Thailand. Association for Computational Linguistics. Robert Woolson. 2005. Wilcoxon signed-rank test. Encyclopedia of Biostatistics, 8. Mengxia Yu, De Wang, Qi Shan, Colorado Reed, and Alvin Wan. 2024. The super weight in large language models. Preprint, arXiv:2411.07191. Qinan Yu, Jack Merullo, and Ellie Pavlick. 2023. Characterizing mechanisms for factual recall in language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 99249959, Singapore. Association for Computational Linguistics. Zeping Yu and Sophia Ananiadou. 2024. Neuron-level knowledge attribution in large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 32673280, Miami, Florida, USA. Association for Computational Linguistics. Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, and Jie Zhou. 2024. Multilingual knowledge editing with language-agnostic factual neurons. Preprint, arXiv:2406.16416. Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. 2024. How do large language models handle multilingualism? Preprint, arXiv:2402.18815."
        },
        {
            "title": "A Related Work",
            "content": "Mechanistic interpretability (MI) is growing subfield of interpretability that aims to understand LLMs by breaking them down into smaller components and fundamental computations. It has gained significant attention for studying how LLMs recall factual knowledge learned during pretraining (Meng et al., 2022; Dai et al., 2022; Geva et al., 2023; Yu et al., 2023; Lv et al., 2024; Wang et al., 2024b). Following Olah et al. (2020); Rai et al. (2024), MI research can be categorized into two areas: the study of features and the study of circuits, based on the type of decomposed components. Features refer to human-interpretable properties encoded in model representations or represented by model components, such as neurons and attention heads (Elhage et al., 2022; Gurnee et al., 2023). Circuits are subgraphs of the models computation graph responsible for implementing specific behaviors (Wang et al., 2023b; Elhage et al., 2021). In this work, we focus on neuron-level featurebased interpretability analysis to localize relationspecific neurons, which are responsible for encoding and recalling specific types of factual knowledge. Existing studies have utilized various approaches for neuron interpretation, each offering unique advantages and limitations (Sajjad et al., 2022; Rai et al., 2024). The visualization method (Olsson et al., 2022; Elhage et al., 2022; Lieberum et al., 2023; Bills et al., 2023; Liu et al., 2024) involves visualizing neuron activations and manually identifying the underlying concept across input text. While being straightforward, it relies heavily on human effort and risks overgeneralization. Statistics-based methods (Bau et al., 2019; Cuadros et al., 2022; Kojima et al., 2024; Yu and Ananiadou, 2024; Tang et al., 2024; Wang et al., 2024b), on the other hand, aggregate activation statistics across data to establish connections between neurons and concepts, identifying patterns through the co-occurrence of neuron activation values and specific input features. Probing-based methods (Dalvi et al., 2019; Durrani et al., 2020; Antverg and Belinkov, 2022; Gurnee et al., 2024) train diagnostic classifiers on neuron activations to identify neurons associated with predefined concepts. These methods are scalable, enabling the discovery of neuron sets across large datasets, though they depend on supervised data annotations. Causationbased methods (Vig et al., 2020; Meng et al., 2022, 2023; Kramár et al., 2024; Song et al., 2024) take different approach by directly varying the values of specific neurons or components and analyzing changes in model behavior; significant changes indicate the importance of these neurons or components to particular functionalities. Building on this foundation, our work adopts the statistics-based method proposed by Cuadros et al. (2022) to identify relation-specific neurons neurons uniquely fired for queries concerning facts sharing the same relation. This approach facilitates scalable and targeted analysis of neuron behavior in relation to factual knowledge recall."
        },
        {
            "title": "B Entity Overlap Across Relations",
            "content": "We show the number of distinct subjects (resp. objects) in each relation and the number of overlapping subjects (resp. objects) between any two relations in the identification prompt set det ri of the 7B model and the 13B model in Figure 9 and 10 respectively. Most two relations have no common or very limited overlapping (less than 11) subjects, except for person_mother and person_father, which are mostly celebrities, possibly resulting in extensive neuron overlap between the two relations as we show in 4.1. Similarly, no two relations share many objects. Additionally, we show the number of overlapping entities in the evaluation set eva (the 7B and 13B models share the ri same evaluation set) in Figure 11. The results also show almost no entity overlap across different relations: among all relations, only person_mother and person_father share one subject and the rest of the relations do not share any subject or object overlap. The entity analysis suggests that entities are not confounding factor in our experiments and the identified relation-specific neurons are only concerning the relation itself but not entities. Analysis On the 13B Model We perform similar analysis on the 13B model as we do for the 7B model. We first show how the identified 3,000 relation-specific neurons are distributed across layers for each relation in Figure 12. The trend is similar to what we observe in the 7B model (cf. Figure 2). Most of the relation-specific neurons are distributed in the middle layers. Then we show the overlap of relation-specific neurons across relations in Figure 13. Surprisingly, the overlap pattern is very different from what we observe in the 7B model. First, it seems that many relations that share concept of location share extensive neurons, e.g., company_hq, landmark_country, landmark_country and star_constellation. This explains the difference in inter-relation results between the models (cf. Figure 4) where we see deactivating neurons of landmark_country significantly influence other relations also concerning location for the 13B model but not for the 7B model. We then demonstrate the effect of varying numbers of relation-specific neurons using the same numbers: 10, 50, 200, 500, 1,000, 3,000, 10,000, 20,000, and 50,000. Figure 15 presents the results. The global trend is similar to what we observe for the 7B model: deactivating more neurons results in further drop in accuracy across all relations. This indicates the neuron cumulativity is universal across models. Relation-specific neurons for most relations present similar cumulative effect to the 13B model. The original two outliers in the 7B model (person_occupation and product_company where the accuracy does not drop to 0 in the 7B model) even show plateau, i.e., the accuracy remains almost unchanged or only slightly decreases. This might suggest that facts belonging to these two relations might be wellmemorized by the models and are less sensitive to the deactivation of relation-specific neurons. Lastly, we show whether the identified relationspecific neurons from the 13B model are also multilingual. We use the same translated prompt sets as we use for the 7B model. We deactivate the 3,000 neurons identified using English and see how this affects the performance in other languages: German (deu), Spanish (esp), French (fra), Chinese (zho), and Japanese (jpn). The results are presented in Figure 14. We observe similar results as from the 7B model: when we deactivate the relation-specific neurons identified using English prompts, many relations are influenced across Figure 9: Subject (left) and object (right) overlap across 12 relations obtained from the 7B model. The diagonal in each figure shows the number of distinct subjects or objects for each relation. It can be seen that factual knowledge from different relations has almost no entity overlap except for person_mother and person_father, which are mostly celebrities. Figure 10: Subject (left) and object (right) overlap across 12 relations obtained from the 13B model. The trend is very similar to that in the 7B model: person_mother and person_father share many subjects. Figure 11: Subject (left) and object (right) overlap across 12 relations in the held-out evaluation prompt set eva ri . Almost no two relations share any subjects or objects. Figure 12: Distribution of relation-specific neurons across layers for the 13B model. Similar to Figure 2, identified relation-specific neurons are mostly located in the middle layers, except for person_mother. languages, suggesting models with different sizes also have multilingual relational neurons. We also see some interesting counterexamples: deactivating landmark_country neurons completely deteriorates the relation in English but not in German. This indicates while some neurons have multilingual relational functionalities, there are still some relations dealt with in language-specific manner."
        },
        {
            "title": "D Translation Process",
            "content": "We take two-step approach to ensure the translation quality of individual prompts from English into the target languages across relations. Figure 13: Overlap of the relation-specific neurons across 12 relations in the 13B model. The overlap distribution is not similar to what we observe for the 7B model shown in Figure 3, explaining the difference in inter-relation results (cf. Table 4). cally, we consider five additional different varieties when selecting the top 3,000 neurons for the 7B model: all (neurons in any matrices), self_attn (neurons in self-attention matrices), up_proj (neurons in up_proj matrices), gate_proj (neurons in gate_proj matrices), down_proj (neurons in down_proj matrices). We first draw the distribution of the neuron types across relations for variety all in Figure 18 and report the inter-relation results in Figure 19 (all), 20 (self_attn), 21 (up_proj), 22 (gate_proj), and 23 (down_proj). According to the results, we observe that simply considering self_attn does not offer consistent accuracy drop for the relation itself (by looking at the diagonal: some relations are not influenced too much). This can be explained by the fact the self_attn is shared across relations (as shown by Elhelo and Geva (2024)) and facts are mainly stored in the FFNs. Only considering down_proj offer similar results as self_attn. Interestingly, deactivating up_proj neurons does not influence all relations much in general, indicating it does not make sense to consider up_proj alone. Considering all or gate_proj neurons offer similar results compared to considering neurons in FFNs (shown in Figure 1). However, by considering neurons in FFNs (i.e., up_proj, gate_proj and down_proj), we see more obvious inter-relation accuracy drop as shown on the diagonal in Figure 1. Therefore, our additional analysis supports our choice of considering neurons in FFNs. Concept-Specific Neurons Concept-Relation Overlap in the 7B Model Figure 24 illustrates the overlap between individual relationand concept-specific neurons the overlap of conin the 7b model. There, cepts connected to the abstract notion of location and the relations are mostly concentrated on the landmark_country relation in comparison to the 13b model, where they are spread over company_hq, landmark_continent and landmark_country. This aligns with the difference between the 7B and 13B models in terms of their patterns of inter-relation results (cf. Figure 4): deactivating the landmark_country neurons results in significant accuracy drop in other relations concerning location in the 13B model while not in the 7B model. Another difference between both models is that there is more distributed neuron overlap in the 7b model between the subject Figure 14: Accuracy on 12 relations across 6 languages from the 13B model. The upper bars (resp. lower bars) show the accuracy before (resp. after) the deactivation of 3,000 relations-specific neurons. Translating subject-object pairs. The first step concerns mapping entities, i.e., subject and object pairs into the target language. The default way of doing this is by identifying if the entity is available in Wikidata and the target language using the Wikidata API.7 If the entity of interest is available in the target language, we directly take the entity name in that language. If the entity is not available, we then resort to Google Translate to translate the entity from English to the target language.8. By performing this step, we obtain the subject-object pairs in all target languages and all relations. Translating prompt templates. We take the prompt templates of different relations written in English and use Google Translate to translate them into target languages. We then investigate how the LLama-2 7B model performs on these prompts using eva in the target languages. If the model ri performs suboptimally (<30% accuracy) for relation in specific language, then we manually check the prompt template in that language and update the template accordingly until satisfactory accuracy (>30%) is achieved. For Chinese and Japanese, we do not ensure more than 30% accuracy because the models perform very badly for some relations even if we have tried many prompt templates."
        },
        {
            "title": "E Influence of Neuron Type",
            "content": "We consider the neurons in the FFNs (including up_proj, gate_proj, and down_proj matrices) as our major setup. In this section, we explore the individual effects of different types of neurons. Specifi7https://www.wikidata.org/w/api.php 8https://translation.googleapis.com/language/ translate/v2 Figure 15: Influence of deactivating different numbers of relation-specific neurons for each relation (the 13B model). The variation of accuracy on the relation itself and the average accuracy on other relations is shown. Figure 16: Influence of deactivating different numbers of relation-specific neurons in the 7B model for each relation. The variation of accuracy on the relation itself (noted with * and dashed line style) and the accuracy on all other relations is shown in each figure. Similar to Figure 5, increasing the number of neurons clearly affects the relation itself, but the effect on other individual relations does not become clearly noticeable until 3,00010,000 neurons. Figure 17: Influence of deactivating different numbers of relation-specific neurons in the 13B model for each relation. The variation of accuracy on the relation itself (noted with * and dashed line style) and the accuracy on all other relations is shown in each figure. Figure 18: The distribution of the neuron types in the identified 3,000 neurons for the variety all across all relations. Figure 21: Inter-relation results of the 7B model when considering the neuron type variety as up_proj. Figure 19: Inter-relation results of the 7B model when considering the neuron type variety as all. Figure 22: Inter-relation results of the 7B model when considering the neuron type variety as gate_proj. Figure 20: Inter-relation results of the 7B model when considering the neuron type variety as self_attn. Figure 23: Inter-relation results of the 7B model when considering the neuron type variety as down_proj. Figure 25: Accuracy results of evaluation prompts for 11 concepts in the 7b and 13b model. We report the performance of the original model (without any deactivation) e.g., 7b-original, the model 3000 random deactivated neurons, e.g. 7b-random, and the model with deactivating the top 3000 identified concept-specific neurons, e.g., 7b-concept. small accuracy drop. Or, the 3000 concept-specific neurons store knowledge, though concerning the concept, unrelated to the prompts. For instance, the validation prompts of the concept presidents all demand historical dates as predicted answers, which is only one kind of knowledge that might be expected in connection with presidents. This phenomenon actually aligns with our neuron interference hypothesis: deactivating neurons that store unhelpful knowledge can less confuse the model, therefore improving the performance."
        },
        {
            "title": "G Experimental Environment",
            "content": "We run all experiments on NVIDIA RTX A6000 GPUs. The Python environment we use is the same as Kojima et al. (2024)."
        },
        {
            "title": "H Prompt Templates",
            "content": "6 3,"
        },
        {
            "title": "Table",
            "content": "across considered landmark_country in landmark_continent templates (with We show the actual prompt an object-subject example) we use for each languages: relation company_hq company_ceo in Tain Table 4, ble 6, 5, person_father in Table 7, person_mother in Tain Table 9, ble in Table 10, Table person_sport_position Table 11, and 12, star_constellation in Table 14. person_occupation person_plays_instrument person_pro_sport product_company in Table in Table in in 13, 8, 9Kojima et al. (2024)s GitHub repository is available at https://github.com/kojima-takeshi188/lang_neuron Figure 24: Overlap between the top 3000 identified neurons for each relation and concept in the 7B model. concept person and all corresponding relations. Validation of Concept-Specific Neurons The top neurons on concept are evaluated on random selection of 100 prompts from the LRE dataset that include the specified concept as subject. Examples for the concept person are \"Tom Hankss father is named? Answer:\", \"Hilary Hahn plays the instrument of? Answer:\", or \"Thomas Mann went to university at? Answer:\". Figure 25 shows the results for the validation on these validation prompts for both models with the original accuracy score, baseline that ablates 3000 neurons randomly, and the ablation of 3000 concept-specific neurons. Note that the impact of ablating certain amount of expert neurons varies between concepts. The observed drop in performance due to the ablation of 3000 neurons for concepts like pokemon, superhero, and star is very large, while accuracy scores of other concepts in the 13b model, such as person appear stable, or even improve, e.g., presidents. We assume the neuron cumulativity also applies to the conceptspecific neurons. That is, the knowledge on specific concept is distributed over much larger population of neurons, and further accuracy drop can be observed once more concept-specific neurons are deactivated similar to what we observe for relation-specific neurons (cf. Figure 5). As only partial knowledge is withheld from the deactivation of 3000 concept-specific neurons, this might be too little knowledge to affect the facts concerning that concept (substantial knowledge on the concept is stored in the remaining neurons), resulting in only Language Subject-Object Pair Prompt English (Panasonic Corporation, Kazuhiro Tsuga) Panasonic Corporations CEO is? Answer: The CEO of Panasonic Corporation is? Answer: Expected Output Kazuhiro Tsuga German (Panasonic, Kazuhiro Tsuga) Spanish (Panasonic, Kazuhiro Tsuga) French (Panasonic, Kazuhiro Tsuga) Japanese (パナソニック株式会社, 津賀一宏) Chinese (松下公司, 津贺一宏) Der Name des CEO von Panasonic lautet Wer ist der CEO von Panasonic? Ihr Name ist Kazuhiro Tsuga Por favor, responda directamente por su nombre. El nombre del director general de Panasonic es Por favor, responda directamente por su nombre. Quién es el director general de Panasonic? Su nombre es Veuillez répondre directement avec le nom. Le nom du président-directeur général de Panasonic est Veuillez répondre directement avec le nom. Le PDG de Panasonic est nommé 名前で直接お答えください パナソニック 株式会社 のCEOの名前は 名前で直接お答えください パナソニック 株式会社 のCEOは誰ですか彼らの名前は 松下公司 的首席执行官名字叫做 松下公司 的CEO名字叫做 Kazuhiro Tsuga Kazuhiro Tsuga 津賀一宏 津贺一宏 Table 3: Prompts for company_ceo in different languages. We use the triple (Panasonic, company_ceo, Kazuhiro Tsuga) as an example. The subject-object pair is represented in the respective language."
        },
        {
            "title": "Language",
            "content": "Subject-Object Pair"
        },
        {
            "title": "Prompt",
            "content": "English (Cadillac, Detroit) German (Cadillac, Detroit) Spanish (Cadillac, Detroit) French (Cadillac, Détroit) Japanese (キャデラック, デトロイト) Chinese (凯迪拉克, 底特律)"
        },
        {
            "title": "Expected Output",
            "content": "Detroit The headquarters of Cadillac are in the city of? Answer: The headquarters of Cadillac are in the city of? Answer: Cadillac hat seinen Hauptsitz in der Stadt von Der Hauptsitz von Cadillac befindet sich in der Stadt von Detroit Cadillac tiene su sede en la ciudad de La sede de Cadillac se encuentra en la ciudad de Detroit Le nom de la ville où se trouve le siège social de Cadillac est La ville où se trouve le siège social de Cadillac sappelle Détroit キャデラックの本社がある都市はどこ ですか キャデラックの本社はどの都市にあり ますか デトロイト 凯迪拉克总部所位于的城市名字叫做 凯迪拉克的总部所在的城市名字叫 底特律 Table 4: Prompts for company_hq in all languages. We use the triple (Cadillac, company_hq, Detroit) as an example. The subject-object pair is represented in the respective language. Language Subject-Object Pair Prompt Expected Output"
        },
        {
            "title": "English",
            "content": "(Elbe, Europe)"
        },
        {
            "title": "German",
            "content": "(Elbe, Europa)"
        },
        {
            "title": "Spanish",
            "content": "(Elba, Europa)"
        },
        {
            "title": "French",
            "content": "(Elbe, Europe) Elbe is on the continent of? Answer: Bitte geben Sie den Kontinentnamen direkt an, z. B. Europa, Afrika usw. Der Name des Kontinents, auf dem Elbe liegt, lautet"
        },
        {
            "title": "Europa",
            "content": "Veuillez répondre directement avec le nom du continent. Le nom du continent où se trouve Elbe est"
        },
        {
            "title": "Chinese",
            "content": "(エルベ川, ヨーロッパ) (易北河, 欧洲) エルベ川が所在する大陸の名前は ヨーロッパ 易北河所位于的大洲/大陆名字叫做 欧洲 languages. We use the triple (Elbe, Table 5: Prompts for the landmark_continent relation in all landmark_continent, Europe) as an example. The subject-object pair is represented in the respective language. Language Subject-Object Pair Prompt English (Namba Station, Japan) German (Namba Station, Japan) Spanish (Namba Station, Japan) Expected Output Japan Namba Station is in the country of? Answer: In welchem Land liegt Namba Station? Es liegt in Japan El nombre del país donde se encuentra Namba Station es Japan French Japanese Chinese (Namba Station, Japan) Le nom du pays où se trouve Namba Station est Japan (難波駅, 日本) (难波站, 日本) 難波駅が所在する国の名前は 难波站所位于的国家名字叫做 日本 日本 Table 6: Prompts for the landmark_country relation in all languages. We use the triple (Namba Station, landmark_country, Japan) as an example. The subject-object pair is represented in the respective language."
        },
        {
            "title": "Language",
            "content": "Subject-Object Pair"
        },
        {
            "title": "Prompt",
            "content": "English (Ronald Reagan, Jack Reagan) Ronald Reagans father is named? Answer: German Spanish French Japanese (Ronald Reagan, Jack Reagan) Der Vater von Ronald Reagan heißt (Ronald Reagan, Jack Reagan) El padre de Ronald Reagan se llama (Ronald Reagan, Jack Reagan) Le père de Ronald Reagan sappelle (ロナルドレーガン, ジャックレーガン) 名前で直接お答えくださいロナルド レーガンの父親の名前は"
        },
        {
            "title": "Expected Output",
            "content": "Jack Reagan Jack Reagan Jack Reagan Jack Reagan ジャックレーガン Chinese (罗纳德里根, 杰克里根) 罗纳德里根的父亲名字叫做 杰克里根 Table 7: Prompts for the person_father relation in all languages. We use the triple (Ronald Reagan, person_father, Jack Reagan) as an example. The subject-object pair is represented in the respective language."
        },
        {
            "title": "Language",
            "content": "Subject-Object Pair"
        },
        {
            "title": "English",
            "content": "(Demi Moore, Virginia King) Demi Moores mother is named? Answer:"
        },
        {
            "title": "French",
            "content": "(Demi Moore, Virginia King) Die Mutter von Demi Moore heißt (Demi Moore, Virginia King)"
        },
        {
            "title": "La madre de Demi Moore se llama",
            "content": "(Demi Moore, Virginia King) Qui est la mère de Demi Moore ? Leur mère sappelle"
        },
        {
            "title": "Japanese",
            "content": "(デミムーア, ヴァージニアキング) 名前で直接お答えくださいデミムー アの母親の名前は ヴァージニアキン グ"
        },
        {
            "title": "Chinese",
            "content": "(黛米摩尔, 维吉尼亚金) 黛米摩尔的母亲名字叫做 维吉尼亚金 Table 8: Prompts for the person_mother relation in all languages. We use the triple (Demi Moore, person_mother, Virginia King) as an example. The subject-object pair is represented in the respective language. Language Subject-Object Pair Prompt English (Martin Burrell, politician) German (Martin Burrell, Politiker) Spanish (Martin Burrell, político) French (Martin Burrell, personnalité politique) Martin Burrell works as a? Answer: By profession, Martin Burrell is a? Answer: Martin Burrell arbeitet als Von Beruf ist Martin Burrell ein Por favor especifique el nombre de su ocupación. Martin Burrell trabaja profesionalmente como Por favor especifique el nombre de su ocupación. Por profesión, Martin Burrell es un(a) Veuillez répondre directement par le nom de votre profession. Le nom de la profession de Martin Burrell est Veuillez répondre directement par le nom de votre profession. Martin Burrell travaille professionnellement comme Expected Output politician Politiker político personnalité politique Japanese (マーティンバレル, 政治家) Chinese (马丁巴雷尔, 政治人物) マーティンバレルさんの職業名は マーティンバレルさんの職業名は 马丁巴雷尔从事的职业是一个 职业上来说, 马丁巴雷尔是一名 政治家 政治人物 Table 9: Prompts for the person_occupation relation in all languages. We use the triple (Martin Burrell, person_occupation, politician) as an example. The subject-object pair is represented in the respective language."
        },
        {
            "title": "Language",
            "content": "Subject-Object Pair"
        },
        {
            "title": "Expected Output",
            "content": "English (Anson Funderburgh, guitar) German (Anson Funderburgh, Gitarre) Spanish (Anson Funderburgh, guitarra) French (Anson Funderburgh, guitare) Japanese (アンソンファンダーバーグ, ギター) Anson Funderburgh plays the instrument of? Answer: Bitte geben Sie den Namen des Instruments direkt an. Das Instrument, das Anson Funderburgh spielt, heißt guitar Gitarre Por favor responda directamente el nombre del instrumento Qué instrumento toca Anson Funderburgh? Tocan el guitarra Veuillez répondre directement au nom de linstrument. De quel instrument joue Anson Funderburgh ? Ils jouent du guitare アンソンファンダーバーグはどの楽器を 演奏しますか ギター Chinese (安森芬德伯格, 吉他) 安森芬德伯格所演奏的乐器名字叫做 吉他 Table 10: Prompts for the person_plays_instrument relation in all languages. We use the triple (Anson Funderburgh, person_plays_instrument, guitar) as an example. The subject-object pair is represented in the respective language."
        },
        {
            "title": "Language",
            "content": "Subject-Object Pair"
        },
        {
            "title": "Prompt",
            "content": "English (Frédéric Piquionne, soccer) Frédéric Piquionne plays the sport of? Answer:"
        },
        {
            "title": "Expected Output",
            "content": "soccer German (Frédéric Piquionne, Fußball)"
        },
        {
            "title": "Spanish",
            "content": "(Frédéric Piquionne, fútbol)"
        },
        {
            "title": "French",
            "content": "(Frédéric Piquionne, football)"
        },
        {
            "title": "Japanese",
            "content": "(フレデリックピキオンヌ, サッカー) Welchen Sport betreibt Frédéric Piquionne? Sie betreiben Fußball Por favor, responda directamente el nombre del deporte, como fútbol, baloncesto, etc. El nombre del deporte que juega Frédéric Piquionne es: fútbol Veuillez répondre directement par le nom du sport, comme le football, le basket-ball, etc. Frédéric Piquionne joue professionnellement dans le sport de football サッカーバスケットボールなどスポー ツの名前を直接答えてくださいフレデ リックピキオンヌはどのスポーツをしま すか彼らはスポーツ名をしていま す サッカー"
        },
        {
            "title": "Chinese",
            "content": "(费德历比基安尼, 足球) 费德历比基安尼从事的运动叫做 足球 Table 11: Prompts for the person_pro_sport relation in all languages. We use the triple (Frédéric Piquionne, person_pro_sport, soccer) as an example. The subject-object pair is represented in the respective language. Language Subject-Object Pair Prompt English (Ju Yingzhi, midfielder) German (Ju Yingzhi, Mittelfeldspieler) Spanish (Ju Yingzhi, centrocampista) Ju Yingzhi plays in the position of a? Answer: In their sport, Ju Yingzhi plays as a? Answer: Ju Yingzhi spielt auf der Position von In ihrer Sportart spielt Ju Yingzhi als Por favor, responda directamente el nombre de la posición deportiva, como delantero, defensor, etc. La posición de Ju Yingzhi en el campo deportivo es: Por favor responda directamente con el nombre de la posición deportiva, como delantero, defensor, etc. En su deporte, Ju Yingzhi juega en la posición de un: Expected Output midfielder Mittelfeldspieler centrocampista French (Ju Yingzhi, milieu de terrain) Japanese (ジュインジー, ミッドフィールダー) Chinese (鞠盈智, 中场) Ju Yingzhi évolue au poste de Dans son sport, Ju Yingzhi occupe le rôle de milieu de terrain 彼がプレーするスポーツではジュイン ジーのポジションは ジュインジー競技場のポジションは ミッドフィールダー 鞠盈智在运动场上的位置名字叫做 在他/她从事的运动中,鞠盈智的位置是 中场 Table 12: Prompts for the person_sport_position relation in all languages. We use the triple (Ju Yingzhi, person_sport_position, midfielder) as an example. The subject-object pair is represented in the respective language."
        },
        {
            "title": "Language",
            "content": "Subject-Object Pair"
        },
        {
            "title": "Prompt",
            "content": "English (Jeep Grand Cherokee, Chrysler) German (Jeep Grand Cherokee, Chrysler) Spanish (Jeep Grand Cherokee, Chrysler) Jeep Grand Cherokee was created by which company? Answer: Jeep Grand Cherokee is product of which company? Answer: Bitte geben Sie direkt den Firmen-/Ländernamen an. Das Unternehmen/Land, das Jeep Grand Cherokee entwickelt hat, ist Bitte geben Sie direkt den Firmen-/Ländernamen an. Welches Unternehmen hat Jeep Grand Cherokee entwickelt? Es wurde entwickelt von Por favor, responda directamente el nombre de la empresa/país. Qué empresa desarrolló Jeep Grand Cherokee? Fue desarrollado por Por favor responda directamente con el nombre de la empresa/país. La empresa que desarrolló Jeep Grand Cherokee se llama"
        },
        {
            "title": "Expected Output",
            "content": "Chrysler Chrysler Chrysler"
        },
        {
            "title": "French",
            "content": "(Jeep Grand Cherokee, Chrysler)"
        },
        {
            "title": "Japanese",
            "content": "(ジープグランドチェロキー, クライスラー)"
        },
        {
            "title": "Chinese",
            "content": "(吉普大切诺基, 克莱斯勒) Jeep Grand Cherokee été développé(e) par Jeep Grand Cherokee est un produit de lentreprise"
        },
        {
            "title": "Chrysler",
            "content": "会社名/国名を直接お答えくださいジー プグランドチェロキーを開発したのはど の会社ですか? 開発したのは次の会社は 会社名/国名を直接お答えくださいジー プグランドチェロキーを開発した会社は クライスラー 开发了吉普大切诺基的公司名字叫做 开发产品吉普大切诺基的公司名字叫 克莱斯勒 Table 13: Prompts for the product_company relation in all languages. We use the triple (Jeep Grand Cherokee, product_company, Chrysler) as an example. The subject-object pair is represented in the respective language."
        },
        {
            "title": "Language",
            "content": "Subject-Object Pair"
        },
        {
            "title": "Expected Output",
            "content": "English (50 Persei E, Perseus) German (50 Persei E, Perseus) Spanish (50 Persei E, Perseus) French (50 Persei E, Persée) Japanese (50 ペルセウス座 E, ペルセウス座) 50 Persei is part of the constellation named? Answer: Bitte geben Sie den Namen des Sternbildes direkt an. Das Sternbild, zu dem 50 Persei gehört, heißt Perseus Perseus 50 Persei forma parte de la constelación denominada Perseus Le nom de la constellation dans laquelle se trouve 50 Persei est Persée 50 ペルセウス座 Eはどの星座に属していま すかそれは星座名という星座の一部 です ペルセウス座 Chinese (50 英仙座E, 英仙座) 50 英仙座E所位于的星座名字叫做 英仙座 Table 14: Prompts for the star_constellation relation in all languages. We use the triple (50 Persei E, star_constellation, Perseus) as an example. The subject-object pair is represented in the respective language."
        }
    ],
    "affiliations": [
        "Bosch Center for Artificial Intelligence",
        "Center for Information and Language Processing, LMU Munich",
        "Google DeepMind, Zürich, Switzerland",
        "Munich Center for Machine Learning (MCML)",
        "Sorbonne Université, CNRS, ISIR, France",
        "Technical University of Munich"
    ]
}