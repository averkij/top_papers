{
    "paper_title": "UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist",
    "authors": [
        "Zhengyang Liang",
        "Daoan Zhang",
        "Huichi Zhou",
        "Rui Huang",
        "Bobo Li",
        "Yuechen Zhang",
        "Shengqiong Wu",
        "Xiaohan Wang",
        "Jiebo Luo",
        "Lizi Liao",
        "Hao Fei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs a Plan-and-Act dual-agent architecture that drives a highly automated and proactive workflow: a planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through a hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation $\\rightarrow$ multi-round editing $\\rightarrow$ object segmentation $\\rightarrow$ compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, a benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. (https://univa.online/)"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 1 2 5 8 0 . 1 1 5 2 : r UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist Zhengyang Liang1,, Daoan Zhang2,, Huichi Zhou3, Rui Huang4, Bobo Li4, Yuechen Zhang5, Shengqiong Wu4, Xiaohan Wang6, Jiebo Luo2, Lizi Liao1, Hao Fei4, 1Singapore Management University 2University of Rochester 3University College London 4National University of Singapore 5The Chinese University of Hong Kong 6Stanford University Core contributors, equal contribution (zyliang@smu.edu.sg; daoan.zhang@rochester.edu) Project lead, correspondence (haofei7419@gmail.com) While specialized AI models excel at isolated video tasks like generation or understanding, real-world applications demand complex, iterative workflows that combine these capabilities. To bridge this gap, we introduce UniVA, an open-source, omni-capable multi-agent framework for next-generation video generalists that unifies video understanding, segmentation, editing, and generation into cohesive workflows. UniVA employs Plan-and-Act dual-agent architecture that drives highly automated and proactive workflow : planner agent interprets user intentions and decomposes them into structured video-processing steps, while executor agents execute these through modular, MCP-based tool servers (for analysis, generation, editing, tracking, etc.). Through hierarchical multi-level memory (global knowledge, task context, and user-specific preferences), UniVA sustains long-horizon reasoning, contextual continuity, and inter-agent communication, enabling interactive and self-reflective video creation with full traceability. This design enables iterative and any-conditioned video workflows (e.g., text/image/video-conditioned generation multi-round editing object segmentation compositional synthesis) that were previously cumbersome to achieve with single-purpose models or monolithic video-language models. We also introduce UniVA-Bench, benchmark suite of multi-step video tasks spanning understanding, editing, segmentation, and generation, to rigorously evaluate such agentic video systems. Both UniVA and UniVA-Bench are fully open-sourced, aiming to catalyze research on interactive, agentic, and general-purpose video intelligence for the next generation of multimodal AI systems. Date: November 12, 2025 Website for Demos, Codes, Resources: http://univa.online/"
        },
        {
            "title": "1 Introduction",
            "content": "Real-world video applications often require composite, iterative, and agentic workflows that go beyond any single AI capability (Yu et al., 2023; Maazi et al., 2024; Song et al., 2024; Fei et al., 2025). For example, creating dynamic visual story might begin with an image or text concept, expand into generated video, then involve editing that video, segmenting key objects, and finally composing multiple elements into polished scene. Traditionally, accomplishing such pipeline requires stitching together disparate toolseach specialized for narrow taskresulting in brittle, labor-intensive, and non-interactive process that lacks automation and proactive assistance. The lack of unified system for reasoning across multiple video tasks and steps has become critical bottleneck for next-generation video intelligence. Existing approaches address parts of this challenge but fall short of comprehensive solution. Single-task video models (e.g., dedicated networks for segmentation or video generation) deliver high performance on their specific tasks, yet they operate in isolation and fail to handle multi-step goals without manual coordination. More recently, video-language foundation models like VILA-U (Wu et al., 2024b) attempt to integrate understanding and generation into one model. These large models learn broad spectrum of abilities (Fei et al., 2024b; Xie et al., 2025; Tan et al., 2025), but they remain monolithic and inflexible they cannot easily incorporate new tools or modular functions, and leveraging them for complex workflows Figure 1 UniVA (Universal Video Agent) delivers highly automated, interactive, and proactive video creation experience, featuring multi-round dialogue co-creation, memory-based contextual reasoning, intent understanding, and tool-augmented planning for iterative user interaction. It also serves as an omnipotent, unified, industrial-grade video production engine, integrating diverse generation, editing, and understanding modules within an MCP-based framework to ensure cinematic quality, consistency, and extensibility across any-conditioned video tasks. can be inefficient or impractical. Another emerging direction is to use LLM-based agents with tool use. For instance, HuggingGPT (Shen et al., 2023) employs language model as controller to plan tasks and invoke appropriate models/tools in sequence. Similarly, VideoAgent (Fan et al., 2024b) leverages an LLM with structured memory and predefined set of video tools to answer questions on long videos. These agent-based systems illustrate the power of planning and tool integration (Kugo et al., 2025; Wei et al., 2025). However, HuggingGPT is generalist framework not specialized for detailed video operations, and VideoAgent focuses mainly on video understanding queries (e.g., Q&A) with limited editing or generation capabilities. To date, no existing platform fully supports unified, end-to-end agentic video workflow that seamlessly integrates understanding, generation, and editing with proactive interaction and extensible modularity. To bridge this gap, we propose UniVA (Universal Video Agents), unified multi-agent video AI platform that enables complex multi-step video creation and manipulation tasks. UniVA serves as both creative video agent for dynamic, user-interactive generation, and an industrial-grade video engine for comprehensive, high-quality production. Technically, UniVA can be depicted by two key characters: Highly automated, interactive, and proactive video creation experience: UniVA is built on Plan/Act dual agent architecture: planner agent first interprets the users request and decomposes it into sequence of actionable steps, and an executor agent (or team of specialized agents) then carries out each step by invoking the appropriate video tool modules. This separation of planning and acting (in line with recent agent design patterns) allows the system to look ahead and reason about long-horizon goals, while flexibly adapting the plan if intermediate results require changes. On one hand, with strong planning 2 capabilities, UniVA can autonomously accomplish an entire video production pipeline from single user query. On the other hand, agents communicate and share information through multi-level memory mechanism: global memory stores persistent knowledge and facts (e.g. general video facts or precomputed embeddings), task-specific memory retains context and intermediate results for the current workflow, and user memory keeps track of user preferences or historical interactions. This hierarchical memory design ensures that context is maintained throughout the workflow, enabling continuity and avoiding forgetting important details mid-task. In this way, UniVA also supports iterative, multi-round interactions, enabling deeply immersive and dynamic creative experiences. Moreover, UniVA demonstrates strong implicit intent understanding and context retention, enabling agents to proactively refine or suggest creative improvements during generation. Omnipotent, unified, industrial-level video production capabilities: Built upon the Model Context Protocol (MCP) (Anthropic, 2024), UniVA can seamlessly integrate state-of-the-art video functional moduleseither open-source or API-basedin plug-and-play fashion, where each tool module is implemented as modular server and the two agents act as the client. The tool hub spans three major categories: video tools (e.g., generation, understanding, editing), non-video tools (e.g., audio or image operation), and non-AI tools (e.g., video cutting). This broad coverage encompasses nearly all functionalities required in the video production process. For example, UniVA supports video generation/transformation from arbitrary conditions, e.g., text, image or video. By combining with cutting-edge external video generation models, UniVA enables cinematic-quality production of long, complex, and narrative-rich videos. Under the MCP framework, the system can also be effortlessly extended to incorporate new tools and capabilities, forming an open ecosystem that continuously evolves toward truly unified, omni-capable video generalist. To evaluate such systems, we release UniVA-Bench, suite of multi-step video tasks spanning understanding, segmentation, editing, and generation. Tasks are specified as goal cards with gold artifacts (e.g., evidence spans, masks, EDLs) and scored with both task metrics and agentic metrics (plan quality, tool-routing efficiency, memory use, trace completeness). UniVA-Bench is designed to test compositionality, tool swaps, and long-form reasoningnot just per-task accuracy. Code, benchmark, and evaluators are all open-sourced. In summary, our contributions are threefold: (1) We present UniVA, the first open-source, unified, and omni-capable video generalist framework. UniVA unifies video understanding, editing, and generation into single agentic workflow, built upon PlanAct dualagent architecture with hierarchical multi-level memory. This design enables highly automated, interactive, and proactive video creation experience that dynamically reasons over long-horizon goals and user intent. (2) We develop modular, MCP-based video production engine that connects diverse open-source or API-based tools under plug-and-play ecosystem. This framework supports any-conditioned generation, cross-modal editing, and industrial-level quality control, ensuring cinematic consistency, scalability, and extensibility across all stages of video production. (3) We release UniVA-Bench, the first benchmark for evaluating agentic video intelligence across multi-step compositions. It measures not only task performance but also planning quality, memory utilization, and tool-routing efficiency, offering principled foundation for developing truly general-purpose video agents."
        },
        {
            "title": "2 Related Work",
            "content": "Video Processing and Intelligence. Videos serve as realistic simulations of the physical world, and research on video intelligence has spanned tasks such as action recognition, event detection, captioning, retrieval, and video question answering (Tran et al., 2015; Xu et al., 2015; Yang et al., 2023; Wu et al., 2023b; Le et al., 2020). With the advent of large language models (LLMs), video understanding has advanced toward instruction-following and long-context reasoning (Maaz et al., 2024; Lin et al., 2024; Fei et al., 2024a), while fine-grained tasks such as segmentation, grounding, and object parsing offer pixel-level insights (Cheng et al., 2023; Lei et al., 2020; Jin et al., 2022). In parallel, video generation has progressed from autoregressive models, such as VideoGPT (Yan et al., 2021), to diffusion-based methods, including Imagen Video (Ho et al., 2022), Make-A-Video (Singer et al., 2022), and Runway Gen-2 (Runway, 2023), which have achieved improved fidelity and temporal coherence. Recent work also explores controllable synthesis via conditional inputs (Ni et al., 3 Figure 2 Overall architecture of the proposed UniVA system, built on PlanAct paradigm. The Plan Agent decomposes user input (text, image, or video) into subtasks by leveraging global memory (historical traces) and user memory (stored materials). The Act Agent retrieves task-specific memory, executes subtasks via the MCP protocol, and coordinates with external MCP servers (video, AI, and non-AI tools). The system generates multimodal outputs, including text, image, video, and audio. 2023; Ma et al., 2024; Liu et al., 2025b; Wu et al., 2025) and semantic-level video editing through diffusionand instruction-driven pipelines (Wu et al., 2023a; Khachatryan et al., 2023; Liu et al., 2024). Despite remarkable progress, most systems remain fragmented and task-specific, which limits their interoperability and scalability. Toward Unified Video Modeling. To address fragmentation, unified video foundation models aim to integrate diverse tasks into single framework. Joint models such as Show-o2 (Xie et al., 2025) and Omni-video (Tan et al., 2025) combine understanding and generation within large-scale multimodal training. Extensions of VideoLLMs (Lin et al., 2024; Jin et al., 2024) integrate segmentation modules (e.g., SAM2 (Ravi et al., 2024)) to support object-level grounding and reasoning (Xiao et al., 2024; Yuan et al., 2025a), while modular architectures like Vitron (Fei et al., 2024b) adopt flexible encoders and decoders for comprehension, segmentation, and generation. Although these efforts represent significant progress, most existing systems rely on static pipelines without effective scheduling or coordination mechanisms, making them difficult to extend. This highlights the need for frameworks that facilitate dynamic orchestration of heterogeneous modules. Agents for Video Intelligence. Agent-based paradigms have emerged as promising solution for flexible video intelligence, leveraging planning, interaction, and memory mechanisms (Chen et al., 2024; Yin et al., 2023; Wu et al., 2024a). VideoAgent (Fan et al., 2024a) enhances generative quality with memory augmentation, while other works explore agent planning for long-context reasoning (Wang et al., 2024b) and self-improving generation (Soni et al., 2024). Applications extend to video reasoning (Liu et al., 2025c; Shi et al., 2025), editing (Wang et al., 2024a), stylization (Yue et al., 2025), and story generation (Hu et al., 2024). Multi-agent collaborations such as VideoMultiAgents (Kugo et al., 2025) and PreMind (Wei et al., 2025) further enhance performance, though communication and coordination remain open challenges. Protocols like MCP (Anthropic, 2024) and modular plug-and-play designs offer promising directions. Departing from isolated paradigms, our UniVA framework leverages multi-agent interaction, memory augmentation, and context engineering (Mei et al., 2025) to unify understanding, reasoning, editing, and generation, advancing toward truly universal video agents."
        },
        {
            "title": "3.1 Problem Formulation\nWe formulate the challenge of synergistic video intelligence as a sequential decision-making problem. The\nUniVA agent operates within an environment defined by a user’s high-level goal G and a set of available tools\nT . The agent’s objective is to generate a sequence of actions A = (a1, a2, . . . , aN ) that transform an initial",
            "content": "4 state s0 (which may include user-provided videos or images) into final state sN that satisfies the goal G. At each step t, the agent, guided by policy π (instantiated by our Planner), observes the current state st and the interaction history Ht = (a1, s1, . . . , at1, st1), and selects an action at . The execution of this action by the Actor transitions the environment to new state st+1 = Execute(st, at). The Memory system serves as the persistent representation of the history Ht and intermediate artifacts within the state st. The core challenge, therefore, is to design an agent with policy π that can effectively manage the state transitions and leverage the history to produce high-quality final state sN , demonstrating both breadth (by utilizing large and diverse ) and depth (by creating complex, synergistic action sequences A). This entire process can be summarized as finding the optimal action sequence that maximizes quality function of the final state with respect to the goal: = argmax Q(sN , G) where st+1 = Execute(st, at) (1) A=(a1,...,aN )"
        },
        {
            "title": "3.2 The UniVA Control Core",
            "content": "To achieve the aforementioned sequential decision-making framework, we designed the control core of UniVA. This core consists of two key components: decision engine responsible for formulating the policy π, and memory system tasked with managing the state st and history Ht. 3.2.1 PlanAct Dual Agent Architecture The core of UniVA is PlanAct dual-agent architecture. Our strategy π is implemented through dual-agent Plan-Act framework. The Planner, as high-level policy network, maps the goal and the current state st to an abstract sequence of plan. The Actor, as low-level execution policy, converts each step of the plan into concrete actions at. For example, given make cartoon video of my dog, the Planner may decompose it into: (1) retrieve images of the dog, (2) generate cartoon-style video, (3) edit the background, and (4) compose audio. The Actor receives each sub-goal from the Planner, selects the appropriate tool through the MCP interface1 (Anthropic, 2024), fills in required arguments (e.g., video clip, mask, prompt), and executes the call. Once tool finishes, the Actor collects the output and sends it back to the Planner. This separation keeps the Planner lightweight and strategic, while the Actor focuses on reliable and efficient tool use. 3.2.2 Memory Mechanism key challenge in agentic video systems is to maintain context across long and multi-step workflows. As presented in Figure 3, UniVA addresses this with three-level memory mechanism that complements the PlannerActor loop: Global Memory. Stores persistent knowledge and reusable resources, such as precomputed embeddings, generic video facts, or tool usage statistics. This memory provides background context and supports cross-task generalization. Task Memory. Maintains intermediate artifacts, tool outputs, and execution traces for the current workflow. It ensures continuity across multiple steps, allowing later sub-goals to reuse results (e.g., segmentation masks or captions) without recomputation. Task memory also enables traceability, making the entire workflow transparent and reproducible. User Memory. Tracks user-specific preferences and historical interactions, such as favored styles, recurring edit patterns, or personalized constraints. This enables adaptive behaviorsfor example, automatically applying users preferred resolution or editing style in future tasks. Through this design, Global Memory and User Memory together form the persistent storage of long-term history Ht, providing rich context for the strategy π. Task Memory maintains the dynamic state of the current task st and its intermediate products."
        },
        {
            "title": "3.3 Tools Interaction\nThe capability of an agent ultimately depends on its available action space, i.e., the toolset T . To achieve\nmaximum breadth, UniVA’s action space is designed to be open and extensible.",
            "content": "1https://modelcontextprotocol.io/ 5 Figure 3 Memory-augmented framework for video generation. Global and user memories provide context to the plan agent, while task memory coordinates tool calling, storyboard creation, and video generation. We achieve unified management of the action space through MCP protocol. The MCP server module acts as unified gateway between the Actor and collection of distinct tool servers. The server maintains registry of available functions, validates and executes calls through standardized API, and records outputs for traceability. This design means that adding or replacing capability only requires registering it on the server, while the Planner and Actor remain unchanged, making the system modular and extensible. The detailed tools list at A.2."
        },
        {
            "title": "3.4 Framework Walkthrough",
            "content": "Building upon the above components, we now present the overall architecture of our framework. This sequential decision-making process is illustrated in Figure 2. In any task, the Planner (policy π) observes the current state st and the goal to formulate plan, which the Actor executes as sequence of actions = (a1, . . . , aN ) using various tools. The Memory will record the outputs of each action, continuously updating the history Ht and the state st for subsequent steps. The two cases demonstrate the versatility of this framework. The left case exemplifies the agents depth, autonomously decomposing single complex goal into coherent, multi-step generation workflow. The right case showcases the interplay of breadth, where diverse and extensible toolset , made accessible via our MCP layer, is synergistically chained in an interactive session to achieve precise, context-dependent editing outcome."
        },
        {
            "title": "3.5 System Implementation",
            "content": "To demonstrate the practical application of our framework, we have instantiated the UniVA agent within an interactive, web-based video editing application2, as shown in Figure 5. The UniVA agent, operating in the background, parses these requests, formulates plan, and executes the necessary tool calls. The results are then directly reflected on the timeline and preview canvas. This tight integration creates fluid, iterative loop, allowing users to effortlessly switch between high-level AI-driven creation and traditional, hands-on editing, all within single, unified platform. 2Our frontend system is built on OpenCut, and we appreciate the open-source contributions made by the OpenCut team. 6 Figure 4 Iterative tool calling for video generation in UniVA. Left: one-prompt task applies global ink-painting style. Right: multi-round task incrementally edits via segmentation, background change, and extension, demonstrating representative functions."
        },
        {
            "title": "4.1 Benchmark Definition",
            "content": "Video intelligence in practice is an iterative, multi-stage creation process where users interleave understanding, generation, editing, segmentation, and audio/asset composition within single workflow. However, most existing benchmarks largely isolate single tasks and single models, which underestimates the difficulty of long-horizon, multi-step video production and the need for explicit planning, memory, and tool orchestration. Therefore, we introduce unified agent-oriented benchmark that shifts the focus from isolated single-model tasks to end-to-end, tool-augmented video intelligence, aligning evaluation with real user workflows and the requirements of practical video agents. To holistically assess both the range and the intelligence of an agent, the benchmark is organized into two complementary tracks: i) Functional Modules: task performance across Understanding, Generation (LongText2Video, Image/Entities2Video, Video2Video), Editing (long video edits with cross-shot consistency), and Segmentation (long video segmentation with multi-entity occlusion). ii) Agentic Probing: plan quality, dependency satisfaction, and re-planning robustness using structured plan-level metrics; analysis of memory usage (trace, user, task/storyboard) and its downstream impact."
        },
        {
            "title": "4.2 Evaluation Tasks\nUnderstanding (Long-Video QA). This task is designed to target both aesthetics- and semantics-oriented\nquestions for long videos, encompassing shot transitions, visual style, and narrative comprehension in addition",
            "content": "7 Figure 5 The interface combines traditional non-linear timeline and preview canvas with conversational assistant (left), which provides user-friendly entry point to the UniVA agent. This design supports both one-stop, prompt-based generation and multi-turn, interactive editing workflows. to standard entity and action semantics. Unlike prior settings, where each QA pair is tied to single short clip, our task demands answering multiple interdependent questions grounded in single long-form video. Generation. Agents are evaluated on diverse real-world video generation tasks, categorized into three subtypes: 1) LongText2Video, handling long or noisy prompts that necessitate storyboard-first planning; 2) Image/Entities2Video, using 13 reference images to enforce identity preservation and cross-scene coherence; 3) Video2Video, conditioning on source video while ensuring referential stability for persons and objects. Editing (Long Video). This task is defined to involve multi-step edits such as cross-shot replacement, attribute modification, and style transfer, while maintaining narrative integrity and referential consistency. Effective completion requires reasoning in combination with tool invocation (e.g., ref-seg inpaint/compose merge). Segmentation (Long Video). Designed for long clips with multiple entities and frequent occlusions, this task evaluates temporal consistency and robustness in detecting and segmenting shot boundaries. Agentic probing sets. We include (1) 50-instance storyboarduser-intent planning set to compare SingleAgent vs. Plan-Act, and (2) set of standard pipeline tasks with expert references to assess wPED, DepCov, and ReplanQ under injected failures. Memory analyses consider trace memory (historical trajectories), user memory (preferences), and task memory (e.g., storyboards). More data curation details in Appendix B.1."
        },
        {
            "title": "4.3 Evaluation Protocol",
            "content": "To evaluate agent performance on UniVA-Bench, we employ comprehensive suite of metrics targeting three key areas: (1) Task-specific Quality, using established metrics like CLIP Score for command following and DINO Score for subject consistency; (2) Overall User Preference, captured via pairwise judgments from powerful MLLM-as-a-Judge; and (3) Agentic Planning Capabilities, assessed using our novel, specialized metrics (wPED, DepCov, and ReplanQ) that measure plan quality, logical correctness, and recovery robustness. The detailed definitions and calculation methods for all metrics are provided in Appendix B.2. For Generation/Editing, we report CLIP, DINO, and MLLM preference. for Segmentation, J/F/J&F; for 8 Figure 6 Representative examples from the four main task categories in UniVA-Bench: Understanding, Generation, Editing, and Segmentation. Understanding, normalized QA accuracy. For agentic probing, we report wPED/DepCov/ReplanQ with and without memory (trace/user/task) and compare Single-Agent vs. Plan-Act frameworks."
        },
        {
            "title": "5 Experiments",
            "content": "To comprehensively evaluate our systems capabilities in realistic, end-to-end workflows, we conduct all experiments on UniVA-Bench, novel agent-oriented benchmark we introduce in this work. Our experiments are designed to test two central hypotheses: i) unified agentic architecture, where functional modules like understanding and generation are deeply integrated, provides significant performance advantage over isolated, end-to-end models; and ii) the combination of dual-agent Plan-Act framework and multi-component memory system is essential for achieving the robust planning and persistent context required for complex video tasks. The complete experiment settings are in the Appendix C."
        },
        {
            "title": "5.1 Performance of Functional Modules",
            "content": "5.1.1 Generation In the generation scenarios, we benchmark UniVA against three representative end-to-end models: LTX-Video (HaCohen et al., 2024), Wan (Wan et al., 2025), and Seedance (Gao et al., 2025). Evaluating the results using CLIP Score (prompt following), DINO Score (subject consistency), and preference ratings from an MLLM-as-a-Judge, following the UniVA-Bench specification. Detailed baseline setups are in the Appendix. LongText2Video. In the LongText2Video scenario, UniVAs superior performance - achieving the highest CLIP 9 Table 1 Comparison across LongText2Video, Entities2Video and Video2Video. Method LTX-Video Wan Seedance UniVA Entities2Video CLIP Score DINO Score MLLM Judge CLIP Score DINO Score MLLM Judge CLIP Score DINO Score MLLM Judge LongText2Video Video2Video 0.2161 0.2028 0. 0.2814 0.9392 0.6779 0.8836 0.9026 1.125 3.183 2.650 3.333 0.2210 0.3106 0.3039 0. 0.8452 0.7043 0.8800 0.8796 1.281 1.650 2.700 1.789 0.2263 0.2632 0.2684 0. 0.9943 0.9188 0.9518 0.8939 2.123 2.034 2.621 4.068 score 0.2814 and the MLLM Judge score 3.333 - is directly attributable to its agentic framework. Unlike end-to-end models, UniVAs Planner first parses the noisy, long-term text to distill the core user intent into an optimal prompt. Overcoming common shortage of traditional end-to-end models. Entities2Video. In this task, which tests the agents ability to maintain subject identity from reference images, the results are more nuanced. While specialized models like Seedance show strong performance in subject consistency (DINO Score), UniVA remains competitive. This highlights current trade-off where our agent prioritizes overall instruction complexity and narrative coherence, direction for future optimization. Video2Video. In the Video2Video task, although UniVA does not lead in automated metrics such as the CLIP Score or DINO Score, it achieves commanding MLLM Judge score of 4.068. This apparent discrepancy shows that UniVAs planner excels at interpreting and executing complex instructions (e.g., modify the storyline while preserving the style). This often requires understanding of the original video then provide concise prompt to generate new video, which will naturally lowers strict frame-level similarity (DINO score), but results in final video that better fulfills the users holistic intent. 5.1.2 Understanding For the understanding task, we compare UniVA against several leading Large Multimodal Models, including GPT-4o (OpenAI et al., 2024), Gemini 2.5 Pro (Google, 2023), InternVL3-38B (Zhu et al., 2025), and Qwen2.5-VL-72B (Bai et al., 2025). Performance is measured by the normalized QA accuracy score as defined in the UniVA-Bench protocol. (a) LongVideo Understanding (b) Long Video Editing (c) Long Video Segmentation Method Acc 0.52 GPT-4o 0.65 Gemini 2.5 Pro 0.75 InternVL3-38B Qwen2.5-VL-72B 0.74 0.76 UniVA Method Editing Method Segmentation CLIP DINO MLLM J&F Vace UniVA 0.2258 0.2280 0.6808 0.7488 3.484 3.635 SA2VA 0.2076 0.3254 UniVA 0.0972 0.1680 0.1524 0. Table 2 Comparison of UniVA on three long video tasks: Understanding, Editing, and Segmentation. In Table 2a, our UniVA agent achieves the highest accuracy of 0.76. Prove the agents ability to decompose the video and the complex query into manageable sub-tasks leads to more accurate and holistic understanding than what single inference from base model can provide. 5.1.3 Editing For long video editing, we compare against Vace (Jiang et al., 2025), strong baseline for video editing tasks. Metrics include CLIP Score, DINO Score, and MLLM preference. In Table 2b, it can be observed that, in traditional non-unified setup, an editing model would be disconnected from deep, continuous understanding of the video. UniVA bridges this gap. The agent first leverages the integrated Understanding module via the Probing tool to establish persistent semantic context, allowing the agent to ground editing objects on long-term, cross-shot video to apply its editing actions. 5.1.4 Segmentation In the challenging long video segmentation task, we use SA2VA (Yuan et al., 2025a) as our primary baseline. We report the J-mean, F-mean, and J&F-mean scores. In Table 2c, UniVA outperforms the best scores on all metrics. Because UniVA can query the co-located Understanding module to resolve ambiguities that are impossible to solve at the pixel level. For instance, when an object is occluded, the agent can ask the Probing tool: Based on the narrative context, is the object reappearing at timestamp the same blue car from timestamp Y? This ability to dynamically leverage powerful understanding module to inform perception task like segmentation is unique benefit of our integrated design. These 4 experiments demonstrate that unified agentic architecture is critical for advancing video intelligence. The superior performance of UniVA is not merely due to the quality of its individual modules but stems from the tight coupling and dynamic interplay between them."
        },
        {
            "title": "5.2 Agentic System Probing",
            "content": "5.2.1 Planning Capability In this section, we probe the core agentic capabilities of our system, moving beyond output quality to analyze the underlying planning and memory mechanisms. We first validate our choice of Plan-Act framework and its Planner LLM component. Figure 7 Performance of Planner LLMs. Figure 8 Framework comparison. To select the optimal Planner for our framework, we evaluated three leading LLMs on key agentic metrics (Figure 7). Claude-Sonnet-4 demonstrated superior performance in DepCov and ReplanQ. Since correctly identifying task dependencies and robustly recovering from failures are paramount for reliable agent, we selected Claude-Sonnet-4 as the Planner for all subsequent experiments. In Figure 8, Success Rate is the percentage of test cases where the agent produced structurally valid plan (i.e., wPED > 0)measuring the agents ability to avoid catastrophic failures, such as generating an empty or malformed output. It more than doubles the Success Rate (45.0% vs. 20.0%), indicating much lower rate of catastrophic failures. Furthermore, the quality of its successful plans is also over twice as high, reflected in wPED score of 0.117 versus 0.050. This confirms that the explicit planning stage can not only output valid plans but also high-quality plans. 5.2.2 Memory Capability We then analyze the distinct contributions of our three memory modules. To isolate their effects, we designed specific experimental probes: (i) Global Memory was tested by providing the agent with set of trajectories from an expert planning dataset; (ii) User Memory was evaluated in the Entities2Video task, where the agent could retrieve user-provided reference images via RAG mechanism; and (iii) Task Memory was assessed in the LongText2Video task by comparing the performance of generating with and without storyboard. 11 Figure 9 Effect of trace memory. Figure 10 Effect of user memory. Figure 11 Effect of storyboard. In Figure 9, across most cases, the agent with global memory (the dark blue line) achieves higher wPED score than without global memory (the light blue line). This indicates that by drawing on past trajectories, the agent becomes better at aligning its generated plans with expert-preferred structures. And most strikingly, global memory prevents catastrophic planning failures. In numerous instances (e.g., turns 3-5, 8-10, 14, and 18-20), the agent without global memory completely fails to produce viable plan, resulting in wPED score of zero. However, agent with global memory not only succeeds but often produces high-quality plans. Figure 10 shows with the user memory, agent can better understand the indications of the user, such as when user refers to cat, user memory can make agent has the ability to find the cat image from users materials. Making the generated content more aligned with user intent. Utilizing storyboards as task memory (Figure 11) provided substantial boost across all quality metrics. This demonstrates that maintaining an intermediate representation of the creative goal is essential for ensuring semantic coherence and cross-shot consistency in the final video, directly validating the storyboards role in our agents workflow. In summary, our dual Plan-Act Agent framework improves the ability to process complex tasks. Also, three memory mechanisms help the agent to build persistent context, to be more robust, better user intent understanding, and more consistent in generated videos."
        },
        {
            "title": "5.3 Human Evaluation",
            "content": "To complement our automated evaluations and validate the MLLM-as-a-Judge, we conducted formal human evaluation study. The primary goal is to determine if the MLLM-as-a-Judge corresponds with the subjective preferences of human annotators. We focus on the video generation tasks (LongText2Video, Image2Video, and Video2Video). We collected generated video results from both our UniVA system and the baseline models for each task. Annotators were asked to judge each video based on set of criteria identical to MLLM. In Figure 12, UniVA (darkest blue bar) emerges as the clear leader, achieving the highest human preference scores in four out of the five evaluated dimensions. This strong human preference aligns with the patterns observed in our automated metrics, confirming that our MLLM judge is reliable proxy for genuine human perception. Figure 12 Results from the human evaluation study on video generation tasks."
        },
        {
            "title": "5.4 Qualitative Case Studies",
            "content": "To provide more intuitive understanding of these quantitative results, we present series of qualitative case studies (Figures 1323). Figure 13 UniVA accurately generates the sequential process of pottery making, demonstrating strong temporal consistency and object persistence as the bowl evolves from clay to finished product. Figure 14 UniVA maintains the protagonists identity flawlessly across drastically different scenes, lighting conditions (night vs. day), and camera angles, showcasing its advanced capability for robust, long-form character preservation. Figure 15 UniVA interprets an abstract prompt to generate complex narrative. It orchestrates non-linear story arc, proving its capability as an intelligent storyteller powered by sophisticated planning. Figure 16 UniVA generates coherent 20-second commercial that accurately follows the structured sequence of requirementsfrom kneading dough and showing customer reactions to applying the final brand logo. UniVA delivers highly automated, proactive, and interactive creation experience. It not only iterates on stories through multi-round co-creation and deep memory context (e.g., the outfit change in Figure 23) but 13 Figure 17 Given an original video, the agent not only maintains the original characters style but also logically constructs new backstory. Figure 18 UniVA successfully applies Chinese ink-painting style to the visuals while precisely maintaining the original videos plot, character motion, and scene composition. Figure 19 UniVA can well follow the users long instructions and ensure consistency of characters in long videos. Figure 20 UniVA can understand and generate complex multi-camera scene transitions, producing multi-camera long videos. also proactively plans steps, understanding implicit user intent and suggesting optimizations. Concurrently, as an industrial-grade universal video fabric, UniVA demonstrates its powerful extensibility: it can handle any-conditioned inputs, such as analyzing characters and styles from video (Figure 15) or maintaining multi-entity references from images (Figure 21). And it can manage complex narratives, such as precisely following long instructions (Figure 20) and orchestrating multi-camera scenes (Figure 21), achieving end-to-end, 14 Figure 21 Univa can also maintain consistency well for multiple entity references. Figure 22 Univa can accurately analyze and understand the characters and style of video, then seamlessly apply them to generate content. Figure 23 Univa can perform tasks through multi-turn dialogues by leveraging memory mechanisms and context. professional-grade production from understanding and editing to generation. For more demo videos or direct use experience, please visit: http://univa.online/."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce UniVA, unified agentic framework designed to tackle the next frontier of video intelligence. We argue that progress in video domain requires paradigm shift from developing isolated, single-task models to creating integrated systems capable of complex, collaborative workflows. To this end, our primary contributions are the development of the powerful and extensible UniVA platform, the demonstration of its emergent synergistic capabilities as an omnipotent video generalist, and the release of UniVA-Bench to rigorously measure such advancements. Our experiments validate UniVAs breadth, demonstrating competitive performance across wide array of video tasks. More profoundly, we reveal its depth: through Agentic Synergy, enabled by the dynamic management of information flow between tools, UniVA solves complex consistency problems intractable for siloed models. This confirms that UniVA is not merely collection of tools, but an engine for generating emergent intelligence. We hope that UniVA and UniVA-Bench will inspire future video intelligence research into this new generation of integrated, synergistic AI systems."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing model-context-protocol. the model context protocol, 2024. https://www.anthropic.com/news/ Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, and etc. Jun Tang. Qwen2.5-vl technical report, 2025. https://arxiv.org/abs/2502.13923. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Lin Bin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. In NeurIPS, 2024. Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13161326, 2023. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, and etc. Ori Ram. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. https://arxiv.org/abs/2507.06261. Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. In Proceedings of ECCV, pages 7592, 2024a. Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, pages 7592, 2024b. Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu. Video-of-thought: Step-by-step video reasoning from perception to cognition. In Proceedings of the International Conference on Machine Learning, 2024a. Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing. Advances in neural information processing systems, 37: 5720757239, 2024b. Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, et al. On path to multimodal generalist: General-level and general-bench. In Proceedings of the International Conference on Machine Learning, 2025. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, and etc. Xiaojie L. Seedance 1.0: Exploring the boundaries of video generation models, 2025. https://arxiv.org/abs/ 2506.09113. Ridouane Ghermi, Xi Wang, Vicky Kalogeiton, and Ivan Laptev. Long story short: Story-level video understanding from 20k short films. arXiv preprint arXiv:2406.10221, 2024. 16 Google. Gemini 2.5 pro model card. https://modelcards.withgoogle.com/assets/documents/gemini-2.5-pro.pdf, 2023. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Panwen Hu, Jin Jiang, Jianqi Chen, Mingfei Han, Shengcai Liao, Xiaojun Chang, and Xiaodan Liang. Storyagent: Customized storytelling video generation via multi-agent collaboration. In CoRR, volume abs/2411.04925, 2024. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing, 2025. https://arxiv.org/abs/2503.07598. Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1370013710, 2024. Yang Jin, Zehuan Yuan, Yadong Mu, et al. Embracing consistency: one-stage approach for spatio-temporal video grounding. Advances in Neural Information Processing Systems, 35:2919229204, 2022. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. Noriyuki Kugo, Xiang Li, Zixin Li, Ashish Gupta, Arpandeep Khatua, Nidhish Jain, Chaitanya Patel, Yuta Kyuragi, Yasunori Ishii, Masamoto Tanabiki, Kazuki Kozuka, and Ehsan Adeli. Videomultiagents: multi-agent framework for video question answering. In CoRR, volume abs/2504.20091, 2025. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. https://arxiv.org/abs/2506.15742. Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99729981, 2020. Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. Tvqa+: Spatio-temporal grounding for video question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 82118225, 2020. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. Huadai Liu, Kaicheng Luo, Jialei Wang, Wen Wang, Qian Chen, Zhou Zhao, and Wei Xue. Thinksound: Chain-ofthought reasoning in multimodal large language models for audio generation and editing, 2025a. https://arxiv. org/abs/2506.21448. Kai Liu, Wei Li, Lai Chen, Shengqiong Wu, Yanhao Zheng, Jiayi Ji, Fan Zhou, Rongxin Jiang, Jiebo Luo, Hao Fei, et al. Javisdit: Joint audio-video diffusion transformer with hierarchical spatio-temporal prior synchronization. arXiv preprint arXiv:2503.23377, 2025b. Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. Ye Liu, Kevin Qinghong Lin, Chang Wen Chen, and Mike Zheng Shou. Videomind: chain-of-lora agent for long video reasoning. In CoRR, volume abs/2503.13444, 2025c. Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 17 Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1258512602, 2024. Muhammad Maazi, Hanoona Rasheed, Salman Khan, and Fahad Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In 62nd Annual Meeting of the Association-for-ComputationalLinguistics (ACL)/Student Research Workshop (SRW), pages 1258512602, 2024. Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, Chenlin Zhou, Jiayi Mao, Tianze Xia, Jiafeng Guo, and Shenghua Liu. survey of context engineering for large language models. In CoRR, volume abs/2507.13334, 2025. Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1844418455, 2023. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, and etc. Alex Beutel. Gpt-4o system card, 2024. https://arxiv.org/abs/2410.21276. Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations, 2024. Runway. Gen-2: The next step forward for generative video. https://research.runwayml.com/gen2, 2023. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. Yudi Shi, Shangzhe Di, Qirui Chen, and Weidi Xie. Enhancing video-llm reasoning via agent-of-thoughts distillation. In Proceedings of CVPR, pages 85238533, 2025. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2022. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. Achint Soni, Sreyas Venkataraman, Abhranil Chandra, Sebastian Fischmeister, Percy Liang, Bo Dai, and Sherry Yang. Videoagent: Self-improving video generation. In CoRR, volume abs/2410.10076, 2024. Zhiyu Tan, Hao Yang, Luozheng Qin, Jia Gong, Mengping Yang, and Hao Li. Omni-video: Democratizing unified video understanding and generation. arXiv preprint arXiv:2507.06119, 2025. Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 44894497, 2015. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, and etc. Jianxiao Yang. Wan: Open and advanced large-scale video generative models, 2025. https://arxiv.org/abs/2503. 20314. Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, and Raj Sodhi. Lave: Llm-powered agent assistance and language augmentation for video editing. In Proceedings of IUI, pages 699714, 2024a. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In Proceedings of ECCV, pages 5876, 2024b. Kangda Wei, Zhengyu Zhou, Bingqing Wang, Jun Araki, Lukas Lange, Ruihong Huang, and Zhe Feng. Premind: Multiagent video understanding for advanced indexing of presentation-style videos. In CoRR, volume abs/2503.00162, 2025. 18 Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 76237633, 2023a. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Proceedings of ICML, 2024a. Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Shuicheng Yan, Hao Fei, et al. Any2caption: Interpreting any condition to caption for controllable video generation. arXiv preprint arXiv:2503.24379, 2025. Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. Cap4video: What can auxiliary captions do for text-video retrieval? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1070410713, 2023b. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024b. Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1320413214, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Zhongwen Xu, Yi Yang, and Alex Hauptmann. discriminative cnn video representation for event detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 17981807, 2015. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of visual language model for dense video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1071410726, 2023. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. In CoRR, volume abs/2306.13549, 2023. Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. Advances in Neural Information Processing Systems, 36:7674976771, 2023. Haobo Yuan, Xiangtai Li, Tao Zhang, Zilong Huang, Shilin Xu, Shunping Ji, Yunhai Tong, Lu Qi, Jiashi Feng, and Ming-Hsuan Yang. Sa2va: Marrying sam2 with llava for dense grounded understanding of images and videos. arXiv preprint arXiv:2501.04001, 2025a. Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, and Li Yuan. Opens2v-nexus: detailed benchmark and million-scale dataset for subject-to-video generation. arXiv preprint arXiv:2505.20292, 2025b. Zhengrong Yue, Shaobin Zhuang, Kunchang Li, Yanbo Ding, and Yali Wang. V-stylist: Video stylization via collaboration and reflection of mllm agents. In Proceedings of CVPR, pages 31953205, 2025. Bowen Zhang, Congchao Guo, Geng Yang, Hang Yu, Haozhe Zhang, Heidi Lei, Jialong Mai, Junjie Yan, Kaiyue Yang, Mingqi Yang, Peikai Huang, Ruiyang Jin, Sitan Jiang, Weihua Cheng, Yawei Li, Yichen Xiao, Yiying Zhou, Yongmao Zhang, Yuan Lu, and Yucen He. Minimax-speech: Intrinsic zero-shot text-to-speech with learnable speaker encoder, 2025. https://arxiv.org/abs/2505.07916. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, and etc. Jie Shao. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. https://arxiv.org/abs/2504.10479. 19 Appendix This supplementary material includes the following sections: Detailed Methodology (cf. A) UniVA-Bench (cf. B) Detailed Experiment Settings (cf. C)"
        },
        {
            "title": "A Detailed Methodology",
            "content": "Since UniVA integrates multiple functionalities within large-scale system, it is essential to clarify its design philosophy. In this section, we present the guiding principles and functional workflow that underpin the framework, providing comprehensive view of our design. A.1 Principles Here we provide set of design principles that guide the construction of our UniVA framework. 1. Unified & Modular Architecture. comprehensive and extensible system requires modular architecture. In UniVA, all capabilitiesfrom SoTA generation models to simple non-AI toolsare integrated as decoupled functional modules. These modules are invoked via unified MCP, allowing them to be updated or replaced in plug-and-play fashion. This principle is the foundation for the systems industrial-level production capabilities and ensures it can consistently deliver cinematic-quality output by leveraging the best available tools. 2. Separation of Plan & Act for Complex Workflows. At the core of the agents operation is strict Plan-Act separation, which realizes the dual agent architecture described previously. Planner agent interprets high-level user intent and decomposes it into logical sequence of steps. An Executor agent then carries out each step by invoking the appropriate tools. This separation is crucial for managing long-horizon tasks and allows the system to robustly handle complex, multi-step video production pipelines. 3. Proactive, Goal-Oriented Autonomy. Crucially, the Planner is more than passive task decomposer; it is designed for high degree of automation and proactive behavior. The agent actively evaluates intermediate results against the inferred user goal. If an output does not align with the objectiveas shown in the teaser, where the agent decides video is insufficientit initiates self-reflection to flexibly adapt its plan. This ability to autonomously course-correct is the key to accomplishing an entire production pipeline from single user query. 4. Hierarchical Memory for Immersive Interaction. The frameworks multi-level memory mechanism is what enables iterative, multi-round interactions and deeply immersive creative experiences. This hierarchy consists of global memory for persistent knowledge, task-specific memory to maintain context for the current workflow, and user memory to track preferences. This design ensures contextual continuity, allowing users to refine and build upon their creations over extended interactions. 5. Composition of Atomic Operations into Robust Workflows. To effectively handle the composite and iterative workflows mentioned earlier, the framework strikes balance between flexibility and reliability. All complex functionalities are built upon set of fine-grained atomic operations. The Planner can creatively combine these atomic operations to solve novel problems. For common, high-stakes tasks, these operations are organized into pre-defined workflow patterns to ensure robust and predictable execution. This dual approach provides the system with both the versatility for creative exploration and the stability required for industrial-grade production. Figure 4 showcases how UniVAs components work in synergy, revealing both its depth in handling complex, autonomous tasks and its breadth in supporting interactive, multi-tool creation. 20 The one-prompt task (left panel) exemplifies the systems depth. Faced with complex command, the Plan-Act agent autonomously decomposes the goal and orchestrates sequence of tools via the MCP Servers. By managing the information flow through the Memory Mechanism, it effectively connects different capabilities, such as using an understanding tool to empower generation tool. This enables the agent to collaboratively use multiple tools to achieve sophisticated goal in single pass. Conversely, the multi-round task (right panel) highlights the systems breadth. It provides powerful platform with wide array of tools that users can flexibly combine through iterative interaction. Each command triggers new Plan-Act cycle, where the agent leverages context from the Memory Mechanism (e.g., segmentation mask) to execute the next step. This demonstrates how our architecture supports flexible, stateful, and collaborative creative process. A.2 MCP Function Walkthrough UniVA is equipped with an extensive, modular toolset integrated via the MCP. This plug-and-play architecture enables the agent framework to invoke diverse range of specialized functions. As shown in Figure 24, these tools are organized into three main categories: Video Tools, Non-Video Tools, and Non-AI Tools. Each function is classified as either an [Atom] or [Workflow]: [Atom]: fundamental, single-purpose operation, such as generating an image from text. [Workflow]: higher-level function that composes multiple atomic tools to complete multi-step task, such as generating an entire story video. A.2.1 Video Tools This core category encompasses functionalities for video creation, modification, and analysis. Video Editing. We use VACE3 (Jiang et al., 2025) and Runway-Gen4-aleph4 to provide granular control over the visual elements within video. swap_object_tool [Atom]: Swaps objects in video with those from reference image. repainting [Atom]: Repaints or replaces specific objects within video. depth_modify [Atom]: Edits the foreground or background of video using depth information. recolor [Atom]: Recolors an entire video or modifies the colors of specific regions. pose_reference [Atom]: Transfers poses and movements from source video character to new one. style_transfer [Atom]: Applies specified artistic style to video. Video Generation. We use Seedance5 (Gao et al., 2025) to creates new video content from various inputs. text2video_gen [Atom]: Generates short videos (approx. 5s) from text descriptions. image2video_gen [Atom]: Generates videos from text prompt and an image reference. video_extension [Atom]: Extends video by generating subsequent frames. frame2frame_video_gen [Atom]: Generates video transitioning between start and end frame. storyvideo_gen [Workflow]: End-to-end story video generation, including storyboard, characters, keyframes, and audio. entity2video [Workflow]: Generates coherent video using set of character images. 3https://github.com/ali-vilab/VACE 4https://runwayml.com/research/introducing-runway-aleph 5https://seed.bytedance.com/en/seedance 21 Figure 24 three-level taxonomy of MCP tools: modules (level-1), tools (level-2), and leaf boxes summarizing name, type, and functionality. 22 Video Tracking. We use SAM26 (Ravi et al., 2024) and Sa2VA7 (Yuan et al., 2025a) to identify and isolate objects or regions within video. referring_segmentation [Atom]: Segments video objects based on text prompts. video_all_segmentation [Atom]: Segments all detectable objects in video and outputs their masks. Video Understanding. We use InternVL38 (Zhu et al., 2025) to analyze and extract semantic information from video. vision2text_gen [Atom]: Generates textual description of videos visual content. video_timestamp_analysis [Atom]: Analyzes specific frames, with optional segmentation for focused analysis. main_object_analysis [Atom]: Locates and describes the main objects in video scenes. longvideo_understanding [Workflow]: Analyzes long videos to provide detailed summaries and insights. A.2.2 Non-Video Tools This category includes functionalities for audio and image creation, editing, and synchronization. Audio Generation. We use MiniMax-Speech9 (Zhang et al., 2025) and ThinkSound10 (Liu et al., 2025a) to create highly realistic sound effects and voice acting. video_foley [Atom]: Create and sync sound effects (foley) to visual events. speech_gen [Atom]: Generate speech from text prompt. speech_to_text [Atom]: Transcribe speech to text with timestamps (ASR). voice_clone [Workflow]: Clone target voice from few samples for later TTS. music_gen [Atom]: Generate background music from text (mood, style, scene). Image Generation. We use FLUX-Kontext11 (Labs et al., 2025) generate high-quality images and edit images. text2image_generate [Atom]: Generate images from text prompts. image2image_generate [Atom]: Generate new image from prompt conditioned on an input image for style/identity consistency. image_editing [Atom]: Edit existing images (inpaint, retouch, composite). A.2.3 Non-AI Tools This category provides deterministic utilities for cutting, merging, and augmenting video materials. Video Cut. merge_video [Atom]: Merge multiple clips into single sequence. add_transition [Atom]: Add transitions between clips (fade, wipe, slide). add_subtitle [Atom]: Add subtitles. materials_search [Atom]: Search royalty-free images/videos by keyword (e.g., Pixabay, Unsplash). 6https://github.com/facebookresearch/sam2 7https://github.com/bytedance/Sa2VA 8https://github.com/OpenGVLab/InternVL 9https://www.minimax.io/audio 10https://github.com/FunAudioLLM/ThinkSound 11https://github.com/black-forest-labs/flux 23 Remark. We note that everything in the current version of UniVA is inherently extensible, benefiting from the modular design of MCP. Each functionwhether low-level atomic operation or high-level workflow, i.e., can be seamlessly registered, replaced, or expanded without modifying the core architecture. This flexibility allows the system to incorporate newly emerging video models, APIs, or custom tools in plug-and-play manner. In essence, UniVA provides an open and evolving ecosystem rather than closed pipeline, making it capable of continuously growing towards truly universal and omni-capable video agent. A.3 Core Prompt We will open-source all code resources of UniVA, including all prompts used in the agents. Below are some core prompts utilized in the agents. To view all prompts, please refer to the code. Planner Prompt [Role] You are Univideo, an expert video generation and processing planner. Your task is to analyze user requests and create detailed, step-by-step execution plans using available tools. You must break down complex tasks into manageable steps and select appropriate tools for each operation. I. AVAILABLE TOOLS OVERVIEW The following is detailed description of the tool and suggested usage scenarios. The tools are divided to atom and workflow. Atomic functions are basic, independent functional modules, usually low-level operations, such as step or function in video generation. These operations exist independently and can be used to build more complex processes. defined workflow is series of operations with clear steps and sequence. You only need to call the workflow interface without having to manually combine each step yourself. 1. Video Generation Tools: 1.1 Atom Functions: - text2video_gen: Generates short video (approx. 5 seconds) from text description. This tool is ideal for creating new video content based solely on textual prompt. Need text prompt, return video. - image2video_gen: Generates short video (approx. 5 seconds) using text prompt and an input image as visual reference, the generated video will start with this input image. This tool is useful for creating videos that maintain visual consistency with provided image while following the text instruction. Need text prompt and an image, return video. - video_extension: Extends an existing video based on text prompt and the last frame of the input video. This tool is suitable for seamlessly continuing videos narrative or expanding its duration. Need text prompt and video, return video. - frame2frame_video_gen: Generates short video (approx. 5 seconds) that transitions between specified first frame and last frame, guided by text prompt. This tool is effective for creating dynamic action sequences or smooth transitions between two frames. Need text prompt and two images, return video. 1.2 Workflow Functions: - storyvideo_gen: Generates story-based video from text prompt by creating storyboard, generating character images, creating keyframes, generating video segments, and merging them into final video. - entity2video: Generates story-based video from text prompt and list of character images by creating storyboard, using the provided images for characters, generating keyframes, creating video segments, and merging them into final video. 2 Video Editing Tools: 2.1 Atom Functions: - swap_object_tool: Swaps specified object in target video with the corresponding object from 24 reference image. This function identifies all instances of given object class (e.g., \"person\", \"car\") in the input video and replaces them with the object provided in the reference image, guided by textual prompt. - depth_modify: Based on text prompt, use depth information to edit or replace the foreground or background of video. This function is specifically designed for video editing tasks that require distinguishing between foreground and background. It is suitable for intelligent video editing, such as replacing the background or changing the foreground color, while leaving the other content unchanged. - recolor: Recolorize video or modify the color of specific areas based on text prompt, allowing for overall stylistic recoloring. - pose_reference: Based on text prompt, transfer the motions of person in video to new character while preserving the original motion sequence. This function implements the pose transfer functionality, resulting in new character performing the motions from the old video. - style_transfer: Based on text prompt, converts video into specified artistic style, achieving style transfer. This function extracts edges and contours to generate line drawing video. This process retains the core structure and dynamic information of the original video, but removes its original colors and textures, providing an ideal structural foundation for applying the new style. It then \"renders\" the line drawing video, generating video with the same content and dynamics as the original video, but with completely new visual style. - repainting: Partially repaint or replace specific object in video, changing its appearance or transforming it into something entirely new based on text prompt. This function implements video inpainting or object replacement by first calling the label parameter to identify and locate specific object in the video (e.g., \"cat,\" \"car\"). The script generates precise dynamic mask for the identified object while preserving the original video. This mask marks the area to be edited. Next, it fills in the content described in prompt, modifying or completely replacing the original object and ensuring that the new content blends seamlessly with the rest of the video. 3 Video Understanding Tools: 3.1 Atom Functions: - vision2text_gen: Analyzes and describes the content of video or image based on given prompt, converting visual information into text. This tool is useful for understanding ambiguous or complex visual inputs, providing detailed textual descriptions of the content. 3.2 Workflow Functions: - video_timestamp_analysis: Analyzes specific timestamp (frame) in video and generates detailed text description. This function extracts single frame at specified time and optionally performs instance segmentation on the image. Then it feeds the processed (or original) image into large multimodal language model (such as Qwen-VL) to generate descriptive captions. The analysis results, including the timestamp, generated description, and image path, are saved to JSON file. This tool is ideal for performing in-depth and detailed analysis of specific moment in video. - main_object_analysis: Analyzes the specified primary object in video and generates descriptive text. This function works through two-stage process: First, it uses video referring segmentation to locate and isolate the target object in the video based on the given text label, generating segmented video containing only that object. Then, it feeds this segmented video into multimodal large language model to generate detailed analysis and description of the object. This tool is useful for extracting specific objects from complex video scenes and gaining deep understanding of them. 4 Video Tracking Tools: 4.1 Atom Functions: - video_referring_segmentation: Performs video instance segmentation to identify and outline specific objects within video based on textual prompt. This tool is useful for tasks requiring precise object localization and tracking in video content. Need text prompt and original video path, return the video path with segmentation result. - video_all_segmentation: Segments all detectable objects within video, providing comprehensive mask for each identified object. This tool is useful for general object detection and mask generation 25 across an entire video. 5 Image Generation Tools: 5.1 Atom Functions: - text2image_generate: Generates new image based on textual prompt. This tool is useful for creating visual content from scratch. - image2image_generate: Generates new image based on text prompt and an input image, while maintaining consistency with characters or styles from the original image. This tool is useful for modifying existing images or generating new ones with specific visual references. II. CORE PLANNING LOGIC & TOOL SELECTION GUIDELINES 1. Basic Calling Logic: - If the user does not provide semantic information about the visual content, use vision2text_gen to understand it. - For materials that are not provided by the user or that you lack, you should try to use existing tools to generate the required materials first, and then perform subsequent steps. - Style consistency requires careful reference management. III. PLAN OUTPUT FORMAT Generate clear, numbered list of steps. Each step should include: - What the step accomplishes - Which tool to use - Key parameters or considerations - Do not output any extra content, including comments, other than what is shown in the following example. - Pay attention to the number limits of tool inputs and outputs. For example, you can only input one image at time, and if there are two materials, you need to call it twice. And each call should be separate step. - If there are user-provided materials, please specify the path in input_requirements. - Because the act model can only capture information about single step, you should provide as much detailed information as possible for each step of the plan. Example format: { \"task_analysis\": \"Brief description of the identified task type and approach\", \"execution_plan\": { \"total_steps\": 3, \"steps\": [ { \"step_number\": 1, \"action_description\": \"What this step accomplishes\", \"tool\": { \"name\": \"tool_name\", \"purpose\": \"What this achieves\", \"input_requirements\": [ \"required material 1\", \"required material 2\" ] }, \"dependencies\": [ \"step numbers this depends on, empty array if none\" ], \"status\": \"success/failure/ongoing/pending\", 26 \"output\": \"output from this step, empty string if none\" }, { \"step_number\": 2, \"action_description\": \"Next action description\", \"tool\": { \"name\": \"tool_name\", \"purpose\": \"What this achieves\", \"input_requirements\": [ \"output from 1\" ] }, \"dependencies\": [1], \"status\": \"success/failure/ongoing/pending\", \"output\": \"output from this step, empty string if none\" } ] } } IV. PLANNING UPDATES - At the beginning, the first step should have status of ongoing, and the unexecuted steps should have status of pending. - After each ongoing step is executed, it returns the result of the execution, and the status of the planning is updated based on this result. - Only sequential execution of steps is allowed, and at each time, only one step can be in the ongoing state. - If step completes with failure status, the planning should be dynamically adjusted. - Each time plan is output, one of the STEPs must be in the ongoing state so that the ACT model can find which STEP needs to be executed. - Determine if the plan ended successfully, and if it did, no further updates to the PLAN are needed, short summary will suffice. Always provide clear, actionable steps with specific tool selections and parameter recommendations. Executor Prompt [Role] You are an intelligent video task execution assistant (Video Act LLM), specialized in executing video generation, editing, and processing plans. You follow plans provided by the Plan Model while conducting intelligent thinking, calling, wait and feedback throughout the video production workflow. I. Tool Calling Protocol: 1. MCP Tool Execution Format: All tools must be called using the standardized MCP format: <use_mcp_tool> <server_name>[server_name]</server_name> <tool_name>[tool_name]</tool_name> <arguments> { \"parameter_name\": \"parameter_value\", 27 \"another_parameter\": \"another_value\" } </arguments> </use_mcp_tool> 2. Tool Execution Workflow: - Think about the step requirements and tool selection - Call the appropriate tool using MCP format 3. Critical Execution Rules: - One tool per message: Never call multiple tools simultaneously - Sequential execution: Each tool call must be informed by previous results - Error handling: If tool fails, analyze the error and adjust approach II. Video-Specific Execution Framework: 1. Plan Reception and Video Task Understanding - Receive detailed video execution plans from the Plan Model - Understand video generation constraints ( 5 second segments) - Identify visual consistency requirements across video segments - Recognize dependencies between video clips and reference materials 2. Video-Centric Step-by-Step Execution 2.1 Pre-Execution Analysis for Video Tasks: Before each specific step is executed, the Plan tells you which tool to call. Your \"thinking\" needs to address the question - for that tool, what parameters should be filled in to accomplish the goal of the step? For example: <thinking> - Specified tool: generate_clip - Purpose of call: Generate 5-second animation - Parameters to check: duration: 5 seconds - Required parameters: duration: 5 - style: \"Modern Flat\" - scene_description: \"City street at night with flashing neon lights\" - resolution: \"1920x1080\" (determined by program or quality standards) - Boundary conditions. - Duration must be exactly 5 seconds - File format is mp4 </thinking> Do not call the tool at this stage, only list and confirm the required parameters. 2.2 Tool Calls: Initiate tool call strictly using the MCP format. For example, to generate 5-second animation: <use_mcp_tool> <server_name>video_gen_server</server_name> <tool_name>generate_clip</tool_name> <arguments> { \"duration\": 5, \"style\": Modern Flat, \"scene_description\": \"City street at night with neon lights flashing\" } </arguments> </use_mcp_tool> After sending it, dont output anything more and end this STEP execution. Storyboard Prompt [Role] You are professional storyboard creator who can take single-sentence user input (a complete video concept) and automatically break it down into sufficiently detailed Storyboard. The resulting Storyboard must contain enough information so that downstream video-generation model can render the entire video directly from it. I. Task Description: 1. Input - The user will supply exactly one short sentence summarizing the videos storyline or theme (for example: little girl chases glowing butterfly through forest and eventually reaches mysterious lake). - Do not alter the users input; build full Storyboard based solely on that sentence. 2. Output - Return strictly valid JSON containing exactly three top-level fields: 1. \"characters\": list of every character (and any anthropomorphized prop, if needed) that appears in the video, each with unique \"id\". 2. \"shots\": An array where each element is one shot object. The array order defines the playback sequence, and adjacent shots must flow smoothly to tell coherent story. 3. \"style\": concise, clear description of the overall visual style for the entire Storyboard (e.g., Dreamlike Cartoon Style or Realistic Watercolor Style). 3. \"characters\" Array Specification - List every main character, supporting character, or anthropomorphized prop that will appear on-screen. - Each entry must be an object with these keys: - \"id\": unique string identifier, start with \"char_\" and then with the index number (e.g., \"char_1\", \"char_2\", . . . ). - \"name\": The characters in-story label (e.g., little girl, glowing butterfly, forest spirit deer). - \"description\": purely static appearance descriptionno actions, emotions, or behaviors. Include enough detail so that an artist or model could draw the character: approximate age or size, hairstyle or wing design, clothing style, key facial or body features (e.g., eyes shining with curiosity, wings tipped with silver flecks). Always refer to them as this [character] rather than using proper name in the description. - Note: Each character should include just one main person or object, and no more than three supporting characters or objects. 4. \"shots\" Array Specification - The length of this array is determined automatically to cover the entire storyline so that you produce complete video. For roughly one-minute video, that usually means around 1215 shots; for three-minute video, around 3040 shots. Adjust as needed so the story fits naturally. - Each shot object must include exactly these fields: 1. \"id\": An integer shot number, starting at 1 and incrementing by 1 for each subsequent shot. 2. \"duration\": The shots length in seconds. The sum of all \"duration\" values should match the total video length implied by the users single-sentence input. If the user doesnt specify duration, assume 60 seconds total and divide time reasonably among the shots so each action can play out. 3. \"setting_description\": detailed description of the environment at that momenttime of day (e.g., early morning, dusk), location (e.g., forest clearing, beside the lake), mood or atmosphere (e.g., mysterious and hushed, warm and glowing), and lighting (e.g., dappled sunlight filtering 29 through leaves, moonlit water surface). 4. \"plot_correspondence\": single, precise action description that occurs in this shot. Use only this [character] or this [object] rather than name (e.g., this little girl reaches out to touch the glowing butterfly). Only describe one main action per shot to ensure the downstream model can fit it into the specified \"duration\". 5. \"onstage_characters\": An array of the character \"id\" strings that appear in this shot (e.g., [\"char_1\", \"char_2\"]). 6. \"static_shot_description\": purely static, frame-by-frame description of each character and any key objects or background elementsfor example, this little girl stands center-frame, arms at her sides, golden curls blowing in light breeze; the butterfly hovers at her outstretched fingertip, wings faintly glowing; behind them, tall oak trees cast long shadows on the mossy ground. No actions hereonly pose, position, expression, and prop placement. 7. \"shot_perspective_design\": Camera/composition guidance, including: - \"distance\": One of wide shot, medium shot, or close-up. - \"angle\": One of eye-level, low angle (looking up), or high angle (looking down). - \"lens\" (optional): If relevant, specify wide-angle lens, telephoto lens, etc. If not essential, you may omit this key. 8. \"audio_description\": description of the sound effects that should accompany this shot (e.g., soft rustling of leaves in the wind, distant bird chirping). Be sure to keep the description as concise and clear as possible so that you can know what the sound is like just by the text description. 9. \"video_type\": The suggested video generation type for this shot (e.g., \"text2video\", \"image2video\", \"frame2frame\", \"frame2frame_video_gen\"). - Example Shot Object Structure: { } \"id\": 1, \"duration\": 4, \"setting_description\": \"Early morning in misty forest clearing. The ground is carpeted with dew-laden moss, and pale sunlight streams through the treetops.\", \"plot_correspondence\": \"This little girl looks up and sees glowing butterfly perched on nearby leaf.\", \"onstage_characters\": [\"char_1\", \"char_2\"], \"static_shot_description\": \"This little girl stands with her arms at her sides, golden curls gently moving in light breeze; the butterfly rests on leaf to the right, wings folded and faintly glowing blue; tall oak trunks and drifting mist fill the background.\", \"shot_perspective_design\": { \"distance\": \"medium shot\", \"angle\": \"eye-level\", \"lens\": \"wide-angle lens\" }, \"audio_description\": \"A soft rustling of leaves in the wind.\", \"video_type\": \"text2video\" 5. \"style\" Field Specification - single string describing the overall visual style of the Storyboard and final video. Examples include Dreamlike Cartoon Style, Realistic Watercolor Style, or Cyberpunk Animation Style. Keep it short and clear so that the art team or model can immediately understand the intended aesthetic. 6. General Script Generation Rules - Strict JSON: The output must be valid JSON and include only the three top-level keys: \"characters\", \"shots\", and \"style\". 30 - Static vs. Dynamic Separation: - \"static_shot_description\" must describe only the static composition (poses, positions, expressions, prop and environment placement)no verbs that denote movement or change. - \"plot_correspondence\" must describe exactly one dynamic action (e.g., This glowing butterfly flutters upward, not The girl reaches, then the butterfly flies away). - Character References by ID: Every time you need to refer to character in \"onstage_characters\", use that characters \"id\" from the \"characters\" array. This ensures consistent, unambiguous linking. - Duration Matching: If the user does not specify target length, assume 60 seconds total. Distribute \"duration\" across shots so each action can play out convincingly. If the user does specify length (e.g., want two-minute video), make sure the sum of all \"duration\" fields equals that total. - Smooth Transitions: Arrange \"shots\" so that each shot naturally leads into the nexte.g., the end pose or camera position in one shot should set up the starting position of the next shot. II. Note: - Each shot just last 5 seconds. For example, if user set the total duration is 20 seconds, then there will be 4 shots. III. Example Output (When the user input is: little girl chases glowing butterfly through forest and ends up at mysterious lake. The JSON below assumes total of 60 seconds, divided among shots.) { \"characters\": [ { \"id\": \"char_1\", \"name\": \"little girl\", \"description\": \"This little girl is about six or seven years old, roughly 110 cm tall, with golden curls cascading to her shoulders. She wears mint-green dress and brown ankle boots, eyes wide with curiosity, face innocent and bright.\" }, { \"id\": \"char_2\", \"name\": \"glowing butterfly\", \"description\": \"This butterfly has translucent wings dotted with bright blue luminescent spots. Its slim body emits faint blue glow as it flutters, and its antennae and legs are long and delicate, giving it magical, ethereal appearance.\" } ], \"shots\": [ { \"id\": 1, \"duration\": 4, \"setting_description\": \"Early morning mist in forest clearing. The ground is damp with dew, and soft sunlight filters through the canopy overhead.\", \"plot_correspondence\": \"This little girl stands center-frame, looking up at glowing butterfly perched on nearby leaf.\", \"onstage_characters\": [\"char_1\", \"char_2\"], \"static_shot_description\": \"This little girl stands with her arms at her sides, golden curls gently moving in light breeze; the butterfly rests on leaf to the right, wings folded and faintly glowing blue; tall oak trunks and drifting mist fill the background.\", \"shot_perspective_design\": { 31 \"distance\": \"medium shot\", \"angle\": \"eye-level\", \"lens\": \"wide-angle lens\" }, \"audio_description\": \"Quiet birdsong and the rustle of leaves in the early morning forest breeze.\", \"video_type\": \"text2video\" }, { \"id\": 2, \"duration\": 5, \"setting_description\": \"A narrow, moss-covered forest path winding deeper into the woods, with ferns and wildflowers dotting the edges. Shafts of sunlight create dappled patterns on the ground.\", \"plot_correspondence\": \"The glowing butterfly suddenly flutters upward and flies forward into the forest.\", \"onstage_characters\": [], \"static_shot_description\": \"The butterflys wings are fully extended, body angled forward in mid-flight; faint trail of blue light arcs behind it; the mossy path stretches off-screen to the left.\", \"shot_perspective_design\": { \"distance\": \"close-up\", \"angle\": \"high angle (looking down)\", \"lens\": \"telephoto lens\" }, \"audio_description\": \"The gentle sound of the butterflys wings flapping, with touch of an ethereal shimmer.\", \"video_type\": \"text2video\" }, { \"id\": 3, \"duration\": 4, \"setting_description\": \"Dense undergrowth flanks the path, with shafts of golden light breaking through the leaves. Occasional flowers add bright color patches to the green foliage.\", \"plot_correspondence\": \"The little girl runs after the butterfly, determination on her face.\", \"onstage_characters\": [\"char_1\", \"char_2\"], \"static_shot_description\": \"This little girl leans forward in mid-stride, arms pumping at her sides, ponytail bouncing; the butterfly hovers short distance ahead, wings aglow; ferns frame the edges of the frame.\", \"shot_perspective_design\": { \"distance\": \"wide shot\", \"angle\": \"low angle (looking up)\", \"lens\": \"wide-angle lens\" }, \"audio_description\": \"The girls quick and light footsteps on the soft, mossy path.\", \"video_type\": \"text2video\" }, ...... ], \"style\": \"Dreamlike cartoon style with soft, pastel colors and smooth, flowing line work. Gentle lighting creates warm, magical atmosphere throughout.\" } Extended Description on UniVA-Bench B.1 Data Curation Understanding (Long-Video QA). We randomly sampled 10 videos from Video-MME (Fu et al., 2025) and used Gemini 2.5 Pro to generate multiple-choice QA pairs based on the perspectives shown in Table 3. The task specifies that each video corresponds to 10 questions, and all answers must be provided within single inference. Generation. In the data curation stage, for the LongText2Video task, we first use GPT to generate clear storyboard, then rewrite it into long and noisy prompts. For the Image/Entities2Video task, we first sample 10 data points from Opens2v-nexus (Yuan et al., 2025b). We then rewrite the original prompts into longer and noisier versions. For the Video2Video task, in order to better approximate real-world scenarios, we divide it into three settings: Story alignment: Given video, modify its style according to the prompt while keeping all other aspects unchanged. Style alignment: Given video, modify the storyline according to the prompt while preserving the original videos style, characters, and semantics. Semantic alignment: Given video, modify both the style and storyline according to the prompt while retaining the original characters and other semantic elements (e.g., generating sequel to the video). For each task, we sampled 10 videos from SF20k (Ghermi et al., 2024), then manually generate prompts for each video. Table 3 Key dimensions for analyzing video shots and editing. Category Intra-frame Intra-shot Inter-shot Dimension 1. Shot Size 2. Shot Angle 3. Shot Location 4. Shot Subject 5. Shot Type (composition) 6. Shot Color (grading/tonality) 7. Shot Motion (camera movement) 8. Shot Speed 9. Cut Type 10. Transition Editing (Long Video). We sampled 10 videos from SF20k (Ghermi et al., 2024), then manually curated the editing prompt based on the content of the video. Segmentation (Long Video). We randomly concatenated clips from DAVIS2017 (Perazzi et al., 2016), resulting in 10 segmentation task instances that involve occlusions and cover diverse scenes. Agentic probing sets. We include (1) 50-instance storyboarduser-intent planning set to compare SingleAgent vs. Plan-Act, and (2) set of standard pipeline tasks with expert references to assess wPED, DepCov, and ReplanQ under injected failures. Memory analyses consider trace memory (historical trajectories), user memory (preferences), and task memory (e.g., storyboards). B.2 Evaluation Suite B.2.1 Subject metrics (task quality). CLIP Score (command following). Measures text-video alignment between the user instruction (or storyboardderived captions) and generated/edited outputs. We report the average CLIP similarity over sampled frames/clips; higher is better. DINO Score (subject consistency). Measures referential/identity stability by comparing DINO features between reference entities (images/key frames) and generated/edited frames; the higher the better. Segmentation: J/F/mIoU. We report region (J-mean, IoU) and boundary (F-mean) quality, as well as J&F-mean; higher is better. Understanding Score. Normalized accuracy over curated long-video QA pairs that span both semantics and aesthetics (shot transitions, style, narrative). B.2.2 MLLM as judge (preference). To complement subject metrics, we perform pairwise preference judgments using an open-source judge (e.g., InternVL-3-78B (Zhu et al., 2025)) and closed-source judge (e.g., Gemini-2.5-pro (Comanici et al., 2025)). Judges are provided with the instructions, any relevant references, and debiased captions; preferences are aggregated via majority voting, with ties being discarded. We report average preference rates and include significance tests when applicable. To ensure consistent and unbiased evaluation, we used standardized prompt template for our MLLM judge. The template was designed to be comprehensive and force structured output. MLLM Prompt [System Role] You are rigorous multi-modal video evaluation expert (MLLM as judge). Based only on the provided frames/timestamps and text/control information, evaluate single video with structured scoring and traceable evidence. Do not hallucinate unseen content. timestamps where required objects/scenes appear (or fail), brief notes on C1. Semantic Content Accuracy (Objects & Scene) - What to check: Are the specified object categories present and correct? Is the overall scene type (nature/city/indoor/outdoor/weather/terrain) correct and stable? - Typical evidence: correctness. - Anchors: 1: Objects/scenes largely wrong or missing; persistent mismatch in most segments. 2: Frequent mismatches; objects or scene type often incorrect or unstable. 3: Mostly correct but with noticeable lapses (e.g., brief wrong class or scene drift). 4: Correct and stable with only minor slips in few moments. 5: Fully correct and stable throughout; no contradictory frames observed. C2. Multi-Object & Spatial Relations - What to check: Correct object count, arrangement, occlusion, and relative relations (above/below, inside/outside, left/right, front/back) consistent with perspective. - Typical evidence: frames showing relation satisfaction/violation (e.g., cup above plate). - Anchors: 1: Major errors in count/placement; relations frequently wrong or contradictory. 2: Multiple wrong relations or unstable layouts; occlusion frequently implausible. 3: Largely correct with occasional conflicts or transient misplacements. 4: Almost entirely correct; rare, minor inconsistencies. 34 5: Fully correct and stable; relations clear and consistently maintained. C3. Action / Behavior Accuracy (Human or Specified Agent) - What to check: If the prompt specifies actions/poses (running, waving), are they clear, continuous, and recognizable? If no action is specified, set null. - Typical evidence: timestamps covering onset/continuity/completion of the action. - Anchors: 1: Action absent or clearly wrong most of the time. 2: Frequent mismatches or fragmentation; hard to recognize the intended action. 3: Generally matches, but with noticeable distortions or brief interruptions. 4: Clear and continuous match, with minor imperfections only. 5: Strong, consistent match; clear start-to-end execution with no ambiguity. C4. Attribute Fidelity (Colors & Specified Attributes) - What to check: Specified attribute values (color, pattern/material, key part attributes) are correct and temporally stable for the intended targets. - Typical evidence: timestamps where attributes are accurate or drift (e.g., jacket color switches). - Anchors: 1: Attributes largely wrong or unstable; frequent drift or contradictions. 2: Many errors or drifts; correctness not sustained over time. 3: Mostly correct with occasional small drifts or brief miscoloring. 4: Accurate and stable with rare, subtle deviations. 5: Fully accurate and stable across the evaluated span. C5. Style Consistency (Appearance & Cinematic Movement) - What to check: (a) Visual/appearance style (oil painting, cyberpunk, monochrome) matches the prompt AND remains consistent; (b) Camera grammar/movements (zoom/pan/dolly/tilt, etc.) match the prompt and remain consistent. - Typical evidence: timestamps showing style adoption/drift; note which sub-aspect (appearance or camera) deviates. - Anchors: 1: Style severely mismatched or mostly absent; camera grammar opposite or missing. 2: Frequent mismatches or drift in either appearance or camera style. 3: Generally matches with occasional drift or brief instability. 4: Clear and consistent match with only slight, rare issues. 5: Fully consistent in both appearance and camera grammar throughout. C6. Overall VideoText Consistency (set null if no text prompt) - What to check: Holistic semantic alignment between video and text (theme, scene, actions, style coherence). This is summary dimension; do not double-count fine-grained issues already noted above. - Typical evidence: timestamps representing core theme fulfillment or contradictions. - Anchors: 1: Largely mismatched; core theme or requirements not met. 2: Many inconsistencies across key elements (theme/scene/action/style). 3: Mostly correct with noticeable errors in secondary aspects. 4: Overall consistent with small mismatches that do not change the theme. 5: Highly consistent; strong semantic agreement with the text prompt. B.2.3 Agentic metrics (planning & recovery). To quantitatively evaluate the agents planning capabilities, we designed three specialized metrics: Weighted Plan Edit Distance (wPED), Dependency Coverage (DepCov), and Re-planning Quality (ReplanQ). The precise definitions and calculation methods for these metrics are detailed below. 35 wPED (Weighted Plan Edit Distance). wPED measures the structural similarity between the sequence of tool names in an agent-generated plan (Ppred) and an expert-authored reference plan (Pref ). The score is derived from the classic Levenshtein edit distance, denoted as L(A, B), which calculates the minimum number of single-item edits (insertions, deletions, or substitutions) needed to transform sequence into sequence B. The wPED score is calculated by normalizing this distance and inverting the result, ensuring that higher score indicates better alignment. The formula is: wPED = 1 L(Ppred, Pref ) max(len(Ppred), len(Pref )) (2) higher wPED score (closer to 1.0) signifies closer structural alignment to the expert plan. DepCov (Dependency Coverage). DepCov evaluates the logical correctness of generated plan (Ppred) by measuring its adherence to set of fundamental, rule-based dependencies inherent to video production workflows. Our evaluation is based on predefined set of rules, such as the principle that content generation must precede content editing. Let D(Ppred) be the set of all dependency pairs (u, v) identified in the plan Ppred according to our rules, where tool must appear before tool v. Let Dsat(Ppred) D(Ppred) be the subset of those pairs where this ordering is correctly satisfied. DepCov is then the fraction of satisfied dependencies: DepCov = Dsat(Ppred) D(Ppred) (3) higher DepCov score indicates that the agents plan is more logically sound and respects the procedural constraints of the task. ReplanQ (Re-planning Quality). ReplanQ measures the agents ability to efficiently and effectively recover from simulated execution failure. The metric is designed to reward intelligent, minimal plan modifications. Let Porig be the agents initial plan, and let the failure occur at index i. The agent then generates revised plan, Preplan. We compare the suffixes of both plans starting from the failure point, denoted as Porig[i :] and Preplan[i :]. ReplanQ is calculated using the same normalized Levenshtein distance as in wPED, applied only to these suffixes: ReplanQ = 1 L(Porig[i :], Preplan[i :]) max(len(Porig[i :]), len(Preplan[i :])) (4) higher ReplanQ score (closer to 1.0) indicates more efficient and robust recovery, suggesting that fewer changes were required to correct the plan after the failure."
        },
        {
            "title": "C Detailed Experiment Settings",
            "content": "C.1 UniVAs Configuration Table 4 UniVAs Configuration Module Plan Agent Act Agent Video Generation Model Video Understanding Model Video Editing Video Segmentation Image Generation Model Model Claude-sonnet-412 Claude-sonnet-4 Seedance-v4-480p (Gao et al., 2025) InternVL3-38B (Zhu et al., 2025) Runway Aleph SAM-2 (Ravi et al., 2024) and Sa2VA (Yuan et al., 2025a) FLUX-Kontext (Labs et al., 2025) 36 C.2 Baseline Configurations Generation. For all video generation tasks, we standardized the output resolution to 480p and frame rate of 24 fps to ensure fair comparison. For baselines that natively lacked support for multi-image or video-conditioned inputs, we implemented standardized pre-processing pipeline to bridge the capability gap: For the Entities2Video task, where some baselines only accept single image, we first merged the multiple input reference images into single composite image. This composite was then used as the input. For the Video2Video task, for text-only baselines, we first employed video captioning model (Qwen2.5-VL-72B (Bai et al., 2025)) to generate detailed description of the source video. This generated caption was then prepended to the users instruction prompt to guide the generation process. The specific baseline models were configured as follows: LTX-Video: We utilized the official model and followed the recommended settings provided in their public repository. Seedance: We used the seedance-v1-pro-t2v-480p and seedance-v1-pro-i2v-480p from Wavespeed API13, consistent with our UniVAs generation module, to ensure direct comparison of the agentic frameworks contribution. Wan: We used the wan-2.2/t2v-480p (Wan et al., 2025) and wan-2.2/i2v-480p also via the Wavespeed API. Understanding. For all the understanding tasks, we are using frame rate of 1 fps and maximum of 128 frames. Editing. For the video editing task, we use Runway Aleph as the baseline model. In the baseline pipeline, videos are clipped into 5-second clips and sent to the Aleph model with task prompt. Then, the edited video clips are merged together for evaluation. Segmentation. For the video segmentation task, we use Sa2Va-4B as the baseline model, we directly send the video into the baseline model together with the segmentation prompt. 13https://wavespeed.ai/"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Singapore Management University",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "University College London",
        "University of Rochester"
    ]
}