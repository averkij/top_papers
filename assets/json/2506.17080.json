{
    "paper_title": "Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs",
    "authors": [
        "Ricardo Rei",
        "Nuno M. Guerreiro",
        "José Pombal",
        "João Alves",
        "Pedro Teixeirinha",
        "Amin Farajian",
        "André F. T. Martins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching state-of-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing general-purpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require a mixture of skills. In this paper, we introduce Tower+, a suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve a Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing a novel training recipe that builds on Tower (Alves et al., 2024), comprising continued pretraining, supervised fine-tuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we introduce for evaluating both translation and instruction-following. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 0 8 0 7 1 . 6 0 5 2 : r TOWER+: Bridging Generality and Translation Specialization in Multilingual LLMs Ricardo Rei*1, Nuno M. Guerreiro1,2,3,4, José Pombal1,2,3, João Alves1 Pedro Teixeirinha1, Amin Farajian1, André F. T. Martins1,2,3 1Unbabel 2Instituto de Telecomunicações 3Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit) 4MICS, CentraleSupélec, Université Paris-Saclay"
        },
        {
            "title": "Abstract",
            "content": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for reaching stateof-the-art performance on specific tasks like machine translation. However, this process of adaptation often implies sacrificing generalpurpose capabilities, such as conversational reasoning and instruction-following, hampering the utility of the system in real-world applications that require mixture of skills. In this paper, we introduce TOWER+, suite of models designed to deliver strong performance across both translation and multilingual general-purpose text capabilities. We achieve Pareto frontier between translation specialization and multilingual general-purpose capabilities by introducing novel training recipe that builds on TOWER (Alves et al., 2024), comprising continued pretraining, supervised finetuning, preference optimization, and reinforcement learning with verifiable rewards. At each stage of training, we carefully generate and curate data to strengthen performance on translation as well as general-purpose tasks involving code generation, mathematics problem solving, and general instruction-following. We develop models at multiple scales: 2B, 9B, and 72B. Our smaller models often outperform larger general-purpose open-weight and proprietary LLMs (e.g., LLAMA 3.3 70B, GPT-4o). Our largest model delivers best-in-class translation performance for high-resource languages and top results in multilingual Arena Hard evaluations and in IF-MT, benchmark we introduce for evaluating both translation and instructionfollowing. Our findings highlight that it is possible to rival frontier models in general capabilities, while optimizing for specific business domains, such as translation and localization."
        },
        {
            "title": "Introduction",
            "content": "have shown that state-of-the-art proprietary LLMs, such as GPT-4o and Claude 3.7 are currently stateof-the-art in translation quality (Kocmi et al., 2023, 2024a; Deutsch et al., 2025). At the same time, several studies have shown that open-weight LLMs can be effectively adapted for machine translation, reaching parity with or even surpassing the translation quality of leading proprietary models (Alves et al., 2024; Rei et al., 2024; Xu et al., 2025; Cui et al., 2025a). However, these task-specific adaptations often come at the cost of general-purpose capabilities1 such as instruction following and conversational reasoning. For example, TOWER V2, the best-performing system in the WMT24 shared task, ranked first in 8 out of 11 language pairs (Kocmi et al., 2024a), yet underperforms models like GPT4o or Claude 3 on general chat evaluations. This tradeoff is illustrated in Figure 1, which shows that translation-specialized models tend to fall short on the Pareto frontier that balances translation quality and general-purpose utility. In addition, the degradation of instruction-following capabilities can hinder the ability to handle complex translation scenarios, such as adhering to terminology or formatting rules. Indeed, translation-specific systems greatly underperform general-purpose models on IF-MT, novel benchmark we introduce for evaluating both translation and instruction-following. To address this challenge, we investigate how to develop state-of-the-art translation models without compromising performance on general chat benchmarks. Our post-training pipeline builds on the framework introduced in earlier TOWER models (Alves et al., 2024; Rei et al., 2024), but introduces several important methodological improvements. Large Language Models (LLMs) are rapidly emerging as the de facto solution for multilingual machine translation. Recent studies and shared tasks *Core contributor. (cid:66) ai-research@unbabel.com 1The term general-purpose capabilities refers to the realworld utility of language models in handling queries that require core knowledge, instruction-following, and conversational reasoning (Zheng et al., 2023) 1 a f s i a C r G ) H r - ( 70 60 50 40 30 20 10 0 80 CLAUDE SONNET 3. GPT-4O QWEN 2.5 72B TOWER+ 72B GEMMA 2 9B GEMMA 2 27B ALMA-R 13B LLAMA 3.3 70B GEMMAX 9B TOWER+ 9B TOWER V2"
        },
        {
            "title": "84\nTranslation Performance (XCOMET-XXL on WMT24++)",
            "content": "85 83 86 87 88 Figure 1: Translation and general capabilities performance of state-of-the-art translation-specific (circles) and general-purpose LLMs (logos) of varying sizes (from 9B to 72B). For their size, TOWER+ models outperform or match open-weight models on both axes, and TOWER+ 72B is competitive to state-of-the-art closed models. We omit 2B models for better visualization; we show detailed results in Table 1. Gray lines connect each TOWER+ model to their respective instruction-tuned counterpart. We begin with continued pretraining (CPT) on curated mixture of monolingual and parallel data to enhance multilingual fluency and translation performance. Unlike previous versions, the CPT stage now also includes small fraction (1%) of highquality instruction-like data, which helps preserve general capabilities during early specialization. For supervised fine-tuning (SFT), we refine the data mixture and significantly rebalance task weights: while translation remains key component, it now accounts for only 22% of the SFT corpus, with the remainder covering instruction-following tasks such as mathematics, code generation, and question answering. In addition, we improve the quality of SFT responses through automated refinement using open-weight models, resulting in better quality answers. Finally, we introduce new preference optimization stage using Weighted Preference Optimization (Zhou et al., 2024, WPO), complemented by Group Relative Policy Optimization (Shao et al., 2024, GRPO) and verifiable reward models (RLVR), to further align the model with highquality outputs across tasks. We evaluate our models on both general chat benchmarks and translation tasks. Crucially, we analyze the contribution of each stage of the training pipeline in Section 3.3, and assess the role of the base model in balancing general-purpose and translation-specific capabilities in Section 3.4. Our contributions are as follows: We present, to the best of our knowledge, the first systematic study on balancing translation quality and general-purpose capabilities in openweight LLMs. While most prior work (Alves et al., 2024; Xu et al., 2024a; Rei et al., 2024; Cui et al., 2025a) has focused solely on maximizing translation performance, our approach explicitly targets broader trade-off. We introduce post-training pipeline that integrates diverse multilingual signals without compromising general chat abilities. Our approach can serve as blueprint for adapting LLMs to domainor task-specific business use cases while preserving general capabilities. We introduce and release IF-MT,2 novel benchmark for evaluating both translation and on two langauge pairs (EnglishSpanish and EnglishChinese). instruction-following capabilities We release TOWER+,2 suite of models that demonstrate strong performance across translation, general capabilities, and benchmark that mixes the two. We match or exceed the translation quality of prior TOWER models and GPT4O-1120, while also surpassing general-purpose open-weight models like Llama-3.3 70B and Qwen2.5 72B on M-ArenaHard."
        },
        {
            "title": "2 TOWER Post-Training",
            "content": "Our refined post-training pipeline consists of four stages: Continued Pretraining (CPT), supervised fine-tuning (SFT) (as in TOWER-V2, Rei et al. 2024), preference optimization using WPO (Zhou 2We make IF-MT and all TOWER+ models available on Huggingface. 2 Figure 2: Process for creating and curating data for our final dataset for SFT. et al., 2024), and finally, reinforcement learning with verifiable rewards using GRPO (Shao et al., 2024). In this section, we describe each stage in detail and explain how translation signals are incorporated throughout the pipeline."
        },
        {
            "title": "2.1 Continued Pretraining",
            "content": "The continued pretraining phase leverages monolingual, parallel, and general-purpose instructionfollowing data. Similarly to previous TOWER models, the data distribution follows 66%/33% split between monolingual and parallel data; in this version, we include 1% of instruction-following data. All monolingual data is sourced from FineWebEdu (Penedo et al., 2024). Most parallel data is sourced from OPUS (Tiedemann, 2012) and filtered using COMETKIWI (Rei et al., 2022b). Additionally, the parallel data is formatted as translation instruction followed by the corresponding translation.3 For language pairs where it is available, we include document-level translation data from EuroParl (Koehn, 2005), ParaDocs (Wicks et al., 2024), and CosmoPedia-v2 (Ben Allal et al., 2024), each totaling 10% of the data of each lanInstruction-following data is samguage pair. pled from FineWeb-Edu using dsir (Xie et al., 2023) to be similar to high-quality instructionfollowing data. For monolingual data, we apply the EuroFilter-v1 (Martins et al., 2025) quality fil3We use multiple templates to prepare the parallel corpora. See examples in Appendix Figure 7. ter, multilingual educational classifier built using mDeBERTA (He et al., 2023). Using these high-quality parallel and monolingual datasets, the first step of our post-training is to continue the pre-training (CPT) of base openweight LLM model with standard next-token prediction objective. This process covers 27 languages/dialects4 and 47 language pairs, totaling 32B tokens. Then, before proceeding to the next phase, we merge the CPT checkpoint back with the base checkpoint. Our ablations show that merging back to the base checkpoint can improve general performance with little impact on translation quality."
        },
        {
            "title": "2.2 Supervised Fine-tuning",
            "content": "For SFT, we collect instructions from several publicly available datasets, including OpenHermes-2.5, Aya (Singh et al., 2024), Daring-Anteater (Wang et al., 2024), Magpie (Xu et al., 2024b), Tülu (Lambert et al., 2025), and others. Using Llama 3.3 70B (Llama Team, 2024), we assign two scores from 1 to 5 to each instance that represent 1) an estimate of the amount of reasoning required to answer and 2) the instructions readability.5 We filter out most data where the reasoning score or readability falls below 4.6 We then collect answers 4Complete list of languages available in Appendix A. 5The prompt used to classify the conversations can be found in Appendix Figure 8. 6We do not filter out all data with low reasoning and readability because it is also important for the model to learn how 3 from four top-performing open-weight LLMs to create pool of candidate answers: the original, DeepSeek V3 (DeepSeek-AI, 2025), Qwen 2.5 72B (Qwen Team, 2025), Tülu 3 (Lambert et al., 2025), and Llama 3.3 answers. The answer we ultimately use for training is the one that ranks the highest when evaluated using state-of-the-art general-purpose reward model, Skywork-Gemma2-27B (Liu et al., 2024). This process follows the increasingly common paradigm of distillation from multiple teacher models, where several strong openweight LLMs provide candidate completions, and the best one is selected based on learned reward function. Given the multilingual nature of our corpus, this approach is closely aligned with the multilingual arbitrage method proposed by Odumakinde et al. (2024), which applies the same principle of multi-teacher selection to multilingual prompts. Additionally, similar to TOWER-V2 (Rei et al., 2024), we collect data from pre-translation, translation, and post-translation tasks. Pre-translation tasks involve preprocessing steps typically performed before translation, such as grammar error correction, named entity recognition, and the removal of PII content. Translation tasks cover broad spectrum, including sentence-level translation, style adaptation (e.g., formal vs. informal), document-level translation, and multilingual translation (single source, targets in multiple languages). Some of these data are proprietary, but the majority comes from WMT shared tasks and Flores test sets (excluding WMT 2024). Post-translation tasks focus on processes that follow translation, such as automatic post-editing and machine translation quality evaluation. As with translation tasks, part of the data comes from proprietary sources, while the rest comes from WMT shared tasks ranging from 2017 to 2023. The final corpus consists of 1.3 million samples, with translation tasks accounting for approximately 22% of the total. In Figure 2, we illustrate the full SFT data curation process, including the filtering steps, along with the proportion each category contributes to the final corpus."
        },
        {
            "title": "2.3 Preference Optimization",
            "content": "After SFT, our models undergo offline reinforcement learning using weighted preference optimization (WPO) (Zhou et al., 2024). This phase uses to respond to poorly formulated instructions. We only keep such prompts if they are from OpenHermes-2.5 and we discard the remaining ones coming from other datasets. (i) mixture of prompts from two sources: subset of SFT prompts, and (ii) new prompts from UltraFeedback (Tunstall et al., 2024). These two datasets serve complementary roles. The SFT-derived prompts are richer in multilingual coverage, safety-critical scenarios, and multiturn interactionsareas underrepresented in UltraFeedback. Preferences for these prompts are collected off-policy from several high-quality open-weight LLMs that permit commercial use.7 UltraFeedback data, in contrast, is used onpolicy, leveraging samples generated directly by our model. For both sources, we apply the INFORM reward model8 to identify the best and worst completions, which are then used for WPO updates. While we experimented with alternative reward models, such as Skywork (used in the SFT phase), we observed that Gemma 2based reward models tended to over-prefer completions from their own base models (e.g., Gemma 2 27B Instruct). This bias made them less suitable for preference optimization, where candidate responses include outputs from Gemma models. In contrast, INFORM, which ranked first on RewardBench at the time of writing, showed no such preference for its own Llama 3 base and was therefore selected for this phase. Using Skywork during SFT, however, did not pose the same issue, as none of the candidate completions used to build the final SFT dataset came from Gemma 2 models. Additionally, given the large scale of the SFT data and the significantly higher inference cost of INF-ORM, we opted for Skywork. Finally, to improve performance on machine translation, we incorporate human preference data collected by professional linguists. This data comes from two sources: (i) post-edits of TOWER V2 outputs, where the edited version is treated as preferred over the original system output, and (ii) direct preference annotations between translation variants produced during previous quality evaluations of earlier TOWER models. We experimented with various formats and reused the collected data. Using post-edits as preference data has been shown to effectively improve translation quality (Berger et al., 2024). This approach is similar in spirit to the 7Models used for off-policy data: DEEPSEEK V3, LLAMA-3.3-70B, QWEN-2.5-72B, MISTRAL-SMALL-3.1, GEMMA-2-27B, T\"ULU-3-70B. 8https://huggingface.co/infly/ INF-ORM-Llama3.1-70B 4 Figure 3: Process for creating and curating data for our final dataset for PO. methodology described in the LLAMA 3 technical report (Llama Team, 2024), where expert-edited outputs were repurposed as preference data (albeit for non-MT tasks). The remaining MT preferences are collected using COMET22 (Rei et al., 2022a) for MBR9 and picking the best and worse translations from the resulting MBR scores. To avoid metric-specific biases (Kocmi et al., 2024a; Pombal et al., 2025b) we then double check that best > worse using METRICX24-XXL (Juraska et al., 2024) and Llama 3.3 focusing on fluency and instruction following.10 Regarding the choice of the optimization algorithm we found WPO to outperform DPO (Rafailov et al., 2023) specially in terms of translation quality where we saw little to no improvement over the SFT model."
        },
        {
            "title": "The complete reinforcement learning pipeline is",
            "content": "summarized in Figure 3."
        },
        {
            "title": "2.4 RL with Verifiable Rewards",
            "content": "At this stage, our models already demonstrate state-of-the-art performance in both translation and general-purpose tasks. However, we find that their instruction-following, mathematical, and reasoning capabilities can be further improved through training on data with verifiable rewards. To this end, we leverage the Tülu 3 verifiable rewards dataset (Lambert et al., 2025) and augment it with two translation-specific signals: translation9We sample 24 candidates using temperature of 1.0. 10While neural MT metrics capture adequacy we found that an LLM-as-a-judge can better score fluency and how well the translation respects the provided user instruction. Translation-verifiable instructions target verifiable instruction and translation preference evaluation. We provide template in Appendix D. the models ability to apply text transformations during translation (e.g., converting date formats from DD-MM-YYYY into MM-DD-YYYY formatting). To generate this training data, we first defined list of 28 broad text transformations (e.g., email formatting, date formatting, etc.; we provide all categories in Appendix D). Along with each transformation category, we include corresponding transformations (e.g., for date formatting, some transformations include month abbreviation, day of week abbreviation, timezone format, etc.). These transformations come with description, verification (in the form of regular expression), and one example of input/output. We show one such transformation template in Appendix D. We then prompted LLaMA 3.3 70B Instruct (Llama Team, 2024) to generate list of precise source-dependent transformation guidelines and source document (that does not follow them) given (i) list of guideline categories (whose length was sampled uniformly between 1 and 4), description and one example of input/output transformation when required (we show examples of such information in Appendix), (ii) desired length (1/2 sentences, 1 paragraph), and (iii) topic/sub-topic pair out of list of over 625 pairs (topics vary wildly spanning from pairs like \"Sports Industry Athletic Equipment\" to \"Journalists and Writers Ezra Klein\"). We provide the prompt for this step in Appendix and one example in Figure 4. Next, we used the same LLM to verify the generated data. sample was kept only if its source text violated all"
        },
        {
            "title": "Example of a verifiable translation instruction",
            "content": "Metadata for Prompt Length: 1 sentence Topic: Economic Policy - Tax Reform Guideline Category: Date Formatting: DATE_001 Source Text The new tax reform bill, announced on January 10th, 2024, is expected to have significant impact on the economy, with major corporations already adjusting their financial plans in anticipation of the changes that will take effect on February 20, 2025. Guideline Convert dates to MM/DD/YYYY. Figure 4: Examples of verifiable translation instruction. The guideline category \"DATE_001\" is shown in Appendix D. of the associated guidelines, ensuring every transformation was applicable. Finally, we translated these sentences using TOWER and asked different LLMs to apply the transformations on the translated output. We filtered out any examples where the verification (e.g., regex template) did not match or where the final translation quality (measured by COMETKIWI) was below 0.8. During GRPO, the model is rewarded when its output matches the regex in the target translation. This task is designed to encourage more precise instruction-following during translation. Translation preference evaluation reuses the curated translation preferences from the WPO phase. Here, we prompt the model to compare two translation outputs, provide quality assessment (reasoning), and deliver final judgment. The model is rewarded when it selects the better translation. Note that this is done without any thinking tokens. All the reasoning that leads to the final decision is part of the final answer."
        },
        {
            "title": "3.1 Evaluation Setup",
            "content": "Our primary objective is to achieve state-of-theart machine translation (MT) performance and improve on general capabilities over the base model. To evaluate this, we use three benchmarks that span these two dimensions: WMT24++ (Deutsch et al., 2025) for translation, and M-ArenaHard (Dang et al., 2024; Li et al., 2024) and IFEval (Zhou et al., 2023) for general-purpose performance. WMT24++ For translation evaluation, we use the WMT24++ test set, which extends the official WMT24 set (Kocmi et al., 2024b) to cover 55 languages and dialects by collecting new humanwritten references. The WMT shared task is major annual competition in the field.11 Each year, organizers curate new data to build test set spanning diverse domains and languages. Since WMT24++ includes all 22 source languages covered by our models, we evaluate translation into 24 target variants.12 For evaluation, we rely on state-of-the-art MTspecific automatic metrics, including XCOMETXXL (Guerreiro et al., 2024) as our primary metric, and METRICX24-XXL (Juraska et al., 2024) and CHRF (Popovic, 2015) for supplementary analysis. The latter two are reported in the appendix (Tables 3 and 2 respectively). IFEval Many real-world applications of LLMs require following instructions to complete specific tasks (e.g., formatting text according to given guidelines). Instruction-following is therefore key component of evaluating general-purpose capabilities. To assess this, we use IFEval (Zhou et al., 2023), widely adopted benchmark for evaluating instruction-following behavior. IFEval consists of 541 instructions whose outputs can be automatically verified using simple code or regular expres11With over fifteen editions, WMT has become one of the flagship events at *ACL conferences. 12We include both Brazilian and European Portuguese, as well as Simplified and Traditional Chinese, totaling 24 language directions. While our models support more variants, WMT24++ does not currently provide references for all of them."
        },
        {
            "title": "Params",
            "content": "M-ArenaHard"
        },
        {
            "title": "IFEval",
            "content": "WMT24++ IF-MT 7 lang. 15 lang. 24 lang. IF MT Closed GPT-4O-1120 CLAUDE-SONNET-3.7 >100B >100B Open Weights ALMA-R GEMMAX TOWER-V2 GEMMA-2 GEMMA-2 QWEN-2.5 LLAMA-3.3 Ours TOWER+ TOWER+ TOWER+ 13B 9B 70B 9B 27B 72B 70B 2B 9B 72B 61.19 67.00 0.2 0.02 4.01 13.38 22.81 50.00 13.15 6.33 33.47 54.52 85.20 89.95 0.0 0.17 51.22 66.86 66.60 88.44 92. 67.32 83.84 89.02 86.69 86.41 80.97 84.26 86.40 81.93 83.68 84.72 82.74 81.88 86.25 86.68 84.33 84.24 78.74 83.88 75.35 79.02 77.44 78. 78.42 83.57 83.29 85.21 85.19 75.66 83.74 76.34 80.18 76.62 79.48 79.13 84.38 83.74 5.81 89.35 1.71 1.52 3.14 5.07 5.29 5.49 5.38 2.90 4.85 5.55 78.11 68.95 87.82 88.51 88.67 88.79 88.13 87.65 88.51 88.95 Table 1: Results of several translation-specific () and general-purpose open-weight and closed API models across M-ArenaHard, IFEval, WMT24++, and IF-MT (EnglishChinese). We consider two evaluation dimensions on IF-MT: instruction-following (IF) and raw MT quality (MT). For WMT24++ we report XCOMET-XXL and we split the language pairs into three categories: (1) seven high-resource languages, (2) the 15 languages from TOWER-V2 (our submission to WMT24), and (3) all languages supported by our new models. This categorization enables more equitable comparison with other systems, which, in all cases, support at least the seven high-resource languages. We boldface the best overall system, and the best open-weight system if the former is proprietary. sions. Models are evaluated based on the percentage of instructions executed correctly. M-ArenaHard Traditional benchmarks such as for evaluating general capabilities, MMLU (Hendrycks et al., 2020), typically rely on multiple-choice questions, which limit diversity and fail to capture real-world complexity. To address these limitations, the Chatbot Arena (Zheng et al., 2023) adopts more realistic evaluation setup by crowdsourcing open-ended prompts and collecting pairwise human preferences over model responses. Its main advantage is its alignment with real-world usage, where users issue diverse, complex queries. With over one million interactions, it has become the de facto standard for evaluating general-purpose LLM performance. However, its public and slow evaluation process makes it unsuitable for rapid model development. To mitigate this, Li et al. (2024) proposed ArenaHard curated set of 500 representative prompts from Chatbot Arenawhere model performance is measured via win rates against fixed baseline, using an LLM-based judge rather than human annotators. They show that rankings on ArenaHard strongly correlate with those from Chatbot Arena. In this work, we use M-ArenaHard (Dang et al., 2024), multilingual extension of ArenaHard, to evaluate general capabilities across five languages: English, German, Spanish, Chinese, and Russian. We employ LLaMA 3.3 70B Instruct (Llama Team, 2024) as the evaluator and use Qwen2.5 72B (Qwen Team, 2025) Instruct as the baseline reference model.13 translation + instruction-following IF-MT: Many real-world translation tasks go beyond simple language conversion, often requiring adherence to specific guidelines and rulessuch as maintaining consistent terminology, adapting date and currency formats, or converting units of measurement to align with local standards. Thus, it is important for translation models to be capable of translating text, while having to follow instructions, which requires mix of translation and general capabilities. However, no existing benchmarks evaluate both dimensions, so we create one following the zero-shot benchmarking methodology (Pombal et al., 2025a, ZSB): IF-MT. ZSB is task-agnostic framework for automatically creating benchmarks that correlate strongly 13Using Qwen2.5 72B Instruct as baseline also gives us direct comparison to the Instruct version of Qwen2.5 72B which we study in this paper as base model to TOWER+ 72B 7 with human rankings by prompting language models for both data generation and evaluation. It requires the creation of meta prompt for generating test instances (containing description and some attributes of the task), and judgement prompt, for evaluation. for"
        },
        {
            "title": "We generate",
            "content": "two LPs test data EnglishChinese and EnglishSpanish (Latin America)using CLAUDE-SONNET-3.7. We do not consider fixed set of instructions, but rather prompt the data generator model to come up with 2 to 4 instructions that can be applied to the source text it generates.14 For evaluation, we disentangle translation quality from instruction-following by considering two metrics: 1) COMET-22 (Rei et al., 2022a), stateof-the-art MT metric with larger context length than XCOMET-XXL (our generated sources are large enough that this is an issue); and 2) CLAUDESONNET-3.7 as judge for evaluating the extent to which models follow the instructions. On the latter, the judge scores each instance from 1 (worst) to 6 (best), as specified in the judgment prompt. LLMs have been shown to be good evaluators of these capabilities (Zheng et al., 2023; Zeng et al., 2024; Pombal et al., 2025a). For each model and evaluation dimension, we report the average score over all instances. We omit CLAUDE-SONNET-3.7 from the results since its performance would be overestimated due to intramodel family bias. In Appendix F, we include the data generation and judgment prompts we used, two examples from our benchmark (one for each LP), and results for EnglishChinese (conclusions are similar across LPs). Baselines We evaluate our models against both closed-source API models and open-weight LLMs. For closed models, we include GPT-4O-1120 and CLAUDE-SONNET-3.7. Among open-weight models, we consider leading general-purpose LLMs with fewer than 80B parameters, as well as translation-focused models such as ALMA-R (Xu et al., 2024a), GEMMAX (Cui et al., 2025b), and TOWER-V2 (Rei et al., 2024), the winning submission of the WMT24 MT shared task. For most models, we use standardized prompting format (see Fig. 9 in Appendix); exceptions include translation14We consider only verifiable instructionse.g., currency/date formatting, glossary followingas opposed to subjective ones, like style guides. specific models, where we adopt the prompts recommended by their respective authors."
        },
        {
            "title": "3.2 Main Results",
            "content": "From Table 1, we observe that the new TOWER models achieve strong balance between translation performance and general chat capabiliTOWER+ 72B achieves competitive reties. sults on instruction-following benchmarks (IFEval), performing on par with leading models such as CLAUDE-SONNET-3.7 and GPT-4O-1120, and surpassing strong open-weight baselines like QWEN-2.5 on M-ArenaHard. Notably, TOWER+ 72B matches the translation performance of the previous TOWER-V2 model while substantially improving win rates against QWEN-2.5 on MArenaHard, from 4% to 54.5%. On IF-MT, TOWER+ 72B once again surpasses all other open models on both evaluation dimensions, showcasing its ability to leverage both translation and general capabilities. Meanwhile, TOWER+ 9B, despite having only 9B parameters, achieves competitive performance across 24 language pairs (LPs) in machine translation and outperforms GEMMA-2 on IFEval, MArenaHard, and IF-MT. Finally, TOWER+ 2B, our smallest model with just 2B parameters, matches the machine translation performance of LLAMA3.3 and outperforms the previous TOWER-V270B model on both M-ArenaHard, IFEval, and the instruction-following side of IF-MT, highlighting the effectiveness of our improved post-training pipeline. We note that, when comparing the previous TOWER-V2-70B model to our new 72B model (TOWER+ 72B), there is slight decrease in translation quality on the subset of 15 languages originally used in WMT24. We attribute this drop not to the expanded language coverage in the new version (from 15 to 22 languages), but rather to the more limited multilingual capabilities of the QWEN 2.5 base model. This limitation is also evident when comparing QWEN 2.5 72B Instruct with LLAMA 3.3 70B: while QWEN 2.5 72B Instruct performs strongly on general chat capabilities (with win rate of 86.85% over LLAMA 3.3) and excels in translation for high-resource languages, its performance sharply declines when evaluated across broader set of 15 or 22 languages. We further analyze the impact of base model selection in Section 3.4. Although LLAMA 3 models exhibit stronger translation capabilities, their more a f s i a l n d a A - 35 32 29 23 Gemma 2 9B + SFT 86 TOWER+ + WPO + GRPO + CPT TOWER+ + GRPO 85 E 84 Gemma 2 9B + CPT + SFT + WPO 81 82 83 84 83 85 Translation Performance (WMT24++)"
        },
        {
            "title": "T\nW",
            "content": "Figure 5: Performance comparison across progressive training stages using GEMMA 2 9B as the foundation model. We represent total of 4 training setups: (1) SFT (base): Supervised Fine-Tuning directly on the base model; (2) CPT+SFT: Continued Pre-Training followed by SFT; (3) CPT+SFT+WPO: Addition of Preference Optimization; and (4) CPT+SFT+WPO+GRPO: Integration of GRPO with verifiable rewards. The left plot omits (3) because performance is identical to (4) on M-ArenaHard; GRPO brings gains on IFEval. restrictive licensingincluding mandatory attribution and naming requirementsled us to prioritize QWEN 2.5 and GEMMA 2 for this work. All MT-specific models perform poorly on IFMT. In fact, we had to remove the instructions from the prompt of ALMA-R and GEMMAX so that the models were able to translate at all, highlighting the need to create more flexible models specialized on MT.15 Crucially, TOWER+ 72B greatly outperforms TOWER-V2 on this benchmark, which further speaks to the effectiveness of our approach in balancing translation quality and general capabilities. 3."
        },
        {
            "title": "Impact of Different Training Stages",
            "content": "Our first ablation study examines the impact of each stage in our post-training pipeline on achieving strong balance between general capabilities and translation quality. To this end, we perform the following comparisons: 15We also noticed that the performance of these two models was hurt because they often failed to translate all paragraphs in sources with line breaks (e.g., they stopped generating tokens after the first or second paragraph). This inability to translate long sources with line breaksdue to overfitting to specific formatcan be seen as another shortcoming of inflexible MTspecific models. 1. Impact of CPT: We take GEMMA 2 9B model and run only the SFT stage without any CPT. This allows us to isolate and measure the effect of the CPT phase on both general capabilities and translation performance. 2. Impact of WPO: After establishing the contribution of CPT, we evaluate the gains introduced by the WPO stage. 3. Impact of GRPO: Finally, we assess the effect of Reinforcement Learning with Verifiable Rewards by comparing the SFT+WPO model to our full pipeline (CPT+SFT+WPO+GRPO). Figure 5 summarizes model performance at each training stage, using both general-purpose benchmarks and translation-specific evaluations based on XCOMET-XXL. The CPT phase primarily improves translation quality, particularly for midand low-resource languages. While it increases XCOMET-XXL by only 0.77 points on the 7 high-resource language pairs, it delivers an overall gain of 3.3 points when evaluating across all languages. However, this improvement comes at cost: we observe consistent degradation in general capabilities as measured by M-ArenaHard. Although the exact cause is difficult to pinpoint without full access to the base models training details, we hypothesize that this degradation stems from disrupting the delicate balance achieved during the final pretraining annealing phases. These phases often involve carefully curated data, gradual learning rate schedules, and internal optimizations that are difficult to reproduce. Restarting trainingeven with high-quality datamay shift the data distribution away from key domains (Wang et al., 2025) such as math, code, or STEM, or reset optimizer states in way that hurts general capabilities. Nonetheless, given our primary goal of maximizing translation quality, we consider the observed gains in multilingual performance to outweigh the loss on M-ArenaHard.16 The WPO stage contributes substantial improvements across instruction following, general chat ability, and translation, confirming its central role in aligning the model. While GRPO appears promising, it is the stage where we observed the least overall gains. Improvements were primarily limited to IFEval, which 16As open-weight models become increasingly multilingual, and given that CPT requires significantly more compute than subsequent stages, we question the necessity of CPT in future pipelines. 9 a f s i a l n ) H r - ( 40 35 25 20"
        },
        {
            "title": "All\nlanguages",
            "content": "High-resource languages TOWER+ Qwen 2.5 14B TOWER+ Qwen 2.5 7B TOWER+ Gemma 2 9B Qwen 2.5 7B Qwen 2.5 14B Gemma 2 9B 60 65 70 75 85 Translation Performance (WMT24++) Figure 6: Comparison of GEMMA 2 9B and QWEN 2.5 7B/14B on translation quality (WMT24++) and general chat capabilities (M-ArenaHard). Arrows show the performance drop when including all languages vs high-resource ones. aligns with the fact that the Tülu 3 dataset is specifically designed to target IFEval and GSM8k. However, more indirect signalssuch as translation performanceshowed no measurable improvement. Although the gains on IFEval are initially encouraging, our analysis suggests they may stem from overfitting to the benchmark. In particular, we found that several prompts in the Tülu 3 dataset are poorly formatted, yet the verifiable reward still expects exact compliance with the instructions (see Figure 10). After cleaning the dataset to remove such inconsistencies, GRPO no longer yielded improvements over the WPO-only model. These findings suggest that while GRPO with verifiable rewards remains promising strategy for improving targeted model capabilities, its broader effectiveness depends heavily on the quality and structure of the reward-aligned data. Further research is needed to better integrate GRPO with VR into our post-training pipeline in way that generalizes beyond narrow benchmarks like IFEval. 3."
        },
        {
            "title": "Importance of base model",
            "content": "Our second ablation examines the importance of base model selection. Since our goal is to balance strong translation support with general-purpose capabilities, we focus on two prominent model families: QWEN 2.5 and GEMMA 2. While QWEN 2.5 models demonstrate outstanding performance across range of general-purpose benchmarks, they exhibit limited multilingual capabilities and comparatively weaker performance on translation tasks (Cui et al., 2025b). In contrast, the GEMMA 2 family achieves state-of-the-art machine translation performance among open-weight models while maintaining competitive results on general tasks. By comparing these two model families, we directly study the trade-off between more multilingual-oriented base model (GEMMA 2) and model optimized for general-purpose capabilities but less robust in multilingual settings (QWEN 2.5). To better understand this trade-off, we compare QWEN 2.5 models (both 7B and 14B) with GEMMA 2 9B by running the first two stages of our pipeline (CPT and SFT). Figure 6 illustrates the results. As shown, while QWEN 2.5 models demonstrate stronger capabilities on M-ArenaHard, they consistently lag behind in translation quality, particularly when midand low-resource languages are included. Notably, even with larger parameter count, QWEN 2.5 14B fails to match the translation performance of GEMMA 2 9B. In contrast, for general chat capabilities, QWEN 2.5 7B surpasses GEMMA 2 9B despite having fewer parameters. This trend is also reflected in the released Instruct models, where QWEN 2.5 7B INSTRUCT outperforms GEMMA 2 9B INSTRUCT on M-ArenaHard but shows significantly weaker results on translation tasks."
        },
        {
            "title": "4 Conclusions",
            "content": "In this paper, we presented complete post-training pipeline designed to balance task-specific use casemachine translationwith general-purpose language model capabilities. We demonstrated that it is possible to build models that achieve competitive translation performance while maintaining strong results on instruction-following and general chat benchmarks. Through extensive ablations, we analyzed the contribution of each stage in our pipeline. Notably, we found that while Continued Pretraining (CPT) boosts translation performanceespecially for midand low-resource languagesit can slightly degrade general capabilities, highlighting an important trade-off. The WPO stage consistently improves both translation and general-purpose performance, while GRPO shows targeted gains in instruction following but limited impact on broader capabilities. 10 Our final models not only outperform the Instruct-tuned versions released by the original model authors, but also surpass leading openweight models such as LLAMA 3 and QWEN 2.5 72B on general-purpose benchmarks like MArenaHard. In translation, our models achieve results on par with frontier systems such as GPT-4 and CLAUDE 3.7, while maintaining competitive general chat abilities. Overall, our findings suggest that post-training pipelines can be tailored to business-specific needs, such as localization, without sacrificing broader general intelligence. Future work includes further exploration of RL with verifiable rewards to better align models across wider range of tasks, and improving the handling of midand low-resource languages."
        },
        {
            "title": "Limitations",
            "content": "While our work demonstrates the effectiveness of carefully designed post-training pipelines in balancing translation performance and general capabilities, several limitations remain. First, as open-weight models continue to improve their native multilingual capabilities, the marginal benefit of task-specific adaptations such as CPT may decrease. Although our experiments show that even relatively multilingual base models like GEMMA 2 9B still benefit from CPT, further research is needed to understand how these dynamics evolve as base models become increasingly capable out-of-the-box. Second, while our study covers models up to 72B parameters, it remains unclear whether similar posttraining pipelines would provide sufficient gains for much larger frontier models (e.g., 100B+ parameters). As models scale up, the relative improvements from targeted post-training may diminish compared to the associated compute cost. Nonetheless, we argue that for many real-world business use casesparticularly those involving well-defined tasks like translation and localizationbuilding more efficient, specialized models remains highly valuable and preferable to deploying very large, general-purpose systems. Third, while our pipeline improves translation quality and general chat performance, it was specifically validated for multilingual translation tasks and does not explicitly optimize for other domainspecific areas such as biomedicine or legal reasoning. Nonetheless, we believe the overall recipe could serve as inspiration for designing similar post-training pipelines tailored to specialized domains. Finally, while we evaluated across wide range of languages and tasks, the available benchmarks are still skewed toward highand mid-resource languages. Further improving performance in truly low-resource and code-switched scenarios is an important area for future exploration."
        },
        {
            "title": "Acknowledgments",
            "content": "Part of this work was supported by the EUs Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), and by the Portuguese Recovery and Resilience Plan through project C645008882-00000055 (Center for Responsible AI). We thank EuroHPC for the HPC resources used to support this work through grant EHPC-EXT-2023E01-042 and grant EHPC-AI2024A01-085."
        },
        {
            "title": "References",
            "content": "Duarte Miguel Alves, José Pombal, Nuno Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. In First Conference on Language Modeling. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. Smollm-corpus. Nathaniel Berger, Stefan Riezler, Miriam Exel, and Matthias Huck. 2024. Post-edits are preferences too. In Proceedings of the Ninth Conference on Machine Translation, pages 12891300, Miami, Florida, USA. Association for Computational Linguistics. Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025a. Multilingual machine translation with open large language models at practical scale: An empirical study. Preprint, arXiv:2502.02481. Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, and Bin Wang. 2025b. Multilingual machine translation with open large language models at practical scale: An empirical study. Preprint, arXiv:2502.02481. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom 11 Kocmi, Florian Strub, Nathan Grinsztajn, Yannis FletBerliac, and 26 others. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. Preprint, arXiv:2412.04261. DeepSeek-AI. 2025. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Daniel Deutsch, Eleftheria Briakou, Isaac Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, and Markus Freitag. 2025. Wmt24++: Expanding the language coverage of wmt24 to 55 languages & dialects. Preprint, arXiv:2502.12404. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Juraj Juraska, Daniel Deutsch, Mara Finkelstein, and Markus Freitag. 2024. MetricX-24: The Google submission to the WMT 2024 metrics shared task. In Proceedings of the Ninth Conference on Machine Translation, pages 492504, Miami, Florida, USA. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, and 3 others. 2024a. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pages 146, Miami, Florida, USA. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, and 2 others. 2023. Findings of the 2023 conference on machine translation (WMT23): LLMs are here but not quite there yet. In Proceedings of the Eighth Conference on Machine Translation, pages 142, Singapore. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, and 1 others. 2024b. Findings of the wmt24 general machine translation shared task: the llm era is here but mt is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, USA. Association for Computational Linguistics. Philipp Koehn. 2005. Europarl: parallel corpus for In Proceedings of statistical machine translation. machine translation summit x: papers, pages 7986. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, and 4 others. 2025. Tulu 3: Pushing frontiers in open language model post-training. Preprint, arXiv:2411.15124. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024. From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. Preprint, arXiv:2406.11939. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451. AI Llama Team. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Pedro Henrique Martins, João Alves, Patrick Fernandes, Nuno Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte Alves, José Pombal, Manuel Faysse, and 1 others. 2025. Eurollm-9b: Technical report. arXiv preprint arXiv:2506.04079. Ayomide Odumakinde, Daniel Dsouza, Pat Verga, Beyza Ermis, and Sara Hooker. 2024. Multilingual arbitrage: Optimizing data pools to accelerate multilingual progress. Preprint, arXiv:2408.14960. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, and 1 others. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849. José Pombal, Nuno Guerreiro, Ricardo Rei, and André FT Martins. 2025a. Zero-shot benchmarking: framework for flexible and scalable automatic evaluation of language models. arXiv preprint arXiv:2504.01001. José Pombal, Nuno M. Guerreiro, Ricardo Rei, and André F. T. Martins. 2025b. Adding chocolate to mint: Mitigating metric interference in machine translation. Preprint, arXiv:2503.08327. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. AI Qwen Team. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: your language model is secretly reward model. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Ricardo Rei, Jose Pombal, Nuno M. Guerreiro, João Alves, Pedro Henrique Martins, Patrick Fernandes, Helena Wu, Tania Vaz, Duarte Alves, Amin Farajian, Sweta Agrawal, Antonio Farinhas, José G. C. De Souza, and André Martins. 2024. Tower v2: Unbabel-IST 2024 submission for the general MT In Proceedings of the Ninth Confershared task. ence on Machine Translation, pages 185204, Miami, Florida, USA. Association for Computational Linguistics. Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, and 14 others. 2024. Aya dataset: An open-access collection for multilingual instruction tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11521 11567, Bangkok, Thailand. Association for Computational Linguistics. Jörg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC12), Istanbul, Turkey. European Language Resources Association (ELRA). Lewis Tunstall, Edward Emanuel Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro Von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander Rush, and Thomas Wolf. 2024. Zephyr: Direct distillation of LM alignment. In First Conference on Language Modeling. Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, and Daniel Dajun Zeng. 2025. Learning dynamics in continual pre-training for large language models. Preprint, arXiv:2505.07796. Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy J. Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024. Helpsteer 2: Open-source dataset for training top-performing reward models. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Rachel Wicks, Matt Post, and Philipp Koehn. 2024. Recovering document annotations for sentence-level bitext. arXiv preprint arXiv:2406.03869. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. 2023. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems (NeurIPS). Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, and Huda Khayrallah. 2025. X-ALMA: Plug & play modules and adaptive rejection for quality translation at scale. In The Thirteenth International Conference on Learning Representations. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024a. Contrastive preference optimization: pushing the boundaries of llm performance in machine translation. ICML24. JMLR.org. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024b. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. Preprint, arXiv:2406.08464. 13 Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. 2024. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. Preprint, arXiv:2311.07911. Wenxuan Zhou, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi, Sanqiang Zhao, Kaiqiang Song, Silei Xu, and Chenguang Zhu. 2024. WPO: Enhancing RLHF with weighted preference optimization. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 83288340, Miami, Florida, USA. Association for Computational Linguistics."
        },
        {
            "title": "A Covered languages",
            "content": "TOWER models presented in this paper cover the following 27 languages/dialects: German, Spanish, Spanish (Latin America), French, Italian, Korean, Dutch, Russian, English, Portuguese (Portugal), Portuguese (Brazilian), , Chinese (Simplified), Chinese (Traditional), Czech, Ukrainian, Hindi, Icelandic, Japanese, Polish, Swedish, Hungarian, Romanian, Danish, Norwegian (Nynorsk), Norwegian (Bokmål), Finnish. Table 2, 3 and 4 show results for all supported languages from WMT24++ testset (Deutsch et al., 2025) using CHRF (Popovic, 2015), METRICX24XXL (Juraska et al., 2024) and XCOMET-XXL respectively."
        },
        {
            "title": "B Translation Templates used in CPT",
            "content": "This section presents template examples used to prepare parallel data for continued pre-training. We used hundreds of such templates created with Jinja2. Placeholders: {{ source }}: source sentence Figure 7 shows 8 examples of templates used to create both CPT and SFT data."
        },
        {
            "title": "C Prompts used to clean SFT data",
            "content": "In his section you can find the prompt used to clean and prepare the SFT data. Figure 8 presents the prompt we used on LLAMA-3.3 to assign scores for reasoning and readability along with category for each conversation."
        },
        {
            "title": "Instructions for Training",
            "content": "D.1 Broad topics for data generation Below we present the list of broad transformation categories for verifiable translation instructions. Email Formatting Phone Numbers Mathematical Notation Code Elements Brand Elements Time Formatting List Formatting Links and URLs Case Formatting Number Formatting Date Formatting Special Characters Text Structure Term Formatting Units and Measurements Citation and References Chemical Formulas Temperature Version Control {{ target }}: target sentence Geographic Coordinates {{ lp0 }}: source language name Technical Specifications {{ lp1 }}: target language name Financial Notation 14 _ S _ X _ R _ R _ P _ N _ K _ 2 2 . 6 5 6 2 . 8 5 7 7 . 8 3 3 6 . 7 2 7 7 . 1 5 5 6 . 4 7 5 . 9 4 9 9 . 1 5 0 6 . 6 5 0 7 . 1 5 6 0 . 6 5 2 6 . 5 4 5 . 6 6 1 6 . 7 6 6 0 . 0 3 0 2 . 4 4 2 3 . 1 6 7 3 . 3 4 1 . 0 6 4 1 . 4 6 0 5 . 3 6 1 2 . 2 6 2 0 . 5 6 4 2 . 4 5 6 . 8 6 4 0 . 9 6 3 3 . 5 5 7 0 . 5 6 2 7 . 6 6 0 5 . 7 9 5 . 6 6 1 2 . 7 6 1 6 . 7 6 4 5 . 5 6 3 6 . 7 6 3 8 . 7 1 1 . 3 5 2 7 . 4 5 3 5 . 6 4 2 1 . 0 5 7 4 . 0 5 9 4 . 1 9 9 . 7 4 8 4 . 1 5 2 7 . 2 5 1 8 . 7 4 1 3 . 2 5 6 1 . 3 5 6 . 4 6 8 6 . 5 6 3 7 . 6 4 0 3 . 9 2 3 4 . 9 5 7 7 . 1 5 2 . 8 5 6 0 . 2 6 7 5 . 2 6 8 5 . 1 6 0 9 . 3 6 5 5 . 2 0 9 . 9 4 6 2 . 2 5 2 9 . 3 3 2 0 . 0 5 3 5 . 7 4 6 8 . 8 4 7 . 7 4 7 1 . 9 4 0 2 . 9 4 5 1 . 8 4 3 2 . 0 5 1 6 . 9 5 7 . 1 5 6 7 . 4 5 0 2 . 1 2 8 3 . 6 4 3 8 . 8 4 8 4 . 1 9 1 . 8 4 5 1 . 2 5 0 3 . 0 5 8 9 . 0 5 1 8 . 3 5 4 0 . 3 6 2 . 4 3 9 1 . 9 3 8 7 . 1 1 0 5 . 4 3 0 2 . 1 3 5 9 . 2 1 3 . 3 3 6 6 . 6 2 0 6 . 5 3 1 4 . 2 3 4 3 . 6 3 2 1 . 7 _ 8 6 . 4 3 0 1 . 8 3 7 4 . 1 2 4 3 . 3 3 9 5 . 2 2 3 . 4 3 7 6 . 3 3 8 8 . 2 3 9 7 . 4 3 6 4 . 2 3 8 0 . 6 5 9 . 5 3 _ 9 9 . 6 6 3 1 . 8 6 2 1 . 2 5 5 1 . 5 5 1 . 4 6 6 4 . 5 6 1 2 . 4 6 3 4 . 5 6 0 9 . 6 6 2 4 . 3 6 5 . 5 6 0 1 . 6 6 _ 7 5 . 9 4 3 3 . 0 5 5 0 . 2 7 0 . 3 1 5 7 . 2 3 0 6 . 8 3 2 5 . 4 3 2 7 . 0 4 4 6 . 8 1 9 . 5 4 4 9 . 8 4 5 2 . 7 4 _ 8 3 . 6 5 3 9 . 6 2 5 . 9 2 1 0 . 5 1 8 1 . 8 4 3 2 . 1 5 8 1 . 4 4 4 6 . 2 2 8 . 2 5 1 3 . 0 5 8 7 . 3 5 1 4 . 2 5 _ 8 6 . 0 1 7 . 0 4 4 8 . 7 5 3 . 9 3 4 9 . 7 3 5 0 . 9 3 5 3 . 6 4 4 . 8 3 9 2 . 0 4 3 6 . 8 3 5 2 . 0 4 6 6 . 9 3 _ 3 2 . 1 6 4 6 . 1 6 7 1 . 4 5 7 5 . 9 5 0 1 . 8 5 1 2 . 9 2 6 . 6 5 2 7 . 8 5 6 8 . 0 6 8 4 . 7 5 5 7 . 9 5 4 0 . 0 _ 3 0 . 5 6 8 7 . 5 6 2 0 . 4 5 0 0 . 2 6 4 6 . 2 2 6 . 3 6 3 4 . 3 6 2 1 . 3 6 2 8 . 3 6 4 2 . 1 6 9 1 . 3 4 6 . 3 6 _ fi 4 7 . 2 6 9 9 . 4 6 4 1 . 1 3 1 0 . 7 7 7 . 5 5 3 1 . 7 5 4 7 . 1 5 3 3 . 8 5 9 0 . 8 5 4 8 . 7 5 0 . 1 6 8 6 . 9 5 _ D _ C _ T _ N _ P _ B _ 2 1 . 2 6 6 5 . 2 6 8 8 . 8 6 2 . 0 6 1 2 . 8 5 4 3 . 0 6 0 0 . 9 5 3 4 . 0 6 8 1 . 2 8 0 . 9 5 6 9 . 0 6 4 0 . 1 6 6 3 . 6 6 3 1 . 8 6 9 6 . 0 1 2 . 4 4 0 2 . 1 6 5 9 . 3 6 5 3 . 9 5 2 1 . 3 6 6 4 . 2 5 7 . 3 6 3 6 . 5 6 5 2 . 5 6 1 1 . 7 5 8 3 . 8 5 9 0 . 6 9 1 . 3 5 9 6 . 0 5 3 1 . 3 5 5 9 . 0 5 6 3 . 3 5 2 8 . 5 6 7 . 1 5 4 4 . 5 5 0 1 . 5 5 6 3 . 2 3 4 1 . 4 3 3 9 . 3 2 4 . 9 1 4 1 . 0 3 4 2 . 2 3 2 9 . 2 3 4 0 . 7 2 4 2 . 2 1 5 . 0 3 5 7 . 2 3 2 9 . 2 3 0 8 . 9 3 6 6 . 0 4 7 6 . 7 2 1 . 9 3 8 9 . 5 3 1 6 . 8 3 8 1 . 0 4 7 1 . 8 2 3 4 . 0 7 6 . 5 3 7 6 . 9 3 2 5 . 2 4 1 2 . 3 6 1 2 . 3 6 4 4 . 0 4 8 . 9 5 9 2 . 0 6 5 7 . 1 6 2 2 . 1 6 2 6 . 0 6 2 5 . 2 3 9 . 9 5 8 2 . 2 6 1 4 . 2 6 9 9 . 5 6 6 5 . 6 6 9 9 . 2 6 7 . 3 6 3 5 . 3 6 8 7 . 4 6 7 1 . 4 6 5 4 . 4 6 4 6 . 5 1 3 . 3 6 6 0 . 5 6 0 5 . 5 6 . 9 1 . 5 5 0 6 . 6 1 4 . 6 3 1 1 . 3 4 5 8 . 0 5 5 8 . 2 5 2 6 . 0 5 4 8 . 1 2 7 . 3 5 1 8 . 1 5 0 6 . 4 5 6 4 . 4 5 5 1 . A 7 . 6 9 . 3 5 9 2 . 5 5 8 9 . 9 3 8 5 . 7 7 7 . 9 4 9 6 . 1 5 4 0 . 0 5 2 2 . 0 5 1 7 . 3 5 6 4 . 0 7 3 . 3 5 1 6 . 3 5 1 1 . 0 6 3 9 . 0 6 7 9 . 8 4 3 8 . 7 7 3 . 7 5 7 6 . 8 5 0 6 . 7 5 4 9 . 6 5 1 7 . 9 5 5 3 . 6 2 0 . 9 5 3 8 . 9 5 7 . 3 - N - A 0 2 1 1 - 4 - s o M"
        },
        {
            "title": "X\nA\nM\nM\nE\nG",
            "content": "2 - E 2 - E 5 . 2 - Q - A 3 . 3 -"
        },
        {
            "title": "A\nM\nA\nL\nL",
            "content": "B 2 + O 9 + O T"
        },
        {
            "title": "2\nV\n-\nR\nE\nW\nO\nT",
            "content": "B 2 7 + O 15 e p s l / a a n i a t , ) b ( u r - h r e e e n l , a a a t l n + + 4 2 r t e ) ("
        },
        {
            "title": "F\nr\nh\nC",
            "content": ": 2 a l d n 5 1 . , t n e a u a a a h h e . ) a i ( d e o d o s a a a , ) r ( ) 4 2 0 2 , . e ("
        },
        {
            "title": "2\nV\n-\nR\nE\nW\nO\nT",
            "content": "7 . o e g e U _ E _ M _ R _ R _ P _ N _ R _ 0 0 . 5 - 2 1 . 5 - 8 5 . 6 - 5 2 . 5 - 0 1 . 6 - 9 9 . 4 - 7 5 . 6 - 4 3 . 6 - 1 9 . 4 - 4 0 . 6 - 9 0 . 5 - 0 2 . 5 - 4 3 . 4 - 0 5 . 4 - 2 7 . 7 - 7 5 . 6 - 6 4 . 5 - 8 2 . 4 - 6 8 . 5 - 3 1 . 5 - 6 3 . 4 - 8 1 . 5 - 3 4 . 4 - 6 6 . 4 - 5 2 . 4 - 0 4 . 4 - 2 7 . 4 - 2 3 . 4 - 9 6 . 4 - 4 0 . 4 - 4 5 . 4 - 4 8 . 4 - 3 0 . 4 - 7 7 . 4 - 5 1 . 4 - 5 9 . 3 - 0 6 . 4 - 3 5 . 4 - 1 9 . 4 - 4 5 . 5 - 7 6 . 5 - 2 4 . 4 - 6 1 . 5 - 3 8 . 5 - 1 7 . 4 - 1 2 . 6 - 5 9 . 4 - 1 6 . 4 - 2 6 . 5 - 4 8 . 5 - 7 9 . 8 - 8 4 . 7 - 0 9 . 6 - 7 7 . 5 - 1 0 . 8 - 0 6 . 6 - 2 6 . 5 - 6 6 . 6 - 5 6 . 5 - 2 8 . 5 - 7 3 . 6 - 7 5 . 6 - 4 8 . 9 - 9 1 . 7 - 3 6 . 7 - 0 5 . 6 - 7 7 . 7 - 9 5 . 7 - 3 7 . 6 - 7 5 . 7 - 0 3 . 6 - 2 3 . 6 - 6 6 . 5 - 2 8 . 5 - 2 0 . 6 - 1 3 . 6 - 0 0 . 7 - 4 9 . 5 - 0 9 . 8 - 4 9 . 6 - 2 0 . 7 - 1 8 . 6 - 9 9 . 5 - 1 2 . 6 - 2 4 . 4 - 4 4 . 4 - 4 7 . 9 - 2 1 . 5 - 3 9 . 5 - 5 4 . 4 - 9 1 . 5 - 5 4 . 5 - 5 5 . 4 - 3 5 . 5 - 2 8 . 4 - 9 6 . 4 - _ 7 3 . 4 - 5 5 . 4 - 9 2 . 6 - 6 3 . 5 - 6 2 . 5 - 5 5 . 4 - 2 9 . 4 - 3 2 . 5 - 7 5 . 4 - 1 2 . 5 - 1 7 . 4 - 8 5 . 4 - _ 9 2 . 4 - 8 3 . 4 - 4 2 . 5 - 5 4 . 4 - 3 9 . 4 - 4 1 . 4 - 0 7 . 4 - 8 9 . 4 - 7 1 . 4 - 4 0 . 5 - 1 2 . 4 - 9 0 . 4 - _ 6 4 . 7 - 1 5 . 7 - 2 4 . 7 - 6 1 . 8 - 7 4 . 6 1 - 5 6 . 9 - 9 1 . 7 1 - 8 1 . 2 1 - 1 8 . 6 - 0 3 . 8 - 8 2 . 7 - 6 6 . 7 - _ I _ E _ F _ 8 5 . 6 - 4 7 . 6 - 6 6 . 3 1 - 2 1 . 0 1 - 8 8 . 8 - 3 3 . 7 - 4 6 . 0 1 - 5 7 . 7 - 9 9 . 6 - 0 4 . 8 - 0 0 . 7 - 4 4 . 7 - 8 2 . 4 - 8 3 . 4 - 9 4 . 7 - 1 2 . 5 - 3 3 . 5 - 8 3 . 4 - 7 5 . 5 - 3 1 . 5 - 6 8 . 4 - 8 2 . 5 - 9 7 . 4 - 1 9 . 4 - 4 6 . 2 - 4 7 . 2 - 7 8 . 2 - 1 2 . 3 - 2 4 . 3 - 6 6 . 2 - 2 2 . 3 - 9 3 . 3 - 2 6 . 2 - 3 5 . 3 - 7 7 . 2 - 6 7 . 2 - 1 5 . 4 - 0 6 . 4 - 3 2 . 5 - 3 8 . 4 - 2 2 . 5 - 4 4 . 4 - 2 7 . 4 - 6 2 . 5 - 2 4 . 4 - 3 3 . 5 - 7 5 . 4 - 8 3 . 4 - _ fi 2 0 . 6 - 6 0 . 6 - 8 5 . 3 1 - 9 8 . 9 - 2 5 . 8 - 7 1 . 6 - 5 1 . 0 1 - 8 7 . 7 - 1 9 . 6 - 0 9 . 7 - 2 3 . 6 - 1 0 . 7 - _ D _ C _ W _ C _ P _ B _ 7 7 . 3 - 2 8 . 3 - 4 3 . 5 - 8 9 . 3 - 4 7 . 4 - 4 7 . 3 - 2 4 . 4 - 1 3 . 4 - 8 5 . 3 - 7 4 . 4 - 6 7 . 3 - 1 6 . 3 - 8 6 . 4 - 0 7 . 4 - 1 5 . 6 - 4 8 . 6 - 9 7 . 5 - 1 5 . 4 - 3 5 . 6 - 8 6 . 5 - 9 8 . 4 - 3 4 . 5 - 2 0 . 5 - 3 9 . 4 - 2 1 . 6 - 8 2 . 6 - 2 6 . 6 - 0 1 . 7 - 0 0 . 8 - 9 3 . 6 - 4 8 . 7 - 4 6 . 7 - 2 1 . 6 - 2 9 . 7 - 5 5 . 6 - 4 5 . 6 - 5 8 . 2 - 5 8 . 2 - 3 0 . 3 - 3 9 . 2 - 0 3 . 3 - 3 8 . 2 - 1 0 . 3 - 6 2 . 3 - 6 7 . 2 - 3 3 . 3 - 4 9 . 2 - 4 8 . 2 - 0 1 . 3 - 8 1 . 3 - 3 3 . 3 - 4 5 . 3 - 0 6 . 3 - 7 0 . 3 - 7 2 . 3 - 7 7 . 3 - 3 1 . 3 - 0 7 . 3 - 5 2 . 3 - 9 0 . 3 - 3 1 . 5 - 0 2 . 5 - 9 9 . 5 - 6 3 . 5 - 5 7 . 5 - 9 0 . 5 - 2 4 . 5 - 1 9 . 5 - 8 0 . 5 - 2 7 . 5 - 0 1 . 5 - 9 8 . 4 - 5 6 . 4 - 6 8 . 4 - 0 8 . 5 - 3 3 . 5 - 3 3 . 5 - 0 6 . 4 - 1 1 . 5 - 6 5 . 5 - 1 9 . 4 - 4 4 . 5 - 6 8 . 4 - 8 8 . 4 - . 1 8 . 4 - 2 9 . 4 - 2 8 . 6 - 3 8 . 5 - 6 2 . 6 - 5 9 . 4 - 7 4 . 6 - 4 9 . 5 - 5 9 . 4 - 2 8 . 5 - 8 9 . 4 - 0 0 . 5 - 5 . 7 . 3 5 . 4 - 3 6 . 4 - 3 8 . 5 - 0 1 . 5 - 5 0 . 6 - 8 6 . 4 - 9 8 . 5 - 1 7 . 5 - 3 5 . 4 - 8 4 . 5 - 0 7 . 4 - 4 6 . 4 - 0 0 . 4 - 0 1 . 4 - 8 5 . 4 - 6 4 . 4 - 9 6 . 4 - 1 9 . 3 - 9 3 . 4 - 0 8 . 4 - 0 0 . 4 - 6 8 . 4 - 1 1 . 4 - 7 9 . 3 - 7 . 3 - N - A 0 2 1 1 - 4 - s o M"
        },
        {
            "title": "X\nA\nM\nM\nE\nG",
            "content": "2 - E 2 - E 5 . 2 - Q - A 3 . 3 -"
        },
        {
            "title": "A\nM\nA\nL\nL",
            "content": "B 2 + O 9 + O T"
        },
        {
            "title": "2\nV\n-\nR\nE\nW\nO\nT",
            "content": "B 2 7 + O 16 r u c i / a a n i e , ) b ( u r - h r e e h g u i , a a a t l o + + 4 2 W f u ) ( 4 2 r : 3 a s l 5 1 . , t n e a u a g v e a t . ) a i ( e w u d o s a a a a , ) r ( ) 4 2 0 2 , . e R ("
        },
        {
            "title": "2\nV\n-\nR\nE\nW\nO\nT\ny\nb",
            "content": "7 . o e g e s A _ 1 5 2 8 . 0 6 1 2 8 . 0 5 9 8 6 . 0 7 9 6 7 . 0 3 4 4 7 . 2 0 9 7 . 0 4 7 8 4 . 0 7 3 5 7 . 0 6 1 3 8 . 0 8 3 6 7 . 0 7 3 2 8 . 3 8 0 8 . 0 _ 7 0 1 9 . 0 7 5 0 9 . 0 9 6 3 6 . 0 9 2 6 7 . 0 8 3 8 . 0 8 8 6 8 . 0 5 9 2 6 . 0 7 9 6 8 . 0 2 5 9 8 . 0 5 4 5 8 . 2 9 9 8 . 0 0 5 8 8 . 0 _ 7 9 7 8 . 0 4 2 7 8 . 0 4 0 2 8 . 7 2 6 8 . 0 9 0 4 8 . 0 4 2 5 8 . 0 8 6 9 7 . 0 5 5 4 8 . 0 4 3 8 8 . 7 5 4 8 . 0 4 6 7 8 . 0 8 2 8 8 . 0 _ 0 0 3 8 . 0 8 5 3 8 . 5 8 0 8 . 0 7 0 9 7 . 0 9 0 7 7 . 0 0 8 9 7 . 0 1 4 7 6 . 0 5 1 8 7 . 1 7 2 8 . 0 0 3 6 7 . 0 9 1 2 8 . 0 5 8 2 8 . 0 _ 3 4 5 8 . 6 2 4 8 . 0 9 3 6 5 . 0 8 8 0 6 . 0 5 0 3 7 . 0 8 0 8 7 . 0 6 4 3 4 . 1 3 9 7 . 0 3 5 3 8 . 0 5 7 7 7 . 0 4 1 4 8 . 0 4 6 2 8 . 0 _ 7 2 3 8 . 0 1 8 3 8 . 0 0 5 9 5 . 0 2 7 0 8 . 0 2 9 5 7 . 0 4 9 9 7 . 6 7 4 5 . 0 0 0 8 7 . 0 5 9 1 8 . 0 2 5 7 7 . 0 7 4 4 8 . 0 5 9 3 8 . _ 2 8 4 8 . 0 3 1 6 8 . 0 _ 3 1 5 8 . 0 9 1 6 8 . 8 4 7 6 . 0 9 3 7 7 . 0 4 1 9 7 . 0 1 1 3 8 . 0 9 4 9 4 . 0 2 9 0 8 . 8 3 5 7 . 0 1 9 0 8 . 0 0 2 5 8 . 0 1 7 3 8 . 0 1 3 6 4 . 0 4 4 1 8 . 4 8 4 7 . 0 9 1 0 8 . 0 7 6 1 6 . 0 7 5 6 7 . 0 1 9 4 8 . 0 7 0 8 7 . 0 2 4 8 . 0 6 5 4 8 . 0 _ 7 1 4 8 . 0 0 5 5 8 . 0 6 7 9 5 . 2 8 7 7 . 0 6 5 6 7 . 0 3 9 9 7 . 0 1 5 4 6 . 0 7 6 8 7 . 0 2 5 4 8 . 6 9 7 7 . 0 1 9 3 8 . 0 3 5 4 8 . 0 _ 1 4 6 8 . 0 6 0 6 8 . 8 8 8 7 . 0 5 5 5 8 . 0 8 9 0 8 . 0 9 0 3 8 . 0 9 9 4 7 . 0 3 9 2 8 . 8 5 6 8 . 0 4 9 1 8 . 0 4 0 7 8 . 0 8 9 6 8 . 0 _ 5 1 8 7 . 6 3 8 7 . 0 7 7 2 7 . 0 4 0 9 4 . 0 8 7 0 4 . 0 5 6 2 5 . 0 7 1 7 2 . 9 5 8 5 . 0 0 7 9 7 . 0 4 1 3 7 . 0 1 2 9 7 . 0 1 7 5 7 . 0 _ 4 7 6 8 . 0 5 4 6 8 . 0 3 3 4 4 . 0 9 7 2 5 . 0 7 5 3 7 . 0 7 3 0 8 . 1 1 1 3 . 0 1 4 0 8 . 0 4 4 3 8 . 0 3 5 6 7 . 0 2 3 4 8 . 0 4 4 0 8 . _ 7 9 1 7 . 0 0 4 0 7 . 0 2 4 9 4 . 0 9 0 1 6 . 0 9 5 0 6 . 3 2 6 6 . 0 3 3 3 3 . 0 7 9 6 6 . 0 2 0 5 6 . 0 7 6 0 6 . 0 9 1 7 6 . 2 4 4 6 . 0 _ 9 2 3 9 . 0 4 6 2 9 . 0 3 5 1 9 . 0 2 3 1 9 . 9 6 9 8 . 0 5 0 1 9 . 0 0 9 4 8 . 0 4 8 0 9 . 0 2 1 3 9 . 0 8 8 9 8 . 7 8 2 9 . 0 6 7 2 9 . 0 _ 7 2 4 8 . 0 9 7 3 8 . 0 7 2 5 7 . 6 7 0 8 . 0 8 7 7 7 . 0 8 7 9 7 . 0 6 6 2 7 . 0 7 0 9 7 . 0 5 2 3 8 . 1 0 7 7 . 0 9 9 2 8 . 0 5 9 3 8 . 0 _ fi 9 8 7 8 . 0 9 2 8 8 . 2 7 1 4 . 0 9 5 0 5 . 0 1 3 1 7 . 0 9 9 7 7 . 0 8 6 3 3 . 0 4 0 8 7 . 0 6 1 8 . 0 6 5 6 7 . 0 1 9 4 8 . 0 2 5 1 8 . 0 _ 4 9 9 8 . 8 0 0 9 . 0 4 0 9 7 . 0 2 0 9 8 . 0 3 1 4 8 . 0 6 4 7 8 . 0 3 6 4 7 . 3 9 6 8 . 0 6 6 0 9 . 0 8 9 5 8 . 0 6 4 9 8 . 0 6 9 9 8 . 0 _ 1 5 9 8 . 0 7 8 9 8 . 0 9 5 5 6 . 0 2 9 4 7 . 0 7 2 2 8 . 0 7 0 6 8 . 3 6 8 5 . 0 7 1 5 8 . 0 1 7 7 8 . 0 1 0 5 8 . 0 0 1 8 8 . 0 7 0 8 8 . _ 9 8 1 8 . 0 0 7 1 8 . 0 8 5 5 7 . 0 2 1 7 7 . 0 0 1 0 7 . 4 0 5 7 . 0 7 5 9 4 . 0 6 9 3 7 . 0 1 5 1 8 . 0 6 4 2 7 . 0 3 8 9 7 . 0 3 9 7 . 0 _ 0 3 3 8 . 0 8 6 3 8 . 0 _ 6 8 3 8 . 1 0 4 8 . 0 9 4 5 7 . 0 2 3 9 7 . 0 3 4 9 7 . 0 0 9 1 8 . 0 5 7 3 7 . 6 8 9 7 . 0 6 4 1 8 . 0 1 9 8 7 . 0 1 9 2 8 . 0 4 8 3 8 . 0 5 6 8 7 . 9 2 1 8 . 0 9 0 0 8 . 0 7 6 1 8 . 0 0 5 8 7 . 0 9 5 9 7 . 0 8 5 3 8 . 8 3 9 7 . 0 9 7 3 8 . 0 6 6 4 8 . 0 _ 1 1 7 8 . 0 0 0 7 8 . 6 2 9 7 . 0 4 0 5 8 . 0 3 5 2 8 . 0 3 6 3 8 . 0 8 7 9 7 . 0 7 1 3 8 . 6 1 7 8 . 0 4 5 3 8 . 0 6 8 6 8 . 0 0 4 7 8 . 0 _ 6 0 8 8 . 8 5 7 8 . 0 5 5 9 7 . 0 6 5 5 8 . 0 6 7 3 8 . 0 1 1 5 8 . 0 0 5 0 8 . 7 0 4 8 . 0 2 2 7 8 . 0 0 1 4 8 . 0 5 2 7 8 . 0 3 2 7 8 . 0 . A 1 2 . 5 8 9 1 . 5 8 5 3 . 8 6 6 6 . 5 7 4 3 . 6 7 8 1 . 0 6 2 . 0 6 8 4 . 9 7 4 7 . 3 8 3 1 . 9 7 8 3 . 4 8 4 7 . 3 5 1 . 7 . 3 3 . 4 8 4 2 . 4 6 7 . 2 7 4 7 . 8 7 5 3 . 5 7 2 0 . 9 7 6 1 . 4 6 0 3 . 8 8 8 . 3 8 2 4 . 8 7 7 5 . 3 8 9 2 . 3 8 9 6 . 6 8 1 4 . 6 7 9 . 0 8 6 2 . 4 8 3 9 . 1 8 8 6 . 3 8 5 9 . 6 7 4 7 . 2 0 4 . 6 8 8 8 . 1 8 5 2 . 6 8 8 6 . 6 8 7 . 3 - N - A 0 2 1 1 - 4 - G e M"
        },
        {
            "title": "X\nA\nM\nM\nE\nG",
            "content": "2 - E 2 - E 5 . 2 - Q - A 3 . 3 -"
        },
        {
            "title": "A\nM\nA\nL\nL",
            "content": "B 2 + O 9 + O T"
        },
        {
            "title": "2\nV\n-\nR\nE\nW\nO\nT",
            "content": "B 2 7 + O 17 e p s l / a a n i e , ) b ( u r - h r e e h i c , a a a t t a + + 4 2 r t e ) ( O : 4 a o s l 5 1 . , t n e a u e e e s t e . ) a n ( d e o e p s u l a , ) r ( ) 4 2 0 2 , . t R ("
        },
        {
            "title": "2\nV\n-\nR\nE\nW\nO\nT",
            "content": "7 . o e g e t"
        },
        {
            "title": "Translation Templates used for both CPT and SFT",
            "content": "Source: {{ source }} Translate the source text from {{ lp0 }} to {{ lp1 }}. Target: {{ target }} Source: {{ source }} Translate from {{ lp0 }} to {{ lp1 }}. Target: {{ target }} Write the text in {{ lp0 }} in {{ lp1 }}. Text: {{ source }} Target: {{ target }} Translate the following text from {{ lp0 }} to {{ lp1 }}: Text: {{ source }} Translation: {{ target }} Translate the following {{ lp0 }} source text to {{ lp1 }}: {{ lp0 }}: {{ lp1 }}: {{ source }} {{ target }} Please translate this text from {{ lp0 }} into {{ lp1 }}. {{ lp0 }}: {{ lp1 }}: {{ source }} {{ target }} Make translation of the given text from {{ lp0 }} to {{ lp1 }}. {{ lp0 }}: {{ lp1 }}: {{ source }} {{ target }} {{ source }} {{ lp0 }}: Translate the {{ lp0 }} text above into {{ lp1 }}. {{ target }} Figure 7: Examples of prompt templates used to construct translation instructions for both CPT and SFT. We used jinja to create hundreds of such instructions. Location Codes D.2 Prompts for data generation and Emoji Substitutions Social Media Formatting Music Legal References Sports Statistics Music Notation File Paths Research Citations verification Problems Found in Tülu RLVR"
        },
        {
            "title": "Datasets",
            "content": "As discussed in Section 3.3, the GRPO stage led to improvements only on IFEval. Upon closer inspection, we found that the Tülu RLVR dataset was constructed by appending artificial suffixes and prefixes to existing prompts in order to make them verifiable. However, this process introduces several problematic prompts that can be confusing for the model, as illustrated in Figure 10. In the math partition of the dataset, we also observed that all prompts begin with the same three incontext examples, which are not essential to solving the task and likely cause the model to overfit to specific prompt format. After removing such inconsistencies and filtering for quality, the dataset 18 is reduced to less than 50% of its original size. When training on this cleaned version, we found that the 9B model performed worse than its WPOonly counterpart. The only model that showed improvements from GRPO was the 2B variant. These findings suggest that while GRPO with verifiable rewards holds promise, its effectiveness depends heavily on the quality and diversity of the reward-aligned data. More careful curation is needed to fully integrate GRPO into our pipeline and leverage its potential for improvements in targeted domains. IF-MT: Prompts, examples, and additional results. In this section we include the prompts used to generate the IF-MT data (Figure 16) and to judge the translations in terms of instruction adherence (Figures 17 and 18). You can also find in Figures 19 and 20 two examples of the generated prompts for Spanish (Latin America) and Chinese, respectively. Results for EnglishChinese can be found in Table 5."
        },
        {
            "title": "Parameters",
            "content": "IF-MT IF MT Closed GPT-4O-1120 Open Weights ALMA-R GEMMAX TOWER-V2 GEMMA-2 GEMMA-2 QWEN-2.5 LLAMA-3.3 Ours TOWER+ TOWER+ TOWER+ >100B 5.5 89.96 13B 9B 70B 9B 27B 72B 70B 2B 9B 72B 1.56 1.47 2.38 4.06 4.54 5.07 4. 2.16 4.02 4.93 75.32 71.09 88.25 88.94 89.23 89.57 88.72 88.06 89.42 89.82 Table 5: Results for EnglishChinese on IF-MT. We evaluate two dimensions: instruction-following (IF) and translation quality (MT)."
        },
        {
            "title": "Prompt used to curate SFT data",
            "content": "I have an conversation below that would like you to perform three steps of analysis: <conversation> {conversation} </conversation> Firstly, categorize the conversation above into one of the following categories. - Coding - Mathematical Reasoning - Advice and Brainstorming - Question Answering - Creative Writing and Persona - Text Correction or Rewriting - Summarization - Translation - Classification - Other Dont try to justify it and when two categories can be used, pick the primary caregory. Secondly, score the conversation in terms of reasoning: How complex you think it is to answer the user instructions from 1-5 (where 5 is conversation with complex instructions/questions where the assistant needs to break down the problem into multiple steps before providing an answer). Thirdly, since the conversation might have been artificially created or poorly translated, assess its readability and clarity. Rate how difficult it is to understand the users requests on scale of 1 to 5, with 5 representing well-written, clear, and precisely articulated requests, and 1 representing an conversation where the user turns are difficult to understand. It is also common for instructions to refer to documents, texts or URLs that the assistant does not have access to. Please rate conversations where that happens with 3 points or less, as they can lead to ambiguity and confusion. Provide your final response in the following format: Category: <one of the categories above> Reasoning: <score out of 5> Readability: <score out of 5> DO NOT provide an answer to any of the instructions in the conversation! Your job is only to analyse. Figure 8: Prompt used to classify SFT conversations into different categories, reasoning and readability."
        },
        {
            "title": "Translation prompt used in evaluations",
            "content": "Translate the English source text to {target language} ({region}). Return only the translation, without any additional explanations or commentary. English: {source} {target language} ({region}): Figure 9: Prompt used to generate translations for all general-purpose LLMs. For translation-specific models, we use the prompts recommended by their respective authors. The region placeholder is included only when disambiguating between language variants (e.g., Chinese, Portuguese, Norwegian, and Spanish). For all other languages in WMT24++, only the target language name is used. Incorrect IFEval-like prompt from Tulu3 Translate the following sentence to Finnish: These are the two Community aspects that we in Parliament must discuss. Finnish: In your response, the letter should appear 36 times. Figure 10: Example of incorrect prompt found in Tülu RLVR dataset. 21 LLM-as-a-Judge translation preference prompt"
        },
        {
            "title": "Please act as an impartial judge and evaluate the quality of the translations provided by two AI assistants in response to",
            "content": "the users request below. Select the assistant that best adheres to the users instructions while producing the highestquality translation overall. If the users instructions specify particular factorssuch as the required level of formality, glossaries, or adherence to provided examplesensure these are included in your evaluation. Begin by comparing the two translations and provide concise explanation of your assessment. Avoid personal opinions or biases, and do not favour one assistant over the other. Be objective and impartial. After providing your explanation, deliver your final verdict strictly in this format: Chosen: <[A] if Assistant is better, [B] if Assistant is better, or [T] if both are equally good or bad.> [User Instruction] {instruction} [End of User Instruction] [Start of Assistant As Response] {assistant response} [End of Assistant As Response] [Start of Assistant Bs Response] {assistant response} [End of Assistant Bs Response] Figure 11: Translation LLM-as-a-judge prompt used to validate the chosen and rejected answers after MBR. All samples where the LLM-judge disagrees with the chosen and rejected are discarded."
        },
        {
            "title": "Date Formatting",
            "content": "ID: DATE_001 NAME: MM/DD/YYYY Format DESCRIPTION: Convert dates to MM/DD/YYYY REQUIRES EXAMPLE: False VERIFICATION: b(0[1-9]1[0-2])/(0[1-9][12]d3[01])/d{4}b EXAMPLE INPUT: \"January 5, 2024\" EXAMPLE OUTPUT: \"01/05/2024 Figure 12: Examples of transformation template for generation of translation-verifiable instructions. 23 IF-MT Training Data Source Docs and Guidelines Prompt Requirements: The document must be EXACTLY the specified length Must naturally incorporate elements that match ALL guidelines Keep the text coherent and natural For paragraphs, use 23 sentences per paragraph Do not mention the guidelines explicitly in the text Output format: ###SOURCE### [Your text here] ###GUIDELINES### [Copy the given guidelines exactly] ###END### Here are two examples: Example 1: LENGTH: 1 sentence TOPIC: Technology - Software Development GUIDELINES: 1) 2) [Date Formatting] Convert dates to MM/DD/YYYY [Terminology] Add full form in parentheses after acronyms ###SOURCE### The AI team announced on March 15th that their new NLP system had achieved breakthrough performance in code generation. ###GUIDELINES### 1) Convert dates to MM/DD/YYYY, e.g., March 15th to 03/15/2022 2) Add full form Natural Language Processing in parentheses after acronyms, e.g., NLP (Natural Language Processing) ###END### Example 2: LENGTH: 1 paragraph TOPIC: Social Media - Digital Marketing GUIDELINES: 1) 2) 3) [Case Formatting] Convert all text to lowercase [Social Media] Add hashtags at end of sentence for: brands (#brand), actions (#marketing) [Email Formatting] Convert email mentions to [EMAIL]address[/EMAIL] ###SOURCE### Instagram and TikTok launched new advertising features last week. Digital marketers can now contact our support team at help@instagram.com for early access to these tools, while brands on TikTok are already reporting increased engagement rates. ###GUIDELINES### 1) Convert all text to lowercase 2) Add hashtags at end of sentence for: brands (#brand), actions (#marketing) 3) Convert email mentions to [EMAIL]address[/EMAIL] ###END### Your task: LENGTH: {length} TOPIC: {topic} GUIDELINES: {guideline_txt} Important Instructions for Source Text: 1. Write text that contains all necessary elements that COULD be transformed according to the guidelines, but deliberately does NOT follow the guidelines yet 2. For example: If guideline requires formatting dates as MM/DD/YYYY, write dates in different format If guideline requires wrapping emails in tags, include email addresses without tags If guideline requires expanding acronyms, use acronyms without expansions 3. The text should be natural and coherent, reading as normal document would 4. Make sure every guideline has corresponding elements in the text that can be transformed 5. Think of the source text as the \"before\" version that will later be transformed into an \"after\" version following the guideline Figure 13: Prompt for generation of verifiable translation instructions. 24 LLM-as-a-Judge IF-MT Training Data Verification Prompt You are an expert judge evaluating source documents that will be used for guideline-based text rewriting tasks. Your task is to carefully analyze whether text follows any given guidelines. First, analyze each guideline carefully, then decide if the text follows ANY of the guidelines. Example 1: Guidelines: 1) [Email Format] Convert email to [EMAIL]address[/EMAIL] 2) [Case] Convert product names to UPPERCASE Source Text: Contact us at help@company.com about the zenith software. ###EVALUATION### Analysis: Guideline 1 (Email Format): - Text contains raw email \"help@company.com\" - Email is NOT wrapped in [EMAIL] tags - This guideline is NOT followed Guideline 2 (Case): - Text contains product name \"zenith\" - Product name is in lowercase - This guideline is NOT followed Number of guidelines followed: 0/2 Guidelines Check: 0 ###END### Example 2: Guidelines: 1) [Email Format] Convert email to [EMAIL]address[/EMAIL] 2) [Case] Convert product names to UPPERCASE Source Text: Contact us at [EMAIL]help@company.com[/EMAIL] about the ZENITH software. ###EVALUATION### Analysis: Guideline 1 (Email Format): - Text contains email wrapped in [EMAIL] tags [EMAIL]help@company.com[/EMAIL] - This guideline is FOLLOWED Guideline 2 (Case): - Text contains product name \"ZENITH\" in UPPERCASE - This guideline is FOLLOWED Number of guidelines followed: 2/2 Guidelines Check: 1 ###END### Example 3: Guidelines: 1) Convert month names to 3 letter abbreviations 2) Convert lists to 1., 2., format Source Text: The meeting is scheduled for January 1st, 2023. The agenda includes: 1) Budget review, 2) Project updates. ###EVALUATION### Analysis: Guideline 1 (Month Abbreviations): - Text contains full month name \"January\" - Month is NOT in 3-letter format (should be \"Jan\") - This guideline is NOT followed Guideline 2 (List Format): - Text contains list with format \"1)\" and \"2)\" - Lists are NOT in \"1.\" format - This guideline is NOT followed Number of guidelines followed: 0/2 Guidelines Check: 0 ###END### Now evaluate this input: Topic: {topic} Length: {length} Guidelines: {guidelines} Source Text: {source_text} Your evaluation must: 1. Analyze each guideline separately and explicitly state if its followed 2. Count the total guidelines followed 3. Conclude with Guidelines Check score: Score 1 if ANY guideline is followed; Score 0 if NO guidelines are followed Use exactly this format: ###EVALUATION### Analysis: (Analysis of each guideline) Number of guidelines followed: [X/Y] there is no such thing as half guideline, so should be an integer between 0 and (also an integer) Guidelines Check: [1 for ANY followed, 0 for NONE followed] ###END### Figure 14: Prompt for verification of verifiable translation instructions. 25 LLM-as-a-Judge translation scoring prompt You are professional translator and evaluator. Your task is to evaluate how well an assistant has handled translation request from user. Evaluate the translation based on the following criteria: 1. Adequacy (Accuracy of Meaning) - Assess whether the translation fully and accurately conveys the meaning of the source text. - Penalize mistranslations, omissions, or additions that distort the intended message. 2. Fluency (Readability & Grammar) - Ensure the translation reads naturally and is grammatically correct in the target language. - Penalize awkward phrasing, unnatural word choices, or structural issues. - It should be easy to read and understand, as if it were originally written in the target language. 3. Cultural Appropriateness - Ensure that the translation is culturally appropriate for the target audience. 4. Instructions Adherence - If provided, evaluate how well the translation adheres to any specific instructions or guidelines provided by the user. Otherwise, ignore this criterion. Provide detailed feedback on any issues and suggest improvements. Conclude with score from 1 to 5: - 5 Perfect translation (fully accurate and fluent in the target language while adhering to instructions). - 4 Good translation (minor errors but generally fluent and natural sounding) and adheres to instructions. - 3 Acceptable but flawed (some errors in meaning, fluency, or structure). - 2 Translation is acceptable but it does not adhere to the instructions. - 1 Poor translation (major errors affecting comprehension). [User Instructions] {instruction} [End of User Instructions] [Assistant Translation] {answer} [End of Assistant Translation] NOTE: Your answer must terminate with the following format: Final Score: <score> Figure 15: LLM-as-a-judge prompt used to score Translation data for supervised fine-tuning. 26 IF-MT meta prompt for data generation. As an expert prompt engineer, create detailed prompt for language model to perform the following task: translation of source text, given set of ${n_rules} rules. The source text should abide by the followin parameters: - Source language: ${source_language} - Topic: ${topic} - Subtopic: ${subtopic} - Style: ${style} - Source length: ${source_length} The translation should be in ${target_language}, and your generated prompt must specify set of ${n_rules} rules. IMPORTANT: These rules must be objectively verifiable and should be clearly stated in the prompt. The language model should be instructed to follow these rules when translating the source text. An example of verifiable rule is Convert dates to the format DD/MM/YYYY.; an example of an unverifiable rule is Make the translation sound more professional.. Keep in mind that the rules should make sense in the context of the source text and the target language. IMPORTANT: Make sure that the source you create has elements that correspond to the rules you set. To demonstrate the expected output, also provide reference translation following the requested requirements at the end. IMPORTANT: Your response should be structured as follows: <START OF PROMPT> [INSERT ONLY THE PROMPT HERE COMBINING SOURCE, RULES, AND AN INSTRUCTION. REMIND THE MODEL TO RETURN ONLY THE TRANSLATION. NOTHING ELSE.] <END OF PROMPT> <START OF REFERENCE> [INSERT ONLY THE REFERENCE TRANSLATION. NOTHING ELSE.] <END OF REFERENCE> ABIDE STRICTLY BY THE REQUESTED FORMAT. Figure 16: IF-MT meta prompt for data generation. We sample attributes (e.g., topic, subtopic) from the lists provided by Pombal et al. (2025a). We ask the model for 2 to 4 rules. 27 IF-MT judgement prompt: Part You are an expert judge evaluating translation quality. You will be presented with: - text, prompting model for translation of source following some rules - translation to evaluate Rate the translation on scale of 1-6 based on how well it follows the specified rules and instructions in the prompt, regardless of overall translation quality, according to the following criteria: - Rule Adherence: Does the translation follow all explicit rules stated in the prompt? - Instruction Compliance: Are specific formatting, style, or technical instructions followed? - Constraint Observance: Are any limitations or restrictions properly respected? - Specification Accuracy: Does the output match the exact specifications requested? - Requirement Fulfillment: Are all mandatory elements present as instructed? Scoring Rubric: 6 - Perfect Compliance - Follows every single rule and instruction precisely - No deviations from any specified constraints - All requirements fully met as requested - Complete adherence to formatting/style directives - Perfect execution of all procedural instructions - Zero rule violations of any kind 5 - Excellent Compliance - Follows nearly all rules with only trivial deviations - Minor lapses that dont affect core requirements - Strong adherence to most constraints and directives - Formatting/style mostly correct - Very few rule violations, all inconsequential 4 - Good Compliance - Follows most important rules correctly - Some minor rule violations that dont undermine main objectives - Generally respects constraints and limitations - Adequate adherence to formatting requirements - Few significant rule violations Figure 17: IF-MT judgement prompt. We follow the 1-to-6 direct assessment approach of Pombal et al. (2025a) due to its reported effectiveness (part 1/2). 28 IF-MT judgement prompt: Part II 3 - Fair Compliance - Follows some rules but misses several others - Notable violations of stated constraints - Inconsistent adherence to instructions - Some formatting/style requirements ignored - Multiple rule violations affecting compliance 2 - Poor Compliance - Fails to follow many stated rules - Significant violations of constraints and limitations - Poor adherence to specific instructions - Formatting/style requirements largely ignored - Frequent and notable rule violations 1 - No Compliance - Ignores most or all stated rules - Complete disregard for constraints and limitations - Fails to follow basic instructions - No attention to specified requirements - Systematic rule violations throughout Provide your evaluation in this JSON format: {\"feedback\": \"<detailed explanation of the score based on the criteria>\", \"result\": \"<only number from 1 to 6>\"} <START OF SOURCE TEXT> ${prompt} <END OF SOURCE TEXT> <START OF TRANSLATION> ${answer} <END OF TRANSLATION> You may proceed to evaluate the translation. Focus on evaluating the extent to which the translation follows the rules in the prompt, not its quality. Ensure the output is valid JSON, without additional formatting or explanations. Figure 18: IF-MT judgement prompt. We follow the 1-to-6 direct assessment approach of Pombal et al. (2025a) due to its reported effectiveness (part 2/2). 29 IF-MT example for EnglishSpanish (Latin America) You are professional translator specializing in English to Spanish (Latin American) translations. Your task is to translate the following short text about fashion technology, written in casual style: \"Hey fashion lovers! Just got my hands on the new SmartFit app (released on 5/15/2023) that scans your body in 3D and suggests clothes from over 50+ brands that would fit your measurements perfectly. Ive already saved $120 on returns this month! Check out their website at www.smartfit-tech.com or email them at help@smartfit-tech.com if you have questions.\" Follow these three specific rules when translating: 1. Convert all dates to DD/MM/YYYY format 2. Keep email addresses and website URLs in their original form without translation 3. Convert all dollar amounts to Mexican pesos (using an approximate conversion rate of 1 USD = 18 MXN) Return only the Spanish (Latin American) translation, nothing else. Figure 19: IF-MT example for EnglishSpanish (Latin America). IF-MT example for EnglishChinese. Translate the following English text about Stockholms cultural scene into Simplified Chinese. Follow these two specific rules: 1. Translate all proper names of museums, theaters, and cultural venues by providing both the Chinese translation and the original English name in parentheses. 2. Convert all years mentioned in the text to both the Gregorian calendar year and the corresponding Chinese zodiac animal year in parentheses. Text to translate: Stockholms vibrant cultural landscape captivated me during my visit in 2018. The citys artistic heart beats strongly at the Moderna Museet, where spent hours admiring contemporary masterpieces. In the evening, attended moving performance at the Royal Dramatic Theatre, which has been showcasing theatrical excellence since 1788. The following day, explored Fotografiska, photography museum housed in beautiful Art Nouveau building from 1906. What makes Stockholm truly special is how seamlessly it blends historical traditions dating back to with cutting-edge artistic expressions of 2022. Return only the Chinese translation following the rules above. No explanations or additional text. Figure 20: IF-MT example for EnglishChinese."
        }
    ],
    "affiliations": [
        "Instituto Superior Técnico & Universidade de Lisboa (Lisbon ELLIS Unit)",
        "Instituto de Telecomunicações",
        "MICS, CentraleSupélec, Université Paris-Saclay",
        "Unbabel"
    ]
}