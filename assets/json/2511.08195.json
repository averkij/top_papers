{
    "paper_title": "UI2Code^N: A Visual Language Model for Test-Time Scalable Interactive UI-to-Code Generation",
    "authors": [
        "Zhen Yang",
        "Wenyi Hong",
        "Mingde Xu",
        "Xinyue Fan",
        "Weihan Wang",
        "Jiele Cheng",
        "Xiaotao Gu",
        "Jie Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "User interface (UI) programming is a core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and single-turn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2Code$^\\text{N}$, a visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UI-to-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2Code$^\\text{N}$ establishes a new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/zai-org/UI2Code_N."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 2 5 9 1 8 0 . 1 1 5 2 : r Under review UI2CODEN: VISUAL LANGUAGE MODEL FOR TESTTIME SCALABLE INTERACTIVE UI-TO-CODE GENERATION Zhen Yang1, Wenyi Hong1, Mingde Xu2, Xinyue Fan2, Weihan Wang2, Jiele Cheng1, Xiaotao Gu2, Jie Tang1 1Department of Computer Science and Technology, Tsinghua University, 2Zhipu AI yang-zhen@mail.tsinghua.edu.cn, wenyi.hong@outlook.com jietang@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "User interface (UI) programming is core yet highly complex part of modern software development. Recent advances in visual language models (VLMs) highlight the potential of automatic UI coding, but current approaches face two key limitations: multimodal coding capabilities remain underdeveloped, and singleturn paradigms make little use of iterative visual feedback. We address these challenges with an interactive UI-to-code paradigm that better reflects real-world workflows and raises the upper bound of achievable performance. Under this paradigm, we present UI2CodeN, visual language model trained through staged pretraining, fine-tuning, and reinforcement learning to achieve foundational improvements in multimodal coding. The model unifies three key capabilities: UIto-code generation, UI editing, and UI polishing. We further explore test-time scaling for interactive generation, enabling systematic use of multi-turn feedback. Experiments on UI-to-code and UI polishing benchmarks show that UI2CodeN establishes new state of the art among open-source models and achieves performance comparable to leading closed-source models such as Claude-4-Sonnet and GPT-5. Our code and models are available at https://github.com/ zai-org/UI2Code_N."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in visual language models (VLMs) have opened up new possibilities for user interface (UI) coding, such as the automatic transformation of UI screenshots into executable code. As user interfaces constitute central part of software systems, automating their development could significantly reduce costs and expand access to front-end application creation. Unlike general programming tasks, UI coding is cyclical and tightly interwoven process of visual observation, reasoning, and code expression, continually refined through real-time visual feedback. At the same time, UI development poses unique challenges: from grasping overall layouts to correctly identifying nested components, while also capturing subtle visual details such as spacing, color, and typography. Crucially, all of these elements must be faithfully translated into long, executable code. Although visual language models (VLMs) have made remarkable progress on general vision understanding benchmarks, their performance in UI coding remains notably insufficient. Qualitatively, as shown in Figure 1, even advanced proprietary VLMs such as Gemini-2.5-Pro (Comanici et al., 2025) and Claude-4-Sonnet-Thinking encounter significant challenges in UI-to-code generation. Quantitatively, on the Design2Code benchmark (Si et al., 2024), commercial VLMs like Claude4-Sonnet achieve only 76.3, falling short of human evaluation standards, while leading open-source VLMs such as Qwen2.5-VL-72B (Bai et al., 2025), InternVL3-78B (Zhu et al., 2025), and Step-3321B (Team, 2025) score below 45/100. The gap becomes even more pronounced on more demanding tasks, such as UI polishing toward target prototypes or instruction-based editing from reference * Equal contribution; Corresponding author. Work was done when ZY, WH, MX, XF, JC interned at Zhipu AI. 1 Under review Figure 1: Top: Comparison of UI-to-code generation outputs from leading models versus our model, using the same reference screenshot. Our model achieves the highest fidelity, further enhanced by our UI polishing capability. Additional qualitative examples with diverse content, aspect ratios, and layouts are provided in Appendix A.5. Bottom left: Performance comparison on UI-to-code and UI polishing tasks. Bottom right: Test-time scaling curve of our model on the UI-to-code task, enabled by our interactive UI-to-code paradigm. designs, where both openand closed-source models consistently struggle  (Table 1)  . More recent approaches attempt to orchestrate complex agent-style workflows at inference (Jiang et al., 2025a; Wan et al., 2024; Wu et al., 2025), yet these remain fundamentally constrained by rigid heuristics and the inherent ceiling of current VLM capabilities. We attribute the current limitations of VLMs in UI coding to two key challenges. First, existing models lack strong multimodal coding capability, which is essential for reliably translating complex visual layouts into executable code. This weakness is further compounded by the tension between the complexity of UI-to-code generation, which demands intensive training, and the scarcity of high-quality paired data. Real webpages are abundant but their HTML is noisy and entangled with external resources, whereas synthetic datasets are clean but overly simplistic (Gui et al., 2025; Yun et al., 2024). Second, there is fundamental disconnect between existing single-turn UI-to-code paradigms and real-world UI development workflows, which limits both their performance ceiling and practical utility. Fundamentally, UI-to-code is inherently an interactive process of reasoning with visual feedback: rendered results cannot be inferred from code alone, and runtime factors such as font fallback, browser defaults, and DPI scaling make pixel-level fidelity unverifiable without actual rendering. In this work, we propose novel Interactive UI-to-Code paradigm that fundamentally departs from prior single-turn generation approaches, redefining UI-to-code as an iterative and interactive process of generation, editing, and polishing. Such paradigm provides flexible usage with enhanced performance and enables test-time scaling in UI-to-code generation. Guided by this paradigm, we present UI2CodeN, powerful visual language model trained via three-stage training pipeline: large-scale 2 Under review pretraining on noisy real-world data to build broad multimodal foundations, supervised fine-tuning on synthetic datasets to improve code quality, and reinforcement learning with carefully designed verifier to exploit unpaired real webpages while maintaining generation fidelity. Experimental results demonstrate that our UI2CodeN achieves state-of-the-art performance in UI coding. Building upon the core task of UI-to-code, UI2CodeN further extends its capabilities to UI polishing and UI editing. To sum up, our main contributions include: We propose Interactive UI-to-Code, new paradigm that reconceptualizes UI-to-code generation as iterative reasoning with visual feedback, enabling flexible code generation and editing. This approach naturally supports test-time scaling, e.g. achieving 12% improvement with four rounds of UI polishing. We present UI2CodeN, the first open-source VLM to incorporate UI-to-code, UI polishing and UI editing. UI2CodeN achieves state-of-the-art results across benchmarks including Design2Code (Si et al., 2024), Flame-React-Eval (Ge et al., 2025) and Web2Code (Yun et al., 2024), surpassing closed-source leading VLMs including Gemini-2.5-Pro and Claude-4-Sonnet, and advancing the open-source UI-to-code frontier by 35% on average. This work is the first to present the full training recipe of foundational coding VLM, systematically covering pre-training, fine-tuning, and reinforcement learning with novel reward design. Through this three-stage framework, we acquire broad foundational knowledge while balancing data realism and generation quality."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 UI-TO-CODE BENCHMARKS Design2Code (Si et al., 2024) introduced the first benchmark built from real-world webpages, along with visual-centric metrics such as Block-Match and CLIP similarity. Its construction pipeline prunes raw HTML by removing external dependencies and replacing images with placeholders, preserving real-world sources while simplifying the resulting webpages compared to their original distribution. Subsequent benchmarks, including Web2Code (Yun et al., 2024) and Flame-React (Ge et al., 2025), refined the data pipeline but continued to rely heavily on LLM-synthesized HTML. More recently, WebGen-Bench (Lu et al., 2025) broadened the evaluation scope to functional website generation, employing automated agents to test interactivity and functionality. 2.2 UI-TO-CODE DATASETS Progress in UI-to-code generation has been driven largely by dataset scaling. Early large-scale efforts were primarily driven by synthetic data. For instance, WebSight (Laurencon et al., 2024) introduced two million synthetically generated screenshotcode pairs using Tailwind CSS. Similarly, Web2Code (Yun et al., 2024) curated large-scale instruction-tuning dataset by combining LLMsynthesized data with refined existing resources. Later efforts such as WebCode2M (Gui et al., 2025) and Vision2UI (Gui et al., 2024) constructed million-scale datasets sourced from real-world webpages (e.g., Common Crawl (Common Crawl Foundation, 2007)), followed by extensive pruning and filtering. While these datasets preserve structural integrity, pruning often leads to oversimplified webpages. Despite their scale, all of the aforementioned datasets either rely on LLM-synthesized content or heavily pruned HTML that removes dependencies such as CSS, thereby limiting their fidelity to complex real-world webpage distributions. 2.3 UI-TO-CODE GENERATION MODELS AND SYSTEMS Large-scale Vision-Language Models (VLMs) excel in many multimodal tasks but struggle with UIto-code generation, often producing incomplete, inaccurate, or non-compilable code. Early efforts trained standalone models, e.g., Pix2code (Beltramelli, 2018), SightSeer (Laurencon et al., 2024), Flame (Ge et al., 2025), and WebCode2M (Gui et al., 2025), using synthetic datasets. These models, however, showed limited generalization, frequent failures, and remain unavailable to the community. 3 Under review In contrast, series of recent works move to leverage commercial VLMs through agent-based workflows. DECLARUI (Zhou et al., 2024) decomposes UI-to-code into detection, segmentation, and classification; DCGen (Wan et al., 2025) adopts divide-and-conquer strategy; and ScreenCoder (Jiang et al., 2025b) introduces modular multi-agent framework with grounding, planning, and generation."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 INTERACTIVE UI-TO-CODE PARADIGM UI coding is inherently process of continuous reasoning driven by interactions and visual feedback. In practice, developers rarely write code in isolation. To translate prototype into functional frontend code, they typically begin by writing high-fidelity draft implementation that aims to match the prototype as closely as possible, then iterateviewing the code editor alongside the real-time rendered output and constantly comparing the results with the intended design. Developers may also generate UI code directly from reference screenshots while refining it through targeted visual adjustments (e.g., producing code aligned with webpage screenshot but adapted to new color scheme). This workflow illustrates why interactive coding is natural: the process itself is structured around immediate feedback loops between code and rendering. From technical perspective, interactivity is not only natural but also necessary. Even highly skilled developers who can write precise HTML that deterministically maps to pixels cannot guarantee visual fidelity without feedback, since rendering depends on runtime factors such as font fallback, browser defaults, and DPI scaling. deviation as small as 1px is imperceptible without actually rendering the output. Consequently, one-shot generation methods are fundamentally limited in correctness and cannot push the performance ceiling. Interactive coding, by contrast, integrates feedback into the loop, enabling developers to both detect and correct these subtle discrepanciesthereby achieving higher accuracy and broader applicability. Therefore, to support real-world development with coding VLMs, it is crucial to enable interactive UI-to-code generation. Interactivity not only raises the performance ceiling of code generation but also broadens the scope and quality of applications. However, most existing UI-to-code approaches overlook this need. Early efforts with coding VLMs (and VLM-based agent workflows) and their evaluations, such as WebSight (Laurencon et al., 2024), Web2Code (Yun et al., 2024), and ScreenCoder (Jiang et al., 2025a), treat UI coding primarily as single-turn image-to-text task, neglecting the iterative, feedback-driven nature of real-world development. To address this gap, we propose new paradigm: Interactive UI-to-code generation. Unlike the traditional one-shot formulation, this paradigm extends the traditional notion of UI-to-code and redefines it as an iterative and interactive process. It explicitly captures how developers work in practice, emphasizing drafting, refinement, and targeted editing, all driven by rendering feedback. The paradigm comprises three key components. (1) UI-to-code. The entry point of the workflow, where the model generates UI code directly from given UI image. This aligns with prior definitions of UI-to-code. While this stage yields usable draft, achieving pixel-level fidelity remains challenge. (2) UI polishing. This stage refines the draft code by taking three inputs: the target UI image, the initial code, and its rendered output. The model then produces improved code that better matches the design. Through repeated iterations, residual discrepancies can be progressively reduced, allowing the generated UI to converge toward higher visual fidelity transform UI2Code to UI2CodeN. On one hand, this explicitly leverages the VLMs capacity to reason with images: rendered outputsderived from generated text tokensserve as additional visual feedback for refinement. On the other hand, it can be viewed as form of test-time scaling: recursively applying UI polishing enables trade-off between inference cost and output quality. (3) UI editing. Beyond polishing, UI editing addresses scenarios where existing UIs require targeted modifications. Given visual reference and modification instructions, the model generates code reflecting the requested changes. This supports flexible, user-driven adaptation and extends the paradigm from static reproduction to interactive design collaboration. 4 Under review Figure 2: Our interactive UI-to-code paradigm integrates UI-to-code, UI polishing, and UI editing. Iterative polishing enables continuous refinement, achieving test-time scaling for the UI-to-code task. 3.2 MULTI-STAGE TRAINING Although recent VLMs have demonstrated substantial progress on general vision benchmarks, their performance on UI coding remains limited. The challenges primarily arise from two aspects. First, the inherent difficulty of the UI coding task. The model must accurately perceive UI-style images, capturing fine-grained details such as icons, fonts, and line stylesdespite their distributions differing significantly from the natural images used in most pretraining. It must further contend with the complexity of code, as HTML frequently exceeds 10,000 tokens and is densely interwoven with CSS and JavaScript. Beyond these difficulties, precise alignment between UI images and code is required, spanning from global layout structures to individual elements. Second, the limitations of available training data. Although real webpages are abundant, their HTML is often noisy and entangled with external resources, making them unsuitable for direct use. In contrast, synthetic or pruned datasets provide clean structures but lack the richness of real-world complexity (Gui et al., 2025)(Yun et al., 2024). Faced with this trade-off, previous VLMs have typically resorted to synthetic or heavily pruned datasets to ensure basic UI-to-code generation (Laurencon et al., 2024; Yun et al., 2024). However, this reliance excludes large-scale real-world web data from pretraining and shifts complex webpages out of the training domain, thereby constraining performance in practical applications. To address these challenges, we adopt three-stage training pipeline. We first conduct continual pre-training on large-scale real-world webpage imageHTML pairs to establish broad UI coding knowledge. We then perform supervised fine-tuning on clean, curated datasets to enhance diverse functionalities such as UI-to-code, UI polishing, and UI editing. Finally, we leverage reinforcement learning to adapt the model to complex real-world distributions without relying on paired groundtruth HTML. Next, we detail each stage together with its tailored data and training strategy. 3.2.1 CONTINUAL PRE-TRAINING The objective of continual pre-training is to acquire knowledge from large-scale but noisy web data. Our primary corpus is built by crawling webpages that contain both HTML and full-page screenshots, yielding 10M UIcode pairs. Direct use of Common Crawl (Common Crawl Foundation, 2007) proved infeasible due to missing components (e.g., figures, CSS) that hinder faithful rendering. Instead, we use its URLs as seeds for large-scale crawling, followed by tag whitelisting and redundancy removal. To better align UI blocks with DOM elements and control sequence length, we adopt the GUI Referring Expression Generation paradigm (Hong et al., 2024). During training, the model receives the full webpage and the bounding box of randomly sampled DOM node, and must predict the corresponding HTML, ensuring tighter grounding between UI segments and underlying code. We further incorporate external UI-to-code datasets, including WebCode2M (Gui et al., 2025) and WebSight Laurencon et al. (2024). Although their HTML is synthetic or pruned, they preserve 5 Under review high-fidelity imagecode mappings, complementing our crawled corpus and improving whole-page alignment. To preserve general visionlanguage capabilities and enhance transferability, we interleave coding-related data with broad VLM tasks such as image captioning, VQA, OCR, grounding, and video understanding. In addition, we include more than 1B tokens of languagecode data to strengthen foundational coding competence. Training is initialized from an early checkpoint of GLM-4.1V-9B-Base (Hong et al., 2025), with learning rate of 2e-5, tensor parallel size of 2, and global batch size of 1,536. Continual pre-training covers 20M visioncode samples in total. 3.2.2 SUPERVISED FINE-TUNING for reinforcement To initialize the model with diverse UI coding capabilities (UI-to-code, UI polishing, learning, we introduce supervised fineUI editing) and prepare it tuning (SFT) stage using deep reasoning format. Model outputs follow the structure <think>{think content}</think><answer>{answer content}</answer>, ensuring transparent reasoning and consistent answering style. Unlike later stages, SFT emphasizes stylistic alignment and data fidelity. To this end, we employ state-of-the-art LLMs1 to generate diverse, complex, and well-structured single-page HTML files as ground-truth answers, while queries are constructed in reversed manner to guarantee correctness. To further enhance robustness, we carefully design task-specific data construction strategies. For UI polishing, we diversify rendered inputs using multiple VLMs (our model, GLM-4.5V (Hong et al., 2025), Claude-4-Sonnet) and derive reasoning traces via VLM-generated comparisons rather than direct prompts, yielding more accurate rationales. For UI editing, we cover addition, deletion, replacement, and adjustment operations, filter candidates with heuristic rules and manual checks, and address the difficulty of component addition by reversing high-quality deletion pairs. These details, though labor-intensive, ensure data diversity, precision, and reliabilityreflecting the deliberate care invested in our SFT stage. In total, we construct 80K high-quality samples, and train for 5 epochs with sequence length of 32,768, batch size of 256 (with packing) and learning rate of 5e-6. 3.2.3 REINFORCEMENT LEARNING RL offers two advantages over teacher-forcing objectives in pre-training and SFT: First, it directly optimizes visual similarity rather than token-level accuracy, better aligning with human judgment. Second, it trains directly from screenshots, avoiding noisy HTML and improving transferability. We jointly train on complementary UI-to-code and UI polishing tasks using GRPO (Shao et al., 2024), excluding KL and entropy regularization to raise the performance ceiling and improve stability. The corpus includes 12K real-world webpages (from Mind2Web (Deng et al., 2023) and 30K LLM-synthesized examples. For polishing, the input generated HTMLs are diversified with outputs from GLM-4.5V, UI2CodeN checkpoints, Claude-4-Sonnet, and RL intermediates. Additional data come from earlier UI2CodeN runs, where generated code is polished for U[1, 4] rounds. We use batch size of 64 with the rollout number of 16, and RL for 400 steps in total. Reward Design Reward design is the central challenge. CLIP-based similarity (Si et al., 2024) is brittleoverly sensitive to positional shifts and background colors yet blind to fine details. We therefore adopt GLM-4.5V, leading open-source VLM, as our verifier for both stability and scalability. second difficulty lies in calibration: in UI polishing tasks, candidates often appear visually similar, but VLMs assign inconsistent absolute scores. We address these issues through three refinements. First, baseline verifier scoring computes = verifier score(Itarget, Irollout) for each candidate, with 1 for render failures and 0 if worse than the reference; however, independent queries cause calibration drift. To mitigate this, we introduce comparator function comp score(target, cand1, cand2) that jointly evaluates candidate and reference within single query, ensuring consistent scaling; GLM-4.5V is finetuned with SFT to improve robustness. Finally, to ensure fairness across multiple rollouts (N 16), we adopt round-robin comparator: candidates are compared pairwise, and each is assigned score 1Here we use Claude-3-7-Sonnet-20250219-Thinking Under review equal to its number of wins, yielding consistent rankings at the cost of O(N 2) calls. Further details of the reward design are provided in the Appendix A.1. Our system is implemented on the modular decomposed reward framework of GLM-4.5V, supporting efficient ablations and flexible extensions."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Table 1: Experimental results on UI-to-Code and UI Polishing benchmarks. Bold text indicates the best score among open-source models, and underlined text indicates the best score across all models. Model UI-to-Code UI Polishing Design2Code Flame Web2Code UI2Code-Real UIPolish-Real UIPolish-Synthetic InternVL3-9B InternVL3-78B Qwen2.5-VL-7B Qwen2.5-VL-72B MiMo-VL-7B-SFT MiMo-VL-7B-RL Kimi-VL-A3B-Instruct Kimi-VL-A3B-Thinking GLM-4.1V-9B-Thinking Claude-4-Sonnet-thinking Claude-3.7-Sonnet-thinking GPT-5 GPT-4o o4-mini Gemini-2.5-pro Gemini-2.5-flash Doubao-1.5-thinking-vision Doubao-1.6-thinking-250715 UI2CodeN -9B-SFT UI2CodeN -9B-RL 15.3 30.0 29.1 41.9 28.3 28.7 27.3 38.8 64.7 81.2 77.7 89.7 35.3 63.8 89.5 70.5 53.7 62.4 79.3 88.6 4.1 EVALUATION SETUP Open-source VLM 11.3 51.3 25.0 46.3 10.0 8.8 50.0 36.3 72.5 12.3 45.5 37.2 64.1 44.3 38.3 69.1 46.6 71.3 Closed-source VLM 76.3 80.0 91.3 75.0 83.8 87.5 72.5 78.8 67.7 85.0 95. 85.1 73.3 93.7 62.7 77.9 90.6 85.7 55.6 67.2 80.8 92.5 16.5 30.4 26.1 40.9 33.9 30.4 26.1 27.0 53.0 63.5 55.8 67.8 21.7 59.1 68.7 62.6 38.3 43.4 67.0 76.5 4.0 10.0 11.0 23.0 17.0 16.0 14.0 14.0 42. 78.0 75.0 85.0 26.0 65.0 74.0 17.0 51.0 61.0 76.0 80.0 7.0 15.0 14.0 38.0 33.0 30.0 40.0 27.0 46.0 65.0 62.0 68.0 14.0 65.0 68.0 24.0 61.0 67.0 89.0 94.0 To evaluate the effectiveness of UI2CodeN on UI-to-Code generation task, we Benchmarks: conduct experiments on several widely used benchmarks, including Design2Code (Si et al., 2024), Flame-React-Eval (Ge et al., 2025), and Web2Code (Yun et al., 2024). However, these benchmarks primarily consist of relatively simple screenshots that may not fully capture the complexity of realworld webpages. To address this limitation, we further construct UI2Code-Real, benchmark of 115 webpages collected from in-the-wild sources. This benchmark serves as more realistic evaluation setting, allowing us to assess whether models trained with synthetic and curated data can generalize effectively to real-world UI-to-code scenarios. For the UI polishing task, we further construct UIPolish-bench, which consists of 100 synthetic webpages and 100 real-world webpages, providing balanced evaluation of both controlled and in-the-wild scenarios. more detailed description of these benchmarks, along with our curated UIPolish-bench and UI2Code-Real, is provided in Appendix A.4. Evaluation Metrics: We consider two main evaluation approaches: (1) CLIP scoring, which uses CLIP-based similarity to assess semantic alignment, as in Design2Code (Si et al., 2024); and (2) VLM scoring, which leverages visual large language models (VLMs) to provide human-aligned judgments of design fidelity and usability, as in Web2Code (Yun et al., 2024). In this work, moti7 Under review vated by the stronger visual and semantic capabilities of VLMs, we follow Hong et al. (2025) and adopt VLM-based scoring metrics. This choice is further supported by our reinforcement learning ablation studies (Sec. 4.3.2), where VLM rewards consistently outperform CLIP-based ones. For UI polishing, we design an evaluation protocol based on comparison with the original UI screenshot. Given an initial screenshot A, the model first generates corresponding rendering through UI-tocode generation, followed by polished rendering C. The evaluation then compares whether is visually closer to than B. If > in similarity to the ground-truth design, we count the instance as successful polish and increment the accuracy by one. 4.2 MAIN RESULTS To verify the effectiveness of UI2CodeN , we carried out experiments on two types of UI coding tasks, including UI-2 code generation and UI polishing. Table 1 reports the experimental results compared with both open-source and closed-source VLMs. Compared to several open-source VLMs, our proposed UI2CodeN -9B-SFT and UI2CodeN -9B-RL achieve substantial improvements across all benchmarks. In particular, on the UI-to-code benchmarks (a three public benchmarks like Design2Code, Flame, Web2Code, and curated real-world UI2Code-Real benchmark), UI2CodeN demonstrates consistent and significant gains. Notably, the performance of open-source VLMs on UI polishing is generally unsatisfactory. As shown in Table 1, all open-source VLMs achieve less than 50% accuracy on both real and synthetic polishing benchmarks. Intuitively, we set 50% as threshold: if the probability of successfully polishing given UI screenshot falls below 50%, the model effectively fails to demonstrate reliable polishing capability. Under this criterion, existing open-source VLMs cannot be regarded as possessing genuine UI polishing ability. In contrast, our UI2CodeN -9B-RL achieves 80.0% on UIPolish-Real and 94.0% on UIPolish-Synthetic, surpassing all open-source models by large margin and even matching the performance of leading closedsource systems such as Claude-4-Sonnet-thinking and Gemini-2.5-pro. These results verify that our interactive paradigm, coupled with multi-stage training, not only strengthens UI-to-code generation but also equips the model with robust UI polishing capability. Test-Time Scaling with UI Polishing. The interactive UI-to-code generation paradigm endows UI2CodeN with the ability to perform test-time scaling. Specifically, for UI-to-code generation task, we begin with an initial round of generation and then recursively refine the output by polishing the UI using the HTML and renderings produced in the previous round. To assess this approach across diverse web pages, we conduct experiments on both real and synthetic subsets of our selfconstructed UI2Code benchmark, evaluating performance over multiple interaction rounds , where = 1 corresponds to single round of generation without polishing. The results in Table 2 show that performance steadily improves on both real and synthetic datasets as the number of interaction rounds increases. Interestingly, performance on UI2Code-Synthetic saturates early at = 3, likely due to the lower difficulty of synthetic data (thus we omit evaluations for > 3). In contrast, performance on UI2Code-Real continues to improve consistently from = 1 through = 5. Table 2: Test-time scaling performance of interactive UI-to-code generation Benchmark = 1 = 2 = = 4 = 5 UI2Code-Real UI2Code-Synthetic 66.0 92.0 68.0 97.0 70.0 97. 73.0 74.0 4.3 ABLATION STUDY: THE IMPACT OF REWARD DESIGN In this section, we conduct thorough ablation study on the impact of RL reward design, including both UI polishing and UI-to-code. For all ablation experiments, we start from the SFT checkpoint of UI2CodeN, and run RL with batch size of 32, rollout number of 16, and learning rate of 1e-6. 4.3.1 REWARD DESIGN FOR UI POLISHING For UI polishing, we design the reward functions described in Section 3.2.3. We evaluate three strategies for assessing UI polishing performance: the vanilla verifier, the verifier with comparator function, and the verifier with both comparator function and round-robin strategy. The results 8 Under review in Table 3a highlight two key findings regarding reward design. First, the round-robin comparator verifier consistently achieves the best results. Unlike reward designs based on local judgments, this approach enables global ranking across candidates, aligning more closely with the practical goal of UI refinementselecting the best improvement among alternatives. Second, the effectiveness of comparator-based rewards depends heavily on the reliability of the underlying vision-language model (VLM). Using GLM-4.5V without fine-tuning as the verifier reduced accuracy and degraded performance by 3%, whereas our tailored verifier produced substantial gains. This underscores that weak or noisy reward signals can misdirect reinforcement learning rather than improve it. Table 3: Comparison of different reward designs across tasks. (a) UI polishing. (b) UI-to-code. RL Reward Design UIPolish-Synthetic UIPolish-Real Avg. Method Design2Code Flame SFT + RL (vanilla verifier) + comparator verifier + round-robin strategy 89.0 91.0 93.0 93.0 76.0 75.0 78.0 79.0 82.5 83.0 85.5 86.0 UI2CodeN -SFT RL with CLIP Reward RL with GLM-4.5V Reward 72.3 62.0 74.6 85.0 72.2 89.0 (a) Reward design in UI polishing task (b) Reward design in UI-to-code generation task 4.3.2 REWARD DESIGN FOR UI-TO-CODE GENERATION To enable reinforcement learning for UI-to-code generation, we leverage automatic similarity measures and human-aligned judgments to investigate their effectiveness as reward signals. Specifically, we design two experimental settings: 1) the CLIP score (Radford et al., 2021) is employed to provide continuous and fine-grained reward that reflects semantic consistency between the rendered UI from the generated HTML code and the original UI screenshot; and 2) the VLM score, where open-source visual language models (e.g., GLM-4.5V) offer human-aligned evaluations of layout fidelity. As shown in Table 3 (b), GLM-4.5V reward consistently surpasses CLIP reward across both the Design2Code and Flame-React-Eval benchmarks. Moreover, we observe that using CLIP reward fails to improve performance and in fact leads to degradation compared to the SFT baseline. This indicates that purely visual similarity signals are insufficient for capturing the semantic and structural fidelity required in UI-to-code generation, and may even misguide the optimization process. These findings highlight the importance of reward design: relying solely on visual similarity metrics may misalign reinforcement learning, whereas VLM-based rewards provide richer and more reliable feedback. 4.4 ABLATION STUDY: THE IMPACT OF REAL-WORLD WEBPAGES IN RL STAGE To further investigate the role of real-world webpages in the reinforcement learning (RL) stage of UI2CodeN , we conduct controlled comparison under identical data budgets (20k RL samples) and training steps (100 iterations) to isolate the effect of incorporating real webpages in the RL stage. While synthetic datasets provide controlled environments with clean labels and diverse coverage of UI patterns, they may fail to capture the complexity, noise, and distributional shifts that occur in real-world interfaces. To bridge this gap, we augment the RL stage with curated set of real webpages, where the original UI screenshots are collected from in-the-wild sources. Table 4 demonstrates that including real webpages in the RL stage consistently improves both semantic fidelity and rendering quality. Notably, the improvement is more pronounced on evaluation benchmarks that share similar distributional characteristics with real-world webpages, highlighting the necessity of real data for bridging the sim-to-real gap in UI-to-code generation. Table 4: The impact of real-world webpage data in reinforcement learning stage. RL Data Design2Code UI2Code-Real UIPolish-Synthetic UIPolish-Real without real data with real data 81.5 82.4 92.0 93. 65.0 80.0 68.7 75.0 9 Under review"
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduce UI2CodeN, 9B-parameter visionlanguage model with advanced UI coding capabilities. We further propose the Interactive UI-to-Code Generation paradigm, which formulates UI-tocode as an iterative, interactive process that extends both performance and applicability. UI2CodeN achieves state-of-the-art results on UI-to-code and UI polishing benchmarks, surpassing leading VLMs including Claude-4-Sonnet, and Gemini-2.5-Pro. We open-source UI2CodeN to support broader adoption and research."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude 4. 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Tony Beltramelli. pix2code: Generating code from graphical user interface screenshot. In Proceedings of the ACM SIGCHI symposium on engineering interactive computing systems, pp. 16, 2018. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Common Crawl Foundation. Common Crawl. http://commoncrawl.org, 2007. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Tong Ge, Yashu Liu, Jieping Ye, Tianyi Li, and Chao Wang. Advancing vision-language models in front-end development via data synthesis. arXiv preprint arXiv:2503.01619, 2025. Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Shaoling Dong, Xing Zhou, and Wenbin Jiang. Vision2ui: real-world dataset with layout for code generation from ui designs. CoRR, 2024. Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Bohua Chen, Yi Su, Dongping Chen, Siyuan Wu, Xing Zhou, et al. Webcode2m: real-world dataset for code generation from webpage designs. In Proceedings of the ACM on Web Conference 2025, pp. 18341845, 2025. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, 10 Under review Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, and Zuquan Song. Seed1.5-vl technical report, 2025. URL https://arxiv.org/abs/2505.07062. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv e-prints, pp. arXiv2507, 2025. Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael Lyu, and Xiangyu Yue. Screencoder: Advancing visual-to-code generation for front-end automation via modular multimodal agents. arXiv preprint arXiv:2507.22827, 2025a. Yilei Jiang, Yaozhi Zheng, Yuxuan Wan, Jiaming Han, Qunzhong Wang, Michael R. Lyu, and Xiangyu Yue. Screencoder: Advancing visual-to-code generation for front-end automation via modular multimodal agents, 2025b. URL https://arxiv.org/abs/2507.22827. Hugo Laurencon, Leo Tronchon, and Victor Sanh. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029, 2024. Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Webgen-bench: Evaluating llms on generating interactive and functional websites from scratch. arXiv preprint arXiv:2505.03733, 2025. OpenAI. Gpt-5. 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: Benchmarking multimodal code generation for automated front-end engineering. arXiv preprint arXiv:2403.03163, 2024. Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025a. URL https://arxiv.org/abs/2506.03569. 11 Under review Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, arXiv preprint Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv:2504.07491, 2025b. StepFun Team. Step3: Cost-effective multimodal intelligence, 2025. URL https://stepfun. ai/research/step3. Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang, Shuqing Li, Yintong Huo, and Michael Lyu. Automatically generating ui code from screenshot: divide-and-conquer-based approach. arXiv preprint arXiv:2406.16386, 2024. Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang, Shuqing Li, Yintong Huo, and Michael Lyu. Divide-and-conquer: Generating ui code from screenshots. Proceedings of the ACM on Software Engineering, 2(FSE):20992122, 2025. Fan Wu, Cuiyun Gao, Shuqing Li, Xin-Cheng Wen, and Qing Liao. Mllm-based ui2code automation guided by ui layout information. Proceedings of the ACM on Software Engineering, 2(ISSTA): 11231145, 2025. Sukmin Yun, Rusiru Thushara, Mohammad Bhat, Yongxin Wang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, et al. Web2code: large-scale webpageto-code dataset and evaluation framework for multimodal llms. Advances in neural information processing systems, 37:112134112157, 2024. Ting Zhou, Yanjie Zhao, Xinyi Hou, Xiaoyu Sun, Kai Chen, and Haoyu Wang. Bridging design and development with automated declarative ui code generation. arXiv preprint arXiv:2409.11667, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 12 Under review"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 REWARD DESIGN Here we illustrate our reward design in detail. We progressively refine our reward design. Each refinement builds on the previous one: Algo. 1 establishes baseline verifier scoring method; Algo. 2 corrects calibration drift between candidates and references; Algo. 3 ensures fairness across all candidates through pairwise round-robin comparisons. This sequence yields steady improvements in RL performance, with the final design achieving the strongest results. Here we denote the target UI as Itarget, the current UI-polish round number as t, and the reference webpage image that the last round generated as It1,ref. 1: Verifier scoring. Reward Algo. define verifier score(target, candidate) [0, 1]as the similarity score returned by the VLM when comparing candidate image against the target. Each candidate It,i is independently scored as Si = verifier score(Itarget, It,i). We also compute the reference score Sref = verifier score(Itarget, It1,ref). Rewards are then defined as Si, with penalties of 1 for failed rendering and 0 for candidates worse than the reference. While simple, this approach suffers from calibration drift, since Si and Sref come from separate queries. baseline design, the we In Reward Algo. 2: Comparator scoring. To place Si and Sref under the same calibration, we introduce pairwise comparator comp score(target, cand1, cand2). Given target, it evaluates both the candidate It,i and the reference It1,ref within single query, returning two scores that are directly comparable. This eliminates scale inconsistency between Si and Sref observed in Algo. 1. Since off-the-shelf VLMs remain unreliable in multi-image comparison, we fine-tune GLM-4.5V with SFT to improve accuracy and robustness. Reward Algo. 3: Comparator with round-robin. Beyond calibration, we also address fairness among multiple rollouts {It,i}N i=1. Evaluating them separately still risks misaligned similarity scores across queries. naive solution would input all rollouts jointly, but this is impractical since is large ( 16 in our setup) and current VLMs degrade with many images. Instead, we adopt roundrobin scheme: candidates are compared pairwise under the comparator, and each It,i is assigned score equal to its number of wins. This design achieves consistent and fair ranking across all candidates, at the cost of O(N 2) verifier calls. Algorithm 1 Scoring via Verifier Round Require target image Itarget, reference image from round (t1) denoted It1,ref , candidate set {It,1, . . . , It,N } Reward[t, i] 1 continue attempt to render It,i if rendering fails then Output reward map Reward[t, i] 1: Initialize Reward[t, i] 0 for all 2: for 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return Reward Reward[t, i] Si Reward[t, i] 0 end if else end if Sref VLM verifier score(Itarget, It1,ref ) Si VLM verifier score(Itarget, It,i) if Si Sref then 13 Under review Algorithm 2 Scoring via Comparator Round Reward[t, i] 1 continue attempt to render It,i if rendering fails then Require target image Itarget, reference image from round t1 denoted It1,ref , candidate set {It,1, . . . , It,N } Output reward map Reward[t, i] 1: Initialize Reward[t, i] 0 for all 2: for 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: return Reward end if (Sref , Si) VLM comparator score(Itarget, It1,ref , It,i) if Si Sref then Reward[t, i] Si Reward[t, i] 0 end if else Algorithm 3 Scoring via Comparator and Round-Robin Round candidates that pass the first-stage screening attempt to render It,i if rendering fails then Require target image Itarget, reference image from round denoted It1,ref , candidate set {It,1, . . . , It,N } Output reward map Reward[t, i] 1: Initialize Reward[t, i] 1 for all {1, . . . , } 2: Pool 3: First-stage screening (vs. round-t1 reference) 4: for 1 to do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for end if (Sref , Si) VLM comparator score(cid:0)Itarget, It1,ref , It,i if Si Sref then Reward[t, i] 1 continue Pool Pool {i} Reward[t, i] 0 end if else (cid:1) no improvement over round-t kept for round-robin hard penalty (cid:1) else if Sj > Si then Reward[t, i] Reward[t, i] + 1 (Si, Sj) VLM comparator score(cid:0)Itarget, It,i, It,j if Si > Sj then 17: Round-robin among candidates of round 18: for all unordered pairs {i, j} Pool with = do 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: end for 29: return Reward Reward[t, i] Reward[t, i] + 0.5 Reward[t, j] Reward[t, j] + 0.5 Reward[t, j] Reward[t, j] + 1 end if else A.2 REWARD IMPLEMENTATION A.2.1 GLM-4.5V VISUAL SCORE FOR UI-TO-CODE RL TRAINING To provide reward signal for the RL stage of UI-to-code training, we employ GLM-4.5V as visual evaluator. Given the original UI screenshot and the rendering generated from the rollout HTML code during training, GLM-4.5V produces similarity score for the rendered output. Specifically, it assigns value in the range 0100, reflecting how closely the rendering matches the ground-truth 14 Under review screenshot. We then normalize this score to the range [0, 1] and use it as the reward signal. This continuous formulation provides fine-grained feedback, enabling more stable optimization compared to binary success/failure rewards. The prompt provided to the GLM-4.5V model for this evaluation is as follows: Prompt You will be given two images: The first image is the reference image (design draft or target rendering). The second image is the code rendering, which is generated based on the first image using HTML/CSS/frontend code. Your task is as follows: Compare the overall similarity between the two images, on scale from 0 to 100: - 0 means completely dissimilar. - 100 means perfectly identical. When scoring, you should comprehensively consider the following aspects: - Layout (whether the structural positions are consistent) - Color scheme (whether the colors are faithfully reproduced) - Typography (font, font size, line spacing, etc.) - Spacing and alignment (whether element spacing and alignment are accurate) - Fine details (button styles, icons, shadows, borders, etc.) Strictly follow the output format below: First, provide the final score, where the value must be enclosed in LaTeX boxed{}. Then, provide justification for the score, explaining which aspects are similar, which aspects differ, and the main factors influencing the score. A.2.2 OUR VERIFIER VISUAL SCORE FOR UI POLISHING RL TRAINING To provide reliable reward signal for reinforcement learning in the UI polishing task, we design visual verifier that evaluates the fidelity of polished renderings against the original UI screenshots. Given an initial rendering and its polished counterpart C, the verifier compares both with the ground-truth screenshot and produces similarity score. Specifically, the verifier assigns score in the range of 0100 based on multiple visual dimensions such as layout, color, typography, spacing, and fine-grained details. This raw score is then normalized to the range [0, 1] and used as reward signal for training. In our RL framework for UI polishing, we adopt triplet-based evaluation scheme. Given the reference screenshot A, the initial rendering B, and the polished rendering C, the visual verifier computes similarity scores score(A, B) and score(A, C). The reward is then defined as the normalized score of the polished output: = score(A, C) , [0, 1]. (1) In addition to this absolute reward, the triplet formulation enables relative success criterion: if score(A, C) > score(A, B), (2) the polishing step is considered an improvement over the initial rendering. This dual use of absolute scoring and relative comparison ensures that the model not only maximizes fidelity to the reference but also consistently outperforms its own initial generations, leading to more stable and effective RL optimization. 15 Under review Prompt You will be given three images: - The first image is the reference design (target screenshot). - The second and third images are code renderings generated based on the reference. Your task is as follows: 1. Assign similarity score (0100) to both the second and third images with respect to the reference: - 0 = completely dissimilar. - 100 = perfectly identical. - When scoring, consider the following dimensions with approximate weights: - Layout structure (30%): element positions, alignment, and overall layout. - Color fidelity (25%): background, text, button colors, etc. - Typography (20%): font size, weight, spacing, line height, etc. - Spacing ratios (15%): margins, paddings, and spacing between elements. - Element details (10%): button corners, borders, icon styles, etc. - Ignore differences in actual image content (e.g., photos, icons), and only evaluate style fidelity. 2. Provide brief justification for each score: - List 23 major differences and explain why they affect the score. - If the rendering is highly consistent, state the reasons (e.g., layout and colors are almost identical). 3. Provide final conclusion: indicate which rendering (second or third) is closer to the reference. - The conclusion must be enclosed in LaTeX boxed{}. - For example: boxed{The second image is better} 4. The output format must strictly follow this template: A.3 EVALUATION METRICS SPECIFICATIONS A.3.1 EVALUATION FOR UI-TO-CODE For the UI-to-code task, we employ o4-mini as the visual evaluator to assess the fidelity of generated renderings. Given the reference screenshot and the rendering generated from the predicted HTML/CSS code, o4-mini outputs similarity score score(A, B) in the range [0, 100], where higher values indicate greater visual resemblance. To obtain robust evaluation metric, we define the final accuracy as the proportion of samples whose similarity score exceeds threshold of 80: Accuracy = 1 N (cid:88) i=1 {score(Ai, Bi) 80}, (3) where denotes the total number of evaluated UI examples. This threshold-based criterion ensures that only renderings with sufficiently high fidelity to the reference are considered successful. The prompt provided to judge the similarity between the original UI screenshot and the rendering image is as follows: 16 Under review Prompt the rendering generated from the first You will be given two images: - The first image is the reference screenshot (design draft or target rendering). - The second image is HTML/CSS/frontend code. Your task is to evaluate the similarity between the two images and assign score on scale from 0 to 100: - 0 means completely dissimilar. - 100 means perfectly identical. The output must follow the required format: 1. Provide the final score, where the value must be enclosed in LaTeX boxed{}. 2. Provide short justification, explaining the key similarities and differences that influenced your score. image using A.3.2 EVALUATION FOR UI POLISHING For the UI polishing task, we employ Gemini-2.5-Pro as the visual evaluator. The model is prompted with triplet comparison: reference screenshot A, an initial rendering B, and polished rendering C. It is asked to assign similarity scores in the range [0, 100] to both and C, provide brief reasoning for each score, and determine which rendering is closer to the reference. The prompt template is shown below: Prompt You will be given three images: - The first image is the reference (target design draft). - The second and third images are code-rendered results based on the reference. Please complete the following tasks: 1. Assign score to both the second and third images, with range of 0100: - 0 means completely dissimilar to the reference. - 100 means exactly the same as the reference. 2. When scoring, consider layout, color scheme, typography, spacing, and element details. 3. Briefly explain the reason for each score. 4. Provide final conclusion: which image is closer to the reference. The conclusion should be wrapped in LaTeX boxed{}, for example: Second image score: 85 Reason: Overall layout is consistent, but the font is slightly smaller. Colors are mostly accurate. Third image score: 78 Reason: Most elements are reproduced, but button styles and spacing differ significantly. boxed{The second image is better} A.4 DETAILS OF BENCHMARKS Here we illustrate the details of benchmarks that we evaluate on, along with our curated UIPolishbench and UI2Code-Real. To ensure fair comparison between open-source and closed-source systems on our proposed benchmarks, we evaluate diverse set of models. Specifically, we select 5 groups representative open-source VLMs, such as InternVL3 (Zhu et al., 2025), Qwen2.5-VL (Bai et al., 2025), MiMo-VL (Team et al., 2025a), Kimi-VL (Team et al., 2025b), and GLM-4.1V9B-Thinking (Hong et al., 2025). For closed-source systems, we evaluate 4 widely-used models: Claude-4 (Anthropic, 2025), Gemini-2.5 (Comanici et al., 2025), Doubao (Guo et al., 2025), and GPT-5 OpenAI (2025). This setup allows us to benchmark UI-to-code and UI polishing performance across both research and industrial systems under the same evaluation protocol. 17 Under review A.4.1 EXISTING BENCHMARKS Web2Code (Yun et al., 2024): this benchmark comprises 1,198 webpage screenshot images to evaluate the ability of HTML code generation for multi model. Different from traditional code-level evaluations, this benchmark assesses the generated webpages fidelity at the image level. This evaluation method converts the predicted HTML codes back into images using Selenium WebDriver to allow direct visual comparison with the ground truth images. Flame-React-Eval (Ge et al., 2025): benchmark of 80 curated design-to-React cases. In the original evaluation, the generated code is judged correct if it compiles, renders without error, and the rendered screenshot matches the reference with DINOv2 embedding cosine similarity above threshold. Design2Code (Si et al., 2024): contains 484 real-world webpages (plus an 80-example HARD subset) as input screenshots. Models must output corresponding HTML/CSS. The original evaluation is done via rendered visual similarity (CLIP) plus element-level matching (position, text, color), with human judgments used to validate metrics. A.4.2 OUR PROPOSED BENCHMARKS Almost all the existing benchmarks are constructed with synthetic or heavily pruned HTMLs, and none of them can evaluate the UI-polish ability. To analyze the UI-to-code and UI-polish capability on real-world webpage distribution, we propose the following benchmarks. UI-to-code-Real: benchmark consisting of 115 real-world webpage screenshots. Unlike synthetic datasets, which typically feature simplified layouts and over-pruned structures, UI2Code-Real directly reflects the complexity, visual diversity, and noise inherent in real webpages. This benchmark therefore provides more realistic and challenging setting for evaluating UI-to-code generation models. UIPolish-bench: benchmark specifically designed to evaluate UI polishing. Each sample consists of reference screenshot A, an initial rendering B, and the corresponding HTML/CSS code used to produce B. The goal of UI polishing is to compare and B, identify the discrepancies between them, and modify the underlying HTML/CSS code so that the rendered result better aligns with A. This design directly captures the iterative refinement process of UI development. UIPolish-Bench is further divided into two subsets: 1) UIPolish-Synthetic: constructed from synthetic webpages with controlled structures, which ensures clean annotations and facilitates fine-grained evaluation of polishing behavior. 2) UIPolish-Real: collected from real-world webpages, which preserves noise, complex layouts, and design diversity, providing challenging benchmark for assessing polishing in practical settings. A.5 DEMO CASES To provide an intuitive understanding of the proposed UI2CodeN , we present several representative demo cases focusing on UI-to-code and UI Editing: UI-to-Code: Given raw UI screenshot, the model automatically generates executable HTML/CSS code that faithfully reproduces the layout, color scheme, and visual elements of the design. The demos show that our model is able to handle both simple layouts and complex, nested structures with high fidelity. UI Editing: Starting from an existing rendering, the model is able to perform targeted edits such as modifying layout alignment, adjusting typography, changing color themes, or inserting new components. These cases demonstrate the models ability to act as an interactive assistant in iterative design workflows. These demo cases highlight the versatility of our system across different aspects of UI development, demonstrating its potential as both code generator and an interactive design assistant. 18 Under review A.5.1 CASES OF UI2CODE Figure 3: UI2CodeN Demo Cases: UI-to-code (1/4) 19 Under review Figure 4: UI2CodeN Demo Cases: UI-to-code (2/4) Under review Figure 5: UI2CodeN Demo Cases: UI-to-code (3/4) 21 Under review Figure 6: UI2CodeN Demo Cases: UI-to-code (4/4) Under review A.5.2 CASES OF UI EDITING Figure 7: UI2CodeN Demo Cases: UI Editing (1/2) 23 Under review Figure 8: UI2CodeN Demo Cases: UI Editing (2/2)"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University",
        "Zhipu AI"
    ]
}