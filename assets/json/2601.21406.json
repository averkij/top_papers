{
    "paper_title": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation",
    "authors": [
        "Zihan Su",
        "Hongyang Wei",
        "Kangrui Cen",
        "Yong Wang",
        "Guanhua Chen",
        "Chun Yuan",
        "Xiangxiang Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified Multimodal Models (UMMs) integrate both visual understanding and generation within a single framework. Their ultimate aspiration is to create a cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified Multi-Representation Generation), a simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop a deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities."
        },
        {
            "title": "Start",
            "content": "Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Zihan Su 1 2 * Hongyang Wei 1 * Kangrui Cen 3 * Yong Wang 2 Guanhua Chen 4 Chun Yuan 1 Xiangxiang Chu 2 6 2 0 2 9 2 ] . [ 1 6 0 4 1 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Unified Multimodal Models (UMMs) integrate both visual understanding and generation within single framework. Their ultimate aspiration is to create cycle where understanding and generation mutually reinforce each other. While recent post-training methods have successfully leveraged understanding to enhance generation, the reverse direction of utilizing generation to improve understanding remains largely unexplored. In this work, we propose UniMRG (Unified MultiRepresentation Generation), simple yet effective architecture-agnostic post-training method. UniMRG enhances the understanding capabilities of UMMs by incorporating auxiliary generation tasks. Specifically, we train UMMs to generate multiple intrinsic representations of input images, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives. By synthesizing these diverse representations, UMMs capture complementary information regarding appearance, spatial relations, and structural layout. Consequently, UMMs develop deeper and more comprehensive understanding of visual inputs. Extensive experiments across diverse UMM architectures demonstrate that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities. 1. Introduction What cannot create, do not understand. Richard Feynman Work done during internship at AMAP, Alibaba Group *Equal contribution Project lead 1Tsinghua University 2AMAP, Alibaba Group 3Shanghai Jiao Tong University 4Southern University of Science and Technology. Preprint. January 30, 2026. 1 Figure 1. Examples of depth and segmentation generation. Offthe-shelf UMMs (e.g., Harmon-1.5B (Wu et al., 2025c)) struggle to generate plausible depth/segmentation maps from input images, often producing outputs closer to RGB reconstruction. UniMRG post-trains UMMs to generate these intrinsic representations, encouraging them to internalize geometric cues (depth) and structural cues (segmentation) that are beneficial for visual understanding. Unified Multimodal Models (UMMs) (Wu et al., 2025c; Xie et al., 2025b; Pan et al., 2025; Deng et al., 2025; Chen et al., 2025a; Wei et al., 2025) unify visual understanding and generation capabilities in single architecture, representing significant advancement in multimodal AI. These models can simultaneously handle image understanding tasks (e.g., visual question answering, image captioning) and generation tasks (e.g., text-to-image generation, image editing), providing greater flexibility and efficiency compared to traditional single-task models. key aspiration of such unified frameworks is that understanding and generation capabilities can mutually reinforce each other, leading to more powerful multimodal intelligence. Recently, several works have begun to explore the posttraining of UMMs. For instance, RecA (Xie et al., 2025a) utilizes rich semantic information from UMMs own understanding encoder for reconstruction, significantly improving visual generation capability. SRUM (Jin et al., 2025) employs the understanding capability of UMMs to score generated images, enabling self-rewarding training. These works suggest that UMMs understanding capability can Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation be leveraged to improve generation capability. However, the reverse direction of leveraging generation to improve understanding remains largely unexplored. This leads to natural question: can UMMs generation capabilities also enhance their understanding capabilities? To investigate this question, we study simple yet revealing setup: prompting UMMs to generate intrinsic visual representations of an input image beyond RGB appearance. We choose representations that (i) capture intrinsic factors that are weakly constrained by pixel reconstruction, (ii) provide dense supervision aligned with common failure modes of visual understanding (e.g., spatial reasoning errors and hallucinations). Based on these criteria, we choose depth and segmentation, leaving other representations for future work. Depth explicitly encodes geometry and relative distance, while segmentation delineates object boundaries and region partitions, offering an object-centric structural prior. However, as shown in Figure 1, off-the-shelf UMMs often fail to generate such representations, producing outputs that resemble RGB reconstruction rather than plausible depth/segmentation. This suggests that these intrinsic geometric and structural cues are not well captured in UMMs internal representations. We therefore post-train UMMs to generate depth and segmentation maps, which explicitly encode geometry and structure, respectively. This auxiliary generation encourages UMMs to internalize these regularities, which in turn transfers to stronger understanding. Figure 2 provides concrete example of this idea. Focusing on depth as representative intrinsic signal, offthe-shelf UMMs fail to generate plausible depth maps and also struggle with spatial understanding, whereas imageto-depth post-training enables UMMs to produce coherent depth maps and yields noticeably stronger spatial understanding. Building on this insight, we propose UniMRG (Unified Multi-Representation Generation), simple yet effective architecture-agnostic post-training method that improves understanding via auxiliary generation of intrinsic representations. Specifically, UniMRG trains UMMs to generate multiple intrinsic image representations, including pixel (reconstruction), depth (geometry), and segmentation (structure), along with standard visual understanding objectives. By learning to synthesize these complementary representations, UMMs can better capture appearance cues, spatial relations, and structural layout, leading to stronger and more comprehensive visual understanding. We validate our approach across various UMM architectures, including autoregressive, masked autoregressive, and diffusion-based UMMs. Experimental results consistently show that our method notably improves fine-grained perception, mitigates hallucinations, and strengthens spatial understanding, while also enhancing generation performance. Our contributions are summarized as follows: Figure 2. Motivation: Intrinsic visual representation generation enhances visual understanding. Top: Off-the-shelf UMMs fail to generate plausible depth maps and struggle with spatial understanding. Bottom: After image-to-depth training, UMMs generate coherent depth maps and exhibit stronger spatial understanding, correctly identifying spatial relationships. We propose UniMRG, simple yet effective architectureagnostic post-training method for UMMs that leverages generation capabilities to enhance understanding. We introduce multi-representation generation strategy that trains UMMs to generate multiple intrinsic image representations, namely pixel (reconstruction), depth (geometry), and segmentation (structure), alongside standard visual understanding objectives, thereby capturing diverse visual information to enhance understanding. Extensive experiments across different UMM architectures show that our method notably enhances fine-grained perception, reduces hallucinations, and improves spatial understanding, while simultaneously boosting generation capabilities. 2. Related Work Unified Multimodal Models (UMMs). Unified multimodal models can be categorized into three architectural paradigms: (1) AR. Models like Janus-Pro (Chen et al., 2025b) and Show-o (Xie et al., 2025b) encode images into discrete tokens via VQ-VAE (Van Den Oord et al., 2017), enabling autoregressive prediction similar to text tokens. (2) AR+MAR. Models like Harmon (Wu et al., 2025c) leverage Masked Autoregressive (MAR) (Li et al., 2024b) modeling, using MAR features as unified representations. (3) AR+Diffusion. Models such as OpenUni (Wu et al., 2025b) and BLIP-3-o (Chen et al., 2025a) condition diffusion decoders on MLLM hidden states, while models like BAGEL (Deng et al., 2025) adopt the Mixture-ofTransformer-Experts (MoT) architecture to jointly perform understanding and generation. 2 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Figure 3. Overview of UniMRG. The input image is fed into the visual understanding encoder, and the UMM is jointly trained on four tasks: (1) Image reconstruction: reconstructing the input image to enhance generation capabilities. (2) Image-to-depth: generating depth maps to learn geometric cues and spatial relations. (3) Image-to-segmentation: generating segmentation maps to learn structural cues and region partitions. (4) Image understanding: performing standard vision-language understanding tasks. The understanding encoder is updated for UMMs with shared encoder for generation and understanding; otherwise it is frozen. Post-training for UMMs. Recently, several works have begun exploring post-training for UMMs. RecA (Xie et al., 2025a) reconstructs images by exploiting semantic representations extracted from the UMMs understanding encoder, which markedly enhances its generation ability. SRUM (Jin et al., 2025) turns the UMMs understanding capacity into scoring signal for synthesized images, supporting selfrewarding training paradigm. Han et al. (2025) propose an internal gap-based self-improvement framework, which mitigates internal gaps in UMMs by leveraging understanding to guide generation. UniCorn (Han et al., 2026) distills latent understanding into explicit generative signals in UMMs. However, existing works primarily focus on leveraging UMMs understanding capabilities to enhance generation, while we explore using UMMs generation capabilities to improve understanding. Depth Estimation and Segmentation. Depth estimation is fundamental dense prediction task that infers per-pixel scene geometry and relative distance. Early work studies supervised or self-supervised monocular depth prediction (Godard et al., 2019), while later works emphasize stronger cross-dataset generalization via multi-dataset training (Ranftl et al., 2020). Recent models such as Depth Anything V2 (Yang et al., 2024) further improve robustness and generalization, making depth practical representation for capturing spatial relations. Segmentation provides region-level scene decomposition (Long et al., 2015; Chen et al., 2018; Zhang et al., 2022) via semantic segmentation, as well as instance-level masks (He et al., 2017) for object-wise partitioning. Beyond category-aware semantic/instance segmentation, Segment Anything (SAM) (Kirillov et al., 2023) enables general-purpose, instance-like mask generation, offering strong structural cues about object boundaries and region partitions. Motivated by the complementarity of these two representations, UniMRG treats depth maps and segmentation maps as auxiliary generation targets, enabling UMMs to acquire richer geometric and structural knowledge that transfers to downstream understanding. 3. Unified Multi-Representation Generation We propose UniMRG (Unified Multi-Representation Generation), simple yet effective post-training method that enhances UMMs understanding capabilities via auxiliary generation tasks. In this section, we first detail our multirepresentation generation strategy in Section 3.1. Then, we provide further discussion and insights in Section 3.2. Preliminaries on different generation paradigms of UMMs are provided in Appendix A. The overall pipeline is illustrated in Figure 3. 3.1. Multi-Representation Generation Strategy Most existing post-training recipes for UMMs primarily exploit understanding signals to improve generation (Han et al., 2025; Jin et al., 2025). In contrast, we study the reverse direction: can generation improve visual understanding? We answer this by leveraging the input images intrinsic visual representations as auxiliary generation targets. Consequently, UniMRG trains UMMs on four tasks simultaneously: image reconstruction, image-to-depth, image-tosegmentation, and image understanding. Multi-Task Objective. We use to represent the UMM, for the visual embedding of the input image extracted by the UMMs understanding encoder, and L(, ) for the loss function. For understanding tasks, L(, ) is the crossentropy loss. For generation tasks, L(, ) is the diffusion loss for diffusion-based generation models or the crossentropy loss for autoregressive generation models. The individual loss terms are defined as follows. (1) The visual understanding loss improves the models understanding capabilities through standard vision-language tasks: Lund = L(f (h, tquestion), tanswer) (1) where tquestion is the question prompt and tanswer is the answer. (2) The image reconstruction loss enhances the UMMs 3 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation generation capabilities: Lpixel = L(f (h, tpixel), Ipixel) (2) where tpixel is the prompt Generate original image for the input image and Ipixel is the input image (for simplicity, we omit the VAE decoding process here and in subsequent formulations). (3) The image-to-depth loss compels UMMs to learn geometric cues and spatial relations: Ldepth = L(f (h, tdepth), Idepth) (3) where tdepth is the prompt Generate depth map for the input image and Idepth is the target depth map preprocessed from the input image using Depth Anything V2 (Yang et al., 2024). (4) The image-to-segmentation loss compels UMMs to learn structural cues and region partitions: Lseg = L(f (h, tseg), Iseg) (4) where tseg is the prompt Generate segmentation map for the input image and Iseg is the target segmentation map preprocessed from the input image using automatic mask generation with Segment Anything (Kirillov et al., 2023). The overall training objective of UniMRG combines these four loss terms: Ltotal = λpixelLpixel + λdepthLdepth + λsegLseg + λundLund (5) where λpixel, λdepth, λseg, and λund are the weights for each loss term. In our experiments, we set all weights to 1. Training and Inference Details. We freeze the VQVAE (Van Den Oord et al., 2017) and text encoder/decoder during training. The understanding encoder is updated for UMMs with shared encoder for generation and understanding (e.g., Harmon (Wu et al., 2025c)); otherwise it is frozen. All other components of the UMM are trainable. During inference, UniMRG operates identically to standard UMMs without any architectural modifications or additional computational overhead. 3.2. Discussion Why Intrinsic Visual Representations Are Necessary. To improve understanding via generation, natural starting point is image reconstruction, which strengthens appearance modeling and has been shown effective for boosting generation (Xie et al., 2025a). However, pixel-level supervision is dominated by textures and colors and provides only weak constraints on intrinsic factors such as geometry and structure. As result, reconstruction alone is often insufficient to induce the intrinsic cues needed for challenging understanding scenarios, such as fine-grained perception, spatial understanding, and hallucination reduction. We therefore supervise UMMs to generate intrinsic visual representations as auxiliary targets, explicitly encouraging the model to capture complementary factors beyond RGB appearance. Which Intrinsic Representations Are Suitable? Representations that capture intrinsic factors of scene beyond RGB appearance are suitable. In this work, we focus on depth and segmentation representations. The exploration of more representations is left for future work. Depth maps expose scene geometry and relative distance, directly supporting spatial relations (e.g., front/behind, near/far). This directly benefits spatial understanding and reasoning in downstream VQA. Segmentation maps delineate object boundaries and region partitions, providing an object-centric structural prior that helps disentangle entities and reduces spurious attribute binding, common source of hallucinations. (1) Reconstruction for Differences from Prior Work. UMMs. RecA (Xie et al., 2025a) uses UMM understanding representations to guide reconstruction, thereby improving generation, whereas UniMRG studies the reverse direction by using auxiliary generation to strengthen visual understanding. (2) Reconstruction for Understanding. DIVA (Wang et al., 2024b) and ROSS (Wang et al., 2024a) show that reconstruction-style objectives in pixel space or VAE latent space can benefit understanding; in contrast, UniMRG emphasizes intrinsic visual representations beyond appearance-level reconstruction. (3) Representation Alignment. REPA (Yu et al., 2024) aligns generative features to pretrained encoders to improve generative training, while UniMRG instead uses intrinsic-representation generation for better understanding. (4) Depth Cues for Understanding. SpatialRGPT (Cheng et al., 2024) and DepthVLA (Yuan et al., 2025) enhance spatial understanding in VLM by incorporating depth cues via additional modules. In contrast, UniMRG targets the UMM setting and introduces no architectural changes to the UMM. 4. Experiments 4.1. Experimental Setup Model Architectures. We validate UniMRG on three representative UMM architectures with different generation paradigms: AR: Show-o-1.3B (Xie et al., 2025b) is unified transformer that generates 512512 images through autoregressive masked prediction. It processes text tokens autoregressively while handling image tokens via masked prediction. We use the CLIP variant in our experiments. AR+MAR: Harmon-1.5B (Wu et al., 2025c) employs shared Masked Autoregressive (MAR) (Li et al., 2024b) Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Table 1. Comparison of UniMRG with other post-training methods for UMMs. The green row shows the Gain () over the base model. SFT denotes supervised fine-tuning using only the visual understanding loss. Method Base SFT RecA UniMRG Gain () Base SFT RecA UniMRG Gain () Base SFT RecA UniMRG Gain () General Fine-Grained Hallucination Spatial Understanding Generation"
        },
        {
            "title": "GenEval DPG",
            "content": "50.43 50.43 49.50 52.23 +1.80 81.19 79.81 81.19 81.44 +0.25 47.85 48.02 47.42 48.11 +0.26 60.00 60.00 61.00 62.67 +2.67 71.67 73.33 71.67 74.67 +3.00 50.00 50.67 47.33 51.33 +1. Harmon-1.5B 46.69 47.95 47.11 49.32 +2.63 OpenUni-3.6B 60.88 64.25 60.88 64.56 +3.68 Show-o-1.3B 46.06 45.85 46.69 47.00 +0. 46.67 50.20 49.54 51.90 +5.23 65.23 63.14 65.23 66.01 +0.78 38.17 37.65 36.21 39.87 +1.70 60.88 60.88 59.00 61.21 +0.33 66.69 72.18 66.69 73.90 +7.21 54.26 51.55 52.70 54.42 +0. 71.37 0.30 83.86 85.26 +13.89 50.97 38.07 58.01 55.82 +4.85 67.32 67.09 71.52 71.40 +4.08 80.52 2.85 86.61 85.27 +4.75 79.41 70.24 81.54 81.78 +2.37 81.94 81.98 84.44 84.55 +2. encoder for both visual generation and comprehension, generating 384384 images. MAR learns rich semantics through mask-and-reconstruct processes, enabling consistent semantic grounding across tasks. AR+Diffusion: OpenUni-3.6B (Wu et al., 2025b) serves as an open-source implementation following the MetaQueries (Pan et al., 2025) architecture. It bridges frozen multimodal LLM (i.e., InternVL3-2B (Zhu et al., 2025)) with diffusion generator (i.e., SANA-1.6B (Xie et al., 2024)) using learnable queries and lightweight transformerbased connector, generating 512512 images. Implementation Details. We use the image understanding datasets LLaVA Mix-665K (Liu et al., 2023b) and LLaVANext-Data (Liu et al., 2024a). Our experiments are conducted on 8 NVIDIA H20 GPUs. Notably, our method is resource-efficient, requiring only about 3 hours for OpenUni, 5 hours for Harmon, and 8 hours for Show-o. For more implementation details, please refer to Appendix B. Evaluation Metrics. For generation capabilities, we evaluate UniMRG on GenEval (Ghosh et al., 2023) and DPGBench (Hu et al., 2024). For understanding capabilities, we evaluate UniMRG on (i) general understanding: MMBench (Liu et al., 2024b) English dev split, (ii) fine-grained perception: MMVP (Tong et al., 2024), (iii) hallucination: HallusionBench (Guan et al., 2024), and (iv) spatial understanding: RealWorldQA (RWQA) (xAI, 2024) and Visual Spatial Reasoning (VSR) (Liu et al., 2023a). All understanding evaluations are conducted with VLMEvalKit (Duan et al., 2024). 4.2. Main Results Comparison with UMM Post-training Methods. We compare UniMRG with SFT (i.e., training with only image understanding loss) and RecA (Xie et al., 2025a) (which performs image reconstruction using semantic features) across different UMM architectures. Results are shown in Table 1, and detailed generation benchmark metrics are provided in Table 4 in the Appendix. Notably, SFT, which only trains UMMs on understanding tasks, leads to dramatic decline in generation metrics. For instance, on Harmon, GenEval drops from 71.37 to 0.30, and DPGBench from 80.52 to 2.85 after SFT. While RecA markedly improves generation capabilities, it provides no gains for understanding. In contrast, UniMRG achieves remarkable improvements in both understanding and generation capabilities. For understanding metrics, UniMRG achieves state-of-the-art results across different UMM architectures, notably enhancing UMMs fine-grained perception, hallucination mitigation, and spatial understanding capabilities. For example, on OpenUni-3.6B, MMVP improves from 71.67 to 74.67 (+3.00), HallusionBench from 60.88 to 64.56 (+3.68), and VSR from 66.69 to 73.90 (+7.21), demonstrating notable 5 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Table 2. Comparison with state-of-the-arts on visual understanding benchmarks. Our model is OpenUni post-trained with UniMRG. Type Model # LLM Params General Fine-Grained Hallucination Spatial Understanding MMBench MMVP Hallusion RWQA O . d fi U LLaVA-OV (Li et al., 2024a) InternVL3 (Zhu et al., 2025) Qwen2.5-VL (Bai et al., 2025b) Qwen3-VL (Bai et al., 2025a) Emu3-Chat (Wang et al., 2024c) Chameleon (Team, 2024) Janus (Wu et al., 2025a) Janus-Pro (Chen et al., 2025b) Harmon (Wu et al., 2025c) Show-o (Xie et al., 2025b) BAGEL (Deng et al., 2025) UniLIP (Tang et al., 2025) OpenUni (Wu et al., 2025b) UniMRG (Ours) 0.5B 2B 3B 2B 8B 7B 1.3B 7B 1.5B 1.3B 3B 2B 2B 2B 55.15 81.19 78.26 77.32 64.35 35.70 53.52 66.67 50.43 47.85 79.20 80.70 81.19 81. 53.67 71.67 64.67 72.00 67.00 0.00 56.67 63.00 60.00 50.00 54.70 73.00 71.67 74.67 51.95 60.88 52.16 50.16 60.15 46.69 46.06 60.57 60.88 64.56 46.41 65.23 56.99 43.66 41.83 46.67 38.17 64.18 65.23 66. VSR 51.47 66.69 71.93 61.37 71.03 60.88 54.26 65.55 66.69 73.90 improvements across all three capabilities. For generation metrics, UniMRG achieves results comparable to RecA, significantly improving UMMs generation capabilities. Additionally, Show-o shows smaller improvements on understanding metrics, which we analyze in Section 4.4. Comparison with State-of-the-Arts. We compare OpenUni post-trained with UniMRG against state-of-the-art UMMs and understanding-only models of similar scale. Results are shown in Table 2. Our method achieves stateof-the-art results in fine-grained perception, hallucination mitigation, and spatial understanding, evidencing the effectiveness of UniMRG. For example, on HallusionBench, UniMRG achieves 64.56, significantly outperforming the second-best result of 60.88. Qualitative Results. Qualitative generation results are shown in Figure 4a, and understanding results are shown in Figure 4b. For generation capabilities, UMMs posttrained with UniMRG better follow prompts involving multiple objects, spatial relationships, and complex attributes. For example, given the prompt photo of pink skateboard and black train, the baseline generates train with mixed red and black colors, while our method correctly generates black train. For understanding capabilities, UMMs post-trained with UniMRG exhibit improved fine-grained perception, fewer hallucinations, and stronger spatial understanding. For example, on the spatial question Which is closer?, the baseline answers incorrectly, whereas UniMRG gives the correct answer. This indicates that the image-to-depth objective encourages the model to internalize depth-related cues and relative spatial ordering, making distance comparisons more reliable. On the hallucination check Is there any yogurt in this figure?, the baseline answers yes by fabricating non-existent object, while UniMRG correctly answers no. This improvement is consistent with the image-to-segmentation objective, which emphasizes object boundaries and region-level scene decomposition, helping the model better ground object presence in the visual content. 4.3. Ablation Study Quantitative Ablation Results. The quantitative ablation study results on Harmon-1.5B are shown in Table 3. After SFT (only understanding loss), generation capabilities dramatically decline, with GenEval dropping from 71.37 to 0.30, indicating that training solely on understanding tasks severely degrades UMMs generation capabilities. After adding pixel representation generation, generation capabilities considerably improve. However, understanding capabilities show no gains at this point. After adding depth representation generation, understanding capabilities notably improve. For example, for spatial understanding, VSR improves from 59.00 to 60.39. After adding segmentation representation generation, understanding capabilities continue to improve. For example, for hallucination detection, Hallusion improves from 48.26 to 49.32. The results show that adding depth and segmentation representation generation markedly improves understanding capabilities, while not harming generation capabilities (GenEval: 83.8685.26, DPGBench: 86.6185.27). Qualitative Ablation Results. We explore the impact of different representation generations on the quality of UMMgenerated images, with results shown in Figure 5. With only SFT for image understanding, the quality of UMMgenerated images severely degrades, resulting in meaningless noise. With only depth representation generation, the generated image distribution becomes biased toward depth maps, leading to darkened images with loss of texture details. Similarly, with only segmentation representation generation, the generated image distribution also becomes biased toward 6 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation (a) Qualitative generation results. UMMs post-trained with UniMRG better follow prompts involving multiple objects, spatial relationships, and complex attributes. (b) Qualitative understanding results. UMMs post-trained with UniMRG exhibit improved fine-grained perception, reduced hallucinations, and enhanced spatial understanding. Figure 4. Qualitative results on generation and understanding. segmentation maps rather than natural images. However, after adding pixel representation generation on top of depth and segmentation representation generation, the quality of UMM-generated images notably improves, producing images that closely resemble natural images. This indicates that pixel representation generation is essential for maintaining the quality of UMM-generated images. 4.4. Discussion We investigate two questions: (i) whether UniMRG induces genuine intrinsic representation generation ability that internalizes regularities useful for visual understanding, rather than merely overfitting to the training data distribution, and (ii) whether stronger intrinsic representation generation cor7 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Table 3. Quantitative Ablation Results. General Fine-Grained Hallucination Spatial Understanding Generation Method MMBench MMVP Hallusion RWQA Harmon-1.5B + Visual Understanding + Pixel Generation + Depth Generation + Segmentation Generation 50.43 50.43 49.50 50.60 52.23 60.00 60.00 61.00 62.00 62.67 46.69 47.95 47.11 48.26 49.32 46.67 50.20 49.54 50.46 51.90 VSR 60.88 60.88 59.00 60.39 61.21 GenEval DPG 71.37 0.30 83.86 85.15 85.26 80.52 2.85 86.61 84.96 85.27 Figure 5. Qualitative ablation study on the quality of UMMgenerated images with different representation generations. denotes depth representation generation, denotes segmentation representation generation, and denotes pixel representation generation. relates with gains in understanding. To answer these questions, we evaluate depth representation generation on MidjourneyV6 (CortexLM, 2024), synthetic image dataset whose distribution differs substantially from the real image distribution used for training. We randomly sample 1,000 images and compare three UMMs using their original weights and the corresponding UniMRG-posttrained weights. For each model, we prompt it to generate depth representation and measure its similarity to targets predicted by Depth Anything V2. We measure similarity using 1 MAE, where MAE denotes the mean absolute error between the predicted and target depth maps (higher is better). Results are shown in Figure 6. UniMRG substantially improves depth generation ability for Harmon and OpenUni, increasing from 0.623 to 0.822 and from 0.617 to 0.834, respectively. This shows that the enhanced depth representation generation generalizes well to synthetic, out-of-distribution images. In contrast, Show-o exhibits only marginal gain (0.637 0.664). We attribute this to representational bottleneck: Show-o relies on VQ codebook with only 4,096 tokens, which limits its expressive capacity for jointly generating multiple representations (pixel, depth, and segmentation). Consequently, UniMRG 8 Figure 6. OOD depth representation generation on MidjourneyV6. We sample 1,000 images from MidjourneyV6 and measure the similarity between model-generated depth maps and Depth Anything V2 targets using 1 MAE (higher is better), comparing base and UniMRG weights. yields only marginal improvements in understanding capability on Show-o. Similar modest gains due to capacity constraints in Show-o are also observed in RecA (Xie et al., 2025a). These results suggest that when the generative representation space is limited (e.g., by small VQ codebook), producing reliable intrinsic targets becomes difficult, which may bound the gains in understanding performance. 5. Conclusion In this work, we explore improving understanding via generation in Unified Multimodal Models (UMMs). We propose UniMRG, simple yet effective architecture-agnostic posttraining method that trains UMMs with auxiliary generation of intrinsic visual representations, including pixel, depth, and segmentation, alongside standard understanding objectives. Synthesizing these representations encourages UMMs to internalize geometric and structural regularities that improve understanding. Across diverse UMM architectures, UniMRG consistently improves fine-grained perception, reduces hallucinations, and strengthens spatial understanding, while also enhancing generation quality. Looking forward, we will extend UniMRG to more intrinsic representations (e.g., pose, sketches) and to video settings. We hope this work motivates further research on synergies between understanding and generation in multimodal models. Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation"
        },
        {
            "title": "Impact Statement",
            "content": "This paper studies how auxiliary generation of intrinsic visual representations (pixel, depth, and segmentation) can improve visual understanding in unified multimodal models, leading to better fine-grained perception, reduced hallucinations, and stronger spatial understanding. These improvements may have positive societal impact by making multimodal systems more reliable for applications that require accurate grounding (e.g., assistive tools, education, humancomputer interaction, and downstream decision-support settings where hallucinations are harmful). At the same time, strengthening generation capabilities can also increase the risk of misuse of image generation (e.g., creating deceptive or misleading synthetic content). Our method does not by itself provide complete solution to these risks; responsible deployment should include appropriate safeguards such as content provenance and policy-based filtering for generated media, careful evaluation across diverse data to mitigate bias, and human oversight in highstakes use cases."
        },
        {
            "title": "References",
            "content": "Bai, S., Cai, Y., Chen, R., Chen, K., Chen, X., Cheng, Z., Deng, L., Ding, W., Gao, C., Ge, C., Ge, W., Guo, Z., Huang, Q., Huang, J., Huang, F., Hui, B., Jiang, S., Li, Z., Li, M., Li, M., Li, K., Lin, Z., Lin, J., Liu, X., Liu, J., Liu, C., Liu, Y., Liu, D., Liu, S., Lu, D., Luo, R., Lv, C., Men, R., Meng, L., Ren, X., Ren, X., Song, S., Sun, Y., Tang, J., Tu, J., Wan, J., Wang, P., Wang, P., Wang, Q., Wang, Y., Xie, T., Xu, Y., Xu, H., Xu, J., Yang, Z., Yang, M., Yang, J., Yang, A., Yu, B., Zhang, F., Zhang, H., Zhang, X., Zheng, B., Zhong, H., Zhou, J., Zhou, F., Zhou, J., Zhu, Y., and Zhu, K. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., Zhong, H., Zhu, Y., Yang, M., Li, Z., Wan, J., Wang, P., Ding, W., Fu, Z., Xu, Y., Ye, J., Zhang, X., Xie, T., Cheng, Z., Zhang, H., Yang, Z., Xu, H., and Lin, J. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Chen, J., Xue, L., Xu, Z., Pan, X., Yang, S., Qin, C., Yan, A., Zhou, H., Chen, Z., Huang, L., et al. Blip3o-next: Next frontier of native image generation. arXiv preprint arXiv:2510.15857, 2025a. Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pp. 801818, 2018. Chen, X., Wu, Z., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., and Ruan, C. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Cheng, A.-C., Yin, H., Fu, Y., Guo, Q., Yang, R., Kautz, J., Wang, X., and Liu, S. Spatialrgpt: Grounded spatial reasoning in vision-language models. Advances in Neural Information Processing Systems, 37:135062135093, 2024. CortexLM. Cortexlm/midjourney-v6. Hugging Face Datasets, 2024. URL https://huggingface.co/ datasets/CortexLM/midjourney-v6. Deng, C., Zhu, D., Li, K., Gou, C., Li, F., Wang, Z., Zhong, S., Yu, W., Nie, X., Song, Z., Shi, G., and Fan, H. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Duan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu, Y., Dong, X., Zang, Y., Zhang, P., Wang, J., et al. Vlmevalkit: An open-source toolkit for evaluating large In Proceedings of the 32nd multi-modality models. ACM international conference on multimedia, pp. 11198 11201, 2024. Fan, L., Tang, L., Qin, S., Li, T., Yang, X., Qiao, S., Steiner, A., Sun, C., Li, Y., Zhu, T., et al. Unified autoregressive visual generation and understanding with continuous tokens. arXiv preprint arXiv:2503.13436, 2025. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Godard, C., Mac Aodha, O., Firman, M., and Brostow, G. J. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38283838, 2019. Guan, T., Liu, F., Wu, X., Xian, R., Li, Z., Liu, X., Wang, X., Chen, L., Huang, F., Yacoob, Y., et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14375 14385, 2024. Han, R., Fang, Z., Sun, X., Ma, Y., Wang, Z., Zeng, Y., Chen, Z., Chen, L., Huang, W., Xu, W.-J., et al. Unicorn: Towards self-improving unified multimodal models through self-generated supervision. arXiv preprint arXiv:2601.03193, 2026. 9 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Han, Y., Chen, H., Han, A., Wang, Z., Liu, X., Zhang, Y., Zhang, S., and Zou, D. Turning internal gap into selfimprovement: Promoting the generation-understanding unification in mllms. arXiv preprint arXiv:2507.16663, 2025. He, K., Gkioxari, G., Dollar, P., and Girshick, R. Mask rcnn. In Proceedings of the IEEE international conference on computer vision, pp. 29612969, 2017. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hu, X., Wang, R., Fang, Y., Fu, B., Cheng, P., and Yu, G. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. Jin, W., Niu, Y., Liao, J., Duan, C., Li, A., Gao, S., and Liu, X. Srum: Fine-grained self-rewarding for unified multimodal models. arXiv preprint arXiv:2510.12784, 2025. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollar, P., and Girshick, R. Segment anything. arXiv:2304.02643, 2023. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:56424 56445, 2024b. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, F., Emerson, G., and Collier, N. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023a. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023b. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasonJanuary 2024a. https://llava-vl.github.io/blog/ and Lee, Y. J. ing, ocr, URL 2024-01-30-llava-next/. and world knowledge, Llava-next: Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. Long, J., Shelhamer, E., and Darrell, T. Fully convolutional In Proceedings networks for semantic segmentation. of the IEEE conference on computer vision and pattern recognition, pp. 34313440, 2015. Pan, X., Shukla, S. N., Singh, A., Zhao, Z., Mishra, S. K., Wang, J., Xu, Z., Chen, J., Li, K., Juefei-Xu, F., et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Ranftl, R., Lasinger, K., Hafner, D., Schindler, K., and Koltun, V. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. Tang, H., Xie, C., Bao, X., Weng, T., Li, P., Zheng, Y., and Wang, L. Unilip: Adapting clip for unified multimodal understanding, generation and editing. arXiv preprint arXiv:2507.23278, 2025. Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wang, H., Zheng, A., Zhao, Y., Wang, T., Ge, Z., Zhang, X., and Zhang, Z. Reconstructive visual instruction tuning. arXiv preprint arXiv:2410.09575, 2024a. Wang, P., Peng, Y., Gan, Y., Hu, L., Xie, T., Wang, X., Wei, Y., Tang, C., Zhu, B., Li, C., et al. Skywork unipic: Unified autoregressive modeling for visual understanding and generation. arXiv preprint arXiv:2508.03320, 2025. Wang, W., Sun, Q., Zhang, F., Tang, Y., Liu, J., and Wang, X. Diffusion feedback helps clip see better. arXiv preprint arXiv:2407.20171, 2024b. 10 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024c. Zhang, B., Tian, Z., Tang, Q., Chu, X., Wei, X., Shen, C., et al. Segvit: Semantic segmentation with plain vision transformers. Advances in Neural Information Processing Systems, 35:49714982, 2022. Wei, H., Xu, B., Liu, H., Wu, C., Liu, J., Peng, Y., Wang, P., Liu, Z., He, J., Xietian, Y., et al. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model. arXiv preprint arXiv:2509.04548, 2025. Zhu, J., Wang, W., Chen, Z., Liu, Z., Ye, S., Gu, L., Tian, H., Duan, Y., Su, W., Shao, J., et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Wu, C., Chen, X., Wu, Z., Ma, Y., Liu, X., Pan, Z., Liu, W., Xie, Z., Yu, X., Ruan, C., et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025a. Wu, S., Wu, Z., Gong, Z., Tao, Q., Jin, S., Li, Q., Li, W., and Loy, C. C. Openuni: simple baseline for unified multimodal understanding and generation. arXiv preprint arXiv:2505.23661, 2025b. Wu, S., Zhang, W., Xu, L., Jin, S., Wu, Z., Tao, Q., Liu, W., Li, W., and Loy, C. C. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025c. xAI. Grok-1.5 Vision Preview, 2024. URL https://x. ai/news/grok-1.5v. Accessed: 2026-01-13. Xie, E., Chen, J., Chen, J., Cai, H., Tang, H., Lin, Y., Zhang, Z., Li, M., Zhu, L., Lu, Y., et al. Sana: Efficient highresolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Xie, J., Darrell, T., Zettlemoyer, L., and Wang, X. Reconstruction alignment improves unified multimodal models. arXiv preprint arXiv:2509.07295, 2025a. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025b. Yang, L., Kang, B., Huang, Z., Zhao, Z., Xu, X., Feng, J., and Zhao, H. Depth anything v2. arXiv:2406.09414, 2024. Yu, S., Kwak, S., Jang, H., Jeong, J., Huang, J., Shin, J., and Xie, S. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Yuan, T., Liu, Y., Lu, C., Chen, Z., Jiang, T., and Zhao, H. Depthvla: Enhancing vision-language-action models with depth-aware spatial reasoning. arXiv preprint arXiv:2510.13375, 2025. 11 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation A. Preliminaries In this section, we provide brief introduction for the three primary generative paradigms utilized in Unified Multimodal Models (UMMs): Autoregressive (AR) modeling, Diffusion Modeling, and Masked Autoregressive (MAR) modeling. A.1. Autoregressive Modeling Visual Autoregressive models typically operate on discrete latent space derived from VQ-VAE (Van Den Oord et al., 2017). Let = [x1, . . . , xT ] denote the flattened sequence of discrete tokens from codebook. Standard AR models decompose the joint distribution via the chain rule of probability. The model maximizes the log-likelihood: log pθ(x) = (cid:88) t= log pθ(xt x<t), where x<t denotes the causal context. The training objective is to minimize the negative log-likelihood (NLL): LCausal(θ) = Expdata (cid:35) log pθ(xt x<t) . (cid:34) (cid:88) t=1 (6) (7) To enforce the autoregressive property, causal mask is applied to the self-attention mechanism, such that the attention logits satisfy Aij = if > i, ensuring strict sequential dependency. To enable parallel token prediction and utilize bidirectional context, MaskGit (Chang et al., 2022) employs masked prediction strategy. binary mask {0, 1}T is sampled, where mi = 1 indicates visible token and mi = 0 masked token. The model predicts the masked tokens conditioned on all visible tokens xm (and the mask pattern) simultaneously. The objective minimizes the cross-entropy loss on the masked positions: LMaskGit(θ) = Ex,m (cid:35) log pθ(xi xm) . (cid:34) (cid:88) m (8) Unlike standard AR, this approach uses unmasked bidirectional attention, allowing the model to attend to both past and future tokens. During inference, it employs iterative decoding to generate tokens in parallel steps based on confidence scores. UMMs like Chameleon (Team, 2024), Janus-Pro (Chen et al., 2025b), and Show-o (Xie et al., 2025b) adopt autoregressive generation paradigms. A.2. Diffusion Modeling This paradigm generates data by reversing continuous corruption process. We detail the stochastic framework (DDPM) and the deterministic ODE framework (Flow Matching). Denoising Diffusion Probabilistic Models (DDPM). DDPM (Ho et al., 2020) defines forward diffusion process that gradually adds Gaussian noise to the data x0 pdata over timesteps. The transition kernel q(xt xt1) is Gaussian: q(xt xt1) = (xt; (cid:112)1 βtxt1, βtI), (9) where βt is variance schedule. key property allows sampling xt directly from x0. Let αt = 1 βt and αt = (cid:81)t the marginal distribution is: s=1 αs, q(xt x0) = (xt; αtx0, (1 αt)I). (10) Using the reparameterization trick, we can express xt = αtx0 + 1 αtϵ, where ϵ (0, I). The reverse process pθ(xt1 xt) aims to recover the data. It is modeled as Gaussian with learnable mean µθ and fixed variance βt: pθ(xt1 xt) = (xt1; µθ(xt, t), βtI). (11) 12 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Ideally, µθ should predict the posterior mean. We parameterize µθ via noise prediction network ϵθ(xt, t): µθ(xt, t) = (cid:18) xt 1 αt βt 1 αt (cid:19) ϵθ(xt, t) . The simplified training objective is to minimize the error of the noise prediction: (cid:2)ϵ ϵθ(xt, t)2(cid:3) . LDDPM(θ) = Ex0,t,ϵ (12) (13) Flow Matching (FM). Flow Matching (Lipman et al., 2022) models the generative process via Continuous Normalizing Flow (CNF). We define probability path pt(x) that interpolates between source noise distribution p0(x) = (0, I) and the target data distribution p1(x) pdata. This flow is governed by an Ordinary Differential Equation (ODE): dx dt = vt(x), x(0) p0, (14) where vt() is time-dependent vector field. The goal is to learn neural network vθ(x, t) that approximates vt. To make training tractable, we use Conditional Flow Matching (CFM). During training, we sample source-target pair (x0, x1) with x0 p0 and x1 pdata. We adopt the Optimal Transport (OT) path (linear interpolation) between x0 and x1: The corresponding (pair-conditioned) vector field is the time derivative of this path: ψt(x0, x1) = (1 t)x0 + tx1, [0, 1]. ut(ψt(x0, x1) x0, x1) = dt ψt(x0, x1) = x1 x0. The Flow Matching objective regresses the model vθ to this vector field: LFM(θ) = EtU [0,1], x0p0, x1pdata (cid:2)vθ(ψt(x0, x1), t) (x1 x0)2(cid:3) . This formulation is equivalent to Rectified Flow (Liu et al., 2022), enabling fast ODE solver sampling. (15) (16) (17) UMMs like BAGEL (Deng et al., 2025), BLIP-3o (Chen et al., 2025a), and MetaQueries (Pan et al., 2025) adopt diffusion generation paradigms. A.3. Masked Autoregressive Modeling Masked Autoregressive (Li et al., 2024b) modeling combines the sequence modeling capability of AR with the density estimation quality of Diffusion, eliminating the need for discrete codebooks. Continuous Factorization. Let = [z1, . . . , zT ] be sequence of continuous feature vectors (e.g., from VAE encoder). The joint distribution is factorized autoregressively: pθ(z) = (cid:89) i= pθ(zi z<i). (18) Crucially, since zi is continuous, modeling pθ(zi z<i) with simple unimodal loss (like MSE) results in blurry predictions. Instead, the conditional distribution is modeled using Diffusion Loss. Diffusion Loss per Token. For each step i, the prediction of the current token zi is treated as generative denoising task. We diffuse zi to z(k) by adding noise. The model is trained to denoise z(k) , conditioned on the clean history z<i: LMAR(θ) = Ez,i,k,ϵ (cid:104) ϵ ϵθ(z(k) , k, cond = z<i)2(cid:105) . (19) Here, the head of the autoregressive model is effectively small diffusion model. This allows for generating high-fidelity continuous tokens sequentially without quantization artifacts. UMMs like Harmon (Wu et al., 2025c), Fluid (Fan et al., 2025), and Skywork UniPic (Wang et al., 2025) adopt Masked Autoregressive generation paradigms. Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation B. More Implementation Details B.1. Training Details We present the detailed hyperparameters and training configurations for each UMM architecture in our experiments. Harmon: We use AdamW optimizer with learning rate of 1e-5. The batch size is 16 per task (64 total per GPU across 4 tasks) with gradient accumulation of 4 steps. The model is trained for 4,000 steps. The overall training time is approximately 5 hours. OpenUni: We use AdamW optimizer with learning rate of 1e-5. The batch size is 4 per task (16 total per GPU across 4 tasks) with gradient accumulation of 4 steps. The model is trained for 2,000 steps. Additionally, for OpenUnis decoupled architecture where generation and understanding are separated, we then freeze the understanding component and conduct 2,000 steps of pixel representation generation training to further improve its generation capability (taking only about 30 minutes). The overall training time is approximately 3 hours. Show-o: We use AdamW optimizer with learning rate of 1e-6. The batch size is 2 per task (8 total per GPU across 4 tasks) with gradient accumulation of 5 steps. The model is trained for 2,500 steps. The overall training time is approximately 8 hours. B.2. Depth Map Generation We employ Depth-Anything-V2 (Yang et al., 2024) with the ViT-Large encoder to generate monocular depth estimation maps. The input images are first resized to 518 518 pixels using cubic interpolation. After inference, the predicted depth maps are resized back to the original image resolution using bilinear interpolation. The depth values are then normalized to the range [0, 255] using min-max normalization: Dnorm = Dmin Dmax Dmin (20) where represents the raw depth prediction. To ensure compatibility with RGB image processing pipelines during training, the single-channel grayscale depth map is replicated across three channels. B.3. Segmentation Map Generation For segmentation, we utilize the Segment Anything Model (SAM) (Kirillov et al., 2023) with the ViT-H backbone. We employ SAMs automatic mask generator with default parameters, sampling uniform grid of 32 32 = 1024 point prompts across the image. Candidate masks are filtered based on predicted IoU threshold (0.88) and stability score threshold (0.95), with duplicate masks removed via non-maximum suppression (box IoU threshold 0.7). After obtaining all valid masks, we set the mask boundaries to white and the background to black. The resulting map is then converted to 3-channel image for consistency with the training pipeline. B.4. Prompt Templates for UniMRG During UniMRG training, to prevent the model from overfitting to specific prompts, we use multiple prompts with similar meanings but different phrasings. The prompts for image reconstruction are from RecA, while the prompts for image-to-depth and image-to-segmentation generation are shown below. Image-to-Depth Prompts: Generate the depth map of this image. Produce depth estimation for this image. Compute the depth map from this photograph. Extract depth information from this picture. Show me the depth map. 14 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation What is the depth of this image? Provide the depth estimation. Perform depth estimation on this image. Apply monocular depth prediction. Generate per-pixel depth values. Image-to-Segmentation Prompts. Example prompts include: Generate the segmentation mask of this image. Create segmentation map for this image. Produce semantic segmentation mask from this image. Segment this image into different regions. Predict the object masks in this image. Compute the segmentation from this photograph. Produce an object separation mask. Segment the foreground from background. Segment everything in this image. Generate automatic masks for all objects. C. Qualitative Results on Representation Generation Figure 7 shows the depth and segmentation maps generated by different UMMs before and after UniMRG training. As can be seen, the base models of Harmon, OpenUni, and Show-o cannot generate reasonable depth or segmentation maps, with outputs resembling image reconstruction rather than structured representations. Show-o in particular fails to generate detailed information. After UniMRG training, Harmon and OpenUni exhibit notable improvements in generating depth and segmentation maps. Specifically, both models can generate semantically correct objects from the original images. For example, in the first row, they correctly generate the two boys and the grassland from the input image. Moreover, the depth maps show that distant backgrounds (sky) are rendered in black while closer foreground objects are rendered in white, demonstrating that the models have learned to distinguish relative distances and thus enhanced their spatial understanding. Additionally, the generated representations differ from the ground truth in fine-grained details, as the input images are processed through the visual understanding encoder, which primarily captures semantic information. For Show-o, the generated outputs are purely black when generating depth and segmentation representations, likely due to representational bottleneck (as noted in Section 4.4): its codebook contains only 4,096 tokens, which fundamentally limits its capacity. As result, it fails to jointly generate informative pixel, depth, and segmentation representations, and collapses to black outputs for depth and segmentation generation. 15 Generation Enhances Understanding in Unified Multimodal Models via Multi-Representation Generation Figure 7. Depth and segmentation map generation by different UMMs before and after UniMRG training. Method Base SFT RecA Ours Base SFT RecA Ours Base SFT RecA Ours Table 4. Detailed results on generation benchmarks. GenEval Single Obj. Two Obj. Counting Colors Position Color Attri. Overall DPGBench 99.38 1.77 100.00 99.90 91.46 90.62 91.56 92. 97.40 97.81 98.44 98.85 86.11 0.00 97.98 97.39 55.56 32.91 64.73 63.05 82.91 83.50 90.82 90.07 66.98 0.00 69.58 72.71 46.67 25.21 50.52 49. 67.92 65.73 67.81 67.92 Harmon-1.5B 83.87 0.00 92.82 89.63 45.00 0.00 74.17 80.42 OpenUni-3.6B 21.92 10.08 35.00 27. 27.08 27.58 36.75 38.92 62.15 58.69 64.72 63.56 Show-o-1.3B 80.59 79.52 80.23 80.67 16 46.92 0.00 68.58 71. 28.08 10.92 41.50 39.00 48.00 48.42 55.08 52.00 71.37 0.30 83.86 85.26 50.97 38.07 58.01 55.82 67.32 67.09 71.52 71.40 80.52 2.85 86.61 85. 79.41 70.24 81.54 81.78 81.94 81.98 84.44 84."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Shanghai Jiao Tong University",
        "Southern University of Science and Technology",
        "Tsinghua University"
    ]
}