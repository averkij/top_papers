{
    "paper_title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
    "authors": [
        "Xiaoming Zhu",
        "Xu Huang",
        "Qinghongbing Xie",
        "Zhi Deng",
        "Junsheng Yu",
        "Yirui Guan",
        "Zhongyuan Liu",
        "Lin Zhu",
        "Qijun Zhao",
        "Ligang Liu",
        "Long Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium."
        },
        {
            "title": "Start",
            "content": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation XIAOMING ZHU*, Tsinghua University, China XU HUANG*, Tencent, China QINGHONGBING XIE, Tsinghua University, China ZHI DENG, Tencent, China JUNSHENG YU, Southeast University, China YIRUI GUAN, Tencent, China ZHONGYUAN LIU, Tencent, China LIN ZHU, Tencent, China QIJUN ZHAO, Tencent, China LIGANG LIU, University of Science and Technology of China, China LONG ZENG, Tsinghua University, China 5 2 0 2 7 1 ] . [ 1 4 6 5 5 1 . 0 1 5 2 : r Fig. 1. Some high-quality 3D scene layouts generated by our vision-guided system not only exhibit strong performance in indoor environments but can also be extended to outdoor scenes. The complete text prompts are provided in Appendix A.2.1. Equal contribution, Corresponding author. Authors Contact Information: Xiaoming Zhu*, Tsinghua University, Shenzhen, China, zxiaomingthu@163.com; Xu Huang*, Tencent, Shenzhen, China, ydove1031@gmail. com; Qinghongbing Xie, Tsinghua University, Shenzhen, China, xqhb23@mails. tsinghua.edu.cn; Zhi Deng, Tencent, Shenzhen, China, zhideng@mail.ustc.edu.cn; Junsheng Yu, Southeast University, Shenzhen, China, junshengyu33@163.com; Yirui Guan, Tencent, Shenzhen, China, guan1r@outlook.com; Zhongyuan Liu, Tencent, Shenzhen, China, lockliu@tencent.com; Lin Zhu, Tencent, Shenzhen, China, hahnna0918@shu. edu.cn; Qijun Zhao, Tencent, Shenzhen, China, qijunzhao@tencent.com; Ligang Liu,"
        },
        {
            "title": "Permission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed",
            "content": "University of Science and Technology of China, Hefei, China, lgliu@ustc.edu.cn; Long Zeng, Tsinghua University, Shenzhen, China, zenglong@sz.tsinghua.edu.cn. for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). 2025 Copyright held by the owner/author(s). ACM 1557-7368/2025/12-ART https://doi.org/10.1145/3763353 ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 2 Zhu, Huang, et al. Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents novel vision-guided 3D layout generation system. We first construct high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium. CCS Concepts: Computing methodologies Graphics systems and interfaces; Artificial intelligence. Additional Key Words and Phrases: 3D scene layout, image generation model, visual foundation model, coherent pose estimation ACM Reference Format: Xiaoming Zhu*, Xu Huang*, Qinghongbing Xie, Zhi Deng, Junsheng Yu, Yirui Guan, Zhongyuan Liu, Lin Zhu, Qijun Zhao, Ligang Liu, and Long Zeng. 2025. Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation. ACM Trans. Graph. 44, 6 (December 2025), 24 pages. https: //doi.org/10.1145/"
        },
        {
            "title": "1\nGenerating logically coherent and visually appealing customized\nscene layouts from predefined asset collections presents significant\nchallenges in digital content creation. This issue is particularly criti-\ncal in fields such as game scene generation and computer-generated\nimagery (CGI) for films.",
            "content": "Traditional methods [Chang et al. 2014, 2017; Fisher and Hanrahan 2010; Jiang et al. 2018; Merrell et al. 2011; Yeh et al. 2012] frame this as complex graph-based optimization problem, sampling from pre-modeled layout distributions and iteratively optimizing using predefined scene priors (e.g., layout guidelines, object category distributions). However, defining precise rules is both time-consuming and requires substantial artistic expertise. Furthermore, predefined rules may limit the expression of complex and diverse scene combinations. More recent deep generative approaches [Nie et al. 2023; Paschalidou et al. 2021a; Tang et al. 2024; Wang et al. 2021] learn layout generators from pre-constructed 3D scene layout datasets. However, due to the high costs, privacy concerns, and time-consuming nature of collecting 3D data, these datasets remain relatively limited, leading to outputs that lack diversity and fail to meet the practical needs of artistic experts. This scarcity is particularly pronounced in new game or film productions, where preparing numerous diverse, high-quality 3D scene layouts in advance is nearly impossible, limiting the applicability of generators trained on native 3D data. While large language model-based scene generation methods [AguinaKang et al. 2024; Feng et al. 2024; Yang et al. 2024b] have emerged by extracting layout priors from language models and optimizing them with scene logic rules, they fundamentally lack spatial intuition ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. and geometric precision, struggling to accurately represent complex spatial relationships, model object poses, and adhere to aesthetic design principles, ultimately limiting their effectiveness in creating realistic and coherent layouts. Moreover, existing asset libraries like Objaverse [Deitke et al. 2024] and 3D Future [Fu et al. 2020], are often constrained by poor mesh quality, limited stylization options, and heavy reliance on composite assets (e.g., bookshelf with ornaments treated as single asset), which restricts layout flexibility. To address these limitations, we curated high-quality collection of 2,037 indoor and outdoor assets, which professional artists used to create 147 high-quality scene layoutsa dataset we plan to open-source to benefit the research community. Recent advancements in image generation, driven by the explosive growth of image data and progress in diffusion-based models [Ho et al. 2020; Ruiz et al. 2023; Saharia et al. 2022], have significantly enhanced 2D generative capabilities. Building upon these developments and the substantial progress in foundational visual models [Liu et al. 2025, 2023; Yang et al. 2024a](e.g., detection, segmentation, and depth estimation), we developed visual-guided 3D scene layout generation system. This system is designed to transfer the rich and controllable generative capabilities of 2D image models to the task of 3D layout generation. Our pipeline first utilizes the image generation model Flux [Labs 2024] to expand user-input prompt into guided image. After fine-tuning with our high-quality scene layout data, Flux generates images of higher quality that are also more consistent with the asset collection. Subsequently, we construct an image analysis module based on pre-trained visual model, which integrates visual semantic segmentation, geometric parsing from single image, and graph-based scene graph logic construction module. Next, we adopt semantic feature matching strategy to retrieve objects from the asset collection that are most similar to the guidance image. We then iteratively solve for the rotation, translation, and scaling transformations corresponding to each foreground object based on combination of visual semantic features, geometric information, and scene layout logic. Finally, we perform consistency optimization on the overall 3D scene layout using scene graph logic and image semantic parsing, ensuring that the final scene layout is visually and logically close to the guided image. Image generation models excel at producing aesthetically pleasing and detailed 2D layouts, and our approach leverages these capabilities for 3D scene layout tasks. Unlike previous methods that often rely on rigid composite assets (e.g., treating \"a bowl of fruit on the table\" as single object), which leads to redundancy and insufficient diversity, our approach positions objects in varied poses and placements based on the guidance image. Furthermore, we introduce an internal layout function that allows assets to be arranged within other assets, optimizing space usage and improving scene realism. These capabilities result in more natural, detailed, and visually appealing 3D scene layouts. Experimental results show significant improvements in layout quality compared to previous methods. In summary, our contributions are as follows: We have developed an innovative visual-guided system for high-quality scene layout generation. We have established high-quality 3D scene layout dataset, which will be open-sourced for community benefit. We propose robust scene object pose estimation algorithm integrating visual semantics with geometric information."
        },
        {
            "title": "2 Related Work\n2.1 Data-Driven Scene Layout Generation",
            "content": "Data-driven scene layout generation methods fall into two main categories. The first employs manually defined scene priors and classical graphical models, optimized through non-linear optimization [Chang et al. 2014; Fisher et al. 2012; Qi et al. 2018; Xu et al. 2013; Yu et al. 2011] or manual interaction [Chang et al. 2017; Merrell et al. 2011; Savva et al. 2017]. These priors follow design guidelines [Merrell et al. 2011; Yeh et al. 2012], object frequency distributions [Chang et al. 2014, 2017], or human activity spaces [Fisher et al. 2015; Fu et al. 2017; Jiang et al. 2012; Ma et al. 2016; Qi et al. 2018]. While effective, this approach is limited by the time-intensive nature of manual prior design and model expressiveness constraints. Recently, with advances in deep learning and improved 3D scene datasets [Fu et al. 2020], research has shifted toward end-to-end generators. Various approaches have emerged, including Spatial And-Or Graphs [Jiang et al. 2018], autoregressive models [Nie et al. 2023; Paschalidou et al. 2021b; Wang et al. 2018, 2021], 3D GANs [Yang et al. 2021b], and Variational Autoencoders (VAEs) [Purkait et al. 2020; Yang et al. 2021c,a]. Despite offering quality improvements, these methods struggle with diversity, stability issues, and realism [Xiao et al. 2021]. Recent diffusion-based models [Dahnert et al. 2024; Tang et al. 2024] have enhanced layout richness by encoding object attributes (e.g., object categories, 6D poses, and textual descriptions from predefined asset libraries) in latent space. Building on this, InstructScene [Lin and Mu 2024] first learns scene-graph prior with graph neural network (GNN) and uses it as the conditioning signal for the diffusion process, further improving layout fidelity and global coherence. Another line of work [Dhamo et al. 2021; Wald et al. 2020; Zhai et al. 2024, 2023] model scene graphs from datasets and learn generative distribution over them; at inference time, scene graph is first generated and then used to reconstruct the corresponding 3D scene. However, these approaches remain limited by scarce 3D scene data, leading to overfitting and generalization challenges. Our method addresses these limitations by leveraging pretrained image generation models [Labs 2024] to reconstruct 3D layouts from 2D images, significantly improving scene generation diversity."
        },
        {
            "title": "2.2 Language-Driven Scene Layout Generation",
            "content": "The advent of large language models (LLMs) [Achiam et al. 2023; Brown et al. 2020; Touvron et al. 2023] has enabled textual-to-spatial scene synthesis through code interfaces. Pioneering works like HOLODECK [Yang et al. 2024b] leverage LLMs to predict object categories, sizes, and positions via geometric constraints, while LayoutGPT [Feng et al. 2024] generates CSS-formatted layouts through chain-of-thought prompting. I-Design [Ã‡elen et al. 2024] introduces multi-agent LLM collaboration. SceneCraft [Hu et al. 2024] treats an LLM as an agent that authors Blender scripts, which are then executed to synthesize the 3D scene. However, these LLM-based Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 3 methods often exhibit instability and artifacts, such as providing only four discrete pose estimation options, and face inherent limitations in scene complexity and aesthetics. Recent multimodal approaches show promising directions. Fireplace [Huang et al. 2025] renders 3D scenes as images to equip VLMs with 3D reasoning, thereby planning how objects are arranged. [Deng et al. 2025] represents the scene as hierarchical tree and uses VLM to plan 3D object placements in top-down manner. ARCHITECT [Wang et al. 2024a] synergizes language guidance with diffusion models via hierarchical 2D inpainting to generate more detailed layouts, while LayoutVLM [Sun et al. 2024] combines vision-language models with differentiable optimization for physically valid layouts. Recent advances like CAST [Yao et al. 2025] reconstruct 3D scenes by generating individual objects and predicting poses through point cloud alignment with generative model representations. However, such approaches overlook the reusability of industrial assets with predefined properties beyond geometry, such as animations and interactive attributes. While these methods demonstrate improved visual-semantic alignment, their reliance on fixed orientations and hard relational constraints for asset placement often leads to unnatural poses. Furthermore, the mismatch between arbitrarily generated image content and the available set of 3D assets creates domain adaptation problem, resulting in final placements that significantly differ in style from the reference images. In contrast, our method directly extracts scene layout knowledge from visual models, leveraging style-consistent image guidance and continuous pose estimation to generate more natural and aesthetically pleasing scenes. The integration of scene graphs and geometric constraints further enhances system stability."
        },
        {
            "title": "2.3 Pose Estimation of Novel Objects",
            "content": "Novel object pose estimation has evolved through geometric and learning approaches. Early works like PPF [Drost et al. 2010] used geometric hashing, later enhanced by CNN features [Sundermeyer et al. 2020]. CAD alignment approaches emerged with Mask2CAD [Kuo et al. 2020], followed by ROCA [GÃ¼meli et al. 2022], SPARC [Langer et al. 2022], and DiffCAD [Gao et al. 2024], which improved alignment through coordinate regression, iterative rendering, and diffusion modeling, respectively. However, their reliance on specific CAD libraries inherently limits open-set applicability. Complementary template matching methods achieve enhanced robustness with unseen objects by operating solely in the 2D domain. [Nguyen et al. 2022] applied CNN features for rotation estimation, while [Thalhammer et al. 2023] demonstrated Vision Transformers superiority in this task. MegaPose [LabbÃ© et al. 2022] employed Coarse2Fine optimization strategy on massive dataset, effectively generalizing to unseen objects. Building on this, FoundPose [Ã–rnek et al. 2024] combined DINOv2 features with efficient template matching. Recently, GigaPose [Nguyen et al. 2024a] integrated template matching with local features, enhancing speed and robustness by fine-tuning DINOv2 through contrastive learning on the BOP challenge dataset. In our task, the discrepancy between predefined assets and image content complicates pose estimation. We address this by utilizing GigaPoses finetuned DINOv2 [Nguyen et al. 2024b] for category-based ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 4 Zhu, Huang, et al. Fig. 2. Overview of our method. We first transforms text prompt into detailed 2D guide image using fine-tuned model, ensuring stylistic consistency with our asset library. This image is then analyzed for semantic, geometric, and relational information, guiding the retrieval, transformation estimation, and optimization of 3D assets into the final, coherent layout. See Appendix A.1.7 for additional visualizations of intermediate steps. template rotation estimation, enhanced with geometric constraints and scene logic to ensure global consistency."
        },
        {
            "title": "3 Method",
            "content": "Problem Statement. We aim to generate high-quality 3D scene layouts from predefined set of 3D assets ğ´ based on user prompt. Mathematically, this is defined as function ğº that generates 3D layouts as follows: ğº (ğ‘‚ prompt, ğ´) = {ğ‘œ1, ğ‘œ2, ğ‘œ3, . . . , ğ‘œğ‘›, }, where prompt is textual description (e.g., \"the bosss office\"). Each ğ‘œğ‘– consists of {objğ‘–, ğ‘…ğ‘–, ğ‘¡ğ‘–, ğ‘ ğ‘– }, where objğ‘– is an asset from ğ´ (geometry and texture), ğ‘…ğ‘– ğ‘†ğ‘‚ (3) is the rotation, ğ‘¡ğ‘– R3 is the translation, and ğ‘ ğ‘– R3 is the scale of the asset. (1) Method Overview. The proposed vision-guided 3D scene layout generation system, shown in Fig. 2, consists of three key stages. In Sec. 3.1, we create high-quality 3D scene dataset from ğ´ and fine-tune the Flux-model to generate images that align with the stylistic characteristics of ğ´ and established design practices. In Sec. 3.2, we develop scene image analysis module that integrates visual semantic segmentation, geometric analysis, and scene graph construction. In Sec. 3.3, we use semantic feature matching to retrieve assets from ğ´ that match the guiding image. We then estimate the rotation, translation, and scaling transformations of foreground objects based on visual and geometric data. Finally, in Sec. 3.4, we refine these transformations through scene graph constraints and physical optimization to ensure plausible 3D layout."
        },
        {
            "title": "3.1 Prompt Expander under Predefined Assets",
            "content": "Fine-tune Image Generation Model. Given prompt input, we aim to generate 2D scene images that capture visual characteristics of predefined 3D assets ğ´, serving as guides for 3D scene layout reconstruction. Generating images that align with the style of ğ´ ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. will robustly enhance visual asset retrieval and layout transformation estimation in later stages. To address limited view challenges, we focus on axonometric and frontal views for their comprehensive spatial coverage and design convention alignments. Following DreamBooth [Ruiz et al. 2023], we use unique tag [V] to identify scene data, enabling efficient Flux model fine-tuning with minimal high-quality 3D layout renderings. We constructed high-quality 3D scene dataset based on asset library ğ´ for fine-tuning and evaluating. Our experiments reveal that the fine-tuned generated model as prompt-to-scene expander trained on scenes built with ğ´: it acquires consistent global patterns (viewpoint, rendering style) and moderate object-level features (textures, shapes), while maintaining creative layout flexibility. The visual similarity between objects in generated scenes and those in asset library ğ´ effectively enhances visual asset retrieval and layout transformation estimation in subsequent stages. High-quality 3D scene layout dataset. We have developed comprehensive 3D scene layout dataset that addresses critical limitations in existing resources, such as the prevalence of composite assets and limited variety in 3D-Future [Fu et al. 2020], and the stylization issues and low-quality models in Objaverse [Deitke et al. 2024]. As shown in Fig. 3, it comprises 2,037 high-quality 3D models across 500 classes and 237 categories, with realistic textures and materials. These assets have been used to create 147 expertly designed scene layouts across 20 different types. Compared to 3D-Future, our dataset offers significantly higher asset diversity (500 classes vs. 34) and scene complexity (31.86 objects per scene vs. 5.09), enabling the creation of diverse, complex, and realistic scenes for both indoor and outdoor environments. These scenes were rendered into images for fine-tuning generative models. The dataset was meticulously curated from combination of custom-commissioned models, high-quality open-source content, and licensed marketplace items, which were then arranged into Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 5 Fig. 3. Overview of our high-quality 3D scene layout dataset: (a) Representative 3D scenes with interior layouts. (b) Diverse 3D assets from our collection. (c) Structured metadata schema for scenes and assets. (d) Comparison with 3D-Future, highlighting our datasets superior variety and complexity. cohesive scenes by 20 professional artists with over three years of experience. To maximize its utility, the dataset is accompanied by comprehensive, multi-level annotations. At the asset level, annotations include descriptive captions and bounding box dimensions, with the crucial addition of manually annotated internal, placeable subspaces for assets that can contain other objects. At the scene level, we provide detailed scene captions, the spatial transformation matrix for each object (including the camera), parent-child hierarchical relationships, segmentation maps with individual object masks, and depth maps. Finally, all scenes were rendered using carefully positioned cameras to capture optimal axonometric and frontal viewpoints, ensuring maximum information content for subsequent 3D reconstruction tasks. full statistical breakdown and visual examples are provided in Appendix A.3."
        },
        {
            "title": "3.2 Scene Image Analysis",
            "content": "We utilize the prompt expander described in Sec. 3.1 to transform the prompt into more expressive scene image ğ¼ . Subsequently, we need to analyze the content of the image, which includes the semantic segmentation map of the foreground objects ğ‘†fg, the geometric proxy models for each object in the image, specifically the 3D oriented bounding boxes (OBBs) of the foreground objects, plane detection for walls, floors, and ceilings, as well as the logical relationships among the objects depicted in the scene. Foreground Objects Semantic Parsing. First, Based on the Chain of Thought (CoT) strategy [Wang et al. 2022], we design prompt incorporating predefined asset library categories (see Appendix A.2.2) and input it with the image into GPT-4o to parse objects in the image. We transform these categories into format suitable for grounding-dino detection through category merging map M, conğ‘” ğ‘– } = {M (cateğ´ verting {cateğ´ ğ‘– )}. Using grounding-dinoğ‘– } into {cate 1.5 [Ren et al. 2024], we obtain 2D bounding boxes {bbox2ğ· ğ‘– }, which we input into SAM [Kirillov et al. 2023] to generate foreground segmentation results ğ‘†fg = {mğ‘– }. Geometry Content Analysis. We employ Depth Anything V2 [Yang et al. 2024a] to estimate the depth map ğ· of the scene image and convert it to point cloud ğ‘ƒ using camera intrinsics ğ¾. For foreground regions ğ‘†fg = {mğ‘– }, we extract corresponding point clouds {ğ‘ƒ mğ‘– } and fit oriented bounding boxes (OBBs) {obbmğ‘– }. For background regions, we apply RANSAC [Fischler and Bolles 1981] to identify perpendicular planes representing walls, floor, and ceiling, by minimizing the Hausdorff distance between these planes and background points while enforcing orthogonality constraints. Scene Graph Construction. Based on multimodal model capabilities, we selected two key geometric relationships as shown in Fig. 4, which are easily interpreted from images and generalize well, even in quasi-outdoor scenes: (1) Support Relationship: Object obja supports objb (obja objb) when objb is positioned above obja, suspended by ceiling, or contained within obja; and (2) Wall Proximity Relationship: Object objb has contact with structural elements (walls, ceilings), defined as ğ‘‘ (obbb, (ğ‘›ğ‘¤, ğ‘¡ ğ‘¤)) = 0. We construct the scene graph through three-step process: (1) Analysis of the Floor Support Tree Structure using GPT-4o to determine floor-supported objects and establish recursive support tree with vertical relative distances ğ‘‘ vertical; (2) Analysis of CeilingSupported Objects; and (3) Analysis of Objects Against Walls, determining which objects contact specific walls. Detailed implementation of this procedure is provided in Appendix A.1.1. Due to occlusions causing incomplete depth maps, we refine OBBs using above scene graph logical relationships. For objects supported by the floor, we ensure their OBBs maintain perpendicular relationships with the floor plane and extend them to make proper contact. (a) (b) Fig. 4. (a) Scene graph constraints extracted by our algorithm. (b) Close-up of the support relationship tree structure (highlighted in red box in (a)). ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 6 Zhu, Huang, et al."
        },
        {
            "title": "3.3 Scene layout Reconstruction",
            "content": "After analyzing the scene image, we reconstruct the scene layout corresponding to the predefined asset set ğ´ through asset retrieval and transformation estimation based on visual features and geometric semantics to obtain the coarse scene layout. 3D Asset Retrieval. For each masked region ğ¼mğ‘– , we retrieve 3.3.1 the most suitable 3D asset objmğ‘– from our assets library by combining inverse category mapping 1 with visual feature similarity and size compatibility metrics (see Appendix A.1.2)."
        },
        {
            "title": "3.3.2 Transformations Estimation Module. We first design a multi-\nstep strategy based on visual features and geometric semantics\nto estimate the rotation transformations corresponding to the 3D\nassets {objmğ‘– }. Then, we infer a coarse translation transformation\nbased on the centers of {obbmğ‘– }. Finally, while ensuring that the\ndeformation of the assets remains visually coherent, we maximize\nthe intersection volume between obbmğ‘– and obbobjmğ‘– to obtain the\ncorresponding scale transformation for objmğ‘– .",
            "content": "Rotation Transformation Estimation. We employ coarse-to-fine strategy combining visual semantics and geometric information: Fig. 5. Coarse-to-fine view selection. Coarse selection ranks views by keypoint match quality, while fine selection uses homography transformation to identify the most viewpoint-similar match (selecting ğ‘£vis in this example). 1 ğ‘˜=1 as (objmğ‘– Visual-semantic based candidates. Following works [LabbÃ© et al. 2022; Nguyen et al. 2024b], we first render the asset objmğ‘– from , ğ‘£ğ‘˜ )), and then 162 pre-sampled viewpoints ğ‘‰ = {ğ‘£ğ‘˜ }162 , ğ‘£ğ‘˜ ))img using the feaextract pose-sensitive features ğ¹ae (R (objmğ‘– ture extractor ğ¹ae ()img from GigaPose [Nguyen et al. 2024b], which excels at detecting rotations perpendicular to the image plane (i.e., in-plane rotations). Finally, we establish the similarity to measure the overall similarity between the two images through the matching relationship of these features. The similarity is computed as follows: (cid:68) ğ¹ae (ğ¼ ğ´ (cid:69) mğ‘– ,ğ‘£ğ‘˜ )img ( ğ‘— ), ğ¹ae (ğ¼mğ‘– )img ( ğ‘— ) simimg (ğ¼ ğ´ , ğ¼mğ‘– ) = mğ‘– ,ğ‘£ğ‘˜ cos (2) where ğ¼ ğ´ ğ‘— mğ‘– ,ğ‘£ğ‘˜ = (objmğ‘– ğ‘£ğ‘˜ , ğ‘£ğ‘˜ ) and ğ‘£ğ‘˜ represents the set of matching points determined by semantic feature similarity. Coarse selection. We aim to select the top ğ‘˜ candidate views ğ‘‰can from the 162 sampled viewpoints, focusing on views with higher keypoint correspondences and stronger semantic similarity. The top ğ‘˜ candidate views are selected based on the feature similarity simimg (, ). In our experiments, ğ‘˜ is set to 10, ensuring the optimal view is among the candidates. Fine selection. For each candidate ğ‘£ğ‘– ğ‘‰can, we first compute the homography transformation matrix ğ»ğ‘£ between the candidate view ğ¼ obj ğ‘£ğ‘– and the input image ğ¼mğ‘– by RANSAC. We then analyze the difference between the homography transformation ğ»ğ‘£ and the identity matrix, as in Eq. 3, this homography transformation analysis effectively suppresses errors in correspondences arising from symmetrical ambiguities  (Fig. 5)  . The final top ğ‘˜ = 4 views are those with the smallest Frobenius norm: ğ‘– }ğ‘˜ ğ¹ , ğ‘£ I2 ğ‘ˆğ‘£ğ‘‰ ğ‘‡ {ğ‘£ vis ğ‘–=1 = arg (3) (ğ‘˜ ) min ğ‘£ ğ‘‰can where ğ»ğ‘£ = ğ‘ˆğ‘£ Î£ğ‘‰ ğ‘‡ ğ‘£ is the singular value decomposition of ğ»ğ‘£, and ğ¹ denotes the Frobenius norm. Geometric enhancement of candidates. Leveraging the geometric consistency from single-image depth recovery, particularly with cuboid-like assets, we refine rotation estimation using the accurate ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Fig. 6. Top-view illustration of candidates geometric enhancement. Each row compares orientation estimations for different query scenarios, showing ground truth (GT), OBB-based (ğ‘£obb ) estimations. The best estimation (ğ‘£best) is highlighted, demonstrating the adaptive integration of geometric guidance. ), and vision-based (ğ‘£vis 1 OBBs obtained in Sec. 3.2. For well-defined cuboid objects, the four orientations of the OBBs vertical planes, {ğ‘£ obb }4 ğ‘–=1, guide the rotation transformation {[ğ‘…ğ‘£obb ğ‘–=1. However, for non-cuboid shapes or incomplete point clouds due to occlusions or errors, we use an adaptive strategy to ensure robustness. The final rotation is selected by minimizing the angular difference between candidate viewpoints and geometric viewpoints: ]}4 ğ‘– ğ‘– arccos (cid:32) Trace(ğ‘…ğ‘£visğ‘‡ ğ‘…ğ‘£obb ) 1 2 (cid:33) (4) (ğ‘£ obb , ğ‘£ vis ) = arg min ğ‘£obb {ğ‘£obb }, ğ‘– ğ‘£vis {ğ‘£vis ğ‘— } (cid:26) ğ‘£ obb , ğ‘£ vis 1 ğ‘£best = , if if ğœƒ ğœ, ğœƒ > ğœ . and ğ‘£ vis Here, ğ‘£best is the selected viewpoint, ğœƒ is the angle between the view ğ‘£ obb 5 in our experiments. This approach prioritizes OBB-based estimation for cuboid-like objects and defaults to ğ‘£ vis 1 when the OBB guidance is unreliable  (Fig. 6)  . , and ğœ = ğœ‹ Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 7 Here, we set ğœ†1 = 0.1 in our experiments. The variables ğ‘¡ğ‘– and ğ‘¡ update represent the initial and optimized positions of the object ğ‘– objmğ‘– , respectively. The function Rm (, ) renders the geometry of the object to obtain mask image, where ğ‘£ref denotes reference viewpoint for depth conversion into consistent point cloud, shared across all objects in the experiments. The values ğ‘§(obj)min and ğ‘§(obj)max denote the minimum and maximum ğ‘§ values, respectively. ğ‘‘ (ğ´, ğµ) = inf {ğ‘ ğ‘ ğ‘ ğ´, ğ‘ ğµ}. We solve this optimization in two steps: preprocessing support and wall constraints, then applying simulated annealing [Skiscim and Golden 1983] using efficient voxel-based intersection calculations. Full details are in Appendix A.1.4."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our system through comprehensive user studies and experiments focusing on: quality assessment, rotation estimation from single images, and ablation studies. Fig. 7. Internal placement logic illustration. Left: query object (yellow outline). Right: internal subspaces of the target container. Objects are placed in the nearest subspace based on the vertical distance ğ‘‘ vertical between the centers of the object and the subspace along the gravity direction(G). Translation and Scale Transformation Estimation. For translation, we begin by approximating object positions using the OBB centers. For scaling, we optimize asset dimensions according to the object type: vertically adjustable, slender objects with two principal axes, or fully scalable objects. This ensures both practical placement and the preservation of each assets inherent design integrity (more details in Appendix A.1.3)."
        },
        {
            "title": "3.4 Refinement of Scene Layout",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "After individually estimating transformations for foreground objects, ambiguities may arise from depth errors and asset discrepancies. We resolve these through novel three-stage refinement: optimizing rotation and scale using scene graph relationships, formulating constrained optimization for translations that preserves visual alignment while enforcing physical constraints, and applying physics simulation to ensure realistic object behaviors."
        },
        {
            "title": "3.4.2 Global Post-optimization for Translation Transformations. We\noptimize object positions to ensure physical plausibility through\na constrained formulation that enforces non-intersection between\nobjects, proper support hierarchies, ceiling attachments, and wall\nproximity requirements. we construct an objective function balances\nadherence to initial positions with visual segmentation alignment,\nwhile satisfying spatial constraints derived from the scene structure:",
            "content": "min {ğ‘¡ update ğ‘– ğœ†1 ğ‘¡ğ‘– ğ‘¡ update ğ‘– 2 + mğ‘– Rm (objmğ‘– 2 , ğ‘£ref ) 2 . } ğ‘– objmğ‘– objmğ‘— = , ğ‘§ (objmğ‘– )max = ğ‘¡ğ‘, , objğ‘¤ ) = 0, ğ‘‘ (objmğ‘– ğ‘§ (objmğ‘— )min = ğ‘§ (objmğ‘– ) , if ğ‘– ğ‘—, if ğ‘– C, Supported by Ceiling, if objmğ‘– is against objğ‘¤, if objmğ‘– and objmğ‘— meet T. s.t. (5) We finetune the Flux model on our proposed dataset, which contains 147 unique scenes. The training data consists of images rendered with Blender at resolution of 10241024 pixels. To ensure comprehensive representation of the scene layout, camera perspectives were manually selected, focusing on axonometric and frontal views. The training is conducted on two A100 GPUs for 15 epochs using LoRA with rank of 16 and learning rate of 1e-4. Following the DreamBooth [Ruiz et al. 2023] strategy, we employ regularization technique that uses unique identifier, [V], for our in-domain data while including samples without this token for generalization. Our system takes approximately 240 seconds to run on single A100, with the following time distribution: text-to-image generation (10 seconds), scene image analysis (110 seconds), scene layout reconstruction (60 seconds), and layout refinement (60 seconds)."
        },
        {
            "title": "4.2.1 Evaluation by Senior Art Students. We invited 100 senior art\nstudents (ages 20-24) to evaluate our 3D scenes against HOLODECK\n[Yang et al. 2024b], LayoutGPT [Feng et al. 2024], DiffuScene [Tang\net al. 2024], and InstructScene [Lin and Mu 2024]. These methods\nrepresent two layout generation strategies: LLM-guided approaches\nand data-driven generative models. For each method, we prepared 15\nscenes per scene type (living room, dining room, and bedroom), total-\ning 45 scenes. Note that DiffuScene only supports these three scene\ntypes, while LayoutGPT is further limited to living room and bed-\nroom scenes. For fair comparison, we removed all textures to focus\non layout quality and standardized the asset database for Holodeck\nand LayoutGPT (we couldnâ€™t replace DiffuScene and InstructSceneâ€™s\nassets due to its training-based nature requiring substantial data).\nParticipants answered two questions:",
            "content": "Q1: Which layout appears more reasonable and realistic? Q2: Which layout is more coherent and aesthetically pleasing? ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 8 Zhu, Huang, et al. Fig. 8. Comparison of our generated 3D scene layouts with other state-of-the-art methods, illustrating the richness of our 3D generated layouts. More examples of our generated layouts are shown in Appendix A.5. Table 1. Comparison of preferable rates (%) for different methods. Our vs. Reasonable & Realistic"
        },
        {
            "title": "DiffuScene\nHolodeck\nLayoutGPT\nInstructScene",
            "content": "75.69 79.27 66.33 82.59 77.08 76.69 68.46 79.37 76.79 76.50 61.29 74.86 82.72 69.39 85.57 72.92 77.54 75.17 80.72 74.55 81.11 72. As shown in Table 1, our method consistently outperforms all baselines. For reasonableness and realism, our approach achieves average preference rates of 79.22%, 77.71%, 76.60%, and 65.36% compared to DiffuScene, HOLODECK, LayoutGPT, and InstructScene respectively. For aesthetic quality, our method demonstrates even stronger advantages with preference rates of 80.38%, 76.73%, 79.33%, and 72.49% against the same competitors. Fig. 8 provides visual comparisons of these results."
        },
        {
            "title": "4.2.2 Evaluation by Professional Artists on Richness. We recruited\n20 professional artists, each with at least three years of experience,\nto evaluate 60 scenes across three room types (Living Room, Dining\nRoom, and Bedroom). The artists rated three dimensionsâ€”overall\ncomposition, semantic logic, and aesthetic appealâ€”on a 1-5 scale.\nTo ensure a fair comparison with baseline methods, we conducted\nadditional evaluations where textures were removed. These scenes\nwere also evaluated by GPT-4o on the same dimensions. A score of",
            "content": "ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 3 was set as the baseline, representing the average level compared to professionals. Detailed in Appendix A.2.4. Table 2. Expert and GPT-4o evaluation comparison."
        },
        {
            "title": "Method\nOurs\nDiffuScene\nHOLODECK\nLayoutGPT\nInstructScene",
            "content": "Composition Semantic Aesthetic Overall 3.34/3.06 2.83/2.97 2.69/2.77 2.34/2.92 2.85/2.99 3.29/2.86 2.80/2.78 2.56/2.55 2.26/2.83 2.75/2.83 3.37/3.16 2.83/3.07 2.80/2.86 2.35/2.97 2.89/3.08 3.35/3.16 2.86/3.07 2.71/2.91 2.42/2.97 2.91/3.07 As seen in Table 2, our method consistently outperforms all baseline approaches, scoring 3.34 from human artists and 3.06 from GPT-4o, indicating performance on par with or slightly better than professional standards."
        },
        {
            "title": "4.2.3\nFidelity and Similarity of 3D Scene Layout Reconstruction.\nWe randomly selected 30 scenes from our dataset and used their\nrendered images to evaluate our systemâ€™s reconstruction ability\nagainst ground-truth layouts. Objects supported by the ground or\nceiling, or located near walls, were classified as primary objects\ncrucial for scene structure, while others were considered secondary\nobjects. Our evaluation includes seven key metrics: object recovery\nrates, category preservation rates, rotation AUC@60Â°, translation\nAUC@0.5m, scene graph relationship accuracy, CLIP similarity, and\nGPT-4oâ€™s assessment of layout fidelity.",
            "content": "Results in Table 3 show high fidelity in primary object recovery (92.31%) and category preservation (95.83%). The system also achieves strong geometric accuracy in rotation (74.83% AUC@60) and translation (84.32% AUC@0.5m), along with 93.26% scene graph accuracy. Secondary objects achieve lower recovery rates (70.41%) due to resolution limitations and detection model constraints on smaller objects. CLIP similarity and GPT-4o ratings further confirm layout fidelity. Additional 3D scene layouts with their corresponding guide images are presented in Appendix 4.4.1. Table 3. Fidelity and layout similarity evaluation using dataset scenes."
        },
        {
            "title": "Similarity",
            "content": "Object Recovery Category Preservation Rotation (AUC@60) Translation (AUC@0.5m) Scene Graph Accuracy CLIP (Guide Image) CLIP (Render Image) GPT-4o Rating"
        },
        {
            "title": "Secondary",
            "content": "92.31% 95.83% 74.83% 84.32% 70.41% 91.67% 71.51% 80.40% 93.26% 27.03 25.83 8.29/"
        },
        {
            "title": "4.3 Comparison of Rotation Transformation Estimation",
            "content": "We evaluate our rotation transformation estimation on the 3DFuture category asset pose estimation dataset, 3DF-CLAPE, which we derived from the 3D-Future dataset to better align with layout generation scenarios. It contains two subsets: (1) 3DF-CLAPECategory with 5,833 query-template pairs across 34 categories for category-level evaluation, and (2) 3DF-CLAPE-Instance with 3,252 pairs for instance-level evaluation. Following standard practice [Li et al. 2020; Shotton et al. 2013; Wang et al. 2019], we report mean average precision (mAP) at various rotation error thresholds and the area under the curve (AUC). Due to our unique task of open-set pose estimation for categorylevel CAD models from single images, we select several benchmarks that have shown potential in this domain: DINOv2, SPARC, and DiffCAD, AENet, GigaPose, Orient Anything [Wang et al. 2024b]. Table 4. Quantitative comparison of rotation estimation methods using AUC@60. (OrientA: Orient Anything; GigaP: GigaPose) AUC@60 DINOv2 SPARC DiffCAD OrientA GigaP AENet Ours Category-level 31.68% 52.54% 26.45% 56.07% 39.85% 45.32% 70.06% 31.38% 61.46% 25.44% 56.24% 57.43% 62.16% 81.44% Instance-level As shown in Table 4, our approach achieves an AUC@60 of 70.06% for category-level and 81.44% for instance-level evaluation, significantly surpassing all benchmarks. Fig. 9 further demonstrates that our method outperforms existing approaches in category-level rotation estimation, achieving mAP values of 50.5%, 65.5%, and 80.5% at thresholds of 5, 15, and 45 respectively. Despite GigaPose using the same keypoint extractor (AENet) as our method, it underperforms due to limitations in handling template-query discrepancies. The results demonstrate both CAD-based approaches superiority for 3D scene layout and the critical role of query-template similarity in pose estimation, shown by template matching methods outperforming non-template approaches (Orient Anything: 56.24% Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation AUC) and the marked improvements in instance-level tasks where query-template similarity is highest. Fig. 9. Comparison of performance in category and instance level rotation estimation with other methods."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "We conduct comprehensive ablation studies to validate our key design choices across three components: (1) finetuning the Flux diffusion model, (2) rotation transformation estimation with homography and geometric information, and (3) scene layout refinement pipeline. These studies demonstrate that each component meaningfully contributes to system performance while maintaining generative diversity and physical plausibility."
        },
        {
            "title": "4.4.1 Ablation study of finetuned Flux. We evaluate the impact of\nFlux finetuning through comprehensive ablation studies. While our\nsystem functions with off-the-shelf Flux, targeted finetuning en-\nhances retrieval accuracy and pose estimation without sacrificing\ngenerative diversity. As shown in Fig. 10, the finetuned model gen-\nerates images better aligned with our asset library given identical\nprompts. Table 4 demonstrates substantial pose estimation improve-\nments (AUC@60Â° from 70.06% to 81.44%) when query objects match\nCAD models. We compare vanilla and finetuned Flux regarding\nretrieval accuracy, overfitting potential, and diversity preservation.",
            "content": "Retrieval Accuracy. Based on the generation of 100 scene images each by Vanilla Flux and Finetuned Flux, we utilized our image analysis pipeline to identify 2343 objects and 2204 objects in the corresponding scenes, respectively. In addition, we manually identified the ground truth matches for these objects from our 3D asset library. The retrieval performance was evaluated using Top-1 and Top-3 accuracy: Table 5. Accuracy comparison between vanilla and finetuned models."
        },
        {
            "title": "Finetuned Flux",
            "content": "Top-1 Accuracy Top-3 Accuracy 48.57% 68.57% 68.70% 83.21% ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 10 Zhu, Huang, et al. Fig. 10. Comparison between Finetuned Flux and Vanilla Flux generated images. Given identical prompts (left column), Finetuned Flux (second column) generates images with objects that more closely resemble assets in our 3D library (third column), compared to Vanilla Flux (right column). This alignment improves retrieval accuracy and pose estimation, enabling more precise scene parsing and strengthening system robustness. The substantial improvement demonstrates that finetuning enhances the models ability to generate scenes aligned with our 3D asset library. More layouts with guide images, as show in Fig. 11. Table 6. Comparison of overfitting and diversity metrics. diversity (Intra-set Scene Sim.) using average pairwise scene-toscene similarity within each prompt set. Table 6 shows that the finetuned model maintains comparable visual and layout diversity to the vanilla model."
        },
        {
            "title": "Overfitting",
            "content": "NN LPIPS Scene Sim. to Training"
        },
        {
            "title": "Vanilla Flux\nFinetuned Flux",
            "content": "0.6375 0.5981 0.3665 0."
        },
        {
            "title": "Diversity",
            "content": "DIV (LPIPS) Intra-set Scene Sim."
        },
        {
            "title": "Vanilla Flux\nFinetuned Flux",
            "content": "0.5782 0.5901 0.2974 0.3178 Overfitting Evaluation. To evaluate whether the Flux model is overfitting, we initially employed the Nearest Neighbor (NN) LPIPS distance to measure the visual similarity between the generated scene images and their closest matches in the training set. Additionally, following previous studies [Henderson and Ferrari 2017; Ritchie et al. 2019], we adopted scene-to-scene similarity function to specifically assess the similarities in scene layouts (the detailed methodology is provided in Appendix A.1.5). As shown in Table 6, higher NN LPIPS values indicate less visual overfitting, while lower scene similarity scores suggest reduction in layout overfitting. The finetuned Flux exhibits comparable NN LPIPS to the vanilla model, with only slightly higher scene similarity, indicating minimal overfitting. This confirms that our model generates novel arrangements rather than memorizing the training set. Diversity Preserving. Following DreamBooth we generated 20 images for each of 6 diverse prompts and calculated both visual diversity (DIV) using average pairwise LPIPS distances and layout ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Learning Dynamics Analysis. Our experiments reveal clear learning hierarchy: the finetuned Flux model readily learns style and viewpoint (as visually apparent in Figs. 8, 10, 11, and 13), moderately captures object textures, but preserves layout diversity. We hypothesize this stems from varying supervision strengthsstyle and viewpoint provide strong global patterns across all training data, shapes and textures offer moderate signals through repeated object appearances, while layouts remain weakly learned due to scene uniqueness and multi-body constraints complexity. This hierarchical learning aligns with our goal of enhancing retrieval and pose estimation while maintaining generative flexibility. Table 7. Ablation study of our rotation transformation estimation module. AENet Homography Geometry mAP@5 mAP@15 mAP@45 4.30% 5.21% 36.22% 66.57% 15.34% 59.42% 71.73% 75.28% 67.92% 76.07% 77.16% 80.61%"
        },
        {
            "title": "4.4.2 Ablation study of rotation transformation estimation. Table 7\npresents the ablation study of our rotation transformation estima-\ntion. The results highlight the significance of each component in\nour coarse-to-fine approach. The incorporation of homography\nsignificantly enhances performance(as show in Fig. 5), achieving\nmAP@45â—¦ of 76.07% and mAP@15â—¦ of 59.42%. Furthermore, the\nintegration of geometric information further improves estimation\naccuracy, particularly at lower error thresholds, with mAP@5â—¦ in-\ncreasing from 5.23% to 36.22%. Our complete model, which combines\nall components (AENet, homography, and geometry), achieves the",
            "content": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 11 Fig. 11. Additional results showcasing our methods ability to generate coherent 3D layouts from diverse guide images. The generated scenes (bottom) demonstrate high fidelity to the inputs spatial arrangement and style. best performance across all metrics. This demonstrates the effectiveness of our approach in combining visual-semantic features with geometric information for precise rotation estimation."
        },
        {
            "title": "Method",
            "content": "Supp. Corr. (%) Inter. Pairs GPT-4o (1-5) Initial Estimation + Local Refinement + Global Optimization + Physical Constraints 62.45 72.86 90.80 91.34 5.43 4.43 2.21 2.20 2.83 3.07 3.26 3."
        },
        {
            "title": "5 Application",
            "content": "Generating 3D scenes typically requires significant time and expertise from skilled artists, making straightforward method for re-editing essential. Unlike previous approaches based on large language models (LLMs) or 3D generation models, our method allows for more granular editing based on image manipulation techniques. Fig. 12. It showcases some re-editing examples that we generated using the Image Generation model. Using the text prompts from the second column, we re-paint the local information within the red box of the images in the first column using Flux, thereby controlling the 3D layout. This control over local information can achieve very robust effect. As shown in Fig. 12, we present several detailed editing examples, including global scene completion, object replacement, and local object addition. After generating the 3D scene, we can obtain renderings of both the global scene and any specific local area. By leveraging the capabilities of image generation models to fill in masked regions, we can perform fine-grained, controllable re-painting of any part of the scene, including specific objects and their exact positions. After re-painting, we fix the objects outside the masked area and utilize our algorithm to re-retrieve and estimate the relevant poses of the objects within the masked region. ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 12 Zhu, Huang, et al."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "We present visual-guided 3D scene layout system that generates coherent, aesthetic scenes from text or Canny images within 240 seconds, significantly reducing the 2.5-hour time typical in professional workflows. Our approach integrates Flux for layout generation, fine-tuned on our asset library for style consistency and more aligned asset selection. Unlike previous methods, we dynamically use image guidance for object orientations, creating more natural 3D layouts. User studies with 100 art students and 20 professional artists demonstrate significant performance advantages over current SoTA methods. Limitation and Future Work. While our approach achieves highfidelity layouts, it is constrained by certain limitations. Despite our current progress, fine-tuning the image Generation model to achieve high consistency across multiple objects in complex scenes remains primary challenge. Additionally, accurate pose estimation from single images remains challenging, particularly with severe occlusions. These failure modes are visually detailed in Appendix A.4. We anticipate these limitations will diminish as visual foundation models advance. To specifically address pose ambiguity, incorporating multi-view perspective information from methods like MVD [Liu et al. 2024b,c,a] offers promising path for more robust scene analysis. Looking forward, our system shows promise as an automated 3D data generation engine by transforming abundant 2D vision model placement knowledge into 3D asset placement data, addressing data scarcity in 3D scene generation tasks [Ost et al. 2021; Raistrick et al. 2024]. This enables more efficient training of models for 3D scene understanding and layout generation. Finally, introducing more coherent editing capabilities between 2D and 3D [Deng et al. 2023; Xu et al. 2025; Yan et al. 2024] is meaningful exploration for making our future system more user-friendly."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the National Key R&D Program of China (Grant No. 2022YFB3303101, 2022YFB3303400) and the National Natural Science Foundation of China (62025207). We thank Yihang Wang for his efforts on the first demo and constructive suggestions on flux fine-tuning, and Zhi Ji, Yuxuan Xie, and Town Zhang for their helpful discussions and technical support."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). Rio Aguina-Kang, Maxim Gumin, Do Heon Han, Stewart Morris, Seung Jean Yoo, Aditya Ganeshan, R. Kenny Jones, Qiuhong Anna Wei, Kailiang Fu, and Daniel Ritchie. 2024. Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases. arXiv:2403.09675 [cs.CV] https://arxiv.org/abs/2403. 09675 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL] https://arxiv.org/abs/2005.14165 Ata Ã‡elen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni, Anton Obukhov, and Xi Wang. 2024. I-design: Personalized llm interior designer. arXiv preprint arXiv:2404.02838 (2024). ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Angel Chang, Manolis Savva, and Christopher Manning. 2014. Learning spatial knowledge for text to 3D scene generation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 20282038. Angel Chang, Mihail Eric, Manolis Savva, and Christopher Manning. 2017. SceneSeer: 3D scene design with natural language. arXiv preprint arXiv:1703.00050 (2017). Manuel Dahnert, Angela Dai, Norman MÃ¼ller, and Matthias NieÃŸner. 2024. Coherent 3D Scene Diffusion From Single RGB Image. Advances in Neural Information Processing Systems 37 (2024), 2343523463. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. 2024. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems 36 (2024). Wei Deng, Mengshi Qi, and Huadong Ma. 2025. Global-local tree search in vlms for 3d indoor scene generation. In Proceedings of the Computer Vision and Pattern Recognition Conference. 89758984. Zhi Deng, Yang Liu, Hao Pan, Wassim Jabi, Juyong Zhang, and Bailin Deng. 2023. Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via Single Sketch. IEEE Trans. Vis. Comput. Graph. 29, 9 (2023), 38263839. Helisa Dhamo, Fabian Manhardt, Nassir Navab, and Federico Tombari. 2021. Graphto-3d: End-to-end generation and manipulation of 3d scenes using scene graphs. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 16352 16361. Bertram Drost, Markus Ulrich, Nassir Navab, and Slobodan Ilic. 2010. Model globally, match locally: Efficient and robust 3D object recognition. In 2010 IEEE computer society conference on computer vision and pattern recognition. Ieee, 9981005. Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2024. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems 36 (2024). Martin Fischler and Robert Bolles. 1981. Random sample consensus: paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM 24, 6 (1981), 381395. Matthew Fisher and Pat Hanrahan. 2010. Context-based search for 3d models. In ACM SIGGRAPH Asia 2010 papers. 110. Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. 2012. Example-based synthesis of 3D object arrangements. ACM Transactions on Graphics (TOG) 31, 6 (2012), 111. Matthew Fisher, Manolis Savva, Yangyan Li, Pat Hanrahan, and Matthias NieÃŸner. 2015. Activity-centric scene synthesis for functional 3D scene modeling. ACM Transactions on Graphics (TOG) 34, 6 (2015), 113. Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, 3D-FUTURE: 3D Furniture shape with TextURE. and Dacheng Tao. 2020. arXiv:2009.09633 [cs.CV] https://arxiv.org/abs/2009.09633 Qiang Fu, Xiaowu Chen, Xiaotian Wang, Sijia Wen, Bin Zhou, and Hongbo Fu. 2017. Adaptive synthesis of indoor scenes via activity-associated object relation graphs. ACM Transactions on Graphics (TOG) 36, 6 (2017), 113. Daoyi Gao, DÃ¡vid Rozenberszki, Stefan Leutenegger, and Angela Dai. 2024. Diffcad: Weakly-supervised probabilistic cad model retrieval and alignment from an rgb image. ACM Transactions on Graphics (TOG) 43, 4 (2024), 115. Can GÃ¼meli, Angela Dai, and Matthias NieÃŸner. 2022. Roca: Robust cad model retrieval and alignment from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 40224031. Paul Henderson and Vittorio Ferrari. 2017. generative model of 3d object layouts in apartments. arXiv preprint arXiv:1711.10939 (2017). Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. arXiv:2006.11239 [cs.LG] https://arxiv.org/abs/2006.11239 Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and Alireza Fathi. 2024. Scenecraft: An llm agent for synthesizing 3d scenes as blender code. In Forty-first International Conference on Machine Learning. Ian Huang, Yanan Bao, Karen Truong, Howard Zhou, Cordelia Schmid, Leonidas Guibas, and Alireza Fathi. 2025. Fireplace: Geometric refinements of llm common sense reasoning for 3d object placement. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1346613476. Chenfanfu Jiang, Siyuan Qi, Yixin Zhu, Siyuan Huang, Jenny Lin, Lap-Fai Yu, Demetri Terzopoulos, and Song-Chun Zhu. 2018. Configurable 3D Scene Synthesis and 2D Image Rendering with Per-pixel Ground Truth Using Stochastic Grammars. International Journal of Computer Vision 126, 9 (June 2018), 920941. https://doi. org/10.1007/s11263-018-1103Yun Jiang, Marcus Lim, and Ashutosh Saxena. 2012. Learning object arrangements in 3d scenes using human context. arXiv preprint arXiv:1206.6462 (2012). Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. 2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 40154026. Weicheng Kuo, Anelia Angelova, Tsung-Yi Lin, and Angela Dai. 2020. Mask2cad: 3d shape prediction by learning to segment and retrieve. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16. Springer, 260277. Yann LabbÃ©, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. 2022. Megapose: 6d pose estimation of novel objects via render & compare. arXiv preprint arXiv:2212.06870 (2022). Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux. Florian Langer, Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. 2022. Sparc: Sparse render-and-compare for cad model alignment in single rgb image. arXiv preprint arXiv:2210.01044 (2022). Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. 2020. DeepIM: Deep Iterative Matching for 6D Pose Estimation. International Journal of Computer Vision (Mar 2020), 657678. https://doi.org/10.1007/s11263-019-01250-9 Chenguo Lin and Yadong Mu. 2024. Instructscene: Instruction-driven 3d indoor scene synthesis with semantic graph prior. arXiv preprint arXiv:2402.04717 (2024). Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. 2024b. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1007210083. Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. 2024c. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems 36 (2024). Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. 2024a. Syncdreamer: Generating multiview-consistent images from single-view image. In ICLR. Yang Liu, Muzhi Zhu, Hao Chen, Xinlong Wang, Bo Feng, Hao Wang, Shiyu Li, Raviteja Vemulapalli, and Chunhua Shen. 2025. Segment Anything in Context with Vision Foundation Models. International Journal of Computer Vision (2025), 126. Yang Liu, Muzhi Zhu, Hengtao Li, Hao Chen, Xinlong Wang, and Chunhua Shen. 2023. Matcher: Segment anything with one shot using all-purpose feature matching. arXiv preprint arXiv:2305.13310 (2023). Rui Ma, Honghua Li, Changqing Zou, Zicheng Liao, Xin Tong, and Hao Zhang. 2016. Action-driven 3D indoor scene evolution. ACM Trans. Graph. 35, 6 (2016), 1731. Paul Merrell, Eric Schkufza, Zeyang Li, Maneesh Agrawala, and Vladlen Koltun. 2011. Interactive furniture layout using interior design guidelines. ACM transactions on graphics (TOG) 30, 4 (2011), 110. Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit. 2024a. GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 99039913. https://doi.org/10.1109/CVPR52733. 2024.00945 Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit. 2024b. Gigapose: Fast and robust novel object pose estimation via one correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 99039913. Van Nguyen Nguyen, Yinlin Hu, Yang Xiao, Mathieu Salzmann, and Vincent Lepetit. 2022. Templates for 3d object pose estimation revisited: Generalization to new objects and robustness to occlusions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 67716780. Yinyu Nie, Angela Dai, Xiaoguang Han, and Matthias NieÃŸner. 2023. Learning 3d scene priors with 2d supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 792802. Evin Pinar Ã–rnek, Yann LabbÃ©, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, and Tomas Hodan. 2024. FoundPose: Unseen Object Pose Estimation with Foundation Features. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XXVI (Lecture Notes in Computer Science, Vol. 15084), Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and GÃ¼l Varol (Eds.). Springer, 163182. https://doi.org/10.1007/978-3-031-73347-5_10 Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. 2021. Neural scene graphs for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 28562865. Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. 2021a. ATISS: Autoregressive Transformers for Indoor Scene Synthesis. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 1201312026. https://proceedings.neurips. cc/paper/2021/hash/64986d86a17424eeac96b08a6d519059-Abstract.html Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. 2021b. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems 34 (2021), 1201312026. Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 13 Pulak Purkait, Christopher Zach, and Ian Reid. 2020. SG-VAE: Scene Grammar Variational Autoencoder to Generate New Indoor Scenes. 155171. https://doi.org/10.1007/978-3030-58586-0_10 Siyuan Qi, Yixin Zhu, Siyuan Huang, Chenfanfu Jiang, and Song-Chun Zhu. 2018. Human-centric indoor scene synthesis using stochastic grammar. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 58995908. Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, et al. 2024. Infinigen indoors: Photorealistic indoor scenes using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2178321794. Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. 2024. Grounding DINO 1.5: Advance the\" Edge\" of Open-Set Object Detection. arXiv preprint arXiv:2405.10300 (2024). Daniel Ritchie, Kai Wang, and Yu-an Lin. 2019. Fast and flexible indoor scene synthesis via deep convolutional generative models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 61826190. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subjectdriven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2250022510. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). Manolis Savva, Angel Chang, and Maneesh Agrawala. 2017. Scenesuggest: Contextdriven 3d scene design. arXiv preprint arXiv:1703.00061 (2017). Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. 2013. Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images. In 2013 IEEE Conference on Computer Vision and Pattern Recognition. https://doi.org/10.1109/cvpr.2013.377 Christopher C. Skiscim and Bruce L. Golden. 1983. Optimization by simulated annealing: preliminary computational study for the TSP. In Proceedings of the 15th conference on Winter simulation, WSC 1983, Arlington, VA, USA, December 12-14, 1983, Stephen D. Roberts, Jerry Banks, and Bruce W. Schmeiser (Eds.). ACM, 523535. http://dl.acm. org/citation.cfm?id= Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. 2024. LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models. arXiv preprint arXiv:2412.02193 (2024). Martin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius, Kai Arras, and Rudolph Triebel. 2020. Multi-path learning for object pose estimation across domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1391613925. Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, and Matthias NieÃŸner. 2024. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2050720518. Stefan Thalhammer, Jean-Baptiste Weibel, Markus Vincze, and Jose Garcia-Rodriguez. 2023. Self-supervised vision transformers for 3d pose estimation of novel objects. Image and Vision Computing 139 (2023), 104816. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. 2020. Learning 3d semantic scene graphs from 3d indoor reconstructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 39613970. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001 (2022). He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas Guibas. 2019. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 26422651. Kai Wang, Manolis Savva, Angel Chang, and Daniel Ritchie. 2018. Deep convolutional priors for indoor scene synthesis. ACM Transactions on Graphics (TOG) 37, 4 (2018), 70. Xinpeng Wang, Chandan Yeshwanth, and Matthias Niesner. 2021. SceneFormer: Indoor Scene Generation with Transformers. In 2021 International Conference on 3D Vision (3DV). https://doi.org/10.1109/3dv53792.2021.00021 ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 14 Zhu, Huang, et al. Yian Wang, Xiaowen Qiu, Jiageng Liu, Zhehuan Chen, Jiting Cai, Yufei Wang, TsunHsuan Johnson Wang, Zhou Xian, and Chuang Gan. 2024a. Architect: Generating Vivid and Interactive 3D Scenes with Hierarchical 2D Inpainting. Advances in Neural Information Processing Systems 37 (2024), 6757567603. Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. 2024b. Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models. arXiv preprint arXiv:2412.18605 (2024). Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. 2021. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804 (2021). Kun Xu, Kang Chen, Hongbo Fu, Wei-Lun Sun, and Shi-Min Hu. 2013. Sketch2Scene: Sketch-based co-retrieval and co-placement of 3D models. ACM Transactions on Graphics (TOG) 32, 4 (2013), 115. Zhentong Xu, Long Zeng, Junli Zhao, Baodong Wang, Zhenkuan Pan, and Yong-Jin Liu. 2025. Sketch123: Multi-spectral channel cross attention for sketch-based 3D generation via diffusion models. Computer-Aided Design (2025), 103896. Ziyang Yan, Lei Li, Yihua Shao, Siyu Chen, Zongkai Wu, Jenq-Neng Hwang, Hao Zhao, and Fabio Remondino. 2024. 3dsceneeditor: Controllable 3d scene editing with gaussian splatting. arXiv preprint arXiv:2412.01583 (2024). Haitao Yang, Zaiwei Zhang, Siming Yan, Haibin Huang, Chongyang Ma, Yi Zheng, Chandrajit Bajaj, and Qixing Huang. 2021c. Scene Synthesis via Uncertainty-Driven Attribute Synchronization. Cornell University - arXiv,Cornell University - arXiv (Aug 2021). Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. 2024a. Depth Anything V2. arXiv preprint arXiv:2406.09414 (2024). Ming-Jia Yang, Yi Guo, Bin Zhou, and Xin Tong. 2021a. Indoor Scene Generation from Collection of Semantic-Segmented Depth Images. Cornell University - arXiv,Cornell University - arXiv (Aug 2021). Ming-Jia Yang, Yu-Xiao Guo, Bin Zhou, and Xin Tong. 2021b. Indoor scene generation from collection of semantic-segmented depth images. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1520315212. Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. 2024b. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1622716237. Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Lan Xu, Wei Yang, Jiayuan Gu, and Jingyi Yu. 2025. Cast: Component-aligned 3d scene reconstruction from an rgb image. arXiv preprint arXiv:2502.12894 (2025). Yi-Ting Yeh, Lingfeng Yang, Matthew Watson, Noah Goodman, and Pat Hanrahan. 2012. Synthesizing open worlds with constraints using locally annealed reversible jump mcmc. ACM Transactions on Graphics (TOG) 31, 4 (2012), 111. Lap Fai Yu, Sai Kit Yeung, Chi Keung Tang, Demetri Terzopoulos, Tony Chan, and Stanley Osher. 2011. Make it home: automatic optimization of furniture arrangement. ACM Transactions on Graphics (TOG)-Proceedings of ACM SIGGRAPH 2011, v. 30,(4), July 2011, article no. 86 30, 4 (2011). Guangyao Zhai, Evin PÄ±nar Ã–rnek, Dave Zhenyu Chen, Ruotong Liao, Yan Di, Nassir Navab, Federico Tombari, and Benjamin Busam. 2024. Echoscene: Indoor scene generation via information echo over scene graph diffusion. In European Conference on Computer Vision. Springer, 167184. Guangyao Zhai, Evin PÄ±nar Ã–rnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, and Benjamin Busam. 2023. Commonscenes: Generating commonsense 3d indoor scenes with scene graph diffusion. Advances in Neural Information Processing Systems 36 (2023), 3002630038. ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Supplementary Materials A.1 Technical Implementation Details Scene Graph Construction. Our scene graph construction A.1.1 involves extracting geometric logical relationships from foreground regions ğ‘†fg = {mğ‘– }. Due to complex object shapes and occlusions, we combine qualitative image analysis using multimodal models with precise geometric methods. The process consists of three steps: Step 1: Analysis of the Floor Support Tree Structure. We build tree-structured scene graph supported by the floor using recursive approach, as illustrated in Algorithm 1 and Algorithm 2. Using GPT-4o, we analyze each foreground object = {ğ¼mğ‘– } through prompting to determine floor support. For floor-supported objects, we identify assets located within or above them through recursive geometric search, establishing complete support tree while retaining vertical relative distances ğ‘‘ vertical for subsequent asset placement. Our experimental results show 91.95% accuracy for this analysis. ALGORITHM 1: Establishing Tree Node Relationships Supported by the Floor Input: Foreground object parsing from the scene image = {ğ¼mğ‘– } and corresponding oriented bounding boxes obbmğ‘– ; Result: tree representing relationships based on floor support. Queue = {} ; // Identify floor-supported subnodes for ğ¼m do // Determined by GPT-4o prompt if is supported by the floor then AddLeafNode(T, floor, ğ¼m); (ğ¼m ) [ğ‘‘ vertical ] 0 Queue.insert(ğ¼m); end end // Recursively constructing the support relationship tree while !Queue.empty() do ğ¼m Queue.pop() ; for ğ¼mğ‘› do if ğ‘‘ (mğ‘›, m) < ğœ– then ğ‘†mğ‘› m, ğ‘‘ vertical algorithm 2. if ğ‘†mğ‘› then AddLeafNode(T, ğ¼m, ğ¼mğ‘› ); (ğ¼m ) [ğ‘‘ vertical ] ğ‘‘ vertical mğ‘› Queue.insert(ğ¼mğ‘› ); mğ‘› as analyzed by supported relationship end end end end Step 2: Analysis of Ceiling-Supported Objects. We apply GPT-4os prompting mechanism to identify objects supported by the ceiling, creating set of ceiling-supported objects {ğ¼mğ‘– ğ‘– C}. These objects typically exhibit singular logical relationships in our experiments. Step 3: Analysis of Objects Against Structural Elements. We use GPT-4o to determine which objects contact walls, yielding set Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 15 ALGORITHM 2: Determine whether ğ¼mğ‘ is supported by ğ¼mğ‘ based on the content of the image ğ¼ . Input: Mask images ğ¼mğ‘ and ğ¼mğ‘ , along with their corresponding oriented bounding boxes (OBBs) obbmğ‘ and obbmğ‘ .; Result: Support relationship between ğ¼mğ‘ and ğ¼mğ‘ : ğ‘†mğ‘ mğ‘ ; the relative vertical distance between obbmğ‘ and obbmğ‘ : ğ‘‘ vertical mğ‘ mğ‘ ; // Analyze the supporting relationship. if ğ‘§ (obbmğ‘ )max ğ‘§ (obbmğ‘ )min < ğœ– then ğ‘†mğ‘ mğ‘ true; ğ‘‘ vertical mğ‘ mğ‘ 0; return ğ‘†mğ‘ mğ‘ , ğ‘‘ vertical mğ‘ mğ‘ end // Check if the internal relationship is satisfied. if obbmğ‘ obbmğ‘ then ğ‘†mğ‘ mğ‘ true; ğ‘‘ vertical mğ‘ mğ‘ ; // (obbm )â„ is the vertical height of the obbm return ğ‘†mğ‘ mğ‘ (ğ‘§ (obbmğ‘ ) max+ğ‘§ (obbmğ‘ ) min )/2ğ‘§ (obbmğ‘ ) min (obbmğ‘ )â„ , ğ‘‘ vertical mğ‘ mğ‘ end // Handle cases of excessive occlusion, analyzed based on GPT-4o prompts. if obbmğ‘ is supported by obbmğ‘ as determined by GPT-4o then ğ‘†mğ‘ mğ‘ true; ğ‘‘ vertical mğ‘ mğ‘ 0; return ğ‘†mğ‘ mğ‘ , ğ‘‘ vertical mğ‘ mğ‘ end ğ‘†mğ‘ mğ‘ false; ğ‘‘ vertical mğ‘ mğ‘ 0; return ğ‘†mğ‘ mğ‘ , ğ‘‘ vertical mğ‘ mğ‘ {mğ‘– ğ‘– W}. We then analyze the distance from each objects oriented bounding box obbmğ‘– to specific structural planes using ğ‘‘ (obbmğ‘– , (ğ‘›ğ‘¤, ğ‘¡ ğ‘¤)), resulting in sets of objects against specific walls {mğ‘– ğ‘– Wallğ‘¤, ğ‘¤ ğ‘Štotal}, where ğ‘Štotal denotes all walls. For regions without clear logical relationships (set {ğ‘†q}), we exclude these areas to enhance scene layout controllability, updating the foreground region to ğ‘†fg = ğ‘†fg ğ‘†q. Refinement of OBBs. Occlusions between objects result in incomplete depth maps from DepthAnything-V2. As shown in Fig. 4.(a), the cabinet obscured by the table has an inaccurate depth-derived OBB. Using the floors simple structure as reference, we correct foreground object OBBs based on scene graph relationships. For floor-supported objects like ğ¼mğ‘ , we ensure their OBBs maintain perpendicular relationships with the floor plane (ğ‘›ğ‘“ , ğ‘¡ ğ‘“ ) and extend them to make proper contactas illustrated by the cabinets corrected OBB in Fig. 4.(b). This approach significantly improves spatial accuracy in the final layout. 3D Asset Retrieval. For each mask region ğ¼mğ‘– in the scene A.1.2 image, our goal is to identify the most suitable 3D asset objmğ‘– from the predefined asset library ğ´. Specifically, for given mask ğ¼mğ‘– and its associated category categ ğ‘– , we first utilize the inverse mapping 1 of the category merge map related to the 3D assets (see Sec. 3.2) to obtain the relevant set of categories in ğ´, denoted as ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 16 Zhu, Huang, et al. mğ‘– } = 1 (categ {assetğ´ ğ‘– ). Subsequently, we match the most similar 3D asset within the subset {assetğ´ mğ‘– } of the 3D asset library. In particular, we define the similarity between the mask ğ¼mğ‘– and the rendered images of the assets based on the semantic similarity of visual features, which in turn informs the similarity between the mask and the assets. Furthermore, inspired by HOLODECK, we introduce an absolute size difference to adjust the matching similarity, aiming to address challenging scenarios, such as those involving significant occlusion (e.g., bedside table obstructed by bed). ğ´ match(asset mğ‘– , ğ¼mğ‘– ) = ğ´ simcls (R (asset mğ‘– Î”(ğ‘†) = (cid:205)ğ‘£ ğ‘‰ simcls (ğ¼mğ‘– , (assetğ´ mğ‘– , ğ‘£)) Num(ğ‘‰ ) , ğ‘£), ğ¼mğ‘– ) = cos(cid:10)ğ¹ğ· (R (asset ğ‘™ ğ‘™mğ‘– assetğ´ assetğ´ mğ‘– mğ‘– â„mğ‘– â„ + ğ‘¤ â„ assetğ´ mğ‘– assetğ´ mğ‘– ğ´ mğ‘– , ğ‘£)), ğ¹ğ· (ğ¼mğ‘– )(cid:11) , ğ‘¤mğ‘– â„mğ‘– . ğ›¼Î”ğ‘†, (6) Here, simcls () denotes the cosine similarity computed between two high-dimensional feature vectors. The feature map ğ¹ğ· () represents the last hidden layer features extracted by the original DINOv2 model. ğ¼mğ‘– is the image corresponding to the mask, and (assetğ´ , ğ‘£) is the rendered image of the asset assetğ´ mğ‘– , obtained mğ‘– from specific viewpoint ğ‘£ using the camera intrinsic parameters ğ¾. The viewpoint ğ‘£ corresponds to an extrinsic parameter matrix [ğ‘…ğ‘£ ğ‘¡ ğ‘£]. We uniformly sampled 20 viewpoints along the central axis of the wrapped regular dodecahedron of the asset, denoted as ğ‘‰ . The term Î”ğ‘† represents the average absolute difference between the estimated dimensions and the actual dimensions of the model. The , ğ‘¤ parameters ğ‘™ correspond to the length, assetğ´ mğ‘– width, and height of the asset assetğ´ mğ‘– , respectively. In contrast, ğ‘™mğ‘– , ğ‘¤mğ‘– , and â„mğ‘– represent the length, width, and height outputs generated by GPT-4o for ğ¼mğ‘– through prompt. In our experiments, we set the parameter ğ›¼ to 0.1. , and â„ assetğ´ mğ‘– assetğ´ mğ‘– Scale Transformation. In the task of 3D scene layout, designA.1.3 ers adjust the scale and proportions of asset models according to the specific requirements of the current scene. On one hand, it is crucial to ensure that the dimensions of the asset models align with the overall layout design after placement; on the other hand, the \"unique design\" characteristics of the original assets must be preserved. For instance, TV cabinet can be scaled in all three dimensions of its oriented bounding box (OBB), while floor lamp is typically scaled only in the vertical direction, with the horizontal dimensions maintaining proportional scaling. This differentiated scaling approach for various assets not only takes the global layout into significant consideration but also preserves the inherent design characteristics of each asset. We primarily optimize the scale transformation based on the OBB of the objects, as illustrated in Eq. 7. ğ‘ best = arg max ğ‘  V(obbmğ‘– obbobjmğ‘– (ğ‘  ) ) V(obbmğ‘– obbobjmğ‘– (ğ‘  ) ) (7) where V() denotes the operator that computes the volume of geometric body, obbmğ‘– is the oriented bounding box corresponding to the mask image ğ¼mğ‘– in the scene image, and obbobjmğ‘– (ğ‘ ) corresponds to the oriented bounding box of the retrieved asset objmğ‘– with the scale variable ğ‘ . ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. The optimization strategy for the scale transformation ğ‘  of obbobjmğ‘– is primarily informed by the habitual layout practices of professional artists, and it analyzes the following three scenarios: (ğ‘ ) (1) The scale transformation ğ‘  has two degrees of freedom. Objects maintain their original length-to-width ratio while height can be adjusted independently. This mode is ideal for items where height modification doesnt affect aesthetic quality, such as decorative objects and lighting fixtures. (2) The scale transformation ğ‘  has two degrees of freedom. Objects are scaled along their two longest oriented bounding box axes, with the third dimension scaled proportionally based on the average of the other dimensions. This approach works well for slender objects like picture frames, wooden boards, and curtains. (3) The scale transformation ğ‘  has three degrees of freedom. Objects can be freely scaled in all dimensionslength, width, and height. This mode is appropriate for furniture pieces like tables, cabinets, and beds that can be proportionally adjusted in any direction. Based on these scenarios, we classified the assets and derived the scale transformation for the foreground objects based on Eq.7. A.1.4 Global Translation Optimization. To analyze the solution of Eq. 5, we divide the solving process into two distinct steps: Step 1: Hard Constraint Processing. To ensure that the solution adheres to certain hard constraints, we perform preliminary processing of the support and wall constraints. Specifically, for the support constraints, we utilize the support tree established in Sec. 3.2. Starting from the root node, we sequentially update the ğ‘§ values of the child node objects according to the support constraints, ensuring that these ğ‘§ values are not optimized in subsequent stages. For the object objmğ‘– that is adjacent to the wall object objğ‘¤, we only need to move objmğ‘– along the direction of the normal vector ğ‘›ğ‘¤ to position that is in close contact with the wall object objğ‘¤. Furthermore, in the subsequent optimization, the position of objmğ‘– will not be adjusted along the direction of ğ‘›ğ‘¤. Step 2: Nonlinear Optimization. The variables corresponding to the support and wall constraints in Eq. 5 have changed as result of the updates from the first step. For instance, the ğ‘§ values of objects that satisfy the support relationships are no longer subject to optimization. If objmğ‘– is adjacent to the wall object objğ‘¤, then the change of objmğ‘– in the direction of ğ‘›ğ‘¤ is zero. However, the remaining problem still constitutes highly nonlinear optimization challenge, which we address using simulated annealing algorithm. Additionally, to accelerate the convergence of the solution, we employ voxel representation of the objects as proxy for the polygon mesh, significantly reducing the computational complexity associated with intersection calculations. Scene Layout Similarity Function. To quantitatively evaluate A.1.5 layout similarities between two scenes, we implemented scene-toscene similarity function following prior works. For each generated scene, we first applied our scene image analysis pipeline (Section 3.2) to obtain semantic segmentation results, point cloud oriented bounding boxes (OBB), and the center and plane equations of walls and floors. With this information, we project all objects onto the floor plane, aligning the scene orientation with the wall direction to standardize the coordinate system (aligning with either the x-axis or y-axis). We then construct grid with each cell measuring 0.1m 0.1m. For each grid point, we record the class of the furniture item present, or none if empty. To enable direct comparison, we pad both scenes to the same dimensions before calculation. The similarity between ground-truth room and generated sample is calculated as the fraction of grid points with matching classes. To ensure comprehensive comparison, we allow for rotations of 90, 180, and 270 degrees, as well as mirroring transformations, to identify the maximum layout similarity between the compared scenes. Fig. 13 shows several example scenes. A.1.6 More Details in Physical Constraints. Table 9 summarizes the simulation parameters used in Blender for physics simulations. A.1.7 Additional Visualization of Intermediate Results. To offer comprehensive visual overview of our method, we present visualizations from different stages of our pipeline. Fig. 2 illustrates the overall workflow, showcasing the initial guide image and the layout results before and after the final layout refinement. Fig. 4 details the constructed scene graph. Complementing these, Fig. 14 provides granular breakdown of the intermediate algorithmic details, demonstrating the process from scene parsing and asset retrieval to the final rotation estimation for individual objects. A.2 Prompts A.2.1 Complete scene generation prompts. The following are the complete text prompts used to generate the scenes shown in Fig. 1: 1. vibrant florist shop filled with diverse potted plants and wooden display shelf showcasing vibrant greenery . 2. modern - shaped kitchen with walnut wood cabinets and white marble countertops , featuring kitchen island with three wooden bar stools , white microwave , and decorative potted plants . 3. cozy living room featuring comfortable armchairs , gallery wall , and stylish coffee table . 4. An industrial storage space with pallets , barrels , and various industrial equipment . 5. minimalist living room with abstract art , white sofas , and floor lamp , emphasizing simplicity and elegance . 6. modern conference room with large oval table , ergonomic chairs , and wall - mounted display . 7. warm dining room with chandelier , modern table , and decorative shelving for cozy dining experience . 8. An entertainment room with pool table and arcade machines . Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 17 Analyze the given image of an indoor or outdoor scene in structured , hierarchical manner , adhering strictly to predefined list of objects . Provide the results in JSON format with the following steps : 1. ** Identify ALL distinct areas or zones ** in the scene , no matter how small or seemingly insignificant . Include transitional spaces , corners , and any visible partial areas . 2. ** For EACH identified area , detect and list EVERY visible object ** , focusing solely on parent object names and their associated child object names , ** WITHOUT mentioning their locations or other relationships **. Use the specified list of categories , referred to as predefined_objs_list : { predefined_eng_categories_list }. a. Large elements (e.g., furniture , major appliances , architectural features ) as parent objects b. Medium - sized objects (e.g., decorations , electronics , LCD_TV ) as parent or child objects c. Small items (e.g., accessories , utensils , personal items ) primarily as child objects ** Important Note :** Every identified object must be named according to the predefined_objs_list . Objects not fitting predefined categories should be matched with the closest available category . 3. ** Ensure absolute thoroughness ** in your analysis . Capture every detail visible in the image , from the largest architectural elements to the smallest discernible objects . Represent objects without child elements as an empty array . Unassigned objects should be listed as their own parent object with an empty array . ** Structure your response ** as tree - structured JSON object with three levels : areas - parent objects - child objects . Each identified area should be top - level key , with its value being an object containing parent objects as keys and arrays of their associated child objects as values . Example structure : { \" area1 \": { \" parent_object1 \": [\" child_object1 \" , \" child_object2 \"], \" parent_object2 \": [] }, \" area2 \": { 9. musician 's bedroom with wooden bed , desk , \" parent_object3 \": [\" child_object3 \"] guitar and bookshelf . } } A.2.2 Prompt for Object Extraction. The following prompt is designed to extract all objects within scene using Chain-of-Thought (CoT) approach. The output is formatted as JSON object list. ** SCENE OBJECT EXTRACTION PROMPT :** A.2.3 Prompt for Scene Layout Analysis. The following prompt is specifically crafted for analyzing the structural dependency relationships among objects in structured and hierarchical manner. The analysis follows stringent guidelines and produces results in JSON format. ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 18 Zhu, Huang, et al. Table 9. Summary of simulation parameters used in Blender for physics simulation."
        },
        {
            "title": "Simulator Parameters",
            "content": "scene.frame_start scene.frame_end scene.gravity 1 200 (0,0,-9.81) scene.rigidbody_world.solver_iterations scene.rigidbody_world.substeps_per_frame 3 3 obj.rigid_body.mass obj.rigid_body.friction obj.rigid_body.restitution obj.rigid_body.linear_damping 10 10"
        },
        {
            "title": "Rigid Body Simulation Parameters",
            "content": "obj.rigid_body.collision_shape obj.modifiers modifier.decimate_type modifier.angle_limit obj.rigid_body.collision_margin obj.rigid_body.use_deform MESH Decimate-DECIMATE DISSOLVE 15 degrees 0.001 TRUE Fig. 13. Example scene visualizations with 3D layout and 2D visual similarity comparisons. The first column shows generated scene images. The second column displays their corresponding grid-based layout visualizations, with each color representing different furniture category. The third column presents the nearest neighbors based on 3D layout similarity, and the fourth column shows the nearest visual (2D) neighbors from the training set. ** GENERATE SCENE GRAPH PROMPT :** Task Overview :"
        },
        {
            "title": "Create a scene graph for the objects identified in",
            "content": "pic_1 < image - placeholder >, ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 19 Fig. 14. Additional visualization of key intermediate steps. The process begins with comprehensive scene analysis where (a) objects are detected via grounding-dino-1.5 and SAM, guided by categories parsed by GPT-4o, and (b) segmentation masks are generated. Concurrently, (c) RANSAC is employed to fit orthogonal floor and wall planes (ceiling as floors opposite normal), establishing robust geometric frame for the scene. For each segmented object, we (d) retrieve the top-2 candidate assets from our library based on semantic category, visual similarity, and size compatibility. Our rotation estimation module then combines (e) strong initial candidate from visual-semantic feature matching with (f) constraints from Oriented Bounding Boxes (OBBs), which are geometrically corrected using scene graph logic. This fusion results in (g) the final pose, high-quality input for the subsequent scene layout refinement stage. specifically those within the designated region , referred to as ** items_in_region **: { items_in_region }. Reference Images : pic_2 < image - placeholder >: The complete scene image , listing all objects , is referred to as ** all_items_list **: { all_items_list }. { wall_color_name } Object Attributes : For each object in pic_1 , populate the following attributes : 1. isAgainstWall : Determine if the object is directly against wall , specifically with its back touching the wall . This means the object is placed in such way that its rear surface is aligned with or adjacent to the wall . If it is , set this to true ; otherwise , set it to false . 2. isOnFloor : Determine if the object is directly on the floor . This means the base of the object is resting on the ground surface without any elevation . If it is , set this to true ; otherwise , set it to false . 3. isHangingFromCeiling : Determine if the object is hanging from the ceiling . This implies the object is suspended from above , without any support from below . If it is , set this to true ; otherwise , set it to false . 4. isHangingOnWall : Determine if the object is hanging on the wall . This indicates the object is affixed to the wall , typically using hooks or nails , without resting on any horizontal surface . If it is , set this to true ; otherwise , set it to false . Follow the steps below to complete the task : ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Zhu, Huang, et al. step1 : Identify the object 's isAgainstWall attribute and give the reason . step2 : Identify the object 's isOnFloor attribute and give the reason . step3 : Identify the object 's isHangingFromCeiling attribute and give the reason . step4 : Identify the object 's isHangingOnWall attribute and give the reason . step5 : Output the result in the following format : Example Format : {{ \" bed_0 \": {{ \" isAgainstWall \": true , \" isOnFloor \": true , \" isHangingFromCeiling \": false , \" isHangingOnWall \": false , }} , \" TV_0 \": {{ \" isAgainstWall \": true , \" isOnFloor \": false , \" isHangingFromCeiling \": false , \" isHangingOnWall \": false , }} \" chandelier_0 \": {{ \" isAgainstWall \": false , \" isOnFloor \": false , \" isHangingFromCeiling \": true , \" isHangingOnWall \": false , }} , ... }} Remember , any object not listed in ** items_in_region ** ({ items_in_region }) should not be included in the scene graph generation process . A.2.4 Prompt for GPT4 evalutation. Below are the prompts used in expert evaluation and layout similarity assessment experiments. We utilized the GPT-4 model and set the temperature to 0. - Lighting quality and atmosphere - Design style consistency Please analyze the image carefully and return your evaluation in the following JSON format : { } \" composition_score \": , \" semantic_score \": , \" aesthetic_score \": , \" brief_comments \": \"A very brief overall assessment in one sentence \" ** Layout Similarity between Rendered Images and Guide Images :** Compare these two images in terms of layout and composition only . The first image is rendered result , and the second is the guide / reference image . Please evaluate how well the rendered image matches the guide image 's layout and composition , focusing ONLY on : - Spatial arrangement of furniture and objects - Overall composition and layout matching - Positioning and scale of major elements - Room structure and proportions Ignore texture , materials , colors , and detailed decorations . Rate the layout matching on scale of 1 -10 ( where 1 means completely different layout and 10 means perfect layout match ). Return your evaluation in this JSON format : { \" layout_score \": , \" comments \": \" Brief explanation of the score focusing only on layout similarities / differences \" ** Expert Evaluation :** As professional interior design and architecture } expert , please evaluate this scene image in three dimensions ( score 1 -5 , where 1 is poor and 5 is excellent , 3 represents the average level of professional human practitioners ): 1. Composition (1 -5) : - Balance and distribution of elements - Use of space and proportions - Visual hierarchy and focal points - Alignment and grid structure - Overall spatial organization 2. Semantic Logic (1 -5) : - Functional arrangement of furniture and objects - Practical usability of the space - Logical flow and circulation - Appropriate spacing between elements - Realistic placement of objects 3. Aesthetic Appeal (1 -5) : - Overall visual harmony - Color coordination and contrast - Material and texture combinations ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. A.3 Dataset Details Our dataset addresses several key limitations of existing 3D scene layout resources, significantly enhancing both quality and diversity. As illustrated in Fig. 15, we present variety of high-quality, handcrafted 3D scenes that showcase diverse room functions. Fig. 16 showcases examples of our diverse assets, with the left side displaying representative high-quality models and the right side presenting bar chart that illustrates the distribution across various categories. For common items, we offer multiple variants to capture different styles. Additionally, as shown in Fig. 17, we provide statistical analysis of the number and distribution of object types in sample of scenes. A.4 Analysis of Failure Cases The failure cases illustrated in Fig. 18 highlight two core challenges. semantic-structural mismatch can occur when the image generator produces objects with novel topologies not present in our finite asset library (top row). This leads to incorrect asset retrieval, Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 21 Fig. 15. More 3D scenes from our high-quality, handcrafted dataset. These scenes showcase diverse range of room functions and include both indoor and outdoor assets, illustrating the variety and detail of our manual scene construction. which in turn invalidates downstream geometric and relational constraints derived from the scene graph. Furthermore, pose ambiguity from severe occlusion remains key limitation (bottom row). As an inherently ill-posed problem, the partial view from an occluded object provides ambiguous visual features for our matching module, leading to an unreliable initial pose estimate that subsequent optimization stages may fail to correct. A.5 More Qualitative Results To further demonstrate the ability of our algorithm to generate diverse 3D scene layouts, we present additional 3D scenes produced by our method in Fig. 19. ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 22 Zhu, Huang, et al. Fig. 16. Dataset asset overview. Left: examples of asset classes such as backrest chairs and TV cabinets. Right: bar chart showing the number of assets per class, highlighting the most common categories. ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation 23 Fig. 17. Statistics of our high-quality, manually arranged dataset, encompassing 21 different scene types. The chart illustrates the number of scenes, total objects, average objects per scene, and class count for each scene type, highlighting the datasets diversity and complexity. Fig. 18. Analysis of Failure Cases. This figure illustrates two primary limitations of our method. Top Row: Discrepancy between Generated Content and Asset Library. (a) The image generator creates an object with novel topologya hybrid of wardrobe and bookshelf. (b) Our system retrieves the closest semantic match from the asset library, standard wardrobe, which lacks the open shelves depicted. This semantic-structural mismatch prevents the correct placement of child objects (e.g., books), leading to layout inconsistencies. Bottom Row: Pose Estimation Ambiguity from Severe Occlusion. (c) An object, correctly identified as chair, is heavily occluded, revealing only its backrest. (d) While feature matching can be performed on this partial view, the limited information introduces ambiguity, as multiple poses could yield similar appearance, potentially leading to inaccurate rotation estimation. ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. 24 Zhu, Huang, et al. ACM Trans. Graph., Vol. 44, No. 6, Article . Publication date: December 2025. Fig. 19. Additional 3D generated scene layouts by our system."
        }
    ],
    "affiliations": [
        "Southeast University, China",
        "Tencent, China",
        "Tsinghua University, China",
        "University of Science and Technology of China, China"
    ]
}