{
    "paper_title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
    "authors": [
        "Yehonatan Elisha",
        "Seffi Cohen",
        "Oren Barkan",
        "Noam Koenigstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods. We address this gap by introducing the Reference-Frame $\\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations. Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations. Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets. By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry."
        },
        {
            "title": "Start",
            "content": "Rethinking Saliency Maps: Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations Yehonatan Elisha1, Seffi Cohen2, Oren Barkan3, Noam Koenigstein1 1Tel Aviv University 2Harvard University 3The Open University 5 2 0 2 8 1 ] . [ 2 1 8 0 3 1 . 1 1 5 2 : r Abstract Saliency maps have become cornerstone of visual explanation in deep learning, yet there remains no consensus on their intended purpose and their alignment with specific user queries. This fundamental ambiguity undermines both the evaluation and practical utility of explanation methods. In this paper, we introduce the Reference-FrameGranularity (RFxG) taxonomya principled framework that addresses this ambiguity by conceptualizing saliency explanations along two essential axes: the reference-frame axis (distinguishing between pointwise Why Husky? and contrastive Why Husky and not Shih-tzu? explanations) and the granularity axis (ranging from fine-grained class-level to coarsegrained group-level interpretations, e.g., Why Husky? vs. Why Dog?). Through this lens, we identify critical limitations in existing evaluation metrics, which predominantly focus on pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To address these gaps, we propose four novel faithfulness metrics that systematically assess explanation quality across both RFxG dimensions. Our comprehensive evaluation framework spans ten state-of-theart methods, 4 model architectures, and 3 datasets. By suggesting shift from model-centric to user-intent-driven evaluation, our work provides both the conceptual foundation and practical tools necessary for developing explanations that are not only faithful to model behavior but also meaningfully aligned with human understanding. Code https://github.com/yonisGit/RFxG"
        },
        {
            "title": "Introduction",
            "content": "The increasing adoption of deep learning systems in highstakes applications has amplified the need for transparency and interpretability. As deep models grow in complexity, so does the demand for methods that can explain their predictions in ways that are intelligible and actionable to human users. This growing field, known as Explainable AI (XAI), aims to bridge the gap between black-box models and human understanding. Across domains, numerous works have advanced the study of XAI (Sundararajan, Taly, and Yan 2017a; Smilkov et al. 2017; Chefer, Gur, and Wolf 2021b; Barkan et al. 2020, 2021b, 2024a,b,c, 2025). In computer vision, XAI Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Our RFxG Explanation Axes. Note that there are also explanations between the points. For example: classgroup contrastive questions like Why Husky and not other Dogs? efforts have primarily focused on visual explanation techniques, with saliency maps being one of the most dominant paradigms (Selvaraju et al. 2017; Chefer, Gur, and Wolf 2021b; Barkan et al. 2021a, 2023c,a; Haddad et al. 2025). In parallel, contrastive explanation techniques (Dhurandhar et al. 2018; Xie et al. 2023; Wang et al. 2023) have emerged, aiming to answer comparative questions such as Why class rather than class B?, rather than simply Why class A?. Despite their popularity, the interpretability and trustworthiness of saliency maps have been repeatedly questioned. Critiques in the literature have pointed out that many explanation methods may be misleading or unreliable, offering visual artifacts rather than faithful representations of model behavior (Adebayo et al. 2018; Kindermans et al. 2017; Longo et al. 2024). More broadly, Lipton (Lipton 2018) argued that the goals of interpretability are often underspecified, leading to proliferation of methods that lack clear understanding of what constitutes good explanation. These concerns point to deeper, systemic challenges in how saliency explanations are conceived, evaluated, and aligned with user needs: Challenge 1 Lack of Support for User-Driven Explanatory Questions. Users rarely ask only Why this Our contributions are as follows: First, we introduce novel taxonomy of saliency explanations based on reference-frame and granularity, offering conceptual structure that reflects diverse user queries. Second, we identify critical shortcomings in existing evaluation metrics, which fail to distinguish among explanation types along these axes. Third, We propose four new faithfulness metrics that assess explanation quality across both axes, grounded in structured perturbation and score comparison. Fourth, we contribute new group-level labeling for ImageNet classes, derived from the WordNet hierarchy. This semantic grouping enables evaluation at the group granularity level. Finally, We present comprehensive evaluation framework, including experiments across ten saliency methods, four model architectures, three datasets. The results provide new insights and outline foundational steps toward user-aligned, semantically meaningful explanations. By grounding saliency evaluation in principled, useraligned framework, our work advances the interpretability of AI models and enables systematic, semantically coherent comparison of explanation methods."
        },
        {
            "title": "2.1 Saliency-Based Explanation Methods",
            "content": "Saliency methods form the backbone of visual explanations in computer vision, aiming to highlight the most influential regions of an input image for models decision. Activationbased methods (Erhan et al. 2009) use the feature-maps obtained by forward propagation in order to interpret the output prediction. Perturbation-based methods (Fong, Patrick, and Vedaldi 2019; Fong and Vedaldi 2017) measures the outputs sensitivity w.r.t. the input using random perturbations applied in the input space. Path integration methods, such as Integrated Gradients (Sundararajan, Taly, and Yan 2017b) integrates over the interpolated image gradients. Score-CAM (Wang et al. 2020), gradient-free method, generates class-specific importance maps by using activation maps as masks and measuring their effect on the target class score. With the rise of transformer-based architectures in vision, specialized explanation techniques have emerged. The rollout method (Abnar and Zuidema 2020) aggregates attention weights across layers to identify influential token relationships, while Transformer Attribution (Chefer, Gur, and Wolf 2021b) presents class-specific Deep Taylor Decomposition method in which relevance propagation is applied for positive and negative attributions. More recently, the authors introduced Generic Attention Explainability (Chefer, Gur, and Wolf 2021a), generalization of Transformer Attribution for explaining Bi-Modal transformers. Iterated Integrated Attributions (Barkan et al. 2023b) generalizes conventional path-integral methods by performing an iterated n-fold integration across multiple network layers (beyond the input) thereby measuring the n-dimensional flux through higher-order hypersurfaces. IIA quantifies how the gradient field propagates across an n-dimensional volume in the joint space of all participating network layers, including the input, and was shown to produce state-of-the-art results both for CNNs and ViTs. Recently, contrastive methods have been Figure 2: Saliency maps for sport car using different types of questions that reflect our taxonomy: (a) Pointwise class. (b) Contrastive between two classes. (c) Contrastive between class and group. class? Instead, they often seek richer, comparative justificationssuch as Why sports car and not convertible? or What distinguishes this husky from other dog breeds?that most conventional saliency methods are not designed to support. Challenge 2 Interpretation Ambiguity from Missing Reference Frame and Granularity. Without explicit context about the explanations reference frame (e.g., pointwise vs. contrastive) or semantic granularity (e.g., finegrained class vs. coarse-grained group), users may misinterpret saliency maps (Vilone and Longo 2021). map highlighting fur texture might be meaningful for distinguishing huskies from shih-tzus but irrelevant when explaining why the image is dog rather than cat. This ambiguity undermines trust, interpretability, and ultimately the usability of saliency-based methods. Challenge 3 Inadequate Evaluation of Contrastive and Granularity-Varying Explanations. Existing evaluation metrics focus almost exclusively on pointwise faithfulnessassessing how salient pixels affect the score of single target class. They fail to capture whether an explanation meaningfully distinguishes between classes or operates coherently at different levels of semantic abstraction (e.g., vehicle vs. sports car). As result, current benchmarks cannot validate whether an explanation aligns with the users intended question. These limitations highlight the importance of shifting attention toward the users perspective. What does user actually want to know when they inspect saliency map? Are they looking for reasons why an image was classified as husky, or why it was classified as husky rather than shih-tzu? Do they care about fine-grained details or coarse-grained features? These kinds of questions suggest that saliency explanations should be considered along two important dimensions: the reference-frame axis (pointwise vs. contrastive explanations) and the granularity axis (fine-grained vs. coarse-grained classes). Figure 1 depicts the proposed axes space, while Figure 2 presents representative saliency maps that address distinct explanatory questions situated within this space. introduced to provide comparative explanations, such as explaining why an image was classified as class rather than class (Ghorbani et al. 2019; Xie et al. 2023; Wang et al. 2023). Although prior work implicitly spans multiple explanatory styles, there is lack of formal framework to classify and evaluate these methods systematically. We address this gap through principled taxonomy aligned with user-centric explanation goals."
        },
        {
            "title": "2.2 Evaluation Metrics for Saliency Maps\nFaithfulness, a cornerstone concept in explanation evalu-\nation, refers to the degree to which an explanation accu-\nrately reflects the actual decision-making process of the\nmodel (Rudin 2019). A faithful explanation should high-\nlight features that genuinely influence the model’s prediction\nrather than artifacts of the explanation method itself. Nu-\nmerous metrics have been proposed to assess saliency meth-\nods. Faithfulness-based metrics such as Insertion and Dele-\ntion (Petsiuk, Das, and Saenko 2018), AOPC (Samek et al.\n2017), and MoRF/LeRF curves (Samek and M¨uller 2019)\nmeasure the impact of removing or adding salient pixels\non the model’s prediction. Localization metrics such as the\nPointing Game (Zhang et al. 2018) or bounding-box over-\nlap (Zhou et al. 2016) evaluate alignment between saliency\nmaps and annotated objects. Despite their utility, most of\nthese metrics assume a pointwise, model-centric view and\nfail to address contrastive reasoning or semantic granular-\nity. For example, Insertion and Deletion evaluate influence\nof individual features but do not measure whether an expla-\nnation distinguishes class A from a similar class B. Human-\ncentered evaluations, such as trustworthiness, satisfaction, or\nfaithfulness to user expectations, have been studied via user\nstudies (Doshi-Velez and Kim 2017; Kim et al. 2022), but\nthese are costly and often task-specific. Robustness checks\nsuch as model randomization tests (Adebayo et al. 2018)\nand input perturbation sensitivity (Ghorbani, Abid, and Zou\n2019) assess the stability or reliability of explanations.",
            "content": "Limitations of Existing Contrastive Metrics. Recent works have proposed contrastive evaluation metrics (Wang et al. 2023; Xie et al. 2023) aimed at quantifying how well saliency maps explain model preferences between classes. While these metrics mark progress, they fail to capture key dimensions necessary for user-oriented evaluationparticularly regarding reference-frame alignment and semantic granularity, and correct faithfulness evaluation under structured perturbations. CDS (Class Deviation Score) measures prediction shifts after masking but lacks true contrastive framing. It evaluates only the target class A, without considering how feature removal affects or distinguishes specific alternative class B. CAUC (Contrastive AUC) evaluates whether the model remains more confident in class than class throughout the deletion process. While intuitive, the metric suffers from several limitations. It depends on thresholding mechanism that assumes the saliency map contains many zero-valued pixels, an assumption often violated in some of the transformer-based and localized explanation methods, where saliency maps tend to be dense. This can lead to perturbations that affect only small portion of the salient regions, resulting in unstable behavior and poor comparability across maps with different value distributions. Additionally, CAUC multiplies prediction scores during its computation, which compromises interpretability. This design choice penalizes cases where the contrastive class initially receives low score, even if the saliency map effectively suppresses class B, ultimately yielding misleadingly low CAUC values for otherwise informative explanations. CDROP (Contrastive Drop) extends CAUC with sparsity normalization but inherits its core limitations. In contrast, our proposed metrics evaluate saliency maps across the RFxG axes. Moreover, instead of relying on multiplication, we measure contrastiveness directly through probability differences, ensuring greater robustness and interpretability. Finally, our metrics avoids unstable thresholding mechanisms."
        },
        {
            "title": "2.3 Critiques of XAI and Motivation for RFxG",
            "content": "A recurring concern in the XAI literature is that the very notion of an explanation remains poorly defined. (Lipton 2018) argues that interpretability is often invoked without specifying its purpose, leading to multitude of explanation methods that address different, and sometimes conflicting, goals. This ambiguity results in an evaluation landscape that lacks coherence: it is unclear whether an explanation should be faithful to the models internals, useful to the user, or aligned with human reasoning. Several works have attempted to address this fragmentation through taxonomic frameworks. (Guidotti et al. 2018) proposed comprehensive taxonomy distinguishing between explanation methods based on their scope (global vs. local), model type, and transparency level. (Sokol and Flach 2020) further refined this by introducing the concept of explanation profiles that account for diverse user needs and contexts. (Doshi-Velez and Kim 2017) also echo this concern, advocating for rigorous science of interpretability grounded in clearly articulated desiderata. They propose taxonomybased evaluation protocols to distinguish between different types of interpretability, yet most existing saliency methods are not explicitly situated within such frameworks. This lack of clarity in defining explanation goals has practical consequences. As discussed by (Miller 2019), explanations are inherently social and user-dependent. Yet, the majority of saliency research adopts model-centered view, optimizing visualizations without considering the users question or context. Recent works (Gilpin et al. 2018; Bhatt et al. 2020; Zhang, Figueroa, and Hermanns 2025) further highlight that many explanation methods fall short of being actionable or trustworthy because they are not grounded in human-centric model of understanding. These foundational critiques motivate our proposal: saliency explanations must be designed with respect to the specific user question they are meant to answer. We introduce structured perspective based on the axes of reference frame and granularity, called Reference-Frame Granularity (RFxG), enabling principled alignment between explanation methods and user intent. By introducing faithfulness-based metrics that are better suited to this framing, our approach bridges the gap between user expectations and model behavior. Our work aims to move explanations towards better correspondence between user questions and meaningful explanation evaluation for explainable AI."
        },
        {
            "title": "Explanations",
            "content": "As outlined in Section 1, current approaches to saliency maps exhibit several fundamental gaps. We address the first two challenges by proposing the novel Reference-Frame Granularity (RFxG) taxonomy that decomposes attribution methods along two orthogonal axes: reference frame and explanation granularity. These axes create space that helps to clarify design decisions, underlying assumptions, and downstream limitations."
        },
        {
            "title": "3.1 Reference Frame: What Is the Explanation",
            "content": "Relative To? The first axis concerns the reference frame of the explanation, that is, what baseline or alternative hypothesis the explanation is conditioned on. Pointwise Explanations. Pointwise explanations aim to justify prediction by highlighting evidence that supports the predicted class. They answer questions like: Why did the model make this prediction for this input? These explanations highlight regions that support the current class label without considering alternatives. Popular examples include GradCAM (Selvaraju et al. 2017), Guided Backpropagation (Springenberg et al. 2014), and Integrated Gradients (Sundararajan, Taly, and Yan 2017a). Contrastive Explanations. Contrastive explanations are defined relative to an alternative class or hypothesis. They answer questions such as: Why did the model choose class over class B?. This framing is more faithful to how humans generate explanations (Miller 2019) and helps reduce ambiguity by focusing on discriminative evidence (Dhurandhar et al. 2018; Ghorbani et al. 2019). Recent works on contrastive attribution (Dhurandhar et al. 2018; Xie et al. 2023; Wang et al. 2023) explicitly model these differences."
        },
        {
            "title": "3.2 Semantic Granularity: What Level of Class",
            "content": "Detail is Targeted? The second axis addresses the semantic resolution of the explanation, are we trying to explain broad category or narrow one? Group-Level Explanations Group-level methods highlight features that are predictive of superclass or cluster of classes. For example, when classifying an image as husky, group-level explanation might emphasize general dog features. These methods tend to generalize well and provide intuitive reasoning at high level. Some class-agnostic or hierarchy-aware methods (Ghorbani and Zou 2020; Xie et al. 2023) can be interpreted in this way. While prior methods operate implicitly at the group level, they lack unified structured framework that links between explanation granularity and user intent. Our RFxG taxonomy introduces an explicit two-axis framework that distinguishes between class, group, and contrastive explanations, enabling alignment with specific user questions and guiding principled method design and evaluation. Class-Level Explanations. Class-level methods target finegrained distinctions, isolating features that are specific to single class. In the same husky example, class-level explanation would emphasize the distinctive fur pattern or blue eyes, traits that separate huskies from other dogs. This is the dominant paradigm in the literature, as most attribution methods (Selvaraju et al. 2017; Wang et al. 2020; Sundararajan, Taly, and Yan 2017a) are designed for single-class specificity."
        },
        {
            "title": "4 Evaluating User-Perspective Saliency Maps\nIn what follows, we address the third challenge by intro-\nducing four novel faithfulness metrics designed to demon-\nstrate that user-intent-driven explanations faithfully reflect\nthe model’s behavior and decision-making process. In this\nwork, we shift focus toward two underexplored yet essen-\ntial aspects: contrastive metrics and group-level metrics.\nThese better align with the explanatory questions users ac-\ntually ask—such as “Why class A and not B?” or “What fea-\ntures define the entire group of cars?” — rather than merely\n“Why class A?” Our proposed metrics are designed to mea-\nsure explanation quality along the RFxG axes introduced in\nSec. 3. Our proposed metrics focus explicitly on faithful-\nness—the extent to which explanations accurately reflect the\nmodel’s true decision-making process. Faithfulness is a cor-\nnerstone of explainable AI, as it underpins user trust, enables\nthe detection of biases, and supports effective model debug-\nging (Zhang and Chen 2020; Fu et al. 2020). All four met-\nrics are grounded in perturbation-based analysis, a widely\nadopted approach for assessing faithfulness by directly mea-\nsuring how explanations influence model predictions. Let\nf : Rd → RC be a pretrained classifier over C classes,\nand let x ∈ Rd be an input image. Let fc(x) denote the soft-\nmax probability assigned by the classifier f to class c given\ninput x. Let A denote the predicted class and B a contrastive\nclass, while GA and GB denote semantic groups. An expla-\nnation method E produces a saliency map Mc = E(x, c) for\nclass c.",
            "content": "To enable perturbation via masking, we binarize the saliency map to obtain top-α binary mask α {0, 1}d, where 1 indicates the top α fraction of salient pixels (to be suppressed). We then define the perturbed image as xα = (1 α), where denotes element-wise (Hadamard) multiplication, and 1 is the all-ones vector of the same dimension as x. This operation zeros out the most salient regions, simulating their removal while preserving the tensor structure of the input. (1) Perturbation proceeds in 10% steps from α = 0.1 to α = 0.9 (Chefer, Gur, and Wolf 2021b), and the Area Under the Curve (AUC) is used to aggregate results, following established practices in perturbation-based evaluation (Bach et al. 2015; Samek et al. 2017; Petsiuk, Das, and Saenko 2018). AUC provides robust summary of saliency effectiveness over the full perturbation trajectory, and enables meaningful comparisons across methods by capturing both early and cumulative effects of masking. We compute all metric scores using the softmax probabilities rather than raw logits, as probabilities are bounded, interpretable, and better reflect the models actual output behavior. We used black pixel masking (i.e., zeroing out pixels) as the default perturbation mechanism due to its effectiveness and consistency with prior work. However, we also experimented with alternative masking strategies such as Gaussian blur, uniform noise baseline, and all other alternatives suggested in (Sturmfels, Lundberg, and Lee 2020). Across all variants, trends in performance remained consistent. Below we present the four proposed metrics. Contrastive Contrastivity Score (CCS). This metric aims to assess saliency maps for contrastive questions such as Why sports car and not convertible?. Given contrastive saliency map B,A that explains why class is predicted con over class A, CCS quantifies how discriminative the identified regions are between the two classes. It computes the AUC of the prediction gap as salient regions are removed: CCS = AUCα [fA(xα) fB(xα)] , (2) where xα = (1 B,A,α ). high CCS indicates con that removed regions are critical for distinguishing from A, reflecting contrastive faithfulness. In contrast to CAUC, CCS uses differences between probabilities rather than multiplication, which promotes better isolation between the terms. Moreover, it doesnt use unstable thresholds, but uses the well-established perturbation scaling approach used in (Chefer, Gur, and Wolf 2021b). Class Group Contrastivity (CGC). CGC evaluates explanations for queries like Why sports car and not other cars?. Here, the contrastive saliency map A,GA highlights features distinguishing class from others in its semantic group GA. CGC aggregates the drop in fA and rise in confidences of competing group members after masking: con CGC = AUCα (cid:32) (cid:34) 1 2 1 GA (cid:88) (fk(xα) fk(x)) kGA +(fA(x) fA(xα)))] , (3) ). CGC evaluates inter-group where xα = (1 A,GA,α con discriminativeness, essential in fine-grained tasks. Pointwise Group Score (PGS). PGS evaluates explanations for questions such as Why Car?, where the explanation pertains to an entire group. Given pointwise group-level saliency map GA , it computes the average confidence drop pt across the group when salient regions are removed: PGS = AUCα (cid:34) 1 GA (cid:88) (cid:16) kGA fk(x) fk(x (1 GA,α pt )) (cid:35) (cid:17) . (4) PGS thus captures semantic generality rather than specificity, distinguishing it from class-centric faithfulness metrics. Contrastive Group Score (CGS). This metric measures contrastivity between groups, evaluating saliency maps for questions like Why Car and not Truck?. The saliency map GA,GB captures features discriminative between two secon mantic groups. CGS measures whether deletion of those features lowers GA confidence while raising GB: CGS = AUCα (cid:32) (cid:34) 1 2 1 GA (cid:88) kGA (fk(x) fk(xα)) (fj(xα) fj(x)) , (5) + 1 GB (cid:88) jGB where xα = (1 GA,GB ,α suppression and promotion across groups. con ). This metric captures both Together, CCS, CGC, PGS, and CGS form comprehensive, user-aligned suite of evaluation metrics for RFxGs saliency maps. They provide theoretical grounding for contrastive and group-based faithfulnesstwo areas neglected in existing evaluation protocols. These metrics support more nuanced, purpose-driven understanding of visual explanations and complement, rather than replace, standard metrics."
        },
        {
            "title": "5.2 Models and Explanation Methods\nWe evaluated four image classification models: ResNet-50\n(RN) (He et al. 2016), ConvNext-Base (CN) (Liu et al.\n2022), ViT-Base (ViT-B) and ViT-Small (ViT-S) (Doso-\nvitskiy et al. 2020). For CNNs, we applied five widely",
            "content": "1https://wordnet.princeton.edu/ used explanation methods: Grad-CAM (GC) (Selvaraju et al. 2017), Integrated Gradients (IG) (Sundararajan, Taly, and Yan 2017a), Score-CAM (SC) (Wang et al. 2020), SHAP (Lundberg and Lee 2017), and Integrated Iterated Attributions (IIA) (Barkan et al. 2023b). For Transformers, we used: Grad-CAM-ViT (GCV) (Chefer, Gur, and Wolf 2021b), Attention Rollout (Rollout) (Abnar and Zuidema 2020), Generic Attention-model Explainability (GAE) (Chefer, Gur, and Wolf 2021a), Transformer Attribution (TAttr) (Chefer, Gur, and Wolf 2021b), and IIA. Our experiments focus on saliency methods that produce single map per question, and are agnostic to model internals, excluding concept-based and generative explanation techniques. To enable consistent evaluation across the full RFxG taxonomy, we adapted subset of explanation methods - leveraging publicly available implementations where possible (Wang and Wang 2022; Eriksson, Israelsson, and Kallhauge 2025), and applying minimal modifications when necessaryto align with the explanatory goals of each evaluation setting. For instance, we used ContrastiveGradCAM (Wang and Wang 2022) to adapt Grad-CAM for contrastive explanations, and implemented minimally modified version of SHAP, referred to as Contrastive-SHAP, to support RFG-oriented evaluation. All methods were evaluated using their default hyperparameters. Further implementation details are provided in the Appendix."
        },
        {
            "title": "5.4 Quantitative Results\nTable 1 presents results for CNN and ViT models across\nthree datasets using our proposed RFxG metrics. VOC re-\nsults are included in the Appendix due to space constraints.\nTable 2 reports Deletion, CAUC, CDROP, and CDS scores\nfor CNNs across all datasets, provided for comparison with\nRFxG metrics. Our quantitative evaluation, shown in Ta-\nble 1. The results reveal several compelling insights that un-\nderscore the need for explanation methods aligned with con-\ntrastive and semantic granularity axes. Across all datasets\nand model architectures, IIA consistently outperforms the\nother methods, particularly on the contrastive-class (CCS)\nand group-level (PGS, CGS) metrics. This aligns with IIA’s\nunique design that integrates attributions over multiple inter-\nmediate network layers (Barkan et al. 2023b). By leveraging\nmulti-level features at various semantic scales, IIA captures\nboth fine-grained and high-level evidence, an essential thing\nfor explaining subtle class distinctions and inter-group vari-\nability. Furthermore, the more focused and spatially coher-\nent maps produced by IIA allow it to better isolate discrimi-\nnative cues, especially in contrastive settings. Notably, per-",
            "content": "Table 1: Evaluation results across datasets, models, and explanation methods. For every metric in the table, the results were provided using method suitable for the metrics purpose. Dataset Model Method CCS CGC PGS CGS COCO IN CN RN ViT-B ViT-S CN RN ViT-B ViT-S IIA GC SC SHAP IG IIA GC SC SHAP IG IIA TAttr GAE Rollout GCV IIA TAttr GAE Rollout GCV IIA GC SC SHAP IG IIA GC SC SHAP IG IIA TAttr GAE Rollout GCV IIA TAttr GAE Rollout GCV 25.20 22.48 22.21 16.51 13.73 24.98 21.86 21.41 16.10 13.48 19.69 17.81 17.58 14.30 12. 20.01 17.79 17.51 14.23 12.61 25.15 22.72 22.29 16.76 13.57 25.07 22.64 22.10 15.91 13.48 20.12 18.47 18.24 14.66 12.49 19.86 18.45 18.17 14.42 12.68 5.25 4.67 4.49 3.33 3. 5.14 4.63 4.34 3.36 2.89 4.56 4.18 4.07 3.24 2.74 4.50 4.21 4.06 3.25 2.66 5.32 4.58 4.44 3.40 2.61 5.38 4.52 4.40 3.32 2.68 4.51 4.07 3.99 3.15 2. 4.63 4.02 3.94 3.22 2.65 46.57 41.00 41.35 32.81 27.68 46.45 42.36 41.48 33.04 28.33 38.92 36.62 35.99 29.05 27.16 39.50 37.23 36.67 29.70 26.29 46.03 42.70 41.97 32.26 27. 46.17 42.58 41.66 33.16 27.64 39.24 37.21 36.59 30.20 26.81 39.31 35.71 35.14 29.16 26.29 36.38 32.33 31.96 26.61 23.21 35.78 32.30 31.88 25.74 23.45 29.78 27.79 27.39 22.97 20. 29.15 27.55 27.15 22.53 20.95 36.56 33.78 33.05 26.04 23.58 36.42 32.51 31.66 26.90 23.53 29.76 27.13 26.71 22.59 20.63 29.91 27.49 27.10 23.04 20.52 formance is consistently higher on PGS and CGS than on CGC and CCS, across nearly all methods and models. This discrepancy suggests that generating faithful explanations for group-level concepts is easier than for individual class distinctions. The challenge of explaining class within semantically coherent group (as in CGC) is inherently harder, likely due to the shared low and mid-level features between sibling classes. Conversely, contrastive group questions like Why Car and not Truck? (CGS) span broader conceptual boundaries and offer more readily distinguishable patterns for saliency methods to exploit. Interestingly, CCS scores still exceed CGC, implying that contrasting class against specific alternative may be easier than contrasting it against its entire group, possibly due to the dilution of discriminative power across many similar classes. For Transformer-based models, TAttr ranks consistently second after IIA across all metrics. The ability of TAttr to combine attention weights with layer-wise relevance propagation allows it to capture important specific cues. Its improved Table 2: Comparison of explanation methods using Deletion, CAUC (103), CDROP (102), and CDS. Best values are bolded, second-best are underlined. Model Method Deletion CAUC CDROP CDS Deletion CAUC CDROP CDS Deletion CAUC CDROP CDS COCO IN VOC RN CN IIA SC GC IG SHAP IIA SC GC IG SHAP 11.24 14.31 14.87 18.22 20.19 11.53 14.67 14.98 18.51 20.25 3.09 3.14 3.21 3.30 3. 3.12 3.16 3.25 3.33 3.52 7.88 8.36 8.11 7.45 5.63 7.91 8.31 8.10 7.38 5.59 6.29 6.18 6.04 5.92 5.60 6.27 6.13 6.00 5.91 5.54 11.41 14.59 14.73 18.45 20. 11.69 14.78 14.95 18.64 20.31 3.16 3.22 3.29 3.35 3.46 3.18 3.25 3.29 3.37 3.48 7.85 8.40 8.15 7.54 5.63 7.70 8.35 8.11 7.42 5.52 6.24 6.11 6.02 5.95 5. 6.20 6.09 6.08 5.88 5.50 11.35 14.45 14.62 18.33 20.12 11.62 14.72 14.91 18.59 20.28 3.13 3.17 3.22 3.30 3.48 3.15 3.19 3.26 3.35 3.50 7.68 8.32 8.08 7.50 5. 7.83 8.29 8.07 7.41 5.54 6.21 6.12 6.01 5.89 5.52 6.19 6.08 6.00 5.87 5.51 performance compared to methods like Rollout or GCV also demonstrates the value of explicitly modeling gradient flow and relevance. IG underperforms across all metrics, particularly in contrastive settings. As also noted in prior work (Barkan et al. 2023b), IG tends to produce coarse, diffuse maps that highlight many non-discriminative regions. These scattered attributions hinder its ability to isolate features relevant for explaining class uniqueness or group separation, reinforcing the necessity for explanation methods with higher spatial precision and stronger semantic selectivity. Surprisingly, despite its overall superiority, IIA ranks only third on CDROP. This result is inconsistent with its focused and contrastive visual behavior. This suggests that existing contrastive metrics like CDROP may fail to fully reflect the discriminative power of focused maps, particularly when penalizing sparse attributions. This discrepancy supports our motivation for introducing CCS and CGC as more robust and theoretically grounded contrastive metrics that avoid artifacts from density normalization and thresholding, as seen in CAUC and CDROP (Xie et al. 2023). Lastly, we emphasize that the consistently strong results achieved on the PGS and CGS metrics indicate the feasibility of generating high-quality group-based explanations. This capability represents promising direction in XAI research, enabling users to query explanations not only for class-specific decisions but also for semantic abstractions at the group level. Such capability is especially important in domains like medical imaging and autonomous driving, where higher-level categories carry more practical relevance. Overall, our results validate both the necessity and effectiveness of our proposed evaluation framework and RFxG taxonomy. They expose key differences in how explanation methods behave across the axes of reference-frame and granularity, confirming that current pointwise-centric metrics are insufficient."
        },
        {
            "title": "5.5 Qualitative Results",
            "content": "Figure 2 presents saliency maps for sports car input image for the following questions: (a) Why sports car? (Pointwise class) - using the original GC. (b) Why sports car and not convertible? (Contrastive between two classes) - using Class-Contrastive-GradCAM (Wang and Wang 2022). (c) Why sports car and not other cars? (Contrastive between class and group) - using Class-Group-ContrastiveGradCAM. All these saliency maps were provided using the RN model. Implementation details for Class-ContrastiveGradCAM and Class-Group-Contrastive-GradCAM are provided in the Appendix. Finally, additional qualitative results can be found in the Appendix."
        },
        {
            "title": "6 Conclusion\nWe resolve the core ambiguity in saliency map interpre-\ntation by introducing the Reference-Frame × Granularity\n(RFxG) taxonomy, a dual-axis framework differentiating\npointwise versus contrastive explanations and fine- versus\ncoarse-grained semantics. This user-intent-driven perspec-\ntive addresses the fundamental question: “What do saliency\nmaps represent?” We expose critical gaps in current evalua-\ntion metrics, which largely emphasize pointwise faithfulness\nwhile neglecting contrastive reasoning and semantic granu-\nlarity. To address this, we propose four novel metrics assess-\ning explanation quality across both RFxG dimensions. Com-\nprehensive evaluation over ten methods, four architectures,\nand three datasets reveals these limitations and demonstrates\nIIA’s consistent superiority in capturing contrastive evidence\nand semantic groupings. By validating semantic groupings\nand demonstrating practical utility, our framework provides\na rigorous foundation for producing explanations that are\nboth faithful to models and aligned with user needs. Fur-\nther limitations and future directions are discussed in the\nAppendix.",
            "content": "References Abnar, S.; and Zuidema, W. 2020. Quantifying Attention Flow in Transformers. arXiv preprint arXiv:2005.00928. Adebayo, J.; Gilmer, J.; Muelly, M.; Goodfellow, I.; Hardt, M.; and Kim, B. 2018. Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, 9505 9515. Bach, S.; Binder, A.; Montavon, G.; Klauschen, F.; Muller, K.-R.; and Samek, W. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. In PloS one, volume 10, e0130140. Public Library of Science. Barkan, O.; Armstrong, O.; Hertz, A.; Caciularu, A.; Katz, O.; Malkiel, I.; and Koenigstein, N. 2021a. GAM: Explainable Visual Similarity and Classification via Gradient Activation Maps. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 68 77. Barkan, O.; Asher, Y.; Eshel, A.; Elisha, Y.; and Koenigstein, N. 2023a. Learning to explain: model-agnostic frameIn 2023 IEEE Inwork for explaining black box models. ternational Conference on Data Mining (ICDM), 944949. IEEE. Barkan, O.; Elisha, Y.; Asher, Y.; Eshel, A.; and Koenigstein, N. 2023b. Visual Explanations via Iterated Integrated Attributions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 20732084. Barkan, O.; Elisha, Y.; Toib, Y.; Weill, J.; and Koenigstein, N. 2024a. Improving LLM Attributions with Randomized Path-Integration. In Findings of the Association for Computational Linguistics: EMNLP 2024, 94309446. Barkan, O.; Elisha, Y.; Weill, J.; Asher, Y.; Eshel, A.; and Koenigstein, N. 2023c. Stochastic integrated explanations for vision models. In 2023 IEEE International Conference on Data Mining (ICDM), 938943. IEEE. Barkan, O.; Elisha, Y.; Weill, J.; and Koenigstein, N. 2025. BEE: Metric-Adapted Explanations via Baseline Exploration-Exploitation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 18351843. Barkan, O.; Fuchs, Y.; Caciularu, A.; and Koenigstein, N. 2020. Explainable recommendations via attentive multipersona collaborative filtering. In Proceedings of the 14th ACM Conference on Recommender Systems, 468473. Barkan, O.; Hauon, E.; Caciularu, A.; Katz, O.; Malkiel, I.; Armstrong, O.; and Koenigstein, N. 2021b. Grad-sam: Explaining transformers via gradient self-attention maps. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, 28822887. Barkan, O.; Toib, Y.; Elisha, Y.; and Koenigstein, N. 2024b. Learning-based Approach for Explaining Language Models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 98108. Barkan, O.; Toib, Y.; Elisha, Y.; Weill, J.; and Koenigstein, N. 2024c. LLM Explainability via Attributive Masking Learning. In Findings of the Association for Computational Linguistics: EMNLP 2024, 95229537. Bhatt, U.; Xiang, A.; Sharma, S.; Weller, A.; Taly, A.; Jia, Y.; Ghosh, S.; Yona, G.; Mirel, D.; et al. 2020. Explainable machine learning in deployment. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* 20). Chefer, H.; Gur, S.; and Wolf, L. 2021a. Generic attentionmodel explainability for interpreting bi-modal and encoderdecoder transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 397406. Chefer, H.; Gur, S.; and Wolf, L. 2021b. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 782791. Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and FeiFei, L. 2009. ImageNet: Large-Scale Hierarchical Image Database. In Computer Vision and Pattern Recognition (CVPR). Dhurandhar, A.; Chen, P.-Y.; Luss, R.; Tu, C.-C.; Ting, P.; Shanmugam, K.; and Das, P. 2018. Explanations based on the missing: Towards contrastive explanations with pertinent negatives. Advances in neural information processing systems, 31. Doshi-Velez, F.; and Kim, B. 2017. Towards rigorous science of interpretable machine learning. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Erhan, D.; Bengio, Y.; Courville, A.; and Vincent, P. 2009. Visualizing higher-layer features of deep network. University of Montreal, 1341(3): 1. Eriksson, A.; Israelsson, A.; and Kallhauge, M. 2025. Reproducibility review of Why Not Other Classes: Towards arXiv Class-Contrastive Back-Propagation Explanations. preprint arXiv:2501.11096. Everingham, M.; Gool, L. V.; Williams, C. K. I.; Winn, J. M.; and Zisserman, A. 2009. The Pascal Visual Object International Journal of ComClasses (VOC) Challenge. puter Vision, 88: 303338. Fong, R.; Patrick, M.; and Vedaldi, A. 2019. Understanding deep networks via extremal perturbations and smooth In Proceedings of the IEEE International Confermasks. ence on Computer Vision, 29502958. Fong, R. C.; and Vedaldi, A. 2017. Interpretable explanaIn Protions of black boxes by meaningful perturbation. ceedings of the IEEE International Conference on Computer Vision, 34293437. Fu, Z.; Xian, Y.; Gao, R.; Zhao, J.; Huang, Q.; Ge, Y.; Xu, S.; Geng, S.; Shah, C.; Zhang, Y.; et al. 2020. Fairnessaware explainable recommendation over knowledge graphs. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval, 6978. Ghorbani, A.; Abid, A.; and Zou, J. Y. 2019. Interpretation In Proceedings of the AAAI of neural networks is fragile. Conference on Artificial Intelligence. Ghorbani, A.; Wexler, J.; Zou, J.; and Kim, B. 2019. Towards automatic concept-based explanations. In Advances in Neural Information Processing Systems (NeurIPS). Ghorbani, A.; and Zou, J. Y. 2020. Neuron shapley: Discovering the responsible neurons. Advances in neural information processing systems, 33: 59225932. Gilpin, L. H.; Bau, D.; Zoran, D.; Bajwa, A.; Specter, M.; and Kagal, L. 2018. Explaining explanations: An overview In 2018 IEEE 5th of interpretability of machine learning. International Conference on data science and advanced analytics (DSAA), 8089. IEEE. Guidotti, R.; Monreale, A.; Ruggieri, S.; Turini, F.; Giannotti, F.; and Pedreschi, D. 2018. survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5): 142. Haddad, Z. W.; Barkan, O.; Elisha, Y.; and Koenigstein, N. 2025. Soft Local Completeness: Rethinking Completeness in XAI. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1979419804. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770778. Kim, B.; Wattenberg, M.; Gilmer, J.; Cai, C.; Wexler, J.; Viegas, F.; et al. 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, 26682677. PMLR. Kim, S. S.; Meister, N.; Ramaswamy, V. V.; Fong, R.; and Russakovsky, O. 2022. HIVE: Evaluating the human interpretability of visual explanations. In European Conference on Computer Vision, 280298. Springer. Kindermans, P.-J.; Hooker, S.; Adebayo, J.; Alber, M.; Schutt, K. T.; Dahne, S.; Erhan, D.; and Kim, B. 2017. The (un) reliability of saliency methods. arXiv preprint arXiv:1711.00867. Lin, T.-Y.; Maire, M.; Belongie, S.; Bourdev, L.; Girshick, R.; Hays, J.; Perona, P.; Ramanan, D.; Zitnick, C. L.; and Dollar, P. 2014. Microsoft COCO: Common Objects in Context. Lipton, Z. C. 2018. The mythos of model interpretability. Queue, 16(3): 3157. Liu, Z.; Mao, H.; Wu, C.; Feichtenhofer, C.; Darrell, T.; and Xie, S. 2022. ConvNet for the 2020s. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1196611976. Longo, L.; Brcic, M.; Cabitza, F.; Choi, J.; Confalonieri, R.; Del Ser, J.; Guidotti, R.; Hayashi, Y.; Herrera, F.; Holzinger, A.; et al. 2024. Explainable Artificial Intelligence (XAI) 2.0: manifesto of open challenges and interdisciplinary research directions. Information Fusion, 106: 102301. Lundberg, S. M.; and Lee, S.-I. 2017. unified approach In Advances in Neural to interpreting model predictions. Information Processing Systems, 47654774. Miller, T. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence, 267: 138. Petsiuk, V.; Das, A.; and Saenko, K. 2018. Rise: Randomized input sampling for explanation of black-box models. arXiv preprint arXiv:1806.07421. Rudin, C. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5): 206215. Samek, W.; Binder, A.; Montavon, G.; Lapuschkin, S.; and Muller, K.-R. 2017. Evaluating the visualization of what IEEE transactions on deep neural network has learned. neural networks and learning systems, 28(11): 26602673. Samek, W.; and Muller, K.-R. 2019. Towards explainable In Explainable AI: interpreting, exartificial intelligence. plaining and visualizing deep learning, 522. Springer. Selvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D. 2017. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, 618626. Smilkov, D.; Thorat, N.; Kim, B.; Viegas, F.; and Wattenberg, M. 2017. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825. Sokol, K.; and Flach, P. 2020. One explanation does not fit all: The promise of interactive explanations for machine learning transparency. KI-Kunstliche Intelligenz, 34(2): 235250. Springenberg, J. T.; Dosovitskiy, A.; Brox, T.; and Riedmiller, M. 2014. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806. Sturmfels, P.; Lundberg, S.; and Lee, S.-I. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill. Https://distill.pub/2020/attribution-baselines. Sundararajan, M.; Taly, A.; and Yan, Q. 2017a. Axiomatic In Proceedings of the 34th attribution for deep networks. International Conference on Machine Learning-Volume 70, 33193328. JMLR. org. Sundararajan, M.; Taly, A.; and Yan, Q. 2017b. Axiomatic In Proceedings of the Attribution for Deep Networks. 34th International Conference on Machine Learning, ICML 2017, 33193328. Vilone, G.; and Longo, L. 2021. Notions of explainability and evaluation approaches for explainable artificial intelligence. Information Fusion, 76: 89106. Wang, H.; Wang, Z.; Du, M.; Yang, F.; Zhang, Z.; Ding, S.; Mardziel, P.; and Hu, X. 2020. Score-CAM: Score-weighted visual explanations for convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, 2425. Wang, H.; Zhang, M.; Zhang, S.; He, D.; Guo, Y.; and Liu, Y. 2023. Counterfactual-based Saliency Map: Towards Visual Contrastive Explanations for Neural Networks. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). Wang, Y.; and Wang, X. 2022. Why Not Other Classes?: Towards Class-Contrastive Back-Propagation Explanations. In Koyejo, S.; Mohamed, S.; Agarwal, A.; Belgrave, D.; Cho, K.; and Oh, A., eds., Advances in Neural Information Processing Systems, volume 35, 90859097. Curran Associates, Inc. Xie, W.; Li, X.-H.; Lin, Z.; Poon, L. K.; Cao, C. C.; and Zhang, N. L. 2023. Two-stage holistic and contrastive explanation of image classification. In Uncertainty in Artificial Intelligence, 23352345. PMLR. Zhang, H.; Figueroa, F. T.; and Hermanns, H. 2025. Saliency Maps Give False Sense of Explanability to Image Classifiers: An Empirical Evaluation across Methods and Metrics. In Nguyen, V.; and Lin, H.-T., eds., Proceedings of the 16th Asian Conference on Machine Learning, volume 260 of Proceedings of Machine Learning Research, 479494. PMLR. Zhang, J.; Bargal, S. A.; Lin, Z.; Brandt, J.; Shen, X.; and Sclaroff, S. 2018. Top-down neural attention by excitaInternational Journal of Computer Vision, tion backprop. 126(10): 10841102. Zhang, Y.; and Chen, X. 2020. Explainable recommendation: survey and new perspectives. Foundations and Trends in Information Retrieval, 14(1): 1101. Zhou, B.; Khosla, A.; Lapedriza, A.; Oliva, A.; and Torralba, A. 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, 29212929. Supplementary Materials: Rethinking Saliency Maps: Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations Appendix Overview The appendix provides supplementary materials and detailed analyses that support the findings and discussions presented in the main paper. summary of its contents is as follows: Section outlines additional evaluation details and describes the procedure used to construct the semantic group labels. Section explains how existing explanation methods were utilized or adapted to conform to the RFxG framework. Section presents extended quantitative results, including evaluations on the VOC dataset. Section contains additional qualitative examples illustrating the behavior of explanation methods under the RFxG framework. Finally, Section discusses the limitations of our work and outlines directions for future research."
        },
        {
            "title": "B Evaluation Details",
            "content": "Dataset Evaluation Details (1) IN: We evaluated on the IN dataset using the original images and their corresponding ground-truth labels. (2) VOC: To ensure non-overlapping semantic classes with IN, we manually removed image that belong to subset of VOC classes (horse, cow, potted plant, and person) that has no overlap with IN classes. For the remaining images in the validation set, we used the top predicted class from the ViT-B model as ground-truth. (3) COCO: The evaluation protocol applied to VOC was also adopted for the COCO dataset. Semantic Group Construction Semantic groups were constructed based on the WordNet hierarchy. For each class, we first identified whether it belonged to an existing semantic group comprising at least five classes. If such group was found, the class was assigned to it. Otherwise, we merged the class with neighboring semantic groups until the combined group included at least five classes. If the merged group had unique superordinate category, we used that as the group label. Otherwise, we concatenated the names of the contributing groups to form the label. This process also used git repo2. Method Implementation Details General Implementation Notes For the evaluation of SHAP, we utilized the GradientSHAP implementation provided by Captum3. For Rollout, we employed GradientRollout (Barkan et al. 2023b), an extension that incorporates gradients into the standard attention rollout procedure. 2https://github.com/mhiyer/imagenet-hierarchy-fromwordnet/tree/main 3https://captum.ai/ To ensure consistent evaluation across the entire RFxG taxonomy, we adapted subset of explanation methods, leveraging publicly available implementations where available (Wang and Wang 2022; Eriksson, Israelsson, and Kallhauge 2025) and applying minimal modifications as needed to align with the specific explanatory objectives of each evaluation setting. Methods Based on Existing Implementations For CNNs, we used Contrastive GradCAM (GC) (Wang and Wang 2022). For ViTs, we employed both Contrastive GradCAM-ViT (GCV) and Contrastive Gradient-weighted Attention Rollout (Eriksson, Israelsson, and Kallhauge 2025). Methods Implemented with Minimal Modifications For explanation methods that lacked native support for contrastive or group-based settings, we developed general adaptation mechanism that allows alignment with the RFxG taxonomy. Most existing methods operate through core computation (e.g., backpropagation) applied with respect to specific class label. We extended this mechanism to support functions over multiple labels, including class differences and group aggregations. Specifically, we applied the core computation over the following functions: Class-Contrastive Methods: The explanation is computed with respect to the score difference between class and class B, i.e., fA fB. Class-to-Group Contrastive Methods: The explanation is computed using the difference between the score for class and the sum of scores for all classes in contrastive group GB, i.e., fA (cid:80) fc. cGB Pointwise Group Methods: The explanation is computed with respect to the aggregated score of all classes within semantic group G, i.e., (cid:80) cG fc. Contrastive Group Methods: The explanation is computed based on the difference between the sum of scores for classes in group GA and those in group GB, i.e., (cid:80) fc2. This adaptation strategy enables consistent application of explanation techniques under varying reference frames and semantic granularities as required by the RFxG framework. fc1 (cid:80) c2GB c1GA Example: Customizing Contrastive-GradCAM. Note: The implementation presented here is not based on the official method from (Wang and Wang 2022), but rather serves as an illustrative example of how we adapted GC to align with the RFxG framework. This implementation of Contrastive-GradCAM generalizes the GC method (Selvaraju et al. 2017) to generate explanations that highlight features distinguishing target class from an alternative class B. Instead of computing gradients with respect to the class score fA alone, this version of GC computes gradients with respect to the contrastive score: sA,B = fA fB, (6) where fA and fB are the pre-softmax logits for classes and B, respectively. Table 3: Evaluation results across datasets, models, and explanation methods using our metrics: CCS, CGC, PGS, and CGS. Higher scores indicate better performance. For every metric in the table, the results were provided using method suitable for the metrics purpose. For example: for evaluating GC on CCS, we used Contrastive-GradCAM (Wang and Wang 2022). Dataset Model Method CCS CGC PGS CGS VOC CN RN ViT-B ViT-S IIA GC SC SHAP IG IIA GC SC SHAP IG IIA TAttr GAE Rollout GCV IIA TAttr GAE Rollout GCV 25.17 21.96 21.52 16.20 13. 25.45 22.79 22.16 16.38 13.76 19.64 18.47 18.24 14.79 12.93 19.63 18.31 18.03 14.88 12.56 5.19 4.79 4.52 3.26 2.93 5.23 4.65 4.39 3.41 2.79 4.57 4.24 4.01 3.02 2. 4.59 4.22 4.06 3.08 2.73 46.39 42.43 41.74 32.19 27.69 46.48 42.14 40.12 32.27 27.59 39.60 36.47 35.84 29.20 26.66 39.36 35.56 35.00 29.15 26.39 35.61 32.64 31.87 26.86 23. 36.35 32.97 32.03 27.00 23.22 29.82 27.70 27.38 22.36 20.41 29.64 27.01 26.61 22.36 20.39 Let Ak denote the k-th activation map from the final convolutional layer. The corresponding importance weight αA,B is computed via global average pooling over the grak dients: αA,B = 1 (cid:88) i,j sA,B Ak i,j , (7) where is the number of spatial positions and (i, j) indexes spatial coordinates. Unlike standard GC, we omit the final ReLU operation to preserve negative contributions, as these are considered informative in the contrastive setting (Wang and Wang 2022). The resulting contrastive saliency map is given by: A,B con = (cid:88) αA,B Ak. (8) This formulation enables the explanation to focus on regions that are specifically important for differentiating class from class B. Additional Quantitative Results Table 3 presents further results for the VOC datasets. These results follow similar trends to Tab. 1. Qualitative Results Figure 3 presents qualitative comparison of the explanation maps obtained by the methods GC,IIA, and SC. These maps were created using the following setup: is sports car, sport car, is cab, hack, taxi, taxicab and the group named Car with the following classes: sports car,racer,model T, Figure 3: Saliency maps for sport car using different types of questions that reflect our taxonomy with different methods: (a) Class Contrastive. (b) Class-Group Contrastive. (c) Pointwise Group. minivan, limousine, jeep, convertible, cab, beach wagon, and ambulance. Limitations and Future Work While our RFxG framework provides significant advancement in user-aligned saliency map evaluation, several limitations warrant discussion alongside promising future directions. These considerations do not undermine our core contributions but rather highlight opportunities to extend the impact of our principled approach to explanation evaluation. F.1 Methodological Considerations Perturbation-based evaluation framework, while widely adopted in the XAI literature (Petsiuk, Das, and Saenko 2018; Samek et al. 2017), can inherit certain methodological constraints. For instance, our black pixel masking approach we employed could theoretically introduce distribution shift artifacts when removing salient regions. To address this concern, we conducted extensive ablation studies with alternative masking techniques, including Gaussian blur and texture-preserving inpainting. While these alternatives mitigate distribution shift concerns, they yielded qualitatively and quantitatively similar results across all RFxG metrics, confirming that our frameworks insights are not artifacts of the specific perturbation methodology. Future work could explore more sophisticated perturbation approaches that better preserve local image statistics while maintaining computational efficiency. F.2 Scope and Generalizability Our current implementation focuses on image classification tasks, leaving open opportunities for extension to other domains. The RFxG framework naturally extends to visiona comprehensive evaluation suite that balances faithfulness with task-specific utility would further strengthen the practical impact of explanation methods. In conclusion, rather than representing weaknesses of our framework, these limitations highlight fertile ground for extending the RFxG taxonomys impact across AI subfields and applications. By addressing these directions, the research community can build upon our foundation to create explanation systems that are not only technically sound but also genuinely useful for diverse users across myriad contexts. Our work provides the conceptual scaffolding necessary to transform saliency maps from ambiguous visualizations into precise, user-aligned communication tools, critical step toward trustworthy and interpretable AI systems. based recommendation systems, where explaining why item was recommended over item (contrastive reference frame) at varying levels of abstraction (granularity) could enhance user trust and satisfaction. For instance, in fashion recommendation, our framework could elucidate whether recommendation stems from fine-grained attributes (specific pattern details) or broader category features (general garment type). In natural language processing, the RFxG taxonomy offers promising avenues for advancing text explanation methods. Current saliency approaches for text often lack explicit reference frames, making it difficult to distinguish between explanations answering Why this sentiment? versus Why positive and not negative?. Applying our framework to transformer-based language models could yield more precise explanations aligned with user queries, particularly valuable in applications like legal document analysis or medical report interpretation where contrastive reasoning is essential. F.3 Integration with Concept-Based Explanations While our work establishes foundation for user-aligned evaluation of pixel-space saliency maps, an exciting direction involves bridging the gap between low-level visual explanations and high-level conceptual reasoning. The RFxG framework could be extended to evaluate concept-based explanation methods like TCAV (Kim et al. 2018) by formalizing how concepts operate across reference frames (e.g., Why concept supports class versus class B?) and granularities (e.g., How does concept operate at different semantic levels?). This integration would create continuum from pixel-level to concept-level explanations, addressing critical gap in current XAI research. Furthermore, our metrics could be adapted to evaluate explanations in multimodal settings, where understanding the interplay between visual and textual explanations becomes crucial. For instance, in vision-language models, determining whether model relies on appropriate visual evidence versus language priors when answering contrastive questions represents an important application of our framework. F.4 Broader Impact and Deployment"
        },
        {
            "title": "Considerations",
            "content": "As our framework moves toward real-world deployment, particularly in high-stakes domains like medical imaging and autonomous systems, additional considerations emerge. In medical applications, for example, the appropriate granularity for explanations may differ between clinicians (who might prefer organ-level group explanations) and patients (who might benefit from more fine-grained lesion-specific explanations). Future work should investigate domain-specific adaptations of RFxG that respect the unique requirements and constraints of different application areas. Moreover, while our current metrics focus on faithfulness to model behavior, complementary metrics assessing explanation utility for specific tasks (e.g., model debugging versus user education) represent valuable extension. Developing"
        }
    ],
    "affiliations": [
        "Harvard University",
        "Tel Aviv University",
        "The Open University"
    ]
}