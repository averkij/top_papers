{
    "paper_title": "Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration",
    "authors": [
        "Mengyu Yang",
        "Yanming Yang",
        "Chenyi Xu",
        "Chenxi Song",
        "Yufan Zuo",
        "Tong Zhao",
        "Ruibo Li",
        "Chi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 3 5 2 2 . 1 1 5 2 : r Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration"
        },
        {
            "title": "Tong Zhao",
            "content": "Ruibo Li Chi Zhang* AGI Lab, Westlake University Project Page: https://fast3dcache-agi.github.io"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent cachingbased methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to 27.12% speed-up and 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%). 1. Introduction Diffusion models and Flow Matching have demonstrated remarkable success in generating high-fidelity content across various modalities including 2D images [14, 24, 28, 45], videos [1, 18, 55], and 3D assets [51, 61, 63]. However, significant drawback is their computationally intensive and slow inference process, which relies on sequential and iterative denoising procedure. To alleviate this computational bottleneck, recent studies have explored caching-based acceleration techniques that exploit redundancy in intermediate computations [5, 26, 29, 33, 34, 42, 59, 72, 75, 76]. The *Corresponding author. core idea is to cache and reuse computations from previous timesteps, thereby reducing the need for repeated inference. These caching methods have shown considerable success in accelerating 2D image and video generation. However, directly extending these caching strategies to 3D diffusion models presents significant challenges. In 2D or video generation, caching methods typically exploit redundancy in pixel information. They can effectively trade minor quality degradation for faster inference because these tasks are primarily perceptual and small texture inaccuracies are often visually negligible. In contrast, 3D generation requires the model to learn and synthesize precise geometric structures, task where small numerical inaccuracies introduced by caching can accumulate into major inconsistencies. Unlike texture or color errors in 2D images, deviations in voxel or point-level predictions directly affect the topology and spatial integrity of the 3D object. For instance, in TRELLIS [63], state-of-the-art 3D diffusion framework, when naive caching is applied to its geometry generation phase, even minor inaccuracies in cached voxels or latent features can produce structural artifacts such as surface holes, geometric distortions or nonmanifold meshes. This highlights the need for geometryaware caching strategies that exploit computational redundancy while maintaining integrity of both geometry and texture in 3D generation. To address this problem, we propose Fast3Dcache, designed to accelerate inference while preserving structural correctness. Our approach is motivated by key observation derived from analyzing state-of-the-art 3D diffusion frameworks TRELLIS [63]: during the denoising process, the occupancy field, which indicates voxel existence, exhibits progressively stabilizing pattern. Specifically, as denoising proceeds, an increasing number of voxel locations become static, meaning their occupancy values no longer change across subsequent timesteps. Moreover, the number of these active updates decreases approximately following logarithmic pattern. Inspired by this observation, we design dynamic caching mechanism that identifies and caches features corresponding to stable voxel regions in the latent fea1 ture space. By adaptively tuning the cache ratio based on the observed change rate, our method avoids redundant computations in static regions while focusing inference on dynamically evolving parts of the geometry. We formalize this scheduling strategy as Predictive Caching Scheduler Constraint (PCSC), which predicts the number of stable voxels and controls the caching ratios over timesteps effectively. After determining the cache quota for each timestep according to the stabilization pattern, we further design robust selection criterion, termed the Spatiotemporal Stability Criterion (SSC), to accurately identify which tokens should be cached. Intuitively, given the cache quota predicted by the PCSC scheduler, our goal is to cache those tokens that have exhibited stable behavior in recent timesteps, namely, features corresponding to regions whose geometric states have largely converged. To achieve this, SSC evaluates voxel stability from two complementary perspectives. The first is the magnitude of the predicted velocity, which reflects how much voxels latent representation changes between consecutive timesteps. The second is the acceleration, which measures the stability of velocity through the rate of change in size and direction. By jointly considering both magnitude and direction, SSC provides more finegrained measure of voxel stability than either metric alone, enabling accurate and adaptive caching decisions. We conduct comprehensive experiments on 3D generation tasks. Our approach accelerates inference substantially and maintains high geometric quality compared to non-accelerated baselines and naive caching strategies. Our main contributions are summarized as follows: We propose novel geometry-aware caching framework for 3D diffusion models, leveraging the intrinsic stabilization patterns of voxel occupancy during denoising. We design Predictive Caching Scheduler Constraint (PCSC) that dynamically adjusts caching ratios over timesteps based on the predicted stabilization trend. We introduce Spatiotemporal Stability Criterion (SSC), robust token-selection rule that selects stable voxel tokens through joint analysis of velocity magnitude and acceleration magnitude. experiments validate approach achieves state-of-the-art acceleration-performance tradeoffs on 3D generation tasks, and we will release the code and models for future research. that our Extensive 2. Related Work Diffusion and Flow-based 3D Generative Models. Previously, Score Distillation Sampling (SDS) [4, 23, 38, 48, 49, 66, 70] was widely utilized for 3D content creation. However, this approach suffers from significant limitations, including slow per-scene optimization speeds and multiview inconsistencies known as the Janus problem. With the emergence of large-scale 3D datasets such as Objaverse 2 [7, 22], researchers have increasingly utilized this data to train DM or FM capable of generating 3D objects directly. Current direct 3D generation methods utilize different underlying representations, which can be primarily categorized into explicit [60, 61, 63], and implicit latent sets [21, 50, 51, 74]. Compared to implicit latent vectors, explicit voxel representations offer superior control over spatial structure and topology. Among works utilizing explicit representations, TRELLIS [63] stands out due to its distinct two-stage design, which decouples geometry synthesis from texture generation. Consequently, TRELLIS has become foundational framework for various downstream tasks, with subsequent research addressing specific challenges: DSO [20] incorporates physics-based guidance to ensure physical soundness, Amodal3R [62] resolves occlusion issues, and other works focus on refinement and partaware modeling [12, 40, 68, 69, 71]. Acceleration Works of DM / FM. Acceleration for 2D or video diffusion models is broadly categorized into trainingrequired and training-free approaches. (1) Trainingrequired Methods include distillation [6, 17, 35, 37, 41] and consistency models [46, 57]. These methods require expensive retraining and permanently modify model weights. They are often limited by specific frameworks. (2) Training-free Methods reduce inference costs by exploiting redundancies without altering weights. These include adaptive solvers [31, 32] or sampling strategies [8, 16, 39, 43] that reduce step counts, attention optimizations [2, 44, 61, 65, 73], and model pruning [3, 53]. Most relevant to our work is feature caching [5, 10, 11, 26, 29, 33, 34, 42, 59, 72, 75, 76], which reuses features based on spatial / temporal similarity. However, these methods are predominantly designed for 2D / video tasks. Their direct migration to 3D generation often causes fatal topological errors by ignoring unique geometric characteristics. While Hash3D [67] explored 3D acceleration, it is not applicable to diffusion-based frameworks. 3. Preliminaries 3.1. Flow Matching (FM) Many 3D generation frameworks leverage Flow Matching (FM) [9, 19, 24, 28, 56, 57, 64], particularly the efficient rectified flow formulation. The ideal velocity field ut = y1 y0 serves as the ground-truth, defined along the path yt = (1 t)y0 + ty1 that interpolates data y0 pdata and noise y1 (0, I). neural network vθ(yt, t) is trained to approximate this field. Inference generates sample y0 from noise y1 by numerically solving the ODE y0 = y1 (cid:82) 1 0 vθ(yt, t)dt. The characteristics of the predicted velocity field vθ at each step tk inform our caching strategy. 3.2. Sparse Structure Generation The TRELLIS framework [63] generates 3D assets in two stages: Structure Generation and SLAT Generation. Our work accelerates the first stage, which defines the structure as set of active voxel coordinates = {pi}L i=1. Flow Transformer GS, conditioned on DINOv2-processed image c, iteratively predicts the velocity field to evolve noise grid Sϵ RBCDHW . By decoding this latent grid at each step, we observe distinct three-phase stabilization pattern where voxel changes progressively diminish. This predictable behavior provides clear opportunity to accelerate the Structure Generation process. 4. Methodology Our observations of 3D generation are introduced to support our designed acceleration in Subsection 4.1. Following the observations, in Subsection 4.2, we present corresponding modules in our method Fast3Dcache. In Subsection 4.3, we present our Fast3Dcache pipeline by introducing how to apply these two modules to three stages. 4.1. 3D Geometry Synthesis Observation In TRELLIS [63], geometry synthesis is achieved by iteratively rectifying latent feature grid St, which is decoded at each step to determine the underlying 3D structure. To design geometry-aware caching strategy, we first analyze how the generated geometry evolves over time. Our study reveals two complementary forms of redundancy: (1) threephase stabilization pattern in voxel occupancy that follows predictable log-linear decay, and (2) stabilization of latent features, reflected in the magnitude and temporal variation of the predicted velocity field. Together, these observations indicate that large portions of the grid become progressively stable as sampling proceeds, revealing substantial computational redundancy and suggesting that feature caching can be safely exploited in these regions. Voxels Evolution in Binary 3D Grid. To determine when caching can be safely applied, we require measure of geometric change across timesteps. Instead of analyzing latent tokens directly, we decode the latent grid St into binary occupancy grid Ot RN 3 . This representation allows us to quantify dynamic voxels, i.e. the voxels whose occupancy state changes between consecutive timesteps: st = (cid:88) i,j,k (Ot+1(i, j, k) Ot(i, j, k)) , (1) where i, j, 0, 1, . . . , 1 and denotes the XOR operation, which directly reflects whether each voxel has flipped its state. large value of st indicates that the global geometry is still rapidly evolving, whereas small value suggests that the structure has largely stabilized. (a) Orginal curve. (b) PCSC curve. Figure 1. Observed Voxel Stabilization Trend and the PCSC Motivation. (a) The Original curve plots the empirically observed number of dynamic voxels (log-scale) per inference step, revealing distinct three-phase pattern. (b) The PCSC curve illustrates our approach, motivated by this observation. We identify that the decay in Phase 2 can be reliably approximated by log-linear function (red dashed line). This predictability forms the foundation for our scheduler, which we calibrate at an anchor step to forecast the stabilization budget. Fig. 1a plots st over time for TRELLIS [63]. We consistently observe clear three-phase pattern: (i) an initial unstable phase, where st is large and the coarse geometry is being formed, (ii) an intermediate phase, where st decreases approximately log-linearly as the geometry progressively stabilizes, (iii) final phase, where st drops sharply and only minor refinements occur. This phase separation is crucial for our method: it suggests that caching should be disabled in the early unstable phase, gradually introduced with growing budget in the intermediate phase, and applied most aggressively in the final refinement phase. In other words, the voxel evolution curve provides principled way to allocate different caching budgets to different stages of the generative process. Feature Dynamics in Latent Grid. The voxel trend provides global caching budget at each timestep, but it does not specify which tokens can be safely cached. To select specific tokens, we need more fine-grained measure of local stability in latent space. For this purpose, we examine the velocity field predicted by the network at each timestep and analyze both its magnitude and its temporal variation. (1) Velocity Field Analysis: We define the velocity magnitude for token at timestep as Vi(t) = vi(t)2, which represents the intensity of feature updates. As shown in Fig. 2a, the distribution of Vi(t) also exhibits three-phase evolution. In Phase 1 (blue region), many tokens have large velocity magnitudes, reflecting the need for substantial updates to establish the coarse object structure. Caching in this stage would risk corrupting the emerging geometry. In Phase 2 (orange region), the number of tokens with large Vi(t) gradually decreases, indicating that more regions of In Phase 3 (red rethe grid become stable over time. gion), most tokens exhibit small velocity magnitudes, and the model only performs subtle refinements. These obser3 (a) Visualization of velocity field feat map. This panel displays the temporal evolution of the predicted velocity field vt for each token within central spatial slice of the feature grid St. (b) Visualization of acceleration field feat map. This panel illustrates the temporal evolution of the difference between consecutive velocity fields (vt vt12, acceleration), representing the instantaneous caching error for each token within the central slice. Figure 2. Visualization of velocity field and acceleration field feat maps in St. The maps illustrate the temporal dynamics of (a) velocity magnitude and (b) acceleration magnitude (rate of change). These tiny dynamics mirror the three-phase stabilization pattern observed in Fig. 1a. The progressive decay in both velocity and acceleration magnitudes confirms their efficacy as robust criteria for identifying stable tokens suitable for caching. vations suggest that tokens with persistently small velocity magnitudes are natural candidates for caching. (2) Acceleration Field Analysis: We define Instantaneous Caching Error (ICE), equivalent to the acceleration magnitude Ai(t), to quantify the potential error incurred by approximating the current velocity with the previous step: ICEi(t) Ai(t) = vi(t) vi(t 1)2. (2) Intuitively, Ai(t) measures how much the current update direction deviates from the previous one. In Fig. 2b, high-acceleration events correlate strongly with the structural changes observed in the velocity field but provide more rigorous measure of instability. Consequently, we leverage both velocity and acceleration metrics as the joint criteria for token selection. 4.2. Fast3Dcache Core Components Building upon the observations, we introduce two complementary components that jointly determine the caching strategy in Fast3Dcache. The Predictive Caching Scheduler Constraint (PCSC) specifies how many tokens may be cached at each timestep, while the Spatiotemporal Stability Criterion (SSC) determines which specific tokens can be safely cached without degrading geometric fidelity. Together, these modules translate geometric evolution into dynamic and fine-grained computational policy. Predictive Caching Scheduler Constraint (PCSC). The goal of PCSC is to allocate an appropriate caching budget at each timestep. Motivated by the distinct stabilization pattern observed in Phase 2 of Fig. 1a, we approximate the decline in dynamic voxels using log-linear curve. This predictive approach enables the model to dynamically determine the optimal cache quota at each timestep, ensuring that the computational budget adapts flexibly to the evolving stability of the geometry. To construct the decay schedule efficiently with minimal computational overhead, we perform one-time calibration at the end of Phase 1. We designate specific timestamp as the anchor step, calculated as ρa, where ρa governs the duration of the full-sampling stage. At this anchor, we quantify the initial magnitude of voxel changes, denoted as σ, by comparing the decoded grids of adjacent steps. Empirically, the rate at which dynamic voxels decay is consistent across diverse samples, allowing us to adopt fixed slope parameter µ to extrapolate future changes. As illustrated in Fig. 1b, we model the decay of dynamic voxels as straight line in log-coordinate system: log(ˆs) = µ + λ, (3) where ˆs is the predicted dynamic voxel between adjacent timesteps and λ is the vertical intercept. The line in logcoordinate yields final predictive curve: ˆs = σ eµ(tN ρa). (4) 4 Figure 3. Overview of the Fast3Dcache three-stage acceleration strategy. Phase 1 (Full Sampling): The process begins with full sampling to establish initial geometric stability. At the end of this phase, the PCSC is calibrated by measuring voxel change (σ) at the anchor step. Phase 2 (Dynamic Caching): In the main phase, the SSC identifies stable tokens for caching based on the dynamic budget predicted by PCSC. Only unstable tokens are processed by the FT. Phase 3 (CFG-Free Refinement): The final stage employs an aggressive fixed-ratio schedule. high and fixed ratio ξ is used to determine the proportion of tokens to cache, maximizing computational savings during these stable refinement steps. This predictive curve provides time-varying estimate of geometric change ˆst across intermediate timesteps. To translate this geometric prediction into computational budget for the flow transformer, we derive the number of tokens to be cached ct. Since the dynamic voxels are defined in the upsampled output space, we normalize them by the upsampling factor γup to estimate the necessary active calculations, and designate the remainder as the cache quota: ct = D3 ˆst γup , (5) where D3 represents the total number of latent tokens and γup denotes the upsampling ratio. This derived ct serves as the dynamic caching budget that strictly constrains the token selection process in the subsequent Phase 2. This dynamic budget specifies an upper bound on the number of cached tokens for Phase 2 and ensures that caching aggressiveness adapts to the stability of the underlying geometry. Spatiotemporal Stability Criterion (SSC). While PCSC establishes the global cache budget ct, the complementary challenge is to pinpoint exactly which tokens can be safely cached without compromising geometric fidelity. This selection process requires metric that is both accurate and computationally lightweight. To achieve this, we introduce the Spatiotemporal Stability Criterion (SSC), which evaluates token-wise stability based on instantaneous velocity and acceleration dynamics. SSC is applied throughout Phases 2 and 3 to distinguish tokens that require fresh computation from those whose features have converged. Guided by the observations in Sec. 4.1, we define for each token cache ability score Ci(t) that integrates two normalized quantities: the acceleration Ai(t) (representing temporal variation of updates) and the velocity magnitude Vi(t) (representing update intensity). Formally, Ci(t) = ω norm(Ai(t)) + (1 ω) norm(Vi(t)) , (6) where norm(cid:0)Ai(t)(cid:1) = norm(cid:0)Vi(t)(cid:1) = Ai(t) minj Aj(t) maxj Aj(t) minj Aj(t) Vi(t) minj Vj(t) maxj Vj(t) minj Vj(t) . , (7) Intuitively, this score captures how (un)stable token is: tokens with small, slowly-varying updates are more stable and therefore more suitable for caching, whereas tokens with large or rapidly-changing updates are less stable and should be recomputed more frequently. The 3D latent state St is first flattened into dense sequence of tokens x(t) RBNpdmodel, where Np = denotes the total number of tokens. Given computational budget ct (the maximum number of tokens we can afford to actively update at step t), the SSC ranks tokens according to their cache ability scores and identifies the indices of the active subset, denoted by (t) active. We apply an index-based selection to extract the corresponding features and obtain reduced input sequence x(t) active = x(t)[:, (t) active, :], which contains only the tokens chosen under the budget ct. Since the attention mechanism [54] is the primary computational bottleneck in the generation process, we perform self-attention exclusively on this active subset. Only the less stable, more informative tokens are recomputed, while the more stable tokens reuse their cached states. This stability-aware selection allows us to respect the budget ct and substantially reduce the cost of attention. 5 4.3. Fast3Dcache Integration 5.1. Implementation Details Having established the PCSC budget scheduler and the SSC token selector in Subsection 4.2, we now integrate these components into unified, end-to-end acceleration workflow Fast3Dcache. As illustrated in Fig. 3, Fast3Dcache segments the inference process into three strategic phases. This multi-stage design balances the necessity for geometric stability in the early steps with the opportunity for aggressive acceleration in the later convergent stages. Phase 1: Full Sampling. Consistent with observations in other generative modalities [15, 30, 52, 72], the initial steps of 3D generation exhibit high volatility in voxel evolution. Consequently, we employ full sampling during this phase to guarantee fundamental geometric accuracy. Crucially, the final step of this phase serves as the anchor point to calibrate the PCSC scheduler, allowing us to predict the decay trajectory and determine the cache budget for the subsequent phase. Phase 2: Dynamic Caching. In this intermediate phase, we deploy the SSC module to execute precise, token-level caching based on the dynamic budget provided by PCSC. However, relying exclusively on cached features for extended periods can lead to geometric errors. To mitigate this, we enforce an Error Accumulation Elimination constraint defined by the interval τ , where τ is the interval controlling the frequency of full refresh steps. We mandate full-sampling update every τ steps to rectify the latent states and limit the propagation of approximation errors. This periodic reset keeps the latent grid aligned with the correct generative trajectory. Phase 3: CFG-Free Refinement. As shown in Fig. 1a, the generation process enters highly stable regime once Classifier-Free Guidance (CFG) [13] is disabled, focusing primarily on minor structural refinements. To streamline efficiency during this stage, we transition to simplified fixed-ratio caching strategy. We continue to utilize the SSC for token selection, but the cache budget ct is governed by constant, aggressive ratio ξ. We can calculate it as ct = D3 ξ, where D3 denotes the total token volume. To counteract potential error accumulation over this extended sequence, we introduce periodic correction cycle governed by the parameter fcorr. The model operates in cached mode for fcorr 1 steps, followed by Full Correction Step every fcorr-th step, where all tokens are recalculated to fully realign the feature grid. 5. Experiments In this section, we present our experiments details to show the ability of our method. In Subsection 5.1, we introduce the implementation details. In Subsection 5.2, relative results of Fast3Dcache are demonstrated, including quantitative results, visualization results and ablation study. Setting. Our experiments are conducted on TRELLIS [63] and its variant DSO [20], focusing on accelerating the inference in the initial sparse structure generation stage. single NVIDIA GeForce RTX 4090 GPU is used in our experiment. To ensure fairness across all methods, we use FlashAttention by default in all our experiments. Evaluation. To evaluate acceleration in geometry generation, we measure throughput (iters/s) and FLOPs (T) for inference efficiency. For geometric fidelity, we adopt Chamfer Distance (CD) and F-Score (threshold = 0.05), following standard protocols in 3D generation [27, 36, 58]. All generated meshes are normalized into unit cube and aligned with ground truth using the Iterated Closest Point (ICP) algorithm prior to metric computation. We evaluate on the Toys4K dataset [47] following TRELLIS [63] for evaluation. For each object, we select its corresponding mesh as ground truth, render it from 12 fixed viewpoints, apply background removal, and filter out low-quality images. This yields 71 objects with 852 valid image prompts. Each image is fed independently into the model, and we report mean metrics across all samples for fair comparison. 5.2. Results Analysis Quantitative Results. Table 1 reports comprehensive comparison between Fast3Dcache and several methods. In addition to the vanilla TRELLIS and DSO configurations, we include modality-aware caching method, RAS [30], originally designed for 2D DiT models. To ensure fair evaluation, we re-implement RAS in 3D setting by extending its sampling and region-selection mechanisms to voxel grids, thereby constructing strong and competitive method for structured 3D generation. Across all metrics, Fast3Dcache delivers substantial efficiency gains while maintaining high geometric fidelity. By contrast, the 3Dadapted RAS method fails to preserve structural integrity and leads to significant artifacts, 26.53% drop in F-Score on TRELLIS. This performance gap underscores key observation: caching strategies developed for 2D image synthesis do not directly generalize to 3D geometry, as they overlook the distinct stabilization patterns and topologysensitive dynamics of volumetric structures. By explicitly modeling these 3D-specific behaviors through PCSC and SSC, Fast3Dcache achieves better accelerationquality trade-off. The results validate that geometry-aware caching is essential for reliable and efficient 3D generative modeling, and that the proposed method provides both principled and practical advantages over existing 2D techniques. Complementarity with Modality-Agnostic Accelerators. We further examine whether Fast3Dcache can serve as complementary module to existing state-of-the-art, modality-agnostic acceleration methods. To this end, we in6 Table 1. Quantitative comparison on TRELLIS [63] and DSO [20] frameworks. We benchmark Fast3Dcache against TRELLIS and existing modality-aware method (RAS [30]). Our method consistently outperforms the baseline, achieving higher throughput and lower FLOPs while preserving geometric fidelity (CD and F-Score) across various settings. (best and second-best) Frameworks Acceleration Methods Throughput (iter/s) FLOPs (T) TRELLIS vanilla 0.5055 244.2 CD 0.0686 F-Score 54.8244 RAS [30] (sample ratio 25%) RAS [30] (sample ratio 12.5%) Fast3Dcache (τ = 3) Fast3Dcache (τ = 5) Fast3Dcache (τ = 8) 0.6337 ( 25.36 %) 0.6177 ( 22.20 %) 0.5850 ( 15.73 %) 0.6344 ( 25.50 %) 0.6426 ( 27.12 %) 125.1 ( 48.77 %) 125.8 ( 48.48 %) 142.4 ( 41.69 %) 121.3 ( 50.33 %) 110.3 ( 54.83 %) 0.0867 ( 26.38 %) 0.0846 ( 23.32 %) 0.0697 ( 1.60 %) 0.0712 ( 3.79 %) 0.0703 ( 2.48 %) 40.2769 ( 26.53 %) 43.9622 ( 19.81 %) 54.0900 ( 1.34 %) 53.5003 ( 2.42 %) 53.7528 ( 1.95 %) DSO [20] vanilla 0.3496 244. 0.0687 54.8350 RAS [30] (sample ratio 25%) RAS [30] (sample ratio 12.5%) Fast3Dcache (τ = 3) Fast3Dcache (τ = 5) Fast3Dcache (τ = 8) 0.4341 ( 24.17 %) 0.4047 ( 15.76 %) 0.3955 ( 13.13 %) 0.4114 ( 17.68 %) 0.4071 ( 16.45 %) 125.0 ( 48.81 %) 125.8 ( 48.48 %) 146.5 ( 40.01 %) 126.0 ( 48.40 %) 115.4 ( 52.74 %) 0.0805 ( 17.18 %) 0.0820 ( 19.36 %) 0.0698 ( 1.60 %) 0.0711 ( 3.49 %) 0.0704 ( 2.47 %) 46.4990 ( 15.20 %) 45.5584 ( 16.92 %) 54.0451 ( 1.44 %) 53.5506 ( 2.34 %) 53.5487 ( 2.35 %) Table 2. Results of Fast3Dcache combined with modalityagnostic SOTA method. Integrating our method with the modality-agnostic acceleration framework Teacache yields further speedup while also improving reconstruction quality. Table 3. Ablation study of the PCSC module. We evaluate the effectiveness of our adaptive scheduler compared to fixed-rate sampling methods. Additionally, we analyze the sensitivity of the decay slope µ, demonstrating that optimal slope calibration is essential for preserving generation quality. Acceleration Methods Throughput (iters/s) CD F-Score Vanilla Teacache [25] Teacache + ours 0.51 (1.00) 1.45 (2.84) 1.74 (3.41) 0.0686 0.0705 0.0701 54.8244 53.5567 53.9420 tegrate Fast3Dcache with TeaCache [25], leading trainingfree accelerator originally developed for video diffusion. Since TeaCache is not tailored for 3D geometry, we first adapt its timestep-based caching mechanism to the 3D sparse transformer architecture. As shown in Table 2, the 3D-adapted TeaCache alone achieves 2.84 speedup. When combined with our geometry-aware Fast3Dcache, the acceleration further improves to 3.41 throughput, demonstrating that the two methods provide complementary gains. Remarkably, the combined approach also yields improved geometric fidelity, achieving better CD and F-Score scores than TeaCache [25] alone, indicating that Fast3Dcache contributes not only additional efficiency but also stabilizes geometric updates during sampling. These results confirm that Fast3Dcache is highly compatible with existing accelerators and can be seamlessly integrated to produce compounding improvements in both speed and quality. Visualization Results. We present some 3D generation results in Fig. 4. Results demonstrate that our method is better than existing modality-aware approache RAS [30]. Impact of PCSC Scheduler. Table 3 Ablation Study. presents ablation analysis for the PCSC module. To validate the necessity of our adaptive approach, we compare PCSC Schedule CD F-Score w.o. PCSC (fixed 25%) w.o. PCSC (fixed 12.5%) PCSC ( µ = 0.7 ) PCSC ( µ = 0.07 ) PCSC ( µ = 0.007 ) 0.0956 0.0899 0.0707 0.0697 0.0701 34.5122 39.0563 53.4978 54.0900 53.7803 against static, non-adaptive strategies with fixed sampling rates. The results demonstrate that PCSC significantly outperforms fixed-rate methods by dynamically tailoring the cache budget to the specific geometric complexity of each input prompt. Furthermore, we investigate the sensitivity of the slope parameter µ. Since the cache quota ct is discretized, minor fluctuations (e.g., 10%) have negligible effect on the final budget. Consequently, we vary the slope by an order of magnitude (factor of 10) to clearly delineate the impact of the decay rate on generation quality. Effectiveness of SSC Components. Table 4 details ablation analysis of SSC module. In Eq. 6, the caching score is weighted fusion of velocity (Vi) and acceleration (Ai). We evaluate the contribution of each component against the RAS method (Row 1), which utilizes standard deviation for screening. We further test single-component settings where only Vi or Ai is active. The results confirm that neither component alone is sufficient. The best performance is achieved by jointly considering both metrics, validating their complementary role in assessing geometric stability. 7 Figure 4. Visualization comparison of 3D geometry synthesis. The leftmost column presents the input image. Subsequent columns display 3D meshes generated by original TRELLIS, RAS method (at varying sampling ratios). Observe that while RAS introduces noticeable geometric artifacts and surface noise, Fast3Dcache preserves structural fidelity comparable to the original TRELLIS framework, achieving acceleration without compromising quality. Table 4. Ablation study of the SSC module. We evaluate the individual contributions of the velocity (Vi) and acceleration (Ai) components. The results demonstrate that relying on single metric is insufficient, while the joint consideration of both fields yields better geometric fidelity. w. Vi(t) w. Ai(t) ω 0.3 0.4 0.5 0.6 0.7 CD F-Score 0.0743 0.0836 0.0709 0.0703 0.0706 0.0703 0.0705 0.0697 50.9974 44.9630 53.5394 53.9156 53.5711 53.7132 53.8326 54.0900 Effectiveness of Elimination Step. Finally, we validate the critical role of the elimination step τ  (Table 1)  . The 8 results demonstrate that this constraint is indispensable for maintaining stability. Completely disabling the correction mechanism leads to significant geometric degradation, with CD deteriorating to 0.0724 and F-Score dropping to 51.8157. This confirms that periodic full-sampling updates are essential to rectify accumulated approximation errors and preserve high-fidelity generation. These experiments all demonstrate the effectiveness of each of our modules. 6. Conclusion We present Fast3Dcache, training-free acceleration framework tailored for the TRELLIS series to expedite 3D geometry synthesis. Our approach exploits intrinsic stabilization patterns within the generation process through two synergistic modules: the Predictive Caching Scheduler Constraint (PCSC), which dynamically allocates the computational budget based on voxel decay trends, and the Spatiotemporal Stability Criterion (SSC), which precisely identifies the minimal subset of active tokens requiring updates. Extensive experiments demonstrate that Fast3Dcache significantly reduces the FLOPs while strictly preserving geometric fidelity, offering robust and efficient solution for high-quality 3D generation."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. ToarXiv preprint ken merging: Your vit but arXiv:2210.09461, 2022. 2 faster. [3] Fuhan Cai, Yong Guo, Jie Li, Wenbo Li, Xiangzhong Fang, and Jian Chen. Fastflux: Pruning flux with blockarXiv preprint wise replacement and sandwich training. arXiv:2506.10035, 2025. 2 [4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF International Conference on Computer Vision (ICCV), pages 2224622256, 2023. 2 [5] Zhiyuan Chen, Keyi Li, Yifan Jia, Le Ye, and Yufei Ma. Accelerating diffusion transformer via increment-calibrated caching with channel-aware singular value decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1801118020, 2025. 1, [6] Quan Dao, Hao Phung, Trung Tuan Dao, Dimitris Metaxas, and Anh Tran. Self-corrected flow distillation for consistent one-step and few-step image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 26542662, 2025. 2 [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 2 [8] Xin Ding, Lei Yu, Xin Li, Zhijun Tu, Hanting Chen, Jie Hu, and Zhibo Chen. Rass: Improving denoising diffusion samplers with reinforced active sampling scheduler. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1292312933, 2025. 2 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [10] Zhentao Fan, Zongzuo Wang, and Weiwei Zhang. Taocache: Structure-maintained video generation acceleration. arXiv preprint arXiv:2508.08978, 2025. 2 [11] Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, et al. Hicache: Training-free acceleration of diffusion models via hermite polynomial-based feature caching. arXiv preprint arXiv:2508.16984, 2025. 2 [12] Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. Sparseflex: High-resolution and arbitrary-topology 3d shape modeling. arXiv preprint arXiv:2503.21732, 2025. 2 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [15] Wongi Jeong, Kyungryeol Lee, Hoigi Seo, and Se Young Chun. Upsample what matters: Region-adaptive latent sampling for accelerated diffusion transformers. arXiv preprint arXiv:2507.08422, 2025. 6 [16] Myunsoo Kim, Donghyeon Ki, Seong-Woong Shim, and Byung-Jun Lee. Adaptive non-uniform timestep sampling for accelerating diffusion model training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25132522, 2025. 2 [17] Yeongmin Kim, Sotiris Anagnostidis, Yuming Du, Edgar Schonfeld, Jonas Kohler, Markos Georgopoulos, Albert Pumarola, Ali Thabet, and Artsiom Sanakoyeu. AutoregresIn Proceedings sive distillation of diffusion transformers. of the Computer Vision and Pattern Recognition Conference, pages 1574515756, 2025. [18] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1 [19] Sangyun Lee, Zinan Lin, and Giulia Fanti. Improving the training of rectified flows. Advances in neural information processing systems, 37:6308263109, 2024. 2 [20] Ruining Li, Chuanxia Zheng, Christian Rupprecht, and Andrea Vedaldi. Dso: Aligning 3d generators with simuarXiv preprint lation feedback for physical soundness. arXiv:2503.22677, 2025. 2, 6, 7 [21] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman3d: High-fidelity mesh generation with 3d native genarXiv preprint eration and interactive geometry refiner. arXiv:2405.14979, 2024. 2 [22] Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, and Cindy Le. Objaverse++: Curated 3d object dataset with quality annotations. arXiv preprint arXiv:2504.07334, 2025. [23] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution In IEEE Conference on Comtext-to-3d content creation. puter Vision and Pattern Recognition (CVPR), 2023. 2 [24] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 1, 2 [25] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang 9 Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 73537363, 2025. 7, 1 [26] Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Junjie Chen, and Linfeng Zhang. From reusing to forecasting: Accelerating diffusion models with taylorseers. arXiv preprint arXiv:2503.06923, 2025. 1, 2 [27] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36:2222622246, 2023. [28] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 1, 2 [29] Xuejie Liu, Anji Liu, Guy Van den Broeck, and Yitao Liang. Plug-and-play context feature reuse for efficient masked generation. arXiv preprint arXiv:2505.19089, 2025. 1, 2 [30] Ziming Liu, Yifan Yang, Chengruidong Zhang, Yiqi Zhang, Lili Qiu, Yang You, and Yuqing Yang. Regionadaptive sampling for diffusion transformers. arXiv preprint arXiv:2502.10389, 2025. 6, 7 [31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in neural information processing systems, 35:5775 5787, 2022. 2 [32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pages 122, 2025. 2 [33] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee Wong. Fastercache: Training-free video diffusion model acceleration with high quality. arXiv preprint arXiv:2410.19355, 2024. 1, 2 [34] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: In Proceedings of Accelerating diffusion models for free. the IEEE/CVF conference on computer vision and pattern recognition, pages 1576215772, 2024. 1, 2 [35] Xinyin Ma, Runpeng Yu, Songhua Liu, Gongfan Fang, and Xinchao Wang. Diffusion model is effectively its own teacher. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1290112911, 2025. 2 [36] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any object from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 84468455, 2023. [37] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1429714306, 2023. 2 [38] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. 2 [39] Sucheng Ren, Qihang Yu, Ju He, Alan Yuille, and LiangChieh Chen. Grouping first, attending smartly: Trainingfree acceleration for diffusion transformers. arXiv preprint arXiv:2505.14687, 2025. 2 [40] Nuri Ryu, Jiyun Won, Jooeun Son, Minsu Gong, Joo-Haeng Lee, and Sunghyun Cho. Elevating 3d models: High-quality texture and geometry refinement from low-quality model. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 2 [41] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. [42] Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. 1, 2 [43] Huiyang Shao, Xin Xia, Yuhong Yang, Yuxi Ren, Xing Wang, and Xuefeng Xiao. Rayflow: Instance-aware diffusion acceleration via adaptive flow trajectories. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1811318123, 2025. 2 [44] Xuan Shen, Chenxia Han, Yufa Zhou, Yanyue Xie, Yifan Gong, Quanyi Wang, Yiwei Wang, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Draftattention: Fast video diffusion via low-resolution attention guidance. arXiv preprint arXiv:2505.14708, 2025. 2 [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. arXiv preprint Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1 [46] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 2 [47] Stefan Stojanov, Anh Thai, and James Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 17981808, 2021. 6 [48] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior, 2023. 2 [49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In The Twelfth International Conference on Learning Representations, 2024. 2 [50] Tencent Hunyuan3D Team. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation, 2024. 2 [51] Tencent Hunyuan3D Team. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation, 2025. 1, 2 [52] Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, and Bin Cui. Training-free diffusion acceleration with bottleneck sampling. arXiv preprint arXiv:2503.18940, 2025. [53] Jintao Tong, Wenwei Jin, Pengda Qin, Anqi Li, Yixiong Zou, Yuhong Li, Yuhua Li, and Ruixuan Li. Flowcut: Rethinking redundancy via information flow for efficient visionlanguage models. arXiv preprint arXiv:2505.19536, 2025. 2 10 [66] Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu, and Guosheng Lin. Learn to optimize denoising scores: unified and improved diffusion prior for 3d generation. ECCV 2024, 2024. 2 [67] Xingyi Yang, Songhua Liu, and Xinchao Wang. Hash3d: Training-free acceleration for 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2148121491, 2025. 2 [68] Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, YanPei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. arXiv preprint arXiv:2507.06165, 2025. 2 [69] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. arXiv preprint arXiv:2503.22236, 2025. [70] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In CVPR, 2024. 2 [71] Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, and Baining Guo. Gaussian variation field diffusion for high-fidelity video-to-4d synthesis. arXiv preprint arXiv:2507.23785, 2025. 2 [72] Hui Zhang, Tingwei Gao, Jie Shao, and Zuxuan Wu. Blockdance: Reuse structurally similar spatio-temporal features to accelerate diffusion transformers. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1289112900, 2025. 1, 2, 6 [73] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025. 2 [74] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Trans. Graph., 43(4), 2024. 2 [75] Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, and Xiang Bai. Less is enough: Training-free video diffusion acceleration via runtime-adaptive caching. arXiv preprint arXiv:2507.02860, 2025. 1, [76] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with tokenarXiv preprint arXiv:2410.05317, wise feature caching. 2024. 1, 2 [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 5 [55] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1 [56] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 2 [57] Hanyang Wang, Fangfu Liu, Jiawei Chi, and Yueqi Duan. Videoscene: Distilling video diffusion model to generate 3d scenes in one step. In 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16475 16485. IEEE, 2025. 2 [58] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convoIn European conference on lutional reconstruction model. computer vision, pages 5774. Springer, 2024. [59] Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. 1, 2 [60] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. Advances in Neural Information Processing Systems, 37:121859121881, 2024. 2 [61] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, et al. Direct3d-s2: Gigascale 3d generation arXiv preprint made easy with spatial sparse attention. arXiv:2505.17412, 2025. 1, 2 [62] Tianhao Wu, Chuanxia Zheng, Frank Guan, Andrea Vedaldi, and Tat-Jen Cham. Amodal3r: Amodal 3d reconstruction from occluded 2d images. arXiv preprint arXiv:2503.13439, 2025. 2 [63] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2146921480, 2025. 1, 2, 3, 6, 7, 4 [64] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. [65] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025. 2 11 Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration"
        },
        {
            "title": "Appendix",
            "content": "Table 5. Full quantitative results of Fast3Dcache combined with modality-agnostic method. Our combined methods obtain superior results in terms of speed and geometry quality. Table 6. Ablation study of the hyperparameter ξ. The CD of ξ = 0.7 and F-Score of ξ = 0.9 is better than those of other In default hyperparameter in our method, ξ = 0.7 is strategy. chosen. Throughput (iters/s) CD F-Score Method Vanilla 0.51 Teacache Teacache + ours 1.45 (2.84) 1.74 (3.41) Easycache Easycache + ours 1.95 (3.82) 5.27 (10.33) 0. 54.8244 0.0705 0.0701 0.0692 0.0694 53.5567 53.9420 54.5051 54.7722 Strategy"
        },
        {
            "title": "Full sample",
            "content": "CD F-Score 0.0699 54.0608 Aggressive ( ξ = 0.7 ) Aggressive ( ξ = 0.8 ) Aggressive ( ξ = 0.9 ) 0.0697 0.0698 0. 54.0900 54.0776 54.1517 7. More Results 7.1. Full Results of Complementarity with Modality-Agnostic Accelerators To evaluate the extensibility of our approach, we integrated Fast3Dcache with existing state-of-the-art acceleration methods, specifically TeaCache [25] and EasyCache [75]. As presented in Table 5, the combination yields substantial performance gains. For TeaCache, integrating our method boosts the speedup to 3.41 while simultaneously surpassing the geometric quality of the standalone baseline. The results are even more pronounced with EasyCache, where the combined framework achieves remarkable 10.33 acceleration while maintaining high F-Score (54.77). These findings confirm that Fast3Dcache is orthogonal to existing caching strategies, enabling compounding efficiency improvements without compromising generation quality. 7.2. More Ablation Study More ablation studies are conducted in Phase 3, including the fixed sampling ratio ξ in Table 6 and full sampling step fcorr in Table 7. Combined with the ablation studies in ξ and fcorr, we determine the default hyperparameters of our Fast3Dcache method: µ = 0.07, ω = 0.7, τ = 3, ξ = 0.7, fcorr = 3, ρa = 0.2, ρCFG-OFF = 0.75. The hyperparameters can be controlled flexibly based on users requirement of quality and speed trade-off. 8. More Visualizations 8.1. More Visualizations of Voxel Dynamics To further validate the universality of our PCSC design, we present extended visualizations of voxel evolution across diverse input prompts in Fig. 5. Consistent with our primary findings, Phase 2 exhibits stable decay in dynamic voxels across all test cases. These empirical results strongly corTable 7. Ablation study of the hyperparameter fcorr. Based on our observations of visualization results, fcorr = 0 does not obtain better quality although it gets better metrics of CD and FScore. And the FLOPs of fcorr = 0 is more than that of fcorr = 3. After careful consideration of balancing speed and quality, we set fcorr = 3 as the default parameter of Fast3Dcache. Strategy FLOPs (T) CD F-Score fcorr = 0 (w.o. fcorr) fcorr = 1 (Full sampling) fcorr = 2 fcorr = 3 138. 155.5 146.7 115.4 0.0695 54.1603 0.0700 0.0698 0.0697 53.9062 54.0241 54.0900 roborate the efficacy of using log-linear approximation to predict the caching budget. 8.2. More Visualizations of Velocity Field In Fig. 8 - 11, we visualize more velocity field and acceleration field feat maps in St to form the observations. The laws are similar with the case in the body of paper and we can leverage these observations to select active voxels during inference. 8.3. More Visualizations of Generation More visualizations are demonstrated in this section, including Fig. 12 and 13. The prompts are mainly from examples in TRELLIS [63], including human, building, normal object, animal and creative object. The outcomes of different parameters of τ obtain the best visualization quality of geometry and our combination methods still maintain high level of quality. 9. Impact of Sampling Parameters on Voxel"
        },
        {
            "title": "Dynamics",
            "content": "In Fig. 6, we investigate the influence of the shifting factor η and the Classifier-Free Guidance (CFG) interval on the 1 ment stage, resulting in precipitous drop in the number of active voxels. The parameter η determines the precise step index where this transition occurs. We verify this theoretically and empirically: Uniform Schedule (η = 1): The transition occurs at the midpoint of the inference process (Fig. 6a). Conversely, applying continuous CFG (t [0, 1]) eliminates the sharp drop entirely (Fig. 6b). Shifted Schedule (η = 2): Solving for the cutoff = 0.5 yields tℓ = 1/3. In 25-step inference process, this shifts the turning point to step 25(11/3) = 17, consistent with Fig. 6c. Shifted Schedule (η = 3): This setting further delays the refinement stage, as observed in Fig. 6e. Consequently, the choice of η and the CFG interval directly dictates the duration of the stabilization phases. While our Fast3Dcache phase division is calibrated to the default TRELLIS [63] parameters (η = 3, CFG [0.5, 1]), the framework remains inherently flexible and can be adapted to arbitrary user-defined schedules. 10. Implement of Calculated Quantity and Geometry Quality Fig. 7 illustrates the counter-intuitive phenomenon observed in our quantitative results: higher computational cost does not strictly correlate with superior generation quality. Interestingly, we observe that lower sampling ratios can yield better geometric metrics (lower CD, higher F-Score) in certain regimes. This empirical evidence underscores the feasibility of our approach, demonstrating that it is possible to achieve significant acceleration while simultaneously maintaining or even enhancing geometric fidelity. 11. Algorithm of Fast3Dcache The details of the Fast3Dcache algorithm are presented in Algorithm 1. 12. FLOPs Calculation For the metric FLOPs, we mainly calculate the floating point operations inside flow transformer blocks because the computational workload of other modules is significantly less than that within the flow transformer. 1. Modulation (conditional): FLOPsMod-Block 5 Dmodel + 2 Dmodel (6Dmodel). 2. LayerNorm 1: FLOPsLN-Block 7 Ntok Dmodel. Figure 5. More visualizations of dynamic voxels in inferences of different cases. Phase 1 is unstable which is implemented that the outline is being formed. In Phase 2, the number of dynamic voxels starts to decrease and can be predicted via PCSC. Despite the fluctuations in the downward trend during the second phase, the experimental results confirm that the log-linear approximation is acceptable. Phase 3 is also CFG Off Phase. generation process. Our experiments reveal that voxel stabilization dynamics are highly sensitive to these sampling configurations. Standard Flow Matching implementations typically apply CFG during the interval [0.5, 1] to ensure the initial generation adheres closely to the condition c. To optimize this process, non-uniform time schedule is introduced via the shifting factor η: = η tℓ 1 + (η 1) tℓ , (8) where tℓ denotes the original timestep in uniform schedule. As illustrated in Fig. 6, setting η > 1 biases the sampling density, allocating more inference steps to the initial stages governed by CFG. Crucially, the termination of CFG guidance triggers transition to an unconditional refine2 (a) η = 1 with CFG [0.5, 1] (b) η = 1 with CFG [0, 1] (c) η = 2 with CFG [0.5, 1] (d) η = 2 with CFG [0, 1] (e) η = 3 with CFG [0.5, 1] (f) η = 3 with CFG [0, 1] Figure 6. The trends of dynamic voxels with different shifting factors η and CFG interval. The point of CFG turning off will result in significant drop in the number of dynamic voxels. The red plots (a, c, e) correspond to TRELLISs default CFG interval [0.5, 1], where the timing of the Sharp Drop is controlled by the shifting factor η. The green plots (b, d, f) correspond to full CFG interval [0, 1]. direct comparison between the rows (a v.s. b)(c v.s. d)(e v.s. f) demonstrates that continuous CFG guidance removes the sharp drop in dynamic voxels. 3. Self-Attention: FLOPsSA 2BNtokDmodel(3Dmodel) (cid:125) (cid:124) (cid:123)(cid:122) QKV + 2BHN 2 (cid:124) tok(Dmodel/H) (cid:125) (cid:123)(cid:122) QKT + 2BHN 2 tok(Dmodel/H) (cid:125) (cid:124) + + 5BHN 2 tok (cid:125) (cid:123)(cid:122) Softmax (cid:124) (cid:123)(cid:122) Attn , model (cid:125) 2BNtokD2 (cid:124) (cid:123)(cid:122) OutProj so, FLOPsSA 8BNtokD2 5BHN 2 tok. model + 4BN 2 tokDmodel + 4. LayerNorm 2 is the same as LayerNorm 1. Figure 7. Impact of sampling ratio on geometric quality. The plot illustrates the relationship between sampling ratio (computational volume) and geometric metrics (CD and F-Score). Counterintuitively, it reveals that higher computational volume does not consistently lead to superior generation quality. For example, sampling ratio of 0.3 yields lower CD (better quality) compared to 0.4, indicating that judicious selection of sampling density perhaps improve fidelity while reducing computation. 5. Cross-Attention: FLOPsCA 2BNtokD2 (cid:124) model (cid:125) + 2BNcondDcond(2Dmodel) (cid:123)(cid:122) (cid:125) KV (cid:124) + (cid:123)(cid:122) 2BHNtokNcond(Dmodel/H) (cid:123)(cid:122) (cid:125) (cid:124) QKT + 5BHNtokNcond (cid:123)(cid:122) (cid:125) Softmax (cid:124) + 2BHNtokNcond(Dmodel/H) (cid:123)(cid:122) (cid:125) (cid:124) Attn , model (cid:125) 2BNtokD2 (cid:124) (cid:123)(cid:122) OutProj + so, FLOPsCA 4BNtokD2 4BNtokNcondDmodel + 5BHNtokNcond. 6. LayerNorm 3 is the same as LayerNorm 1. 7. FFN (MLP): model + 4BNcondD2 model + FLOPsMLP 2BNtokDmodelDmlp (cid:125) (cid:124) + 5BNtokDmlp (cid:125) (cid:123)(cid:122) Activation (cid:124) (cid:123)(cid:122) fc1 , + 2BNtokDmlpDmodel (cid:125) (cid:123)(cid:122) fc2 (cid:124) because Rmlp = 4, Dmlp = 4Dmodel, FLOPsMLP 16BNtokD2 model + 20BNtokDmodel. Above all, FLOPsBlock = FLOPsMod-Block + 3 FLOPsLN-Block + FLOPsSA + FLOPsCA + FLOPsMLP. 3 Algorithm 1 Fast3Dcache Inference Pipeline Require: Initial feature grid Sϵ, Total steps , Condition Ensure: S0: The final denoised feature grid 1: Hyperparameters: ρa, µ, τ, ω, ξ, ρCFG-OFF, fcorr 2: function SSC(vt1, vt2, ct, τ ) 3: Compute stability score Ci(t) via velocity and acceleration (weighted by ω). 4: 5: Icache Indices of top-ct tokens with the lowest Ci(t) if consecutive caches τ then Icache Error"
        },
        {
            "title": "Accumulation Elimination",
            "content": "Iactive All Indices Icache return Iactive 6: 7: 8: end function 9: Initialize St Sϵ, vcache 0 10: Pre-calculate cache budget schedule Ncache(t) using PCSC curve. 11: for 1 to do 12: tk, tprev tk+1 If the Fast3Dcache method is leveraged, then Ntok(t) = ct = Nactive(t). 13. Limitation and Future Work Our current implementation of Fast3Dcache is optimized for the sparse voxel grid representation employed by the state-of-the-art TRELLIS [63] framework. While the core principle of leveraging spatiotemporal redundancy is universally applicable, applying our specific voxel-based metrics to continuous or implicit representations (e.g. Signed Distance Fields) requires tailoring the stability criteria to those respective domains. In future work, we plan to extend this geometry-aware caching paradigm to broader spectrum of 3D representations, aiming to establish unified efficient synthesis framework across diverse modalities. Step 1: Determine Cache Budget ct if ρa then Phase 1: Full Sampling ct else if < ρCFG-OFF then Phase 2: Dynamic Caching ct Ncache(t) else Phase 3: CFG-Free Refinement krefine ρCFG-OFF if (krefine + 1) (mod fcorr) = 0 then ct 0 else ct D3 ξ end if end if Full correction step Fixed ratio caching Step 2: Token Selection & Model Inference (t) active SSC(vcache, vprev cache, ct, τ ) vactive FlowTransformer(St[I (t) active], t, c) Step 3: State Update vt vcache vt[I (t) active] vactive others Update active tokens, reuse 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: St St (t tprev) vt vprev cache vcache; vcache vt 29: 30: 31: end for 32: return 4 Figure 8. Visualization of velocity field and acceleration field feat maps in St with prompt example 1. Figure 9. Visualization of velocity field and acceleration field feat maps in St with prompt example 2. Figure 10. Visualization of velocity field and acceleration field feat maps in St with prompt example 3. 5 Figure 11. Visualization of velocity field and acceleration field feat maps in St with prompt example 4. Figure 12. More generation visualization results of different methods. 6 Figure 13. More generation visualization results of different methods."
        }
    ],
    "affiliations": [
        "AGI Lab, Westlake University"
    ]
}