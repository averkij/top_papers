{
    "paper_title": "Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations",
    "authors": [
        "Evan Miller"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 ] . s [ 1 0 4 6 0 0 . 1 1 4 2 : r Adding Error Bars to Evals: Statistical Approach to Language Model Evaluations Evan Miller Anthropic evanmiller@anthropic.com November 4, 2024 Abstract Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make number of specific recommendations for running language model evaluations and reporting experiment results in way that minimizes statistical noise and maximizes informativeness."
        },
        {
            "title": "1 Introduction",
            "content": "Language models are measured in the literature by evaluations, or evals. Evals are commonly run and reported with highest number is best mentality; industry practice is to highlight state-of-the-art (SOTA) result in bold, but not necessarily to test that result for any kind of statistical significance.[15] Chatbot Arena[4] has popularized the use of confidence intervals in its Elo scores, but error bars remain noticeably absent from traditional question-and-answer evals. One recent and notable exception is the technical report on the Llama 3 model family[7], which includes simple confidence intervals on number of evals. In this article, we seek to introduce rigorous statistical thinking into the world of language model evaluations, so that researchers may quantify the precision with which they are able to answer questions and test hypotheses using evals. After developing comprehensive analytic framework, we make specific recommendations for the computation of confidence intervals and the reporting of eval results. Using this framework, we show that the confidence intervals recently reported in [7] are likely too narrow in some cases and too wide in other cases. short hypothetical example will motivate the overall discussion. Imagine that two competing models, code-named Galleon and Dreadnought, are being considered for deployment in particular application (say, with bent toward coding and mathematical reasoning tasks). As part of the decision-making process, three popular language model evaluations are performed on the two models: MATH, mathematical reasoning eval[9]; HumanEval, Python coding eval[3]; and MGSM, multilingual eval covering grade-school math[18]. The fictional results from this hypothetical comparison are presented in Table 1. 1 Eval Model MATH HumanEval MGSM Galleon 65.5% 83.6% 75.3% Dreadnought Difference 63.0% 87.7% 78.0% +2.5% 3.1% 2.7% Table 1: Hypothetical data from two fictional models across three (non-fictional) evals On its face, the data table presents conflicting results: Galleon appears to outperform Dreadnought on MATH (65.5% vs 63.0%), but Dreadnought has performed better on HumanEval and MGSM by slightly wider margins. Is it safe to conclude from the results that Dreadnought is generally better suited for coding and mathematical tasks, given its margin of victory on two of three evals? Or should something in the data potentially give us pause? Evaluating the evaluations is complex undertaking fraught with both qualitative and quantitative considerations.[8] Remaining agnostic about the relative and qualitative merits of various evals, this article develops framework for answering quantitative questions about specific eval results. With the aim of informing holistic decision-making, we offer series of recommendations for running and reporting evals in way that enables researchers to test well-formed hypotheses about competing models, competing hyperparameters, and competing prompts. Our specific recommendations to researchers include: 1. Computing standard errors of the mean using the Central Limit Theorem 2. When questions are drawn in related groups, computing clustered standard errors 3. Reducing variance by resampling answers and by analyzing next-token probabilities 4. When two models are being compared, conducting statistical inference on the questionlevel paired differences, rather than the population-level summary statistics 5. Using power analysis to determine whether an eval (or random subsample) is capable of testing hypothesis of interest Drawing on statistical theory and the experimental design literature, we demonstrate that small number of conceptual assumptions unlocks rich theoretical landscape for researchers studying language model evaluations, and show practitioners how to conduct statistical inference on often-noisy eval data. For an overview of experiment design, we refer the reader to [11]."
        },
        {
            "title": "2 Analysis framework",
            "content": "Suppose that the questions in an eval do not represent all possible questions, but instead were drawn at random from (hypothetical, infinite, unseen) super-population of questions. This simple supposition lets us jump through the looking glass of the specific questions that appear in an eval in order to study the underlying skill that the eval is attempting to measure. We modify this assumption in Section 2.2 to study questions that may have been drawn together."
        },
        {
            "title": "2.1 Independent questions",
            "content": "More formally, suppose an eval consists of independently drawn questions. We write the score on question as si, decomposing the score into mean component xi and zero-mean random 2 # Questions MATH 5,000 HumanEval 164 MGSM 2,500 Galleon 65.5% (0.7%) 83.6% (3.2%) 75.3% (0.9%) Dreadnought 63.0% (0.7%) 86.7% (3.0%) 78.0% (0.9%) Table 2: We suggest two new reporting practices: eval, and the standard error of each estimate in parentheses (fictional models and numbers). including the number of questions in each component ϵi: si = xi + ϵi We refer to xi as the conditional mean and the variance of the random component ϵi as the conditional variance, that is, the mean and variance conditional on the selection of question in the eval. Denote this latter quantity σ2 = Var(ϵi). We also write an unconditional version of these numbers (that is, unconditional on the selection of i) as = + ϵ Let µ represent the unobserved mean value of the super-population with µ = E[s] = E[x]. We wish to conduct inference on (that is, the true mean eval score) given only the observed question scores s1, . . . , sn. Let = 1 It follows from the Law of Large Numbers that µ may be estimated using ˆµ = s, and from the Central Limit Theorem (C.L.T.) that the standard error of ˆµ can be estimated as si represent the average of the observed scores. (cid:80) SEC.L.T. = (cid:112)Var(s)/n = (cid:32) (cid:118) (cid:117) (cid:117) (cid:116) 1 1 (cid:88) (cid:33) (si s) /n (1) In the special case where si {0, 1} (a Bernoulli variable), Equation 1 becomes SEBernoulli = (cid:112)s(1 s)/n We note that it is common practice to compute the standard error of the mean by bootstrapping; see, for instance, the OpenAI evals[16] frameworks. But the Central Limit Theorem is applicable to any evals having scores with finite variance and large number of questions, and so we regard bootstrapping as unnecessary unless complicated sampling scheme or estimator is being used. We also note that [7] incorrectly estimates all of its standard errors using SEBernoulli, even when si takes fractional values, such as with an F1 score. In these cases, SEBernoulli will tend to be conservative (too wide) compared to SEC.L.T.. The Inspect framework[12] correctly computes SEC.L.T. with its built-in stderr() metric. (2) We suggest reporting the standard error of the mean alongside (beneath) the mean when reporting eval scores. common practice in other sciences is to report the standard error in parentheses; we suggest emulating this practice. See Table 2 for an example. 95% confidence interval may be computed from such table as CI 95% = 1.96 SEC.L.T. (3) 3 # Questions # Clusters DROP 9,622 588 RACE-H 3,498 1,045 MGSM 2,500 250 Galleon 87.1 (0.8) 91.5% (0.5%) 75.3% (1.6%) Dreadnought 83.1 (0.9) 82.9% (0.7%) 78.0% (1.5%) Table 3: We suggest including the cluster count alongside the question count when reporting cluster-adjusted standard errors (fictional models and numbers). DROP RACE-H MGSM SEclustered (1.34) (0.51%) (1.62%) SEC.L.T. (0.44) (0.46%) (0.86%) Ratio 3.05 1.10 1.88 Table 4: Clustered and naive standard errors computed on two popular evals using Anthropic models (non-fictional numbers). Analyzing the same data, clustered standard errors can be over 3X larger than naive standard errors."
        },
        {
            "title": "2.2 Clustered questions",
            "content": "We next consider eval questions that are drawn in groups, or clusters. For instance, DROP[6], QuAC[5], RACE[13], and SQuAD[17] are reading-comprehension evals having multiple related questions about independently selected passages of text, and multilingual evals such as MGSM[18] consist of the same question translated into many languages. Because the inclusion of questions is non-independent, key assumption of the Central Limit Theorem (or bootstrap) is violated, and so naive application of Equation 1 will yield inconsistent standard errors. Here we show how to use clustered standard errors[1], technique developed in the social sciences, to account for the dependence and correlation structure present in question clusters. Let si,c denote the ith question score within cluster c, and assume that draws of clusters are independent. Continue to denote the mean observed score as s. The cluster-adjusted standard error of the mean score can be computed as SEclustered = SE2 C.L.T. + 1 (cid:88) (cid:88) (cid:88) (si,c s)(sj,c s) i j=i 1/2 (4) The clustered standard error acts as kind of sliding scale between cases where scores within cluster are perfectly correlated (in which case each cluster acts as single independent observation) and perfectly uncorrelated (in which case the clustered standard error is equivalent to the unclustered case). The intra-cluster correlations (or lack thereof) are captured by the triple summation (over clusters and cross-terms within clusters); for derivation of Equation 4, see Appendix A. suggested format for reporting cluster-adjusted standard errors is presented in Table 3, and some real-world numbers are reported in Table 4. We note that the cluster adjustment in our real-world example is far from trivial (up to 3X). Failure to adjust standard errors for clustered sampling may lead an unsuspecting analyst to suppose that the measurement of the overall eval 4 score is much more precise than it actually is. We therefore advise that the confidence intervals for reading-comprehension evals reported in [7] are likely anti-conservative (too narrow)."
        },
        {
            "title": "3 Variance reduction",
            "content": "The standard error of ˆµ quantifies the uncertainty associated with our estimate of the overall eval score. Reducing this quantity (which is the square root of the estimate variance) improves the precision of the estimate. The variance associated with ˆµ may be decomposed into two components: the variance of the conditional mean, that is, the variance associated with choosing questions from the superpopulation, and the mean conditional variance, which is the mean variance of scores associated with the questions that were chosen. This decomposition is additive, and follows from the law of total variance. Mathematically, Var(ˆµ) = Var(s)/n = (Var(x) + E[σ2 ])/n This equation has few implications: the simplest way to reduce the variance of ˆµ is to increase n, the number of sampled questions. The variance of the conditional mean, Var(x), is property of the super-population and therefore immutable; but we have couple of strategies available for reducing the overall variance via the expected conditional variance, E[σ2 ]."
        },
        {
            "title": "3.1 Resampling",
            "content": "The first strategy for reducing the expected conditional variance is to resample the model number of times, and to compute the standard error using the question-level mean scores from the resamples. Suppose each question is sampled (answered) times, and the score si is the mean of these answer scores. Since the conditional variances are equal for all answer scores, the overall conditional variance becomes Var(si) = σ2 /K E[σ2 This relation should clarify the issue of how many times to resample given question. Once ]/K Var(x), increasing further will have little effect on the standard error of ˆµ. We work through an example to show how to compute value for K. Suppose scores are binary (0 or 1) and question difficulty is uniformly distributed, [0, 1]. Then ϵi = 1 xi with probability xi and ϵi = xi otherwise. bit of integration reveals that Var(x) = 1/12 and E[σ2 ] = 1/6. The required relation reduces to 2, or equivalently 2/K 1. Writing the variance of this estimator with arbitrary in terms of the estimator with = 1, we have Var(ˆµK > 1) = Var(ˆµK = 1) (1 + 2/K)/ Going from = 1 (no resampling of answers) to = 2, the total variance is reduced by 1/3. Increasing to = 4, we have variance reduction of 1/2, and setting = 6, we reduce variance by 5/9. The upper limit on variance reduction via resampling in this example is 2/3. Note that computing pooled standard error across all KN answers will be inconsistent, as multiple answers to the same question would violate the assumption of independent draws. Refer to Section 2.2 for discussion of questions drawn in related groups."
        },
        {
            "title": "3.2 Next-token probabilities",
            "content": "The second strategy for reducing the expected conditional variance E[σ2 ] is to eliminate the term altogether. For language model evals that do not utilize chain-of-thought reasoning, the conditional variance can be removed by analyzing next-token probabilities, rather than evaluating the models sampled (or resampled) output. Consider for instance multiple-choice eval, and prompt that induces model to produce its answer in its first token. If correct answer is worth 1 and an incorrect answer is worth 0, and the probability of the correct token is denoted pi, then si = xi = pi and ϵi = 0. This yields Var(ˆµ) = Var(p)/n. Using the uniform-difficulty example from the previous section, next-token probabilities will reduce the variance of the estimator by 2/3 (the upper limit achievable via resampling) compared to grading single sample from each question."
        },
        {
            "title": "3.3 Don’t touch the thermostat!",
            "content": "It may be tempting to reduce the sampling temperature[10] of the model in order to reduce (or eliminate) the conditional variance. However, we advise against this practice, unless the purpose is to study the model at the new temperature. Besides altering the models behavior, adjusting the sampling temperature may simply shift the conditional variance (which can be mitigated using the two techniques above) into the variance of the conditional means (which cannot), or else reduce conditional variance by injecting bias into the estimator. Two short examples will illustrate these points. Consider single-token true/false eval where the conditional score means at = 1 are uniformly distributed, xT =1 [0, 1]. As in Section 3.1, Var(xT =1) = 1/12. But at = 0, xT =0 = 1{xT =1 > 0.5} and the uniform distribution is rounded into Bernoulli distribution with = 1/2. So Var(xT =0) = 1/4. In this case, reducing the sampling temperature, and thereby eliminating the conditional variance, has inadvertently tripled the minimum variance in the score data from 1/12 to 1/4. In the above case, E[xT =1] = E[xT =0], but this does not always hold. Consider similar (single-token, true/false) case where xT =1 [1/3, 1] and (as consequence) xT =0 is rounded to Bernoulli distribution with = 1/4. Then E[xT =1] = 2/3 < E[xT =0] = 3/4 and Var(xT =1) = 1/27 Var(xT =0) = 3/16; that is, not only has the temperature change shifted the expected score, but the variance of the conditional means has increased approximately five-fold. We therefore recommend two-pronged variance-reduction strategy. When next-token probabilities are available, and the language model eval can be conducted using next-token probabilities (i.e. without token generation), compute the expected score for each question, and compute the standard error of expected scores across questions. When next-token probabilities are not available, or the answer requires chain of thought or other complex interaction, choose such that E[σ2 ]/K Var(x) and compute the standard error across question-level mean scores. In neither case should the sampling temperature be adjusted for the sake of reducing variance in the scores."
        },
        {
            "title": "4 Comparing models",
            "content": "Thus far we have only analyzed the standard error of eval scores considered in isolation. But particular models score on given eval usually does not have any inherent meaning; it primarily makes sense in relation to the scores of other models. In this section we provide formulas for 6 comparing the scores of two models so that an analyst might determine if model is outperforming another model in statistically significant way, or if the difference between two models is indistinguishable from noise."
        },
        {
            "title": "4.1 Unpaired analysis",
            "content": "We introduce model subscripts and for the remainder of the article. naive comparison between eval scores can be made by computing the difference between mean eval scores ˆµAB = ˆµA ˆµB and an associated standard error SEAB = (cid:113) SE + SE2 This two quantities can be used to compute the usual 95% confidence interval and z-score CIAB,95% = ˆµAB 1.96 SEAB zAB = ˆµAB/SEAB (5) (6) If two models independently report their eval scores and standard errors, it is possible for an analyst to test their difference for statistical significance even if the two model reports used non-identical random subsets of eval questions."
        },
        {
            "title": "4.2 Paired analysis",
            "content": "The naive comparison above misses an opportunity to reduce the standard error when two models evaluate the same set of questions. Let sAB,i = sA,i sB,i represent the difference between scores on question i, and let sAB = sA sB represent the observed average difference. Then we can estimate the standard error of the estimated difference as SEAB,paired = (cid:112)Var(sAB)/n = (cid:32) (cid:118) (cid:117) (cid:117) (cid:116) 1 1 (cid:88) (cid:33) (sAB,i sAB)2 /n (7) This revised standard error can be plugged into Equations 5 and 6 to compute confidence interval and z-score."
        },
        {
            "title": "We can compute the reduction in variance achieved with this paired differences test over the",
            "content": "unpaired test. First, write out an expression for the variance in the unpaired case Var(ˆµAB,unpaired) = (Var(sA) + Var(sB))/n and the paired case Var(ˆµAB,paired) = (Var(sA) + Var(sB) 2 Cov(sA, sB))/n Combining, and recognizing that the cross-model residuals are uncorrelated, we have Var(ˆµAB,paired) = Var(ˆµAB,unpaired) 2 Cov(xA, xB)/n 7 Eval MATH Model Galleon Dreadnought HumanEval Galleon Dreadnought Galleon Dreadnought Baseline MGSM Model Baseline +2.5% (0.7%) 3.1% (2.1%) 2.7% (1.7%) 95% Conf. Interval Correlation (+1.2%, +3.8%) (7.2%, +1.0%) (6.1%, +0.7%) 0.50 0.64 0.37 Table 5: Suggested presentation of pairwise differences, standard errors, confidence intervals, and correlation values as supplement to main results. In the fictional data above, the difference between the two models on MATH is statistically significant (the confidence interval is positive), but the differences on HumanEval and MGSM are not statistically significant at the 5% level. We can thus reduce the variance with paired differences as long as the conditional means of the model scores are correlated; that is to say, if the two models have some amount of agreement on which questions are easy and which questions are hard. short calculation will demonstrate the degree of variance reduction that might be expected. Suppose an eval uses next-token probabilities to form continuous scores with zero conditional variance, and that these scores are uniformly distributed for two models over the [0, 1] interval. Suppose that the model scores have correlation coefficient of 0.5. Then Var(sA) = Var(sB) = 1/12 and Cov(xA, xB) = 0.5(cid:112)Var(sA)Var(sB) = 1/24. In this case, using paired differences will reduce the variance of the estimator by 1/3 in relative terms (that is, from 1/6 to 1/9 in absolute terms). Because eval question scores are likely to be positively correlated, even across unrelated models, paired differences represent free reduction in estimator variance when comparing two models. We therefore recommend using the paired version of the standard error estimate wherever practicable. We encourage authors of technical reports to include pairwise differences, pairwise standard errors, and score correlations whenever two or more models are being evaluated. Pairwise standard errors may be computed either directly on the differences, or using the single-sample standard errors, the Pearson product-moment correlation, and the relation SEAB,paired = (cid:113) SE2 + SE2 2 SEASEBCorr(sA, sB) clustered version of the standard error, appropriate for DROP, QuAC, RACE, SQuAD, MGSM, and other evals where questions are drawn in related groups, is directly computable from the differences as SEAB,paired,clustered = 1 (cid:88) (cid:88) (cid:88) (sAB,i,c sAB)(sAB,j,c sAB) (8) 1/2 where sAB,i,c denotes the score difference on the ith question within cluster c. suggested table format for reporting pairwise results is provided in Table 5. 95% confidence interval on model differences may be computed from the base estimate and standard error, as in Equation 3. We now possess the analytic tools needed to rigorously answer the questions posed in the Introduction. Using pairwise analysis on all three evals, and ensuring that standard errors were appropriately clustered on MSGM, the numbers in Table 5 would lead us to conclude that the Galleon indeed outperformed Dreadnought on MATH in statistically significant way but that the differences on HumanEval and MGSM are indistinguishable from statistical noise. In 8 other words, while superficial reading of the eval data might have originally tempted us to conclude that Dreadnought was the overall better-performing model, closer examination of the data would tend to lead the careful analyst to the opposite conclusion."
        },
        {
            "title": "5 Power analysis",
            "content": "Power refers to the ability of an experiment to make measurement of interest in the presence of statistical noise.[14] In the context of language model evals, we may wish to know whether model represents an improvement of some magnitude over another model.[2] Due to the variance implied by sampling questions from the super-population (plus the conditional variance after the questions are chosen), power must always be defined in terms of probability. In this section we present sample-size formula needed to conduct power analysis for language model evals, and apply it in worked example to answer the empirical question posed in Section 1. The sample-size formula describing the relationship between the hypothesized difference between two models and the number of questions included in an experiment ought to prove useful in several ways. Consumers of existing evals may use the formula to determine the number of questions to subsample from large eval, or to determine an appropriate value of defined in Section 3.1. If the number of questions in the eval is fixed, consumers can calculate the Minimum Detectable Effect and decide whether the eval is worth running. The authors of new evals may use the formula to decide how many questions should be commissioned. The inputs into the sample-size formula include: Significance level α, which represents the Type error rate under the null hypothesis Power level 1β, where β represents the Type II error rate under the alternative hypothesis Minimum Detectable Effect δ, which represents the mean score difference between two models under the alternative hypothesis To simplify notation, let ω2 = Var(xA) + ar(xB) 2Cov(xA, xB) σ2 = E[σ2 = E[σ2 σ2 A,i] B,i] Let zp represent the (1 p)th percentile of standard normal distribution. We assume paired analysis described in Section 4.2, and that answers will be sampled KA times from model and KB times from model (in the simplest case, KA = KB = 1). Then the number of independent questions required to achieve Type error rate α and Type II error rate β with given Minimum Detectable Effect δ is = (zα/2 + zβ)2(ω2 + σ A/KA + σ2 B/KB)/δ2 (9) The quantities ω2, σ2 A, and σ2 may be estimated from previous eval data. short derivation of the above formula is presented in Appendix B. As simple example, suppose σ2 = 0 and ω2 = 1/9, following the conditions described in Section 4.2. Suppose we wish to detect an absolute difference of δ = 0.03 at least 80% of the time (β = 0.20) with false-positive rate of 5% (α = 0.05). Then the eval will need to contain at least = σ2 = (z0.025 + z0.20)2(1/9)/(0.03)2 969 independent questions. Although these parameters are fictional, they are reasonable, and suggest that new evals should contain at least 1,000 questions in order to have good signaling ability. If the number of questions is fixed, and the practitioner wishes to know the Minimum Detectable Effect associated with n, Equation 9 is easily inverted as δ = (zα/2 + zB) (ω2 + σ2 A/KA + σ B/KB)/n (cid:113) (10) The above equation may be used, for instance, to predict the effect of increasing the perquestion sample counts KA and KB on the Minimum Detectable Effect in nondeterministic eval. Suppose that σ2 = 1/6 and ω2 = 1/9, following the conditions of Section 3.1 with an additional assumption that Corr(xA, xB) = 0.5. Suppose = 198, α = 0.05, and β = 0.20. It follows from Equation 10 that increasing KA = KB from 1 to 10 reduces the Minimum Detectable Effect from 13.2% to 7.5%. = σ2 Cluster-adjusted versions of Equations 9 and 10 are included in Appendix C."
        },
        {
            "title": "6 Conclusion",
            "content": "This article has presented statistical treatment of language model evaluations, drawing heavily from existing literature in experiment design. For single-model analysis, we presented analytic formulas for naive and clustered standard errors, and for two-model analysis, we presented formulas for unpaired, paired, and pairedand-clustered standard errors. We recommended several techniques for reducing the variance of estimates, including resampling answers, analyzing next-token probabilities, and computing question-level differences between models, and advised against adjusting the sampling temperature for the sake of variance reduction. We suggest that practitioners include standard errors of their eval scores in their technical reports, and also include pairwise differences, pairwise standard errors, and score correlations when multiple models are being compared. We presented sample-size formula so that model evaluators can determine in advance the size of difference that may be reliably detected between two models on given eval using given resampling strategy. Experiment design represents large and venerable literature. We hope that with proper statistical tools, such as those presented in this article, machine learning practitioners will think of their model evaluations as informative experiments rather than series of contests to produce the largest number. We encourage researchers to continue exploring statistical techniques found in other experimental fields in order to further enrich our shared understanding of language models and their capabilities."
        },
        {
            "title": "References",
            "content": "[1] Alberto Abadie et al. When should you adjust standard errors for clustering? In: The Quarterly Journal of Economics 138.1 (Oct. 2022), pp. 135. issn: 0033-5533. doi: 10. 1093/qje/qjac038. eprint: https://academic.oup.com/qje/article-pdf/138/1/1/ 47915437/qjac038.pdf. url: https://doi.org/10.1093/qje/qjac038. [2] Dallas Card et al. With Little Power Comes Great Responsibility. 2020. arXiv: 2010.06595 [cs.CL]. url: https://arxiv.org/abs/2010.06595. 10 [3] Mark Chen et al. Evaluating Large Language Models Trained on Code. 2021. arXiv: 2107. 03374 [cs.LG]. url: https://arxiv.org/abs/2107.03374. [4] Wei-Lin Chiang et al. Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. 2024. arXiv: 2403.04132 [cs.AI]. [5] Eunsol Choi et al. QuAC : Question Answering in Context. 2018. arXiv: 1808 . 07036 [cs.CL]. url: https://arxiv.org/abs/1808.07036. [6] Dheeru Dua et al. DROP: Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Ed. by Jill Burstein, Christy Doran, and Thamar Solorio. Minneapolis, Minnesota: Association for Computational Linguistics, June 2019, pp. 23682378. doi: 10.18653/v1/N191246. url: https://aclanthology.org/N191246. [7] Abhimanyu Dubey et al. The Llama 3 Herd of Models. 2024. arXiv: 2407.21783 [cs.AI]. url: https://arxiv.org/abs/2407.21783. [8] Deep Ganguli et al. Challenges in evaluating AI systems. Oct. 4, 2023. url: https:// www.anthropic.com/index/evaluating-ai-systems. [9] Dan Hendrycks et al. Measuring Mathematical Problem Solving With the MATH Dataset. 2021. arXiv: 2103.03874 [cs.LG]. url: https://arxiv.org/abs/2103.03874. [10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in Neural Network. 2015. arXiv: 1503.02531 [stat.ML]. url: https://arxiv.org/abs/1503.02531. [11] Guido W. Imbens and Donald B. Rubin. Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press, 2015. [12] Inspect: An open-source framework for large language model evaluations. https://inspect. ai-safety-institute.org.uk/. Accessed: 2024-09-03. [13] Guokun Lai et al. RACE: Large-scale ReAding Comprehension Dataset From Examinations. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Ed. by Martha Palmer, Rebecca Hwa, and Sebastian Riedel. Copenhagen, Denmark: Association for Computational Linguistics, Sept. 2017, pp. 785794. doi: 10.18653/v1/D17-1082. url: https://aclanthology.org/D17-1082. [14] John List, Sally Sadoff, and Mathis Wagner. So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design. Working Paper 15701. National Bureau of Economic Research, Jan. 2010. doi: 10.3386/w15701. url: http: //www.nber.org/papers/w15701. [15] Lovish Madaan et al. Quantifying Variance in Evaluation Benchmarks. 2024. arXiv: 2406. 10229 [cs.LG]. url: https://arxiv.org/abs/2406.10229. [16] OpenAI Evals. https://github.com/openai/evals. Accessed: 2024-09-03. [17] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know What You Dont Know: Unanswerable Questions for SQuAD. 2018. arXiv: 1806.03822 [cs.CL]. url: https://arxiv. org/abs/1806.03822. [18] Freda Shi et al. Language Models are Multilingual Chain-of-Thought Reasoners. 2022. arXiv: 2210.03057 [cs.CL]. url: https://arxiv.org/abs/2210.03057."
        },
        {
            "title": "A Clustered standard errors",
            "content": "We approach the problem with linear regression. Let si,c denote the ith of nc question scores within cluster c, decomposed into mean and random component as si,c = xi,c + ϵi,c Let δi,c = xi,c µ represent the deviation of the conditional (question-level) mean from the true mean (that is, the hypothetical mean across all questions and clusters). Then the regression can be specified as si,c = µ + δi,c + ϵi,c where ϵi,c is random component and δi,c acts as question-level fixed effect that is not separately estimated. We continue to estimate ˆµ = and denote the regression residual ui,c = si,c s. The traditional clustered standard error formula is Varclustered(ˆµ) = (X X) (cid:33) cΩXc (X X)1 (cid:32) (cid:88) where represents the full matrix of covariates, Xc represents the covariates within cluster c, and Ωc represents the residual covariance matrix within cluster c. In our application, = 1n (a vector of 1s), Xc = 1nc (a vector of nc 1s), and Ωc = ucu c. j(si,c s)(sj,c s). Plugging in ui,cuj,c = (cid:80) cΩcXc = (cid:80) (cid:80) (cid:80) Thus (X X)1 = 1/n and these values yields Varclustered(ˆµ) = (cid:88) (cid:88) (cid:88) (si,cs)(sj,cs)/n2 = (cid:88) (cid:88) (si,cs)2/n2+ (cid:88) (cid:88) (cid:88) (si,cs)(sj,cs)/n2 i c j=i Recognizing that the first term is equal to the unclustered variance, we can write Varclustered(ˆµ) = Varunclustered(ˆµ) + (cid:88) (cid:88) (cid:88) j=i (si,c s)(sj,c s)/n2 The two-sample version can be developed by analyzing the question-level score differences rather than the scores. Sample-size formula derivation Following [14], we set up the power analysis with hypothetical measurement sAB that will trigger Type error with probability α and Type II error rate with probability β. The z-scores on such measurement under the null hypothesis and the alternative hypothesis are zα/2 = sAB/ (cid:113) (ω2 + σ2 A/KA + σ B/KB)/n zβ = (δ sAB)/ (cid:113) (ω2 + σ2 A/KA + σ2 B/KB)/n Combining to eliminate sAB, we have an expression for the MDE in terms of the other variables 12 δ = (zα/2 + zB) (ω2 + σ2 A/KA + σ B/KB)/n (cid:113) Or inverting the equation, we have sample-size formula for the number of questions required to produce desired MDE δ = (zα/2 + zβ)2(ω2 + σ2 A/KA + σ B/KB)/δ2 Cluster-adjusted sample-size formula In order to account for clustered questions, the sample-size formula requires cluster-adjusted versions of ω2, σ2 B. In this section we develop formulas for estimating these quantities from previous eval data. A, and σ"
        },
        {
            "title": "Start with the clustered score variance estimator",
            "content": "Varclustered(ˆµAB) = 1 (cid:88) (cid:88) (cid:88) (sAB,i,c sAB)(sAB,j,c sAB) j we can decompose into and ϵ, Varclustered(ˆµAB) = 1 (cid:88) (cid:88) (cid:88) j (xA,i,cxB,i,c+ϵA,i,cϵB,i,csAB)(xA,j,cxB,j,c+ϵA,j,cϵB,j,csAB) which, dropping cross-terms which are zero in expectation, will reduce to Varclustered(ˆµAB) = 1 (cid:88) (cid:88) (cid:88) (xA,i,c xB,i,c sAB)(xA,j,c xB,j,c sAB) + 1 (cid:88) (cid:88) (cid:88) j ϵA,i,cϵA,j,c + 1 (cid:88) (cid:88) (cid:88) j ϵB,i,cϵB,j,c We can denote the three terms on the right-hand side as Varclustered( ˆmuAB) = ω clustered + σ2 A,clustered + σ2 B,clustered with ω2 clustered = Varclustered(xA) + Varclustered(xB) 2Covclustered(xA, xB) Varclustered(xA) = Varclustered(xB) = 1 1 (cid:88) (cid:88) (cid:88) (xA,i,c sA)(xA,j,c sA) (cid:88) (cid:88) (cid:88) (xB,i,c sB)(xB,j,c sB) j Covclustered(xA, xB) = 1 (cid:88) (cid:88) (cid:88) (xA,i,c sA)(xB,j,c sB) j 13 σ2 A,clustered = σ2 B,clustered = 1 1 (cid:88) (cid:88) (cid:88) i (cid:88) (cid:88) (cid:88) i ϵA,i,cϵA,j,c ϵB,i,cϵB,j,c These clustered versions of ω2, σ2 A, and σ can be plugged into Equations 9 and 10 without further modification. In practice, in both the clustered and non-clustered cases, the mean conditional variance and variance of conditional means will need to be estimated from previous data having 1. For the sake of completeness, we briefly walk through this estimation process. Let sM,i,c,k represent the kth score on the ith question within the cth cluster on model clustered. The and estimate ˆxM,i,c = 1 clustered mean conditional variance on model may then be estimated as k=1 sM,i,c,k. This estimate is sufficient to estimate ˆω (cid:80)K ˆσ2 M,clustered = 1 n(K 1) (cid:88) (cid:88) (cid:88) (cid:88) j (sM,i,c,k ˆxM,i,c)(sM,j,c,k ˆxM,j,c) Note that we divide by 1 instead of in order to obtain consistent variance estimator with small K. If subsample of questions is being used for variance estimation, we recommend sampling at the cluster level (i.e. drawing clusters in their entirety) in order to capture the intra-cluster variance structure."
        }
    ],
    "affiliations": [
        "Anthropic"
    ]
}