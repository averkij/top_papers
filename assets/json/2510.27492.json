{
    "paper_title": "ThinkMorph: Emergent Properties in Multimodal Interleaved Chain-of-Thought Reasoning",
    "authors": [
        "Jiawei Gu",
        "Yunzhuo Hao",
        "Huichen Will Wang",
        "Linjie Li",
        "Michael Qizhe Shieh",
        "Yejin Choi",
        "Ranjay Krishna",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes a meaningful interleaved chain of thought. We posit that text and image thoughts should function as complementary rather than isomorphic modalities that mutually advance reasoning. Guided by this principle, we build ThinkMorph, a unified model fine-tuned on approximately 24K high-quality interleaved reasoning traces spanning tasks with varying visual engagement. ThinkMorph learns to generate progressive text-image reasoning steps that concretely manipulate visual content while maintaining coherent verbal logic. It delivers large gains on vision-centric benchmarks (averaging 34.7 percent over the base model) and generalizes to out-of-domain tasks, matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits emergent multimodal intelligence, including unseen visual manipulation skills, adaptive switching between reasoning modes, and better test-time scaling through diversified multimodal thoughts. These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 2 9 4 7 2 . 0 1 5 2 : r THINKMORPH: EMERGENT PROPERTIES IN MULTIMODAL INTERLEAVED CHAIN-OF-THOUGHT REASONING Jiawei Gu,1, Yunzhuo Hao,2, Huichen Will Wang,3, Linjie Li,3, Michael Qizhe Shieh1, Yejin Choi4, Ranjay Krishna3, Yu Cheng5 1National University of Singapore, 2Zhejiang University, 3University of Washington, 4Stanford University, 5The Chinese University of Hong Kong Homepage: https://thinkmorph.github.io Code: https://github.com/ThinkMorph/ThinkMorph Models and Datasets: https://huggingface.co/ThinkMorph"
        },
        {
            "title": "ABSTRACT",
            "content": "Multimodal reasoning requires iterative coordination between language and vision, yet it remains unclear what constitutes meaningful interleaved chain of thought. We posit that text and image thoughts should function as complementary, rather than isomorphic, modalities that mutually advance reasoning. Guided by this principle, we build ThinkMorph, unified model fine-tuned on 24K highquality interleaved reasoning traces spanning tasks with varying visual engagement. ThinkMorph learns to generate progressive textimage reasoning steps that concretely manipulate visual content while maintaining coherent verbal logic. It delivers large gains on vision-centric benchmarks (averaging 34.7% over the base model) and generalizes to out-of-domain tasks, matching or surpassing larger and proprietary VLMs. Beyond performance, ThinkMorph exhibits emergent multimodal intelligence, including unseen visual manipulation skills, adaptive switching between reasoning modes, and better test-time scaling through diversified multimodal thoughts. These findings suggest promising directions for characterizing the emergent capabilities of unified models for multimodal reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal reasoning (Lin et al., 2025) is not single-pass perception task but an iterative process that interweaves language and vision reasoning. This process remains particularly challenging for current models in vision-centric domains such as spatial reasoning (Li et al., 2025c; Cai et al., 2025), where success requires moving beyond describing images toward interrogating and manipulating visual elements. While textual Chain-of-Thought (hereafter, text thought) (Wei et al., 2022) has advanced verbal reasoning, it contributes little to multimodal reasoning: models still falter when problems demand more than textual description (Hao et al., 2025; Jiang et al., 2025). These limitations motivate shift from language-driven reasoning to genuinely cross-modal reasoningmirroring the human ability to tackle complex problems through think-and-sketch strategies. To emulate such think-and-sketch behavior, researchers have explored multimodal interleaved Chain-of-Thought (hereafter, interleaved thought), yet existing approaches remain limited. Toolaugmented designs rely on external visual modules such as cropping tools (OpenAI) or specialized sketching models (Hu et al., 2024; Zhou et al., 2024), making the reasoning process indirect and brittle. Unified models (Team, 2024; Chern et al., 2024) offer more integrated alternative but have yet to yield generalizable recipe for mutual advancement between text and image reasoning. For instance, MVoT (Li et al., 2025b) introduces interleaved action representations for maze solving, Equal contribution. 1 yet its textual component is confined to simplistic action labels isomorphic (Fu et al., 2024a) to its generated images, showing little generalization beyond training domains. We posit that achieving generalizable multimodal reasoning requires treating text and images as complementary, rather than isomorphic, modalities that jointly advance reasoning. Building on this principle, we introduce ThinkMorph unified model fine-tuned on 24K interleaved traces spanning four tasks with varying levels of visual engagement, from chart highlighting to spatial path overlays (Figure 2). Each instance is meticulously designed to ensure high-quality multimodal supervision, where textual reasoning and visual manipulation progress hand-in-hand toward solutions. ThinkMorph achieves substantial gains on vision-centric tasks, averaging 34.74% improvement over its base model, with striking increases of 85.84% on Spatial Navigation and 38.75% on Jigsaw Assembly. Beyond these quantitative improvements, it provides controlled setting to examine when and how interleaved reasoning helps multimodal problem solving. Compared across reasoning modes, ThinkMorphs interleaved reasoning consistently outperforms text-only and vision-only approaches by 5.33%. Despite its modest data scale, ThinkMorph generalizes robustly to out-of-domain benchmarks, surpassing the larger InternVL3.5-38B on spatial reasoning in SAT by achieving 52.67% compared to 49.33%, and matching Gemini 2.5 Flash on MMVP perception at 80.33%. Beyond accuracy, ThinkMorph reveals emergent properties indicative of higher-level multimodal intelligence: PROPERTY 1 Unseen Visual Manipulations, where the model generates visual edits unseen during training; PROPERTY 2 Autonomous Mode Switching, where it adaptively allocates reasoning effort between modalities; and PROPERTY 3 Better Test-time Scaling with Diversified Thoughts, where it explores broader multimodal solution spaces, yielding stable accuracy gains such as +8.0% on Jigsaw Assembly. Collectively, these properties indicate that interleaved reasoning is not merely coordination mechanism but an engine for emergent behaviors, offering window into how unified models internalize and adapt multimodal problem-solving strategies. Overall, our work makes the following contributions: Systematic analysis of interleaved multimodal reasoning. We present ThinkMorph as unified framework for scalable investigation of interleaved reasoning, providing the first systematic study of when and how multimodal interleaving surpasses text-only and imageonly modes. Through carefully curated, high-quality data and the scalable generation of 24K mutually reinforcing interleaved traces, ThinkMorph achieves substantial improvements across diverse out-of-domain benchmarks while revealing the underlying dynamics that make interleaved reasoning effective and generalizable. Emergent properties in interleaved reasoning. We identify distinctive emergent behaviors that arise from interleaved reasoning, including unseen visual manipulations, autonomous switching between reasoning modes, and diversified multimodal exploration. These behaviors highlight the models ability to internalize adaptive strategies that balance symbolic and perceptual reasoning. New avenues for test-time scaling. We show that the advantages of interleaved reasoning persist and amplify during test-time scaling, enabling broader exploration of multimodal solution spaces and improved robustness across unseen domains."
        },
        {
            "title": "2 THINKMORPH: INTERLEAVED CHAIN-OF-THOUGHT GENERALIZATION",
            "content": "Let Pθ denote unified multimodal model with parameters θ. We consider multimodal question = (Qtext, Qimg) that may contain both textual and visual elements. For reasoning tasks, the model is prompted to generate sequence of intermediate tokens to reach final answer. Unlike conventional Chain-of-Thought approaches that only produce textual tokens ˆt, ThinkMorph can also generate image tokens ˆv, resulting in interleaved thoughts that combine both modalities. Formally, the thought sequence is defined as = ( ˆm1, ˆm2, . . . , ˆmn), where ˆmi Pθ(mi x, m0, ˆm1, . . . , ˆmi1) and ˆmi {ˆti, ˆvi}. In practice, while special tokens are omitted from the notation for simplicity, modality transitions are controlled via delimiter tokens. For instance, image thoughts are delimited by <image start> and <image end> tokens, enabling seamless switching between textual and visual reasoning within the sequence. 2 Figure 1: Design of Interleaved Training Data for Progressive Multimodal Reasoning Interleaved Thought Collection Advancing multimodal reasoning through interleaved thought presents foundational challenge: defining what counts as meaningful interleaving is inherently difficult. Unlike textual reasoning, visual thinking is hard to externalize, whether through language or sketches. For many visual reasoning tasks (Hao et al., 2025; Li et al., 2025c; Yin et al., 2025), humans often use arrows, rough shapes, or symbols that show relationships but not exact details. This ambiguity makes it hard to set clear criteria and to collect data at scale. To address this challenge, we construct an enriched dataset encompassing four representative tasks that demand different levels of visual engagement and cross-modal interaction, as illustrated in Figure 2. Each task supports concrete, verifiable intermediate visual thoughts grounded in specific visual manipulations. We carefully design task-specific interleaved reasoning sequences where text and images are not treated as isomorphic representations but provide complementary cues that progressively guide the reasoning process toward solution, as shown in Figure 1. The following tasks demonstrate how alternating between textual and visual tokens facilitates cross-modal reasoning: Jigsaw Assembly (Wang et al., 2025c) requires determining the correct arrangement of scrambled image patches to reconstruct the original image. To recover the patch ordering σ, the initial ˆt tokens provide piece-wise textual descriptions of each puzzle pieces local content. The subsequent ˆv tokens then visualize the re-arranged pieces according to the current ordering hypothesis σ, supplying holistic spatial context that text alone cannot capture. The final ˆt tokens perform syntactic verification of the reconstructed assembly. Spatial Navigation (Wu et al., 2024) involves finding safe route from starting point to goal on grid map, avoiding obstacles. To determine safe path π through maze, the initial ˆt tokens establish coarse global abstraction. The ˆv tokens then render the visual trajectory of π, and the final ˆt tokens articulate and verify the corresponding sequence of moves. Visual Search (Wu & Xie, 2024) involves answering question about target object in an image Qimg. To locate the target object, the initial ˆt tokens hypothesize and describe the area of interest. The ˆv tokens subsequently draw bounding box, offering an explicit visual anchor. The final ˆt tokens verbalize the objects attributes and confirm the prediction. Chart Refocus (Fu et al., 2025) requires answering question about data visualization. To do so, the initial ˆt tokens identify relevant data elements. The ˆv tokens highlight corresponding regions of interest, and the final ˆt tokens perform value extraction and computation. Task Data Source Jigsaw Assembly al., 2024), SAT (Ray et ADE20K (Zhou et al., 2017), Omni3D (Brazil et al., 2023) Spatial Navigation N/A Visual Search Visual CoT (Shao et al., 2024), GQA (Hudson & Manning, 2019), VSR (Liu et al., 2023) Count 6,000 6,000 6, Visual Manipulation Curation Steps Visualizing re-arranged pieces Newly generate questions from customized pipeline Overlaying mazes with paths highlighted with red lines and arrows Newly generated questions from customized pipeline Highlighting Regions with Red Bounding Boxes for Filtering valid (question, answer) with MLLMs + other criteria Chart Refocus ChartQA (Masry et al., 2022), Refocus (Fu et al., 2025) 6, Highlighting Regions with Red Bounding Boxes or Overlays for Filtering valid (question, answer) with MLLMs + other criteria Table 1: Summary of Questions Used for Training ThinkMorph. 3 Figure 2: ThinkMorph Overview. ThinkMorph synergistically interleaves language and vision to advance multimodal reasoning across four representative tasks (top). Beyond performance gains on inand out-ofdomain benchmarks, interleaved reasoning unlocks emergent properties (bottom). Data Synthesis Table 1 summarizes the data sources, curation pipeline, and visual manipulations used for each task. In total, we curate 24,990 questions spanning diverse domains. Questions for Jigsaw Assembly and Spatial Navigation are generated using our custom synthesis pipeline, whereas those for Visual Search and Chart Refocus are carefully curated through human-in-the-loop MLLM filtering process. For instance, in the Visual Search task, we observe many questions from existing Visual CoT datasets (e.g., GQA and VSR) are ambiguously phrased, contain incorrect answers, or highlight irrelevant objects in the solution images. To enhance quality and difficulty, we enforce constraint that the target objects bounding box must occupy between 1% and 30% of the image area. This selective filtering reduces the dataset from 144K to 6,990 high-quality questions. In addition to the interleaved traces, we derive two unimodal baselines: textual thoughts obtained by prompting GPT-4.1 to solve each task step-by-step, and visual thoughts using only the image outputs from the interleaved reasoning traces. All details are provided in Appendices B.2 and D. Training and Evaluation We adopt Bagel as our base model and train on its official implementation. As shown in Figure 1, we optimize dual objectives: Mean Squared Error (MSE) loss Limg for image tokens and negative log-likelihood loss Ltext for text tokens. Hyperparameters vary across training settings, and detailed configurations are provided in Appendix B.4. For in-domain evaluation, we use VSP-main-task (Wu et al., 2024) as the benchmark for Spatial Navigation, our constructed VisPuzzle for Jigsaw Assembly, and the official Chart Refocus (Fu et al., 2025) test set (a subset of ChartQA (Masry et al., 2022)). For out-of-domain evaluation, we further test on broad suite of vision-centric multimodal benchmarks, including VStar (Wu & Xie, 2024), BLINK (Fu et al., 2024b), MMVP (Tong et al., 2024c), SAT (Ray et al., 2024) and CV-Bench Tong et al., 2024a. Specifically, for BLINK, its subset BLINK-Jigsaw falls under the jigsaw assembly task, which differs substantially from our task VisPuzzle. We treat it as distinct metric, hereafter denoted as BLINK-J. All evaluations are conducted using the vlmevalkit framework (Duan et al., 2024) for consistency and reproducibility. For most benchmarks, we follow the frameworks original eval4 uation pipeline. For tasks where answer extraction and correctness could not be determined by exact matching, we adopt GPT-5 as an LLM-as-a-Judge. Additional details provided in Appendix B.3."
        },
        {
            "title": "3 WHEN DOES INTERLEAVING IMPROVE MULTIMODAL REASONING?",
            "content": "ThinkMorph exploits the complementarity between text and images to enable interleaved reasoning, where each modality contributes distinct yet synergistic information toward solving problem. To probe the scope and underlying mechanisms of this advantage, we ask two central questions: when does interleaved reasoning outperform unimodal approaches, and how does this advantage emerge? To answer these, we fine-tune Bagel-7B under three distinct reasoning modestext-only, visiononly, and interleavedand evaluate their performance across all tasks  (Table 2)  . Bagel-7B Text Reasoning Visual Reasoning Interleaved Reasoning Spatial Navigation Visual Search Jigsaw Assembly Chart Refocus VSP 0.83* 49.17 85.50 86.67 VStar VisPuzzle BLINK-J ChartQA MMVP 55.49 56.02 58.63 63. 35.00* 63.50 61.25 73.75 67.33 68.67 47.33 73.33 62.05 70. 81.66 73.08 79.78 76.33 73.00 82.66 Table 2: Reasoning Mode Comparison. Bagel-7B is tested under think mode (*: no-think mode for tasks where thinking prevents Bagel from generating answers). ChartQA results are the average performance on horizontal and vertical bar chart questions. : out-of-domain benchmarks. Best , second-best . Interleaved reasoning excels on vision-centric tasks. On tasks that demand sustained visual engagement, ThinkMorphs interleaved reasoning consistently outperforms all other modes. The effect is most pronounced in Spatial Navigation, where the base model nearly fails at 0.83% but interleaved reasoning reaches 86.67%, marking dramatic 85.84% improvement. Substantial gains also appear in Jigsaw Assembly, with 38.75% in-domain improvement and strong out-of-domain generalization on BLINK-J (+6.00%). For Visual Search, ThinkMorph improves performance on the out-ofdomain VStar benchmark by 8.38%. Averaged across these three vision-centric tasks, interleaved reasoning yields 34.74% improvement over the base model and surpasses the next-best mode by 5.33%, establishing it as the most effective reasoning strategy for visually grounded problems. interleaved reasoning necessary? When is Chart Refocus highlights when visual manipulation in reasoning traces is essential versus supplementary. On the in-domain ChartQA benchmark, text-only reasoning slightly outperforms interleaved reasoning (+1.88%), indicating that visual input adds little beyond text. In contrast, on the out-of-domain MMVP benchmark, interleaved reasoning generalizes better, surpassing text-only reasoning by 6.33%. This contrast clarifies both when interleaved reasoning helps and how its advantage arises. Across vision-centric tasks, interleaved reasoning works best when text and images continuously inform each other. Visual tokens enable steps that text alone cannot: in Jigsaw Assembly, re-arranged pieces reveal mismatches; in Spatial Navigation, overlaid arrows validate routes; and in Visual Search, bounding boxes pinpoint object locations. Chart Refocus, however, shows that the need for interleaving depends on task demands (Figure 3). In ChartQA, textual reasoning already identifies key elements (e.g., Austria and Norway 5 Figure 3: Visual Highlighting: Role varies from supplementary (ChartQA) to essential (MMVP). with their values), making later visual highlighting helpful but unnecessary. In MMVP, visual grounding is essential for spatial cues that text cannot express, such as confirming that the ducks beak points rightward. Overall, text-only reasoning suffices when additional visual information in the reasoning traces is redundant, but interleaved reasoning is crucial for generalizing to tasks requiring precise visual grounding or manipulation."
        },
        {
            "title": "4 EMERGENT PROPERTIES IN INTERLEAVED REASONING",
            "content": "Beyond performance improvements, interleaved reasoning also exhibits emergent properties that arise naturally during training and evaluation, showing behaviors characteristic of multimodal intelligence (see lower panel of Figure 2). EMERGENT PROPERTY 1 : Unseen Visual Manipulations The model develops accurate and meaningful visual manipulations unseen in training data when generalizing to out-of-domain multimodal tasks, actively advancing the reasoning process. We identify eight distinct types of unseen visual manipulations, with zoom-in operations being the most common. As shown in Figure 2 (lower panel) and Figure 4, these manipulations also include inpainting, multi-box generation, motion forecasting, perspective transformation, and region cropping, among others. These emergent behaviors are not rare: on some benchmarks, unseen manipulations account for up to 10% of all visual operations produced during inference. Importantly, these operations are not arbitrary artifacts but precise and task-effective visual actions that contribute directly to problem solving. For example, when asked Is the bell pepper red or yellow?, the model automatically generates zoomed-in view to better distinguish subtle color differences, which closely mirroring human visual inspection without explicit prompting. Figure 4: Examples of More Unseen Manipulations deeper analysis reveals systematic patterns underlying these behaviors. Statistical evidence shows that specific textual cues reliably trigger corresponding visual manipulations: phrases such as examine closely or focus on consistently elicit zoom-in operations, while terms like restore and reconstruct prompt image inpainting. These correlations are both consistent and contextually appropriate, suggesting principled rather than random generation. This capability originates from Bagels large-scale multimodal pretraining, which exposes the model to interleaved visualtext patterns encompassing diverse manipulation. ThinkMorphs interleaved reasoning fine-tuning then provides critical alignment by enabling the unified model to activate these manipulation skills within structured reasoning steps for problem solving. In essence, pretraining supplies the raw manipulation ability, while interleaved fine-tuning directs it toward reasoning-oriented visual behaviors. Additional examples and analyses are provided in Appendix C.2. EMERGENT PROPERTY 2 : Autonomous Mode Switching The model adaptively switches from interleaved to text-only reasoning based on task complexity, despite being trained exclusively on interleaved data. When trained solely on interleaved Chart Refocus and Jigsaw Assembly data, the model still eneralizes strongly to the out-of-domain MMVP benchmark, achieving 79.6% and 82.66% respec6 Figure 5: Autonomous Mode Switching Based on Task Complexity. tively  (Table 2)  , well above the Bagel-7B baseline of 70.33%. closer look reveals striking phenomenon. Despite being trained exclusively on interleaved traces, the model autonomously switches to text-only reasoning in 5.3% of inference cases (Figure 5). Notably, these switched instances reach 81.25% accuracy overall, 7.29% improvement over the same samples when solved using interleaved reasoning (73.96%), demonstrating that the model can selectively adapt its reasoning mode for higher effectiveness. Mode switching is task-adaptive, not arbitrary. As shown in Figure 5, the model adapts its reasoning behavior based on visual complexity. For the question Can you see stems of bananas in the image?, it maintains interleaved reasoning, generating zoomed-in view of the upper region where the stem would appear. The close-up enables clear stem identification, illustrating that continuous visual engagement remains essential when fine-grained details are critical to the solution. In contrast, for We cannot see the window on the school bus?, the model switches to pure textual reasoning, describing visible features such as the yellow paint and lettering, to infer the absence of windows. This contrast reflects form of front-loaded visual engagement: after processing the image and question, the model implicitly decides whether text alone can complete the reasoning. When the initial visual encoding captures information that text can express, it shifts to text-only reasoning for efficiency; when fine-grained cues remain unresolved, interleaved reasoning continues. This autonomous adaptation shows that interleaved training not only improves multimodal coordination but also enables models to dynamically allocate reasoning effort based on task demands, implicitly recognizing when each modality is essential versus supplementary. The result is enhanced efficiency, robustness, and flexibility across diverse task types. Further examples and analysis are provided in Appendix C.3. EMERGENT PROPERTY 3 : Better Test-Time Scaling via Diversified Thoughts Interleaved reasoning enables superior test-time scaling by generating diversified thoughts that explore broader multimodal solution spaces, delivering stable accuracy gains that consistently outperform unimodal approaches. Figure 6: Test-Time Scaling Across Reasoning Modes. Interleaved reasoning demonstrates robust scaling advantages, particularly on challenging benchmarks where unimodal approaches plateau or decline. 7 Having established the effectiveness of interleaved reasoning, we next examine more nuanced question: how do different reasoning modes scale at test time? We compare interleaved and unimodal reasoning under Best-of-N sampling across four benchmarks representing continuum of distribution shifts (Table 4, Figure 6). VSP serves as the in-domain reference. VStar shares the same task setup as VCoT but stress test on smallerscale of target objects. MMVP represents moderate shift toward general perception, containing open-ended question types similar to those in VCoT data. Finally, BLINK-J presents the most substantial deviation, with task setup distinct from Jigsaw Assembly that demands stronger compositional and multimodal adaptation. Interleaved reasoning scales more effectively, with gains amplifying under distribution shifts. Across all benchmarks, interleaved reasoning maintains consistent improvements: +5.2% on VSP, +1.0% on VStar, +0.7% on MMVP, and substantial +8.0% on BLINK-J. This peak occurs under the most demanding generalization conditions: on BLINK-J, ThinkMorph improves from 65.33% to 73.33%, while visual reasoning drops by 2.0% and text reasoning rises only 2.67%. The 10-point gap between interleaved and visual modes highlights that multimodal exploration becomes most critical when single modalities cannot generalize effectively. The scaling advantage arises from richer trajectory diversity in multimodal solution spaces. As illustrated in Figure 2 (lower panel), unimodal reasoning chains are confined to single representational spaces, whereas interleaved reasoning explores both modalities simultaneously, spanning broader search space. This multimodal exploration produces diverse reasoning trajectories that succeed on complementary subsets of problems. Under Best-of-N sampling, such diversity becomes crucial: as increases, independently sampled chains cover more regions of the solution space, greatly improving the likelihood that at least one trajectory reaches the correct answer. These results indicate that interleaved reasonings benefit extends beyond single-inference performance to testtime scaling, where trajectory diversity plays central role in discovering higher-quality solutions."
        },
        {
            "title": "5 GENERALIZATION OF INTERLEAVED REASONING",
            "content": "To extend interleaved reasoning gains to broader vision-centric tasks, we fine-tune ThinkMorph on 24K high-quality interleaved thought samples drawn from all four training tasks and evaluate it across diverse benchmarks. 5.1 RESULTS Size VSP VisPuzzle ChartQA VStar BLINK-J MMVP SAT BLINK CV-Bench - 33.50 GPT-4o - 57.33 GPT-5 Gemini 2.5 Flash - 59.33 8B 8.17 38B 20.16 7B 2.16 72B 41.83 Qwen2.5-VL InternVL3.5 Visual Understanding-only VLM 43.75 78.00 47.00 34.75 36.50 34.75 40.00 76.34 80.85 83.79 76.26 80.44 78.12 82.03 61.78 71.73 70.68 68.59 76.96 76.44 85.86 72.67 77.33 66.00 71.33 80.67 59.33 61.33 Unified Models 84.67 86.33 80.33 76.33 80.33 77.33 82. 28.00 73.30 56.00 45.33 49.33 51.33 64.67 60.28 69.86 67.49 59.60 62.65 55.92 61.91 Janus-pro Chameleon Bagel 7B 00.00 33.50 7B 00.83 30.50 7B 00.83* 35.00* 43.08 5.74 61.82 38.22 28.27 55. ThinkMorph 7B 75.83 79.00 78.10 67.02 50.67 00.67 67.33 72. 63.33 47.67 70.33 22.00 10.67 44.67 38.51 16.52 47.66 80.33 52.67 60. (vs Bagel) +75.00 +44.00 +16.28 +11.53 +4.67 +10.00 +8.00 +12.41 75.61 85.46 85.07 81.99 85.96 75.20 82. 67.83 36.52 76.03* 80.82 +4.79 Table 3: Comparison of ThinkMorph with Other Models. Bagel-7B is tested under think mode (*: no-think mode for tasks where thinking prevents Bagel from generating answers). : out-of-domain benchmarks. Baselines We evaluate ten leading models to establish strong baseline, including seven VisionLanguage Models (VLMs) and three unified multimodal models (UMMs). The VLMs tested in8 clude open-source models InternVL3.5 (8B and 38B) (Wang et al., 2025b) and Qwen2.5VL (7B and 72B) (Bai et al., 2025), as well as proprietary models GPT-4o, GPT-5, and Gemini 2.5 Flash. Analysis As shown in Table 3, two advantages stand out. (1) ThinkMorph delivers large and consistent gains over unified baselines. Compared to its base model, Bagel-7B, ThinkMorph achieves significant improvements across all benchmarks, with an average gain of 20.74% over nine diverse tasks. For instance, on BLINK, ThinkMorph improves by 12.42%, demonstrating robust interleaved reasoning that generalizes to unfamiliar task configurations. Other unified baselines, such as Janus-Pro-7B and Chameleon-7Bperform notably worse (e.g., 38.22% and 28.27% on VStar, and near-zero on SAT), whereas ThinkMorph surpasses them by margins ranging from 28.8% to 42.7%. These results indicate that interleaved training not only strengthens multimodal coordination but also enables generation and understanding to reinforce each other, yielding far more capable and generalizable unified models. (2) ThinkMorph rivals or exceeds large-scale VLMs, particularly on reasoning-intensive tasks. Despite being fine-tuned on only 24K samples, ThinkMorph achieves performance comparable to, and in several cases exceeding, models an order of magnitude larger. It outperforms Qwen2.5VL-72B by 34% on VSP and 10.67% on BLINK-J, and surpasses InternVL3.5-38B on SAT while maintaining similar 3D spatial reasoning on CV-Bench. Against proprietary systems, ThinkMorph remains highly competitive, excelling especially on reasoning-heavy evaluations: it outperforms GPT-4o by 24.67% on SAT (52.67% vs. 28.00%) and matches Gemini 2.5 Flash on general perception in MMVP (80.33%). Further qualitative examples are provided in Appendix C.1. 5.2 ADDITIONAL ANALYSIS ON EMERGENT PROPERTIES Having established three representative emergent properties, we now examine how they behave across broader vision-centric benchmarks. This analysis reveals not only their robustness but also new insights into their task-dependent characteristics. The first two properties remain consistent: unseen visual manipulations continue to emerge on out-of-domain tasks, while autonomous mode switching remains adaptive, with the model transitioning between textonly and interleaved reasoning based on task complexity on BLINK and CV-Bench. Test-time scaling behaviors vary across task types. While interleaved reasoning consistently outperforms unimodal approaches under test-time scaling (PROPERTY, 3 ), the nature of this improvement differs across tasks. We analyze ThinkMorphs scaling trends under Best-of-N sampling across diverse benchmarks (Figure 7). Two distinct scaling patterns emerge. For reasoning-intensive tasks, performance improves monotonically with larger : VStar shows the strongest gain of +5.89% at = 8, and CV-Bench follows similar trend with +2.39% increase. In contrast, perception-focused benchmarks exhibit U-shaped scaling: MMVP and BLINK-J initially decline at intermediate sampling levels, as BLINK-J drops 2.91% from = 2 to = 4, before recovering at = 8 with modest gains of +1.22% and +0.96%, respectively. These patterns indicate that the benefits of test-time scaling depend on task characteristics: reasoning-oriented benchmarks gain steadily from expanded multimodal exploration, whereas perception-heavy tasks require larger sample sizes to escape local optima and fully realize the benefits of diversified reasoning trajectories. Figure 7: Relative Improvement Exploring Mode Switching with Test-time Scaling. PROPERTY, 2 shows that the model can autonomously select between reasoning modes. To study this behavior in greater depth, we train dedicated model using balanced dataset of 24K examples spanning all four tasks, ensuring that the training data cover the three reasoning modes. Based on the results in Table 2, we use visual Figure 8: Despite being trained on interleaved reasoning traces, ThinkMorph sometimes adopts purely textual reasoning strategies on out-of-domain benchmarks. reasoning data for Spatial Navigation and text-only reasoning data for Chart Refocus, as both perform comparably to interleaved reasoning on their respective tasks. For the remaining two tasks, we adopt interleaved reasoning data, producing hybrid model that enables analysis of how multi-mode exposure influences the emergence and dynamics of mode switching under test-time scaling. We evaluate the hybrid model on three out-of-domain benchmarks: MMVP, VStar, and BLINK-J. For each benchmark, we apply test-time scaling by sampling eight responses per question. Figure 8 summarizes the resulting reasoning-mode distribution, grouping questions by the number of purely textual responses. Overall, 6.38%, 8.64%, and 1.25% of responses are purely textual on MMVP, VStar, and BLINK-J, respectively. Interestingly, performance tends to improve when the model selects to reason purely in text. On questions where ThinkMorph produces both textual and interleaved responses, textual reasoning achieves 9.75% and 1.84% higher accuracy than interleaved reasoning on MMVP and VStar, respectively, but 2.98% lower accuracy on BLINK-J. These findings suggest that mode diversity amplifies the benefits of test-time scaling: when models can flexibly switch between reasoning modes, they not only explore multiple reasoning trajectories but also alternate between modality strategies, unlocking potential for more effective and efficient scaling in future multimodal systems."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Multimodal Chain-of-Thought Explicit multimodal Chain-of-Thought (CoT) approaches can be broadly divided into two lines. The first adopts tool-augmented design (OpenAI; Zheng et al., 2025; Su et al., 2025; Zhou et al., 2025; 2024; Gao et al., 2025), in which interleaving remains indirect and fragile. The second line builds on unified models. Within this category, one direction emphasizes enhanced image generation guided by textual CoT (Chern et al., 2025; Qin et al., 2025; Huang et al., 2025), while another explores preliminary forms of interleaving. However, these attempts remain shallow. MetaMorph (Tong et al., 2024b) introduces visual thinking data but collapses into fixed textual outputs into pretraining. Zebra-CoT (Li et al., 2025a) creates large-scale interleaved dataset without effectively exploring its quality and generalization. There also exists implicit multimodal CoT research, which aims to adapt understanding-only VLMs by introducing intermediate image representations as visual tokens. Such representations include perception tokens (Bigverdi et al., 2025; Yu et al., 2025) and latent visual tokens (Yang et al., 2025), which provide additional visual cues for text-based reasoning without explicit interleaving. In summary, prior work highlights the potential of multimodal CoT. However, it leaves open the question of when multimodal CoT can extend beyond text-only and image-only CoT, specifically regarding how to achieve effective and generalizable interleaved reasoning. Multimodal Understanding and Generation Most existing works on unified multimodal models frequently report that optimizing diffusion-based generative objectives tends to degrade understanding capabilities (Team, 2024; Wang et al., 2025a) and learned representations, and vice versa, making joint training fragile and brittle. MetaMorph (Tong et al., 2024b) demonstrated that visual understanding and generation are nevertheless deeply synergistic: during training, increasing data for either capability often benefits both simultaneously. Furthermore, for generative tasks, leveraging the models deep understanding and reasoning abilities further contributes to improved image 10 generation (Pan et al., 2025; Deng et al., 2025; Yan et al., 2025; Qin et al., 2025). However, when it comes to reasoning tasks, this synergy remains elusive. We introduce ThinkMorph, unified thinking model designed to enable effective and genuinely interleaved reasoning, where visual generation actively supports and refines textual reasoning. The interleaved training allows unified models to jointly leverage their dual capacities for generation and understanding, with each reinforcing the other to deliver stronger multimodal reasoning performance. As result, we provide generalizable recipe for advancing multimodal reasoning, demonstrating that generative processes can directly enhance understanding under an interleaved Chain-of-Thought framework."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We introduced ThinkMorph, unified model that unlocks generalizable multimodal reasoning by enabling text and images to truly reinforce each other. With light interleaved fine-tuning, ThinkMorph yields large gains on vision-centric benchmarks and even matches or surpasses proprietary systems far larger in scale. More importantly, ThinkMorph reveals surprising capabilities often viewed as hallmarks of intelligence: spontaneous visual manipulation, autonomous mode switching, and diversified exploration that enhances test-time scaling. These emergent behaviors demonstrate that unified models can develop reasoning skills that go beyond what is explicitly supervised. Looking ahead, unifying and nurturing such interleaved reasoning behaviorsthrough adaptive mode selection, stronger cross-modal alignment objectives, and coherent visual-text thought integrationoffers compelling path toward more robust and human-like multimodal intelligence."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 38363845, 2025. Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: large benchmark and model for 3d object detection in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1315413164, 2023. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, et al. Has gpt-5 achieved spatial intelligence? an empirical study. arXiv preprint arXiv:2508.13142, 2025. Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 1119811201, 2024. Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations. arXiv preprint arXiv:2404.01266, 2024a. 11 Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390, 2024b. Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. Interleaved-modal chain-of-thought. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1952019529, 2025. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, et al. Interleaving reasoning for better text-toimage generation. arXiv preprint arXiv:2509.06945, 2025. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025a. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025b. Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, and Ranjay Krishna. Unfolding spatial cognition: Evaluating multimodal models on visual simulations. arXiv preprint arXiv:2506.04633, 2025c. Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, and Jitao Sang. Mind with eyes: from language reasoning to multimodal reasoning. arXiv preprint arXiv:2503.18071, 2025. Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. OpenAI. Thinking with images. https://openai.com/index/ thinking-with-images/. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 12 Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv e-prints, pp. arXiv2412, 2024. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. arXiv preprint arXiv:2403.16999, 2024. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms, 2024a. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 95689578, June 2024c. Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng Yu, Zhongyu Wei, and Jiaqi Wang. arXiv preprint Autoregressive semantic visual reconstruction helps vlms understand better. arXiv:2506.09040, 2025a. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Internvl3.5: Advancing open-source multimodal Linglin Jing, Shenglong Ye, Jie Shao, et al. models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025b. Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, and Matthew Blaschko. Jigsaw-r1: study of rule-based visual reinforcement learning with jigsaw puzzles. arXiv preprint arXiv:2505.23590, 2025c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Qiucheng Wu, Handong Zhao, Michael Saxon, Trung Bui, William Yang Wang, Yang Zhang, and Shiyu Chang. Vsp: Assessing the dual challenges of perception and reasoning in spatial planning tasks for vlms. arXiv preprint arXiv:2407.01863, 2024. Zhiyuan Yan, Kaiqing Lin, Zongjian Li, Junyan Ye, Hui Han, Zhendong Wang, Hao Liu, Bin Lin, Hao Li, Xue Xu, et al. Can understanding and generation truly benefit togetheror just coexist? arXiv preprint arXiv:2509.09666, 2025. Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. 13 Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, Saining Xie, Manling Li, Jiajun Wu, and Li Fei-Fei. Spatial mental modeling from limited views, 2025. URL https: //arxiv.org/abs/2506.21458. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Introducing visual perception token into multimodal large language model. arXiv preprint arXiv:2502.17425, 2025. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633641, 2017. Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought prompting for visual reasoning refinement in multimodal large language models. arXiv preprint arXiv:2405.13872, 2024. Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, arXiv preprint Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv:2509.01656, 2025."
        },
        {
            "title": "A OVERVIEW OF THE APPENDIX",
            "content": "This Appendix is organized as follows: Section provides detailed experimental specifications and results; Section showcases qualitative case studies across tasks and benchmarks; Section provides all prompts used to generate finetuning data."
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "B.1 TEST-TIME SCALING RESULTS = 1 = 2 = 4 ="
        },
        {
            "title": "Text Reasoning\nVisual Reasoning",
            "content": "ThinkMorph-Spatial Navigation"
        },
        {
            "title": "VStar",
            "content": "48.67 83.83 87."
        },
        {
            "title": "Text Reasoning\nVisual Reasoning",
            "content": "ThinkMorph-Visual Search 61.26 56.02 65.97 BLINK-J"
        },
        {
            "title": "Text Reasoning\nVisual Reasoning",
            "content": "ThinkMorph-Jigsaw Assembly 65.33 51.33 65."
        },
        {
            "title": "Text Reasoning\nVisual Reasoning",
            "content": "ThinkMorph-Chart Refocus 74.67 74.33 81.33 48.33 83.83 87.33 60.73 56.54 67.02 64.67 51.33 64.00 75.33 73.00 78. 51.33 88.50 90.67 63.87 58.64 67.54 67.33 52.00 70.00 78.67 74.00 82.00 56.83 91.33 92.33 63.35 61.26 67. 68.00 49.33 73.33 80.33 75.00 82.00 Table 4: Test-Time Scaling Across Reasoning Modes. Interleaved reasoning demonstrates robust scaling advantages. B.2 DETAILS ON QUESTION CONSTRUCTION AND FINETUNING DATA CURATION Jigsaw Assembly We construct scalable pipeline that converts images into multiple-choice jigsaw puzzles with two to four pieces across grid configurations (12, 21, 13, 31, and 22), presenting multiple arrangement options as answers. Two-piece jigsaw puzzles offer two arrangement options, while threeand four-piece puzzles provide four sampled arrangement options including the correct configuration. We source 6,000 images from three datasets3,300 from SAT (Ray et al., 2024), 1,900 from ADE20K (Zhou et al., 2017), and 800 from Omni3D (Brazil et al., 2023)spanning synthetic spatial scenes, real-world environments, and 3D perspectives. This yields 6,000 questions distributed evenly across the five layout configurations. To construct finetuning data, we first prompt GPT-4.1 with the original question and ground truth answer, requesting it to describe the visual content of each piece and reason about the correct assembly without revealing in its response that it was provided the answer.1 For threeand four-piece puzzles, we find that textual descriptions of 1To ensure the generated reasoning leads to the correct answer, we provide the ground truth to the model while instructing it not to reveal this information in its reasoning trace. We follow this same process for subsequent tasks but omit these details for brevity. 15 individual pieces are particularly helpful for guiding arrangement decisions, as they eliminate many implausible configurations. We then provide the original natural image and prompt the model to verify the proposed arrangement by analyzing factors such as object continuity, lighting consistency, and perspective alignment. Visual Search We begin by collecting 144k visual search problems from GQA (Hudson & Manning, 2019), VSR (Liu et al., 2023), and Open Images (Kuznetsova et al., 2020). To ensure problems are challenging while keeping target objects discernible, we filter for images whose target objects bounding box occupies 1%-30% of the total image size. After manually reviewing the problems, we observe that many problems suffer from ambiguous phrasing, incorrect answers, or misplaced bounding boxes. We distill these error patterns into prompt and develop filtering pipeline using Gemini 2.5 Pro and GPT-5 to remove questions deemed inappropriate by either model. This pipeline yields 6,990 visual search problems in total. To construct interleaved reasoning, we prompt GPT-4.1 to parse the query to identify where to place the bounding box. This is akin to how humans first map the textual query to localize the area of interest. We also provide the image with the target object highlighted and prompt the model to name the target object. Spatial Navigation We create pipeline that generates Frozen Lake navigation problems using OpenAI Gym (Brockman et al., 2016). These problems range from 33 to 66 grid sizes, with 1,500 problems generated for each size. To visualize intermediate reasoning steps, our pipeline depicts potential paths with red lines and arrows. Similar to how humans first scan the maze to identify the starting position, goal position, and hole positions, we prompt GPT-4.1 to first describe the maze layout. Then, we pass in the maze image overlaid with correct path found via A* search. Finally, we prompt the model to verify the path in the image and articulate the moves. Chart Refocus We collect chart question answering problems on horizontal and vertical bar charts originally from ChartQA (Masry et al., 2022), which are subsequently processed by Fu et al. (2025) to highlight or draw bounding boxes around areas relevant to answering the questions. To ensure that not too much of the chart is emphasized, we filter for questions whose solutions require only one highlighting or drawing operation. After manually reviewing the remaining 8.4k questions, we find that small portion contain errors in answers or highlighting, so we filter these using GPT-5. This leaves us with 8.1k questions, from which we sample 6,000 to achieve as balanced distribution as possible across highlighting and drawing operations. Similar to the visual search task, we structure our prompts so that we first ask the model to identify region of interest, then pass in the processed image with the region called attention to, and finally request the model to provide the answer given the scaffolding. B.3 EVALUATION DETAILS For answer prompting, we use the official prompts for all tasks except VSP-main, where we adopt the official prompt used in VSP for baseline models but apply our custom prompt for our trained model, provided below. VSP Custom Prompt You are maze solver. Your goal is to guide player from the start to the goal on grid map while avoiding holes. The player can move one square at time in the directions left (L), right (R), up (U), or down (D). The frozen lake is not slippery; the player will always move in the intended direction. Moving off the edge or falling into hole results in failure. Reaching the goal means success. Provide your solution as sequence of moves wrapped in boxed{}, such as boxed{L,R,U,D}. The moves should be comma-separated. For answer judging, we follow either the official judging pipelines or the standard VLMEvalkit pipeline for Vstar, VSP-main, BLINK-J, BLINK, VisPuzzle, MMVP, SAT and CV-Bench to ensure consistency and reproducibility, all excuted within the VLMEvalkit framework. SAT is evaluated under its standard circular setting. 16 For ChartQA, we first perform answer extraction with GPT-5 as an LLM-as-a-Judge using our custom prompt and then accurately match the extracted answer with the ground truth, following the official pipeline."
        },
        {
            "title": "ChartQA Answer Extraction Prompt",
            "content": "Role: You are an Answer Extraction Assistant. You are given question and models response. The response contains the final answer to the question. Task: Extract only the final answer from the response and output it. Do not include any If the final answer does not appear in the response, extra words, punctuation, or units. output: None. Rules: 1. Output only the answer itselfno explanations, labels, or extra text. 2. If the answer is numeric, remove units and extra symbols (e.g., %, currency); keep the minus sign and decimal point. Examples: [example1] Question: What is the difference in value between mutton and corn? Models response: subtract the value of corn from the value of mutton: 103.7 - 103.13 = 0.57. Therefore, the difference in value between mutton and corn is 0.57. Your output: 0.57 [example2] Question: Is the average of all bars in 55 to 64 age group greater than average of 25 to 64 age group? Models response: No Your output: No [example3] Question: How much does the value of Approve decrease from Jul 2015 to Sep 2015? Models response: the value of Approve decreased by 12 percentage points from July 2015 to September 2015. Your output: 12 Question: Models response: Your output: B.4 TRAINING AND INFERENCE DETAILS We train Bagel-7B on curated interleaved traces as unified autoregressive streams using two nodes with 16A100 80GB GPUs. In our training setup, we modify the official Bagel codebase to support both training and inference, with hyperparameters varying across different experimental settings, see in Table 5. Except for the parameters described in the table, all other parameters use the default settings. Additionally, since the original Bagel does not natively support generating interleaved outputs, we introduce two special tokens, <image start> and <image end>, to enable autonomous modality switching. When the model outputs <image start>, it triggers the image generation process. Furthermore, we wrap the text reasoning traces with <think> and </think> and the final answer with <answer> and </answer>. For inference, single-pass run uses temperature=0 with max tokens=4096, whereas under test-time compute scaling we set the temperature to 0.7 while keeping max tokens number unchanged. Table 5: Hyperparameters used in different training setting. N/A indicates that the parameter was not applicable to that stage. Hyperparameter Optimizer & Scheduler Learning Rate (LR) LR Scheduler Total Training Steps Model & Loss CE Loss Weight MSE Loss Weight Frozen Components Batching & Tokenization Max Tokens per Batch Regularization (Dropout) Text Condition Dropout ViT Condition Dropout VAE Condition Dropout Text Reaonsing Visual Reaonsing Interleaved Reaonsing ThinkMorph 1 105 Cosine Decay 3,000 1 10 1 105 Cosine Decay Cosine Decay Cosine Decay 3,000 1 105 3,000 8,000 1.0 (Implicit) N/A Generation Expert 1.0 1.0 None 1.0 1.0 None 1.0 1.0 None 10240 32768 32768 N/A N/A N/A 0 0 0.3 0 0 0.3 0.1 0.3 0."
        },
        {
            "title": "C CASE STUDY",
            "content": "C."
        },
        {
            "title": "INTERLEAVED REASONING CASES",
            "content": "Figure 9: sample correct case in BLINK Jigsaw 19 Figure 10: sample correct case in BLINK Jigsaw 20 Figure 11: sample correct case in VisPuzzle Figure 12: sample correct case in Vstar 22 Figure 13: sample correct case in VSP 23 C.2 EMERGENT MANIPULATIONS Figure 14: sample correct case with zoom-in 24 Figure 15: sample correct case with zoom-in 25 Figure 16: sample correct case with motion prediction 26 Figure 17: sample correct case with perspective shifting. 27 Figure 18: sample correct case with elimination. 28 Figure 19: sample correct case with inpainting. 29 C.3 MODE SWITCHING Figure 20: sample correct case with mode switching 30 Figure 21: sample correct case with mode switching 31 Figure 22: sample correct case with mode switching Figure 23: sample correct case with mode switching 33 Figure 24: sample incorrect case with mode switching"
        },
        {
            "title": "D PROMPTS",
            "content": "This section provides prompts for generating finetuning data for all four tasks."
        },
        {
            "title": "Visual Search Text Thought Prompt",
            "content": "System Prompt: You are given visual reasoning problem and the answer. Your task is to produce standalone, easy-to-understand explanation of how to solve the problem. Your reader will not have access to the answer like you do. Your explanation will be used as direct output to users, so it must read naturally and independently. Guidelines: - Include specific visual details about objects, their locations, colors, relationships, etc. - Make reasoning concrete and grounded in what is visible in the image - Build up logically from observations to the final answer - Do not reveal or hint that you were given the right answeryour reasoning should read as if it independently arrived at the right answer - End by stating the answer clearly User Prompt: Question: {question} Answer: {answer} Please analyze the image and provide detailed reasoning for how to arrive at this answer. Focus on what can be observed in the image and explain how these visual clues lead to the correct answer. Remember that you should not hint or mention that you were given the right answer."
        },
        {
            "title": "Visual Search Interleaved Thought Prompt",
            "content": "System Prompt: You are given visual reasoning problem consisting of: - textual question - The original image - set of reasoning steps - modified version of the image with red bounding box highlighting an item critical to solving the problem - The correct answer Your task is to produce standalone, easy-to-understand explanation of how to solve the problem. Your reader will not have access to the intermediate materials (e.g., answer, reasoning steps, or the fact that an image was modified). Your explanation will be used as direct output to users, so it must read naturally and independently. Your output must follow this structure and be formatted as JSON object: { image cot: Step-by-step reasoning that explains how to determine where the red bounding box should go in the original image. Do not reveal the final answer here. Only focus on how to derive the bounding box. Do not include details on subsequent steps, which fall into the next section., edited image analysis: Detailed explanation of how the highlighted region helps solve the question and leads to the correct answer. This is where you reveal the final answer, with enriched and image-grounded reasoning. Only provide the answer in the last sentence. } Guidelines: Part 1: image cot - Describe how to identify the key item or region in the original image that should be highlighted with red bounding box. - Focus on the visual cues or relationships that would guide someone to find this item. - Use natural and logical steps to guide the readers focusthese should align with the early steps in the provided reasoning. - You must NOT reveal or mention the answer to the question in this part. - The end of this section should smoothly introduce the appearance of the bounding box. - Make sure to include detailed descriptions and locations of items. The reasoning steps likely do not include these, but you should add them. Part 2: (implicit) - The modified image with the red bounding box will be displayed here. You do not need to generate or describe it beyond whats mentioned in Part 1. Part 3: edited image analysis - Now that the key visual element is highlighted, explain how it leads to the correct answer. - Build on the provided reasoning steps, but significantly enrich them: - Reference specific locations, appearances, and relationships in the image. - Make the reasoning concrete and visually grounded. - Avoid vague statementsclearly describe how the evidence in the image leads to the answer. - Reveal the final answer naturally at the end of this explanation. User Prompt: 36 ChartQA Text-Thought Prompt System Prompt: You are an expert in visual reasoning and chart analysis. Your goal is to provide clear, step-by-step thought process to answer given query based on visualization. User Prompt: You are provided with an image containing visualization and query about it. Your task is to generate detailed, step-by-step reasoning that leads to the correct answer for the query. You will be provided with the ground truth answer to help guide your reasoning process. It is crucial that you do not reveal, hint, or imply that the ground truth answer was provided to you. Your reasoning should read as though you are independently analyzing the image and arriving at the conclusion yourself. Your entire response should feel like an inner monologue. The query is: {query} The answer to this question is: {answer} Note that the longer your response is, the better. Try to gradually build towards the correct answer. And ensure that the answer you give is the provided answer. You do not need to emphasize the answer by wrapping it in **."
        },
        {
            "title": "ChartQA Interleaved Thought Prompt",
            "content": "System Prompt: You are an expert in visual reasoning and chart analysis. First-Round Prompt: You are provided with two images and query. Both images contain visualization. The first image contains the original visualization that is paired with the query, and the second image contains the same visualization but with red bounding box or highlight that emphasizes part(s) of the chart that helps answer the query. Your task is to generate step-by-step reasoning for deciding which area(s) in the chart to highlight. Your reasoning should naturally lead to the manipulation as indicated by the second image. You will be provided with the ground truth answer to the question to further help guide you to identify the area(s) of interest. Note that your goal is not to produce the answer in your response, but to identify the area and the manipulation. The query is: {query} The answer to this question is: {answer} Please provide your analysis as JSON object with the key image cot containing your detailed reasoning. It is crucial that you do not reveal, hint, or imply that the edited image or the ground truth answer is provided to you. Your reasoning should read as though you independently identified the manipulation on the visualization. The introduction of the manipulation should be smooth. Do not say the manipulation should be... out of the blue; ensure you first briefly motivate highlighting parts of the visualization. Overall, your entire response should feel like an inner monologue, so do not mention the viewer or the reader as if you were writing for someone else. Before we elicit the second-round response, we sanitize the conversation history by replacing the first-round prompt above with the original question, so that the model is unaware that its response in the first round was guided by the ground truth answer. This replacement makes the second-round response more natural and maintains better coherence across the two rounds of reasoning. Second-Round Prompt: Looking at this edited visualization, provide detailed reasoning to arrive at the answer for the original query. The answer to this question is: answer. Make sure this is the answer you provide at the end. am providing this to you so that you generate accurate reasoning. Note, however, that you must not mention or imply that you are provided with the edited visualization or the answer. Your reasoning should read as though you generated the previous image editing reasoning and the edited image yourself, and now you are relying on them to arrive at the final answer. Please provide your response as JSON object with the key final reasoning containing how you arrive at the answer given the edited visualization. 38 Jigsaw Puzzle Interleaved Thought Prompts Jigsaw Puzzle Text-Thought Training Data are First-Round TIT Responses System Prompt: You are an expert specializing in solving jigsaw puzzles. Your task is to solve jigsaw puzzle. You must present your entire analysis as coherent, multi-turn monologue that reads as single, independent thought process. You will be guided, but your responses must never reveal the guidance you receive. Your final output for each turn must be JSON object with the specified key. First-Round Prompt: {question} The goal is to arrive at the answer boxed{provided answer}. the first shows the separate pieces, and the second shows You are given two images: their **correct assembly**. Your task is to construct line of reasoning that explains how to arrive at the correct assembly, as if you were deducing it from the separate pieces yourself. **Crucial Instructions:** - Present your analysis as if you are solving the puzzle independently from scratch. - **Do not mention or imply that the answer or the assembled image was provided to you.** - Your whole response must be JSON object with the key image cot. - Describe the visual content of each piece in detail. - Provide compelling reasoning for your conclusion. - Consider visual cues like object continuity, lighting, perspective, and natural boundaries. - Conclude with your reasoning with the answer boxed{provided answer}. Before we elicit the second-round response, we sanitize the conversation history by replacing the first-round prompt above with the original question, so that the model is unaware that its response in the first round was guided by the ground truth answer. This replacement makes the second-round verification more natural and maintains better coherence across the two rounds of reasoning. Second-Round Prompt: Your initial analysis appears correct. As next step, you will analyze the resulting assembled image. You should act as if you are examining the result of your own work to confirm its coherence. **Crucial Instructions:** - The image provided shows the assembled result based on your reasoning. - Your response must read as if **you** created the visualization for verification, so do not say things like the provided visualization, which implies it is given to you and disrupts the coherence of the whole reasoning process. - Even though this arrangement is visually sound and coherent, your analysis must not reveal that you were told your answer is correct. Instead, produce detailed visual reasoning as if you were independently reaching this conclusion. - Your response should read as if you initiated this verification, so that it reads like seamless continuation of your thought process. - Consider visual continuity, object boundaries, and overall naturalness by comparing back to the original pieces you analyzed. - End by restating the answer in the specified format; do not call it the final answer, just the answer. - Your whole response must be JSON object with the key **edited image analysis**."
        },
        {
            "title": "Frozen Lake Interleaved Thought Prompt",
            "content": "We adopt two versions of the user prompt when generating training data with GPT-4.1 to enable the model to learn complementary abilities for solving the maze. Each prompt generates half of the training data. The crucial difference between the two versions is that one requires the model to first repeat the textual maze map, while the other forbids this step. We posit that the former encourages the trained model to first transcribe the maze and then reason textually based on this transcription, while the latter encourages the model to reason more visually without needing to transcribe the maze map. User Prompt Version 1: {question} Here is the precise maze layout and the required final answer to guide your analysis: - Maze Text Map: {formatted map} - Required Final Answer: boxed{correct path} **Very Important Instructions for Your Reasoning:** The text map and the answer are provided to you so that you can leverage them to produce accurate reasoning. Your response must be completely self-contained analysis that reads naturally to user who can only see the maze image. - **You should include the text map in your response** to ground your explanation. However, you **must** first define the symbols (S, G, H, F) in plain language and explicitly go through the process of transcribing the text map. - **Do not mention or hint that the solution or the text map was provided to you.** Your reasoning should appear to be your own independent work. - Using coordinates to aid reasoning is encouraged, as long as your reasoning is clear to user who only sees the maze image. Provide step-by-step reasoning that logically leads to the given answer. User Prompt Version 2: {question} Here is the precise maze layout and the required final answer to guide your analysis: - Maze Text Map: {formatted map} - Required Final Answer: boxed{correct path} **Very Important Instructions for Your Reasoning:** The text map and the answer are provided to you so that you can leverage them to produce accurate reasoning. Your response must be completely self-contained analysis that reads naturally to user who can only see the maze image. - **Crucially, do not repeat the text map in your response.** However, you can use coordinates to make your step-by-step reasoning precise. - Describe the start, goal, and holes in plain language (e.g., the starting square, the goal, the ice holes). - **Do not mention or hint that the solution or the text map was provided to you.** Provide step-by-step reasoning that logically leads to the given answer as if you are solving it independently."
        },
        {
            "title": "Frozen Lake Interleaved Thought Prompt",
            "content": "First-Round Prompt: {question} Here is the precise maze layout to guide your analysis: {formatted map} Legend: - = Start - = Goal - = Hole - = Frozen Surface In your response, DO NOT provide the answer to the question (i.e., the path). You will be given chance to answer it later. Now, your goal is to provide description of the whole maze, including where the starting point, the goal, and the ice holes are located. Begin by saying something to the effect of Lets first map out the maze. Do not say this verbatim though. **Important Instructions for Your Response:** The text map is provided to you so that you can accurately describe the maze. However, your output must be clear to user who only sees the maze image. - Do not mention or imply that you are given this textual maze map. - Describe the start, goal, and holes in plain language (e.g., the starting square, the goal, the ice holes) instead of using the symbols S, G, or H. - Using coordinates to describe the maze map is encouraged, as long as you clearly define everything so that user who only sees the maze image can still understand it. - Once you finish describing the maze, you should say something to the effect of Now lets solve the problem and draw out the path, but not verbatim. DO NOT end the response by repeating the rules or instructions, such as the player must go from the start to the goal or that they must avoid all holes, or with this overview, you have complete understanding of the positions of the starting square, the goal, and all ice holes in the maze. Simply end with short paraphrase of Now lets solve the problem and draw out the path. Make sure to mention the action of plotting, visualizing, or drawing. - You should not sound like you are writing this for another person. This should read like an inner monologue. Second-Round Prompt: The image above visualizes solution path in red. The path is {correct path}. Your task is to perform verification. Your response must be self-contained analysis that reads as if *you* solved the problem and created the visualization for final check, so do not say things like the provided visualization, which implies it is given to you and disrupts the coherence of the whole reasoning process. Instead, call it my solution. Visually analyze the path in the image and check if the path is correct. **Do not act as if you were responding to user or knew the correct answer beforehand.** Your initial response, the visualized path, and your next response should read like standalone, coherent solution. Visually analyze the path in the image, check if it is correct (even though you know it is), and output the correct path again in boxed{}. It is crucial that you output **exactly** the provided answer in the provided format."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Stanford University",
        "The Chinese University of Hong Kong",
        "University of Washington",
        "Zhejiang University"
    ]
}