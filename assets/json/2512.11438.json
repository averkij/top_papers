{
    "paper_title": "Flowception: Temporally Expansive Flow Matching for Video Generation",
    "authors": [
        "Tariq Berrada Ifriqi",
        "John Nguyen",
        "Karteek Alahari",
        "Jakob Verbeek",
        "Ricky T. Q. Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 3 4 1 1 . 2 1 5 2 : r Flowception: Temporally Expansive Flow Matching for Video Generation Tariq Berrada Ifriqi1,2, John Nguyen1, Karteek Alahari2, Jakob Verbeek1, Ricky Chen1 1FAIR at Meta, 2Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, France We present Flowception, novel non-autoregressive and variable-length video generation framework. Flowception learns probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation. Correspondence: First Author at tariqberrada@meta.com Figure 1 Examples of image-to-video (I2V) generation and video interpolation with Flowception . Input frames marked by dashed boundaries. Flowception enables variable-length non-autoregressive generation by learning to both denoise and insert frames in any order."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in diffusion and flow-matching have unlocked high-fidelity image generation Esser et al. (2024); Labs (2024) and are rapidly migrating to video. Current video generation models typically adopt one of two paradigms: full-sequence generation denoises all frames jointly with full attention Wan et al. (2025); HaCohen et al. (2024); Zheng et al. (2024), while temporal autoregressive (AR) generation produces frames (or blocks of frames) sequentially in left-to-right order Bruce et al. (2024); Song et al. (2025). Fullsequence models benefit from bidirectional attention, enabling the model to correct errors during de-noising and achieving superior generation quality. However, parallel denoising the entire sequence prevents real1 to-video, image-to-video, video-to-video, video interpolation, and scene completion by toggling which conditioning input frames are active (allow insertions to their right) or passive (no insertions). See Figure 1 for an illustration of image-to-video and video interpolation results generated by our model. Concretely, at each timestep the model predicts velocity field over the existing frames and per-frame insertion rate. By sampling based on the insertion rate, new frame may be inserted and initialized with sample from unit Gaussian prior distribution. The new frame is then subsequently denoised in the context of other already instantiated and partially denoised frames. This yields coupled ODEjump process over variable-length sequences and supports any-order, any-length generation by design. See Figure 2 for an illustration of the generation process. Beyond more flexible generative model, our interleaved insertion-denoising schedule delivers an efficiency gain, since early in sampling only small active subset of frames is actively denoised while unrevealed frames are marginalized out. Under linear frame insertion schedule, the computational cost of the quadratic attention term along the generation trajectory averages to third of the attention cost incurred by full-sequence model. Compared to AR with KV caching, Flowception shows comparable sampling cost with more robustness under low NFEs. We conduct extensive experiments with three datasets, Tai-Chi-HD, RealEstate10K and Kinetics 600, and consider both image-to-video generation and video interpolation tasks. We find that, beyond being more flexible framework, Flowception leads to comparable or better performance than full-sequence and autoregressive paradigms under the same compute budget, with consistent improvements in terms of FVD and VBench metrics. Contributions: 1. We introduce Flowception , theoretically grounded video generation framework, which couples frame insertions with continuous flow matching in unified model. 2. We show how Flowception can solve different tasks by conditioning on any set of frames, based on their relative order alone. 3. We present an efficiency analysis showing an average of 3 FLOPs reduction during training and (1.5 during sampling) w.r.t. full-sequence model. 4. We present extensive experimental results improving over full-sequence and autoregressive approaches on multiple datasets. Figure 2 Flowception sampling. At each iteration, the model predicts, for each frame i, velocity field and an insertion rate λi. Velocities are used to denoise frames while the insertion rates define the probability to insert new frame to the right of existing ones. The model uses per-frame time values, set to ti = 0 when they are inserted, and reaching ti = 1 when they are fully denoised. time streaming, as frames cannot be returned until fully denoised. Moreover, full-sequence models use fixed generation length and incur quadratic attention cost in the number of frames, limiting long-term generation. In contrast, AR methods enable streaming generation by making each frame immutable once generated, allowing immediate display to users while subsequent frames attend to prior outputs. However, these AR approaches suffer from critical exposure bias Zhang and Agrawala (2025); Bengio et al. (2015): training uses ground-truth frames as context, while inference conditions on the models own imperfect generations. This train-test mismatch prevents the model from learning to recover from its mistakes, causing minor artifacts to cascade as errors accumulate across frames, rapidly degrading video quality. Moreover, to enable KV caching, without which sampling from AR models becomes prohibitively expensive, AR methods typically use causal attention mask which limits the expressiveness of the models. These trade-offs hinder long-term and efficient video synthesis. In this work, we address the error accumulation and the limiting causal attention pattern of AR methods, while also avoiding the fixed-length generation requirement and reducing the computational cost of full-sequence models. To this end, we introduce Flowception , variable-length, non-autoregressive video generation framework that interleaves two processes throughout sampling: (i) continuous flow matching denoising of existing frames, and (ii) stochastic discrete insertion of frames in between existing ones. Because the model flexibly determines the insertion locations, the same model naturally performs text-"
        },
        {
            "title": "2 Related work",
            "content": "Diffusion & Flows. Diffusion (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021; Dhariwal and Nichol, 2021) and flow models (Lipman et al., 2023; Ma et al., 2024; Esser et al., 2024) are class of generative models that operate by learning to model the infinitesimal transitions of the probability path going from source (noise distribution) to target (data manifold). They are considered state-of-the-art generative modeling tools for modalities ranging from images (Rombach et al., 2022; Podell et al., 2024; Chen et al., 2024; Esser et al., 2024), audio (Wang et al., 2023; Levy et al., 2023), and video (Jin et al., 2025; Polyak et al., 2025). These models generate data by starting with simple prior such as unit Gaussian, and iteratively refining samples to recover samples from the data distribution model. The simple training recipe underlying these models proved very effective through scalability and stable training Berrada et al. (2024). Video generation. The success achieved by scaling text-to-image models Podell et al. (2024); Esser et al. (2024); Labs (2024) paved the way towards applying the same paradigms to video generation. Such works fall broadly into two categories. The first line of work Zheng et al. (2024); Peng et al. (2025); Wan et al. (2025) follows the full-sequence paradigm where the video is treated as one large tensor and all frames are denoised simultaneously. However, this results in prohibitively large attention computations, making the development of such methods prohibitive for long sequences. Another line of work Deng et al. (2024); Chen et al. (2025) takes an autoregressive approach to iteratively predict the next frame (or block of frames) conditioned on previous frames. This approach is known to have difficulty generating long videos, as sampling fixes early frames once they are generated and later frames are conditioned on (imperfectly) generated earlier frames (unlike in training, where conditioning is always based on ground-truth frames), leading to error accumulation that harms long-term generation. To resolve this, some methods rely on partial noising of the context Song et al. (2025), but this prevents KV caching, making sampling prohibitively expensive. Even without KV caching, our Flowception remains significantly faster than these autoregressive methods due to its interleaved insertion and denoising mechanism, whereas autoregressive generation is fundamentally constrained by sequential frame-by-frame generation. Another family of approaches factorizes long video generation into sparse keyframe synthesis followed by video interpolation. MovieDreamer Zhao et al. (2024) first autoregressively predicts sequence of visual keyframes and then applies an image-to-video model to synthesize short clips around each keyframe, using CLIP features of the anchor frame rather than the last generated frame to mitigate drift across clips. ARTV Weng et al. (2024) instead performs frame-wise autoregressive generation in text-image-to-video (TI2V) setting: each new frame is produced by diffusion model conditioned on both the input keyframe and the previously generated frames, with masked diffusion mechanism that selectively reuses information from the given frame to reduce appearance drift over long horizons. Several other directions have been explored to reduce complexity. These include the use of highly compressive tokenizers HaCohen et al. (2024); NVIDIA et al. (2025), hierarchical or packed representations that compress remote frames before attending to them Jin et al. (2025); Zhang and Agrawala (2025), and architectural changes that distribute or route attention over multiple context sources Fan et al. (2025); Cai et al. (2025); Song et al. (2025); Zhao et al. (2024). However, these approaches still depend on design choices about which context sources or time ranges to emphasize, rather than fully learning relevance end-to-end. To enable multiple use cases, such as image-to-video and video-to-video, some works Wan et al. (2025); HaCohen et al. (2024) concatenate frame-wise mask to the input of the model, which allows to condition on clean frames at various positions in the video. Similarly, Flowception is capable of performing various tasks by conditioning on multiple input frames, and optionally constraining insertions to be to the right of some of those."
        },
        {
            "title": "3 Flowception",
            "content": "We propose natural combination of the continuous Flow Matching (Lipman et al., 2023) and the discrete Edit Flow (Havasi et al., 2025) frameworks, with the goal of flexible video generation where existing frames are simultaneously denoised and new frames are inserted during the generation process. This is our Flowception model. We first describe the parameterization and the generation procedure for this model, then derive training scheme that adheres to the generation procedure. We will present notation loosely with high-level intuitive explanations; full derivations are in the supplementary material."
        },
        {
            "title": "3.1 Setup and model parameterization",
            "content": "Flowception operates in the space of variable-length sequences of frames = (cid:83) RnHW C, where denotes the length of each sequence, and H, W, are the fixed height, width, and channel dimensions for each frame. We denote ℓ(X) as the length of sequence . n=0 During generation, Flowception transports sequences and time values (X, t) from an initial noise distribution psrc to data distribution pdata through continuous-time framework. We handle variable length by assigning per-frame time values, collected in vector = (cid:83) n=0[0, 1]n, associating each frame to different noise level. Hence the input of Flowception model is sequence of frames and vector of per-frame time values , where and are the same length. To incorporate sequence length changes, we make use of an insertion operation (Havasi et al., 2025). Given sequence , an insertion index i, and noise frame ε (0, I), we define ins(X, i, ε) = (X 1, . . . , i, ε, i+1, . . . , n), (1) where the RHW are frames in the sequence. The insertion operation inserts 0-SNR frame which will then be denoised following the flow matching framework. Frame insertions and flows form the primitive operations that we use during generation. Hence the output of Flowception model with parameters θ has two components at every position {1, . . . , ℓ(X)}: (X, t) R0, which predicts 1. An insertion rate λθ the number of missing frames. 2. velocity vθ (X, t) RHW C, for denoising the existing frames. These two components allow the model to simultaneously denoise an existing sequence of frames and interpolate or extend the existing frame sequence. step simultaneously flows the existing frames while also potentially inserting new frames with probability given by the insertion rates. Global time value. The stopping criterion for the flow is when ti = 1 for all frames, i.e. we no longer modify any frame. To impose stopping criterion for the insertions, we introduce global time value tg which starts at tg = 0 and frame insertions are only allowed while tg < 1. Implementation-wise, we dont feed tg explicitly into the model, and is only tracked for sampling. This closely follows the design choice of OneFlow Nguyen et al. (2025) where insertion rates are not conditioned on time. Accordingly, with constant step size during inference, the maximum number of iterations is twice the amount needed for tg to arrive to 1. Scheduler. We impose distribution on the fraction of visible frames based on monotonic scheduler κ(tg) that is function of the global time value, with κ(0) = 0, κ(1) = 1. Given data sequence X1 pdata, the probability of each non-starting frame being in the sequence at global time tg is, for each frame X1, P(X in global time tg) = κ(tg). (3) We will primarily work with the linear scheduler κ(t) = t. Transport step. Given sequence with per-frame times t, each transport step performs the following two operation simulatenuosly at all positions {1, . . . , ℓ(X)}: 1. Apply the flow step = + hvθ (X, t). (4) 2. With probability κ(tg) 1κ(tg) λθ , insert new frame = ins(X, i, ε), = ins(t, i, 0). (5)"
        },
        {
            "title": "3.2 Generation procedure",
            "content": "We start the generation process by sampling fixed number of starting frames nstart, psrc(X) = (cid:81)nstart i=1 (X i; 0, I), (2) and assigning each frame the time ti = 0. We use non-zero number of starting frames to start the generation process, otherwise the initial generation steps will simply be inserting noise frames anyway. We then iteratively apply transport step until all frames are clean, i.e., all ti = 1. Each transport κ(tg) 1κ(tg) The ratio ensures insertions occur in aligment with the distribution of visible frames imposed by the scheduler. To summarize, we start with fixed number of nstart starting frames initialized as noise, advance global time value tg from 0 to 1, and perform frame insertions along the way following the scheduler κ() in Equation (3). Noise frames are inserted and then subsequently denoised, naturally producing per-frame time values ti and with delay in the time values of later-inserted frames (i.e., ti tg). We stop the 4 generation when all frames have reached ti = 1. An illustration of transport step is shown in Figure 2. pseudocode implementation is in the supplementary material."
        },
        {
            "title": "3.3 Training procedure",
            "content": "The Flowception generation procedure induces particular distribution over the visible frames and their time values. Frames may be at different time values, and some may not be inserted yet. We must align with this distribution of time values and missing frames during training so that there is no distribution mismatch between training and generation. Despite this complexity, the distribution is controlled completely by the scheduler κ. In this section, we describe simple training recipe to easily sample all time values and the states of each frame during training. Extended time values. Extended time values are denoted τ and can take values outside of the interval [0, 1], so we define clip operation that results in real time values, = clip(τ ) := max{0, min{1, τ }}. (6) Firstly, after the global time value reaches tg = 1, while no insertion of new frames can occur, existing frames may still need more steps to be fully denoised. Thus we introduce an extended global time τg [0, 2], with tg = clip(τg). Secondly, while τg 1, insertions occur according to the scheduler κ(). For the linear schedulerfor the general case, please see the detailed derivations in the supplementary materialnew frames are inserted uniformly across the time interval tg [0, 1]. Therefore, the time delay between the global extended time and per-frame extended time values follows: and so ui = τg τi Unif(0, 1), τi = τg ui, (7) where ti = clip(τi) computes the real per-frame times. Note that τi [1, 2] according to (7), When τi < 0, the frame is yet to be inserted and thus in deleted state. Figure 3 Illustration of the extended time scheduler for Flowception training. In Flowception , each frame has its own denoising time which depends on its insertion time. The global extended time τg progresses from 0 to 2, where insertion of new frames only occur when τg < 1. Starting frames (in blue) are instantiated at τg = 0, other frames (in orange) are inserted later (when τg > 0) and thus have delay. With linear scheduler, the insertion delays follow uniform distribution. During training, we sample noisy sequences using ti = clip(τi), visible = tX visible τg p(τg), τi = τg ui, = (X X1τi > 0), + (1 t)X0, X0 (0, I), 1 1 (8) (9) (10) where X1 pdata is target sequence sample. Insertion loss. At each position of the noisy sequence X, we denote ki as the number of missing frames to the right, between positions and + 1. Following OneFlow Nguyen et al. (2025), the loss for training the insertion output λi, which predicts the number of missing frames, is given by the negative log-likelihood of the Poisson distribution Lins = ℓ(X) (cid:88) i= (X, t) ki log λθ λθ (X, t). (11) Velocity loss. At each position of the noisy sequence X, we learn to denoise the frame with the standard Flow Matching loss (Lipman et al., 2023), Lvel = (cid:13) (cid:13)vθ(X, t) (X visible 1 X0)(cid:13) 2 (cid:13) . (12) Figure 3 illustrates the different frame states according to the extended time schedule. Each frame can either be in deleted state (τi < 0), exist and be in flow state (0 τi < 1) , or be frozen and in terminal state (τi 1)."
        },
        {
            "title": "3.4 Practical considerations & implications",
            "content": "Architecture with per-frame time conditioning. Differently from other frameworks, our model has two prediction heads: the dense velocity prediction head and the insertion rate prediction head. In order to enable different timesteps per frame, we change the AdaLN Peebles and Xie (2023) modulation to operate"
        },
        {
            "title": "4.1 Experimental setup",
            "content": "Model architecture. Our architecture builds on the popular DiT diffusion transformer architecture Peebles and Xie (2023), with minimal changes to enable insertion rate predictions. We equip each attention block with an additional learnable token which is replicated for each frame and takes part in the attention computation, ultimately each of these tokens maps to the non-negative rate prediction using an exponential activation. Our baseline model is made of 38 attention blocks with hidden dimension of 1536, each attention head has dimension of 64, resulting in 24 total heads, for model with approximately 2.1B learnable parameters. Positional embeddings follow the VideoROPE method Wei et al. (2025). We enable full bidirectional attention between frames and optionally concatenated text tokens in an MMDiT Esser et al. (2024) fashion. Datasets. We evaluate our model on three different datasets. We used two narrow-domain datasets: Tai-Chi-HD Siarohin et al. (2019) and RealEstate10K Zhou et al. (2018), as well as the class-structured Kinetics 600 dataset Carreira et al. (2018). Unless specified otherwise, we train our models at 256 resolution and generate up to 145 frames at 16 FPS. Furthermore, we trained general domain model using proprietary dataset of around 20M videos ranging approximately from 20 to 240 seconds. We show results for this model in the supplementary material. We use the LTX autoencoder HaCohen et al. (2024), which has spatial downsampling factor of 32 and temporal downsampling factor of 8. To correctly support variable length video decoding, we modify the decoder architecture to propagate frame validity mask which is used to mask out invalid frames in the hidden layers, preventing border effects where padding frames leak into the video. When prompting models with one or more video frames to generate video, we choose these reference frames from the validation set. Metrics. To evaluate the performance of our models, we rely on the standard FVD metric Unterthiner et al. (2019), computed with respect to subset of 5k videos from the training set, as well as selected metrics from VBench Huang et al. (2024). Specifically, we report imaging and aesthetic quality, background and subject consistency, motion smoothness and dynamic degree. Figure 4 Flowception natively supports different tasks. By choosing active and passive context frames the model can be used for image-to-video, video-to-video, text-to-image, frame interpolation, text-to-video, and scene completion. on per-frame basis. To distinguish between noisy and input/conditioning frames, we inflate the channel dimension of the inputs so that the first channels contain the noisy frames while the second group contains the input/conditioning frames (or zero padding). Finally, for the rate prediction head, learnable token is concatenated to each frames tokens before being projected with simple MLP and an exponential activation. Supporting different tasks. Since the frames present at time are only specified by their relative order, Flowception supports multiple tasks by feeding different context frames. We distinguish two sorts of context frames: active ones which can induce insertions, and passive ones which do not. As illustrated in Figure 4, mixing these context frames in different ways allows for image-to-video, video-to-video interpolation and scene completion without needing to specify the size of the gaps but only the relative order of the context frames. 0 τ 2 Computational cost. Early in sampling, only small subset of frames is visible, which reduces the computational cost of these denoising steps compared to full-sequence denoising. Under linear κ we have Ep(τg)[κ(τg)2] = (cid:82) 1 dτg = 1/3, and so the expected attention FLOPs are 1/3 of full-sequence Flow Matching, assuming an equal number of denoising steps used in both models. During sampling, as the latest flow update occurs at τg = 2, our model uses at most twice the flow iterations of the full-sequence model assuming the same time discretization, i.e. 2/3 the FLOP count of full-sequence Flow Matching. Compared to AR models, Flowception has comparable sampling cost with more robustness under low NFEs. more detailed analysis is available in the supplementary material. 6 Dataset Method Imaging Background Aesthetic Motion Subject Dynamic FVD VBench Metrics Quality Kinetics-600 Full Seq Autoreg. Flowception Tai-Chi-HD Full Seq. Autoreg. Flowception RealEstate10K Full Seq. Autoreg. Flowception 37.09 38.77 41.92 47.48 47.15 48.42 50.11 48.55 51. 94.75 92.69 96.96 94.42 95.93 95.93 93.48 93.84 96.93 39.42 38.17 42.05 54.93 54.98 54.96 44.53 44.48 48. 99.39 98.11 99.21 99.26 99.37 99.67 99.08 99.16 99.30 92.46 85.82 94.74 92.18 93.41 94.43 85.85 87.29 87. 44.35 54.66 47.07 18.61 21.23 20.02 81.64 72.60 78.59 204.65 201.34 164.73 27.30 25.30 25.21 26.17 47.48 21. Table 1 Image-to-Video generation results. Models trained and sampled at 256 resolution. Comparing our Flowception approach to the Full Sequence and (causal) Autoregressive baselines. VBench Quality metrics include imaging quality, background consistency, aesthetic quality, motion smoothness, subject consistency, and dynamic degree. Best results for each dataset are highlighted. Figure 5 Image-to-video generation using model trained on Tai-Chi-HD. We show two examples (one per row) of generated video frames along the insertion time of each frame. In general, frames inserted early, with 0, define the movement dynamics (large changes w.r.t. the context frame), while later frames smoothly interpolate motion, resulting in smaller changes w.r.t. to neighboring frames. Baselines. We compare our method with popular video generation paradigms including (i) full-sequence generation where all frames are flowed simultaneously, sharing global timestep and bidirectional attention over all frames, and (ii) per-frame autoregressive generation where the model iteratively predicts the next frame(s) conditioned on the previously available ones. For autoregressive generation, we experiment with both bidirectional and causal attention (which allows KV caching for efficient inference). We sample videos from Flowception and baselines without using guidance, unless specified otherwise. The complete videos from which we show selected frames in Figures 1, 5 and 6 are included in the supplementary material to better appreciate the video quality."
        },
        {
            "title": "4.2 Main results",
            "content": "In Table 1 we comImage-to-Video (I2V) generation. pare our Flowception approach to the full-sequence and autoregressive baselines for the I2V task where we condition on the first frame to generate the rest of the video. We set the generated video length to 145 frames. Overall, we find the autoregressive and fullsequence model to give fairly similar results. For the VBench metrics, Flowception leads to better results in most cases, or second best otherwise. For video quality as measured in FVD, Flowception improves results across the board. In Figure 5 we show two qualitative examples of I2V generation, which demonstrate the ability of the model to generate smooth video sequences with substantial motion and appearance consistency. Additionally, we observe an emergent coarse-to-fine structure in the order of generated frames: early on in the generation process far-away frames are generated that tend to vary and define the overall motion of the video, while later frames mostly interpolate between them. Video interpolation. Another interesting task directly supported by Flowception is video interpolation, where rather than single starting frame, we provide an ordered set of context frames as conditioning, and let the model interpolate the video freely between these. Differently from other frameworks Wan et al. (2025); HaCohen et al. (2024), Flowception does not need to be given the number of frames between successive conditioning frames, but it rather inserts frames as it sees fit, resulting in more flexible interpolation capability. We illustrate this use-case in Figure 6. We observe that the model is able to 7 Figure 6 Video interpolation results obtained with with Flowception trained on the Kinetics-600 dataset. Context frames are highlighted with dashed lines, insertion times of other frames are marked using color map and printed on each frame. Table 2 Comparing insertion schemes. Using the Flowception rate prediction and baselines that insert frames in random order, in hierarchical scheme (see text for detail), and left-to-right. Models trained on RealEstate10K. Table 3 Effect of rate guidance on motion smoothness. We report dynamic degree and motion smoothness for various values of the insertion rate guidance parameter ws. Insertion method FVD Motion Dynamic Random Hierarchical Left-to-right Flowception rate prediction 25.03 23.94 23.61 21. 99.09 99.10 99.03 99.30 70.68 71.20 73.04 78.59 adapt the number of insertions to generate coherent scenes with smooth transitions. For instance, in the second row, no frame is inserted after the penultimate context frame in order to preserve motion continuity."
        },
        {
            "title": "4.3 Ablation experiments",
            "content": "Impact of learned insertions. To assess the importance of the insertion head that learns to predict where to insert frames, we compare it to inserting frames in data-independent manner. In particular, we first determine the video length n. We then consider inserting frames in random order, using hierarchical scheme where we iteratively insert them in the middle of the largest interval in {1, . . . , n} where no frame has been inserted yet, or using leftto-right pattern as used in AR models. We present the results of this experiment in Table 2. We find that across metrics, results improve as we move from the random pattern to the hierarchical pattern, to the left-to-right, and finally to the data-driven insertion patterns in Flowception . Note that for the left-to-right pattern, the FVD results in Table 2 (23.61) are much better than the FVD for the AR baselines in Table 4 (47.48 and 45.13). We hypothesize that this difference is due to the fact that the AR baselines denoise the next frame once previous ones are already fully denoised ws 1.0 2.0 5.0 FVD Motion Dynamic 21.80 22.69 25. 99.30 99.31 99.33 78.59 78.61 77.78 and committed to, while in Flowception new frames start denoising while previous ones are still not fully denoised (more similar to the full-sequence baseline which yields FVD of 26.17 on RealEstate10K). Insertion guidance. Classifier-free guidance (CFG) is commonly used to achieve better prompt alignment and image quality in diffusion and flow models Ho and Salimans (2021). Similarly, in Flowception , we can perform CFG on the insertion rates. Following CFG, we randomly drop the conditional information during training, and define the guided update as λcfg(Xtc) = λ(Xtc)ws λ(Xt)1ws, (13) where ws 1 is the guidance scale for the insertion rate. Figure 7 presents scatter plot comparing video lengths for I2V obtained with and without guidance, initiating generation from the same seed for each starting frame. For efficiency we limit the number of frames to 20 in this experiment. The result clearly indicates that using rate guidance (ws = 5) biases the model towards generating longer videos. Without guidance, or using lower values, we find insertion guidance to prevent problems of under-insertions which can result in choppy transition between two frames. This is confirmed in Table 3 where motion smoothness increases while dynamic degree decreases as ws increases. Causality in AR models. In the literature, most au8 Figure 8 Comparing local attention variants. We report FVD on RealEstate10K after training for 300k iterations with different attention context windows. For each frame, its context is made of itself and the previous/next context window frames. ically, we observe that Flowception tends to insert distant frames first, and then progressively fills in the blanks. This suggests that Flowception could be more amenable to using efficient local attention patterns, as far-away frames can still attend to each other early in the denoising process when the sequence is still short. To assess whether Flowception is more amenable to using local attention windows than the full-sequence baseline, we compare models using global attention and using local attention windows restricted to the previous and next frames in the sequence. The results in Figure 8 indicate that Flowception suffers far less from using local attention windows than the full-sequence model, presumably due the communication between distant frames early in the Flowception denoising process, when the intermediate frames have not yet been inserted, even when using small attention windows."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented Flowception , novel video generation model that interleaves denoising existing frames with inserting new ones at appropriate locations, enabling efficient variable-length non-autoregressive generation. Our method outperforms full sequence and autoregressive baselines in both generation quality and training efficiency. Flowception naturally supports various prediction tasks including image-tovideo, video-to-video generation, and frame interpolation. Our work offers promising alternative to standard autoregressive and full-sequence approaches for long-term generation and flexible editing. While training on partial sequences allows for these emergent behaviors,it doubles the number of iterations in order to ensure complete denoising of all frames. Exploring different interleaved schedules for improved efficiency is an exciting future direction. Figure 7 Impact of rate guidance on I2V video length. Samples obtained without guidance (ws = 1, horizontal) and with guidance (ws = 5, vertical) on RealEstate10K, using the same seed for each conditioning frame. Table 4 Comparison of autoregressive attention patterns. Autoregressive baseline with differnt attention amskes and comparison to Flowception in terms of VBench metrics and FVD, Models trained on Real Estate10K. AR causal AR non-causal Flowception FVD Imaging quality Background consistency Aesthetic quality Motion smoothness Subject consistency Dynamic degree 47.48 48.55 93.84 44.48 99.16 87.29 72. 45.13 48.70 93.88 45.28 99.20 87.46 73.52 21.80 51.18 96.93 48.09 99.30 87.02 78.59 toregressive video generation models make use of frame-wise causal attention Deng et al. (2024); Chen et al. (2025), which allows for KV caching and therefore efficient inference for autoregressive generation. Other works improve on quality and long term rollout consistency by using bidirectional attention in the context and applying frame-wise noising for each frame Song et al. (2025). Such an approach, while effective, significantly raises inference time expected compute from O(L2n2) to O(L2n3) where is the number of video frames, and is the number of tokens per frame. more detailed analysis of FLOPs is provided in the supplementary material. We provide comparison between both autoregressive methods in Table 4. The results demonstrate slightly better performance of the non-causal attention pattern, outperforming the causal one on every metric. Flowception yields further improvements over the non-causal AR baseline on all metrics but subject consistency. Local vs. global attention. Flowception generates video by progressively inserting new frames. Empir-"
        },
        {
            "title": "References",
            "content": "S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, 2015. Tariq Berrada, Pietro Astolfi, Melissa Hall, Reyhane Askari-Hemmat, Yohann Benchetrit, Marton Havasi, Matthew Muckley, Karteek Alahari, Adriana RomeroSoriano, Jakob Verbeek, and Michal Drozdzal. On improved conditioning mechanisms and pre-training strategies for diffusion models. In Advances in Neural Information Processing Systems, 2024. https:// proceedings.neurips.cc/paper_files/paper/2024/file/ 18023809c155d6bbed27e443043cdebf-Paper-Conference. pdf. Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, and Tim Rocktäschel. Genie: Generative interactive environments. In International Conference on Machine Learning, 2024. https://arxiv.org/abs/2402. 15391. Shengqu Cai, Ceyuan Yang, Lvmin Zhang, Yuwei Guo, Junfei Xiao, Ziyan Yang, Yinghao Xu, Zhenheng Yang, Alan Yuille, Leonidas Guibas, Maneesh Agrawala, Lu Jiang, and Gordon Wetzstein. Mixture of contexts for long video generation. arXiv preprint, 2508.21058, 2025. https://arxiv.org/abs/2508.21058. Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. short note about Kinetics-600. arXiv preprint, 1808.01340, 2018. https: //arxiv.org/abs/1808.01340. Boyuan Chen, Diego Martí Monsó, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. In Advances in Neural Information Processing Systems, 2025. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. PixArt-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In International Conference on Learning Representations, 2024. Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint, 2412.14169, 2024. Prafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, Yi Wang, Yuming Jiang, Yaohui Wang, Peng Gao, Xinyuan Chen, Hengjie Li, Dahua Lin, Yu Qiao, and Ziwei Liu. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint, 2501.08453, 2025. https: //arxiv.org/abs/2501.08453. Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. LTX-Video: Realtime video latent diffusion. arXiv preprint, 2501.00103, 2024. Marton Havasi, Brian Karrer, Itai Gat, and Ricky TQ Chen. Edit flows: Variable length discrete flow matching with sequence-level edit operations. In Advances in Neural Information Processing Systems, 2025. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS Workshop on Deep Generative Models and Downstream Applications, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. Peter Holderrieth, Marton Havasi, Jason Yim, Neta Shaul, Itai Gat, Tommi Jaakkola, Brian Karrer, Ricky T. Q. Chen, and Yaron Lipman. Generator matching: Generative modeling with arbitrary markov processes. In International Conference on Learning Representations, 2025. https://arxiv.org/abs/2410.20587. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Conference on Computer Vision and Pattern Recognition, 2024. https://arxiv.org/abs/2311.17982. Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. In International Conference on Learning Representations, 2025. https://arxiv.org/abs/2410.05954. 10 Black Forest Labs. Flux. black-forest-labs/flux, 2024. https://github.com/ Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, and Tom Nickson. Controllable music production with diffusion models and guidance gradients. In Advances in Neural Information Processing Systems, 2023. http://arxiv.org/abs/2311.00613. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Machine Learning, 2023. Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and SainSiT: Exploring flow and diffusion-based ing Xie. generative models with scalable interpolant transIn European Conference on Computer formers. Vision, 2024. https://www.ecva.net/papers/eccv_ 2024/papers_ECCV/papers/09828.pdf. John Nguyen, Marton Havasi, Tariq Berrada, Luke Zettlemoyer, and Ricky T. Q. Chen. OneFlow: Concurrent mixed-modal and interleaved generation with edit flows. arXiv preprint, 2510.03506, 2025. https: //arxiv.org/abs/2510.03506. NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klár, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, MingYu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical AI. arXiv preprint, 2501.03575, 2025. https://arxiv.org/abs/2501.03575. William Peebles and Saining Xie. Scalable diffusion models with transformers. In International Conference on Computer Vision, 2023. Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commerciallevel video generation model in $200k. arXiv preprint, 2503.09642, 2025. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffuIn sion models for high-resolution image synthesis. International Conference on Learning Representations, 2024. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie Gen: cast of media foundation models. arXiv preprint, 2410.13720, 2025. https://arxiv.org/abs/2410.13720. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition, 2022. Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In Advances in Neural Information Processing Systems, 2019. https: //arxiv.org/abs/2003.00196. Jascha Sohl-Dickstein, and Surya Ganguli. heswaranathan, unsupervised thermodynamics. ence //proceedings.mlr.press/v37/sohl-dickstein15.html. Eric Weiss, Niru MaDeep using nonequilibrium International Conferhttps: on Machine Learning, learning 2015. In Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun 11 video generation. arXiv preprint, 2504.12626, 2025. https://arxiv.org/abs/2504.12626. Canyu Zhao, Mingyu Liu, Wen Wang, Weihua Chen, Fan Wang, Hao Chen, Bo Zhang, and Chunhua Shen. MovieDreamer: Hierarchical generation for coherent long visual sequence. arXiv preprint, 2407.16655, 2024. https://arxiv.org/abs/2407.16655. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-Sora: Democratizing efficient video production for all. arXiv preprint, 2412.20404, 2024. Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. In SIGGRAPH, 2018. https://arxiv.org/abs/1805.09817. Du, Russ Tedrake, and Vincent Sitzmann. Historyguided video diffusion. In International Conference on Machine Learning, 2025. https://arxiv.org/abs/2502. 06764. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. https://openreview. net/forum?id=PxTIG12RRHS. Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. In ICLR Workshop on Deep Generative Models for Highly Structured Data, 2019. https://arxiv.org/abs/1812.01717. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint, 2503.20314, 2025. Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, and sheng zhao. Audit: Audio editing by following instructions with In Advances in Neural latent diffusion models. Information Processing Systems, 2023. https:// proceedings.neurips.cc/paper_files/paper/2023/file/ e1b619a9e241606a23eb21767f16cf81-Paper-Conference. pdf. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. VideoRoPE: What makes for good video rotary position embedding? In International Conference on Machine Learning, 2025. Wenming Weng, Ruoyu Feng, Yanhui Wang, Qi Dai, Chunyu Wang, Dacheng Yin, Zhiyuan Zhao, Kai Qiu, Jianmin Bao, Yuhui Yuan, Chong Luo, Yueyi Zhang, and Zhiwei Xiong. ART-V: Auto-regressive text-tovideo generation with diffusion models. In CVPR Workshop on Generative Models for Computer Vision, 2024. Lvmin Zhang and Maneesh Agrawala. Packing input frame contexts in next-frame prediction models for"
        },
        {
            "title": "A Full derivations",
            "content": "In this section, we provide the derivations for our model. We start with brief summarization of the Edit Flows framework in video space before deriving the interleaved time schedule for concurrent frame insertions and denoising and training losses. A.1 Edit flows and frame insertions Setup. As explained in the main manuscript, we model videos as sequences of frames from the space . We use blank token to mark empty positions in sequence of videos. Let = (cid:83)N n=0(X {})n define the space were the (augmented) videos live and fstrip : the mapping from augmented to observable space where fstrip removes all blanks (i.e Xt = fstrip(Zt)), and define the product delta on sequences δz1(z2) = (cid:81) Under this parameterization, sample Z0 in augmented space is series of noise frames interleaved with blank tokens at random locations, as illustrated in Figure 9. δzi 2). (zi 1 Conditional probability path. We prescribe coupling between source and target distributions. We use the standard independent coupling where each clean frame is paired with an independent Gaussian noise frame. Concretely, for the source we take X0 (cid:89) i=1 (X 0; 0, I), and use an augmented variable Zt (X {})n to model masked insertions. At = 0, we start from the all-blank sequence Z0 = (, . . . , ) and gradually reveal the clean frames X1 according to the scheduler κt. Given X1 pdata video with frames, we define conditional masked path over Zt (X {})n interpolating between X0 (0, I)k (with n) and X1 where transitions from blank frames to real frames follow the probability law: pt(Xt, Zt X1) = pt(Xt Zt) pt(Zt Z1) = δfstrip(Zt)(Xt) (cid:89) (cid:104) i=1 (1 κt) δ(Z ) + κt δX 1 (14) (cid:105) (Z ) with κ0 = 0, κ1 = 1. Each token in Zt is blank with with probability κt. probability 1 κt or equals 1 13 Figure 9 Illustration of the coupling between source and target distributions in augmented space Z. Starting frames are initialized as noise vectors ϵ (0, I) while others are blank tokens in augmented space which are transformed into noise vectors at their corresponding insertion time. Horizontal lines on the arrows indicate the time when frame is revealed in the schedule. Continuous-time Markov chain (CTMC). We describe the binary reveal process for frame insertions by CTMC in augmented space. As described previously, let Zt denote the augmented sequence with blanks, and Xt = fstrip(Zt) its observable subsequence. The CTMC acts only on the discrete reveal decisions in Zt; continuous evolution of frame contents is handled separately by the flow-matching ODE which we develop later on. Marginally over Zt, the induced evolution on observable sequences has the infinitesimal transition kernel P(Xt+h Xt) = δXt(Xt+h) + ut(Xt+h Xt) + o(h), (15) where ut is the marginal insertion rate obtained from the underlying CTMC on (Xt, Zt). Conditional CTMC rate. As demonstrated in Havasi et al. (2025), conditional CTMC that samples from (14) can be written ut(x, Xt, Zt, X1) = (15) (cid:18) (cid:88) i= κt 1 κt (cid:2)δX 1 (zi) δZi (cid:19) (zi)(cid:3) δfstrip(z)(x), where = ins(Xt, i, a) for some [n], . This gives the infinitesimal probability shift from (Xt, Zt) to (x, z), restricted to next states that differ by one insertion at most. Intuitively, any masked position transitions from to with rate κt/(1 κt), 1 ensuring that sample started at Z0=[ , . . . , ] reaches X1 as 1. Training loss. We train model that transports sequences via insertions, uθ (x Xt), where = ins(Xt, i, ϵ) for some i, ϵ, by marginalizing the auxiliary Zt and the data X1. As seen in Havasi et al. (2025), the marginalized ground-truth rate ut( Xt) = (cid:80) Ept(ztXt) ut(, Xt, zt, X1) generates pt(Xt), any Bregman divergence Dϕ(a, b) = ϕ(a) ϕ(b) b, ϕ(b) can be used to regress the marginal rate. Following Havasi et al. (2025); Nguyen et al. (2025); Holderrieth et al. (2025), we use Bregman divergence between measures over next states and marginalize over all such that = fstrip(z): where the expectation is over τ, X1 pdata and (Xt, Zt) pt(Xt, Zt X1), while = clip(τ, 0, 1). As pointed out by Havasi et al. (2025), keeping the factor preserves direct ELBO interpretation, while removing it can be more stable in practice; both choices recover the familiar Poisson loss over insertion counts. So in practice, the loss we use is κt 1κt EX1pdataE(Xt,Zt)pt(Xt,ZtX1) (cid:32) (cid:88) Dϕ ut(, Xt, Zt, X1), uθ ( Xt) . (16) (cid:33) Lins(θ) = E() (cid:34) ℓ(Xt) (cid:88) j=1 (cid:0)λj(Xt) Aj log λj(Xt)(cid:1) (cid:35) . (20) Choosing the entropy as potential function ϕ(u) = u, log gives the explicit loss (see Theorem B.2 for the derivation of this term) = Et,X1,Xt,Zt (cid:34) (cid:88) uθ (x Xt) x=Xt (cid:88) i=1 1(Z = ) κt 1 κt log uθ (cid:0)ins(cid:0)Xt, j, 1 (cid:1) Xt (cid:1) where is the slot in Xt corresponding to the first noncoordinate to the left of . This alignment ensures that inserting at position corresponds to changing t : 1 . Loss simplification. As in Nguyen et al. (2025), we adopt an equivalent t-independent parameterization for one-insertion moves. Rather than modeling separate rates for each individual missing frame, we define single slot-level insertion rate. For one-insertion move = ins(Xt, i, ϵ) (inserting fresh noise frame ϵ at slot i), we write uθ (cid:0)ins(Xt, i, ϵ) Xt (cid:1) = κt 1 κt λi(Xt), (18) where λi(Xt) 0 is the total insertion rate at slot i. The actual frame content is always sampled from fixed noise prior (e.g. ϵ (0, I)) and is not parameterized. Let Aj denote the set of missing frames associated with slot j, i.e. those to be inserted to the right of at time under the alignment. Substituting this parameterization into Equation (17) and collecting the θ-dependent terms yields L(θ) = E() κt 1 κt ℓ(Xt) (cid:88) j=1 λj(Xt) (cid:88) aAj logλj(Xt) = E() κt 1 κt ℓ(Xt) (cid:88) (cid:16) j=1 λj(Xt) Aj log λj(Xt) (cid:124) (cid:125) (cid:123)(cid:122) Poisson NLL (cid:17) + cte. (19) A.2 Velocity flow-matching objective Now we have the loss for the insertion term (where to insert and with with what probability), now we need to model how to update the currently active frames. We briefly recall the derivation of the velocity loss used for denoising active frames. Following rectified flow matching, for ti [0, 1], each frame follows linear probability path ti (17) (cid:35) , ti = tiX 1 + (1 ti)X 0 pi t. (21) Consider rectified flow coupling between source X0 p0 and target X1 p1 governed by the ODE dXt dt = v(Xt, t), [0, 1]. (22) Under this coupling, the population-optimal velocity field satisfies v(Xt, t) = (cid:2)X1 (cid:12) (cid:12) Xt, t(cid:3) . (23) The Conditional Flow Matching (CFM) objective then trains neural velocity field vθ to regress onto this target: (cid:104) 1[0,1)(τ ) (cid:13) Lvel = Eτ,X0,X 2(cid:105) (cid:13)vθ(Xt, t) (X1 X0)(cid:13) , (cid:13) (24) where = clip(τ, 0, 1). As shown in Lipman et al. (2023), this CFM loss has the same optimum as the original Flow Matching objective and is minimized uniquely by vθ = in function space. In Flowception we apply this objective at the frame level. Each frame has local time ti [0, 1) induced by the extended-time construction described above, and we write for its state along the linear path. ti The loss (24) is evaluated only on active frames, i.e. those with τi [0, 1], and we mask out both frozen frames (τi < 0) and terminal frames (τi 1) during training and sampling. 14 A.3 Interleaved time schedule for frame in- (frozen): τi < 0 sertions We now derive the interleaved schedule used to concurrently insert new frames and denoise existing frames, ensuring that training and sampling observe the same joint law over local times. Design choice. At the instant of insertion we can either (i) fully or partially denoise the frame, or (ii) insert pure noise and denoise afterwards. We adopt (ii) for concurrency and parallelism: single forward pass handles both velocity prediction for present frames and insertion decisions, while newly inserted frames start at local time 0 and are denoised in context thereafter. Since the source psrc = (0, I) and target pdata distributions of the data are usually decoupled, any sample from psrc is valid for the inserted frame. Let tg [0, 1] denote the Global and local times. sequence (global) time that advances monotonically during generation. Each frame has local time ti [0, 1] (used by the rectified flow coupling). We must respect the causal constraint that frame cannot be more denoised than the sequence has progressed, i.e., tg ti for all frames currently present. Insertion-time law and inverse-CDF sampling. Let κ : [0, 1] [0, 1] be the monotone reveal scheduler with κ0 = 0, κ1 = 1, and define the hazard ρκ(t) = κt/(1 κt). We model insertion times by the density p(tins) = κt, equivalently tins = κ1(u), Unif(0, 1). For frame inserted at sequence time tg, we set its local time to zero: ti 0. Hence the instantaneous offset between the global and local times is distributed as tg ti = tins, 0 tg, ti, tins 1, (25) so that local time always lags behind global time by random, scheduler-consistent delay. Multi-frame generalization. To make Equation (25) hold during training for all frames, we lift time to an extended interval and tie each frame to an independent offset. Let τg [0, 2], tg = clip(τg), = clip(τ, 0, 1). For every potential Unif(0, 1) and define the extended local time (26) frame index i, draw ui τi = τg κ1(ui), ti = clip(τi). (27) This yields three phases per frame: (flowing): τi [0, 1] (terminal): τi > 1 Let = {i : τi [0, 1]} denote the active set τi [0, 1). During training we sample (τg, {ui}), form (tg, {ti}), delete frames with τi < 0, and apply the flow matching loss only to indices in A. By construction, the marginal law of (tg, {ti}) exactly matches that encountered at sampling, thus samples remain in-distribution."
        },
        {
            "title": "B Additional derivations and remarks",
            "content": "B.1 Deriving the insertion rate Let κ : [0, 1] [0, 1] be nondecreasing insertion schedule with κ(0) = 0, κ(1) = 1, differentiable almost everywhere. For single frame, define its reveal time with cumulative density function (CDF) FT (t) = P[T t] = κ(t) on [0, 1] and survival S(t) = 1 FT (t) = 1 κ(t). Lemma B.1 (Instantaneous reveal hazard). The instantaneous reveal hazard for insertion schedule κ is given by P(cid:0)T [t, + ) > t(cid:1) ρκ(t) = lim 0+ κ(t) 1 κ(t) = , (28) for (0, 1) and ρκ(t) = 0 outside (0, 1). Proof. By definition, for [t, + ) [0, 1), we have P(T [t, + ) > t) = = = P(T [t, + ), > t) P(T > t) FT (t + ) FT (t) S(t) κ(t + ) κ(t) 1 κ(t) . (29) Plugging this in Equation (28) results in ρκ(t) = κ(t) . For / (0, 1), FT is constant, hence ρκ(t) = 1κ(t) 0. B.2 From Bregman divergence to the insertion loss Let φ(z) = log on R0 and Dφ(uλ) = (cid:80) j{uj log(uj/λj) + λj uj}. 15 Proposition B.2 (Bregman objective). For fixed snapshot, where Kj(Xt) is the pending-count in slot j. The general form of the insertion loss then becomes (cid:2)λj uj log λj (cid:3) and Dφ(uλ) (cid:88) differ by constant in u; thus they have the same minimizers. The training objective Lins(θ) = E() (cid:104) (cid:88) (cid:105) λt,j(Xt) ut,j(Xt) log λt,j(Xt) is therefore valid form to learn that converges towards the marginal expectation while only the conditional expectation is used. Corollary B.3 (Pointwise optimum). The persnapshot objective in Prop. B.2 is strictly convex in each λj > 0 and is minimized uniquely at λj = ut,j. Proof. Similarly to Edit Flows Havasi et al. (2025), we make use of the cross-entropy generator function to define the potential function for the Bregman divergence : (z 0), ϕ(z) = log z. Given ground-truth nonnegative targets = {uj}j and model predictions λ = {λj}j, the separable Bregman divergence is Dϕ(uλ) = (cid:88) (cid:16) (cid:17) ϕ(uj) ϕ(λj) ϕ(λj) (uj λj) . Which simplifies for each coordinate to Lins(θ) = (cid:88) (cid:88) = λt,j(Xt) ut,j(Xt) log λt,j(Xt) λt,j(Xt)ρκ(t)Kj(Xt)1[0,1](t)logλt,j(Xt) . (32) In particular, when using the linear scheduler κ(t) = t, then ρκ(t) = 1 1t and Lins(θ) = (cid:104)(cid:80) jλt,j(Xt) Kj (Xt) 1[0,1](t) 1t (cid:105) log λt,j(Xt) (33) B.3 Generalized insertions with Poisson thinning From Bernoulli to Poisson. The Bernoulli-thinning sampler in Algorithm 1 draws single bit per slot and per step, which permits at most one insertion in slot during the step of size . As 0, the sum of independent Bernoulli micro-trials with success probability 1 exp(λ ) converges in law to Poisson random variable with mean (cid:82) λ(s) ds. This suggests finite-step scheme that explicitly allows multiple insertions. Poisson process. time-inhomogeneous Poisson process on [t, + ) with instantaneous rate ρ(s) 0 has independent increments and satisfies ϕ(uj) ϕ(λj) ϕ(λj)(uj λj) (cid:1) (cid:0)λj log λj λj =(cid:0)uj log uj uj Collecting terms gives (cid:1) (log λj) (uj λj). (30) ([t, + )) Poisson (cid:32)(cid:90) t+ (cid:33) ρ(s) ds . Dϕ(uλ) = = (cid:88) (cid:16) (cid:88) (cid:16) uj log uj λj + λj uj (cid:17) λj uj log λj (cid:17) + (cid:88) (cid:0)uj log uj uj (cid:1) . (cid:124) (cid:125) (31) Since (cid:80) j(uj log uj uj) does not depend on λ, minimizing E[Dϕ(uλ)] over model parameters is equivalent to minimizing (cid:123)(cid:122) constant in λ (cid:104) (cid:80) (cid:105) . λj uj log λj For the reveal schedule κ : [0, 1] [0, 1] with hazard ρκ(t) = κ(t)/(1 κ(t)), the true marginal insertion rate at snapshot (Xt, t) is ut,j(Xt) = ρκ(t) Kj(Xt) 1[0,1](t), 16 If ρ(s) is approximately constant on the step, then Poisson(ρ(t) ). Conditioned on = n, the event times are i.i.d. uniform in the interval. Drawing from Poisson process. To enable multiple insertions per slot, we replace the Bernoulli draw with Poisson draw Nt,j (cid:12) (cid:12) (Xt, t) Poisson(cid:0)Λt,j (cid:1), (34) Λt,j ut,j(Xt) = ρκ(t) Kj(Xt). This enables us to insert Nt,j new elements into slot (each initialized with independent base noise) and doing this in parallel across all slots. This preserves the expected number of births per step, E[Nt,j] ut,j(Xt), and removes the at most one insertion per slot constraint."
        },
        {
            "title": "C Discussions",
            "content": "C.1 Baseline implementation details Full-sequence. For the full-sequence model training, we follow standard setups Wan et al. (2025); HaCohen et al. (2024) and sample timesteps during training according to lognorm schedule. Similarly to Flowception, we expand the channel dimension of the input by factor two, and use the second half to encode the (clean) context frames, allowing to support the image-to-video framework. For training on variable length videos, we experiment with two strategies. (1) In each batch we have videos of different length, and we mask out the loss on padded frames. (2) in each batch we collect videos of the same length only. We find (2) to perform more favorably in preliminary experiments and consequently use it for our remaining experiments. Autoregressive. We also use lognorm schedule for the timestep sampler, while the number of context frames is sampled randomly with uniform probability across the length of the video. C.2 Flowception as implicit temporal compression When frame is inserted as pure noise (xnew = ε, tnew = 0), its clean identity among the (K) pending in-slot frames is unresolved at birth. Under the masked Flow Matching objective, the populationoptimal first velocity is the conditional mean over the posterior of the missing frames (integrating both which clean frame it will become and that frames content): new = Emiss = Emiss (cid:2)X1,new Xt, t, (cid:3) ε (cid:2)Z Xt, t, (cid:3) ε, (35) (36) where is random clean frame drawn from the posterior over the not-yet-revealed frames in that slot induced by the snapshot law and the insertionrate hazard ut,j(Xt) = ρκ(t), Kj(Xt), with ρκ(t) = κ(t)/(1 κ(t)), and ρκ(t) = 1/(1 t) for linear κ. Thus the first update points from noise toward group-wise conditional expectation over the missing frames, not toward single target. In full-sequence flow matching, by contrast, all frames are present (as noise) at every step, so there is no identity ambiguity: the optimal direction for index is the per-index conditional mean E[X1,j Xt, t] X0,j, i.e., no marginalisation over missing content. The Flowception update therefore acts like an implicit temporal aggregator early on: active tokens move toward expectations that average over as-yet unseen in-between motion, while the unrevealed frames are integrated out."
        },
        {
            "title": "D Efficiency Comparison",
            "content": "We now study the impact of using Flowception on sampling efficiency compared to full-sequence diffusion and autoregressive paradigms. Let = denote the number of tokens per frame, and the number of frames. The total sequence length is therefore nL. For this analysis we disregard the text tokens and the per-frame extra rate token for Flowception since we are only interested in orders of magnitude. Full-sequence diffusion and flows Full-sequence. evolve all frames simultaneously, sharing global timesteps between the frames. At each sampling step all nL tokens are active, and self-attention dominates with quadratic cost. The total complexity over Tfull steps is therefore Cfull Tfull (nL)2. In autoregressive Autoregressive (no caching). diffusion/flow-matching, frames are generated sequentially, one at time, each conditioned on all previously generated frames. generation step involves appending noise frame to the end of the sequence before evolving it using flow matching, which requires TAR inner steps in order to evolve the noise sample into valid frame by predicting the velocity field for that frame. At step j, the active sequence length is jL, yielding cumulative cost CAR TAR (cid:88) (jL)2 1 3 TAR L2n3. j=1 This cubic dependence on makes autoregressive diffusion substantially more expensive than fullsequence diffusion. Autoregressive diffusion (with caching). With keyvalue (KV) caching, past tokens do not need to be recomputed. At step j, attention is computed only between the new tokens and the cached jL past tokens, for cost O(jL2). Summing across frames yields CAR+cache TAR (cid:88) j=1 jL2 1 2 TAR L2n2. 17 Table 5 Comparison of expected FLOPs during sampling. Full-seq AR AR+cache Flowception Complexity Tfull(nL) 3 TAR(nL)2 1 2 TAR(nL)2 α 3 Tfull(nL) Flowception. When starting from the empty sequence and under linear insertion scheduler, the active fraction is κ(τ )=τ and the (expected) active sequence length art any point is Rτ =τ nL. Averaging the quadratic self-attention cost over the trajectory yields Cflow Tflow (nL)2 Eτ [τ 2] = 1 3 Tflow (nL)2. Allowing Flowception to take α times more steps than the baseline (Tflow = αTfull), to account for the delayed denoising of frames interted later, gives Cflow α 3 Tfull (nL)2. In our experiments we set α = 2 to roughly allow the same number of denoising steps per frame as the full-sequence model, even for frames inserted close to tg = 1. Comparison. The asymptotic speedups between the different methods are 5: 6: Algorithm 1 Flowception generation procedure 1: function FlowceptionGeneration(step size h) 2: 3: 4: (cid:81)nstart i=1 (X i; 0, I) [0, . . . , 0] = [0]nstart tg 0 while min{ti} < 1 do per-frame times global time iterate until all frames are clean X, t, tg FlowceptionStep(X, t, tg, h) end whilereturn 7: 8: end function 1: function FlowceptionStep(X, t, tg, h) 2: 3: 4: 5: 6: v, λ FlowceptionModel(X, t; θ) hi = min{h, 1 ti} clean frames are frozen apply flow step to denoise + hv clip(t + h, 0, 1) tg max(t) all insertions are implemented in parallel for all {1, . . . , ℓ(X)} do with probability hiλi update time trackers : κ(tg) 1κ(tg) inserted frames are set to pure noise = ins(X, i, ε) where ε (0, I) inserted time values are set to zero = ins(t, i, 0) 7: 8: 9: 10: 11: 12: 13: 14: 15: end function end forreturn X, 3 α is inserted, we insert new frame as pure noise ins(X, i, ϵ), ϵ (0, I). speedup FC vs. Full = speedup FC vs. AR speedup FC vs. AR+cache Cfull Cflow α 3 2α TAR Tfull TAR Tfull We summarize the complexities of the different frameworks in Table 5. Algorithms & implementation We provide algorithms for Flowception training and sampling procedures. In Algorithm 1, we provide sketch of the sampling algorithm, assuming number of starting frames nstart and step size that is shared between insertions and flow matching. For simplicity, we do not include context frames in the sketch of the algorithms. We start with ti = 0 for the starting frames. Each sampling step iterates two operations, flow matching on the current set of frames, followed by insertions to the right of each frame {1, . . . , ℓ(X)}, which happens with probability hiλi where λi is the rate associated with the frame i. When new frame κ(tg) 1κ(tg) 18 We detail the training algorithm in Algorithm 2. First, we sample set of timesteps according to the Flowception schedule, τi, {0, . . . , n}, we map these timesteps to deletion operations according to the insertion schedule fstrip(Xtarget, ), fstrip(t, ). Next, the remaining frames are noised according the rectified flow matching schedule = tX + (1 t)X0. The model is then fed these noised frames and their associated timesteps, predicting their associated velocities and insertion rates. The training losses are detailed in the main manuscript. using the time sampling in AlgoTime sampling. rithm 2, it can happen that for the sampled τg all frames in video are already denoised (in particular when all frames are inserted early), rendering the video useless for training. To ensure that there is at least one evolving frame per video, we can instead first sample the terminal time for the last frame in the video (when the last flow step happens), before deriving τg and sampling the individual offsets. To do this we proceed in the following manner: 1. Sample the insertion times ti ins Unif(0, 1) Algorithm 2 Flowception training procedure Require: scheduler κ 1: function FlowceptionTrainingStep(κ) 2: Xtarget ptarget τg p(τg) ui Unif(0, 1) τi τg κ1(ui) times can choose e.g. logit normal per-frame extended Mi 1[τi0] i-th frame is deleted if τi < 0 sample noisy frames ti clip(τi) X0 (0, I) = tXtarget + (1 t)X0 remove deleted frames fstrip(X, ) fstrip(t, ) v, λ FlowceptionModel(X, t; θ) . . . compute insertion and velocity 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: losses θ optimizer_step(θ, L) 16: 17: end function 2. Compute maximum τmax max{ti ins} + that we need: 3. Sample τg Unif(0, τmax) 4. Compute τi = τg ti ins Additionally, following other works Nguyen et al. (2025); Wan et al. (2025); HaCohen et al. (2024), we used lognorm global schedule during training to be beneficial τg lognorm(0, 1) τmax. Loss reduction. Another important point is about the loss reduction for both the velocity and Poisson likelihood, since each sample in the training batch can have varying number of frames which are still evolving, i.e. with 0 τi < 1, question arises around how to reduce the velocity loss across these frames. We experimented with both per-sample mean reduction (each video contributes equally independently from the number of active frames) and mean reduction across all active frames in the batch (all frames in the batch contribute equally, so shorter videos contribute less). We found the latter to be more stable, especially for longer sequences while the former tends to over-optimize for short sequences (early stages of sampling or shorter videos) which in turns biases the model to under-insert. In video generation, early Local sampling schedules. diffusion timesteps are particularly important, as they establish global structure, motion, and temporal alignment before later steps primarily refine appear19 ance. This effect is amplified in our setting, where frames are inserted asynchronously into an evolving sequence: immediately after insertion, frame is highly uncertain and must rapidly become consistent with its temporal neighbors. To address this, the denoising time steps can be biased towards the start of the process. For example, Polyak et al. (2025) uses linear-quadratic scheduler where the first portion of sampling follows linear schedule with small step size 1/1000, while the remaining time steps follow quadratic schedule to arrive at = 1. To prioritize this post-insertion regime in Flowception , we introduce framewise time reparameterization: each frame maintains its own solver coordinate ui [0, 1] and physical diffusion time ti [0, 1], linked by strictly increasing function ti = (ui). During sampling, we advance the solver coordinates by fixed step for all active frames, compute the corresponding physical increments ti = (ui + u) (ui), and update xi xi + ti vθ(xi, ti) in the same scalar time variable [0, 1] used during training. This preserves consistency with the training setup while allowing the effective step size in diffusion time to depend on frames age since insertion. In practice, we instantiate this family with power schedule ti = uγ (with γ > 1), which yields small ti for newly inserted frames (small ui) and larger ti for older, well-established frames. As result, the sampler allocates more computation to the crucial early timesteps after each insertion and fewer steps to later, easier denoising phases, leading to improved temporal coherence and fewer artifacts compared to uniform schedule (γ = 1) under the same compute budget."
        },
        {
            "title": "F Additional experiments",
            "content": "In this section we provide additional experimental results to complement those in the main paper. Length modeling. Here we assess the ability of Flowception to model the length distribution of the sequences in the training set. We create toy dataset where the number of frames is either 15, 20, 25 or 30. Each sample is 3 pixel video where the middle pixel makes discrete jump between two pixel values, while the boundary pixels make up gradient between two colors that moves along the boundary with constant speed in order to achieve an integer number of rotations. After training Flowception on this dataset, we compare the histogram of ground truth and generated video lengths and plot them in Figure 10. As expected the generated video lengths follow similar distribution to the data distribution,"
        },
        {
            "title": "G Broader societal impact",
            "content": "We recognize that our work could lead to potential negative societal impacts, as our method can help generate photorealistic videos, especially if combined with conditioning on real photos or videos. Nonetheless, our work also paves the way for efficient and flexible video generation that can be beneficial in domains such as the entertainment or film industry, as well as world modeling frameworks. As an example, animators could create coherent animations by providing set of frames (either drawings or AI generated), which can significantly speed-up animation work flows. Our method can be used to hierarchically generate very long videos of high quality, at lower computational cost by adopting local attention variants, thereby reducing the energy footprint of generative video models."
        },
        {
            "title": "H Additional qualitative examples",
            "content": "In Figure 12 we compare generations of Flowception and the autoregressive and Full-Sequence baselines trained on the Tai-Chi-HD dataset for 300k iterations, the generations are of 145 frames with an FPS of 16. For the autoregressive model, we observe that later frames suffer from drift due to error accumulation, hindering their quality (see,e.g., the legs). For the full-sequence model, the model struggles to accurately generate the high-frequency details of the video accurately (see, e.g., the face and foliage in the background). In contrast, Flowception results in sharp video without error accumulation as the video progresses. In Figure 13 and Figure 16 we provide further examples of image-to-video results obtained with Flowception on the Tai-Chi-HD and RealEstate10k datasets respectively. In figure 14 and Figure 15, we provide additional video interpolation results obtained with Flowception on the Kinetics 600 and RealEstate10K datasets, respectively; extending the results in Figure 6 of the main paper. We always provide the first and last frames as context, plus at most two additional intermediate frames. Figure 10 Video length matching. Our framework is able to accurately reproduce the length of videos from the toy dataset. Figure 11 Efficiency comparison. We compare the sampling efficiency of autoregressive model (with caching) with Flowception , we plot the FVD on RealEstate10K as function of the FLOPs used for sampling when varying the number of sampling steps. with peaks around 15, 20, 25 and 30, while rarely generating videos with lengths outside these four modes. Efficiency comparison. To evaluate practical sampling efficiency, we perform sweep over the number of sampling steps for both the autoregrssive model and Flowception to compute how FVD performance changes as function of the number of sampling steps and FLOPs. We report the results in Figure 11. First, we find that the autoregressive model has somewhat lower number of FLOPs for given number of sampling steps per frame, this is because Flowception denoise frames asynchronously so the total number of sampling steps is larger than the number of per-frame sampling steps. Second, we find that Flowception obtains significantly better FVD for given number of FLOPs, with FVD plateauing at around 32 denoising steps per frame, where the autoregressive model continues to improve at least up to 64 steps, but without closing the gap with Flowception . 20 Figure 12 Comparing different methods using models trained on the Tai-Chi-HD dataset. Using the same input frame (left) and random seed, we compare generations with the autoregressive, full-sequence and Flowception models. Figure 13 Additional qualitative examples on Taichi. Each row corresponds to different video obtained with our method for image-to-video generation 21 Figure 14 Additional qualitative examples on Kinetics 600 interpolation. Each row corresponds to different video where the first and last frame are given and up to two extra middle frames are also given. Insertion time is highlighted in the boder color of each frame, context frames are highlighted with daashed lines. Figure 15 Additional qualitative examples on RealEstate10K interpolation. Each row corresponds to different video where the first and last frame are given and up to two extra middle frames are also given. Insertion time is highlighted in the boder color of each frame, context frames are highlighted with daashed lines. Figure 16 Qualitative examples of Image-to-Video generation. Using Flowception trained on the RealEstate10K dataset. First shown frame is given as context. Given the initial frame, we generate videos of at most 145 frames at 16 FPS, corresponding to 9.06 secs."
        }
    ],
    "affiliations": [
        "CNRS",
        "FAIR at Meta",
        "Grenoble INP",
        "Inria",
        "LJK",
        "Univ. Grenoble Alpes"
    ]
}