{
    "paper_title": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens",
    "authors": [
        "Nikita Dragunov",
        "Temurbek Rahmatullaev",
        "Elizaveta Goncharova",
        "Andrey Kuznetsov",
        "Anton Razzhigaev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recently proposed Large Concept Model (LCM) generates text by predicting a sequence of sentence-level embeddings and training with either mean-squared error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer that \"thinks\" in the same continuous SONAR embedding space, yet is supervised through token-level cross-entropy propagated via the frozen SONAR decoder. This hybrid objective retains the semantic abstraction of LCM while eliminating its diffusion sampler and restoring a likelihood-based training signal. Across model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive generation quality. We report scaling trends, ablations, benchmark results, and release the complete training code and all pretrained checkpoints to foster reproducibility and future research."
        },
        {
            "title": "Start",
            "content": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens Nikita Dragunov1, 2, Temurbek Rahmatullaev1, 2, Elizaveta Goncharova1, 3, Andrey Kuznetsov1, 4, Anton Razzhigaev1, 5 1AIRI 2MSU 3HSE 4Innopolis University 5Skoltech dragunov@airi.net 5 2 0 2 7 ] . [ 1 5 0 3 5 0 . 8 0 5 2 : r Abstract The recently proposed Large Concept Model (LCM) (Barrault et al. 2024) generates text by predicting sequence of sentence-level embeddings and training with either meansquared error or diffusion objectives. We present SONAR-LLM, decoder-only transformer that thinks in the same continuous SONAR (Duquenne, Schwenk, and Sagot 2023) embedding space yet is supervised through tokenlevel cross-entropy propagated via the frozen SONAR decoder. This hybrid objective retains the semantic abstraction of LCM while eliminating its diffusion sampler and restoring likelihood-based training signal. Across model sizes from 39 to 1.3 parameters, SONAR-LLM attains competitive generation quality. We report scaling trends, ablations, benchmark results and release the complete training code and all pretrained checkpoints to foster reproducibility and future research. Introduction Most autoregressive language models learn token-by-token: they minimise cross-entropy over discrete vocabulary and emit one token per forward step (Brown et al. 2020; Raffel et al. 2020). This fine-grained decoding is simple to train and evaluate but becomes throughput bottleneck for long sequences. Metas recently introduced Large Concept Model (LCM) (Barrault et al. 2024) addresses the latency issue by predicting much shorter trajectory of sentence-level embeddings trained with diffusion or MSE objective. Yet removing token-level likelihoods makes optimization less stable. We present SONAR-LLM, an autoregressive decoderonly transformer that keeps LCMs think in sentence embeddings idea while leveraging the advantages of crossentropy learning. The model predicts SONAR sentence embeddings but propagates loss through the frozen SONAR decoder down to individual tokens, coupling continuous reasoning with discrete supervision. This yields singleshot sentence generator that is diffusion-free, likelihoodconsistent, and fast at inference time. Our contributions are: 1. Token-Aware Embedding Objective. We introduce training objective that back-propagates token-level crossentropy through frozen SONAR decoder, aligning continuous predictions with discrete targets. 2. Scaling Laws Analysis. We provide detailed scaling law fit for validation losses across model sizes, quantifying the scaling exponents for LLM, LCMs, and SONARLLM architectures. 3. Summarization Evaluation. We compare models on summarization tasks using XSum and CNN/DM benchmarks, showing that SONAR-LLM matches or exceeds the performance of other sentence-level approaches. 4. Inference Efficiency Analysis. We present theoretical analysis of inference FLOPs, showing that SONARLLM achieves superior computational efficiency on long sequences compared to standard LLMs. 5. Reproducible Open-Source Release. All training code, evaluation scripts, and model checkpoints are publicly released to facilitate follow-up research."
        },
        {
            "title": "Related Works",
            "content": "Token-level autoregressive models. Large language models are trained by next-token prediction with crossentropy over discrete vocabulary (Brown et al. 2020), inheriting the Transformer architecture (Vaswani et al. 2017). Recent research has explored alternatives to selfattention for faster long-sequence processing; for example, MAMBA replaces attention with selective state-space updates and achieves linear-time generation while matching Transformer quality (Gu et al. 2023). Latent-variable text generators. Continuous and discrete VAEs generate sentences from latent codes (Bowman et al. 2016). Vector-Quantised VAE (VQ-VAE) models compress sentences into short sequence of discrete indices and decode them with an autoregressive prior (van den Oord, Vinyals, and Kavukcuoglu 2017). The SONAR encoderdecoder extends this idea to language-agnostic, multimodal sentence embedding space covering 200 languages (Duquenne, Schwenk, and Sagot 2023). Metas Large Concept Model (LCM) builds an autoregressive prior over SONAR embeddings and investigates MSE, quantisation and diffusion losses in that space (Barrault et al. 2024). Our SONAR-LLM also operates in SONAR space 1Available LLM/tree/main at https://github.com/FusionBrainLab/SONARExample 1 Prefix: Once upon time, in small yard, there was small daisy. The daisy had name. Generated: Her name was Daisy. Daisy lived in garden full of flowers. She loved to play with her friends. One day, Daisy saw big butterfly. The butterfly was very pretty. Daisy wanted to be friends with the butterfly. She said, \"Hi, butterfly! Do you want to play with me?\" The butterfly said, \"Yes, want to play with you!\" Daisy and the butterfly played all day. They had so much fun. At the end of the day, Daisy was happy to go home. She knew she would have new friend to play with. Example 2 Prefix: Once upon time, in small house, there lived kind and honest girl named Lily. She loved to bake cakes for her friends and family. Generated: One day, Lily decided to make big cake for her friend. Lily went to the store to mix cake, flour, and sugar. She mixed and mixed all the ingredients until it was time to bake. Then, she put the cake in the oven and waited for it to bake. When the cake was done, Lily took it out of the house and shared it with her friends. They all loved the cake and said, \"Thank you, Lily!\" Lily felt happy and proud that she could make her friends happy. Figure 1: Examples of texts generated by SONAR-LLM 900M but reinstates token-level cross-entropy by back-propagating through the frozen decoder. Diffusion and discrete denoising models for text. Diffusion-LM denoises continuous word-embedding sequences to enable controllable generation without left-toright constraints (Li et al. 2022). Discrete Denoising Diffusion Probabilistic Models (D3PMs) corrupt token sequences and learn to reverse the process in discrete space (Austin et al. 2021). Recent work improves training with scoreentropy objective, narrowing the perplexity gap to autoregressive baselines (Lou, Meng, and Ermon 2024). Flow and ODE-based generators. Flow Matching trains continuous normalising flows without expensive simulation and subsumes diffusion as special case (Lipman et al. 2023). Applying flow matching to text, FLOWSEQ generates high-quality sentences in handful of ODE steps, greatly accelerating sampling (Hu et al. 2024). In summary, research has progressed from token-wise decoding to latent concept prediction (LCM), diffusion and Figure 2: Architecture of SONAR-LLM. The model autoregressively predicts the next sentence embedding given prefix of embeddings and decodes it via the frozen SONAR decoder. flow-based models. SONAR-LLM bridges these by learning an autoregressive prior in sentence embedding space while retaining likelihood-based supervision. SONAR-LLM The proposed SONAR-LLM is an autoregressive dethat operates directly in the coderonly Transformer SONAR sentence-embedding space while being supervised with token-level cross-entropy. The overall architecture of our approach is illustrated in Figure 2. Pre-processing and Sentence Segmentation We segment text into small units using the Punkt unsupervised sentence tokenizer implemented in NLTK (Kiss and Strunk 2006). Each sentence st is encoded with the frozen multilingual SONAR encoder (Duquenne, Schwenk, and Sagot 2023), yielding fixed-length vector et Rd (d=1024 in all experiments). Given prefix of sentence embeddings (e1, . . . , et), the model predicts the embedding ˆet+1 of the next sentence. This predicted vector is then decoded using the frozen SONAR decoder, and the generated sentence is compared to the true next sentence st+1, which serves as the training target. Model Architecture SONAR-LLM is decoder-only Transformer with the same layer pattern as Llama 3 (Llama Team, AI @ Meta 2024) but Figure 3: Scaling laws: validation loss dynamics vs. number of trainable parameters. an embedding vocabulary of size one: the model predicts continuous vector rather than discrete token at each step. Formally, given prefix e<t = (e1, . . . , et1), the network outputs ˆet = fθ(e<t) Rd. We train variants from 39 to 900 parameters by scaling width and depth; all use rotary position encodings and RMS-norm. Cross-Entropy Through the Frozen Decoder To avoid MSE or diffusion objectives yet keep likelihoodbased training, we decode ˆet back to token logits with the frozen SONAR decoder D: zt = D(ˆet) RV. We minimise standard cross-entropy between zt and the ground-truth token sequence of sentence st: = (cid:88) t=1 log pθ(st e<t) = (cid:88) st (cid:88) t=1 i=1 log (cid:0)softmax(zt)st,i (cid:1) (1) Back-propagation flows through keeping SONAR frozen and reducing memory overhead. Teacherforcing supplies the ground-truth embedding et at the next time step."
        },
        {
            "title": "End of sequence",
            "content": "a literal special append sentence \"End of We sequence.\" to every document and encode it once with the SONAR encoder to obtain eeot. At inference, generation halts when the cosine similarity between the latest predicted embedding and eeot exceeds τstop=0.98, or when Tmax = 32 sentences are produced. Results We trained large language models (LLMs) of four different scales (39 M, 100 M, 300 M, 600 M, and 900 parameters) for four epochs each, using the Llama 3 architecture on the TINYSTORIES dataset (Eldan and Li 2023). Each run was conducted on server equipped with up to 8 NVIDIA A100 GPUs (80GB). When reporting model sizes for LLMs, we included the embedding matrices in the parameter list, as these were fully trained. We also trained SONAR-LLM, MSE-based LCM, and diffusion-based LCM. For SONARLLM and MSE-based LCM models, we used the same architecture configurations as their LLM counterparts, but excluded the embedding and decoder parameters from training. As result, these models contain fewer trainable parameters: 11 M, 34 M, 170 M, 450 M, and 700 M, respectively, having the same depth and width. For consistency, we refer to model sizes (39 900 M) based on the full LLM configurations, even when the number of trainable parameters is smaller. For the diffusion-based LCM, we employed the two-tower architecture from the original paper. Both LCM versions were trained using the official implementation provided by the authors (Barrault et al. 2024). All models were trained using cosine learning rate scheduler. We experimented with two learning rates: 5 104 and 1 103. Based on validation loss performance, we found 1 103 to be optimal for SONAR-LLM, while the other models (LLM, MSE-based LCM, and diffusionbased LCM) performed better with learning rate of 5 104. Examples of generated texts can be found in Figure"
        },
        {
            "title": "Scaling laws",
            "content": "The empirical scaling properties of the evaluated architectures, illustrated in Figure 3, offer insights into their efficiency in leveraging increased model parameters and trainFigure 4: GPT-4o-based evaluation scores (grammar, creativity, consistency, plot) by model and size. Trainable parameter counts are shown above bars for SONAR-LLM and MSE LCM. ing compute. This analysis focuses on the implications of these observed validation loss dynamics for each model type. We fitted the classical scaling law L(N ) = aN α + to the validation losses of all models at epoch 4. The results  (Table 1)  confirm that SONAR-LLM achieves strong scaling exponent (α 0.569), matching or surpassing other embedding-based models. For all models, the scaling laws exhibit an excellent fit to the data, with an R2-score exceeding 0.995. This demonstrates that SONAR-LLM can efficiently leverage increased model capacity, benefiting from both semantic abstraction and effective scaling behaviour. Table 1: Fitted scaling law parameters L(N ) = aN α + for each model at epoch 4. Model LLM MSE LCM (Meta) Diffusion LCM (Meta) SONAR-LLM (ours) 4.06 105 3.21 104 1.58 105 2.09 103 α 0.791 0.515 0.485 0.569 1.24 199 84.0 1.73 Automatic Evaluation with GPT-4o We evaluated the performance of all four model types on dataset consisting of 512 generated stories, assessing grammatical correctness, creativity, coherence, and plot consistency, following the methodology proposed by (Eldan and Li 2023). To initiate story generation, we used the first two sentences from validation set stories as prompts. During evaluation, GPT-4o was shown the full storyincluding the prompt and the generated continuationbut was explicitly instructed to assess only the continuation starting from the third sentence. All models were evaluated after four epochs of training. For the LLM, we experimented with both greedy decoding and beam sampling with four beams. As illustrated in Figure 4, the classic token-level LLM clearly demonstrates the best performance. Among the concept-based models, our proposed SONAR-LLM achieves the highest story generation quality, significantly outperforming both the diffusion-based and MSE-based LCM variants."
        },
        {
            "title": "NLG Metrics",
            "content": "To assess how effectively models capture the distribution of the original data, we evaluated standard NLG metrics, including BLEU, ROUGE-L, and METEOR. Specifically, we selected 512 stories from the validation set and used the first two sentences from each story as context (short prefix) to generate the third sentence. We then measured similarity between the generated sentence and the corresponding reference sentence from the validation set using the aforementioned metrics. Additionally, we performed the same evaluation using half of each story in terms of sentence count Figure 5: NLG scores by model and size; trainable parameter counts are shown above bars for SONAR-LLM and MSE LCM. as context (long prefix), to investigate model performance under varying context lengths. Results are provided in Figure 5. The NLG evaluation demonstrates that SONAR-LLM achieves results closely matchingand frequently slightly surpassingthose of standard autoregressive LLM across all metrics. In contrast, original concept-based methods, such as diffusion-based and MSE-based LCMs, consistently show lower-quality generations, lagging notably behind both SONAR-LLM and standard LLMs, regardless of prompt length or model size. Summarization Evaluation Summarization is vital benchmark for sentence-level language models, as it directly assesses their capability to capture semantic content and produce coherent, structured text. Prior works on sentence-level LLMs, including the original LCM paper, emphasized summarization as crucial test of their abstraction and compression abilities. Motivated by this, we evaluated SONAR-LLM and relevant baselines on standard abstractive summarization benchmarks. We pretrained 1.3B-parameter models (1.1B trainable parameters for SONAR-LLM and MSE LCM, excluding embedding matrix) on diverse mixture of datasets, including TINYTEXTBOOKS, TINYORCATEXTBOOKS, TINYSTRANGETEXTBOOKS, TEXTBOOKSAREALLYOUNEED (Jain et al. 2023), WIKITEXT-103DETOKENIZED (Merity et al. 2017), XSUM (Narayan, Cohen, and Lapata 2018), CNNDAILYMAIL (Hermann et al. 2015). We then evaluated summarization performance on test examples from the XSUM and CNNDAILYMAIL datasets, generating the same number of sentences as in the reference summaries (typically one sentence for XSum and three sentences for CNN/DM). Results were measured using ROUGE-L and METEOR metrics. Model XSum R-L MET CNN/DM R-L MET SONAR-LLM (ours) LLM-beam LLM-greedy MSE LCM (Meta) Diffusion LCM (Meta) 19.3 18.7 18.9 12.2 12. 15.2 15.4 14.9 8.7 8.3 16.0 18.3 18.7 7.6 10.2 10.4 16.5 14.1 3.7 5.1 Table 2: Summarization results on XSum and CNN/DM (512 examples each). The results in Table 2 indicate that SONAR-LLM substantially outperforms existing sentence-level baselines (MSE LCM and Diffusion LCM) on both datasets, confirming its effectiveness for summarization tasks. Compared to token-level LLMs, SONAR-LLM achieves comparable or slightly better performance on the more abstractive XSum dataset but remains behind on CNN/DM, which tends to favor more extractive approaches. These observations indicate that SONAR-LLM can be promising approach for sentence-level tasks involving abstraction and semantic compression."
        },
        {
            "title": "Inference Efficiency",
            "content": "We compared the theoretical inference complexity in FLOPs of SONAR-LLM and standard LLM depending on the input sequence length. The comparison was performed for models with identical architectures configured at 600 parameters. In the case of SONAR-LLM, we assumed an average sentence length of 60 tokens and, in addition to the complexity of the main SONAR-LLM model, we also included the FLOPs of the SONAR encoder and decoder. The inference setup of SONAR-LLM follows the same structural principles as the MSE-based LCM proposed by Barrault et al. (2024), suggesting that both models exhibit similar inference efficiency due to similar design. Figure 6: Theoretical inference FLOPs for autoregressive LLM and SONAR-LLM as function of sequence length (loglog scale). The results presented in Figure 6 indicate that, for shorter sequences, standard token-level LLMs maintain computational advantage due to their optimized token-wise autoregressive decoding. However, as the input length increases, this advantage diminishes: starting from approximately 4096 tokens, SONAR-LLM surpasses the standard LLM in inference efficiency. This is attributable to SONARLLMs design, which processes entire sentences as atomic units, thereby reducing the number of required decoding steps relative to token-based models. While the theoretical computational complexity remains quadratic for both approaches, the effective cost for SONAR-LLM grows much more slowly with sequence length because it operates on compressed sequence of sentence embeddings. In practice, this yields an almost linear growth in FLOPs up to 1 million tokens, as the quadratic term is scaled by the inverse square of the average sentence length. Conclusion We presented SONAR-LLM, decoder-only Transformer that predicts sentence embeddings and is supervised via token-level cross-entropy propagated through frozen SONAR decoder. This approach retains the semantic abstraction of concept-based models like LCM while restoring likelihood-based training signal. As proof of concept, we trained SONAR-LLM on the TINYSTORIES dataset. It showed faster loss reduction across training epochs than both MSE-based and diffusionbased LCMs, and demonstrated favorable scaling behaviour as model size increased. In GPT-4o evaluations, SONARLLM outperformed both LCM variants in grammar, coherence, creativity, and plot consistency. On standard NLG metrics, SONAR-LLM demonstrated strong performance, consistently matching or slightly surpassing the standard tokenlevel LLM. It also outperformed both the MSE-based and diffusion-based LCMs across all prefix lengths, establishing it as promising alternative for sentence-level generation tasks. To broaden the evaluation scope, we pretrained all models on diverse mixture of instructional and open-domain corpora. This enabled us to assess summarization capabilities on standard datasets such as XSum and CNN/DM. SONARLLM achieved consistently stronger performance than prior sentence-level baselines and demonstrated its ability to handle summarization tasks with competitive quality, further validating the effectiveness of our proposed objective in more realistic settings. Our theoretical FLOPs analysis further demonstrates that SONAR-LLM achieves superior inference efficiency for long contexts compared to token-level LLMs: beyond 4096 tokens, its total computational cost grows almost linearly with sequence length up to 1 million tokens. Importantly, this effect results from operating on sentence-level segments, but the underlying complexity is still quadratic. This property enables SONAR-LLM to serve as practical and scalable architecture for long-context generation. We plan to extend our research to more diverse and openended datasets, as well as explore scaling to larger model sizes to further assess the generalization and expressiveness of SONAR-LLM. Limitations While our study reveals clear trends among the evaluated model architectures, several limitations remain. First, our evaluation of generation quality combines standard automatic metrics (BLEU, ROUGE-L, METEOR) with GPT-4o-based assessments of grammar, coherence, creativity, and plot consistency. While the latter offers stronger proxy for human judgment, it is still limited by the behavior and biases of the underlying model. more complete evaluation would benefit from direct human annotation or broader qualitative analysis. Second, due to computational constraints, we limited training to four epochs and model sizes up to 900M parameters when constructing scaling laws, with minimal hyperparameter tuning. Reported results are based on single runs Li, X. L.; Thickstun, J.; Gulrajani, I.; Liang, P.; and Hashimoto, T. 2022. Diffusion-LM improves controllable text generation. In Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Lipman, Y.; Chen, R. T. Q.; Ben-Hamu, H.; Nickel, M.; and Le, M. 2023. Flow matching for generative modeling. arXiv:2210.02747. Llama Team, AI @ Meta. 2024. The Llama 3 herd of models. arXiv:2407.21783. Lou, A.; Meng, C.; and Ermon, S. 2024. Discrete diffusion modeling by estimating the ratios of the data distribution. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, 3281932848. PMLR. Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2017. Pointer sentinel mixture models. In Proceedings of the International Conference on Learning Representations. Narayan, S.; Cohen, S. B.; and Lapata, M. 2018. Dont give me the details, just the summary! Topic-agnostic text summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of transfer learning with unified textto-text transformer. Journal of Machine Learning Research, 21(140): 167. van den Oord, A.; Vinyals, O.; and Kavukcuoglu, K. 2017. Neural discrete representation learning. In Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 63066315. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need. In Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 59986008. per configuration, which may introduce some variance; however, we observed consistent trends across preliminary runs. Larger-scale training or more extensive exploration may influence the observed scaling behavior and is left for future work. References Austin, J.; Johnson, D. D.; Ho, J.; Tarlow, D.; and van den Berg, R. 2021. Structured denoising diffusion models in discrete state spaces. arXiv:2107.03006. Barrault, L.; Duquenne, P.; Elbayad, M.; Kozhevnikov, A.; Alastruey, B.; Andrews, P.; Coria, M.; Couairon, G.; Costajuss`a, M. R.; Dale, D.; Elsahar, H.; Heffernan, K.; Janeiro, J. M.; Tran, T.; Ropers, C.; Sanchez, E.; Roman, R. S.; Mourachko, A.; Saleem, S.; and Schwenk, H. 2024. Large concept models: Language modeling in sentence representation space. arXiv:2412.08821. Bowman, S. R.; Vilnis, L.; Vinyals, O.; Dai, A. M.; Jozefowicz, R.; and Bengio, S. 2016. Generating sentences from continuous space. In Proceedings of the 20th Conference on Computational Natural Language Learning, 1021. Association for Computational Linguistics. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language models are few-shot learners. In Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2020), 18771901. Curran Associates. Duquenne, P.; Schwenk, H.; and Sagot, B. 2023. SONAR: Sentence-level multimodal and language-agnostic represenIn Findings of the Association for Computational tations. Linguistics: ACL 2023, 49694983. Eldan, R.; and Li, Y. 2023. TinyStories: How small can language models be and still speak coherent English? arXiv:2305.07759. Gu, A.; Dao, T.; Dohan, D.; Bommasani, R.; and Liang, P. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752. Hermann, K. M.; Kocisky, T.; Grefenstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems 28. Hu, V.; Wu, D.; Asano, Y.; Mettes, P.; Fernando, B.; Ommer, B.; and Snoek, C. 2024. Flow matching for conditional text generation in few sampling steps. In Proceedings of the 18th European Chapter of the Association for Computational Linguistics (Short Papers), 380392. St. Julians, Malta: Association for Computational Linguistics. Jain, S.; Sun, S.; Yu, X.; and Raffel, C. 2023. Textbooks are all you need. arXiv:2306.11644. Kiss, T.; and Strunk, J. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4): 485525."
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE",
        "Innopolis University",
        "MSU",
        "Skoltech"
    ]
}