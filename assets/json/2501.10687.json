{
    "paper_title": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation",
    "authors": [
        "Linrui Tian",
        "Siqi Hu",
        "Qi Wang",
        "Bang Zhang",
        "Liefeng Bo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose a novel audio-driven talking head method capable of simultaneously generating highly expressive facial expressions and hand gestures. Unlike existing methods that focus on generating full-body or half-body poses, we investigate the challenges of co-speech gesture generation and identify the weak correspondence between audio features and full-body gestures as a key limitation. To address this, we redefine the task as a two-stage process. In the first stage, we generate hand poses directly from audio input, leveraging the strong correlation between audio signals and hand movements. In the second stage, we employ a diffusion model to synthesize video frames, incorporating the hand poses generated in the first stage to produce realistic facial expressions and body movements. Our experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, such as CyberHost and Vlogger, in terms of both visual quality and synchronization accuracy. This work provides a new perspective on audio-driven gesture generation and a robust framework for creating expressive and natural talking head animations."
        },
        {
            "title": "Start",
            "content": "EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Linrui Tian Siqi Hu Qi Wang Bang Zhang Liefeng Bo Institute for Intelligent Computing, Alibaba Group {tianlinrui.tlr, husiqi.hsq, wilson.wq, zhangbang.zb, liefeng.bo}@alibaba-inc.com https://humanaigc.github.io/emote-portrait-alive-2/ 5 2 0 2 8 1 ] . [ 1 7 8 6 0 1 . 1 0 5 2 : r Abstract In this paper, we propose novel audio-driven talking head method capable of simultaneously generating highly expressive facial expressions and hand gestures. Unlike existing methods that focus on generating full-body or half-body poses, we investigate the challenges of co-speech gesture generation and identify the weak correspondence between audio features and full-body gestures as key limitation. To address this, we redefine the task as two-stage process. In the first stage, we generate hand poses directly from audio input, leveraging the strong correlation between audio signals and hand movements. In the second stage, we employ diffusion model to synthesize video frames, incorporating the hand poses generated in the first stage to produce realistic facial expressions and body movements. Our experimental results demonstrate that the proposed method outperforms state-of-the-art approaches, such as CyberHost (Lin et al., 2024) and Vlogger (Corona et al., 2024), in terms of both visual quality and synchronization accuracy. This work provides new perspective on audio-driven gesture generation and robust framework for creating expressive and natural talking head animations. 1. Introduction Audio-driven human video generation, which aims to create synchronized facial expressions and body gestures, remains critical research area with wide range of applications. While there have been noteworthy achievements in generating synchronized facial expressions from audio (Tian et al., 2025) and human-centric videos (Kong et al., 2025), challenges persist in creating audio-synchronized human videos that exhibit richly vivid motions, particularly in the realm of co-speech video generation. Some methods (Corona et al., 2024; Lin et al., 2024; He et al., 2024) have attempted to address co-speech video generation, but they often fall short in reproducing rich body motion or lack the generalization ability necessary for diverse scenarios. Figure 1. The motivation behind our method. Human motion, similar to that of robots, involves planning the end-effector (EE), typically the hands, towards the target position. The rest of the body then cooperates accordingly with the EE, abiding by inverse kinematics principles. One of the fundamental challenges in this domain stems from the complexity of the human body, which operates as sophisticated multi-joint articulated system (Winter, 2009). This complexity makes it particularly difficult to predict rich gestures that are synchronized with audio, especially within pixel space. Several approaches (Pavlakos et al., 2019; Corona et al., 2024; Yi et al., 2023; Qi et al., 2024) have sought to define human body structure by focusing on predicting full-body or half-body poses, using either joint angles (Chen et al., 2024; Yi et al., 2023) or positions (Bie et al., 2022; Idrees et al., 2024). However, these approaches struggle with capturing the intricate coupling of multiple joints in natural human motion, particularly in speech-related activities, leading to suboptimal results. key observation in our work is that the correlation between audio and different body joints varies significantly. We observed that robotic control systems such as manipulators and humanoid robots are often designed to mimic human behavior. As illustrated in Figure 1, robotic systems typically define tools or grippers at the end as the end-effector (EE). Tasks for robots are often defined as steering the EEs 6-DoF (degree of freedom) posture beEMO2: End-Effector Guided Audio-Driven Avatar Video Generation cause the EEs motion provides more straightforward and intuitive task representation. This simplification eases the control process compared to managing each robot joint angle separately. In some robotic implementations, including manipulation (Tian et al., 2021) and motion planning (Hu et al., 2021), robot joint poses are often determined through Inverse Kinematics (IK). This principle inspires us to redefine the target for co-speech human motion generation. Hands, which act as the EE in daily life, are more directly tied to human intention and exhibit stronger relationship with audio than other body joints. For instance, when speaking or singing, individuals frequently produce intentional hand gestures that align with their expressive content. However, obtaining joint angles through IK can sometimes result in singularities, leading to suboptimal motion planning solution. To address this issue, some robotic methods (Rakita et al., 2018) incorporate prior knowledge into IK to ensure plausible joint solutions. Similarly, recent 2D pixel generation models (Rombach et al., 2022; Guo et al., 2023; Kong et al., 2025) have shown the integration of human body structure knowledge, suggesting that pixel generation models may implicitly incorporate insights about human IK. Thus, we could potentially use hand motion to represent partial body movement while leveraging 2D generative models capability to generate other body parts, which we refer to as pixels prior IK. This approach enables the reproduction of entire characters, synchronization of audio with lip movements, and preservation of proper body structure, ultimately resulting in coherent and continuous co-speech videos. Inspired by this insight, we propose two-stage framework for audio-driven gesture and facial expression generation. In the first stage, we focus on mapping audio to hand poses, capitalizing on the strong correlation between audio and hand movements. This approach simplifies the problem by reducing the complexity of the mapping space and allows for more precise control over gesture generation. In the second stage, we employ diffusion-based model to synthesize video frames, incorporating the generated hand poses to produce realistic facial expressions and body movements. By leveraging these insights, our method not only generates coherent and continuous co-speech videos but also ensures proper synchronization of audio with lip movements and realistic body dynamics. In summary, the main contributions of our work are as follows:: We identify the strong correlation between audio and hand movements and propose simplified, two-stage framework for audio-driven gesture generation. We introduce diffusion-based model for synthesizing realistic facial expressions and body movements from generated hand poses. We demonstrate through experiments that our method outperforms state-of-the-art approaches in terms of visual quality, synchronization accuracy, and motion diversity. 2. Related Work Co-speech gesture generation. Over the years, co-speech gesture synthesis, like expression synthesis, has evolved from rule-based methods to datadriven approaches. Rule-based methods implement hard mapping from speech to gestures by linking phonemes to specific gesture patterns (Cassell et al., 1994; Kipp, 2005; Wagner et al., 2014). Data-driven methods explore various neural network architectures to learn the connection between speech and gestures from extensive training data. For instance, Faceformer (Fan et al., 2022) employs transformer to learn audio-to-expression connections, Codetalker (Xing et al., 2023) incorporates learned codebook to strengthen connections with discrete motion priors, Talkshow (Yi et al., 2023) introduces speech-gesture paired dataset and implements an autoencoder for facial motions along with compositional VQ-VAE for body and hand motions, while (Habibie et al., 2021) utilizes GAN-based pipeline. Recently, inspired by the remarkable success of diffusionbased methods in image and video generation, motion generation approaches have begun to incorporate diffusion processes. DiffGesture (Zhu et al., 2023a) and DiffSHEG (Chen et al., 2024) developed diffusion processes for skeleton and SMPL (Pavlakos et al., 2019) pose sequences, using transformer to attend to audio or other modality inputs. Cocogesture (Qi et al., 2024) trained an audio control-net to integrate audio embeddings into gesture features. It is noteworthy that most methods initially generate parameters from predefined body structures such as SMPL and then convert them into gesture movements in 3D coordinates, with the final gesture qualities being constrained by the simplified body structure. Few methods attempt to implement diffusion process directly on hand postures. Audio-driven human video generation. Recent works (Kong et al., 2025; HaCohen et al., 2024; Yang et al., 2024) have significantly advanced human-centric video generation. However, research on audio-driven human video generation often focuses primarily on talking heads. EMO (Tian et al., 2025) introduces an audio-driven video diffusion framework for creating expressive talking head videos. Loopy (Jiang et al., 2024) incorporates longer sequences of historical information to enhance video vividEMO2: End-Effector Guided Audio-Driven Avatar Video Generation ness. Hallo (Xu et al., 2024; Cui et al., 2024a;b) explores methods to enhance audio control. Some studies aim to animate both the head and body motion with audio. Vlogger (Corona et al., 2024) develops two-stage method that uses audio to drive 3D human representation, followed by animation from this representation, but it shows insufficient correlation with audio input. Cyberhost (Lin et al., 2024) directly drives body movement with audio in pixel space. Echomimic (Meng et al., 2024) animates partial body parts using pre-extracted control signals while driving other parts with audio. He et al. (He et al., 2024) utilize latent motion signals for body movement. 3. Method 3.1. Overview Given single reference image of character, our approach can animate the character by inputting musical/vocal audio clip, preserving the natural facial expression and body motion in harmony with the variation of the fed audio. We propose two-stage method to drive body animation from single image using audio. In the first stage, as illustrated in Figure 2, we develop motion diffusion model where audio inputs are translated to synchronized partial body motion signals. These generated motion signals are then utilized in the second stage, as shown in Figure 3. The second stage employs ReferenceNet-based diffusion architecture (Hu et al., 2023; Tian et al., 2025), guided by both the audio and motion signals, animating the reference image into co-speech video. 3.2. Preliminaries Our two-stage methodology is based on the framework of diffusion models (DDPM) (Ho et al., 2020), which assume forward noising process where noise ϵ (0, 1) is gradually applied to real data x0, resulting in noisy data xt at specific timestep t. The diffusion model is trained to learn the reverse process, aiming to remove the noise ϵ from the noisy data xt. During this denoising process, control signals c, such as audio and mapping data in our method, are introduced to achieve the desired outcome x0. The training objective for this denoising process is defined as: = Et,c,xt,ϵ (cid:2)ϵ ϵθ(xt, t, c)2(cid:3) (1) where ϵθ denotes the diffusion models. In Stage 1, our method implements ϵθ using transformer-based structure (Vaswani et al., 2017), discussed in Section 3.3. Stage 2 employs the ReferenceNet (Zhu et al., 2023b) backbone, which is an evolution of the Latent Diffusion Model (LDM) (Rombach et al., 2022). It utilizes Variational Autoencoder (VAE) (Kingma & Welling, 2014) to map the original image feature distribution x0 into latent space z0 to reduce computational load, while using ReferenceNet to blend reference image features with the latent features. In Stage 2, ϵθ represents the Denoising U-Net, discussed in Section 3.4. 3.3. Partial Body Motion Generation Given audio inputs, instead of directly operating on pixels like EMO(Tian et al., 2025) does with facial expressions, partial body motion generation model is devised to produce co-speech gestures as the intermediate driving signal for generating the human body in videos. The rationale is that human body movements are naturally articulated and can be represented by predefined gestures using 3D models such as SMPL for the body and MANO(Romero et al., 2017) for the hands. These models provide unified parameters and facilitate easy control over body motion, but they compromise on expressiveness and diversity. Interestingly, we discover that using hand movements alone suffices to describe upper body motion, as the movements of other body parts can be derived through Inverse Kinematics (IK). Unlike the IK used in roboticswhich often encounters issues with singular solutions, necessitating robots to be hardcoded or trained with prior knowledge to avoid singularitiesIK in human video generation is effectively managed. This is because the video generation backbone is pretrained with large amount of human body structure data in the form of pixels, which provides robust prior knowledge for resolving human IK challenges. Therefore, in the motion generation stage, we only generate co-speech hand motions as the driving signal for the next stage of video generation. Experiments show that the video generation model can produce reasonable upper body motions given only the control signal of the hands. Additionally, releasing control over the arms and other parts of the body further increases the expressiveness and intensity of upper body motion, compared to existing motion generation methods such as DiffSHEG, Talkshow and CoCoGesture that operate on all joints of the upper body. To generate movement of the hands, we adopt the diffusion transformer (DiT)(Peebles & Xie, 2022) as the backbone due to its scalability to large-scale datasets. The model consists of multiple DiT blocks, each performing self-attention on noisy motion latents and cross-attention between audio features and noisy motion latents. Inspired by Pixartα(Chen et al., 2023), AdaLN-single is implemented to inject timestep embeddings into each DiT block. With the attention mechanism, coherent MANO hand coefficients consistent with speech rhythms are generated. To ensure smooth transitions between clips, the last frames of the motion sequence in the previous clip are concatenated to the current motion sequence. The audio features are extracted through Wav2vec(Schneider et al., 2019). Aside from hand movements, predefined keypoints of the human upper body are included as weak supervision for video generation. 3 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Figure 2. Overview of the stage 1 hand motion generation framework. The framework includes serveral DiT blocks as backbone. Audio embeddings are injected via cross-attention, style and speed embeddings are added on timestep, previous motion latent sequence is concatenated on current noisy motion latent sequence for smooth transition. Hand masks that mask out invisible hands frames are directly added on noisy motion latent. Hand Motion Mask and Hand Offset. In some frames of training dataset, the MANO hand annotations are inaccurate or missing because the hands are covered or out of sight in the frame. To filter out the influence of these bad cases, motion masks indicating the frames of valid annotated MANO hand parameters are used and added to the noisy motion latents. Motion masks also indicate the padding length of the sequence. An offset embedding that describes the relative position and rotation of the body is also added to disentangle hand motion from root body posture differences in the dataset. Style, Speed, and Reference Image Embedding. For generating hand motion in different styles like singing, speaking, and gesture dance, style embedding can be added to the timestep embedding. In addition, we filtered the hand movements into different speed buckets, similar to EMO, where each bucket has center and radius, and specific speed value is encoded based on its distance to each bucket. Speed embeddings are then added to the timestep to control the speed of each hand respectively. In practice, we found that using the variance of the hand translation to represent movement amplitude, rather than speed, can achieve more pronounced control over hand movement. Optionally, the information of reference image can be injected into the backbone to generate motion more suitable for the reference image context, such as when there are objects like guitar or microphone in hand. Given reference image, we adopt the vision model from CLIP(Radford et al., 2021) to obtain the encoded class embedding of the image and add it to the timestep, thus enabling the model to generate hand motion accordingly. 3.4. Co-Speech Video Generation The design of our video generation framework is based on the EMO(Tian et al., 2025). As shown in Figure 3, Our Backbone Network is fed with multi-frame noise latent input, and tries to denoise them to the consecutive video frames during each time step. The framework can be divided into four parts: 1) Denoising: The Backbone Network is denoising 2D-UNet integrated with temporal modules 4 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Figure 3. The overview of the Stage 2 video generation pipeline, which is based on the Parallel Reference Network structure. The ReferenceNet extracts visual features from both the reference image and motion frames. The MANO maps and keypoint maps generated in Stage 1 are passed through the denoising Backbone Network to guide the characters motion. Additionally, trainable hand confidence embeddings enhance the quality of the generated hands. The audio embeddings are injected to ensure synchronization between audio and visual elements. from AnimateDiff(Guo et al., 2023); 2) Frames Reference: To maintain the character ID, we deploy the ReferenceNet parallel to the Backbone, it inputs the reference image and motion frames(Tian et al., 2025) to get the 2D image features. Those features are injected into the Backbone through cross attention in the spatial and temporal dimensions respectively; 3) Audio-Driven: To drive the character by audio, the audio features shared with the stage 1 are integrated through cross-attention; 4) Motion Guidance: The MANO maps and keypoint maps generated in Stage 1 are concatenated channel-wise and integrated with latent features to modulate body motion. Hand Motion Control. The MANO maps produced in stage 1 guide the characters motion. They explicitly describe the hand movement in the generated frames, detailing aspects such as shape, size, and pose. However, even with these explicit control signals, poor MANO annotation in training dataset can lead to bad hand representations. Similar to the design of hand clarity control (Lin et al., 2024), we employ the hand confidence scores of MANO hand detection in training frames. These scores may decrease in situations of significant occlusion or motion blur, serving as conditional input to enhance the quality of generated hands. Specifically, we multiply trainable embedding by these confidence scores to create hand confidence embedding, which are directly added to the latent features. During inference, assigning higher confidence scores results in clearer and structurally correct hands. Keypoints Guidance Control. Similar to EchoMimicV2 (Meng et al., 2024), our initial implementation utilized handonly control signals, allowing other body parts to synchronize with audio signals and hand movements. However, we observed that large movements of the MANO hand signals were often incompatible with static torso, leading to unnatural performance in the videos. To address this issue, we introduce joint keypoints to supplement the motion-driven approach. The keypoints maps indicate the 2D positions of joints in the arms and legs. It is important to note that since the stage 1 model is trained using only 2D keypoint annotations, challenges arise in accurately maintaining 3D body structures, such as arm length. Consequently, the generated keypoints may not precisely define the entire body. How5 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Table 1. The Quantitative comparisons with other motion generation methods. Method DIV Talkshow 0.0961 Diffsheg 0.0158 0.0886 Ours on SMPL Ours on MANO 0.1345 BA 0.6743 0.7198 0.7290 0.7626 PCK 0.7872 0.8300 0.8163 0.8126 FGD DIV(smpl) BA(smpl) PCK(smpl) FGD(smpl) 0.0329 0.03676 0.0301 0.0373 0.0400 0.0306 0.1442 - 0.6769 0.7312 0.7285 - 0.9529 0.7635 0.7345 - 0.4170 4.2189 4.5746 - Figure 4. The distribution of the generated hand positions from co-speech gesture generation methods based on Talkshow dataset. From left to right: Ours MANO based,Ours SMPL based, Talkshow, Diffsheg. ever, they still serve as effective indicators of joint movement trends. Specifically, we apply large-kernel median filtering to the 2D keypoints along the temporal dimension during the training of the stage 2 model. This filtering approach forces the keypoints to intentionally misalign with the body joints, allowing the generated body movements to be guided by these points while still permitting the model some creative freedom. As result, this strategy facilitates the generation of dynamic performance videos. Pose Discriminator. Moreover, to enhance the character body structure, we implement pose discriminator during the training stage. Specifically, at each training timestep, we utilize one-step sampling to compute the latent prediction zt0 from the models output zt at timestep t. This latent result zt0 is then passed through the pose discriminator, which predicts the body pose keypoints and limb heatmap ˆH. The pose discriminator loss Lpd = ˆH2 is incorporated into the denoising loss during training, where represents the ground truth heatmap. The pose discriminator is based on ResNet (He et al., 2015) and is pretrained in the latent space. 4. Experimets 4.1. Implement Details We train our two generation models individually. In the first part of hand motion generation, 24 DiT blocks with hidden size of 512 are concatenated to form the backbone. The MANO hand model is used to describe hand movements, which includes 48 joint rotation values for each hand in axisangle representation and 3 translation values. To eliminate the ambiguity in rotation, the 48 axis angles are converted to 64 quaternion parameters, yielding total of 134 parameters for both hands. Hand parameter sequences are padded to 300 frames for arbitrary length generation, and the previous sequence of 12 frames is used for smooth transition from the previous clip to the current one. We trained the first part of the model on an A100 GPU with batch size of 8 for 400k training steps from scratch. In the second part of our approach, the video generation model training is divided into two stages. The first stage focuses on image training, where two frames are sampled and cropped to resolution of 704 512 pixels, serving as the reference frame and the target frame. During this stage, we optimize the ReferenceNet, the motion guidance layer, and the basic modules within the Backbone Network. The second stage involves audio-video training, during which the temporal modules, audio attention layers, and additional components are integrated into the model for optimization. For memory efficiency, the ReferenceNet is frozen in this stage. Each video clip comprises 24 frames at resolution of 704 512 pixels, with the number of motion frames set to 12. Both stages utilize 4 A100 GPUs, with batch sizes of 32 and 4 for the image and audio-video training stages, respectively. Each stage undergoes 100k training steps with learning rate of 1 105. Our training dataset is sourced from MOSEI(Zadeh et al., 2018) and AVSPEECH(Ephrat et al., 2018), which contain half-body speech scenarios. Additionally, we enhance our dataset by collecting videos from the internet, resulting in total duration of approximately 275 hours. 4.2. Hand Motion Generation Comparisons We compared our methods to other baseline motion generation methods using the Talkshow dataset, focusing primarily on hand movements. Since other methods mostly operate on SMPL poses, we first extracted hand translations from the generated SMPL poses before calculating hand movement 6 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Table 2. Quantitative comparisons with various body animation methods. The * symbol indicates evaluations performed on CyberHosts 12 demo videos, while ** denotes evaluations conducted on the Vloggers 30 demo videos from its homepage. SSIM Sync-C Method PNSR CSIM EFID HKV HKC FVD FID EchoMimicV2 MimicMotion w/o motion gen w/o hand confidence Ours 33.42 25.38 21.07 25.82 27.28 217.71 248.95 102.19 134.14 129.41 CyberHost* Ours* Vlogger** Ours** - - - - - - - - 0.662 0.585 0.751 0.659 0.662 - - - - 65.13 64.09 67.88 64.47 64.62 - - - - 4.44 2.68 4.59 4.11 4.58 4.54 4.70 1.94 5. 1.052 0.617 0.224 0.200 0.218 - - - - 0.425 0.356 0.461 0.537 0.553 0.723 0.723 0.611 0. 0.150 0.169 0.175 0.191 0.198 0.107 0.150 0.068 0.154 0.519 0.608 0.683 0.666 0.650 0.708 0.746 0.491 0. metrics for these methods. To demonstrate that directly generating hand motions with our methods outperforms all SMPL poses-based methods in terms of hand movement, we also trained our method using the same setup with SMPL poses. Each method was tested on 100 input audios from the Talkshow test dataset, and for each input audio, 50 results were generated. For metrics, DIV is used to measure the diversity of the generated results, BA describes the beat alignment between audio and the results, PCK is the percentage of results close to the ground truth (GT) motion, and Frechet Gesture Distance (FGD) represents the distance between the distribution of generated motion and GT motion. Table 1 demonstrates that our MANO version method leads by large margin regarding DIV, which we believe relates most to the vividness and expressiveness of hand motion, whereas other SMPL-based methods often generate monotonous results. Despite various starting gestures, these methods tend to move hands in front of the chest or stay in the first place. In terms of BA, our MANO-based method also shows superiority. These two metrics play the most important role in providing vivid and expressive co-speech driving signal for the next stage of video generation. Our methods lower scores in PCK and FGD are expected, as the hand movements from other methods are calculated based on the forward winematics of SMPL, thus restricted to be distributed closer to the GT hand movements. Our MANObased method, in contrast, has more freedom to move hands differently from the GTs. Qualitative results also show that discarding SMPL forward kinematics produces more flexibility without compromising the reasonability of the generation. We also compared our SMPL-based method with baselines in SMPL poses, and our method still holds distinct lead in diversity. In Figure 4, the 2D hand position distribution generated by various hand motion generation methods is illustrated. Our method is capable of producing positions with wider range and movements in more diverse patterns. 4.3. Video Generation Comparisons We conduct the upper body animation comparisons on the EMTD(Meng et al., 2024) dataset. To demonstrate the superiority of our proposed method, we evaluate the models using several metrics. We employ Frechet Inception Distance (FID)(Heusel et al., 2017), SSIM(Wang et al., 2004), and PSNR(Hore & Ziou, 2010) to assess the quality of the generated frames. Frechet Video Distance (FVD)(Unterthiner et al., 2019) is used to gauge the overall coherence of the generated videos. To evaluate identity consistency, we calculate the cosine similarity (CSIM) between the facial features of the reference image and the generated video frames. We also utilize Sync-C, as proposed by SyncNet(Chung & Zisserman, 2017), to assess the synchronization quality between lip movements and audio signals. Furthermore, we measure Hand Keypoint Confidence (HKC) to evaluate the quality of hand representation in generated frames, while Hand Keypoint Variance (HKV) serves as an indicator of the richness of hand movement. Additionally, EFID(Tian et al., 2025) is adopted to quantitatively assess the divergence in expressions between the synthesized videos and those in the ground truth dataset. There are few works that have achieved audio-driven body animation; however, some, such as DiffTED(Hogue et al., 2024), have not open-sourced their pre-trained models, while others like Vlogger(Corona et al., 2024) and CyberHost(Lin et al., 2024) have only released demo videos. To facilitate comparisons with other state-of-the-art methods, we compare our approach with EchoMimicV2(Meng et al., 2024), which animates the hands using pre-extracted hand pose sequences while driving the rest of the body through audio. We also compare our work with MimicMotion(Zhang et al., 2024), video-driven body reenactment method. Both EchoMimicV2 and MimicMotion rely on the pose sequences from the ground truth to animate the body. 7 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Figure 5. The qualitative comparisons with pose-driven body animation methods, based on the EMTD dataset. Since CyberHost and Vlogger have not open-sourced their models, we conduct relevant comparison experiments based on demo videos retrieved from their respective homepages. As illustrated in Table 2, w/o motion gen denotes that our model utilizes the pose sequence from the ground truth instead of the motion generation results, compared to EchoMimicV2 and MimicMoton, the results demonstrate an advantage in video quality assessment, as evidenced by the lower FVD scores. Additionally, our method outperforms others in terms of individual frame quality, as indicated by improved image quality (FID, SSIM, PSNR) scores. Compared w/o motion gen with other validation sets, using the original pose as driver will naturally make the generated results consistent with GT, thereby improving the quality metrics of images and videos, however, higher HKV denotes that our model could generate more diverse motion sequence. Meanwhile, our models have the ability to maintain the identity proved by the CSIM. Lower EFID proved that our model could generate more lively facial expressions. Besides, as illustrated in Figure 5, our model could generate hands with clear structure and logical interaction. We also compared our model with CyberHost and Vlogger based on demos from their homepages. The results in Table 2 show that, since most of Vloggers demos exhibit minimal motion (indicated by very low HKV), the HKC is relatively high. However, proved by the highest HKV, our model has the ability to generate much more diverse motion than other methods. The quality comparisons are shown in Figure 6, our method could generate frames with higher quality, while better preserve the original facial features. 5. Conclusion This paper devises two-stage framework for co-speech human video generation based on diffusion models. In this framework, EMO (Tian et al., 2025) is extended to enable the generation of holistic facial expressions and upper body motions. We introduce the concept of pixels prior IK , because we find that hand movements are the most correlated with audios among the different body parts, and the video generation backbone naturally incorporates human body Inverse Kinematics priors in form of pixels. Consequently, in Stage 1, only hand movements are generated and subsequently used as control signals in Stage 2. The results demonstrate that this framework can produce more expressive and vivid human videos compared to other methods. We hope that this work can offer new thought for audio-driven talking head animation. 8 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Figure 6. The qualitative comparisons with audio-driven body animation methods."
        },
        {
            "title": "References",
            "content": "Bie, X., Guo, W., Leglaive, S., Girin, L., Moreno-Noguer, F., and Alameda-Pineda, X. Hit-dvae: Human motion generation via hierarchical transformer dynamical vae. arXiv preprint arXiv:2204.01565, 2022. Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Becket, T., Douville, B., Prevost, S., and Stone, M. Animated conversation: rule-based generation of facial expression, gesture & spoken intonation for multiple conversational agents. In Proceedings of the 21st Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 94, pp. 413420, New York, NY, USA, 1994. Association for Computing Machinery. ISBN 0897916670. doi: 10.1145/192161.192272. URL https://doi.org/10.1145/192161.192272. image synthesis, 2023. URL https://arxiv.org/ abs/2310.00426. Chen, J., Liu, Y., Wang, J., Zeng, A., Li, Y., and Chen, Q. Diffsheg: diffusion-based approach for real-time speech-driven holistic 3d expression and gesture generation, 2024. URL https://arxiv.org/abs/2401. 04747. Chung, J. S. and Zisserman, A. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13, pp. 251263. Springer, 2017. Corona, E., Zanfir, A., Bazavan, E. G., Kolotouros, N., Alldieck, T., and Sminchisescu, C. Vlogger: Multimodal diffusion for embodied avatar synthesis, 2024. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart-α: Fast training of diffusion transformer for photorealistic text-toCui, J., Li, H., Yao, Y., Zhu, H., Shang, H., Cheng, K., and Zhou, H. Hallo2: Long-duration and high-resolution audio-driven portrait image animation, 2024a. 9 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Cui, J., Li, H., Zhang, Y., Shang, H., Cheng, K., Ma, Y., Mu, S., and Zhou, H. Hallo3: Highly dynamic and realistic portrait image animation with diffusion transformer networks, 2024b. Hore, A. and Ziou, D. Image quality metrics: Psnr vs. ssim. In 2010 20th International Conference on Pattern Recognition, pp. 23662369, 2010. doi: 10.1109/ICPR. 2010.579. Ephrat, A., Mosseri, I., Lang, O., Dekel, T., Wilson, K., Hassidim, A., Freeman, W. T., and Rubinstein, M. Looking to listen at the cocktail party: speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. Fan, Y., Lin, Z., Saito, J., Wang, W., and Komura, T. Faceformer: Speech-driven 3d facial animation with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Guo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., and Dai, B. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Hu, L., Gao, X., Zhang, P., Sun, K., Zhang, B., and Bo, L. Animate anyone: Consistent and controllable image-tovideo synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. Hu, S., Babaians, E., Karimi, M., and Steinbach, E. Nmpcmp: Real-time nonlinear model predictive control for safe motion planning in manipulator teleoperation. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 83098316, 2021. doi: 10.1109/IROS51168.2021.9636802. Idrees, S., Choi, J., and Sohn, S. Advmt: Adversarial motion transformer for long-term human motion prediction. arXiv preprint arXiv:2401.05018, 2024. Habibie, I., Xu, W., Mehta, D., Liu, L., Seidel, H.-P., PonsMoll, G., Elgharib, M., and Theobalt, C. Learning speechdriven 3d conversational gestures from video, 2021. URL https://arxiv.org/abs/2102.06837. Jiang, J., Liang, C., Yang, J., Lin, G., Zhong, T., and Zheng, Y. Loopy: Taming audio-driven portrait avatar with long-term motion dependency, 2024. URL https: //arxiv.org/abs/2409.02634. HaCohen, Y., Chiprut, N., Brazowski, B., Shalem, D., Moshe, D., Richardson, E., Levin, E., Shiran, G., Zabari, N., Gordon, O., Panet, P., Weissbuch, S., Kulikov, V., Bitterman, Y., Melumian, Z., and Bibi, O. Ltxvideo: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. URL http://arxiv.org/abs/1512.03385. He, X., Huang, Q., Zhang, Z., Lin, Z., Wu, Z., Yang, S., Li, M., Chen, Z., Xu, S., and Wu, X. Co-speech gesture video generation via motion-decoupled diffusion model, 2024. URL https://arxiv.org/abs/2404.01862. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hogue, S., Zhang, C., Daruger, H., Tian, Y., and Guo, X. Diffted: One-shot audio-driven ted talk video generation with diffusion-based co-speech gestures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 19221931, June 2024. Kingma, D. P. and Welling, M. Auto-Encoding Variational In 2nd International Conference on Learning Bayes. Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. Kipp, M. Gesture generation by imitation: from to computer character animation. human behavior 2005. URL https://api.semanticscholar. org/CorpusID:26271318. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q., Yuan, J., Long, Y., Wang, A., Wang, A., Li, C., Huang, D., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J., Xue, J., Wang, J., Wang, K., Liu, M., Li, P., Li, S., Wang, W., Yu, W., Deng, X., Li, Y., Chen, Y., Cui, Y., Peng, Y., Yu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y., Lu, Q., Liu, S., Zhou, D., Wang, H., Yang, Y., Wang, D., Liu, Y., Jiang, J., and Zhong, C. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. Lin, G., Jiang, J., Liang, C., Zhong, T., Yang, J., and Zheng, Y. Cyberhost: Taming audio-driven avatar diffusion model with region codebook attention, 2024. URL https://arxiv.org/abs/2409.01876. Meng, R., Zhang, X., Li, Y., and Ma, C. Echomimicv2: Towards striking, simplified, and semi-body human animation, 2024. URL https://arxiv.org/abs/2411. 10061. 10 EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Pavlakos, G., Choutas, V., Ghorbani, N., Bolkart, T., Osman, A. A. A., Tzionas, D., and Black, M. J. Expressive body capture: 3D hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pp. 1097510985, 2019. Peebles, W. and Xie, S. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Qi, X., Zhang, H., Wang, Y., Pan, J., Liu, C., Li, P., Chi, X., Li, M., Xue, W., Zhang, S., Luo, W., Liu, Q., and Guo, Y. Cocogesture: Toward coherent co-speech 3d gesture generation in the wild, 2024. URL https://arxiv. org/abs/2405.16874. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PMLR, 2021. Rakita, D., Mutlu, B., and Gleicher, M. Relaxedik: Realtime synthesis of accurate and feasible robot arm motion. In Robotics: Science and Systems, volume 14, pp. 2630. Pittsburgh, PA, 2018. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Romero, J., Tzionas, D., and Black, M. J. Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 36 (6), November 2017. Schneider, S., Baevski, A., Collobert, R., and Auli, M. wav2vec: Unsupervised pre-training for speech recognition. pp. 34653469, 09 2019. doi: 10.21437/Interspeech. 2019-1873. Tian, L., Wu, J., Xiong, Z., and Zhu, X. Vote for grasp poses from noisy point sets by learning from human. In 2021 27th International Conference on Mechatronics and Machine Vision in Practice (M2VIP), pp. 349356. IEEE, 2021. Tian, L., Wang, Q., Zhang, B., and Bo, L. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pp. 244260. Springer, 2025. Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. Fvd: new metric for video generation. 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Wagner, P., Malisz, Z., and Kopp, S. Gesture Speech and speech in interaction: An overview. Communication, 57:209232, 2014. ISSN 01676393. doi: https://doi.org/10.1016/j.specom.2013.09. 008. URL https://www.sciencedirect.com/ science/article/pii/S0167639313001295. Wang, Z., Bovik, A., Sheikh, H., and Simoncelli, E. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13 (4):600612, 2004. doi: 10.1109/TIP.2003.819861. Winter, D. A. Biomechanics and motor control of human movement. John wiley & sons, 2009. Xing, J., Xia, M., Zhang, Y., Cun, X., Wang, J., and Wong, T.-T. Codetalker: Speech-driven 3d facial animation with discrete motion prior, 2023. URL https://arxiv. org/abs/2301.02379. Xu, M., Li, H., Su, Q., Shang, H., Zhang, L., and Liu, C. Hallo: Hierarchical audio-driven visual synthesis for portrait image animation, 2024. URL https:// arxiv.org/abs/2406.08801. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yi, H., Liang, H., Liu, Y., Cao, Q., Wen, Y., Bolkart, T., Tao, D., and Black, M. J. Generating holistic 3d human motion from speech. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 469480, June 2023. Zadeh, A. B., Liang, P. P., Poria, S., Cambria, E., and Morency, L.-P. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 22362246, 2018. Zhang, Y., Gu, J., Wang, L.-W., Wang, H., Cheng, J., Zhu, Y., and Zou, F. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. EMO2: End-Effector Guided Audio-Driven Avatar Video Generation Zhu, L., Liu, X., Liu, X., Qian, R., Liu, Z., and Yu, L. Taming diffusion models for audio-driven co-speech gesture In Proceedings of the IEEE/CVF Confergeneration. ence on Computer Vision and Pattern Recognition, pp. 1054410553, 2023a. Zhu, L., Yang, D., Zhu, T., Reda, F., Chan, W., Saharia, C., Norouzi, M., and Kemelmacher-Shlizerman, I. Tryondiffusion: tale of two unets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 46064615, 2023b."
        }
    ],
    "affiliations": [
        "Institute for Intelligent Computing, Alibaba Group"
    ]
}