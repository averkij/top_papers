{
    "paper_title": "Vibe Spaces for Creatively Connecting and Expressing Visual Concepts",
    "authors": [
        "Huzheng Yang",
        "Katherine Xu",
        "Andrew Lu",
        "Michael D. Grossberg",
        "Yutong Bai",
        "Jianbo Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods."
        },
        {
            "title": "Start",
            "content": "Huzheng Yang1 Katherine Xu1 Andrew Lu1 Michael D. Grossberg2 Yutong Bai3 Jianbo Shi1 1UPenn 2CUNY 3UC Berkeley https://huzeyann.github.io/VibeSpace-webpage/ 5 2 0 D 6 1 ] . [ 1 4 8 8 4 1 . 2 1 5 2 : r Figure 1. What are the most relevant attributes for blending the violin player and guitar player? It is the instrument and how it is played, not the color or background. We call these attributes the vibe1. Recent diffusion-based morphing methods such as Yu et al. [47] and LLMs like GPT [30] and Gemini [12] struggle to blend the vibe, instead interpolating pixels, transferring style, or composing parts. We propose Vibe Space for identifying the vibe between input images and generating coherent, continuous blends that merge the vibe (Vibe Blending). With the discovered vibe, we can extrapolate to nontrivial but related concepts, such as Hilary Hahn playing guitar (Vibe Analogy)."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributestheir vibe. We introduce Vibe Blending, novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design cognitively inspired framework combining human judgments, LLM reasoning, and geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods. In Fig. 1, imagine blending musician playing violin with one playing guitar. What are the most relevant attributes for blending? Large language models (LLMs) like Gemini or GPT might focus on object parts or style transfer, whereas musician would attend to the instrument and how it is played. The intuitive process of identifying and fusing meaningful attributesthe vibe1reveals creative connections between distinct concepts. We call this process Vibe Blending: creating coherent hybrids that merge the relevant shared attributes between images. In Fig. 1, blending the vibe of violin and guitar yields lute: an instrument played like guitar but similar in size to violin. Psychologist Sarnoff A. Mednick proposed that creativity arises from linking distant concepts in new and mean1The term vibe, short for vibration, originated in 1960s jazz slang to describe the mood or feeling conveyed by music, person, or space. 1 ingful ways [28], such as poet fusing lions ferocious chrysanthemum head or scientist uniting disparate theories. Memory, attention, and control are central to creativity [3]. LLMs have expansive memory through training data but limited attention and control, often struggling to recognize which attributes matter (e.g., the instrument and body pose in the violinguitar example) and how visual features should be merged to express those attributes coherently. The path of creatively blending the vibe is rarely linear. When two concepts lie far apart in latent space, naive linear interpolation produces incoherent images that lack semantic integrity. We propose Vibe Space to discover and traverse non-linear paths connecting distant visual concepts in pretrained feature spaces like CLIP. Our key insight is to reframe the problem: instead of navigating the full ambient feature space, we learn compact, low-dimensional manifolda small worldfrom few context images that captures the relevant geometry. Steps in this small world correspond to meaningful transitions in the original feature space, enabling coherent creative blending. Evaluating creativity requires new metrics and datasets. We take clues from cognitive psychology and study whether we can quantify human judgments of creative potential and blend difficulty using foundation models like CLIP and LLMs. We curate evaluation datasets by selecting conceptually interesting image pairs from Totally Looks Like [36], as well as from architectural design images. Our results show that LLMs offer useful starting point for quantifying creativity. We also introduce path-based metric for blend difficulty, enabling the curation of more challenging and engaging image pairs for future blending tasks. Our key contributions are the following: 1. Vibe Blending, task for generating creative, continuous morphs between visual concepts by discovering and merging the most relevant shared attributes. 2. We propose Vibe Space, method to learn hierarchical graph manifold that guides geodesics in ambient feature spaces (e.g., CLIP or DINO) for Vibe Blending with minimal on-the-fly training (under 1M parameters). 3. Drawing inspiration from cognitive psychology, we curate datasets and conduct human-subject experiments to measure creativity in Vibe Blending through creative potential and blend difficulty. 4. Our method generates more creative blends than strong baselines, such as Gemini and GPT. 2. Related Work Creativity is often regarded abstract, but can be quantified through semantic memory networks, where concepts are nodes connected by associations [3]. Cognitive psychology suggests that more creative individuals traverse the network further, connecting distant or weakly linked concepts [20, 23, 28]. Rather than jumping directly between remote ideas (e.g., apple house), they move through intermediate associations (e.g., apple tree wood house), following nonlinear paths across clusters of related concepts. This perspective motivates Vibe Blending, which fuses the relevant attributes of different concepts. Prior methods have mainly focused on pixel-level image blending in the latent space of generative models, such as GANs [31] and diffusion models [8, 19, 21, 39, 44, 47, 48]. Additional work [24, 25, 32, 41] explored finding semantic directions in the latent space of diffusion models, such as for the noise space [5, 37], weight space [13, 17], and text embedding space [2, 27]. Closest to our setting is AID [21], which performs attention interpolation inside diffusion models by fusing the attention. AID treats all attributes uniformly and relies on attention correspondences rather than identifying most relevant attributes for the blend. Recent work on creative image blending [33, 38] has relied on language, which often fails to capture precise visual attributes. In contrast, our method operates directly in image feature space and outperforms custom-trained diffusion models for morphing and recent multimodal LLMs. Figure 2. Our method generates coherent blends that focus on the most relevant attributes shared by the input imagesthe hairstyle. Diffusion-based morphing methods like DiffMorpher [48] and Yu et al. [47] struggle to produce realistic blends of distant concepts, and AID [21] fails to capture hairstyle as the relevant attribute. 3. Creative Path Finding 3.1. Finding Paths to Connect Concepts Creatively Creativity often emerges from finding connections between different concepts through key attributes, or vibe. There are many possible attributes, and each attribute can link concepts through multiple paths within the conceptual space. We aim to discover simple path that captures the vibe without unnecessary complexity. This idea parallels how human creativity involves navigating rich semantic networks while using attention and cognitive control to focus on meaningful connections [3, 4]. We model the creative process using co-saliency, which reveals the relevant attributes that are jointly prominent across concepts. Figure 3. Top: Using 2D point cloud example, the forward mapping involves computing the (a.) affinity graph of the points on the manifold and (b.) generalized eigenvectors Ψ(x) of the graph Laplacian as manifold coordinates of the point cloud x. (c.) The inverse mapping performs linear interpolation in the manifold space and uses graph diffusion inversion to obtain the corresponding path in the original point cloud space. Bottom: (d.) On real images, we extract patch tokens from DINO features as graph nodes and compute token-wise affinity W. (e.) The top graph eigenvectors produce co-salient segments across two images for blending, and manifold coordinates for expressing vibe features for blending. (f.) Similar to the point cloud example, we apply graph diffusion inversion to obtain path in CLIP space. We render pixel images from CLIP features using frozen IP-Adapter [45]. We train two lightweight MLP networks in under 30 seconds: an encoder to simulate and compress the forward mapping, and decoder to mimic the inverse mapping. However, finding meaningful paths within the conceptual space is difficult. High-dimensional feature spaces are highly nonlinear and contain numerous holesregions corresponding to implausible, distorted, or low-quality images. Naive linear interpolation in latent space often produces inconsistent intermediate images or ghosting artifacts. We hypothesize that these problematic holes arise from fundamental mismatch between the intrinsic dimension of the data manifold and the higher-dimensional latent space. Recent diffusion-based morphing methods like DiffMorpher [48], Yu et al. [47], and AID [21] also struggle to connect distant or spatially misaligned concepts  (Fig. 2)  . Graph Diffusion and Unfolded Manifold Coordinates. We derive latent graph diffusion space that captures the intrinsic geometry of the data manifold, where connecting concepts yields geodesics2 that stay close to the manifold. Finding path along manifold whose structure is approximated by feature similarity graph relates to the 2A geodesic is shortest path (curve) between points on manifold. generalized eigenvectors of the graphs Laplacian matrix (Fig. 3a). The process of unfolding the manifold aims to reveal these paths. The graph Laplacian is defined as = W, where is the diagonal degree matrix. The eigenvectors of (or its normalized variants) in particular the eigenvectors associated with the smallest nonzero eigenvalues, ψ2, . . . , ψm+1 serve as the new coordinates for the data points (Fig. 3b, 3e). These graph eigenvectors capture the intrinsic geometric structure of the manifold. Note that the graph diffusion map [11] denoted by Ψ is classical manifold-learning construction based on graph eigenvectors (not related to diffusion-based image generation). Projecting the original manifold into the graph diffusion map makes the originally curved manifold paths approximately linear and easy to compute. Graph Diffusion for Manifold Distance and Geodesics. Let Dt(xA, xB) between two points xA and xB represent the probability that random walk will connect them in time steps. key property of diffusion maps [11] is that this diffusion distance equals the Euclidean distance between 3 points in the embedding space Ψt (Fig. 3b right): 4. Vibe Space and Image Generation Dt(xi, xj) = Ψt(xi) Ψt(xj)2, (1) 1ψ1(i), λt where the diffusion map Ψt projects each data point xi into the first eigenvectors of the graph Laplacian Ψt(xi) = (λt mψm(i)). This embedding enables constructing geodesics by iteratively querying midpoints that have smoothly varying diffusion distance with respect to the endpoints. 2ψ2(i), . . . , λt Geodesics from Inverse Diffusion Mapping. Having embedded the data into graph diffusion space, we can now recover geodesics in the original manifold by inverse mapping. Given two points xA and xB with diffusion coordinates Ψt(xA) and Ψt(xB), we seek path γ(α), α [0, 1] with γ(0) = xA and γ(1) = xB. We first compute the interpolated diffusion coordinate Ψt(xα) = (1 α) Ψt(xA) + α Ψt(xB), then recover the corresponding path γ(α) on the original manifold via inverse mapping (Fig. 3c): (cid:13)Ψt(x) Ψt(xα)(cid:13) (cid:13) (cid:13) (2) . γ(α) = arg min 2 2 Inverse map optimization is tractable because (1) the Jacobian Ψt(x) admits closed-form expression via eigenvalue perturbation theory [35], and (2) the Nystrom approximation [15] enables efficient updates of the Ψt(x) under small perturbations. Together, these yield practical solver for Eq. (2): we iteratively adjust to align its diffusion coordinates with the target Ψt(xα), producing path γ(α) that stays close to the data manifold. 3.2. Flag Space for Multiscale Paths Why Flag Space. Graph Laplacian eigenvectors produce manifold coordinates to guide geodesic path, and they capture geometry at different scales: leading eigenvectors describe global structure, while higher-order eigenvectors encode local variations. How many eigenvectors should we keep? Truncating to fixed Ψ1:m selects one scale and discards the rest, leading to path that focuses on too many or too few attributes. We propose to use flag space3 [7], hierarchy of nested embeddings Ψ1:m1 Ψ1:m2 Ψ1:mM , where m1 < m2 < < mM , to encapsulate both the coarse and fine manifold structures and to alleviate the impact of picking the wrong number of eigenvectors (Fig. 3b, 3e). Multi-scale Path. We extend inverse diffusion mapping in Equation (2) to operate over flag space. Using the same linear interpolation Ψt(xα) in diffusion space, the manifold path γ(α), α [0, 1] is recovered by minimizing the average reconstruction error across set of scales M: γ(α) = arg min (cid:88) 1 (cid:124) mkM (cid:123)(cid:122) (cid:125) flag space (cid:13) (cid:13)Ψ1:mk (cid:124) (x) Ψ1:mk (cid:123)(cid:122) inverse mapping (xα)(cid:13) (cid:13) 2 2 (cid:125) . (3) To speed up the inverse mapping, we learn two small MLPs on the fly: an encoder mimics the flag-space diffusion map, and decoder learns the inverse mapping. Feature extraction and graph eigenvector computation4 run in milliseconds. Encoderdecoder training runs in under 30s. 4.1. Learning the Vibe Space Given dense DINO [9] features R(HW )D, the encoder : DINO Vibe maps each token to latent representation = (x) R(HW )d, where (typically 6) (Fig. 3d). The decoder : Vibe CLIP maps this latent representation back to CLIP [34] space. We refer to the latent embedding produced by as the Vibe Space. Its geometry is trained to align with the multiscale structure of flag-space diffusion maps, so that Euclidean distances and linear paths in Vibe Space correspond to geodesic distances and semantic paths on the underlying manifold (Fig. 3e, 3f). We enforce geometric alignment by matching the Gram matrix zz to the flag-space kernel matrix S(Ψ(x)): Lflag enc(f ) = (cid:13) Lflag dec(f, g) = (cid:13) 2 (cid:13)zz S(Ψ(x))(cid:13) , (cid:13) (cid:13)zz S(Ψ(g(z)))(cid:13) (cid:13) (cid:80) = (x), (4) 2 2 , where S(Ψ(x))ij = 1 mkM Ψ1:mk (xi)Ψ1:mk (xj) aggregates inner products across nested eigen-embeddings. To improve generalization beyond observed data, we add extrapolation regularization via random samples zsample: Lsample(g) = (cid:13) (cid:13)zsamplez sample S(cid:0)Ψ(cid:0)g(zsample)(cid:1)(cid:1) (cid:13) (cid:13) 2 2 . (5) Finally, we bridge perception and generation by encoding DINO features into Vibe Space and decoding them to CLIP, retaining DINOs semantic richness while remaining compatible with CLIP-conditioned diffusion generators [45]. Lrecon(f, g) = xclip g(f (xdino))2 2. (6) Since zz S(Ψ(x)), linear interpolation in Vibe Space approximates the multiscale inverse-diffusion geodesic in closed form (Fig. 3f). images (IA, IB), Vibe Blending. Given two input our goal is to generate intermediate blended images {Iα}α[0,1]. Vibe Blending operates in four steps: (1) train the Vibe Space, (2) determine which pair of attributes to blend by correspondence matching, (3) interpolate path in Vibe Space, and (4) decode the path and generate images. Algorithm 1 summarizes the full procedure. Algorithm 1 lines 1-3 optionally accept additional related exemplars. Although two images suffice to train the Vibe space and identify the dominant attributes, adding related exemplars can enhance the dominant attributes. This enforces consistency across global and local geometry when finding the path γ(α) between xA and xB. 3Flag space: https://en.wikipedia.org/wiki/Flag (linear algebra) 4Graph eigenvectors are computed efficiently in milliseconds [43]. 4 Figure 4. From Vibe Blending to Vibe Analogy. Vibe Space enables creative connections between input images and B. path approximately linear in Vibe Space results in continuous manifold-following path in the ambient feature space, such as CLIP. We can lift the vibe AB to non-trivial but related image to extrapolate an analogous path in the ambient space, resulting in image that reflects the same vibe. For example, we can morph Leonardo DiCaprios face into playing card. Algorithm 1 Vibe Blending xdino 2 Wij ), Dii = (cid:80) Input: Images (IA, IB), image features (xdino, xclip) Output: Generated intermediate images {Iα}α[0,1] xdino σ2 1: Wij = exp( Graph 2: (D W)Ψ(xdino) = λDΨ(xdino) Graph Diffusion Map 3: f, Train(xclip, xdino, Ψ(xdino)) Train Vibe Space ); zB = (xdino 4: zA = (xdino Encode vibe ) 5: π Match(xdino , xdino Cluster correspondence ) 6: AB = π(zB) zA Path direction 7: for α [0, 1] do 8: 9: 10: 11: end for zα = zA + α AB xclip Iα IPAdapter(xclip α ) Path interpolation Decode vibe Generate image α = g(zα) Algorithm 1 lines 5-6 determine which attributes should be blended. Since concepts rarely align at the pixel level, we first cluster DINO tokens in each image into semantic segments using k-way NCut [46]. We then compute segmentlevel correspondence using the Hungarian algorithm. Each segment in IA is matched to segment in IB, yielding bijection π : IB IA. This mapping AB = π(zB) zA establishes how attributes in IA should morph toward IB. Algorithm 1 line 10 generates images with IP-Adapter [45], diffusion model conditioned on dense CLIP image features. No finetuning of the IP-Adapter is required. In Fig. 4, after learning transition IA Vibe Analogy. IB, we reuse its displacement field AB. We align IA to IA via the same region correspondences, apply AB in Vibe Space to obtain zB, then decode to IB. 4.2. Creative Control Just as how set of positive exemplars can be used to drive Vibe Blending, we can also exclude negative vibes with negative exemplars  (Fig. 5)  . Given set of images Ipos for blending and set Ineg that captures negative vibe to remove, we suppress undesired attributes by projecting the positive attribute basis away from directions spanned by the negative exemplars. We compute flag-space eigenvectors for both positive (Ψpos) and negative (Ψneg) sets 5 Figure 5. Negative vibe control. Vibe attributes are implicitly extracted by Vibe Space. The blending pair defines desired vibes (rotation + style). The negative pair defines vibes to suppress (style). Blending without negative examples transfers both attributes. Subtracting the negative vibe, only rotation is blended. and orthogonalize the positive basis against the negative: Ψfiltered = Ψpos β Ψneg(Ψ negΨpos). Training the Vibe Space to match the kernel of this filtered basis, S(Ψfiltered), learns representation that avoids the undesired attributes. 5. How to Measure the Creativity of Blend? Measuring creativity is challenging and underdefined: what counts as creative is context-dependent and shifts as human expectations and model capabilities evolve. We take clues from cognitive psychology [3] to assess blend creativity, and we examine these insights through human evaluation. We also investigate how well LLMs can approximate human judgments of creativity via step-by-step reasoning. Specifically, we seek to answer the following questions: 1. How do humans rate the creativity of blend? 2. How to score blend difficulty using pretrained models? 3. How do human preferences reflect blend creativity? 4. How well do LLMs gauge the creativity of blend? We construct evaluation datasets where each image pair contains distinct concepts that share visual attributes but are not spatially aligned. We curate 44 image pairs from the human-annotated Totally Looks Like dataset [36], which exhibits humorous similarities in unrelated concepts. We also create 300 image pairs of architectural designs, where shared attributes exist but coherent blends are non-obvious. We compare using our Vibe Space for blending images with recent multimodal LLMs: OpenAIs GPT Image 1 [30] Figure 6. Left: To gauge human perceptions of creativity, we ask raters to compare image pairs along two axes: Creative Potential refers to how interesting blend might be, and Blend Difficulty indicates how challenging it is to form coherent blend. Image pairs with higher Blend Difficulty tend to have higher Creative Potential and are often more conceptually different. Numbers in each cell indicate the number of examples. Right: When evaluating blend creativity across methods, raters first identify the key shared attributes in the input images. and Gemini 2.5 Flash Image [12] with step-by-step reasoning [33, 40]. We also implement CLIP Avg, which averages the CLIP image embeddings of the inputs and feeds the resulting embedding into IP-Adapter [45]. Figure 7. Top: Vibe-decoded (curved) vs. straight path in CLIP space for single image pair; we sample 20 points to capture curvature. Pairs that humans judge as more difficult exhibit more curvature. Bottom: For each comparison pair (two different image pairs shown together), we plot PNS = PNSpreferred PNSother. Positive values indicate that PNS agrees with the human choice. Larger PNS correlates with higher rater consensus. Negative values indicate disagreement with human consensus. 5.1. How Do Humans Rate Blend Creativity? To gauge human perceptions of creativity, we ask raters to compare different image pairs along two related but distinct 6 axes, independent of any methods output: 1. Creative Potential: How much the content of the image pair allows for an interesting or compelling blend. 2. Blend Difficulty: How challenging it would be to create coherent hybrid that fuses the shared attributes. These axes are informed by cognitive psychology studies suggesting that human creativity involves both attention and cognitive control to produce meaningful connections between concepts [3, 4]. Creative Potential reflects attention to the key visual attributes shared by the imagesthe vibe that makes blend interesting or compellingwhile Blend Difficulty reflects the cognitive control required to integrate these attributes into coherent hybrid. We conducted user study on image pairs from Totally Looks Like [36]. Humans were presented with two image pairs at time and asked to indicate which pair has higher Creative Potential and which has higher Blend Difficulty, or select tie if they are similar. Each image pair was compared multiple times across ten raters, and these pairwise judgments were aggregated to assign each image pair score along both axes. The scores were discretized into low, medium, and high bins of Creative Potential and Blend Difficulty, as illustrated in Fig. 6. We observe that image pairs with higher Blend Difficulty tend to have higher Creative Potential and are generally more conceptually different. 5.2. How to Estimate Blend Difficulty? Cognitive psychology suggests that creatively blending distant concepts requires humans to flexibly traverse semantic networks via intermediate associations rather than direct links [3, 23]. Following this insight, we hypothesize that blending conceptually distant pairs (e.g., man and banana) involves traversing longer, curved paths in pretrained feature spaces, while blending nearby pairs (e.g., cat and dog) follows simpler, approximately linear paths. To estimate the difficulty of blending two images, we Figure 8. Our method creatively blends the relevant visual attributes that are similar between the inputs, or vibe. For example, the vibe between the map and chicken is the shape, not color or texture. In contrast, the baselines often fail to recognize the key attributes or fuse them effectively. CLIP Avg refers to averaging the input image embeddings and feeding the resulting embedding into IP-Adapter [45], akin to using weight 0.5 in CLIP linear interpolation. Gemini [12] and GPT [30] often perform part-level composition or style transfer. define path nonlinearity score (PNS) for each image pair based on how much path decoded from Vibe Space deviates from linear interpolation in CLIP space  (Fig. 7)  . Given vibe decoded path in CLIP space γ(α) with α [0, 1], we sample equally-spaced points α0 = 0, α1, . . . , αn1 = 1, and quantify excess path length and directional changes: length ratio = γcurved γlinear , γcurved = n2 (cid:88) (cid:13)γ(αi+1) γ(αi)(cid:13) (cid:13) (cid:13) i=0 γlinear = (cid:13) (cid:13)γ(αn1) γ(α0)(cid:13) (cid:13)2 , , (7) direction change = 1 2 n3 (cid:88) i=0 cos1 (cid:18) δi, δi+1 (cid:19) δi2δi+12 , (8) where δi = γ(αi+1)γ(αi). We normalize and average the length ratio and direction change scores to obtain the PNS for an image pair. This metric serves as computational proxy for conceptual distance: higher PNS means the path traverses multiple intermediate regions of feature space. To check whether PNS reflects human-perceived Blend Difficulty, we compare two image pairs at time and test whether the pair humans rate as harder to blend also has higher PNS. To reduce noise, we use comparisons with high human consensus ( 66%). In Fig. 7, as humans agree more on which image pair is harder, the PNS of the humanpreferred pair is correspondingly higher. We find 80.0% agreement between PNS and human-rated Blend Difficulty, indicating that PNS effectively estimates perceived difficulty and can help curate challenging image pairs. 5.3. How Do Human Preferences Reflect Creativity? To compare the creativity of blends from different methods, we conduct human preference study in which participants first identify the main attributes that are similar between images  (Fig. 6)  , and then rank the outputs based on how well they coherently blend those attributes. We determine Totally Looks Like High Difficulty Medium Difficulty Low Difficulty Architecture Method Human LLM Human LLM Human LLM Human LLM CLIP Avg 13.3% 13.3% 21.4% 7.14% 26.7% 0.00% 39.0% 21.7% Gemini [12] 6.67% 6.67% 7.14% 7.14% 6.67% 13.3% 5.00% 8.33% 20.0% 40.0% 21.4% 50.0% 40.0% 66.7% 14.0% 30.3% GPT [30] 60.0% 40.0% 50.0% 35.7% 26.7% 20.0% 42.0% 39.7% Ours Table 1. We report human preferences for blends from different methods using input image pairs from Totally Looks Like [36] (grouped by high, medium, and low Blend Difficulty via human ratings) and from our Architecture dataset. We also report LLM preferences and compare human and LLM judgments in Fig. 9. Our method is effective overall and most preferred by humans and the LLM on image pairs that are more difficult to blend. consensus using first-place votes, followed by second-place votes to break ties. We recruit six raters for Totally Looks Like [36] and four raters for our Architecture dataset. As reported in Table 1, humans prefer our method 3 more often as the second-ranked GPT [30] on high Blend Difficulty examples from Totally Looks Like [36], and 2.4 more often on medium difficulty examples. On easier image pairs, human preference for GPT and CLIP Avg increases. This may explain the smaller margin in performance between our method and CLIP Avg on Architecture, in which the paired images are more conceptually related than in Totally Looks Like. We also provide qualitative comparisons in Fig. 8. Our method effectively identifies the relevant attributes from both input images and creatively blends them, while baselines like Gemini and GPT often miss key attributes or merge them less successfully. Humans show notable agreement on which methods blend is preferred within each pairwise comparison of methods in Table 2. We compute Agreement as the fraction of matching judgments between two raters, and we report Cohens κ [10] to account for chance. For instance, 76.6% agreement on Gemini vs. Ours means, on average, raters 7 Totally Looks Like Architecture Comparison Agreement (%) Cohens κ Agreement (%) Cohens κ CLIP Avg vs. Gemini CLIP Avg vs. GPT CLIP Avg vs. Ours Gemini vs. GPT Gemini vs. Ours GPT vs. Ours 74.9 6.0 63.1 9.1 63.1 7.9 72.9 9.5 76.6 9.1 65.8 11.1 0.23 0.22 0.18 0.16 0.20 0.17 0.33 0.23 0.16 0.23 0.15 0.20 Human Top-1 vs. LLM 35.7 7.9 Human Top-2 vs. LLM 55.1 4.7 74.0 4.5 70.0 3.6 65.5 3.0 66.8 4.7 75.0 4.0 70.8 4.7 31.3 5.6 51.8 3. 0.18 0.09 0.26 0.14 0.31 0.06 0.31 0.09 0.17 0.08 0.26 0.12 Table 2. Top: Mean standard deviation (SD) of human agreement on which methods blend is preferred within each pairwise comparison of methods. Reported as fraction of matching judgments (Agreement) and Cohens κ [10] (1 = perfect disagreement; 1 = perfect agreement). For example, 76.6% agreement on Gemini vs. Ours indicates that, on average, humans chose the same methods blend as preferred for 76.6% of input image pairs. Bottom: Mean SD of agreement between human top-1 / top-2 preferred blends and the LLM-selected blend, where top-2 counts match if the LLMs choice ranks first or second for the human. chose the same methods blend as preferred for 76.6% of input image pairs. We observe that agreement ranges from moderate to high (6377% on Totally Looks Like; 6675% on Architecture), demonstrating that humans are fairly consistent even on this subjective task. Both agreement and Cohens κ show greater variation on Totally Looks Like than on Architecture, suggesting that humans are more consistent when evaluating blends of conceptually related images. 5.4. How Well Do LLMs Gauge Blend Creativity? Figure 9. LLM judges offer useful approximation of human preferences but can exhibit failure modes. The LLM correctly identifies the key attribute as the hairstyle. However, the LLM also references irrelevant attributes like color and bases its preference on texture and body composition rather than focusing on hairstyle. LLMs have recently been used to approximate human judgments [18], which can be expensive to obtain. To probe whether LLMs can assess the creativity of blend, we use GPT-5 (LLM) [29] with step-by-step reasoning [33, 40] to select the best blend among outputs from different methods for each input image pair. Specifically, we prompt the LLM to (1) identify the main objects and shared attributes between the image pair, (2) evaluate how well each output merges those attributes, and (3) choose the blend it judges as best, providing its reasoning at each step. We adopt this 8 structured setup to encourage the LLM reason explicitly about the vibe of the image pair. As shown in Table 1, the LLM most frequently prefers our method (and GPT) for high Blend Difficulty examples in Totally Looks Like [36], and consistently favors our method on Architecture. The LLM judge offers promising approximation of human preferences for the creativity of blend, but it might incorrectly infer the underlying vibe in the input image pair. We observe two primary failure modes: (1) the LLM identifies the wrong shared attributes, or (2) it identifies the correct attributes but overemphasizes irrelevant ones, leading it to prefer different blend than humans. For example, in the mapchicken pair in Fig. 8, the LLM focuses on attributes such as centered subject and earthy color palette but overlooks the shared object shape. In Fig. 9, the LLM bases its preferred blend on color and texture rather than the hairstyle, but humans consistently identify the hairstyle. These observations suggest that while LLM can approximate human judgments of creative blends, improving their ability to prioritize the most relevant shared attributes between images is essential for reliable evaluation. In Table 2, we quantify agreement between humans and the LLM on which blend is preferred among outputs from different methods, where random chance agreement is 25%. Using only the human most preferred blend (top-1) yields lower agreement with the LLM (35.7% on Totally Looks Like; 31.3% on Architecture). Expanding to the human top-2 preferred blends increases agreement to 55.1% and 51.8%, respectively, suggesting that the LLM tends to select subset of blends that humans rate highly. Overall, humans are more consistent with one another, and the LLM captures partial but meaningful overlap with human preferences. 6. Conclusion Creatively blending visual concepts involves identifying the most relevant shared attributes for an input pair and merging them into coherent and meaningful hybrid, which we call Vibe Blending. We propose Vibe Space, hierarchical graph manifold that guides geodesic paths in ambient feature spaces like CLIP to produce such blends. To evaluate the creativity of blend, we introduce cognitively-inspired framework using human and LLM preferences, and path nonlinearity score for blend difficulty based on path geometry in feature space. Our method generates blends that humans prefer more often than those from strong baselines like Gemini and GPT, especially on difficult image pairs. Curating compelling and challenging input image pairs remains an open problem. Our path nonlinearity score provides principled approach to identifying such examples, offering direction for dataset expansion. Another promising direction is improving LLM judges to better attend to the key visual attributes that underlie creative blending, enabling more reliable and scalable evaluation in future work."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by the funds provided by the National Science Foundation and by DoD OUSD (R&E) under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence)."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Comparisons to Current Methods In the main paper, we compared Vibe Space with CLIP Avg (our baseline), GPT [30], and Gemini [12]. In this section, we also compare with recent concept-level and pixel-level blending methods, including AID [21], Yu et al. [47], and DiffMorpher [48]. Our experiments demonstrate that CLIP Avg is stronger than these additional baselines. We use image pairs from the Totally-Looks-Like dataset. Our user studies provide two annotations for each pair: (1) Blend Difficulty level (low, medium, high) and (2) short text description of the main shared attribute, or vibe. Qualitative Comparison. As illustrated in Figs. 11 to 13, pixel-level blending methods such as Yu et al. [47] and DiffMorpher [48] often produce blurred or low-quality intermediate images. CLIP Avg and AID [21] generate high-quality images but frequently fail to capture the main shared attribute (e.g., Fig. 12b: Hair Style; Fig. 11b: Teeth and Eyes). GPT and Gemini typically perform style transfer or part-level composition rather than attribute-level blending. In contrast, our Vibe Space consistently produces highquality images while keeping the blending trajectory centered on the attribute identified by users. Quantitative Comparison. We evaluate concept-level blending performance using our Attribute-Masked DreamSim metric. For each image pair (IA, IB), our user study provides text description of the main shared attribute between the images, with examples in Fig. 13. We use this text to obtain segmentation mask using an open-vocabulary segmentation model [26]. All image feature similarity computations are restricted to pixels inside the attribute mask. Specifically, given an image I, let xdreamsim(I) denote its dense DreamSim [16] features (prior to global pooling). For masked region Mask, we compute the attribute-masked DreamSim embedding via mean pooling: v(I; Mask) = 1 Mask (cid:88) pMask xdreamsim(I)p. (9) DreamSim similarity between two images is defined as cosine similarity between the attribute-masked embeddings: sim = v(IA; Mask), v(IB; Mask) v(IA; Mask)2 v(IB; Mask)2 . (10) For each method, we measure how well the midpoint blend Imid preserves the target attribute region by computing: (1) sim(Imid, IA), (2) sim(Imid, IB), and (3) the average of these two scores. As shown in Table 3, Vibe Space achieves the strongest performance among concept-level blending methods, while our CLIP Avg baseline remains the best-performing pixellevel blending method. The Attribute-Masked DreamSim metric does not fully capture the differences between the methods. While the scores for Vibe Space and the CLIP Avg baseline are numerically similar in Table 3, Fig. 10 shows qualitative discrepancies. Specifically, CLIP Avg fails to accurately capture the primary attributes, such as the mouth in the first row, the hairstyle in the second row, and the hairstyle in the third row. Instead, CLIP Avg often blends all possible attributes simultaneously, whereas our Vibe Space approach effectively prioritizes and blends the main attributes before considering sub-attributes. Figure 10. Limitation of quantitative evaluation. While CLIP Avg achieved higher score on the first three pairs, it fails to capture the main attributes (mouth in the first row, hairstyle in the second row, hairstyle in the third row) and instead blends all possible attributes. Our Vibe Space, on the other hand, successfully blends the main attributes without blending distracting attributes. Method Concept-level blending methods VibeSpace (ours) AID [21] GPT [30] Gemini [12] Pixel-level blending methods VibeSpace (pixel-level blending) CLIP Avg (our baseline) Yu et al. [47] DiffMorpher [48]"
        },
        {
            "title": "Low Difficulty",
            "content": "Input 1 Input 2 Mean Input 1 Input 2 Mean Input 1 Input 2 Mean 0.632 0.483 0.412 0.410 0.552 0.594 0.554 0.390 0.540 0.531 0.336 0.388 0.621 0.573 0.505 0.483 0.586 0.507 0.374 0.399 0.586 0.584 0.530 0. 0.642 0.570 0.314 0.300 0.600 0.584 0.493 0.440 0.562 0.468 0.290 0.285 0.544 0.581 0.482 0.586 0.602 0.519 0.302 0.292 0.572 0.583 0.487 0. 0.708 0.599 0.444 0.518 0.660 0.632 0.571 0.547 0.596 0.531 0.355 0.454 0.591 0.648 0.581 0.591 0.652 0.565 0.399 0.486 0.626 0.640 0.576 0. Table 3. Comparison to current methods on the Totally Looks Like dataset [36]. We report Attribute-Masked DreamSim computed in three steps: (1) the main shared attribute for each image pair is obtained from our user study; (2) an open-vocabulary segmentation model [26] is used to generate mask for this attribute; (3) DreamSim features are extracted and cosine similarity is computed only over the masked region. For each method, the blended midpoint image is compared to both input images: the Input 1 and Input 2 columns report DreamSim similarity between the midpoint and each input, respectively, and the Mean column reports their average. Bolded numbers denote the best-performing method among concept-level methods and among pixel-level methods within their respective groups, underlined numbers mark the second best. Insights: (1) Vibe Space achieves the strongest performance among concept-level blending methods; (2) CLIP Avgour baselineis strong pixel-level method and performs best within its category. 2 (a) User rated blend difficulty is: High. User annotated main attribute: Hair Style (b) User rated blend difficulty is: High. User annotated main attribute: Teeth and Eyes (c) User rated blend difficulty is: High. User annotated main attribute: Mouth Shape (d) User rated blend difficulty is: High. User annotated main attribute: Curly Hair Figure 11. Comparison to current methods. In this set of examples, blend difficulty is High. Insight: Our method captures the main attribute, AID [21] failed to capture the main attribute and sometimes generate images unrelated to input 1 and input 2, Yu et. al [47] and DiffMorpher [48] produce low-quality images. Although the visual difference between ours vs CLIP Avg is highCLIP Avg does not capture the main attribute while ours doesthe quantitative measure in Table 3 shows small difference between Ours and CLIP Avg, this highlights the limitation of quantitative metrics in evaluating Vibe Blending. 3 (a) User rated blend difficulty is: Medium. User annotated main attribute: Facial Expression (b) User rated blend difficulty is: Medium. User annotated main attribute: Hair Style (c) User rated blend difficulty is: Medium. User annotated main attribute: Grin (d) User rated blend difficulty is: Medium. User annotated main attribute: Hair Style Figure 12. Comparison to current methods. In this set of examples, blend difficulty is Medium. Insight: Our method captures the main attribute, AID [21] failed to capture the main attribute and sometimes generate images unrelated to input 1 and input 2, Yu et. al [47] and DiffMorpher [48] produce low-quality images. 4 (a) User rated blend difficulty is: Low. User annotated main attribute: Spiky Hair (b) User rated blend difficulty is: Low. User annotated main attribute: Grimace (c) User rated blend difficulty is: Low. User annotated main attribute: Hair Style (d) User rated blend difficulty is: Low. User annotated main attribute: Clothing Figure 13. Comparison to current methods. In this set of examples, blend difficulty is Low. Insight: Our method captures the main attribute, AID [21] failed to capture the main attribute and sometimes generate images unrelated to input 1 and input 2, Yu et. al [47] and DiffMorpher [48] produce low-quality images. The visual difference between Ours vs CLIP Avg is smallboth methods can generate coherent blend on the main attributes. 5 (a) Architecture dataset. (b) Architecture dataset. (c) Architecture dataset. (d) Architecture dataset. Figure 14. Comparison to current methods. Insight: Our method captures the main attribute, AID [21] failed to capture the main attribute and sometimes generate images unrelated to input 1 and input 2, Yu et. al [47] and DiffMorpher [48] produce low-quality images. 6 B. Ablation Studies We conduct series of ablations to understand the contribution of each component in Vibe Space. Experiments are performed on the Totally-Looks-Like dataset and evaluated using Attribute-Masked DreamSim (see Section A). Across all studies, we report performance separately for high-, medium-, and low-difficulty image pairs. Flag Loss. The flag loss aligns the learned Vibe Space with the multiscale geometry of the underlying diffusion map by matching the Gram matrix of Vibe features to the flag-space kernel (see Equation (4)). This loss depends on set of scales M, where each corresponds to prefix of low-frequency Laplacian eigenvectors. We ablate the following setting: multi-scale coarse-to-fine (M = {4, 8, . . . , 64}) single-scale fine-only (M = {64}) single-scale coarse-only (M = {4}) no flag loss (M = ) Results are shown in Table 4 and Fig. 15. Multiscale supervision provides the most robust performance across all difficulty levels. Single-scale variants work well only in specific regimesfine-grained-only excels on lowdifficulty cases where attributes are small and localized; coarse-only works best on high-difficulty pairs where finegrained attribute connections are hard to discover. Removing flag loss significantly degrades performance, confirming that multiscale geometric alignment is essential for coherent Vibe Blending. Correspondence. Vibe Blending relies on region-level correspondence between DINO token clusters in IA and IB (see Section D.1). We ablate both the number of clusters and the complete removal of correspondence. In Table 5, using correspondence consistently outperforms removing it on mediumand low-difficulty cases. When difficulty is high, using fewer clusters (e.g., 10) is beneficial because high-difficulty pairs have less spatial alignment, making fine-grained clustering unstable. Without correspondence (i.e., pixel-level blending), performance is competitive only for high-difficulty pairs but fails on medium/low cases and produces incoherent blends when inputs are spatially misaligned (see qualitative examples in Section D.1). Overall, correspondence is crucial for concept-level blending. DINOCLIP Feature Mixing. Vibe Space is trained by mapping dense DINO features into Vibe Space and decoding back into CLIP space to leverage both fine-grained semantics (DINO) and generative compatibility (CLIP). We ablate this design by replacing DINO features with CLIP features as the encoder input. Figure 15. Ablation study on Flag Loss. Insights: without flag loss, the blending failed to capture the main attribute. As shown in Table 6, mixing DINO and CLIP features achieves the best performance on mediumand lowdifficulty pairs. For the most challenging pairs, CLIP-only encoding sometimes performs slightly better, likely because CLIP already captures high-level semantics that dominate these pairs. However, DINOCLIP mixing remains the more stable choice across all difficulty levels."
        },
        {
            "title": "Method",
            "content": "Input 1 Input 2 Mean Input 1 Input 2 Mean Input 1 Input 2 Mean Multi-scale (coarse-to-fine) = {4, 8, . . . , 64} Single-scale (fine) = {64} Single-scale (coarse) = {4} No flag loss = 0.626 0. 0.523 0.669 0.490 0.580 0.668 0. 0.643 0.650 0.459 0.554 0.621 0. 0.555 0.731 0.573 0.652 0.625 0. 0.575 0.651 0.496 0.573 0.749 0. 0.647 0.450 0.579 0.514 0.568 0. 0.556 0.658 0.546 0.602 Table 4. Ablation of our methods on the Totally-Looks-Like dataset. We report Attribute-Masked DreamSim computed in three steps: (1) the main shared attribute for each image pair is obtained from our user study; (2) an open-vocabulary segmentation model [26] is used to generate mask for this attribute; (3) DreamSim features are extracted and cosine similarity is computed only over the masked region. For each method, the blended midpoint image is compared to both input images: the Input 1 and Input 2 columns report DreamSim similarity between the midpoint and each input, respectively, and the Mean column reports their average. Bold numbers denote the best-performing method. Insights: (1) With flag loss works better than no flag loss across all low-to-high difficulty cases. (2) Single-scale fine-grained-only works best on low difficulty cases; coarse-only works best on high difficulty cases. Multi-scale works well on all cases, and multi-scale is more robust than single-scale."
        },
        {
            "title": "Method",
            "content": "Input 1 Input 2 Mean Input 1 Input 2 Mean Input 1 Input 2 Mean w/ correspondence nclusters = 10 w/ correspondence nclusters = 20 w/ correspondence nclusters = 30 w/ correspondence nclusters = 40 w/ correspondence nclusters = 50 w/o correspondence (pixel-level blending) 0.632 0.540 0.586 0. 0.572 0.582 0.714 0.557 0.636 0. 0.451 0.566 0.617 0.554 0.585 0. 0.555 0.639 0.658 0.466 0.562 0. 0.562 0.602 0.710 0.552 0.631 0. 0.545 0.563 0.625 0.517 0.571 0. 0.596 0.652 0.630 0.459 0.544 0. 0.532 0.586 0.720 0.540 0.630 0. 0.621 0.586 0.600 0.544 0.572 0. 0.591 0.626 Table 5. Ablation of our methods on the Totally-Looks-Like dataset. We report Attribute-Masked DreamSim computed in three steps: (1) the main shared attribute for each image pair is obtained from our user study; (2) an open-vocabulary segmentation model [26] is used to generate mask for this attribute; (3) DreamSim features are extracted and cosine similarity is computed only over the masked region. For each method, the blended midpoint image is compared to both input images: the Input 1 and Input 2 columns report DreamSim similarity between the midpoint and each input, respectively, and the Mean column reports their average. Bold numbers denote the best-performing method. Insights: (1) When increasing blend difficulty from low to high, computing correspondence with less clusters works better, this is because correspondence is harder to compute for large number of clusters on high difficulty cases (see Section D.1) (2) Although w/o correspondence works great on high difficulty examples, it doesnt work well on medium and low difficulty cases."
        },
        {
            "title": "Method",
            "content": "Input 1 Input 2 Mean Input 1 Input 2 Mean Input 1 Input 2 Mean : DINO Vibe, : Vibe CLIP : CLIP Vibe, : Vibe CLIP 0.626 0.420 0.523 0.669 0. 0.580 0.668 0.618 0.643 0.648 0. 0.546 0.731 0.422 0.577 0.665 0. 0.579 Table 6. Ablation of our methods on the Totally-Looks-Like dataset. We report Attribute-Masked DreamSim computed in three steps: (1) the main shared attribute for each image pair is obtained from our user study; (2) an open-vocabulary segmentation model [26] is used to generate mask for this attribute; (3) DreamSim features are extracted and cosine similarity is computed only over the masked region. For each method, the blended midpoint image is compared to both input images: the Input 1 and Input 2 columns report DreamSim similarity between the midpoint and each input, respectively, and the Mean column reports their average. Bold numbers denote the best-performing method. Insights: Mixing DINO and CLIP features works best on medium difficulty and low difficulty cases, however on high difficulty cases, using CLIP without mixing DINO works better. 9 C. Path Finding Details C.1. Proof of Vibe Space Analytical Minimum This section provides the mathematical justification for the claim made in the main text: linear interpolation in Vibe Space decodes to an approximate geodesic in the original feature manifold. We show this by starting from the formulation of the multiscale inverse diffusion map, rewriting it in terms of the flag-space kernel, and then substituting the geometric alignment constraint learned during Vibe Space training. This results in latent-space surrogate objective whose unique minimizer is exactly the linear interpolation between endpoints. Notation Recap. (cid:80) Term 1: 1 Ψ1:mk in2 collects all mk ner products Ψ1:mk ), Ψ1:mk (x ) across scales. Across tokens, these are exactly the diagonal entries of the flag-space kernel S(Ψ(x)). (cid:80) (x)2 (x Term 2: (xα)2 mk and is absorbed into constant. Ψ1:mk 1 2 does not depend on Term 3 (cross term): 2 (cid:88) mk (cid:10)Ψ1:mk (x), Ψ1:mk (xα)(cid:11). Element-wise, each pair of tokens (i, j) contributes 1 (cid:88) mk Ψ1:mk (x ) Ψ1:mk (xα,j), R(HW )D : DINO feature tokens. : encoder mapping tokens to Vibe Space. R(HW )d: Vibe Space embeddings, = (x). g: decoder mapping Vibe Space to CLIP feature space. Ψ1:mk (x) R(HW )mk : first mk diffusion-map which is precisely the (i, j) entry of S(Ψ(x)) matched to S(Ψ(xα)). Putting the three terms together, the entire objective in (A.1) is the Frobenius norm between the two corresponding flag-space kernel matrices: the kernel eigenvectors. = {m1, . . . , mk}: set of multiscale truncation levels. S(Ψ(x)): flag-space kernel Sij(Ψ(x)) = 1 (cid:88) mkM Ψ1:mk (xi) Ψ1:mk (xj). zA, zB : latent endpoints. α [0, 1]: interpolation weights. γ(α): path in original manifold. zα: latent interpolation: zα,i = (1 α)zA + α zB. Inverse Diffusion Map in Flag-Space Kernel Form Given interpolated diffusion map coordinates Ψt(xα) = (1 α)Ψt(xA) + α Ψt(xB), the multiscale inverse diffusion map seeks γ(α) = arg min 1 (cid:88) mkM (cid:13) (cid:13)Ψ1:mk (x)Ψ1:mk (xα)(cid:13) 2 2. (cid:13) (A.1) Expand the squared norm (cid:13) (cid:13)Ψ1:mk (x) Ψ1:mk 2 = (cid:13) (xα)(cid:13) 2 (cid:13) + (cid:13) (cid:13)Ψ1:mk (cid:13)Ψ1:mk (cid:68) Ψ1:mk (x)(cid:13) 2 (cid:13) 2 (xα)(cid:13) 2 (cid:13) 2 (x), Ψ1:mk 2 (11) (cid:69) . (xα) (A.2) Summing (A.2) over all mk produces three corresponding sums: 10 γ(α) = arg min (cid:13)S(Ψ(x)) S(Ψ(xα))(cid:13) (cid:13) 2 . (cid:13) (A.3) Thus the inverse diffusion map reduces to matching multiscale flag-space kernels. Vibe Space Approximation. Vibe Space training enforces the alignment between Gram matrix zz and flagspace kernel S(Ψ(x)) zz S(Ψ(x)), = (x). (A.4) Let = (x) and zα = (1 α) zA + α zB. Substituting (A.4) into (A.3) gives the latent surrogate: = arg min (cid:13) (cid:13)zz zαz α (cid:13) 2 . (cid:13) (A.5) Because the Gram matrix zz matches zαz α uniquely when = zα, the unique minimal is = zα . (A.6) Thus γ(α) g(zα), i.e., linear interpolation in Vibe Space provides closed-form approximation to the multiscale inverse-diffusion geodesic. C.2. Graph Laplacian and Diffusion Map Affinity graph and Laplacian. Let {xi}N i=1 denote the DINO feature tokens used as graph nodes. We construct weighted affinity graph with (cid:18) Wij = exp xi xj2 2 σ2 (cid:19) , Dii = (cid:88) j=1 Wij, (A.7) Var(x(d) where σ > 0 is bandwidth parameter. We set σ2 = (cid:80) ) to match the global feature variance, ensuring that the affinity kernel adapts to the scale of the DINO feature distribution. i"
        },
        {
            "title": "The graph Laplacian is",
            "content": "L = W. (A.8) The corresponding random-walk diffusion operator is the row-stochastic matrix = D1W, (A.9) where Pij gives the one-step transition probability from node to node j. Nystrom approximation. Constructing the full affinity matrix RN is O(N 2) in memory and compute, which is prohibitive for tens of thousands of DINO tokens per image. We therefore employ the Nystrom approximation [43]: Sample subset of anchor tokens. Form the subgraph affinities WSS exactly. Compute cross-affinities WN between all nodes and anchors. Approximate the full kernel by (cid:102)W = WN W1 SS S. (A.10) This reduces the eigen-decomposition cost from O(N 3) to O(S3) and the kernel construction cost from O(N 2) to O(N S). In practice, we use = 500; the graph eigenvectors can be computed in milliseconds. Generalized eigenproblem. The diffusion-map coordinates are obtained by solving Ψ = λ Ψ, (A.11) which is equivalent to diagonalizing the diffusion operator: Ψ = (1 λ) Ψ. (A.12) D. Vibe Space Details D.1. Correspondence Matching To ensure that the vibe transition preserves semantic structure (e.g., blending head to another head rather than to background element), we establish semantic correspondence π between the input images. This corresponds to the Match function in Algorithms 1 and 2. Since pixelwise matching is computationally expensive and noisy, we operate on semantic regions derived from DINO features. As shown in Fig. 16, we first segment each image into distinct semantic regions using NCut [43]. Given the DINO feature tokens xdino, we construct token-wise affinity graph and compute the leading eigenvectors of the graph Laplacian. We then discretize these eigenvectors into segmentation masks using k-way clustering approach. This results in set of masks {Mask1, . . . , Maskk} for each image, grouping visually and semantically similar patches. Figure 16. Examples of correspondence matching. We use NCut clustering [43] and Hungarian matching to compute correspondence between segments. For each identified segment i, we compute representative feature centroid ci. This is calculated by averaging the DINO feature embeddings of all patch tokens belonging to that segment: ci = 1 Maski (cid:88) xdino , pMaski (12) where represents the token indices within the mask Maski. To define the correspondence mapping π, we formulate the problem as linear assignment task. We compute cost matrix Rkk representing the pairwise feature distances between the cluster centroids of the source image IA and the target image IB: Cij = c(A) c(B) 2. (13) We apply the Hungarian algorithm to find the optimal bijection that minimizes the total feature distance between matched segments. This mapping π allows us to compute the vibe displacement AB relative to the corresponding semantic structures in the Vibe Space. 11 inputs into the Vibe Space to obtain zA, zB, and zA. To transfer the vibe from the reference pair to the new subject, we must establish correspondence chain (Lines 6-7). Using the matching procedure defined in Section D.1, we compute two distinct mappings: (1) πAB matches semantic segments in the reference source IA to the reference target IB. (2) πAA matches semantic segments in the new subject IA to the reference source IA. . We define the vibe as the displacement field AB between semantically corresponding regions in the reference pair. For cluster centroid c(A) and its match c(B) (where = πAB(i)), the local displacement is vi = c(A) c(B) To transfer this vibe to IA, we use the mapping πAA to pull the appropriate displacement vector for each segment in (Lines 8-9). For segment in IA, if it maps to segment in IA (i.e., = πAA(k)), we assign it the displacement vi. This constructs the target displacement field AB, ensuring that the specific semantic transformation (e.g., turning face into flower) is applied to the correct anatomical regions of the new subject. Finally, we generate the output image IB by applying the transferred displacement field to the latent code of the new subject (Lines 10-13): zα = zA + αAB. This latent path is decoded by into CLIP space and rendered via IP-Adapter [45]. D.3. Negative Vibe Control The Principle of Vibe Subtraction. Conceptually, our goal is to subtract the negative vibe from the positive vibe. In the geometric framework of Vibe Space, both positive and negative vibes can be represented as subspaces spanned by the eigenvectors of their respective graph Laplacians. The positive subspace, spanned by Ψpos, contains the desired attributes, while the negative subspace, spanned by Ψneg, contains the unwanted ones. It is highly probable that these subspaces are not orthogonal; they share common components. For instance, if the positive vibe is rotation and style change  (Fig. 5)  , and the negative vibe is style change, both subspaces will contain eigenvectors related to texture, color palette, and artistic rendering. To isolate rotation, we must remove the style change components from the positive vibes basis. We achieve this by orthogonalizing the positive basis vectors against the negative basis vectors. This process effectively projects Ψpos onto new basis, Ψfiltered, that is orthogonal to the subspace of Ψneg. The mathematical formulation for this operation in the flag-space hierarchy is: Ψfiltered = Ψpos β Ψneg (cid:0)(Ψneg)Ψpos (cid:1) . (14) Here, we assume the eigenvectors forming the columns of Ψneg are orthonormal. The term Ψneg((Ψneg)Ψpos) thus Figure 17. Vibe Analogy examples. Vibe Analogy can transfer the vibe of images to another non-trivial but related image to generate B. Examples of transferred vibes are human-object interaction and art styles between similar facial expressions. Algorithm 2 Vibe Analogy xdino 2 ); Wij ), Dii = (cid:80) Input: Images (IA, IB, IA ), image features (xdino, xclip) Output: Generated intermediate images {Iα}α[0,1] xdino σ 1: Wij = exp( Graph 2: (D W)Ψ(xdino) = λDΨ(xdino) Graph Diffusion Map 3: f, Train(xclip, xdino, Ψ(xdino)) Train Vibe Space ); zB = (xdino 4: zA = (xdino 5: zA = (xdino ) , xdino 6: πBA Match(xdino ) , xdino 7: πAA Match(xdino ) 8: AB = πBA(zB) zA 9: AB = πAA (AB) 10: for α [0, 1] do 11: 12: 13: 14: end for zα = zA + α AB xclip Iα IPAdapter(xclip α ) Path interpolation Decode vibe Generate image Cluster correspondence Path direction Encode vibe α = g(zα) D.2. Vibe Analogy Vibe Analogy applies the semantic transformation observed between reference pair (IA, IB) to new subject IA, as shown in Fig. 17. The procedure is detailed in Algorithm 2. We first compute the Vibe Space for all three images IA, IB, IA simultaneously (Lines 1-5). We extract DINO features xdino, construct the joint affinity graph, and train the encoder-decoder pair (f, g) to map between the shared spectral manifold and CLIP space. We then encode all three 12 Figure 18. Negative vibe control. Vibe attributes are implicitly extracted by Vibe Space. The blending pair defines desired vibes (rounded shape and material). The negative pair defines vibes to suppress (angular shape and white color). Blending without negative examples may transfer more attributes than desired. Subtracting the negative vibes, we better preserve the rounded shape and tan color of the building. represents the orthogonal projection of the positive eigenvectors onto the subspace spanned by the negative eigenvectors. By subtracting this projection, we are left with the component of the positive vibe that is orthogonal to the negative vibe. The hyperparameter β controls the strength of this subtraction: β = 1 performs full orthogonalization, while β > 1 can be used to actively push the resulting vibe away from the negative attributes. Learning Filtered Vibe Space. To incorporate negative vibes into our framework, we modify the training objective of the Vibe Space autoencoder. Instead of training the encoder to match the geometry of the positive flag-space kernel S(Ψ(xpos)), we train it to match the kernel of the filtered basis, S(Ψfiltered). The flag-space encoder loss from Eq. (4) becomes: Lflag enc(f ) = zz S(Ψfiltered)2 , (15) where = (xpos). The encoder is trained only on the positive exemplars xpos, but the target geometry S(Ψfiltered) is now computed from both the positive (Ψpos) and negative (Ψneg) eigenvector sets. Fig. 18 shows an example of vibe blending path with and without negative exemplars. D.4. Vibe Visualization by Gradient To understand which specific visual attributes drive the geometric alignment in Vibe Space, we propose gradientbased visualization method. Since our framework relies on DINO features for semantic correspondence and graph construction, we identify the specific feature channels within the high-dimensional DINO embedding that are most responsible for specific semantic region or vibe. We leverage the differentiability of the spectral clustering process. Let RN denote the extracted DINO feature tokens for an images, where is the number of patches and is the feature dimension. We compute the generalized eigenvectors Ψ of the graph Laplacian L. Algorithm 3 Vibe Blending with Negative Vibe Control pos , xclip Input: Pos images (IA, IB) with pos image features (xdino Neg images (IA, IC ) with neg image features (xdino neg ) pos ), negΨpos) Pos graph Neg graph Pos eigvecs Neg eigvecs Orthogonalize Train Vibe Space Encode pos vibes Cluster correspondence Path direction in filtered space Output: Generated intermediate images {Iα}α[0,1] 1: Wpos, Dpos ComputeAffinity(xdino pos ) 2: Wneg, Dneg ComputeAffinity(xdino neg ) 3: Ψpos GraphDiffusionMap(Wpos, Dpos) 4: Ψneg GraphDiffusionMap(Wneg, Dneg) 5: Ψfiltered Ψpos β Ψneg(Ψ 6: f, Train(xclip pos , xdino pos , Ψfiltered) 7: zA (xdino ); zB (xdino ) 8: π Match(xdino , xdino ) 9: AB π(zB) zA 10: for α [0, 1] do 11: 12: 13: 14: end for zα zA + α AB xclip Iα IPAdapter(xclip α ) Path interpolation Decode vibe Generate image α g(zα) To visualize the features corresponding to specific semantic cluster (e.g., the foreground object), we first obtain the discretized cluster indicator yk {0, 1}N from the eigenvectors via k-way Ncut [46]. We the define maximization objective Lvis to identify the feature channels contributing to this cluster: Lvis = 1 Maskk (cid:88) Ψi,k, iMaskk (16) where Maskk is the set of indices belonging to cluster k, and Ψ,k is the eigenvector corresponding to that cluster. We treat the input features as learnable parameter with respect to this loss. By computing the gradient XLvis via backpropagation through the eigendecomposition, we obtain saliency map RN C: = XLvis. (17) 13 Figure 19. Heatmaps illustrating the top-ranked DINO channels corresponding to the coarse and fine attributes shared by the input images. For example, in the second row, the coarse attributes may include the body pose and object identity, while the fine-grained attributes include the hand-object interaction. We then aggregate the gradients across the spatial dimensions of the cluster mask to score each feature channel c: sc = 1 Maskk (cid:88) Gi,c. iMaskk (18) The channels with the highest scores sc represent the specific DINO feature dimensions encoding the attribute. We visualize these top-ranked channels as heatmaps as shown in Fig. 19. D.5. Vibe Blending: Selecting the Optimal Blend Weight α via Dip in CLIP Consistency Selecting single representative frame from the generated continuous blend path is non-trivial. naive choice, such as the midpoint α = 0.5, often fails to capture the most compelling hybrid. This stems from the inherent asymmetry of our correspondence-based interpolation: we compute semantic displacement field AB and add it to the token clusters of the source image IA. Since the transformation is anchored to the source images semantic structure, the conceptual transition is not perfectly linear with respect to α. Consequently, the point of optimal blending may shift away from α = 0.5, requiring an adaptive selection strategy. To address this, we propose an automated procedure to select the optimal α by identifying the point of greatest conceptual transition along the blend path. Our intuition is that the most challenging point for the generative model to render often represents the most novel synthesis of attributes. We formalize this procedure in Algorithm 4, which measures the consistency between an ideal path decoded from Figure 20. Our method generates continuous path of blends for any interpolation weight α [0, 1]. We propose an automated procedure to select the best α by identifying the point of greatest conceptual transition along the blend path. Specifically, we compute consistency score to measure the difference between an ideal path decoded from Vibe Space and realized path obtained by re-encoding the generated images. In this example, the optimal weight α = 0.6 as determined by the dip in the score. Qualitatively, we observe that the α obtained via this algorithm achieves the best creative blend of the inputs, and we use this approach for our evaluation. Vibe Space and realized path obtained by re-encoding 14 Figure 21. Examples of blended images selected via the optimal blend weight α determined by our dip in CLIP consistency score, described in Fig. 20. We observe that the dip in consistency score generally results in the best blend of the relevant attributes in both input images. the generated images. The point of minimum consistency, or the dip in the score, identifies our target α. Algorithm 4 Optimal Blend Selection (α) Input: Vibe decoder g, vibe endpoints zA, zB, source image IA, candidate steps Asteps [0, 1] Initialize list to store scores Output: Optimal interpolation weight α 1: Slist [] 2: for α Asteps do 3: 4: 5: 6: 7: zα (1 α)zA + αzB xideal Iα IPAdapter(xideal α ) xrealized πα, Cdown Interpolate in Vibe Space Ideal Path: Decode to CLIP space Generate image α CLIP(Iα) Realized Path: Re-encode image α ComputeCorrespondence(IA, Iα) α g(zα) , Cdown them into meaningful regions. The Hungarian algorithm is then used to find an optimal matching πα between the cluster centers. With the correspondence established, we compute the consistency score Sα (line 8). For each semantic region in the source image, we calculate its mean CLIP feature vector along both the ideal path, µideal , and the corresponding realized path, µrealized : µideal µrealized (α) = mean(xideal α [c]) (α) = mean(xrealized α [πα(c)]) The overall score is the average cosine similarity over all matched cluster pairs: Correspondence: See Appendix D.1 Sα ComputeConsistency(xideal α , xrealized α 8: Score: See Eq. 19 , πα, Cdown ) S(α) = 1 CA (cid:88) cCA cos (cid:0)µideal (α), µrealized (α)(cid:1) (19) Slist.append(Sα) 9: 10: end for 11: arg min(Slist) Identify Dip: Find index of min score 12: α Asteps[i] 13: return α The core of this procedure involves generating and comparing two conceptual paths for each candidate α. As shown in Algorithm 4 (lines 3-6), the ideal path is formed by decoding the Vibe Space interpolation directly into CLIP space, representing the intended manifold-respecting trajectory. The realized path is formed by generating pixelspace image with the IP-Adapter and then re-encoding it with CLIP, representing what the model actually produces. direct token-wise comparison of these paths is brittle to minor spatial shifts. Therefore, we establish robust semantic correspondence between the source image IA and each generated image Iα (line 7). We use DINO features to perform spectral clustering on both images, segmenting where CA is the set of source clusters and x[c] denotes the CLIP tokens belonging to cluster c. Finally, the optimal interpolation weight α is selected as the one that minimizes this consistency score (lines 1011). This dip in consistency marks the point of greatest semantic tension and, frequently, the most creative and compelling synthesis of the two concepts  (Fig. 20)  . Fig. 21 shows examples of selecting α across blend trajectory. D.6. Vibe Blending: Training with Many Images Vibe Space can be trained with more than two images, and using additional related images helps in identifying the main attributes. Algorithm 5 outlines the steps for training Vibe Space with multiple additional images. The key difference between Algorithm 1 and Algorithm 5 is that the latter uses more images to solve the graph diffusion map Ψ and train the Vibe Space. However, the blending process is still executed using two images. 15 Fig. 22 presents qualitative examples comparing training with only two images to training with additional images. The middle blend, when additional images are included, successfully captures the glass window vibe when trained with extra images featuring glass windows. Algorithm 5 Vibe Blending with extra images Input: Multiple images (IA, IB, . . . ), image features for all images (xdino, xclip) Output: Generated intermediate images {Iα}α[0,1] xdino σ2 1: Wij = exp( Graph 2: (D W)Ψ(xdino) = λDΨ(xdino) Graph Diffusion Map 3: f, Train(xclip, xdino, Ψ(xdino)) Train Vibe Space (all ), Dii = (cid:80) Wij 2 xdino images) ); zB = (xdino 4: zA = (xdino ) 5: π Match(xdino , xdino ) 6: AB = π(zB) zA 7: for α [0, 1] do 8: 9: 10: 11: end for zα = zA + α AB xclip Iα IPAdapter(xclip α ) α = g(zα) Encode vibe (two images) Cluster correspondence Path direction Path interpolation Decode vibe Generate image D.7. Extrapolating Vibe Blending Paths Since our framework constructs locally linear manifold, we are not limited to interpolation within the convex hull of the input images (α [0, 1]). We can perform extrapolation by setting the interpolation weight α > 1, effectively continuing along the geodesic path defined by the displacement vector AB. We observe that this can yield an exaggerating effect, where specific visual characteristics are amplified beyond their presence in the target image IB  (Fig. 23)  . D.8. N-Image Blending We extend our framework to blend images {I1, . . . , IN } simultaneously, enabling applications such as barycentric interpolation within triangle of concepts  (Fig. 24)  . Unlike pairwise blending, establishing consistent multiway correspondence is non-trivial. We designate one input as the base image (Ibase), which serves as the structural anchor for the blend. We then compute 1 pairwise correspondence mappings, aligning the semantic clusters of every other image Ik to the clusters of the base image: πkbase = Match(xdino base, xdino ) where πkbase maps cluster index in the base image to the corresponding cluster index in image k. With semantic correspondences established across all inputs, we generalize the path interpolation logic to multiway vector summation. Let {α1, . . . , αN } be the scalar interpolation weights for the images. We generate the blended latent code zblend by starting with the base embedding and accumulating the weighted semantic displacements from all other images. For specific semantic region (cluster) in the base image, let c(base) be its centroid in Vibe Space. For every other image k, the centroid of the corresponding matched cluster is c(k) , where = πkbase(i). The blending equation for tokens belonging to region is: zblend[i] = zbase[i] + (cid:16) αk (cid:88) k=1 πkbase(i) c(base) c(k) (cid:17) This formulation computes weighted average of the vibe displacements relative to the base structure. If αk = 1 (and all others 0), the term simplifies to shifting the base embedding by the exact vector difference between the base and target k, approximating full morph to image k. The resulting zblend is decoded into CLIP space and rendered via the IP-Adapter [45]. E. Implementation Details E.1. Training Procedure We train the encoderdecoder (f, g) for each image pair independently using lightweight MLP architecture. Optimizer: Adam (learning rate = 0.001). Batch size: 2 images. Total iterations: 1000 steps. MLP layers: 4. MLP hidden dimension: 256. Number of Parameters: 0.72M. Vibe Blending running time is under 30 seconds on RTX4090 GPU. In Algorithm 5 lines 1-2, solving the graph diffusion map for 2-5 images (512 input image resolution, 1024 DINO tokens each image) only takes milliseconds with the help of Nystrom approximation [43]. In Algorithm 5 line 3, training encoder-decoder MLPs for 1000 In Algorithm 5 line 5, computsteps takes 15 seconds. ing correspondence matching takes milliseconds. In Algorithm 5 line 10, image generation with Stable Diffusion takes 2 seconds per image. Memory usage is under 1GB for graph diffusion map and training the Vibe Space. Memory usage is 12GB after loading the Stable Diffusion model. E.2. Loss Balancing Our full training objective is: = λflag encLflag enc + λflag decLflag dec + λsampleLsample + λreconLrecon (20) The loss terms include: 16 Figure 22. Vibe Blending with extra image training. We compare training Vibe Space with only two images (middle row) and with five images (bottom row), input images and extra images are in the first row. Training with extra glass window images helps capture the glass window vibe, resulting in glass texture pyramid in the middle-blend image. Lflag enc, Lflag dec: multiscale flag-space geometry preservation for the encoder and decoder respectively. F. Additional Evaluation Details Lflag enc(f ) = (cid:13) Lflag dec(f, g) = (cid:13) (cid:13)zz S(Ψ(x))(cid:13) 2 2, (cid:13) (cid:13)zz S(Ψ(g(z)))(cid:13) 2 2, (cid:13) = (x), F.1. LLMs for Generating and Evaluating Blends Lsample: flag-space geometry consistency in extrapolated space, for the decoder. Lsample(g) = (cid:13) (cid:13)zsamplez sample S(cid:0)Ψ(cid:0)g(zsample)(cid:1)(cid:1) (cid:13) 2 2. (cid:13) Lrecon: Reconstruct CLIP features. Lrecon(f, g) = xclip g(f (xdino))2 2. Loss weight balancing: λflag enc = 1, λflag dec = 0.01, λsample = 0.01, λrecon = 1, We observe that Lflag dec and Lsample can overwhelm the CLIP feature reconstruction loss Lrecon, this leads to poor image generation quality. Thus, we down-weighted Lflag dec and Lsample by factor of 0.01. Additionally, Lsample can cause training instability (NaN) during the warm-up periods, thus we only apply Lsample after first 500 steps of training iterations. 17 Vibe Blending involves reasoning about the relevant visual attributes shared between images, and then coherently fusing these attributes. Accordingly, we compare our method with multimodal LLMs that exhibit strong visual reasoning capabilities, such as Gemini 2.5 Flash Image [12] and GPT Image 1 [30]. To promote their reasoning for this task, we use chain-of-thought prompting [40] inspired by Peng et al. [33]. Specifically, we ask Gemini to (1) identify the main objects in each input image, (2) recognize the main visual attributes that are similar between the images, and (3) generate an image that creatively blends these attributes, as shown in Fig. 25. Since the OpenAI API does not provide public model that supports multi-turn conversations with both image and text inputsand image and text outputs we combine these steps into single prompt for GPT [30]. The full prompts for generating blends are presented below. The parentheses indicate (inputs outputs) where IA and IB are input images, Iblend is the blended output, denotes text, and denotes no additional inputs besides the prompt. Figure 23. Paths computed by Vibe Blending can be extrapolated to exaggerate attributes. Top: Extrapolating the path from dog to fish emphasizes the body shape of the fish and some elements of the background. However, extrapolation behavior is not well controlled at higher weights. Bottom: Extrapolating from an orange sports car to red sedan continues the color shift, resulting in further darkening. Gemini 2.5 Flash Image Blending Prompt (IA T) Identify the object in this first input image. (IB T) Identify the object in this second input image. ( T) want to blend the objects in both images so their main visual attributes merge into single, coherent hybrid. First, identify the main visual attributes that are shared by the input images, such as facial expression, hair style, or object shape. Additionally, interpret what the blend of objects might be intended to convey. ( Iblend) Based on all previous turns, generate an image that blends the main attributes in the input images. The output should not be collage, overlay, or style transfer. You may modify any image region. GPT Image 1 Blending Prompt (IA, IB Iblend) Generate an image that blends the objects in the input images so their main visual attributes merge into single, coherent hybrid. First, identify the main visual attributes that are shared by the input images, such as facial expression, hair style, or object shape. SecFigure 24. N-Image Blending extends Vibe Blending to an arbitrary number of images. In this visualization, we show 3-Image Blending between dog, ram, and horse. The top image (dog) is used as the base image to anchor correspondence matching and blending. ond, blend the main attributes in the images. The output should not be collage, overlay, or style transfer. You may modify any image region. We also employ an LLM to judge the output blends from different methods using similar step-by-step reasoning approach. We ask GPT-5 [29] to (1) identify the main objects in each input image; (2) recognize the main visual attributes that are similar between the images; (3) for each output, assess how well it blends the main attributes; and (4) choose the best blended output and provide reasoning. An abbreviated example chat is shown in Fig. 26. The full prompt for evaluating blends is provided below. LLM Judge Evaluation Prompt (IA T) Identify the object in this first input image. (IB T) Identify the object in this second input image. ( T) want to blend the objects in both images so their visual attributes merge into single, coherent hybrid concept. First, identify the main visual attributes that are similar between the inputs, such as facial expression, hair style, or object shape. Additionally, interpret what the blend of objects might be intended to convey. 18 Figure 25. Example chat with Gemini [12] to generate creative blend. LLM Judge Evaluation Prompt (continued) ( T) Assess how well this first output image blends the main attributes in both input images. ( T) Assess how well this second output image blends the main attributes in both input images. ( T) Assess how well this third output image blends the main attributes in both input images. ( T) Assess how well this fourth output image blends the main attributes in both input images. ( T) Based on all previous turns, which output image (1, 2, 3, or 4) best blends the main attributes in both input images? Do not pick any output that does not perform blending, such as collage, overlay, style transfer, or juxtaposition of the inputs. Do not pick any output that preserves the exact structure or identity of the inputs. Do not pick any output that is part-level composition, where parts of one input are simply attached to parts of the other input. Provide your reasoning. Fig. 27 illustrates examples where human raters and the LLM judge agree and disagree on the best blended output. We observe that common LLM failure cases include identifying extra irrelevant attributes such as body pose, color palette, and image composition. F.2. User Study Details Human-Rated Creative Potential vs. Blend Difficulty. We conducted user study with ten human participants to compare 44 image pairs sourced from the Totally Looks Like dataset [36]. Fig. 29 shows the prompt to users and examples with high rater agreement. Participants were presented with two image pairs side-by-side and asked to select which of the two pairs exhibited higher Creative Potential and which exhibited higher Blend Difficulty. They performed total of 110 side-by-side comparisons, ensuring each of the 44 image pairs was included in 5 unique comparisons. We then fit Bradley-Terry model [6] to these comparison ratings to produce robust ranking of Creative Potential and Blend Difficulty across image pairs. Finally, we used these continuous Bradley-Terry scores to categorize the image pairs into low, medium, and high bins, as depicted in Fig. 28. Human Preference of Creative Blends. We conducted another user study to assess the quality of blended outputs generated by different methods using input image pairs from Totally Looks Like [36]. Fig. 30 (a) displays the prompt given to participants, who answered two subques19 Figure 26. Example chat with the LLM judge [29] to select the best blended output. Figure 27. Examples where human raters and the LLM judge agree (left) and disagree (right) on the best blended output. tions. First, users identified the main attributes shared by the images (Fig. 30 (b)). Then, users ranked the outputs based on how well they coherently and creatively blended those main attributes (Fig. 30 (c)). This two-step process encourages users to actively consider the relevant attributes for blending before making their selection. We determined the final ranking for each example via majority vote based on first-place votes, followed by second-place votes. To encourage participants to focus on blending coherence and creativity, instead of other irrelevant characteristics like image quality, we utilized the following enhancement prompt with Gemini 2.5 Flash Image [12] to standard20 Figure 28. Extension of Fig. 6. To gauge human perceptions of creativity, we ask raters to compare image pairs along two axes: Creative Potential refers to how interesting blend might be, and Blend Difficulty indicates how challenging it is to form coherent blend. Image pairs with higher Blend Difficulty tend to have higher Creative Potential and are often more conceptually different. Numbers in each cell indicate the number of examples. ize the image quality of our blended output to match that of the outputs from models like Gemini and GPT [30]. Enhancement Prompt Enhance this image to high-resolution, professional quality while preserving all details and textures. Improve sharpness, smooth noise, and remove text and image artifacts for clean, realistic, and well-balanced look. While Creative Potential and Blend Difficulty describe how blend could be formed, creativity can also be evaluated in the outputs. We examine one complementary metric, output diversity, echoing Hofstadters insight that making variations on theme is the crux of creativity [22]. We quantify output diversity by generating multiple blends per input image pair and computing pairwise perceptual distances using DreamSim [16], denoted by . The diversity for one example is: F.3. How Does Creativity Relate to Diversity? Totally Looks Like Architecture Method CLIP DreamSim CLIP DreamSim CLIP Avg Gemini [12] GPT [30] Ours 0.079 0.189 0.121 0.223 0.096 0.305 0.193 0.339 0.067 0.129 0.088 0.150 0.112 0.257 0.177 0.291 Table 7. We generate multiple blends for each input image pair and measure the mean pairwise diversity of the output images using CLIP [34] and DreamSim [16]. Higher diversity is better. Our method produces more diverse blends across multiple trials, in addition to more creative blends as shown in Table 1. = 1 (cid:1) (cid:0)n 2 (cid:88) i<j dist(F (Ii), (Ij)) . (21) where Ii and Ij are different outputs from the same input, dist(, ) denotes DreamSim distance, and = 3. Higher indicates greater output diversity, and we report the mean across all examples. We also implement comparable measure of output diversity using CLIP image similarity [34]. Shown in Table 7, our method generates the most varied blends, in addition to the best creative blends according to humans and the LLM in Table 1. Interestingly, Gemini creates the second-most diverse outputs, even though it lags behind CLIP Avg and GPT in terms of blend quality. 21 Figure 29. User study for evaluating the Creative Potential and Blend Difficulty of image pairs. Top: The prompt shown to users. Bottom: Two examples with high rater agreement. Participants select which image pair has higher Creative Potential and higher Blend Difficulty. F.4. Curated Datasets G. Failure Cases The Totally Looks Like dataset [36] contains image pairs that humans judge as visually similar. To curate subset suitable for our evaluation, we focus on image pairs depicting distinct concepts while ensuring high image quality. We first automatically exclude pairs with overly similar content by removing those with both CLIP image similarity [34] above 0.65 and DreamSim distance [16] below 0.65. human annotator then filters out any remaining pairs containing text or low-quality images. This process yields 44 high-quality and semantically distinct image pairs. Additionally, we curate 300 pairs of architectural design images. We begin by collecting images spanning diverse architectural styles and architects from several existing datasets [1, 14, 42]. We then form random pairs such that the two images in each pair come from different architectural styles and different architects, and we recruit human annotator to verify the quality of each image pair. Entangled Vibes limitation of negative vibe blending arises when desired and undesired attributes are entangled within the feature space. The orthogonalization process assumes that the positive and negative vibes can be separated into distinct, albeit non-orthogonal, subspaces. Fig. 31 illustrates failure example where the vibe of style change is entangled with color change, which prevents the entangled vibe from being filtered out. Extrapolation Uncertainty While effective in certain cases, extrapolating Vibe Blending does not always produce images with meaningful exaggeration of relevant attributes  (Fig. 32)  . Correspondence Failure. Vibe Space relies on unsupervised region-level correspondence matching between DINO token clusters (see Section D.1). This correspondence is essential for concept-level blending, as it determines which semantic regions in IA and IB should be merged. However, Figure 30. User study for assessing human preference of creative blends. (a.) The prompt shown to users. (b.) First, we ask participants to describe the main visual attributes shared by the two input images. (c.) Second, raters rank the blended output images from different methods based on how well they blend the main attributes. Figure 31. Negative vibe failure case. The positive inputs capture both style change based on the types of car and color change. The negative inputs intend to capture only color change. However, the attributes of style and color are entangled, making them difficult to separate with negative examples. because the matching is unsupervised, it is not always reliable. When correspondence is incorrect (vary by random seeds), the resulting blends can degrade significantly, often producing incoherent or mismatched object-part combinations  (Fig. 33)  . 23 Figure 32. Failure case of Vibe Blending extrapolation. Extrapolating beyond α > 1 does not produce further rotation of the object in the input images. Reconstruction Failure. Our method depends on IPAdapter [45] to generate images conditioned on dense CLIP features. However, IP-Adapter is not always reliable outside the training distribution of Stable Diffusion. To isolate this limitation, we perform the following diagnostic: (1) extract dense CLIP features from an input image; (2) perform no blending and do not train Vibe Space; (3) feed the extracted Figure 33. Failure cases of unsupervised correspondence. Our method depends on correspondence matching to identify which semantic regions should be blended. Different random seeds lead to different clusterings and therefore different correspondence maps. Good correspondence (top row) yields clean and semantically meaningful blends, whereas poor correspondence (middle and bottom rows) leads to low-quality blends with incorrect part-level mixing. Figure 34. Failure cases of IP-Adapter reconstruction. Left column is input images; right four columns are reconstructed images from 4 random seeds. IP-Adapter reliably reconstructs images that lie within the Stable Diffusion training distribution (top row), producing consistent outputs across random seeds. In contrast, for out-of-distribution inputs (middle and bottom rows), IP-Adapter consistently fails to reproduce the original image, illustrating fundamental limitation of SD-based decoders when applied to concept-level blending tasks. features directly into IP-Adapter to reconstruct the input image. As shown in Fig. 34, IP-Adapter can consistently reconstruct in-distribution images (e.g., human faces) across random seeds, but fails on out-of-distribution (OOD) inputs, producing unstable and inconsistent reconstructions."
        },
        {
            "title": "References",
            "content": "[1] Galaxy Architects. Architects Dataset. https://www. kaggle . com / datasets / galaxyarchitects / architects-dataset, 2023. Accessed: 2025-02-11. [2] Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, and Bjorn Ommer. Continuous, subject-specific attribute control in t2i models by identifying semantic directions. arXiv preprint arXiv:2403.17064, 2024. [3] Roger Beaty and Yoed Kenett. Associative thinking at the core of creativity. Trends in cognitive sciences, 27(7): 671683, 2023. [4] Mathias Benedek and Andreas Fink. Toward neurocognitive framework of creative cognition: The role of memory, attention, and cognitive control. Current opinion in behavioral sciences, 27:116122, 2019. [5] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: Instructing text-to-image models using semantic guidance. Advances in Neural Information Processing Systems, 36: 2536525389, 2023. [6] Ralph Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [7] Michel Brion. Lectures on the geometry of flag varieties, 2004. v1, submitted 9 Oct 2004. [8] Yukang Cao, Chenyang Si, Jinghao Wang, and Ziwei Liu. Freemorph: Tuning-free generalized image morphing with diffusion model. 2025. [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. [10] Jacob Cohen. coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):3746, 1960. [11] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggioni, B. Nadler, F. Warner, and S. W. Zucker. Geometric diffusions as tool for harmonic analysis and structure definition of data: Diffusion maps. Proceedings of the National Academy of Sciences, 102(21):74267431, 2005. [12] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [13] Amil Dravid, Yossi Gandelsman, Kuan-Chieh Wang, Rameen Abdal, Gordon Wetzstein, Alexei Efros, and Kfir Aberman. Interpreting the weight space of customized diffusion models. arXiv preprint arXiv:2406.09413, 2024. [14] Dumitrux. Architectural https : / / www . kaggle . com / datasets / dumitrux / architectural-styles-dataset, 2020. Accessed: 2025-02-11. Styles. [15] C. Fowlkes, S. Belongie, F. Chung, and J. Malik. Spectral grouping using the nystrom method, 2004. [16] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. [17] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models. In European Conference on Computer Vision, pages 172188. Springer, 2024. [18] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [19] Jiayi Guo, Xingqian Xu, Yifan Pu, Zanlin Ni, Chaofei Wang, Manushree Vasu, Shiji Song, Gao Huang, and Humphrey Shi. Smooth diffusion: Crafting smooth latent spaces in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7548 7558, 2024. [20] Li He, Yoed Kenett, Kaixiang Zhuang, Cheng Liu, Rongcan Zeng, Tingrui Yan, Tengbin Huo, and Jiang Qiu. The relation between semantic memory structure, associative abilities, and verbal and figural creativity. Thinking & Reasoning, 27(2):268293, 2020. [21] Qiyuan He, Jinghao Wang, Ziwei Liu, and Angela Yao. Aid: arXiv Attention interpolation of text-to-image diffusion. preprint arXiv:2403.17924, 2024. [22] Douglas Hofstadter. Metamagical themas: Questing for the essence of mind and pattern. Basic books, 2008. [23] Yoed Kenett and Joseph Austerweil. Examining search processes in low and high creative individuals with ranIn Proceedings of the Annual Meeting of the domwalks. Cognitive Science Society, 2016. Inbar Huberman- [24] Vladimir Kulikov, Matan Kleiner, InversionSpiegelglas, and Tomer Michaeli. free text-based editing using pre-trained flow models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1972119730, 2025. [25] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have semantic latent space. arXiv preprint arXiv:2210.10960, 2022. Flowedit: [26] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation In 2024 IEEE/CVF Conference via large language model. on Computer Vision and Pattern Recognition (CVPR), pages 95799589, 2024. [27] Chihaya Matsuhira, Marc A. Kastner, Takahiro Komamizu, Takatsugu Hirayama, and Ichiro Ide. Investigating conceptual blending of diffusion model for improving nonwordto-image generation. In Proceedings of the 32nd ACM International Conference on Multimedia, page 73077315, New York, NY, USA, 2024. Association for Computing Machinery. [28] Sarnoff A. Mednick. The associative basis of the creative process. Psychological Review, 69(3):220232, 1962. 25 [44] Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu, Jaskirat Singh, Jing Zhang, Dylan Campbell, Peter Tu, and Richard Hartley. Image morphing with perceptuallyuniform sampling using diffusion models. arXiv preprint arXiv:2311.06792, 2023. Impus: [45] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. [46] Yu and Shi. Multiclass spectral clustering. In Proceedings Ninth IEEE International Conference on Computer Vision, pages 313319 vol.1, 2003. [47] Qingtao Yu, Jaskirat Singh, Zhaoyuan Yang, Peter Henry Tu, Jing Zhang, Hongdong Li, Richard Hartley, and Dylan Campbell. Probability density geodesics in image diffusion latent space. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2798927998, 2025. [48] Kaiwen Zhang, Yifan Zhou, Xudong Xu, Bo Dai, and Xingang Pan. Diffmorpher: Unleashing the capability of difIn Proceedings of fusion models for image morphing. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 79127921, 2024. [29] OpenAI. GPT-5. https://platform.openai.com/ docs/models/gpt-5, 2024. Accessed: 2025-02-11. [30] OpenAI. GPT Image 1. https://platform.openai. com/docs/models/gpt-image-1, 2024. Accessed: 2025-02-11. [31] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):74747489, 2021. [32] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. Advances in Neural Information Processing Systems, 36: 2412924142, 2023. [33] Yongqian Peng, Yuxi Ma, Mengmeng Wang, Yuxuan Wang, Yizhou Wang, Chi Zhang, Yixin Zhu, and Zilong Zheng. Probing and inducing combinational creativity in visionlanguage models. arXiv preprint arXiv:2504.13120, 2025. [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [35] F. Rellich. Perturbation Theory of Eigenvalue Problems. Gordon and Breach, 1969. [36] Amir Rosenfeld, Markus Solbach, and John Tsotsos. Totally looks like-how humans compare, compared to machines. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 19611964, 2018. [37] Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, and Gal Chechik. Norm-guided latent space exploration for text-to-image generation. Advances in Neural Information Processing Systems, 36:5786357875, 2023. [38] Zhida Sun, Zhenyao Zhang, Yue Zhang, Min Lu, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Creative blends of visual concepts. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 117, 2025. [39] Clinton Wang and Polina Golland. Interpolating between images with diffusion models. 2023. [40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [41] Chen Henry Wu and Fernando De la Torre. latent space of stochastic diffusion models for zero-shot image editing and guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73787387, 2023. [42] Zhe Xu, Dacheng Tao, Ya Zhang, Junjie Wu, and Ah Chung Tsoi. Architectural style classification using multinomial latent logistic regression. In European conference on computer vision, pages 600615. Springer, 2014. [43] Huzheng Yang. Nystrom Normalized Cuts PyTorch (ncutpytorch). Accessed: 2025-11-13."
        }
    ],
    "affiliations": [
        "CUNY",
        "UC Berkeley",
        "UPenn"
    ]
}