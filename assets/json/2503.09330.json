{
    "paper_title": "Group-robust Machine Unlearning",
    "authors": [
        "Thomas De Min",
        "Subhankar Roy",
        "Stéphane Lathuilière",
        "Elisa Ricci",
        "Massimiliano Mancini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from a model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of non-uniformly distributed forget sets, which we call group-robust machine unlearning, by presenting a simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (Mutual Information-aware Machine Unlearning), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available at https://github.com/tdemin16/group-robust_machine_unlearning."
        },
        {
            "title": "Start",
            "content": "Group-robust Machine Unlearning Thomas De Min1, Subhankar Roy1 Stephane Lathuili`ere2, 3 Elisa Ricci1, 4 Massimiliano Mancini1 1University of Trento 2LTCI, Telecom Paris, Institut Polytechnique de Paris 3Inria Grenoble, Univ. Grenoble Alpes 4Fondazione Bruno Kessler 5 2 0 2 M 2 1 ] . [ 1 0 3 3 9 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Machine unlearning is an emerging paradigm to remove the influence of specific training data (i.e., the forget set) from model while preserving its knowledge of the rest of the data (i.e., the retain set). Previous approaches assume the forget data to be uniformly distributed from all training datapoints. However, if the data to unlearn is dominant in one group, we empirically show that performance for this group degrades, leading to fairness issues. This work tackles the overlooked problem of nonuniformly distributed forget sets, which we call grouprobust machine unlearning, by presenting simple, effective strategy that mitigates the performance loss in dominant groups via sample distribution reweighting. Moreover, we present MIU (MUTUAL INFORMATION-AWARE MACHINE UNLEARNING), the first approach for group robustness in approximate machine unlearning. MIU minimizes the mutual information between model features and group information, achieving unlearning while reducing performance degradation in the dominant group of the forget set. Additionally, MIU exploits sample distribution reweighting and mutual information calibration with the original model to preserve group robustness. We conduct experiments on three datasets and show that MIU outperforms standard methods, achieving unlearning without compromising model robustness. Source code available here. 1. Introduction In several countries, recent regulations grant users greater control over their digital privacy, explicitly recognizing their right to request the removal of personal data, known as the right to be forgotten [32, 45]. Therefore, to comply with such regulations, machine learning systems should be able to forget specific training data upon user requesta challenge addressed by machine unlearning [9, 28]. In the machine unlearning literature, the standard setup assumes that group of individuals requests the removal Corresponding Author: thomas.demin@unitn.it Figure 1. Comparing unlearning approaches. Previous works assume the forget set to be uniformly distributed. However, reallife unlearning requests do not comply with the uniform distribution assumption [3]. If the forget set distribution is predominant in some groups (e.g., old males), it can lead to performance degradation in such dominant forget groups (i.e., the blue group in the figure). Group-robust Unlearning prevents this from happening. of their data. machine unlearning algorithm then fulfills this request by making the model forget the designated data (i.e., the forget set) while maintaining its accuracy on the test set [28]. The naive solution, called exact unlearning, consists in retraining the model without the forget data, however, this is computationally prohibitive for large models [52]. Approximate unlearning methods [13, 23, 28] overcome this limitation by unlearning the model with significantly less resources but less unlearning guarantees. Most approaches assume that the forget sets are uniformly sampled from the training data [7, 13, 18, 20, 23, 43, 52]. However, studies [3, 48] show that individuals from different social and cultural backgrounds (i.e., groups) request to be forgotten at varying rates: [3] reports that 44.4% of unlearning requests to news websites relate to professional wrongdoing or crimes, while [48] finds that wealthier, highly educated individuals are more likely to request unlearning. Existing methods overlook this imbalance, potentially degrading the accuracy of dominant groups in the 1 forget set and leading to unfair outcomes (see Figs. 1 and 2). This can be critical in scenarios requiring high accuracy across all groups. In recommendation system, if unlearning requests predominantly come from specific group (e.g., young males), the systems quality for that group may degrade, potentially making it unusable. Thus, machine unlearning algorithms must account for distribution shifts. We target this unexplored scenario, which we name group-robust machine unlearning, that aims to unlearn the users data while minimizing the performance deterioration of groups dominating the forget set. Differently from previous works, we tackle both exact and approximate unlearning by (i) finding retraining strategy that preserves the original group robustness [30, 42], and (ii) proposing an approximate unlearning method that efficiently forgets data while preserving robustness for unbalanced forget sets. For (i), we show that reweighting the sampling distribution during retraining compensates for information loss with minimal robustness impact. We validate this strategy (called REWEIGHT) against GROUP-DRO [42], popular group-robust optimization method, showing that REWEIGHT better preserves model group robustness in exact unlearning. For (ii), we introduce an approximate unlearning method called MIU (MUTUAL INFORMATIONAWARE MACHINE UNLEARNING) leveraging mutual information [2] minimization and calibration to unlearn the forget set while preserving model robustness. By minimizing the mutual information between forget-set features and ground-truth labels, we decorrelate unlearning from spurious attributes [30], mitigating performance loss for dominant groups. To prevent affecting other groups, we calibrate the unlearned models mutual information to match the original one. Coupled with REWEIGHT, MIU outperforms established unlearning approaches (L1-SPARSE [23], SALUN [13], and SCRUB [28]) on CelebA [31], Waterbirds [42], and FairFace [24] in both unlearning efficacy and preserved group robustness.1 Contributions. In summary, our contributions are: We are the first to identify the issue of group robustness in approximate unlearning, showing how existing unlearning algorithms degrade model robustness in this setting. We propose simple and effective sample distribution reweighting strategy to mitigate the group accuracy degradation in exact unlearning. We introduce MIU, the first approximate unlearning approach tailored for this task that minimizes the mutual information on the forget set while calibrating it to match the group robustness of the original model. We benchmark existing baselines and MIU on grouprobust machine unlearning using CelebA [31], Waterbirds [42], and FairFace [24], and showing that MIU outperforms existing methods in this task. 1We use robustness and group-robustness interchangeably in this paper. 2. Related work Machine unlearning. Exact machine unlearning methods [1, 4, 46] guarantee that sensitive data is removed. However, they are impractical [9, 28] as retraining (part of) the Instead, approximate unlearnmodel is prohibitive [35]. ing concentrates on computational feasibility by relaxing the guarantees constraints [5, 9, 13, 23, 28]. Most works focus on random [7, 13, 18, 20, 23, 43, 52], and class [5, 8 10, 19, 51] unlearning, respectively forgetting i.i.d. data points, and all data points from single class. While the former lowers the forget-set accuracy to match that of the model retrained without the unlearning data, the latter aims at scoring zero accuracy on the unlearned class. few works [8, 9, 14] explore subclass unlearning, where subset of class (e.g., subclass car from superclass vehicles) is removed from model weights. While subclass and grouprobust unlearning share similarities, the latter focuses on preserving group accuracies, whereas subclass unlearning alters the model to behave as if the target subclass was never in the training set. Related to our research, [12] suggests bi-level optimization to sample adversarial forget sets that are difficult to unlearn. Also related, [6] proposes machine unlearning as an efficient debiasing technique. Instead, [48] investigates how uniform and non-uniform data sampling affect fairness in MLPs and tabular data, limiting the evaluation to exact unlearning. Compared to previous works, this paper investigates and addresses the performance loss caused by non-uniformly distributed forget datasets, proposing an exact and approximate unlearning algorithm to mitigate this issue. Group-robust learning. Methods for group-robust optimization train deep learning models to be robust to spurious correlations. Algorithms are categorized based on their access to group information. Within those that assume access to group annotations [15, 21, 26, 42, 49], group-DRO [42] dynamically reweights the misclassification penalty for each group to optimize the worst-group accuracy. Instead, Idrissi et al. [21] propose simple baseline that subsamples each group to match the size of the smallest one. While these works usually achieve better results, accessing the group information can be challenging (e.g., annotation cost). Within methods agnostic to group information [21, 30, 33, 44, 50], JTT [30] increases the sampling probability of wrongly classified data to improve worst-group accuracy. Correct-N-Contrast [50] uses contrastive loss to pull correctly and misclassified samples of the same class while pushing apart wrongly classified data points of different categories. However, methods that require group information also show state-of-the-art performance on groups discovered from data [11, 25]. This paper is the first to study the intersection between group robustness and machine unlearning. For this reason, we assume complete access to the group information. 2 3. Method section formulates the machine This unlearning task (Sec. 3.1) and the group-robust machine unlearning problem (Sec. 3.2). Then, it introduces the proposed sample distribution reweighting strategy (Sec. 3.3), showing its effectiveness in group-robust machine unlearning. Finally, Sec. 3.4 describes MIU, our approximate unlearning method tailored for group-robust unlearning. 3.1. Machine unlearning Let hφ fθ : be learnable function, where fθ() : is non-linear feature extractor parameterized by θ, and hφ() : is linear classifier parameterized by φ, mapping inputs from the image to the target space . Let Dtr = {(x, y)i}Ntr i=1 be training dataset of size Ntr , where xi is an image, and yi its target label (e.g., age). The machine unlearning goal is to scrub the influence of desired forget set Df Dtr from the pretrained model, trained on Dtr , without altering its performance on retain set Dr = Dtr Df and test data Dte . Let the original model trained on Dtr with algorithm be denoted as hφo fθo (or PRETRAIN). successful unlearning algorithm outputs scrubbed weights {φu, θu} such that hφu fθu is as close as possible to the exact unlearning model hφr fθr (or RETRAIN), trained solely on Dr with algorithm [13, 28, 52]. 3.2. Group-robust machine unlearning In group-robust machine unlearning, we consider the forget data non-uniformly distributed. Therefore, let us redefine the training dataset as Dtr = {(x, y, a)i}Ntr i=1, where xi and yi are defined as in Sec. 3.1, and ai is the protected or sensitive attribute (e.g., gender, ethnicity). Now, let : be the set of all groups, defined as the cartesian product between the target label set and the protected attribute set [26, 42]. We denote the i-th datapoint group as gi = (yi, ai). Each target-sensitive attribute pair (e.g., males between the ages of 20-29) identifies unique group.2 The more samples of group are removed, the lower the resulting group accuracy. Intuitively, removing nonuniformly distributed data changes the retain set group distribution, ultimately harming the models generalization performance on the dominant group of the forget set. Figure 2 shows the accuracy degradation caused by removing different percentages of attractive males from the CelebA [31] dataset, using the L1-SPARSE [23], SALUN [13], and SCRUB [28] approaches. Further analysis is provided in the Appendix. 3.3. Frustratingly easy group-robust unlearning Section 3.2 introduces the group-robust machine unlearning task and shows the extent to which the accuracy of the forget 2Target and protected attributes are color-coded. Figure 2. Unlearning non-uniformly distributed data. We test standard model retraining, and popular approximate unlearning methods (L1-SPARSE [23], SALUN [13], and SCRUB [28]) in group-robust unlearning. The more attractive males are unlearned from CelebA [31], the lower the model accuracy on that group. set dominant group(s) drops after unlearning. Unlike prior works (see Sec. 1), we aim at unlearning non-uniform forget data while preserving the model accuracy on the dominant group of the forget set. Therefore, this section proposes an exact unlearning strategy tailored to this task. To mitigate the performance degradation of the dominant group of Df , we argue that reweighting the data distribution to account for the removed samples is simple and effective baseline that retrains the model with minimal performance drop. Intuitively, increasing the sampling likelihood for partly unlearned groups rebalances the retain set group statistics to match those of the training dataset. Formally, let (xi) = 1 be the probability of samNtr pling xi, let νtr , νr NG respectively be the group frequencies of the training and retain datasets. We reweight : (xi) = αgi (xi) according to the ratio α = νtr , νr αgj where αgi is the adjusted group weight for sample xi. We denote the data distribution reweighting strategy (or REWEIGHT) as ω(), i.e., ω(Dr ) denotes the reweighing strategy applied to the retain dataset. (cid:80) We validate REWEIGHT by comparing the model retrained with REWEIGHT and GROUP-DRO [42]. The popularity of GROUP-DRO [42] in group-robust optimization suggests it should minimize drops in the forget-set dominant group accuracy. Figure 3 summarizes the outcome of our analysis. As byproduct of strongly optimizing worstgroup accuracies, we notice that GROUP-DRO [42] can also increase the forget set accuracy. This issue makes approximate unlearning evaluation more difficult if RETRAIN + GROUP-DRO is used as the gold standard. Assuming hypothetical original forget-set accuracy of 70%, if an approximate unlearning algorithm targeting such gold standard leads to higher accuracy (e.g., 80%), then was the knowledge unlearned if forget-set accuracy increased? Answer3 where (x, g) refers to sampling from the joint distribution, (x, g) refers to sampling from the product of the marginal distributions. To sample from the product of the marginal distributions we sample twice from the joint distribution: (x, g) s.t. (x, g) D, (x, g) D, and keep only and as in [2]. We now use this definition to derive MIU (MUTUAL INFORMATION-AWARE MACHINE UNLEARNING), our proposed method. Unlearning term. We minimize the mutual information between forget set features and their group label to unlearn the forget set Df while maintaining good trade-off between information removal and robustness preservation. Therefore, we denote the unlearning term as: M(Df ; ψ, θ). (3) Intuitively, this term is high when the forget features correlate with the group information. Ideally, we want this term to be low as this will imply that we are unlearning the relation between forget-set features and labels while decorrelating the unlearning process from group information. Calibration term. As unlearning might also affect other groups, we designed an extra term to improve group performance retention. Thus, we minimize the mutual information discrepancy between the original and unlearning model on the reweighted retain dataset ω(Dr ): Lc(ω(Dr ); ψ, θ, θo) = M(ω(Dr ); ψ, θ) + M(ω(Dr ); ψ, θo)2 , (4) where M(ω(Dr ); ψ, θo) estimates the mutual information using features computed by the original backbone fθo(). Equation (4) encourages the unlearned model to mimic the original robustness, preserving its behavior across groups. Retaining term. Since unlearning generally causes performance degradation [13, 23, 28], we perform gradient descent steps on the retain data to ensure that the original model group accuracy is preserved after unlearning. Additionally, we use REWEIGHT to limit group robustness degradation. Therefore, we optimize the cross-entropy loss on the reweighted retain dataset ω(Dr ): Lr(ω(Dr ); φ, θ) = (x,y) ω( Dr ) LCE(x, y; φ, θ). (5) Putting all together. The retaining term in MIU preserves the models original discriminative capabilities. The unlearning term removes Df from model weights while minimizing performance loss for dominant groups in the forget set. Finally, the calibration term ensures that the unlearned model maintains its original robustness. Therefore: φu, θu = arg min φ,θ Lr(ω(Dr ); φ, θ) (cid:123)(cid:122) (cid:125) (cid:124) Retaining term + + M(Df ; ψ, θ) (cid:125) (cid:124) (cid:123)(cid:122) Unlearning term +λ Lc(ω(Dr ); ψ, θ, θo) . (cid:125) (cid:123)(cid:122) Calibration term (cid:124) (6) Further details are in the Appendix. Figure 3. REWEIGHT vs. GROUP-DRO [42]. RETRAIN + REWEIGHT achieves better test and group accuracy alignment with the original model (higher is better). Thus, it better preserves original performance after unlearning. ing this question is non-trivial as the retrained model cannot be accessed for real applications. Furthermore, retraining with GROUP-DRO [42] causes test accuracy drop (-5.8%) in FairFace [24]. On the contrary, REWEIGHT does not suffer from these issues. 3.4. Group-robust unlearning without retraining Section 3.3 shows that sample reweighting is simple and effective approach to machine unlearning that preserves the original models robustness. However, model retraining is inefficient [9, 28] and unpractical [35]. Therefore, this section proposes MIU, our approximate machine unlearning algorithm that jointly tackles unlearning while decorrelating unlearning and group robustness. As scrubbing unlearning data affects the forget set dominant group accuracy, we propose unified objective for jointly unlearning while preserving original group robustness. We use mutual information between output features and group annotation, which upon minimization on forget data jointly unlearns and mitigates the performance degradation on the dominant group of the forget set [40]. Formally, let I(Z; G) = I(Z; (Y, A)) be the mutual information between random variables and G, associated with model features = fθ(x) and group = (y, a), then: (cid:90) (cid:90) I(Z; G) = (Z,G)(z, g) log (cid:18) (Z,G)(z, g) PZ(z)PG(g) (cid:19) dz dg, (1) (Z,G) is the joint pdf of and G, and PZ, PG are where marginal pdfs of and G. Minimizing I(Z, G) reduces the dependency between and while also reducing it between and A, jointly unlearning the network and disentangling features and protected attributes. However, computing the mutual information for continuous variables is generally intractable [2, 36]. Therefore, we follow [2], and estimate the mutual information with an MLP Tψ: M(D; ψ, θ) = (x,g) Tψ(z, g)log (x,g) (cid:104) eTψ(z,g)(cid:105) (2) , 4 4. Experiments This section describes the experimental protocol (Sec. 3.1) and compares MIU with multiple methods in group robust unlearning (Sec. 4.2). We then compare existing approaches and ours when varying the unlearning ratio of the forget set dominant group (Sec. 4.3) and when sampling the forget set from multiple groups (Sec. 4.4). Finally, Sec. 4.5 shows complete ablation study of MIUs components. 4.1. Experimental protocol Datasets. We follow established works in group-robust optimization [21, 30, 37, 38, 42] and conduct experiments on CelebA [31] and Waterbirds [42] datasets, setting attractive and male as the target and protected attributes for CelebA [31], while setting waterbirds and land as the target and sensitive attributes for Waterbirds [42]. We additionally experiment with FairFace [24], given its numerous annotations, using age as the downstream task and randomly choosing the class 20-29 and the ethnicity afro-american as target and protected attributes, respectively. Unless stated otherwise, forget-set images are sampled from groups described by the above target-sensitive attribute pairs. Moreover, unless stated otherwise, the forget set simulates the worst-case scenario where single group is responsible for the unlearning request, leading to high forget distribution imbalance. After unlearning the model must have unlearned the forget data and maintained its original robustness. Baselines. We compare MIU against three state-of-theart machine unlearning approaches. L1-SPARSE [23] forgets sensitive information by fine-tuning the original model on the retain set with sparsity regularization term. SALUN [13] proposes saliency-based unlearning that forgets data via random labeling. SCRUB [28] minimizes the divergence between the unlearned and original model on the retain set while maximizing it on the unlearning data. Following previous works [23, 28], we report PRETRAIN, and RETRAIN, which are computed by fine-tuning an ImageNet [41] pre-trained ResNet-18 [17]. We also report RETRAIN + GROUP-DRO [42] to validate the proposed RETRAIN + REWEIGHT. Further details are in the Appendix. Metrics. To evaluate unlearning and group-robustness, we rely on six different metrics. The first three are, retain (RA), forget (UA), and test (TA) accuracy. We also report the membership inference attack (MIA) [47], which measures the MIA-Efficacy as described in [13, 23]. Finally, we assess the change in group robustness by looking at equalized odds (EO) [16], and the test accuracy of the forget-set dominant group (GA). To ease the interpretation over six metrics, we follow previous works [13, 23] and compute the average gap (Avg. Gap) and per-metric deltas with the gold standard.3 Following them, we do not report whether 3Deltas are computed per each seed and then averaged over three runs. Figure 4. REWEIGHT for group-robust unlearning. As in Fig. 2, we test different methods and REWEIGHT in group-robust unlearning on CelebA [31]. Darker colors are used for methods without the reweighting, while lightened ones correspond to methods coupled with REWEIGHT. As the unlearning ratio grows, methods GA degrade. Instead, adding REWEIGHT restores the original GA. metrics should be maximized or minimized as the machine unlearning objective is to reduce the discrepancy with the gold standard on each metric, except for Avg. Gap which must be maximized. We use RETRAIN + REWEIGHT as the gold standard since it better reduces the gap with PRETRAIN in TA, EO, and GA, compared to RETRAIN and RETRAIN + GROUP-DRO [42]. Further details are in the Appendix. 4.2. Results on group unlearning Tables 1 to 3 show results for group-robust unlearning on CelebA [31], Waterbirds [42], and FairFace [24] using an unlearning ratio of 0.5. CelebA. RETRAIN + REWEIGHT achieves the best tradeoff between original performance preservation and unlearning, showing the best gap with TA (-0.4), EO (-0.1), and GA (+0.5). The forget accuracy (UA) equals that of PRETRAIN, which might seem strange at first glance. Yet, UA and GA share similar values for both PRETRAIN and RETRAIN. This suggests that the model achieves low generalization error, as the forget accuracy aligns with that of unseen samples (from the same group) regardless of whether the forget set was part of the training data. Additionally, as CelebA [31] counts numerous images, REWEIGHT easily preserves original group accuracies, thus, recovering the original model performances and showing the same UA of PRETRAIN. Retraining the model with GROUP-DRO [42] generally leads to unwanted unlearning behaviors (UA increases by 16.2) and reduced TA (-2.9). Plain RETRAIN, instead, suffers performance degradation in the dominant group of the forget set, as we highlight in Fig. 2. Adopting the proposed reweighting strategy leads to better trade-off in group-robust unlearning. MIU achieves the best performance preservation (Tab. 1) by scoring the highest GA both when using (69.0%) and not 5 REWEIGHT Method PRETRAIN RETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU RA 84.6 84.7 81.4 84.5 83.8 (0.7) 84.9 (0.4) 82.1 (2.5) 84.8 (0.3) 83.4 (1.1) 84.4 (0.3) 84.4 (0.3) 84.2 (0.4) UA 66.6 54.6 82.8 66.6 44.3 (22.3) 47.6 (19.0) 40.6 (26.0) 55.3 (11.3) 60.6 (6.0) 64.9 (3.7) 61.6 (5.0) 68.8 (2.6) TA 82.8 82.3 79.9 82.4 81.3 (1.1) 82.4 (0.2) 79.8 (2.6) 82.6 (0.3) 81.3 (1.1) 82.5 (0.2) 82.6 (0.3) 82.5 (0.1) MIA 0.1 0.1 11.3 0.3 0.2 (0.1) 0.1 (0.2) 0.4 (0.2) 0.3 (0.2) 0.2 (0.2) 0.1 (0.2) 0.5 (0.3) 0.1 (0.2) EO 20.9 31.6 3.9 20.8 37.5 (16.7) 31.6 (10.8) 40.1 (19.3) 27.4 (6.6) 27.9 (7.2) 21.4 (1.5) 24.0 (3.2) 20.2 (0.6) GA 67.3 56.9 83.5 67.8 47.5 (20.2) 48.8 (19.0) 42.6 (25.1) 55.9 (11.9) 62.0 (5.7) 66.2 (3.0) 62.9 (4.8) 69.0 (1.3) Avg. Gap - - - - 89.8 91.7 87.4 94.9 96.5 98.5 97.7 99.2 Table 1. Group-robust machine unlearning in CelebA [31]. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap and deltas are computed against RETRAIN + REWEIGHT. To avoid confusion, other reference models are in light gray. Method PRETRAIN RETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA 98.9 98.7 94.7 99. 99.0 (0.2) 100.0 (1.0) 98.8 (0.3) 100.0 (1.0) 98.7 (0.2) 100.0 (1.0) 98.9 (0.2) 99.9 (0.9) UA 84.5 52.4 89.3 59.5 59.5 (7.1) 50.0 (9.5) 60.7 (10.7) 53.6 (8.3) 64.3 (11.9) 47.6 (16.7) 66.7 (11.9) 54.8 (4.8) TA 87.7 86.5 91.6 87.2 85.6 (1.6) 81.8 (5.4) 86.9 (0.7) 86.1 (1.2) 85.0 (2.2) 81.1 (6.1) 87.0 (0.6) 85.8 (1.4) MIA 33.3 54.8 21.4 53. 44.0 (9.5) 90.5 (36.9) 45.2 (10.7) 58.3 (7.1) 46.4 (7.1) 91.7 (38.1) 44.0 (9.5) 59.5 (6.0) EO 26.2 30.4 7.3 28.3 32.2 (4.1) 38.7 (10.4) 31.9 (4.3) 28.3 (3.0) 30.6 (2.3) 39.0 (10.7) 30.9 (3.4) 28.3 (1.8) GA 56.6 49.4 83.1 51.6 48.8 (11.1) 39.3 (12.3) 41.7 (9.8) 53.8 (7.5) 53.7 (8.3) 39.0 (12.5) 44.3 (7.3) 53.7 (4.0) Avg. Gap - - - - 94.4 87.4 93.9 95.3 94.7 85.8 94.5 96.9 Table 2. Group-robust machine unlearning in Waterbirds [42]. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap and deltas are computed against RETRAIN + REWEIGHT. To avoid confusion, other reference models are in light gray. using (55.9%) REWEIGHT. These results highlight that mutual information improves group performance retention. Instead, existing approaches must rely on REWEIGHT to close the gap in GA and EO with MIU as they are agnostic to group-robust machine unlearning. We also notice that using REWEIGHT increases the UA for all tested algorithms. Yet, despite the method used, coupling REWEIGHT with approximate unlearning always recovers most of the original GA. We visualize this behavior in Fig. 4, which shows the same experiment as Fig. 2 while highlighting the REWEIGHT contribution. However, as MIU is strictly designed for this task, it generally achieves higher GA and lower EO than existing approaches, scoring the best Avg. Gap. Waterbirds. Similarly to CelebA [31] experiments, RETRAIN + REWEIGHT achieves the best discrepancy (Tab. 2) in terms of TA (-0.5), EO (+2.1), and GA (-5.0), better preserving original group robustness. RETRAIN + GROUP-DRO [42], increases the UA by 4.8% above PRETRAIN and by 36.9% above RETRAIN which can instead, negatively affect the unlearning evaluation if used as the gold standard for approximate machine unlearning. Compared to the CelebA [31] case, RETRAIN achieves better unlearning-preservation trade-off. Nonetheless, RETRAIN + REWEIGHT is still better candidate for group-robust unlearning, always achieving the best calibration with the original model. In Waterbirds [42], our method outperforms existing methods both with and without REWEIGHT. Yet, REWEIGHT does not provide substantial improvement as only the forget accuracy (from delta of 8.3 to 4.8) and the EO (from delta of 7.5 to 4.0) get signifiInstead, REWEIGHT substantially incantly enhanced. creases L1-SPARSE [23] and SCRUB [28] average UA (+4.8 and +6.0), achieving higher values than RETRAIN + REWEIGHT (59.5%). Nonetheless, it also improves the GA, showing better dominant forget group preservation. We argue that the subtle improvement provided by REWEIGHT is caused by the limited dataset size of Waterbirds [42]. By in6 REWEIGHT Method PRETRAIN RETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU RA 65.6 66.8 61.7 66.7 64.0 (2.7) 66.3 (0.3) 66.9 (0.3) 66.7 (0.1) 64.4 (2.3) 65.1 (1.5) 66.7 (0.2) 64.7 (1.9) UA 79.0 57.8 56.3 69.3 74.1 (4.8) 66.6 (3.0) 65.4 (3.9) 74.7 (5.4) 72.9 (3.6) 69.8 (5.6) 73.4 (4.1) 71.6 (2.3) TA 57.2 56.5 51.4 56.7 56.9 (0.5) 55.9 (0.8) 56.7 (0.6) 57.2 (0.8) 56.0 (0.9) 54.8 (1.8) 57.2 (0.6) 57.1 (0.5) MIA 0.2 0.9 10.2 0.7 0.2 (0.7) 0.3 (0.6) 1.0 (0.5) 0.3 (0.5) 0.3 (0.4) 0.3 (0.4) 0.7 (0.3) 0.3 (0.3) EO 5.4 9.2 2.3 5.6 6.1 (0.9) 9.0 (3.4) 9.9 (4.3) 6.0 (1.1) 6.1 (2.0) 6.6 (1.7) 6.2 (0.7) 5.8 (1.5) GA 71.2 58.7 57.4 69.6 69.4 (0.4) 60.3 (9.3) 61.3 (8.3) 66.1 (3.5) 67.0 (7.1) 63.7 (5.9) 70.2 (1.8) 70.3 (1.2) Avg. Gap - - - - 98.3 97.1 97.0 98.1 97.3 97.2 98.7 98.7 Table 3. Group-robust machine unlearning in FairFace [24]. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap and deltas are computed against RETRAIN + REWEIGHT. To avoid confusion, other reference models are in light gray. creasing the sampling likelihood of the few dominant forget group images left, the network overfits those few samples, limiting robustness preservation. Even without REWEIGHT, MIU outperforms all baselines, regardless of whether they use REWEIGHT, further validating our design choices. FairFace. Differently from other datasets, RETRAIN + GROUP-DRO [42] struggles to preserve original model accuracy (Tab. 3) in both the dominant forget group (-13.8) and the test set (-5.8). We ascribe this behavior to numerous FairFace [24] groups that make the group-robust optimization challenging. Instead, RETRAIN + REWEIGHT overcomes this issue as it simply reweights group sampling probabilities to match the original training dataset statistics, achieving better GA (-1.6) and TA (-0.5). Importantly, our experiments highlight key advantage of REWEIGHT, which functions effectively off the shelf with the original model hyperparameters. Unlike RETRAIN + GROUPDRO [42], it requires no additional tuning. All methods show good alignment to RETRAIN + REWEIGHT, even without reweighting. TA and GA are generally preserved, with SALUN [13] scoring lowest at 55.9% (delta 0.8) and 60.3% (delta 9.3) respectively. However, methods without REWEIGHT show higher UA than RETRAIN, indicating partial scrubbing of the forget set. Regardless, Sec. 4.5 shows that the high UA of MIU is not caused by poor unlearning but the calibration term, which recovers the original group robustness. REWEIGHT further enhances model robustness, reflected in higher GA and lower EO. Finally, MIU + REWEIGHT and SCRUB [28] + REWEIGHT achieve the same Avg. Gap, but while our method shows better UA and GA alignment with RETRAIN + REWEIGHT, SCRUB [28] obtains better alignment in EO terms. Although REWEIGHT does not fundamentally improve the Avg. Gap, it generally enhances GA and EO, promoting performance retention. Discussion. UA strongly correlates with GA after unlearnFigure 5. Group-robust unlearning across different unlearning ratios. We compare L1-SPARSE [23], SALUN [13], and SCRUB [28] against our approach while using the REWEIGHT strategy on all methods. MIU achieves overall the best Avg. Gap when varying the unlearning ratio. ing in all three datasets. high UA-GA correlation suggests that the forget set behaves as unseen data of the same group, indicating proper unlearning. Moreover, MIU consistently approximates RETRAIN+REWEIGHT better than existing methods or is on par, while REWEIGHT reliably preserves performance without drawbacks in model retraining or approximate unlearning. 4.3. Impact of different unlearning ratios Figure 5 analyzes methods Avg. Gap across different unlearning ratios. On CelebA [31], all methods show similar discrepancies from RETRAIN + REWEIGHT at low unlearning ratio (i.e., 0.1), likely because the retain sets group statistics are close to the training distribution. Controversy, all algorithms struggle at 0.1 unlearning ratio in Waterbirds [42], where the small forget set size causes UA fluctuations and affects the BatchNorm [22] estimation. As the unlearning ratio grows (i.e., 0.5 and 0.9), MIU outperDataset Eq. (5) Eq. (3) Eq. (4) RW CelebA Waterbirds FairFace UA 53.6 55.3 41.7 68.8 47.6 53.6 16.7 54.8 63.1 74.7 87.1 71. GA 54.0 55.9 42.3 69.0 51.1 53.8 16.8 53.7 59.2 66.1 81.1 70.3 Gap 94.1 94.9 78.9 99. 92.5 95.3 81.9 96.9 96.1 98.1 93.0 98.7 Figure 6. Sampling the forget set from multiple groups. We evaluate our method against L1-SPARSE [23], SALUN [13], and SCRUB [28] when the forget set is sampled from multiple FairFace [24] groups. MIU is more consistent across experiments, always achieving the best result. Table 4. MIU ablations. We compute MIU ablations on each of the three investigated datasets. From left to right, we report the investigated dataset, the retaining term, the unlearning term, the calibration term, and REWEIGHT. We measure performance using UA, GA, and Avg. Gap. The configuration that corresponds to MIU + REWEIGHT is highlighted. forms baselines by growing margin. At unlearning ratio 0.9, MIU achieves 98.3% on CelebA [31] (vs. 94.8% of L1SPARSE [23]), and 97.2% on Waterbirds [42] (vs. 93.6% of SCRUB [28]). Furthermore, MIU remains more consistent across unlearning ratios, confirming that our design choices effectively narrow the Avg. Gap with RETRAIN + REWEIGHT. Full Fig. 5 tables are reported in the Appendix. 4.4. Multi-group unlearning This section compares MIU and existing approaches in multi-group unlearning, i.e., when the forget set data is sampled from multiple groups. Figure 6 shows the outcome of this experiment on FairFace [24], given its numerous groups, with varying number of groups in the forget set and an unlearning ratio fixed to 0.5. All methods achieve high scores and good discrepancy with RETRAIN + REWEIGHT, though SCRUB [28] performs worst (97.1% of Avg. Gap). Performance gaps shrink as more groups are included, as forget-set statistics align with the original training distribution, reducing non-uniform sampling effects. Nonetheless, MIU is the most consistent, highlighting its effectiveness in group-robust machine unlearning. 4.5. Ablations Table 4 shows the ablation of MIU components in all three datasets by systematically adding each element to understand its contribution. We mark with when the component is used in the experiment and when it is not. From left to right, we list retaining term, unlearning term, calibration term, and REWEIGHT. In the first row, we consider our method when only the unlearning term and the retaining term are used. We highlight how the UA is low for all three datasets, demonstrating that mutual information minimization can be used to unlearn. This baseline already achieves remarkable Avg. Gap with RETRAIN + REWEIGHT, scoring 94.1% in CelebA [31], which is the best among methods that do not use REWEIGHT. Adding our calibration term (Eq. (4)), leads to an increase in GA in all three datasets, with FairFace [24] showing growth of 6.9%. We highlight that the forget accuracy also grows. Yet, this increase is caused by Eq. (4) that calibrate the mutual information to match the original group robustness. Therefore, the high UA cannot be blamed on the poor unlearning. Although most previous approaches exploit retaining term [9, 13, 23, 28], we also ablate this component for completeness. As previous works suggest [9, 28], removing the retaining term negatively impacts model utility, resulting in the lowest performance overall. Similarly, when adding REWEIGHT, the Avg. Gap gets improved in all three datasets, as we also highlight in Fig. 4. These results highlight the contribution of each of MIU components that allow for unlearning (Eq. (3)) while preserving forget set dominant group performance (Eq. (4)). 5. Conclusion This paper is the first to address the performance degradation caused by non-uniformly distributed forget sets in both model retraining and approximate unlearning. We show that adopting simple data distribution reweighting (REWEIGHT) for retraining the model is simple and better alternative than retraining with GROUP-DRO [42]. Moreover, we propose the first approximate unlearning method (MIU) that unlearns personal data while reducing the risk of degradation of the forget set dominant group. Our evaluation demonstrates that RETRAIN + REWEIGHT consistently improves over simple RETRAIN while MIU outperforms existing baselines in group-robust machine unlearning. 8 Acknowledgements. We acknowledge the CINECA award under the ISCRA initiative for the availability of highperformance computing resources and support. E.R. and M.M. are supported by the MUR PNRR project FAIR - Future AI Research (PE00000013), funded by NextGeneration EU. E.R. is also supported by the EU projects AI4TRUST (No.101070190) and ELIAS (No.01120237) and the PRIN project LEGO-AI (Prot.2020TA3K9N). T.D.M. is funded by NextGeneration EU. This work has been supported by the French National Research Agency (ANR) with the ANR-20-CE23-0027. We thank Olivier Laurent for his valuable feedback and insightful suggestions."
        },
        {
            "title": "References",
            "content": "[1] Nasser Aldaghri, Hessam Mahdavifar, and Ahmad Beirami. Coded machine unlearning. IEEE Access, 2021. 2 [2] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mine: mutual information neural estimation. In ICML, 2018. 2, 4, 12 [3] Theo Bertram, Elie Bursztein, Stephanie Caro, Hubert Chao, Rutledge Chin Feman, Peter Fleischer, Albin Gustafsson, Jess Hemerly, Chris Hibbert, Luca Invernizzi, et al. Five years of the right to be forgotten. In SIGSAC, 2019. 1 [4] Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In SP, 2021. 2 [5] Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang. Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary. In CVPR, 2023. 2 [6] Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey Tianyi Zhou, Jian Wu, and Zuozhu Liu. Fast model debias with machine unlearning. In NeurIPS, 2024. [7] Jiali Cheng and Hadi Amiri. Multimodal machine unlearning. arXiv preprint arXiv:2311.12047, 2023. 1, 2 [8] Xinwen Cheng, Zhehao Huang, and Xiaolin Huang. Machine unlearning by suppressing sample contribution. arXiv preprint arXiv:2402.15109, 2024. 2 [9] Vikram Chundawat, Ayush Tarun, Murari Mandal, and Mohan Kankanhalli. Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher. In AAAI, 2023. 1, 2, 4, 8 [10] Vikram Chundawat, Ayush Tarun, Murari Mandal, and Mohan Kankanhalli. Zero-shot machine unlearning. In TIFS, 2023. 2 [11] Moreno DInc`a, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, and Nicu Sebe. Openbias: Open-set bias detection in text-to-image generative models. In CVPR, 2024. [12] Chongyu Fan, Jiancheng Liu, Alfred Hero, and Sijia Liu. Challenging forgets: Unveiling the worst-case forget sets in machine unlearning. In ECCV, 2024. 2 [13] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. In ICLR, 2024. 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18 [14] Jack Foster, Stefan Schoepf, and Alexandra Brintrup. Fast machine unlearning without retraining through selective synaptic dampening. In AAAI, 2024. 2 [15] Karan Goel, Albert Gu, Yixuan Li, and Christopher Re. Model patching: Closing the subgroup performance gap with data augmentation. arXiv preprint arXiv:2008.06775, 2020. 2 [16] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In NeurIPS, 2016. 5, 12, 16 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 5, 11 [18] Zhengbao He, Tao Li, Xinwen Cheng, Zhehao Huang, and Xiaolin Huang. Towards natural machine unlearning. arXiv preprint arXiv:2405.15495, 2024. 1, 2 [19] Tuan Hoang, Santu Rana, Sunil Gupta, and Svetha Venkatesh. Learn to unlearn for deep neural networks: Minimizing unlearning interference with gradient projection. In WACV, 2024. 2 [20] Mark He Huang, Lin Geng Foo, and Jun Liu. Learning to unlearn for robust machine unlearning. In ECCV, 2024. 1, 2 [21] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves comIn Conference on Causal petitive worst-group-accuracy. Learning and Reasoning, 2022. 2, 5 [22] Sergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 7 [23] Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. Model sparsity can simplify machine unlearning. In NeurIPS, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, [24] Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In WACV, 2021. 2, 4, 5, 7, 8, 11, 12, 14, 15, 16, 17, 18, 19 [25] Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin. Discovering and mitigating visual biases through keyword explanation. In CVPR, 2024. 2 [26] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. In ICLR, 2023. 2, 3 [27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 11 [28] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. In NeurIPS, 2024. 1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, [29] Matt Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In NeurIPS, 2017. 16 9 [46] Haonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua Li, and Xiaodong Lin. Arcane: An efficient architecture for exact machine unlearning. In IJCAI, 2022. 2 [47] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In CSF, 2018. 5, 11 [48] Dawen Zhang, Shidong Pan, Thong Hoang, Zhenchang Xing, Mark Staples, Xiwei Xu, Lina Yao, Qinghua Lu, and Liming Zhu. To be forgotten or to be fair: Unveiling fairness implications of machine unlearning methods. AI and Ethics, 2024. 1, [49] Jingzhao Zhang, Aditya Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, and Suvrit Sra. Coping with label shift via distributionally robust optimisation. In ICLR, 2021. 2 [50] Michael Zhang, Nimit Sohoni, Hongyang Zhang, Chelsea Finn, and Christopher Re. Correct-n-contrast: contrastive approach for improving robustness to spurious correlations. In ICML, 2022. 2 [51] Hongbo Zhao, Bolin Ni, Junsong Fan, Yuxi Wang, Yuntao Chen, Gaofeng Meng, and Zhaoxiang Zhang. Continual forgetting for pre-trained vision models. In CVPR, 2024. 2 [52] Kairan Zhao, Meghdad Kurmanji, George-Octavian Barbulescu, Eleni Triantafillou, and Peter Triantafillou. What makes unlearning hard and what to do about it. In NeurIPS, 2024. 1, 2, 3 [30] Evan Liu, Behzad Haghgoo, Annie Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In ICML, 2021. 2, 5, 16 [31] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. In ICCV, 2015. Deep learning face attributes in the wild. 2, 3, 5, 6, 7, 8, 11, 12, 13, 14, 15, 16, 17, 18, 19 [32] Alessandro Mantelero. The eu proposal for general data protection regulation and the roots of the right to be forgotten. Computer Law & Security Review, 2013. [33] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: De-biasing classifier from biased classifier. In NeurIPS, 2020. 2 [34] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural In NeurIPSW, images with unsupervised feature learning. 2011. 11 [35] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022. 2, 4 [36] Liam Paninski. Estimation of entropy and mutual information. Neural computation, 2003. 4 [37] Sungho Park and Hyeran Byun. Fair-vpt: Fair visual prompt tuning for image classification. In CVPR, 2024. 5 [38] Sungho Park, Jewook Lee, Pilhyeon Lee, Sunhee Hwang, Dohyung Kim, and Hyeran Byun. Fair contrastive learning for facial attribute classification. In CVPR, 2022. 5 [39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 11 [40] Ruggero Ragonesi, Riccardo Volpi, Jacopo Cavazza, and Vittorio Murino. Learning unbiased representations via mutual information backpropagation. In CVPRW, 2021. 4, 11 [41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015. 5, 11 [42] Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worstcase generalization. In ICLR, 2020. 2, 3, 4, 5, 6, 7, 8, 11, 12, 14, 15, 16, 17, 18, 19 [43] Shaofei Shen, Chenhao Zhang, Yawen Zhao, Alina Bialkowski, Weitong Tony Chen, and Miao Xu. Labelagnostic forgetting: supervision-free unlearning in deep models. In ICLR, 2024. 1, [44] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Re. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. In NeurIPS, 2020. 2 [45] Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). Practical Guide, 1st Ed., Cham: Springer International Publishing, 2017. 1 10 Group-robust Machine Unlearning"
        },
        {
            "title": "Supplementary Material",
            "content": "This supplementary material details the main papers experimental protocol and additional results. Appendix discusses the implementation details, reporting MIU pseudocode, the hardware used, training hyperparameters, and the implementation of chosen baselines. Then, Appendix formalizes the metrics used, Appendix reports results that could not be included in the main paper, and Appendix describes our works limitations and potential negative societal impacts. A. Implementation details This section presents general overview of the implementation details of this paper, by highlighting MIU pseudocode, the hardware used to perform the experiments, the software, and the algorithms of choice. MIU pseudocode. Algorithm 1 presents MIU pseudocode in PyTorch-like [39] style. We follow previous works [13, 28] and compute unlearning and retaining steps separately. Thus, we alternate between computing an unlearning epoch using Eq. (3) and retaining one using Eqs. (4) and (5). Like SCRUB [28], we find it beneficial to stop performing unlearning steps after predefined number of epochs (i.e., 5 out of 10). Contrary to Ragonesi et al. [40], we do not update the mutual information representation at every step. Instead, we empirically observed that updating it via 100 gradient updates for the first epoch and 10 updates for the remaining iterations is sufficient to achieve satisfactory results and limit the required resources. As we keep the optimization steps number fixed, the overall mutual information estimation overhead depends on the dataset size. For small dataset like Waterbirds [42], we estimate an overhead of about 1.80. Instead, for CelebA [31] and FairFace [24], we estimate an increase of approximately 1.03 in unlearning time, which is negligible. Hardware and training details. As Sec. 4.1 mentions, we compute PRETRAIN and RETRAIN by fine-tuning ResNet18 [17] pre-trained on ImageNet [41] for 30 epochs, using SGD with 0.9 momentum and weight decay. The learning rate is decayed with cosine annealing scheduler for the entire training. We additionally warm-up the learning rate for the first two epochs using linear scheduler. We apply standard data augmentation techniques such as random resized crop, random horizontal flip, and input normalization [17]. We limited the fine-tuning to 10 epochs for unlearning methods, searching for the optimal configuration for the other hyperparameters. The λ parameter of MIU is set between 1 and 10 (see Appendix C.4). We ran all experiments on single A100 Nvidia GPU, using PyTorch [39]. Baselines. As Sec. 4.1 mentions, we reimplemented all three investigated baselines to experiment easily in grouprobust unlearning following the existing codebases. For L1SPARSE [23], we perform 10 fine-tuning epochs on the retain set with linearly decaying L1 regularization that follows this rule: γt = (1 t/T )γ, where is the epoch, is the total number of iterations, and γ the initial penalty. For SALUN [13] we followed the small-scale implementation of the original codebase (i.e., the implementation for CIFAR-10 [27] and SVHN [34]) as it is meant for datasets with few classes. We first compute the saliency mask using the gradient information from the forget set, pruning 50% of the network weights. We tune the remaining weights for 10 epochs by alternating full pass on the forget set and full pass on the retain set. Also, SCRUB [28] is implemented by separating forget and retain steps, following the original code. The forget step maximizes the KL divergence between the original and the unlearned model in the forget data set. Following the original paper, we stop computing it after 3, 5, or 7 epochs. Instead, the retain step minimizes the linear combination between the cross-entropy loss and the KL divergence between the original and unlearned model, respectively scaled by 0.99 and 0.001, as reported in [28]. We note that all methods use the same dataset splits; therefore, they must unlearn the same forget set. B. Metrics This section provides further details on the adopted metrics, i.e., the RA, UA, TA, the membership-inference attack (MIA) [47], the dominant forget group accuracy (GA), the equalized odds (EO), and the average gap (Avg. Gap) [13]. RA, UA, and TA. We evaluate the model on the retain, forget, and test sets to compute the retain, forget, and test accuracy. Therefore, we report the ratio of correctly classified samples for each of these subsets of the dataset. MIA. To compute the membership inference attack, we follow previous works [13, 23] and train model on retain and validation losses to predict membership. Therefore, we assign different binary label to retain and validation losses and train the model to discriminate them. After model training, we use such model to infer the membership of forget set data. In all tables, we report the MIA-Efficacy [23] that is computed as follows: MIA-Efficacy = TN Df , (7) where TN are the true negatives, i.e., the number of samples the MIA predicted as non-members. Instead of training Algorithm 1 PyTorch-like MIU pseudocode. def MIU(model, mine, mine original, optim, train dl, retain dl, forget dl): unlearned = copy.deepcopy(model) for epoch in range(epochs): # Update mine as in [2] tune mine(unlearned, mine, train dl) # Use mine to unlearn the forget dataset for the first forget epochs epochs if epoch < forget epochs: for image, group in forget dl: group bar = torch.randint like(group, num groups) loss = mine(unlearned(image), group, group bar) update model(loss, optim) # Apply fine-tuning steps at every epoch using the reweighted sampler for image, target, group in reweight(retain dl): group bar = torch.randint like(group, num groups) loss = F.cross entropy(head(unlearned(image)), target) loss += lambda * F.mse loss( mine(unlearned(image), group, group bar), mine original(model(image), group, group bar).detach() ) update model(loss, optim) return unlearned Figure 7. Unlearning non-uniformly sampled data. We test standard model retraining, and popular approximate unlearning methods (L1-SPARSE [23], SALUN [13], SCRUB [28]) in group-robust unlearning. The more samples from specified group are unlearned, the lower the model accuracy on that group. While the drop is more evident in CelebA [31], methods also show significant performance degradation in Waterbirds [42] and FairFace [24] overall. support vector machine as in [13, 23], we used random forest as accuracies are comparable with an SVM. Moreover, training is faster since it can be easily parallelized. The higher the MIA-Efficacy, the better the model privacy protection. EO. Equalized Odds [16] measures model fairness or prediction dependencies on protected attributes. To compute the EO we measure model performance discrepancy by varying the sensitive attribute value and averaging over different target labels. Formally EO for binary classification model is computed as follows [16]: EO = 1 2 1 (cid:88) y=0 ( ˆY = 1 = y, = 0) + (8) ( ˆY = 1 = y, = 1) , where ˆY , Y, are random variables describing model predictions, target attributes, and sensitive attributes. EO measures the absolute difference in outputting positive prediction when the protected attribute equals 1 and 0, averaging In the FairFace [24] case, over the two target attributes. we set all classes and protected attributes that do not match those of the dominant group of the forget set as = 0 and = 0, as FairFace counts more than two classes and more than two attributes. The lower the EO the better the model fairness. GA. Short for Group Accuracy, GA measures the accuracy of the dominant group of the forget set. In other words, it measures the ratio of correctly classified test samples that belong to the same dominant group(s) of the forget set. Therefore, let Dgf te = {(xi, yi, ai) gi = gf , gi = (yi, ai)} be the subset of the test set composed of all samples of Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 83.8 83.8 83.7 82.2 83.1 84.4 83. 82.2 83.5 84.4 83.8 0.0 0.1 0.2 0.1 0.8 0.0 0.1 0.2 0.1 0.0 0.2 67.9 62.0 63.2 52.8 69.2 64.9 63.1 60.7 61.9 66.8 67.2 0.5 1.6 1.6 0.5 8.5 0.6 3.8 2.0 2.0 0.7 5.6 82.7 82.5 82.7 81.5 81.8 82.9 82. 81.5 82.6 82.9 82.4 0.1 0.1 0.2 0.0 0.9 0.2 0.0 0.2 0.1 0.1 0.2 MIA 0.1 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1 0.2 0. 0.1 0.1 0.0 0.0 0.1 0.0 0.0 0.0 0.1 0.0 0.1 EO GA Avg. Gap 20.8 21.9 21.2 29.8 23.8 22.0 22.3 27.4 22.7 20.4 20. 1.0 1.3 0.5 0.1 1.8 0.3 0.4 0.6 1.0 0.2 0.6 69.0 64.0 65.8 56.0 69.1 65.0 64.0 63.4 63.1 67.6 67.8 0.9 1.9 1.7 1.4 8.0 0.1 3. 2.2 2.0 0.5 4.3 - - - 94.8 96.9 99.1 98.6 97.3 98.3 98.8 98. 0.5 2.0 0.2 0.8 0.8 0.2 0.6 1.2 Table 5. Group-robust machine unlearning in CelebA [31] with 0.1 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.1. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 84.6 84.7 84.5 83.8 84.9 82.1 84.8 83.4 84.4 84.4 84. 0.1 0.4 0.1 0.2 0.2 2.3 0.0 0.1 0.2 0.5 0.1 66.6 54.6 66.6 44.3 47.6 40.6 55.3 60.6 64.9 61.6 68.8 1.8 4.1 2.4 3.5 4.1 4.8 0. 1.8 2.4 2.3 0.3 82.8 82.3 82.4 81.3 82.4 79.8 82.6 81.3 82.5 82.6 82. 0.0 0.1 0.1 0.2 0.2 2.4 0.2 0.2 0.2 0.4 0.1 MIA 0.1 0.1 0.3 0.2 0.1 0.4 0.3 0.2 0.1 0.5 0.1 0.0 0.1 0.1 0.1 0.1 0.1 0.1 0.0 0.0 0.1 0.0 EO GA Avg. Gap 20.9 31.6 20.8 37.5 31.6 40.1 27.4 27.9 21.4 24.0 20.2 0.7 0.9 0. 0.2 0.7 2.7 0.4 0.4 1.4 1.3 0.6 67.3 56.9 67. 47.5 48.8 42.6 55.9 62.0 66.2 62.9 69.0 2.4 4.3 1.6 3.0 3.9 5.4 1.0 2.1 2.2 1.0 1.2 - - - 89.8 91.7 87.4 94.9 96.5 98.5 97.7 99.2 0.4 0.8 3.6 0. 1.2 1.0 1.5 0.6 Table 6. Group-robust machine unlearning in CelebA [31] with 0.5 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. group gf , then GA is computed as follows: GA = 1 Dgf te gf te (cid:88) i=1 1 [(hφ fθ)(xi) = yi] , (9) where 1 is the indicator function, returning 1 if the argument is True, and 0 otherwise. Like other accuracy metrics, the closer GA is to 1 (or 100%), the better the model is at classifying data of the dominant group of the forget set. Avg. Gap. Following previous works [13, 23], we compute the Avg. Gap to simplify the comparison among different methodologies. Avg. Gap is computed as the average metric discrepancy between the unlearned model let = and the retrained gold standard. {RA, UA, TA, MIA, EO, GA} be the set of all metrics used in this paper, then Avg. Gap is computed as follows: (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) r, θs r) (cid:12) (cid:12) (10) where m(φs r, θs r) are calculated using unlearned and retrained model weights, is the experiment u) and m(φs u) m(φs Avg. Gap = Formally, (cid:88) m(φs u, θs u, θs (cid:88) 1 m , seed and (cid:80) is the average. The closer Avg. Gap is to 1 (or 100 in the tables), the better the approximation of the retrained model. C. Additional results This section contains the additional results that could not be included in the main paper. Appendix C.1 intuitively shows the accuracy degradation caused by non-uniformly sampled forget sets in all three investigated datasets. Appendices C.2 and C.3 report tables associated with experiments on different unlearning ratios and sampling the forget set from multiple groups. Appendix C.4 further expands MIU ablations. Finally, Appendix C.5 further evaluates MIUs and REWEIGHTs fairness and robustness preservation. C.1. Unlearning non-uniformly sampled data Figure 2 of the main paper shows how the forget set dominant group accuracy drops when increasing the unlearning ratio, limiting the analysis to CelebA [31] due to space constraints. This section reports the same experiment on all 13 Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 85.3 86.8 84.2 86.9 87.0 71.5 87. 85.0 85.1 64.2 82.9 0.1 0.4 0.9 0.1 0.3 1.2 0.1 0.4 1.0 5.2 3. 67.6 27.8 64.2 0.9 2.4 4.5 15.6 17.3 34.0 31.6 1.5 10.0 1.6 2.0 56.0 59.7 66.3 64.1 3.2 11.6 6.2 3.7 82.7 80.7 81.8 79.6 80.0 67.0 81. 80.6 81.4 62.8 79.8 0.1 0.2 0.2 0.2 1.0 1.1 0.2 0.2 0.6 4.4 2. MIA 0.1 1.3 0.4 0.1 0.2 0.4 10.0 8.2 0.9 1.9 2.6 5.4 0.9 0. 5.1 1.0 0.2 1.5 2.1 0.8 0.1 1.2 EO GA Avg. Gap 20.0 50.0 22.9 54.6 44.4 36.6 38.3 0.8 1.0 0.9 0.8 0.8 3.3 0.2 68.0 29.9 66.8 17.0 18.3 34.6 32.8 0.9 2.2 4.8 1.1 9.9 2.1 1.8 31.1 23.5 16.0 25.8 0.4 2.2 10.7 3.1 58.5 60.6 66.9 66. 2.7 10.8 7.4 4.6 - - - 75.9 78.4 82.6 85.5 94.8 94.3 87.8 98.3 1.9 2.9 1.1 1. 1.5 1.0 1.3 0.6 Table 7. Group-robust machine unlearning in CelebA [31] with 0.9 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.9. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA MIA EO GA 99.0 99.0 98. 0.1 0.2 0.3 73.3 46.7 46.7 18.9 24.9 24.9 99.0 100.0 98.9 99. 0.1 0.0 0.1 0.0 99.0 99.9 98.8 100.0 0.1 0.1 0.2 0.0 60.0 53.3 53.3 60. 16.3 9.4 9.4 16.3 73.3 53.3 53.3 73.3 18.9 9.4 9.4 18.9 88.0 87.1 87. 85.3 78.3 86.9 87.1 85.3 76.7 86.8 87.3 0.5 0.8 0. 0.8 3.3 0.4 0.2 1.2 2.9 0.7 0.3 53.3 60.0 66.7 9.4 28.3 24.9 46.7 66.7 53.3 33. 24.9 9.4 18.9 18.9 53.3 86.7 60.0 73.3 24.9 9.4 16.3 24.9 25.9 27.4 26. 29.2 42.7 31.1 26.1 29.1 45.9 31.5 26.2 0.7 1.7 1. 1.7 5.1 1.1 2.1 2.8 4.7 0.8 0.3 57.6 57.4 57.6 57.9 33.3 44.3 59.2 58.2 29.6 42.8 61.7 2.0 2.9 4. 2.6 7.3 3.5 5.3 3.8 7.4 3.5 0.8 Avg. Gap - - - 92.3 81.6 91.3 91.2 92.2 81.3 89.8 93.3 3.0 6.7 2.0 9.1 3.0 2.9 0.4 0.7 Table 8. Group-robust machine unlearning in Waterbirds [42] with 0.1 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.1. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. three investigated datasets for completeness. sociated with Fig. 7 using all investigated metrics. Figure 7 shows accuracy variations of attractive males, waterbirds on land, and 20-29 years old afro-americans, when unlearned from respectively CelebA [31], Waterbirds [42], and FairFace [24] with different unlearning ratios: 0.1, 0.5, 0.9. While the accuracy drop is more evident on CelebA [31], it is also visible in the other two datasets. Interestingly, some unlearning methods are more robust to this performance drop than others (e.g., L1-SPARSE [23] scores 49.9% in Waterbirds [42] with unlearning ratio 0.9 vs. 30.8% of SALUN [13], which is the second best). Although some methods are more robust, we argue that the quality of the unlearning process influences the accuracy of the dominant group of the forget set. The less the forget set was unlearned, the more the performance retention. As an example, Sec. 4.2 highlights that L1-SPARSE [23] fails to effectively unlearn the forget set in the FairFace [24] experiment, showing higher UA and GA (74.1% and 69.4%) than the RETRAIN (57.8% and 58.7%). Thus, to better investigate the relationship between unlearning effectiveness and performance retention, Appendix C.2 reports tables asC.2. Impact of different unlearning ratios For completeness purposes, this section reports all tables associated with Figs. 2, 4, 5 and 7. For these experiments, the main paper summarizes the results to reduce the occupied space and simplify the interpretation (i.e., limiting the reported metrics to one). Therefore, Tabs. 5, 7, 8, 10, 11 and 13 present experiments in CelebA [31], Waterbirds [42], and FairFace [24] datasets, with unlearning ratios of 0.1, and 0.9. Additionally, Tabs. 6, 9 and 12 report the standard deviations of Tabs. 1 to 3. Overall, we notice that in CelebA [31], the higher the unlearning ratio, the lower the forget accuracy (from 63.1% to 31.6% using MIU), with the gap being reduced when REWEIGHT is included in the retaining step (around 64-68% with MIU). RA, TA, and MIA remain stable across different unlearning ratios. Instead, EO behaves similarly to GA and UA, with high values at high unlearning ratios (e.g., MIU scores 22.3% vs. 38.3% with 0.1 and 0.9 unlearning ratios). In Waterbirds [42], metrics do not show global trends, 14 Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA MIA EO GA 98.9 98.7 99.0 0.3 0.3 0.1 99.0 100.0 98.8 100.0 0.1 0.0 0.2 0.0 98.7 100.0 98.9 99.9 0.1 0.0 0.2 0.1 84.5 52.4 59.5 1.7 8.9 11.8 59.5 50.0 60.7 53.6 8.9 5.1 7.7 7.7 64.3 47.6 66.7 54.8 5.8 4.5 1.7 14.7 87.7 86.5 87.2 85.6 81.8 86.9 86.1 85.0 81.1 87.0 85.8 0.5 0.2 0.3 0.4 0.4 0.6 1.0 1.2 1.9 0.5 0.7 33.3 54.8 53.6 6.1 9.4 8.7 44.0 90.5 45.2 58.3 11.8 3.4 8.9 8.9 46.4 91.7 44.0 59.5 12.7 7.3 8.9 12.1 26.2 30.4 28.3 32.2 38.7 31.9 28.3 30.6 39.0 30.9 28.3 1.9 0.5 2.0 1.8 1.2 1.8 1.7 1.4 2.2 1.3 2.9 56.6 49.4 51.6 48.8 39.3 41.7 53.8 53.7 39.0 44.3 53. 6.0 1.6 6.0 7.4 3.3 1.7 2.6 4.3 1.6 3.1 3.8 Avg. Gap - - - 94.4 87.4 93.9 95.3 94.7 85.8 94.5 96.9 0.3 3.5 1.4 0. 1.1 4.2 1.3 1.6 Table 9. Group-robust machine unlearning in Waterbirds [42] with 0.5 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA MIA EO GA 98.6 98.9 98. 0.6 0.2 0.1 98.9 100.0 97.8 100.0 0.2 0.0 0.1 0.0 76.0 41.3 41. 60.0 40.0 30.7 66.7 9.1 5.7 2.5 3.3 4.3 1.9 5.7 99.0 100.0 98.0 98. 0.2 0.0 0.1 0.2 59.3 45.3 33.3 44.7 10.6 2.5 3.4 3.4 86.5 84.3 85. 82.9 81.3 86.1 85.7 84.6 80.3 86.2 83.1 0.4 0.3 0. 1.2 0.9 0.5 0.7 0.6 0.7 0.7 1.3 44.7 68.7 62.7 50.7 92.7 52.7 58.7 45.3 87.3 54.7 65.3 4.1 6.8 3. 5.0 3.4 3.4 4.7 6.8 1.9 5.2 3.4 28.3 36.4 33. 35.3 41.4 36.6 32.1 31.5 41.8 35.9 35.7 1.9 1.4 1. 0.4 1.3 1.0 1.5 0.7 0.5 1.5 2.2 55.9 41.7 43.0 49.9 30.8 25.1 49.8 55.0 31.9 28.0 45.0 5.2 3.9 2. 4.6 2.2 1.5 3.3 4.0 4.8 3.4 1.7 Avg. Gap - - - 92.9 89.6 92.7 93.4 91.5 90.9 93.6 97.2 1.0 2.0 0.7 1.3 4.1 1.0 1.2 0.3 Table 10. Group-robust machine unlearning in Waterbirds [42] with 0.9 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.9. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. except for EO and GA, which worsen as the unlearning ratio grows. Therefore, methods lose accuracy on the dominant group of the forget set as the ratio of unlearned samples grows, with SCRUB [28] showing the highest drop (from 44.3% to 25.1%). Similarly, GA and EO worsen even when adding REWEIGHT. As the unlearning ratio grows, the number of forget set dominant group samples left in the retain set lowers. Given the small size of Waterbirds [42] (4795 samples, of which 56 are in the smallest group), REWEIGHT strongly increases the sampling likelihood of restricted number of samples to preserve original accuracy, causing overfitting. Thus, the overall benefit of REWEIGHT is reduced (GA increases by only 1.3 with RETRAIN). Also FairFace [24] shows general drop in UA, which grows when REWEIGHT is applied. EO and GA also behave like in CelebA [31], with an enhanced degradation at higher unlearning ratios. However, MIU shows good robustness even without REWEIGHT, scoring EO and GA that are close to RETRAIN+REWEIGHT, e.g., 10.4% vs. 11.9% in EO and 56.8% vs. 53.9 in GA with 0.9 unlearning ratio (Tab. 13). C.3. Multi-group unlearning This section further expands the experimental protocol of Sec. 4.4, highlighting the sampling composition and reporting all investigated metrics. For the 9 groups experiment we sampled the forget set data with either 20-29 y.o., 5059 y.o., and 3-9 y.o. target attributes and afro-american, latino-hispanic, and white sensitive attributes. Instead, for the 25 groups experiment, we additionally sample from the more than 70 y.o. and 30-39 y.o. target attributes, and east asian and middle eastern sensitive attributes. The choice of groups is random and the unlearning ratio is fixed to 0.5. Full results for unlearning multiple groups in grouprobust unlearning are reported in Tabs. 14 and 15. We notice overall the same trend as in Fig. 6. REWEIGHT contribution gets less important as the number of groups from which the forget set is sampled decreases, highlighted by unchanged UA, EO, and GA. The GA when using MIU, e.g., increases by nearly 5% in the 9 groups experiment (Tab. 14), while it remains unchanged in the 25 groups one (Tab. 15). Test accuracies are better preserved when unlearning 1 or 9 groups, 15 Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 66.2 67.3 66.8 63.7 65.9 68.4 66.9 64.0 66.2 68.4 67.4 0.7 0.1 0.1 0.3 0.8 0.5 0. 0.3 0.4 0.5 0.5 79.2 71.7 72.0 78.9 73.9 78.7 81. 72.7 80.1 79.2 82.3 2.5 0.8 1.7 3.5 3.9 0.8 0.2 0.7 1.5 1.0 1. 57.2 57.0 56.8 56.1 55.1 57.5 57.3 56.4 55.3 57.5 57.6 0.1 0.4 0.4 0.8 1.1 0.3 0. 0.6 0.4 0.4 0.3 MIA 0.7 1.0 0. 0.0 0.5 0.2 0.2 0.2 0.2 0.2 0.0 0.2 0.8 0. 0.0 0.0 0.2 0.2 0.2 0.2 0.2 0.0 EO 5.8 5.4 4.3 5.5 2.9 5.7 5.3 5.3 4.7 5.6 6. 0.2 1.1 0.6 2.6 1.1 0.2 0.6 1.2 0.9 1.1 0.7 GA 71.6 69.0 71.1 69.7 69.8 70.4 70.4 69.1 73.3 70.9 71.2 2.1 2.8 0.6 2.2 7.0 1.5 0.5 0.7 4.6 1.7 0.5 Avg. Gap - - - 97.3 97.8 97.9 97.8 98.6 97.1 97.8 97.6 0.7 0.9 0.5 0.5 0.3 0.3 0.6 0. Table 11. Group-robust machine unlearning in FairFace [24] with 0.1 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.1. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 65.6 66.8 66.7 64.0 66.3 66.9 66.7 64.4 65.1 66.7 64.7 0.7 0.4 0.2 0.3 0.4 0.1 0.2 0.1 0.4 0.1 0. 79.0 57.8 69.3 74.1 66.6 65.4 74.7 72.9 69.8 73.4 71. 1.2 3.3 0.5 1.2 3.4 1.6 1.2 2.1 6.3 2.2 2.8 57.2 56.5 56.7 56.9 55.9 56.7 57.2 56.0 54.8 57.2 57.1 0.4 0.1 0.2 0.5 0.6 0.7 0.7 0.9 0.6 0.5 0. MIA 0.2 0.9 0.7 0.2 0.3 1.0 0.3 0.3 0.3 0.7 0.3 0.1 0.2 0.5 0.1 0.1 0.0 0. 0.1 0.2 0.3 0.2 EO 5.4 9.2 5.6 6.1 9.0 9.9 6.0 6.1 6.6 6.2 5.8 1.7 0.9 1. 0.7 0.5 1.3 2.0 2.1 2.1 1.1 0.4 GA 71.2 58.7 69.6 69.4 60.3 61.3 66.1 67.0 63.7 70.2 70.3 2.4 3.0 0.7 0.7 2.4 2.5 4. 6.8 3.4 2.7 1.6 Avg. Gap - - - 98.3 97.1 97.0 98.1 97.3 97.2 98.7 98.7 0.2 1.1 0.5 0.4 0.3 0.4 0.7 0.8 Table 12. Group-robust machine unlearning in FairFace [24] with 0.5 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. while we notice drop (about 1.5%) in the 25 groups experiment. We argue that this decline is caused by the larger forget set, which reduces the number of available samples in the retain set. Overall, MIU approximates RETRAIN + REWEIGHT better than baselines, consistently achieving the best Avg. Gap. C.4. Extended ablation study Section 4.5 shows comprehensive ablation of MIUs components. However, Tab. 4 limits the analysis only to the UA, GA, and Avg. Gap to reduce space usage. Thus, Tab. 17 reports all metrics investigated for completeness. Although Tab. 4 metrics are limited, we chose subset that shows great variance along different components and is more interesting to evaluate. For instance, EO variations are more nuanced compared to GA (e.g., it drops by 4.5 in FairFace [24], while GA grows by 11.1). The MIA shows subtle oscillations in CelebA [31] and FairFace [24] while it drops when adding the calibration term in Waterbirds [42], getting closer to RETRAIN + REWEIGHT value (59.5% of MIU vs. 53.6% of RETRAIN + REWEIGHT). Finally, TA and RA are relatively stable across components, with RA showing negative trend (64.7% using all three components vs. 65.6% of PRETRAIN). Fig. 8, instead, reports the ablation study on parameter λ. We show results for λ {0, 1, 5, 10} and average each experiment over three different seeds. Overall, all three datasets benefit from the calibration term. However, while CelebA [31] and FairFace [24] achieve better results when λ = 1 (i.e., an Avg. Gap of 99.2 and 98.7), Waterbirds [42] minimizes the gap when λ = 10 (i.e., an Avg. Gap of 96.9). Nonetheless, results are generally stable, and when in doubt we suggest setting λ = 1 as it always shows an alignment improvement with RETRAIN + REWEIGHT. C.5. Additional Fairness Metrics To further study the methods fairness after unlearning. We investigate three additional fairness metrics alongside the Equalized Odds (EO) [16], namely, Demographic Parity [29], Equal Opportunity [16], and Worst Group Accuracy (WG) [30, 42]. To satisfy the Demographic Parity, the model probability of outputting positive prediction must 16 Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 66.0 67.3 67.1 64.5 65.7 60.2 68.2 64.0 65.5 61.2 64.7 0.0 0.6 0.4 0.2 0.5 1.0 0. 0.4 0.5 1.1 0.2 77.5 38.5 53.6 57.1 46.5 52.7 64. 74.5 66.1 65.5 67.1 2.1 1.8 1.2 2.1 6.2 4.4 2.5 3.0 3.7 3.5 1. 56.6 56.0 56.6 55.2 53.9 53.3 56.5 55.9 55.3 54.5 56.7 0.4 0.4 0.4 0.6 0.1 0.5 0. 0.4 0.2 0.3 0.2 MIA 0.2 2.7 1. 0.4 0.5 2.4 0.5 0.3 0.5 1.3 0.5 0.0 0.3 0. 0.1 0.1 0.7 0.1 0.2 0.3 0.1 0.1 EO GA Avg. Gap 5.5 23.1 11.9 1.1 1.5 1.0 13.0 15.3 15.6 10.4 0.7 1.0 1.4 0.7 5.6 7.2 9.5 8.8 0.6 1.2 0.9 0.2 69.0 37.1 53.9 51.0 42.8 48.7 56.8 69.8 62.8 64.4 63. 2.8 2.1 0.7 1.9 4.9 4.9 2.6 4.6 4.9 2.7 1.6 - - - 97.7 95.2 95.7 97.0 91.9 94.9 94.5 94.9 0.3 1.8 0.6 1.0 1.5 1.7 1.6 0. Table 13. Group-robust machine unlearning in FairFace [24] with 0.9 unlearning ratio. We build the forget set by sampling data points from single group. The unlearning ratio is set to 0.9. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 64.6 66.5 64.4 63.5 64.3 67.2 66.3 63.7 63.7 66.7 63.4 0.5 0.5 1.1 0.6 0.5 0.4 0.4 0.1 0.8 0.4 0. 81.5 60.1 72.1 69.9 64.6 74.3 74.2 75.2 74.4 80.5 73. 0.5 0.9 2.2 3.9 1.2 0.7 0.4 0.6 1.5 0.1 0.3 57.4 55.4 56.3 55.3 54.0 56.9 56.8 56.3 55.5 57.4 56.7 0.3 0.4 0.3 0.1 0.1 0.2 0.5 0.1 0.4 0.5 0. MIA 0.4 1.1 0.5 0.5 0.2 0.3 0.3 0.2 0.4 0.4 0.4 0.0 0.1 0.0 0.0 0.1 0.1 0. 0.1 0.0 0.1 0.1 EO 1.1 5.6 2.0 2.6 3.6 1.8 1.7 1.3 2.3 1.7 1.5 0.1 0.4 0. 1.0 0.3 0.7 0.4 0.3 0.3 0.3 0.5 GA 72.5 60.4 71.8 64.4 59.7 65.6 65.7 69.6 69.0 71.3 70.3 0.7 1.7 2.7 4.4 1.5 0.6 0. 1.2 1.6 0.4 1.0 Avg. Gap - - - 97.7 96.0 97.7 97.9 98.6 98.4 97.4 99.0 1.1 0.6 0.3 0.4 0.2 0.2 0.3 0.1 Table 14. Group-robust machine unlearning in FairFace [24] by sampling from 9 groups. We build the forget set by sampling data points from 9 groups. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. Figure 8. Ablating parameter λ. MIU Avg. Gap when varying parameter λ in CelebA [31], Waterbirds [42], and FairFace [24]. While λ = 1 is optimal in CelebA [31] and FairFace [24], Waterbirds [42] benefits from higher lambdas. be independent of the sensitive attribute. Therefore, we measure it as: DP = ( ˆY = 1 = 0) ( ˆY = 1 = 1). Similarly, to satisfy Equal Opportunity (EO), the model true positive rate must be independent of the sensitive attribute: EP = ( ˆY = 1 = 1, = 0) ( ˆY = 1 = 1, = 1). Instead, the Worst Group Accuracy measures the average accuracy of the worst group of the test set. Thus, the Worst Group Accuracy is computed as: WG = min , where 1 [(hφ fθ)(xi) = yi] gi te (cid:111) (cid:80)D i=1 (cid:110) 1 gi te gi Dgi te is the subset of images of group gi of the test set. The lower the first two metrics (DP and EP) the better the model fairness, while the higher the WG the higher the model robustness. Table 16 reports our evaluation using the additional metrics on CelebA [31], Waterbirds [42], and FairFace [24]. While plain model retraining generally shows performance degradation on all fairness metrics (e.g., +8.2 DP, +12.6 EP, +10.6 EO, and -9.9 WG, in CelebA [31]), using REWEIGHT recovers the original model fairness and overall robustness (i.e., -0.4 DP, -0.3 EP, -0.1 EO, and +1.2 WG, in CelebA [31]). Similarly, the proposed MIU preserves PRETRAIN robustness by approximating RETRAIN + REWEIGHT (i.e., -0.7 DP, -1.4 EP, -0.7 EO, and +2.3 WG, in CelebA [31]). D. Limitations and negative societal impacts Limitations. One limitation of our work is assuming knowledge of the data point group. Group annotations are difficult to obtain. Therefore, natural follow-up for this 17 Method PRETRAIN RETRAIN RETRAIN L1-SPARSE [23] SALUN [13] SCRUB [28] MIU L1-SPARSE [23] SALUN [13] SCRUB [28] MIU REWEIGHT RA UA TA 65.1 66.5 66.1 65.5 64.5 66.7 66.8 64.3 62.5 65.3 66.2 0.3 1.0 0.6 0.8 0.5 0.6 0.2 0.7 1.6 0.3 1. 70.6 55.5 60.5 61.8 60.0 62.4 64.8 66.7 64.6 71.4 63. 0.5 2.1 0.7 0.3 3.8 1.0 0.5 0.4 2.0 0.6 1.9 56.9 54.8 55.5 55.5 55.2 55.4 56.4 55.8 54.6 57.0 54.3 0.3 0.7 0.4 0.5 1.0 0.3 0.5 0.4 0.8 0.3 0. MIA 0.3 0.8 0.7 0.5 0.5 0.3 0.3 0.4 0.3 0.2 0.9 0.1 0.1 0.1 0.1 0.1 0.1 0. 0.1 0.1 0.0 0.3 EO 1.6 1.9 2.3 1.6 1.1 1.7 1.7 2.1 2.2 1.9 3.5 0.6 0.3 0. 0.8 0.4 0.9 0.7 0.8 0.4 0.9 0.9 GA 62.2 56.0 60.5 57.0 57.0 55.2 57.5 60.8 60.1 63.7 57.6 1.4 2.1 0.5 0.9 3.8 0.6 1. 1.0 2.0 0.7 3.3 Avg. Gap - - - 98.7 98.0 98.4 98.3 98.3 98.0 97.1 98.3 0.2 0.7 0.2 0.2 0.3 0.3 0.1 0.1 Table 15. Group-robust machine unlearning in FairFace [24] by sampling from 25 groups. We build the forget set by sampling data points from 25 groups. The unlearning ratio is set to 0.5. We compare MIU against L1-SPARSE [23], SALUN [13], and SCRUB [28]. The Avg. Gap is computed against RETRAIN + REWEIGHT. in the forget set. Thus, MIU and REWEIGHT counteract biases introduced by unlearning, mitigating the negative societal impacts of its misuse. Method DP EP EO WG CelebA [31] PRETRAIN RETRAIN RETRAIN+RW MIU 44.1 52.3 (8.2) 43.7 (-0.4) 43.4 (-0.7) 21.3 33.9 (12.6) 21.0 (-0.3) 19.9 (-1.4) 20.9 31.5 (10.6) 20.8 (-0.1) 20.2 (-0.7) 65.5 55.6 (-9.9) 66.7 (1.2) 67.8 (2.3) Waterbirds [42] PRETRAIN RETRAIN RETRAIN+RW MIU 20.6 23.2 (2.6) 21.5 (0.9) 22.9 (2.3) 36.1 43.4 (7.3) 40.6 (4.5) 38.1 (2.0) 26.1 30.4 (4.3) 28.3 (2.2) 28.3 (2.2) 56.6 49.4 (-7.2) 51.6 (-5.0) 53.7 (-2.9) PRETRAIN RETRAIN RETRAIN+RW MIU FairFace [24] 2.0 5.3 (3.3) 3.8 (1.8) 1.1 (-0.9) 7.6 17.9 (10.3) 7.6 (0.0) 8.0 (0.4) 5.4 9.2 (3.8) 5.6 (0.2) 5.8 (0.4) 9.4 8.3 (-1.1) 6.1 (-3.3) 16.3 (6.9) Table 16. Additional Fairness Metrics. Fairness metrics are computed on each of the three investigated datasets (using the same splits as Tabs. 1 to 3). From left to right, we report the method, DP, EP, EO, and WG. MIU + REWEIGHT is highlighted. work is group-agnostic methodology that preserves model robustness like REWEIGHT. Furthermore, our evaluation is limited to the image classification task. Using the proposed techniques and baselines in different scenarios might be non-trivial and there is no guarantee that unlearning effectiveness and accuracy will be preserved. Nevertheless, our work is the first step toward quantifying and mitigating the accuracy degradation caused by non-uniformly sampled forget sets. Negative societal impacts. Machine unlearning is designed to remove user data from trained model. While its primary goal is privacy preservation, it can also be misused for malicious purposes. Targeted unlearning of specific groups may lead to biased models with harmful consequences. However, the proposed group-robust machine unlearning seeks to minimize performance degradation for dominant groups 18 Dataset Eq. (5) Eq. (3) Eq. (4) RW RA UA TA MIA EO GA Avg. Gap CelebA [31] Waterbirds [42] FairFace [24] 53.6 55.3 41.7 68. 82.6 0.7 82.6 0.8 34.2 62.1 82.5 0.3 0.0 0.0 7.1 0.1 85.1 84.8 63.1 84.2 100.0 100.0 93.0 99.9 0.0 47.6 0.0 53.6 16.7 3.3 54.8 0.1 85.0 7.3 86.1 7.7 80.3 9.4 14.7 85.8 65.2 66.7 59.1 64.7 0.1 0.2 3.1 0.3 63.1 74.7 87.1 71.6 1.6 1.2 6.8 2.8 56.9 57.2 54.5 57.1 0.1 0.2 7.0 0.1 0.6 1.0 2.9 0. 0.3 0.7 1.8 0.3 0.2 0.3 0.0 0.1 73.8 58.3 64.3 59. 0.0 0.1 0.0 0.0 28.6 27.4 10.4 20.2 32.3 3.4 28.3 8.9 12.7 35.8 12.1 28.3 0.1 0.4 5.4 0.6 0.8 1.7 7.8 2.9 0.3 0.3 0.0 0.3 0.0 0.0 0.0 0.2 10.3 6.0 3.1 5. 0.8 2.0 0.5 0.4 54.0 55.9 42.3 69.0 1.0 1.0 34.1 1.2 51.1 53.8 16.8 53. 59.2 66.1 81.1 70.3 0.6 2.6 8.9 3.8 1.9 4.4 6.2 1.6 94.1 94.9 78.9 99. 92.5 95.3 81.9 96.9 96.1 98.1 93.0 98.7 0.6 0.7 8.8 0. 4.6 0.7 5.5 1.6 0.8 0.4 3.1 0.8 Table 17. MIU ablations. We compute MIU ablations on each of the three investigated datasets. From left to right, we report the investigated dataset, the retaining term, the unlearning term, the calibration term, and REWEIGHT. We measure performance using all metrics. The configuration that corresponds to MIU + REWEIGHT is highlighted."
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler",
        "Inria Grenoble, Univ. Grenoble Alpes",
        "LTCI, Telecom Paris, Institut Polytechnique de Paris",
        "University of Trento"
    ]
}