{
    "paper_title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
    "authors": [
        "Hua Farn",
        "Hsuan Su",
        "Shachi H Kumar",
        "Saurav Sahay",
        "Shang-Tse Chen",
        "Hung-yi Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuning large language models (LLMs) for downstream tasks is a widely adopted approach, but it often leads to safety degradation in safety-aligned LLMs. Currently, many solutions address this issue by incorporating additional safety data, which can be impractical in many cases. In this paper, we address the question: How can we improve downstream task performance while preserving safety in LLMs without relying on additional safety data? We propose a simple and effective method that maintains the inherent safety of LLMs while enhancing their downstream task performance: merging the weights of pre- and post-fine-tuned safety-aligned models. Experimental results across various downstream tasks, models, and merging methods demonstrate that this approach effectively mitigates safety degradation while improving downstream task performance, offering a practical solution for adapting safety-aligned LLMs."
        },
        {
            "title": "Start",
            "content": "Safeguard Fine-Tuned LLMs Through Preand Post-Tuning Model Merging Hua Farn Saurav Sahay Hsuan Su Shang-Tse Chen Shachi Kumar Hung-yi Lee National Taiwan University Intel Lab alhena.farn@gmail.com 4 2 0 2 7 2 ] . [ 1 2 1 5 9 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Fine-tuning large language models (LLMs) for downstream tasks is widely adopted approach, but it often leads to safety degradation in safetyaligned LLMs. Currently, many solutions address this issue by incorporating additional safety data, which can be impractical in many cases. In this paper, we address the question: How can we improve downstream task performance while preserving safety in LLMs without relying on additional safety data? We propose simple and effective method that maintains the inherent safety of LLMs while enhancing their downstream task performance: merging the weights of preand post-fine-tuned safetyaligned models. Experimental results across various downstream tasks, models, and merging methods demonstrate that this approach effectively mitigates safety degradation while improving downstream task performance, offering practical solution for adapting safety-aligned LLMs."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement and increasing accessibility of Large Language Models (LLMs) necessitate critical focus on aligning these technologies with human values, cultural norms, and trustworthiness (Huang et al., 2023). To address these challenges, researchers and developers have introduced safety techniques such as preference tuning (Ouyang et al., 2022; Rafailov et al., 2023; Grattafiori et al., 2024; OpenAI et al., 2024), aimed at preventing LLMs from generating harmful or inappropriate content. Consequently, an increasing number of applications leverage safety-aligned modelsreferred to as base models in this paperthat are further customized for downstream tasks via Supervised FineTuning (SFT) (Chung et al., 2024). However, recent studies (Yang et al., 2023; Qi et al., 2024; Zhan et al., 2024) highlight critical challenge: fine-tuning base models can degrade Figure 1: Beyond standard SFT for downstream task adaptation, our proposed method effectively mitigates safety degradation by combining the base and the finetuned model while simultaneously enhancing downstream task performance. their safety, even when using benign datasets. To address this issue, mainstream approaches often incorporate additional safety data during fine-tuning (Qi et al., 2024; Bianchi et al., 2024). Additionally, some methods that utilize model merging techniques (Hazra et al., 2024; Hsu et al., 2024) rely on auxiliary safety or harmful datasets in their complex re-alignment processes. These approaches are often constrained by the scarcity of safety data and can be resource-intensive, requiring datasets with labeled safe examples in specific domains. Inspired by Wortsman et al. (2022b), we propose simple yet effective method to improve downstream task performance while mitigating safety degradation. As shown in Figure 1, our approach consists of two steps: first, we fine-tune the base model on the downstream task; second, we merge the base model with the fine-tuned model. We evaluate this approach across various models, downstream tasks, and merging techniques. The experimental results demonstrate that our method enhances downstream task performance while significantly preserving model safety, offering an easy and robust solution for fine-tuning safety-aligned LLMs. Our key contributions are as follows: Our key contributions are as follows: We propose simple and effective method that improves downstream task performance while reducing the Attack Success Rate (ASR) by up to 30%. We conduct comprehensive evaluation using three models, four downstream tasks, four merging methods, and two safety benchmarks, demonstrating the robustness of our method in preserving model safety."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Jailbreaking LLMs through Fine-Tuning Despite significant efforts to ensure the safety of Large Language Models (LLMs) (Ouyang et al., 2022; Rafailov et al., 2023; Grattafiori et al., 2024; OpenAI et al., 2024), adversarial attacks known as \"jailbreaking\" remain major threat (Xu et al., 2024; Yi et al., 2024). Additionally, studies (Yang et al., 2023; Qi et al., 2024; Zhan et al., 2024) have shown that fine-tuning safety-aligned LLMs can degrade their safety, enabling these models to generate harmful responses. common mitigation strategy involves incorporating additional safety data during fine-tuning (Qi et al., 2024; Bianchi et al., 2024). Other approaches use complex training or post-training techniques (Huang et al., 2024b; Zong et al., 2024; Huang et al., 2024c). However, these methods are often limited by data availability and high computational costs. To overcome these challenges, we propose simple yet effective method that mitigates safety degradation by merging the base model with the fine-tuned model, without requiring additional safety data or significant computational resources."
        },
        {
            "title": "2.2 Model Merging",
            "content": "Model merging combines multiple models into single unified model. straightforward method averages the weights of different models (Wortsman et al., 2022a). Advanced techniques such as SLERP (White, 2017), DARE (Yu et al., 2024), and Model Stock (Jang et al., 2024) have been developed to enhance the merging process. Another approach leverages task vectors (Ilharco et al., 2023), enabling flexibility and composability through arithmetic operations for various applications (Huang et al., 2024a; Su et al., 2024). Some studies use task vectors to improve LLM safety by fine-tuning additional safe or harmful models to extract safety vector (Bhardwaj et al., 2024; Hazra et al., 2024; Hsu et al., 2024) and determining merging ratios based on safety data (Hammoud et al., 2024). In contrast, our method utilizes the inherent safety of the base model, significantly reducing the need for additional safety data."
        },
        {
            "title": "3 Methodology",
            "content": "To maintain the safety of safety-aligned LLMs after fine-tuning without relying on additional safety data, we propose two-step approach. First, we fine-tune the base model on target downstream task. Second, we merge the base model with the fine-tuned model by interpolating their weights. This merging step effectively mitigates safety degradation introduced during fine-tuning by leveraging the original safety-aligned LLMs. Step 1: Fine-Tuning the Large Language Model We fine-tune the base model on downstream tasks. For each task t, given an instruction xt and the corresponding response yt, we optimize the model using the following loss function: LF = log fθ(ytxt) (1) where represents the language model parameterized by θ. Step 2: Merging the fine-tuned LLM with the base model After fine-tuning, we merge the base model parameters (θbase) with those of the finetuned model (θt) using the interpolation formula: θmerged = (1 λ)θbase + λθt (2) Here, θmerged represents the parameters of the merged model, and λ [0, 1] determines the contribution of the fine-tuned model. The ratios are normalized to ensure they sum to 1. In our experiments, we explore various merging techniques based on Equation 2, including methods that adjust model parameters before merging and strategies to identify the optimal λ."
        },
        {
            "title": "4 Experimental Setups",
            "content": "In this section, we describe the experimental setup to address the following research questions: Q1: Can merging fine-tuned models with their base models prevent safety degradation after fine-tuning on different downstream tasks? Q2: How do different merging methods perform? Q3: What is the impact of λ on downstream task performance and safety? Q4: Does model merging work on multiple models? 4.1 Downstream Tasks We evaluate our proposed method on four downstream tasks: reasoning, medical assistance, code generation, and tool usage proficiency. Reasoning is improved with Chain-of-Thought data from the Flan V2 dataset (Longpre et al., 2023) and is evaluated on the Big Bench Hard (BBH) dataset (Suzgun et al., 2023). Medical assistance uses patient-doctor interactions from the ChatDoctor dataset (Li et al., 2023). Code generation uses the MagiCoder dataset (Wei et al., 2024) and is assessed via the HumanEval dataset (Chen et al., 2021). Tool usage proficiency uses the OpenFunctions dataset (Patil et al., 2023) for enhancing API call generation. For medical assistance and tool usage proficiency, response similarity to reference answers is measured using BERTScore1 (Zhang* et al., 2020). See Appendix for more details on the downstream tasks."
        },
        {
            "title": "4.2 Safety Evaluation",
            "content": "Safety is assessed using harmful instructions from the AdvBench dataset (Chen et al., 2022) and the HEx-PHI dataset (Qi et al., 2024). AdvBench contains 520 harmful instructions and HEx-PHI comprises 330 harmful instructions. We employ the WildGuard safety classification model (Han et al., 2024), which demonstrates performance comparable to GPT-4 (OpenAI et al., 2024). The Attack Success Rate (ASR) is used as the metric for both datasets. Further details on safety evaluation are provided in Appendix B."
        },
        {
            "title": "4.3 Large Language Models",
            "content": "Our experiments involve two LLM families: Llama-3 (Grattafiori et al., 2024) and Gemma (Team et al., 2024). We use their instruct-tuned versions, which are aligned with human preferences. Within the Llama-3 family, we evaluate Llama3-8B-Instruct and Llama-3.1-8B-Instruct. For the Gemma family, we use the Gemma-2B-it model. Each model is fine-tuned with three different seeds using LoRA (Hu et al., 2022) for each downstream 1Embedding extraction from the 40th layer of microsoft/deberta-xlarge-mnli. task. Additional details on training and inference setups are provided in Appendix C. 4.4 Merging Methods We evaluate three merging methods: Linear Merging, where models are directly combined by interpolating their weights following Equation 2, Model Stock (Jang et al., 2024), SLERP (White, 2017), and DARE (Yu et al., 2024). For Linear Merging, SLERP, and DARE, each fine-tuned model is merged with the base model using λ selected based on validation set performance on the downstream tasks. Specifically, in the Model Stock approach, three models fine-tuned with different seeds are merged with the base model. More details about the methods and implementations are provided in Appendix D."
        },
        {
            "title": "5 Results and Discussion",
            "content": "In this section, we present the results addressing our research questions. From Q1 through Q4, we focus primarily on the Llama-3-8B-Instruct model, other models are discussed in the context of Q5. Task Reasoning Medical Assistance Code Generation Tool Using Proficiency Method Base FT + Linear Merging + Model Stock Base FT + Linear Merging + Model Stock Base FT + Linear Merging + Model Stock Base FT + Linear Merging + Model Stock Performance AdvBench HEx-PHI 0.61 0.68 0.69 0. 0.52 0.57 0.57 0.55 0.72 0.74 0.75 0.72 0.93 0.96 0.96 0. 0 4.25 0.64 0.97 0 30.06 0.32 0.58 0 2.25 0.71 0. 0 0.83 0.77 0.77 1.22 14.80 6.38 4.57 1.22 38.85 4.06 2. 1.22 11.67 4.27 6.99 1.22 3.45 2.44 2.15 Table 1: Results across four tasks. The table reports the average scores of three models, which are fine-tuned with different seeds, for FT and Linear Merging. Performance indicates task-specific effectiveness measured by the respective metrics. Q1: Can merging fine-tuned models with their base models prevent safety degradation after fine-tuning on different downstream tasks? The results for Llama-3-8B-Instruct are presented in Table 1. While the fine-tuned model (FT) demonstrates improved performance on the downstream task, it significantly compromises safety. However, merging the fine-tuned model with the base model substantially enhances safety across all downstream tasks, as highlighted in the green cells in Table 1 and subsequent tables. Although the merged model does not fully match the base model in safety, it substantially reduces the ASR on both the AdvBench and HEx-PHI safety datasets. Notably, on the medical assistance task, the ASR is reduced by over 30%, bringing safety closer to the base models level. This result underscores the base models stronger inherent safety features. Additionally, Linear Merging improves downstream task performance across all downstream tasks, consistent with the findings of Wortsman et al. (2022b). Given the pronounced effect on the medical assistance task, subsequent experiments for questions Q2 through Q4 primarily focus on this task. Method Base FT + Linear Merging + Model Stock + SLERP + DARE Performance AdvBench HEx-PHI 0. 0.57 0.57 0.55 0.58 0.58 0 30.06 0.32 0.58 5.76 5.61 1.22 38.85 4.06 2.13 24.27 23.41 Table 2: Results of various merging methods on the medical assistance task. The results show that different merging methods are beneficial for the safety of the finetuned model while also enhancing task performance. Q2: How do different merging methods perform? The performance of alternative merging methods is shown in Table 2. Both SLERP and DARE effectively reduce ASR on AdvBench and HEx-PHI. The results emphasize the strong performance of Linear Merging, demonstrating that simpler approaches can effectively mitigate safety degradation. This finding suggests that Linear Merging can be viable and computationally efficient alternative for practical applications. LLM Llama-3.1-8B-Instruct Gemma-2B-it Method Base FT + Linear Merging + Model Stock Base FT + Linear Merging + Model Stock Performance AdvBench HEx-PHI 0.53 0.57 0.58 0. 0.52 0.55 0.57 0.55 2.70 4.16 0.32 2.50 3.68 4.75 4.55 4. 5.85 9.79 1.92 6.17 3.04 7.03 4.60 3.05 Table 3: Results of Llama-3.1-8B-Instruct and Gemma-2B-it The results show that our method can generalize to more models, restoring safety while maintaining task performance. Q3: What is the impact of λ on downstream task performance and safety? Figure 2 illustrates the impact of λ on downstream task performance and Figure 2: Impact of λ on downstream task performance and safety. These plots illustrate how varying λ affects the performance of the Medical Assistance task (left y-axis) and the ASR for the HEx-PHI dataset (right y-axis) across three merging methods. Results for AdvBench are shown in Figure 3 in the Appendix. model safety. As λ increases, downstream task performance improves, but the ASR also rises, indicating trade-off between performance and safety. However, the ASR remains lower than that of the SFT model. The optimal λ is observed around 0.5 0.6, suggesting that more evenly combining the weights of the two models yields better downstream task performance while maintaining safety. Additionally, the rate of ASR increase for Linear Merging, as shown in Figures 2 and 3, is slower than that of SLERP and DARE, further demonstrating that Linear Merging is more practical method in real-world applications. Q4: Does model merging work on multiple models? To evaluate the generalizability of model merging, we test Llama-3.1-8B-Instruct and Gemma-2B-it, with results shown in the Table 3. For these models, safety degradation on both benchmarks after fine-tuning is relatively mild. Nevertheless, model merging effectively restores safety without significantly compromising downstream task performance, demonstrating the methods applicability across different LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose two-step method to address safety degradation in aligned LLMs by merging the weights of preand post-fine-tuned models. This approach allows aligned LLMs to acquire new downstream task capabilities while preserving their original safety features, without the need for additional safety data. Experiments demonstrate its effectiveness across diverse downstream tasks, models, and merging techniques, providing simple yet robust solution for adapting aligned LLMs."
        },
        {
            "title": "7 Limitations",
            "content": "Task and Model Selection In our experiments, we evaluate only on benign data from the reasoning, medical assistance, code generation, and toolusing proficiency tasks, leaving out other areas such as law, finance, and others. While Section 5 examines the efficacy of our methods on these four downstream tasks, the performance of aligned models fine-tuned on other domains, languages, or datasets contaminated with harmful examples remains an area for future investigation. Furthermore, our methods are tested only on models with sizes of 2B and 8B from two model families. The efficacy of the approach on larger models and different model families is uncertain and warrants further exploration. Safety Classifier for Safety Evaluation Due to the higher cost of methods like LLM-as-Judge (Chiang and Lee, 2023; Liu et al., 2023), which involve using LLMs such as GPT-4 (OpenAI et al., 2024) to evaluate the safety of model responses, we utilize an alternative approach by employing classifier, WildGuard (Han et al., 2024), to classify model responses as safe or unsafe. While this reduces costs, WildGuard has limitations: it struggles with more complex instructions, may produce false positives or false negatives, and provides less detailed evaluations. Consequently, we are unable to analyze which types of harmful instructions the models are more vulnerable to or which types are most effectively defended against after applying our method. More detailed safety analysis is left for future work."
        },
        {
            "title": "8 Ethics Statement",
            "content": "While our method effectively addresses safety degradation in aligned LLMs without requiring additional safety data, our approach relies on merging preand post-fine-tuned models to preserve safety, which may inadvertently inherit any latent biases or unsafe behaviors that are still presented in the base model. Further in vestigation is needed to explore the impact of these inherited biases in the base model."
        },
        {
            "title": "References",
            "content": "Rishabh Bhardwaj, Duc Anh Do, and Soujanya Poria. 2024. Language models are Homer simpson! safety re-alignment of fine-tuned language models through task arithmetic. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14138 14149, Bangkok, Thailand. Association for Computational Linguistics. Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou. 2024. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. In The Twelfth International Conference on Learning Representations. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Yangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, and Maosong Sun. 2022. Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial NLP. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1122211237, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1560715631, Toronto, Canada. Association for Computational Linguistics. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2024. Scaling instruction-finetuned language models. J. Mach. Learn. Res., 25:70:1 70:53. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. 2024. Arcees MergeKit: toolkit for merging large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 477485, Miami, Florida, US. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Hasan Abed Al Kader Hammoud, Umberto Michieli, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem, and Mete Ozay. 2024. Model merging and safety alignment: One bad model spoils the bunch. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1303313046, Miami, Florida, USA. Association for Computational Linguistics. Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. 2024. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of LLMs. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Rima Hazra, Sayan Layek, Somnath Banerjee, and Soujanya Poria. 2024. Safety arithmetic: framework for test-time safety alignment of language models by steering parameters and activations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2175921776, Miami, Florida, USA. Association for Computational Linguistics. Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. 2024. Safe loRA: The silver lining of reducing safety risks when In The Thirtyfinetuning large language models. eighth Annual Conference on Neural Information Processing Systems. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Shih-Cheng Huang, Pin-Zu Li, Yu-Chi Hsu, KuangMing Chen, Yu Tung Lin, Shih-Kai Hsiao, Richard Tzong-Han Tsai, and Hung yi Lee. 2024a. Chat vector: simple approach to equip llms with instruction following and model alignment in new languages. Preprint, arXiv:2310.04799. Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. 2024b. Lisa: Lazy safety alignment for large language models against harmful finetuning attack. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Tiansheng Huang, Sihao Hu, and Ling Liu. 2024c. Vaccine: Perturbation-aware alignment for large language models against harmful fine-tuning attack. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, Kaiwen Cai, Yanghao Zhang, Sihao Wu, Peipei Xu, Dengyu Wu, Andre Freitas, and Mustafa A. Mustafa. 2023. survey of safety and trustworthiness of large language models through the lens of verification and validation. Preprint, arXiv:2305.11391. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations. Dong-Hwan Jang, Sangdoo Yun, and Dongyoon Han. 2024. Model stock: All we need is just few finetuned models. In Proceedings of the European Conference on Computer Vision. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. 2023. Chatdoctor: medical chat model fine-tuned on large language model meta-ai (llama) using medical domain knowledge. Preprint, arXiv:2303.14070. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. Preprint, arXiv:2301.13688. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arXiv:2203.02155. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2024. Finetuning aligned language models compromises safety, even when users do not intend to! In The Twelfth International Conference on Learning Representations. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems. Hsuan Su, Hua Farn, Fan-Yun Sun, Shang-Tse Chen, and Hung-yi Lee. 2024. Task arithmetic can mitigate synthetic-to-real gap in automatic speech recognition. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 89058915, Miami, Florida, USA. Association for Computational Linguistics. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, Toronto, Canada. Association for Computational Linguistics. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex CastroRos, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. Preprint, arXiv:2403.08295. ing, volume 235 of Proceedings of Machine Learning Research, pages 5263252657. PMLR. Tom White. 2017. Sampling generative networks. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022a. Model soups: averaging weights of multiple finetuned models improves accuracy without increasing inference time. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2396523998. PMLR. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. 2022b. Robust fine-tuning of zero-shot models. Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. 2024. comprehensive study of jailbreak attack versus defense for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 74327449, Bangkok, Thailand. Association for Computational Linguistics. Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. 2023. Shadow alignment: The ease of subverting safely-aligned language models. Preprint, arXiv:2310.02949. Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. 2024. Jailbreak attacks and defenses against large language models: survey. Preprint, arXiv:2407.04295. Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. 2024. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning. Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. 2024. Removing RLHF protections in GPT-4 via fine-tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 681687, Mexico City, Mexico. Association for Computational Linguistics. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2024. Magicoder: Empowering code generation with OSS-instruct. In Proceedings of the 41st International Conference on Machine LearnYaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Hospedales Timothy. 2024. Safety finetuning at (almost) no cost: baseline for vision large language models. arXiv preprint arXiv:2402.02207. Domain-Specific Tasks Detail Reasoning We randomly select 10,000 zero-shot chain-of-thought instructions from the Flan V2 dataset then split them into training set and validation set with ratio 9 : 1. Performance is assessed using the BBH dataset, with results reported as the average 3-shot accuracy across all BBH tasks. We use lm-evaluation-harness (Gao et al., 2024) as our code base. Medical Assistance We randomly select 10,000 real patient-doctor conversations from the ChatDoctor dataset (Li et al., 2023) then split them into training set and validation set with ratio 9 : 1. Model performance is evaluated on 1,000 unseen patient queries using BERTScore to calculat similarity of reference responses and models responses, we report the F1 score in our results. Code Generation We select 10,000 samples from the MagiCoder dataset (Wei et al., 2024) to improve code generation capabilities. Specifically, we uniformly sampled from each coding languages. We report Pass@10 in our experiment results. Tool Using Proficiency Due the the smaller dataset size of OpenFunctions (Patil et al., 2023), we split its full training set into two subsets with ratio 9 : 1 for training and validating to enhance API call generation capabilities. The model is evaluated on the full OpenFunctions test set, with performance measured using BERTScore to calculate similarity of reference responses and models responses, and we report the F1 score in our results."
        },
        {
            "title": "B Safety Dataset and Classifier",
            "content": "B.1 Safety Dataset AdvBench (Chen et al., 2022) is set of 520 harmful behaviors formulated as instructions. The goal of these instructions is to identify single attack string that causes the model to generate responses compliant with the instruction across as many harmful behaviors as possible. HEx-PHI (Qi et al., 2024) is dataset that contains 330 harmful instructions for evaluating the harmfulness of LLMs. It comprises 30 examples for each of 11 prohibited categories, such as Illegal Activity, Child Abuse Content, Hate/Harass/Violence, and so on. B.2 Safety Classifier WildGuard (Han et al., 2024) is an open, one-stop moderation model that achieves three goals: 1) Detection of harm in user prompts, 2) Detection of harm in LLM responses, and 3) Refusal evaluation of LLM responses. Comprehensive evaluations on WildGuardTest (Han et al., 2024) and ten existing public benchmarks that are shown in Han et al. (2024) demonstrate that WildGuard outperforms the strongest existing open-source baselines in F1 scores and matches GPT-4 across various tasks. When evaluating the responses generated by LLMs to harmful instructions, we apply the recommended settings, such as the instruction format and decoding strategy of WildGuard2. After evaluation, WildGuard outputs the response in the following format: Harmful request: yes/no Response refusal: yes/no Harmful response: yes/no We then parse the result of the harmful response. If the result cannot be parsed, we count it as miss. Our final ASR is calculated as follows: ASR = Ryes Rtotal Rmiss where Ryes is the number of harmful responses classified as \"yes\", Rtotal is the total number of responses, and Rmiss is the number of responses that failed to be parsed. In our experiments, Rmiss is usually less than 5 for all tested models across both safety datasets."
        },
        {
            "title": "C Experimental Detail",
            "content": "C.1 Prompt Template For base models, we directly apply their own prompt templates during the training and inference phases. For fine-tuned models, we apply the prompt templates of their respective base models. For the Llama-3 family, we use the following prompt template with system prompt: You are 2https://huggingface.co/allenai/wildguard helpful assistant. for the tasks of reasoning, code generation, and tool usage proficiency: <begin_of_text><start_header_id>system<end_header_id> You are helpful assistant.<eot_id> <start_header_id>user<end_header_id> {Instruction}<eot_id> <start_header_id>assistant<end_header_id> {Response} For the medical assistance task, we use another prompt provided in the ChatDoctor dataset (Li et al., 2023) as the system prompt. Hence, the prompt is as follows: stated otherwise, we report model training after 500 steps for reasoning, medical assistance, and code generation, and after 200 steps for tool usage proficiency due to the smaller size of the OpenFunctions training set. C.3 Inference We use greedy decoding to ensure result consistency, except for the HumanEval benchmark. For HumanEval, we apply sampling-based decoding with temperature of 0.6, top_p of 0.9, top_k of 50, and repetition penalty of 1.2. To accelerate the inference process, we utilize the VLLM engine for model inference. <begin_of_text><start_header_id>system<end_header_id>"
        },
        {
            "title": "D Model Merging",
            "content": "If you are doctor, please answer the medical questions based on the patient's description.<eot_id> D.1 Merging Methods <start_header_id>user<end_header_id> {Instruction}<eot_id> <start_header_id>assistant<end_header_id> {Response} The prompt for Gemma-2B-it for the tasks of reasoning, code generation, and tool usage proficiency is shown below: <bos><start_of_turn>user You are helpful assistant.{Instruction}<end_of_turn> <start_of_turn>model {Response}"
        },
        {
            "title": "The prompt for the medical assistance task is as",
            "content": "follows: <bos><start_of_turn>user If you are doctor, please answer the medical questions based on the patient's description. {Instruction}<end_of_turn> <start_of_turn>model {Response} C.2 Fine-tuning For all tasks, we fine-tune three model instances using different random seeds: 42, 1024, and 48763. We employ LoRA with = 8 and α = 16 for all linear layers, utilizing the AdamW optimizer with learning rate of 1 104 and cosine learning rate scheduler. We use batch size of 8 and train for 3 epochs. All models are trained on either an RTX A6000 GPU or an RTX 6000 Ada Generation GPU using LLaMA-Factory (Zheng et al., 2024) as the codebase. Although we initially fine-tuned each task for 3 epochs, we observed stronger model performance at an earlier stage. Consequently, unless explicitly Linear Merging Linear Merging involves directly combining the weights of the base model and the fine-tuned model by interpolating their parameters. Specifically, the weights of the merged model are calculated as weighted average of the base and fine-tuned models weights, following Equation 2. This method is straightforward and computationally efficient, making it popular choice for basic model integration. SLERP Spherical Linear Interpolation (SLERP) (White, 2017) is an advanced merging technique that interpolates between model weights on hypersphere, ensuring smoother and more natural transition between the models. Unlike Linear Merging, SLERP accounts for the angular relationship between weight vectors, which aim to better preserve the base models features while effectively integrating the fine-tuned models task-specific enhancements. DARE Drop and Rescale (DARE) (Yu et al., 2024) is method used to prepare models for merging techniques such as Linear Merging. It operates by randomly dropping parameters according to specified drop rate and rescaling the remaining parameters. This process helps reduce the number of redundant and potentially interfering parameters among multiple models. In our experiments, we apply DARE only to the fine-tuned models and then use Equation 2 to merge them with the base model. Model Stock Model Stock (Jang et al., 2024) utilizes the geometric properties of the weights of fine-tuned models. It determines the optimal merging ratio for each model by minimizing the distance between the merged weights and the center of the weights of the fine-tuned models and the base model. However, since this method requires at least two fine-tuned models and base model, we choose to merge three models with the base model in our experiments. Therefore, when implementing Model Stock, we first uniformly average the weights of the three fine-tuned models, denoted as θm = θ42 + θ1024 + θ48763 , where θ42, θ1024, θ48763 represent the model trained with different seed, then we merge this average with the base model using the optimal ratio λoptimal determined by Model Stock. The weight of the final merged model is expressed as: θmerged = (1 λoptimal)θbase + λoptimalθm. D.2 Model Merging Implementation For Linear Merging, we determine the interpolation weight λ by testing values in the set {0.2, 0.4, 0.6, 0.8}. For SLERP and DARE, we test the interpolation weight λ in the same set, {0.2, 0.4, 0.6, 0.8}, but do not experiment with the threshold for the dot product in the SLERP algorithm. Regarding Model Stock, since it can automatically approximate the interpolation weight based on the geometric nature of the training trajectory, no hyperparameters need to be specified. We use MergeKit (Goddard et al., 2024) as our codebase. Impact of λ for Safety We also illustrate the impact of λ on AdvBench in Figure 3. Similar to the results for HEx-PHI, the ASR increases as λ increases, but it remains better than the results of the SFT model. Figure 3: Impact of λ on Task Performance and Safety. This figure illustrates how varying λ affects the ASR for the AdvBench dataset across different merging methods."
        }
    ],
    "affiliations": [
        "Intel Lab",
        "National Taiwan University"
    ]
}