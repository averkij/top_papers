{
    "paper_title": "Revisiting the Shape Convention of Transformer Language Models",
    "authors": [
        "Feng-Ting Liao",
        "Meng-Hsi Chen",
        "Guan-Ting Yi",
        "Da-shan Shiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by a feed-forward network (FFN) with a narrow-wide-narrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual wide-narrow-wide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrow-wide-narrow design. To study this, we develop a Transformer variant that replaces the conventional FFN with a deeper hourglass-shaped FFN, comprising a stack of hourglass sub-MLPs connected by residual pathways. We posit that a deeper but lighter hourglass FFN can serve as a competitive alternative to the conventional FFN, and that parameters saved by using a lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt a rethinking of the narrow-wide-narrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models."
        },
        {
            "title": "Start",
            "content": "Feng-Ting Liao * 1 Meng-Hsi Chen * 1 Guan-Ting Yi * 1 2 Da-shan Shiu 1 6 2 0 2 6 ] . [ 1 1 7 4 6 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Dense Transformer language models have largely adhered to one consistent architectural shape: each layer consists of an attention module followed by feed-forward network (FFN) with narrowwidenarrow MLP, allocating most parameters to the MLP at expansion ratios between 2 and 4. Motivated by recent results that residual widenarrowwide (hourglass) MLPs offer superior function approximation capabilities, we revisit the long-standing MLP shape convention in Transformer, challenging the necessity of the narrowwidenarrow design. To study this, we develop Transformer variant that replaces the conventional FFN with deeper hourglass-shaped FFN, comprising stack of hourglass sub-MLPs connected by residual pathways. We posit that deeper but lighter hourglass FFN can serve as competitive alternative to the conventional FFN, and that parameters saved by using lighter hourglass FFN can be more effectively utilized, such as by enlarging model hidden dimensions under fixed budgets. We confirm these through empirical validations across model scales: hourglass FFNs outperform conventional FFNs up to 400M and achieve comparable performance at larger scales to 1B parameters; hourglass FFN variants with reduced FFN and increased attention parameters show consistent improvements over conventional configurations at matched budgets. Together, these findings shed new light on recent work and prompt rethinking of the narrow-widenarrow MLP convention and the balance between attention and FFN towards efficient and expressive modern language models. 1. Introduction Despite rapid advances in scale and training methodology, the architectural shape of dense Transformer language *Equal contribution 1MediaTek Research 2National Taiwan University. Correspondence to: <ft.liao@mtkresearch.com>, <meng-hsi.chen@mtkresearch.com>. Preprint. February 9, 2026. Figure 1. Performance frontiers of Transformers with hourglass (wide-narrow-wide) versus conventional (narrow-widenarrow) FFNs (Touvron et al., 2023). We revisit the shape convention of Transformer by replacing the narrow-wide-narrow FFN with hourglass FFN, composing stacks of wide-narrow-wide sub-MLPs connected by residuals. We observe that Hourglass FFNs achieve comparable performance to the conventional design up to 1B parameters. Here we also show conventional variant trained based on OLMo-2 architecture. Only the non-embedding parameters are accounted for the FLOPs. models has remained remarkably stable since early scaling studies (Kaplan et al., 2020). Modern models consistently adopt narrowwidenarrow multilayer perceptron (MLP) in feed-forward network (FFN), expanding the model dimension dmodel to an intermediate width dh before projecting back, with expansion ratios typically fixed between 2 and 4 (Vaswani et al., 2017; Kaplan et al., 2020). This design choice has become de facto standard across contemporary dense LLMs (Touvron et al., 2023; Qwen et al., 2025; Team et al., 2025; OLMo et al., 2025). At the same time, the FFN dominates parameter allocation in Transformers, accounting for majority of model parameters relative to attention. As result, the MLP shape implicitly determines how capacity is distributed between depth, width, and attention. While the shape convention in Transformer is effective, we raise fundamental question: is the narrowwidenarrow MLP shape in FFN essential, or merely one convenient instantiation of residual-compatible transformation? Recent results suggest that this convention may be unnecessarily restrictive. Theoretical studies have shown that residual MLPs with widenarrowwide (hourglass) structures can act as optimal function approximators, including universality results for networks with extremely narrow intermediate layers (Lin & Jegelka, 2018; Liu et al., 2024a). From an expressivity perspective, such hourglass architectures permit deeper residual compositions at fixed parameter budgets, Revisiting the Shape Convention of Transformer Language Models Figure 2. Overview: revisiting the shape convention of Transformer through studying the relaxation of MLP shape in FFN. Inspired by (Liu et al., 2024a; Chen et al., 2025), we compare transformer architectural variants with conventional FFN and hourglass FFN. (a) Conventional Transformer Block with layers, consisting of an attention module and conventional FFN with narrow-wide-narrow MLP. (b) Hourglass Transformer Block with layers, consisting of an attention module followed by an hourglass FFN with hourglassshaped MLP sub-blocks. We explore the design space by tuning parameters such as dmodel, dh, K, and L, allowing the Hourglass layer count to differ from the baseline L. setting known to favor rapid growth in linear regions and representational complexity (Hanin & Rolnick, 2019; Joyce & Verschelde, 2026). Empirical evidence has also begun to challenge conventional MLP shape assumptions. Motivated by high-dimensional representation learning, Chen et al. (2025) demonstrates that hourglass-shaped MLPs outperform standard designs in generative vision models. Related work further shows that expert-routed FFNs with hourglass MLPs in Mixtureof-Experts Transformers can surpass dense baselines (Wang et al., 2024) at scale. Together, these results indicate that residual hourglass MLPs constitute strong class of building blocks, motivating re-examination of FFN shape conventions in Transformer language models. In this work, we revisit the shape convention of Transformer and study the design space by substituting the conventional narrow-wide-narrow FFN with hourglass FFN variant, consisting of stacked widenarrowwide MLPs with residuals. Such variant offers several advantages. Embedding representations into higher-dimensional spaces before and after narrow transformations increases flexibility for linear separability, consistent with classical results such as Covers theorem and random feature methods for kernel approximation (Cover, 1965; Rahimi & Recht, 2007). Moreover, hourglass MLPs allow parameters to be allocated more efficiently across depth and width, principle that has been successfully exploited in convolutional networks (Zagoruyko & Komodakis, 2016; Zhou et al., 2020) and other deep architectures (Shazeer et al., 2017; Hu et al., 2022). We therefore hypothesize that FFN with wide-narrow-wide MLPs can outperform the conventional counterpart in Transformer language models. hourglass FFN transformers with conventional FFN baselines at various parameter sizes between 113M and 1B. We provide extensive architecture search results to further isolate the contributions of model width, block layer depth, and hourglass FFN structure. To summarize, our contributions are as follows: We challenge the standard narrow-wide-narrow FFN shape in conventional Transformer, proposing an efficient wide-narrow-wide Hourglass FFN that maintains expressivity with hourglass sub-MLPs connected by residuals. We demonstrate parameter reallocation strategy where redirecting parameters from FFN width to attention width or internal depth (K > 1) yields superior performance at 113M. We provide architectural insights by identifying U-shaped width-depth trade-off, finding optimal dh/dmodel and dmodel/L ratios and robust deeper Hourglass FFN structures. Based on the architectural insights at 113M, we validate the scalability of the Hourglass FFN from 113M to 1B, demonstrating it is competitive and efficient alternative to standard baselines. 2. Background and Related Works 2.1. Narrow-wide-narrow MLP in Transformer FFN In Transformer architectures, the FFN serves as positionwise non-linear transformation applied independently to each token representation. Formally, given hidden state Rdmodel , the conventional MLP with residual in FFN computes We experimentally validate the hypothesis by comparing FFN(z) = + W2σ(W1norm(z)) (1) 2 Revisiting the Shape Convention of Transformer Language Models where σ() is non-linear activation function, norm() is normalization function, W1 Rdhdmodel , W2 Rdmodeldh, and dh > dmodel. This expansionprojection structure has been retained across most modern dense LLMs (Touvron et al., 2023; Qwen et al., 2025; Team et al., 2025). Beyond functional form, the FFN plays central role in determining the overall shape of Transformer. It accounts for majority of parameters and floating-point operations per layer, often exceeding those of the attention module. As result, the choice of the shape expansion ratio dh/dmodel, together with the model dimension dmodel and the number of block layers L, implicitly governs how capacity is allocated across width, depth, and attention. While prior work has explored variations in activation functions (Shazeer, 2020), normalization strategies (Xiong et al., 2020; Jiang et al., 2023b), and parameter sharing (Lan et al., 2020; Li et al., 2022), the overall narrowwidenarrow shape of the FFN and its interaction with Transformer scaling (Kaplan et al., 2020) have remained largely unchanged. 2.2. Revisiting Shape Through Hourglass MLPs Theoretical analyses of wide-narrow-wide (hourglass)1 MLP with residual connections have shown its universal approximation capability of continuous function. When sufficient network depth is allowed, (Lin & Jegelka, 2018) establishes that residual networks with extremely narrow hidden layers retain universal approximation capability while (Liu et al., 2024a) further shows that residual hourglass MLP with narrow constant width can even achieve optimal approximation of Lebesgue-integrable functions. Empirically, while we focus on the FFN of transformer, several work focusing on efficient architecture has explored employing hourglass architecture with residual connections. The computer vision literature on networks such as U-Net (Ronneberger et al., 2015), MobileNet (Zhou et al., 2020), and Wide ResNets (Zagoruyko & Komodakis, 2016) utilize bottleneck structures and separable convolutions in depth to optimize the parameter-compute ratio. These works illustrate that carefully structured bottlenecks can reduce computational cost while preserving, or even enhancing, representational power through increased depth or improved gradient flow. In the context of language modeling, low-rank and bottleneck-inspired techniques have primarily appeared in the form of parameter-efficient methods. Approaches such as LoRA (Hu et al., 2022; Liu et al., 2024b) introduce lowrank projections within linear layers to reduce the number of trainable parameters during fine-tuning, effectively imposing bottleneck on weight updates. Mixture-of-Experts (MoE) (Shazeer et al., 2017; Mu & Lin, 2025) scales ca1Prior works often use the term - bottleneck - to describe the wide-narrow-wide shape while we use hourglass here interchangeably. pacity without proportional increase in inference cost by replacing the dense FFN with multiple sparse expert networks. Moreover, historically, the hourglass MLP shape in dense Transformer FFN is shown to be worse than the narrowwide-narrow counterpart in the earlier work of scaling law by (Kaplan et al., 2020). However, recent work in general MLP study on generative vision tasks paints it differently (Chen et al., 2025), where residual-connected hourglass MLPs, which iteratively refine representations in expanded feature spaces, outperform conventional MLPs. While prior research focused on general MLP properties, our work studies hourglass MLPs in FFN as an alternative for the conventional FFN in Transformer, enabling reconsidering of model shape and parameter alllocation between attention and FFN. We validated such hourglass FFNs effectiveness as parameter-efficient alternative to the de facto narrowwide-narrow standard across language model scales. 3. Transformer with Hourglass FFN To relax and study the shape constraints of conventional Transformer, we propose Transformer variant that replaces the conventional wide-expansion MLP with narrowcontracted MLP in FFN (Hourglass FFN) as illustrated in Figure 2. Based on theoretical and empirical background discussed in Section 2, we posit that Hourglass FFN can achieve comparable performance to conventional FFN under the same parameter budget. 3.1. Network Architecture The architecture follows the conventional LLaMA style Transformer backbone (Touvron et al., 2023) consisting of input embeddings, stacked model layers L, and final output projection. Hourglass Transformer Layer Each layer consists of an attention module followed by an Hourglass FFN. The attention module performs global information aggregation across the sequence, while the Hourglass FFN performs local feature refinement within each token. Both components are wrapped with residual connections and layer normalization. Specifically, given the input to the l-th layer z(ℓ), the intermediate representation u(ℓ) is computed via the attention mechanism (Attn): u(ℓ) = z(ℓ) + Attn(norm(z(ℓ))) (2) Then, we set h(ℓ) 0 = u(ℓ) as the input to the Hourglass FFN. Hourglass Feed-Forward Network The Hourglass FFN refines the representation through stacked hourglassRevisiting the Shape Convention of Transformer Language Models shaped MLP sub-blocks. For = 0, . . . , 1: i+1 = h(ℓ) h(ℓ) + MLPi(h(ℓ) ) (3) where MLPi denotes the i-th hourglass sub-block. Finally, the output of the layer is z(ℓ+1) = h(ℓ) . While the convention expands the hidden dimension dmodel to wider dh, the Hourglass FFN utilizes compressionexpansion structure with bottleneck dimension dh < dmodel. Each sub-block MLPi consists of down-projection (i) Rdhdmodel , non-linear activation σ, and an upprojection (i) Rdmodeldh. Formally: MLPi(x) = (i) σ(W (i) norm(x)) (4) This structure allows independent control over the FFNs depth (K) and width (dh). In practice, we implemented the hourglass MLP following (Touvron et al., 2023) using the SwiGLU activation function. Specifically, each sub-block MLPi consists of two downprojection matrices (i) d2 Rdhdmodel and one upprojection matrix (i) Rdmodeldh . That is (SiLU(W (i) where = norm(x) and denotes element-wise multiplication. MLPi(x) = (i) d1 x) (W (i) d1 , (i) d2 x)) (5) 3.2. Hourglass FFN Transformer Shape With the proposed hourglass FFN, the total number of transformer parameters is L(Attn(dmodel) + K(3dhdmodel)). To achieve and optimal performance at given parameter budget, we need to balance the design parameters: K, L, dh, and dmodel. Such design introduces two fundamental shifts in resource allocation: Parameter Redistribution from FFN to Attention. The parameter efficiency of the Hourglass FFN (dh < dmodel) decouples model depth from parameter explosion. The parameters saved by narrowing the FFN can be reinvested to increase the model dimension dmodel or the attention capacity. This effectively shifts the models compute bias from pointwise FFN operations to pairwise attention interactions. As shown in Section 4, this reallocation yields improvement performance at fixed parameter budgets. Trading FFN Width for Depth. Conventional FFNs rely on extreme width to approximate complex functions. In contrast, the Hourglass FFN leverages depth by stacking sub-blocks within single layer. This increases the sequence of non-linear transformations without expanding the activation space dimensionality. This deep and narrow topology enhances the networks compositional depth and expressivity, allowing for more complex feature refinement than shallow, wide baselines. 4. Experiments We evaluate the proposed hourglass FFN architecture against baseline Transformer-based LMs across multiple model scales. Rather than merely benchmarking performance, our experiments serve as test for the conventional narrow-wide-narrow shape. We aim to verify whether the parameter redundancy in standard FFNs can be effectively repurposed to enhance attention capacity without sacrificing model depth or width. 4.1. Experimental Setup Baselines. We compare our approach against strong, representative open-source baselines (see Table 3 for configuration) to ensure fair and reproducible evaluation: Standard Transformer (113M, 403M, 906M, 1074M): Canonical Transformer-based LMs following the LLaMA architecture (Touvron et al., 2023; OLMo et al., 2025), serving as the primary conventional baselines for parameter efficiency comparisons (Gu & Dao, 2024; Sun et al., 2025b). Conventional (OLMo-2): We take configuration of state-of-the-art open-weights 1B-scale model (OLMo et al., 2025), used as high-water mark for performance. OLMo2 differs from the standard Transformer baselines at the order of normalization, where the layer normalization taking place after the attention and the MLP but before the residuals. Transformer with Hourglass FFN. For each baseline size, we construct corresponding variant by replacing the conventional FFN with the Hourglass FFN, while keeping the attention module design identical to the baseline. Comparisons are carried out with approximately matched total parameter budgets within the parameter count difference 0.001%. Training and Evaluation Details. All models are trained on the same dataset and tokenized using identical preprocessing pipelines. Optimization uses AdamW with cosine learning rate schedule. The details of experiment settings can be found in Appendix A.1. Following the convention in (Kaplan et al., 2020; Hoffmann et al., 2022), we report only the non-embedding parameters as the model parameter for model configuration. Evaluation-wise, we report averaged validation loss and validation perplexity (PPL) over validation sets for all experiments. In Section 4.2.5, the performance on the downstream tasks on model scales is further presented. 4.2. Main Results We present the main experimental results in three parts. First, we compare the hourglass FFN Transformer and the Revisiting the Shape Convention of Transformer Language Models Table 1. Hourglass FFNs vs. Conventional FFNs under fixed dmodel and (113M parameters). The Hourglass variant achieves lower validation loss and perplexity compared to the conventional baseline. We fix dmodel = 768 and = 12. Model Attention Size FFN Size Conventional Hourglass 28M 28M 28M 28M 28M 85M 85M 85M 85M 85M dh 3072 614 512 384 Val Loss Val PPL 1 5 6 8 10 3.464 3.458 3.464 3.457 3.465 36. 36.235 36.369 36.179 36.438 baseline Transformer under fixed attention size configuration to isolate the effect of the Hourglass FFN. Second, under matched total parameter budgets, we explore the design space by varying the intermediate dimension ratio and jointly adjusting FFN depth (K) and Transformer layer count (L). Furthermore, we study the interplay between Transformer model dimension dmodel and the number of model layers given fixed Hourglass FFN and dh. Finally, we evaluate both architectures across multiple model scales under matched total parameter budgets. Figure 3. Validation loss across different dh/dmodel ratios for Hourglass FFNs with varying depth at = 12. The lowest validation loss is observed at = 4 and dh/dmodel 0.4. We fixed the total model size to 113M parameters. 4.2.1. BREAKING THE CONVENTIONAL SHAPE OF TRANSFORMER FFN We first evaluate the hourglass FFN Transformer and the baseline Transformer at the 113M scale under fixed attention size configuration, where dmodel = 768 and = 12 for both models. This controlled setting ensures identical attention parameter counts, providing fair basis to isolate the effect of replacing the conventional FFN with the Hourglass FFN. In this experiment, we vary the Hourglass FFN depth {5, 6, 8, 10} and adjust the intermediate dimension dh accordingly, while keeping the total model size fixed at 113M parameters. The results in Table 1 show that, under identical dmodel and attention parameter counts, the hour5 glass FFN Transformer achieves comparable or lower validation loss, validation perplexity (PPL) than the baseline. For example, validation perplexity drops from 43.19 in the baseline to 42.16 at = 8. This improvement suggests that the hourglass FFN structure itself is more parameter-efficient. By forcing information through bottleneck (narrowing dh) and then expanding it, combined with residual pathways, the architecture may act as more effective feature filter than the brute-force width of standard FFNs. These findings motivate further analysis of optimal configurations under matched total parameter budgets, specifically investigating whether this efficiency gain holds when we rebalance the entire models parameter distribution. 4.2.2. HOW NARROW THE HOURGLASS FFN SHOULD BE? With = 12 (matching the baseline LLM), we evaluate the impact of varying dh/dmodel for three configurations: (L = 1, = 12), (K = 2, = 12), and (K = 4, = 12). As shown in Figure 3, (K = 4, = 12) generally achieves lower cross-entropy loss than (K = 2, = 12), with the best performance observed at dh/dmodel 0.4, indicating that increasing can be beneficial when paired with an appropriate intermediate dimension ratio. Across most tested ratios, both configurations outperform the baseline loss (dashed line). These results highlight crucial trade-off: FFNs do not need to be uniformly wide to be effective. The success of narrower, deeper FFNs (e.g., = 4, dh/dmodel 0.4) implies degree of redundancy in the standard expansion ratio of 4. By accepting narrower intermediate state, we can afford deeper stacking (K > 1), which appears to offer better representational balance than width alone. 4.2.3. CAN THE PARAMETERS BE REDISTRIBUTED MORE TO THE ATTENTION MODULE? We first compare (K, L) {(2, 12), (2, 6), (4, 12), (4, 6)} to study the interaction between FFN depth and Transformer layer count L, as well as the distribution of parameters between attention and FFN modules. As shown in Table 2, all configurations are matched to total model size Revisiting the Shape Convention of Transformer Language Models Table 2. Hourglass FFN variants with fewer layers while maintaining at 113M. Hourglass FFN variants with reduced layers (L = 6) achieve lower validation loss and perplexity to the baseline (L = 12). Model Attention Size FFN Size dmodel dh Val Loss Val PPL Conventional Hourglass 28M 66M 53M 51M 45M 85M 47M 60M 62M 68M 768 1176 1488 1032 1368 3072 553 1122 418 694 1 2 2 4 12 12 6 12 6 3.464 3.426 3.428 3.458 3.418 36.441 35.346 35.447 36.392 35. of approximately 113M parameters, yet achieve very similar performance: validation perplexity differences range from 0.05 to 1.34, and validation losses differ by at most 0.046 across settings. Surprisingly, the best result is obtained with (K = 4, = 6), yielding validation perplexity of 35.101, closely followed by (K = 2, = 12) at 35.346. Crucially, this architecture inverts the conventional parameter allocation: the attention module becomes the dominant consumer of parameters, surpassing the FFN (e.g., 66M vs 47M in the = 2, = 12 setting). This shift is significant because it suggests that for small-scale models, contextual processing (mediated by attention) is more valuable resource than static factual retrieval (mediated by FFNs). The hourglass FFN allows us to buy back parameters from the FFN and reinvest them into wider, more capable attention mechanism. 4.2.4. HOW DOES HOURGLASS FFN CHANGE THE TRANSFORMER WIDTH AND DEPTH LANDSCAPE? Figure 4. Validation loss versus dmodel/L ratio for different Hourglass FFN configurations at 113M parameters. The validation loss is minimized when the ratio dmodel/L is around 110 for = 4; around 180 for = 2; around 144 for = 1. Encouraged by the positive results in Section 4.2.3, we conducted comprehensive parameter search to understand the optimal shape of the hourglass Transformer. We varied the ratio dmodel/L (effectively trading network width for depth) while keeping the total parameter count fixed at 6 113M. We evaluated multiple hourglass configurations with different internal depths (K {1, 2, 4}) and width ratios (dh/dmodel), selected best on best results from Figure 3. The results, summarized in Figure 4, reveal several key insights. First, rather than flat plateau, the width-depth trade-off exhibits distinct U-shaped curve. Across all configurations, there is clear sweet spot for the dmodel/L ratio, generally falling between 100 and 250. This indicates that neither extreme depth (small ratio) nor extreme width (large ratio) is optimal; instead, balanced allocation where the model dimension is roughly 100 to 250 times the number of layers yields the lowest validation loss. Second, increasing the hourglass depth consistently improves performance and robustness. As shown in Figure 4, the curve for = 4 (green squares) lies below the curves for = 2 (orange triangles) and = 1 (blue circles/red diamonds) across the entire sweep. In fact, the = 4 configuration outperforms the conventional baseline (purple dashed line) across nearly the entire tested range of width-depth ratios. This suggests that deeper internal FFN structure (K > 1) is more parameter-efficient way to gain expressivity than simply reshaping the global attention-layer topology. Finally, the optimal configurations for the Hourglass architecturespecifically those with = 2 or = 4 in the optimal width-depth rangesignificantly outperform the conventional baseline. This confirms that reallocating parameters from the FFNs brute-force width to combination of increased attention width and deeper, narrower FFN stacks is preferable design strategy for this scale. 4.2.5. SCALABILITY ACROSS MODEL SCALES Having validated the potential of Hourglass FFN under fixed attention size at the 125M scale, we now extend our evaluations to multiple model sizes (113M, 403M, 906M, and 1B) under matched total parameter budgets, where baseline configurations are listed in Table 3. These comparisons allow dmodel to vary, enabling each architecture to optimize its parameter allocation while maintaining the same overall capacity. For Hourglass FFN, we scaled the model by searching for configurations following the guidance presented in Section 4.2.2 and 4.2.4. For 113M, 403M, and Revisiting the Shape Convention of Transformer Language Models Table 3. Configurations of different model sizes for conventional baselines and hourglass variants. We follow (Brown et al., 2020) for setting model parameters of conventional baselines and (Hoffmann et al., 2022) for learning parameters. We construct hourglass variants from these baselines with approximately matched parameter budgets within less than 0.001% difference. The scaled model configurations of the hourglass variants are chosen based on searched configurations at 113M described in Section 4.2.2 and 4.2.4. Model Size Attention Size FFN Size dmodel dh LR Batch (Tokens) Tokens (B) Conventional Hourglass Conventional Hourglass Conventional Hourglass Conventional Hourglass 113M 403M 906M 1074M 28M 51M 101M 150M 227M 415M 269M 649M 85M 62M 302M 253M 679M 491M 805M 425M 768 1032 1024 1536 2080 2048 2848 3072 418 4096 557 6144 819 8192 12 12 24 24 24 24 16 20 1 4 1 1 4 1 1 6 104 3 104 2.5 104 4 10 0.5M 0.5M 2M 4M 2.5 16 21 Table 4. Performance comparison of Hourglass vs. Conventional FFN Transformers across model scales (113M1B). On validation sets, the Hourglass architecture achieves lower loss at smaller scales (up to 906M) and comparable performance at the 1B parameter scale. For downstream tasks, Hourglass models show higher accuracy in all reasoning and QA tasks below 403M and comparable performance up to the 1B parameter scale. The configurations of the models are from Table 3. The Conventional (OLMo-2) is trained from configuration of OLMo2-1B. Downstream Tasks are report in accuracy (higher is better unless noted as PPL). Validation Downstream Tasks Size Model Val Loss Val PPL Arc Easy HellaSwag PIQA SciQ CommonsenseQA TriviaQA (PPL) NaturalQS (PPL) 113M 403M 906M 1074M Conventional Hourglass Conventional Hourglass Conventional Hourglass Conventional Hourglass Conventional (OLMo-2) 3.464 3. 3.087 3.064 2.943 2.934 2.810 2.810 2.833 36.441 35.335 25.398 24.909 22.473 22. 20.002 20.082 20.466 0.453 0.456 0.518 0.521 0.553 0.542 0.554 0.577 0.568 0.283 0. 0.351 0.349 0.385 0.383 0.407 0.403 0.405 0.611 0.622 0.666 0.640 0.676 0. 0.684 0.682 0.669 0.683 0.696 0.768 0.777 0.788 0.798 0.806 0.825 0.810 0.293 0. 0.332 0.333 0.362 0.361 0.369 0.359 0.373 1.914 1.877 1.586 1.591 1.505 1. 1.408 1.422 1.428 1.604 1.537 1.427 1.386 1.325 1.324 1.311 1.272 1.323 906M models, we select configurations with dmodel/L ratios between 57 and 87 and dh/dmodel 0.4 following results in Figure 3; for 1B, we select the configuration with dmodel/L 142, dh/dmodel 0.85, = 1, and = 20, the setup with the lowest validation perplexity on Figure 4. Configurations and results of the setup are presented in Table 3 and Table 4. Scalability of the Shape. Hourglass FFN variants consistently outperform conventional baselines in validation loss and perplexity at 113M, 403M, and 906M scales (see Table 4). For instance, at 906M, validation perplexity improves from 22.473 to 22.282. This advantage is largely maintained in downstream evaluations, where Hourglass models show particular strength in reasoning tasks, improving accuracy on Arc Easy, HellaSwag, PIQA, SciQ, and CommonsenseQA. At the 1B scale (1074M), the Hourglass variant performs on par with the strong narrow-widenarrow baseline (Val PPL 20.082 vs 20.002) and surpasses the OLMo2 configuration (20.466). The diminishing relative gain at 1B compared to smaller scales may be attributed to the limit of how much we can compress the FFN capacity, suggesting that minimal FFN capacity is still required for larger models. Parameter Distribution. The Hourglass architecture enables flexible reallocation of the parameter budget. As shown in Table 3, while conventional Transformers are consistently FFN-dominated (allocating 3 more parameters to FFN than Attention), our optimized Hourglass configurations shift this balance. At 906M, the parameters are roughly balanced (415M Attn vs 491M FFN). Notably, at the 1B scale, the configuration discovered by our search strategy at = 1 favors an Attention-dominated allocation (649M Attn vs 425M FFN). This extreme shift aligns with our findings at smaller scales: contextual processing (Attention) is highly valuable resource, and the Hourglass FFN allows us to reallocate budget to it efficiently. 4.3. Ablation Studies We conduct series of ablation studies on the 113M hourglass FFN Transformer to investigate the impact of key architectural hyperparameters. All experiments use the same dataset, tokenization, and training setup as described in Section 4.1. 4.3.1. VARYING With dmodel = 1032, dh = 418, and = 12 fixed, we vary {1, 2, 4, 6, 8} to study the effect of Hourglass Revisiting the Shape Convention of Transformer Language Models Table 5. Impact of increasing hourglass depth K. Performance (validation loss and perplexity) improves as depth increases from 1 to 8. We fixed dmodel = 1032, dh = 418, and = 12. effective. Overall, the results delineate practical operating regime for FFN width reduction in hourglass architectures. Model Size Attention Size FFN Size Val Loss Val PPL 5. Discussions and Future Work 1 2 4 6 8 67M 82M 113M 144M 175M 51M 51M 51M 51M 51M 16M 31M 62M 93M 124M 3.551 3.489 3.426 3.391 3.357 40.153 37.632 35.335 34.051 32. FFN depth. As shown in Table 5, increasing consistently improves performance: validation perplexity drops from 40.153 at = 1 to 32.832 at = 8, with corresponding reductions in both training and validation loss. The gains are most pronounced when moving from shallow configurations (K = 1 or = 2) to moderate depth (K = 4 or = 6), after which improvements become more incremental. Notice that under this setting, larger also increases model size from 67M at = 1 to 175M at = 8, suggesting trade-off between depth and efficiency. This indicates that while deeper Hourglass FFNs yield better performance, practical deployments may prefer moderate values (e.g., = 4 or = 6) to balance accuracy and parameter budget. Table 6. Impact of varying dh. The validation perplexity increases gradually as the ratio decreases from 0.8 to 0.1. We fixed dmodel = 1032, = 4, and = 12. dh 836 627 418 209 103 dh/dmodel Model Size Attention Size FFN Size Val Loss Val PPL 0.8 0.6 0.4 0.2 0.1 175M 144M 113M 82M 66M 51M 51M 51M 51M 51M 124M 93M 62M 31M 15M 3.355 3.384 3.426 3.500 3. 32.788 33.747 35.335 38.075 39.741 4.3.2. VARYING dh With dmodel = 1032, = 12, and = 4 fixed, we vary dh/dmodel {0.8, 0.6, 0.4, 0.2, 0.1} to study the effect of reducing the intermediate FFN dimension relative to the hidden dimension. As shown in Table 6, decreasing the ratio leads to smaller model sizes from 175M at ratio = 0.8 down to 66M at ratio = 0.1 while performance degrades gradually. Validation perplexity increases from 32.788 at ratio = 0.8 to 39.741 at ratio = 0.1, with the largest jumps occurring when the ratio drops below 0.4. These results characterize the trade-off between FFN width and model efficiency under fixed architectural configuration. Reducing dh leads to substantial parameter savings in the FFN, while validation performance degrades gradually over broad range of ratios. In particular, configurations with dh/dmodel 0.4 maintain comparable performance despite significant reductions in model size. Below this range, performance degradation becomes more pronounced, indicating transition where further FFN compression is less 8 Our investigation challenges the long-standing narrowwide-narrow shape constraint in Transformer feed-forward networks (FFNs). We demonstrate that this design constraint is largely redundant: by constricting the FFN into narrower, deeper hourglass bottleneck, we can achieve competitive or superior performance while drastically reducing the parameter count of the FFN itself. This efficiency gain allows for fundamental shift in resource allocation. Unlike standard Transformers where the FFN dominates the parameter budget, the Hourglass FFN enables the reallocation of parameters to the attention mechanism, allowing it to become the dominant component (e.g., utilizing 66M parameters for attention vs. 47M for FFN at the 113M scale). This inversion suggests that for smaller-scale models, enhancing contextual processing capacity is more valuable than expanding the static factual memory typically associated with wide FFNs. Furthermore, our results show that this performance is robust across various depth allocations (K vs. L) and intermediate ratios (dh/dmodel 0.40.6), highlighting the architectural flexibility of our proposal. While these results are promising, specific limitations remain. First, our comprehensive parameter search was constrained to the 113M scale due to compute resources, with findings extrapolated to models up to 1B parameters with limited ablation to confirm the extrapolation. Consequently, the scalability of the Hourglass FFN to tens or hundreds of billions of parameters remains an open empirical question. Second , this study isolated the FFN modification while fixing the attention module to MHA, leaving the interaction between Hourglass FFNs and advanced attention mechanisms unexplored. Alternative attention module such as Group-Query Attention (Jiang et al., 2023a) or Multi-head Latent Attention (DeepSeek-AI et al., 2025) could change the attention-FFN ratio landscape. Future work would prioritize verifying these findings at larger scales. Specifically, continued scaling of model dimensions (dmodel and K) is promising avenue, as similar manifold-constrained architectures have shown benefits from expanded width (Xie et al., 2025). However, as models deepen, careful attention must be paid to signal propagation challenges, known as the curse of depth (Sun et al., 2025a). Deeper hourglass FFN may result in accumulation of residual noise, causing diminishing gradient updates at the later layers. Ultimately, we envision the Hourglass FFN as step towards more efficient architectures that could break free from historical design conventions to lift language models towards greater representational limits. Revisiting the Shape Convention of Transformer Language Models"
        },
        {
            "title": "Impact Statement",
            "content": "This work revisits the established architectural conventions of Transformer language models, specifically the design of Feed-Forward Networks. By demonstrating that alternative Hourglass structures can improve parameter and computational efficiency without sacrificing performance, our findings contribute to the ongoing effort to make large language models more efficient. This research has potential positive impacts by reducing the computational resources and energy consumption required for training and deploying models, thereby lowering the environmental footprint of AI development. Additionally, improved architectural efficiency may help democratize access to powerful language models. We do not foresee immediate negative societal consequences specific to this architectural modification, though the general ethical considerations of Large Language Models remain relevant."
        },
        {
            "title": "References",
            "content": "Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot In Larochelle, H., learners. Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf. Chen, M.-H., Lee, Y.-A., Liao, F.-T., and shan Shiu, D. Rethinking the shape convention of an mlp, 2025. URL https://arxiv.org/abs/2510.01796. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Cover, T. M. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. IEEE Transactions on Electronic Computers, EC-14(3):326334, 1965. doi: 10.1109/PGEC.1965. 264137. DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Zhang, H., Ding, H., Xin, H., Gao, H., Li, H., Qu, H., Cai, J. L., Liang, J., Guo, J., Ni, J., Li, J., Wang, J., Chen, J., Chen, J., Yuan, J., Qiu, J., Li, J., Song, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Xu, L., Xia, L., Zhao, L., Wang, L., Zhang, L., Li, M., Wang, M., Zhang, M., Zhang, M., Tang, M., Li, M., Tian, N., Huang, P., Wang, P., Zhang, P., Wang, Q., Zhu, Q., Chen, Q., Du, Q., Chen, R. J., Jin, R. L., Ge, R., Zhang, R., Pan, R., Wang, R., Xu, R., Zhang, R., Chen, R., Li, S. S., Lu, S., Zhou, S., Chen, S., Wu, S., Ye, S., Ye, S., Ma, S., Wang, S., Zhou, S., Yu, S., Zhou, S., Pan, S., Wang, T., Yun, T., Pei, T., Sun, T., Xiao, W. L., Zeng, W., Zhao, W., An, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Li, X. Q., Jin, X., Wang, X., Bi, X., Liu, X., Wang, X., Shen, X., Chen, X., Zhang, X., Chen, X., Nie, X., Sun, X., Wang, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Song, X., Shan, X., Zhou, X., Yang, X., Li, X., Su, X., Lin, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhu, Y. X., Zhang, Y., Xu, Y., Xu, Y., Huang, Y., Li, Y., Zhao, Y., Sun, Y., Li, Y., Wang, Y., Yu, Y., Zheng, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Tang, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Wu, Y., Ou, Y., Zhu, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Zha, Y., Xiong, Y., Ma, Y., Yan, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Huang, Z., Zhang, Z., Xie, Z., Zhang, Z., Hao, Z., Gou, Z., Ma, Z., Yan, Z., Shao, Z., Xu, Z., Wu, Z., Zhang, Z., Li, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Gao, Z., and Pan, Z. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https: //arxiv.org/abs/2312.00752. Hanin, B. and Rolnick, D. Complexity of Linear Regions in Deep Networks. In Proceedings of the 36th International Conference on Machine Learning, pp. 25962604. PMLR, 2019. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre, L. Training compute-optimal large language models, 2022. URL https://arxiv.org/ abs/2203.15556. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation 9 Revisiting the Shape Convention of Transformer Language Models of large language models. In International Conference on Learning Representations (ICLR), 2022. URL https: //openreview.net/forum?id=nZe72R8yS0. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023a. URL https: //arxiv.org/abs/2310.06825. Li, B., Du, Q., Zhou, T., Jing, Y., Zhou, S., Zeng, X., Xiao, T., Zhu, J., Liu, X., and Zhang, M. ODE transformer: An ordinary differential equation-inspired model for sequence generation. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 83358351, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.571. URL https: //aclanthology.org/2022.acl-long.571/. Jiang, Z., Gu, J., Zhu, H., and Pan, D. Z. Pre-RMSNorm and pre-CRMSNorm transformers: Equivalent and efIn Thirty-seventh Conficient pre-LN transformers. ference on Neural Information Processing Systems, 2023b. URL https://openreview.net/forum? id=z06npyCwDq. Johannes Welbl, Nelson F. Liu, M. G. Crowdsourcing multiple choice science questions. 2017. Lin, H. and Jegelka, S. Resnet with one-neuron hidden layers is universal approximator. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips. cc/paper_files/paper/2018/file/ 03bfc1d4783966c69cc6aef8247e0103-Paper. pdf. Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. triviaqa: Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art. arXiv:1705.03551, 2017. Joyce, J. and Verschelde, J. Computing linear regions in neural networks with skip connections. In Boulier, F., Mou, C., Sadykov, T. M., and Vorozhtsov, E. V. (eds.), Computer Algebra in Scientific Computing, pp. 175194, Cham, 2026. Springer Nature Switzerland. ISBN 978-3032-09645-6. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466, 2019. doi: 10. 1162/tacl 00276. URL https://aclanthology. org/Q19-1026/. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: lite bert for selfIn supervised learning of language representations. International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=H1eA7AEtvS. Liu, C., Liang, E., and Chen, M. Characterizing ResNets In Salakhutdiuniversal approximation capability. nov, R., Kolter, Z., Heller, K., Weller, A., Oliver, N., Scarlett, J., and Berkenkamp, F. (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 3147731515. PMLR, 2127 Jul 2024a. URL https://proceedings.mlr. press/v235/liu24am.html. Liu, S.-Y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F., Cheng, K.-T., and Chen, M.-H. Dora: weightIn Proceedings of decomposed low-rank adaptation. the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024b. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016. Mu, S. and Lin, S. comprehensive survey of mixture-ofexperts: Algorithms, theory, and applications, 2025. URL https://arxiv.org/abs/2503.07137. OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., Ettinger, A., Guerquin, M., Heineman, D., Ivison, H., Koh, P. W., Liu, J., Malik, S., Merrill, W., Miranda, L. J. V., Morrison, J., Murray, T., Nam, C., Poznanski, J., Pyatkin, V., Rangapur, A., Schmitz, M., Skjonsberg, S., Wadden, D., Wilhelm, C., Wilson, M., Zettlemoyer, L., Farhadi, A., Smith, N. A., and Hajishirzi, H. 2 olmo 2 furious, 2025. URL https://arxiv.org/abs/ 2501.00656. 10 Revisiting the Shape Convention of Transformer Language Models Qwen, :, Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https: //arxiv.org/abs/2412.15115. Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Proceedings of the 20th International Conference on Neural Information Processing Systems, NIPS07, pp. 11771184, Red Hook, NY, USA, 2007. Curran Associates Inc. Reid, M., Zhong, V., Gururangan, S., and Zettlemoyer, L. M2D2: massively multi-domain language modeling dataset. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 964975, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.63. URL https:// aclanthology.org/2022.emnlp-main.63/. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Navab, N., Hornegger, J., Wells, W. M., and Frangi, A. F. (eds.), Medical Image Computing and Computer-Assisted Intervention MICCAI 2015, pp. 234241, Cham, 2015. ISBN 978-3-319Springer International Publishing. 24574-4. Shazeer, N. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations (ICLR), 2017. URL https://arxiv.org/ abs/1701.06538. Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, E., Zettlemoyer, L., Smith, N., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J., and Lo, K. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1572515788, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.840. URL https: //aclanthology.org/2024.acl-long.840/. Sun, W., Song, X., Li, P., Yin, L., Zheng, Y., and Liu, S. The curse of depth in large language models. arXiv preprint arXiv:2502.05795, 2025a. Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G., Dubois, Y., Chen, X., Wang, X., Koyejo, S., Hashimoto, T., and Guestrin, C. Learning to (learn at test time): Rnns with expressive hidden states, 2025b. URL https: //arxiv.org/abs/2407.04620. Talmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Rame, A., Rivi`ere, M., Rouillard, L., Mesnard, T., Cideron, G., bastien Grill, J., Ramos, S., Yvinec, E., Casbon, M., Pot, E., Penchev, I., Liu, G., Visin, F., Kenealy, K., Beyer, L., Zhai, X., Tsitsulin, A., Busa-Fekete, R., Feng, A., Sachdeva, N., Coleman, B., Gao, Y., Mustafa, B., Barr, I., Parisotto, E., Tian, D., Eyal, M., Cherry, C., Peter, J.-T., Sinopalnikov, D., Bhupatiraju, S., Agarwal, R., Kazemi, M., Malkin, D., Kumar, R., Vilar, D., Brusilovsky, I., Luo, J., Steiner, A., Friesen, A., Sharma, A., Sharma, A., Gilady, A. M., Goedeckemeyer, A., Saade, A., Feng, A., Kolesnikov, A., Bendebury, A., Abdagic, A., Vadi, A., Gyorgy, A., Pinto, A. S., Das, A., Bapna, A., Miech, A., Yang, A., Paterson, A., Shenoy, A., Chakrabarti, A., Piot, B., Wu, B., Shahriari, B., Petrini, B., Chen, C., Lan, C. L., Choquette-Choo, C. A., Carey, C., Brick, C., Deutsch, D., Eisenbud, D., Cattle, D., Cheng, D., Paparas, D., Sreepathihalli, D. S., Reid, D., Tran, D., Zelle, D., Noland, E., Huizenga, E., Kharitonov, E., Liu, F., Amirkhanyan, G., Cameron, G., Hashemi, H., Klimczak-Plucinska, H., Singh, H., Mehta, H., Lehri, H. T., Hazimeh, H., Ballantyne, I., Szpektor, I., Nardini, I., Pouget-Abadie, J., Chan, J., Stanton, J., Wieting, J., Lai, J., Orbay, J., Fernandez, J., Newlan, J., yeong Ji, J., Singh, J., Black, K., Yu, K., Hui, K., Vodrahalli, K., Greff, K., Qiu, L., Valentine, M., Coelho, M., Ritter, M., Hoffman, M., Watson, M., Chaturvedi, M., Moynihan, M., Ma, M., Babar, N., Noy, N., Byrd, N., Roy, N., Momchev, N., Chauhan, N., Sachdeva, N., Bunyan, O., 11 Revisiting the Shape Convention of Transformer Language Models H., Lan, Y., Wang, L., and Liu, T.-Y. On layer normalization in the transformer architecture, 2020. URL https: //openreview.net/forum?id=B1x8anVFPr. Zagoruyko, S. and Komodakis, N. Wide residual networks. In Richard C. Wilson, E. R. H. and Smith, W. A. P. (eds.), Proceedings of the British Machine Vision Conference (BMVC), pp. 87.187.12. BMVA Press, September 2016. ISBN 1-901725-59-6. doi: 10.5244/C.30.87. URL https://dx.doi.org/10.5244/C.30.87. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Zhou, D., Hou, Q., Chen, Y., Feng, J., and Yan, S. Rethinking bottleneck structure for efficient mobile network In Computer Vision ECCV 2020: 16th Eudesign. ropean Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III, pp. 680697, Berlin, Heidelberg, 2020. Springer-Verlag. ISBN 978-3-030-58579-2. doi: 10.1007/978-3-030-58580-8 40. URL https://doi. org/10.1007/978-3-030-58580-8_40. Botarda, P., Caron, P., Rubenstein, P. K., Culliton, P., Schmid, P., Sessa, P. G., Xu, P., Stanczyk, P., Tafti, P., Shivanna, R., Wu, R., Pan, R., Rokni, R., Willoughby, R., Vallu, R., Mullins, R., Jerome, S., Smoot, S., Girgin, S., Iqbal, S., Reddy, S., Sheth, S., Poder, S., Bhatnagar, S., Panyam, S. R., Eiger, S., Zhang, S., Liu, T., Yacovone, T., Liechty, T., Kalra, U., Evci, U., Misra, V., Roseberry, V., Feinberg, V., Kolesnikov, V., Han, W., Kwon, W., Chen, X., Chow, Y., Zhu, Y., Wei, Z., Egyed, Z., Cotruta, V., Giang, M., Kirk, P., Rao, A., Black, K., Babar, N., Lo, J., Moreira, E., Martins, L. G., Sanseviero, O., Gonzalez, L., Gleicher, Z., Warkentin, T., Mirrokni, V., Senter, E., Collins, E., Barral, J., Ghahramani, Z., Hadsell, R., Matias, Y., Sculley, D., Petrov, S., Fiedel, N., Shazeer, N., Vinyals, O., Dean, J., Hassabis, D., Kavukcuoglu, K., Farabet, C., Buchatskaya, E., Alayrac, J.-B., Anil, R., Dmitry, Lepikhin, Borgeaud, S., Bachem, O., Joulin, A., Andreev, A., Hardin, C., Dadashi, R., and Hussenot, L. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. URL https://arxiv.org/abs/2302.13971. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Wang, S., Chen, Z., Li, B., He, K., Zhang, M., and Wang, J. Scaling laws across model architectures: comparative analysis of dense and MoE models in large language models. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 5583 5595, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.319. URL https://aclanthology. org/2024.emnlp-main.319/. Xie, Z., Wei, Y., Cao, H., Zhao, C., Deng, C., Li, J., Dai, D., Gao, H., Chang, J., Yu, K., et al. mhc: Manifold-constrained hyper-connections. arXiv preprint arXiv:2512.24880, 2025. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Zhang, 12 Revisiting the Shape Convention of Transformer Language Models A. Appendix A.1. Details of Experiment Settings A.1.1. EXPERIMENTAL ENVIRONMENT AND TRAINING CORPUS All experiments are conducted based on the official training scripts provided in the OLMo codebase2 and are executed on NVIDIA RTX 6000 Ada and B200 GPUs. The fixed random seed 6198 is used in all experiments. For training data, we adopt the Stage 1 pre-training corpus used to train the original OLMo-2 1B checkpoint. To control for the effects of stochasticity arising from data shuffling, we replicate the exact data ordering employed during the Stage 1 pre-training of OLMo-2 1B. From this fixed-order dataset, we select the first 2.5B, 7B, 16B, and 21B tokens to train the 113M, 403M, 906M, and 1074M models, respectively. A.1.2. VALIDATION DATASETS We conduct validation using the following datasets: Dolma Common Crawl (Soldaini et al., 2024), Dolma The Stack (Soldaini et al., 2024), M2D2 (Reid et al., 2022), and WikiText (Merity et al., 2016). The validation splits follow the official OLMo-2 1B configuration. For both cross-entropy loss and perplexity, we report the average scores computed across these four datasets. Complete validation results are provided in Section A.2. A.1.3. DOWNSTREAM EVALUATION DATASETS We conduct downstream task evaluation on the following benchmark datasets: Arc Easy (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SciQ (Johannes Welbl, 2017), CommonsenseQA (Talmor et al., 2019), TriviaQA (Joshi et al., 2017), and NaturalQS (Kwiatkowski et al., 2019). Among these, TriviaQA and NaturalQS are evaluated using perplexity, while all remaining tasks are assessed using accuracy as the evaluation metric. A.1.4. TRAINING SETTING DETAILS Across all experiments, we use AdamW as the optimizer and adopt the cross-entropy loss augmented with an auxiliary softmax loss as the training objective. For AdamW, the hyperparameters are set to β1 = 0.9, β2 = 0.95, and ϵ = 1 108, with weight decay coefficient of λ = 0.1. SwiGLU is employed as the activation function for all models. Rotary Position Embedding (RoPE) is utilized for positional encoding in both the Transformer architecture and the proposed hourglass blocks. cosine learning rate scheduler with warmup is applied in all experiments, where the number of warmup tokens varies according to model size. Additional hyperparameters and model-size-dependent configurations are reported in Table 3 and Table 7. Table 7. Additional experimental hyperparameters for different model sizes, including learning rates, warmup tokens, attention heads, and sequence lengths. Model Size Learning Rate WarmUp Tokens Attention Heads Max Sequence Size 113M 403M 906M 1074M 6 104 3 104 2.5 104 4 104 50M 50M 50M 200M 12 16 16 2048 2048 2048 4096 A.2. Detailed Validation Results Table 8 presents the complete validation loss and perplexity results for all four model sizes (113M, 403M, 906M, and 1074M) evaluated across four distinct validation datasets. 2https://github.com/allenai/OLMo 13 Revisiting the Shape Convention of Transformer Language Models Table 8. Detailed validation loss and perplexity on our 4 validation datasets, including Dolma Common Crawl, Dolma The Stack, M2D2 and WikiText. Size Model Dolma Common Crawl Dolma The Stack M2D2 WikiText Dolma Common Crawl Dolma The Stack M2D2 WikiText Validation Loss Validation Perplexity 113M 403M 906M 1074M Conventional Hourglass Conventional Hourglass Conventional Hourglass Conventional Hourglass OLMo2 3.86 3.833 3.523 3.512 3.412 3.41 3.306 3.321 3. 2.477 2.414 2.064 2.026 1.837 1.829 1.668 1.659 1.686 3.873 3.858 3.568 3. 3.463 3.457 3.394 3.396 3.407 3.646 3.6 3.195 3.169 3.062 3.04 2.871 2.865 2. 47.448 46.188 33.874 33.502 30.317 30.27 27.275 27.676 28.032 11.907 11.184 7.875 7. 6.278 6.226 5.301 5.256 5.397 48.09 47.384 35.442 34.756 31.924 31.719 29.775 29.849 30. 38.321 36.583 24.403 23.793 21.371 20.915 17.655 17.547 18."
        }
    ],
    "affiliations": [
        "MediaTek Research",
        "National Taiwan University"
    ]
}