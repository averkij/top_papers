{
    "paper_title": "Efficient Test-Time Scaling via Self-Calibration",
    "authors": [
        "Chengsong Huang",
        "Langlin Huang",
        "Jixuan Leng",
        "Jiacheng Liu",
        "Jiaxin Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time."
        },
        {
            "title": "Start",
            "content": "Efficient Test-Time Scaling via Self-Calibration Chengsong Huang1, Langlin Huang1 Jixuan Leng2 Jiacheng Liu3, Jiaxin Huang1 1Washington Univeristy in St. Louis 2Carnegie Mellon University 3University of Washington {chengsong,h.langlin,jiaxinh}@wustl.edu jixuanl@cs.cmu.edu, liujc@cs.washington.edu 5 2 0 2 5 2 ] . [ 1 1 3 0 0 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Increasing test-time computation is straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and SelfConsistency with majority voting are simple and effective, they require fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling SelfConsistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-ofN and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidencebased Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with sample budget of 16 responses, indicating the efficency of the confidence-based sampling strategy at inference time 1."
        },
        {
            "title": "Introduction",
            "content": "Leveraging additional computation during inference can enhance the quality of responses generated by large language models (LLMs) (Snell et al., 2024a; Yao et al., 2023; Wu et al., 2024; Chen et al., 2025a). Among these methods, repeated sampling (Brown et al., 2024) such 1Our codes are available at https://github.com/ Chengsong-Huang/Self-Calibration. 1 Figure 1: Accuracy over response numbers of standard Self-Consistency (SC) vs. confidence-weighted Self-Consistency (SC w/ conf.) on MathQA using our trained Llama-3.1-8B-Instruct model. The horizontal lines mark the response usage difference required for SC w/ conf. to reach the same accuracy with SC. as Best-of-N (Cobbe et al., 2021a) and SelfConsistency (Wang et al., 2022b) generate multiple candidate responses and select the final answer by scoring model or majority voting rule. While these methods have proven effective, they require fixed amount of sampled responses for each query regardless of its difficulty and complexity. Although increasing the sample size generally improves performance, it also increases computational costs and inference time (Amini et al., 2024). This is particularly inefficient for simple questions like 2 + 3 = ?, where few samples are sufficient to find the correct solution (Chen et al., 2024b), and extensive sampling is unnecessary. Previous adaptive sampling strategies (Aggarwal et al., 2023; Li et al., 2024; Wan et al., 2024) typically design lightweight stopping criteria to determine whether additional responses should be sampled. However, they often incorporate manually designed features or heuristic rules, such as stopping when the model generates the same response three times consecutively, which can limit their generalizability across different tasks and models. Therefore, it is critical to design task-independent, model-agnostic approach without heavy reliance on human-designed heuristics. We propose an efficient test-time scaling method by using model confidence for dynamically sampling adjustment, since confidence can be seen as an intrinsic measure that directly reflects model uncertainty on different tasks. However, extracting accurate confidence can be challenging since LLMs are known to be overconfident on their own responses (Lin et al., 2022; Xiong et al., 2023; Leng et al., 2024), and their confidence often exceeds the actual accuracy. Self-Consistency (Wang et al., 2024a) can provide relatively accurate confidence estimation by aggregating answer counts from multiple sampled solutions (Tian et al., 2023a), but it again requires sampling large number of responses for each query beforehand. To address this, we introduce Self-Calibration to train LLMs for accurate confidence estimation in only one forward pass, without requiring any human-labeled data. Specifically, we improve model calibration by distilling Self-Consistencyderived confidence into the model itself. This is done by constructing pseudo training tuples of query, answer, and confidence on diverse training set. At test time, we design efficient test-time scaling strategies using these calibrated confidence scores, such as early stopping for Best-of-N when sampled responses reach target confidence, and Self-Consistency weighted by reliable confidence. Empirical experiments on three LLM architectures across six datasets demonstrate that our confidence-based test-time scaling approaches consistently outperform their baseline counterparts under the same sampling budget. Specifically, both Early Stopping for Best-of-N and confidenceweighted Self-Consistency improve MathQA accuracy over their baselines from 81.0 to 83.6 with an average sampling budget of 16 responses. More importantly, our approaches can achieve comparable performance with substantially fewer computational resources. As shown in Fig. 1, confidenceweighted Self-Consistency can save 94.2% samples to achieve an accuracy of 85.0, compared to standard Self-Consistency, demonstrating that reliable confidence estimation can significantly enhance the computational efficiency of test-time scaling."
        },
        {
            "title": "2 Repeated Sampling",
            "content": "Repeated sampling (Brown et al., 2024) is framework that generates multiple responses with Chainof-Thought prompting (Wei et al., 2022), then uses verifier to get the final results. We will introduce three fundamental repeated sampling strategies, which aim to enhance response quality by selecting the most suitable answer from multiple generated candidates."
        },
        {
            "title": "2.1 Best-of-N",
            "content": "For each input query x, multiple candidate responses {yi} are sampled, where 1 . scoring functionsuch as an additional reward model or confidence generatorassigns each response score ci = Score(yi). The simplest selection strategy, known as Best-of-N (Cobbe et al., 2021a), chooses the response with the highest score as the final answer as ˆy = arg max cj. y"
        },
        {
            "title": "2.2 Self-Consistency",
            "content": "Self-Consistency (Wang et al., 2022b) selects the most frequent response among multiple sampled candidates. Given candidate responses {y1, y2, . . . , yN }, the final answer is determined by majority voting: ˆy = arg max (cid:88) i=1 1(yi = z). This approach enhances robustness by aggregating diverse model outputs rather than relying on single highest-scoring response."
        },
        {
            "title": "2.3 Adaptive Self-Consistency",
            "content": "Adaptive Self-Consistency (ASC) (Aggarwal et al., 2023) enhances the standard Self-Consistency approach by dynamically adjusting the number of samples based on agreement among generated responses. This method iteratively samples responses and calculates the cumulative frequency vk(z) and relative frequency ˆrk(z) of each unique answer after samples: vk(z) = (cid:88) i=1 1(yi = z), ˆrk(z) = vk(z) . The sampling process continues until the maximum relative frequency ˆrk(z) exceeds predefined threshold τ . Formally: 2 Figure 2: Illustration of the Self-Calibration framework. Given query from the seed dataset, we sample responses from the LLM. We use confidence querying prompt to let LLM assign confidence score to each response. Responses are then grouped by their answers, and the Soft Self-Consistency (SSC) score is computed for each group. During training, all data tuples contribute to improving the models calibration, while higher-confidence data is used to enhance the LLMs generation ability. + 1, = arg max ˆrk(z) < τ, if max otherwise. ˆrk(z), This adaptive strategy reduces computational costs by limiting the number of required samples while maintaining high accuracy in the final answer selection."
        },
        {
            "title": "3 Self-Calibration",
            "content": "In this section, we provide an overview of our proposed Self-Calibration framework, illustrated in Fig. 2. First, we synthesize set of input-outputconfidence tuples (xi, yi, ci) from seed dataset for training, without requiring any ground-truth answer (Sec. 3.2). Using this synthetic dataset, we can train language model with combined loss to output calibrated confidence scores (Sec. 3.3)."
        },
        {
            "title": "3.1 Confidence Score Estimation",
            "content": "A naive way to obtain confidence score from LLM is P(True) (Kadavath et al., 2022). Given the input-output pair (xi, yi), we construct prompt as xi yi I, where is confidence querying prompt, Is the answer correct? (Yes/No). The confidence score is then defined as the probability of token Yes in the next position. c(x, y) = pθ(Yesx, y, I) Due to the KV-cache mechanism (Pope et al., 2022), the additional computational cost is roughly 3 equivalent to generating 10 tokens, which is negligible compared to the typically longer input and output sequences. Empirical results suggest that P(True) often lacks calibration, leading to overconfidence in incorrect answers (Tian et al., 2023b). So we aim to use supervised training to improve the calibration of P(True), helping LLMs produce more reliable confidence scores."
        },
        {
            "title": "3.2 Training Data Generation",
            "content": "Our goal is to create labeled dataset Dt = (x, y, c)i without human annotations, where (x, y) is queryresponse pair and is an accurate confidence. To achieve this, we first generate multiple candidate answers for each query and ensure diversity via Dynamic Temperature sampling. Next, we calibrate the confidence of each candidate through Soft Self-Consistency, which integrates the models intrinsic probability estimate with the overall agreement among different responses. Soft Self-Consistency Score. Previous work has shown that self-consistency scores provide strong zero-shot calibration (Wang et al., 2024a), outperforming P(True) or raw logits as confidence measures (Guo et al., 2017a). To further enhance the reliability of the confidence score in the training set, we introduce soft self-consistency score, which integrates P(True) with self-consistency and offers more accurate and robust confidence estimation. For each query x, we use the LLM to generate different responses, each with an associated confidence score. Given the set of triplets (x, yn, cn) where 1 , we compute the soft selfconsistency (SSC) score as: SSC(y) = (cid:80) i:yi=y ci (cid:80)N i=1 ci . Using this score, we construct the final training set as (x, yi, SSC(yi)), where SSC(yi) provides calibrated confidence estimation for each response. Dynamic Temperature. To generate more diverse and high-quality responses, we adopt the Entropy-based Dynamic Temperature (EDT) Sampling method (Zhang et al., 2024b) when generating each response y. By adaptively increasing the temperature when the entropy of the output distribution is low, EDT promotes greater response diversity while preserving output quality. Formally, the temperature (H) is defined as: (H) = (cid:40) T0 γ/H , 0, if T0 γ/H τ0, otherwise, where T0 is the base temperature, is scaling factor, γ affects the scale of temperature variations, and τ is threshold that sets the temperature to zero if T0 γ/H is below τ0."
        },
        {
            "title": "3.3 Training Objective",
            "content": "We optimize the models confidence estimation by minimizing the difference between the predicted confidence and the target confidence using the SmoothL1 loss. To ensure that training on confidence estimation does not degrade the models reasoning ability, we incorporate the standard generation loss of Chain-of-Thought answers into the objective (Huang et al., 2022). Specifically, only responses with confidence scores above threshold η are selected for training to guarantee the quality of the reasoning path. weighting coefficient is introduced to balance these two loss terms. The overall loss function is formulated as:"
        },
        {
            "title": "4.1 Early Stopping with Confidence",
            "content": "Early Stopping improves the efficiency of Best-ofN by dynamically terminating the sampling process once response with sufficient confidence is found. Given sequential sampling process where each response yi is assigned confidence score ci, we follow this rule: (cid:40) + 1, = yi, if ci < τ, otherwise. This means that we continue sampling responses one by one until response meets the confidence threshold τ , and such response is selected as the final answer, avoiding unnecessary additional sampling and reducing computational overhead."
        },
        {
            "title": "4.2 Self-Consistency with Confidence",
            "content": "Self-Consistency with Confidence extends the traditional Self-Consistency approach by incorporating confidence scores into the voting process. Instead of treating all sampled responses equally, we assign each response yi confidence score ci, leading to weighted aggregation: = arg max (cid:88) i=1 ci 1( yi = ). This modification ensures that responses with higher confidence contribute more significantly to the final selection, enhancing robustness by prioritizing more reliable predictions."
        },
        {
            "title": "Confidence",
            "content": "Similar to Self-Consistency with Confidence, we use confidence as the weight when calculating the relative frequency in Adaptive Self-Consistency. vk(z) = (cid:88) i=1 ci1(yi = z), ˆrk(z) = vk(z) (cid:80)k i=1 ci ."
        },
        {
            "title": "5 Experiments",
            "content": "Ltotal(θ) = (cid:88) SmoothL1 (cid:16) pθ(Yes xj, yj, I), cj (cid:17)"
        },
        {
            "title": "5.1 Experiment Setup",
            "content": "(xj , yj )D (cid:88) + ω (cid:16) (xi, yi) ci>η log pθ (cid:0)yi (cid:12) (cid:12) xi (cid:1)(cid:17) ."
        },
        {
            "title": "4 Confidence-Guided Test-Time Scaling",
            "content": "Models. To evaluate our self-calibration framework and our efficient test-time scaling methods, we conduct experiments on three open-source LLMs: Llama-8B-3.1-Instruct 2 (Dubey et al., 2024), Qwen2.5-7B-Instruct 3 (Team, 2024) and We then introduce how to incorporate reliable confidence scores obtained from Self-Calibration to existing test-time scaling methods. 2https://huggingface.co/meta-llama/Llama-3. 1-8B-Instruct 3https://huggingface.co/Qwen/Qwen2. 5-7B-Instruct 4 Dataset Metric In-Domain Datasets GSM8K SVAMP ARC_easy ECE AUC ACC ECE AUC ACC ECE AUC ACC Out-of-Domain Datasets ARC_challenge ECE AUC ACC ECE Object Counting AUC ACC MathQA ECE AUC ACC Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct DeepSeek-R1-Distill-1.5B Vanilla Self-Calibration Vanilla Self-Calibration Vanilla Self-Calibration 13.70 72.43 77. 28.03 74.17 72.60 5.45 81.16 87.73 7.01 80.67 80.87 27.85 53.84 60.68 12.55 85.23 44.18 3.79 75.36 80. 10.17 75.79 75.29 5.00 76.89 89.21 6.03 80.41 82.38 9.69 59.47 65.88 8.64 87.21 52.34 87.39 68.61 89. 89.60 75.10 90.48 57.58 66.10 92.11 55.19 64.21 89.37 72.41 68.07 72.41 62.35 72.48 49.85 16.88 82.21 88. 24.49 87.46 92.00 5.62 76.75 92.01 10.11 78.33 89.05 5.82 81.02 74.22 18.92 69.80 64.18 46.66 64.31 73. 30.40 49.33 52.27 20.19 62.89 54.00 11.42 64.07 43.39 47.26 50.39 55.33 13.16 78.89 37.69 40.03 55.57 75. 12.00 71.27 57.48 11.36 66.86 56.74 5.46 65.27 45.77 4.60 67.61 58.13 4.34 66.09 43.21 Table 1: Self-Calibration results across both in-domain and out-of domain datasets on three different models. DeepSeek-R1-Distill-Qwen-1.5B 4 (DeepSeek-AI, 2025). These models represent diverse architectures and training strategies, allowing us to test the adaptability of our methods. Seed Datasets. We construct our training dataset with diverse reasoning datasets, including: ARC_easy (Clark et al., 2018), commonsense QA (Talmor et al., 2019), LogiQA (Liu et al., 2020), GSM8K (Cobbe et al., 2021b), OpenBookQA (Mihaylov et al., 2018), ReClor (Yu et al., 2020), SciQ (Welbl et al., 2017), SVAMP (Patel et al., 2021) and WindGrande (Sakaguchi et al., 2019). For each dataset, we randomly sample 2,000 questions from the training set to construct our training data. Additional details are shown in Appendix E. Evaluation Datasets and Prompts. We evaluate our methods on three benchmark datasets: ARC-Challenge (Clark et al., 2018), Object-Counting (Suzgun et al., 2022) and MathQA (Amini et al., 2019), covering mathematical and commonsense reasoning tasks in both multiple-choice and open-ended formats. ARC-Challenge includes difficult science questions requiring external knowledge and reasoning. Object-Counting focuses on numerical and spatial reasoning by counting objects in various contexts. MathQA tests mathematical problem-solving 4https://huggingface.co/deepseek-ai/ DeepSeek-R1-Distill-Qwen-1.5B across arithmetic, algebra, and calculus. These three datasets are considered out-ofdomain as they differ from the datasets used in training, which we refer as in-domain datasets. To evaluate performance in an in-domain setting, we also use the test sets of GSM8K, SVAMP, and ARC_easy. The system prompt and the task prompt of each dataset are shown in Appendix A. Baseline Methods. In addition to the repeated sampling methods mentioned in Sec. 2, we also include other adaptive test-time scaling methods such as Early-Stopping Self-Consistency (ESC) (Li et al., 2024) and Reasoning-Aware SelfConsistency (RASC) (Wan et al., 2024) for comparison. ESC divides the sampling process into sequential windows and halts further sampling when high-confidence consensus is reached within window. RASC enhances sampling efficiency by dynamically evaluating both the generated answers and their corresponding reasoning paths."
        },
        {
            "title": "5.2 Evaluation on Self-Calibration",
            "content": "Evaluation Metrics. We first evaluate how well our Self-Calibration approach enable models to output accurate confidence estimation. We adopt three standard metrics for evaluating model calibration: Expected Calibration Error (ECE) (Guo et al., 2017b), Area Under the Receiver Operating Characteristic Curve (AUC) (Hendrycks and Gimpel, 2017), and accuracy (ACC) on both in-domain 5 Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct DeepSeek-R1-Distill-1.5B Methods Pass@1 Obj_C. MathQA ARC_C. Obj_C. MathQA ARC_C. Obj_C. MathQA ARC_C. 67. 71.5 82.8 76.8 82.9 88.5 61. 89.9 58.2 91.2 86.3 81.0 81. 81.2 87.1 76.0 76.8 (+0.8) 83.4 (+2.4) 87.4 (+0.3) 80.8 (-0.4) 87.5 (+1.2) 90.5 (-0.7) 70.8 (0.0) 76.8 (+0.8) 83.6 (+2.6) 87.7 (+0.6) 81.2 (0.0) 87.8 (+1.5) 90.8 (-0.4) 70.8 (0.0) 69.2 SC SC w/ Conf.* SC w/ Conf. Best-of-N Early Stopping* 76.8 (+7.6) 83.4 (+2.4) 87.3 (+0.9) 80.8 (+4.0) 87.5 (+0.7) 90.5 (+0.3) 64.8 (+10.8) 91.6 (+1.6) 65.9 (+7.0) Early Stopping 76.8 (+7.6) 83.6 (+2.6) 87.7 (+1.3) 81.2 (+4.4) 87.8 (+1.0) 90.8 (+0.6) 70.8 (+16.8) 91.6 (+1.6) 66.5 (+7.6) ASC ASC w/ Conf.* 74.8 (0.0) 81.6 (+1.6) 86.6 (+0.1) 81.6 (0.0) 86.9 (+0.7) 90.4 (-0.2) 70.4 (0.0) 75.2 (+0.4) 81.9 (+1.9) 86.6 (+0.1) 81.6 (0.0) 87.2 (+1.0) 91.2 (+0.6) 70.4 (0.0) ASC w/ Conf. 76.0 ESC 76.0 RASC 91.6 91.6 (0.0) 64.7 (+0.4) 91.8 (+0.2) 65.1 (+0.8) 91.3 91.4 91.6 91.8 (+0.2) 65.9 (+0.3) 91.8 (+0.2) 66.5 (+0.9) 90. 70.8 70.8 91.0 90.3 81.2 81.2 65.6 65.8 86.3 86.4 81.0 81. 87.1 87.3 76.8 90.2 74.8 86.4 54. 86.5 80.0 81.6 86.2 70.4 86. 65.6 58.9 90.6 64.3 70.8 Table 2: Accuracy comparison of different test-time scaling methods across three language models when the sample budget equals to 16. The evaluation is conducted on three datasets: Obj_C. (Object_Counting), MathQA, and ARC_C. (ARC_Challenge). Sample budget refers to the average number of responses sampled per query. The improvements of confidence-augmented methods over their baselines are shown in parentheses. All methods use the same responses generated by Self-Calibration trained models, while methods marked with * use confidence scores from the vanilla model. The results when the sample budget equals 4 are shown in Appendix B. and out-of-domain datasets. ECE measures the discrepancy between models predicted confidence and its actual accuracy, defined as: ECE = (cid:88) m=1 Bm acc(Bm) conf(Bm) , where is the number of bins, Bm represents the set of samples in the m-th bin, and is the total number of samples. lower ECE value indicates better calibration, meaning the models confidence aligns more closely with its actual correctness. Results. In Table 1, we compare our models trained on Self-Calibration objective with their vanilla base models on multiple in-domain and out-of-domain datasets. Self-Calibration trained models consistently lower the ECE score while generally improve accuracy. On GSM8K, SelfCalibration reduces ECE from 13.70 to 3.79 while improving accuracy from 77.44% to 80.43%. Even in cases where ECE slightly increases, such as ARC_easy for Llama-3.1-8B-Instruct, accuracy still improves from 87.73% to 89.21%. Moreover, the strong results on out-of-domain tasks demonstrate the generalizability of our method, as seen in MathQA, where accuracy improves from 49.85% to 64.18% for Qwen2.5-7B-Instruct. Ablation Study. We conduct an ablation study to investigate the impact of key components in Self-Calibration, including Dynamic Temperature (EDT), Soft Self-Consistency (SSC), and L1smooth loss. Table 3 presents our ablation results on the MathQA and Object Counting datasets. ReMathQA Object Counting Method ECE ACC ECE ACC ours (full) w/o EDT w/o SSC w/o L1-smooth 8.64 9.14 10.85 6.43 52.34 51.44 52.18 50.86 9.69 10.40 16.02 10. 65.88 62.88 61.12 56.48 Table 3: Ablation study results on MathQA and Object Counting in Llama-3.1-8B-Instruct. w/o L1-smooth means using MSE loss instead of L1-smooth. moving the dynamic temperature or the soft selfconsistency score leads to noticeable increases in ECE and/or drops in accuracy. Meanwhile, replacing the L1-smooth objective with MSE achieves slightly lower ECE on MathQA but reduces accuracy on both tasks, suggesting that our chosen loss formulation is more robust overall. These results demonstrate that each module contributes to model calibration and reasoning performance."
        },
        {
            "title": "5.3 Evaluation on Efficient Test-time Scaling",
            "content": "To ensure fair comparison across different test-time scaling methods, we use the same sample budgets for each of them. Sample budget refers to the average number of responses each method samples per query. For dynamic methods such as Early Stopping and ASC w/ Conf., we set internal thresholds so that the actual number of samples collected in practice is close to target budget. To ensure fair comparison, all methods use responses sampled from Self-Calibration trained models. Table 2 shows the accuracy comparison of different methods with sample budget of 16. We observe that SC w/ Conf., Early Stopping, and ASC 6 w/ Conf. consistently outperform their base counterparts. On Llama-3.1-8B-Instruct, SC w/ Conf. surpasses SC on MathQA (81.0 to 83.6), while on DeepSeek-R1-Distill-1.B, Early Stopping outperforms Best-of-N on ARC_challenge (58.9 to 66.5). These results highlight that integrating calibrated confidence enhances test-time scaling with the same sampling budget. We also compare our approach with methods that use uncalibrated confidenc scores from the vanilla model (indicated by *). These methods generally underperform confidence from Self-Calibration trained model, indicating the necessity of confidence calibration. The results when the sample budget equals 4 are shown in Appendix B."
        },
        {
            "title": "Score from Reward Models",
            "content": "We compare our self-generated confidence scores with established open-source reward model approaches. reward model is an additional scoring model used to evaluate the quality of generated responses (Christiano et al., 2017). Deployment of reward model can introduce several limitations: (1) Reward scores are often unbounded or require dataset-specific normalization, thus difficult to apply universal threshold for filtering or reweighting responses; (2) Running an extra reward model increases inference time; and (3) dedicated reward model requires additional GPU memory, and is less efficient for large-scale deployment. For our analysis, we use the following reward models for comparison: for Llama-3.1-Instruct, we use the reward model from RLHFlow 5 (Dong et al., 2024); for Qwen-2.5, we utilize its official Process Reward Model (PRM) 6 (Zhang et al., 2025). For PRM, we use the lowest reward score across all steps. We ensure the size of each reward model matches with their corresponding base models. Table 4 shows that our self-generated confidence scores achieve similar performance to reward model scores across all datasets when using Best-of-N. This means that our method, by generating approximately 10 additional tokens, achieves performance comparable to that of an extra reward model of the same size. 5https://huggingface.co/RLHFlow/Llama3. 1-8B-ORM-Mistral-Data 6https://huggingface.co/Qwen/Qwen2. 5-Math-PRM-7B Model Dataset Reward Confidence Llama Qwen MathQA Object Counting ARC_Challenge MathQA Object Counting ARC_Challenge 82.1 72.6 86.2 87.5 76.6 89.6 84.0 72.0 86.6 86.8 76.4 89.8 Table 4: Accuracy of Best-of-16 on two models (Llama3.1-8B-Instruct and Qwen-2.5-7B-Instruct) on three datasets between self-generated confidence scores and reward scores from additional reward models. Figure 3: Accuracy over varying sample budgets of different inference strategies on MathQA using SelfCalibration trained Qwen-2.5-7B-Instruction. The results of other models and datasets are shown in Appendix D."
        },
        {
            "title": "Sample Budgets",
            "content": "Increasing the sample budget allows for selecting higher-quality outputs but comes at the cost of greater computational expense. To evaluate this trade-off, we compare different methods across multiple sample budgets and visualize their performance trends. As shown in Figure 3, all methods achieve better accuracy as the number of responses increases. Our confidence-guided approaches consistently outperform their original counterparts in most settings. When the sample budget is small, Best-of-N performs better than early stopping because early stopping might stop too soon with low threshold, missing better response."
        },
        {
            "title": "6.3 Can Other Confidence Querying Prompts",
            "content": "Work Well? Since our confidence-based approach was trained using specific confidence querying prompt, we explore whether alternative prompts can achieve similar performance during inference. This analysis is crucial for understanding the robustness of confidence querying prompts different from the training prompt. We evaluate 6 alternative prompts (listed in Ap7 Dataset Method Original New"
        },
        {
            "title": "7.2 Model Calibration",
            "content": "MathQA Obj_C. ARC_C. Early Stopping ASC w/o conf. SC w/o conf. Early Stopping ASC w/o conf. SC w/o conf. Early Stopping ASC w/o conf. SC w/o conf. 81.7 81.9 82.1 67.2 74.8 74.4 86.2 86.6 86.4 81.520.30 81.800.21 81.630.20 70.801.99 74.071.03 73.400.75 86.620.20 86.630.05 86.350. Table 5: Accuracy comparison between the original prompt Is the answer correct? (Yes/No) and 6 alternative confidence querying prompts on three datasets of Llama-3.1B-Instruct-SC. Results are reported as meanstd. We report the detailed results for each alternative prompt respectively in Appendix C.2. pendix C.1) at inference time. Table 5 shows that despite training with specific prompt, other prompts yield comparable performance across all datasets, with only minor variations. This suggests that our confidence querying approach is robust to prompt changes and our training framework improves model calibration rather than overfitting to special prompt."
        },
        {
            "title": "7.1 Test-Time Scaling",
            "content": "Snell et al. (2024b) studied optimal test-time compute allocation to significantly enhance efficiency. Self-Enhanced tree search frameworks (Bi et al., 2024; Lample et al., 2022; Koh et al., 2024) aggregate multiple reasoning paths and employs sparse activation strategies. Beyond that, step-wise verifiers are leveraged to dynamically prune the search tree (Wang et al., 2022a; Li et al., 2022; Lightman et al., 2023a). Additionally, Chen et al. (2024c) developed two-stage elimination-based approach where multiple candidates are iteratively refined through pairwise comparisons. Combining different versions of the same query can also improve the final performance (Huang et al., 2024). Scaling (Chen et al., 2025b; Welleck et al., 2022; Wang et al., 2024b; Chen et al., 2023; Madaan et al., 2023; Aggarwal et al., 2024) that iteratively refines model outputs, leading to improved performance in complex tasks. Muennighoff et al. (2025) proposed s1, simple test-time scaling method that enforces budget constraint on inference length to optimize computational resource utilization. Model calibration aims to align models confidence with its accuracy. LLMs often exhibit overconfidence (Tian et al., 2023b; Chen et al., 2024a; Xiong et al., 2023; Achiam et al., 2023). Prior research has explored scaling-based methods (Deng et al., 2023; Guo et al., 2017b; Zhang et al., 2020) and nonparametric techniques like binning (Zadrozny and Elkan, 2001). More recent work has introduced verbalized confidence, prompting models to directly output confidence scores (Lin et al., 2022). Most studies focus on pre-trained and instruction-tuned LLMs (Lin et al., 2022; Han et al., 2024), others investigate RLHFtrained LLMs and propose calibration through prompting strategies (Xiong et al., 2023; Tian et al., 2023b). Reinforcement learning has also been leveraged for calibration (Xu et al., 2024; Tao et al., 2024), aligning closely with our study. more calibrated reward model can also help model calibration by PPO framework (Leng et al., 2024)."
        },
        {
            "title": "7.3 LLM Verifier",
            "content": "Recently, various LLM verifiers are developed to enhance the reasoning capabilities of LLMs. Our approach is closely related to LLM-based verifiers, as both aim to evaluate whether generated response meets correctness criteria. Lightman et al. (2023b) trained verifiers that assess the correctness of generated solutions, enhancing the selection of accurate responses. LLM-as-a-Judge (Zheng et al., 2023) employs large language models to adjudicate between multiple generated outputs based on learned preferences. Zhang et al. (2024a) trained verifiers using next-token prediction to enhance reasoning performance in large language models. GenRM (Mahan et al., 2024) is an iterative algorithm that trains large language models on selfgenerated reasoning traces to align synthetic preference labels with human judgments."
        },
        {
            "title": "8 Conclusion",
            "content": "We improve the efficiency of test-time scaling methods in LLMs with reliable confidence estimation. Our Self-Calibration enhances LLM confidence estimation in one forward pass, without requiring any labeled data. We then propose efficient testtime scaling by dynamically adjusting sampling strategies based on calibrated confidence scores, such as Early-Stopping for Best-of-N and SelfConsistency with calibrated confidence. Experi8 ments show that our approaches consistently outperform baselines under the same sample budget. Our findings suggest that reliable confidence estimation and dynamic sampling can substantially enhance the effectiveness and efficiency of test-time scaling approaches."
        },
        {
            "title": "Acknowledgment",
            "content": "This research was supported in part by the NVIDIA Academic Grant Program."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. ArXiv preprint, abs/2303.08774. Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. 2023. Lets sample step by step: Adaptiveconsistency for efficient reasoning and coding with llms. In Conference on Empirical Methods in Natural Language Processing. Pranjal Aggarwal, Bryan Parno, and Sean Welleck. 2024. Alphaverus: Bootstrapping formally verified code generation through self-improving translation and treefinement. Afra Amini, Tim Vieira, and Ryan Cotterell. 2024. Variational best-of-n alignment. ArXiv preprint, abs/2407.06057. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proc. of NAACL-HLT, pages 23572367, Minneapolis, Minnesota. Association for Computational Linguistics. Bin Bi et al. 2024. Forest-of-thought: Scaling testtime compute for enhancing llm reasoning. ArXiv preprint, abs/2412.09078. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. ArXiv preprint, abs/2407.21787. Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan Ö. Arik. 2025a. Sets: Leveraging self-verification and self-correction for improved test-time scaling. Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan Ö Arık. 2025b. Sets: Leveraging self-verification and self-correction for improved test-time scaling. ArXiv preprint, abs/2501.19306. Lihu Chen, Alexandre Perez-Lebel, Fabian Suchanek, and Gaël Varoquaux. 2024a. Reconfidencing llms from the grouping loss perspective. ArXiv preprint, abs/2402.04957. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2024b. Do not think that much for 2+3=? on the overthinking of o1-like llms. ArXiv preprint, abs/2412.21187. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023. Teaching large language models to self-debug. Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024c. simple and provable scaling law for the test-time compute of large language models. ArXiv preprint, abs/2411.19477. Paul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 42994307. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv preprint, abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021a. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021b. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168. DeepSeek-AI. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Ailin Deng, Miao Xiong, and Bryan Hooi. 2023. Great models think alike: Improving model reliability via inter-model latent agreement. ArXiv preprint, abs/2305.01481. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. 2024. Rlhf workflow: From reward modeling to online rlhf. ArXiv preprint, abs/2405.07863. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, and etc. 2024. 9 The llama 3 herd of models. abs/2407.21783. ArXiv preprint, Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017a. On calibration of modern neural networks. In Proc. of ICML, volume 70 of Proceedings of Machine Learning Research, pages 13211330. PMLR. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017b. On calibration of modern neural networks. In Proc. of ICML, volume 70 of Proceedings of Machine Learning Research, pages 13211330. PMLR. Haixia Han, Tingyun Li, Shisong Chen, Jie Shi, Chengyu Du, Yanghua Xiao, Jiaqing Liang, and Xin Lin. 2024. Enhancing confidence expression in large language models through learning from past experience. ArXiv preprint, abs/2404.10315. Dan Hendrycks and Kevin Gimpel. 2017. baseline for detecting misclassified and out-of-distribution examples in neural networks. In Proc. of ICLR. OpenReview.net. Chengsong Huang, Langlin Huang, and Jiaxin Huang. 2024. Divide, reweight, and conquer: logit arithmetic approach for in-context learning. ArXiv preprint, abs/2410.10074. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve. ArXiv preprint, abs/2210.11610. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zachary Dodds, Nova Dassarma, Eli TranJohnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, John Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom B. Brown, Jack Clark, Nicholas Joseph, Benjamin Mann, Sam McCandlish, Christopher Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. ArXiv preprint, abs/2207.05221. Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. 2024. Tree search for language model agents. Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner, Aurélien Rodriguez, and Timothée Lacroix. 2022. Hypertree proof search for neural theorem proving. Jixuan Leng, Chengsong Huang, Banghua Zhu, and Jiaxin Huang. 2024. Taming overconfidence in llms: Reward calibration in rlhf. ArXiv preprint, abs/2410.09724. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. Making large language models better reasoners with stepaware verifier. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. 2024. Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning. ArXiv preprint, abs/2401.10480. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023a. Lets verify step by step. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023b. Lets verify step by step. ArXiv preprint, abs/2305.20050. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. ArXiv preprint, abs/2205.14334. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the TwentyNinth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 36223628. ijcai.org. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. Dakota Mahan, Duy Phung, Rafael Rafailov, Chase Blagden, nathan lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. 2024. ArXiv preprint, Generative reward models. abs/2410.12832. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proc. of EMNLP, pages 23812391, Brussels, Belgium. Association for Computational Linguistics. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. ArXiv preprint, abs/2501.19393. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094, Online. Association for Computational Linguistics. 10 Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. ArXiv preprint, abs/2211.05102. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Winogrande. Communications of the ACM, 64:99 106. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024a. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv preprint, abs/2408.03314. Charlie Snell et al. 2024b. Scaling llm test-time compute optimally can be more effective than scaling model parameters. ArXiv preprint, abs/2408.03314. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proc. of NAACL-HLT, pages 4149 4158, Minneapolis, Minnesota. Association for Computational Linguistics. Shuchang Tao, Liuyi Yao, Hanxing Ding, Yuexiang Xie, Qi Cao, Fei Sun, Jinyang Gao, Huawei Shen, and Bolin Ding. 2024. When to trust llms: Aligning confidence with response quality. ArXiv preprint, abs/2404.17287. Qwen Team. 2024. Qwen2.5: party of foundation models. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher D. Manning. 2023a. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. ArXiv preprint, abs/2305.14975. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023b. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. ArXiv preprint, abs/2305.14975. Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. 2024. Reasoning aware self-consistency: Leveraging reasoning paths for efficient llm sampling. Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, and Dong Yu. 2024a. Self-consistency boosts calibration for math reasoning. In Conference on Empirical Methods in Natural Language Processing. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. ArXiv preprint, abs/2203.11171. Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, and Yisen Wang. 2024b. theoretical understanding of self-correction through in-context alignment. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. ArXiv preprint, abs/2201.11903. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2022. Generating sequences by learning to self-correct. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. 2023. Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. ArXiv preprint, abs/2306.13063. Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, and Jing Gao. 2024. Sayself: Teaching llms to express confidence with self-reflective rationales. ArXiv preprint, abs/2405.20974. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. ArXiv preprint, abs/2305.10601. Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: reading comprehension dataset requiring logical reasoning. In Proc. of ICLR. OpenReview.net. Bianca Zadrozny and Charles Elkan. 2001. Obtaining calibrated probability estimates from decision In Proceedtrees and naive bayesian classifiers. ings of the Eighteenth International Conference on Machine Learning (ICML 2001), Williams College, Williamstown, MA, USA, June 28 - July 1, 2001, pages 609616. Morgan Kaufmann. Jize Zhang, Bhavya Kailkhura, and Thomas Yong-Jin Han. 2020. Mix-n-match : Ensemble and compositional methods for uncertainty calibration in deep learning. In Proc. of ICML, volume 119 of Proceedings of Machine Learning Research, pages 11117 11128. PMLR. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024a. Generative verifiers: Reward modeling as next-token prediction. ArXiv preprint, abs/2408.15240. Shimao Zhang, Yu Bao, and Shujian Huang. 2024b. Edt: Improving large language models generation by entropy-based dynamic temperature sampling. ArXiv preprint, abs/2403.14541. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025. The lessons of developing process reward models in mathematical reasoning. ArXiv preprint, abs/2501.07301. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv preprint, abs/2306.05685."
        },
        {
            "title": "A Prompts",
            "content": "A.1 System Prompt Here we show the system prompt to let the model generate responses for Chain-of-Thoughts and format for extracting the final results. question, step-by-step thought the of following For provide explanation process. Use the below for your response. format your demonstrated <Your ```Example Format: detailed Explanation: explanation here, outlining how you arrived at your answer.> <Insert Answer: concise should here, answer include {answer_type} (e.g., {demo})> your which your that Ensure response strictly adheres to this format. Explicitly words the include Explanation:, Answer:. The answer type includes option letter and number. A.2 Dataset Prompts We show the prompts for each dataset in Table 6. All datasets and models are open-sourced."
        },
        {
            "title": "B Full Main Results",
            "content": "Here we show the main results when sample budget = 4 in Table 7. When the sample budget is small, the model has limited opportunities to explore different reasoning paths. In this scenario, output variability is often high, and having an additional confidence signal (as in ASC w/ Conf.) is essential for filtering out noisy or incorrect responses. This confidence-augmented method helps select the most promising candidate under tight sampling constraints. However, when the sample budget increases, the model can generate more candidate solutions, which typically raises the chance of hitting the correct answer. In this setting, Early Stopping approachespecially when coupled with high 12 Dataset gsm8k sciq Query Template Question: {question}n Question: {question}nOptions:n{options_text}n commonsense_qa Question: {question}nOptions:n{options_text}n winogrande openbookqa reclor math_qa Question: {sentence}nOptions:nA. {option1}nB. {option2}n Question: {question}nOptions:n{options_text}n Passage:n{passage}nnQuestion: {question}nnOptions:n{options_text}n Problem: {problem_text}nOptions:n{options_block}n arc_challenge Question: {question}nOptions:n{options_str}n arc_easy Question: {question}nOptions:n{options_str}n logiqa svamp gpqa Article:n{context}nnQuestion: {question}nnOptions:n{options_text}n Question: {Body + Question}n {Question}nOptions:n{options_text}n aqua_rat Question: {question}nOptions:n{options_text}n Table 6: Query templates for each dataset . confidence thresholdcan terminate as soon as it encounters correct reasoning path."
        },
        {
            "title": "Querying Prompts",
            "content": "C.1 Confidence Querying prompts We show the 6 confidence querying prompt we used in Sec. 6.3. I1: Is this the correct answer? I2: Does this answer seem right? I3: Is this the right answer? I4: Is the given answer accurate? I5: Would you say this answer is correct? I6: Is this response correct? C.2 Results of Different Querying Prompts In Table 8, we show the results of different confidence querying prompts for tuned LLama-3.1-8BInstruct."
        },
        {
            "title": "D Results for Different Sample Budgets",
            "content": "Here, we show the performance under different sample budgets of other datasets and models."
        },
        {
            "title": "E Hyperparameters",
            "content": "This section details the hyperparameters used in our experiments. We categorize them into training data generation, training process, and response generation 13 Figure 4: Performance comparison of different inference strategies on ARC_Challenge using SelfCalibration trained Llama-3.1-8B-Instruct. Figure 5: Performance comparison of different inference strategies on Object Counting using SelfCalibration trained Llama-3.1-8B-Instruct. Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct DeepSeek-R1-Distill-1.5B"
        },
        {
            "title": "Methods",
            "content": "Pass@1 Obj_C. MathQA ARC_C. Obj_C. MathQA ARC_C. Obj_C. MathQA ARC_C. 67. 71.5 82.8 76.8 82.9 88.5 61. 89.9 58.2 78.8 78.8 85.7 85. 72.0 72.4 (+0.4) 81.8 (+3.0) 86.4 (+0.7) 78.2 (-0.6) 86.5 (+0.8) 89.8 (-0.2) 60.8 (-3.2) 90.6 (-0.8) 62.6 (-0.4) 72.8 (+0.8) 82.1 (+3.3) 86.4 (+0.7) 78.4 (-0.4) 86.9 (+1.2) 90.3 (+0.3) 64.0 (+0.4) 91.2 (-0.2) 63.2 (+0.2) 67.6 Sample budget = 4 SC SC w/ Conf.* SC w/ Conf. Best-of-N Early Stopping* 65.6 (-2.0) 81.2 (+0.4) 86.1 (-0.3) 76.0 (-0.4) 86.6 (+0.2) 89.6 (-0.2) 55.2 (-0.8) 90.5 (+0.5) 58.8 (-0.2) Early Stopping 67.2 (-0.4) 81.7 (+0.9) 86.2 (-0.2) 78.4 (+2.0) 87.1 (+0.7) 90.1 (+0.3) 56.0 (0.0) 90.6 (+0.6) 59.0 (0.0) ASC ASC w/ Conf.* 73.2 (-1.2) 81.7 (+1.7) 86.5 (0.0) 79.8 (+0.2) 86.9 (+0.7) 90.4 (-0.6) 62.4 (+1.2) 91.6 (+0.3) 64.2 (+0.9) 74.8 (+0.4) 81.9 (+1.9) 86.6 (+0.1) 80.0 (+0.4) 87.2 (+1.0) 90.6 (-0.4) 62.8 (+1.6) 91.6 (+0.3) 64.6 (+1.3) ASC w/ Conf. 72.0 ESC 72.4 RASC 91.2 91.2 78.6 79.0 89.6 89.8 85.8 85. 80.0 80.0 63.0 63.1 58.0 62.6 86.9 86.4 91.4 86. 90.0 86.5 86.2 80.0 86.4 80. 91.3 74.4 76.4 63.6 59.0 89. 63.3 79.6 56.0 91.0 61.2 90. 63.0 Table 7: Accuracy comparison of different test-time scaling methods across three language models. The evaluation is conducted on three datasets: Obj_C. (Object_Counting), MathQA, and ARC_C. (ARC_Challenge). Sample budget refers to the average number of responses sampled per query. The improvements of confidence-augmented methods over their baselines are shown in parentheses. All methods use the same responses generated by Self-Calibration trained models, while methods marked with * use confidence scores from the vanilla model. Dataset Method MathQA Early Stopping ASC w/o conf. SC w/o conf. Early Stopping Object_Counting ASC w/o conf. ARC_challenge SC w/o conf. Early Stopping ASC w/o conf. SC w/o conf. 1 81.7 81.9 81. 70.0 73.6 72.8 86.8 86.7 86.3 2 81.4 81.9 81.4 71.6 73.6 74.0 86.4 86.6 86. 3 81.7 81.8 81.5 69.6 74.4 73.2 86.8 86.6 86.1 4 81.3 81.8 81. 68.0 73.6 72.4 86.5 86.6 86.7 5 81.1 81.4 81.9 73.6 76.0 74.4 86.8 86.7 86. 6 Original 81.9 82.0 81.8 72.0 73.2 73.6 86.4 86.6 86.6 81.7 81.9 82. 67.2 74.8 72.8 86.2 86.6 86.4 Table 8: The results for different confidence querying prompt. Figure 6: Performance comparison of different inference strategies on MathQA using Self-Calibration trained Llama-3.1-8B-Instruct. Figure 7: Performance comparison of different inference strategies on ARC_Challenge using SelfCalibration trained Qwen-2.5-7B-Instruction. with varying proportions of training and evaluation data. Specifically, GSM8K and SVAMP each contribute 15% of the training and evaluation samples. SciQ, CommonsenseQA, Winogrande, OpenBookQA, ReClor, ARC-Easy, and LogiQA each contribute 5% of the training and evaluation samples. During the sample training data selection process, we ensure that the data is evenly distributed across different confidence intervals. This balancing strategy prevents overrepresentation of any specific confidence range, allowing the model to learn from diverse set of samples. By maintaining an equal number of training examples in each confidence bin, we improve the robustness of confidence calibration and reduce potential biases in the learning process. E.3 Response Generation When generating the response, we set the temperature equals to 1.0. Figure 8: Performance comparison of different inference strategies on Object Counting using SelfCalibration trained Qwen-2.5-7B-Instruction. Figure 9: Performance comparison of different inference strategies on MathQA using Self-Calibration trained Qwen-2.5-7B-Instruction. E.1 Training Data Generation When creating the datasets, we set the number of responses for each query = 32. For the parameter in dynamic temperature, we follow the default hyperparameter settings from the original paper: T0 = 0.8, = 0.8, γ = 1.0, and τ0 = 0.001. E.2 Training Process In the training objective, we set the threshold η = 0.75 to filter the response used in generation ability training and the weight = 0.1 to balance two losses. In the training process, we use the AdamW optimizer with learning rate of 5 105. The total number of training samples is set to 100,000, while 1,000 samples are used for evaluation. We employ batch size of 1 with gradient accumulation steps of 64 to simulate larger effective batch size. The model is trained for 1 epoch. For parameter-efficient fine-tuning, we apply LoRA with rank = 32, scaling factor α = 16, and dropout rate of 0.05. In the whole training examples, the ratio of causal language modeling data is 0.7. We train the model on multiple datasets"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "University of Washington",
        "Washington University in St. Louis"
    ]
}