{
    "paper_title": "Robust and Fine-Grained Detection of AI Generated Texts",
    "authors": [
        "Ram Mohan Rao Kadiyala",
        "Siddartha Pullakhandam",
        "Kanwal Mehreen",
        "Drishti Sharma",
        "Siddhant Gupta",
        "Jebish Purbey",
        "Ashay Srivastava",
        "Subhasya TippaReddy",
        "Arvind Reddy Bobbili",
        "Suraj Telugara Chandrashekhar",
        "Modabbir Adeeb",
        "Srinadh Vura",
        "Hamza Farooq"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts."
        },
        {
            "title": "Start",
            "content": "Robust and Fine-Grained Detection of AI Generated Texts Ram Mohan Rao Kadiyala 1,12, Siddartha Pullakhandam 2, Kanwal Mehreen 3,12, Drishti Sharma 3,12, Siddhant Gupta 3,5,12, Jebish Purbey 3,6,12, Ashay Srivastava 4, Subhasya TippaReddy 7, Arvind Reddy Bobbili 8, Suraj Telugara Chandrashekhar 4, Modabbir Adeeb 4, Srinadh Vura 9, Hamza Farooq 1,10,11 1Traversaal.ai 2Vantager 3Cohere for AI Community 4University of Maryland, College Park 5IIT Roorkee 6Pulchowk Campus 7University of South Florida 8University of Houston 9IISc Bangalore 10Stanford University 11University of California, Los Angeles 12M2ai.in Correspondence: contact@rkadiyala.com Resources: Datasets & Models"
        },
        {
            "title": "Abstract",
            "content": "An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces set of models built for the task of token classification which are trained on an extensive collection of humanmachine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce new dataset of over 2.4M such texts mostly coauthored by several popular proprietary LLMs over 23 languages. We also present findings of our models performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have significantly narrowed the gap between machine-generated and human-authored text. As LLMs continue to improve in fluency and coherence, the challenge of reliably detecting AIgenerated content could become increasingly critical. This issue is particularly pressing in domains such as education and online media, where the authenticity of textual material is paramount. While early efforts such as the GLTR (Gehrmann et al., 2019a) provided valuable insights by leveraging statistical methods to differentiate between human and machine text, these methods often lag behind the rapid pace of LLM evolution. Likewise, initiatives aimed at mitigating neural fake news (Zellers et al., 2019a) have made significant strides in addressing the societal implications of AI-generated misinformation. However, as LLMs become more sophisticated, existing detection systems must be re-evaluated and enhanced to maintain their effectiveness. Further, Each domain comes with its version of the issue of detecting machine generated texts. For instance, proprietary LLMs with internet access and better knowledge cutoffs are more likely to be used in domains like academia. Similarly, bad actors might use an open source generators for the task of creating misinformation and deception through machine generated online content as such models can be hosted locally to not leave trail and are more flexible in terms of not denying user requests. Hence, tailoring models and approaches for each specific domain/scenario might be better applicable for practical scenarios. We chose token-classification approach to train model for the task of distinguish writing styles within text if more than one were found. This approach helped us achieve better performance over texts of unseen features (i.e domain, generator, adversarial inputs, non-native speakers texts) as our models were trained to distinguish different styles within text rather than classifying an input text as one of the two classes it was trained on. Further, we explored the findings and results upon testing our models over other benchmarks which consist of texts from unseen domain and generators. We also tested our models over benchmarks which consist of texts with various adversarial inputs and those written by non-native speakers. We feel our findings and datasets can aid in further research into mitigating the harms of AI generated texts. 5 2 0 2 6 1 ] . [ 1 2 5 9 1 1 . 4 0 5 2 : r a"
        },
        {
            "title": "2 Related Works",
            "content": "A major portion of current research in detecting machine-generated content focuses on longer-form writing through binary classification. However, AIgenerated misinformation is more likely to cause harm than its use in academia, making the distinction between AI and human-generated texts on social media platforms critical challenge. Existing methods often struggle with accurately identifying AI-generated content over shorter texts. Moreover, binary classification approaches, which categorize texts as either human or AI-generated (Wang et al., 2024a), (Wang et al., 2024b), (Bhattacharjee et al., 2023), (Zellers et al., 2019b), (Macko et al., 2023), (Ghosal et al., 2023), (Dugan et al., 2024) are less practical in settings where texts could be co-authored by both humans and LLMs. In contrast, binary classification may be more effective for shorter texts commonly found on reviews and social media platforms (Macko et al., 2024a), (Ignat et al., 2024), where content typically consists of one or two sentences. Additionally, some detection works rely on detecting watermarks from AI-generated texts, (Chang et al., 2024), (Dathathri et al., 2024), (Sadasivan et al., 2024), (Zhao et al., 2023) but not all generators utilize watermarking limiting the applicability of such approaches. Few other approaches utilize statistical methods (Mitchell et al., 2023), (Kumarage et al., 2023), (Gehrmann et al., 2019b), (Hans et al., 2024), (Bao et al., 2023), but they can be prone to mis-classification against adversarial methods like rephrasing and humanizing. (Abassy et al., 2024) introduced 4-way classification as entirely human authored, entirely llm authored, human-edited and llm-authored or llm-edited human-authored. An ideal detection system should be capable of identifying AI-generated content from any generator without depending on watermarking, especially since watermarking techniques may not be effective for shorter texts. Further an ideal detector should be robust against adversarial methods. To properly deal with co-authored text cases, token classification approach to detect boundaries (Dugan et al., 2022), (Macko et al., 2024b) between machine authored and human authored portions might be more appropriate. Further in cases of AI usage in scenarios like academic cases, users are likely to use proprietary LLM with better knowledge cutoffs than an open source LLM. Similarly, for AI misuse over social platforms, users are more likely to use open-sourced model due to better flexibility and privacy. Hence, building models and benchmarks with appropriate set of LLMs might be more applicable for practical scenarios. Many proprietary systems struggle at the task of fine-grained detection, further large enough dataset to cover all POS-tag bi-grams of the text boundaries is required for such fine-grained detectors to work well (Kadiyala, 2024). Previous works in the similar direction include (Lee et al., 2022), (Zhang et al., 2024), (Dugan et al., 2023), (Macko et al., 2024b), (Liang et al., 2024) which utilize dataset of limited size and limited number of generators or those less likely to be used, which might not be enough for detector to work well on unseen domains and generators texts."
        },
        {
            "title": "3 Dataset",
            "content": "Our dataset consists of around 2.45M samples. We used 12 different LLMs out of which 9 are popular proprietary LLMs : GPT-o1 (OpenAI, 2024), GPT-4o (etal., 2024), Gemini-1.5-Pro (DeepMind, 2024), Gemini-1.5-Flash, Claude-3.5-Sonnet (Anthropic, 2023), Claude-3.5-Haiku, PerplexitySonar-Large (Perplexity, 2023), Amazon-NovaPro (Intelligence, 2024), Amazon-Nova-Lite. We also included 3 open-source LLMs i.e Aya-23 (Aryabumi et al., 2024), Command-R-Plus (Cohere For AI, 2024), Mistral-large-2411 (Mistral AI, 2024) which produced outputs that are relatively difficult to distinguish from human written texts compared to other similar models in other benchmarks1 as well as our own datasets. The samples range from 30 to 25K words in length with an average length of around 600 words."
        },
        {
            "title": "3.1 Dataset Distribution",
            "content": "The language distribution of the dataset and LLMs used can be seen in Figure 1. Each language-LLM pair has roughly 10000 samples. Among each set of the 10000 samples; training, development and test sets constitute 40%, 10%, 50% respectively. Additionally, among each set of 10000 samples, 10% were Completely human written, another 10% completely machine generated, and the other 80% were human-LLM co-authored i.e few portions of the text are machine generated and the rest are human written. 1https://raid-bench.xyz/ Figure 1: Dataset distribution per each generator and language in our dataset"
        },
        {
            "title": "3.2 Dataset Creation",
            "content": "GPT-4o was used through Azure OpenAI endpoint2. command-r-plus and aya-23 were used through coheres API platform3. Rest of the models were used through open routers4 API. The Rewritten samples were created by providing the generator LLM with the original text and random prompt among writing an alternate version, later update of what happened or rephrased version of the same text. The samples which returned the 2https://azure.microsoft.com/en-us/products/ ai-services/openai-service/ 3https://dashboard.cohere.com/ 4https://openrouter.ai/models exact text or very similar text were once again regenerated. The partially machine generated texts were created by splitting the text at random locations and the generator was asked to finish the text. The split locations were chosen randomly starting from the 30th word to end of text. This was done to provide the LLM with enough context to better work towards text completion."
        },
        {
            "title": "3.3 Original Data Source and Filtering",
            "content": "With goal of training on one domain and testing on every other, we chose to train on old newspapers (HC-Corpora) as it has sufficient number of samples i.e 17.2M for 67 languages of the same domain. We then removed samples which originated after release of gpt-3 to avoid mislabelling of samples in our dataset. Further we sampled texts which were at least 3 sentences or 50 words long. For Chinese and Japanese, we sampled texts which were at least 100 characters long."
        },
        {
            "title": "5.1 Seen Domains & Seen Generators",
            "content": "The results of our models over our datasets test set can be seen in Table 1. The samples from both the data splits are of the same domain and originate from the same set of generators."
        },
        {
            "title": "4 Our System",
            "content": "We have experimented with various multilingual transformer models (He et al., 2023), (Conneau, 2019), (Beltagy et al., 2020) with/without additional LSTM (Hochreiter, 1997) or CRF layers (Zheng et al., 2015) through binary tokenclassification approach. We found that using additional CRF layer produced better results compared to other setups with the same model. All of the transformer models tested have produced nearly identical results over our test set. However, xlmlongformer gave better results over unseen domains and generators texts, and was used in the end given the longer default context length of 16384. The token level predictions by the models were then mapped into word-level predictions. We use the models predictions to separate text portions based on perceived authorship."
        },
        {
            "title": "5 Evaluation and Results",
            "content": "We evaluate the models at 3 levels of granularity : word level, sentence level and overall. For Chinese and Japanese, we performed evaluation at character level instead of word-level. Each domain and user might have different preference towards metrics and evaluation, hence we report 3 metrics at each level of granularity : accuracy, recall and precision. For word level mapping of predictions, in cases where part of word i.e few tokens are classified differently than others, we assigned the same label to the word as its first token. While mapping word level predictions to sentence we used majority voting, and in cases where consensus is not obtained, we assigned the same label as the first word. For evaluation over other benchmarks requiring binary classification of texts as human or machine written, we assign human written label to the text if at least two thirds of the words get classified as human written. We also report several metrics, some of which can be seen in the below tables, rest can be found in Appendix D."
        },
        {
            "title": "5.2 Unseen Domains & Unseen Generators",
            "content": "The models were tested twice over (Wang et al., 2024a): once by training on just 10000 samples of single generator (Aya-23) and again later by training over our complete training data. The benchmark consists of 11,123 samples of peer reviews and student essays (Koike et al., 2024), the generators used were various versions of llama-2 and the samples chat-gpt (earlier version of gpt-4). would hence be from completely unseen domains and generators to our models. The results of both models can be seen in Table 2."
        },
        {
            "title": "5.3 Unseen Domains & Unseen Generators &",
            "content": "Non-Native Speakers The models were tested by training on just 10k samples each from Aya-23 for English and Arabic Separately. The benchmarks samples for Arabic were from (Alfaifi, 2013) and (Zaghouani et al., 2024). The samples for English consist of ETS and IELTS student essays sampled from non-native speakers (Chowdhury et al., 2025). Our models were used for inference directly over these texts and the strings of predicted tokens were then used to for binary classification based on how frequently the perceived authorship changed from human to LLM and vice-versa i.e the number of changes and whether the longest string consists of ones or zeroes. The metrics obtained for each language can be seen in Table 3."
        },
        {
            "title": "5.4 Unseen Domains & Partially Seen\nGenerators & Adversarial Inputs",
            "content": "We have also tested over raid-bench (Dugan et al., 2025) which consists of texts from 11 generators and 8 domains. among them roughly 10% would be from seen domain (news articles) while the rest are unseen by our models. The datasets texts were also created using various sampling strategies (greedy, random, etc.). The texts were also modified to have adversarial methods including homoglyphs, mis-spellings, alternative spellings, article deletion etc. Among the 11 generators used, Gpt4 is one which is similar to the generator whose outputs our model has been trained on (Gpt-4o). Language Partial cases Unchanged cases Rewritten cases Overall 96.44 90.69 86.58 76.28 94.98 79.63 95.31 77.60 96.02 90.23 94.91 74.46 95.28 76.54 94.37 82.21 95.70 80.56 96.34 92.60 96.64 84.92 95.38 80.69 86.13 83.80 95.77 84.13 94.36 88.61 95.94 88.52 94.89 88.51 96.10 78.06 94.02 79.98 94.47 71.60 93.62 83.00 93.53 74.03 89.67 77.99 94.19 81.94 Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Average 97.55 91.40 93.84 94.13 97.68 96.52 95.92 92.08 95.34 97.24 97.19 96.84 92.81 94.74 96.19 92.75 90.29 95.15 95.58 96.69 94.48 96.57 96.65 95.11 97.16 93.13 96.23 96.83 97.32 96.89 96.64 96.25 96.52 97.08 97.20 96.44 92.74 97.29 96.60 96.63 96.46 97.59 96.64 96.38 95.74 95.74 94.41 96.26 Table 1: Word-Level Accuracy (.2f) of the models on the test dataset for each case * Character level evaluations were done instead for Japanese and Chinese Metrics Accuracy Precision Recall F1 Initial Model Final Model 86.51 86.00 91.61 87.16 87.46 92. 89.49 89.63 Table 2: Word level Metrics over Mgtd-bench (.2f) Metrics Accuracy Precision Recall Arabic English 95.9 99.1 96.1 98. 94.5 99.3 95.2 99.0 Table 3: Overall Metrics over ETS essays (.1f) and Figure 5. The texts were classified as machine generated if at least one third of the tokens within the models context length were classified as machine generated. The F1 score obtained with the initial model trained on single generator was 0.63 and the F1 score grew to 0.79 upon being trained on our full dataset. Evaluation was done directly without performing any preprocessing of the texts and neither were our models trained on texts with any of thse adversarial methods."
        },
        {
            "title": "6 Other Observations",
            "content": "However, both of them have different linguistic and stylistic features, similar to how Gpt-4 is different from Gpt-3. We have tested our models performance once again upon being trained on our own full training data. Additionally, we have also performed an error analysis to find out what domains, models, attack strategies and decoding strategies effected the models performance and to what extent. This can be seen in Figure 2, Figure 3, Figure 4 The sentences inside which text authorship switches from human to LLM or vice versa were found to be relatively shorter that the original text portions which they replaced. LLMs may be likely to finish the current sentence earlier than usual to move on to the next sentence in text completion scenarios. The mean length of the original portion and the replaced portions of those sentences for each language and generator can be seen in Table"
        },
        {
            "title": "Arabic\nCzech\nDutch\nEnglish\nFrench\nGerman\nGreek\nHebrew\nHindi\nIndonesian\nItalian\nKorean\nPersian\nPolish\nPortuguese\nRomanian\nRussian\nSpanish\nTurkish\nUkrainian\nVietnamese\nAverage",
            "content": "Length of Original part 17 11 12 15 14 12 15 11 26 11 15 9 17 10 15 14 11 15 10 11 18 13.8 Length of generated part 13 8 10 11 11 9 12 9 12 8 14 7 15 7 11 11 9 12 8 8 14 10.4 Table 4: Median length (words) of original & newly generated parts of the sentences : Language wise"
        },
        {
            "title": "Generator",
            "content": "Amazon-Nova-Pro Amazon-Nova-Lite Aya-23-35B Claude-3.5-Haiku Claude-3.5-Sonnet Command-R-Plus GPT-4o GPT-o1 Gemini-1.5-Pro Gemini-1.5-Flash Mistral-Large-2411 Perplexity-Sonar-large Average Length of original part 14 12 11 18 16 16 12 11 15 9 11 15 13.3 Length of generated part 10 10 10 10 10 10 10 9 10 10 10 11 10 Table 5: Median length (words) of original & newly generated parts of the sentences : Generator wise Figure 2: F1 scores VS text sampling method used and Table 5 respectively. This observation was consistent across all languages and generators with 20-30% reduction and larger reduction in Hindi. For Chinese and Japanese too, we did observe 2030% reduction in character count when comparing the original and replaced portions of the sentence after the text boundary. Although there is good variation in this feature across languages, the mean and medians observed for each language were similar for all the LLMs. This is further elaborated in Appendix A."
        },
        {
            "title": "7 Conclusion",
            "content": "Despite not being trained on the domains or generators, the models built through our approach performed well over several benchmarks as seen in subsection 5.3 and subsection 5.4 over inputs which were from non-native speakers and consist of adversarial methods. Further, one case where many proprietary systems struggle is when the inputs were too short, which our models were able to overcome as seen in Figure 6, which demonstrates our models accuracy over our test set compared to input texts sentence count. Table 6 displays our models performance over English subset of our dataset for each generator. similar trend from subsection 5.4 was observed with models which are likely less instruction-tuned / not instructiontuned tend to produce texts which are harder to distinguish than their alternatives."
        },
        {
            "title": "7.1 Scalability and scope for extension",
            "content": "The original dataset used to train our current models as mentioned in section 3 consists of samples over 60 languages which would cover 70% of the world populations primary language, and all of the languages are supported by existing multilingual transformer models making the process of scaling Figure 3: F1 scores VS the generators texts Figure 4: F1 scores VS each domains texts Figure 5: F1 scores VS adversarial method used in the input texts Figure 6: Accuracy VS length of input texts (sentence count) Generator Amazon-Nova-Pro Amazon-Nova-Lite Aya-23-35B Claude-3.5-Haiku Claude-3.5-Sonnet Command-R-Plus GPT-4o GPT-o1 Gemini-1.5-Flash Gemini-1.5-pro Mistral-Large-2411 Perplexity-Sonar-large Average Accuracy 94.90 95.26 91.75 96.07 95.97 93.92 91.78 96.61 92.34 93.38 93.47 94.91 94.31 Table 6: Word level accuracy (.2f) of our models over our dataset (English) * excluding Chinese and Japanese the work to more languages easier. Despite not being trained on the generators or domains texts, our models were able to perform well on several benchmarks. Even reaching F1 score of 0.79 against adversarial inputs while they were neither trained over them nor pre-processed. Similarly, creation and usage of such large datasets of other domains along with ours might result in robust and better models. We couldnt explore the relation between instruction tuning sample size of LLMs and detectability of their texts due to the proprietary nature of most of the generators we used, but similar study using open-data models could uncover more insights."
        },
        {
            "title": "7.2 Scope for Improvement",
            "content": "As seen in Figure 5, almost none of the adversarial methods affected the models built through our approach other than paraphrasing and homo-glyphs. However homo-glyphs can be pre-processed by mapping them to the actual character they were imitating in the text. This would require large collection of homo-glyph to character mapping set to use for pre-processing. Further, paraphrased samples of various number of iterations being included in the training dataset might lead to further improvements. It is also worth exploring how detectable are texts in cases where multiple generators contribute portion each in human authored text. Other missing adversarial methods that are likely to be used in practical scenarios include usage of proprietary systems that humanize given text in an attempt to evade detection. 7."
        },
        {
            "title": "Ideal Usage",
            "content": "The models were built primarily for human-inthe-loop use cases where the model would try to flag most of the likely machine-generated portions while the flagged content can be validated either through an ensemble of models or human and hence tilt towards higher recall can be observed in the metrics as seen in Table 12."
        },
        {
            "title": "Limitations",
            "content": "Just like any other detector or classifier, no detector can guarantee 100% accuracy and hence the models are not meant to be used directly for decision making but are meant to be used in human-inthe-loop scenarios. Furthermore, the experiments carried out did not include cases of multiple LLMs co-authoring portion each of the same text."
        },
        {
            "title": "References",
            "content": "Mervat Abassy, Kareem Elozeiri, Alexander Aziz, Minh Ngoc Ta, Raj Vardhan Tomar, Bimarsha Adhikari, Saad El Dine Ahmed, Yuxia Wang, Osama Mohammed Afzal, Zhuohan Xie, Jonibek Mansurov, Ekaterina Artemova, Vladislav Mikhailov, Rui Xing, Jiahui Geng, Hasan Iqbal, Zain Muhammad Mujahid, Tarek Mahmoud, Akim Tsvigun, Alham Fikri Aji, Artem Shelmanov, Nizar Habash, Iryna Gurevych, and Preslav Nakov. 2024. Llmdetectaive: tool for fine-grained machine-generated text detection. AYG Alfaifi. 2013. Arabic learner corpus v1: new In Second resource for arabic language research. Workshop on Arabic Corpus Linguistics. Anthropic. 2023. Model card: Claude 3. Technical report, Anthropic. Accessed: 2024-04-27. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. Aya 23: Open weight releases to further multilingual progress. Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. 2023. Fast-detectgpt: Efficient zero-shot detection of machine-generated text via conditional probability curvature. arXiv preprint arXiv:2310.05130. Iz Beltagy, Matthew Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. Amrita Bhattacharjee, Tharindu Kumarage, Raha Moraffah, and Huan Liu. 2023. Conda: Contrastive domain adaptation for ai-generated text detection. Yapei Chang, Kalpesh Krishna, Amir Houmansadr, John Wieting, and Mohit Iyyer. 2024. Postmark: robust blackbox watermark for large language models. arXiv preprint arXiv:2406.14517. Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, and Bhiksha Raj. 2023. Gpt-sentinel: Distinguishing human and chatgpt generated content. arXiv preprint arXiv:2305.07969. Shammur Absar Chowdhury, Hind Almerekhi, Mucahid Kutlu, Kaan Efe Keles, Fatema Ahmad, Tasnim Mohiuddin, George Mikros, and Firoj Alam. 2025. GenAI content detection task 2: AI vs. human academic essay authenticity challenge. In Proceedings of the 1stWorkshop on GenAI Content Detection (GenAIDetect), pages 323333, Abu Dhabi, UAE. International Conference on Computational Linguistics. Cohere For AI. 2024. c4ai-command-r-plus-08-2024 (revision dfda5ab). Conneau. 2019. Unsupervised cross-lingual reparXiv preprint resentation learning at scale. arXiv:1911.02116. Sumanth Dathathri, Abigail See, Sumedh Ghaisas, PoSen Huang, Rob McAdam, Johannes Welbl, Vandana Bachani, Alex Kaskasoli, Robert Stanforth, Tatiana Matejovicova, et al. 2024. Scalable watermarking for identifying large language model outputs. Nature, 634(8035):818823. DeepMind. 2024. Gemini v1.5 report. https: //storage.googleapis.com/deepmind-media/ gemini/gemini_v1_5_report.pdf. 2025-02-08. Accessed: Liam Dugan, Alyssa Hwang, Filip Trhlik, Josh Magnus Ludan, Andrew Zhu, Hainiu Xu, Daphne Ippolito, and Chris Callison-Burch. 2024. Raid: shared benchmark for robust evaluation of machine-generated text detectors. arXiv preprint arXiv:2405.07940. Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, and Chris Callison-Burch. 2022. Real or fake text?: Investigating human ability to detect boundaries between human-written and machinegenerated text. Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, and Chris Callison-Burch. 2023. Real or fake text?: Investigating human ability to detect boundaries between human-written and machinegenerated text. Proceedings of the AAAI Conference on Artificial Intelligence, 37(11):1276312771. Liam Dugan, Andrew Zhu, Firoj Alam, Preslav Nakov, Marianna Apidianaki, and Chris Callison-Burch. 2025. GenAI content detection task 3: Cross-domain machine generated text detection challenge. In Proceedings of the 1stWorkshop on GenAI Content Detection (GenAIDetect), pages 377388, Abu Dhabi, UAE. International Conference on Computational Linguistics. OpenAI etal. 2024. Gpt-4 technical report. Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. 2019a. GLTR: Statistical detection and visualization of generated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 111116, Florence, Italy. Association for Computational Linguistics. Sebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. 2019b. Gltr: Statistical detection and visualization of generated text. arXiv preprint arXiv:1906.04043. Soumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha, and Amrit Singh Bedi. 2023. Towards possibilities & impossibilities of ai-generated text detection: survey. arXiv preprint arXiv:2310.15264. Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2024. Spotting llms with binoculars: Zero-shot detection of machine-generated text. HC-Corpora. Old newspapers. https://www.kaggle. com/datasets/alvations/old-newspapers. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations. Hochreiter. 1997. Long short-term memory. Neural Computation MIT-Press. Oana Ignat, Xiaomeng Xu, and Rada Mihalcea. 2024. Maide-up: Multilingual deception detection of gptgenerated hotel reviews. Amazon Artificial General Intelligence. 2024. The amazon nova family of models: Technical report and model card. Amazon Technical Reports. Ram Mohan Rao Kadiyala. 2024. RKadiyala at SemEval-2024 task 8: Black-box word-level text boundary detection in partially machine generated texts. In Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024), pages 511519, Mexico City, Mexico. Association for Computational Linguistics. Ryuto Koike, Masahiro Kaneko, and Naoaki Okazaki. 2024. Outfox: Llm-generated essay detection through in-context learning with adversarially generated examples. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 2125821266. Tharindu Kumarage, Joshua Garland, Amrita Bhattacharjee, Kirill Trapeznikov, Scott Ruston, and Stylometric detection of aiHuan Liu. 2023. generated text in twitter timelines. arXiv preprint arXiv:2303.03697. Mina Lee, Percy Liang, and Qian Yang. 2022. Coauthor: Designing human-ai collaborative writing dataset for exploring language model capabilities. In Proceedings of the 2022 CHI conference on human factors in computing systems, pages 119. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, et al. 2024. Monitoring ai-modified content at scale: case study on the impact of chatgpt on ai conference peer reviews. arXiv preprint arXiv:2403.07183. Dominik Macko, Jakub Kopal, Robert Moro, and Ivan Srba. 2024a. Multisocial: Multilingual benchmark of machine-generated text detection of social-media texts. Dominik Macko, Robert Moro, Adaku Uchendu, Jason Lucas, Michiharu Yamashita, Matúš Pikuliak, Ivan Srba, Thai Le, Dongwon Lee, Jakub Simko, and Maria Bielikova. 2023. Multitude: Large-scale multilingual machine-generated text detection benchmark. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, page 99609987. Association for Computational Linguistics. Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko, and Maria Bielikova. 2024b. Authorship obfuscation in multilingual machine-generated text detection. arXiv preprint arXiv:2401.07867. Mistral AI. 2024. Mistral large 2407. https:// mistral.ai/en/news/mistral-large-2407. Accessed: 2025-02-08. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher Manning, and Chelsea Finn. 2023. Detectgpt: Zero-shot machine-generated text detection using probability curvature. In International Conference on Machine Learning, pages 2495024962. PMLR. OpenAI. 2024. Openai system card. //cdn.openai.com/o1-system-card.pdf. cessed: 2025-02-08. https: AcPerplexity. 2023. Sonar. https://sonar. perplexity.ai/. Accessed: 2025-02-08. Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2024. Can ai-generated text be reliably detected? Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohammed Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Chenxi Whitehouse, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, and Preslav Nakov. 2024a. Semeval-2024 task 8: Multidomain, multimodel and multilingual machine-generated text detection. Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Osama Mohanned Afzal, Tarek Mahmoud, Giovanni Puccetti, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, and Preslav Nakov. 2024b. M4gtbench: Evaluation benchmark for black-box machinegenerated text detection. Wajdi Zaghouani, Abdelhamid Ahmed, Xiao Zhang, and Lameya Rezk. 2024. Qcaw 1.0: Building qatari In Procorpus of student argumentative writing. ceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 13382 13394. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019a. Defending against neural fake news. Advances in neural information processing systems, 32. Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019b. Defending against neural fake news. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, et al. 2024. Llm-as-acoauthor: Can mixed human-written and machinegenerated text be detected? In Findings of the Association for Computational Linguistics: NAACL 2024, pages 409436. Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Provable robust waterarXiv preprint Yu-Xiang Wang. 2023. marking for ai-generated text. arXiv:2306.17439. Shuai Zheng, Sadeep Jayasumana, Bernardino RomeraParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip HS Torr. 2015. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE international conference on computer vision, pages 15291537. Preand PostBoundary Comparisons The mean and median word counts of the text portions in sentence after the text authorship shifts from human to LLM can be seen in Table 10 and Table 11 in comparison to the texts they replace."
        },
        {
            "title": "B Dataset Creation",
            "content": "The max_new_tokens value specified to the generator during creation of partial cases was randomized between 80% to 200% of the length of the portion that is being replaced. The prompts used for creation of the partial samples and rewritten samples can be seen in Table 7 and Table 8 respectively. continue this text in Language directly : complete this text in Language, respond directly : Table 7: Prompts used in dataset creation : Partial cases Rewrite this in Language different way : Generate an alternative version of this in Language : Generate later update to this in Language : Generate previous version of this in Language ; Table 8: Prompts used in dataset creation : Rewritten cases Hyperparameter Seed (Training) Seed (Shuffling) Number of Epochs Per Device Batch Size (Train) Per Device Batch Size (Eval) Context Length Learning Rate Weight Decay Dropout (CRF Layer) Value 1024 1024 5 12 30 16384 5e-5 0 0.075 Table 9: Training Hyper-parameters used"
        },
        {
            "title": "C Reproducibility",
            "content": "We used multilingual longformer 5 with an additional CRF layer. The hyper-parameters used for training the models can be seen in Table 9. We built separate model for each language, the training was done over A100 SXM over 10h each."
        },
        {
            "title": "D Other Metrics",
            "content": "The metrics over each type of text for each language and LLM separately can be seen in Table 13, Table 14, Table 15, Table 16, Table 17, Table 18, Table 19, Table 20, Table 21, Table 22, Table 23, Table 24."
        },
        {
            "title": "E License",
            "content": "The xlm-longformer model we used was available with an mit license, we are releasing the models and datasets through CC BY-NC 4.06 which permits usage for research purposes. 5https://huggingface.co/hyperonym/ xlm-roberta-longformer-base-16384 6https://creativecommons.org/licenses/by-nc/4. 0/deed.en Language Mean length of Mean length of Median length of Median length of old text portion new text portion Old text portion New text portion"
        },
        {
            "title": "Arabic\nCzech\nDutch\nEnglish\nFrench\nGerman\nGreek\nHebrew\nHindi\nIndonesian\nItalian\nKorean\nPersian\nPolish\nPortuguese\nRomanian\nRussian\nSpanish\nTurkish\nUkrainian\nVietnamese\nAverage",
            "content": "18.73 12.02 13.73 16.04 15.50 13.03 16.74 12.64 40.56 12.44 17.54 9.85 18.83 11.42 16.52 16.30 12.27 17.18 11.81 12.04 20.06 16.19 16.25 9.52 13.39 14.59 13.16 10.89 14.87 10.65 15.42 9.56 16.39 8.08 19.88 8.84 13.29 13.50 10.63 14.81 9.74 10.39 18.01 12.95 17 11 12 15 14 12 15 11 26 11 15 9 17 10 15 14 11 15 10 11 18 13.76 13 8 10 11 11 9 12 9 12 8 14 7 15 7 11 11 9 12 8 8 14 10.43 Table 10: Comparison of replaced and generated text portion lengths (word count) : Language wise Generator"
        },
        {
            "title": "Mean length of Mean length of Median length of Median length of\nold text portion new text portion Old text portion New text portion",
            "content": "Amazon-Nova-Pro Amazon-Nova-Lite Aya-23-35B Claude-3.5-Haiku Claude-3.5-Sonnet Command-R-Plus GPT-4o GPT-o1 Gemini-1.5-Flash Gemini-1.5-pro Mistral-Large-2411 Perplexity-Sonar-large Average 16.02 18.12 13.70 20.19 17.32 16.92 13.49 14.83 19.68 12.50 12.85 17.13 16.06 13.27 12.87 12.87 13.13 12.98 13.28 12.84 12.36 13.44 13.48 12.85 13.45 13.07 12 14 11 18 16 16 12 11 15 9 11 15 13.33 10 10 10 10 10 10 10 9 10 10 10 11 10 Table 11: Comparison of replaced and generated text portion lengths (word count) : Generator wise Language Accuracy Precision Recall F1-score Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Average 96.44 86.58 94.98 95.31 96.02 94.91 95.28 94.37 95.70 96.34 96.64 95.38 86.13 95.77 94.36 95.94 94.89 96.10 94.02 94.47 93.62 93.53 89.67 94.19 92.50 87.03 94.57 93.34 92.34 93.64 94.87 93.69 95.32 89.72 95.61 95.04 85.64 95.29 84.45 96.76 91.92 95.81 86.67 90.02 88.56 86.58 77.23 91.16 97.17 86.46 97.96 97.97 98.44 98.42 98.38 96.51 97.94 96.66 98.29 97.58 94.17 97.69 96.88 97.19 96.07 98.53 97.29 98.14 97.17 97.93 97.44 96.97 94.78 86.75 96.23 95.60 95.29 95.97 96.59 95.08 96.61 93.06 96.93 96.29 89.70 96.48 90.24 96.97 93.95 97.15 91.67 93.90 92.66 91.90 86.17 93.91 Table 12: Word-level Metrics of our models over each language : our test set * Character level evaluations were done instead for Japanese and Chinese Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 97.17 87.00 95.20 95.23 96.60 95.28 95.58 92.96 95.73 96.59 96.60 95.65 96.85 95.39 94.05 95.98 95.28 96.23 94.03 94.97 92.65 93.90 88.74 98.09 91.60 94.36 92.10 98.87 97.14 95.29 82.85 96.26 96.67 95.76 94.70 93.94 93.73 93.22 92.32 96.32 96.64 94.45 96.97 98.17 95.48 97.14 88.10 75.35 80.10 78.00 89.61 72.85 75.73 81.80 70.89 92.82 85.73 80.88 88.57 84.15 89.45 87.52 94.15 76.55 79.10 71.75 82.68 73.81 76.17 97.56 93.70 96.63 95.21 97.77 97.34 96.92 95.65 97.35 96.65 97.27 96.88 97.48 97.68 96.91 96.96 95.28 96.53 96.46 96.92 95.55 95.39 94. Table 13: Case wise accuracies over all languages for each generator : amazon-nova-pro Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 95.88 87.77 96.43 95.19 96.80 96.07 96.76 93.66 96.78 95.38 97.06 95.54 92.78 95.68 95.31 96.63 94.35 96.20 95.10 94.98 93.87 94.24 89.76 95.62 93.57 94.58 92.85 98.73 97.79 95.52 88.01 94.35 95.30 97.18 95.45 93.02 95.42 94.92 93.83 95.09 93.76 96.30 97.15 90.84 99.91 97.03 90.93 76.99 79.40 78.10 89.25 76.28 76.34 79.78 83.84 91.59 85.03 80.81 76.50 82.49 88.48 89.32 87.92 76.44 81.47 71.55 83.99 74.80 78.87 96.56 93.20 97.81 97.07 98.11 97.59 98.01 96.00 98.05 96.49 97.99 96.95 98.07 98.20 97.40 97.55 92.67 97.97 97.22 97.49 96.63 96.74 95. Table 14: Case wise accuracies over all languages for each generator : amazon-nova-lite Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 96.05 82.40 89.24 93.12 93.52 92.72 90.06 91.75 95.32 94.88 92.85 88.87 75.64 94.14 92.98 90.81 92.69 93.17 92.20 91.88 90.77 90.69 88.54 97.76 89.10 87.94 92.99 96.96 95.14 92.53 91.83 91.54 92.96 95.28 88.87 91.21 92.92 95.29 89.15 90.96 95.16 92.12 93.17 98.19 97.57 96.83 92.22 75.66 79.02 79.97 90.49 73.12 77.28 80.69 82.75 93.62 83.55 75.43 78.10 85.46 87.28 86.41 91.17 78.15 79.77 72.87 82.59 73.69 76.08 95.14 85.65 89.75 92.45 93.01 93.28 89.89 92.04 96.71 94.18 90.91 89.21 75.56 95.04 93.81 90.40 92.69 93.65 93.00 91.30 90.19 87.77 87. Table 15: Case wise accuracies over all languages for each generator : Aya-23 Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 97.18 87.75 97.55 96.41 97.76 96.30 97.37 97.46 96.27 97.23 98.05 97.48 97.02 96.93 94.54 97.62 96.52 97.11 95.36 95.91 94.62 93.99 91.85 95.75 86.18 91.56 86.16 98.32 90.15 94.62 82.84 82.61 95.48 94.11 93.49 91.08 94.45 90.00 85.69 82.19 92.11 92.17 93.24 92.86 91.46 92.41 91.63 75.93 80.78 77.57 90.98 74.86 77.23 87.33 83.48 92.35 88.19 81.43 87.97 84.22 89.54 88.75 90.82 78.70 80.44 71.65 81.65 73.52 77.05 98.82 86.51 99.80 99.49 99.37 99.63 99.79 99.90 98.94 98.72 99.55 99.97 98.33 99.40 97.99 99.60 99.17 99.93 99.26 99.31 98.60 99.27 98. Table 16: Case wise accuracies over all languages for each generator : Claude-3.5-Haiku Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 96.58 88.43 97.62 97.30 98.03 97.06 97.33 95.83 96.46 97.63 98.43 98.13 98.04 97.15 96.01 98.20 96.38 97.46 94.55 96.65 95.32 95.62 88.87 100.00 95.14 99.88 99.52 99.76 99.97 99.69 99.62 99.88 99.88 100.00 99.99 99.64 99.87 99.97 99.26 83.74 99.71 99.93 99.92 99.96 99.87 99.92 91.82 78.36 77.22 77.53 90.02 73.66 76.06 80.60 82.69 92.51 84.55 81.13 87.36 83.49 87.92 90.30 89.65 80.36 80.55 71.68 83.13 74.14 77.40 98.63 92.64 99.30 99.30 99.35 99.53 99.52 99.00 97.68 99.12 99.66 99.69 98.59 98.77 98.35 99.00 98.74 99.18 99.33 99.06 98.45 99.06 98. Table 17: Case wise accuracies over all languages for each generator : Claude-3.5-Sonnet Language Arabic Chinese* English French German Italian Japanese* Korean Portuguese Spanish Partial cases Unchanged cases Rewritten cases Overall 87.12 88.90 92.45 89.78 90.23 89.12 87.77 88.56 90.12 90.45 85.34 86.45 90.12 87.21 88.05 87.00 85.88 86.34 88.34 88.12 86 89 91 90 89 89 88 87 89 82 84 88 85 86 86 83 85 87 87 Table 18: Case wise accuracies over all languages for each generator : Command-R-Plus Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 95.01 86.36 91.87 92.58 94.01 90.89 91.97 92.11 91.85 96.16 94.70 92.72 93.84 93.61 93.34 93.73 93.93 93.26 92.61 92.13 91.37 90.33 88.25 96.31 92.51 91.67 90.84 92.84 93.90 93.99 95.67 95.10 96.95 95.88 92.72 94.19 93.09 94.68 89.36 85.07 92.73 97.34 91.87 88.09 96.81 93.32 91.26 77.87 80.96 74.85 90.02 75.18 75.82 81.18 82.51 92.44 84.88 79.95 88.44 84.84 88.32 89.51 88.58 77.44 80.17 69.64 83.80 74.93 77. 95.74 92.63 93.30 94.14 94.94 92.87 93.67 94.22 92.60 96.56 95.08 93.35 93.98 94.44 94.83 94.53 95.50 94.59 92.90 93.54 92.83 91.69 92.10 Table 19: Case wise accuracies over all languages for each generator : GPT-4o Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 97.92 87.30 97.05 97.12 96.91 96.53 97.12 97.05 98.10 97.33 98.45 96.16 78.08 97.09 95.50 97.86 95.18 97.57 95.23 96.12 96.55 96.45 92.24 97.08 86.18 86.30 89.03 94.25 91.74 89.69 85.68 97.33 97.42 99.00 99.10 85.24 83.13 94.43 86.92 83.50 98.37 87.02 92.79 87.33 93.34 91.75 90.46 76.01 80.76 77.47 88.92 76.17 76.66 81.60 83.94 92.12 84.98 80.93 73.63 83.18 87.01 90.29 88.47 76.50 78.13 71.70 82.37 75.07 78. 99.10 95.08 98.84 98.80 99.07 98.77 98.92 98.87 98.97 99.10 98.96 97.04 90.78 99.16 98.72 99.04 98.65 98.77 98.98 98.80 98.94 99.05 98.29 Table 20: Case wise accuracies over all languages for each generator : GPT-o1 Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 94.86 85.09 92.04 94.86 94.64 94.04 95.15 93.52 92.51 95.66 95.92 94.89 87.86 94.66 94.66 94.60 94.28 96.30 92.82 94.09 92.51 91.68 87.71 98.59 94.92 97.15 91.91 97.34 98.72 98.80 98.56 98.50 98.72 98.55 97.03 96.26 99.41 96.54 97.34 94.90 97.53 99.40 99.47 96.67 98.39 99.45 88.40 75.98 78.82 78.10 91.03 76.69 78.67 84.32 69.05 93.52 83.45 76.47 87.53 85.07 89.47 88.07 88.75 75.73 78.53 74.56 82.44 73.96 80. 93.90 89.29 91.43 95.08 96.57 95.92 96.22 92.04 91.90 95.55 96.84 94.70 84.59 94.54 95.68 93.51 95.22 97.31 94.96 95.30 94.82 89.37 91.06 Table 21: Case wise accuracies over all languages for each generator : Gemini-1.5-Pro Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 95.88 87.75 96.67 95.86 95.26 95.59 96.74 98.09 96.82 97.32 97.65 98.28 95.88 97.10 93.87 97.84 95.52 98.49 94.29 94.37 95.47 93.70 91.12 98.00 94.03 99.78 99.39 99.39 99.94 99.58 98.90 99.71 99.62 99.72 99.89 97.35 99.36 95.52 99.27 90.12 99.75 99.07 99.91 99.36 99.67 99.81 89.55 75.81 80.42 78.00 90.02 73.66 74.75 85.16 82.90 93.06 85.64 83.62 88.47 84.40 88.45 87.67 88.12 77.30 81.30 68.98 84.99 75.89 79. 98.55 89.94 98.45 98.22 97.92 98.10 98.71 98.96 97.64 98.17 99.11 98.25 95.71 98.36 96.67 99.01 96.72 99.84 97.62 97.50 97.69 97.46 96.29 Table 22: Case wise accuracies over all languages for each generator : Gemini-1.5-Flash Language Arabic Chinese* Czech Dutch English French German Greek Hebrew Hindi Indonesian Italian Japanese* Korean Persian Polish Portuguese Romanian Russian Spanish Turkish Ukrainian Vietnamese Partial cases Unchanged cases Rewritten cases Overall 95.68 88.07 94.98 94.46 95.91 94.35 94.77 92.69 93.86 95.29 95.55 96.01 96.12 95.02 92.34 95.36 94.69 95.45 93.09 92.76 91.99 93.20 88.25 99.40 97.63 94.73 91.01 99.80 97.22 98.43 96.39 97.30 99.35 96.32 99.30 94.90 97.39 98.55 93.69 93.68 97.42 97.90 98.82 98.47 97.16 98.60 92.00 76.30 78.78 78.10 90.32 74.65 76.14 79.75 83.36 92.00 83.15 82.59 87.46 82.64 89.34 87.13 88.20 77.49 80.58 69.92 80.61 74.58 78. 96.53 95.05 96.99 94.46 96.74 97.06 96.69 95.78 95.40 96.25 96.67 97.34 97.05 96.65 94.75 96.69 96.37 97.22 96.64 95.54 91.99 96.21 92.42 Table 23: Case wise accuracies over all languages for each generator : Mistral-Large-2411 Language English French German Portuguese Spanish Partial cases Unchanged cases Rewritten cases Overall 96.64 93.94 94.29 94.03 93.99 99.71 99.49 99.50 98.17 99.80 91.08 72.58 77.21 89.06 72. 97.10 95.53 94.98 92.66 94.79 Table 24: Case wise accuracies over all languages for each generator : Perplexity-Sonar-Large"
        }
    ],
    "affiliations": [
        "Cohere for AI Community",
        "IISc Bangalore",
        "IIT Roorkee",
        "M2ai.in",
        "Pulchowk Campus",
        "Stanford University",
        "Traversaal.ai",
        "University of California, Los Angeles",
        "University of Houston",
        "University of Maryland, College Park",
        "University of South Florida",
        "Vantager"
    ]
}