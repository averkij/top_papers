{
    "paper_title": "MinMo: A Multimodal Large Language Model for Seamless Voice Interaction",
    "authors": [
        "Qian Chen",
        "Yafeng Chen",
        "Yanni Chen",
        "Mengzhe Chen",
        "Yingda Chen",
        "Chong Deng",
        "Zhihao Du",
        "Ruize Gao",
        "Changfeng Gao",
        "Zhifu Gao",
        "Yabin Li",
        "Xiang Lv",
        "Jiaqing Liu",
        "Haoneng Luo",
        "Bin Ma",
        "Chongjia Ni",
        "Xian Shi",
        "Jialong Tang",
        "Hui Wang",
        "Hao Wang",
        "Wen Wang",
        "Yuxuan Wang",
        "Yunlan Xu",
        "Fan Yu",
        "Zhijie Yan",
        "Yexin Yang",
        "Baosong Yang",
        "Xian Yang",
        "Guanrou Yang",
        "Tianyu Zhao",
        "Qinglin Zhang",
        "Shiliang Zhang",
        "Nan Zhao",
        "Pei Zhang",
        "Chong Zhang",
        "Jinren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large language models (LLMs) and multimodal speech-text models have laid the groundwork for seamless voice interactions, enabling real-time, natural, and human-like conversations. Previous models for voice interactions are categorized as native and aligned. Native models integrate speech and text processing in one framework but struggle with issues like differing sequence lengths and insufficient pre-training. Aligned models maintain text LLM capabilities but are often limited by small datasets and a narrow focus on speech tasks. In this work, we introduce MinMo, a Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction. We address the main limitations of prior aligned multimodal models. We train MinMo through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and a broad range of speech tasks. After the multi-stage training, MinMo achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system. Moreover, we propose a novel and simple voice decoder that outperforms prior models in voice generation. The enhanced instruction-following capabilities of MinMo supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices. For MinMo, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice. The MinMo project web page is https://funaudiollm.github.io/minmo, and the code and models will be released soon."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 2 8 2 6 0 . 1 0 5 2 : r MinMo: Multimodal Large Language Model for Seamless Voice Interaction"
        },
        {
            "title": "FunAudioLLM Team",
            "content": "Tongyi Lab, Alibaba Group FunAudioLLM@list.alibaba-inc.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements of large language models (LLMs) and subsequent multimodal speech-text models have provided promising foundation technologies for achieving seamless voice interactions, that is, real-time, natural, smooth, and human-like voice conversations between the user and the system. Prior works of speech-text multimodal models for voice interactions can be roughly categorized into native and aligned models. Native multimodal models simultaneously model end-to-end understanding and generation of both speech and text with single framework; however, they face the challenges of drastic discrepancy between speech and text sequence lengths, insufficient speech pre-training, and catastrophic forgetting of knowledge of text LLMs. Aligned multimodal models are more successful at maintaining capabilities of text LLMs; yet existing models are usually trained on small-scale speech data, investigated on limited set of speech tasks, and lack systematic exploration of instruction-following In this work, we introduce capabilities for rich and nuanced speaking styles. MinMo, Multimodal Large Language Model with approximately 8B parameters for seamless voice interaction. We address the main limitations of prior aligned multimodal models. We train MinMo through multiple stages of speech-to-text alignment, text-to-speech alignment, speech-to-speech alignment, and duplex interaction alignment, on 1.4 million hours of diverse speech data and broad range of speech tasks. After the multi-stage training, MinMo achieves state-of-the-art performance across various benchmarks for voice comprehension and generation while maintaining the capabilities of text LLMs, and also facilitates full-duplex conversation, that is, simultaneous two-way communication between the user and the system. Moreover, we propose novel and simple voice decoder that outperforms prior models in voice generation. The enhanced instruction-following capabilities of MinMo supports controlling speech generation based on user instructions, with various nuances including emotions, dialects, and speaking rates, and mimicking specific voices. For MinMo, the speech-to-text latency is approximately 100ms, full-duplex latency is approximately 600ms in theory and 800ms in practice. The MinMo project web page is https://funaudiollm.github.io/minmo, and the code and models will be released soon."
        },
        {
            "title": "Introduction",
            "content": "Seamless voice interaction indicates that user experiences real-time, natural, relevant, and human-like spoken conversation with the system. Facilitating seamless voice interaction poses great challenges: (1) the system needs to understand audio accurately and comprehensively, including comprehending the content and also paralinguistic cues in speech (e.g., emotion, prosody) as well Figure 1: Performance comparison between our MinMo(8B parameters) and top-tier speech-text multimodal models, including Moshi(7B) (Defossez et al., 2024), Freeze-Omni(7.5B) (Wang et al., 2024b), GLM-4-Voice(9B) (Zeng et al., 2024), SeamlessM4T Large v2(2.3B) (Communication et al., 2023), NExT-GPT(12.42B) (Wu et al., 2024), speech-to-text model Qwen2-Audio(8B) (Chu et al., 2024), Whisper-large-v3(1.55B) (Radford et al., 2023), and others. We demonstrate capabilities of MinMo on automatic speech recognition (ASR), speech-to-text translation (S2TT), spoken question answering (SQA) encompasses both speech-to-text (S2T) and speech-to-speech (S2S), vocal sound classification (VSC), speech emotion recognition (SER), language identification (LID), age recognition and gender detection. ASR is evaluated using 1-WER%, with Fleurs & Common Voice results are averaged over 10 languages (zh, en, ja, ko, yue, de, fr, ru, es, it). S2TT is evaluated using BLEU, with CoVoST2 results averaged over en2zh, en2ja, zh/ja/de/fr/ru/es/it2en translation directions. SQA is eavaluated using Accuracy. SER is evaluated using Weighted Accuracy. MinMo surpasses the previous SOTA models on all these tasks. as audio events; (2) the system is expected to produce natural and expressive speech response; (3) the system should provide relevant and reasonable response to the user, as an intelligent chatbot; (4) the system is expected to support full-duplex conversation (simultaneous two-way communication), that is, the system listens while speaking and the user is free to interrupt when the system is speaking, then the system either continues the speech, or concedes it, listens to the user, and provides response to the new user query. In recent years, seamless voice interaction systems have gained significant momentum, especially with the advancements in multimodal large language models, such as GPT-4o (Hurst et al., 2024) and Moshi (Defossez et al., 2024). These systems not only produce natural and expressive speech but also understand cues beyond words, including emotional tones and audio events. Current multimodal language models for voice interaction can be categorized into two main categories. The first category includes native multimodal models, such as Moshi (Defossez et al., 2024) and GLM-4-Voice (Zeng et al., 2024). These models typically use decoder-only Transformer as the backbone to simultaneously model understanding and generation of both speech and text modalities within single framework; they usually require pre-training with both speech and text data. These models suffer from two major limitations. Firstly, after speech discretization, speech token sequences are often more than twice the length of text (e.g., 12.5 tokens per second in Moshi). This discrepancy in sequence length poses challenges as model sizes grow, such as the 175B GPT-3 (Brown et al., 2020). Secondly, the scarcity of speech data compared to text leads to highly imbalanced speech-text training data and in turn causes catastrophic forgetting (Wang et al., 2024b). The second category includes aligned multimodal models, integrating voice capabilities while aiming to maintain the capabilities of the existing pre-trained text LLM. This results in intermediate outputs that still contain text, as seen in models such as Llama-Omni (Fang et al., 2024) and Freeze-Omni (Wang et al., 2024b). However, these alignment-based models are typically trained on limited speech data (200K samples for LLaMA-Omni and 120K hours for Freeze-Omni), leading to questions on the impact of larger speech datasets on model capabilities and whether the chat capabilities of the original text-LLM might be compromised. Furthermore, investigation of extensive speech tasks has not been conducted on these models, such as speech translation, emotion recognition, speaker analysis, language identification, and audio event detection. Moreover, these models lack systematic evaluations of instruction-following capabilities for rich and nuanced speaking styles, as well as lacking development and evaluation of full-duplex conversation capabilities, for achieving seamless voice interaction. In this work, we introduce new multimodal large language model MinMo, to address these limitations of existing aligned multimodal models. MinMo is trained on over 1.4 million hours of speech data, encompassing various tasks such as Speech-to-Text, Text-to-Speech, and Speech-to-Speech, as detailed in Table 2. This extensive training enables MinMo to achieve state-of-the-art (SOTA) performance across various benchmarks, as shown in Figure 1. We also apply methods that effectively mitigate catastrophic forgetting of the chat capabilities of the original text-LLM while enhancing voice comprehension and generation after training on such large-scale datasets. We also propose novel voice decoder that balances structural simplicity and competitive voice generation performance. LLaMA-Omni uses non-autoregressive (NAR) streaming Transformer, which takes the output hidden states of the LLM as input and employs connectionist temporal classification (CTC) to predict the discrete speech token sequence of the response. This approach suffers from inferior performance compared to autoregressive speech decoder. Freeze-Omni uses three speech decoders, including NAR prefix speech decoder, NAR speech decoder, and AR speech decoder, which complicates the model structure. Different from both of these strategies, we design an AR streaming Transformer for MinMo, which mixes the output hidden states of the LLM with speech tokens, based on fixed ratio, as shown in Figure 3. Our contributions can be summarized as follows: We propose MinMo, an end-to-end aligned multimodal that gains audio understanding, audio generation, and end-to-end duplex speech interaction capabilities by adapting pre-trained text large language model (LLM) through multi-stage alignment strategy over 1.4 million hours of audio data covering wide range of speech tasks. MinMo achieves state-of-the-art (SOTA) performance on multiple open-source benchmarks, including spoken dialogue, multilingual speech recognition, speech translation, emotion recognition, and speaker analysis. Different from previous multimodal models that often suffer from notable catastrophic forgetting of capabilities of the text LLM and significant performance degradation on text tasks, MinMo has minimal loss in the original capabilities of the text LLM. large model We propose novel alignment method for streaming end-to-end audio generation, by exploring the use of the hidden layer representations of the text model as inputs to the Voice Decoder for aligning the audio output modality. Experimental results demonstrate that our streaming voice decoder effectively balances structural simplicity, low latency, and high voice generation performance, and outperforms previous models. Additionally, while most existing voice interaction systems only support controlling the content of the response, MinMo enhances instruction-following capabilities and enables the generation of speech corresponding to user-specified emotions, dialects, and speaking rates, as well as mimicking specific voices with 98.4% instruction-following accuracy. We develop mechanism that effectively facilitates full-duplex interactions with MinMo. Specifically, we implement full-duplex prediction module that harnesses the text LLMs semantic understanding capabilities to decide whether to continue system response, or concede, listen, and respond to new user query. For MinMo, the speech-to-text latency is approximately 100ms; the full-duplex latency is approximately 600ms in theory and 800ms in practice."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Spoken Dialogue Models variety of speech foundation models have been developed for generic audio understanding, but not systematically explored for voice interaction. 3 (a) An example showcases MinMos capabilities, including speech-to-speech chat, speech-to-text translation, style-controllable speech synthesis, and full duplex interaction. (b) An example showcases MinMos capabilities, including speech-to-speech chat, audio event detection, speaker analysis and speech-to-text translation. Figure 2: Examples demonstrating various capabilities of MinMo. More capabilities of MinMo include the tasks shown in Table 2. For example, Qwen2-Audio (Chu et al., 2024) integrates Whisper speech encoder with pre-trained text LLM and adapts the LLM for speech understanding capabilities through multi-task pre-training and instruction-based supervised fine-tuning. SALMONN (Tang et al., 2024) is another speech-text LLM for generic audio understanding, by integrating separate speech and audio encoders with pre-trained text LLM through Q-Former and adopting LoRA for modality alignment. Since this work aims to develop an end-to-end multimodal model for seamless voice interaction, we focus on comparing MinMo to speech-text models for voice interaction (or called multimodal spoken dialogue models). Contemporaneously or inspired by GPT-4o, there have been active developments of multimodal spoken dialogue models managing to achieve real-time voice conversations with user. (Ji et al., 2024a) provides an in-depth overview of recent spoken dialogue models. Some works support traditional turn-based voice chat (i.e., half-duplex communication), but cannot handle full-duplex voice interaction (i.e., simultaneous two-way communication). These models include collaborative systems and end-to-end frameworks. PSLM (Mitsui et al., 2024) is collaborative system since it replies on ASR to process audio input, which discards paralinguistic information and causes error propagation. PSLM generates speech and text tokens in parallel hence it reduces the speech generation latency; however, it suffers from reduced response quality. Different from the collaborative systems such as PSLM, end-to-end frameworks directly accept audio input and generate audio output. Llama-Omni (Fang et al., 2024) and Mini-Omni (Xie & Wu, 2024) are two recent end-to-end frameworks that have not been trained for full-duplex communication. Llama-Omni integrates Whisper speech encoder, speech adapter, streaming speech decoder, and vocoder with pre-trained text LLM backbone. The speech decoder generates discrete units corresponding to generated text prefix in an NAR manner. The model is trained with two-stage strategy: in the first stage, the speech encoder is frozen, and the speech adapter and LLM are trained autoregressively; in the second stage, the speech encoder, speech adapter, and LLM are frozen and only the speech decoder is trained using the CTC loss. Llama-Omni is evaluated on speech-to-text instruction-following and speech-to-speech instruction-following tasks. Mini-Omni also adopts Whisper encoder and uses adapter for minimal training in order to reserve LLMs capabilities. The model is trained through three stages of modality alignment, adapter training, and multi-modal 4 fine-tuning. Mini-Omni simultaneously generates text and audio tokens, while padding tokens to ensure that the corresponding text tokens are produced first to guide audio token generation. Our MinMo facilitates full-duplex spoken dialogues. Existing full-duplex voice chat systems can also be categorized into collaborative systems and end-to-end models. Among collaborative systems, VITA (Fu et al., 2024) runs two models at the same time, namely, the generation model and the monitoring model, to support full-duplex communication. When the generation model is generating system response, the monitoring model monitors the environment and once it detects effective user interruption, it combines context and provides response to the new user query, while the generation model pauses and switches to the monitoring role. Notably, VITA still relies on an external TTS module to generate speech output. Alternatively, another collaborative system (Wang et al., 2024a) operates with LLM interfacing with an ASR module and streaming TTS module. The system does not require modality alignment; instead, supervised fine-tuning is conducted on pre-trained text LLM with the following paradigm: At each time step, the LLM either processes an input token, or generates text token, or outputs special control token for state transitions between SPEAK and LISTEN. All these tasks are defined as next token prediction on serialized, single-stream view of dialogues. Full-duplex dialogue learning is conducted on data synthesized by GPT-4 to generate dialogues with different types of user interruptions. Notably, with its cascaded architecture, this system suffers from high latency up to 680ms. Among end-to-end full-duplex models, the early work of dGSLM (Nguyen et al., 2022) proposes Siamese architecture to jointly process both audio token streams of user speech and system speech. However, it suffers from several weaknesses: it relies on speech-only training, hence does not leverage capabilities of pre-trained text LLM; it only uses semantic tokens, hence does not sufficiently model acoustic information; it does not support online mode. LSLM (Ma et al., 2024b) uses decoder-only Transformer to generate speaking tokens and streaming SSL encoder to process listening tokens. It introduces an interruption token to stop speaking when detecting turn-taking attempt from the user. However, the model is insufficient in generating reasonable responses. Among the more recent end-to-end full-duplex models, Moshi (Defossez et al., 2024), GLM-4-Voice (Zeng et al., 2024), SyncLM (Veluri et al., 2024), IntrinsicVoice (Zhang et al., 2024b), and Omni-Flatten (Zhang et al., 2024a) are native multimodal models. They simultaneously model understanding and generation of both speech and text modalities within single framework, based on GPT backbone, and require self-supervised autoregressive pre-training using both speech and text data. As discussed in Section 1, these native multimodal models need to tackle the challenges due to significant discrepancy between sequence lengths of speech tokens and text tokens, and also highly imbalanced speech-text training data and the resulting catastrophic forgetting. IntrinsicVoice employs GroupFormer to generate HuBERT tokens from the LLMs hidden states, effectively shortening speech sequences to lengths comparable to text sequences. OmniFlatten utilizes multi-stage progressive post-training strategy that incorporates chunk-based flattened single stream of speech tokens and text tokens to learn full-duplex and text-free speech-to-speech interaction. Different from these native multimodal models, our MinMo is in the category of aligned multimodal models, which also include Llama-Omni, Mini-Omni2 (Xie & Wu, 2024), and Freeze-Omni (Wang et al., 2024b). Aligned multimodal models integrate voice capabilities while aiming to maintain the capabilities of the existing pre-trained text LLM. Mini-Omni2 introduces command-based interruption mechanism for supporting full-duplex conversation; however, it is only evaluated on the ASR task and compared to Whisper, VITA, and Mini-Omni. Freeze-Omni (Wang et al., 2024b) is speech-to-speech model that freezes the pre-trained text LLM to reserve the LLMs capabilities. It supports streaming input speech and generates streaming output speech, uses multi-task training, and conducts chunk-level state prediction for modeling full-duplex voice interaction. Our MinMo differs from these aligned multimodal models in the following ways. We explore training MinMo on much larger speech datasets (1.4 million hours of diverse speech data in contrast to 200K samples for LLaMA-Omni and 120K hours for Freeze-Omni) and on much more extensive speech tasks. MinMo also differs from existing aligned multimodal models with novel speech decoder, enhanced instruction following capabilities, and systematic training and evaluation of full-duplex spoken conversation capabilities. Text Style-Controllable Speech Synthesis distinctive feature of multimodal spoken dialogue models, compared to text-based dialogue models, is their ability to comprehend and generate acoustic information beyond mere textual content. The speech modality not only contains the content but also acoustic information such as emotion, dialect, and speaking rate. An intelligent 5 Figure 3: The overall architecture of MinMo. Table 1 provides detailed descriptions of each module in this diagram. multimodal spoken dialogue model should be able to comprehensively understand the acoustic information in input speech (e.g., emotion) and also ideally generate responses with specified emotions, dialects, and speaking rate, as well as mimicking specific voices, so that the system can achieve deeper level of understanding and response in communication. Collaborative systems, such as ParalinGPT (Lin et al., 2024b), E-Chat (Xue et al., 2024), and Spoken-LLM (Lin et al., 2024a), incorporate paralinguistic features to enhance the understanding of acoustic information such as emotions. These systems can be cascaded with style-controllable Text-to-Speech (TTS) system to generate responses with specific emotion, speaking rate, and volume. Significant progresses have been made in text-style controllable TTS, such as TextrolSpeech (Ji et al., 2024b), PromptTTS (Shimizu et al., 2024), PromptTTS2 (Leng et al., 2024), InstructTTS (Yang In contrast to these collaborative systems, et al., 2024a), and ControlSpeech (Ji et al., 2024c). Moshi (Defossez et al., 2024) uses TTS engine with single actors voice and recorded monologues in over 70 speaking styles to synthesize training data to support understanding and generation of acoustic information in an end-to-end model. GLM-4-Voice (Zeng et al., 2024) employs high-quality, multi-turn spoken dialogues tailored to specific speech style requirements, such as speaking rate, emotion, or dialect, to support style-controllable spoken dialogues. However, to the best of our knowledge, no previous work has demonstrated that aligned multimodal models can support style-controllable voice generation. Contrary to previous claims that aligned multimodal models such as Llama-Omni and Freeze-Omni only allow language models to control the content of speech but not the style and prosody (Zeng et al., 2024), in this work, we propose novel streaming voice decoder for the aligned multimodal model MinMo and find that this decoder enhances instruction-following capabilities and enables MinMo to generate speech corresponding to user-specified emotions, dialects, speaking rates, as well as mimicking specific voices."
        },
        {
            "title": "3 MinMo",
            "content": "3.1 Model Architecture Figure 3 illustrates the model architecture of MinMo. MinMo employs lightweight modality alignment approach on pretrained text LLM. Table 1 provides detailed descriptions of each module in MinMo. The Voice Encoder is initialized with the pretrained SenseVoice-large encoder module (An et al., 2024), which provides robust voice understanding capabilities and supports multilingual speech recognition, emotion recognition, and audio event detection. The Input Projector consists of randomly initialized two-layer Transformer combined with CNN layer for dimensional alignment 6 and downsampling. We use the pretrained Qwen2.5-7B-instruct model (Team, 2024)1 as the pre-trained text LLM, due to its outstanding performance on various benchmarks (Team, 2024). We utilize the streaming audio generation mechanism of CosyVoice 2 (Du et al., 2024b), due to its low latency and competitive speech synthesis performance. For every batch of five text tokens received, we pass these tokens and their corresponding final hidden layer vectors simultaneously to the Output Projector and the Voice Token LM. The Output Projector is single-layer linear module randomly initialized for dimensional alignment. The Voice Token LM uses the pretrained CosyVoice 2 LM module. The Voice Token LM then autoregressively generates fifteen speech tokens, ensuring efficient and seamless audio synthesis. These audio tokens are processed in real time by the Token2wav Synthesizer module to produce the final audio output. The Token2wav Synthesizer comprises pretrained flow-matching model, which converts tokens to mel spectrograms, and pretrained vocoder, which transforms mel spectrograms into waveforms, both sourced from CosyVoice 2. MinMo is fully trained end-to-end using additional hidden embeddings, which facilitate control of speech styles, such as emotion, dialect, and speaking rate, based on user instructions. Details of voice generation are elaborated in Section 3.2. The Full Duplex Predictor module consists of single-layer Transformer and linear softmax output layer, both randomly initialized. This module performs real-time prediction on whether to engage with user commands or temporarily halt the ongoing system broadcast to allow for processing further audio input from the user. Once the Full Duplex Predictor decides that system response is appropriate, MinMo produces text outputs and concurrently generates the audio tokens in token-by-token manner. MinMo has approximately 8 billion parameters in total. The training procedure of MinMo is detailed in Section 3.4. The end-to-end latency from receiving the users audio input to delivering the audio response is approximately 600 ms, when tested on the L20 GPU. 3.2 Streaming Voice Decoder To facilitate natural voice responses for MinMo, we introduce novel voice decoder that transforms textual outputs from an LLM into speech. As illustrated at the top of Figure 3, our voice decoder comprises three components: an output projector, voice token language model (LM), and streaming token-to-wave (token2wav) synthesizer. The output projector aligns the dimensions of the LLM with those of the voice decoder. The hidden states from the LLM contain rich contextual information but are semantically ambiguous; whereas, the sampled text tokens are more precise and consistent with the generated text. Meanwhile, the hidden states of the current round of user input contain explicit instruction information. For every dialog turn, the embeddings of user input, and hidden states of the LLMs last layer output will be concatenated along the feature dimension to form the query embeddings. The query embeddings, and embeddings of five sampled text tokens along with the hidden states of the LLMs last layer output will be concatenated along the sequence dimension and fed into the projector. In this report, the projectors outputs are referred to as semantic vectors, which represent rich and accurate semantic information. Following the output projector, voice token LM is employed to generate speech tokens autoregressively. This LM operates on sequences interleaving text and speech tokens. Specifically, we intermix the semantic vectors and speech tokens in fixed ratio of 5:15, that is, every five semantic vectors are followed by fifteen speech tokens. During training, teacher forcing strategy is applied, and special token is introduced to signal that the next semantic vectors should be concatenated. Once the LLMs textual response is complete and the semantic vectors are exhausted, we insert turn of speech token to signal the voice token LM that subsequent tokens should be speech tokens exclusively. The speech synthesis process concludes when the end of speech token is generated. For reconstructing waveforms from the speech tokens, we utilize an off-the-shelf streaming token2wav synthesizer, as described by Du et al. (2024b). The token2wav synthesizer incorporates chunk-aware flow matching model and mel-to-wave vocoder, capable of synthesizing waveforms in chunks of fifteen tokens. 1https://huggingface.co/Qwen/Qwen2.5-7B-Instruct 7 The theoretical latency of the voice decoder can be computed as follows: Latency = 5dllm + 15dlm + 15dsyn (1) where dllm denotes the computation time for the LLM to generate one text token, dlm denotes the time for the LM to generate one speech token, and dsyn denotes the time for the token2wav synthesizer to generate the waveforms corresponding to each speech token. Module Description"
        },
        {
            "title": "Input Projector",
            "content": "Initialized with the encoder parameters of the pre-trained SenseVoice-Large audio understanding model (An et al., 2024)"
        },
        {
            "title": "Large Language Model",
            "content": "Initialized with Qwen2.5-7B-instruct (Team, 2024) Output Projector Voice Token LM Full Duplex Predictor Linear layer for dimensional transformation Initialized with the LLM of the pre-trained CosyVoice2 (Du et al., 2024b) 1 Transformer layer and 1 linear-softmax output layer, both randomly initialized Number of Parameters 636M 170M 7B 6M 370M 18M Table 1: Descriptions of the modules in MinMo as depicted in Figure 3. MinMo has approximately 8 billion parameters in total. 3.3 Tasks and Training Data Category Specific Tasks Hours Speech-to-Text Text-to-Speech Speech-to-Speech Automatic Speech Recognition (ASR) Speech-to-Text Translation (S2TT) Language Identification (LID) Contextual Bias Speech Recognition Speech Emotion Recognition (SER) Audio Event Detection (AED) Speaker Analysis Spoken Language Smoothing Speech-to-Text Chat Speech Synthesis Instruct Speech Synthesis Speech-to-Speech chat Style-controllable Speech-to-Speech Chat Speech-to-ControlToken Full Duplex Interaction 630k 451k 34k 50k 48k 11k 24k 0.4k 10k 170k 1k 10k 0.1k 4k Table 2: The multitask training data for MinMo. Task specifications can be found in Section 4. The training tasks for MinMo consist of four categories, including Speech-to-Text, Text-to-Speech, Speech-to-Speech, and Speech-to-ControlToken tasks. The specific tasks within each category and their corresponding data scales are presented in Table 2. Speech-to-Text tasks. This category consists of approximately 1.2 million hours of speech-text paired data, including tasks such as automatic speech recognition (ASR), speech-to-text translation 8 (S2TT), language identification (LID), contextual biasing speech recognition, speech emotion recognition (SER), audio event detection (AED), speaker analysis, spoken language smoothing. The training data for these tasks is organized in the ChatML format, illustrated by the following example: Speech-to-text Data Format { \"messages\": [ \"role\": \"system\", \"content\": \"You are helpful assistant.\" \"role\": \"user\", \"content\": \"task_instruction <startofspeech> wav_path <endofspeech>\" \"role\": \"assistant\", \"content\": \"task_output\" { }, { }, { } ] } Here, task instruction corresponds to the natural language descriptions for different speech-to-text tasks. For instance, Speech Transcription may be used for speech recognition tasks, while Translate {SRC LANG} into {TGT LANG} may be used for speech translation tasks. wav path refers to the input audio file path, while task output refers to the output of each task. Text-to-Speech tasks. The data for this category mainly consists of basic speech synthesis data, which is the same data used for training CosyVoice 2. It includes 170,000 hours of text-speech paired data and supports four languages: Chinese, English, Korean, and Japanese. Additionally, there are approximately 1,000 hours of audio generation data controlled by instructions. The instructions are expanded to include natural language descriptions, generated by Qwen-Max2, utilizing human-labeled attributes such as emotion, speaking rate, dialect, and role-playing. User: Please speaking very fast: Today is happy day, full of laughter and joy. Assistant: 7 (Fast speaking rate) Today is happy day, full of laughter and joy. User: Speaking with tone of sadness: miss my dear friend who moved away last month. Assistant: 7 (Sad) miss my dear friend who moved away last month. Table 3: Examples of text-to-speech data controlled by instructions. Speech-to-Speech tasks. The Speech-to-Speech data is primarily sourced through simulation, encompassing approximately 10,000 hours of multi-turn conversational speech and 100 hours of style-controllable multi-turn conversational speech. The method for simulating speech-to-speech chat data is as follows: For text chat data primarily sourced from Alpaca (Taori et al., 2023) and ShareGPT3, we utilize the zero-shot in-context generation method from CosyVoice (Du et al., 2024a) to convert user text into user speech. We fine-tune CosyVoices base model with 2 hours of data from selected speaker to create speech synthesis model for the target speaker, referred to as CosyVoice-SFT. This model synthesizes the assistants speech (i.e., system speech). The advantage of using zero-shot 2https://help.aliyun.com/zh/model-studio/developer-reference/ use-qwen-by-calling-api 3https://sharegpt.com/ 9 in-context generation for user speech synthesis is its ability to ensure diversity in the generated user speech, thereby enhancing the generalizability of MinMo. To address the differences between synthesized and real audio, we select suitable real speech from the ASR data as user speech queries and use the corresponding text as input for Qwen-Max to generate response text, which is then synthesized into assistant speech using the CosyVoice-SFT model. This approach further enhances the models robustness to real user audio inputs. To generate conversational speech that covers different speaking styles, we initially employ Qwen-Max to create rich collection of style-controllable, multi-turn text dialogues. User queries are converted into speech using zero-shot generation by Cosyvoice. Subsequently, we employ Cosyvoice 2 to generate the assistants expressive speech. Specifically, we input the assistants response content along with an instructional prompt into Cosyvoice 2 to synthesize speech in specific styles. Additionally, small, diverse, and preliminary recorded voice corpus is used as prompt speech to synthesize the expressive response speech by zero-shot generation. The former method enhances the diversity of the simulated speech, while the latter more effectively builds the expressiveness of various styles. Speech-to-ControlToken task. The Speech-to-ControlToken data primarily consists of two parts. The first part is extracted from existing real voice interaction data, while the second part is simulated using text dialogue data. Specifically, the existing real voice interaction data includes resources such as Alimeeting (Yu et al., 2022), Fisher (Cieri et al., 2004), and our in-house voice interaction data, total of approximately 3000 hours. The simulated data mainly includes the open-source MOSS dataset (Sun et al., 2024) and spoken dialogues by synthesizing our in-house text dialogue data, yielding about 1000 hours of voice chat data. When constructing duplex training data using these voice interaction data, we apply heuristic rules for automatically annotating duplex labels on the samples, as follows. For assistants turn-taking, the endpoint of the users turn is taken as the starting point of the assistants turn. For users turn-taking, time gap after the assistants turn ends is taken as the starting point of the users turn, where (0.6, 0.42). For users back-channel, we select instances from the voice interaction data when the user (taking one speaker in dialogue as the user) is unable to interrupt the other speaker and treat them as training samples of users back-channels. 3.4 Model Training MinMo is trained progressively through four stages of alignment: (1) Speech-to-Text Alignment, (2) Text-to-Speech Alignment, (3) Speech-to-Speech Alignment, and (4) Duplex Interaction Alignment. Through the four alignment stages, MinMo gains its end-to-end audio comprehension and generation capabilities while retaining the capabilities of the backbone text LLM, achieving low latency and facilitating seamless voice chat experience for the user, similar to GPT-4o. The four stages are detailed as follows. Speech-to-Text Alignment. This first stage aligns the audio modalitys input latent space and the semantic space of pre-trained text LLM using Speech-to-Text data shown in Table 2. This phase includes stepwise updates of the Input Projector and Voice Encoder in Figure 3, as well as updating the text LLM using LoRA. Considering that the Voice Encoder and LLM (Qwen2.5-7B) are pre-trained while the Input Projectors parameters are randomly initialized, we perform pre-alignment training (Pre-align) using subset of the Speech-to-Text data shown in Table 2, updating only the Input Projector. This Pre-align phase effectively prevents the randomly initialized parameters from having large-gradient influences on the pre-trained Voice Encoder at the initial training stage. After Pre-align, we use the full Speech-to-Text data for training both the Input Projector and the Voice Encoder while keeping LLM parameters frozena process called Full-Align. Following Full-Align, instruction fine-tuning (SFT) is conducted using approximately 1.3 million samples covering various tasks. During this stage, LLM is updated using LoRA, enhancing the models ability to follow instructions. The specific data proportions used in the Full-Align and SFT stages are illustrated in Figure 4. The Pre-Align phase uses about 1/10 of the Full-Align data. 10 Figure 4: Detailed training data for the Speech-to-Text Alignment stage. Left: Data distribution for Full-Align training. Right: Data distribution for instruction fine-tuning (SFT). Text-to-Speech Alignment. This second stage aligns the semantic space of the text LLM with the audio modalitys output latent space, using Text-to-Speech data. This phase first trains the Output Projector and then jointly trains the Output Projector and the Voice Token LM while keeping other MinMo parameters frozen. In addition to the basic text-to-speech functionality, we leverage the end-to-end framework to enable MinMo to follow user instructions in voice interactions, delivering more expressive and entertaining audio responses. For instance, user can control the emotion, speaking rate, dialectal accent, or speaker style of the audio output via instructions. Approximately 1,000 hours of Instruct Speech Synthesis data are constructed, formatted as shown in Table 3. Speech-to-Speech Alignment. This third stage continues training of MinMo using about 10,000 hours of paired audio data. Consistent with the Text-to-Speech Alignment phase, we continue updating only the Output Projector and the Voice Token LM. The training data for speech-to-speech alignment includes not only general speech-to-speech dialogues but also audio generation instructions with various settings, such as adopting specific dialects, speaking rates, and emotions for spoken dialogues. We find that even without updating the LLM, just by leveraging embeddings aligned with small-scale instruction dataset (150 hours), the large model can still learn quite effective control capabilities for audio generation. Duplex Interaction Alignment. After completing the first three training stages, MinMo acquires capabilities for audio understanding, audio generation, and half-duplex voice conversation. On this foundation, we further add full-duplex module trained with 4,000 hours of long-form human-human spoken conversation. The Full Duplex Predictor module is exclusively trained during this stage. The Full Duplex Predictor takes the hidden embeddings of the LLM as input to predict whether the model needs to generate response. The Full Duplex Predictor leverages the LLMs inherent semantic understanding capabilities to determine: 1) whether the model should respond to the current user query, and 2) whether the model should stop ongoing audio output to listen to the user query and provide an appropriate response."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate MinMo across multiple benchmarks, as detailed in Table 4. These evaluation benchmarks cover speech recognition and speech translation tasks (multilingual speech recognition, multilingual speech translation, language identification, and contextual biasing speech recognition), speech analysis and understanding tasks (speech emotion recognition, speaker analysis, and audio event understanding), and speech-to-text enhancement tasks (spoken language smoothing, punctuation, and inverse text normalization). Additionally, we evaluate MinMo on voice generation tasks (text-to-speech and instruction-following voice generation) and voice chat tasks (including spoken question answering, spoken dialogue, and full-duplex interaction tasks). 11 Description Dataset Metric Multilingual Speech Recognition Multilingual Speech Translation Language Identification Speech Recognition and Translation Aishell-2 (Du et al., 2018) Wenetspeech (Zhang et al., 2022) Librispeech (Panayotov et al., 2015) Fleurs (Conneau et al., 2023) CommonVoice (Ardila et al., 2019) Fleurs (Conneau et al., 2023) CoVoST2 (Wang et al., 2021) Fleurs (Conneau et al., 2023) Contextual Biasing Speech Recognition Aishell-1-NE Speech Analysis and Understanding CER&WER BLEU Accuracy CER&P&R Speech Emotion Recognition EMOBox (Ma et al., 2024a) UA&WA&F1 Audio Event Understanding Speaker Analysis for Gender and Age Vocal Sound Sound Question AirBench(Yang et al., 2024b) Accuracy Spoken Language Smoothing SWAB (Liu et al., 2025) Speech-to-Text Enhancement Punctuation&ITN In-house testset Voice Generation S-Faithful&S-Formal ChatGPT Score Text-to-Speech Seed-TTS (Anastassiou et al., 2024) WER&CER&NMOS Instruction-following Voice Generation In-house testset Accuracy Spoken Question Answering Spoken Dialogue Full Duplex Voice Chat Web Questions (Berant et al., 2013) Llama Questions (Nachmani et al., 2024) TriviaQA (Joshi et al., 2017) Accuracy AlpacaEval (Li et al., 2023) In-house ChitChat Alimeeting (Yu et al., 2022) Fisher (Cieri et al., 2004) Simulation (Sun et al., 2024) ChatGPT Score Positive F1-Score Table 4: Summary of evaluation benchmarks for MinMo in this report. 4.1 Speech Recognition and Translation Multilingual Speech Recognition We evaluate MinMos speech-to-text transcription capabilities on public test sets in Mandarin, English, Japanese, Korean, and six other languages. These include Aishell-2 (Du et al., 2018), LibriSpeech test clean/other (Panayotov et al., 2015), WenetSpeech (Zhang et al., 2022), Fleurs (Conneau et al., 2023), and Common Voice (Ardila et al., 2019). Table 5 presents the results from different models. For Mandarin (ZH), Japanese (JA), Korean (KO), and Cantonese (YUE), we employ the character error rate (CER) for evaluating transcription performance. For English (EN), German (DE), French (FR), Russian (RU), Spanish (ES), and Italian (IT), the word error rate (WER) is utilized as the evaluation metric. Note that all baseline model results are reproduced and processed using the same procedures as conducted on MinMos results, for fair comparisons. Post-processing is based on modified Whisper normalizer (Radford et al., 2023), with modifications primarily for improved number normalization. In Table 5, the w/ LID column indicates that language identification (LID) information, such as English, Chinese, or Korean, is included as part of the decoding prompt, while the w/o LID column denotes results without the additional LID information. Results in parentheses are directly cited from the papers. As shown in Table 5, MinMo achieves superior ASR performance on most test sets across various languages, compared to Whisper Large v3 (Radford et al., 2023) and 12 Test set Aishell-2 Android Aishell-2 iOS Aishell-2 Mic Wenetspeech test-net Wenetspeech test-meeting Librispeech test-clean Librispeech test-other Fleurs Common Voice Language Whisper Large-v3 w/ LID w/o LID Qwen2-Audio MinMo w/ LID w/o LID w/ LID w/o LID ZH ZH ZH ZH ZH EN EN ZH EN JA KO YUE DE FR RU ES IT Avg. ZH EN JA KO YUE DE FR RU ES IT Avg. 5.14 4.83 4.98 9.72 18.72 2.56 4.34 4.65 4.11 4.23 3.33 7.67 5.05 5.27 5.05 2.9 2.5 4.48 12.4 9.66 10.30 11.74 10.25 5.95 11.22 5.94 4.94 5.77 8.82 4.96 4.76 4.89 9.68 18.54 1.90 3.65 4.75 4.28 4.47 3.20 7.11 5.09 5.16 5.12 2.86 2.41 4.45 12.52 17.91 10.32 5.03 37.71 6.41 11.47 6.48 5.19 6.31 11. 3.02 (2.9) 3.17 (3.0) 3.22 (3.0) 8.14 9.49 1.75 (1.6) 4.03 (3.6) 3.80(7.5) 5.12 10.43 10.57 4.09 10.48 9.36 23.2 7.31 6.74 9.11 6.48 (6.9) 8.82 (8.6) 13.51 17.53 6.01 (5.9) 7.63 9.63 (9.6) 16.82 5.72 6.81 9.90 2.90 3.06 3.15 7.65 8.34 1.74 4.13 4.14 5.03 11.85 22.01 4.07 12.77 11.01 40.4 18.02 8.42 13.77 18.83 9.15 13.58 19.37 6.09 7.93 11.10 22.87 9.14 7.96 12. 2.88 2.73 2.88 6.78 7.44 1.74 3.89 2.95 3.79 3.84 2.92 4.25 5.22 5.15 6.23 3.44 3.48 4.13 6.34 7.92 13.41 6.61 6.35 6.56 8.46 6.97 4.96 6.08 7.37 2.86 2.69 2.91 6.64 7.60 1.64 3.82 3.38 3.70 3.86 2.85 4.22 5.18 5.33 6.18 3.55 3.34 4.16 6.31 8.16 11.04 6.33 6.30 6.61 8.59 7.12 4.99 6.16 7. Table 5: Multilingual speech recognition results from our MinMo and baseline models in terms of word error rate (WER) and character error rate (CER) on Mandarin, English, and multilingual public test sets. Results in parentheses are directly cited from papers. The best result for each test set is boldfaced. Qwen2-Audio (Chu et al., 2024). The w/ LID columns for Whisper Large-v3 and Qwen2-Audio show similar results to those reported in the original papers. Testing on Common Voice with or without LID information as prompt shows significant gap in average error rates for Whisper Large v3 and Qwen2-Audio, indicating that these two models strongly depend on the LID information. In contrast, MinMo demonstrates robust and consistent ASR performance regardless of the presence of the language identification. Multilingual Speech Translation We evaluate speech-to-text translation capabilities on the Fleurs (Conneau et al., 2023) and CoVoST2 (Wang et al., 2021) test sets. On the Fleurs test set, we report results for all translation directions supported by our model; whereas, on the CoVoST2 test set, we only report results for translating from English to other languages (en2xx) and vice versa (xx2en), due to dataset limitations, as it primarily focuses on English-centric translation pairs. As shown in Table 6, our end-to-end MinMo consistently outperforms the cascaded model by pipelining Whisper Large V3 and Qwen2.5-7B-Instruct, in terms of BLEU scores. Compared to other end-to-end baselines, MinMo achieves SOTA performance on ChineseEnglish and JapaneseEnglish translations and top-tier performance on other language pairs. We attribute this strong performance to the extensive speech translation training data (451K hours of S2TT training data as in Table 2) and the powerful audio encoder. Notably, even though we only augment our training data with the CoVoST2 set, excluding the Fleurs set, our model maintains consistent performance across both test sets, indicating high robustness. 13 Test set Fleurs CoVoST2 Language Directions zh2en ja2en ko2en yue2en de2en fr2en ru2en it2en es2en xx2en avg en2zh ja2zh ko2zh yue2zh de2zh fr2zh ru2zh it2zh es2zh xx2zh avg zh2ja en2ja xx2ja avg zh2ko en2ko xx2ko avg en2zh en2ja zh2en ja2en de2en fr2en ru2en it2en es2en CoVoST2 avg Qwen2-Audio SeamlessM4T Large v2 Whisper large-v3 +Qwen2.5-7B-Instruct MinMo 20.50 2.94 6.73 16.16 30.58 29.56 20.09 21.86 21.93 18.93 30.46 7.46 11.63 29.20 28.49 29.03 21.48 26.64 25.53 23.32 16.29 21.10 18.70 8.39 10.74 9.57 45.20 28.76 24.40 20.68 35.20 38.50 40.75 36.30 40.00 34. 22.98 18.23 24.11 19.04 37.00 33.97 30.17 26.50 31.55 27.06 29.81 - - - - - - - - - - 35.24 - - 12.82 - 35.90 39.70 22.18 23.79 39.94 42.10 53.58 39.97 42.94 37. 22.98 20.76 24.01 21.18 37.18 35.46 31.57 27.01 26.80 27.44 38.83 25.57 29.04 32.52 33.38 32.38 31.96 30.37 30.32 31.60 17.36 23.39 20.38 9.97 12.42 11.20 38.86 22.04 20.56 25.49 35.10 35.95 42.58 30.80 38.57 32. 24.71 24.09 25.44 23.87 39.54 36.5 32.71 27.61 27.67 29.13 40.68 25.14 23.39 35.26 35.22 34.31 34.34 33.30 32.05 32.63 19.49 30.26 24.88 17.53 26.06 21.80 46.68 35.10 25.95 28.87 39.91 41.31 48.60 40.62 43.26 38. Table 6: Multilingual speech translation results from MinMo and baseline models on CoVoST2 and Fleurs test sets, in terms of BLEU score (higher is better). Results marked with are obtained using translation prompts that specify the source language, rather than only indicating the target language. Results not reported in the original papers have been reproduced by the authors of this work. 14 Language Identification For evaluation of language identification performance, we use the Fleurs dataset, which covers 102 languages. MinMo achieves language identification accuracy of 85.3%, outperforming all previous models shown in Table 7. Specifically, zero-shot Whisper-V3 often miscategorizes Cantonese as Chinese while MinMo accurately identifies Cantonese. The accuracy of Zero-shot Whisper is not competitive due to the fact that it misses training data for 20 languages in the Fleurs dataset; whereas, the LID training data of MinMo covers all 102 languages in Fleurs. Fleurs(102) w2v-bert-51 (0.6B) mSLAM-CTC (2B) Zero-shot Whisper"
        },
        {
            "title": "MinMo",
            "content": "71.4 77.7 64.5 85.3 Table 7: Language identification results from MinMo and baseline models on the Fleurs data set (covering 102 languages), in terms of Accuracy. Contextual Biasing Speech Recognition Contextual biasing, or hotword customization, allows users to obtain customized ASR results with specific contexts or hotwords. MinMo enhances ASR capabilities by integrating advanced prompts for contextual biasing. We prepare corresponding training data for alignment and SFT stages, by organizing hotwords within prompts preceding speech processing instructions, to enable effective customization. Evaluations include hotwords biasing test and general biasing test, as shown in Table 8. The hotwords biasing test involves three data sets used by SeACo-Paraformer (Shi et al., 2024), which contain hotwords for biasing evaluation. The general biasing test uses data sets with fewer hotwords to assess resistance to irrelevant ones. Hotwords Biasing Test utt. Test Name Test-Commercial 2000 Dev-Aishell1-NE 1334 808 Test-Aishell1-NE hotwords prop. 100% 100% 100% hotwords(hc) 693(72) 600(371) 400(226) General Biasing Test Cor-Chinese Cor-English 1749 1211 5.2% 28.8% 87 106 Table 8: Statistics of the test sets for evaluating contextual biasing speech recognition. (hc denotes hard-case hotwords, which are hotwords with recall rates under 40% in basic speech recognition. We compare MinMo with the baseline model SeACo-Paraformer (Shi et al., 2024), which is an open-source ASR system with strong Chinese hotword biasing performance. Table 9 shows that MinMo outperforms the competitive baseline SeACo-Paraformer in terms of ASR accuracy (both with and without hotwords) and also recall rates of hard-case hotwords. Table 10 further demonstrates that MinMo achieves contextual biasing capability in multiple languages, without compromising its general ASR performance. SeACo-Paraformer MinMo w/o bias w/ bias w/o bias w/ bias CER R-hc CER R-hc CER R-hc CER R-hc Test Commercial Dev-Aishell1-NE Test-Aishell1-NE 3.65 5.47 5.55 90.0 54.0 56.0 11.0 3.0 3.0 3.10 1.98 2.27 97.0 94.0 94. 68.0 84.0 87.0 2.63 2.32 2.41 92.3 70.8 68.3 58.3 56.5 49.3 2.45 1.16 1.19 97.1 93.5 91. 84.9 90.5 86.4 Table 9: Performance of contextual biasing speech recognition from MinMo and the baseline model on the hotwords biasing test sets, in terms of character error rate (CER), recall of hotwords (R), and recall of hard-case hotwords (R-hc). 15 w/o bias w/ bias Correct-Chinese Correct-English CER 12.92 18.68 89.2 85.3 15.9 46.2 CER 12.64 18.48 92.1 87.4 69.0 53. Table 10: Performance of contextual biasing speech recognition from MinMo on the general biasing test sets, in terms of character error rate (CER), precision (P), and recall (R). 4.2 Speech Analysis and Understanding Speech Emotion Recognition We evaluate the Speech Emotion Recognition (SER) capability of MinMo using seven widely used emotion recognition datasets from EmoBox, including CREMA-D (Cao et al., 2014), MELD (Poria et al., 2019), IEMOCAP (Busso et al., 2008), MSP-Podcast (Martinez-Lucas et al., 2020), CASIA (Zhang & Jia, 2008), MER2023 (Lian et al., 2023), and ESD (Zhou et al., 2021). These datasets include both Chinese and English languages and scenarios such as acting, TV dramas, and daily conversations. We adopt unweighted average accuracy (UA), weighted average accuracy (WA), and macro F1 Score (F1) as evaluation metrics. Results on these test sets from the recent SER toolkit EmoBox (Ma et al., 2024a) are cited. We also evaluate the baseline audio-LLM models SALMONN and Qwen-Audio using their released model checkpoints. As shown in Table 11, MinMo outperforms the baseline audio-LLM models on most datasets, particularly achieving nearly 100% accuracy on acting audio datasets (CASIA, CREMA-D, ESD). It is important to point out that the comparison between MinMo and the baseline models SALMONN and Qwen-Audio on the aforementioned datasets is inconclusive, since the optimal prompts and post-processing methods for the baseline models are unclear. Therefore, we further utilize the Air-Bench benchmark, which is specifically designed for evaluating large audio language models with standardized post-processing and scoring scripts for fair comparison. As shown in Table 12, MinMo outperforms all the baseline models on all tasks on this benchmark, including Language ID, Gender, Age, Emotion, Vocal Sound classification tasks, except for being outperformed by Qwen-Audio on the sound question classification task. In addition to Chinese and English languages, we also evaluate MinMo on low-resource languages in zero-shot setting, as shown in Table 13. All utterances and reference labels are derived from the EmoBox benchmark, which provides the official training and evaluation partitions for the 32 publicly available SER datasets in 14 languages. EmoBox evaluates the SER capabilities of 10 different pre-trained models across all datasets, with the classification results used as reference labels in Table 13. Despite the absence of in-domain audio used in training, MinMo achieves the best F1 score on most languages, even for those languages not included in the original training data of MinMo. These results highlight MinMos excellent cross-lingual generalization capability. Test Set CASIA CREMA-D ESD IEMOCAP MELD MSPPodcast MinMo SALMONN Qwen-Audio EmoBox UA WA UA WA F1 UA WA F1 UA WA 98.1 94.8 99.9 74.9 61.0 66.4 98.1 94.8 99.9 75.7 65.1 74.5 98.1 94.8 99.9 74.2 54.0 62.5 35.9 49.7 33.9 59.0 39.2 40.1 35.6 50.2 34.5 60.6 47.2 58.0 33.8 43.7 31.7 59.0 39.6 40. 40.0 82.2 47.3 69.6 49.9 57.9 38.4 83.8 47.4 67.6 56.8 70.0 36.0 83.2 43.6 62.9 46.8 54.6 59.6 76.8 84.6 73.5 31.5 21.4 59.6 76.5 84.6 72.9 51.9 43.4 56.3 76.6 84.3 73.1 32.9 21. Table 11: Speech emotion recognition performance from MinMo and the baseline models on various evaluation benchmarks, in terms of unweighted average accuracy (UA), weighted average accuracy (WA), and macro F1 score (F1). Results for SALMONN and Qwen-Audio are reproduced by the authors of this work. Audio Event Understanding We compare MinMos voice and audio event understanding capabilities against other Audio-LLM models, using the Air-Bench benchmark. The results are shown in Table 12. On the voice sound classification task (Vocal Sound), MinMo surpasses all 16 Task MinMo Language ID Gender Age Emotion Vocal Sound Sound Question 99.2% 86.7% 70.1% 64.5% 93.0% 50.3% Qwen-Audio Turbo 95.9% 82.5% 58.8% 60.0% 78.1% 62.8% Qwen-Audio Pandagpt SALMONN Next-gpt 92.8% 67.2% 36.0% 43.2% 84.9% 64.6% 34.6% 66.5% 42.5% 26.0% 31.6% 48.7% 28.1% 35.5% 48.7% 29.9% 45.3% 28.4% 23.7% 57.0% 62.4% 25.7% 23.5% 18.8% Table 12: Performance comparison between MinMo and the baseline models on the AIR-Bench benchmark, including Language ID, Gender, Age, Emotion, Vocal Sound, and Sound Question classification tasks, in terms of Accuracy. Results of Qwen-Audio and Qwen-Audio-Turbo (Chu et al., 2023), Pandagpt (Su et al., 2023), SALMONN (Tang et al., 2024) and Next-gpt (Wu et al., 2024) are cited from the official AIR-Bench website. Model UA WA WF1 UA WA WF1 UA WA WF1 UA WA WF AESDD (el) CAFE (fr) RESD (ru) ASED (am) wavlm-large data2vec2.0-large whisper-large-v3 MinMo 78.9 72.2 79.1 96. 67.6 72.3 79.1 96.5 67.6 71.8 79.1 96.2 62.2 59.0 69.4 94.9 61.3 58.0 68.8 94.8 61.1 57.5 68.0 94.9 55.8 44.0 54.9 76. 56.4 44.6 55.5 78.6 55.8 44.2 54.9 76.3 Model EmoDB (de) EMOVO (it) MESD (es) wavlm-large data2vec2.0-large whisper-large-v3 MinMo 92.5 79.3 91.2 98.4 92.6 80.4 92.4 98.7 92.5 79.9 91.8 98.7 48.8 45.8 57.8 81.7 48.8 45.8 57.8 81. 44.1 43.6 56.0 68.3 62.5 48.4 69.7 89.5 62.4 48.4 69.6 89.1 62.3 46.8 69.6 88.6 96.4 94.3 96.7 75.3 79.2 74.0 83.2 58. 96.4 94.2 96.7 75.2 96.4 94.2 96.7 73.7 Polish (pl) 79.2 74.0 83.2 58.1 79.0 74.0 82.7 55.9 Model SUBESCO (bn) ShEMO (fa) URDU (ur) TurEV-DB (tr) wavlm-large data2vec2.0-large whisper-large-v3 MinMo 65.3 66.4 73.0 80. 65.3 66.4 73.0 80.5 65.0 66.2 72.9 72.8 71.7 64.0 80.2 70.9 87.1 82.6 89.5 84.0 73.5 68.4 82.9 72.7 86.6 78.1 82.5 82. 86.6 78.1 82.5 82.1 86.6 78.1 82.4 79.4 79.5 63.7 81.3 82.0 80.0 64.1 81.5 82.1 79.5 63.0 81.3 81.6 Table 13: SER performance of MinMo and the baseline models on the multi-languages datasets from EmoBox, in terms of unweighted average accuracy (UA), weighted average accuracy (WA), and macro F1 score (F1). baseline models. However, we find that on more complex sound question-answering tasks, MinMo performs worse than Qwen-Audio although still outperforming other models. This can be attributed to two factors: first, with the voice encoder and the training paradigm, MinMo is primarily designed for voice interaction, hence some sound questions may exceed its scope; second, during evaluation, MinMo predicts what happens in the audio rather than strictly choosing the options provided by the Air-Bench, hence some correct or similar-to-correct responses generated by MinMo are aligned with incorrect choices by the post processing script. Speaker Analysis Speaker analysis involves several tasks that are essential for understanding and interacting with audio data, including gender detection, age estimation, speaker counting, speaker identification, multi-speaker recognition, and target speaker recognition. In this report, we focus on evaluating MinMos performance in gender detection and age estimation. Table 12 compares MinMo against the baseline models on these tasks on the AIR-Bench benchmark, in terms of classification accuracy. The results reveal that MinMo outperforms all the baseline models on gender detection and age estimation tasks. 4.3 Speech-to-Text Enhancement Spoken Language Smoothing The spoken language smoothing task takes the ASR transcripts of spoken language, and outputs formal-style written text. Examples of spoken language smoothing 17 are shown in Table 14. For this task, we construct multi-domain dataset for training and evaluation, by extending the SWAB dataset (Liu et al., 2025) that we create for spoken-to-written conversion of ASR transcripts. The SWAB dataset is derived from Chinese and English meetings, podcasts, and lectures. After the generation of ASR transcripts for the original videos and audios, approximately ten annotators create formal-style written text based on the ASR transcripts while preserving their original content. The training set of SWAB comprises 20,000 paragraphs, and the test set includes 100 randomly sampled paragraphs in both Chinese and English. We conduct full fine-tuning and compare MinMo with Qwen2.5-7B-based model on the SWAB test set, with results shown in Table 15. For objective metrics, we calculate BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BLEURT (Sellam et al., 2020) with the human target as reference. Notably, we observe that the spoken language smoothing task shows significant subjectivity and diversity; therefore, objective metrics based on lexical matching may not adequately reflect the performance. Consequently, we use human and LLM annotations to provide rankings of faithfulness (S-Faithful (i.e., faithfulness to the original content) and formality (S-Formal). The prompts for automated LLM scoring are presented in Appendix A.1. Table 15 shows that the performance of our model and Qwen2.5-7B is comparable, suggesting that MinMo possesses reasonable capability to smooth spoken language. Source Target You have more voices producing in the moment. eh? You have more 1, eh, more one voice, eh, eh, produced in the in the moment. Source so what, what do they need to do left? Target What tasks do they need to complete? Source Target Well, think you two, especially you and, and Daniel, you both had, the less creative, roles and the project. Thats true. Of course. believe that you two, especially you and Daniel, had the less creative roles in the project. Table 14: Examples of the source and the prediction of MinMo for the spoken language smoothing task, sampled from the SWAB test set. Model CER BLEU ROUGE-L BLEURT S-Faithful S-Formal Human LLM Human LLM Qwen2.5-7B MinMo 0.75 0.72 21.06 22.31 42.01 41.42 64.61 61.50 9.04 8. 8.06 7.59 9.80 9.72 7.63 7.52 Table 15: Spoken language smoothing performance of MinMo and Qwen2.5-7B on the SWAB test set, in terms of objective metrics (CER, BLEU, ROUGE-L, and BLEURT) Model PUNC ITN Fleurs-zh Fleurs-en Fleurs-zh Fleurs-en SenseVoice-L whisper-large-v3 MinMo 2.49 1.23 2.65 1.40 2.49 2.58 2.48 1.39 2.61 1.48 2.43 2.57 Table 16: GPT-4 Turbo ranking scores of punctuation insertion and ITN for MinMo and the baseline models on the Chinese and English subsets of the Fleurs dataset. Punctuation Insertion and Inverse Text Normalization For the punctuation insertion (PUNC) and Inverse Text Normalization (ITN) tasks, we use the Chinese and English data from the Fleurs dataset. We compare MinMo against SenseVoice-L and whisper-large-v3, as shown in Table 16. Given the subjectivity of the punctuation insertion and ITN tasks, we employ GPT-4 Turbo to rank the three outcomes for evaluation. The task prompt for automated scoring is available in Appendix A.2. The first place receives 3 points, the second place 2 points, and the third place 1 point. The final score is the average of all scores. When preparing the test data, we use randomized option shuffling and multiple scoring rounds to reduce uncertainty when using ChatGPT for evaluation. 18 The final results demonstrate that MinMo performs better in the subjective evaluations of punctuation insertion and ITN. 4.4 Voice Generation Text-to-Speech (TTS) To evaluate the synthesis accuracy of our voice decoder, we converted the recent SEED test set (Anastassiou et al., 2024) into the ChatLM format. In this format, the text is presented as the user content prefixed with Copy: command, and the LLM is expected to replicate this text. The test set comprises 2,020 cases in Chinese and 1,088 cases in English. For the Chinese cases, we utilized the Paraformer-zh model (Gao et al., 2022), while the English cases were processed using Whisper-large V3 (Radford et al., 2023). Given the instruction non-following issue with LLMs, we applied teacher forcing scheme during inference to minimize discrepancies between the input and output text. The content consistency of the voice decoder was evaluated using CER for Chinese and WER for English. Our findings indicate that even with the teacher forcing scheme, only about 20% of the test cases had identical input and output text from the LLM. Because inconsistent input and output can lead to confused hidden states for the voice decoder, only test cases with consistent input-output text were included for error rate calculation. The results are presented in Table 17. We observed that MinMos voice decoder has slightly reduced content consistency and speech quality on the Chinese test sets compared to the TTS baseline, CosyVoice 2.0-SFT (Du et al., 2024b). On the English test set, MinMo achieves similar content consistency but with slightly lower NMOS score. This reduction can be attributed to the differing acoustic characteristics of the fine-tuned speakers, which affect both the recognition model and NMOS scorer. However, this reduction does not significantly hinder human understanding. Therefore, subjective evaluation might be more appropriate for speech-to-speech voice chat models, which will be explored in our future work. Model zh en CER NMOS WER NMOS CosyVoice 2.0-SFT MinMo 2.06 2.48 3.73 3. 3.19 2.90 3.71 3.56 Table 17: Performance comparison of content consistency (CER/WER) and objective speech quality (NMOS) between MinMo and the TTS baseline CosyVoice 2.0-SFT on the text-to-speech Chinese (zh) and English (en) test sets. Instruction-following Voice Generation To evaluate the performance of instruction-following voice generation, we develop multi-turn Chinese speech-to-speech test set consisting of 30 sessions and 122 turns, incorporating 12 types of instructional controls. These controls include emotions (happy, sad, surprised, angry, fearful), dialects (Cantonese, Sichuan), speaking rates (fast, slow), role-playing (robot, Peppa), and default style. To assess the accuracy of instruction-following voice generation, listeners classify the generated audio according to the instruction type. As shown in Table 18, MinMo demonstrates superior instruction control accuracy compared to the baseline GLM-4-Voice, particularly in dialects and role-playing. Model Emotion Dialect Speaking Rate Role-playing Default Total GLM-4-Voice MinMo 75.6 97.6 42.9 100 80.0 100 70.4 96. 88.2 96.3 63.1 98.4 Table 18: Performance comparison of instruction-following voice generation between MinMo and the baseline GLM-4-Voice on the multi-turn speech-to-speech Chinese test set. 19 4.5 Voice Chat Spoken Question Answering and Spoken Dialogue To transfer the dialog capabilities of the base model to the speech modality, we construct multi-turn conversational data for both speech-to-text (speech2text) and speech-to-speech (speech2speech) scenarios. The speech2text data is primarily divided into two parts. First, it originates from open-source multi-turn text-only data, where we synthesize the user turns using zero-shot Text-to-Speech (TTS) technology. Second, we use real Automatic Speech Recognition (ASR) training data as chat queries to obtain text responses from the large model, thereby generating interactive training data for speech2text. To evaluate the question-answering capabilities of MinMo in the speech modality, we initially utilize three datasets, namely Llama Questions (Nachmani et al., 2024), Trivia QA (Joshi et al., 2017), and Web Questions (Berant et al., 2013), similar to the approach by Defossez et al. (2024), Zeng et al. (2024) and Wang et al. (2024b). These datasets are employed to assess the models knowledge question-answering ability in both speech-to-text and speech-to-speech modes. The baseline results for these datasets are taken from the original table of the respective papers. It is noteworthy that, due to the absence of publicly available specific test set for the Trivia QA, we adhere to the GLM configuration by randomly selecting 1,000 samples from the validation set of web questions as our test set. However, because of the inconsistency in test samples, the results on Trivia QA are not guaranteed to be absolutely comparable and should be considered as reference only. Additionally, since the original format of these three datasets is text-based, during testing, we use CosyVoice2(Du et al., 2024b) to perform TTS synthesis on the input question texts. As illustrated in the table 19, the MinMo model demonstrates significant advantage over existing baselines in the Speech-to-Speech (S2S) mode, achieving new state-of-the-art (SOTA) results. In the Speech-to-Text (S2T) mode, it also attains SOTA performance on the Llama Question and Web Question datasets. However, the test results of MinMo still indicate noticeable performance decline in the S2S mode compared to the S2T mode. We attribute this to the fact that many answers in the test set are rich in textual structure and specialized vocabulary, which imposes greater demands on the models text-to-speech (TTS) capabilities. Additionally, the automatic speech recognition (ASR) model used to obtain the answers text for speech in the S2S evaluation can also impact the S2S metrics to some extent. In order to further analyze MinMos voice interaction capabilities, we additionally constructed two test sets. The evaluation criteria are divided into two parts: first, assessing the preservation of MinMos logical reasoning abilities in the speech modality; second, evaluating MinMos spoken response capabilities in casual voice interactions. To facilitate this analysis, we construct two evaluation test sets: the Alpaca test set (Li et al., 2023), which emphasizes logical reasoning capabilities, and the ChitChat test set, which targets casual conversational scenarios. Referencing the work of Zheng et al. (2023), the evaluation capabilities of current high-quality large models align well with human assessments. Therefore, to enhance evaluation efficiency, we employ automated scoring using large models. The specific task prompt for the automated scoring can be found in the appendix 8. We used Qwen-Max as the scoring model, with each dialogue sample receiving score ranging from 0 to 10. The average score of the samples is taken as the final score. From Table 20, it can be observed that by incorporating additional speech2text task data into MinMo training, we are able to effectively maintain the conversational capabilities of the base model. Compared to the performance of the ASR combined with the text-only base model, MinMos conversational ability remains largely consistent. However, MinMos response scores are slightly lower than the quality of Ground Truth responses. We believe this discrepancy can be attributed to two main reasons. Firstly, the integration of multiple speech tasks and the implementation of LoRA training on the base model have somewhat diminished the logical generation capabilities of the original Large Language Model (LLM). The table shows that, compared to the ChitChat test set, MinMo exhibits greater performance variations on the Alpaca test set. Second, there is room for further improvement in MinMos audio comprehension capabilities, and there remains potential for reducing the Character Error Rate (CER) in ASR tasks. Full Duplex Spoken Dialogue To assess the capabilities of MinMo in full-duplex voice interaction, we construct three test sets: the Chinese Alimeeting dataset, the English Fisher dataset, and simulated test set designed to more closely resemble real human-machine dialogue scenarios. We evaluate MinMos full-duplex capabilities from two perspectives: prediction performance and"
        },
        {
            "title": "Model",
            "content": "Llama Question S2T S2S TriviaQA Web Questions S2T S2T S2S S2S Moshi (Defossez et al., 2024) GLM-4-Voice (Zeng et al., 2024) Freeze-Omni (Wang et al., 2024b) MinMo 62.3 64.7 72.0 78.9 21 50.7 - 64.1 22.8 39.1 53.9 48.3 7.3 26.5 - 37.5 26.6 32.2 44.7 55. 9.2 15.9 - 39.9 Table 19: Comparison of Spoken Question Answering Performance: Results for Moshi, GLM-4-Voice, and Freeze-Omni are sourced from their respective papers. S2T refers to the Speech-to-Text evaluation, while S2S denotes the Speech-to-Speech evaluation. The metric used for these assessments is accuracy. The TriviaQA dataset does not provide public test set, so the numerical results are not directly comparable and should be considered for reference only. Alpaca Test (Li et al., 2023) ChitChat Test Ground Truth ASR + Qwen2.5 MinMo 7.73 6.59 6.48 7.62 7.18 7. Table 20: Performance of MinMo in two in-house multi-turn speech-to-speech test sets: the Alpaca test set and the ChitChat test set. The Alpaca test set focuses on assessing logical reasoning capabilities, while the ChitChat test set is designed to evaluate casual conversational scenarios. For scoring, we utilized the Qwen-Max model. prediction efficiency. Regarding prediction performance, the evaluation is divided into three tasks: assistant turn-taking, user turn-taking, and user back-channeling. For the turn-taking tasks, we employe the positive F1 score as our analytical metric and also introduced the offset distance ( ) to better analyze the models performance. For the user back-channel task, we utilize accuracy to assess MinMos ability to recognize back-channel utterances. From Table 22, it can be observed that the MinMo model demonstrates commendable results on the human-machine conversation dataset, irrespective of whether it is user turn-taking or assistant turn-taking. At K=10, the prediction performance approaches 99%. On the test set of actual human-human conversations, the performance of the MinMo model on assistant turn-taking shows certain degree of decline compared to the human-machine conversation test set. We believe this is primarily due to the high variability in background noise, speech speed, pauses, and other factors in real human conversations, which can lead to some degree of misjudgment by the model on the assistant turn-taking task. However, for user turn-taking prediction in human-human conversations, the MinMo model still maintains high level of sensitivity and predictive performance, ensuring the system promptly stops speaking when the user talks, thereby avoiding overlapping speech with the user. This sensitivity and respect for user speech also explain why the MinMo model maintains prediction accuracy of 70%-80% for user back-channel comments, as shown in the table. This is consistent with the tuning of the user turn-taking model, indicating certain trade-off between the two. For the efficiency analysis of the MinMo duplex mode, we also conduct tests separately on both human-human dialogue and human-machine dialogue test sets. As shown in Table 23, the average response delay of MinMo in user turn-taking is 250ms. The fastest response speed is observed in the human-machine test set, at 88.8ms, while the most challenging Alimeeting test set shows delay of 448.8ms. In terms of assistant turn-taking, the average response delay of MinMo is around 660ms, which is longer compared to the response time required for user turn-taking prediction. We attribute this to the fact that user turn-taking involves the beginning part of the users speech, whereas assistant turn-taking involves the part where the users turn is nearly finished. Therefore, the contextual semantic information for assistant turn-taking is more comprehensive, which results in shorter time lag needed for decision-making. 21 responsible for duplex control, Full Duplex System Latency The duplex interaction of MinMo consists of four modules: the Full-duplex predictor, the speech-to-text module (Voice Encoder+Input Projector+LLM), the text-to-speech token module (Output Projector+Voice Token LM), and the Token2Wav module. The latencies for each module are shown in Table 21. Taking Assistant Turn-taking as an example, when the users actual speech concludes, the duplex model typically requires delay of 250 ms for evaluation. The prediction of the initial five text tokens in the Speech-to-Text process takes approximately 150 ms. Predicting the initial 15 speech tokens requires about 70 ms, and transitioning from speech tokens to the first audio packet takes an additional 130 ms. Consequently, when developing full-duplex voice dialogue system based on MinMo, the standard experiential delay for assistant turn-taking is approximately 250 + 150 + 70 + 130 = 600 ms. The aforementioned numerical estimates are derived during testing using the L20 GPU and BF16 model format. Full-duplex predictor (Assistant turn-taking) Speech-to-text (1 or 5 text tokens) Text-to-speech token Token2Wav (15 speech tokens) 250ms 95ms or 150ms 70ms 130ms Table 21: MinMos system latency analysis in two L20 GPUs. Data Assistants Turn-taking (Pos. F1, @K=1/5/10) Users Turn-taking (Pos. F1, @K=1/5/10) Users Back-channel (Acc. ) Alimeeting Fisher Simulation 0.6138 / 0.7542 / 0.8036 0.6682 / 0.8372 / 0.8813 0.7868 / 0.9616 / 0.985 0.4751 / 0.9366 / 1 0.4271 / 0.9455 / 0.9994 0.2571 / 0.8152 / 0.9942 0.7124 0.8123 - Table 22: Performance evaluation of the duplex prediction module: the turn-taking between assistant and user is measured using the positive F1 score @offset-K metric, while the users back-channel is assessed using the accuracy metric. Data Alimeeting Fisher Simulation Average Latency (ms.) Assistants Turn-taking Users Turn-taking 448.8 189.1 88. 663.4 641.8 673.7 Table 23: Average latency (ms) of MinMo for assistants turn-taking and users turn-taking on various full-duplex voice chat data sets."
        },
        {
            "title": "5 Conclusion",
            "content": "This research introduces MinMo, an advanced multimodal large language model designed to overcome the limitations of existing aligned multimodal models in seamless voice interaction. Trained on an extensive dataset of over 1.4 million hours of speech, MinMo showcases state-of-the-art performance across diverse benchmarks, including spoken dialogue, multilingual speech recognition, and emotion recognition. By leveraging multi-stage alignment strategy, MinMo adeptly balances audio understanding and generation while minimizing the catastrophic forgetting often observed in text-based LLMs. key innovation is MinMos novel alignment method for streaming end-to-end audio generation. By utilizing hidden layer representations of the text model, MinMos voice decoder achieves structural simplicity and competitive performance with low latency. This approach significantly enhances the models instruction-following capabilities, enabling nuanced speech generation that accurately reflects user-specified emotions, dialects, and 22 speaking styles. Furthermore, MinMo supports full-duplex interactions, facilitating seamless conversational experience with latency of approximately 600ms. In conclusion, MinMo represents substantial advancement in the field of voice interaction systems. It not only addresses the inherent challenges of sequence length discrepancies and data imbalance but also sets new standard for natural and expressive voice interactions, paving the way for future developments in multimodal language models."
        },
        {
            "title": "6 Limitations",
            "content": "Firstly, MinMo integrates audio MinMo has certain limitations that need to be addressed. understanding and audio generation capabilities based on pre-trained text large model by using alignment. The text large model only participates in LoRA updates, and its ability to follow diverse instructions, such as language and task following, needs improvement. Further exploration is needed to determine whether using more high-quality text data for more comprehensive updates of the text large model can enhance its instruction-following ability. Secondly, there are some long-tail pronunciation error issues in MinMos end-to-end audio generation. This problem partly arises from retaining some one-to-many tokens of the LLM, and partly because some special symbols in the end-to-end modeled output text cannot be effectively converted into speech. Data scaling can be explored to address these long-tail issues. Additionally, the overall efficiency of audio generation controlled by instructions in MinMo needs to be improved. This is partly due to the overall small size of the current instruction data and the limitation of only using hidden embeddings for end-to-end alignment, which restricts the transmission of historical information. Finally, while MinMo implements duplex module based on semantics, it still requires separate AEC and VAD modules. In the future, fully end-to-end duplex model will be explored."
        },
        {
            "title": "7 Authors (alphabetical order of family name)",
            "content": "Qian Chen Yafeng Chen Yanni Chen Mengzhe Chen Yingda Chen Chong Deng Zhihao Du Ruize Gao Changfeng Gao Zhifu Gao Yabin Li Xiang Lv"
        },
        {
            "title": "8 Acknowledgment",
            "content": "Jiaqing Liu Haoneng Luo Bin Ma Chongjia Ni Xian Shi Jialong Tang Hui Wang Hao Wang Wen Wang Yuxuan Wang Yunlan Xu Fan Yu Zhijie Yan Yexin Yang Baosong Yang Xian Yang Guanrou Yang Tianyu Zhao Qinglin Zhang Shiliang Zhang Nan Zhao Pei Zhang Chong Zhang Jinren Zhou We are immensely grateful for the invaluable discussions, support, and assistance we received from many colleagues during the development and demonstration of the MinMo model. Special thanks go to: Keyu An, Cheng Chen, Luyao Cheng, Rui Li, Jiayi Li, Minjun Liang, Chaohong Tan, Yiwei Wang, Ming Zhao. Additionally, we appreciate to the ModelScope community for providing GPU computational support for the demo services. Their contributions and support have been crucial in bringing this project to fruition."
        },
        {
            "title": "References",
            "content": "Keyu An, Qian Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui Hu, Kai Hu, Shengpeng Ji, Yabin Li, Zerui Li, Heng Lu, Haoneng Luo, Xiang Lv, Bin Ma, Ziyang Ma, Chongjia Ni, Changhe Song, Jiaqi Shi, Xian Shi, Hao Wang, Wen Wang, Yuxuan Wang, Zhangyu Xiao, Zhijie Yan, Yexin Yang, Bin Zhang, Qinglin Zhang, Shiliang Zhang, Nan 23 Zhao, and Siqi Zheng. Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms. CoRR, abs/2407.04051, 2024. Philip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen, Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding, Lu Gao, Mingqing Gong, Peisong Huang, Qingqing Huang, Zhiying Huang, Yuanyuan Huo, Dongya Jia, Chumin Li, Feiya Li, Hui Li, Jiaxin Li, Xiaoyang Li, Xingxing Li, Lin Liu, Shouda Liu, Sichao Liu, Xudong Liu, Yuchen Liu, Zhengxi Liu, Lu Lu, Junjie Pan, Xin Wang, Yuping Wang, Yuxuan Wang, Zhen Wei, Jian Wu, Chao Yao, Yifeng Yang, Yuanhao Yi, Junteng Zhang, Qidi Zhang, Shuo Zhang, Wenjie Zhang, Yang Zhang, Zilin Zhao, Dejian Zhong, and Xiaobin Zhuang. Seed-tts: family of high-quality versatile speech generation models. CoRR, abs/2406.02430, 2024. Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013, 18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, meeting of SIGDAT, Special Interest Group of the ACL, pp. 15331544. ACL, 2013. URL https://aclanthology.org/D13-1160/. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Ebrahim (Abe) Kazemzadeh, Emily Mower Provost, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. Iemocap: interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42: 335359, 2008. Houwei Cao, David G. Cooper, Michael K. Keutmann, Ruben C. Gur, Ani Nenkova, and Ragini Verma. Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE Transactions on Affective Computing, 5(4):377390, 2014. doi: 10.1109/TAFFC.2014.2336244. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models, November 2023. URL http://arxiv.org/abs/2311.07919. arXiv:2311.07919 [cs, eess]. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Christopher Cieri, David Miller, and Kevin Walker. The fisher corpus: resource for the next generations of speech-to-text. In LREC, volume 4, pp. 6971, 2004. Seamless Communication, Loıc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin N. Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Y. Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-juss`a, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzman, Justine Kao, Ann Lee, Alexandre 24 Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler Wang. Seamlessm4t-massively multilingual & multimodal machine translation. CoRR, abs/2308.11596, 2023. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning evaluation of universal In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. representations of speech. 798805. IEEE, 2023. Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. CoRR, abs/2410.00037, 2024. Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. Aishell-2: Transforming mandarin asr research into industrial scale. arXiv preprint arXiv:1808.10583, 2018. Zhihao Du, Qian Chen, Shiliang Zhang, Kai Hu, Heng Lu, Yexin Yang, Hangrui Hu, Siqi Zheng, Yue Gu, Ziyang Ma, Zhifu Gao, and Zhijie Yan. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. CoRR, abs/2407.05407, 2024a. doi: 10.48550/ARXIV.2407.05407. URL https://doi.org/10.48550/arXiv.2407.05407. Zhihao Du, Yuxuan Wang, Qian Chen, Xian Shi, Xiang Lv, Tianyu Zhao, Zhifu Gao, Yexin Yang, Changfeng Gao, Hui Wang, Fan Yu, Huadai Liu, Zhengyan Sheng, Yue Gu, Chong Deng, Wen Wang, Shiliang Zhang, Zhijie Yan, and Jingren Zhou. Cosyvoice 2: Scalable streaming speech synthesis with large language models, 2024b. URL https://arxiv.org/abs/2412.10117. Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, and Yang Feng. Llama-omni: Seamless speech interaction with large language models. CoRR, abs/2409.06666, 2024. Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, and Xing Sun. VITA: towards open-source interactive omni multimodal LLM. CoRR, abs/2408.05211, 2024. doi: 10.48550/ARXIV.2408.05211. URL https://doi.org/10.48550/arXiv.2408. 05211. Zhifu Gao, Shiliang Zhang, Ian McLoughlin, and Zhijie Yan. Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition. In INTERSPEECH, pp. 20632067. ISCA, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Shengpeng Ji, Yifu Chen, Minghui Fang, Jialong Zuo, Jingyu Lu, Hanting Wang, Ziyue Jiang, Long Zhou, Shujie Liu, Xize Cheng, Xiaoda Yang, Zehan Wang, Qian Yang, Jian Li, Yidi Jiang, Jingzhen He, Yunfei Chu, Jin Xu, and Zhou Zhao. Wavchat: survey of spoken dialogue models. CoRR, abs/2411.13577, 2024a. doi: 10.48550/ARXIV.2411.13577. URL https://doi.org/ 10.48550/arXiv.2411.13577. Shengpeng Ji, Jialong Zuo, Minghui Fang, Ziyue Jiang, Feiyang Chen, Xinyu Duan, Baoxing Huai, and Zhou Zhao. Textrolspeech: text style control speech corpus with codec language text-to-speech models. In ICASSP, pp. 1030110305. IEEE, 2024b. Shengpeng Ji, Jialong Zuo, Minghui Fang, Siqi Zheng, Qian Chen, Wen Wang, Ziyue Jiang, Hai Huang, Xize Cheng, Rongjie Huang, and Zhou Zhao. Controlspeech: Towards simultaneous zero-shot speaker cloning and zero-shot language style control with decoupled codec. CoRR, abs/2406.01205, 2024c. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension, 2017. URL https://arxiv.org/ abs/1705.03551. 25 Yichong Leng, Zhifang Guo, Kai Shen, Zeqian Ju, Xu Tan, Eric Liu, Yufei Liu, Dongchao Yang, Leying Zhang, Kaitao Song, Lei He, Xiangyang Li, Sheng Zhao, Tao Qin, and Jiang Bian. In ICLR. OpenReview.net, Prompttts 2: Describing and generating voices with text prompt. 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, Ke Xu, Yu He, Ying Li, Jinming Zhao, Ye Liu, Bin Liu, Jiangyan Yi, Meng Wang, Erik Cambria, Guoying Zhao, Bjorn W. Schuller, and Jianhua Tao. Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning, 2023. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Guan-Ting Lin, Cheng-Han Chiang, and Hung-yi Lee. Advancing large language models to capture varied speaking styles and respond properly in spoken conversations. In ACL (1), pp. 66266642. Association for Computational Linguistics, 2024a. Guan-Ting Lin, Prashanth Gurunath Shivakumar, Ankur Gandhe, Chao-Han Huck Yang, Yile Gu, Shalini Ghosh, Andreas Stolcke, Hung-Yi Lee, and Ivan Bulyko. Paralinguistics-enhanced large language modeling of spoken dialogue. In ICASSP, pp. 1031610320. IEEE, 2024b. Jiaqing Liu, Chong Deng, Qinglin Zhang, Shilin Zhou, Qian Chen, Hai Yu, and Wen Wang. Recording for eyes, not echoing to ears: Contextualized spoken-to-written conversion of ASR transcripts. In Proceedings of the AAAI Conference on Artificial Intelligence, 2025. Ziyang Ma, Mingjie Chen, Hezhao Zhang, Zhisheng Zheng, Wenxi Chen, Xiquan Li, Jiaxin Ye, Xie Chen, and Thomas Hain. Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark. In Proc. INTERSPEECH, 2024a. Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, and Xie Chen. Language model can listen while speaking. CoRR, abs/2408.02622, 2024b. doi: 10.48550/ARXIV.2408.02622. URL https://doi.org/10.48550/arXiv.2408.02622. Luz Martinez-Lucas, Mohammed Abdelwahab, and Carlos Busso. The MSP-Conversation Corpus. In Proc. Interspeech 2020, pp. 18231827, 2020. Kentaro Mitsui, Koh Mitsuda, Toshiaki Wakatsuki, Yukiya Hono, and Kei Sawada. PSLM: parallel generation of text and speech with llms for low-latency spoken dialogue systems. In Findings of EMNLP, pp. 26922700, 2024. URL https://aclanthology.org/2024.findings-emnlp. 151. Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken question answering and speech continuation using spectrogram-powered llm, 2024. URL https:// arxiv.org/abs/2305.15255. Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoˆıt Sagot, Abdelrahman Mohamed, and Emmanuel Dupoux. Generative spoken dialogue language modeling. CoRR, abs/2203.16502, 2022. doi: 10.48550/ ARXIV.2203.16502. URL https://doi.org/10.48550/arXiv.2203.16502. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: method for automatic evaluation of machine translation. In ACL 2002, pp. 311318. ACL, 2002. doi: 10.3115/1073083. 1073135. URL https://aclanthology.org/P02-1040/. 26 Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. MELD: multimodal multi-party dataset for emotion recognition in conversations. In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 527536, 2019. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. In International conference on Robust speech recognition via large-scale weak supervision. machine learning, pp. 2849228518. PMLR, 2023. Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. BLEURT: learning robust metrics for text In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.), ACL generation. 2020, pp. 78817892. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020. ACL-MAIN.704. URL https://doi.org/10.18653/v1/2020.acl-main.704. Xian Shi, Yexin Yang, Zerui Li, Yanni Chen, Zhifu Gao, and Shiliang Zhang. Seaco-paraformer: non-autoregressive asr system with flexible and effective hotword customization ability. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1034610350, 2024. Reo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi, Tatsuya Komatsu, and Kentaro Tachibana. Prompttts++: Controlling speaker identity in prompt-based text-to-speech using natural language descriptions. In ICASSP, pp. 1267212676. IEEE, 2024. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023. Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan Shao, Qiong Tang, Shiduo Zhang, et al. Moss: An open conversational large language model. Machine Intelligence Research, pp. 118, 2024. Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun MA, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=14rn7HpKVk. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Bandhav Veluri, Benjamin N. Peloquin, Bokai Yu, Hongyu Gong, and Shyamnath Gollakota. In EMNLP, Beyond turn-based interfaces: Synchronous llms as full-duplex dialogue agents. pp. 2139021402, 2024. URL https://aclanthology.org/2024.emnlp-main.1192. Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino. Covost 2 and massively multilingual speech translation. In Interspeech, pp. 22472251, 2021. Peng Wang, Songshuo Lu, Yaohua Tang, Sijie Yan, Yuanjun Xiong, and Wei Xia. full-duplex speech dialogue scheme based on large language models. CoRR, abs/2405.19487, 2024a. doi: 10.48550/ARXIV.2405.19487. URL https://doi.org/10.48550/arXiv.2405.19487. Xiong Wang, Yangze Li, Chaoyou Fu, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-omni: smart and low latency speech-to-speech dialogue model with frozen llm. arXiv preprint arXiv:2411.00774, 2024b. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal LLM. In ICML. OpenReview.net, 2024. Zhifei Xie and Changqiao Wu. Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities. arXiv preprint arXiv:2410.11190, 2024. 27 Hongfei Xue, Yuhao Liang, Bingshen Mu, Shiliang Zhang, Mengzhe Chen, Qian Chen, and Lei Xie. E-chat: Emotion-sensitive spoken dialogue system with large language models. CoRR, abs/2401.00475, 2024. Dongchao Yang, Songxiang Liu, Rongjie Huang, Chao Weng, and Helen Meng. Modelling expressive TTS in discrete latent space with natural language style prompt. ACM Trans. Audio Speech Lang. Process., 32:29132925, 2024a. Instructtts: IEEE Qian Yang, Jin Xu, Wenrui Liu, Yunfei Chu, Ziyue Jiang, Xiaohuan Zhou, Yichong Leng, Yuanjun Lv, Zhou Zhao, Chang Zhou, et al. Air-bench: Benchmarking large audio-language models via generative comprehension. arXiv preprint arXiv:2402.07729, 2024b. Fan Yu, Shiliang Zhang, Yihui Fu, Lei Xie, Siqi Zheng, Zhihao Du, Weilong Huang, Pengcheng Guo, Zhijie Yan, Bin Ma, et al. M2met: The icassp 2022 multi-channel multi-party meeting In ICASSP 2022-2022 IEEE International Conference on Acoustics, transcription challenge. Speech and Signal Processing (ICASSP), pp. 61676171. IEEE, 2022. Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: 10000+ hours multi-domain mandarin corpus for speech recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 61826186. IEEE, 2022. JTFLM Zhang and Huibin Jia. Design of speech corpus for mandarin text to speech. In The blizzard challenge 2008 workshop, 2008. Qinglin Zhang, Luyao Cheng, Chong Deng, Qian Chen, Wen Wang, Siqi Zheng, Jiaqing Liu, Hai Yu, and Chaohong Tan. Omniflatten: An end-to-end GPT model for seamless voice conversation. CoRR, abs/2410.17799, 2024a. Xin Zhang, Xiang Lyu, Zhihao Du, Qian Chen, Dong Zhang, Hangrui Hu, Chaohong Tan, Tianyu Zhao, Yuxuan Wang, Bin Zhang, Heng Lu, Yaqian Zhou, and Xipeng Qiu. Intrinsicvoice: Empowering llms with intrinsic real-time voice interaction abilities. CoRR, abs/2410.08035, 2024b. Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html. Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li. Seen and unseen emotional style transfer for voice conversion with new emotional speech dataset, 2021."
        },
        {
            "title": "A Prompts for Voice Understanding Tasks",
            "content": "A.1 Spoken Language Smoothing Here is the prompt for faithfulness and formality evaluation of the spoken language smoothing task, taking meetings as an example. 28 Prompt for Faithfulness Evaluation (Spoken Language Smoothing) Suppose you are professional text editor, please evaluate the quality of the models refined output. The original content is paragraph from Automatic Speech Recognition (ASR), which may contain recognition errors, grammatical errors, and spoken/informal style expressions. The models refined output should be free of noticeable ASR errors and grammatical errors, faithfully preserve the original content, and exhibit clear written style with excellent readability. The humans refined output can be considered as one of the qualified polished result, which can be used as reference for quality assessment. Please evaluate and score the models refined output. Use scoring range from 1 to 10 for each criterion, where 1 represents the worst performance and 10 represents the best performance. The return fields include: 1. ASR Error Correction Score: Check whether the text effectively corrects errors from automatic speech recognition. The higher the score, the more accurate the correction. 2. Faithful Score: Determine whether the text polishing results maintain the intent and content of the original speech. The higher the score, the more complete the preservation of the original meaning. 3. Evaluation Summary: Based on the scores above, briefly explain the reasons for the scores, provide an overall evaluation of the polishing quality, and suggest modifications. The following is the original content: {source} The following is the humans refined output for reference: {human-target} The following is the models refined output to evaluate: {model-target} Please provide scores for each criterion and an overall evaluation with json format: Prompt for Formality Evaluation (Spoken Language Smoothing) Suppose you are professional text editor, please evaluate the quality of the models refined output. The models refined output should be free of grammatical errors, and exhibit clear written style with excellent readability. Please evaluate and score the models refined output. Use scoring range from 1 to 10 for each criterion, where 1 represents the worst performance and 10 represents the best performance. The return fields include: 1. Grammar Score: Evaluate the grammatical accuracy of the text. The higher the score, the fewer grammatical errors there are. 2. Formal Score: Assess the written language style of the text, including the level of formality and suitability for written communication. The higher the score, the stronger the written style of the text. 3. Readability Score: Evaluate the fluency and overall readability of the text, including ease of understanding and natural expression. The higher the score, the better the readability of the text. 4. Evaluation Summary: Based on the scores above, briefly explain the reasons for the scores, provide an overall evaluation of the polishing quality, and suggest modifications. The following is the models refined output to evaluate: {target} Please provide scores for each criterion and an overall evaluation with json format: A.2 Punctuation and Inverse Text Normalization Here is the prompt for evaluation of punctuation and inverse text normalization. 29 Prompt for Faithfulness Evaluation (Spoken Language Smoothing) Compare the punctuation usage in the following sentences (focus only on punctuation, ignore other differences and the issue of full-width and half-width characters). Make ranking and output only the results of the ranking without any explanation. Display the results in the order of ranking, for example, a:1; b:3; c:2, where 1 represents first place, 2 represents second place, and so on. Ranks can be the same. The options are as follows: {option1} {option2} {option3} Prompt for Formality Evaluation (Spoken Language Smoothing) Compare the inverse text normalization in the sentences below, focusing only on the aspect of inverse text normalization and not on differences in the text or punctuation itself. Make ranking and output only the results of the ranking without any explanation. Display the results in the order of ranking, for example, a:1; b:3; c:2, where 1 represents first place, 2 represents second place, and so on. Ranks can be the same. The options are as follows: {option1} {option2} {option3}"
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}