{
    "paper_title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications",
    "authors": [
        "Dawei Gao",
        "Zitao Li",
        "Yuexiang Xie",
        "Weirui Kuang",
        "Liuyi Yao",
        "Bingchen Qian",
        "Zhijian Ma",
        "Yue Cui",
        "Haohao Luo",
        "Shen Li",
        "Lu Yi",
        "Yi Yu",
        "Shiqi He",
        "Zhiling Luo",
        "Wenmeng Zhou",
        "Zhicheng Zhang",
        "Xuguang He",
        "Ziqian Chen",
        "Weikai Liao",
        "Farruh Isakulovich Kushnazarov",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 7 2 6 1 . 8 0 5 2 : r 2025-08AgentScope 1.0: Developer-Centric Framework for Building Agentic Applications Dawei Gao, Zitao Li, Yuexiang Xie, Weirui Kuang, Liuyi Yao, Bingchen Qian, Zhijian Ma, Yue Cui, Haohao Luo, Shen Li, Lu Yi, Yi Yu, Shiqi He, Zhiling Luo, Wenmeng Zhou, Zhicheng Zhang, Xuguang He, Ziqian Chen, Weikai Liao, Farruh Isakulovich Kushnazarov, Yaliang Li, Bolin Ding, Jingren Zhou Alibaba Group https://github.com/agentscope-ai/agentscope"
        },
        {
            "title": "Abstract",
            "content": "Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide scalable evaluation module with visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides practical foundation for building scalable, adaptive, and effective agentic applications."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of Large Language Models (LLMs) (Achiam et al., 2023; Anthropic, 2024b; Meta, 2025; Yang et al., 2025; Team et al., 2025) has led to remarkable progress in artificial intelligence. key feature of modern LLMs is their ability to call and interact with external tools (Achiam et al., 2023; Hurst et al., 2024; Anthropic, 2024b; Meta, 2025; Yang et al., 2025; Team et al., 2025), greatly enhancing their functional scope. This tool-calling capability allows LLMs to automatically process external databases, execute computational tasks, and interact with different APIs, thereby extending their utility beyond intrinsic reasoning and language processing. Such advancements have laid robust foundation for developing powerful LLM-based agent applications that can effectively interface with the world through variety of tools, to perform diverse and complex tasks with increased autonomy and precision (Qin et al., 2024; Qu et al., 2025; Zhang et al., 2024; Cui et al., 2025; Yuan et al., 2024). By interacting with the environment, LLM-based agents have demonstrated immense potential in wide range of applications (Hong et al., 2024; Pan et al., 2024; langchain ai, 2024), proving increasingly capable of solving complex real-world problems while supporting flexible interactions with both users and environments. Following this trend, the focus of LLM-based agent frameworks has shifted from relying solely on intrinsic reasoning to empowering agents to perceive and interact with environments via an array of tools. Consequently, building flexible and efficient agent frameworks that support tool-based perception and interaction has emerged as promising direction in both academic research and industrial practice (Wang et al., 2024a; Agno AGI Team, 2024; langchain ai, 2024). Motivated by these insights and evolving demands, we introduce new version of AgentScope with novel architecture grounded in the ReAct (Yao et al., 2023) paradigm. This paradigm combines explicit *Corresponding authors, email address: {yaliang.li, bolin.ding}@alibaba-inc.com. Figure 1: The overview of AgentScope framework. reasoning with actions, enabling agents to analyze tasks, call tools, observe execution results, and iteratively refine their steps in closed loop. The overall architecture of AgentScope is illustrated in Fig. 1. To maximize flexibility and usability, AgentScope incorporates several design choices that allow developers to assemble, adapt, and extend agentic applications for real-world settings. (a) Foundational Components. At the core of AgentScope is set of foundational components that make building agentic applications both straightforward and flexible. We abstract the required components into four modules: message, model, memory, and tool. Our design emphasizes strong modular decoupling, broad compatibility across different application types, and extensibility for downstream customization. For example, multimodal information can be uniformly formatted as messages for transmission among agents, and diverse Model Context Protocols (MCPs) (Anthropic, 2024a) can be registered as tools to enrich how agents interact with environments. These foundational components can be composed flexibly to serve broad set of practical applications. (b) Agent-level Infrastructure. AgentScope adopts the ReAct paradigm as the primary and recommended agent architecture, as it provides simple yet effective paradigm for agent-environment interaction. Building upon this, AgentScope natively supports parallel tool calls, asynchronous executions, and realtime steering, delivering industrial-grade performance and efficiency for running agentic applications. Additionally, AgentScope integrates several built-in agents, including browser-use agent, deep research agent, and meta-planner agent. These agents are built on the basic ReAct agent and equipped with task-specific tools, hook functions, and prompts to address representative and wellstudied scenarios (Xi et al., 2025; Hu et al., 2025). Developers can use these agents out of the box or treat them as starting points for further customization. (c) Developer-friendly Experiences. To provide developer-friendly experiences throughout all stages of development and deployment, AgentScope integrates comprehensive suite of toolkits designed to streamline the entire workflow. The evaluation module offers unified interface for assessing agent performance and includes two specialized evaluators, enabling users to flexibly balance debugging convenience with computational efficiency. Besides, Studio, graphical interface for process monitoring and result tracing, supports multi-granularity and multi-dimensional analysis of running trajectories and evaluation results. runtime sandbox allows developers to easily configure and launch agent execution and deployment environments according to specific tool requirements. These toolkits ensure that AgentScope delivers smooth, efficient, and developer-friendly experience. Roadmap. In this manuscript, the details of the foundational agentic components will be introduced 2 from agentscope . message import Msg , ToolUseBlock Listing 1: Example of message creation in AgentScope # Example 1: Create Textual Message textual_msg = Msg ( name = \" Jarvis \" , role = \" assistant \" , content = \" Hello ! How can help you ? \" , ) # Example 2: Create Tool Use Message msg_tool_call = Msg ( name = \" Jarvis \" , role = \" assistant \" , content =[ ToolUseBlock ( type = \" tool_use \" , id = \" xxx \" , name = \" get_weather \" , input ={ \" location \" : \" Beijing \" } ) ] ) 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 in the following Sec. 2, including message, model interface, memory and tool for agents. The built-in ReAct-based agent-level functionalities will be elaborated in Sec. 3. The engineering support modules, including the evaluation module, studio, and runtime sandbox, are illustrated in Sec. 4. Last but not least, we present some examples and applications in Sec. 5 to demonstrate the potential of AgentScope."
        },
        {
            "title": "2 Foundational Components",
            "content": "In this section, we introduce the foundational components in AgentScope, including message, model, memory, and tool modules. For each component, we present its design goals and principles, implementation details, and illustrative examples for better understanding. 2.1 Message The message module is the basic data unit in AgentScope, which enables information exchange among agents, presentation in the user interface, and storage in memory. Meanwhile, it serves as the unified information abstraction and medium between AgentScope and different LLM APIs. message object (i.e., Msg) comprises the following key fields: Name: Records the name of the sender that produced the message, distinguish agents in multiagent applications. Role: Indicates the role of sender, which can be one of \"user\", \"assistant\", or \"system\". Content: Contains the main payload of the message. It can be simple text string or sequence of structured ContentBlock objects, such as text blocks, image blocks, audio blocks, video blocks, tool usage blocks, tool results blocks, and thinking blocks. The design of ContentBlock enables agents to exchange multimodal content, tool-usage details, and reasoning information, thereby natively supporting range of practical agentic applications. Metadata: Provides an option to attach additional meta information to the message, such as structured outputs. In addition, each message is automatically assigned timestamp and unique id upon instantiation to ensure traceability. Example 1 shows how to create messages in AgentScope. 2.2 Model The model module provides unified abstraction for integrating diverse LLM APIs, enabling seamless interoperability across model providers while delivering consistent interface and functionality. Such an 3 Table 1: The integrated LLM providers and their features in AgentScope. Provider Class Streaming Tools Vision Reasoning OpenAI, DeepSeek, vLLM OpenAIChatModel DashScope Anthropic Gemini Ollama DashScopeChatModel AnthropicChatModel GeminiChatModel OllamaChatModel abstraction and design philosophy address the inherent heterogeneity among different model providers, who might use different API specifications, parameter formats, and response structures. AgentScope integrates wide range of LLM providers with full feature compatibility, as summarized in Table 1. Built on the ChatModelBase abstract class, different model implementations share unified and standardized interface that includes (a) model-specific formatters, (b) asynchronous model calls, (c) unified response schema, and (d) usage tracking and hook functions. More details are provided in the rest of this subsection. Model-specific Formatters. Different model APIs set their own requirements for the inputs to LLMs, often differing subtly in the input formats, role specifications, and content structures. To bridge the gap between the message in AgentScope and the heterogeneous input format of different LLM APIs, we develop an abstract format method in the FormatterBase class to transform Message objects into provider-specific data structures. We provide two specialized formatters for each model provider, including ChatFormatter for supporting single-agent interactions, and MultiAgentFormatter for handling multi-participant conversations where speaker identification and role management are crucial. Considering that not all model providers support multi-agent messages natively, MultiAgentFormatter utilizes conversation history prompts and structured content to ensure compatibility with standard chat completion endpoints. This module also unifies the processing of multimodal content, which automatically converts local media (e.g., images and audio) to base64 format when required, and preserves URL references according to the providers specific requirements. As result, developers can handle multimodal inputs seamlessly across different model providers without additional application-level format management. Asynchronous Model Calls. The input of model object includes messages, tools, and other parameters supported by the LLM APIs. The messages parameter carries the conversation history as list of dictionaries produced by the corresponding formatter, while the tools parameter is set of JSON schemas describing available tool functions. Besides, for some LLMs, the optional tool_choice parameter controls the tool selection strategy. AgentScope natively supports asynchronous model calls, providing non-blocking design and efficient streaming response via Pythons asynchronous generators. With streaming disabled, the calling method returns single ChatResponse object containing the complete model output. With streaming enabled, it returns an asynchronous generator that yields ChatResponse updates in real time as the model produces content, following cumulative scheme that each chunk includes all content generated so far. In AgentScope, model responses are encapsulated in the ChatResponse Unified Response Schema. dataclass, which abstracts provider-specific output formats into unified schema. Specifically, model response exposes content field that supports heterogeneous content types, including TextBlock for textual responses, ToolUseBlock for function calls, and ThinkingBlock for reasoning traces. Additional metadata includes unique identifiers, creation timestamps, and usage statistics of input tokens, output tokens, and processing time for monitoring and analysis. The unified response schema enables sophisticated reasoning outputs across multiple providers. We use the ThinkingBlock objects to expose internal reasoning traces, with support for models from OpenAI, Anthropic, Gemini, and Ollama that offer explicit reasoning capabilities. AgentScope also provides fine-grained control over reasoning output via provider-specific mechanisms. For example, OpenAIs o-series models support reasoning effort levels (\"low\", \"medium\", and \"high\"), while Anthropic and Gemini expose configurable token budgets for reasoning processes. This abstraction allows developers to leverage advanced reasoning across providers while consuming consistent response schema, regardless of the specific implementations. 4 Usage Tracking and Hook Functions. The ChatUsage object provides fine-grained monitoring of model consumption through comprehensive metrics covering input tokens, output tokens, and processing latency. This unified tracking module enables per-invocation resource accounting across providers, supporting detailed cost analysis, comparative efficiency studies, and the implementation of usage-based billing and rate-limiting mechanisms in production. Its standardized format allows developers to build provider-agnostic cost dashboards and automated budget controls without vendor-specific integrations. AgentScope offers comprehensive extensibility via multi-layer hook system for deep integration with enterprise monitoring and observability stacks. It includes built-in distributed tracing through designed @trace_llm decorator, which automatically instruments model calls with OpenTelemetrycompatible (OpenTelemetry, 2024) spans that capture request parameters, response metadata, tokenusage statistics, and error conditions. These traces integrate seamlessly with systems such as ArizePhoenix (Arize-ai, 2023) and Langfuse (Langfuse, 2024). 2.3 Memory The memory module is designed to provide contextual information for subsequent reasoning and action steps, including conversation history, execution trajectories, and cross-conversation data such as user preferences. In AgentScope, the memory module consists of both short-term and long-term memory components. 2.3.1 Short-term Memory Short-term memory is essential for agents to keep track of recent communications and execution trajectories. In AgentScope, InMemoryMemory serves as the default buffer for storing this information. The implementation maintains an in-memory list of Msg objects, capturing the complete communication context between agents and users, as well as tool execution trajectories. The InMemoryMemory class provides basic operations for memory management, including adding new messages to the dialogue history, retrieving range of memory content, deleting specific messages by index, and clearing the entire memory buffer. During agent execution, particularly within reasoningacting loops, the memory is automatically updated. The incoming messages are promptly added before processing, while the responses and tool usage records are stored to preserve the full interaction trajectory. The design of short-term memory in AgentScope ensures agents maintain contextual awareness throughout multi-step executions and multi-turn conversations, while offering the flexibility to manage memory size and content according to different application needs. 2.3.2 Long-term Memory Long-term memory provides structured mechanism for persistent context management, enabling agents to retain and leverage information across conversations, such as user preferences, task history, and interaction patterns. Design and Abstraction. The abstract class LongTermMemoryBase serves as the core abstraction for all long-term memory implementations within AgentScope, which defines standardized protocol for memory operations for ensuring consistency across different backends and use cases. The abstract class specifies four key methods, organized into two distinct operational paradigms: Developer-Controlled Methods: record: Records structured information from message sequences, typically invoked at predefined stages in the agent workflow (e.g., session start or end). retrieve: Retrieves relevant memory entries based on the content of input messages, enabling context-aware responses. Agent-Controlled Methods: record_to_memory: Allows the agent to autonomously store information it deems important during reasoning. retrieve_from_memory: Enables the agent to perform keyword-driven queries to retrieve specific knowledge when needed. This dual-paradigm design supports flexible memory management strategies. Developer-controlled methods ensure reliable and systematic memory operations at critical points in the agent lifecycle, while 5 Table 2: The provided interfaces in the Toolkit module. Type Interfaces Descriptions Basic usage MCP-related register_tool_function Register target function execute_tool_function Execute the function call remove_tool_function Remove tool function from the toolkit by its name get_json_schemas Get the JSON schema of tools register_mcp_client remove_mcp_clients Register tool functions from an MCP client Remove tool functions from the specific MCP clients Group-wise Management create_tool_group update_tool_groups remove_tool_groups Create tool group within the toolkit Update the activation status of the given tool groups Remove tool functions from the toolkit by their group names agent-controlled methods are automatically registered in the agents toolkit, empowering the agent to make context-sensitive decisions about memory usage during execution. Specific Implementation. The Mem0LongTermMemory class provides specific implementation of longterm memory based on the mem0 library (Chhikara et al., 2025), demonstrating how external memory systems can be integrated into AgentScope while maintaining the frameworks interface and control mechanisms. By inheriting from LongTermMemoryBase, Mem0LongTermMemory implements all memory methods and leverages advanced capabilities in Mem0, such as semantic indexing, retrieval, and memory evolution. To support diverse deployment scenarios, two configuration strategies are provided: Individual Parameter Configuration: If no mem0_config is supplied, the class constructs its configuration from individual parameters. Explicit specification of model and embedding_model is required for proper initialization. Pre-configured Mem0 Configuration: Developers familiar with Mem0 can pass pre-defined mem0_config object. Individual parameters may be used to override specific settings, enabling fine-grained customization. These strategies ensure accessibility for new users and flexibility for advanced users, promoting seamless integration across wide range of applications. In this way, the long-term memory in AgentScope provides comprehensive and extensible solution for persistent knowledge management. Through its abstract base class design, support for multiple control modes, and pluggable backend implementations, this module accommodates both systematic and opportunistic memory usage patterns. 2.4 Tool AgentScope accommodates wide range of callable objects as tools, including various functions and MCPs. We define Toolkit, as the core of the tool module, to achieve flexible tool management by standardizing tool definitions into JSON schema and providing unified interfaces for their registration and execution. The interfaces of Toolkit are summarized in Table 2, and the usage of Toolkit is illustrated in Fig. 2. 2.4.1 Tool Registration and Execution Tool registration in Toolkit is centered on the register_tool_function interface. In addition to adding and maintaining the tool function for future usage, this interface is primarily responsible for preparing JSON schema for the tool function, which is essential for LLMs to accurately interpret tool functions and invoke them at appropriate times. When JSON schemas are not explicitly provided, Toolkit automatically constructs one with information from the function docstring, allowing developers to register tool functions with minimal effort. Besides, the register_tool_function interface is highly extensible. Developers can attach preset arguments (e.g., API keys and credentials), define post-processing logic to refine raw outputs, and dynamically extend the tool schema using BaseModel in Pydantic (Pydantic, 2020). Such extensibility is particularly useful for implementing complex interaction patterns, e.g., developers can programmatically add \"thinking\" parameter to all tools to enable Chain-of-Thought (CoT) reasoning (Wei et al., 2022). As for tool execution, the call_tool_function interface abstracts away the inherent complexity of 6 Figure 2: The usage of the Toolkit module in AgentScope, including tools registration (green), groupwise management (blue), and execution (red). handling various tool outputs by unifying the outputs of all registered functions, whether synchronous or asynchronous, streaming or non-streaming, into consistent asynchronous generator. This design allows developers to invoke different tools through unified interface, simplifying the efforts required to process diverse outputs. It is worth noting that we enhance the robustness of the provided asynchronous generator, especially for running interactive and long-running tasks. If the execution of streaming tool is interrupted (e.g., by an asyncio cancellation event), the toolkit gracefully preserves all results yielded up to that point and appends clear system notification, such as tool execution was interrupted, to the output stream. This mechanism ensures that partial progress is retained and provides explicit context regarding interruptions, which is crucial for building resilient and user-responsive agents. 2.4.2 Fine-grained MCP Management Integrating remote services via MCP is common demand in agentic applications (Hou et al., 2025; Xi et al., 2025; Qu et al., 2025; Wang et al., 2024b; Luo et al., 2025). However, raw remote functions often necessitate client-side adaptations, such as result post-processing, parameter filtering, or composition into more complex workflows. To tackle this, AgentScope provides an advanced MCP client architecture that enables fine-grained management of remote tools at both the client and function levels. Stateful and Stateless Clients. Central to our MCP client architecture is dual-client design, providing both stateful and stateless clients to accommodate different interaction patterns. The choice between them depends on session management requirements. Specifically, stateful client establishes persistent connection to an MCP server via explicit connect and close interfaces. This design ensures that all subsequent tool calls occur within the same session, making it feasible for services where state continuity is essential, such as remote browser session that must maintain cookies and context across multiple actions. In contrast, stateless client follows an ephemeral connection model. It automatically establishes connection immediately before tool call and terminates it right after, thereby minimizing resource overhead. This approach is well-suited for lightweight and transactional services that do not depend on session state. The distinct lifecycle management of these two clients is illustrated in Fig. 3. Client-Side Tool Abstraction. Our MCP clients provide powerful abstraction that transforms remote endpoints into native and first-class tools. Instead of simply listing function names, the client generates 7 (a) Stateless MCP client (b) Stateful MCP client Figure 3: The sequence diagram of the stateless and stateful MCP clients. The stateless client (Left) establishes new session per tool call, while the stateful client (Right) maintains persistent connection. local callable objects that serve as proxies for their remote counterparts. These proxy objects can be directly registered with register_tool_function, rendering remote services indistinguishable from local ones from the agents perspective. This seamless integration is crucial for enabling advanced customization. Since these proxies behave as standard Python objects, developers can easily wrap them in new functions to implement bespoke logic. For example, developers can construct composite function that first invokes remote search tool, then uses local regular expression to filter the results before passing them to another remote summarization tool. Such composability allows developers to adapt and combine raw remote services into high-level and task-specific tools without requiring server-side modifications, greatly enhancing the agents flexibility and capability. 2.4.3 Group-Wise Tool Management As the number of integrated tools increases, agents encounter \"paradox of choice\". Recent studies have shown that an overabundance of tools can actually degrade performance, leading to failures in selecting the appropriate tool or configuring its parameters correctly (Paramanayakam et al., 2025; Liu et al., 2024). This challenge not only increases the cognitive load on the agent but also consumes valuable context length with redundant tool descriptions. To tackle this, AgentScope introduces group-wise tool management strategy. This design is motivated by the observation that many tools are naturally utilized within task-oriented workflows. For example, web automation task typically involves sequence of related actions such as navigating to URL, clicking web elements, and entering text. Rather than presenting these tools as isolated options, grouping them provides more structured and efficient approach. For implementation, we provide several interfaces in Toolkit. Developers can use create_tool_group to logically bundle related tools, such as creating \"browser tools\" group for all web-related functions. Subsequently, the update_tool_groups interface allows for dynamic activation or deactivation of the entire tool set. This mechanism enables an agent to operate with streamlined and context-aware subset of its full capabilities at any given moment. For example, when the agent needs to perform web browsing, it can activate the \"browser tools\" group, making only the relevant tools available. Such lightweight and flexible strategy significantly reduces the search space for tool selection, thereby improving the agents efficiency and reliability."
        },
        {
            "title": "3 Agent-level Infrastructure",
            "content": "In this section, we provide details on the agent-level infrastructure of AgentScope. We discuss our design principles and highlight the features that make this framework effective for real-world agentic applications. We adopt the ReAct (Yao et al., 2023) paradigm as the primary and recommended agent architecture, enabling flexible agent-environment interactions. Building on this, we integrate several built-in agents tailored for specific practical scenarios. Besides, we also introduce how to construct multi-agent applications in AgentScope. 8 Figure 4: The workflow of the ReAct agent in AgentScope. 3.1 Architecture Based on the ReAct Paradigm 3.1.1 Overview The ReAct paradigm (Yao et al., 2023) combines reasoning with actions, providing simple yet effective paradigm for agent-environment interaction. In AgentScope, we adopt the ReAct paradigm as the primary and recommended agent architecture, steering towards truly application-oriented framework. In AgentScope, an agent is designed as an entity that interacts with its environment through well-defined interfaces, encompassing three core functionalities as shown in Fig. 4: Reply: This function serves as the agents primary active response mechanism. When receiving user query, the agent employs this function to perform reasoning, take actions, and generate conclusive responses. Observe: This function enables the agent to process external information, such as environmental changes or broadcast messages, and update its internal state or memory. Applying this function would not produce response to users. Handle Interrupt: To support seamless human-agent collaboration, this function provides means of handling interruptions. Triggered by external signals, it allows the agent to pause ongoing operations and react promptly to interruptions (e.g., urgent requests from the user). The intelligence driving the reply function is powered by the ReAct paradigm. Specifically, the agent initiates an iterative loop of reasoning and acting once receiving user query. This loop continues until the agent reaches conclusion and generates response. In each reasoning-acting cycle, the agent first produces thought to plan its next step, and then performs an action (e.g., calling tool) to interact with the environment and gather action results. While this loop forms the cognitive core of the agent, building an effective framework for real-world applications requires significant effort. Rather than limiting the framework to minimal implementation, AgentScope is equipped with comprehensive suite of features designed to deliver the following key advancements: Advanced Interactivity: We enable fluid, real-time collaboration by allowing users to interrupt and steer the agents reasoning process. Operational Flexibility and Efficiency: We extend the agents tool-using capabilities beyond sequential actions, supporting dynamic, task-aware tool selection and parallel execution. Engineering Robustness and Extensibility: We provide foundational mechanisms for automated state persistence and non-invasive customization, ensuring the framework is deployable, adaptable, 9 and easy to debug. In the rest of this subsection, we provide details of the specific features that enable these advancements. 3.1.2 Real-time Steering Real-time steering empowers users to guide, correct, or redirect the agent during task execution, transforming interactions from rigid and monolithic processes into flexible and collaborative experiences. AgentScope achieves real-time steering by gracefully pausing the ongoing ReAct loop upon receiving an external interruption signal, utilizing asyncio cancellation as the underlying mechanism. Developers can implement various handling strategies in the handle_interrupt method to define how the agent reacts to interruptions. For example, the agent can return quick response or invoke the LLMs to generate context-aware reaction. key innovation in our design is treating interruptions not merely as control signals, but as observable events. Therefore, the agent can capture the context of each interruption and integrate it into its state. For example, partial LLM response or preempted tool output can be preserved in the agents memory, with an annotation to indicate the user interruption. This design enables the agent to maintain contextual awareness of interruptions, informing its subsequent reasoning and decisions about whether to resume, revise, or alter its course of action. 3.1.3 Parallel Tool Calling and Dynamic Tool Provisioning To achieve operational flexibility and efficiency, we move beyond the standard sequential tool-use paradigm by enhancing agents with parallel tool calling and dynamic tool provisioning capabilities. Parallel Tool Calling. To improve efficiency, agents are allowed to generate multiple tool calls within single reasoning step, and these tool calls can be executed in parallel, as introduced in Sec. 2.4.1. This parallel approach reduces task latency compared to sequential execution, and is particularly effective for I/O-bound tasks. The process involves two steps: (a) The LLM is prompted to generate several concurrent tool calls; (b) These calls are dispatched for parallel execution using asyncio.gather. Then these action results are aggregated as observations for the agents next reasoning step. Dynamic Tool Provisioning. To provide functional adaptability, we introduce mechanism for dynamic tool provisioning in AgentScope, centered around the reset_equipped_tools function. This function serves as callable tool for agents, enabling them to autonomously modify their available tool set during task execution, drawing on the group-wise tool management framework introduced in Sec. 2.4.3. Specifically, whenever deemed necessary, the agent can use reset_equipped_tools to activate or deactivate certain groups of tools by specifying the group name. This empowers single agent to seamlessly handle complex and multi-stage workflows, e.g., starting with \"web-browsing\" tool set for research, and later switching to \"programming\" tool set for implementation. In this way, the agent can tailor its capabilities to the specific stage of the task, rather than being constrained by predefined, one-size-fits-all tool set. Meanwhile, by limiting the available tools to those relevant for the current phase, the approach reduces the complexity of agent action selection and conserves valuable context window space. 3.1.4 State Persistence and Non-Invasive Customization To enhance robustness and extensibility, AgentScope incorporates two novel mechanisms: an automated system for state persistence and flexible interface for non-invasive customization. State Persistence. We implement an automated and compositional state management system through the StateModule base class, which supports dual-mode registration. Firstly, attributes of any StateModule instance that are themselves StateModule objects are automatically incorporated into its state. Secondly, the base class provides register_state method for explicitly registering all other attribute types. In AgentScope, core components such as agents and memory inherit from StateModule. This design not only eliminates boilerplate code but also provides developers with state_dict and load_state_dict methods for saving and restoring of the entire nested agent hierarchy. Non-Invasive Customization. For high extensibility, we instrument the agent lifecycle with comprehensive system of hooks, enabling developers to modify runtime behavior without altering the core codebase. Figure 5: The workflow of Deep Research Agent. Hooks are available as preand post-events at key operational points, including reply, observe, reasoning, acting, and the console output method, print. Note that these hooks are not just passive listeners, they can actively modify the inputs and outputs of their respective functions. This capability supports wide range of applications, from implementing detailed logging and validation rules to altering the agents reasoning path. For example, the pre_print hook can intercept messages intended for the console and redirect them to web-based user interface, effectively decoupling the agents core logic from its presentation layer. 3.2 Built-in Agents Deep Research Agent. The Deep Research Agent is designed to search, gather, and combine information from multiple sources using search APIs, e.g., Tavily MCP (Tavily, 2025), to provide report-formatted answers to users queries. It can generate detailed, well-organized reports that help users gain deeper insights towards the queried task. workflow of Deep Research Agent is shown as Fig. 5. The Deep Research Agent focuses on developing three core capabilities: query expansion, reflection, and summarization. These capabilities are abstracted into tools that the agent can invoke as needed. The process of query expansion involves continuously breaking down tasks into manageable sub-tasks, which transforms the linear workflow of the ReAct worker into tree-based structure. During the search process, the agent conducts broad reading by using multiple queries to explore wide range of related knowledge, followed by close reading where it extracts comprehensive content from select valuable web pages. If the information gathered is insufficient, the task is further decomposed into sub-tasks for deeper exploration. The reflection capability of the Deep Research Agent is designed to address different types of failures with tailored strategies for trajectory optimization. Low-level reflection involves corrective measures for issues arising from tool errors, incorrect parameter usage, or ineffective sub-task completion. These are resolved by adjusting decision-making in subsequent steps of the ReAct process. On the other hand, high-level reflection addresses persistent failures that resist simple corrections, often indicating unanticipated practical challenges in the initial planning. In such cases, the agent may rephrase current steps if there is misunderstanding of sub-task objectives or if they are unachievable in their current forms. For summarization, the agent mimics human research behavior by documenting useful results during the search process, forming draft report without strict formatting requirements. This approach ensures that essential information is not overlooked, enabling the agent to proactively explore related topics from multiple perspectives and engage in in-depth reasoning, ultimately resulting in thorough analysis and comprehensive coverage of the subject matter. Another key strength of the Deep Research Agent is its integration with the Memory module in AgentScope. With this feature, the agent can store and revisit important information throughout its research process, further enhancing its ability to produce high-quality and comprehensive reports. Browser-use Agent. The Browser-use Agent is designed to autonomously navigate and interact with websites by integrating LLMs with browser automation tools such as Playwright MCP (Micrsoft, 2025). Typical applications encompass booking flights and hotels, querying stock prices and consolidating relevant news, web scraping and information summarization, submitting online forms, and monitoring real-time updates of specific web content, such as sports events or weather forecasts. An overview of the workflow of the Browser-use Agent is demonstrated in Fig. 6. Key features and advantages of the Browser-use Agent include: Subtask Decomposition and Management: The Browser-use Agent breaks down complex user queries into manageable subtasks, which it executes sequentially. This approach supports task updates and maintenance, enhancing the accomplish of tasks. Integration of Visual and Web Textual Information: By leveraging large models with visual capabilities, Figure 6: The workflow of Browser-user Agent. the Browser-use Agent is capable of reasoning over both webpage screenshots and HTML content, allowing for deep understanding and more accurate interaction with various web pages. Multi-Tab Browsing: The Browser-use Agent supports concurrent management of multiple browser tabs, enabling parallel interactions with several web pages. This can be particularly helpful for workflows that require cross-referencing information and simultaneous monitoring. Efficient Handling of Long Webpages: To address the challenge of processing web pages that might exceed the context length limitation of LLMs, the Browser-use Agent segments long pages into smaller, manageable chunks. It performs webpage observation by chunk and manages cross-chunk contexts to ensure comprehensive information processing. With these abilities, the Browser-use Agent empowers users to efficiently gather information, perform complex interactions, and manage multiple subtasks, ultimately enabling them to solve complex problems through automatic navigation in web environments. Meta Planner. Contemporary autonomous agent systems face significant challenges when tasked with complex, multi-step problems that require sophisticated planning, resource allocation, and coordination capabilities beyond the scope of traditional single-agent approaches. While existing ReAct frameworks demonstrate proficiency in straightforward task execution through iterative reasoning-action cycles, they exhibit limitations when confronting intricate workflows that demand hierarchical task decomposition, specialized tool selection, and systematic progress tracking. To address these constraints, we introduce the Meta Planner, novel architectural agent that extends the ReAct paradigm through the integration of planning capabilities and dynamic worker orchestration. The system operates on dual-mode architecture, automatically transitioning between lightweight ReAct processing for simple tasks and comprehensive planning-execution patterns for complex multi-stage problems, thereby optimizing computational resources while maintaining robust performance across diverse task complexities. The Meta Planner implements sophisticated planning-execution pipeline centered around three core functional modules: hierarchical task decomposition through structured roadmap generation, dynamic worker agent instantiation with specialized toolkit allocation, and persistent state management enabling long-term task continuity. The system employs data structure for maintaining session context tracking. Based on the session information data structure, the RoadmapManager module, as set of tools, facilitates intelligent task breakdown into executable subtasks with defined dependencies and success criteria. Worker agents are dynamically created and managed through the tools provided in WorkerManager module, which allocates appropriate tool combinationsincluding MCP for external service integrationbased on subtask requirements. This architecture enables the system to handle complex workflows such as multi-source data analysis, research synthesis, and iterative content generation, while maintaining transparency through comprehensive progress tracking and state persistence mechanisms that support task resumption and debugging capabilities. The agent features intelligent mode switching that automatically determines whether to use simple ReAct mode for straightforward tasks or advanced planning mode for complex multi-step operations. An illustrative trajectory is provided in Fig. 7. 3.3 Multi-Agent 3.3.1 Agent as Tool In AgentScope, widely used and recommended approach for building multi-agent applications is to treat agents as tools, i.e., allowing agents to function as callable components within large workflow. The intuition behind such an approach is that, while primary agent still manages direct user interactions 12 Figure 7: The key component of Meta Planner and an example of its trajectory. Listing 2: Examples of chaining agents in sequential manner. # 1: functional implementation from agentscope . pipeline import sequential_pipeline msg = await se que nt ial _pip eli ne ( # List of agents to be executed in order agents =[ alice , bob , charlie , david ] , # The first input message , can be None msg = None ) # 2: class - based implementation from agentscope . pipeline import SequentialPipeline # Create pipeline object pipeline = Seq ue nti alPipeline ( agents =[ alice , bob , charlie , david ]) # Call the pipeline msg = await pipeline ( msg = None ) # Reuse the pipeline with different input msg = await pipeline ( msg = Msg ( \" user \" , \" Hello ! \" , \" user \" ) ) 2 3 4 5 6 8 9 10 11 12 14 15 16 17 and conversations, it can autonomously select and invoke specialized agents as tools to handle particular subtasks or domains of expertise. For example, as described in the recent study (Li et al., 2025), knowledge-integrated multi-agent system often requires different agents to manage distinct knowledge bases. When user submits query, the main agent routes the question to the appropriate agents (each instantiated as tool and standing by to be called as needed). Upon receiving request, these agents generate responses based on their knowledge bases. Finally, these outputs can be aggregated to deliver comprehensive response to the user. Such agent-as-a-tool architecture promotes scalability and flexibility of AgentScope. Agents can be independently developed, tested, and added to the system as new tools to rapidly adapt to evolving user requirements, enabling integration of novel capabilities or knowledge sources without disrupting existing workflows. 3.3.2 Agent Conversation Agent conversation represents another standard paradigm for multi-agent applications. To streamline development and reduce complexity, AgentScope provides pipelines and message hubs for managing agent interactions efficiently and minimizing repetitive coding. The pipeline abstraction encapsulates common patterns in agent conversation, including sequential, conditional, and iterative message exchanges, into simple and reusable components. Developers can construct agent conversations by assembling pipelines that handle the flow of messages between agents, enabling clear separation between the interaction logic and the underlying message-passing mechanism. Pipelines can be employed in both functional and object-oriented styles, as shown in Example 2. Beyond basic sequential pipelines, AgentScope also offers constructs for conditional branching (i.e., if-else and switch) and looped interactions (i.e., while-loop and for-loop), making it easy to model complex and adaptive multi-agent behaviors. The message hub abstraction acts as centralized broadcast mechanism for simplifying group conversations among agents. By configuring message hub with set of participant agents and initial messages, developers can facilitate automatic message dissemination whenever any agent generates new message, as illustrated in Example 3. The message hub ensures that all participating agents remain contextually synchronized and supports dynamic group dialogues (Du et al., 2023). async def m _ a st _ s () : \"\"\" Example of broadcasting messages with MsgHub . \"\"\" Listing 3: Broadcasting messagesa with message hub. # Create message hub async with MsgHub ( participants =[ alice , bob , charlie ] , announcement = Msg ( \" user \" , \" Now introduce yourself in one sentence , including your name , age (cid:44) and career . \" , \" user \" , ) , ) as hub : # Group chat without manual message passing await alice () await bob () await charlie () asyncio . run ( m _ a s _ s ag () )"
        },
        {
            "title": "4 Developer-Friendly Experience",
            "content": "1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 Towards developer-friendly experiences, we integrate comprehensive toolkits in AgentScope to further streamline the development, including Evaluation, Studio, and Runtime. 4.1 Evaluation 4.1.1 From Tasks, Solutions and Metrics to Benchmark An overview of the evaluation module is illustrated in Fig. 8. The evaluation module is designed with hierarchical architecture that systematically organizes the several core components: Tasks: Task object represents an individual evaluation unit, encapsulating all the information required for agent execution and assessment. Each task is assigned unique identifier and contains the task input, ground truth, evaluation metrics, and optional metadata such as category labels and additional context. Solutions: The evaluation framework defines solution output class, SolutionOutput, to standardize the representation of agent-generated solutions. This structure captures three critical elements: (a) success flag indicating whether the solution executed without exceptions, (b) the final output produced by the agent (e.g., an answer or the terminal state of the environment), and (c) complete trajectory documenting all tool callings and action results throughout execution. This design enables both outcome-based and process-based evaluation approaches. Metrics: The abstract class MetricBase is implemented to support developer-defined metrics. The framework allows two primary metric types: categorical metrics, which yield discrete classifications (e.g., pass or fail), and numerical metrics, which yield continuous scores. Each metric is callable instance, and is expected to take SolutionOutput object as input and generate MetricResult. The generated MetricResult includes the metric name, computed score, timestamp, and optional messages for additional context. This abstraction ensures the flexible integration of domain-specific evaluation criteria while maintaining consistency across various evaluation approaches. Benchmarks: benchmark aggregates multiple tasks into cohesive evaluation suite by inheriting from BenchmarkBase. Benchmarks provide iterator functionality for systematic task traversal and implement indexing for random access patterns. This structure supports both sequential evaluation workflows and parallel processing strategies, allowing developers to construct domain-specific evaluation suites tailored to their experimental requirements. 14 Figure 8: The evaluation module in AgentScope 4.1.2 Evaluators To orchestrate the evaluation process, we integrate the Evaluator module in AgentScope. Developers can seamlessly transition between debugging-focused sequential evaluation and production-scale distributed evaluation without modifying their solution generation logic or benchmark definitions. The standardized interfaces are defined in EvaluatorBase, which manages evaluation across benchmark tasks and enables customization of application-specific evaluation pipelines. Two specific evaluators are provided, allowing users to prioritize either debugging capabilities or computational efficiency as needed. On one hand, we implement GeneralEvaluator, which executes tasks sequentially within single process, making it particularly suited for development and debugging scenarios. This evaluator can be extended via user-defined solution generation functions that accept task and pre-hook callable as input, and return coroutine producing SolutionOutput. The sequential execution manner in GeneralEvaluator supports straightforward debugging, comprehensive logging, and step-by-step analysis of agent behavior. On the other hand, we provide the RayEvaluator for high efficiency, leveraging the Ray (Moritz et al., 2018) distributed computing framework to enable parallel and distributed evaluation across multiple workers. This evaluator is designed for large-scale benchmark execution, where computational efficiency is essential. The Ray-based implementation automatically distributes tasks across available workers, manages resource allocation, and aggregates results, while maintaining the same interface as the sequential evaluator. Both evaluators are integrated with the storage subsystem via EvaluatorStorageBase, which enables persistent storage of evaluation results, metadata, and experimental configurations. The framework supports evaluation continuation after interruptions by tracking completed tasks and resuming from appropriate checkpoints. Besides, we provide aggregation functionality in these evaluators for computing summary statistics across multiple task repetitions. 4.2 Studio The Studio in AgentScope serves as visual platform designed to enhance transparency and control of the development of agentic applications. It is built upon native integration with the OpenTelemetry standard (OpenTelemetry, 2024), enabling the direct consumption and rendering of detailed telemetry data generated within applications. Demonstrations of Studio are illustrated in Fig. 9. 4.2.1 Chatbot-style Dialogue and Tracing Developers can connect their applications to the Studio via simple init function. Once connected, all messages, user inputs, and tracing data are streamed in real time to Studios web frontend, which visualizes agent interactions in an intuitive and chatbot-style dialogue interface. Such dialogue view provides immediate clarity on the conversational flow by explicitly displaying structured message components, such as thinks, tool invocations, action results, and multimodal content. Furthermore, the Studio also offers granular execution tracing for deep analysis. The execution 15 (a) The chatbot-style dialogue and tracing visualization. (b) built-in copilot Friday. Figure 9: Demonstrations of Studio in AgentScope. trace depicts the entire operation as hierarchical sequence of time-stamped spans, with each span representing discrete computational step, e.g., an LLM invocation, tool execution, or the occurrence of an exception. Notably, each span in the trace view is directly linked to its associated message in the dialogue, allowing developers to seamlessly navigate from observed events in the conversation to underlying performance metrics in the trace. This tight integration enables rapid identification of latency sources, such as slow LLM response or an inefficient tool function, accelerating the debugging and optimization process. 4.2.2 Visualization of Evaluation Results In collaboration with the evaluation module (refer to Sec. 4.1), Studio provides dedicated visualization component that transforms raw evaluation results into interactive visualizations. By representing performance as statistical distribution, developers are empowered to assess an agents stability and 16 capabilities with greater statistical confidence, moving beyond simplistic and single-point metrics. The visualization process begins with the ingestion of evaluation artifacts. When evaluation results are imported, Studio automatically parses and organizes them by their corresponding benchmarks, creating structured foundation for further analysis. Rather than displaying static value, agent performance is visualized as comprehensive probability distribution. The visualization intelligently adapts to the type of metric, e.g., discrete categories are shown differently from continuous metrics. To ensure statistical validity, especially with limited number of trials, Studio employs techniques such as bootstrapping to compute confidence intervals and render the full performance distribution. This approach offers transparent and robust perspective on an agents stability and expected performance range, representing significant improvement over potentially misleading averages. Beyond high-level summaries, Studio diagnoses the sources of performance variation. It provides an aggregated statistical view that analyzes outcomes on per-item basis across all trials, effectively grouping test items into cohorts such as \"consistently correct\", \"consistently incorrect\", or \"unstable\". This breakdown enables developers to quickly identify specific problem types where the agent excels or struggles, guiding optimization efforts toward the most impactful areas. Studio supports trajectory comparisons for fine-grained analysis. When the agent exhibits performance differences in the distribution tails, Studio allows side-by-side visual comparison of the corresponding execution trajectories. By juxtaposing both chains of tool calls, reasoning steps, and LLM responses, developers can conduct fine-grained root-cause analysis. This direct visual comparison makes it possible to pinpoint the exact divergence in the agents behavior that led to failure, effectively closing the loop from high-level statistical observations to actionable and low-level debugging insights. 4.2.3 Built-in Copilot: Friday Studio includes built-in copilot (i.e., an assistant agent) named Friday. This agent serves dual purpose. On the one hand, it is designed to actively assist developers. On the other hand, it serves as practical showcase of the advanced capabilities available in AgentScope, such as real-time steering (Sec. 3.1.2), dynamic tool provisioning (Sec. 3.1), and long-term memory management (Sec. 2.3). Specifically, Friday is equipped with specialized set of tools that grant it access to resources provided in AgentScope, e.g., source code, tutorials, and documents, allowing it to search for technical information and generate helpful responses. In this way, Friday transforms the static documentation into dynamic and conversational resource, providing immediate and context-aware assistance that accelerates both learning and development. developer can ask Friday to retrieve the exact signature of function from the Python SDK or to find answers within the README and FAQs. Furthermore, as showcase agent, Friday offers developers concrete reference implementation. Instead of relying solely on abstract examples, users gain access to live and feature-rich agent that demonstrates advanced usage patterns and facilitates better understanding of the frameworks capabilities. 4.3 Runtime The deployment of agentic applications presents challenges in service orchestration, protocol compatibility, and secure tool execution. To tackle these challenges, we integrate Runtime1 in AgentScope, comprehensive agent runtime system designed for agent deployment and safe sandboxed tool execution. Specifically, Runtime employs dual-core architecture consisting of an Engine and Sandbox. The Engine module provides the underlying infrastructure for deploying and managing agent applications, featuring built-in context management, session handling, and control over the tool sandbox. Meanwhile, the Sandbox module offers isolated environments to ensure secure tool execution. Engine. With the help of Engine module, developers can create Runner object and pass an agent as one of its parameters. With applying the function deploy, the agent can be effortlessly deployed, automatically generating production-ready FastAPI service with integrated health monitoring, graceful lifecycle management, and standardized API protocols. It is worth noting that AgentScope offers native support for multiple agent communication protocols, including Googles Agent-to-Agent (A2A) protocol (Google, 2025) and custom protocol adapters, ensuring seamless interoperability across heterogeneous agent ecosystems. Example 4 provides an example of deployment with A2A protocol support. Sandbox. The Sandbox provides function-style interface that maintains consistent programming patterns while ensuring complete isolation. It supports various specialized environments (e.g., Filesystem 1https://github.com/agentscope-ai/agentscope-runtime 17 Listing 4: Examples of deployment with A2A protocol. # Create and configure agent agent = AgentScopeAgent ( name = \" Friday \" , model = OpenAIChatModel ( \" gpt -4 \" ) , agent_builder = ReActAgent , # Or your agent class built with AgentScope ) # Create executable runner runner = Runner ( agent = agent , context_manager = ContextManager () , environm en t_ma nag er = En vir on mentManager () , ) # Deploy as production service with A2A protocol support await runner . deploy ( deploy_manager = LocalDeployManag er ( host = \" localhost \" , port =8090 , ) , endpoint_path = \" / process \" , protocol_adap ters = 2 st ID au Ad te ( agent = agent ) , ) Listing 5: Examples of using the Sandbox module. # Secure tool execution with automatic sandbox management from agents co pe_ runtime . sandbox . tools . base import run_ipython_cell result = run_ipython_cell ( code = \" import os ; print ( os . listdir () ) \" ) # Persistent sandbox for stateful operations with BaseSandbox () as sandbox : func = run_ipython_cell . bind ( sandbox = sandbox ) func ( code = \" data = [1 , 2 ,3] \" ) # State preserved across calls func ( code = \" print ( sum ( data ) ) \" ) 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 2 3 4 5 6 8 9 10 Sandbox for secure file operations, BrowserSandbox for web automation, and TrainingSandbox for benchmark evaluation) while maintaining consistent interfaces across different sandbox types. Developers can effortlessly extend their applications with additional MCP servers, without the overhead of preparing secure tool execution environments. An example of using the Sandbox module is shown in Example 5. With Runtime, we deliver developer-friendly experience that goes beyond deployment simplicity and backward compatibility with agentic applications. It also offers enhanced features such as structured communication protocols, multi-modal content support, and comprehensive lifecycle management. Runtime not only reduces deployment complexity, but also guarantees enterprise-grade reliability and security for agent applications, allowing developers to focus on agent logic instead of infrastructure concerns."
        },
        {
            "title": "5 Signature Applications",
            "content": "In this section, we introduce several signature applications of AgentScope, offering developers with hands-on tutorials from both implementation and execution. 5.1 User-assistant Conversation In Example 6, we demonstrate how to construct user-assistant conversation by explicitly exchanging messages. The first step is to initialize both the ReAct agent and the user agent. For the ReAct agent, initialization involves specifying its name, system prompt, model, formatter, toolkit, and memory-related settings. The ReAct agent is built in an implementation-agnostic manner, with the main components exposed to the constructor, allowing developers to easily modify their agents without altering the 18 import asyncio , os Listing 6: An example of building user-assistant conversation. from agentscope . agent import ReActAgent , UserAgent from agentscope . formatter import Das hSc op eCh at For mat te from agentscope . memory import InMemoryMemory from agentscope . model import DashScopeChatModel from agentscope . tool import Toolkit , execute_shell_command , execute_python_code (cid:44) , view_text_file async def main () -> None : \"\"\" The main entry point for the ReAct agent example . \"\"\" toolkit = Toolkit () toolkit . gi st r_ oo _ fun cti on ( exec ute_sh ell_co mmand ) toolkit . gi st r_ oo _ fun cti on ( execute_python_code ) toolkit . gi st r_ oo _ fun cti on ( view_text_file ) agent = ReActAgent ( name = \" Friday \" , sys_prompt = \" You are helpful assistant named Friday . \" , model = DashScopeChatM od el ( api_key = os . environ . get ( \" DASH SCOPE_AP I_KEY \" ) , model_name = \" qwen - max \" , enable_thinking = False , stream = True , ) , formatter = hS op Ch tF or mat te () , toolkit = toolkit , memory = InMemoryMemory () , # Additional arguments setting long_term_memory = Mem0Lon gTermMemory () , lo g_t er _me mo y_ mo = \" both \" , para ll el_ to ol _ca ll = True , ) user = UserAgent ( \" Bob \" ) msg = None while True : msg = await user ( msg ) if msg . get_text_content () == \" exit \" : break msg = await agent ( msg ) asyncio . run ( main () ) 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 41 42 43 44 core codebase. It is compatible with various model providers, including but not limited to OpenAI, DashScope, Gemini, Anthropic, and self-hosted open-source models. After the react agent and user agent are configured, the conversation can be built by having them exchange messages. In this setup, the ReAct agent and the user take turns speaking until the user decides to exit the interaction by typing the \"exit\" command. 5.2 Multi-agent Conversation AgentScope natively supports multi-agent conversations, primarily enabled by two key components: MsgHub, which manages message exchange among agents, and Pipelines, which orchestrate the interaction flow. These two components greatly simplify the development of complex conversational dynamics. In Example 7, we provide practical demonstration of building multi-agent conversation. The example begins by instantiating three agents, each with distinct persona (e.g., teacher, student, and doctor). These agents are grouped within MsgHub, which initiates the dialogue by broadcasting system message that prompts each agent to introduce themselves. Then sequential_pipeline is used to ensure the 19 Figure 10: running demonstration of the built-in browser-use agent. agents speak in predefined order. To showcase dynamic group management, we remove the agent \"Bob\" from the MsgHub, announcing his departure to the remaining participants via broadcast message. The example concludes by observing the reactions of the other agents, demonstrating the systems ability to handle dynamic changes within conversation. 5.3 Deep Research Agent The Deep Research Agent2 extends the ReAct agent with specialized research methodologies designed to handle complex queries, excelling at data collection, comprehensive investigation, and synthesis. The agent initialization establishes connection to Tavily search service through MCP integration (Tavily, 2025), providing powerful web search and content extraction capabilities. During execution, one can observe that the agent automatically breaks down research questions into manageable subtasks, conducts targeted searches for each component, identifies knowledge gaps that require further investigation, and synthesizes findings into coherent reports. The agent maintains intermediate memory for tracking research progress and can generate structured outputs including detailed analysis reports, making it particularly suitable for academic research, market analysis, technical investigations, and comprehensive fact-finding missions that require multi-source verification and deep analytical reasoning. 5.4 Browser-use Agent The Browser-use Agent3 extends the ReAct agent with specialized browser capabilities via the PlayWright MCP (Micrsoft, 2025), which provides essential browser operation tools. The initialization begins by establishing stateful connection through the StdIOStatefulClient, which communicates with the MCP server using standard input/output protocols. These tools are then registered to toolkit by integrating the stateful client. The Browser Agent is configured with some specific components, including the model, formatter, memory, and the browser-enabled toolkit, while other parameters are inherited from the ReAct Agent. The agent automatically manages browser states using specialized functions that support task decomposition, subtask manager, screenshot taking, chunk-wise webpage observation, memory summarization, and tool execution result filtering. During each interaction cycle, it captures webpage snapshots (and screenshots if the LLM has vision ability), reasons about the current browser state, and executes appropriate actions such as navigation, clicking, and typing. Through its conversational loop, users can naturally issue web automation commands, such as \"search for Python tutorials\" or \"navigate to GitHub and find trending repositories\", while the agent handles the complex sequences of browser interactions required to fulfill these requests. Fig. 10 shows screenshot of the agent responding to the query \"What is the latest stock price of Alibaba?\", where it successfully finds the relevant information via Google search in the browser. 2The implementation details of the Deep Research Agent can be found at https://github.com/agentscope-ai/ agentscope/tree/main/examples/agent_deep_research. 3The implementation details of the Browser-use Agent can be found at https://github.com/agentscope-ai/ agentscope/tree/main/examples/agent_browser. 20 import asyncio , os Listing 7: An example of building multi-agent conversation. from agentscope . agent import ReActAgent from agentscope . formatter import h p l g F a r from agentscope . message import Msg from agentscope . model import DashScopeChatModel from agentscope . pipeline import MsgHub , sequential_pipeline def create_agent ( name : str , age : int , career : str , character : str ) : \"\"\" Create participant agent with specific name , age , and character . \"\"\" return ReActAgent ( name = name , sys_prompt =( \" You ' re { age } - year - old { career } named { name } and you ' re \" \" { character } person . \" ) , model = DashScopeChatM od el ( model_name = \" qwen - max \" , api_key = os . environ [ \" DA SHSCOPE_A PI_KEY \" ] , stream = True , ) , # Use multiagent formatter because multiple entities involves formatter = h p l g F a r () , ) async def main () -> None : \"\"\" Run multi - agent conversation workflow . \"\"\" # Create multiple participant agents with different characteristics alice = create_agent ( \" Alice \" , 30 , \" teacher \" , \" friendly \" ) bob = create_agent ( \" Bob \" , 14 , \" student \" , \" rebellious \" ) charlie = create_agent ( \" Charlie \" , 28 , \" doctor \" , \" thoughtful \" ) # Create conversation where participants introduce themselves async with MsgHub ( participants =[ alice , bob , charlie ] , # The greeting message will be sent to all participants at the start announcement = Msg ( \" system \" , \" Now you meet each other with brief self - introduction . \" , \" system \" , ) , ) as hub : # Quick construct pipeline to run the conversation await sequ enti al_ pi pel ine ([ alice , bob , charlie ]) # Delete participant agent from the hub and fake broadcast message hub . delete ( bob ) await hub . broadcast ( Msg ( \" bob \" , \" have to start my homework now , see you later ! \" , \" assistant \" , ) , ) await alice () await charlie () asyncio . run ( main () ) 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 41 43 44 45 46 47 49 50 51 52 53 55 56 57 58 59 21 1 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 Listing 8: An example of roadmap generated by the meta planner. { ... , \" roadmap \": { \" original_task \": \" Create comprehensive analysis report of Meta ( META ) (cid:44) stock that includes company overview and key financial metrics (cid:44) from Q1 2025 , with particular focus on profit margins .\" , \" decomposed_tasks \": [ { \" su bta sk_ spe if ic ti on \": { \" su btask _desc ripti on \": \" Research and gather comprehensive (cid:44) company overview information about Meta Platforms Inc (cid:44) .\" , \" input_intro \": \" Need to collect current information about (cid:44) Meta ' business operations , market position , and (cid:44) recent developments \" , \" exact_input \": \" Research Meta Platforms Inc . ( META ) - (cid:44) gather information about : business model and main (cid:44) revenue streams , recent major developments and (cid:44) strategic initiatives , market position in social (cid:44) media / metaverse space , current leadership and (cid:44) corporate structure , main products and services ( (cid:44) Facebook , Instagram , WhatsApp , Reality Labs , etc .) \" , \" expected_output \": \" comprehensive company overview (cid:44) document containing Meta ' business model , recent (cid:44) developments , market position , leadership , and main (cid:44) products / services \" , \" ir _ au ia _ to \": \" tavily - search for current (cid:44) company information and recent news \" } , \" status \": \" Planned \" , \" updates \": [] , \" attempt \": 0 , \" workers \": [] } , ... ] } , } 5.5 Meta Planner The Meta Planner4 extends the ReAct agent with advanced planning mechanisms that decompose complex tasks into manageable subtasks and orchestrate specialized worker agents for efficient completion. During initialization, the agent establishes two distinct toolkits: planner toolkit for high-level planning operations, and worker toolkit equipped with comprehensive tools, including shell command execution, file operations, web search, and filesystem access through MCP clients. Multiple MCP clients are configured to provide external tool integration, allowing the agent to access search functionality and manage filesystem operations within designated working directory. The Meta Planner operates on planning-execution pattern with three core components: (a) dataset structure containing roadmap information for managing session context and user inputs (refer to Example 8); (b) set of roadmap management tools for task decomposition and progress tracking; (c) Worker management tools for creating and supervising specialized worker agents. State persistence is built into the agents workflow, automatically saving progress at key stages, including post-reasoning and post-action states, which supports recovery from interruptions and the resumption of long-running tasks, thereby simplifying debugging during extended sessions. These capabilities make the meta planner especially well-suited for complex workflows such as comprehensive data analysis, research projects, content creation, and sophisticated problem-solving tasks that require coordinated execution across multiple domains. 4The implementation details of the Meta Planner can be found at https://github.com/agentscope-ai/agents cope/tree/main/examples/meta_planner_agent."
        },
        {
            "title": "6 Conclusions",
            "content": "We introduce AgentScope 1.0, flexible and extensible framework that leverages the ReAct paradigm to integrate reasoning and action for LLM-based agents. This integration facilitates seamless interaction between agents and their environments through dynamic tool use. By incorporating modular foundational agent components, efficient agent-level infrastructure, and customizable interfaces, AgentScope provides robust solution that bridges the gap between prototype agents and real-world applications. The framework also features suite of developer-friendly toolkits, which simplify the development process and enhance the usability and flexibility of agentic applications. Looking ahead, we envision AgentScope as practical foundation for building scalable, adaptive, and trustworthy agentic systems. By supporting tool-based perception and interaction, AgentScope effectively addresses the evolving demands of LLMbased applications, equipping agents to tackle increasingly complex real-world tasks with autonomy and precision."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Agno AGI Team. Agno: Full-stack framework for building multi-agent systems with memory, knowledge and reasoning. https://github.com/agno-agi/agno, 2024. Accessed: 2025-08-20. Anthropic. Model context protocol. https://www.anthropic.com/news/model-context-protocol, 2024a. Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024b. Arize-ai. Arize-phoenix. https://github.com/Arize-ai/phoenix, 2023. Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Yue Cui, Liuyi Yao, Shuchang Tao, Weijie Shi, Yaliang Li, Bolin Ding, and Xiaofang Zhou. Enhancing tool learning in large language models with hierarchical error checklists. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1635716375, 2025. Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. In Forty-first International Conference on Machine Learning, 2023. Google. A2a protocal. https://github.com/a2aproject/A2A, 2025. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jrgen Schmidhuber. Metagpt: Meta programming for multi-agent collaborative framework. In ICLR, 2024. Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats, and future research directions. arXiv preprint arXiv:2503.23278, 2025. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for general computing devices use. arXiv preprint arXiv:2508.04482, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. langchain ai. Langchain. https://github.com/langchain-ai/langchain, 2024. Langfuse. Langfuse. https://github.com/langfuse/langfuse, 2024. Zitao Li, Fei Wei, Yuexiang Xie, Dawei Gao, Weirui Kuang, Zhijian Ma, Bingchen Qian, Yaliang Li, and Bolin Ding. Kimas: configurable knowledge integrated multi-agent system. arXiv preprint arXiv:2502.09596, 2025. 23 Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920, 2024. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025. Meta. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https: //ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. Micrsoft. Playwright mcp. https://github.com/microsoft/playwright-mcp, 2025. Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael Jordan, et al. Ray: distributed framework for emerging {AI} applications. In 13th USENIX symposium on operating systems design and implementation, pp. 561577, 2018. OpenTelemetry. Opentelemetry. https://github.com/open-telemetry, 2024. Xuchen Pan, Dawei Gao, Yuexiang Xie, Yushuo Chen, Zhewei Wei, Yaliang Li, Bolin Ding, Ji-Rong Wen, and Jingren Zhou. Very large-scale multi-agent simulation in agentscope. arXiv preprint arXiv:2407.17789, 2024. Varatheepan Paramanayakam, Andreas Karatzas, Iraklis Anagnostopoulos, and Dimitrios Stamoulis. Less is more: Optimizing function calling for llm execution on edge devices. In 2025 Design, Automation & Test in Europe Conference, pp. 17, 2025. Pydantic. Pydantic. https://github.com/pydantic/pydantic, 2020. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International Conference on Learning Representations, 2024. Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8), 2025. Tavily. Tavily mcp. https://github.com/tavily-ai/tavily-mcp, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Chi Wang, Qingyun Wu, and the AG2 Community. Ag2: Open-source agentos for ai agents, 2024a. URL https://github.com/ag2ai/ag2. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yunjia Xi, Jianghao Lin, Yongzhao Xiao, Zheli Zhou, Rong Shan, Te Gao, Jiachen Zhu, Weiwen Liu, Yong Yu, and Weinan Zhang. survey of llm-based deep search agents: Paradigm, optimization, evaluation, and challenges. arXiv preprint arXiv:2508.05668, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang. Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint arXiv:2401.06201, 2024. 24 Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024."
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}