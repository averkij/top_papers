{
    "paper_title": "GenCompositor: Generative Video Compositing with Diffusion Transformer",
    "authors": [
        "Shuzhou Yang",
        "Xiaoyu Li",
        "Xiaodong Cun",
        "Guangzhi Wang",
        "Lingen Li",
        "Ying Shan",
        "Jian Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency."
        },
        {
            "title": "Start",
            "content": "GenCompositor: Generative Video Compositing with Diffusion Transformer Shuzhou Yang1, Xiaoyu Li2, Xiaodong Cun3, Guangzhi Wang2, Lingen Li4, Ying Shan2, Jian Zhang1 1SECE, Peking University 2ARC Lab, Tencent 3GVC Lab, Great Bay University 4The Chinese University of Hong Kong https://gencompositor.github.io/ 5 2 0 2 2 ] . [ 1 0 6 4 2 0 . 9 0 5 2 : r Figure 1. GenCompositor is capable of effortlessly compositing different videos guided by user-specified trajectories and scales. Our proposed method could preserve the background video content and also seamlessly integrate the dynamic foreground elements into the background video, which not only strictly follows user-given instructions but also physically coordinates with background environments."
        },
        {
            "title": "Abstract",
            "content": "Video compositing combines live-action footage to create video production, serving as crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. Corresponding authors. 1 To maintain consistency of the target video before and after editing, we revised light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, DiT fusion block is proposed using full self-attention, along with simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency. 1. Introduction Video compositing aims to edit target background video by adding foreground video from other sources, creating visually pleasing video assets. Compared to textor imageguided video editing, this process edits video productions based on real-world captured videos, acting as bridge between raw live-action video footage and final video work. However, classical process requires collaborative efforts of animators, videographers, and special effects artists, which is labor intensive. In this paper, we present generative video compositing, new video editing task that attempts to automate compositing process with generative models [4, 15]. It allows direct editing of background videos using video footage under user control, aligning with traditional video creation process in video composition stage. To composite visually appealing video results, three main challenges are explored in this paper: 1) Ensuring background consistency of the video before and after editing, 2) Preserving the identity and motion of the injected dynamic elements while harmonizing with background content, and 3) Enabling flexible user control, such as controllable size and motion trajectories. However, existing solutions cannot resolve these issues well. Controllable video generation [16, 17, 26, 40, 42] produces videos based on external conditions, including user-defined trajectories, images, and texts. Here, images provide identity information, and texts describe the motion to be generated. But images and text cannot precisely control the added elements at the pixel level, and current methods do not support exIn Contrast, video harmonization ternal video condition. methods [9, 29, 49] directly paste foreground element onto the target video frame-by-frame, and adjust its RGB values to blend with background. However, these cannot adaptively specify the position, size, motion trajectory, and other attributes of the added element. Moreover, pasting foreground video requires accurate corresponding segmentation masks, which may not always be feasible in practical use. To address these issues, we propose the first generative video compositing method, GenCompositor, aiming to automatically composite the background video with dynamic foreground elements. As shown in Fig. 1, given foreground video, background video, and user-provided trajectory in background (depicted by red line), GenCompositor injects foreground element to background video faithfully following trajectory, and predicts realistic interaction between added element and background. In Fig. 1 (a), the explosion effect even influences the content of background video, i.e., the cars fuel tank disappears after the explosion in the last frame. In Fig. 1 (b), GenCompositor predicts realistic shadow of the added butterfly across frames. We realize this by curating the first high-quality dataset for video compositing and proposing novel Diffusion Transformer (DiT) pipeline specifically tailored for generative video compositing. Our pipeline consists of three main components. Firstly, lightweight DiT-based background preservation branch is designed to ensure the consistency of the background in the edited results with the input videos. Secondly, to inject dynamic foreground elements, we propose DiT fusion block with full self-attention to fuse tokens from foreground elements with that of background videos. The generation mainstream consists of 42 DiT fusion blocks. Our ablation study demonstrates the advantages of this design over current cross-attention injection approaches [7, 43, 47, 52, 55]. Considering that layouts of added foreground elements should follow user control and often differ from those of background videos, directly applying RoPE to the tokens of foreground videos introduces leakage artifacts. Therefore, we propose novel position embedding method, Extended Rotary Position Embedding (ERoPE) to adaptively adjust the positions of foreground tokens. This enables high-quality user-specified conditional generation, even under pixel-unaligned conditions. Moreover, we develop some practical operations to enhance generalization and robustness. On the one hand, luminance augmentation is applied to foreground video for training, which strengthens the models generalization to diverse foreground conditions. On the other hand, we propose mask inflation to feed inaccurate masks to the model, enabling realistic interaction between newly added objects and environment. Experiments show that GenCompositor enables high-quality video compositing and can be used to add video effects. Overall, our contributions are as follows: We propose practical video editing task and solution, generative video compositing, that can automatically inject dynamic footage into the target video in generative manner, utilizing the diffusion model. We design novel techniques targeted to the unique characteristics of video compositing, including revised position embedding, full self-attention DiT fusion block, and lightweight background preservation branch. To train this model, we construct data curation pipeline and develop some practical training operations, such as mask inflation and luminance augmentation, to improve the generalization ability. Experiments prove that the proposed method outperforms existing potential solutions, enables effortless generative video compositing based on user-given instructions, and can be used for automatic video effects creation. 2. Related Work 2.1. Diffusion-based Video Editing In the field of AI-generated content, diffusion-based video editing [5, 11, 37] has made great progress with the success of video diffusion models [4, 21, 23, 24, 41]. Early works attempted to use priors from pre-trained models [8, 51]. 2 Such as finetuning T2I models [48] or designing attention mechanisms tailored to video [6, 13, 32, 44, 53, 54]. Recently, some methods have trained specialized video editing models to realize more effective and prolific results. Mou et al. [30] trained two additional ControlNet [57] branches to specify motion and inject ID, respectively. Tu et al. [40] followed similar strategy, but enabled more fine-grained control through denser key points and trajectories. Liu et al. [28] trained two control branches to guide spatial and temporal editing respectively. Bian et al. [2] trained video inpainting model, and edited videos through masking the target area, using Flux-fill-dev [3] to edit the first frame, and painted subsequent frames to propagate the editing effect. In general, training dedicated model achieves better performance than training-free methods. However, existing approaches mainly edit videos based on images or textual prompts, making it hard to precisely control the appearance and motion details of the editing effects. To this end, we first propose generative video compositing task, which directly edits videos based on dynamic footage from other sources, realizing more practical and fine-grained video editing. 2.2. Video Harmonization Video harmonization is classical low-level vision task [10, 25, 39], which aims to adjust the lighting of the added foreground element in the composited video to make it visually harmonious. Similar to other low-level methods [27, 50], Huang et al. [18] firstly introduced adversarial training to video harmonization. Lu et al. [29] collected dedicated dataset for training. Ke et al. [22] attempted to work in white-box manner, which regresses the image-level filter argument to predict harmonized results. Guo et al. [14] directly trained Triplet Transformer to simultaneously resolve multiple low-level video tasks, including harmonization. However, all of these methods focus only on adjusting the color of added elements. They all operate on composited videos and require foreground video and its accurate and pixel-aligned mask as input. Besides, these methods do not allow users to freely modify the size and motion trajectory of the added elements in the final video. To address these limitations, we introduce generative video compositing task that accommodates arbitrary mask inputs. Using foreground videos, source background videos, and userdefined trajectories and scales, our model autonomously generates composited videos that adhere to all conditions. 3. Task Definition Here, we define the task of generative video compositing. The inputs to this task are: background video vb to be edited, foreground video vf to be injected, and usergiven control c. The objective is to composite vb and vf following c, resulting in the final output z0. Compared with other video editing methods, there are some unique characteristics of this problem. First, the layout of the videos to be edited vb is highly consistent with editing results z0. Meanwhile, layouts of condition vf are not pixel-aligned with vb. We should consider these two different conditions while getting results z0. We use Latent Diffusion Model (LDM) [36] to realize this goal, starting from random latent noise zT , we aim to denoise zT to z0 based on the three inputs mentioned above. For training, given the Ground Truth video z0 containing desired foreground element. We add noise ϵ of various scales to it following predefined schedule defined as: zt = αtz0 + 1 αtϵ. (1) To train LDM, the added noise ϵ in zt is expected to be predicted by network ϵθ(), whose objective function is: min θ Ez0,ϵN (0,I),tϵ ϵθ(ztvb, vf , c, t)2 2, (2) where is sampling step, vb, vf , and are the three input signals. After training, ϵθ() could receive random latent noise zT , denoises it step by step, and decodes final latent through VAE decoder to get desired composited results. 4. Method We first introduce our input conversion in Sec. 4.1, showing how we convert and augment user-given instructions to model input. Then, we explain the proposed compositing pipeline, including background preservation branch (Sec. 4.2), DiT fusion block (Sec. 4.3), and Extended Rotary Position Embedding (ERoPE) (Sec. 4.4). 4.1. Input Conversion During inference, our inputs include background video, centered foreground video, as well as the user-given scale factor and trajectory curve, which control the size and position of added dynamic element in compositing result. However, for training, the motion curve is too sparse to be perceived by network. Moreover, giving source video with foreground objects, it is hard to get ground truth background video without foreground objects as the input. Therefore, we propose to convert inputs to trainable form. As shown in the left of Fig. 2, our compositing starts from two input videos (background and foreground). Given background video to be edited, users drag trajectory on its first frame, denoting the movement of added elements. Alternatively, users can click point (as shown in Fig. 1 (a)), and our method will automatically track the trajectory of this point in subsequent frames based on video optical flow. On this basis, the remaining problem is to determine the size of the newly added element. For the foreground input, we get its corresponding binary mask video through Grounded SAM2[35], then adjust its region size according to the user-given rescale factor. Finally, we adjust 3 Figure 2. Workflow of GenCompositor. GenCompositor takes background video and foreground video as input. Users can specify the trajectory and scale of the added foreground elements. We firstly convert user-given instructions to model inputs, then generate with background preservation branch and foreground generation mainstream, consisting of the proposed ERoPE and DiT fusion blocks. Following this, our model automatically composites input videos. ensure consistency between composited result and vb. Considering the layout of the edited result is pixel-aligned with that of vb. Directly plus the latent of vb with model latent has been able to faithfully inject background content, which is proven by previous ControlNet-like methods [2, 20, 31]. Intuitively, only using masked video has been able to inject background. As shown in Fig. 2, in masked video, the area where we aim to insert the element is marked black. However, some background videos naturally contain black content, which is not the desired masks and could confuse the network. Hence, we concatenate the masked video with the corresponding mask video as input. This guides the branch to focus on background preservation. More importantly, as mask video and masked video are pixel-aligned, we apply the same Rotary Position Embedding (RoPE) [38] to both, as shown in Fig. 4A. This strictly aligns the positions of the two and enables precise mask guidance. Subsequently, we inject these to the foreground generation mainstream for video compositing. As BPBranch is designed to inject background only, its main purpose is to align masked video features with the latent of mainstream model. Instead of deeply analyzing masked features, we simply design lightweight control branch, which consists of two normal DiT blocks, to align with the latent of mainstream. Meanwhile, as we only want to use the background region of its output, masked token injection method is applied to prevent interference from BPBranch to the generated foreground content, which is formulated as: zt = zt + (1 M) zBP Branch, (3) where zt is the latent of mainstream model, zBP Branch is the output of BPBranch, is the binary mask video. This process is visualized in detail in the upper part of Fig. 3. 4.3. Foreground Generation Mainstream In foreground generation, our goal is to faithfully preserve the identity and dynamic messages of foreground elements from other sources in composited results. Previously, crossattention is commonly used to inject conditions [12, 56], Figure 3. DiT fusion block receives concatenated tokens, some are to-be-generated tokens (blue) and some are unaligned conditional tokens (green). It fuses the two via pure self-attention. The final tokens with gradient color represent the mixture of generated tokens and masked conditional tokens from the BPBranch. the position of this rescaled mask video based on the designated trajectory curve. In this way, we adaptively rescale and reposition foreground masks to produce mask video. In order to integrate with environment realistically, Gaussian filter is used to inflate the mask video, i.e., mask part actually covers the region outside the object boundary, which brings buffered region around the added element, and forces generative model to predict it for integration. Meanwhile, an accurate foreground mask is usually inaccessible. Mask inflation allows for generating reasonable results despite the imperfect foreground mask. Finally, masked video can be obtained by simply masking the object in the source video with mask video for training. By converting user inputs of background video and trajectory to masked video and mask video as model input, we eliminate the need for paired training data (background and composited videos), and enable training with source videos and masks, where mask video could be extracted by existing tools like SAM2. Details of our data construction pipeline are shown in Sec. 7 of supplementary material. 4.2. Background Preservation Branch As video editing task, our primary objective is to preserve background content and only edit desired regions of vb. We propose Background Preservation Branch (BPBranch) to 4 such as text prompts or camera poses. However, we find that although cross-attention is advantageous for addressing semantic conditions, it does not effectively utilize low-level conditional information for our task. To faithfully inherit foreground condition, we propose to concatenate the tokens of foreground with the tokens to be denoised as shown in Fig. 2, then calculate its self-attention to fully fuse these two messages through the DiT fusion block. As depicted in Fig. 3, given the tokens of noisy latent and foreground condition, DiT fusion block concatenates them in token-wise manner, instead of classical channelwise concatenation. This is because the layout features of foreground condition and generated results are not pixelaligned. Roughly concatenating their features in channel dimension will cause severe content interference, which leads to training collapse. DiT fusion block then predicts the noise in noisy latent by calculating self-attention on the concatenated tokens, which contain both foreground condition and masked background. Note that our generated results are the processed latent that is boxed by red in Fig. 3. Hence, we fuse the tokens of BPBranch only with the processed noisy tokens and pass them to the next block. Finally, we only decode the part corresponding to the input noisy tokens to obtain composited video, as shown in the right of Fig. 2. Meanwhile, newly added content should visually coordinate with background. Some of its attributes, such as the lighting, need to be properly adjusted during generation. To enable our model to learn this adjustment adaptively, we develop luminance augmentation strategy for training. In each iteration, we use gamma correction to the foreground video, with the gamma parameter randomly selected from range of 0.4 to 1.9. This changes the lightness of foreground conditions, offsetting from the source video. Consequently, based on our DiT fusion block that fully fuses foreground elements with model latent, foreground generation mainstream automatically learns the capabilities of foreground harmonization. It is important to note that luminance augmentation is only used in the training process. 4.4. Extended Rotary Position Embedding Layout-consistent or pixel-aligned conditions can be well utilized by DiT with the same RoPE as the input latent. However, for pixel-unaligned conditions, this operation could cause content interference. As shown in Fig. 5, in addition to the input background video, our model also takes the mask video that indicates the position and size of added objects, and the foreground videos, which are pixelunaligned with the background video or generated results. We first concatenate these tokens with the same RoPE (w/ RoPE), which leads to obvious artifacts in composited results (boxed in red). We found that the shapes and positions of artifacts are consistent with the foreground videos, and using the same RoPE will make the model fuse at the same Figure 4. RoPE and our ERoPE. (A) 3D RoPE assigns labels to embeddings of videos. (B) For videos with inconsistent layout, our ERoPE extends RoPE by assigning unique labels to each embedding from different videos, avoiding interference. Figure 5. ERoPE is superior in fusing layout-unaligned videos. As dynamic content in foreground video is often centered, using RoPE leads to content interference as shown in red box of w/ RoPE, while our ERoPE resolves this issue well. position between the foreground and background videos, leading to content interference. Another optional way is cross-attention, which extracts abstract semantics to support layout-unaligned control signals. However, such highlevel feature cannot faithfully inherit detailed appearance and action features of foreground videos and is not suitable for video compositing, as proved by Sec. 5.4. To this end, we developed new embedding strategy tailored to the unaligned nature between foreground footage and background video, named ERoPE. ERoPE fuses unaligned videos in more faithful manner without additional training parameters. As shown in Fig. 4B, we extend the RoPE range and assign unique position label to each embedding of the two unaligned videos in the height dimension, strictly staggering the feature position of the two videos. As shown in Fig. 5, this strategy efficiently increases compositing performance and eliminates artifacts caused by interference of unaligned content. Our proposed ERoPE can exploit arbitrary layout-unaligned video conditions without additional training parameters, and here we use it for video compositing. 5 Figure 6. Visual comparison with video harmonization. The compared methods cannot achieve satisfactory results with jagged artifacts at the edges of foreground elements, inconsistent color or lighting, while our method achieves better performance. Table 1. Quantitative comparison with video harmonization methods. The best results are highlighted in bold. Table 2. Quantitative results of comparison with trajectorycontrolled generation. The best results are highlighted in bold. Metrics PSNR SSIM CLIP LPIPS Harmonizer VideoTripletTransformer GenCompositor 39.7558 0.9402 0.9614 0.0412 40.0251 0.9297 0.9564 0.0455 42.0010 0.9487 0.9713 0.0385 Metrics Subject Cons Background Cons Motion Smooth Aesthetic Quality"
        },
        {
            "title": "Revideo GenCompositor",
            "content": "88.44% 88.02% 92.45% 92.90% 98.03% 96.85% 49.33% 48.56% 89.75% 93.43% 98.69% 52.00% 5. Experiments Given that there are no existing works for generative video compositing like ours, we compare GenCompositor with two related tasks, i.e., video harmonization, and trajectorycontrolled video generation. We first introduce implementation details in Sec. 5.1, showcase comparisons in Sec. 5.2 and Sec. 5.3. To validate the effectiveness of our key components, we conduct an ablation study in Sec. 5.4. 5.1. Implementation Details GenCompositor is DiT model with 6B Transformer, consisting of the proposed DiT fusion blocks and background preservation branch. We reuse pre-trained VAE module of CogVideoX to generate videos in latent space. We train our new architecture on 8 H20 GPUs from scratch. In inference, GenCompositor takes about 65s to generate video at 480720 with 49 frames within 34GB VRAM. 5.2. Comparison with Video Harmonization Considering the limited number of open-source methods in video harmonization, we compare our method with two recent approaches whose codes are available: Harmonizer [22] and VideoTripletTransformer [14]. As these methods cannot control the motion trajectory, we only compare the ability to harmonize foreground elements. We also manually paste foreground footage from other sources into the background video for comparison. As shown in left of Fig. 6, noticeable jagged artifacts appear at the edges of added elements in manually paste and Harmonizer, due to the imperfection of the segmentation mask of the foreground video. Additionally, the color style of the newly added explosion effect does not harmonize well with background video in these harmonization approaches. In contrast, our method effectively addresses these jagged artifacts caused by inaccurate masks and produces more harmonious results. In other examples of Fig. 6, we manually adjust the lighting of the foreground elements and test the video harmonization ability. Our method consistently outperforms the other methods, demonstrating its superiority. For quantitative comparison, we use the well-known HYouTube [29] dataset, where the foreground videos, segmentation masks, and source videos are available. Four well-known metrics, PSNR, SSIM [46], CLIP [33] and LPIPS [58] are used to measure performance in Tab. 1. One can see that GenCompositor outperforms other harmonization methods across all metrics, showing its superiority over specialized methods in video harmonization. We also conduct user study in Sec. 10 of supplementary material. 5.3. Comparison with Controllable Generation Our method enables trajectory-controlled generation, where users can specify the motion trajectory and size of newly added elements in results. Considering our added element is dynamic video and there is no method using the same condition as ours, we compare with SOTA trajectory-controlled video generation and editing methods, Tora [59] and Revideo [30]. Visual results are given in Fig. 7, where we report background videos, foreground videos, Tora generation re6 Figure 7. Visual comparison with trajectory-controlled video generation. All methods share the same trajectory, which is plotted as red line in the results of Tora. Obviously, Tora and ReVideo remain with problems such as inconsistent foreground element IDs across frames and uncontrollable foreground element motions. Our method resolves these issues well. sults containing the red trajectory curves, Revideo editing results, and our results. Note that Tora still requires additional textual prompts as condition, and Revideo has to edit the first frame as image condition, but GenCompositor requires neither of these priors. Although Tora and GenCompositor could generate results that follow the trajectory, Tora cannot maintain the ID consistency of the added element and cannot strictly follow the user-specified trajectory. In contrast, GenCompositor could strictly follow the trajectories to generate composited videos, and the ID and motion of the element are inherited from foreground videos faithfully. We believe these advantages come from different task settings. Compared with directly generating video from an image such as Tora, we composite foreground video with background following the trajectories. This is inherently more conducive to inheriting the ID and detailed motion of foreground elements. Meanwhile, Revideo also aims to drag the added element in the first frame to move along given trajectory in subsequent frames, but its limited performance leads to non-robust results, elements may disappear in its predicted frames as shown in Fig. 7. To conduct quantitative evaluations, we utilize common benchmark, i.e., VBench [19], to analyze the quality of generation from 4 dimensions: 1) Subject Consistency: consistency about subjects in the video. 2) Background Consistency: consistency about the video background. 3) Motion Smoothness: motion quality of the generated video. 4) Aesthetic Quality: subjective visual quality of the video. We believe these four metrics to be most relevant to our task and generate 40 sets of videos using Tora, Revideo, and our method, respectively. As shown in Tab. 2, our method achieves the best average scores across all four metrics. In addition, we conduct user study in Sec. 10 of supplementary material to compare intuitive quality of different methods. 5.4. Ablation Study To analyze the effectiveness of each proposed component, we conduct an ablation study with four settings. Specifically, we attempt two potential designs of GenCompositor, one removes the background preservation branch (w/o BPBranch), the other uses cross-attention to inject foreground elements without the proposed DiT fusion block (w/o fusion block). We discuss their architecture details in Sec. 12 of supplementary material. Moreover, we delete two designs. One does not inflate binary mask (w/o mask inflation), the other directly inputs original foreground videos for training, without luminance augmentation (w/o augmentation). Visual comparisons are given in Fig. 8, where we provide the input foreground element and inflated mask in the first row. The following three rows show the results of different settings. An interesting observation is that results of manually paste, w/o augmentation and w/o mask inflation all exhibit obvious jagged artifacts at the edges of foreground elements. We believe that, for w/o augmentation, due to the powerful learning ability of network, it totally inherits and overfits the content of the foreground video without any adjustment. For w/o mask inflation, as we provide pixel-aligned mask to model, it can only pro7 Figure 8. Visual ablation results. The manually paste results showcase jagged artifacts. w/o fusion block cannot inject element faithfully. w/o augmentation and w/o mask inflation contain jagged artifacts at the edge of element. w/o BPBranch cannot realistically adjust foreground element. The fullmodel setting performs best, which naturally fuses foreground elements with the background video. Table 3. Quantitative results of ablation study. The best results are highlighted in bold. Settings w/o fusion block w/o BPBranch w/o augmentation w/o mask inflation fullmodel PSNR 19.8940 40.0099 39.8040 41.8553 42.0010 SSIM CLIP LPIPS 0.1535 0.9341 0.8015 0.0432 0.9709 0.9378 0.0520 0.9629 0.9295 0.0409 0.9701 0.9422 0.0385 0.9713 0.9487 Subject cons Background cons Motion smooth Aesthetic quality 88.85% 88.77% 88.00% 89.72% 89.75% 92.21% 89.62% 89.97% 91.62% 93.43% 98.34% 97.25% 98.30% 98.28% 98.69% 48.85% 51.51% 50.73% 50.87% 52.00% cess the foreground element in this limited region and cannot adjust surrounding pixels to fuse with the background, leaving artifacts at the edge. The other two settings address the border artifacts, but present other limitations. w/o fusion block cannot faithfully inject ID and detailed motion of foreground elements, but successfully predicts the flame, which is semantically consistent with the foreground condition, indicating that cross-attention is good at injecting semantic information, but is not applicable in our task. Although w/o BPBranch also produces realistic background, this end-to-end learning of both background video and added foreground elements increases training difficulty, limiting its performance. In its results, the foreground element is not perceptually consistent with main video. For quantitative evaluation, we use four ablation settings, together with fullmodel, to realize video harmonization and trajectory-controlled video generation, respectively. Related test data is the same as Sec. 5.2 and Sec. 5.3. As shown in Tab. 3, fullmodel outperforms all ablation settings on all metrics. We believe this objectively demonstrates the significance of each component in our method. 6. Conclusion This paper introduces novel video editing task, generative video compositing, which allows interactive video editing using dynamic visual elements. Specifically, we developed the first generative method, GenCompositor, which is designed to address three main challenges of this task: maintaining content consistency before and after editing, It injecting video elements, and facilitating user control. comprises three main contributions. Firstly, lightweight background preservation branch is utilized to inject tokens of background videos into the mainstream. Secondly, the foreground generation mainstream incorporates novel DiT fusion blocks to effectively fuse the external video condition with the background latent. Finally, we revised novel position embedding, ERoPE, to force the model to add external elements to the desired positions in results with desired scales, adaptively. Notice that ERoPE points out new effective way to utilize layout-unaligned video conditions for generative model without any additional computational cost. The first paired dataset, VideoComp, is also proposed in this paper. Comprehensive experiments demonstrate the effectiveness and practicality of our proposed method. Future work. Although this paper enables video injection and realistic interaction, some challenging topics still exist in extreme conditions. For example, we employ gamma correction, simple yet effective augmentation, to address most common cases well. But it may not be robust enough for sophisticated extreme background lighting. Meanwhile, the added elements cannot undergo complex occlusion changes with environment. We believe the former 8 can be solved by replacing other luminance augmentation methods, and the latter can be achieved by adding depthaware or 3D priors. Since GenCompositor already works for most scenarios, we leave these issues for future work."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 1 [2] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Anylength video inpainting and editing with plug-and-play context control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, 2025. 3, 4 [3] Blackforestlabs. Flux. https://github.com/blackforest-labs/flux, 2024. 3 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [5] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [6] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 3 [7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 2 [8] Duygu Ceylan, Chun-Hao P. Huang, and Niloy J. Mitra. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 2 [9] Xiuwen Chen, Li Fang, Long Ye, and Qin Zhang. Deep video harmonization by improving spatial-temporal consistency. Machine Intelligence Research, 2024. 2 [10] Yi Dong, Yuxi Wang, Zheng Fang, Wenqi Ouyang, Xianhui Lin, Zhiqi Shen, Peiran Ren, Xuansong Xie, and Qingming Huang. Movingcolor: Seamless fusion of fine-grained video color enhancement. In Proceedings of the 32nd ACM International Conference on Multimedia, 2024. 3 [11] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 2 [12] Xiao FU, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di ZHANG, and Dahua Lin. 3DTrajmaster: Mastering 3d trajectory for multientity motion in video generation. In The Thirteenth International Conference on Learning Representations, 2025. 4 [13] Jiaqi Guo, Lianli Gao, Junchen Zhu, JiaxinZhang, Siyang Li, and Jingkuan Song. Magicvfx: Visual effects synthesis in just minutes. In Proceedings of the 32nd ACM International Conference on Multimedia, 2024. 3 [14] Zonghui Guo, Xinyu Han, Jie Zhang, Shiguang Shan, and Haiyong Zheng. Video harmonization with triplet spatiotemporal variation patterns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3, 6 [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In Advances in Neural Information Processing Systems, 2022. 2 [16] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [17] Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, and Qinglin Lu. Hunyuancustom: multimodal-driven architecture for customized video generation. arXiv preprint arXiv:2505.04512, 2025. 2 [18] Hao-Zhi Huang, Sen-Zhe Xu, Jun-Xiong Cai, Wei Liu, and Shi-Min Hu. Temporally coherent video harmonization using adversarial networks. IEEE Transactions on Image Processing, 2020. 3 [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 7 [20] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European conference on computer vision, 2024. 4 [21] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Multi-task video generative foundation model with full attention. arXiv preprint arXiv:2503.19907, 2025. 2 [22] Zhanghan Ke, Chunyi Sun, Lei Zhu, Ke Xu, and Rynson W. H. Lau. Harmonizer: Learning to perform white-box imIn European conference on age and video harmonization. computer vision, 2022. 3, 6 [23] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [24] Lingen Li, Zhaoyang Zhang, Yaowei Li, Jiale Xu, Wenbo Hu, Xiaoyu Li, Weihao Cheng, Jinwei Gu, Tianfan Xue, and Ying Shan. Nvcomposer: Boosting generative novel view 9 synthesis with multiple sparse and unposed images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 2 [25] Yuhang Li, Jincen Jiang, Xiaosong Yang, Youdong Ding, and Jian Jun Zhang. Harmony everything! masked autoencoders for video harmonization. In Proceedings of the 32nd ACM International Conference on Multimedia, 2024. 3 [26] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Ying Shan, and Yuexian Zou. Image conductor: Precision control for interactive video synthesis. Proceedings of the AAAI Conference on Artificial Intelligence, 2025. 2 [27] Risheng Liu, Zhiying Jiang, Shuzhou Yang, and Xin Fan. Twin adversarial contrastive learning for underwater image enhancement and beyond. IEEE Transactions on Image Processing, 2022. 3 [28] Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo. Vfx creator: Animated visual effect generation with controllable diffusion transformer. arXiv preprint arXiv:2502.05979, 2025. [29] Xinyuan Lu, Shengyuan Huang, Li Niu, Wenyan Cong, and Liqing Zhang. Deep video harmonization with color mapping consistency. arXiv preprint arXiv:2205.00687, 2022. 2, 3, 6 [30] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. In Advances in Neural Information Processing Systems, 2024. 3, 6 [31] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. Proceedings of the AAAI Conference on Artificial Intelligence, 2024. 4 [32] Chenyang QI, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 3 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, 2021. 6 [34] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations, 2025. [35] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 3, 1 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 3 [37] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Edit-A-Video: Single video editing with object-aware consistency. In Proceedings of the 15th Asian Conference on Machine Learning, 2024. 2 [38] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 4 [39] Jianchao Tan, Jose Echevarria, and Yotam Gingold. Palettebased color harmonization. IEEE Transactions on Visualization and Computer Graphics, 2025. 3 [40] Yuanpeng Tu, Hao Luo, Xi Chen, Sihui Ji, Xiang Bai, and Hengshuang Zhao. Videoanydoor: High-fidelity video object insertion with precise motion control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, 2025. 2, [41] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2 [42] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 2 [43] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation In Proceedings of with 360-degree video diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2 [44] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. 3 [45] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 6 [59] Zhenghao Zhang, Junchao Liao, Menghao Li, ZuoZhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 6 [60] Xianpan Zhou. Tiger200k: Manually curated high visual arXiv preprint quality video dataset from ugc platform. arXiv:2504.15182, 2025. 1 pretrained language models. In Advances in Neural Information Processing Systems, 2024. 1 [46] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 2004. 6 [47] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, 2024. [48] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 3 [49] Zeyu Xiao, Yurui Zhu, Xueyang Fu, and Zhiwei Xiong. Tsa2: Temporal segment adaptation and aggregation for video harmonization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024. 2 [50] Shuzhou Yang, Moxuan Ding, Yanmin Wu, Zihan Li, and Jian Zhang. Implicit neural representation for cooperaIn Proceedings of the tive low-light image enhancement. IEEE/CVF International Conference on Computer Vision, 2023. 3 [51] Shuzhou Yang, Chong Mou, Jiwen Yu, Yuhan Wang, Xiandong Meng, and Jian Zhang. Neural video fields editing. arXiv preprint arXiv:2312.08882, 2024. 2 [52] Shuzhou Yang, Xiaodong Cun, Xiaoyu Li, Yaowei Li, and Jian Zhang. 4dvd: Cascaded dense-view video diffusion model for high-quality 4d content generation. arXiv preprint arxiv:2508.04467, 2025. 2 [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [54] Danah Yatim, Rafail Fridman, Omer Bar-Tal, and Tali Dekel. Dynvfx: Augmenting real videos with dynamic content. arXiv preprint arXiv:2502.03621, 2025. 3 [55] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arxiv:2308.06721, 2023. 2 [56] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 4 [57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 3 [58] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of GenCompositor: Generative Video Compositing with Diffusion Transformer"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 10. Generalizability. After training for video compositing, our method seamlessly enables video inpainting and the removal of target objects from videos with blank foreground condition. output label of QWen, we employ Grounded SAM2 [35] to segment the elements in the video and save its foreground video and mask video. Note that the mask video shows the original motion trajectory. However, when saving the foreground video, we center the dynamic element in each frame, eliminating its global position and trajectory information. This approach allows us to control the trajectory of the resulting video totally based on the mask video, rather than relying on the position in the foreground video. 7.2. Data filtering . To ensure high-quality dataset construction, we filter out unsatisfactory cases according to several rules. As illustrated in right of Fig. 9, our filtering principles are as follows: First, we exclude cases where QWen returns NULL, indicating that there is no significant object in the video. Second, for videos containing multiple elements, we select only the element with the highest probability. Third, we exclude suboptimal cases where elements have incomplete or excessively fragmented structures. Finally, we manually filter out videos that are visually unappealing. 8. Generalization Ability In addition to the effective video compositing capability of the proposed GenCompositor, our method also demonstrates impressive generalizability in video inpainting and the removal of target objects from videos, which can be achieved by simply replacing the foreground condition with blank foreground video. As shown in Fig. 10, we employ blank foreground condition to remove target objects in source videos through the trained GenConpositor. We first use SAM2 [34] to obtain mask videos corresponding to the objects to be removed in source videos. Then, as mentioned in Sec. 4.1, the same mask inflation operation is applied to get the inflated mask videos and masked videos. Given the blank foreground Figure 9. Dataset construction pipeline. We construct VideoComp dataset with two stages: data curation and data filtering. The former includes three steps: collection, labeling, and segmentation. Then, we select high-quality subset based on several rules. 7. Dataset Construction Since there is no existing dataset for video compositing, we carefully build dataset called VideoComp for this task. We present scalable dataset construction pipeline that utilizes advanced models as tools for data construction [1, 35, 45]. The proposed VideoComp dataset contains 61K groups of videos, each containing three video samples, i.e., the source video, the foreground video, and its corresponding mask video. The source video is high-quality raw video and the other two are extracted from it. As shown in Fig. 9, our dataset construction process consists of two main steps, data curation and data filtering. 7.1. Data curation . (1) We firstly collect about 240K cinematic HD videos that meet high aesthetic standards and large motion. Moreover, recently released Tiger200K [60], which consists of 169K videos, is also included as our data source. For these total 409K videos, we propose an end-to-end workflow to process each of them as follows. (2) We distinguish the labels of prominent dynamic elements in the source video. We set up two questions to call CogVLM [45] and QWen [1] respectively, where the former is visual language model and the latter is large language model. We input the video frames and then ask CogVLM to get the detailed description of video. Based on this answer and our second question, QWen [1] is used to identify prominent dynamic elements, or return NULL if there are none. (3) Based on the condition, GenCompositor removes objects and paints the masked region well. This successful generalization study demonstrates the significance of our proposed generative video compositing task, i.e., this new video editing task inherently supports other video-related downstream tasks. 9. Implementation Details We refer to CogVideoX-I2V-5B to design our model. Similarly to other DiT models, our GenCompositor also consists of 3 main components, Transformer, VAE, and text encoder, where we inherit the pre-trained weights of VAE and text encoder of CogVideoX in our model and do not train them. We only train Transformer here. Notice that as we provide null text during training, although GenCompositor inherits text encoder, for inference, video compositing is totally based on the input videos and user-specified control, which is consistent with classical handmade process. The number of parameters of each component is: VAE (215,583,907), text encoder (4,762,310,656), Transformer (5,872,247,936), totaling 10,850,142,499. Following previous work, we only consider parameter number of Transformer in latent diffusion model, and claim that GenCompositor is 6B model. Our Diffusion Transformer contains two branches, one background preservation branch containing 301,568,576 parameters, and one foreground generation mainstream with 5,570,679,360 parameters. Since the original CogVideoX-I2V-5B is designed for image-to-video generation, we revised novel module (DiT Fusion Block) to meet the characteristics of our task, video compositing, and train this new model from scratch. As shown in Fig. 2 in main paper, our new DiT model has three patchify modules, which aim to patchify the features of input videos encoded by the VAE encoder into tokens that can be processed by Transformer, where the features encoded by VAE encoder contains 16 feature channels. Notice that for the background preservation branch, its inputs are mask video and masked video, which are concatenated on channel dimension. Hence,the input channel number of its patchify is 32. For foreground generation mainstream, its inputs are masked video, noise input, and foreground video condition. During training, the noise input is the combination of random noise and masked video, similar to other diffusion models. For inference, the noise input is pure gaussian noise. Notice that noise input is concatenate with masked video in channel dimension. Hence the input channel number of blue patchify in Fig. 2 is 32, and the input channel number of green patchify for foreground video condition is 16. Figure 11. User study comparison. Our method receives the most user preference for both video harmonization(a) and trajectorycontrolled generation(b). Figure 12. Loss curves of applying ERoPE. We apply ERoPE by extending along different optional dimensions, height, width, and timing. For comparison, we showcase the loss curve of ablation setting that applies the same RoPE on both masked video and foreground video, which is marked in pink. 10. User Study We conduct user study for both video harmonization and trajectory-controlled video generation, compared with Harmonizer and VideoTripletTransformer, Tora and Revideo, respectively. For each task, we provide 20 sets of visual comparisons and invite 19 professional volunteers to select their most preferred results. As shown in Fig. 11(a), most users prefer the video harmonization capability of our method, while Harmonizer and VideoTripletTransformer are equally favored. As to trajectory-controlled video generation, as shown in Fig. 11(b), our method obviously outperforms other related algorithms, which is consistent with the visual and quantitative results provided in the main paper. 11. Loss Curve of Applying ERoPE To study the impact of extending position embedding in different dimensions on performance, we visualize the training loss curves of four settings in Fig. 12, where we apply ERoPE by applying position embedding along three dimensions, i.e., height, width, and timing, which are marked as brown curve (w/ ERoPE h), blue curve (w/ ERoPE w), Figure 13. Architectures of ablation settings. We visualize architecture details of the two ablation settings, w/o BPBranch (upper) and w/o fusion block (lower). The former deletes the control branch, and the latter uses cross-attention to inject foreground condition instead of the proposed fusion block. Their control signal conversion is the same as full model. So we omit it and mainly showcase model designs. and green curve (w/ ERoPE t), respectively. For comparison, we also attach the loss curve of an ablation setting as pink curve in Fig. 12, which is the training loss of using the same RoPE for both masked and foreground videos (w/o ERoPE). One can see that the training loss curves of three settings that uses ERoPE are all obviously lower than w/o ERoPE. Meanwhile, the loss curves of the three settings are highly consistent with each other, i.e., the brown, blue, and green curves almost overlap. This comparison strongly prove the importance of our proposed ERoPE for utilizing layout-unaligned video conditions, and we use w/ ERoPE in the main paper. 12. Ablation Architectures As shown in Fig. 13, we visualize the architecture details of our two ablation settings, w/o BPBranch abd w/o fusion block, mentioned in the main paper. The former eliminates the background preservation branch, but still concatenates foreground condition with noisy tokens at the spatiallevel and utilize the proposed DiT fusion block to fusing them, combined with the EROPE. Whilst w/o fusion block remains the background branch, but replace the DiT fusion block with normal DiT module. To inject foreground control, it utilize cross-attention as shown in the lower of Fig. 13. 13. Interactivity In order to validate the flexible interoperability of the proposed method, we provide additional examples where the user specifies different trajectories and scale factors, respectively. In Fig. 14, given the foreground and background videos, we manually specify two different trajectories to GenCompositor for generation. The added elements exhibit different trajectories, which are highly consistent with the input trajectories. In Fig. 15, we provide three different scale factors in the same example. Obviously, the userspecified factors directly affect our mask videos and control the size of elements in the final results. 14. More Visual Results We provide more visual results in Fig. 16 to showcase our superior video compositing capability. GenCompositor enables seamless video elements injection according to the user-given interaction. We visualize the designated trajectories as right dots and lines in the first frame of background videos. One can see that GenCompositor precisely injects foreground elements into the user-given positions and harmonizes their color. On the other hand, the background environmental changes caused by the inserted objects (such as the shadows in the red box) are also automatically predicted by our generative model, proving the superiority of generative video compositing task. 3 Figure 14. Generating with different user-provided trajectories. We apply two different trajectories to the same foreground-background video pairs. We can see that, given the same foreground and background videos with different trajectories, GenCompositor could generate different contents that strictly follow their corresponding trajectories. Figure 15. Generating with different user-provided scale factors. Our method produces mask videos that follow the user-provided factors and control the size of added elements in the final results. Figure 16. More visual results of GenCompositor. Our method enables seamless video elements injection and user interaction. On the one hand, GenCompositor precisely injects foreground elements into the user-given position and harmonizes their color. On the other hand, the background environmental changes caused by the inserted objects (such as the shadows in the red box) are also automatically predicted by our generative model, proving the superiority of generative video compositing."
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent",
        "GVC Lab, Great Bay University",
        "SECE, Peking University",
        "The Chinese University of Hong Kong"
    ]
}