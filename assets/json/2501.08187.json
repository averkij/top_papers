{
    "paper_title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
    "authors": [
        "Yin Fang",
        "Xinle Deng",
        "Kangwei Liu",
        "Ningyu Zhang",
        "Jingyang Qian",
        "Penghui Yang",
        "Xiaohui Fan",
        "Huajun Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights."
        },
        {
            "title": "Start",
            "content": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following Yin Fang 1,3,*, Xinle Deng 1,4,*, Kangwei Liu 1,4,*, Ningyu Zhang 1,4,, Jingyang Qian 3,5, Penghui Yang 3,5, Xiaohui Fan 3,5,6,, Huajun Chen 1,2,, 1 College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China. 2 ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou 311200, China. 3 College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China. 4 School of Software Technology, Zhejiang University, Ningbo 315048, China. 5 Future Health Laboratory, Innovation Center of Yangtze River Delta, Zhejiang University, Jiaxing 314100, China. 6 Innovation Center in Zhejiang University, State Key Laboratory of Component-Based Chinese Medicine, Hangzhou 310058, China. Equal contribution Corresponding Author: Prof. Ningyu Zhang, e-mail: zhangningyu@zju.edu.cn, Prof. Xiaohui Fan, e-mail: fanxh@zju.edu.cn, and Prof. Huajun Chen, e-mail: huajunsir@zju.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models excel at interpreting complex natural language instructions, enabling them to perform wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the language of cellular biology, capturing intricate gene expression patterns at the single-cell level. However, interacting with this language through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, multi-modal AI copilot that leverages natural language as medium for more direct and flexible single-cell analysis. We construct comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical taskssuch as cell type annotation, conditional pseudo-cell generation, and drug sensitivity predictionusing straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights. 5 2 0 2 4 1 ] . [ 1 7 8 1 8 0 . 1 0 5 2 : r"
        },
        {
            "title": "Main",
            "content": "Advances in artificial intelligence (AI), particularly large language models (LLMs) such as GPT-4 [1], PaLM [2], LLaMA [3], and Claude [4], have transformed natural language into powerful medium for managing real-world tasks [5, 6]. Similarly, in the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as language of cellular biology, encoding diverse gene expression patterns. Like grammar and vocabulary in natural language, scRNA-seq data reveals the complexity of biological systems, with each cell type and state presenting unique expression [7, 8, 9]. To unlock the potential of this life science language, researchers have increasingly turned to natural language processing (NLP) technologies to analyze single-cell gene expression data. These approaches typically rely on extensive gene expression datasets [10, 11] for pre-training, followed by fine-tuning for specific downstream tasks [12, 13, 14, 15]. While effective, such methods often demand significant domain expertise and face challenges in adaptability. As an alternative, recent efforts have reformulated single-cell gene expression data into sequences of gene names ranked by expression levels, enabling language models to process both task-specific instructions and gene name lists directly [16, 17]. However, focusing exclusively on top-expressed genes overlooks important information from lower-expressed ones, while incorporating all genes inflates context length and computational demands. Furthermore, converting quantitative gene expression data into text risks losing essential numerical precision. To address these limitations, it is crucial to develop an approach that bridges the structured numerical nature of single-cell data with the expressive flexibility of natural language. Drawing inspiration from how humans integrate multiple sensory inputs to enhance comprehension, we propose InstructCell, multi-modal AI copilot specifically designed for single-cell analysis. It interprets numerical single-cell data and natural language instructions, as well as generates outputs in either modality, effectively bridging the gap between these distinct data types. First, we construct multi-modal single-cell instruction dataset that unifies essential single-cell analysis tasks into cohesive collection. Each cell in the dataset is represented by its gene expression profile and enriched with biological attributes specifying species, tissue, sequencing protocol, and other pertinent biological contexts, all organized into natural language instructions. To equip our AI copilot with fundamental conversational abilities, we incorporate traits reflecting diverse personalities, motivations, and proficiency levels. Using LLMs, we simulate various communication styles, enabling the copilot to adapt to diverse research contexts and expertise levels. Second, building upon this dataset, we develop multi-modal cell language model capable of harmonizing single-cell gene expression data with textual information. The architecture includes Q-Former module for embedding gene expression profiles, backbone pre-trained language model (LM) for robust textual processing, and cell reconstruction block for generating detailed gene expression profiles. Through instruction tuning, the model gains domain-specific expertise in single-cell analysis, enabling it to adeptly handle interleaved biological and textual data. This design allows InstructCell to handle diverse input and output formats, unifying tasks of cell comprehension and generation. Finally, we thoroughly evaluate InstructCell across range of single-cell analysis tasks, demonstrating robustness to varied instruction styles and its ability to produce accurate, relevant outputs. Extensive experiments highlight its capacity to uncover biological insights and validate the necessity of each architectural component. InstructCell extends instruction-following capabilities to the cell-language multi-modal space, laying the groundwork for advancing single-cell research."
        },
        {
            "title": "Overview of InstructCell",
            "content": "In this paper, we propose InstructCell, multi-modal AI copilot specifically designed for single-cell analysis. The development of InstructCell involves two critical components: (1) constructing multi-modal single-cell instruction data and (2) training multi-modal cell LM. An overview of InstructCell is illustrated in Fig. 1. Multi-modal single-cell instruction data construction InstructCell aims to advance single-cell analysis by leveraging natural language as an interactive medium. Current models typically rely on single modality, focusing either on single-cell data alone [14, 12, 13], text data alone [16, 17], or deriving cell embeddings directly from text [18, 19]. Consequently, available datasets are predominantly rooted in scRNA-seq data [10, 11] or textual representations derived from it [16, 18]. Thus, our primary goal is to construct multi-modal single-cell instruction dataset that enables the model to comprehend and process both the language of life sciences and human language effectively. 2 Fig. 1: Overview of InstructCell. a, Summary of incorporated single-cell data. InstructCell incorporates 299,155 scRNA-seq samples from human and mouse origins, spanning multiple organs. CPCG denotes Conditional Pseudo-cell Generation, CTA denotes Cell Type Annotation, and DSP denotes Drug Sensitivity Prediction. b, Architecture of the multi-modal cell language model. The model processes both text and single-cell data via three primary components: Q-Former to capture single-cell gene expression knowledge, pre-trained LM as the backbone, and cell reconstruction module for generating single-cell gene expression profiles. c, Construction of multi-modal single-cell instruction data. Complete instruction-response pairs are formed by combining required and optional attributes from text and single-cell modalities. d, Simulation of diverse communication styles. LLMs generate chat templates with varying traits (personality, motivation, and proficiency) to produce instructions that convey task-related information in different communication styles. 3 As shown in Fig. 1(a), we focus on two species (human and mouse) and collect scRNA-seq datasets from multiple tissues. These datasets are organized into gene expression count matrices, where rows represent individual cells, columns represent genes, and entries indicate gene expression levels. To provide essential biological context, we also record attributes such as tissue, species, cell type, and sequencing protocol for each dataset. Using these biological datasets, we create complete instruction-response pairs in natural language. Since different tasks emphasize distinct aspects of the data, the corresponding instructions and responses draw on varying subsets of attributes. As illustrated in Fig. 1(c), our multi-modal single-cell instruction dataset covers three key tasks: Conditional Pseudo-cell Generation (CPCG), which focuses on generating gene expression profiles tailored to specific cell types and biological conditions; Cell Type Annotation (CTA), which seeks to accurately classify cells based on their gene expression profiles; and Drug Sensitivity Prediction (DSP ), which aims to predict how cells respond to various drugs. Each task requires specific biological attributes (highlighted in pink in Fig. 1(c)), which are transformed into natural language instructions using GPT-4o [20]. To enrich biological context, optional attributes (blue tags) provide additional background without affecting the instructions core completeness. While LLMs can jointly generate instructions and responses, this often results in brief, less informative outputs [21]. To address this, we employ two separate LLMs: the first generates instruction templates, and the second produces response templates based on the output of the first. We produce two types of response templates: interleaved formats that combine dialogue text with answer labels for interactive applications, and simplified formats that provide only answer labels for concise outputs. These two formats support the training of chat and instruct versions of InstructCell, accommodating both interactive dialogues and task-specific outputs. To enable InstructCell to comprehend instructions from users with diverse cultural and linguistic backgrounds, we use LLMs to simulate different language styles, specialized terminology usage, and question structures. Specifically, we define three main traits (Fig. 1(d)): personality, which influences speaking styles; motivation, which adds richness to the dialogue context; and proficiency, which determines the level of technical jargon used. By combining these traits with task-related attributes, we generate wide range of instruction templates that enhance the models ability to handle complex queries, adapt to real-world scenarios, and personalize user interactions. Multi-modal cell language model To enable the model to handle both text and single-cell data modalities concurrently, InstructCell is built upon multi-modal LM architecture designed to facilitate cross-modal knowledge sharing and enhance its ability to process diverse input types. As illustrated in Fig. 1(d), the architecture comprises three main components: (1) Q-Former [22] that extracts and encodes features from single-cell gene expression data; (2) pre-trained LM [23, 24, 25] serving as the backbone for text processing; (3) cell reconstruction module that enables the generation of single-cell gene expression profiles. To differentiate single-cell data from natural language, we introduce special tokens <CELL> and </CELL> into the tokenization process. These tokens delineate single-cell data segments, ensuring the model correctly identifies and processes this modality [26, 27]. We employ the Q-Former to extract cell features from the gene expression data. The Q-Former includes transformer submodule with set of learnable query embeddings that capture information through self-attention and cross-attention layers. The output is collection of encoded cell features. These features, along with the processed textual data, are fed into the pre-trained LMs transformer blocks. At this stage, both the cell features and text have been transformed into vector representations, eliminating the need for further embeddings. Once the model encodes both modalities, it produces sequence of hidden states that represent their contextual relationships. Attention mechanisms guide the model to relevant segments of the input and previously generated outputs. Generation continues until reaching an end-of-text token or special <SIGNAL> token [28, 29] (decoded from the hidden state vector h<SIGNAL>), which indicates the need to produce single-cell data next. If no single-cell generation is required, the <SIGNAL> token does not appear. For single-cell generation tasks, the model learns to output <SIGNAL> at the appropriate time, signaling the transition to cell data generation. After producing the <SIGNAL> token, it and the previously generated text are fed back into the LM. Through attention, the model obtains final hidden state vector h<CPCG> that integrates both the instruction information and the preceding textual context. To generate the single-cell gene expression profile, we employ cell reconstruction module based on conditional variational autoencoder (CVAE) [30]. Given the target gene expression profile and h<CPCG>, the CVAE encoder and decoder guide the model to reconstruct the original profile. This approach allows the model to accurately reproduce observed patterns in the data while navigating the latent space smoothly, producing new, biologically coherent instances. During inference, we retain only the cell decoder block. single-cell gene expression 4 profile is generated by sampling latent vector from an isotropic Gaussian distribution conditioned on h<CPCG>. In summary, this multi-modal cell language model flexibly processes and generates essential biological information for single-cell analysis tasks. By bridging the language of life sciences and human communication, it lowers knowledge barriers, accelerating the acquisition of accurate and robust biological insights. InstructCell enables conditional pseudo-cell generation CPCG is task that requires the model to generate single-cell gene expression profile matching specified conditions, such as cell type, tissue, and species, as illustrated in Fig. 1(c). To explore the generative modeling capability of InstructCell, experiments are conducted using scRNA-seq data from 9 tissues bladder, blood, liver, lung, spleen, thymus, and vasculaturecollected from humans and mice. The datasets utilized in these experiments include TabularSapiens, Mouse-Atlas, and PBMC68K. First, we directly provide the generation conditions to InstructCell in textual form, and it generates cells based on these conditions. Fig. 2(a) visualizes the comparison between the real cells gene expression profiles and those generated by InstructCell. The generated cell distributions closely resemble those of real cells, even accurately replicating some outliers. Notably, the instruction-response templates used for generation are not included in the training phase, demonstrating that the model is robust to unseen input templates. This suggests that InstructCell effectively generalizes to new instructions by capturing the underlying biological patterns rather than overfitting to specific training templates. Additionally, we observe that the UMAP visualizations of generated cells exhibit slightly higher clustering compared to real cells. While this clustering might suggest slight oversimplification of the inherent biological variability, it reflects the models ability to faithfully capture the overall structure of the data and preserve key features in the reduced-dimensional space. Furthermore, we investigate the models ability to learn single-cell expression patterns specific to each cell type. As depicted in Fig. 2(b), each dot plot organizes genes on the x-axis and different cell types on the y-axis, with each cell type being grouped by its top three most significant genes. To identify the key expressive genes for each cell type, we compare individual cells to the average expression levels of other cells within the same cell type. Specifically, for each cell, we consider its gene expression levels and compare them to the average gene expression levels across all other cells of the same cell type. We conduct Welchs t-test [31] for each gene, comparing the expression level in the individual cell to the average expression in the other cells. significant result indicates that the gene is particularly expressive or distinctive in that cell compared to others of the same type. We systematically identify the top three most significant genes for each cell type, recognizing these as the key expressive genes. For the cells generated by the model, we directly display the expression proportions and average expression levels of these significant genes. By closely examining these patterns, we find that the generated cells faithfully replicate the gene expression profiles of real cells, particularly in terms of gene-specific expression levels and cell-type-specific patterns. These results demonstrate the models capacity to generate biologically plausible gene expression profiles that preserve intricate gene-level and cell-type-specific patterns critical for realistic simulations. We further compare InstructCell against existing cell generation methods across three dimensions, as illustrated in Fig. 2(c). As detailed in Methods, sKNN quantifies the structural divergence between generated and real data by examining label consistency within cell clusters in the reduced-dimensional space, while pKNN evaluates positional alignment by assessing how accurately generated cells match specific cell types. We report the average sKNN and pKNN values for of 5, 10, 25, and 50 across all datasets in the radar charts. The radial axis range is set according to the maximum value observed in the data, providing clear visual reference. All radar charts in this paper follow this convention, and no further explanation will be provided for subsequent figures. Both the instruct and chat versions of InstructCell closely align with real cells, effectively replicating biological structures and maintaining accurate cell positioning. The MMD metric, which measures the overall similarity between generated and real cells, shows that both versions of InstructCell achieve significantly lower MMD values compared to baseline models. Among the baseline methods, scDiffusion [32] and scGAN [33] focus solely on cell generation. Unlike the end-toend training strategy of InstructCell, scDiffusion requires separately trained modules, which introduces additional complexity. scGAN, on the other hand, is known for training difficulties, mode collapse, and instability. Moreover, scDiffusion and scGAN produce continuous gene expression values, whereas scRNA-seq data are inherently discrete, and some downstream tasks require discrete inputs. Similarly, the gene expression profiles generated by Cell2Sentence [16] are reconstructed in expression space through normalization and log1p-transformation, losing their original discrete characteristics. In contrast, InstructCell preserves the discrete nature of gene expression profiles by treating them as distinct modality, maintaining precise numerical information and capturing low-expression genes. This approach makes 5 Fig. 2: Conditional pseudo-cell generation results by InstructCell. a, UMAP visualizations of real and generated cells. The left plot shows the overlap between real and generated cells. The middle and right plots display real and generated cells, respectively, with distinct colors indicating different cell types. b, Dot plots of gene expression patterns derived from real (top) and generated (bottom) cells. Based on the test set from Tabular-Sapiens, we use Welchs t-test to identify top three significant genes for each cell type and display them along x-axis. Cell types are arranged along y-axis. The size of each dot indicates the proportion of single cells within the corresponding cell type that express the gene, while the color of the dot represents the mean expression level of the gene within that cell type. The results of the remaining two datasets are available in Fig.12. c, Quantitative evaluation of cell generation performance across four datasets. lower sKNN value indicates better structural alignment, higher pKNN value reflects improved positional correspondence, and lower MMD value denotes more accurate approximation of the global data distribution. InstructCell more scalable and flexible, offering distinct advantages in handling complex datasets and adapting to diverse experimental scenarios. Overall, these findings suggest that InstructCell effectively captures the essential characteristics of different cell types, ensuring robust performance in cell simulation tasks. Its intuitive design allows the direct generation of single-cell 6 gene profiles from verbal descriptions of conditions, providing practical and adaptable tool for producing specific cellular data in controlled experimental contexts."
        },
        {
            "title": "InstructCell boosts the performance of cell type annotation",
            "content": "Annotating cell types based on scRNA-seq data is fundamental task in single-cell analysis. InstructCell innovates in CTA by reframing it as single-cell question-answering task. Given an interleaved sequence of texts and gene expression profile, InstructCell generates the textual response that conveys its prediction regarding the cell type (Fig. 1(c)). To assess whether the model can accurately differentiate between various cell types, we first benchmark it across 5 scRNA-seq datasetsXin-2016, Segerstolpe-2016, He-2020-Liver, Ma-2020, and Bastidas-Ponce-2019. These datasets span 3 major organs/tissues (liver, pancreas, and skin) in humans and mice, covering more than 40 cell types and over 94,000 cells, utilizing mainstream scRNA-seq protocols such as Smart-seq2 and HiSeq Ten sequencing. The datasets chosen showcase range in both data size and complexity, as depicted in Fig. 1(a) and Fig. 3(a). From Fig. 3(a), we observe that InstructCell performs on par with, or even surpasses, foundation models (scBERT, scGPT, and Geneformer) across nearly all datasets, despite not relying on large-scale unlabeled pre-training. This indicates that InstructCell remains robust and reliable when handling diverse range of single-cell data. In contrast, foundation models often require extensive fine-tuning on each dataset for multiple tasks, resulting in greater training complexity, more cumbersome deployment, and substantially larger number of parameters. While Cell2Sentence simplifies representation, it may omit valuable quantitative details and exclude potentially informative, lower-expressed genes. By contrast, InstructCell preserves broader spectrum of gene expression information. This design enables the model to better capture the biological complexity of single-cell data, enhancing its ability to align with natural language instructions and deliver more accurate outputs. Fig. 3(b) shows that the cell types predicted by InstructCell are highly consistent with the original cell types. This is particularly evident for similar cell types with overlapping characteristics, where the model demonstrates an ability to distinguish subtle differences with minimal errors. Fig. 3(c) indicates that InstructCell achieves high accuracy (0.9) for most cell types displayed in the confusion matrix. This consistent performance across diverse cell types demonstrates the models capacity to accurately interpret single-cell data and predict cell types, even in the presence of interleaved text, without requiring extensive pre-training or dataset-specific fine-tuning. The confusion matrices for the remaining datasets are presented in Fig. 13. Beyond its consistent performance across diverse cell types, InstructCell demonstrates adaptability in handling data from different species, ensuring accurate predictions across varied biological contexts. This cross-species compatibility enhances its potential as practical tool for single-cell data analysis, supporting tasks such as comparative studies and integrated research pipelines."
        },
        {
            "title": "InstructCell enhances precision of drug sensitivity prediction",
            "content": "As illustrated in Fig. 1(c), DSP involves providing the model with drug information and single-cell gene expression data, allowing it to predict whether cell is resistant or sensitive to given drug. For our experiments, we gathered scRNA-seq data from three organs, including datasets from humans (GSE149383 and GSE117872) and mice (GSE110894), paired with drug sensitivity information. Notably, the GSE117872 dataset includes an additional category, labeled holiday, which refers to observations made during off-treatment periods. Fig. 4(a) shows that InstructCell achieves superior performance across all three evaluation metrics on the GSE117872 and GSE110894 datasets, while on the GSE149383 dataset, it delivers results comparable to single-cell foundation models. These results demonstrate the models ability to predict single-cell responses to different drugs based on textual descriptions of conditions and gene expression data. Fig. 4(b) presents UMAP visualization comparing annotated and predicted single cells with different labels. The visualization demonstrates that InstructCell effectively distinguishes these categories, accurately capturing the separation between drug-sensitive and drug-resistant cells while appropriately positioning holiday cells. This suggests that the model successfully reflects biological differences in drug responses within the reduced-dimensional space. Fig. 4(c) presents detailed confusion matrix for each label, showing that InstructCell achieves high accuracy levels (0.95) across different species and tissues. This consistent performance highlights the models robustness in integrating single-cell gene expression data with drug information, making it reliable tool for drug response prediction across diverse biological contexts. 7 Fig. 3: Cell type annotation results by InstructCell. a, Evaluation of InstructCells CTA performance across human heart, liver, pancreas, and mouse skin and pancreas datasets. Performance is quantified using weighted F1, macro F1, and accuracy metrics, with different colors representing different models. b, UMAP visualization of three different datasets. The left panel is colored by expert-annotated cell types from the original research, and the right panel is colored by InstructCell prediction results. c, Confusion matrices between predicted cell types and actual annotations for the three datasets. Darker shades denote higher frequency of agreement between the models predictions and the actual cell type annotations. 8 Fig. 4: Drug sensitivity prediction results by InstructCell. a, Evaluation of InstructCells CTA performance across human oral, lung, and mouse bone datasets. Performance is quantified using weighted F1, macro F1, and accuracy metrics, with different colors representing different models. b, UMAP visualization of the three datasets, with cells colored by drug sensitivity labels (sensitive, resistant, and holiday) for both expert-annotated results and InstructCell predictions. c, Confusion matrices between predicted cell types and actual annotations for the three datasets. Darker shades denote higher frequency of agreement between the models predictions and the actual drug sensitivity annotations. 9 Fig. 5: Robustness of InstructCell. a, Quantitative comparison of the CPCG task under seen and unseen instruction templates. Results are shown for sKNN and pKNN metrics at varying numbers of neighbors K, as well as for MMD. Different colors denote whether the instruction templates are seen or unseen. b, Average performance of InstructCell under instruct and chat modes across each task. On the left side (classification tasks), the shape of each scatter point indicates whether options are provided or not, while the color distinguishes model versions. Each configuration includes 40 scatter points (20 with options and 20 without). On the right side (generative task), different colors represent different model versions."
        },
        {
            "title": "InstructCell demonstrates robustness to varied instruction styles",
            "content": "As tool designed to assist researchers, InstructCell must effectively understand and address questions phrased differently. Fig. 5 provides quantitative evidence for this adaptability. To assess how the model reacts to unfamiliar instruction templates, we first test it on the CPCG task, which is more demanding than the two classification tasks. We measure several metrics on cells generated using both unseen and seen templates across three datasets. As shown in Fig. 5(a), the sKNN, pKNN, and MMD values remain similar in both conditions, indicating that InstructCell maintains consistent performance regardless of prior exposure to the instruction templates. This implies that InstructCell preserves both detailed structural features and overall distributional patterns, allowing it to adapt effectively to different researchers phrasing styles. To further investigate this adaptability, we evaluate both the instruct and chat versions of InstructCell on all tasks, as shown in Fig. 5(b). The first two panels focus on classification scenarios. In some templates, we provide multiple-choice options; however, whether these options are present or not has almost no impact on accuracy. This flexibility is beneficial, as it means the model does not need pre-defined options to achieve high performance, helping keep inputs short and avoiding constraints that could slow inference due to quadratic complexity. That said, the instruct version proves slightly more robust than chat, likely because it is optimized for direct, goal-oriented instructions, thus maintaining stable performance across varied input formats. In contrast, the chat variant, designed for open-ended exchanges, may be more sensitive to subtle stylistic shifts. Finally, the last panel in Fig. 5(b) presents average results for the CPCG task over three datasets. Both instruct and chat perform comparably, with consistently low standard deviations, indicating that either approach can reliably produce realistic pseudo-cell data. This outcome suggests that the models internal representation of biological patterns 10 is sufficiently robust that differences in instruction style exert minimal influence on generation quality."
        },
        {
            "title": "InstructCell identifies biologically significant marker genes",
            "content": "Fig. 6: Top 10 significant genes identified by InstructCell for each cell type in two datasets. a, Heatmaps of the significant genes extracted from InstructCell by using gradient saliency-based method for (a) He-2020-Liver and (b) Xin-2016 datasets. The color gradient from red to blue represents gene importance, with red indicating higher importance scores and blue indicating lower scores. Red markers in each row indicate that genes among the top 10 key genes identified by the model, are either reported as marker genes for the corresponding cell type in the CellMarker2.0 database or in recent literature. To evaluate the models ability to uncover biologically meaningful insights, we explore whether it can identify marker genes without prior knowledge of known cell-type markers injected into the model. Since scRNA-seq datasets provide gene expression profiles, we employ gradient-based saliency methods to determine which genes most strongly influence the models predictions. Specifically, we employ the Vanilla Gradient method [34] to extract the top 10 most significant genes for each cell type, as detailed in the Methods section. We then evaluate the biological relevance of these genes by comparing them with documented marker genes in the CellMarker2.0 database and recent literature [35]. Fig.6 presents results for two datasets: (a) He-2020-Liver and (b) Xin-2016. Red markers in each row indicate that genes among the top 10 key genes identified by the model, are either reported as marker genes for the corresponding cell type in the CellMarker2.0 database or in recent literature. From two heatmaps, we observe that the genes assigned higher scores by the model are highly likely to be documented as marker genes for their respective cell types. For most cell types, more than half of the top 10 genes identified by the model are found in the CellMarker2.0 database. In cases where gene from the top 10 is not explicitly reported as marker gene in the database, it may still be highlighted in the literature as relevant marker. For example, HHEX is identified as highly expressed gene for human Delta cells [36, 37], although it is not categorized as marker gene for human Delta cells in the database. These findings suggest that InstructCell not only effectively identifies established marker genes but also has the potential to uncover novel marker genes. InstructCell exhibits high-quality expressive capabilities To maximize the scientific utility of InstructCell, it is crucial that its interactions with researchers remain both accurate and efficient. Accordingly, we assess the quality of its responses by evaluating fluency, grammatical integrity, and the inclusion of predictive results. We randomly sample 500 entries from the multi-modal single-cell instruction templates for quality assessment. Although manual evaluation is the gold standard for capturing human preferences, it is slow and resource-intensive. As an alternative, we adopt an LLM-as-a-judge approach, using Claude 3.5 Sonnet [4] as surrogate evaluator. This 11 Fig. 7: closer look of InstructCell. a, Evaluation of response quality in InstructCell using the LLM-as-a-judge approach. Response quality is assessed based on fluency, grammar, and inclusion of predictive results, with Claude 3.5 Sonnet [4] serving as an unbiased evaluator. Text highlighted in purple indicates additional content for the CPCG task compared to the two classification tasks. b, Impact of Q-Former on model performance. Performance comparison between the Q-Former and standard MLP for encoding single-cell data. c, Impact of query embedding quantity on model performance. Performance comparison across different numbers of query embeddings. d, Comparative performance of multi-task vs. single-task instruction tuning. For single-task instruction tuning, we divide our multi-modal instruction dataset by task type and train separate models for each specific task. We report the average metrics across all datasets for each task using the InstructCell-instruct version. e, Comparative performance of without vs. with pre-trained LM weights. We conduct experiments using the InstructCell-chat version to explore the impact of employing or not employing pre-trained weights on model performance. f, Impact of varying template ratios on model outputs. Four chat version models are trained on all classification datasets using multi-task instruction tuning with varying ratios of templates (0.5%, 5%, 50%, and 100% of the total templates). For the CTA and DSP tasks, 40 unseen instruction templates are selected for evaluation: 20 with multiple-choice options and 20 without. The mean performance and standard deviation for each template are calculated across all datasets for these two tasks. Additionally, we sample 500 unseen instruction templates and use Claude 3.5 Sonnet to score the models outputs for expressiveness, while unigram analysis is conducted to assess lexical diversity. 12 strategy mitigates evaluation bias [38, 39] because Claudes underlying architecture differs substantially from GPT-4o, which generated the original data. Fig. 7(a) shows the default prompt template used for these LLM-based assessments. According to the evaluation ratings, 97% of InstructCell-generated responses meet or exceed the quality of the provided ground truth. Furthermore, the close alignment in scores between InstructCell-generated responses and the ground truth underscores the models ability to produce contextually accurate outputs. This competence in effective, high-quality communication suggests that InstructCell is well-positioned to serve as tool in scientific collaborations. Q-Former facilitates smooth integration of single-cell data To assess the effect of using the Q-Former as feature extractor for single-cell data, we focus on classification tasks in single-cell analysis. We choose these tasks because the modifications primarily affect the input module rather than the models generative components for the single-cell modality. We employ the InstructCell-instruct model for two classification tasksCTA and DSP and report the average performance across all datasets. To determine the necessity of the Q-Former, we replace it with standard Multi-Layer Perceptron (MLP) and compare the results. As shown in Fig. 7(b), using conventional MLP to encode single-cell data consistently yields lower performance across multiple metrics. This finding suggests that the Q-Formers specialized architecture is essential for capturing and processing the complex patterns inherent in single-cell gene expression, highlighting its critical role in our system. Furthermore, since the Q-Formers query embeddings aggregate information via attention mechanisms, we explore how varying the number of queries (1, 2, 4, 8, and 16) affects performance. Fig. 7(c) shows that using 8 queries yields the best results, while increasing this number further leads to diminished performance. More queries provide the model with diverse, independent information streams, enabling each query to capture distinct aspects or features of the input single-cell data. However, an excessively large number of queries may introduce redundancy, making it harder for the model to distinguish relevant signals from noise and ultimately degrading performance. Thus, balancing query count is key to achieving both robust information coverage and optimal model accuracy. Multi-task instruction tuning supports comprehensive understanding across tasks InstructCell is designed to handle multiple tasks within single-cell analysis, addressing the diverse demands of this field. Compared to single-task tuning, multi-task instruction tuning exposes the model to broader range of problems during training, potentially fostering more general and adaptable representations. To directly compare the performance gains from multi-task versus single-task training, we partition our multi-modal instruction dataset by task type and train separate single-task models for each. As illustrated in Fig.7 (d), we compute the average performance across all datasets for CTA and DSP. The results demonstrate that multi-task instruction tuning consistently outperforms its single-task counterpart on these tasks. This advantage likely arises from multi-task learnings capacity to share parameters within the model architecture, thereby promoting knowledge transfer and integration across diverse tasks [40, 41]. Moreover, balancing training across multiple tasks helps prevent overfitting to any single task, yielding more well-rounded improvement in overall model performance. For the CPCG task, we use pKNN and sKNN as evaluation metrics, where higher pKNN and lower sKNN indicate superior performance. Under these criteria, multi-task instruction tuning also exhibits clear advantage over single-task tuning. This suggests that even in complex generative scenarios requiring an understanding of underlying data distributions, leveraging knowledge across multiple tasks enhances the models capacity to capture detailed patterns, leading to more accurate and biologically meaningful pseudo-cell generation. Pre-trained LM weights enhance classification performance of InstructCell To examine the influence of pre-trained weights on the performance of LMs in specialized single-cell analysis tasks, we alter the backbone of our InstructCell-chat model by replacing the pre-trained LM with one initialized from scratch. As shown in Fig. 7(e), the absence of pre-trained weights leads to marked performance declines in two classification tasksCTA and DSP. However, for the CPCG task, performance remains comparable, and the model without pre-trained LM weights even achieves slight improvement on the sKNN metric. This discrepancy arises because classification tasks heavily depend on the LMs language processing and generation capabilities, which benefit significantly from exposure to diverse linguistic patterns and vocabularies during pre-training. 13 In contrast, CPCG relies more on dedicated downstream modules that reconstruct cells from hidden state vectors, making it less sensitive to the presence of pre-trained weights. This finding suggests that the effectiveness of the generation task is driven more by the specialized reconstruction module than by the LMs pre-trained parameters."
        },
        {
            "title": "Diversified instruction templates improve the validity and quality of InstructCell outputs",
            "content": "To investigate how the number of instruction templates influences model performance, we evaluate InstructCell on previously unseen templates. As shown in Fig. 7(f), we vary the ratio of available templates (0.5%, 5%, 50%, and 100% of the total) and assess the resulting performance. Experiments on the CTA and DSP tasks indicate that increasing the number of templates leads to more robust performance, as reflected by reduced standard deviations. Although the results at 50% and 100% of the templates are comparable in terms of accuracy, the model trained with 100% of the templates exhibits greater robustness. The exposure to more diverse instructions enhances the models ability to align its outputs more closely with the gene expression data. We do not consider the generation task in this analysis because the model sometimes fails to produce cells under the extremely low template ratios (5%) complicating the evaluation. In addition, we use LLM-based assessments (Claude 3.5 Sonnet) to gauge response quality. The findings show that as the diversity of instruction templates increases, the models expressiveness improves, and performance variability decreases. This suggests that the model becomes more stable when trained on broader range of instructions, critical factor for practical applications in scientific research, where questions and data can be highly varied. Lastly, we measure lexical diversity by calculating the unigram ratio, defined as the ratio of unique unigrams to the total number of unigrams in the response. The results indicate that as the variety of instruction templates grows, the lexical richness of the models output increases. This expanded vocabulary usage reflects the models capacity to produce more natural and contextually appropriate responses."
        },
        {
            "title": "Discussion",
            "content": "In this study, we present InstructCell, multi-modal AI copilot that bridges the language of cellular biology and human natural language, establishing foundation for advancing single-cell analysis. By developing multi-modal single-cell instruction dataset spanning diverse species, tissues, and experimental conditions, we unify single-cell generation and understanding tasks within cohesive framework. Building upon this dataset, InstructCell employs multi-modal architecture capable of processing interleaved biological and textual data. By providing accurate, biologically relevant outputs tailored to researchers specific analytical needs, InstructCell lowers technical barriers and facilitates intuitive interaction with single-cell data through natural language. Extensive experiments reveal that single-cell generation and understanding tasks are mutually reinforcing, further enhancing the robustness and versatility of InstructCell. Despite these advancements, InstructCell leaves room for future exploration and development. Expanding task coverage to include areas like predicting transcriptional responses to genetic perturbations [42] or generating descriptive summaries of individual cells [16] could significantly increase its versatility. Large-scale multitask instruction tuning [6, 43, 44, 45] offers promising avenue for improving adaptability and achieving zero-shot capabilities by leveraging extensive scRNA-seq datasets. Developing multi-round dialogue frameworks could enable richer and more interactive analyses. Furthermore, incorporating additional modalities beyond text and scRNA-seqsuch as single-cell ATAC-seq [46] or graph-based representations [47]would expand the scope of multi-modal single-cell analysis. These directions could pave the way for more capable and generalizable AI copilot, driving forward single-cell research."
        },
        {
            "title": "Methods",
            "content": "Multi-modal instruction data construction We first gather publicly available scRNA-seq datasets for each task and convert them into multimodal instructionfollowing data using instruction-response templates. However, constructing instruction-response templates for each task by hand is labor-intensive, resulting in limited number of templates that often lack diversity. Such constraints can lead to trained models that are less robust and produce outputs with limited variety (Fig. 7(f)). To address this issue, we propose leveraging an existing LLM to synthesize wide range of instruction-response templates. We further increase 14 Fig. 8: The examples of the prompts used to construct instruction-response templates for CPCG. a, An example of the prompts for generating personality traits. b, An example of the prompts used to generate motivations for CPCG. c, An example of the prompts used to generate instruction templates for CPCG. d, An example of the prompts used to generate response templates for CPCG. e, An example of the prompts for rewriting instruction templates to enhance diversity. 15 template diversity by considering both the conversational context (i.e., the questioners motivation) and style (i.e., the questioners personality and level of expertise in single-cell analysis). As illustrated in Fig. 8(ab), we use GPT-4o [20] to generate various personality traits and each tasks motivations. Experts then evaluate each motivation to ensure its validity. In contrast, personality traits are not reviewed, as GPT-4o reliably maps them to distinct conversational styles, even if trait seems unrelated to the task. For example, the generous trait often results in more polite tone in the instruction templates. Additionally, we define two levels of proficiency in single-cell analysis, distinguishing between high and low expertise. Next, we prompt GPT-4o to generate set number of instruction-response templates for each task. During template generation, we specify the questioners personality trait, motivation, and familiarity level, asking GPT-4o to adopt the questioners perspective in formulating each instruction. We also provide GPT-4o with relevant placeholders (Fig. 1(c)), emphasizing that these placeholders must appear in any generated instruction template. To help GPT-4o produce appropriate and coherent templates, we inform it of the expected response for each task. Fig. 8(c) demonstrates an example of the prompts used to synthesize instruction templates for CPCG. Each generated template is filtered out if it exceeds 70 words or omits any essential placeholder. If it passes this filter, we calculate its ROUGE-L [48] similarity with previously generated instruction templates; if the maximum similarity surpasses 0.75, we ask GPT-4o to rewrite the instruction template (Fig. 8(e)). After three rewrites, if the highest similarity still exceeds 0.75, the instruction template is discarded. We use GPT-4o to generate corresponding response templates for the instruction templates that pass the filter. Each response template includes the {output} placeholder (Fig. 8(d)). In CTA, {output} represents the cell type; in DSP, it specifies how single cell responds to particular drug; and in CPCG, it denotes the generated gene expression profile. For CPCG, the response template must end with {output} and must not include ambiguous entities such as GENE or Gene 1. Response templates that fail to meet these criteria or exceed 70 words are filtered out, and the process proceeds to the next round of instruction-response template synthesis. Otherwise, we conclude that the instruction-response template has been successfully synthesized, and therefore retain it."
        },
        {
            "title": "Input embeddings",
            "content": "InstructCell can take mixture of text and single cells as input. Consider sequence = (x1, x2, , xm) of length m, where text and single cells are interleaved. Each single cell is encapsulated between two special tokens. Specifically, if xi represents single cells gene expression profile, then xi1 and xi+1 are marked with <CELL> and </CELL>, respectively. If represents text token within the input sequence, it is converted into d-dimensional text embedding by the LMs input embedding layer embed(; ): = embed(x; ). (1) Here, is learnable matrix with dimensions (V + 3) d, where is the original vocabulary of the pre-trained LM. Note that three special tokens, <CELL>, </CELL>, and <SIGNAL>, are added to the vocabulary. These tokens are initialized randomly at the beginning of training. On the other hand, if is single cells gene expression profile in the input sequence, it is transformed into (k 1) cell embeddings of dimension by the cell encoder qformer(; ): (e1, e2, , ek) = qformer(x; ), (2) where  represents the learnable parameters of the cell encoder. All input embeddings, whether derived from text tokens or single cells, are then concatenated in sequence to form the matrix EX = (e1, e2, , en), where the i-th embedding ei could be text embedding or cell embedding. This entire process is denoted as EX = map(X; , ). Q-Former module The Q-Former module comprises three primary components: set of learnable d-dimensional query vectors Q(0) = (q1, q2, , qk), multi-layer perceptron (MLP) that encodes raw input cells into keys and values, and transformer model [49] that connects the outputs of the MLP to the query vectors. 16 Given an input cell x, the MLP with residual connections [50] outputs d-dimensional vector xout which is then split into individual d-dimensional vectors: xout = MLP(x), (u1, u2, , ut) = split(xout, t), (3) where split(xout, t) denotes the operation of evenly dividing xout into vectors {ui}t i=1. These vectors, denoted as = (u1, u2, , ut), act as the keys and values for the transformers cross-attention mechanism. Within each transformer block, the query vectors first interact with each other through self-attention layer and then interact with through cross-attention layer [51]. This process is iteratively applied across Lcell stacked transformer blocks: Q(l+1) = (q(l+1) 1 , q(l+1) 2 , , q(l+1) ) = qformerblock(l)(Q(l), ), where 0 < Lcell. The outcome of the entire computation process is represented as: (e1, e2, , ek) = Q(Lcell) = qformer(x; ). (4) (5)"
        },
        {
            "title": "Cell reconstruction module",
            "content": "The cell reconstruction module of InstructCell is conditional variational autoencoder (CVAE) parameterized by  = (e, d) where e and d represent the encoder and decoder parameters, respectively. During training, the encoder encodes the current condition and the corresponding single-cell to produce posterior distribution. The decoder then reconstructs the original input single-cell from the latent variable sampled from the posterior distribution q(zs, c; e). The log probability density of the conditional distribution for the single-cell can be expressed as: log p(sc) = log = log (cid:90) p(s, zc) dz (cid:90) p(s, zc)q(zs, c) dz q(zs, c) (cid:20) p(s, zc) q(zs, c) Ezq(zs,c) log (cid:21) (6) = Ezq(zs,c) [log p(sz, c)] + Ezq(zs,c) (cid:20) log (cid:21) p(zc) q(zs, c) = Ezq(zs,c;e) [log p(sz, c; d)] KL(q(zs, c; e)p(zc)). The right-hand side of the inequality represents the evidence lower bound (ELBO). The training objective of the cell reconstruction module is to maximize the ELBO: [52]: Lrecon(; s, c) = Ezq(zs,c;e) [log p(sz, c; d)] + KL(q(zs, c; e)p(zc)). (7) scRNA-seq data have several key characteristics including overdispersion [53, 54], sparsity [55, 56], and non-negative integer expression values for each gene. Considering these properties, specifying the decoder-generated distribution as Gaussian distribution is inappropriate. Researchers commonly analyze scRNA-seq data using either the Negative Binomial (NB) distribution or the Zero-Inflated Negative Binomial (ZINB) distribution [57, 58]. For the NB distribution, the variance increases with the mean, which aligns with the overdispersion observed in scRNA-seq data. The ZINB distribution builds on the NB distribution by accounting for non-biological factors contributing to excess zeros. Since the NB distribution is encompassed by the ZINB distribution, we select the ZINB distribution as the output distribution of the decoder. The inference and generative processes in the cell reconstruction module are similar to those in scVI [59]. Specifically, the latent variable is divided into two independent variables zs and . Therefore, the loss function can be rewritten as: Lrecon(; s, c) = Ezsq(zss,c;e),q(s,c;e) [log p(szs, , c; d)] +  [KL(q(zss, c; e)p(zsc)) + KL(q(s, c; e)p(c))] , (8) where  balances the trade-off between the reconstruction quality and output diversity of CVAE [60]. We specify p(zsc) as an isotropic Gaussian distribution (0, I), and p(c) as log-normal distribution Lognormal(, 2 ) where  = Es[log((cid:80)  = Es[(log((cid:80) si)], and 2 si) )2]. 17 To disentangle the sampling operation from the optimization, we employ the reparameterization trick. The encoder e outputs the means and variances of the distributions based on the input cell and the condition c. We perform the sampling operation by drawing samples from standard normal distribution (0, I). These samples are subsequently scaled by the computed variances and shifted by the computed means. The Kullback-Leibler divergence between two Gaussian distributions has closed-form solution: KL(N (x; 1, 1)N (y; 2, 2)) = 1 2 [log 2 1 + tr(2 11) + (2 1)T  1(2 1)], (9) where is the dimensionality of the random variables and y, and tr() denotes the trace of matrix. We can efficiently compute the second term and third term in the loss function using the means and covariances of the target and estimated distributions. The generative process is defined as sequence of transformation and sampling operations, described as follows: zs Normal(0, I), ),  Lognormal(, 2  = f1(, c),  = f2(zs, c), gi = f3(i), wi Gamma(i, gi), vi Poisson(wi),  = f4(zs, c), bi Bernoulli(i), si = (cid:40) vi, 0, bi = 0 bi = 1 . (10) Here, f1, f2, f3, and f4 are the mapping functions parameterized by d. gi represents the inverse dispersion of the i-th gene, and  ensures (cid:80) i = 1. During training, zs and  are sampled from their respective posterior distributions q(zss, c; e) and q(s, c; e). The model then computes the log probability of s, log p(szs, , c; d), under the generated ZINB distribution, which is stable computational process [59]. Therefore, the first term of the loss function can also be efficiently computed. Note that if no conditional information is necessary, can be set to null vector [61]. Pre-trained LM The architecture of pretrained language models (PLMs) can be categorized as encoder-only [62, 63, 64, 65], decoderonly [66, 67, 68, 3], or encoder-decoder [69, 24, 70, 71]. Since the three single-cell tasks considered in this study can all be formulated as sequence-to-sequence (seq2seq) tasks, any decoder-only or encoder-decoder PLM can flexibly serve as the backbone network of InstructCell. To simplify the notation in mathematical formulations, we denote the backbone networks forward computation process (excluding the transformation of input sequences into input embeddings) as pPLM(; ). Single-cell understanding task For the single-cell understanding task, given an input sequence and the corresponding target sequence = (y1, y2, , yLtarget), where yi is the i-th token in the target sequence, the objective of LM is to minimize the negative log joint probability of the target sequence [62]: Lscu(, , ; X, ) = = Ltarget (cid:88) i=1 Ltarget (cid:88) i=1 log pPLM(yimap(X; , ), map(Y i1; , ); ) log pPLM(yiEX , EY i1; ) (11) where i1 = (y1, y2, , yi1). = Ltext(, , ; X, ), 18 Single-cell generation task In the single-cell generation task, the target sequence , with length of Ltarget, ends with special signal token <SIGNAL> that marks the transition to generating single cell. In this task, the LMs objective is not only to generate the target sequence but also to produce conditional information h<CPCG> based on (X, ). This conditional information h<CPCG> enables the decoder in the cell reconstruction module to generate the target single cell s. The overall process can be formalized as follows: Lscg(, , , ; X, , s) = Ltext(, , ; X, ) + Lrecon(; s, h<CPCG>), (12) where, in this study, h<CPCG> represents the hidden state of the <SIGNAL> token in the last layer of the LM. By leveraging the LM as an encoder for the biological characteristics of single cells, InstructCell enhances the flexibility and expressiveness of user-specified input conditions. Collectively, InstructCell can be trained end-to-end and the corresponding optimization objective is defined as: arg min ,W ,, E(X,Y ,s)D[I(s = )Lscu(, , ; X, ) + I(s = )Lscg(, , , ; X, , s)], (13) where represents the training set, = indicates the absence of the cell modality in the output, and I() is the indicator function. From the perspective of multitask learning, the majority of the models parameters (i.e., the LM without the language modeling head) are shared across multiple tasks, which effectively prevents overfitting on any single task [72, 73]."
        },
        {
            "title": "Evaluation",
            "content": "Answer extraction Unlike InstructCell-instruct, InstructCell-chat generates free-form responses, making direct evaluation of its performance challenging. For the CPCG task, the generated gene expression profiles are placed at the end of the output, which enables straightforward extraction and analysis. In contrast, answers for the other two classification tasks are embedded within conversational text, which requires reliable extraction tool. To address this, we employ xFinder [74], state-of-the-art extraction model whose reliability is evaluated meticulously in our setting. Specifically, we first construct test set by replacing the {output} placeholders in single-round dialogue templates with the labels derived from all datasets. For example, GSE110894 contains 2 distinct labels (Figure 9). Since there are 2,359 different templates used for DSP (see the section Instruction-response template construction details), we can construct 4,718 test samples. Overall, the test set comprises 178,411 samples. We find that xFinder achieves an accuracy exceeding 99.97% on this test set, demonstrating its effectiveness in automatically extracting answers. Metrics For the CTA and DSP tasks, we use standard evaluation metrics, including weighted F1, macro F1, and accuracy. When evaluating InstructCell-chat, there are rare instances where the model fails to provide an answer. In calculating accuracy, we use the true accuracy metric, treating unanswered cases as incorrect predictions. However, for F1 scores, we exclude unanswered cases because F1 relies on valid predictions for precision and recall calculations. Including unanswered cases as errors would misrepresent the balance between precision and recall, leading to an inaccurate evaluation of the models performance. For the CPCG task, we evaluate the models performance using Maximum Mean Discrepancy (MMD), sKNN, and pKNN. Specifically, for real single-cell dataset {(yi, ci)}M i=1 unseen during the training, the trained model generates pseudo single-cell dataset {(xi, ci)}M i=1. Since both real and generated single-cell data are count data, we normalize each sample to ensure the gene counts sum to 10,000, followed by log1ptransformation. To evaluate the model, we first apply principal component analysis (PCA) to reduce the dimensionality of the real single-cell data to 50 dimensions, then further reduce it to two-dimensional embedding space using UMAP [75]. The pseudo single-cell data is mapped into the same two-dimensional embedding space. Finally, we compute MMD, sKNN, and pKNN within this embedding space to assess the models performance. i=1 based on the corresponding label set {ci}M MMD is kernel-based statistical test, used to measure the difference between two probability distributions [76]. j=1 from the real distribution q, the empirical estimate Given samples {xi}M of MMD between and can be calculated using the following formula: i=1 from the estimated distribution and {yj}M MMD[p, q] = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:88) i= (xi) 1 (cid:88) j=1 (yj) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)H , 19 (14) where is universal Reproducing Kernel Hilbert Space (RKHS) and () is the corresponding reproducing kernel. Similar to [77, 33], we use kernel function K(x, y) that is the summation of three different Gaussian kernels to compute the MMD: 3 (cid:88) exp(ix y2), K(x, y) = i = MMD[p, q] = i=1 1 2i22 , (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nM 2",
            "content": "M (cid:88) i,j=1 K(xi, xj)"
        },
        {
            "title": "2\nM 2",
            "content": "M (cid:88) (cid:88) i=1 j=1 K(xi, yj) +"
        },
        {
            "title": "1\nM 2",
            "content": "M (cid:88) i,j=1 K(yi, yj), (15) where  is the median of the average distance between real single cell to its nearest 25 neighbors. smaller MMD value indicates that the learned distribution is more similar to the target distribution q. sKNN focuses on evaluating the intrinsic biological structure of single-cell data. It is calculated by finding the nearest neighbors for each cell, and then evaluating the label consistency based on the neighbors labels: sKNN ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i="
        },
        {
            "title": "1\nK",
            "content": "(cid:88) (cj = ci) , jNeighborK (xi) (16) where NeighborK(xi) denotes the nearest neighbors of the i-th cell xi. The function I() is an indicator function that returns 1 if the condition is true (i.e., if two labels match) and 0 otherwise. In our evaluation setting, label corresponds to cell type. It is important to note that low sKNN value does not necessarily indicate significant discrepancy between the distribution learned by the model and the real distribution. This may occur if the clusters formed by each cell type in the real single-cell dataset are not compact. Therefore, it is recommended to calculate sKNN on the real single-cell dataset as reference. In our evaluation setting, we report sKNN, which represents the absolute difference between the sKNN values of the generated data and the real data: sKNN = sKNNgenerated sKNNreal . (17) smaller sKNN value indicates closer alignment of the intrinsic biological structure between the generated and real data, while larger value implies greater divergence. pKNN, in contrast, evaluates whether the label of each pseudo cell aligns with the majority label of its nearest real cells, thereby assessing the positional proximity of the generated cells to the real cells. To compute pKNN, we use {(yi, ci)}M i=1, the real single-cell data with their corresponding labels, as the training set for KNN classifier, and {(xi, ci)}M i=1, the model-generated single-cell data with their corresponding labels, as the test set. The pKNN metric is then defined as the classification accuracy of this KNN classifier when predicting the labels of the generated cells. higher pKNN value indicates closer alignment of the spatial distribution between the generated and real data. Overall, these metrics form holistic evaluation framework spanning all three tasks. For the two classification tasks, macro F1 accounts for class imbalances, while weighted F1 emphasizes the models performance on majority classes. For CPCG, the two KNN-based metrics focus on different aspects of local patterns, whereas MMD captures global distributional differences. Extraction of key genes using gradient saliency-based methods We use the Vanilla Gradient method [34] to identify genes that play significant role in the models decision-making process. Although this method is relatively simple to implement, it is effective for extracting important features because it is sensitive to both the models parameters and the dataset [78]. Given an input sequence from the test set, denoted as = (x1, x2, , xm), where the i-th element xi represents single cells gene expression profile s, we compute the gradient gs as follows: gs = xi (cid:12) (cid:12) (cid:12) (cid:12)xi=s , (18) where is the loss function with respect to xi, as defined by the trained model and the corresponding target sequence . We define as the set of indices that correspond to the genes of interest, such as all protein-coding genes in the gene vocabulary. We then compute mask ms, where the i-th element msi is defined as: msi = (cid:40) 1, 0, if gsi < 0 si > 0, otherwise. 20 (19) Here, gsi represents the i-th element of gs. We subsequently derive the feature importance scores os as follows: os = gs ms. (20) We apply min-max normalization to the elements of os, scaling them to the range [0, 1]. After normalization, we sort the elements of os in descending order, considering the top-ranked gene as the most critical for determining the cell type of that sample. Given that the test set typically contains multiple single cells, we compute the normalized feature importance scores for each cell. We then group the feature importance scores by cell type and perform an average aggregation, deriving aggregated feature importance scores for each cell type. Ultimately, we identify the top (where = 10) significant genes for each cell type, treating them as potential marker genes. UMAP visualizations Dimensionality reduction methods such as PCA, t-SNE [79], and UMAP [75] are widely used to visualize high-dimensional data. In this study, we select UMAP because of its effectiveness in visualizing scRNA-seq data [80]. For an scRNA-seq dataset to be visualized, we first normalize each sample to ensure that the gene counts sum to 10,000. Following normalization, log1p-transformation is applied to each sample. Next, we compute the top 50 principal components using the PCA algorithm and further reduce the dimensionality to two dimensions with UMAP. Finally, the resulting two-dimensional data points are visualized."
        },
        {
            "title": "Experimental setup",
            "content": "Datasets and gene vocabulary All datasets used in this study are publicly available. We collect all raw cell-by-gene matrices and employ Scanpy toolkit [81] to preprocess them independently. Specifically, for the purpose of quality control [82], we filtered out single cells where fewer than 200 unique genes are expressed. After removing low-quality cells, we exclude genes expressed in fewer than 8 different single cells. Aberrant cells, such as those with the excessive number of mitochondrial genes expressed or abnormally high total counts, are also removed. Since the original data from Xin-2016 [83] does not provide cell type labels, we annotate each sample using cell-type-specific marker genes based on the findings of Muraro et al. [36] and the method employed by Tritschler et al. [84]. For all datasets used for CTA, rare cell types (i.e., those with fewer than 20 cells) are excluded. To harmonize gene features across two species, we process the datasets collected from mice. Concretely, we convert provided gene symbols into Ensembl gene IDs using pyEnsembl [85]. We then utilize pyBiomart [86] to map mouse genes to their human orthologs, whereas mouse-specific genes are kept. To standardize gene features across all datasets, we identify the top 3,600 highly variable genes for each datasets cell-by-gene matrix using Seurat [87, 88]. These sets of highly variable genes are merged to create unified gene vocabulary. The resulting gene vocabulary, containing 18,961 unique genes, is denoted as Vg. Finally, each dataset is divided into training, validation, and test sets in an 8:1:1 ratio. Figure 9 provides detailed overview and UMAP visualizations of the processed datasets. These datasets originate from diverse sources encompassing two species and eleven distinct tissue types. The datasets also exhibit significant variation in complexity. For the CPCG task, Mouse-Atlas contains 50 distinct cell types, whereas Tabular-Sapiens contains only 6 distinct cell types. For the CTA task, as illustrated in Figure 3(a), the performance of baselines and InstructCell varies substantially across Ma-2020 and Segerstolpe-2016. Collectively, these datasets enable comprehensive evaluation of InstructCell across the three tasks. Instruction-response template construction details The construction of instruction-response templates is based on the GPT-4o model [20]. The hyperparameter of decoding algorithm, top p, is set to 0.95. Initially, we leverage the world knowledge of GPT-4o to generate list of relevant personality traits and each tasks motivations. All 79 personality traits listed by GPT-4o are considered, and the listed motivations for each task are manually checked. After the review, the numbers of motivations for CTA, DSP, and CPCG are 40, 50, and 47, respectively. As discussed in the multi-modal instruction data construction section, three types of traits are considered during the synthesis of instruction templates: personality, motivation, and proficiency. To improve the diversity of instruction templates, randomness is introduced in the number of trait types included in the prompt used for the synthesis of instruction templates (Fig. 8(c)). Specifically, there is 27% probability of including all three types of traits, 45% probability of including two randomly selected traits, an 18% probability of including one randomly selected trait, and 10% probability that no traits related to the questioner are provided. To accelerate the synthesis process, we generate instruction-response templates in parallel, using 10 different API keys with 10 distinct random seeds independently. Each API key is invoked 1,080 times (360 instruction-response pairs Task Type Species Dataset Protocol Drug Tissue # Samples Cell type annotation Drug sensitivity prediction Human Mouse Human Mouse He-2020-Liver [89] Segerstolpe-2016 [90] Xin-2016 [83] Ma-2020 [91] Bastidas-Ponce-2019 [92] HiSeq Ten System Smart-seq2 SMARTer SHARE-seq 10x GSE117872 [93] GSE149383 [94] GSE110894 [95] PBMC68K [96] Fluidigm C1 Drop-seq Cel-Seq 10x 10x Human Tabular-Sapiens [97] Conditional pseudo-cell generation Mouse Mouse-Atlas [98] 10x - - - - - Cisplatin Erlotinib Liver Pancreas Pancreas Skin Pancreas Oral cavity Lung BET inhibitor (I-BET-762) Bone marrow - - - Blood Spleen Blood Thymus Vasculature Bladder Liver Spleen Thymus Bladder Lung 2561 2188 20528 34135 1302 2254 1419 68185 12864 5129 7842 8112 11404 7292 35718 9275 8945 24521 Fig. 9: Detailed overview and UMAP visualizations of scRNA-seq datasets used in this work. total of 11 datasets are utilized, spanning 2 species and 11 tissue types. Among them, 5 datasets are employed for CTA, 3 for DSP, and 3 for CPCG. The number of samples for each dataset is listed under the column # Samples. The middle three UMAP plots present the single-cell data landscape, showcasing the distributions across different datasets, species, and tissues. The lower UMAP plot shows the label distribution for Ma-2020, GSE110894, and PBMC68K. Covering three distinct tasks, our data include various labels, such as response labels for DSP and cell type labels for other two tasks. 22 for each task). For two classification tasks (CTA and DSP ), there is 50% probability that the prompt will require the model to generate an instruction template containing an option placeholder {option}. After synthesis, the resulting 10 sets of instruction-response templates are merged. We define the similarity between two instruction-response pairs as the value of ROUGE-L [48] between their instruction templates. Instruction-response pairs with maximum similarity score greater than 0.75 are discarded to ensure diversity. The final numbers of instruction-response templates for CTA, DSP, and CPCG are 2,787, 2,395, and 2,080, respectively. Lastly, each tasks instruction-response templates are split into training, validation, and test sets in an 8:1:1 ratio. InstructCell employs the T5-base [70] model as the backbone. For the Q-Former module, Implementation details Lcell is set to 4, the number of query tokens is 8, and the number of key-value pairs is 6. The MLP in the Q-Former module is composed of three fully connected layers, with skip connections present in the last two layers. fully connected layer connects the cell reconstruction module to the language model, transforming the dimension of the hidden state h<CPCG> from 768 to 256. The resulting output serves as conditioning vector for the cell reconstruction module. The cell reconstruction module consists of an encoder and decoder. The encoder e contains two separate MLPs which compute posterior distributions for zs and  respectively. The decoder d includes three distinct MLPsf1, f2, and f4with f2 and f4 sharing parameters except in the final layer. learnable vector g, with dimension equal to the size of gene vocabulary, parameterizes f3. The dimension of the latent variable zs is set to 256. All experiments related to InstructCell are conducted on single-node server equipped with eight 32GB V100 GPUs. For multi-task instruction tuning, we use the gene vocabulary Vg to unify gene features across all datasets. We then merge all training splits into single mixed training set and format each sample using template randomly selected from the constructed set of instruction-response templates. Likewise, this process is also applied to generate the mixed validation set and test set. We train InstructCell using the Adafactor optimizer [99] at learning rate of 1 103 for total of 160 epochs. The batch size is set to 32 for InstructCell-chat and 64 for InstructCell-instruct. In particular, all response templates used for training InstructCell-instruct are replaced with {output}. The model with the lowest loss computed on the validation set is selected for evaluation."
        },
        {
            "title": "Related work",
            "content": "Single-cell analysis Single-cell analysis studies individual cells to understand their roles in biological systems. key tool in this field is single-cell RNA sequencing (scRNA-seq), which measures the activity of genes in each cell and their expression levels [8, 9]. The data from scRNA-seq are arranged into gene expression matrices, where rows represent genes, columns represent cells, and the numbers show how much each gene is expressed [7]. These matrices allow researchers to investigate tasks like identifying different cell types, studying tissue composition, and exploring changes in cell states. In addition to scRNA-seq, other technologies such as single-cell ATAC-seq and spatial transcriptomics further enhance the resolution and context of single-cell analysis, enabling researchers to study chromatin accessibility and spatial organization of gene expression, respectively [100, 101]. Single-cell analysis has been applied in various fields, such as uncovering tumor heterogeneity in cancer [102], profiling immune cells in health and disease [103], and mapping developmental trajectories of tissues and organs. However, this field faces significant challenges, such as managing large and complex datasets [104, 105], dealing with sparse data [56], and handling the computational needs of large-scale studies [81]. Recent advances in computational tools and algorithms are addressing these issues, allowing for more comprehensive biological insights [59, 58]. Single-cell foundation models The success of foundation models in natural language processing (NLP), such as BERT [106] and BART [24], has inspired their application in the single-cell domain, leading to the development of single-cell foundation models with broad applicability across various analytical tasks. Early approaches to analyzing single-cell gene expression matrices primarily relied on machine learning methods and autoencoder-based frameworks [107, 108, 109]. While effective for specific tasks, these models often lacked the generalizability required for wide range of analyses [110]. Recent advances in single-cell foundation models aim to address this limitation. For example, scBERT [12] leverages the BERT framework to process 23 millions of normalized scRNA-seq datasets, enabling insights into both individual and combinatorial gene expression patterns. Geneformer [13] employs self-supervised masked token prediction objective to uncover gene networks, with fine-tuning enabling chromatin and network dynamics predictions. scGPT [14] utilizes generative pre-training, achieving strong performance in cell type annotation, gene perturbation prediction, and pseudo-cell generation tasks. LangCell [111] integrates single-cell data and natural language for unified representation. scFoundation [15] leverages self-supervised pre-training on transcriptomics data to learn shared representations of diverse gene expression patterns. Instruction-following models The primary strength of large language models (LLMs) lies in their ability to understand and execute human instructions. By training on specialized instruction datasets, these models develop nuanced understanding of complex tasks, providing greater flexibility and adaptability compared to traditional foundation models. This capability has driven innovations across biology, including language-guided molecular design [112, 113, 114, 115], medical questionanswering [116], and automated experimental design [117]. Instruction-following models are now being explored in the single-cell domain. For instance, GPTCelltype [17] demonstrates the feasibility of using GPT-4 for cell type annotation, highlighting the potential of language-guided single-cell analysis. Similarly, Cell2Sentence [16] translates gene expression profiles into sequences of gene names, showcasing the integration of LLMs into single-cell data analysis. Apart from them, InstructCell introduces multi-modal architecture that integrates scRNA-seq data with natural language instructions. Through instruction tuning, it extends LLM capabilities across diverse single-cell analysis tasks. Details of multi-modal instruction data Fig. 10: Statistics of synthetic instruction-response templates. a, Length of instruction templates for different communication styles. Traits such as personality (orange), motivation (green), proficiency (red), and their combination (purple) are systematically removed to evaluate their impact, compared to the full trait version (blue). Variance for each distribution is shown in parentheses. b, Similarity of instruction templates across different communication styles. Template similarity is measured pairwise, with samples exceeding similarity threshold of 0.75 excluded. Average similarity values for each style are reported in parentheses. c, Lexical diversity of instruction templates across communication styles. Lexical diversity is quantified using the unigram ratio, defined as the proportion of unique unigrams to the total number of unigrams in each instruction template. d, Length distribution of instruction and response templates across tasks. scatter plot illustrates the lengths of instructions and responses for distinct tasks, with different colors representing task categories. 24 The multi-modal instruction dataset used in our study is pivotal in unlocking the full capabilities of InstructCell. It offers structured framework that enables the model to effectively navigate the intricate field of single-cell analysis. Fig. 10 presents detailed examination of the synthetic templates. The observed variations in instruction template length and unigram ratio indicate that personality traits significantly influence linguistic style and complexity, resulting in more diverse outputs. Motivation traits also contribute to this diversity, though to lesser extent. Proficiency traits have minimal effect on linguistic variation. Notably, higher mean similarity among samples without personality traits suggests more standardized, less nuanced language use, underscoring the value of incorporating range of traits to enhance linguistic richness and adaptability. Such diversity in communication styles is crucial for producing dynamic, effective task-oriented dialogues. Fig. 11 provides qualitative examples illustrating how InstructCell addresses various single-cell analysis tasks. These examples include the instructions, the models responses, corresponding ground-truth answers, and references to the data sources. By directly interfacing with complex biological information, InstructCell simplifies user interactions, enabling life science researchers to efficiently obtain reliable and accurate insights."
        },
        {
            "title": "Baselines",
            "content": "To demonstrate the effectiveness of InstructCell, we evaluate its performance in comparison with current state-ofthe-art single-cell analysis methods. scGAN [33] is conditional generative adversarial network tailored for generating realistic single-cell RNA-seq data. It learns complex genegene interactions to produce specified cell types, improving downstream tasks like marker gene detection and enhancing algorithm assessments. This approach may also reduce reliance on animal experiments and cut costs. scDiffusion [32] is generative model that combines diffusion and foundation modeling techniques to generate high-quality, condition-controlled scRNA-seq data. It utilizes multiple classifiers to steer the diffusion process for varied condition combinations and introduces Gradient Interpolationa novel control strategy enabling the generation of continuous cell development trajectories from specified states. Cell2Sentence [16] transforms gene expression matrices into expressions using ranked gene names, proving the effectiveness of these cell sentences and establishing framework for integrating these sentences with GPT-2. This framework involves four tasks: random cell generation, conditional cell generation, cell label prediction, and deriving natural language insights from single-cell data. scBERT [12] is deep neural network-based model that utilizes the bidirectional encoder representations from transformers (BERT) architecture tailored for single-cell analysis. Pretrained on extensive amounts of unlabeled single-cell RNA sequencing (scRNA-seq) data, scBERT captures intricate gene-gene interactions. The model is specifically designed to adapt through fine-tuning to cell type annotation tasks, leveraging its pretrained knowledge to efficiently handle both unseen and user-specific scRNA-seq data in supervised learning context. scGPT [14] is single-cell foundation model pre-trained on over 33 million cells and adapts the transformer architecture to learn cell and gene representations simultaneously. This approach establishes generative pretraining workflow tailored for non-sequential omics data. The model also includes fine-tuning pipelines with task-specific objectives to support variety of single-cell analysis tasks. Geneformer [13] is an attention-based deep learning model pretrained on around 30 million single-cell transcriptomes to enable precise predictions in network biology. It captures network dynamics and hierarchy through self-supervised learning during pretraining. Geneformer improves predictive accuracy on various downstream tasks by fine-tuning with limited task-specific data."
        },
        {
            "title": "Baseline experimental setup",
            "content": "For the comparative analysis in our study, we benchmarked InstructCell against several well-established baseline methods, including scGAN [33], scDiffusion [32], Cell2Sentence [16], scBERT [12], scGPT [14], and Geneformer [13]. We reproduced the baseline results by using the code provided in their respective GitHub repositories. To ensure fair and 25 Fig. 11: Qualitative examples of InstructCell-chat. Illustrative examples for each task are presented, showcasing instructions, model-generated responses from InstructCell-chat, the corresponding ground truth answers, and their sample sources. consistent evaluation, all models were tested using the same data splits for training, validation, and testing, mirroring our datasets configuration."
        },
        {
            "title": "Addtional experimental results",
            "content": "In this section, we provide further experimental results to highlight the capabilities of InstructCell in single-cell analysis. Fig. 12 offers an expanded view on our conditional pseudo-cell generation task by presenting the top three significant genes for each cell type across additional test datasets. The bubble plot visualizes the expression ratios and average expression levels of these genes in the generated cells, offering insights into how closely the model replicates the expected biological profiles. Fig. 13 displays confusion matrices and UMAP visualizations for cell type annotation across additional datasets. The confusion matrices demonstrate the agreement between the models predictions and ground-truth annotations, with darker shades indicating higher classification accuracy. The accompanying UMAP visualizations provide qualitative comparison of expert-annotated cell types and the predictions generated by InstructCell. The visual alignment between the two highlights the models accuracy in classifying cells based on gene expression data. 27 Fig. 12: Bubble plot for conditional pseudo-cell generation. As supplement to Fig. 2(b), this plot highlights the top three significant genes for each cell type from the remaining two datasets in our study. For model-generated cells, we display the expression ratios and average expression levels of these significant genes. Redder hues indicate higher average gene expression, while larger circles represent higher proportion of gene expression within the corresponding cell type. 28 Fig. 13: Confusion matrices and UMAP visualizations for cell type annotation datasets. a, Supplementary confusion matrices for the remaining two datasets, complementing Fig. 3(c). Darker shades represent higher frequency of agreement between model predictions and the ground-truth cell type annotations. b, UMAP visualizations comparing expert-annotated cell types (left panel) with the predictions generated by InstructCell (right panel). Different colors represent distinct cell types, illustrating the models ability to accurately reproduce the structure of the original data."
        },
        {
            "title": "Data availability",
            "content": "All datasets used in this work are publicly available and were downloaded from the following sources: Xin-2016 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE114297), Segerstolpe-2016 (https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-5061), He-2020 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE159929), PBMC68K (https://figshare.com/s/49b29cb24b27ec8b6d72), GSE117872 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE117872), GSE149383 (https://github.com/OSU-BMBL/scDEAL), Ma-2020 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE140203), Bastidas-Ponce-2019 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE132188), GSE110894 (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110894), Mouse-Atlas (https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSM4505404)."
        },
        {
            "title": "Code availability",
            "content": "The source code of this work is freely available on GitHub at https://github.com/zjunlp/InstructCell. The models developed in this work are accessible on Hugging Face at: zjunlp/InstructCell-chat (https://huggingface.co/zjunlp/InstructCell-chat) zjunlp/InstructCell-instruct (https://huggingface.co/zjunlp/InstructCell-instruct)"
        },
        {
            "title": "References",
            "content": "[1] GPT-4. https://openai.com/gpt-4, Accessed 15 Apr 2024. [2] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1240:113, 2023. [3] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. [4] Claude anthropic. https://www.anthropic.com/claude, Accessed 27 Jun 2024. [5] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [6] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike TianJian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In ICLR. OpenReview.net, 2022. [7] Alvis Brazma and Jaak Vilo. Gene expression data analysis. FEBS letters, 480(1):1724, 2000. [8] Mireya Plass, Jordi Solana, Alexander Wolf, Salah Ayoub, Aristotelis Misios, Petar Glazar, Benedikt Obermayer, Fabian Theis, Christine Kocks, and Nikolaus Rajewsky. Cell type atlas and lineage tree of whole complex animal by single-cell transcriptomics. Science, 360(6391):eaaq1723, 2018. [9] Junyue Cao, Malte Spielmann, Xiaojie Qiu, Xingfan Huang, Daniel Ibrahim, Andrew Hill, Fan Zhang, Stefan Mundlos, Lena Christiansen, Frank Steemers, et al. The single-cell transcriptional landscape of mammalian organogenesis. Nature, 566(7745):496502, 2019. [10] Tanya Barrett, Stephen Wilhite, Pierre Ledoux, Carlos Evangelista, Irene Kim, Maxim Tomashevsky, Kimberly Marshall, Katherine Phillippy, Patti Sherman, Michelle Holko, et al. Ncbi geo: archive for functional genomics data setsupdate. Nucleic acids research, 41(D1):D991D995, 2012. [11] Aviv Regev, Sarah Teichmann, Eric Lander, Ido Amit, Christophe Benoist, Ewan Birney, Bernd Bodenmiller, Peter Campbell, Piero Carninci, Menna Clatworthy, et al. The human cell atlas. elife, 6:e27041, 2017. [12] Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. scbert as large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data. Nature Machine Intelligence, 4(10):852866, 2022. [13] Christina Theodoris, Ling Xiao, Anant Chopra, Mark Chaffin, Zeina Al Sayed, Matthew Hill, Helene Mantineo, Elizabeth Brydon, Zexian Zeng, Shirley Liu, et al. Transfer learning enables predictions in network biology. Nature, 618(7965):616624, 2023. 31 [14] Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. scgpt: toward building foundation model for single-cell multi-omics using generative ai. Nature Methods, pages 111, 2024. [15] Minsheng Hao, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Xuegong Zhang, and Le Song. Large-scale foundation model on single-cell transcriptomics. Nature Methods, pages 111, 2024. [16] Daniel LeVine, Syed Asad Rizvi, Sacha Levy, Nazreen Pallikkavaliyaveetil, David Zhang, Xingyu Chen, Sina Ghadermarzi, Ruiming Wu, Zihe Zheng, Ivan Vrkic, Anna Zhong, Daphne Raskin, Insu Han, Antonio Henrique de Oliveira Fonseca, Josue Ortega Caro, Amin Karbasi, Rahul Madhav Dhodapkar, and David van Dijk. Cell2sentence: Teaching large language models the language of biology. In ICML. OpenReview.net, 2024. [17] Wenpin Hou and Zhicheng Ji. Assessing gpt-4 for cell type annotation in single-cell rna-seq analysis. Nature Methods, pages 14, 2024. [18] Yiqun Chen and James Zou. Simple and effective embedding model for single-cell biology built from chatgpt. Nature Biomedical Engineering, pages 111, 2024. [19] Tianyu Liu, Tianqi Chen, Wangjie Zheng, Xiao Luo, and Hongyu Zhao. scelmo: Embeddings from language models are good learners for single-cell data analysis. bioRxiv, pages 202312, 2023. [20] GPT-4o. https://openai.com/index/hello-gpt-4o, Accessed 29 May 2024. [21] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP, pages 30293051. Association for Computational Linguistics, 2023. [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2023. [23] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [24] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL, pages 78717880. Association for Computational Linguistics, 2020. [25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2020. [26] Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang, Wenxiu Sun, Qiong Yan, Xiaohong Liu, Guangtao Zhai, Shiqi Wang, and Weisi Lin. Towards open-ended visual quality comparison. In ECCV (3), volume 15061 of Lecture Notes in Computer Science, pages 360377. Springer, 2024. [27] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. [28] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. [29] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal language models. In NeurIPS, 2023. [30] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In NIPS, pages 34833491, 2015. 32 [31] Bernard Welch. The generalization of studentsproblem when several different population varlances are involved. Biometrika, 34(1-2):2835, 1947. [32] Erpai Luo, Minsheng Hao, Lei Wei, and Xuegong Zhang. scdiffusion: conditional generation of high-quality single-cell data using diffusion model. Bioinformatics, 40(9):btae518, 2024. [33] Mohamed Marouf, Pierre Machart, Vikas Bansal, Christoph Kilian, Daniel Magruder, Christian Krebs, and Stefan Bonn. Realistic in silico generation and augmentation of single-cell rna-seq data using generative adversarial networks. Nature communications, 11(1):166, 2020. [34] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR (Workshop Poster), 2014. [35] Congxue Hu, Tengyue Li, Yingqi Xu, Xinxin Zhang, Feng Li, Jing Bai, Jing Chen, Wenqi Jiang, Kaiyue Yang, Qi Ou, Xia Li, Peng Wang, and Yunpeng Zhang. Cellmarker 2.0: an updated database of manually curated cell markers in human/mouse and web tools based on scrna-seq data. Nucleic Acids Res., 51(D1):870876, 2023. [36] Mauro Muraro, Gitanjali Dharmadhikari, Dominic Grun, Nathalie Groen, Tim Dielen, Erik Jansen, Leon Van Gurp, Marten Engelse, Francoise Carlotti, Eelco Jp De Koning, et al. single-cell transcriptome atlas of the human pancreas. Cell systems, 3(4):385394, 2016. [37] Leon van Gurp, Leon Fodoulian, Daniel Oropeza, Kenichiro Furuyama, Eva Bru-Tari, Anh Nguyet Vu, John Kaddis, Ivan Rodrguez, Fabrizio Thorel, and Pedro Herrera. Generation of human islet cell type-specific identity genesets. Nature communications, 13(1):2020, 2022. [38] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023. [39] Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. Llms as narcissistic evaluators: When ego inflates evaluation scores. CoRR, abs/2311.09766, 2023. [40] Md. Rabiul Awal, Rui Cao, Roy Ka-Wei Lee, and Sandra Mitrovic. Angrybert: Joint learning target and emotion for hate speech detection. In PAKDD (1), volume 12712 of Lecture Notes in Computer Science, pages 701713. Springer, 2021. [41] Yu Zhang and Qiang Yang. survey on multi-task learning. IEEE Trans. Knowl. Data Eng., 34(12):55865609, 2022. [42] Yusuf Roohani, Kexin Huang, and Jure Leskovec. Predicting transcriptional outcomes of novel multigene perturbations with gears. Nature Biotechnology, 42(6):927935, 2024. [43] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [44] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. [45] Bingxiang He, Ning Ding, Cheng Qian, Jia Deng, Ganqu Cui, Lifan Yuan, Huan-ang Gao, Huimin Chen, Zhiyuan Liu, and Maosong Sun. Zero-shot generalization during instruction tuning: Insights from similarity and granularity. arXiv preprint arXiv:2406.11721, 2024. [46] Tal Ashuach, Daniel Reidenbach, Adam Gayoso, and Nir Yosef. Peakvi: deep generative model for single-cell chromatin accessibility analysis. Cell reports methods, 2(3), 2022. [47] Tianyu Liu, Yuge Wang, Rex Ying, and Hongyu Zhao. Muse-gnn: learning unified gene representation from multimodal biological graph data. Advances in neural information processing systems, 36, 2024. 33 [48] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [49] Ashish Vaswani. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. [50] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [51] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [52] DP Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [53] Michael Love, Wolfgang Huber, and Simon Anders. Moderated estimation of fold change and dispersion for rna-seq data with deseq2. Genome biology, 15:121, 2014. [54] Saket Choudhary and Rahul Satija. Comparison and evaluation of statistical error models for scrna-seq. Genome biology, 23(1):27, 2022. [55] Jiarui Ding, Xian Adiconis, Sean Simmons, Monika Kowalczyk, Cynthia Hession, Nemanja Marjanovic, Travis Hughes, Marc Wadsworth, Tyler Burks, Lan Nguyen, et al. Systematic comparison of single-cell and single-nucleus rna-sequencing methods. Nature biotechnology, 38(6):737746, 2020. [56] Gerard Bouland, Ahmed Mahfouz, and Marcel JT Reinders. Consequences and opportunities arising due to sparser single-cell rna-seq datasets. Genome biology, 24(1):86, 2023. [57] Beate Vieth, Christoph Ziegenhain, Swati Parekh, Wolfgang Enard, and Ines Hellmann. powsimr: power analysis for bulk and single cell rna-seq experiments. Bioinformatics, 33(21):34863488, 2017. [58] Gokcen Eraslan, Lukas Simon, Maria Mircea, Nikola Mueller, and Fabian Theis. Single-cell rna-seq denoising using deep count autoencoder. Nature communications, 10(1):390, 2019. [59] Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, and Nir Yosef. Deep generative modeling for single-cell transcriptomics. Nature methods, 15(12):10531058, 2018. [60] Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang, and Tarek Abdelzaher. Controlvae: Controllable variational autoencoder. In International conference on machine learning, pages 86558664. PMLR, 2020. [61] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [62] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), pages 41714186. Association for Computational Linguistics, 2019. [63] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [64] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: lite BERT for self-supervised learning of language representations. In ICLR. OpenReview.net, 2020. [65] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: decoding-enhanced bert with disentangled attention. In ICLR. OpenReview.net, 2021. [66] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [67] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 34 [68] Ben Wang and Aran Komatsuzaki. Gpt-j-6b: 6 billion parameter autoregressive language model, 2021. [69] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. In NeurIPS, pages 1304213054, 2019. [70] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [71] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International conference on machine learning, pages 1132811339. PMLR, 2020. [72] Jonathan Baxter. bayesian/information theoretic model of learning to learn via multiple task sampling. Machine learning, 28:739, 1997. [73] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017. [74] Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, and Ding Chen. xfinder: Robust and pinpoint answer extraction for large language models. CoRR, abs/2405.11874, 2024. [75] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. [76] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alexander Smola. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012. [77] Uri Shaham, Kelly Stanton, Jun Zhao, Huamin Li, Khadir Raddassi, Ruth Montgomery, and Yuval Kluger. Removal of batch effects using distribution-matching residual networks. Bioinformatics, 33(16):25392546, 2017. [78] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian J. Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In NeurIPS, pages 95259536, 2018. [79] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [80] Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai Guan Ng, Florent Ginhoux, and Evan Newell. Dimensionality reduction for visualizing single-cell data using umap. Nature biotechnology, 37(1):3844, 2019. [81] Alexander Wolf, Philipp Angerer, and Fabian Theis. Scanpy: large-scale single-cell gene expression data analysis. Genome biology, 19:15, 2018. [82] Davis McCarthy, Kieran Campbell, Aaron TL Lun, and Quin Wills. Scater: pre-processing, quality control, normalization and visualization of single-cell rna-seq data in r. Bioinformatics, 33(8):11791186, 2017. [83] Yurong Xin, Jinrang Kim, Haruka Okamoto, Min Ni, Yi Wei, Christina Adler, Andrew Murphy, George Yancopoulos, Calvin Lin, and Jesper Gromada. Rna sequencing of single human islet cells reveals type 2 diabetes genes. Cell metabolism, 24(4):608615, 2016. [84] Sophie Tritschler, Moritz Thomas, Anika Bottcher, Barbara Ludwig, Janine Schmid, Undine Schubert, Elisabeth Kemter, Eckhard Wolf, Heiko Lickert, and Fabian Theis. transcriptional cross species map of pancreatic islet cells. Molecular Metabolism, 66:101595, 2022. [85] pyEnsembl. https://github.com/openvax/pyensembl?tab=readme-ov-file. [86] pybiomart. https://github.com/jrderuiter/pybiomart. [87] Rahul Satija, Jeffrey Farrell, David Gennert, Alexander Schier, and Aviv Regev. Spatial reconstruction of single-cell gene expression data. Nature biotechnology, 33(5):495502, 2015. [88] Tim Stuart, Andrew Butler, Paul Hoffman, Christoph Hafemeister, Efthymia Papalexi, William Mauck, Yuhan Hao, Marlon Stoeckius, Peter Smibert, and Rahul Satija. Comprehensive integration of single-cell data. cell, 177(7):18881902, 2019. [89] Shuai He, Lin-He Wang, Yang Liu, Yi-Qi Li, Hai-Tian Chen, Jing-Hong Xu, Wan Peng, Guo-Wang Lin, Pan-Pan Wei, Bo Li, et al. Single-cell transcriptome profiling of an adult human cell atlas of 15 major organs. Genome biology, 21:134, 2020. [90] Asa Segerstolpe, Athanasia Palasantza, Pernilla Eliasson, Eva-Marie Andersson, Anne-Christine Andreasson, Xiaoyan Sun, Simone Picelli, Alan Sabirsh, Maryam Clausen, Magnus Bjursell, et al. Single-cell transcriptome profiling of human pancreatic islets in health and type 2 diabetes. Cell metabolism, 24(4):593607, 2016. [91] Sai Ma, Bing Zhang, Lindsay LaFave, Andrew Earl, Zachary Chiang, Yan Hu, Jiarui Ding, Alison Brack, Vinay Kartha, Tristan Tay, et al. Chromatin potential identified by shared single-cell profiling of rna and chromatin. Cell, 183(4):11031116, 2020. [92] Aimee Bastidas-Ponce, Sophie Tritschler, Leander Dony, Katharina Scheibner, Marta Tarquis-Medina, Ciro Salinno, Silvia Schirge, Ingo Burtscher, Anika Bottcher, Fabian Theis, et al. Comprehensive single cell mrna profiling reveals detailed roadmap for pancreatic endocrinogenesis. Development, 146(12):dev173849, 2019. [93] Ankur Sharma, Elaine Yiqun Cao, Vibhor Kumar, Xiaoqian Zhang, Hui Sun Leong, Angeline Mei Lin Wong, Neeraja Ramakrishnan, Muhammad Hakimullah, Hui Min Vivian Teo, Fui Teen Chong, et al. Longitudinal single-cell rna sequencing of patient-derived primary cells reveals drug-induced infidelity in stem cell hierarchy. Nature communications, 9(1):4931, 2018. [94] Alexandre Aissa, Abul BMMK Islam, Majd Ariss, Cammille Go, Alexandra Rader, Ryan Conrardy, Alexa Gajda, Carlota Rubio-Perez, Klara Valyi-Nagy, Mary Pasquinelli, et al. Single-cell transcriptional changes associated with drug tolerance and response to combination therapies in cancer. Nature communications, 12(1):1628, 2021. [95] Charles Bell, Katie Fennell, Yih-Chih Chan, Florian Rambow, Miriam Yeung, Dane Vassiliadis, Luis Lara, Paul Yeh, Luciano Martelotto, Aljosja Rogiers, et al. Targeting enhancer switching overcomes non-genetic drug resistance in acute myeloid leukaemia. Nature communications, 10(1):2723, 2019. [96] Grace XY Zheng, Jessica Terry, Phillip Belgrader, Paul Ryvkin, Zachary Bent, Ryan Wilson, Solongo Ziraldo, Tobias Wheeler, Geoff McDermott, Junjie Zhu, et al. Massively parallel digital transcriptional profiling of single cells. Nature communications, 8(1):14049, 2017. [97] The Tabula Sapiens Consortium*, Robert Jones, Jim Karkanias, Mark Krasnow, Angela Oliveira Pisco, Stephen Quake, Julia Salzman, Nir Yosef, Bryan Bulthaup, Phillip Brown, et al. The tabula sapiens: multiple-organ, single-cell transcriptomic atlas of humans. Science, 376(6594):eabl4896, 2022. [98] single-cell transcriptomic atlas characterizes ageing tissues in the mouse. Nature, 583(7817):590595, 2020. [99] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 45964604. PMLR, 2018. [100] Aaron Newman, Chih Long Liu, Michael Green, Andrew Gentles, Weiguo Feng, Yue Xu, Chuong Hoang, Maximilian Diehn, and Ash Alizadeh. Robust enumeration of cell subsets from tissue expression profiles. Nature methods, 12(5):453457, 2015. [101] Patrik Stahl, Fredrik Salmen, Sanja Vickovic, Anna Lundmark, Jose Fernandez Navarro, Jens Magnusson, Stefania Giacomello, Michaela Asp, Jakub Westholm, Mikael Huss, et al. Visualization and analysis of gene expression in tissue sections by spatial transcriptomics. Science, 353(6294):7882, 2016. [102] Levy and Bartosch. Metabolic reprogramming: hallmark of viral oncogenesis. Oncogene, 35(32):41554164, 2016. [103] Alexandra-Chloe Villani, Rahul Satija, Gary Reynolds, Siranush Sarkizova, Karthik Shekhar, James Fletcher, Morgane Griesbeck, Andrew Butler, Shiwei Zheng, Suzan Lazo, et al. Single-cell rna-seq reveals new types of human blood dendritic cells, monocytes, and progenitors. Science, 356(6335):eaah4573, 2017. [104] Yan Wu and Kun Zhang. Tools for the analysis of high-dimensional single-cell rna sequencing data. Nature Reviews Nephrology, 16(7):408421, 2020. [105] Alejandro Tejada-Lapuerta, Paul Bertin, Stefan Bauer, Hananeh Aliee, Yoshua Bengio, and Fabian Theis. Causal machine learning for single-cell genomics. arXiv preprint arXiv:2310.14935, 2023. [106] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), pages 41714186. Association for Computational Linguistics, 2019. [107] Jiajia Liu, Zhiwei Fan, Weiling Zhao, and Xiaobo Zhou. Machine intelligence in single-cell data analysis: advances and new challenges. Frontiers in Genetics, 12:655536, 2021. [108] Sergio Oller-Moreno, Karin Kloiber, Pierre Machart, and Stefan Bonn. Algorithmic advances in machine learning for single-cell expression analysis. Current Opinion in Systems Biology, 25:2733, 2021. [109] Yuge Ji, Mohammad Lotfollahi, Alexander Wolf, and Fabian Theis. Machine learning for perturbational single-cell omics. Cell Systems, 12(6):522537, 2021. [110] Philipp Angerer, Lukas Simon, Sophie Tritschler, Alexander Wolf, David Fischer, and Fabian Theis. Single cells make big data: new challenges and opportunities in transcriptomics. Current opinion in systems biology, 4:8591, 2017. [111] Suyuan Zhao, Jiahuan Zhang, Yushuai Wu, Yizhen Luo, and Zaiqing Nie. Langcell: Language-cell pre-training for cell identity understanding. In ICML. OpenReview.net, 2024. [112] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 375413, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [113] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: large-scale biomolecular instruction dataset for large language models. ICLR, 2024. [114] Zheni Zeng, Bangchen Yin, Shipeng Wang, Jiarui Liu, Cheng Yang, Haishen Yao, Xingzhi Sun, Maosong Sun, Guotong Xie, and Zhiyuan Liu. Chatmol: interactive molecular discovery with natural language. Bioinformatics, 40(9):btae534, 2024. [115] Xiangru Tang, Andrew Tran, Jeffrey Tan, and Mark Gerstein. Mollm: unified language model for integrating biomedical text with 2d and 3d molecular representations. Bioinformatics, 40(Supplement 1):i357i368, 2024. [116] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023. [117] A. Boiko Daniil, MacKnight Robert, Kline Ben, and Gomes Gabe. Autonomous chemical research with large language models. Nature, pages 570578, 2023."
        }
    ],
    "affiliations": [
        "College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China",
        "College of Pharmaceutical Sciences, Zhejiang University, Hangzhou 310058, China",
        "Future Health Laboratory, Innovation Center of Yangtze River Delta, Zhejiang University, Jiaxing 314100, China",
        "Innovation Center in Zhejiang University, State Key Laboratory of Component-Based Chinese Medicine, Hangzhou 310058, China",
        "School of Software Technology, Zhejiang University, Ningbo 315048, China",
        "ZJU-Hangzhou Global Scientific and Technological Innovation Center, Hangzhou 311200, China"
    ]
}