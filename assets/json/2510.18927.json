{
    "paper_title": "BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping",
    "authors": [
        "Zhiheng Xi",
        "Xin Guo",
        "Yang Nan",
        "Enyu Zhou",
        "Junrui Shen",
        "Wenxiang Chen",
        "Jiaqi Liu",
        "Jixuan Huang",
        "Zhihao Zhang",
        "Honglin Guo",
        "Xun Deng",
        "Zhikai Lei",
        "Miao Zheng",
        "Guoteng Wang",
        "Shuo Zhang",
        "Peng Sun",
        "Rui Zheng",
        "Hang Yan",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settings--where stale data from past policies are used for training--improves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), a simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenarios--including sample replay and partial rollout--BAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking."
        },
        {
            "title": "Start",
            "content": "2025-10-23 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Zhiheng Xi1, Xin Guo1, Yang Nan1, Enyu Zhou1, Junrui Shen1, Wenxiang Chen1, Jiaqi Liu1, Jixuan Huang1, Zhihao Zhang1, Honglin Guo1, Xun Deng2, Zhikai Lei2, Miao Zheng2, Guoteng Wang2, Shuo Zhang2, Peng Sun2, Rui Zheng2, Hang Yan2, Tao Gui1,3, Qi Zhang1, Xuanjing Huang1 1Fudan University 2Shanghai Qiji Zhifeng Co., Ltd. 3Shanghai Innovation Institute zhxi22@m.fudan.edu.cn, {tgui,qz}@fudan.edu.cn Reinforcement learning (RL) has recently become the core paradigm for aligning and strengthening large language models (LLMs). Yet, applying RL in off-policy settingswhere stale data from past policies are used for trainingimproves sample efficiency, but remains challenging: policy entropy declines sharply, optimization often becomes unstable and may even collapse. Through theoretical and empirical analysis, we identify two key insights: (i) an imbalance in optimization, where negative-advantage samples dominate the policy gradient, suppressing useful behaviors and risking gradient explosions; and (ii) the derived Entropy-Clip Rule, which reveals that the fixed clipping mechanism in PPO-like objectives systematically blocks entropy-increasing updates, thereby driving the policy toward over-exploitation at the expense of exploration. Building on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), simple yet effective method that dynamically adjusts clipping bounds to adaptively re-balance positive and negative contributions, preserve entropy, and stabilize RL optimization. Across diverse off-policy scenariosincluding sample replay and partial rolloutBAPO achieves fast, stable, and data-efficient training. On AIME 2024 and AIME 2025 benchmarks, our 7B BAPO model surpasses open-source counterparts such as SkyWork-OR1-7B, while our 32B BAPO model not only achieves state-of-the-art results among models of the same scale but also outperforms leading proprietary systems like o3-mini and Gemini-2.5-Flash-Thinking. 5 2 0 2 1 2 ] . [ 1 7 2 9 8 1 . 0 1 5 2 : r Figure 1 Performance of BAlanced Policy Optimization with Adaptive Clipping (BAPO). *Equal contribution. Corresponding authors. 1Our code are available at https://github.com/WooooDyy/BAPO. BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping 1. Introduction Reinforcement learning (RL) has become pivotal paradigm for optimizing large language models (LLMs) (Zhang et al., 2025), delivering significant improvements in complex tasks such as reasoning (Guo et al., 2025; Jaech et al., 2024), coding (Anthropic, 2025), and agentic decision-making (Bai et al., 2025). Among RL methods, off-policy RLwhere the rollout policy (behavior policy) differs from the training policy (target policy)emerges as particularly promising (Arnal et al., 2025; Roux et al., 2025). It offers high sample efficiency and tolerance to data staleness, making it well-suited for extremely long-horizon and challenging scenarios, while also aligning more naturally with features in modern AI infrastructures such as partial rollout (Fu et al., 2025; Team et al., 2025). However, applying off-policy RL to LLMs introduces substantial challenges (Arnal et al., 2025; Yu et al., 2025). As shown in Figure 2, increasing data staleness leads to unstable optimization, exploding gradient and even collapse. Meanwhile, policy entropy declines sharply, reflecting reduced exploratory capacity and bias toward over-exploitation. By contrast, on-policy trainingwhere rollout and target policies coincideremains stable across metrics, consistent with prior studies (Arnal et al., 2025; Roux et al., 2025; Tang et al., 2024). Figure 2 Preliminary results with different data staleness. As the staleness increases, the model suffers from unstable optimization, decreasing entropy, and even sudden collapse in training. To understand the instability of off-policy training, we conduct comprehensive theoretical and empirical analysis to reveal two key insights. We first demonstrate an imbalance in optimization: policy updates are often dominated by negative-advantage samples, producing excessive penalization signals that suppress even neutral or correct actions and may cause gradient explosions (G칲l칞ehre et al., 2023). We then derive and empirically validate the Entropy-Clip Rule in the widely-used PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024), showing that the clipping mechanism in PPOlike objectives blocks many low-probability positive tokens while over-penalizing low-probability negatives. This systematically excludes entropy-increasing updates, sharpens the output distribution, and drives policies toward over-exploitation at the cost of exploration. Based on these insights, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), new method for stable and effective off-policy RL. BAPO dynamically adjusts the clipping bounds to re-balance positive and negative contributions for each update step, incorporate low-probability positives while filtering excessive negatives, and preserve policy entropyachieving better balance between exploration and exploitation. An overview of our approach is illustrated on the right side of Figure 3. Experiments across diverse off-policy scenariosincluding sample replay, partial rollout, and varying degrees of stalenesson base models such as DeepSeek-R1-Distill-Qwen-7B (Guo et al., 2025) and OctoThinker-Llama3.2-3B-Long-Zero (Wang et al., 2025b) show that BAPO consistently yields significant improvements. Our 7B model achieves scores of 70.8 on AIME24 and 62.5 on AIME25, 2 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Figure 3 An illustration of our proposed BAPO. (Left) Baseline methods like GRPO use symmetric fixed clipping bounds, reinforcing high-probability positive tokens while penalizing excessive lowprobability negatives, leading to sharp distributions and entropy collapse. (Right) BAPO dynamically adjusts the clipping bounds 洧녫low and 洧녫high based on the loss contributions from positive tokens. It excludes overly negative tokens to maintain smoother distribution and incorporates previously clipped positive tokens to preserve entropy balance. surpassing open-source counterparts such as SkyWork-OR1-7B (He et al., 2025). Moreover, our 32B model reaches 87.1 on AIME24 and 80.0 on AIME25, outperforming both comparably scaled open-source models like Qwen3-32B (Yang et al., 2025a) and leading proprietary systems including o3-mini-medium (OpenAI, 2025) and Gemini-2.5-Flash-Thinking (Comanici et al., 2025). Our contributions are summarized as follows: We identify and analyze two key insights behind instability in off-policy RL for LLMs: the imbalanced optimization and the Entropy-Clip Rule. (3) We propose BAPO, new RL algorithm that dynamically adjusts clipping bounds to balance positive and negative signals, preserving entropy for exploration, and stabilizing training. (4) We validate BAPO across multiple backbones, model scales, and off-policy settings, showing that it achieves stable optimization and competitive results with proprietary systems. (5) 2. Preliminaries 2.1. Policy Gradient In the field of LLM RL (Jaech et al., 2024; Trung et al., 2024), policy gradient-based (PG) algorithms (Williams, 1992) are widely used. Specifically, given an input prompt 洧눛, an LLM 洧랢洧랚 sequentially generates 洧녢-token response 洧눜 = ( 洧녽1, ..., 洧녽洧녢 ): Given training dataset = {洧눛1, ..., 洧눛洧녜 } and reward function 洧녠, the RL objective is to maximize the expected reward: 洧랢洧랚( 洧눜洧눛) = (cid:206)洧녢 洧노=1 洧랢洧랚( 洧녽洧노 洧눛, 洧눜<洧노). (1) PG algorithms then leverage gradient ascent to optimize the policy with the following gradient: 洧냫 (洧랚) = 洧댶洧눛D, 洧눜洧랢洧랚 ( 洧눛) [洧녠(洧눛, 洧눜)] . 洧랚 洧냫 (洧랚) = 洧댶洧눛D, 洧눜洧랢洧랚 ( 洧눛) 洧랚 log 洧랢洧랚( 洧녽洧노 洧눛, 洧눜<洧노) 洧냢洧노 , (cid:35) (cid:34) 洧녢 洧노=1 (2) (3) BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping where 洧냢洧노 denotes the estimated advantage at time step 洧노, i.e., how much better action 洧녽洧노 is than the expected action under the current policy. 2.2. Importance Sampling and PPO Objective To improve sample efficiency and adapt to modern infrastructure, mainstream RL algorithms for LLMs typically adopt PPO-like surrogate objective (Schulman et al., 2017): 洧냫PPO(洧랚) = 洧댶洧눛D, 洧눜洧랢洧랚rollout ( 洧눛) 洧녢 洧노=1 [min(洧洧노 洧냢洧노, clip(洧洧노, 1 洧, 1 + 洧) 洧냢洧노)] , (4) 洧랢洧랚rollout ( 洧녽洧노 洧눛,洧눜<洧노 ) is the importance weight that corrects for the distribution mismatch, estiwhere 洧洧노 = 洧랢洧랚 ( 洧녽洧노 洧눛,洧눜<洧노 ) under the target mating the expected advantage of tokens generated by the behavior policy 洧랢洧랚rollout policy 洧랢洧랚. The clipping mechanism in PPO serves to implicitly enforce trust region between the behavior and target policies, preventing overly large policy updates that could destabilize training. The hyperparameter 洧 (0, 1) determines the width of this clipping interval. We then analyze data with positive and negative advantages respectively. The policy gradient can then be expressed as: 洧냫PPO = 洧랢洧랚( 洧녽洧노) 洧{洧洧노 < 1 + 洧} 洧냢洧노 log 洧랢洧랚( 洧녽洧노) + 洧랢洧랚( 洧녽洧노) 洧{洧洧노 > 1 洧} 洧냢洧노 log 洧랢洧랚( 洧녽洧노) , (5) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 洧냢洧노 >0 (cid:124) (cid:123)(cid:122) positive tokens 洧냢洧노 <0 (cid:124) (cid:125) (cid:123)(cid:122) negative tokens (cid:125) where 洧 represents the indicator function. 3. Motivation: Imbalanced Optimization and Entropy-Clip Rule In this section, we first conduct preliminary experiments to show the influence of data staleness on the RL optimization process. Next, we perform in-depth empirical and theoretical analysis to reveal the underlying mechanisms and provide new insights. Figure 4 Contribution of positive and negative tokens to the policy-gradient loss and their proportion of tokens during training. Figure 5 Relationship between token probability and importance sampling weight. Training instability with data staleness. We perform experiments under different levels of data staleness using the popular GRPO algorithm. Results in Figure 2 show that, compared to on-policy training, off-policy RL typically suffers from instability, and entropy decreases rapidly, reflecting reduced exploratory capacity (He et al., 2025). As staleness increases, the entropy decline becomes more severe and larger number of tokens are clipped; meanwhile, training becomes more unstable. In the following paragraphs, we attempt to explain this phenomenon from different perspectives and summarize the motivation behind our method. 4 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Excessive negative samples lead to imbalanced optimization. Within the PPO-like objective for policy updates, we analyze tokens with positive and negative advantages separately, as shown in Equation 5. Empirical results in Figure 4 reveal pronounced imbalance: positive samples constitute minority both in number and in their contribution to the policy-gradient loss. We attribute this skew to two main factors: (i) the model tends to generate longer trajectories on difficult queries, thereby producing more tokens in negative samples (Figure 6); and (ii) in early stages of training, the model has not yet acquired sufficient capability, resulting in higher proportion of negative samples. This observation may help explain the effectiveness of certain curriculum-based approaches (Xi et al., 2024; Yuan et al., 2025). Figure 6 Average model response length during training. In the RL training of LLMs, reinforcing positive samples is often more efficient for driving performance gains than attempting to suppress the vast number of negative samples (G칲l칞ehre et al., 2023; Zhu et al., 2025). To this end, prior work has proposed amplifying positive signals through the clip-higher technique (Yu et al., 2025). However, merely enlarging the clipping upper bound does not mitigate the influence of negative data, thus failing to prevent them from dominating the optimization process. Moreover, as shown in Equation 5, the accumulation of low-probability negative tokens (i.e., 洧랢洧랚( 洧녽洧노) 0, driving the log term toward ) may trigger gradient explosion, further destabilizing training (Yang et al., 2025c). The Entropy-Clip Rule exposes insufficient entropy promotion in optimization, leading to entropy collapse. Theoretically, we derives Equation 6 (see Appendix for detailed derivations) for PPO surrogate objective to reveal the factors that influence the policy entropy (Roux et al., 2025): 풊H (洧랢洧랚) 洧랙 Cov洧눜洧랢洧랚 ( 洧눛) [log 洧랢洧랚( 洧녽洧노 洧눛, 洧눜<洧눗), 洧냢洧노 X( 洧녽洧노) + 洧냤] , where 洧냤 is constant, and X( 洧녽洧노) = 1, if 洧냢洧노 > 0 & 洧洧노 < 1 + 洧랬 or 洧냢洧노 < 0 & 洧洧노 > 1 洧랬 0, otherwise. (6) (7) We observe that changes in policy entropy are driven by the influence of unclipped tokens, which is determined by the covariance between their log probabilities and advantages. We term this as the Entropy-Clip Rule. The left side of Figure 3 illustrates how the optimization of different tokens influences the probability distribution, thereby affecting entropy. The Entropy-Clip Rule theoretically explains the following statement: Specifically, updating the policy with positive high-probability tokens (high advantage, high probability) and negative low-probability tokens (low advantage, low probability) sharpens the distribution and consequently reduces entropy. Conversely, updating the policy with negative high-probability tokens and positive low-probability tokens smooths the distribution, resulting in an increase in entropy (detailed proofs are available in Appendix B.4.2). Empirically, our statistical analysis on token probabilities and their importance sampling (IS) weights further clarifies this dynamic. As shown in Figure 5, we find that tokens with either very high or very low IS weights tend to have low probabilities. However, in standard algorithms with symmetric clipping bounds (e.g., [0.8,1.2]), majority of positive, low-probability tokens are prevented from contributing to the optimization. This systematic exclusion of entropy-increasing upBAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping dates causes continuous decline in entropy, ultimately crippling the models exploratory capacity and resulting in performance bottleneck. Summary of motivation. Based on the above analysis, we summarize two main motivations: (1) to balance the contributions of positive and negative tokens while preventing gradient explosion, and (2) to preserve policy entropy for sustaining exploration and preventing collapse. 4. Methodology 4.1. Validation Experiment: Asymmetric Clipping The main idea of our method is to stabilize the training and maintain exploration ability of the policy by asymmetrically adjusting the trust region for positive and negative tokens, i.e., adjusting 洧녫low and 洧녫high. We then conduct preliminary experiments to examine whether asymmetrically adjusting the clipping range could influence training dynamics. The results, shown in Figure 7, together with Figure 5, reveal that increasing the upper bound 洧녫high (which introduces more lowprobability positive tokens to policy updates) improves performance while counteracting the downward trend of entropy, albeit at rapid pace. In contrast, relaxing the lower bound 洧녫low (which introduces more low-probability negative tokens to policy updates) not only degrades performance but also accelerates entropy collapse. These findings confirm the effectiveness of entropy control through asymmetric clipping. Nevertheless, this strategy remains relatively rigid and manually specified, providing limited flexibility and adaptation. Figure 7 Training dynamics of asymmetric clipping experiments. 4.2. BAPO: BAlanced Policy Optimization with Adaptive Clipping To this end, we propose BAlanced Policy Optimization with Adaptive Clipping (BAPO), new method to achieve stable, fast RL optimization for LLMs. The core insight of BAPO lies in its adaptive clipping mechanism, which dynamically adjusts the clipping bounds 洧녫high and 洧녫low, to regulate the positive contribution to the policy loss and maintain balance in entropy throughout RL training. Formally, for each update with batch, our goal is to find pair of 洧녫high and 洧녫low that satisfy: (cid:205)洧냢洧노 >0 洧랢洧랚rollout ( 洧녽洧노) (cid:2)min(洧洧노 洧냢洧노, clip(洧洧노, 0, 洧녫high) 洧냢洧노)(cid:3) 洧랢洧랚rollout ( 洧녽洧노) (cid:2)min(洧洧노 洧냢洧노, clip(洧洧노, 洧녫low, 洧녫high) 洧냢洧노)(cid:3) (cid:205)洧냢洧노 where 洧랣0 is the target contribution of positive signals to the policy gradient loss. Specifically, BAPO gradually increases 洧녫high and 洧녫low with step sizes of 洧1 and 洧2, respectively, until the condition in Equation 8 is met. We present an overview of BAPO in Figure 3 and summarize it in Algorithm 1. 洧랣0 , (8) Overall, BAPO offers several significant benefits. First, by dynamically adjusting 洧녫high and 洧녫low for each step, we can increase the contribution of positive tokens to the policy-gradient loss while preventing negative tokens from excessively dominating the optimization objective. Second, based on our earlier analysis of the relationship between IS weights and token probabilities in Figure 5, BAPO incorporates more low-probability positive tokens and filters out more low-probability negative 6 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Algorithm 1: BAPO Input: Initialized LLM policy 洧랢洧랚, training dataset D, reward function 洧녠, staleness 洧냦, movable range of clipping bounds [洧녩, 洧녪] and [洧녩+, 洧녪+], step size of upper bound 洧1, step size of lower bound 洧2, positive token contribution threshold 洧랣0 1 for step 洧 = 1...洧녡 do 2 Procedure Sample and filter out responses Update the old LLM policy 洧랢洧랚rollout 洧랢洧랚 ; Sample the 洧-th batch D洧 from ; Sample 洧냨 responses { 洧눜洧녰}洧냨 Compute reward and advantage for each 洧눜洧녰 based on reward function 洧녠 ; 洧녰=1 洧랢洧랚rollout (洧눛), where 洧눛 D洧 ; for staleness = 0...洧냦 do Procedure Dynamically adjusting the clipping bounds 洧녫high and 洧녫low Initialize clipping bounds 洧녫low = 洧녩 and 洧녫high = 洧녩+ ; while the positive token contribution 洧랣 < 洧랣0 and 洧녫low + 洧2 洧녪 do if 洧녫high + 洧1 洧녪+ then 洧녫high 洧녫high + 洧1 else 洧녫low 洧녫low + 洧2 end end Procedure Update the LLM policy 洧랢洧랚 Update the LLM policy 洧랢洧랚 by maximizing the following objective: (cid:2)min(洧洧노 洧냢洧노, clip(洧洧노, 洧녫low, 洧녫high) 洧냢洧노)(cid:3) 洧냫BAPO(洧랚) = 洧댶洧눜洧랢洧랚rollout ( 洧눛) (cid:205)洧녢 洧노=1 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 end 21 22 end tokens, both of which contribute to maintaining entropy. Third, by setting the target contribution from positive tokens, BAPO prevents uncontrolled entropy growth, avoids situations where positive tokens overwhelm the loss, and mitigates tail degradationwhere the model overfits to easy problems but fails to handle more challenging ones (Ding et al., 2025). 4.3. Analysis Stable and fast training of BAPO. As shown in Figure 9, BAPO enables more stable optimization process, characterized by rapidly increasing training rewards, greater contributions from positive tokens, steady gradient normalization, and stable policy entropyresulting in an improved balance between exploration and exploitation. We further visualize the adjustment process of the clipping bounds in BAPO. As shown in Figure 8, the averaged upper and lower bounds both fluctuate during training, confirming that BAPO dynamically adjusts the clipping for both types of data and adaptively balances their contributions to the loss. In contrast to approaches such as DAPO (Yu et al., 2025) or the asymmetric clipping in Section 4.1, which rely on empirical tuning, BAPO eliminates the need for complex manual hyperparameter tuning, making it simple yet effective. Figure 8 Clipping bounds. 7 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Figure 9 Training dynamics of BAPO. Figure 10 Relationship among token probabilities, importance sampling weights, and entropy. Effectiveness of BAPO across different staleness. We conduct experiments using the R1-Distill model (Guo et al., 2025) on the SkyWork-OR1-RL dataset (He et al., 2025), with maximum sequence length of 32洧녲. The results in Figure 11 show that under different data staleness, our method consistently outperforms both the baseline and the clip-higher approach, demonstrating its superiority. Figure 11 Results with different data staleness. The working mechanism of BAPO and its connection to prior work. To better understand the working mechanism of BAPO, we present the relationship among token probabilities, IS weights, and entropy during training in Figure 10. We find that as IS weights deviate further from 1, the corresponding token probabilities decrease, and such low-probability tokens often exhibit higher entropy. Based on this observation, we explain how BAPO relates to prior work. For example, Clip-Higher in Yu et al. (2025) sets the clipping upper bound to 1.28, thereby including more low-probability positive tokens in training, which stabilizes entropy while balancing the contributions of positive and negative tokens. Similarly, Wang et al. (2025a) retain only the top 20% highest-entropy tokens for training, ensuring stable entropy throughout optimization and preserving the models exploratory capability, and the target entropy technique in He et al. (2025) plays similar role, which aligns with our motivation. 5. Experiments and Discussion 5.1. Experimental Setups Datasets and Models. We use SkyWork-OR1-RL-Data (He et al., 2025) as our RL dataset, as it is widely adopted and of high quality. For evaluation, we employ both the AIME 2024 and the 8 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Table 1 Main evaluation results. Model Model Size AIME 2024 AIME 2025 Average 100B Models and Proprietary Models Qwen3-235B-A22B (Yang et al., 2025a) DeepSeek-R1 (Guo et al., 2025) DeepSeek-R1-0528 (Guo et al., 2025) o1medium (Jaech et al., 2024) o3-minimedium (OpenAI, 2025) o3-minihigh (OpenAI, 2025) Gemini-2.0Flash-Thinking (Google, 2024) Gemini-2.5Flash-Thinking-0520 (Comanici et al., 2025) 235B 671B 671B - - - - - Qwen3-30B-A3B (Yang et al., 2025a) R1-Distill-Qwen-32B (Guo et al., 2025) QwQ-32B (Qwen, 2025) Qwen3-32B (Yang et al., 2025a) SkyWork-OR1-32B (He et al., 2025) BP-Math-32BSFT BP-Math-32BGRPO BP-Math-32BBAPO 10B - 100B Scale Models 30B 32B 32B 32B 32B 32B 32B 32B 10B Models R1-Distill-Qwen-7B (Guo et al., 2025) Light-R1-7B-DS (Wen et al., 2025) AReaL-boba-RL-7B (Fu et al., 2025) AceReason-Nemotron-7B (Chen et al., 2025) SkyWork-OR1-7B (He et al., 2025) BP-Math-7BSFT BP-Math-7BGRPO BP-Math-7BBAPO 7B 7B 7B 7B 7B 7B 7B 7B 85.7 79.8 91.4 83.3 79.6 87.3 73.3 82.3 72.6 79.5 81.4 82.2 84.4 84.6 87.1 54.2 59.1 61.9 69.0 70.2 66.9 69.2 70. 81.5 70.0 87.5 79.0 76.7 86.5 53.5 72.0 61.3 54.9 65.3 72.9 73.3 78.1 78.8 80.0 38.4 44.2 48.3 53.6 54.6 59.0 59.2 62.5 83.6 74.9 89.5 81.2 78.2 86.9 63.4 77.2 63.8 72.4 77.2 77.8 81.3 81.7 83.5 46.3 51.7 55.1 61.3 62.4 62.9 64.2 66. newly released AIME 2025 (AIME, 2025) benchmarks. Our experiments cover range of backbone models, including DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-32B (Guo et al., 2025), and OctoThinker-Llama3.2-3B-Long-Zero (Wang et al., 2025b). In addition, we incorporate two our own supervised fine-tuning (SFT) models, BP-Math-7B and BP-Math-32B, which are derived from Qwen2.5-Math (Yang et al., 2024) through fine-tuning. Implementation details. We leverage GRPO as the basis for BAPO. Both our preliminary and validation experiments are conducted using DeepSeek-R1-Distill-Qwen-7B, with the maximum response length set to 8洧녲, learning rate to 2 106, and temperature to 0.6. For main results on BPMath models, we set the maximum response length to 64洧녲 to align with the SFT setting. To introduce staleness, we adopt multiple strategies, including experience reuse through ppo_epoch (Schulman et al., 2017) and the modern partial rollouts (Fu et al., 2025; Team et al., 2025). For BAPO, we set the target contribution 洧랣0 = 0.4, the movable range 洧녩 = 0.6, 洧녪 = 0.9, 洧녩+ = 1.2, 洧녪+ = 3.0, and the step size 洧1 = 0.05, 洧2 = 0.02. These hyperparameters are not finely tuned, as they already demonstrate strong empirical performance. For evaluation, we report results averaged over 16 rollouts. Baselines. We include variety of commercial and open-source models of different scales as baselines, as shown in Table 1, and report their performance as extracted from prior work. In addition, we compare different training approaches, including SFT and GRPO. 9 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping 5.2. Main Results The main results are shown in Figure 1 and Table 1. Significant performance improvements across models of varying sizes. For strong SFT models, GRPO provides only marginal benefitsfor instance, it improves performance by just 0.2 and 0.7 points on AIME24 and AIME25 with the BP-Math-32B model. In contrast, BAPO delivers substantial gains across models of different scales. Specifically, with the BP-Math-32B model, BAPO outperforms SFT by 2.7 and 1.9 points on AIME24 and AIME25, respectively; with the BP-Math-7B model, it achieves even larger improvements of 3.9 and 3.5 points. SOTA performance over open-source models of comparable sizes and competitive results against proprietary models. Compared to open-source models of similar sizes, our BAPO-trained models achieve state-of-the-art (SOTA) performance. For instance, among 32B models, BP-Math-32BBAPO outperforms Qwen3-32B by 5.7 and 7.1 points on AIME24 and AIME25, respectively, and surpasses SkyWork-OR1-32B by 4.9 and 6.7 points. Among 7B models, BP-Math-7BBAPO also delivers notable 7.9-point improvement over SkyWork-OR1-7B on AIME25. Moreover, BP-Math-32BBAPO even outperforms some larger-scale modelsfor example, it surpasses DeepSeek-R1 by 7.3 and 10.0 points on AIME24 and AIME25, respectivelywhile achieving performance comparable to o3-mini. Notably, even the smaller BP-Math-7BBAPO yields results on par with Gemini-2.0-Flash-Thinking, underscoring the competitiveness of our approach against commercial models. 5.3. Discussion Partial rollout. To speed up rollouts in LLM reinforcement learning, modern AI infrastructures have introduced several techniques, with partial rollout being particularly noteworthy (Fu et al., 2025; Team et al., 2025). In this approach, long trajectories are split into segments: when rollout exceeds fixed token budget, the unfinished portion is stored in replay buffer and resumed in later iterations instead of being regenerated from scratch. While this improves training efficiency, it also introduces off-policy learning, since different parts of the same trajectory may come from multiple outdated policies. We evaluate BAPO under this setting, as shown in Figure 12. Compared to the baseline GRPO, BAPO exhibits greater robustness to such off-policy infrastructures and achieves more stable optimization. Figure 12 Training dynamics with partial rollout. In Results on OctoThinker-Llama3.2-3B-Long-Zero. addition to the DeepSeek-R1-Distill-Qwen, we also conducted experiments on Llama-based models (Wang et al., 2025b). As shown in Table 2 and Figure 13 in Appendix A, our method achieves more competitive results and exhibits greater stability in training dynamics. Table 2 Performance of Llama-based models. Method AIME 2024 AIME 2025 MATH GRPO BAPO 2.5% 5.4% 2.9% 5.8% 58.4% 66.0% 10 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping 6. Related Work Recent landmark models, like OpenAI o1 (Jaech et al., 2024), DeepSeek-R1 (Guo et al., 2025), Gemini 2.5 (Comanici et al., 2025), QwQ (Qwen, 2025), have demonstrated that reinforcement learning can effectively enable long chain-of-thought reasoning in LLMs (Shao et al., 2024; Zhang et al., 2025). Mainstream algorithms include PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024): PPO constrains updates via clipping-based surrogate objective, while GRPO enhances longhorizon reasoning through group-based rewards. Despite the remarkable success of RL for LLMs, ensuring stability and efficiency in optimization remains major challenge (Cui et al., 2025; Yu et al., 2025). Recent studies have sought to better understand the underlying mechanisms of RL and proposed new methods to achieve balance (Cui et al., 2025; Wang et al., 2025a; Yang et al., 2025b; Zheng et al., 2025). For example, DAPO (Yu et al., 2025) introduces techniques such as Clip-Higher and dynamic sampling to raise the performance ceiling; Wang et al. (2025a) explore optimizing only small subset of high-entropy tokens for improved efficiency. He et al. (2025), Cui et al. (2025), and other works (Cheng et al., 2025; Liu et al., 2025; Zheng et al., 2025) systematically investigate how to maintain entropy stability during training, thereby preserving the models exploration ability. For off-policy RL, Roux et al. (2025) and Arnal et al. (2025) introduce asymmetric clipping mechanisms. The most similar to our work is DCPO (Yang et al., 2025b), which adjusts token-level clipping based on token prior probabilities. However, our approach takes holistic optimization perspective: we observe the imbalance in loss contributions and derive the Entropy-Clip Rule for the PPO objective, enabling dynamic control over global clipping bounds. We further validate the effectiveness of our method through larger-scale experiments. 7. Conclusion In this paper, we begin by analyzing the impact of data staleness on model training through both empirical and theoretical studies. We reveal the imbalance between positive and negative samples in RL optimization, and derive as well as empirically validate the Entropy-Clip Rule for PPO-like objectives. Building on these insights, we propose BAPO, which dynamically adjusts the clipping bounds to balance positive and negative samples while preserving the models exploratory capability during training. We conduct extensive experiments across different models and settings to validate our method. We hope our work provides key insights for the LLM RL community."
        },
        {
            "title": "References",
            "content": "AIME. Aime problems and solution, 2025. URL https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. Anthropic. Claude code, 2025. URL https://docs.anthropic.com/en/docs/claude-code. Charles Arnal, Ga칢tan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, and R칠mi Munos. Asymmetric REINFORCE for off-policy reinforcement learning: Balancing positive and negative rewards. CoRR, abs/2506.20520, 2025. doi: 10.48550/ARXIV.2506.20520. URL https://doi. org/10.48550/arXiv.2506.20520. Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, Yichen Feng, Kelin Fu, Bofei Gao, Hongcheng BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Gao, Peizhong Gao, Tong Gao, Xinran Gu, Longyu Guan, Haiqing Guo, Jianhang Guo, Hao Hu, Xiaoru Hao, Tianhong He, Weiran He, Wenyang He, Chao Hong, Yangyang Hu, Zhenxing Hu, Weixiao Huang, Zhiqi Huang, Zihao Huang, Tao Jiang, Zhejun Jiang, Xinyi Jin, Yongsheng Kang, Guokun Lai, Cheng Li, Fang Li, Haoyang Li, Ming Li, Wentao Li, Yanhao Li, Yiwei Li, Zhaowei Li, Zheming Li, Hongzhan Lin, Xiaohan Lin, Zongyu Lin, Chengyin Liu, Chenyu Liu, Hongzhang Liu, Jingyuan Liu, Junqi Liu, Liang Liu, Shaowei Liu, T. Y. Liu, Tianwei Liu, Weizhou Liu, Yangyang Liu, Yibo Liu, Yiping Liu, Yue Liu, Zhengying Liu, Enzhe Lu, Lijun Lu, Shengling Ma, Xinyu Ma, Yingwei Ma, Shaoguang Mao, Jie Mei, Xin Men, Yibo Miao, Siyuan Pan, Yebo Peng, Ruoyu Qin, Bowen Qu, Zeyu Shang, Lidong Shi, Shengyuan Shi, Feifan Song, Jianlin Su, Zhengyuan Su, Xinjie Sun, Flood Sung, Heyi Tang, Jiawen Tao, Qifeng Teng, Chensi Wang, Dinglu Wang, Feng Wang, and Haiming Wang. Kimi K2: open agentic intelligence. CoRR, abs/2507.20534, 2025. doi: 10.48550/ARXIV.2507.20534. URL https://doi.org/10.48550/arXiv.2507.20534. Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. CoRR, abs/2505.16400, 2025. doi: 10.48550/ARXIV.2505.16400. URL https://doi. org/10.48550/arXiv.2505.16400. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. CoRR, abs/2506.14758, 2025. doi: 10.48550/ARXIV.2506.14758. URL https://doi.org/10.48550/arXiv.2506.14758. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit S. Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, NanJiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ila칦 Deutel, Nam Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Ram칠, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Sharon Silver, Ayzaan Wahid, Sergey Brin, Yves Raimond, Klemen Kloboves, Cindy Wang, Nitesh Bharadwaj Gundavarapu, Ilia Shumailov, Bo Wang, Mantas Pajarskas, Joe Heyward, Martin Nikoltchev, Maciej Kula, Hao Zhou, Zachary Garrett, Sushant Kafle, Sercan Arik, Ankita Goel, Mingyao Yang, Jiho Park, Koji Kojima, Parsa Mahmoudieh, Koray Kavukcuoglu, Grace Chen, Doug Fritz, Anton Bulyenov, Sudeshna Roy, Dimitris Paparas, Hadar Shemtov, Bo-Juen Chen, Robin Strudel, David Reitter, Aurko Roy, Andrey Vlasov, Changwan Ryu, Chas Leichner, Haichuan Yang, Zelda Mariet, Denis Vnukov, Tim Sohn, Amy Stuart, Wei Liang, Minmin Chen, Praynaa Rawlani, Christy Koh, JD Co-Reyes, Guangda Lai, Praseem Banzal, Dimitrios Vytiniotis, Jieru Mei, and Mu Cai. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261, 2025. doi: 10.48550/ARXIV.2507.06261. URL https://doi.org/10.48550/arXiv.2507.06261. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models. CoRR, abs/2505.22617, 2025. doi: 10.48550/ARXIV.2505.22617. URL https://doi. org/10.48550/arXiv.2505.22617. Yiwen Ding, Zhiheng Xi, Wei He, Lizhuoyuan Lizhuoyuan, Yitao Zhai, Shi Xiaowei, Xunliang Cai, Tao Gui, Qi Zhang, and Xuanjing Huang. Mitigating tail narrowing in LLM self-improvement In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings via socratic-guided sampling. 12 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 1062710646. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.NAACL-LONG.533. URL https://doi.org/10. 18653/v1/2025.naacl-long.533. Wei Fu, Jiaxuan Gao, Xujie Shen, Chen Zhu, Zhiyu Mei, Chuyi He, Shusheng Xu, Guo Wei, Jun Mei, Jiashu Wang, Tongkai Yang, Binhang Yuan, and Yi Wu. Areal: large-scale asynchronous reinforcement learning system for language reasoning. CoRR, abs/2505.24298, 2025. doi: 10. 48550/ARXIV.2505.24298. URL https://doi.org/10.48550/arXiv.2505.24298. Google. Introducing gemini 2.0: 2024. our new ai model the agentic era, Dehttps://blog.google/technology/google-deepmind/ for cember google-gemini-ai-update-december-2024/. URL 칂aglar G칲l칞ehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. Reinforced self-training (rest) for language modeling. CoRR, abs/2308.08998, 2023. doi: 10.48550/ARXIV.2308.08998. URL https://doi.org/10. 48550/arXiv.2308.08998. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Honghui Ding, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jingchang Chen, Jingyang Yuan, Jinhao Tu, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaichao You, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingxu Zhou, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. doi: 10.1038/s41586-025-09422-z. URL https://doi.org/10.1038/s41586-025-09422-z. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang 13 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Liu, and Yahui Zhou. Skywork open reasoner 1 technical report. CoRR, abs/2505.22312, 2025. doi: 10.48550/ARXIV.2505.22312. URL https://doi.org/10.48550/arXiv.2505.22312. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024. doi: 10.48550/ARXIV.2412.16720. URL https://doi.org/10.48550/arXiv.2412.16720. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. CoRR, abs/2503.20783, 2025. doi: 10.48550/ARXIV.2503.20783. URL https://doi.org/10.48550/arXiv.2503.20783. OpenAI. Openai o3-mini o3-mini-system-card-feb10.pdf. system card, 2025. URL https://cdn.openai.com/ Qwen. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https:// qwenlm.github.io/blog/qwq-32b/. Nicolas Le Roux, Marc G. Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves, Alexandre Fr칠chette, Carolyne Pelletier, Eric Thibodeau-Laufer, S치ndor T칩th, and Sam Work. Tapered offpolicy REINFORCE: stable and efficient reinforcement learning for llms. CoRR, abs/2503.14286, 2025. doi: 10.48550/ARXIV.2503.14286. URL https://doi.org/10.48550/arXiv.2503. 14286. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707. 06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi. org/10.48550/arXiv.2402.03300. Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, R칠mi Munos, Bernardo 츼vila Pires, Michal Valko, Yong Cheng, and Will Dabney. Understanding the performance gap between online and offline alignment algorithms. CoRR, abs/2405.08448, 2024. doi: 10.48550/ARXIV.2405.08448. URL https://doi.org/10.48550/arXiv.2405.08448. 14 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. doi: 10.48550/ARXIV.2501. 12599. URL https://doi.org/10.48550/arXiv.2501.12599. Luong Quoc Trung, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 76017614. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.410. URL https://doi.org/10.18653/v1/2024.acl-long.410. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning. CoRR, abs/2506.01939, 2025a. doi: 10. 48550/ARXIV.2506.01939. URL https://doi.org/10.48550/arXiv.2506.01939. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. CoRR, abs/2506.20512, 2025b. doi: 10.48550/ARXIV.2506.20512. URL https://doi.org/10.48550/arXiv.2506.20512. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, DPO and RL for long COT from scratch and beyond. CoRR, abs/2503.10460, 2025. doi: 10.48550/ARXIV.2503.10460. URL https://doi.org/10.48550/arXiv.2503.10460. Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8:229256, 1992. doi: 10.1007/BF00992696. URL https://doi.org/ 10.1007/BF00992696. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, and Xuanjing Huang. Training large language models for reasoning through reverse curriculum reinforcement learning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=t82Y3fmRtk. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement. CoRR, abs/2409.12122, 2024. doi: 10.48550/ARXIV.2409.12122. URL https: //doi.org/10.48550/arXiv.2409.12122. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025a. doi: 10.48550/ARXIV.2505.09388. URL https://doi.org/10.48550/arXiv.2505.09388. Shihui Yang, Chengfeng Dou, Peidong Guo, Kai Lu, Qiang Ju, Fei Deng, and Rihui Xin. Dcpo: Dynamic clipping policy optimization. arXiv preprint arXiv:2509.02333, 2025b. Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, and Yunjian Xu. Do not let low-probability tokens over-dominate in RL for llms. CoRR, abs/2505.12929, 2025c. doi: 10.48550/ARXIV.2505.12929. URL https://doi.org/10.48550/arXiv.2505.12929. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV. 2503.14476. URL https://doi.org/10.48550/arXiv.2503.14476. Ruifeng Yuan, Chenghao Xiao, Sicong Leng, Jianyu Wang, Long Li, Weiwen Xu, Hou Pong Chan, Deli Zhao, Tingyang Xu, Zhongyu Wei, Hao Zhang, and Yu Rong. Vl-cogito: Progressive curriculum reinforcement learning for advanced multimodal reasoning. CoRR, abs/2507.22607, 2025. doi: 10.48550/ARXIV.2507.22607. URL https://doi.org/10.48550/arXiv.2507.22607. Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. CoRR, abs/2507.18071, 2025. doi: 10.48550/ARXIV.2507.18071. URL https://doi.org/10. 48550/arXiv.2507.18071. Xuekai Zhu, Daixuan Cheng, Dinghuai Zhang, Hengli Li, Kaiyan Zhang, Che Jiang, Youbang Sun, Ermo Hua, Yuxin Zuo, Xingtai Lv, et al. Flowrl: Matching reward distributions for llm reasoning. arXiv preprint arXiv:2509.15207, 2025. BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping A. Performance on OctoThinker-Llama"
        },
        {
            "title": "Appendix",
            "content": "We illustrate the training dynamics on OctoThinker-Llama in Figure 13. Since Llama family models behave badly in RL training, we choose the model after mid-training (Wang et al., 2025b) to show the robustness of BAPO. We can find that BAPO provides consistent and significant improvement in training. For training details, we set the low bound as 0.8-0.9, high bound as 1.2-2.0, and target positive loss contribution as 0.45. Figure 13 Training dynamics of OctoThinker-Llama-3B-Long-Zero. B. Proofs of Equation 6 B.1. Explanations for all variables and expressions All notation used in the following justification, including variables and expressions, is provided with detailed explanations in Table 3. B.2. Preparation: Rewrite the PPO derivatives To facilitate the justification of the propositions below, we rewrite the PPO loss function in the following form: 洧냫PPO = 洧랢洧랚( 洧녽洧노) 洧{洧( 洧녽洧노) < 1 + 洧} 洧냢( 洧녽洧노) log 洧랢洧랚( 洧녽洧노) 洧냢( 洧녽洧노 ) >0 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) positive tokens + 洧랢洧랚( 洧녽洧노) 洧{洧( 洧녽洧노) > 1 洧} 洧냢( 洧녽洧노) log 洧랢洧랚( 洧녽洧노) 洧냢( 洧녽洧노 ) <0 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) negative tokens where 洧랢洧랚( 洧녽洧노) = 洧랢洧랚( 洧녽洧노 洧눛, 洧눜<洧눗) , 洧( 洧녽洧노) = 洧랢洧랚( 洧녽洧노 洧눛, 洧눜<洧눗) 洧랢洧랚rollout ( 洧녽洧노 洧눛, 洧눜<洧눗) , 洧냢( 洧녽洧노) = 洧냢( 洧녽洧노 洧눛, 洧눜<洧눗) . 17 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Table 3 Notation used in justification below. Category Symbol Meaning Variables 洧랢洧랚 洧랢洧랚洧洧녶洧녳洧녳洧녶洧녹洧노 洧눛 洧눜 洧녽洧노 洧랙 洧랢洧랚(洧눛, 洧눜<洧노) 洧랢洧랚洧洧녶洧녳洧녳洧녶洧녹洧노 ( 洧녽洧노 洧눛, 洧눜<洧노) 洧냢(洧눛, 洧눜<洧눗) The policy parameterized by 洧랚 The standard sampling policy Given prompt T-token response generated by 洧랢洧랚 when given 洧눛 The t-th token of Learning rate Probability of generating token under policy 洧랢洧랚 given input 洧눛 and previous tokens 洧눜<洧노 Probability of generating token under standard sampling policy 洧랢洧랚洧洧녶洧녳洧녳洧녶洧녹洧노 given input 洧눛 and previous tokens 洧눜<洧노 The measurement of how much better(or worse) selecting token is compared to the expected value under the current policy, given 洧눛 and 洧눜<洧눗 (洧눛, 洧눜<洧노) The information entropy of policy given 洧눛 and 洧눜<洧노 洧냤洧녶洧녺洧녽洧노洧랢洧랚 ( 洧눛,洧눜<洧노 ) (洧녩( 洧녽洧노), 洧녪( 洧녽洧노)) The expected covariance of 洧녩( 洧녽洧노) and 洧녪( 洧녽洧노) over 洧녽洧노 sampled from the policy 洧랢洧랚, given 洧눛 and 洧눜<洧노 Expressions 洧(洧녩 = 洧녪) Indicator function that equals 1 if 洧녩 = 洧녪 and 0 otherwise 洧녟 (洧랢洧랚 ) (, 洧눛) 洧녤 (洧랢洧랚 ) (洧눛) 洧녾 洧눜,洧눛 洧랚洧녽洧노 ,洧눛 洧냫 (洧랚) The expected cumulative reward obtained by taking token given input 洧눛 and previous tokens under policy 洧랢洧랚 The expected return of the new taking token given input 洧눛 and previous tokens under policy 洧랢洧랚 quantity representing the cumulative weight of sequence 洧눜 given input 洧눛 under policy 洧랢洧랚, reflecting its contribution to the policy taken at the current optimization step The gradient of the policy taken with respect to the logit parameter 洧랚洧녽洧노,洧눛, representing how the policy 洧랢洧랚 should be adjusted for token 洧녽洧노 given input 洧눛 B.3. Proofs of the main Propositions The following derivation is inspired by the proof framework in Cui et al. (2025). While the original work focuses mainly on the basic gradient formulation of naive REINFORCE to provide heuristic explanation, our study advances this approach by deriving the gradient expression specific to the PPO objective. This refinement offers specific, intuitive yet theoretical account of how policy entropy is intrinsically shaped by the interaction between token-level advantages and their sampling 18 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping probabilities. B.3.1. Preclaims Proofs of these three lemmas below are available in Cui et al. (2025). Lemma 1. Let the actor policy 洧랢洧랚 be tabular softmax policy, the difference of information entropy given prompt 洧논 between two consecutive steps 洧녲 and 洧녲 + 1 satisfies (洧랢洧녲+1 洧랚 洧눛, 洧눜<洧눗) (洧랢洧녲 洧랚洧눛, 洧눜<洧눗) Cov 洧녽洧노洧랢洧녲 洧랚 ( 洧눛,洧눜<洧눗 ) (cid:16)log 洧랢洧녲 洧랚( 洧녽洧노), 洧녾洧녲+1 洧눜,洧눛 洧녾洧녲 洧눜,洧눛 (cid:17) . Lemma 2 (Derivative of softmax function). log 洧랢洧랚( 洧녽洧노) 洧랚洧녽 洧노 ,洧눛 = 洧{ 洧녽洧노 = 洧녽 洧노 } 洧랢洧랚( 洧녽 洧노 ) Lemma 3 (Expectation of Advantage function given prompt 洧논). 洧댶洧녽洧노洧랢洧랚 ( 洧논,洧눜<洧눗 ) (cid:2)洧냢洧랢洧랚 ( 洧녽洧노)(cid:3) = 洧댶洧녽洧노洧랢洧랚 ( 洧논,洧눜<洧눗 ) = 洧댶洧녽洧노洧랢洧랚 ( 洧논,洧눜<洧눗 ) = 洧녤 (洧눛) 洧녤 (洧눛) = 0 (cid:2)洧녟洧랢洧랚 ( 洧녽洧노, 洧눛) 洧녤 洧랢洧랚 (洧눛)(cid:3) (cid:2)洧녟( 洧녽洧노, 洧눛)(cid:3) 洧댶洧녽洧노洧랢洧랚 ( 洧논,洧눜<洧눗 ) (cid:2)洧녤 (洧눛)(cid:3) B.3.2. Principle Propositions Proposition 1: Assume the actor policy 洧랢洧랚 follows tabular softmax policy and is optimized via the PPO objective, the difference of 洧녾 洧눜,洧눛 between two consecutive steps and k+1 satisfies where 洧녾洧녲+1 洧눜,洧눛 洧녾洧녲 洧눜,洧눛 = 洧랙 洧랢洧랚( 洧녽洧노) [ 洧냢( 洧녽洧노) X( 洧녽洧노) + 洧냤], X( 洧녽洧노) = 1, if 洧냢( 洧녽洧노) > 0 & 洧( 洧녽洧노) < 1 + 洧랬 or 洧냢( 洧녽洧노) < 0 & 洧( 洧녽洧노) > 1 洧랬 0, otherwise and 洧냤 includes all clauses irrelevant to 洧녽洧노. It is worth noting that X(洧눜洧눗) = 0 if and only if 洧눜洧눗 is clipped. Proof. In tabular softmax policy, each trajectory-prompt pair ( 洧눜, 洧눛) is associated with an individual logit parameter 洧녾 洧눜,洧눛 = 洧랚洧녽洧노,洧눛. Through gradient backtracking, 洧녾 洧눜,洧눛 is updated via 洧녾洧녲+1 洧눜,洧눛 + 洧랙 洧랚洧녽洧노 ,洧눛 洧냫 (洧랚). According to the loss function of PPO, we have 洧눜,洧눛 = 洧녾洧녲 19 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping 洧녾洧녲+1 洧눜,洧눛 洧녾洧녲 洧눜,洧눛 = 洧랙 洧랚洧녽洧노 ,洧눛 = 洧랙 洧댶洧녽 洧냫洧녞洧녞洧녝(洧랚) 洧노 洧랢洧랚 ( 洧눛,洧눜<洧눗 ) 洧노 ) >0 洧냢( 洧녽 + 洧랙 洧댶洧녽 洧노 洧랢洧랚 ( 洧눛,洧눜<洧눗 ) 洧노 ) <0 洧냢( 洧녽 (cid:2)洧{洧( 洧녽 洧노 ) < 1 + 洧} 洧랚洧녽洧노 ,洧눛 (cid:2)洧{洧( 洧녽 洧노 ) > 1 洧} 洧랚洧녽洧노 ,洧눛 log 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 )(cid:3) log 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 )(cid:3) = 洧랙 洧댶洧녽 (cid:124) 洧노 洧랢洧랚 ( 洧눛,洧눜<洧눗 ) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 洧노 )(cid:3) log 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:2)洧랚洧녽洧노 ,洧눛 (cid:123)(cid:122) 1 (cid:2)洧{洧( 洧녽 洧랙 洧댶洧녽 洧노 洧랢洧랚 ( 洧눛,洧눜<洧눗 ) 洧노 ) >0 洧냢( 洧녽 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 洧노 ) > 1 + 洧} 洧랚洧녽洧노 ,洧눛 log 洧랢洧랚( 洧녽 洧노 )(cid:3) 洧노 ) 洧냢( 洧녽 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) 2 洧노 ) < 1 洧} 洧랚洧녽洧노 .洧눛 (cid:2)洧{洧( 洧녽 洧랙 洧댶洧녽 洧노 洧랢洧랚 ( 洧눛,洧눜<洧눗 ) 洧노 ) < 洧냢( 洧녽 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) log 洧랢洧랚( 洧녽 洧노 )(cid:3) 洧노 ) 洧냢( 洧녽 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:124) (cid:125) (cid:125) (cid:123)(cid:122) 3 = 1 ( 2 + 3 ) (8) We first perform the derivation on the term marked as 1 : 1 = 洧랙 洧댶洧녽 洧노 洧랢洧랚 ( 洧눛,洧눜<洧눗 ) (cid:21) 洧냢( 洧녽 洧노 ) (cid:20) log 洧랢洧랚( 洧녽 洧노 ) 洧랚洧녽洧노,洧눛 洧노 ) (洧{ 洧녽 (cid:2)洧랢洧랚( 洧녽 洧노 = 洧녽洧노} 洧랢洧랚( 洧녽洧노)) 洧냢( 洧녽 洧노 )(cid:3) Lemma 2 = 洧랙 洧녽 洧노 = 洧랙 洧랢洧랚( 洧녽洧노) = 洧랙 洧랢洧랚( 洧녽洧노) (1 洧랢洧랚( 洧녽洧노)) 洧냢( 洧녽洧노) 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 ) 洧녽 洧노 洧녽洧노 洧냢( 洧녽洧노) 洧녽 洧노 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 ) Lemma 3 = 洧랙 洧랢洧랚( 洧녽洧노) [ 洧냢( 洧녽洧노) 0] = 洧랙 洧랢洧랚( 洧녽洧노) 洧냢( 洧녽洧노) To keep the presentation concise, we provide only the resulting derivations of Term 2 and 3 , as the detailed steps follow similarly to those for Term 1 . 2 + 3 = 洧랙 洧랢洧랚( 洧녽洧노) 洧냢( 洧녽洧노) (1 X( 洧녽洧노)) 洧랙 洧랢洧랚( 洧녽洧노) 洧랙 洧랢洧랚( 洧녽洧노) 洧냢( 洧녽 洧노 ) >0 洧냢( 洧녽 洧노 ) <0 (cid:2)洧{洧( 洧녽 洧노 ) > 1 + 洧} 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 )(cid:3) (cid:2)洧{洧( 洧녽 洧노 ) < 1 洧} 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 )(cid:3) 20 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping By substituting the results of the above derivation into Clause (8), we observe that: (8) = 1 ( 2 + 3 ) (cid:110) 洧냢( 洧녽洧노) X( 洧녽洧노) = 洧랙 洧랢洧랚( 洧녽洧노) + (cid:2)洧{洧( 洧녽 洧노 ) > 1 + 洧} 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 )(cid:3) 洧냢( 洧녽 洧노 ) >0 洧냢( 洧녽 洧노 ) <0 + (cid:2)洧{洧( 洧녽 洧노 ) < 1 洧} 洧랢洧랚( 洧녽 洧노 ) 洧냢( 洧녽 洧노 )(cid:3) (cid:111) By grouping all elements unrelated to 洧녽洧노 into 洧냤, we are able to successfully establish our proposition. Building on Proposition 1, we establish the relationship between policy entropy and the covariance of specific tokens, which is stated as Proposition 2 below. Proposition 2 (Equation 6): Let the actor policy 洧랢洧랚 be tabular softmax policy, and 洧랢洧랚 is updated via PPO objective, the difference of information entropy given prompt 洧논 and trajectory part 洧녽<洧노 between two consecutive steps and k+1 satisfies (洧랢洧녲+1 洧랚 洧눛, 洧눜<洧눗) (洧랢洧녲 洧랚洧눛, 洧눜<洧눗) 洧랙 Cov洧녽洧노洧랢洧녲 洧랚 ( 洧눛,洧눜<洧눗 ) (cid:16)log 洧랢洧녲 洧랚( 洧녽洧노), 洧냢( 洧녽洧노) X( 洧녽洧노) + 洧냤 (cid:17) . Proof. Leveraging the conclusions of Lemma 1 and Proposition 1, we find that, under policy optimization and iteration via the PPO algorithm, the following relationship is satisfied: 洧녾洧녲+1 洧눜,洧눛 洧녾洧녲 洧눜,洧눛 = 洧랙 ( 洧냢( 洧녽洧노) X( 洧녽洧노) + 洧냤). Applying this into Lemma 1, we have 洧눛, 洧눜<洧눗) (洧랢洧녲 洧랚洧눛, 洧눜<洧눗) 洧랙 Cov洧녽洧노洧랢洧녲 洧랚 ( 洧눛,洧눜<洧눗 ) (cid:16)log 洧랢洧녲 洧랚( 洧녽洧노), 洧냢( 洧녽洧노) X( 洧녽洧노) + 洧냤 (cid:17) . (洧랢洧녲+1 洧랚 B.4. Analysis B.4.1. Direct Analysis: Why Varying 洧 Alters Entropy? We begin by examining the covariance of the clipped token, denoted as 洧띺. Based on the observation stated above, the contribution of 洧띺 to the entropy can be expressed as: 洧랙 洧랢洧녲 洧랚(洧띺) Cov(cid:16)log 洧랢洧녲 洧랚(洧띺), 洧냤 (cid:17) = 0, which indicates that only the retained tokens contribute to the overall entropy. In other words, we manipulate the number of tokens that can contribute to the entropy by altering the parameter 洧. 21 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping B.4.2. Advanced Analysis : Which Type of Tokens Matter Most for Entropy? To understand how individual tokens contribute to the overall entropy, we first revisit the Proposition B.3.2 established above. In this section, we provide more precise definition of tokens with low/high probabilities and advantages. It should be noted that in the analysis experiment (Figure 5), we adopt the naive REINFORCE algorithm without clipping. Consequently, tokens with high or low advantages are defined according to the sign of their advantage values, i.e., > 0 for high advantage and < 0 for low advantage. (洧랢洧녲+1 洧랚 洧눛, 洧눜<洧눗) (洧랢洧녲 洧랚洧눛, 洧눜<洧눗) 洧랙 Cov洧녽洧노洧랢洧녲 洧랚 ( 洧눛,洧눜<洧눗 ) (cid:16)log 洧랢洧녲 洧랚( 洧녽洧노), 洧냢( 洧녽洧노) X( 洧녽洧노) + 洧냤 (cid:17) = 洧랙 洧녢 洧녷=1 洧랚( 洧녽洧녷洧눛, 洧눜<洧눗) (cid:0) log 洧랢洧녲 洧랢洧녲 洧랚( 洧녽洧녷) 洧댶洧녽洧녰洧랢洧녲 洧랚 ( 洧눛,洧눜<洧눗 ) [log 洧랢洧녲 洧랚( 洧녽洧녰)](cid:1) (cid:0)洧냢( 洧녽洧녷) X( 洧녽洧녷) 洧댶洧녽洧녰洧랢洧녲 洧랚 ( 洧눛,洧눜<洧눗 ) [ 洧냢( 洧녽洧녰) X( 洧녽洧녰)](cid:1) . where is the size of the dictionary. For convenience, we denote 洧댶洧녽洧녰 as 洧댶洧녽洧녰洧랢洧녲 洧랚 ( 洧눛,洧눜<洧눗 ) . As only retained tokens contribute to the entropy, we focus only on tokens that are not clipped. We begin by making the following simplification: 洧댶洧녽洧녰 ( 洧냢( 洧녽洧녰) X( 洧녽洧녰)) = 洧댶洧녽clipped ( 洧냢( 洧녽洧녰) 0) + 洧댶洧녽retained ( 洧냢( 洧녽洧녰) 1) = 洧댶洧녽retained ( 洧냢( 洧녽洧녰)) . So for selected token 洧녽洧, its contribution to the overall entropy can be expressed as: 洧랙 洧랢洧랚( 洧녽洧) (log 洧랢洧랚( 洧녽洧) 洧댶洧녽洧녰 (log 洧랢洧랚( 洧녽洧녰))) ( 洧냢( 洧녽洧) 洧댶洧녽retained 洧냢( 洧녽retained)). Next, we analyze how different types of tokens contribute to the overall entropy. To avoid ambiguity, we first give strict definitions that distinguish between tokens with high/low probabilities and tokens with high/low advantages. Definition 1. For token 洧녽洧, we classify it as follows: High advantage: if 洧냢( 洧녽洧) > 洧댶洧녽retained 洧냢( 洧녽retained) Otherwise, it is called low advantage. High probability: if 洧랢洧랚( 洧녽洧) > exp(cid:0)洧댶洧녽洧녰 (log 洧랢洧랚( 洧녽洧녰)))(cid:1) Otherwise, it is called low probability. Secondly, we present two propositions that directly follow from the above definitions. Proposition 3. For token 洧녽洧, we have 洧냢( 洧녽洧) 洧댶洧녽retained 洧냢( 洧녽retained) > 0, if 洧녽洧 is high-advantage token, < 0, if 洧녽洧 is low-advantage token. 22 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Proposition 4. For token 洧녽洧, we have 洧랢洧랚( 洧녽洧) (log 洧랢洧랚( 洧녽洧) 洧댶洧녽洧녰 (log 洧랢洧랚( 洧녽洧녰))) > 0, if 洧녽洧 is high-probability token, < 0, if 洧녽洧 is low-probability token. Proof. Let us denote 洧냤 = 洧댶洧녽洧녰 (log 洧랢洧랚( 洧녽洧녰))), which is independent of 洧녽洧, and let 洧논 = 洧랢洧랚( 洧녽洧). As 洧랢洧랚( 洧녽) < 1 for every 洧녽, 洧냤 < 0. Consider the function 洧녭 (洧논) = 洧논 (log(洧논) 洧냤). Figure 14 illustrates the behavior of this function. Figure 14 Graph of the function 洧녭 (洧논) = 洧논 (log 洧논 洧냤). The proposition follows directly from the properties of 洧녭 (洧논) as observed in the figure. Due to the propositions given above, we have the table below: 풊H ( 洧녽洧) 洧랙 洧랢洧랚( 洧녽洧) (log 洧랢洧랚( 洧녽洧) 洧댶洧녽洧녰 (log 洧랢洧랚( 洧녽洧녰))) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:123)(cid:122) 4 ( 洧냢( 洧녽洧) 洧댶洧녽retained (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:124) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 洧냢( 洧녽retained) (cid:125) Table 4 Influence of token characteristics on 풊H ( 洧녽洧). The prob denotes the probability 洧랢洧랚( 洧녽洧), and the adv represents the advantage 洧냢( 洧녽洧). Token properties 4 high prob, high adv > 0 > 0 > 0 < 0 high prob, low adv < 0 > 0 low prob, high adv < 0 < 0 low prob, low adv 洧뛂H (洧눜洧눖) (洧럎 4 5 ) < 0 > 0 > 0 < 0 It should be noted that token 洧녽洧 decreases the entropy if 풊H ( 洧녽洧) < 0, and increases it otherwise. 23 BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping Therefore, we observe that tokens which are positive with high probabilities and high advantages, or negative with low probabilities and low advantages, contribute to reduction in the overall entropy. Conversely, positive tokens with high probabilities but low advantages, and negative tokens with high probabilities but low advantages, contribute to an increase in the overall entropy. This observation justifies the statement made in the main part of the thesis."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Innovation Institute",
        "Shanghai Qiji Zhifeng Co., Ltd."
    ]
}