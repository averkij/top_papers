{
    "paper_title": "GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training",
    "authors": [
        "Tong Wei",
        "Yijun Yang",
        "Junliang Xing",
        "Yuanchun Shi",
        "Zongqing Lu",
        "Deheng Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to a phenomenon we termed thought collapse, characterized by a rapid loss of diversity in the agent's thoughts, state-irrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agent's reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 2 5 8 0 . 3 0 5 2 : r GTR: Guided Thought Reinforcement Prevents Thought Collapse in RL-based VLM Agent Training Tong Wei1 Yijun Yang2 Junliang Xing1 Yuanchun Shi1 Zongqing Lu3 Deheng Ye2 1Tsinghua University 2Tencent wt22@mails.tsinghua.edu.cn, yijun.steven.yang@gmail.com Corresponding author 3Peking University Figure 1. Zoom in for more details. Illustration of thought collapse occurring when training VLM agent to solve 24 points through RL. With RL training, the thoughts generated by the VLM agent quickly lose their diversity, turn out to be incorrect, and lead to invalid actions and negative rewards. For example, checkpoints ➌ and ➍ erroneously predict the same thought (I should append 10 to the current formula) and action 10 even in the face of different environment states, catastrophically degrading the agents decision-making capabilities and RLs sample efficiency. We propose Guided Thought Reinforcement (GTR) to prevent this problem."
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable outcome rewards (RLVR) has effectively scaled up chain-of-thought (CoT) reasoning in large language models (LLMs). Yet, its efficacy in training vision-language model (VLM) agents for goal-directed action reasoning in visual environments is less established. This work investigates this problem through extensive experiments on complex card games, such as 24 points, and embodied tasks from ALFWorld. We find that when rewards are based solely on action outcomes, RL fails to incentivize CoT reasoning in VLMs, instead leading to phenomenon we termed thought collapse, characterized by rapid loss of diversity in the agents thoughts, stateirrelevant and incomplete reasoning, and subsequent invalid actions, resulting in negative rewards. To counteract thought collapse, we highlight the necessity of process guidance and propose an automated corrector that evaluates and refines the agents reasoning at each RL step. This simple and scalable GTR (Guided Thought Reinforcement) framework trains reasoning and action simultaneously without the need for dense, per-step human labeling. Our experiments demonstrate that GTR significantly enhances the performance and generalization of the LLaVA-7b model across various visual environments, achieving 3-5 times higher task success rates compared to SoTA models with notably smaller model sizes. 1. Introduction The rapid evolution of large language models (LLMs) and vision-language models (VLMs) has significantly enhanced 1 machines ability to comprehend general text and images. Through comprehensive reinforcement learning (RL) training pipeline, these models can be improved based on evaluations (i.e., rewards) of their outcomes, thus aligning with human values, emerging reasoning capabilities via long chainof-thought (CoT), and achieving higher success rates across various tasks. Yet, few studies have explored how to train VLM agents in dynamic visual environments to infer sequence of correct actions based on their perceived information and ultimately accomplish specific goals. Although prior work [61] has demonstrated the feasibility of RL with verifiable outcome rewards (RLVR) for fine-tuning VLM agents to solve multi-step decision-making tasks, their progress remains limited in environments involving longer episodes, larger state spaces, and more substantial reasoning requirements. In experiments carried out on representative tasks such as the 24 points card game [61] and the embodied environment ALFWorld [38], we identified the bottleneck that restricts further emergence of the models decision-making and reasoning capabilities, which we refer to as thought collapse. In RL training, rewards are entirely derived from the final action, without considering the intermediate reasoning thoughts (see Fig. 1 for more details). When facing challenging and complex tasks, the guiding effect of action rewards is primarily weakened. The agents CoT process rapidly loses diversity, leading to incorrect, incoherent, and rigid thoughts even with different visual and textual inputs, ultimately leading to erroneous actions and negative rewards. Although the agent continues to generate reasoning thoughts, it has already lost the ability to think, hindering the emergence of its full potential, as illustrated by the checkpoints ➌ and ➍ in Fig. 1. In this paper, we highlight that process guidance is critical in mitigating thought collapse during the RLVR training of VLM agents. We propose Guided Thought Reinforcement (GTR), simple and scalable framework that boosts the decision-making capabilities of agents during RL training by combining automatic thought correction and the RL-based optimization of both the agents thoughts and actions. Compared to conventional approaches such as training process reward model [20, 44] or employing external verifier rewards [10, 13, 52, 59, 62], our framework does not rely on meticulous human expert annotations or additional training, but provides more informative process supervision while preserving the flexibility of RLVR, therefore resulting in more efficient and effective solution for training versatile VLM agents in variety of visual environments. Specifically, as shown in Fig. 3, we design plug-andplay VLM corrector model built upon any off-the-shelf VLM to evaluate and refine the agents thoughts at each RL training step, thus automating the correction of collapsed thought trajectories. Inspired by previous works that integrate guidance into RL training [9, 18, 35], our GTR framework allows the agent to perform SFT thought-cloning alongside PPO updates, ensuring both the rationality of the reasoning process and the correctness of the final actions. Furthermore, we address the issue of output format degradation with format rewards and repetition penalties, enhance the accuracy and coherence of thought correction through appropriate tool usage, and mitigate distribution shift in the thought-cloning process by prevalent imitation learning method Dataset Aggregation (DAgger) [33]. The integration of thinking process guidance allows the VLM agent to generate structured and reliable reasoning and action trajectory, leading to more transparent and interpretable decision-making in complex and challenging visual environments. Empirically, we apply GTR to train an LLaVA-7b model [2123]. On the highly challenging card games such as 24 points, GTR achieved over 300% task success rate and significantly higher returns compared to the state-of-the-art (SoTA) methods (including the agents driven by Qwen-VL2.5, Gemini, and GPT-4o), demonstrating that combining thought guidance with RL unleashes the decision-making potential of VLM agents more effectively. Additionally, in experiments on the embodied environment ALFWorld, GTR exhibits better success rate and sample efficiency, proving the generality of our framework across diverse tasks. 2. Related Works 2.1. LLMs/VLMs as Decision-making Agents Recent advancement in foundation models has expanded their applications beyond text and image generation to complex decision-making tasks, focusing on their ability to reason, plan, and interact with dynamic environments [19, 51, 55, 56]. Many previous works have leveraged prompting techniques to activate decision-making capabilities from frozen LLMs [2, 16, 29, 37, 46, 4850, 57, 58]. Furthermore, agents with VLMs as backbones can perform reasoning based on visual inputs, either by translating their observations to text descriptions or aligning their embeddings with LLMs [2, 3, 7, 11, 17, 25, 40, 54, 63]. However, these approaches either do not involve model adjustments or are limited to pretraining additional layers on fixed datasets, making them unable to adapt to dynamic environments through interaction. In contrast, we explore reinforcement learning to finetune the entire VLM, enhancing the models ability to reason and make decisions by interacting with the environment through open-ended text. 2.2. RL Training for LLMs/VLMs Reinforcement learning has been proven to be an effective means of incentivizing the emergence of capabilities in LLMs and VLMs. Some studies focus on RL from human feedback (RLHF), which leverages human feedback datasets to learn reward model and then conducts reinforcement 2 learning [1, 26, 28, 39, 41, 42, 65]. Another common approach involves using human preference data to align the model with humans by modifying the reward function of the RL algorithm [4, 31, 32, 64]. The distinction of our study lies in conducting RL finetuning directly based on rewards provided by the environment. Recently, methods utilizing long Chain-of-Thought and inference-time scaling have demonstrated superior slowthinking capabilities [12, 27]. Techniques such as process reward models [20, 44, 47], searching algorithms [8, 43, 53], and GRPO [36] have achieved breakthroughs in tasks such as solving mathematical problems. In contrast, our work focuses on fast-thinking decision-making for the entire action sequences, aiming to achieve specific goals in interactive environments. Additionally, our approach incorporates multimodal information, integrating vision-language reasoning that expands its applicability to broader range of tasks. Our research is closely related to RL4VLM [61], which proposes method for directly finetuning VLMs using RL. However, as we discuss later, while this approach significantly improves more straightforward tasks, its performance gains are limited in more complex environments. 2.3. Process Guidance Approaches Extensive studies on deep-thinking LLMs in mathematical reasoning have affirmed the contribution of process supervision to enhancing the logical coherence of model outputs. Approaches can be broadly categorized into three types. The first involves training Process Reward Model (PRM) to assess the reasoning process [20, 44], but this method requires costly human annotations to obtain high-quality data. The second approach uses Monte Carlo estimation [5, 24, 47] or credit assignment techniques [6, 45, 60] to infer the quality of thoughts from outcomes. While effective for deepthinking tasks with long intermediate steps, this method is less suitable for the sequential decision-making problems of agents with single-step reasoning. Lastly, methods such as LLM-as-a-judge [10, 52, 62] or length-based rewards [13, 59] can directly evaluate reasoning quality. However, in complex tasks, the numerical rewards provided by these methods often lack sufficient information to guide RL training effectively. 3. Thought Collapse Unlike purely text-based LLM agents, due to the integration of multimodal information and the increased complexity of the decision-making process, training VLM agents in interactive visual environments using RL raises more challenging issues. Previous research has attempted to establish an algorithmic framework that demonstrates the potential of RL training to unleash the decision-making capabilities of VLMs in simple tasks, also highlighting the crucial role of Chain-of-Thought (CoT) reasoning [61]. However, in some Figure 2. Thought collapse persists for larger model scales and training budgets. We train LLaVA 7b and 13b models for 30k RL steps but still observe the degraded performance. complex environments such as the 24-point game and the embodied household tasks from ALFWorld, RL training has shown limited improvement, with no significant emergence performance observed. Our experiments may identify the root of this problem: the agents thought process often becomes irrational or templated during RL training, significantly impairing its decision-making abilities. As illustrated in Fig. 1, the agent fails to improve task success rates or episode returns. Instead, it generates fragmented thoughts and degenerates into strategy of producing similar outputs for different states. Although the agent continues to output thoughts and action decisions at this stage, it has clearly lost its ability to engage in meaningful reasoning and decision-making. We term this catastrophic phenomenon as thought collapse. We first investigated whether these observations stemmed from insufficient base model capabilities or training budgets. To this end, we conducted RL training on two model scales: LLaVA-v1.6-mistral-7B and LLaVA-v1.6-vicuna-13B, and extended the training steps from 15k to 30k. The results in Fig. 2 indicate that models of different scales exhibit similar trends of thought collapse, and extending training duration does not mitigate the issue. Thus, we assume that thought collapse arises from RL training itself, in which rewards are determined entirely by the final actions generated by the agent. As result, the thought process - longer and more fundamental than action output - remains unevaluated and unsupervised. This issue becomes particularly pronounced in tasks with longer episodes, larger state spaces, and greater complexity, ultimately causing the training process to deviate from its intended reasoning trajectory due to accumulated errors. Therefore, we suggest that process guidance can be pivotal in preventing thought collapse during the RL training of VLM agents, and propose novel method, Guided Thought Reinforcement (GTR), built upon the existing RL framework to counteract this problem. Figure 3. Overview of the GTR framework. We modify the RL finetuning (orange region) of VLM agents by introducing automated thought correction (green region) as guidance, leveraging an off-the-shelf VLM model as corrector (purple region). GTR performs SFT updating for the agents thought tokens and PPO updating for its action tokens, thereby training thoughts and actions simultaneously. 4. Guided Thought Reinforcement 4.1. RL Training of VLM Agents Our backbone RL algorithm adopts the well-established framework that uses PPO to finetune VLMs for sequential decision-making [34, 61]. The post-processing of the policy function extracts the keyword action : from the VLMs text outputs as the action. If the output text does not contain this keyword, the agent performs random exploration, selecting randomly from set of all legal actions. Formally, given the VLMs output vout and the set of legal actions A, the post-processing function is defined as: (vout) = (cid:40) a, Unif(A), otherwise if action : vout (1) The action probability required for policy gradient is calculated from the generation probabilities of each token in the output text. Additionally, scaling factor is employed to balance the longer length of CoT outputs compared to action outputs. If πθ denotes the policy, st and at represent the state and action, vin as input tokens, vtht represent CoT reasoning tokens and action tokens respectively, this calculation is shown as: and vact =λ log πθ(vtht log πθ(atst, vin ) ot, vin p(ot, vin p(ot, vin (cid:88) =λ log ) + log πθ(vact , vtht , vtht [:i]) [:i1]) ot, vin ) p(ot, vin p(ot, vin (cid:88) log + , vact [:i]) , vact [:i1]) . (2) Based on the action log probability and the reward feedback from the environment, the agent performs PPO updates to adjust token generation probabilities and discover the optimal policy while interacting with the environment. 4 Figure 4. Comparison among different process guidance methods in the 24 points card game. Trivial numerical rewards provided by the VLM judge and rule-based evaluation cannot incentivize the agents reasoning thoughts and higher success rate. 4.2. Process Guidance from VLM Corrector Various approaches that introduce process guidance to model training have been explored. Process Reward Models (PRMs) represent traditional and widely used method for process supervision, particularly in mathematical reasoning tasks with LLMs [20, 44]. However, high-quality vision-language data annotation is extremely expensive and time-consuming. Moreover, learning with static datasets cannot cover vast variety of decision-making scenarios, often leading to biased policies when deployed to dynamic environments. An intuitive solution to the above challenges is the VLM-as-a-judge approach, which induces VLM to score the agents outputs [10, 52, 62]. However, this method does not demonstrate reasonable performance (see Fig. 4). While episodic returns increase to some extent, task success rates fail to improve. This is because using naive numerical scores does not provide sufficient and accurate guidance for effective RL training, especially given the strong reward-hacking capabilities of large models. In challenging tasks where the models baseline ability is weak, the lack of positive incentives can cause the agent to fall into passive exploration, further hindering performance. Similar issues also arise with methods like length-based rewards [59], which aim to encourage longer thoughts. We find that the guidance provided by these approaches is not enough for the emergence of reasoning capabilities. These limitations call for simple, scalable, and informative process guidance framework, and we demonstrate the corrector model as promising solution. Our GTR framework leverages an external VLM as corrector model, which first evaluates the agents thought at each step for visual recognition accuracy and reasoning correctness, then refines the original thought. When the corrector identifies inconsistencies or errors, the model performs corrections based on the agents outputs. To incorporate corrected thoughts into the agent while preserving scalability, we draw inspiration from the recent work [15] and add simple SFT loss over the thought tokens vtht to the PPO loss, aligning the agents reasoning with the corrected thought trajectories, as shown in Fig 3. Formally, if we denote the agents thought output as th and action as a, given agent model πθ and corrector model πcorr, the objective of GTR can be represented as: min θ s,(th,a)πθ [LPPO(s, a) + LSFT(s, πcorr(s, th))] , (3) where LPPO(s, a) = min (cid:18) πθ(as) πθk (as) Aπθk (s, a), (cid:19) , 1 + c, 1 Aπθk (s, a) clip (cid:18) πθ(as) πθk (as) (cid:88) (cid:19) , (4) LSFT(s, th) = log (thts, th<t; θ). The πcorr eliminates the need for careful human annotations and provides more informative and direct guidance than numerical scores. Unlike behavior cloning, the correction process does not require an expert-level external model to obtain high-quality reference trajectories, which is verified by our empirical experiments in Table 1: our model substantially surpasses the performance of the corrector model (GPT-4o+Tool). 4.3. Mitigate Distribution Shift in Thought Cloning Research such as TD3+BC [9], which also integrates supervised signals with reinforcement learning, typically focuses on the off-policy or offline RL algorithms. However, we encountered distribution shift issue when incorporating thought cloning into the PPO training of VLM agents. As the agents policy iteratively updates, the PPO algorithm Algorithm 1 Training Procedure of the Proposed GTR 1: Input: Environment env, prompt construction function h, post-processing function , agent model πθ0, corrector model πcorr, On-policy data buffer Thought cloning dataset 2: Input: Replay buffer size B, update epoch 3: 4: for = 0 to 1 do 5: 6: 7: 8: 9: st = env.reset() while < do vin = h(st) = (vtht vout , vact at = (vout ) Compute log πθ(atot, vin ˆvtht = πcorr(ot, vtht ) rt, ot+1 = env.step(at) (st, at, rt, vout (st, ˆvtht ) ) with Eqn. 2 Obtain prompt ) = πθk (ot, vin ) Obtain action with Eqn. 1 , log πθ(atot, vin )) Thought correction Sample mini-batch from B, from Compute LPPO with Compute LSFT with θk+1 = arg minθ(LPPO + LSFT) Eqn. 4 Eqn. 5 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: Output: πθK discards previous data and resamples in each round. Performing thought cloning on this non-i.i.d dataset can lead to catastrophic forgetting or error accumulation. To address this, we adopt an interactive imitation learning algorithm, Dataset Aggregation (DAgger) [33], aggregating all historical corrections and samples from them. This approach has been proven to converge to the expert policy, in this case, the outputs of the corrector model. If represents the PPO data buffer and denotes the DAgger thought dataset, the Eq. 3 can be rewritten as below. min θ (s,a)B LPPO(s, a) + LSFT(s, πcorr(s, th)). (5) (s,th)D To summarize, we conclude our method in Algorithm 1. 4.4. Improving Data Quality We observe that RL training, which requires the model to explore and discover better outputs, also introduces the risk of generating poor outputs. During the RL finetuning of VLMs, it is common for the model to produce an invalid format or exhibit excessive repetition, which has also been noted in other recent studies [12, 13]. Therefore, we incorporate repetition penalty during the models response generation process and explicitly integrate format judgment into the corrector model so that answers with valid format receive format reward at each step. These measures significantly improve the stability of the models output format. 5 While general-purpose VLM correction models possess extensive general knowledge and reasoning capabilities, they may lack task-specific expertise. To this end, we leverage the function-calling ability of large models, enabling the corrector model to access specific information or function modules during the evaluation and correction process. This further enhances the accuracy and credibility of corrections. For instance, in the 24 points game, the corrector may invoke piece of Python code that calculates possible equations that evaluate to 24, given the models identification of card ranks in the picture. The introduction of corrector models also enables data sampling control within our framework. By referencing the correctors judgments, we can truncate episodes when the agent enters unsolvable or meaningless states, thereby improving the data quality in the buffer and assisting RL training efficiency. In the 24-point game, where each action is irreversible, the episode is ended if the four cards cannot accomplish 24 or if the agents historical outputs eliminate the possibility of finishing the task. In ALFWorld, we apply truncation when the agents action sequence becomes excessively long or when it continuously repeats the same ineffective action. 5. Experiments 5.1. Experimental Setup Environments We mainly base our experiments on the Points24 task within the gym cards environment [61], which requires the model to perform fine-grained visual recognition of poker cards and then engage in language reasoning. Tasks other than Points24 require fewer recognitions, significantly smaller state and action spaces, and shorter action sequences (typically solvable within five steps). As result, outcome rewards are sufficient to guide RL training and thought collapse rarely occurs. In contrast, the correct action sequence of Points24 can exceed 10 steps, and the combination of long formula and larger action space creates much broader decision-making space. Previous methods have not achieved satisfactory results. Therefore, our experiments focus primarily on this complex problem. At each state st of Points24, the agent observes an image of four poker cards and the current formula under them. For each step, the agent can choose number in [1, 10] or an operator in {+, , , /, =} to append to the current formula. Only numbers appearing in the cards and operators are legal actions, with J, and all treated as 10. The goal is to obtain formula that equals 24 using all the cards. The agent gets -1 as reward for illegal actions and incorrect formulas and gets 10 as reward when forming correct formula. To validate the generality of thought collapse and the GTR framework, we also evaluate performance on the ALFWorld benchmark, multimodal simulation platform featuring various embodied household tasks [38]. These tasks are divided into six categories: Pick & Place, Clean & Place, Heat & Place, Cool & Place, Look in Light, and Pick Two Objects & Place. Given the current visual observation, the agent needs to determine the following action following predefined instruction, such as go to cabinet 1, open drawer 1 or cool apple 1 with fridge 1. The tasks involve navigation and interaction with objects in the environment, presenting significant challenges in visual recognition, long-term planning, and commonsense reasoning. Notably, ALFWorld provides visual and text observations of the environment, and previous works have used both as input, reducing the difficulty of visual recognition. We observe with various models that this setting often leads the agent to rely heavily on text descriptions while neglecting visual input. Therefore, to evaluate the comprehensive multimodal decision-making capability of agents and to better simulate real-world scenarios, we removed the text environment description in our experiments. We also incorporate historical actions as part of the input, making it more suitable to assess the sequential decision-making capabilities of agents. Baselines Our primary baseline is RL4VLM [61], which directly uses environmental rewards for PPO training. It is the first framework to finetune VLM agents with RL and represents the SoTA method. Accordingly, we consider the method that performs only thought cloning, without RL (SFT-only). When evaluating final performance, we also include the LLaVA-mistral-7b base model and commercial API-based models. For Alfworld, the challenging environment with navigation and household tasks, we collect an expert dataset using script policy and fine-tune several SoTA VLM models based on this dataset, using them as baselines. Training Details The GTR framework leverages corrector model with established capabilities for automating thought correction. In our experiment, we select the GPT4o model for this role. Consistent with previous research, to ensure that RL training begins with model possessing reasonable instruction following abilities, all training starts from an SFT-initialized LLaVA-mistral-7b model. We train the model for 15k steps on the Points24 task and 5k steps on the ALFWorld task, aligning with baseline settings. On single A100 (40GB) GPU, the training process with LoRA [14] takes approximately 30 hours to complete. 5.2. Process Guidance Improves VLM Decisionmaking Capabilities As illustrated in Figure 5 and Table 1, our GTR algorithm demonstrates significant and consistent performance improvements on the Points24 task compared to existing models, e.g., 3x-5x increase in success rate and higher return. This achievement underscores the importance of process 6 Model SR(%) ER CNN+RL* Gemini* GPT4-V* GPT4o GPT4o + Tool Qwen2-VL-72b* LLaVA-7b-sft RL4VLM SFT-only GTR 0 0 0 2.5 13.5 4.5 3.0 2.5 11.0 17.5 -1.12 -2.68 -4.39 -6.35 -3.59 / -15.30 -12.95 -2.88 -2.17 Figure 5. Training curves on the 24 points game environment. Compared to the baseline methods, our GTR framework integrating process guidance with RL achieves better performance while maintaining rational reasoning process. Curves are smoothed for better readability. Since GTR and SFT-only employ truncation strategies, we plot γ = 0.9 discounted returns in the figure for fair comparison. Table 1. Evaluation result of different models on the Points24 task. GTR demonstrates significant advantages over other methods in both task success rate and episode returns. SR - success rate, ER - episode return, * - reported in previous work. Model Size Numberline ER SR(%) EZPoints Blackjack SR(%) ER SR(%) ER CNN+RL* Gemini* GPT4-V* GPT4o Qwen-2-VL* LLaVA-sft RL4VLM GTR / API API API 72b 7b 7b 7b 87.1 82.5 65.5 100.0 100.0 59.5 90.5 100.0 0.79 0.74 -0.59 1.00 / -2.61 0.89 1.00 0 2.0 10.5 79.0 100.0 39.0 48.0 94.5 -1.02 -2.57 -1.30 7.0 / 0.67 4.19 9.43 38.8 30.0 25.5 36.0 42.6 25.5 40.1 41. -0.17 -0.35 -0.44 -0.19 / -0.46 -0.16 -0.11 Table 2. Performance of different models in other tasks in gym cards. On simpler tasks where thought collapse is not evident, GTR still achieves improvements over RL4VLM and is comparable to the pretrained model 10x larger in size. SR - success rate, ER - episode return, * - reported in previous work. guidance in RL training for VLMs in challenging tasks. Furthermore, GTR outperforms the thought cloning method and surpasses GPT4o with tool function calling - the corrector model itself. This demonstrates that RL allows the agent to go beyond simple imitation, exploring and discovering superior strategies under the guidance of the corrector. We also assess the performance of GTR on the other tasks in gym cards, shown in Table 2. Due to smaller state and action spaces and significantly shorter episode lengths, thought collapse is not prominent in these tasks. Nevertheless, GTR still achieves noticeable improvements, performing comparably to Qwen-2-VL, which is 10 times larger than our model in size, highlighting the effectiveness of our framework. In Figure 6, we compare the training curves of GTR and RL4VLM on the ALFWorld embodied environment. The results show that RL4VLM, which relies solely on outcome rewards, struggles to provide effective guidance, with both win rates and returns declining, exhibiting signs of thought collapse. In contrast, with thought guidance in GTR, although the corrector model may not be as precise as an expert policy, it is sufficient to stimulate the agents reasoning ability. Table 3 lists the success rates of different methods on tasks in ALFWorld. It is worth noting that using the textual descriptions of scenes in ALFWorld as inputs significantly reduces the difficulty of recognition and decision-making, proved by the heavy reliance observed in the output thoughts. This creates an unreasonable advantage over our setting. Nevertheless, GTR can still achieve competitive success rates through reinforcement learning. 5.3. Ablation Study The necessity of process guidance thoughtout training We aim to investigate whether adjusting the thought cloning SFT loss and PPO loss weights can enable training regime where the agent imitates reasoning early on and focuses on free exploration later. To this end, we apply cosine annealing to the weight of the thought cloning loss, gradually reducing its proportion in the total loss function. As shown in Figure 7, the training curve with annealing fails to achieve the same breakthrough as GTR. closer examination of the models outputs reveals that relaxing process guidance may lead to return to thought collapse, compromising the consistency of reasoning. This fact proves that process guidance is essential throughout the training process to maintain the stability and effectiveness of RL training. The importance of capable corrector model We experiment with removing the tool function calling ability from the GPT-4o corrector, which significantly impairs its ability to analyze and solve the 24-point game task. In this setup, the reference thought provided to the agent lacks rationality, and performance did not improve, which aligns with intuition. Although the agent retained some visual capabilities, its reasoning remained illogical and disconnected from action decisions. This demonstrates that the corrector model must possess enough analytical and problem-solving abilities to synergize with RL training effectively. It also highlights the 7 Model Text Obs Average Pick Clean Heat Cool Look Pick CNN+RL* Gemini* GPT4-V* LLaVA-sft* RL4VLM* MiniGPT-4 BLIP-2 LLaMA-Adapter LLaVA-sft RL4VLM GTR 0 0.14 0.20 0.18 0.22 0.16 0.04 0.13 0.06 0.04 0.17 0 0.35 0.38 0.39 0.47 0.04 0 0.17 0.14 0.15 0.37 0 0 0.18 0.14 0.10 0 0.06 0.10 0.05 0 0.07 0 0 6.7 0.11 0.14 0.19 0.04 0.27 0 0 0. 0 0 0.18 0 0.19 0.17 0.11 0.22 0.06 0 0.33 0 0.16 0.12 0 0.15 0.67 0.06 0 0 0 0.23 0 0.12 0.15 0.29 0.18 0.06 0 0 0 0 0.20 Figure 6. Comparison of training curves between GTR and RL4VLM in the ALFWorld environment. RL4VLM fails to effectively facilitate model learning, leading to thought collapse. GTR, however, enables the agents performance to improve steadily, ultimately achieving superior results. Table 3. Comparison of success rates across different models in the ALFWorld environment. We present the peak performance in the training curve for RL methods. / denotes whether the environment gives textual descriptions of the current observation alongside visual images, which assists the agents decision-making. * - reported in previous work. Figure 7. Ablation study of the GTR framework. Its superior performance highlights the importance of full-process thought guidance, task-specific knowledge, and DAgger. Figure 8. Comparing thought cloning with supervise finetuning on both thoughts and actions. This full response cloning approach does not yield satisfactory results due to its vulnerability to the correctors hallucinations. value of incorporating task-specific knowledge when using general-purpose foundation models as correctors. ceptible to corrector hallucinations, undermining its ability to adjust based on environmental feedback. Cloning thoughts alone remains more balanced and robust approach. GTR benefits from DAgger In Figure 7, we compare the training curves of GTR with and without DAgger. Results show that removing DAgger still allows the model to achieve increased performance in the early stage, but further breakthroughs become increasingly difficult as training progresses. This indicates that distribution shift and forgetting caused by the evolving thought cloning dataset indeed hinder the continual improvement of RL learning. This issue is alleviated by adopting the DAgger method from imitation learning, which constantly expands the dataset for thought cloning. Thought cloning vs. full response cloning We also compare thought cloning with performing SFT on both thoughts and actions. Figure 8 shows that SFT on the full response does not yield promising results. From detailed outputs during training, we identified instances where mismatches between thoughts and actions were incorrectly validated due to corrector hallucinations. We conclude that the constraints on actions from the environment and corrector have interference, and the stronger SFT constraints make the agent more sus6. Conclusion During the RL finetuning of VLM agents for challenging tasks, we identified the thought collapse issue, where the lack of thought supervision leads the agent to generate stateirrelevant reasoning, ultimately losing its ability to think coherently. Therefore, we propose the Guided Thought Reinforcement framework, which introduces an automated corrector to refine the agents reasoning on the fly. This simple yet scalable approach provides effective process guidance. Combining thought reinforcement with the well-established RL framework, GTR unleashes the agents decision-making capabilities, enabling significantly smaller models to achieve substantial advantages in complex, long-horizon tasks. While our study suggests that process guidance is effective for RL training of VLM agents in multi-step tasks, we have not extensively explored the approach promoting o1-like multi-round CoT for action sequence reasoning, which remains an interesting direction for 8 future research. Additionally, due to resource limitations, our research primarily focuses on 7b-scale models. Larger models may further enhance agent performance."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 2 [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 2 [4] Thomas Carta, Clement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International conference on machine learning, pages 36763713. PMLR, 2023. 3 [5] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Alphamath almost zero: process supervision without process. arXiv preprint arXiv:2405.03553, 2024. 3 [6] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. 3 [7] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: International An embodied multimodal language model. conference on machine learning, 2023. [8] Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. 3 [9] Scott Fujimoto and Shixiang Shane Gu. minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:2013220145, 2021. 2, 5 [10] Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. arXiv preprint arXiv:2406.14024, 2024. 2, 3, 4 [11] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and Dorsa Sadigh. Physically grounded vision-language models for robotic manipulation. In 2024 IEEE international conference on robotics and automation, pages 1246212469. IEEE, 2024. 2 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3, 5 [13] Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. Advancing language model reasoning through reinforcement learning and inference scaling. arXiv preprint arXiv:2501.11651, 2025. 2, 3, [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 6, 14 [15] Shengran Hu and Jeff Clune. Thought cloning: Learning to think while acting by imitating human thinking. Advances in Neural Information Processing Systems, 36:4445144469, 2023. 5 [16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pages 91189147. PMLR, 2022. 2 [17] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. 2 [18] Sergey Levine and Vladlen Koltun. Guided policy search. In International conference on machine learning, pages 19. PMLR, 2013. 2 [19] Kanxue Li, Baosheng Yu, Qi Zheng, Yibing Zhan, Yuhui Zhang, Tianle Zhang, Yijun Yang, Yue Chen, Lei Sun, Qiong Cao, et al. Muep: multimodal benchmark for embodied planning with foundation models [c]. [20] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The twelfth international conference on learning representations, 2023. 2, 3, 4 [21] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 2, 14 [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [23] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 14 [24] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2, 2024. [25] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in neural information processing systems, 36:2508125094, 2023. 2 [26] OpenAI. Hello gpt-4o, 2024. 3 [27] OpenAI. Learning to reason with llms, 2024. 3 9 [28] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 3, 14 [29] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. 2 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 14 [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [32] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiante Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241, 2022. 3 [33] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. 2, 5 [34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4 [35] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. 2 [36] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3 [37] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in neural information processing systems, 36:86348652, 2023. [38] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. 2, 6 [39] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. 3 [40] Theodore Sumers, Kenneth Marino, Arun Ahuja, Rob Fergus, and Ishita Dasgupta. Distilling internet-scale visionarXiv preprint language models into embodied agents. arXiv:2301.12507, 2023. 2 [41] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, YuXiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 3 [42] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 3 [43] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. [44] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. 2, 3, 4 [45] Chaojie Wang, Yanchen Deng, Zhiyi Lyu, Liang Zeng, Jujie He, Shuicheng Yan, and Bo An. Q*: Improving multi-step reasoning for llms with deliberative planning. arXiv preprint arXiv:2406.14283, 2024. 3 [46] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. 2 [47] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 2023. 3 [48] Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables openworld multi-task agents. arXiv preprint arXiv:2302.01560, 2023. 2 [49] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837, 2022. [50] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm aparXiv preprint plications via multi-agent conversation. arXiv:2308.08155, 2023. 2 [51] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China information sciences, 68(2):121101, 2025. 2 10 [65] Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 3 [52] Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. Evaluating mathematical reasoning beyond accuracy. arXiv preprint arXiv:2404.05692, 2024. 2, 3, 4 [53] Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024. 3 [54] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. In European conference on computer vision, pages 2038. Springer, 2024. 2 [55] Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129, 2023. [56] Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. Embodied multi-modal agent trained by an llm from parallel textworld. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2627526285, 2024. 2 [57] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:11809 11822, 2023. 2 [58] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International conference on learning representations, 2023. 2 [59] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. 2, 3, 5 [60] Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. 3 [61] Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decisionmaking agents via reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2025. 2, 3, 4, 6, 14 [62] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. 2, 3, 4 [63] Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, and Chengqi Zhang. Wall-e: World alignment by rule learning improves world model-based llm agents. arXiv preprint arXiv:2410.07484, 2024. [64] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024. 3 11 A. Additional Details on Environments We provide detailed introduction to the experimental environments used in this study. A.1. Points24 State and action space. At each state st in the Points24 task, the agent observes an image showing four poker cards and text-based representation of the current formula. The goal is to form formula equal to 24 using the numbers represented by the four cards and basic operators. Card J, Q, are all treated as number 10. The action space includes {1, 2, . . ., 10, +, -, *, /, (, ), =}, and each card can only be used once. Selecting number not present in the image or one that has already been used is considered an illegal action. If the action is legal, the corresponding number or operator is appended to the current formula, forming the next state st+1; if the action is illegal, the state remains unchanged st+1 = st. The environment does not guarantee that the four cards in the image have feasible solution equal to 24. Reward function. At each step, the agent receives reward = 1 for outputting an illegal action and reward = 0 for legal action. The episode terminates when the agent outputs = as an action or the step counts exceeds = 20. At termination, if the formula evaluates to 24, the agent receives an outcome reward = 10; otherwise, it receives = 1. Figure 9. The Points24 task. A.2. ALFWorld State and action space. In the ALFWorld environment in our experiments, the agent receives an RGB observation image and history of past actions at each state st. The action space includes all possible interactions in the current scenario, typically categorized as: (1) go to {recep}, (2) take {obj} from {recep}, (3) put {obj} in/on {recep}, (4) open {recep}, (5) close {recep}, (6) toggle {obj} {recep}, (7) clean {obj} with {recep}, (2) heat {obj} with {recep}, (2) cool {obj} with {recep}, where {obj} and {recep} denote objects and receptacles. After an admissible action is taken, ALFWorld renders the updated scene from the agents view as the next state st+1. st+1 = st if the action is illegal. Notably, the original ALFWorld environment provides text description of the scene in each state without the action history. However, to prevent the agent from relying on the textual description rather than visual observation and to better simulate real-world scenarios, we modified the state by removing the text description and adding the action sequence taken. This adjustment increases the difficulty, emphasizing the agents visual recognition and long-horizon decision-making capabilities. Reward function. The reward system of ALFWorld consists of two components. Each state has set of admissible actions Aadm(s), and illegal actions are penalized. Additionally, each task in ALFWorld has both the final goal gtask and sub-goals gsub, 12 and achieving these goals also provides rewards. Formally, the reward function can be written as: r(st, at, st+1gtask) = 50 1(st+1 = gtask) + 1(st+1 = gsub) 1(at / Aadm(s)). (6) Figure 10. The ALFWorld task. A.3. Other Games in the gym cards Environment We briefly introduce the other tasks in the gym cards environment, which have significantly smaller state and action spaces, shorter episode lengths, and lower complexity than the two tasks we selected. As result, these tasks do not exhibit the thought collapse phenomenon. Nevertheless, GTR still achieves performance improvements in these more straightforward tasks. Numberline. The agent receives an image displaying the text Target: and Current: y, where x, are integers in [0, 5]. The action space is {+, -}, which increments or decrements the current number by 1, respectively. The goal is to make the current number equal to the target number. The agent gets reward of 1 upon achieving the goal and penalty of -1 if an action moves the current number away from the target. The game can always be solved within 5 steps. EZPoints. This task is simplified variant of Points24, with the image containing only two cards, the available operators limited to {+, -, =}, and the target value is 12. In addition, the EZPoints environment guarantees that the two cards in the image always have valid solution. The correct formula always takes 4 steps. Blackjack. The task is to win the blackjack game. The image at each state includes two cards of the dealer (one of them facing down) and all cards from the player. The action space is {stand, hit}. The agent gets one more card when choosing hit, and the game terminates when choosing stand. Theoretically, the player has an expected winning rate slightly below 50%. Figure 11. Other tasks in the gym cards environment. B. Additional Details on Training Drawing inspiration from the RLHF training framework [28] and prior related work [61], we perform 1 epoch of supervised fine-tuning on the base LLaVa-v1.6-mistral-7b model [2123] before RL training, which is referred to as LLaVa-sft in the results. The datasets are sourced from the RL4VLM paper [61], with labels for the gym cards environment provided by task solver and labels for the ALFWorld environment generated by GPT-4V. In Table 4, we provide the hyperparameter settings used for GTR training, which are primarily derived from values proposed in previous work [61]. We employ LoRA [14] to fine-tune the entire VLM model, including the CLIP vision encoder [30], LLM backbone, and MLP projector, enabling it to run on an A100 GPU with 40GB memory. Hyperparameter Value General Setup - Training Learning rate Initial learning rate Final learning rate Maximum learning rate step Discount factor γ GAE λ PPO entropy coefficient PPO value loss coefficient PPO clip parameter PPO epoch Gradient accumulation steps LoRA LoRA α LoRA dropout CosineAnnealingLR 1e 5 1e 9 25 0.9 0.95 0.01 0.5 0.1 4 128 128 256 0.05 General Setup - Models Generation max text length Generation temperature Generation repetition penalty Corrector max text length Corrector temperature For Points24 task Environmental steps Thought probability coefficient For ALFWorld task Environmental steps Thought probability coefficient 256 0.2 1.2 600 0.4 15000 0.5 5000 0.2 Table 4. Hyperparameters of GTR 14 C. Prompts of the Corrector Model Prompt adopted by VLM corrector model for Points24 task If the target formula is NOT DETERMINED, use the find all correct formulas The The Remember the correct formulas, Note that J, Q, and count as 10. According to the rules, does the ranks in the thought match The goal is to output formula that evaluates to 24, and each number can only be used once. System Prompt: You are an expert 24-point card game player. You are observing four cards in the image and the current formula. number or operator include [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, +, -, *, /, (, ), =], and the chosen number or operator will be appended to the current formula to reach the correct target formula that evaluates to 24. Query: You will be given the current formula, the thought of player playing this game, and target formula. players thought may be wrong, please evaluate its correctness by the following aspects: (1) What are the four cards in the image? tool function to find all possible correct formulas by the four cards in the image. and do not output the result. (2) What are the recognized card ranks in the thought? your observation in question (1), regardless of the order? (3) What is the proposed formula the player is trying to reach in the thought? Does the proposed formula match the target formula or, if the target formula is NOT DETERMINED, one of the possible correct formulas in question (1)? (4) Does the player choose the correct action to reach the proposed formula or choose = if the current formula is complete? Please briefly answer the above questions, then give your final evaluation. available information for thought correction: determine the next single number or character to append to the current formula and finally provide the correct thought. Your response should be valid json file in the following format: \"answer1\": {Text, answer to the first question}, \"answer2\": {Text, answer to the second question}, \"answer3\": {Text, answer to the third question}, \"answer4\": {Text, answer to the third question}, \"evaluation\": {YES or NO}, \"possible solution\": \"target formula\": correct. Otherwise, choose an appropriate target formula from all possible correct formulas obtained from the tool function for the player to reach. \"correction\": {Json object, the correct thought. } [Current Formula] ... [Thought] ... [Target Formula] ... {YES or NO, indicating whether there is possible solution. {The given target formula if it is not None. If the thought is incorrect, use all None if the thought is correct}, None if the thought is correct} The proposed formula in the thought if the thought is }, { Prompt adopted by VLM corrector model for ALFWorld task The task is to ... The environment requires the player to ... All admissible actions of the current situation are: System Prompt: You are an expert in the ALFRED Embodied Environment. navigate, take certain objects, interact with objects if necessary, and finally put objects in the designated place to complete the task. Query: You will be given the visual observation and thought of player in this environment. are also given the previous actions the player has taken: ... Please evaluate if the reasoning is correct by the following aspects: (1) What objects are in your sight and whether you are holding certain object? the image? (2) Based on the task description and the action history, what should be the players next sub-goal (notice that the tasks require the player to first pick up certain objects, interact with receptacles if the task is cooling, heating, cleaning or looking in light, and finally placing the object)? Does the thought align with the sub-goal? (3) Based on the task description and the action history, does the player choose one of the admissible actions to If the target object is not in sight, go to an unexplored location; reach the sub-goal? if there is required object, take it; if the task requires cooling, heating, cleaning, or looking in light, navigate and interact with the receptacles. Please briefly answer the above questions, then give your final evaluation. available information for thought correction: finish the task, and finally provide the correct thought. Your response should be valid json file in the following format: \"answer1\": {Text, answer to the first question}, \"answer2\": {Text, answer to the second question}, \"answer3\": {Text, answer to the third question}, \"evaluation\": {YES or NO}, \"correction\": {Json object, the correct thought. } [The players thought] ... choose one correct step from the admissible actions for the player to If the thought is incorrect, include all Does the thought correctly identify None if the thought is correct} Does the action take effect? You { 15 D. Example Trajectories of Thought Collapse In this section, we present real episode examples from the Points24 and ALFWorld tasks that exhibit thought collapse during RL training. These examples demonstrate how the models outputs degrade, becoming state-irrelevant and irrational, significantly impairing the agents decision-making capabilities. This catastrophic phenomenon is the core issue addressed in our work. Figure 12. Examples of thought collapse trajectories in Points24 and ALFWorld."
        }
    ],
    "affiliations": [
        "Peking University",
        "Tencent",
        "Tsinghua University"
    ]
}