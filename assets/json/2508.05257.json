{
    "paper_title": "MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs",
    "authors": [
        "Xiaodong Chen",
        "Mingming Ha",
        "Zhenzhong Lan",
        "Jing Zhang",
        "Jianguo Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Mixture-of-Experts (MoE) architecture has become a predominant paradigm for scaling large language models (LLMs). Despite offering strong performance and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and Kimi-K2-Instruct present serious challenges due to substantial memory requirements in deployment. While recent works have explored MoE compression to address this issue, existing methods often suffer from considerable accuracy drops (e.g., 7-14% relatively) even at modest compression rates. This paper introduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model compression while incurring minimal accuracy drops. Specifically, each up/gate matrix in an expert is decomposed via a rank decomposition as W = AB, where matrix A is unique to each expert. The relatively larger matrix B is further re-parameterized as a linear combination of basis matrices {Bi} shared across all experts within a given MoE layer. The factorization is learned by minimizing the reconstruction error relative to the original weight matrices. Experiments demonstrate that MoBE achieves notably lower accuracy drops compared to prior works. For instance, MoBE can reduce the parameter counts of Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by 24%-30% with only 1%-2% accuracy drop (about 2% drops when measured relatively)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 7 5 2 5 0 . 8 0 5 2 : r MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs Xiaodong Chen2,1, Mingming Ha1, Zhenzhong Lan1,3, Jing Zhang2, Jianguo Li1 1Inclusion AI 2Renmin University of China 3Westlake University"
        },
        {
            "title": "Abstract",
            "content": "The Mixture-of-Experts (MoE) architecture has become predominant paradigm for scaling large language models (LLMs). Despite offering strong performance and computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and Kimi-K2-Instruct present serious challenges due to substantial memory requirements in deployment. While recent works have explored MoE compression to address this issue, existing methods often suffer from considerable accuracy drops (e.g., 7-14% relatively) even at modest compression rates. This paper introduces novel Mixture-of-Basis-Experts (MoBE) method that achieves model compression while incurring minimal accuracy drops. Specifically, each up/gate matrix in an expert is decomposed via rank decomposition as = AB, where matrix is unique to each expert. The relatively larger matrix is further re-parameterized as linear combination of basis matrices {Bi} shared across all experts within given MoE layer. The factorization is learned by minimizing the reconstruction error relative to the original weight matrices. Experiments demonstrate that MoBE achieves notably lower accuracy drops compared to prior works. For instance, MoBE can reduce the parameter counts of Qwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by 24%-30% with only 1%-2% accuracy drop (about 2% drops when measured relatively). 1 introduction Transformer-based large language models (LLMs) (Vaswani et al., 2017) have revolutionized natural language processing, achieving state-of-the-art performance in domains such as creative writing, code generation, and mathematical reasoning. This progress has been largely guided by scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022), which posit that model performance improves with increases in parameter count and training data size. However, scaling dense architectures beyond certain thresholdtypically hundreds of billions of parameters (>100B)has proven challenging and prohibitive. Therefore, the Mixture-of-Experts (MoE) (Jacobs et al., 1991; Jordan & Jacobs, 1994; Cai et al., 2024) architecture has become popular since the sparse activation makes MoEs much easier and more efficient to scale to more than several hundreds of billions of parameters (Liu et al., 2024; Yang et al., 2025; Team et al., 2025a) since last year. Despite the computational advantages of sparse activation, the large total parameter counts of MoE-based LLMs present significant bottleneck for practical deployment. For instance, leading open-source LLMs such as DeepSeek-V3-0324 (671B parameters) (Liu et al., 2024) exhibit performance comparable to top closed-source models. However, their scale imposes prohibitive demands on GPU memory; even high-end infrastructure, such as machine with 8x H100 GPUs, may be insufficient for efficient inference. To address this challenge, much research have been proposed for MoE-based LLM compression, which could be generally categorized into two major categories. First, pruning techniques reduce total parameter counts by either removing entire experts (Xie et al., 2024; Lu et al., 2024; Yang et al., 2024) or merging similar ones (hao Work done at Ant Group. Corresponding Authors. https://github.com/inclusionAI/MoBE 1 Figure 1: Relative performance comparison of different MoE compression methods. Relative accuracy is the ratio of the compressed models performance to that of the original model. The accuracy are averaged over 15 benchmarks as shown in Table 3. Applying D2-MoE to large models like Qwen3-235B-A22B-2507, DeepSeekV3-0324 and Kimi-K2-Instruct is computationally prohibitive on an 8x H100 GPU machine; therefore, it is excluded from these comparisons. MoBE is evaluated at compression rates similar to or higher than the baseline methods (MoLAE, D2-MoE). Absolute performance is detailed in Appendix (Figure 8). Liu et al., 2024; Li et al., 2023b; Chen et al., 2024). However, this approach often leads to permanent loss of specialized knowledge and significant performance degradation (Gu et al., 2025). Second, decomposition techniques employ matrix factorization to compress each experts weight matrices (Gu et al., 2025; Liu et al., 2025; Li et al., 2025). Typical works include D2-MoE (Gu et al., 2025), which extracts shared weights and applies singular value decomposition (SVD) to the residual delta weights, and MoLAE (Liu et al., 2025), which uses SVD to represent each expert weight as product of its unique transformation matrix and shared latent matrix. Although these SVD-based methods generally outperform expert pruning, they can still incur substantial information loss. This is evidenced by the high Mean Squared Error (MSE) between the original and reconstructed matrices, as shown in our reconstruction error analysis (Figures 2-4). In this paper, we introduce the Mixture-of-Basis-Experts (MoBE), novel method for efficient, performancepreserving parameter compression for MoE-based LLMs. MoBE factorizes weight matrix in an expert with rank decomposition = AB, where is unique for each expert and is re-parameterized as linear combination of set of basis matrices {Bi} that are shared across all experts within each MoE layer. This formulation achieves parameter reduction for two reasons. First, the number of basis matrices is much smaller than the number of experts n, i.e. n, and basis {Bi} is shared across all experts within each layer so that we could save considerable parameters for B. Second, the unique transformation matrix is smaller than W, so that the whole MoBE factorization achieves parameter savings. The MoBE factorization is optimized by minimizing the reconstruction error between the factorized representation and the original pretrained weight matrices, typically using the gradient descent method. We conduct comprehensive experiments on diverse set of MoE-based LLMs, including Ling-Lite-Chat (Team et al., 2025b), DeepSeek-V2-Lite-Chat (Shao et al., 2024), DeepSeek-V3-0324 (Liu et al., 2024), Qwen3-30BA3B-2507, Qwen3-235B-A22B-2507 (Yang et al., 2025) and Kimi-K2-Instruct (Team et al., 2025a). direct comparison of reconstruction error on Ling-Lite-Chat, DeepSeek-V2-Lite-Chat, and Qwen3-30B-A3B-2507 demonstrates that MoBE achieves consistently lower MSE than both MoLAE and D2-MoE, often with reductions of over 50%, across all layers (Figures 2-4). Similar results for Qwen3-235B-A22B-2507, DeepSeekV3-0324 and Kimi-K2-Instruct are presented in Appendix C. To assess downstream task performance, we evaluate the compressed models on wide range of benchmarks. As shown in Figure 1, MoBE exhibits 2 (a) Gate matrices (b) Up matrices Figure 2: Comparison of pre-layer MSE for compressing the gate (a) and up (b) matrices of Ling-Lite-Chat using MoBE, D2-MoE and MoLAE. (a) Gate matrices (b) Up matrices Figure 3: Comparison of pre-layer MSE for compressing the gate (a) and up (b) matrices of DeepSeek-V2Lite-Chat using MoBE, D2-MoE and MoLAE. superior performance advance compared to MoLAE and D2-MoE at similar or even higher compression rates. In summary, our contributions can be summarized as follows: We introduce the Mixture-of-Basis-Experts (MoBE), parameter-efficient architecture for MoE model compression. Our analysis shows that this design yields significantly lower reconstruction error compared to existing decomposition techniques. We demonstrate through extensive experiments on leading MoE models, including Qwen3-235BA22B-2507, DeepSeek-V3-0324 and Kimi-K2-Instruct, that MoBE can reduce total parameter counts by 24%-30% while retaining up to 98% of the original performance, outperforming state-of-the-art MoE counterparts by large margin. We open-source our code to facilitate further research and development in efficient MoE architectures."
        },
        {
            "title": "2 Related Works",
            "content": "Research on MoE compression can be categorized into expert pruning-based (Xie et al., 2024; Lu et al., 2024; Yang et al., 2024) and decomposition-based (Li et al., 2025; Liu et al., 2025; Gu et al., 2025). Below we elaborate on related works under these two categories. 2.1 Expert Pruning-based MoE Compression Methods Expert pruning-based methods aim to reduce the total parameter counts of MoE-based LLMs by either directly removing entire experts or merging them. For instance, NAEE (Lu et al., 2024) removes unimportant experts by evaluating expert combinations on calibration dataset to minimize model loss, while STUN (Lee et al., 2024) groups experts based on co-activation frequency and routing weight similarity, retaining only one expert per group. 3 (a) Gate matrices (b) Up matrices Figure 4: Comparison of per-layer MSE loss for compressing the gate (a) and up (b) matrices of Qwen3-30BA3B-2507 using MoBE, D2-MoE and MoLAE. Other approaches focus on merging similar experts. DEK (Zhang et al., 2024), for example, identifies and groups similar experts in the feature space and then merges them in the weight space to reduce redundancy. MC-SMoE (Li et al., 2023b) organizes experts into distinct groups according to routing strategies and merges each group into single expert. Because these methods remove entire expert modules, they risk permanent loss of specialized knowledge, often leading to notable accuracy degradation on certain tasks. 2.2 Expert Matrix Decomposition-based MoE Compression Methods In contrast to expert pruning, expert matrix decomposition-based methods compress MoE-based LLMs by factorizing each experts weight matrices into relatively smaller representations. D2-MoE (Gu et al., 2025) and MoLAE (Liu et al., 2025) are two state-of-the-art examples of this category. D2-MoE approximates each expert matrix with shared matrix and residual delta matrix, in which the shared weight is obtained via Fisher-weighted average of the original weights, and the residual delta weights (the difference between original and shared weights) are decomposed into low-rank matrices using SVD. MoLAE first groups set of up/gate matrices in each MoE layer, and then approximates each matrix in group by an expert-specific transformation matrix and the product of group-shared latent matrix. The approximation is achieved using SVD on the stacked up/gate matrices within the group. Although these methods are effective in reducing parameter counts, their reliance on low-rank assumptions can be limitation. The resulting matrix factorization does not always capture the full information of the original weights, which can introduce substantial reconstruction errors and lead to notable performance drops in downstream tasks. In Appendix B, we analyze the effective rank of expert weight matrices in several leading open-source MoE models. Our results show that this rank consistently exceeds the compression threshold of SVDmeaning that to achieve parameter reduction, the number of retained singular values must fall below this threshold. Eliminating this excess rank reduces the matrixs expressive power, likely explaining the performance degradation observed in these SVD-based compression methods."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first briefly review the standard Mixture-of-Experts (MoE) architecture (Section 3.1). Then, we elaborate our proposed Mixture-of-Basis-Experts (MoBE) architecture and detail the algorithm for converting pretrained MoE model to MoBE architecture (Section 3.2). Finally, we describe the activation functions in MoBE (Section 3.3) and specific Z-score normalization technique applied to the expert weight matrices during the conversion process (Section 3.4). 3.1 Standard Mixture-of-Experts Architecture standard MoE layer replaces the dense Feed-Forward Network (FFN) in the Transformer with sparsely activated structure comprising router and multiple experts. For each input token, the router dynamically selects small subset of these experts for processing, which yields significant computation cost reduction. In typical MoE layer with experts, the i-th expert (Ei) often employs SwiGLU formulation (Shazeer, 2020) 4 Figure 5: The Mixture-of-Basis-Experts (MoBE) architecture. For clarity of explanation, we omit the activation function following the gate matrix. to process an input token embedding Rd as where Wi intermediate dimension of MoE experts. It is observed in most open-source MoE models that < gatex)), down Rdp denote the up, gate, and down projection matrices of Ei, is the up/gate Rpd and Wi upx SiLU(Wi down (Wi Ei(x) = Wi (1) 2 d. The router calculates gating score for each expert and selects the top-K experts for the token: where Wg Rnd denotes the weight matrix of the router G. The final output of the MoE layer is weighted sum of the outputs from the selected experts: G(x) = TopK(Softmax(Wgx)) (2) = i=1 Gi(x)Ei(x), (3) where Gi(x) denotes the gating value (i.e., the router score) of the i-th expert Ei. This operation is applied independently to every token in the input sequence."
        },
        {
            "title": "3.2 Mixture-of-Basis-Experts Architecture\nWhile large MoE models are much more efficient in inference than dense models of a similar size, they are\nalso constrained by higher memory and storage requirements during deployment. To alleviate this, we\nintroduce the Mixture-of-Basis-Experts (MoBE) architecture, as illustrated in Figure 5. The MoBE formu-\nlation begins by factorizing the up/gate matrix Wi ∈ Rp×d of the i-th expert from the perspective of rank\ndecomposition (Golub & Van Loan, 2013) as",
            "content": "Wi = AiBi, where Ai Rpr, Bi Rrd, and is the rank of Wi with min{p, d} = p. MoBE further considers re-parameterizing Bi with set of shared basis matrices as Bi = j=1 αi,jBj, with αi,j 0, j= αi,j = 1, 5 MoE with experts per layer; target basis count n; activation function . MoBE with parameters directly from MoE do MoE. Algorithm 1 Converting standard MoE into MoBE 1: Require: L-layers model 2: Ensure: Parameter-efficient MoBE model 3: Initialize non-MoE parts in 4: for each MoE layer in 5: 6: 7: 8: // Decompose up and gate projection matrices for type {gate, up} do Let {Wi Solve Eq(5) with Adam optimizer Obtain the factorized components {Ai }n MoBE. i=1 be the expert matrices of the l-th layer t}, {Bj t}, {α i,j } end for // Keep down projection matrices unchanged Copy the l-th layer down projection matrices {Wi 9: 10: 11: 12: 13: Assemble the l-th MoBE layer with {At, Bt, αt} and {Wdown}. 14: end for 15: return down}n MoBE i=1 from MoBE j=1 is set of basis matrices shared in one MoE layer, and {αi,j}m where {Bj Rrd}m j=1 are learnable, expertspecific weighted coefficients. Combining these components and introducing non-linear activation function (e.g., SiLU (Ramachandran et al., 2018)) to enhance representational power, we define the final MoBE factorization as: ˆWi = Ai ( αi,jBj), (4) j=1 where ˆWi is the reconstructed version of Wi. This factorization allows the shared basis matrices {Bj} to capture common information across all experts in one layer, while the expert-specific transformation matrices Ai encode specialized information. We apply this factorization to both the gate and up projection matrices. However, we do not decompose the down projection matrices, as prior research indicates they store critical knowledge (Geva et al., 2020; Meng et al., 2022) and are less amenable to effective compression (Liu et al., 2025). We convert pretrained MoE-based LLM into our proposed MoBE formulation by learning the factorized components. This is achieved by minimizing the reconstruction error between the original expert weight matrix Wi and the reconstruction matrix ˆWi as (cid:13)Wi ˆWi(cid:13) (cid:13) (cid:13)Wi Ai ( (cid:13) αi,jBj) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) = (5) 2 i=1 j=1 min Ai,Bj,αi,j i=1 This optimization problem can be solved using various algorithms, such as gradient-based optimizers like Adam (Kingma & Ba, 2014) or the Alternating Optimization (AO) method (Wu & Lange, 2008). In our practice, we find that the Adam optimizer performs sufficiently well across layers and various models, while AO suffers from unstable behavior during its alternating optimization steps. Algorithm 1 details the full procedure for converting standard MoE model to the MoBE formulation. We further analyze the parameter complexity of MoBE compared to standard MoE as illustrated in Table 1. Note that this analysis considers only the total and activation parameter count for single MoE layer, excluding other components such as the embedding and attention layers. The total parameter counts for one MoBE layer is ndp + 2npr + 2mrd, where the first term is for the down matrices Wdown, the second term is for the transformation matrices in the up and gate projection, and the third term is for the basis matrices {Bj}. The parameter count ratio (γ) from MoE to MoBE can be computed as Since < 1 we set = 16, we could have the last term 2mr 2 d, the second term 2r 3d < 1 γ = = 1 3 ndp + 2npr + 2mrd 3ndp 2mr 3np 3 . For the last term, n, for an MoE with = 128 experts, even if 3 + 1 12 < 1. When using MoBE to 12 . Therefore, γ < 1 3np < 1 3 + 2r 3d + + ."
        },
        {
            "title": "Standard MoE",
            "content": "#Total Parameters #Activation Parameters 3ndp 3kdp MoBE ndp + 2npr + 2mrd kdp + 2kpr + 2krd MoBE ndp + 2npr + 2mrd kdp + 2k pr + 2krd Table 1: Comparison of total and activation parameter count for one standard MoE and MoBE layer. MoBE is MoBE variant with further activation expert number reduction. Gate Matrices Mean Std Up Matrices Mean Std Ling-Lite-Chat DeepSeek-V2-Lite-Chat DeepSeek-V3-0324 Qwen3-30B-A3B-2507 Qwen3-235B-A22B-2507 Kimi-K2-Instruct 2.2e-5 2.8e-2 1.0e-6 2.9e-2 -4.2e-6 1.2e- -2.8e-5 2.3e-2 -1.4e-5 1.6e-2 -1.3e-6 2.6e-2 -5.3e-9 1.2e-2 Table 2: Means and stds of the gate matrices and up matrices in various MoE-based LLMs. -1.6e-7 3.0e-2 1.8e-8 1.6e5.3e-7 2.3e-2 2.3e-7 2.8e-2 4.2e-8 2.6e-2 replace MoE, the compression ratio by MoBE is 1 γ. From the analysis, we can draw the conclusion that the MoBE architecture could substantially compress the standard MoE models. Notably, while MoBE reduces the total parameters quite lot, its activation parameter count requires closer examination. The matrices and the down matrices Wdown contribute 2krd + kdp 3kdp (since p) to the activation parameter count, while the transformation matrices introduce an additional 2kpr. This may lead to an increase in the number of activation parameters. To compensate for this increase, inspired by previous work (Chaudhari et al., 2025), we propose variant MoBE, which reduces the number of activated experts during inference from to smaller value k. In many modern MoE models, the number of activated experts is typically set to 8. In MoBE, we reduce this to 6 (i.e., = 6). 3.3 Activation Function in MoBE In Eq(4), we employ an activation function to enhance representational power. However, not all activation functions are equally suitable. For instance, we posit that the commonly used ReLU (Glorot et al., 2011) activation function is suboptimal for this task. ReLU can induce excessive sparsity in the matrix Bi = j=1 wi,jBj), which may cause notable information loss. As the transformation matrix Ai Rpr is smaller (m than Bi Rrd, it may struggle to compensate for this loss with such limited representation capacity. Therefore, bipolar activation function (i.e., one that outputs both positive and negative values like tanh) is highly desirable. Consequently, activation functions such as Tanh (LeCun et al., 1989), SiLU (Ramachandran et al., 2018), and GeLU (Hendrycks & Gimpel, 2016) are more suited for this task, while Sigmoid (Rumelhart et al., 1986) and ReLU are expected to yield inferior results. Our ablation study in Section 4.4 provide evidence supporting this hypothesis."
        },
        {
            "title": "3.4 Z-score Normalization in MoBE\nTo address the impact of a wide range of weight values and obtain stable results in seeking the basis, we\nconsider normalizing all expert weight matrices in each MoE layer. We introduce a Z-score normalization by\nsubtracting the mean and dividing by the standard deviation (std) across all experts’ weights:",
            "content": "µW = mean(W1, W2, ..., Wn), σW = std(W1, W2, ..., Wn), Wi = Wi µW σW . (6) (7) (8) This normalization introduces additional inference overhead. After factorization, the σW term can be folded into the transformation matrix Ai, and the µW term will require an extra bias operation during inference The method (Chaudhari et al., 2025) reduces only activation parameters, not total parameters. Therefore, we consider it complementary approach and did not include it in our experimental comparisons. compared to the original form Eq(4). ˆWi = σW ˆWi + µW = (σW Ai) ( j=1 αi,jBj) + µW. (9) However, we empirically study different off-the-shelf MoE models and find that µW is typically negligibly small as shown in Table 2. We can therefore omit the term µW in Eq(9). That means, we only require absorbing σW into Ai without introducing extra parameters and computing overhead during inference."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we extensively evaluate the proposed MoBE approach on suite of popular open-source MoE models and compare to state-of-the-art MoE compression methods (Section 4.3). We then conduct set of ablation studies on activation functions (Section 4.4) and normalization schemes (Section 4.5). 4.1 Setup Models. We evaluate our method, MoBE, on suite of popular open-source MoE-based LLMs: Ling-LiteChat (Team et al., 2025b), DeepSeek-V2-Lite-Chat (Shao et al., 2024), DeepSeek-V3-0324 (Liu et al., 2024), Qwen3-30B-A3B-2507, Qwen3-235B-A22B-2507 (Yang et al., 2025) and Kimi-K2-Instruct (Team et al., 2025a). Baseline. We compare our approach against two state-of-the-art MoE compression baselines, D2-MoE (Gu et al., 2025) and MoLAE (Liu et al., 2025). Both MoBE and MoLAE are data-free compression methods, whereas D2-MoE requires calibration dataset, for which we use tulu-v3-sft-mixture (Lambert et al., 2024). Due to the high computational cost of its backward pass, applying D2-MoE to very large models like Qwen3235B-A22B-2507, DeepSeek-V3-0324 and Kimi-K2-Instruct is infeasible on single 8xH100 GPU machine. Therefore, comparisons involving D2-MoE are excluded from these three larger models. Hyper-parameters. Hyper-parameters are configured per case (models or methods). For Ling-Lite-Chat and DeepSeek-V2-Lite-Chat, MoBE uses = 4 basis matrices, and MoLAE uses 8 latent matrices. To compensate extra computing cost introduced by extra activation parameters in MoBE (Section 3.2), we reduce its number of activated experts from = 6 to = 4 in MoBE. For Qwen3-30B-A3B-2507 and Qwen3-235B-A22B-2507, both MoBE and MoLAE use 32 basis/latent matrices. For MoBE, the number of activated experts is reduced from = 8 to = 6 in MoBE. For DeepSeek-V3-0324, both MoBE and MoLAE use 64 basis/latent matrices. The number of activated experts is reduced from = 8 to = 6 in MoBE. For Kimi-K2-Instruct, both MoBE and MoLAE use 128 basis/latent matrices. The number of activated experts is reduced from = 8 to = 6 in MoBE. Because jointly optimizing all 384 expert matrices within single layer of Kimi-K2-Instruct is challenging, we instead split the experts into two sequential groups and train each group with its own set of 64 basis matrices. For D2-MoE, the rank of the delta weights is set to 700 for Ling-Lite-Chat and DeepSeek-V2-Lite-Chat, and 420 for Qwen3-30B-A3B-2507. We do not apply compression to the shared weights created by D2-MoE. For simplicity, we set the rank = in all our studies. It gets more compression ratio when setting < while may increasing the accuracy drops. Implementation Details. All experiments are run on H100 GPUs with the Adam optimizer (Loshchilov & Hutter, 2017) and 0.07 learning rate. We will make the training and inference code open-sourced. 4.2 Evaluation Benchmark We perform comprehensive evaluation of all compressed MoE-based LLMs across wide spectrum of benchmark. The evaluation suite covers four primary domains: General Knowledge: BBH (Srivastava et al., 2022), MMLU (Hendrycks et al., 2020), CEval (Huang et al., 2023), CMMLU (Li et al., 2023a). General Reasoning: ARC-Challenge (Clark et al., 2018), IFEval (Zhou et al., 2023), GPQA (Rein et al., 2023). LLM Method Ratio General Reasoning General Knowledge Mathematics Coding ARC-C IFEval GPQA BBH MMLU CEval CMMLU Math GSM8k AIME24 AIME25 MBPP HumanEval Multipl-E LCB Ling-Lite-Chat DeepSeek-V2-Lite-Chat Qwen3-30B-A3BDeepSeek-V3-0324 Qwen3-235B-A22B-2507 MoE D2-MoE MoLAE MoBE MoBE MoE D2-MoE MoLAE MoBE MoBE MoE D2-MoE MoLAE MoBE MoBE MoE MoLAE MoBE MoBE MoE MoLAE MoBE MoBE 0% 14% 12% 16% 16% 0% 13% 11% 15% 15% 0% 24% 24% 24% 24% 0% 30% 30% 30% 0% 24% 24% 24% 89.2 82.4 85.4 87.1 85.8 65.1 62.7 65.8 67.5 63.1 95.6 93.1 92.5 96.6 95.9 97.0 97.3 98.0 96.6 97.0 95.6 96.3 95.6 81.5 78.3 75.1 79.2 79. 49.7 49.0 43.9 46.0 45.1 86.6 83.5 79.2 86.9 85.1 84.8 83.2 84.5 84.3 90.0 85.5 89.9 88.7 33.0 31.2 29.7 29.4 29.9 25.9 29.7 26.9 30.3 26. 56.8 45.2 46.3 52.1 51.0 66.7 54.0 63.6 62.6 60.7 66.2 58.6 58.1 58.7 51.3 51.9 61.5 53.8 36.0 30.1 34.0 33.9 32.5 85.4 69.9 76.5 83.5 83. 85.4 82.9 85.2 85.4 89.5 87.5 89.0 88.8 72.6 64.5 69.5 71.5 70.3 53.7 50.4 53.0 53.7 50.9 87.6 83.3 80.3 85.6 86.0 90.3 87.3 89.5 87. 90.9 88.9 90.4 90.3 65.4 56.5 61.9 66.6 64.3 55.4 48.1 47.9 53.0 53.0 88.2 71.2 76.0 85.1 85.9 90.4 84.4 87.8 87.9 90.9 87.3 90.6 90. 70.6 56.0 62.3 66.2 66.9 58.6 51.0 52.8 56.3 55.3 86.6 68.6 74.9 83.5 83.9 88.6 83.2 87.2 89.4 90.0 86.9 89.7 89.6 72.6 64.9 66.3 70.4 69. 27.6 23.2 18.6 23.5 23.0 93.3 86.1 85.4 92.5 92.6 92.0 87.6 90.3 91.0 94.4 90.5 94.2 93.6 88.1 85.7 83.9 88.0 83.6 61.4 60.0 59.2 58.6 60. 96.4 93.0 91.4 95.2 96.1 94.9 95.5 93.7 94.8 96.7 95.5 96.3 96.0 8.3 8.3 10.0 11.7 11.7 0 0 0.8 0.8 2.5 59.4 38.3 35.2 55.0 54. 56.9 38.5 52.3 49.8 61.9 54.2 64.8 62.9 10.0 10.0 4.2 9.2 12.5 0 0 0 0 0 51.3 29.1 33.1 45.2 45.6 47.3 29.6 40.6 41. 51.7 44.6 54.8 50.8 77.3 70.3 71.4 77.5 77.3 59.0 50.1 46.6 51.5 51.5 86.4 79.5 81.7 87.4 85.3 89.7 87.4 89.9 89.0 93.0 70.7 89.2 87. 81.2 72.6 82.9 82.9 82.6 40.2 39.2 41.5 43.3 50.6 93.1 84.0 82.9 91.8 92.2 93.4 89.5 93.6 93.8 96.3 81.0 93.8 93.1 65.0 50.2 60.3 64.0 62. 34.6 25.8 26.9 31.7 29.0 70.6 44.0 50.8 61.9 61.2 68.2 61.0 73.1 73.0 70.5 30.0 71.9 65.5 21.6 14.4 15.0 14.4 17.4 2.4 1.8 1.8 3.6 3. 41.5 26.9 25.6 35.6 38.0 44.6 34.4 40.9 42.1 48.4 33.5 43.7 45.2 Avg 59.7 53.1 55.3 58.6 57.8 38.0 34.7 34.6 36.9 36. 78.6 66.4 67.5 75.8 75.7 79.3 73.1 78.0 77.9 81.5 73.2 80.9 79.7 Kimi-K2-Instruct 90.8 88.2 91.4 91.7 95.9 96.6 97.0 96. 0% 24% 24% 24% MoE MoLAE MoBE MoBE 82.4 76.6 81.1 80.8 Table 3: Performance comparison of different compression methods on various MoE-based LLMs, where indicates that this model activates fewer experts than the original model to compensate for the increase in activation parameters. The column Ratio refers to the proportion of compressed parameters to the total parameters in the LLMs. 66.7 60.6 68.8 65.0 50.3 40.3 47.2 44.6 50.2 35.0 44.4 44. 64.8 44.8 62.5 61.7 90.8 89.2 90.3 90.2 92.4 89.4 90.2 90.3 89.9 87.8 89.2 89.3 77.4 66.4 73.2 74.6 95.7 90.9 94.9 95. 96.7 93.0 96.3 96.6 90.9 88.8 89.9 90.4 95.4 91.9 94.0 94.1 88.8 86.0 87.2 88.1 Mathematics: Math (Hendrycks et al., 2021), GSM8k (Cobbe et al., 2021), AIME24, AIME25. Coding: MBPP (Austin et al., 2021), HumanEval (Chen et al., 2021), LCB(LiveCodeBench-v5) (Jain et al., 2024), MultiPL-E (Cassano et al., 2022). The evaluation includes several specialized testing protocols: AIME Evaluation: On AIME24 and AIME25, we run inference 16 times per question for each model and report the average accuracy. IFEval Scoring: The final score for IFEval is the average of the strict accuracies at both the prompt and instruction levels. 4.3 Main Results All the compared results of the origin model (MoE) and different compression methods (MoBE, MoBE, D2MoE, and MoLAE) are shown in Tables 3. It shows that our proposed MoBE method generally outperforms all the compared compression methods across various benchmarks. For instance, for the Ling-Lite-Chat and DeepSeek-V2-Lite-Chat models, MoBE improves performance by 2-3% accuracy over the baseline. The performance gains are even more notable for Qwen3-30B-A3B-2507, Qwen3-235B-A22B-2507, DeepSeek-V30324 and Kimi-K2-Instruct, reaching 4-8% accuracy advantages over compared compression methods. We note that converting standard MoE model into our MoBE architecture results in an average performance degradation of approximately 1.4% accuracy compared to the original MoE models. For comparison, simpler variant MoBE that only reduces the number of activated experts from to k, leads to smaller degradation of around 0.5% accuracy. This comparison suggests that it is more challenging to compress the total parameters than activation parameters for an MoE model. As the sparsity ratio (#activated-parameters/#total-parameters) of recent MoE models becomes larger and larger so that the total parameter counts reach trillion-level (1T), it is more useful and practical to compression the total parameters. 4.4 Ablation Study on Activation Functions In Eq(4), we apply non-linear activation function to the linear combination of basis matrices to enhance representational capacity. To determine the most suitable activation function, we conduct experiments on the gate matrices of the Qwen3-30B-A3B model. As shown in Figure 6, among those activation functions, Sigmoid demonstrates inferior performance to the case without activation (i.e., purely linear combination of basis matrices) in terms of the reconstruction MSE, while ReLU has an order-of-magnitude higher MSE loss. This result is consistent with our analysis in Section 3.3. GELU, SiLU, and Tanh activations achieve similar results and outperform the case without activation significantly, while we finally choose SiLU and Tanh as 9 Figure 6: Comparison of per-layer MSE loss for compressing the gate matrices of Qwen3-30B-A3B when using different activation functions. Figure 7: Comparison of per-layer MSE loss for compressing the gate matrices of Qwen3-30B-A3B with- /without Z-score normalization. our activation function as they offer more favorable trade-off between performance and computational efficiency. 4.5 Ablation Study on Z-score Normalization To evaluate the impact of the Z-score normalization technique introduced in Section 3.4, we conduct an ablation study using the gate matrices of the Qwen3-30B-A3B model. All experiments are conducted under identical hyperparameter and optimization settings, varying only the application of normalization. The results, as shown in Figure 7, demonstrate notable reduction in MSE loss when Z-score normalization is applied. We hypothesize that the normalization can rescale the weight values from wide and wild ranges to normal distribution with mean of 0 and std of 1, so that the optimization becomes more stable and effective."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose the Mixture-of-Basis-Experts (MoBE), parameter-efficient architecture designed to address the memory requirement challenges during deployment for large-scale MoE-based LLMs. MoBE effectively combines shared basis matrices with expert-specific transformation matrices in rank decomposition manner to mitigate key limitations of prior work. Extensive experiments demonstrate that MoBE outperforms existing counterpart methods like MoLAE and D2-MoE with large margin in preserving higher performance and better model compression rate. MoBE can compress leading models such as Qwen3-235B-A22B-2507, DeepSeek-V3-0324 and Kimi-K2-Instruct by up to 24%-30% while retaining up to 98% of their original performance across diverse benchmarks. Such practical and effective method may help enable large MoE models for more scalable and efficient applications. Limitations: While our method performs well in compressing MoE models, it still causes slight drop in accuracy compared to the original model. To fix this gap, one potential direction is to employ full network knowledge distillation (KD) between the original and our compressed models. This requires modifying existing training frameworks to support KD training for large LLMs. Another limitation is that MoBE requires multiple times calling of current optimized kernel fused-MoE to mimic the factorization, which is relatively inefficient. Hence, it requires implementing specific mega-kernel for the whole factorization to unleash the power of the MoBE architecture. Future work will address these two limitations."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021. URL https://api.semanticscholar.org/CorpusID:237142385. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts. arXiv preprint arXiv:2407.06204, 2024. Federico Cassano, John Gouwar, Daniel Nguyen, Sy Duy Nguyen, Luna Phipps-Costin, Donald Pinckney, 10 Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. Multipl-e: scalable and extensible approach to benchmarking neural code generation. 2022. URL https://api.semanticscholar.org/CorpusID:254854172. Marmik Chaudhari, Idhant Gulati, Nishkal Hundia, Pranav Karra, and Shivam Raval. Moe lens - an expert is all you need. In Sparsity in LLMs (SLLM): Deep Dive into Mixture of Experts, Quantization, Hardware, and Inference, 2025. URL https://openreview.net/forum?id=GS4WXncwSF. I-Chun Chen, Hsu-Shen Liu, Wei-Fang Sun, Chen-Hao Chao, Yen-Chang Hsu, and Chun-Yi Lee. Retrainingfree merging of sparse moe via hierarchical clustering. 2024. URL https://api.semanticscholar.org/ CorpusID:273323490. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. URL https://api.semanticscholar.org/CorpusID:3922816. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Mor Geva, R. Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. ArXiv, abs/2012.14913, 2020. URL https://api.semanticscholar.org/CorpusID:229923720. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In International Conference on Artificial Intelligence and Statistics, 2011. URL https://api.semanticscholar.org/CorpusID: 2239473. Gene Golub and Charles Van Loan. Matrix computations. JHU press, 2013. Hao Gu, Wei Li, Lujun Li, Qi Zhu, Mark Lee, Shengjie Sun, Wei Xue, and Yi-Ting Guo. Delta decompression for moe-based llms compression. ArXiv, abs/2502.17298, 2025. URL https://api.semanticscholar.org/ CorpusID:276575054. En hao Liu, Junyi Zhu, Zinan Lin, Xuefei Ning, Matthew B. Blaschko, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Efficient expert pruning for sparse mixture-of-experts language models: Enhancing performance and reducing inference costs. ArXiv, abs/2407.00945, 2024. URL https://api.semanticscholar.org/CorpusID:270869609. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016. URL https://api.semanticscholar.org/CorpusID:125617073. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020. URL https://api.semanticscholar.org/CorpusID:221516475. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv, abs/2103.03874, 2021. URL https://api.semanticscholar.org/CorpusID:232134851. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las Casas, Lisa Anne Hendricks, Johannes Welbl, and Aidan Clark. Training compute-optimal large language models. 2022. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322, 2023. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of 11 large language models for code. ArXiv, abs/2403.07974, 2024. URL https://api.semanticscholar.org/ CorpusID:268379413. Michael Jordan and Robert Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. 2020. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1:541551, 1989. URL https://api.semanticscholar.org/CorpusID:41312633. Jaeseong Lee, Seung won Hwang, Aurick Qiao, Daniel F. Campos, Zhewei Yao, and Yuxiong He. Stun: Structured-then-unstructured pruning for scalable moe pruning. ArXiv, abs/2409.06211, 2024. URL https://api.semanticscholar.org/CorpusID:272550518. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023a. Pingzhi Li, Zhenyu (Allen) Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit Bansal, and Tianlong Chen. Merge, then compress: Demystify efficient smoe with hints from its routing policy. ArXiv, abs/2310.01334, 2023b. URL https://api.semanticscholar.org/CorpusID:263605809. Wei Li, Lujun Li, You-Liang Huang, Mark G. Lee, Shengjie Sun, Wei Xue, and Yike Guo. Structured mixtureof-experts LLMs compression via singular value decomposition, 2025. URL https://openreview.net/ forum?id=ho7ZUS1z8A. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Zehua Liu, Han Wu, Ruifeng She, Xiaojin Fu, Xiongwei Han, Tao Zhong, and Mingxuan Yuan. Molae: Mixture of latent experts for parameter-efficient language models. 2025. URL https://api.semanticscholar.org/ CorpusID:277451683. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. URL https://api.semanticscholar.org/CorpusID:53592270. Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, and Hongsheng Li. Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models. In Annual Meeting of the Association for Computational Linguistics, 2024. URL https://api.semanticscholar. org/CorpusID:267782440. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. In Neural Information Processing Systems, 2022. URL https://api.semanticscholar.org/CorpusID: 255825985. Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions. ArXiv, abs/1710.05941, 2018. URL https://api.semanticscholar.org/CorpusID:10919244. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. ArXiv, abs/2311.12022, 2023. URL https://api.semanticscholar.org/CorpusID:265295009. David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by backpropagating errors. Nature, 323:533536, 1986. URL https://api.semanticscholar.org/CorpusID: 205001834. 12 Zhihong Shao, Damai Dai, Daya Guo, Bo Liu (Benjamin Liu), Zihan Wang, and Huajian Xin. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. ArXiv, abs/2405.04434, 2024. URL https://api.semanticscholar.org/CorpusID:269613809. Noam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020. URL https://api. semanticscholar.org/CorpusID:211096588. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adri`a Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, , et al. Kimi k2: Open agentic intelligence, 2025a. URL https://arxiv.org/abs/2507.20534. Ling Team, Binwei Zeng, Chao Huang, Chao Zhang, Changxin Tian, Cong Chen, Dingnan Jin, Feng Yu, Feng Zhu, Feng Yuan, et al. Every flop counts: Scaling 300b mixture-of-experts ling llm without premium gpus. arXiv preprint arXiv:2503.05139, 2025b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Tong Tong Wu and Kenneth Lange. Coordinate descent algorithms for lasso penalized regression. 2008. Yanyue Xie, Zhi Zhang, Ding Zhou, Cong Xie, Ziang Song, Xin Liu, Yanzhi Wang, Xue Lin, and An Xu. Moe-pruner: Pruning mixture-of-experts large language model using the hints from its router. ArXiv, abs/2410.12013, 2024. URL https://api.semanticscholar.org/CorpusID:273375561. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, and Bo Yuan. Moe-i²: Compressing mixture of experts models through inter-expert pruning and intra-expert low-rank decomposition. ArXiv, abs/2411.01016, 2024. URL https://api.semanticscholar.org/CorpusID: 273811289. Zeliang Zhang, Xiaodong Liu, Hao Cheng, Chenliang Xu, and Jianfeng Gao. Diversifying the expert knowledge for task-agnostic pruning in sparse mixture-of-experts. ArXiv, abs/2407.09590, 2024. URL https://api.semanticscholar.org/CorpusID:271212712. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. ArXiv, abs/2311.07911, 2023. URL https://api.semanticscholar.org/CorpusID:265157752. Appendix"
        },
        {
            "title": "A Absolute performance comparison of MoE compression methods",
            "content": "Figure 8: Absolute performance comparison of different MoE compression methods. The accuracy are averaged over 15 benchmarks as shown in Table 3. Applying D2-MoE to large models like Qwen3-235BA22B-2507, DeepSeek-V3-0324 and Kimi-K2-Instruct is computationally prohibitive on an 8x H100 GPU machine; therefore, it is excluded from these comparisons. MoBE is evaluated at compression rates similar to or higher than the baseline methods (MoLAE, D2-MoE). We present the absolute performance comparison of MoE compression methods in Figure 8."
        },
        {
            "title": "B Analysis of the Effective Rank of Expert Weight Matrices",
            "content": "We evaluate the effective rank of expert weight matrices in Qwen3-235B-A22B-2507, DeepSeek-V3-0324, and Kimi-K2-Instruct. The effective rank re is defined as: (cid:40) re = min N+ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) r i=1 σ2 i=1 σ2 (cid:41) > 0.95 where σi is the i-th largest singular value (sorted in descending order) and is the matrix rank. The expert weight matrices in Qwen3-235B-A22B-2507 have dimensions 40961536, while those in DeepSeek-V3-0324 and Kimi-K2-Instruct are 71682048. Figures 911 illustrate the per-layer average effective rank re and its range for each model. Taking the expert weight matrices of Kimi-K2-Instruct as an example, rank decomposition could realize parameter compression only if the intermediate rank satisfies rt 7168 2048 7168 + 1593. However, according to Figure 11, the average effective rank re is larger than 1593 in most layers. This discrepancy implies that the pure rank-decomposition-based method cant produce model compression without performance loss. An interesting finding can be drawn from the analysis: Qwen3-235B-A22B-2507 shows much broader effective rank range than the other two, which may indicate that its experts are far from being well-balanced during the training phase."
        },
        {
            "title": "C Additional MSE Comparisons",
            "content": "We present comparison of reconstruction errors on Qwen3-235B-A22B-2507, DeepSeek-V3-0324 and KimiK2-Instruct in the Figure 12-14. 14 (a) Gate matrices (b) Up matrices (c) Down matrices Figure 9: Average effective rank and effective rank range of the (a) gate, (b) up, and (c) down matrices at each layer in Qwen3-235B-A22B-2507. (a) Gate matrices (b) Up matrices (c) Down matrices Figure 10: Average effective rank and effective rank range of the (a) gate, (b) up, and (c) down matrices at each layer in DeepSeek-V3-0324. (a) Gate matrices (b) Up matrices (c) Down matrices Figure 11: Average effective rank and effective rank range of the (a) gate, (b) up, and (c) down matrices at each layer in Kimi-K2-Instruct. 15 (a) Gate matrices (b) Up matrices Figure 12: Comparison of pre-layer MSE for compressing the gate (a) and up (b) matrices of Qwen3-235BA22B-2507 using MoBE, D2-MoE and MoLAE. (a) Gate matrices (b) Up matrices Figure 13: Comparison of pre-layer MSE for compressing the gate (a) and up (b) matrices of DeepSeek-V30324 using MoBE, D2-MoE and MoLAE. (a) Gate matrices (b) Up matrices Figure 14: Comparison of pre-layer MSE for compressing the gate (a) and up (b) matrices of Kimi-K2-Instruct using MoBE, D2-MoE and MoLAE."
        }
    ],
    "affiliations": [
        "Inclusion AI",
        "Renmin University of China",
        "Westlake University"
    ]
}