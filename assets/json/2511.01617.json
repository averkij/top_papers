{
    "paper_title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers",
    "authors": [
        "Mohamed Eltahir",
        "Ali Habibullah",
        "Lama Ayash",
        "Tanveer Hussain",
        "Naeemullah Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is a long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates' representations. This work introduces Vote-in-Context (ViC), a generalized, training-free framework that re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLM's prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, a compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as a single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as a simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: https://github.com/mohammad2012191/ViC"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 7 1 6 1 0 . 1 1 5 2 : r Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers Mohamed Eltahir1 Ali Habibullah1 Lama Ayash1,2 Tanveer Hussain3* Naeemullah Khan1* 1 King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia 2 Department of Computer Science, King Khalid University (KKU), Abha, Saudi Arabia 3 Department of Computer Science, Edge Hill University, Ormskirk, England {mohamed.hamid, ali.habibullah, lama.ayash}@kaust.edu.sa hussaint@edgehill.ac.uk, naeemullah.khan@kaust.edu.sa Figure 1. Left: R@1 for T2V/V2T on MSR-VTT, DiDeMo, VATEX, and ActivityNet versus strong baselines. Right: Qualitative example where multi-retriever outputs are fused and re-ranked (ViC) to obtain the final list."
        },
        {
            "title": "Abstract",
            "content": "In the retrieval domain, candidates fusion from heterogeneous retrievers is long-standing challenge, particularly for complex, multi-modal data such as videos. While typical fusion techniques are training-free, they rely solely on rank or score signals, disregarding candidates representations. This work introduces Vote-in-Context (ViC), generalized, training-free framework that re-thinks list-wise reranking and fusion as zero-shot reasoning task for Vision-Language Model (VLM). The core insight is to serialize both content evidence and retriever metadata directly within the VLMs prompt, allowing the model to adaptively weigh retriever consensus against visual-linguistic content. We demonstrate the generality of this framework by applying it to the challenging domain of cross-modal video retrieval. To this end, we introduce the S-Grid, compact serialization map that represents each video as an image grid, optionally paired with subtitles to enable list-wise reasoning over video candidates. ViC is evaluated both as single-list reranker, where it dramatically improves the precision of individual retrievers, and as an ensemble fuser, where it consistently outperforms strong baselines like CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the framework establishes new state-of-the-art zero-shot retrieval performance, demonstrating its effectiveness in handling complex visual and temporal signals alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1% (t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive gains of up to +40 Recall@1 over previous state-of-the-art baselines. We present ViC as simple, reproducible, and highly effective recipe for turning modern VLMs into powerful zero-shot rerankers and fusers. Code and resources are publicly available at: https: //github.com/mohammad2012191/ViC * Principal Investigator (PI) 1. Introduction The digital age is characterized by an exponential growth in complex data. This includes vast repositories of unstructured text, which are central to modern applications like Retrieval-Augmented Generation (RAG) [1], as well as complex multimodal data, such as video, which integrates visual, auditory, and temporal signals [2]. The sheer volume and variety of this data make efficient organization, retrieval, and analysis increasingly difficult [3]. To address this, modern retrieval systems seek to align natural language queries with semantically relevant content, enabling users to efficiently locate desired material within large-scale data repositories. Despite significant progress, this task remains challenging due to the complexity of the data itself, such as high dimensionality or temporal structure, and the queries, such as sparsity or ambiguity. Considering this complexity, two-stage retrieval paradigm is commonly adopted [4]. In the first stage, computationally efficient retriever, such as dual-encoder, retrieves broad pool of candidate items. In the second stage, more powerful yet computationally expensive re-ranker refines this shortlist to enhance precision. This two-stage pipeline has become standard framework in both text retrieval and retrieval-augmented generation [1, 5, 6]. Furthermore, two-stage pipelines enable using multiple diverse retrievers as first stage. Fusing their results based on ranks or scores, using Reciprocal Rank Fusion (RRF) [7] or CombSUM/CombMNZ [8], respectively, is common technique that typically offers significant performance gains [9]. However, applying this two-stage template to complex, multimodal data is non-trivial, revealing limitations in the second stage. First-stage retrievers, while computationally efficient, typically rely on global embeddings and may rank irrelevant candidates highly because they fail to capture or verify all query-specific details. second stage is therefore essential, but it presents two key challenges. First, conventional rerankers for single list are often costly, require fine-tuning on in-domain data, or are tied to specific retrievers features [10]. Second, when ensembling multiple retrievers, conventional fusion methods are content-blind, as they operate only on rank/score signals while ignoring the candidates rich content. These limitations motivate the need for universal, training-free framework capable of acting as both content-aware reranker and fuser. Recent advances in large-scale, instruction-following In text reLanguage models offer promising solution. trieval, Large Language Models (LLMs) have proven to be powerful zero-shot listwise rerankers, as seen in work like RankGPT [5]. This paradigm extends to Vision-Language Models (VLMs), such as InternVL 3.5 [11] and QwenVL [12], which demonstrate strong zero-shot reasoning and cross-modal alignment capabilities. By adapting videos into format interpretable by VLMs, these models can themselves serve as powerful zero-shot relevance estimators. To this end, we introduce Vote-in-Context (ViC), generalized, training-free framework that utilizes frozen VLM as universal, list-wise reranker and fuser. Instead of collapsing ranked lists with fixed formula, such as Reciprocal Rank Fusion (RRF), ViC serializes both content evidence (such as images, text) and retriever metadata (such as, per-list ranks, cross-list multiplicity) directly into the VLMs prompt, allowing it to adaptively weigh all signals. In this paper, we apply ViC to video retrieval. We propose the S-Grid, compact content serialization map that represents video as single image grid of uniformly sampled frames, optionally paired with subtitles. This S-Grid acts as the VLM-readable content evidence for each video candidate. The framework operates in two modes. First, as powerful single-list reranker (M = 1), where ViC uses S-Grids to re-evaluate the top-K items from one retriever. Second, as novel ensemble fuser (M > 1), where ViC constructs candidate list by interleaving multiple retrievers. This assembly explicitly encodes rank and consensus metadata in the list order and item multiplicity, allowing the VLM to weigh these signals jointly with the S-Grid content evidence. The experiments show this combination yields massive gains, saturating several benchmarks in zero-shot settings."
        },
        {
            "title": "The main contributions of this work are summarized as",
            "content": "follows: We propose Vote-in-Context (ViC), generalized, training-free framework that turns frozen VLM into powerful list-wise reranker and fuser by serializing both content and retriever metadata into its prompt. We introduce the S-Grid, compact and effective video representation that serves as the content serialization map for ViC, enabling VLM-based reasoning over video without costly sequence processing. We comprehensively evaluate ViC in both its = 1 (single-list) and > 1 (fusion) modes. We show that ViC as reranker (M = 1) dramatically improves all single backbones, and ViC as fuser (M > 1) consistently outperforms strong baselines like RRF and CombSUM. We release our framework and evaluation protocols, including an extensive analysis of ViCs scaling properties, its sensitivity to context size, and the performance of different assembly strategies. This paper is organized as follows. Section 2 reviews related work. Section 3 details the proposed ViC framework and its application for video retrieval. Section 4 presents the experimental results and ablation studies, followed by discussion of the framework-s limitations and future directions. Figure 2. The Vote-in-Context (ViC) framework. VLM Reranker jointly weighs serialized content (Q(), E()) against retriever metadata (rank, multiplicity) encoded in the Candidate Sequence C(q) by Duplicate-Aware Interleaving step to produce the final ranking (cid:98)R(q). 2. Related Work Modern video retrieval has evolved from early architectures that coupled temporal attention mechanisms with language encoders [13, 14] to large-scale unified pretraining [15, 16]. CLIP-style adaptations, which transfer powerful imagetext encoders to video, such as CLIP4Clip [17] and XCLIP [18], became dominant paradigm for zero-shot retrieval. Recent foundation-scale systems have pushed recall even further by incorporating broader multi-modality, such as audio/subtitles in VAST [19], and larger, video-specific backbones, such as InternVideo2 [20]. These models serve as the first-stage retrievers in our work. However, they primarily rely on matching coarse, global representations. While this is computationally efficient for rapidly narrowing large search space to high-recall candidate set, this reliance on coarse similarity means they can struggle to capture fine-grained, query-specific details, often leading to imprecise top rankings. Building upon these first-stage retrievers, subsequent research has explored two-stage architectures that refine coarse candidate sets. When multiple first-stage lists are available, they must be fused. Classical fusion methods operate at the score level, such as CombSUM/CombMNZ [8] or the rank level, such as Reciprocal Rank Fusion (RRF) [7]. These methods are simple, robust, and widely used baselines for aggregating ranked lists, making them key points of comparison for our fusion method. However, despite their efficiency, such methods assume fixed weighting formula and hyperparameters, such as RRF-s k, and operate solely on rank or score signals, leaving other modalities unexploited. The emergence of large language models (LLMs) and Vision-Language Models (VLMs) has introduced new paradigm for re-ranking in retrieval systems. LLMs have recently demonstrated strong zero-shot, list-wise re-ranking capabilities in text retrieval, achieving substantial performance gains by reasoning jointly over ranked lists of passages [5, 21, 22]. Adapting this paradigm to video, however, is non-trivial as VLMs cannot process raw videos. To overcome this, several studies have shown that representing video clip as grid of sampled frames enables imagecentric VLMs to reason effectively about temporal dynamics [23]. At the same time, modern instruction-following VLMs, such as InternVL [11] and Qwen-VL [12], provide the robust zero-shot, multimodal alignment required to make such designs practical. 3. Methodology This paper introduces Vote-in-Context (ViC), general, training-free, and multimodal framework that utilizes the VLM reasoning capabilities discussed in 2 to solve the ranked-list fusion problem. Rather than collapsing lists with fixed formula, ViC provides uniform candidate prompt to the VLM containing both: (a) content evidence (such as images/text), and (b) retriever metadata, including ranks and cross-list multiplicity encoded directly in the prompt. Figure 3. The Vote-in-Context (ViC) framework applied for Text-to-Video (t2v, top) and Video-to-Text (v2t, bottom). The left block shows the initial retrieval stage. The right block (green) shows our ViC framework. The serialization maps (Q(), E()) are modalitydependent: S-Grid Sampling is applied to video inputs, while text inputs use the identity. This approach stands in contrast to classical, non-contentaware fusion methods (such as RRF or CombSUM), which operate only on rank/score signals and ignore candidate content. The VLM receives this meta-signal alongside the candidates content and implicitly weighs retriever metadata versus content evidence on per-query basis in zero-shot setting. candidates rank is conveyed by each lists order, while cross-list consensus is represented by allowing duplicates to appear in the candidate set. Compared to the traditional fusion methods, ViC is hyperparameter-free and modality-aware, yielding per-query decisions that adaptively weight all available signals. The idea is modalityagnostic, requiring only that candidates can be serialized into VLM-readable prompt (such as passages for text search, images with metadata, tables, or audio transcripts). To demonstrate this frameworks generality, ViC is applied to video retrieval as second-stage fuser and reranker. The framework fuses candidate results from multiple first-stage retrievers and serializes each video into uniform visual-linguistic representation, termed the S-Grid. The VLM is subsequently employed to produce list-wise permutation of the candidate set. Both text-to-video (t2v) and video-to-text (v2t) retrieval tasks are evaluated within two-stage pipeline consisting of dual-encoder recall followed by ViC-based re-ranking. 3.1. Problem Setup and Notation The ViC fusion framework is formalized as follows. Let denote the universe of candidate items (such as videos or text passages). For given query q, assume access to retrievers, = {1, . . . , }. Each retriever returns ranked list of items drawn from : Lm(q) = (cid:0)xm,1, xm,2, . . . , xm,nm ViC aggregates the ranked lists into single fused ranking of target length K. (cid:1), where xm,j . Candidate Assembly and Metadata Encoding. The process begins by constructing single candidate sequence C(q) of length K. This sequence retains both the rank and multiplicity metadata from the initial retrieval lists. Define per-list depth as kmax = K/M , and truncate each list accordingly before assembling the final candidate sequence. Topkmax(Lm) = (cid:0)xm,1, . . . , xm,min(kmax,nm) The candidate sequence C(q) is formed by round-robin (RR) interleaving of these truncated lists, preserving dupli- (cid:1). cates: C(q) = RRK (cid:0) Topkmax(L1), . . . , Topkmax(LM )(cid:1). items The RRK() operator appends in the order (cid:0)x1,1, x2,1, . . . , xM,1, x1,2, . . . (cid:1), skipping any exhausted lists, and truncates the final sequence to length K. This sequence C(q) = (C1, . . . , CK) inherently encodes retriever metadata: per-list rank is signaled by position, and cross-list consensus is signaled by an items multiplicity, µC(x) = (cid:80)K i=1 1{Ci = x}. VLM Re-ranking. The sequence is passed to frozen, list-wise VLM gΘ for reranking. Let E() be the content serialization map that converts an item into its VLMreadable format (i.e., the content evidence), and let Q(q) be the serialized query. The VLM computes permutation ˆπ SK, where SK is the set of all permutations of the indices {1, . . . , K}: (cid:16) Q(q), (cid:0)E(C1), E(C2), . . . , E(CK)(cid:1)(cid:17) ˆπ = gΘ . The final fused and reranked output (cid:98)R(q) is the sequence reordered by this permutation: (cid:98)R(q) = (cid:0)Cˆπ(1), Cˆπ(2), . . . , Cˆπ(K) (cid:1). See Fig. 2 for high-level overview. Special Case: Single-List Reranking (M = 1). The ViC framework naturally handles the standard single-list reranking task as special case. When = 1, the roundrobin interleaving simplifies, and the candidate sequence C(q) becomes the standard top-K list from the single retriever: C(q) = TopK(L1(q)) = (x1,1, . . . , x1,K). The VLM call and final output (cid:98)R(q) remain identical. In this = 1 setting, ViC functions as pure list-wise reranker. The VLMs decision is based solely on the content evidence E() of the candidates relative to the query, as the retriever metadata signals (cross-list multiplicity and rank-of-ranks) are absent. 3.2. Applying ViC to Video Retrieval. Applying ViC to video retrieval requires method to serialize video candidates into VLM-readable format. This section first defines this video representation, the S-Grid, and then maps it to the ViC framework. Figure 4. The S-Grid representation. S-Grid: Uniform Video Prompt. video is represented as regular grid of uniformly sampled frames composited into single image, optionally paired with subtitle or Automated Speech Recognition (ASR) string av (if available). Let denote the grid dimension (i.e., the grid has cells). Given video length frames, s2 frame indices {ti}s2 s21 . These frames are extracted, resized to H/s W/s, and tiled in row-major order to form an canvas, denoted as Grid(v; s). When subtitle av is available, it is concatenated to the textual prompt as an auxiliary input. This representation, visualized in Fig. 4, is denoted as: i=1 are selected uniformly via ti = (i1) S-Grid(v) = (cid:0)Grid(v; s), av (cid:1), This design provides the VLM with both visual snapshots and audio transcripts within single prompt. Such uniform interface enables single VLM to process candidates retrieved from any upstream model. Formalizing the Video Retrieval Tasks. The ViC framework is applied to cross-modal video retrieval, where the candidate universe consists of videos and text captions . In the t2v retrieval task, the query is text (Q(q) = q), and the candidates Ci are videos, which are serialized as follows: E(v) = (cid:0)S-Grid(v), av In the v2t retrieval task, the query is video serialized as Q(q) = (cid:0)S-Grid(v), av (cid:1), and the candidates Ci are text captions, so the content map is the identity (E(t) = t). This bidirectional retrieval process is illustrated in Fig. 3. (cid:1). Cost. The re-ranker processes one image per video candidate and short text block per item. The complexity per query is O(K CVLM) where is the number of candidates, and CVLM is one forward pass cost. This cost is independent of the raw video length, as each video is represented by single image, keeping the per-candidate cost effectively constant. The approach is significantly lighter than framelevel cross-attention and permits larger candidate sets to be evaluated within the VLMs context window. 3.3. List Fusion Strategies Given off-the-shelf retrievers that produce ranked lists for query, two standard list-fusion baselines are examined and compared against the proposed ViC. (a) Soft Voting (score fusion). When calibrated similarity matrices are available, normalize each score distribution per query using min-max scaling and aggregate the results with nonnegative weights: S(q, ) = (cid:88) m=1 wm norm(cid:0)S(m)(q, )(cid:1), = TopK(cid:0) S(q, )(cid:1). This family includes classical CombSUM/CombMNZ-style score fusion and serves as strong yet simple baseline when scores are comparable across retrieval systems. (b) Reciprocal Rank Fusion (RRF). When only heterogeneous ranked lists are available, RRF assigns each item fused score as RRF(x) = (cid:88) m=1 1 + rankm(x) , with small smoothing constant (commonly k=60), then returns the Top-K unique items. (c) Ours: Vote-in-Context (ViC). As formally defined in 3.1, the ViC framework defers the fusion logic to the VLM itself. Rather than collapsing ranked lists into single aggregated score, as in Soft Voting or RRF, ViC serializes both the content evidence E() and the retriever metadata (rank, multiplicity) directly into the VLM prompt. This design allows the frozen VLM to adaptively weigh all available signals on per-query basis, thereby functioning as trainingfree, multimodal fusion model. The serialization process also provides practical control mechanisms. round-robin assembly based on kmax ensures balanced coverage across all retrievers, while the candidate sequence C(q) can be optionally reordered to bias the VLMs early context, by prioritizing items from stronger backbones, for instance. Such flexibility is inherently absent from fixed-formula fusion methods. 4. Experiments 4.1. Benchmarks and Protocol is conducted Evaluation the MSR-VTT [24], DiDeMo [25], ActivityNet Captions [26], and VATEX [27] benchmarks, following the standard retrieval protocols established in prior work. Notably, only MSR-VTT and on VATEX provide subtitles, which are incorporated into the S-Grid representation where applicable. On MSR-VTT, the standard 1k-A split is used. For DiDeMo, evaluation is performed at the video level by pooling the moment annotations into single retrieval target per video. ActivityNet Captions is evaluated using the official validation split for retrieval. For VATEX, the community 1.5k test subset is adopted. Out of the intended 1,500 videos from prior work, only 1,252 were successfully recovered due to the online unavailability of some videos. To ensure fair comparison, captions were re-indexed to this fixed subset, and all baselines and the proposed method were reproduced on the same 1,252 test videos. All evaluation items correspond to test-only instances, and the final video list is publicly released to facilitate reproducibility. Only msrvtt and vatex has subtitles. 4.2. Implementation Details The first-stage retrievers are CLIP4Clip[17], VAST[19], GRAM [28], and InternVideo2-6B [20]. CLIP4Clip is canonical CLIP-style video retriever. VAST provides omnimodality pretraining. GRAM is strong global-regional baseline. InternVideo2-6B serves as the strongest recent baseline. Each model is reproduced or re-evaluated using official checkpoints and released evaluation configurations, and all retrievers are kept frozen during experimentation. Tokenization, frame sampling, and text preprocessing strictly follow the original repository implementations to ensure consistency and reproducibility. InternVL 3.5 38B [11] is employed as the main trainingfree VLM reranker. It consumes S-Grid inputs along with the video/text query and is used in zero-shot setting without any dataset-specific fine-tuning. Unless otherwise noted, the same candidate counts are used for each comparison. The standard ensemble configuration fuses all backbones except VAST, as this combination yielded the highest performance on average. notable exception occurs in VATEX, where the ensemble includes only InternVideo2 and VAST, as these were the models successfully reproduced for this benchmark. 4.3. Metrics and Hyperparameter Results are reported using Recall@1 (R@1), the proportion of queries for which the top-ranked result is correct, for both t2v and v2t directions. For t2v, the ViC framework receives = 14 candidate S-Grids per query, while for v2t, it receives = 20 candidate captions, unless stated otherwise. The default S-Grid size is 3 3 frames. For the Soft Voting baseline, similarity scores are min-max normalized per query (row) before aggregation with uniform weights. For ViC ensemble fuser (M > 1), candidate lists are assembled by interleaving each retrievers list up to depth kmax, preserving duplicates. The VLM output is parsed into perBackbone Reranker Input MSR-VTT DiDeMo ActivityNet VATEX t2v v2t t2v v2t t2v v2t t2v v2t BASELINES (NO RERANKING) CLIP4Clip VAST GRAM InternVideo2-6B None None None None 34.4 49.9 53.1 54.5 29.9 46.2 50.8 49.5 27.1 51.0 51.8 59.2 20.3 47.8 49.6 58. 21.6 50.2 61.1 58.2 20.3 48.7 52.1 52.4 VAST CLIP4Clip WITH VIC SINGLE-LIST RERANKING (M = 1) (InternVL 3.5 38B, Grid Size 3x3) 62.8 64.2 67.3 68.7 75.4 76.2 74.0 75.9 Grid S-Grid Grid S-Grid Grid S-Grid Grid S-Grid 60.4 70.2 70.9 78.1 53.8 63.4 63.9 70.7 64.6 79.7 82.4 89.8 61.3 62.5 62.2 63.1 72.3 73.6 74.1 76.6 InternVideo2-6B GRAM 62.8 75.2 77.2 84.9 77.0 77.3 80.7 91.9 92.4 95.5 95.8 77.6 72.5 99.4 99.6 Table 1. Zero-shot t2v and v2t retrieval: R@1 for single backbones without reranking vs. with ViC as single-list reranker (M = 1). Bold indicates the best result for each benchmark. Method MSR-VTT DiDeMo ActivityNet VATEX t2v v2t t2v v2t t2v v2t t2v v2t BASELINE & TRADITIONAL FUSION METHODS InternVideo2 (Prev. SOTA) RRF CombSUM CombMNZ 49.5 80.2 83.0 86.9 54.5 78.3 84.4 85.3 59.2 72.8 80.4 78.0 OUR VLM-BASED RERANKING METHOD ViC (No Duplicates) 80.7 88.1 ViC 84.2 87.1 85.5 87.4 58.8 73.2 83.1 80.8 76.1 84.3 58.2 96.8 95.8 95.0 94.8 96. 52.4 97.4 95.2 92.2 91.9 96.2 80.7 94.7 96.1 96.4 96.1 97.5 Table 2. Zero-shot t2v and v2t retrieval with ensemble fusion methods. All metrics are R@1. Bold indicates the best result for each benchmark. mutation, with the identity mapping used as fallback in very rare cases. The resulting ranked list (cid:98)R(q) may include duplicate candidates; however, only the highest-ranked instance of each is considered during evaluation, consistent with standard practice. 5. Results 5.1. ViC as Single-List Reranker (M = 1) ViC is first evaluated in its simplest form as single-list reranker (M = 1), as defined in 3.1. In this setting, the VLM reranks the top-K candidates from single retriever, using only content evidence (S-Grids and subtitles) without any cross-list fusion metadata, as presented in Table 1. Applying ViC reranking to single backbone yields substantial and consistent R@1 improvements across all datasets and models. For example, on MSR-VTT (t2v), ViC lifts the weakest backbone (CLIP4Clip) by 29.8 points (increases from 34.4 to 64.2) and the strongest (InternVideo2) by 21.4 points (increases from 54.5 to 75.9). On ActivityNet (t2v), the gains are even larger, adding 31.6 R@1 to InternVideo2 (increases from 58.2 to 89.8). On VATEX (v2t), ViC boosts VAST by 22.0 points (increases from 77.6 to 99.6), achieving near-saturation in R@1 performance. These results demonstrate that, even without fusion, the VLM performs highly effective list-wise reasoning over SGrid content evidence, providing training-free mechanism to correct the coarse similarity biases of dual-encoder retrievers. Moreover, comparison between Grid (visuals only) and S-Grid (visuals and subtitles) configurations shows that incorporating textual evidence consistently enhances performance, confirming that the VLM effectively utilizes all available modalities during re-ranking. Figure 5. Efficiency vs. Performance Trade-off. Time per query vs. Avg Recall@1 for t2v retrieval over the benchmarks MSR-VTT, DiDeMo and ActivityNet in zero-shot settings. Marker size represents model parameters. The Pareto frontier highlights optimal trade-offs. Latency is measured on single NVIDIA A100 80GB GPU, averaged over 50 queries for 1k video retrieval task. Figure 6. InternVideo2-6B and InternVL 3.5-38B. (a) Effect of reranker scale (InternVL 3.5, 33 grid) on t2v Recall@1. (b) Impact of grid size on t2v performance, using 5.2. ViC as an Ensemble Fuser (M > 1) The full ViC framework is evaluated as an ensemble fuser (M > 1), utilizing both content evidence and retriever metadata (rank, multiplicity). detailed comparison between ViC fusion and traditional fusion baselines (RRF, CombSUM, and CombMNZ) is summarized in Table 2 ViC consistently outperforms all traditional fusion methReranker T2V grids per query V2T captions per query 10 14 30 10 20 InternVL 3.5 38B Qwen3-VL 30B (A3B) Gemma-3 27B IT R@1/R@10 73.8 / 82.7 76.5 / 82.7 76.2 / 82.7 R@1/R@10 74.0/ 83.8 77.0 / 84.7 76.7 / 84.8 R@1/R@10 71.3 / 84.5 77.0 / 86.5 73.3 / 88.1 R@1/R@10 75.8 / 85.3 59.5 / 85.3 75.8 / 85.3 R@1/R@10 74.1 / 89.0 55.2 / 84.2 71.2 / 90. R@1/R@10 70.0 / 90.8 51.8 / 85.4 69.3 / 91.5 Table 3. Reranker type and context size in one view. Left: T2V vs. grids per query. Right: V2T vs. captions per query. ods across nearly all benchmarks. On MSR-VTT (t2v), ViC achieves 87.1 R@1, surpassing the best baseline (CombMNZ) by +1.8 points. On DiDeMo (t2v), the gain is most significant, where ViCs 87.4 R@1 is +7.0 points higher than the next-best baseline (CombSUM). On VATEX (t2v), ViC reaches 97.5 R@1, once again setting the highest overall performance. While RRF remains strong competitor on ActivityNet, ViC demonstrates substantially greater stability across the other datasets, where RRF and other score-level fusion methods exhibit notable performance fluctuations. Furthermore, Table 2 includes ViC (No Duplicates) ablation. This variant deduplicates the candidate sequence C(q) before passing it to the VLM, thus removing the multiplicity metadata. The resulting performance drop (such as 87.1 to 84.2 on MSR-VTT t2v) confirms that the VLM actively uses cross-list consensus as strong relevance signal. Finally, comparing the ViC fusion result (87.1 on MSRVTT, Table 2) with the best single-backbone re-ranking result (75.9 on MSR-VTT, Table 1) highlights the additive advantage of fusion. Re-ranking single model (ViC, = 1) yields +21.4 point improvement, while incorporating fusion (ViC, > 1) contributes an additional +11.2 points, underscoring the complementary strengths of the two components within the ViC framework. This consistent, stateof-the-art performance across all benchmarks is visualized in Figure 1. Moreover, Figure 5 contextualizes these performance It clearly shows that gains against their inference cost. ViCs reranking and fusion methods establish new, dominant Pareto frontier. While the original retrievers, such as InternVideo2, are fast, their performance is limited, clustering at the bottom-left. In contrast, ViC provides massive leap in average R@1, pushing the SOTA from 57% to 90%. This gain comes at the expected latency cost of secondstage reranker. However, the frontier itself shows promising scaling: the 8B and 14B models already achieve strong results, suggesting that the barrier to high performance is low and that future work on lightweight, fine-tuned rerankers could offer an even better performance-cost balance. 5.3. Ablation Studies 5.3.1. Grid size As ViC relies on content-derived evidence, the S-Grid constitutes the key visual representation driving its performance. Figure 6 (b) studies 1 1 to 4 4 grids. 2 2 and 3 3 are the sweet spots. 1 1 undercovers the video. 4 4 begins to compress each frame too aggressively and can introduce redundant visual tokens. This trend holds across the benchmarks that have been tested. Small grids are well matched to the evaluated datasets: MSR-VTT uses 10-30 clips, DiDeMo videos are about 25-30 s, and VATEX clips are around 10 s. ActivityNet Captions contains longer, untrimmed videos with average durations on the order of minutes, though, the reranker performs strongly. 5.3.2. Reranker scale Scaling the VLM within the ViC framework from 8B to 38B parameters at fixed 33 grid leads to steady improvement in R@1, which eventually saturates with increasing model size, as shown in Figure 6(a). Notably, even the 8B model achieves strong zero-shot re-ranking performance, whereas smaller models fail to produce consistent permutations. This result identifies 8B as the minimum effective scale for zero-shot list-wise re-ranking in the proposed ViC pipeline. The strong performance of the 8B model, even without training, suggests that lightweight finetuning could be very promising direction for developing highly efficient, much smaller rerankers. 5.3.3. VLM Type and Context size Varying the number of candidates supplied to the VLM per query within the ViC framework significantly influences retrieval performance, as summarized in Table 3. Preliminary analyses confirmed that R@30 is effectively saturated near 100% across benchmarks, indicating that the correct item is almost always retrieved within the top 30 candidates. However, the results indicate diminishing returns beyond moderate context size. For t2v retrieval, increasing from 10 to 14 yields higher R@1, but expanding to 30 causes R@1 to drop while providing only negligible improvement in R@10. In practice, most VLMs fail to effectively utilize the additional coverage at = 30, often exhibiting degraded discrimination accuracy due to overextended context. Qwen3-VL, for example, performs strongly on t2v retrieval but deteriorates substantially on v2t when the context window increases. Gemma-3 is an exception, maintaining stable performance at = 30 and achieving the highest R@10 in both directions. Nevertheless, InternVL 3.5 is employed in the main experiments owing to its consistent overall performance and the availability of multiple scale variants for systematic scaling analysis. For v2t, an input size of = 20 captions emerges as the most effective operating point, as performance plateaus or declines beyond this threshold. These observations collectively highlight the practical limitations of current VLMs effective context windows in list-wise relevance judgment tasks. 5.4. Discussion and Conclusion The results make compelling case for new fusion paradigm: ViC. Rather than relying on fixed formulas such as RRF or on trained fusers, ViC reconceptualizes fusion as zero-shot, list-wise reasoning task performed by VLM. This paradigm shift enables the model to adaptively balance retriever metadata, including rank and multiplicity, against content evidence on per-query basis, leading to more context-aware and robust fusion behavior. The practicality and effectiveness of the proposed framework are demonstrated in the challenging domain of video retrieval. By representing video as compact S-Grid, we make it feasible for VLM to process and re-rank an entire list of video candidates simultaneously. This representation maintains computational cost proportional to the number of candidates (K) rather than the raw video length, while still preserving temporal coverage and exploiting multimodal information from both visual and textual sources. The resulting efficiency yields substantial R@1 improvements, enhancing user-perceived retrieval quality under fixed latency constraints and ultimately achieving state-of-the-art performance across benchmarks. The limitations of the proposed approach can be categorized into those inherent to the ViC framework and those specific to its video-retrieval application. The ViC framework itself introduces three main tradeoffs. First, its inference cost is computationally expensive, replacing the near-zero arithmetic cost of RRF or CombSUM with full, list-wise VLM forward pass. Figure 5 plots this trade-off, showing that ViC establishes new Pareto frontier where it achieves significantly higher performance, albeit at higher latency cost than traditional baselines. Second, the framework is strictly bounded by the VLMs context window, as our ablations show performance can degrade when increases. Finally, VLM reliability is factor, as the frameworks fidelity depends entirely on the VLMs instruction-following capabilities. Results might be influenced by positional bias or fail to parse the list format, as we observed with models smaller than 8B. The video-retrieval application of ViC presents two additional limitations. First, the method is inherently recallbound: as with other two-stage retrieval systems, ViC cannot retrieve relevant candidate if it is absent from the initial top-K list produced by the first-stage retriever. Second, while S-Grid serialization is computationally efficient, it remains inherently lossy, since uniform frame sampling from long, untrimmed videos may fail to capture short yet semantically important events that are essential for accurate query matching. These limitations suggest clear directions for future work. At the framework level, promising approaches include prompt engineering and lightweight VLM fine-tuning to enable smaller, more efficient models to perform robustly. At the application level, future research could investigate query-aware or adaptive keyframe selection mechanisms to generate more representative S-Grids within fixed token budget."
        },
        {
            "title": "References",
            "content": "[1] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [2] Thong Nguyen, Yi Bin, Junbin Xiao, Leigang Qu, Yicong Li, Jay Zhangjie Wu, Cong-Duy Nguyen, See-Kiong Ng, and Anh Tuan Luu. Video-language understanding: survey from model architecture, model training, and data perspectives. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 36363657, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [3] Chunhui Zhu, Qi Jia, Wei Chen, et al. Deep learning for video-text retrieval: review. International Journal of Multimedia Information Retrieval, 12(3):126, 2023. [4] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019. [5] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023. [6] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051, 2017. [7] Gordon V. Cormack, Charles Clarke, and Stefan Buettcher. Reciprocal rank fusion outperforms condorcet In Proceedings of and individual rank learning methods. the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 09, page 758759, New York, NY, USA, 2009. Association for Computing Machinery. Shi, et al. Internvideo2: Scaling foundation models for mulIn European Conference on timodal video understanding. Computer Vision, pages 396416. Springer, 2024. [21] Crystina Zhang, Sebastian Hofstatter, Patrick Lewis, Raphael Tang, and Jimmy Lin. Rank-without-gpt: Building gpt-independent listwise rerankers on open-source large language models. In European Conference on Information Retrieval, pages 233247. Springer, 2025. [22] Mofetoluwa Adeyemi, Akintunde Oladipo, Ronak Pradeep, and Jimmy Lin. Zero-shot cross-lingual reranking with large language models for low-resource languages. arXiv preprint arXiv:2312.16159, 2023. [23] Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zero-shot video question answering using vlm. IEEE Access, 2024. [24] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [25] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moIn Proceedings of ments in video with natural language. IEEE International Conference on Computer Vision (ICCV), 2017. [26] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2017. [27] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Lin. Vatex: large-scale, highquality multilingual dataset for video-and-language research. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2019. [28] Giordano Cicchetti, Eleonora Grassucci, Luigi Sigillo, and Danilo Comminiello. Gramian multimodal representation learning and alignment. arXiv preprint arXiv:2412.11959, 2024. [8] Edward Fox and Joseph Shaw. Combination of multiple searches. NIST special publication SP, 243, 1994. [9] Michał Bałchanowski and Urszula Boryczka. comparative study of rank aggregation methods in recommendation systems. Entropy, 25(1), 2023. [10] Kaibin Tian, Yanhua Cheng, Yi Liu, Xinglin Hou, Quan Chen, and Han Li. Towards efficient and effective textto-video retrieval with coarse-to-fine visual representation learning. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 52075214, 2024. [11] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [12] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. [13] Youngjae Yu, Jongseok Kim, and Gunhee Kim. joint sequence fusion model for video question answering and retrieval. In Proceedings of the European conference on computer vision (ECCV), pages 471487, 2018. [14] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 87468755, 2020. [15] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. [16] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training arXiv preprint for zero-shot video-text understanding. arXiv:2109.14084, 2021. [17] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021. [18] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-clip: End-to-end multi-grained contrastive learning for video-text retrieval. In Proceedings of the 30th ACM international conference on multimedia, pages 638647, 2022. [19] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 36:7284272866, 2023. [20] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong"
        }
    ],
    "affiliations": [
        "Edge Hill University",
        "King Abdullah University of Science and Technology (KAUST)",
        "King Khalid University (KKU)"
    ]
}