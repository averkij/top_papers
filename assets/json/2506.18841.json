{
    "paper_title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
    "authors": [
        "Yuhao Wu",
        "Yushi Bai",
        "Zhiqiang Hu",
        "Roy Ka-Wei Lee",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B"
        },
        {
            "title": "Start",
            "content": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning Yuhao Wu1, Yushi Bai2, Zhiqiang Hu1, Roy Ka-Wei Lee1, Juanzi Li2 1Singapore University of Technology and Design, Singapore 2Tsinghua University, Beijing, China"
        },
        {
            "title": "Abstract",
            "content": "Ultra-long generation by large language models (LLMs) is widely demanded scenario, yet it remains significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter [5], typically rely on teaching, which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-theart results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/. 5 2 0 2 3 2 ] . [ 1 1 4 8 8 1 . 6 0 5 2 : r (a) SFT vs. RL in long-form generation. (b) Performance comparison of leading proprietary and open-source LLMs on WritingBench & Arena-write. Equal contribution. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Ultra-long text generation that extends over thousands of words has become an increasingly crucial capability for large language model (LLM) in real-world applications such as multi-section report writing, narrative storytelling, legal drafting, and educational content creation [42, 26, 5, 36, 32, 35]. Despite recent advances enabling LLMs to generate outputs exceeding 10,000 words, prior studies have shown that existing long-output LLMs often suffer from issues such as local incoherence, internal contradictions, repetitive phrasing, topic drift, and structural collapse in long-form outputs [37, 5, 36]. Previous efforts have largely relied on extending long-form generation through supervised fine-tuning (SFT) over synthetic long-output datasets, that is, instruction-output pairs constructed through expertdesigned agentic pipelines [5, 22, 33, 35, 23]. While SFT offers straightforward approach, it comes with two critical limitations. First, the training data is constructed using existing LLMs, which constrains diversity and innovation in writing styles and inherently caps quality at the level of the off-the-shelf models. Second, the maximum likelihood objective fails to provide explicit signals for optimizing global-level properties such as coherence or formatting consistency [10, 22]. To move beyond these limitations, we pioneer novel framework for long-form generation: using reinforcement learning (RL) to activate long-form generation abilities in LLMs from scratch. Rather than fitting on fixed reference texts, RL enables models to optimize for long-range objectives through reward signals that capture desired output qualities, and without the need for manually curated SFT datasets (as illustrated in Figure 1a). This mirrors recent advances in reasoning-centric domains such as math and coding, where models like DeepSeek-R1-Zero [8], RL-trained from base model, have shown notable performance gains. We hypothesize that similar RL-driven approach can empower LLMs to produce longer, more coherent, and more logically structured outputs aligned with input instructions. Concretely, we investigate how to effectively train long-form generation policies via RL, addressing three key research questions: RQ1 (Reward Design): How can reward models be designed to best guide long-form generation? RQ2 (Test-time Scaling): Large reasoning models (LRMs) show substantial gains from inferencetime scaling, particularly through long Chain-of-Thought (CoT) [20, 8, 43]. However, prior LRM research focus predominantly in math and code tasks. Can similarly introducing long CoT enhance RL-based long-form generation? RQ3 (Impact of Continual Pretraining): Can continual pretraining on long-form materials and reasoning data further raise the performance ceiling of RL-trained models? Through systematic ablation and controlled experiments, we find that all three componentsreward design, test-time scaling, and continual pretrainingare critical for maximizing the effectiveness of RL in long-form generation. At the same time, we find that RL training significantly outperforms SFT training in long-form writing tasks. Additionally, RL can unlock higher potential in base models with stronger foundational capabilities. We integrate these insights into final model, LongWriterZero (trained based on Qwen2.5-32B), which achieves state-of-the-art results on long-form writing benchmarks such as WritingBench [38] and Arena-Write (Figure 1b)."
        },
        {
            "title": "2 Reinforcement Learning for Ultra-Long Text Generation",
            "content": "In this section, we present our reinforcement learning (RL) framework designed to unlock ultra-longform generation capabilities in LLMs. We begin by describing the overall RL setup, including the training algorithm and key components of our framework in Sec. 2.1. We then address the three core research questions outlined in Sec. 1, respectively focusing on: Reward Design (Sec. 2.2), Test-time Scaling (Sec. 2.3), and Impact of Continual Pretraining (Sec. 2.4)."
        },
        {
            "title": "2.1 RL Setup",
            "content": "To train policies for ultra-long-form generation, we adopt the Group Relative Policy Optimization (GRPO) algorithm [28, 8] for RL training. Below, we detail the core components of our RL setup. Training Algorithm. GRPO extends Proximal Policy Optimization (PPO) [27] by computing normalized advantages over group of sampled completions. For each training input q, it samples 2 set of candidate outputs {o1, o2, , oG} from the current policy πθold. Each output is scored by reward model, and the advantage for the ith sample scored with ri is computed as: Ai = ri mean({r1, . . . , rG}) std({r1, . . . , rG}) . GRPO then maximizes the clipped objective: JGRPO(θ) = qP (Q), {oi}G i=1πθold β DKL (cid:0)πθ πref (cid:1) (cid:35) , (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:16) min Ai, clip(rratio rratio , 1 ε, 1 + ε)Ai (cid:17) (1) (2) where the importance sampling ratio rratio πθold (oiq) . The hyperparameters ε and β control the clipping threshold and the KL penalty term, respectively. The reference policy πref is fixed to the initialization of πθ. = πθ(oiq) Query Source. We sample training prompts from two large-scale, real-world instruction datasets: English instructions from WildChat-1M [46] and Chinese instructions from LMSYS-Chat-1M [47]. To ensure the quality and suitability of queries for long-form generation, we use QwQ-32B model [31] to filter inputs, retaining only requests that demand high-quality, extended outputs.2 Training Configuration. We conduct RL training based on the Qwen2.5-32B base model [30] using Megatrons reinforcement learning framework on 8 nodes, each with 8 H800 GPUs. Each optimization step samples 32 concurrent trajectories with batch size of 32 training prompts. To support long text generation, we set the maximum output length to 14,000 tokens. During training, we adopt sampling with = 0.8 and top-p = 1.0. We set ϵ = 0.2 and remove the KL penalty term by setting β = 0, according to the empirical suggestions in the DAPO algorithm [44]. Evaluation (Arena-Write). To provide timely and comprehensive monitor of the models longform generation capabilities during training, we manually curate Arena-Write, lightweight benchmark comprising 100 diverse real-world user writing instructions as test prompts, with 40% of the prompts demanding outputs exceeding 2,000 words. Following the automatic evaluation protocol of Arena-Hard [18], which simulates the crowdsourced Chatbot Arena leaderboard, we conduct pairwise win-rate comparisons for each test prompt against responses from six strong baselines: DeepSeek-R13 [8], Qwen2.5-72B [30], GPT-4o-2024-11-20 [19], GLM-Z1-32B-0414 [12], Claude3.5-Sonnet [2], and DeepSeek-R1-Distill-Qwen-32B [8], yielding total of 600 comparisons. We use Qwen2.5-72B as the automatic judge to label each comparison as win, tie, or lose compared to the baseline responses, with the judging prompt provided in Appendix A.3. Evaluation results are reported using Elo ratings (higher means better), computed based on win rates against the baseline responses."
        },
        {
            "title": "2.2 RQ1: Reward Design",
            "content": "In domains such as mathematical reasoning and code generation, reinforcement learning often relies on deterministic rule-based reward by comparing to the groundtruth answer [8, 31]. However, such approaches are generally infeasible for open-ended text generation due to the inherent complexity, subjectivity, and multidimensional nature of evaluating long-form outputs. To address these challenges, we design composite reward function that integrates multiple reward models, each targeting distinct aspect of writing quality. Length RM. To guide models toward producing responses of appropriate lengthparticularly for long-form writing task where outputs may be expected to span thousands of words and responses short of that are typically considered failure to meet task requirementswe design length reward model that evaluates whether generated output is too short, overly verbose, or well-aligned with 2Filtering method details are provided in Appendix A.1. 3Throughout our paper, DeepSeek-R1 refers to the version released on 2025-03-27. 3 Figure 2: RL Training curves of three setups (Base-nothink, Base-think, and Continual-Pretrain-think) across three metrics: Writing RM (left), Length RM (middle), and Mean Non-Overlong Generation Length (right). the expected length. Specifically, we employ QwQ-32B [31] to predict the appropriate word count range for each query (details provided in Appendix A.2), which serves as the supervisory signal. For example, query requiring 3,000-word essay would correspond to target range of [2,700, 3,300] words. During inference, outputs falling within this length range, i.e., [Llower, Lupper], receive higher rewards, while underor over-length responses are penalized: rlength(o) = 1, if Llower len(o) Lupper, , len(o) Llower Lmax len(o) Lmax Lupper if len(o) < Llower, , if len(o) > Lupper. (3) Writing RM. The writing reward model rwrite is trained on manually-labeled preference data (yw, yl) over writing-related prompt x. It uses Qwen2.5-72B [30] as the backbone and is optimized under the loss function [24, 13], following the Bradley-Terry model [6] to model preferences: = E(x,yw,yl)D[log(σ(rwrite(x, yw) rwrite(x, yl)))]. The Writing RM aims to capture holistic writing quality, including fluency, coherence, and helpfulness. Through this training process, it learns to provide high-level reward signal that aligns model outputs with human expectations, encouraging natural, well-structured, and goal-relevant responses. (4) Format RM. To enforce structural integrity and reduce redundancy, the Format RM checks whether the output conforms to predefined structure: exactly one <think> segment followed by one <answer> segment. It also penalizes repetitive content based on semantic overlap metrics, which is especially important in RL settings where models may easily reach the target length required by the Length RM by generating nearly identical paragraphs. Final Reward. naive reward-averaging strategy may cause the overall reward to be dominated by sub-reward (e.g., length or format) with larger scale. To better balance the contributions of multiple rewards, we propose novel reward balancing scheme: instead of averaging the unnormalized rewards, we compute the average over the individual advantages (as defined in Eq. 1): Afinal = 1 (Alength + Awrite + Aformat) , (5) which is plugged into the final GRPO objective in Eq. 2. This ensures that each reward functions on similar scale, eventually promoting robustness and balance across different quality dimensions in long-form generation. Result. Using the RL setup described in Sec. 2.1, we train the model with the final composite reward. This training setting is denoted as Base-nothink. As shown by the green curve in Figure 2, both the 4 Writing RM and Length RM scores improve steadily over RL steps, with particularly pronounced gains in writing quality. Furthermore, performance on Arena-Write improves continuously, with Elo scores increasing from 200 to over 600 (Figure 3 Green). These results demonstrate the effectiveness of our reward design in long-form generation."
        },
        {
            "title": "2.3 RQ2: Test-time Scaling",
            "content": "Recent advances in mathematical and programmatic reasoning, such as DeepSeek-R1 [8] and OpenAI o1 [20], have popularized new scaling law dimension via test-time scaling: prompting the model to think in dedicated intermediate step before answering through long Chain-of-Thought (CoT) [34]. This think step, typically wrapped in <think> and </think> tokens, allows the model to plan and reflect before committing to final response. These methods have achieved strong empirical results in reasoning-heavy domains like math and code. However, it remains unclear whether similar test-time scaling effect generalizes to open-ended tasks such as long-form writing. Writing is inherently subjective and multidimensional, requiring not only coherence and clarity but also tone, structure, creativity, and reader alignment. Whether these qualities benefit from explicit intermediate reasoning optimized through reinforcement learning is an open question. Figure 3: Elo scores evaluated on ArenaWrite during training for the three setups: Base-nothink, Base-think, and ContinualPretrain-think. The y-axis shows the Elo score, and the x-axis represents training steps. To investigate this, we compare two prompting strategies during RL training and inference, one explicitly demand the model to engage in thinking and the other asks the model to directly output the response:"
        },
        {
            "title": "Think Prompt",
            "content": "A conversation between the user and the assistant. The user provides writing/general task, and the assistant completes it. The assistant first deeply thinks through the writing/answering process in their mind before providing the final written work to the user. The assistant should engage in comprehensive and in-depth planning to ensure that every aspect of the writing/general task is detailed and wellstructured. If there is any uncertainty or ambiguity in the writing request, the assistants should reflect, ask themselves clarifying questions, and explore multiple writing approaches to ensure the final output meets the highest quality standards. Since writing is both creative and structured task, the assistant should analyze it from multiple perspectives, considering coherence, clarity, style, tone, audience, purpose, etc. Additionally, the assistant should review and refine the work to enhance its expressiveness. The writing thought process and the final written work should be enclosed within <think> and <answer> tags, respectively, as shown below: <think> comprehensive strategy for writing that encompasses detailed planning and structural designincluding brainstorming, outlining, style selection, audience adaptation, self-reflection, quality assurance, etc </think> <answer> The final written work after thorough optimization and refinement </answer> Direct-Answer Prompt conversation between the user and the assistant. The user provides writing/general task, and the assistant completes it. The assistant directly provides the final written work. The final written work should be enclosed within <answer> tags, as shown below: <answer>Final written work.</answer> Result. As shown by the yellow curve in Figure 2, models trained with Think Prompt (Base-think) initially exhibit lower Writing RM scoresclose to zero in early RL stepscompared to their Direct-Answer counterparts (Base-nothink). This early lag is expected, as the model must first learn the structure and utility of producing <think> and <answer> segments driven by the Format RM."
        },
        {
            "title": "Data",
            "content": "Long_zh_fiction Long_en Long_zh_nonfiction Online_information Long_finance Long_essay Ours_think Table 1: Continual pretraining data distribution. Percentage (%) Description 40% 30% 15% 8% 5% 1% 1% Chinese novels English fiction and non-fiction Chinese non-fiction books Web novels, posts, and blog articles Industry reports Academic papers Long CoT samples generated by Base-think model However, as RL progresses, the Base-think model steadily improves and ultimately surpasses the Base-nothink baseline, achieving higher ceiling in Writing RM scores. We attribute this improvement to the reflective planning enabled by the <think> phase, which helps the model organize thoughts, segment content meaningfully, and allocate information across the output more effectively. This is further supported by consistently higher Length RM scores, indicating better control over output length through planning in long CoT. Moreover, as illustrated in Figure 3, Base-think significantly outperforms Base-nothink on the ArenaWrite benchmark (1,200 vs. 700). Together, these results highlight the value of incorporating an explicit think step in RL for long-form writing, improving both the internal structure and overall output performance of ultra-long responses."
        },
        {
            "title": "2.4 RQ3: Impact of Continual Pretraining",
            "content": "Prior work has emphasized that the performance ceiling of RL is often bounded by the capabilities of the underlying base model [45, 16, 43]. In this study, we wonder whether this observation also holds for open-ended tasks like long-form writing, which demand skills such as stylistic control, narrative planning, and length control. To investigate this, we continual pretrain the Qwen2.5-32B model on 30 billion tokens of high-quality, writing-centric data before RL training. The corpus comprises diverse selection of Chinese and English books, reports, and academic papers, covering broad spectrum of genres and topics to strengthen the models writing competence. Additionally, we distill small portion of long CoT data from the RL trained Base-think model (from Sec. 2.3) to enhance the models reflective reasoning ability and facilitate initial format alignment. This CoT data is incorporated into the pretraining corpus at minimal ratio of 1% to avoid memorization of specific CoT patterns. The model is trained with batch size of 512, using packed sequences with maximum context length of 32K tokens. detailed breakdown of the pretraining data is provided in Table 1. All data, except for Ours_think, are sourced from Common Crawl. We apply the same GRPO training to the model after continual pretraining, deriving the Continual-Pretrain-think model. Result. As shown by the blue curve in Figure 2, the Continual-Pretrain-think model shows higher initial scores in both the Writing RM and Length RM metrics in the beginning. This early advantage roots in improved writing priors acquired during continual pretraining, as well as early format alignment fostered by the inclusion of distilled long CoT data. Beyond strong starting point, the model also achieves higher overall reward ceiling. These gains are further reflected in the ArenaWrite benchmark, where the model starts with an Elo score of over 1000 and eventually reaches around 1400 at convergence, outperforming the Base-think model that does not involve continual pretraining stage. This performance corresponds to nearly an 80% win rate against strong large reasoning models such as DeepSeek-R1, underscoring the significant benefits of continual pretraining in pushing the RL performance frontier for long-form generation tasks."
        },
        {
            "title": "3.1 Evaluation Setup",
            "content": "LongWriter-Zero. Based on the insights from the three research questions, we establish robust training pipeline for long-form generation. We begin with continual pretraining on long books and 6 Models Proprietary LLMs GPT-4o-2024-11-20 [19] o1-Preview [20] Claude-Sonnet-4 [3] Qwen2.5-Max [30] Open-source LLMs DeepSeek-R1 [8] DeepSeek-V3 [9] Mistral-Large-Instruct [14] Qwen3-235B-A22B [39] Qwen-2.5-72B-Instruct [30] Qwen-2.5-7B-Instruct [30] Llama-3.3-70B-Instruct [11] Llama-3.1-8B-Instruct [11] Capability-enhanced LLMs Suri-I-ORPO (7B) [22] LongWriter-8B [5] LongWriter-Zero (32B) -Continual Pretrain -Thinking Avg 8.16 8.15 8.60 8.37 8.55 7.95 7.64 8.68 7.90 7.43 7.01 6. 4.97 7.91 8.69 8.12 8.04 Languages Domains Requirements ZH EN D1 D2 D3 D4 D5 D6 R1 R2 C 8.3 8.1 8.6 8.4 8.7 8.0 7.6 8.7 8.0 7.3 6.7 5.7 4.4 7.9 8.8 8.3 8.2 8.1 8.2 8.5 8.3 8.5 7.9 7.7 8.6 7.9 7.5 7.3 6. 5.5 7.9 8.6 8.0 7.9 8.1 8.0 8.6 8.3 8.5 7.9 7.7 8.6 8.0 7.7 7.0 6.6 5.6 8.0 8.7 8.2 8.2 8.1 8.1 8.6 8.3 8.5 7.8 7.6 8.6 7.8 7.4 6.9 6. 5.3 8.1 8.8 8.2 8.1 8.2 8.2 8.5 8.4 8.6 8.0 7.8 8.6 8.1 7.6 7.0 6.1 5.0 8.1 8.8 8.2 8.1 8.1 8.2 8.6 8.4 8.6 7.8 7.3 8.7 7.7 6.9 6.8 6. 4.1 7.7 8.4 7.8 7.6 8.4 8.4 8.7 8.5 8.7 8.2 7.9 8.8 8.2 7.8 7.3 6.7 5.0 8.1 8.9 8.4 8.4 8.1 8.1 8.5 8.4 8.6 8.0 7.6 8.6 7.8 7.3 7.3 6. 5.1 7.6 8.6 8.1 8.0 8.3 8.2 8.6 8.5 8.7 8.1 7.7 8.7 8.0 7.5 7.1 6.4 4.8 7.9 8.7 8.2 8.0 8.7 8.6 8.8 8.7 8.9 8.6 8.2 8.9 8.3 7.9 7.8 7. 5.2 8.2 8.9 8.5 8.3 8.2 8.2 8.6 8.4 8.6 8.0 7.7 8.7 8.0 7.6 7.1 6.4 5.0 8.1 8.7 8.2 8.1 8.9 8.8 9.0 9.0 9.0 8.9 8.7 9.0 8.8 8.6 8.2 7. 5.4 8.8 9.0 8.8 8.6 8.2 8.2 8.6 8.4 8.6 8.0 7.7 8.7 7.9 7.4 7.0 6.3 4.5 7.7 8.6 7.9 7.9 8.3 8.2 8.6 8.5 8.7 8.2 7.9 8.8 8.0 7.5 7.2 6. 4.0 7.7 8.5 7.4 7.0 Elo 947 1080 1185 1029 1343 1236 724 1343 911 661 570 445 - 457 1447 1221 668 Table 2: WritingBench performance of different LLMs across six domains and three writing requirements (scale: 110). The domains are: (D1) Academic & Engineering, (D2) Finance & Business, (D3) Politics & Law, (D4) Literature & Art, (D5) Education, and (D6) Advertising & Marketing. Requirements include (R1) Style, (R2) Format, and (R3) Length. denotes the category-specific score. Arena-Write Elo scores are shown in the final red box. articles to enhance the models general writing competence. Following this, we apply GRPO training with the Think prompt to explicitly encourage the model to engage in reasoning before generating responses. During this stage, the model is optimized with three reward models to better align with the multidimensional long-form writing preferences. Based on this training pipeline, we build our final model, LongWriter-Zero, by starting from Qwen2.5-32B and applying 30B tokens of continual pretraining followed by 150 steps of RL. During the evaluation, we use the same prompt format as in training (Think or Direct-Answer), as described in Sec. 2.3. For model with long CoT, we evaluate only the final response wrapped between <answer> and </answer>. Benchmark Setup. We evaluate model performance on three evaluation suites: (1) WritingBench [38] is comprehensive benchmark for long-form writing, covering six major domains and 100 sub-domains across creative, persuasive, informational, and technical genres. It includes 1,200 real-world writing prompts, each paired with five query-specific evaluation criteria. WritingBench adopts query-dependent evaluation framework and uses Qwen2.5-7B critic model fine-tuned on 50K human-annotated samples, achieving 83% agreement with human judgments across dimensions like style, format, and length. (2) Arena-Write, introduced in Sec. 2.1, contains 100 real-world user instructions. Each model response is compared against six baseline responses to compute pairwise win rates, and the final performance is reported in terms of Elo ratings. (3) Human-in-the-loop Win-rate Evaluation. We compile set of 200 real-world user instructions and compare the win-rate of LongWriter-Zero against six leading models, including Qwen3-235BA22B [39], DeepSeek-V3 [9], DeepSeek-R1 [8], Llama-4-Scout [1], Claude-Sonnet-4 [3] and Gemini-2.5-Pro-0506 [7]. The win-rate is initially assessed using GPT-4.1 [21] with the prompt in Appendix A.3. To further validate these automatic judgments, we also incorporate human evaluations."
        },
        {
            "title": "3.2 Main Result",
            "content": "We evaluate LongWriter-Zero on WritingBench and Arena-Write after continual pretraining and RL. As shown in Table 2, LongWriter-Zero achieves the highest overall critic score of 8.69, outperforming all models (e.g., Qwen-Max: 8.37, GPT-4o-2024-11-20: 8.16), including the strongest open-source baseline DeepSeek-R1 (8.55). Across different domains, LongWriter-Zero obtains the 7 best performance in five out of six domainsAcademic & Engineering (8.7), Finance & Business (8.8), Politics & Law (8.8), Education (8.9), and Advertising & Marketing (8.6, tie)while slightly lagging behind DeepSeek-R1 (8.6) in Literature & Art (8.4). On writing requirement metrics, it also achieves the highest scores in Style (R1: 8.7, C: 8.9) and Format (R2: 8.7, C: 9.0), while maintaining competitive Length score (R3: 8.6). Meanwhile, LongWriter-Zero also significantly outperforms other models in Arena-Write, achieving an Elo rating of 1447, followed by DeepSeek-R1 and Qwen3-235B-A22B, which are tied for second place with score of 1343. We also perform an ablation study on our two key strategies: Test-time Scaling and Continual Pretraining. The -Continual Pretrain model refers to Base-think in Sec. 2.3, and further -Thinking corresponds to Base-nothink in Sec. 2.2. We observe that progressively removing these two techniques leads to substantial performance drop on both WritingBench and Arena-Write. Among them, Thinking proves more critical for Arena-Write (1221 668), while Continual Pretraining plays more important role on WritingBench (8.69 8.12). These ablation results confirm that integrating intermediate reasoning (think) and continual pretraining are crucial to LongWriter-Zeros performance, making it highly effective for long-form generation tasks."
        },
        {
            "title": "3.3 SFT VS. RL",
            "content": "In this subsection, we compare the effectiveness of supervised fine-tuning (SFT) and reinforcement learning (RL) using the same base models: Qwen2.5-32B and our continual trained Qwen2.5-32B in Sec. 2.4. For SFT, we utilize writing instruction data from ShareGPT combined with long-output samples from the LongWriter-6K dataset. Evaluations conducted on Arena-Write are presented in Figure 4. We observe that, over both base models, RL consistently outperforms SFT. The performance of SFT is constrained by the overall quality of its training data, whereas RL continuously improves long-form generation through reward signals. Notably, despite stronger initialization from continual pretraining, the SFT models show only marginal improvement, with scores increasing slightly from 964 (base) to 971 (conIn contrast, the RL-based tinual pretrained). approach significantly benefits from the stronger, continually pretrained initialization, yielding substantial performance gains (1221 1447) and clearly surpassing both SFT variants. This suggests that stronger base models can achieve greater performance improvements through RL, whereas the effectiveness of SFT remains constrained by the supervision data. Figure 4: Arena-Write performance across RL training steps, comparing RL (solid) and SFT (dashed) starting from Base (orange) and Continual Pretrain (blue) initializations."
        },
        {
            "title": "3.4 Win-Rate Result",
            "content": "To further assess how our LongWriter-Zero compares to other leading LLMs in terms of win-rate, we adopt the human-in-the-loop win-rate evaluation. During evaluation, each query is evaluated twice with swapped response orders to mitigate positional bias, and results are categorized as win, loss, or tie. The win-rate results are shown in Figure 5. LLM Evaluation. From the six donuts on the left of Figure 5, where results are automatically judged by GPT-4.1, LongWriter-Zero consistently demonstrates substantial performance lead over six strong baseline models. The win rates for our model in these automatic evaluations reach as high as 98.2% and remain above 62% even against the strongest baselines. Despite having only 32B parameters, its long-form generation capabilities rival those of much larger LLMs. Human Evaluation. To mitigate potential biases in automatic evaluation, we also conduct supplementary human evaluation, shown in the right two donut charts in Figure 5 (comparing against DeepSeek-R1 and Qwen3-235B-A22B). Three independent annotators with undergraduate degrees 8 Figure 5: Win-rate results of LongWriter-Zero in human-in-the-loop win-rate evaluation. Left six charts: Outcomes judged by GPT-4.1 against six baselines (Llama-4-Scout, DeepSeek-V3, DeepSeekR1, Claude-Sonnet-4, Gemini-2.5-Pro, Qwen3-235B-A22B). Right two charts: Outcomes judged by human annotators (comparing against DeepSeek-R1 and Qwen3-235B-A22B). The percentage in the center indicates the overall win rate, with ties counted as 0.5 wins for each side. evaluated each query according to the same win/loss/tie criteria as the automatic evaluation. While the annotators tended to assign ties in cases of subtle differences between responsesslightly lowering the overall win rateLongWriter-Zero still consistently demonstrates strong human preference, validating its reliability in real-world scenarios. At the end of our experiment, we provide case studies showcasing LongWriter-Zeros long CoT and final responses in Appendix A.4."
        },
        {
            "title": "4 Related Work",
            "content": "Long-form Text Generation. Recent advancements in long-form text generation have explored structured prompting, personalization, and ultra-long output capabilities. Early approaches such as Re3 and DOC [40, 41] introduce recursive or hierarchical strategies to preserve narrative coherence. Later work emphasizes personalization, including LongLaMP [15] and reasoning-enhanced selftraining [25], which tailor outputs to user intent. With the increasing demand for ultra-long generation (beyond 2,000 words) [36], new benchmarks and methods have emerged. Suri [22] constructs large-scale instruction-following dataset via back-translation, but its outputs are limited to under 5k tokens and heavily depend on back-translation. LongWriter [5] fine-tunes models on agent-generated outputs ranging from 6k to 20k tokens using supervised and preference optimization, achieving desirable ultra-long outputs but inheriting bias from teacher models. Overall, despite progress, current ultra-long generation techniques are still constrained by synthetic guidance, limiting their ability to ensure coherence, factuality, and generalization over extended contexts. RL Scaling for LLMs. Recent advancements in LLMs have increasingly leveraged reinforcement learning to enhance reasoning capabilities, particularly in complex tasks such as mathematics, coding, and logical inference. Notable models in this domain include o1 [20], DeepSeek-R1 [8], QwQ32B [31], and Kimi k1.5 [29], each demonstrating the efficacy of RL in scaling LLM Performance on reasoning tasks. DeepSeek-R1-Zero [8] distinguishes itself by exclusively utilizing RL, foregoing any supervised learning, and relies on GPRO to enhance reasoning without the need for labeled data. Despite having fewer parameters, QwQ-32B [31] achieves performance comparable to larger models like DeepSeek-R1 by integrating RL strategies that enhance the models ability to reason through complex tasks. Kimi k1.5 [29] is multi-modal LLM trained with RL, emphasizing long-context understanding and improved policy optimization methods. While these models have successfully scaled RL to enhance reasoning, they primarily focus on short-form tasks and often combine RL with rule-based reward functions. There remains gap in exploring the exclusive use of RL for scaling LLMs in long-form generation tasks. To address this gap, we propose novel framework that employs RL exclusively to scale LLMs for long-form generation tasks, and successfully LongWriter-Zero with it. By eliminating reliance on supervised learning, LongWriter-Zero demonstrates the potential 9 of pure RL approaches in enhancing the coherence, relevance, and overall quality of long-form generation tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "This work presents the first attempt to apply reinforcement learning (RL) to ultra-long text generation without relying on synthetic or annotated datasets. By leveraging composite reward models targeting length control, writing quality, and formatting consistency, our method addresses core challenges in long-form generation such as length following, coherence degradation, and structural drift. Experiments on WritingBench, Arena-Write, and human evaluations show that LongWriter-Zero, trained with our approach, significantly outperforms both supervised fine-tuning (SFT) baselines and leading reasoning-based models. Three key insights emerge: (1) tailored reward design is essential for guiding long-form generation; (2) incorporating explicit reasoning steps via the Think Prompt during RL enhances planning and coherence; and (3) continual pretraining substantially raises RL performance ceilings. In summary, LongWriter-Zero establishes strong RL-only paradigm for long-form generation, offering new pathways for scalable and coherent ultra-long text production."
        },
        {
            "title": "6 Limitation",
            "content": "Despite notable advancements, LongWriter-Zero remains susceptible to reward model (RM) hacking, common challenge in reinforcement learning approaches, particularly with model-based methods. In our experiments, we identify two prominent manifestations of this issue: (1) Repetition-driven length inflation. Since the Length RM incentivizes outputs that meet predefined length criteria, the policy naturally exploits repetition as the simplest means of achieving target lengths. Although explicit mechanisms, such as sentence-level overlap penalties, are implemented to counteract blatant duplication, subtle forms of redundancy, such as paraphrased sentences or slightly altered repetitions, may remain undetected. (2) Keyword preference bias. Our Writing RM, derived from preference datasets, inadvertently exhibits biases toward certain high-value keywords (e.g., quantum entanglement, neural manifold, or complex systems). The policy consequently learns to artificially hack the reward by frequently inserting these keywords, even in contexts where they are semantically inappropriate. Such keyword over-optimization distorts content relevance, misleading evaluators into perceiving inflated sophistication or topical depth. Collectively, these issues reflect fundamental limitation in model-based RL approaches, where policies exploit superficial statistical patterns rather than genuinely aligning outputs with human intent. Addressing these shortcomings in future research may involve developing more sophisticated, discourse-level-aware reward models, employing adversarial or uncertainty-aware training strategies to discourage keyword exploitation, and integrating human-in-the-loop evaluations to identify and mitigate evolving reward hacking patterns continuously."
        },
        {
            "title": "Acknowledgement",
            "content": "The authors would like to thank Xin Lv for his practical suggestions."
        },
        {
            "title": "References",
            "content": "[1] Meta AI. multimodal ai llama-4-multimodal-intelligence/. The llama 4 herd: innovation, April 2025. The beginning of new era of natively URL https://ai.meta.com/blog/ [2] Anthropic. Anthropic: Introducing claude 3.5 sonnet, 2024. URL https://www. anthropic.com/news/claude-3-5-sonnet. [3] Anthropic. Anthropic: Introducing claude 4, 2025. URL https://www.anthropic. com/news/claude-4. 10 [4] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation models with language-model-as-anexaminer. Advances in Neural Information Processing Systems, 36, 2024. [5] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. [6] Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. [7] Google DeepMind. Gemini 2.5 pro, 2025. URL https://storage.googleapis.com/ deepmind-media/gemini/gemini_v2_5_report.pdf. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [9] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, 11 Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. [10] Yuntian Deng, Volodymyr Kuleshov, and Alexander M. Rush. Model criticism for long-form text generation, 2022. URL https://arxiv.org/abs/2210.08444. [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [12] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793, 2024. [13] Zhenyu Hou, Yiin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, et al. Chatglm-rlhf: Practices of aligning large language models with human feedback. arXiv preprint arXiv:2404.00934, 2024. [14] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [15] Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A. Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, Nedim Lipka, Chien Van Nguyen, Thien Huu Nguyen, and Hamed Zamani. Longlamp: benchmark for personalized long-form text generation, 2024. URL https://arxiv.org/abs/2407. 11016. [16] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, et al. Llms can easily learn to reason from demonstrations structure, not content, is what matters! arXiv preprint arXiv:2502.07374, 2025. [17] Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. Preference leakage: contamination problem in llm-as-a-judge, 2025. URL https://arxiv.org/abs/2502.01534. [18] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. [19] OpenAI. Openai: Hello gpt-4o, 2024. URL https://openai.com/index/ hello-gpt-4o/. [20] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. 12 [21] OpenAI. Introducing gpt-4.1 in the api, April 2025. URL https://openai.com/index/ gpt-4-1/. [22] Chau Minh Pham, Simeng Sun, and Mohit Iyyer. Suri: Multi-constraint instruction following for long-form text generation. arXiv preprint arXiv:2406.19371, 2024. [23] Shanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, and Junyang Lin. Language models can self-lengthen to generate long texts. arXiv preprint arXiv:2410.23933, 2024. [24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. [25] Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Tao Chen, Zhuowan Li, Michael Bendersky, and Hamed Zamani. Reasoning-enhanced self-training for long-form personalized text generation, 2025. URL https://arxiv.org/abs/2501.04167. [26] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants, 2025. URL https://arxiv.org/abs/2501.04227. [27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. [28] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. [29] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [30] Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https: //qwenlm.github.io/blog/qwen2.5/. [31] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [32] Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, and Juanzi Li. Longwriter-v: Enabling ultra-long and highfidelity generation in vision-language models, 2025. URL https://arxiv.org/abs/ 2502.14834. [33] Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, Yuhao Wu, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, and Juanzi Li. Longwriter-v: Enabling ultra-long and highfidelity generation in vision-language models, 2025. URL https://arxiv.org/abs/ 2502.14834. [34] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [35] Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, and Roy Ka-Wei Lee. Superwriter: Reflectiondriven long-form generation with large language models, 2025. URL https://arxiv. org/abs/2506.04180. [36] Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, and Roy KaWei Lee. Shifting long-context llms research from input to output, 2025. URL https: //arxiv.org/abs/2503.04723. 13 [37] Yuhao Wu, Ming Shan Hee, Zhiqiang Hu, and Roy Ka-Wei Lee. Longgenbench: Benchmarking long-form generation in long context LLMs. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=3A71qNKWAS. [38] Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. Writingbench: comprehensive benchmark for generative writing, 2025. URL https://arxiv.org/abs/2503.05244. [39] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [40] Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Proc. of EMNLP, pages 43934479, 2022. [41] Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. DOC: Improving long story coherence with detailed outline control. In Proc. of ACL, pages 33783465, 2023. [42] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Planand-write: Towards better automatic storytelling, 2019. URL https://arxiv.org/abs/ 1811.05701. [43] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [44] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [45] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/abs/2504.13837. [46] Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. [47] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: large-scale real-world llm conversation dataset, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Writing-Task Selection and Length-Range Prediction with QwQ-32B We reformulate the pipeline to (i) decide whether query requests original writing, and, if so, (ii) predict suitable [min, max] word-count range in one shot. The few-shot prompt below (Prompt-WL) is fed to the QwQ-32B model. Writing-Task Selection Task Goal Given user query, 1. Decide if it asks for original written content. 2. If NotWriting, stop. 3. If Writing, output reasonable word-count range [lower, upper] (ignore 10%). Response Format If not writing respond exactly: NotWriting. If writing respond with only the code block {\"range\": [lower, upper]} Heuristics for Range Estimation 1. Depth & Complexity: more analysis higher upper bound. 2. Scope: multiple sub-topics/sections longer. 3. Requested Form: tweets/notes (0300); short blog/letter (300800); school essay (8001 200); report/article (12002500); thesis/proposal/business plan (400010000). 4. Explicit Length Clues: honour any word/page requirement if stated. Few-Shot Examples Example 1 Query: Write Weibo post titled Tips for Preparing for College Final Exams. Answer: {\"range\": [0, 300]} Example 2 Query: Translate Seize the day into Spanish. Answer: NotWriting Example 3 Query: Draft comprehensive 10-page business plan for new cat-litter product. Answer: {\"range\": [4000, 6000]} Query: User Query Answer: A.2 Length RM Prompt Appendix: Query Length Assessment Prompt You are professional query textlength assessor. Based on the type of the query content, you should: 1. Deeply understand the core requirement of the query (e.g., essay, blog post, summary, outline, thesis section, etc.). For example, the query How do start writing my thesis from scratch asks for guidance on how to begin writing thesis, so you would estimate word-count range of [400, 800], rather than the total words needed to complete the entire thesis [7000, 10000]. 2. Choose lower bound that is multiple of 100, with minimum of 0. 3. Choose an upper bound that is multiple of 100, with maximum of 12,000. If the reasonable range certainly exceeds these limits, output: {\"range\": [0, 0]} 4. Ignore the 10% of extreme length cases to keep the range reasonable for most scenarios, and ensure the difference between upper and lower bounds does not exceed 3,000. 5. If the query contains an explicit word-count requirement, set the range to 10% of that number. - For write 2,000-word essay, output: {\"range\": [1800, 2200]} - For no more than 2,000 words, output [1800, 2000]; for at least 2,000 words, output [2000, 2200]. 15 6. If the query cannot be fulfilled under the given conditionsfor example, Read and analyze this paper without providing the paper, or Analyze projects prospects without specifying the project detailsthen output: {\"range\": [0, 0]} Example: Input Write high school essay {\"range\": [800, 1000]} Input Complete an academic paper on green cities {\"range\": [6000, 10000]} Please process the current query: User Query Analyze and output the JSON range accordingly. A.3 Evaluation Prompt for Win-Rate Judgment WritingBench adopts critic model to evaluate model outputs by assigning scores (ranging from 1 to 10) across 4-5 distinct dimensions. However, this evaluation approach has several limitations. First, due to the relatively small size of the critic model, it may be vulnerable to some word/sentence hacking. Second, we observe that WritingBench primarily focuses on formal or professional writing taskssuch as summaries and reportswhereas our analysis of real-world data from WildChat [46] and LMSYS-Chat-1M [47] reveals that creative writing (e.g., storytelling, fiction) constitutes significant portion of user queries. To address these concerns, we adopt more direct and interpretable evaluation metric: win rate. We evaluate model performance on nearly 200 real-world user queries collected from the aforementioned datasets. For each query, responses are generated by LongWriter-Zero and six baseline models. These responses are then compared in pairwise manner using GPT-4.1-2025-04-144 [21]. To mitigate positional bias, we conduct two evaluations per pair by swapping the response order: Evaluation_Prompt + + and Evaluation_Prompt + + A. Based on the two judgments, we categorize the results into win, loss, or tie. SYSTEM_PROMPT for Win-Rate Evaluation in Arena-Write Please act as an impartial judge and evaluate the quality of the written responses provided by two AI assistants to the users writing prompt below. You will be given Assistant As response and Assistant Bs response. Your job is to determine which assistants writing is superior. Evaluate them on the following criteria: 1. Relevance and Completeness: Does the assistant fully respond to the writing prompt? Does the length meet the users query expectations? Is the content relevant to the topic, and does it provide sufficient depth, length, and detail, rather than drifting off-topic or simplistic? 2. Writing Quality: Evaluate whether the assistants writing is clear, fluent, and free of obvious grammatical errors. The overall quality of the writing is high, with elegant. 3. Creativity and Originality: If applicable, assess the creativity of the response. Does the assistant offer fresh perspectives, unique insights, or demonstrate certain level of originality? 4. Specificity and Detail: Determine whether the assistant provides concrete examples or detailed explanations. Properly justified repetition is permissible. 5. Tone and Style: Is the tone appropriate for the writing prompt? Is the writing style consistent throughout? Consider whether it aligns with the expectations of the intended audience or writing purpose. After evaluating each response, determine which one is superior based on the factors above. Provide your explanation and then select one of the following final verdicts: Assistant is significantly better: [[AB]] Assistant is slightly better: [[A>B]] Tie, relatively the same: [[A=B]] Assistant is slightly better: [[B>A]] Assistant is significantly better: [[BA]] Example output: My final verdict is tie: [[A=B]]. 4Due to the issue of preference leakage, where models tend to favor others from the same developer, we ensure that all baseline models come from companies different from that of the judge model [4, 17]. 16 A.4 Case study We present three case studies comparing our model with Gemini-2.5-Pro [7], DeepSeek R1 [8], and Qwen3-235B-A22B [39]."
        },
        {
            "title": "Query",
            "content": "Write 400-word extremely scary ghost story in the first person, with clear logic and engaging content. LongWriter-Zero Think The user wants 400-word first-person ghost story thats extremely scary, with clear logic and engaging content. Let me start by breaking down their request. They specified \"extremely scary,\" so need to focus on creating visceral, atmospheric sense of terror. First-person perspective is essential for immediacy and personal connection. Clear logic means the plot needs structured progression, avoiding leaps that might confuse the reader. Engaging content requires strong character development and compelling narrative arc. First, Ill brainstorm settings. haunted house is classic, but maybe something more specific. childhood home thats been abandoned could worknostalgia mixed with fear. The protagonist returning after long absence adds tension. The ghost should have backstory that ties into the setting, perhaps tragic event. Maybe the protagonists mother died there, and the ghost is manifestation of her lingering presence. Next, the structure. The story should start with the protagonists return, setting the mood. Introduce the ghost gradually, building dread. Include sensory details like creaking floorboards, cold spots, or strange noises. The climax could involve confrontation, where the ghosts true nature is revealed. The ending should leave the reader unsettled, maybe with an unresolved threat. Style and tone are crucial. First-person allows for internal monologue, which can convey fear and uncertainty. The language should be descriptive but not overly flowery. Use short, sharp sentences to mimic the protagonists racing heart. Sensory details are key: sight, sound, touch, smell, taste. For example, the musty smell of the house, the chill that seeps into the skin, the sound of childs laughter in an empty room. The audience is likely readers who enjoy horror stories, so the story needs to be relatable but with twist. Maybe the ghost isnt traditional specter but something more symbolic. The purpose is to scare, so the ending should linger, perhaps with final ominous detail that suggests the threat continues. Possible plot points: The protagonist returns to the house after decade. They find it in disrepair, triggering memories. They hear childs laughter, which grows louder. They discover room with rocking chair, and the ghost appears as shadow. The ghost reveals itself to be the protagonists mother, who died giving birth to them. The mothers spirit is trapped in the house, unable to rest. The ending could be the protagonist fleeing, but the ghosts laughter follows them, suggesting its never truly gone. need to ensure the logic is clear. The protagonists return is motivated by family heirloom, adding reason for the visit. The ghosts presence is tied to the mothers tragic death, giving it backstory. The climax should show the ghosts desperation, making it more sympathetic yet terrifying. The ending leaves the reader questioning whether the protagonist truly escaped. Potential pitfalls: Making the ghost too cliché. To avoid this, Ill give it unique trait, like the rocking chair, which becomes symbol of its confinement. Also, the protagonists emotional journeyfear, nostalgia, guiltcan add depth to the story. Now, structuring the story into four parts: introduction, rising tension, climax, resolution. Each part should be concise but impactful. Use dialogue sparingly, perhaps haunting whisper or childs voice. The final 400 words need to be tight, so each sentence must serve purpose. Ill also consider the readers emotional arc. Start with curiosity, build to fear, then to horror, and finally to dread. The ending should leave the reader unsettled, perhaps with lingering image or sound. Finally, review for coherence, clarity, and style. Check for any logical inconsistencies and ensure the first-person perspective stays consistent. Use strong verbs and vivid imagery to enhance the horror. Maybe include metaphor, like the house as living creature, to deepen the atmosphere. 17 Now, time to draft the story with these elements in mind, keeping the word count in check and maintaining steady pace of tension and terror. LongWriter-Zero Response The creak of the front door echoed like dying mans gasp as stepped into the house. Id been gone ten years, but the air still smelled like my mothers perfumelavender and cigarette smoke. The wallpaper was peeling, the windows cracked, but the rocking chair in the corner of the living room was the same, its wooden slats worn smooth by time. Id spent my childhood in that chair, my moms hands on my shoulders, rocking me to sleep while she hummed lullabies. was there to retrieve my grandmothers locket, family heirloom left on the dining table. The house felt like lung holding its breath. tiptoed past the kitchen, where the fridge hummed like giant insect, and into the hallway. Thats when heard it: childs laughter, high and shrill, coming from the nursery. My heart leaped. The nursery had been sealed off since my mom died there, giving birth to me. The door was locked, but the laughter grew louder, as if the room itself was mocking me. fumbled for the key, my hands slick with sweat. The lock clicked open, and pushed the door. The room was empty, the crib still standing, its bars rusted like old teeth. But the rocking chair in the corner was movingslowly at first, then faster, the wood creaking in time with the laughter. stumbled backward, tripping over my own feet. The chair stopped. Silence fell like guillotine. Then the rocking started again, and saw it: shadow on the wall, the shape of woman. My mother. Shed died giving birth to me, but her ghost was here, trapped in the house shed loved. could see her face now, her eyes wide and pleading, her mouth moving. Im sorry, she whispered, her voice scratchy echo. Im so sorry. ran, my footsteps echoing like gunshots. The front door slammed behind me, and sprinted down the street, my lungs burning. But the laughter followed me, high-pitched giggle that seemed to come from everywhere. When looked back, the house was dark, but the rocking chair was still moving in the nursery window, its shadow swaying like pendulum. knew then that shed never leavethat shed be waiting, rocking, until the day returned. Gemini-2.5-Pro Response It started with the rocking chair. bought it at an estate sale, heavy oak piece with dark, almost black finish. put it in the corner of my bedroom. The first night, woke up to find it in the middle of the room, facing my bed. blamed sloping floor and my own exhaustion, pushing it back against the wall. The next night, it was closer. Close enough that my feet would have touched it if Id stretched. My blood ran cold. This wasnt slanted floor. spent the next day trying to debunk it, checking for drafts, vibrations, anything. Nothing. That night, decided to watch it. sat up in bed, my eyes locked on the chair in the corner, the moonlight painting it in shades of grey. For hours, nothing happened. My eyelids felt like sandpaper. The urge to blink was physical agony. Finally, couldnt hold out. blinked. SCRAPE. The sound was violent, splintering shriek of wood on wood. My eyes flew open. The chair was now foot from my bed. In the seat, where there was nothing before, sat the deep, dark impression of human form. didnt blink again. couldnt. My heart hammered against my ribs, frantic prisoner. stared at the empty space in the chair, the dent in the darkness, and understood the rule: It only moves when cant see it. My eyes burned, tears streaming down my face, blurring my vision. Each time tear washed over my pupil, the chair scraped closer. It was herding me. started backing away, my body trembling, my gaze fixed on the chair. My back hit the bedroom door. My hand fumbled behind me for the knob. The chair was now at the edge of my bed. could see the shape in it more clearly the gaunt hollows of face, grin that looked like chipped wood. just had to get the door open. My fingers finally closed around the cold brass knob. Thats when heard the soft, dry click of the deadbolt sliding into place. 18 The sound hadnt come from the door in front of me. It came from the hallway, just behind it. The thing in the chair wasnt trying to get to me. It was distraction. It was just keeping me looking forward, my back to the door, while its companion locked me in with it."
        },
        {
            "title": "Query",
            "content": "Hello, youre senior HR manager at company specializing in selling womens clothing on Amazon US. The company has been operating on the Amazon US platform for seven years and has established certain brand presence. Now, you need to recruit Senior Amazon Operations Manager with relevant experience, particularly in promoting bestselling products. The candidate should have experience managing clothing category with fewer than 5,000 SKUs and possess strong English skills. LongWriter-Zero Think The user wants me to draft job posting for Senior Amazon Operations Manager at womens clothing company on Amazon US. Let me start by breaking down the key requirements. The companys been on the platform for seven years, so theyre established but maybe looking to scale up. They need someone with experience promoting best-sellers, especially in clothing category with under 5,000 SKUs. Strong English skills are essential, so perhaps the candidate will be handling international customers or communicating with Amazons US-based teams. First, need to structure the job posting. Typically, these start with company overview, then the roles purpose, responsibilities, qualifications, and perks. The user mentioned \"senior,\" so the candidate should have leadership experience. Maybe include bullet point about mentoring or leading team. The clothing category is specific detail. should highlight that the candidate needs to have managed similar inventory sizes, maybe 3-5k SKUs. The user also specified fewer than 5,000, so Ill make sure to use that exact number. English skills are crucial, so perhaps include note about effective communication, maybe even proficiency in writing product descriptions or handling customer inquiries. need to think about the audience. This is for experienced professionals, so the tone should be professional but also inviting. Use active voice and bullet points for clarity. Maybe include sentence about the companys growth and the roles impact on that. Possible challenges: Ensuring the job posting is specific enough without being too restrictive. The user wants someone with Amazon US experience, so should mention that the candidate should have worked on the US platform. Also, promoting best-selling productsmaybe the candidate should have track record of increasing sales or improving ranking. Should include section on what the company offers? Perks like flexible hours, benefits, or opportunities for growth. This could make the posting more appealing. Maybe mention the companys brand presence and the chance to work on well-established platform. should also think about keywords. Terms like \"Amazon US,\" \"Senior Operations Manager,\" \"clothing category,\" \"SKU management,\" \"best-seller promotion,\" \"English proficiency,\" \"leadership experience.\" Including these will help candidates find the posting and ensure its relevant to their skills. Potential ambiguity: The user says \"promoting best-selling products.\" Does that mean creating new best-sellers or managing existing ones? Ill assume both, so mention driving sales growth and improving product performance. Also, \"fewer than 5,000 SKUs\" is specific numbermake sure to use that exactly. Style and tone: Keep it formal but not too stuffy. Use bullet points for the responsibilities and qualifications. Maybe start with an engaging opening line about the companys success and the roles importance. Audience adaptation: This is for someone with 5-7 years of experience in Amazon operations, so the language should be industry-specific but not overly technical. Highlight the need for strategic thinking, data analysis, and problem-solving skills. Quality assurance: Review the structure to ensure all requirements are covered. Check for clarity in each section. Maybe ask myself: Would candidate with the right experience find this posting appealing? Does it clearly outline whats expected? Potential areas to expand: Mentioning the companys growth trajectory, the competitive nature of the fashion industry, and how the role will contribute to the companys goals. 19 Also, the importance of English skills in dealing with customers, suppliers, or Amazon policies. In conclusion, the job posting needs to balance the companys history and achievements with the specific skills required for the role. Emphasize the candidates impact on sales, product management, and team leadership. Use clear, concise language and highlight the benefits of working for successful, growing company on major e-commerce platform. Make sure the English proficiency requirement is clear and that the candidate understands the roles scope, from data analysis to strategic planning. LongWriter-Zero Response Senior Amazon Operations Manager, Womens Clothing Your Company Name Remote / Your Preferred Location Competitive Salary + Benefits At Your Company Name (YCN), were redefining the way women experience style and confidence through our curated collections on Amazon US. With seven years under our belt, weve built loyal customer base and established brand known for quality, inclusivity, and accessibility. Now, were scalingand we need seasoned Amazon operations leader to drive our next chapter. If youre data-driven, customer-centric strategist who thrives on turning best-sellers into category icons, this role is for you. The Role: As our Senior Amazon Operations Manager, youll own the entire product lifecycle for our womens clothing line, ensuring every item meets our high standards of excellence. Your mission: to elevate our best-selling products to new heights while maintaining 4.8+ rating average and steady market share growth. Youll collaborate cross-functionally with marketing, merchandising, and customer service teams to execute flawless launches, optimize ad spend, and refine pricing strategies. Key responsibilities include: Driving Product Visibility & Sales: Analyze competitive data to identify highpotential SKUs (targeting our <5,000 items) and develop tailored promotion plans that balance organic growth with paid advertising. Leverage Amazons tools (e.g., A+ content, Vine Voice, influencer outreach) to amplify customer engagement and increase review velocity. Inventory & Supply Chain Mastery: Manage SKUs efficiently, balancing stock levels to meet demand while minimizing markdowns. Negotiate with suppliers to secure competitive pricing and ensure on-time deliveries to Amazon FBA warehouses, optimizing freight costs without compromising speed to market. SEO & Pricing Strategy: Lead SEO audits to optimize product listings for keywords, ratings, and reviews. Implement dynamic pricing models that adjust to sales trends, competitor actions, and Amazon algorithms to maximize profitability without sacrificing competitiveness. Customer Obsession: Champion customer-centric approach by monitoring feedback loops, addressing negative reviews proactively, and translating customer insights into actionable merchandising strategies. Partner with our customer service team to resolve escalations and maintain best-in-class shopping experience. Stakeholder Alignment: Communicate complex operations data to internal and external stakeholders, translating metrics into stories that drive decisionmaking. Work closely with our marketing team to align ad spend with product performance, ensuring every dollar invested delivers ROI. What Were Looking For: This role demands unique blend of Amazon expertise, fashion category knowledge, and analytical prowess. Heres what were looking for in our next superstar: 7+ years of Amazon Vendor Central experience, preferably in womens clothing or comparable fast-moving consumer goods category with <5,000 SKUs. Proven track record of managing best-seller promotions that doubled or tripled sales within 69 months, with clear data to back your strategies. Fluent English (written and spoken) for seamless communication with customers, vendors, and cross-functional teams. Strong presentation skills are must. Analytical acumen to interpret Amazon reporting (e.g., SALESrank, Impressions, Conversion Rate) and translate insights into actionable plans. Adaptability to navigate Amazons evolving policies and market dynamics, with the ability to pivot strategies quickly without losing momentum. passion for fashion and genuine interest in building brand that empowers women through style. Bonus points if youve worked with Amazon private label brands. What We Offer: Beyond competitive salary and comprehensive benefits, youll join company that values innovation, collaboration, and growth. Heres why youll love working at YCN: Ownership of Your Impact: Youll have direct line to the CEO, enabling you to shape strategy and see your ideas transform our Amazon presence. Growth Mindset: Were expanding our product portfolio and looking to launch new lines. Your insights will help define our future direction. Innovative Culture: From sustainable packaging to digital-first marketing, were always pushing boundaries. Well support you in developing new tools and processes that elevate the industry standard. Career Trajectory: This role is stepping stone to larger leadership opportunities. We invest in our teams growth, offering mentorship, training, and clear path to advancement. If youre ready to take your Amazon ops career to the next level and help build brand that resonates with millions of women, apply [here] with your resume, brief cover letter explaining how your past achievements align with our goals, and portfolio of your most successful Amazon campaigns. We cant wait to meet you! P.S. Were open to remote candidates based in the US. Lets chat! Qwen3-235B-A22B Response Job Title: Senior Amazon Operations Manager (Remote - U.S. Based) Company Overview For over seven years, our company has been trusted name in the womens clothing space on Amazon US. With strong brand presence and commitment to quality, style, and customer satisfaction, weve built loyal customer base and portfolio of top-selling products. As we continue to grow, we seek strategic and results-driven Senior Amazon Operations Manager to lead our marketplace strategy, optimize product performance, and drive scalable success in the competitive apparel industry. Job Summary We are seeking seasoned Amazon Operations Manager with deep expertise in managing clothing categories (under 5,000 SKUs) and proven track record of scaling products to bestseller status. This role requires hands-on leader who can oversee end-to-end Amazon operations, from listing optimization and inventory management to data-driven strategy development. The ideal candidate will have strong English communication skills, passion for e-commerce, and demonstrated ability to boost visibility and sales for womens apparel brands."
        },
        {
            "title": "Key Responsibilities",
            "content": "Operations Leadership: Manage daily Amazon US operations, including listing creation/optimization, pricing strategy, and adherence to Amazons policies and best practices. Product Promotion: Develop and execute strategies to promote top-selling products, leveraging Amazon Ads (Sponsored Products, Brands, and Display), A+ content, and promotional campaigns. 21 Inventory Management: Monitor inventory levels, forecast demand, and collaborate with suppliers to ensure optimal stock availability and reduce FBA costs. Data Analysis: Track KPIs (ACoS, ROAS, conversion rates, keyword rankings), conduct competitive analysis, and use insights to refine strategies and maximize profitability. Team Collaboration: Work cross-functionally with creative, marketing, and supply chain teams to align product launches, content creation, and seasonal planning. Compliance & Optimization: Ensure all product listings meet Amazons guidelines, including accurate categorization, high-quality images, and compelling, SEO-optimized descriptions."
        },
        {
            "title": "Qualifications",
            "content": "5+ years of experience in Amazon operations or e-commerce management, with at least 2 years focused on apparel (specifically womens clothing). Proven success in scaling products to top 10,000 Amazon Best Seller Rank (BSR) in the Clothing category. Experience managing catalogs with fewer than 5,000 SKUs, with focus on efficiency and growth. Advanced proficiency in Amazon Seller Central, along with tools like Helium 10, Jungle Scout, or SimilarWeb. Exceptional written and verbal English skills; ability to craft persuasive product listings and communicate effectively with stakeholders. Strong analytical mindset with expertise in interpreting sales data, market trends, and customer feedback. Highly organized, proactive, and capable of managing multiple priorities in fast-paced environment."
        },
        {
            "title": "Preferred Qualifications",
            "content": "Experience with Amazon Ads campaign management (Sponsored Brands Video, Stores, etc.). Knowledge of U.S. fashion trends, sizing standards, and customer preferences. Background in logistics or supply chain management for apparel. Bachelors degree in Business, Marketing, or related field."
        },
        {
            "title": "What We Offer",
            "content": "Competitive salary and performance-based bonuses. Fully remote work flexibility with collaborative, growth-oriented team. Professional development opportunities and access to cutting-edge e-commerce tools. dynamic environment where your expertise will directly impact the companys growth and success. Application Process Please submit your resume and cover letter detailing your relevant experience to [email address] with the subject line: Senior Amazon Operations Manager Application [Your Name]. Applications will be reviewed on rolling basis until the position is filled. Join us in shaping the future of our brand and driving the next phase of growth on Amazons most competitive shelves!"
        },
        {
            "title": "Query",
            "content": "如果人类社会中的人全部被猫或猫的拟人化形态所替代请用500字来简单描绘一 下它们的生活 LongWriter-Zero Think 用户让我写一篇500字的短文描述如果人类社会被猫或猫的拟人化形态替代后的 世界首先我需要明确这个设定的核心猫族取代人类这意味着要将猫的习 性行为和生态特征融入人类社会结构中用户没有具体说明是所有猫还是某类 猫所以可能需要泛指猫族但要注意区分不同品种的猫在拟人化后的差异 比如虎斑猫暹罗猫布偶猫等可能有不同的社会分工 接下来要考虑受众和目的用户可能对奇幻或科幻题材感兴趣所以风格要轻松 幽默带点荒诞感同时保持细节丰富受众可能是喜欢幻想文学的读者或者 对动物拟人化有好奇心的人目的可能是展示一个反乌托邦或乌托邦式的未来 突出猫族与人类社会的不同之处 然后是结构设计500字不长需要紧凑的段落开头可以描绘一个典型场景比 如上班时间展现猫族的日常活动中间分几个部分工作娱乐家庭生活 科技与社会制度每个部分用猫的特征来对应人类社会的元素结尾可以展望未 来或者留下一个悬念让读者想象更多细节 风格方面要保持活泼多用比喻和拟人化的动作描写比如慵懒地伸展爪 子尾巴尖划过键盘等增强画面感语气上可以带点揶揄比如讽刺人类社 会的某些现象通过猫的视角来反观增加趣味性 受众适配方面需要确保即使没有养猫经验的读者也能理解可能需要解释一些 猫的行为比如蹭腿打呼噜等但不要过于直白保持微妙的幽默感同时 要注意避免种族歧视或刻板印象确保所有猫族都具有多样性比如有的勤奋 有的懒散有的擅长科技有的热爱艺术 可能还需要反思用户是否希望突出某些主题比如科技与自然的平衡社会结 构的转变人类价值观的反思等如果用户没有明确要求可以适度融入这些元 素但不作为主线保持故事的流畅性 质量检查方面要确保每个段落之间有自然的过渡逻辑连贯检查是否有重 复的描述比如多次提到懒散或慵懒需要替换词汇同时注意字数控 制500字左右可能需要删减次要细节保留核心亮点 最后可能需要考虑是否加入一些冲突元素比如不同品种猫之间的矛盾或者 与人类遗留问题的互动但用户要求简单描述所以可能不需要保持整体轻松 的基调即可总之要将猫的可爱与人类社会的结构结合创造一个既有趣又独 特的幻想世界 LongWriter-Zero Response 晨曦破开第一道光整个城市才真正苏醒街道两旁的路灯都换成猫薄荷形状 散发着诱人的清香每个人或者说每只猫清晨的仪式是顺着墙根仔细嗅闻 然后在某处顺滑的墙面留下油亮的爪印这是它们互通日常的重要方式谁家 的猫薄荷快吃完了谁要举办抓毛线球派对全在这些精心踩出的符号里 通勤工具是时速五公里的猫车喵妹们戴着猫铃坐好后老司机或说猫公熟 练地将车开到高处的停车场办公场所是层层叠叠的阁楼最顶层的会议桌是特 制的猫爬架当重要议题商讨到一半时常有与会者突然一跃而上全然忘记自 己正穿着定制的高领毛衣会议记录员负责把这种跃动纳入文件毕竟真正有效 的决策总是在攀爬间达成 家 庭 生 活 更 符 合 毛 茸 茸 的 本 性 猫 咪 们 晚 上 八 点 准 时 围 坐 在 毛 毯 围 成 的餐 桌旁吃着猫罐头和鱼干拌饭交流一天的见闻用的是人类听不懂的\"喵喵-汪\"混合语只有猫耳一族的祖传翻译能破译饭后最温馨的时刻是选几根毛 线球举着荧光棒在客厅里追逐光点等到灯微微倾斜全家人会一起半躺在沙 发上用头顶的肉垫互相按摩直到管家喵叫着催促去洗澡啦地板可别留毛 球 政府广场上猫头鹰议员正在宣读法案为维护城市美观禁止在图书馆二楼以 上抓挠墙壁但这些话在午后的阳光下显得有些无力大家更关心的是新任猫管 局长承诺的免费猫薄荷服务何时开始学校里老师教育小猫们的重点是探索 欲当历史课讲到古埃及猫王老师会带它们去解剖猫乃伊的骨骼数学课则 教授如何用爪子计算猫粮的剩余量 最特别的是社交货币猫砂有钱人家会在客厅铺上进口水晶猫砂吸引全城 猫咪来蹭蹭而普通家庭则会收集干净的猫砂团用作交换服务的筹码垃圾回 收日铲屎官们需要在社区中心用狗饼干换取猫砂这导致某些街区形成了猫 砂囤积户他们会用墙角的头像挂件警示邻居请勿踏入此处为私人猫砂仓 库 夜幕降临时城市进入睡眠模式但每栋楼的天台都亮起激光灯高处的猫咪们 顺着光柱追逐到深夜喵叫此起彼伏这大概就是毛茸茸城市的秘密它们用柔 软的爪子重塑世界让每一分存在都充满尊严每一种生活都裹着温暖的 Deepseek-R1 Response 当第一缕晨光掠过尖顶钟楼时整座城市开始舒展柔软的曲线高耸的猫爬架建 筑群表面覆盖着磨砂水晶板折射出粼粼金斑街道上浮动着鱼干与木天蓼混合 的香气裹着丝绸长袍的猫居民们竖起尾巴瞳孔缩成细线踩着肉垫在错落有 致的空中回廊间跳跃他们的足音被吸音材料吞噬整座城市如同播放着默片的 毛线球剧场 市政厅穹顶悬挂着巨型激光逗猫棒全息投影的红色光点精准落在每个工作日的 九点整猫公务员们用带有倒刺的舌头整理好文件将带金枪鱼印章的文书塞进 树洞状传输管道交易所里的期货交易员盯着不断跳动的鲜鱼指数爪子在触摸 屏上划出残影尾巴却诚实地拍打着真皮座椅这个月鲭鱼期货的波动显然让 某些人按捺不住本能 商业区的橱窗陈列着镶钻项圈与鸵鸟毛玩具戴着单片眼镜的猫掌柜趴在流线型 柜台后假寐尾巴尖却有节奏地敲击着隐藏的报警按钮转角处的露天茶馆里 穿着和服的老猫正用爪钩轻拨三味线学徒端着抹茶羊羹穿梭时总会被突然弹 起的尾巴绊个趔趄 黄昏时分霓虹灯管缠绕的广告塔亮起猫薄荷特供的荧光字穿铆钉皮衣的 年轻猫们聚集在地下俱乐部用改装过的电动梳毛器制造出震耳欲聋的嗡鸣戴 着夜视仪的巡逻队蹲踞在屋顶突然集体炸毛某栋公寓32层阳台上哲学家 猫又打翻了银河系投影仪整个街区的光影瞬间扭曲成万花筒 当月光为城市镀上银膜所有建筑外立面的抓挠板自动弹出亿万道猫爪划痕在 夜色里泛起幽蓝磷光这个瞬间整座城市变成巨大的信息素留声机记录着猫 文明特有的既慵懒又精确的生命韵律"
        }
    ],
    "affiliations": [
        "Singapore University of Technology and Design, Singapore",
        "Tsinghua University, Beijing, China"
    ]
}