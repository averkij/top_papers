{
    "paper_title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents",
    "authors": [
        "Yilei Jiang",
        "Yaozhi Zheng",
        "Yuxuan Wan",
        "Jiaming Han",
        "Qunzhong Wang",
        "Michael R. Lyu",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce a modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses a vision-language model to detect and label UI components, the planning agent constructs a hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive prompt-based synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into a scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 7 2 8 2 2 . 7 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SCREENCODER: ADVANCING VISUAL-TO-CODE GENERATION FOR FRONT-END AUTOMATION VIA MODULAR MULTIMODAL AGENTS Yilei Jiang1 Yaozhi Zheng1 Yuxuan Wan2 Jiaming Han1 Qunzhong Wang1 Michael R. Lyu2 Xiangyu Yue1 CUHK 1MMLab & 2ARISE Lab yljiang@link.cuhk.edu.hk, xyyue@ie.cuhk.edu.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "Automating the transformation of user interface (UI) designs into front-end code holds significant promise for accelerating software development and democratizing design workflows. While recent large language models (LLMs) have demonstrated progress in text-to-code generation, many existing approaches rely solely on natural language prompts, limiting their effectiveness in capturing spatial layout and visual design intent. In contrast, UI development in practice is inherently multimodal, often starting from visual sketches or mockups. To address this gap, we introduce modular multi-agent framework that performs UI-to-code generation in three interpretable stages: grounding, planning, and generation. The grounding agent uses vision-language model to detect and label UI components, the planning agent constructs hierarchical layout using front-end engineering priors, and the generation agent produces HTML/CSS code via adaptive promptbased synthesis. This design improves robustness, interpretability, and fidelity over end-to-end black-box methods. Furthermore, we extend the framework into scalable data engine that automatically produces large-scale image-code pairs. Using these synthetic examples, we fine-tune and reinforce an open-source VLM, yielding notable gains in UI understanding and code quality. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Our code is made publicly available at https://github.com/leigest519/ScreenCoder"
        },
        {
            "title": "INTRODUCTION",
            "content": "Automating front-end engineering and user interface (UI) design is critical step toward scalable and efficient software development. It not only accelerates implementation cycles and reduces engineering cost, but also empowers non-developers to participate in interface creation (Si et al., 2024). With the emergence of large language models (LLMs), recent systems such as Qwen3 have made significant progress in generating front-end code directly from natural language instructions (Qwen, 2025; Bolt, 2025). While text-based generation has advanced, it faces several key limitations. First, generating detailed UI layouts from text alone requires extremely long and verbose prompts to capture component structure, styling, and spatial relationships. This imposes high burden on users and leads to brittle model performance. Second, even with elaborate descriptions, it remains difficult to specify fine-grained visual design, such as component proportions, spacing, or alignment using language alone. Third, and most importantly, in practical design workflows, UI designers typically begin with sketches or mockups, not paragraphs of text. Relying solely on textual input, therefore, deviates from standard design pipelines and is sub-optimal for real-world deployment. Equal contribution. Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "To bridge this gap, vision-language models (VLMs) offer the promise of directly interpreting UI design images and translating them into structured code (Yang et al., 2023). While conceptually appealing, we find that current VLMs struggle to perform well on this task. Our analysis reveals that UI-to-code generation requires unified capabilities across visual understanding, structural layout planning, and domain-specific code synthesis, challenges that existing VLMs are not designed to handle holistically. Empirically, we identify two recurring failure modes: (1) understanding errors, where components are missed or misclassified (e.g., omitted sidebars, incorrect button text), and (2) planning errors, where components are placed incorrectly or violate layout constraints (e.g., headers rendered below content, navigation bars misaligned). Moreover, generating high-quality HTML/CSS code demands domain-specific knowledge of front-end engineering, including layout design, container hierarchies, and responsive composition patterns (Chen et al., 2018; Nguyen & Csallner, 2015; Lelli et al., 2015; Moran et al., 2018), none of which are explicitly modeled in current VLMs. To address these limitations, we propose modular multi-agent framework that decomposes the UI-to-code task into three interpretable stages: grounding, planning, and generation. The grounding agent leverages vision-language model to localize and semantically label key UI regions. The planning agent then constructs hierarchical layout tree using domain knowledge of web layout systems. Finally, the generation agent produces HTML and CSS code via adaptive prompt-based synthesis, incorporating both layout context and optional user instructions to support interactive design. This decomposition introduces architectural modularity, enabling more robust component recognition, layout planning, and structured code generation than end-to-end black-box methods. Experiments show that our framework achieves state-of-the-art performance in layout fidelity, structural coherence, and generation quality. Beyond inference-time improvements, our system also functions as scalable data engine for training future VLMs. By generating large volumes of image-code pairs, we use the system to bootstrap vision-language model training. Specifically, we apply cold-start supervised fine-tuning followed by reinforcement learning to improve Qwen2.5-VL, resulting in substantial gains in its UI understanding and code generation capabilities (Bai et al., 2025). Thus, our framework not only delivers strong generation performance, but also contributes practical path for scalable, high-quality dataset creation and model alignment. To sum up, our contributions are as follows: We conduct systematic investigation into the limitations of existing vision-language models (VLMs) on UI-to-code tasks, identifying key failure modes in both visual understanding (e.g., missing or misclassified components) and layout planning (e.g., incorrect spatial relationships). We propose modular multi-agent framework that decouples the UI-to-code generation process into grounding, planning, and generation stages. This design incorporates domain knowledge of front-end engineering and enables interpretable, high-fidelity code synthesis with interactive support. We introduce data engine built upon our framework that automatically generates largescale UI-code pairs. Using this, we perform cold-start supervised fine-tuning and reinforcement learning on an open-source VLM, resulting in substantial improvements in UI understanding and generation quality."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 MULTIMODAL LARGE LANGUAGE MODELS The recent emergence of multimodal large language models (MLLMs) has opened up new opportunities for visual reasoning and structured generation. These models are capable of jointly processing images and text through unified vision-language pipelines. Early architectures such as VisualGPT (Chen et al., 2022) and Frozen (Tsimpoukelli et al., 2021) demonstrated the effectiveness of using pre-trained LLMs as decoders over visual features. Flamingo (Alayrac et al., 2022) introduced gated cross-attention to align image encoders and LLMs more effectively, while BLIP-2 (Li et al., 2023) employed lightweight Q-Former to bridge modalities with minimal overhead."
        },
        {
            "title": "Preprint",
            "content": "More recent systems have significantly scaled up this paradigm. PaLM-E (Driess et al., 2023) integrated continuous sensory inputs from robotic platforms into unified MLLM architecture. Gemini 2.5 (Google, 2024) and GPT-4o (OpenAI, 2024) further pushed the boundaries of multimodal understanding, achieving impressive performance in visual perception, spatial reasoning, and multimodal task completion. These capabilities have enabled applications ranging from OCR-free reasoning to website generation from screenshots (Zhu et al., 2023). However, while MLLMs exhibit strong general-purpose reasoning capabilities, they often underperform on domain-specific structured generation tasks, such as UI-to-code synthesis. This gap is due in part to the lack of inductive biases for spatial layout reasoning, hierarchical planning, and code structuring, which are critical in front-end development. Moreover, current MLLMs lack architectural modularity, making it difficult to inject task-specific priors or interpret their intermediate reasoning steps."
        },
        {
            "title": "2.2 VISUAL-TO-CODE GENERATION",
            "content": "Early efforts in transforming visual UI designs into code leveraged convolutional neural networks (CNNs) to extract visual features from graphical interfaces. For example, models like pix2code (Beltramelli, 2018) employed CNNs and LSTMs to translate UI screenshots into domain-specific languages (DSLs). However, DSL-based outputs lack generalizability and are rarely used in real-world front-end development (Xu et al., 2021). Later works addressed this by targeting more generalpurpose formats like HTML and CSS (Chen et al., 2018), improving applicability to practical web development. Subsequent advances focused on improving component recognition and layout understanding. WebParse (Cizotto et al., 2023), for instance, incorporated class activation mapping to enhance interpretability and segmentation accuracy without dense annotation. Interaction2Code (Xiao et al., 2024; 2025) extended this paradigm by introducing interaction-aware code generation and repair, while MRWeb (Wan et al., 2024) proposed resource-aware generation of multi-page applications, moving beyond single-screen synthesis. Apart from CNN-based methods, other visual recognition strategies emerged. Techniques combining computer vision and OCR (Nguyen & Csallner, 2015; Natarajan & Csallner, 2018) extracted textual and structural information to reconstruct UI hierarchies, though these approaches often struggled with noisy or ambiguous visual inputs. Screen parsing methods such as ScreenParse (Wu et al., 2021) modeled element detection and hierarchy prediction using object detectors (e.g., Faster RCNN) and stack-based parsers, enabling partial structural reasoning. Nevertheless, most of these models are domain-specific, handcrafted, or brittle in the presence of real-world visual complexity. Despite these contributions, visual-to-code generation remains difficult challenge due to the need for joint understanding of visual semantics, spatial layouts, and domain-specific coding patterns. Many prior models rely on synthetic datasets, which can limit generalization to realistic UI designs. Furthermore, they often operate as black-box models without decomposable or interpretable intermediate stages, making it difficult to integrate domain knowledge or support human-in-the-loop design workflows. More recently, DCGen (Wan et al., 2025), LayoutCoder (Wu et al., 2025) and UICopilot (Gui et al., 2025) employ divide-and-conquer-based approaches to automate the translation of webpage design to UI code, but their performances are largely constrained by their heuristicbased segmentation method. In contrast to previous work, our approach introduces modular multi-agent framework that decomposes the visual-to-code task into interpretable subtasks: grounding, planning, and generation. This decomposition enables explicit reasoning over UI structure and leverages domain-specific priors in front-end engineering. Furthermore, our system functions as scalable data engine for training future MLLMs, addressing the scarcity of high-quality image-code datasets and offering practical path for improving model alignment in structured generation scenarios."
        },
        {
            "title": "3 METHOD",
            "content": "We propose modular, multi-agent framework for UI-to-code generation that decomposes the complex task into three sequential agents: grounding, planning, and generation. Each agent is specialized for distinct sub-problem, allowing the system to leverage both visual understanding and structured reasoning in coordinated manner. The grounding agent first identifies and labels key structural components in the UI image using vision-language model. The planning agent then organizes these components into hierarchical layout guided by domain knowledge of CSS Grid-based web"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of ScreenCoder. Given UI screenshots or design sketches as input, the Grounding Agent first detects and labels key components (e.g., header, navbar, sidebar, content). The Planning Agent organizes these components into hierarchical layout using front-end engineering priors. The Generation Agent synthesizes initial HTML code with placeholders, followed by content mapping to produce the final webpage and code. design. Finally, the generation agent translates this structured layout into executable HTML/CSS code using adaptive, prompt-based language modeling. This design not only improves generation accuracy and layout faithfulness but also enables interactive design through natural language instructions embedded directly into the generation process. Our overall framework is shown in Figure 1. 3.1 GROUNDING AGENT The grounding agent serves as the perceptual front-end of our multi-agent UI2Code framework. Its objective is to detect and semantically label major structural components within UI image. Crucially, this design choice, assigning explicit semantic labels to grounded regions, is motivated by the need to support interactive and language-driven UI design. By producing labels such as sidebar, header, navigation, and main content, the system enables downstream agents and end users to reference, manipulate, or modify specific components through natural language prompts (e.g., move the navigation to the top or resize the sidebar). To accomplish this, the grounding agent employs Vision-Language Model (VLM) to detect regions corresponding to fixed set of UI elements: = {sidebar, header, navigation}. (1) We prompt the VLM with explicit queries such as Where is the sidebar?, Locate the header area, or Identify the navigation bar. The VLM returns set of grounded regions in the form of bounding boxes and their associated labels: = {(bi, li) li L}N where each bi = (xi, yi, wi, hi) denotes bounding box in pixel coordinates, and li is the predicted label. Unlike traditional object detection, the use of VLM allows grounding to be flexibly guided by textual prompts, making the system extensible and adaptable to new UI elements or abstract concepts in future iterations. i=1 , (2) To ensure consistency and reliability, the grounding agent performs several post-processing operations: Deduplication and Conflict Resolution: Multiple boxes for the same label (e.g., two candidates for navigation) are filtered using class-specific non-maximum suppression (NMS), retaining only the most confident or spatially canonical region."
        },
        {
            "title": "Preprint",
            "content": "Fallback Recovery: If required component is not returned by the VLM, fallback heuristic is invoked based on spatial priors (e.g., wide, short box near the top is likely header or navigation bar). Main Content Inference: The main content region is not explicitly detected by the VLM. Instead, it is inferred as the largest remaining rectangular area not overlapping with any detected component: main content = max-rect (cid:32) (cid:33) bi . (cid:91) i=1 (3) This ensures consistent and meaningful definition of the content region without requiring it to be explicitly grounded. We also implement variant where the VLM is directly prompted to generate only the main content section. Interestingly, we observe that this approach offers distinct advantages depending on the characteristics of the input screenshot. The final output of the grounding agent is layout dictionary: Layout = {l (cid:55) bl {main content}}. (4) Each bl is raw pixel-space bounding box mapped to its semantic label l. This output serves as the foundation for layout interpretation in the planning agent and semantic condition grounding for interactive manipulation in downstream components. 3.2 PLANNING AGENT The planning agent receives semantic labels and bounding-box coordinates from the grounding stage and constructs lightweight hierarchical structure to provide the necessary structural context for code generation. This hierarchy is derived using simple spatial heuristics and compositional rules, without relying on computationally intensive optimization procedures. Given the set of labelled regions: Layout = (cid:8)l (cid:55) bl (cid:12) (cid:12) {sidebar, header, navigation, main content}(cid:9), (5) the agent organizes these regions into hierarchical layout tree , whose nodes capture spatial adjacency and reflect common web design patterns. The hierarchy is anchored by viewport-filling root container with the style position: 100%. Each top-level region is mapped to norrelative; width: 100%; height: malized bounding box (lr, tr, wr, hr) and rendered as an absolutely positioned .box using inline percentage-based styles. that further regions contain subdivisions, the inner <div inserts For rule .box > .container { display: class=\"container grid\">, where grid; } establishes CSS Grid canvas. Child components are arranged on this grid using Tailwind CSS utility classes such as grid-cols-*, gap-*, and bg-gray-400, among others. Each node in is annotated with grid-template configurations and ordering metadata, resulting in compact and interpretable layout specification that can be directly compiled into HTML/CSS code using CSS Grid and Tailwind. agent the an 3.3 GENERATION AGENT The generation agent synthesizes executable front-end code by translating the semantic layout tree into HTML and CSS. This is achieved through single-stage, prompt-driven generation process. Rather than relying on hand-crafted templates or fixed mappings, the agent constructs natural language prompt for each component in the layout tree and uses language model to generate the corresponding code. The input includes the layout tree , where each node is annotated with semantic label (e.g., header, sidebar), and an optional user instruction u, which describes desired design modifications or behaviors. For each node in , the agent constructs an adaptive prompt that reflects both the semantic identity of the region and its layout context. The prompt is not fixed, but varies based on"
        },
        {
            "title": "Preprint",
            "content": "the label and the spatial or functional role of the component. These adaptive prompts provide highlevel cues about what kind of element should be generated and how it should behave or appear. The user instruction, if provided, is appended to the prompt. This enables the generation agent to support interactive design, allowing users to modify or customize components using natural language. The combined prompt contains both structural context and intent, guiding the generation model toward producing code that is semantically aligned and responsive to user specifications. The generated HTML/CSS for each component is assembled according to the tree structure, preserving hierarchy, ordering, and layout configuration as specified by the planning agent. Semantic consistency is maintained throughout, and component-level modularity is preserved to support further refinement or reuse. By encoding layout semantics and user-driven customization into natural language prompts, the generation agent enables flexible and interpretable UI code synthesis, closing the loop between visual grounding, structural planning, and interactive intent."
        },
        {
            "title": "3.4 PLACEHOLDER MAPPING",
            "content": "We propose component-level image restoration strategy based on UI element detection to restore real images on webpage. Websites often feature real images, like background pictures and profile images, which are also present in screenshots. However, since actual image sources cannot be provided during generation, common practices replace such images with uniform gray placeholders (Si et al., 2025; Gui et al., 2025; Wan et al., 2025; Wu et al., 2025). This reduces visual fidelity and requires developers to manually replace the placeholders. To address this challenge, we propose component-level image restoration strategy, leveraging UI element detection to restore real images to their proper places. For the original screenshot, we apply traditional UI Element Detection (UIED) method (Xie et al., 2020), yielding set of detected bounding boxes = {e1, e2, . . . , en}, where each ej represents UI component. These elements are then partitioned into region-specific subsets based on the layout structure inferred from the generated code, resulting in Eheader, Esidebar, Enavigation and Emain. For each region, we estimate localized affine transformation to align UIED coordinates with the placeholder space. Given the transformed UIED bboxes (ej) and set of placeholder bboxes = {pi}, we construct cost matrix Ci,j using the negative Complete IoU (CIoU) between placeholder boxes pi and transformed detections (ej): Ci,j = CIoU(pi, (ej)). (6) The optimal one-to-one assignment is obtained by solving the bipartite matching problem via the Hungarian Algorithm. The final HTML code is obtained by extracting the matched image patches from the original screenshot and replacing the grey placeholders in the generated code with high-fidelity crops, thereby restoring both the visual and semantic consistency of the rendered UI."
        },
        {
            "title": "4 ENHANCING VLMS WITH SCALABLE DATA GENERATION AND",
            "content": "DUAL-STAGE POST-TRAINING Beyond high-quality inference, our framework also serves as scalable data engine capable of generating training data to enhance the capabilities of vision-language models (VLMs) in UI-to-code generation. In this section, we demonstrate that data generated by our system can be effectively used to improve open-source VLMs through two-stage training pipeline: (1) supervised fine-tuning (SFT) as cold start and (2) reinforcement learning (RL) with guided reward optimization. 4.1 COLD-START SUPERVISED FINE-TUNING We first use our framework to automatically generate large-scale dataset consisting of UI design images paired with accurate HTML/CSS code. Each data point is produced using our grounding, planning, and adaptive generation pipeline, ensuring that the training samples reflect structurally coherent layouts and semantically rich annotations. We apply this dataset to cold-start fine-tune an open-source VLM, Qwen-VL-2.5, which has not been previously exposed to UI design-to-code tasks. The model is trained with the standard autoregressive"
        },
        {
            "title": "Preprint",
            "content": "language modeling objective on the generated code, conditioned on the input image and an adaptive textual prompt (describing the semantic label and layout context of the region). This stage enables the model to acquire fundamental alignment between visual layout structure and code syntax."
        },
        {
            "title": "4.2 REINFORCEMENT LEARNING WITH VISUAL-SEMANTIC REWARDS",
            "content": "To refine the models alignment with layout structure and visual fidelity, we adopt reinforcement learning stage based on Group Relative Policy Optimization (GRPO) (Shao et al., 2024). Let πθ denote the model policy and R(πθ) the expected reward. We optimize: max θ E(x,y)πθ [R(x, y)] , (7) where is the input image and the generated code. We define composite reward R(x, y) that integrates three complementary criteria, each inspired by our evaluation metrics: Block-Match Reward Rblock: Measures layout completeness and precision. Let Amatch be the total area of matched blocks and Aunion the union area of all reference and predicted blocks: Rblock = . (8) Amatch Aunion Text Similarity Reward Rtext: Assesses textual accuracy using the Sørensen-Dice similarity between matched pairs (rp, gq) : Rtext = 2 rp gq rp + gq . (9) Position Alignment Reward Rpos: Measures spatial consistency between matched elements. Let (xp, yp) and (xq, yq) be normalized block centers: Rpos = 1 max (xp xq, yp yq) . The final reward is weighted sum: R(x, y) = λ1Rblock + λ2Rtext + λ3Rpos. (10) (11) These weights λi are tuned to balance semantic fidelity and visual precision. This reward function guides the policy toward producing syntactically valid and visually faithful HTML/CSS outputs."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate our multi-agent framework and dual-stage post training from two complementary perspectives: (1) visual fidelity and semantic consistency of the generated webpages, and (2) its effectiveness as scalable data engine for fine-tuning vision-language models (VLMs). Following Design2Code, we assess the quality of generated HTML/CSS by comparing the rendered output against ground-truth webpage screenshots using both high-level and low-level metrics. 5.1 DATASET CONSTRUCTION To support robust learning of UI-to-code generation, we construct large-scale dataset comprising 50,000 pairs of webpage screenshots and corresponding HTML/CSS code, collected from wide range of real-world and conceptual sources. Our dataset spans diverse domains and layout patterns to ensure broad generalization. Specifically, we include the following categories: Dashboards: Analytics panels, admin interfaces, and system monitoring views. Form Pages: Login, registration, and checkout pages from major platforms. E-commerce Platforms: Pages from Taobao, Amazon, and Shopify, covering both product listings and purchase flows."
        },
        {
            "title": "Preprint",
            "content": "Model Block Text Position Color CLIP GPT-4o GPT-4V Gemini-2.5-Pro LLaVA 1.6-7B DeepSeek-VL-7B Qwen2.5-VL Seed1.5-VL 0.730 0.926 0.718 0.918 0.727 0.924 0.676 0.894 0.683 0.899 0.707 0.919 0.713 0.939 0.755 0.946 ScreenCoder (Agentic) ScreenCoder (Finetuned) 0.733 0.935 0.811 0.802 0.809 0.678 0.693 0.693 0. 0.840 0.825 0.791 0.782 0.788 0.702 0.709 0.715 0.799 0.808 0.796 0.869 0.863 0.867 0.830 0.838 0.849 0.869 0.877 0.870 Table 1: Automatic evaluation results across five metrics: block match, text similarity, positional alignment, color consistency, and CLIP-based visual similarity. Media Platforms: Web versions of YouTube and TikTok, including video feeds and content details. Social Networks: Interfaces from Twitter and Instagram, including timelines and profile pages. Search Engines: Front pages and result layouts from Google, Baidu, and Bing. Mail Services: Web interfaces of Gmail and Outlook, including inbox and compose screens. Personal and Portfolio Pages: We include template-based personal websites, such as cator dog-themed placeholder content to avoid privacy issues, as well as link-in-bio aggregators and resume/CV-style layouts. Design Drafts and Concepts: We also include synthetic and community-shared drafts to enhance structural and stylistic diversity. This dataset provides rich training corpus covering both functional and aesthetic variation in web UI design. 5.2 EVALUATION PROTOCOL Dataset. We construct new benchmark comprising 3,000 high-quality UI-image/code pairs to facilitate rigorous evaluation of UI-to-code generation models. Each pair includes real-world UI design screenshot and its corresponding ground-truth HTML/CSS implementation. To ensure diversity and practical relevance, samples are drawn from various domains such as dashboards, e-commerce, social media, and content platforms. We manually filter and verify each pair to ensure visual correctness, semantic fidelity, and structural consistency. The benchmark covers wide spectrum of component types (e.g., buttons, tables, inputs), layout patterns (e.g., grids, nested containers), and visual styles (e.g., minimal, material, skeuomorphic). During evaluation, each model is given the input screenshot and tasked with generating HTML/CSS code, which is then rendered and compared against the ground-truth rendering to assess both structural and visual alignment. Evaluation Metric. We follow the evaluation metric of Design2Code (Si et al., 2025) and compare the visual output of the generated code to the ground-truth screenshot, focusing on layout accuracy, content fidelity, and stylistic alignment. Specifically, we evaluate visual similarity between the generated code and the original UI design using both high-level and low-level metrics. For high-level assessment, we compute CLIP similarity between the rendered output and the reference screenshot, masking out textual regions to avoid bias and using the ViT-B/32 variant of CLIP (Radford et al., 2021). For low-level evaluation, we extract OCR-based visual blocks from both images and align them using text similarity. Based on the matched elements, we measure block reproduction accuracy, textual consistency, spatial alignment, and color similarity. These metrics collectively offer holistic view of the models visual fidelity and layout correctness."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Qualitative example of the UI-to-code pipeline, showcasing VLM-generated functional partitions, hierarchical layout tree , and front-end code generation. 5.3 BASELINES AND COMPARISONS We benchmark our approach against comprehensive suite of state-of-the-art vision-language models (VLMs), including GPT-4o (OpenAI, 2024), GPT-4V (OpenAI, 2023), Gemini-2.5-Pro (Google, 2024), LLaVA 1.6-7B (Liu et al., 2023), DeepSeek-VL-7B (Lu et al., 2024), Qwen2.5-VL (Bai et al., 2025), and Seed1.5-VL (Guo et al., 2025). Each model is prompted with raw UI design image and simple instruction to generate the corresponding HTML/CSS code. Our method is implemented based on the open-sourced Qwen2.5-VL model (Bai et al., 2025). Table 1 reports the performance of all methods across five metrics: block match, text similarity, position alignment, color consistency, and CLIP-based visual similarity. Our model surpasses all open-source models and performs competitively against the strongest proprietary systems. 5.4 QUALITATIVE RESULTS As shown in the figure below, we efficiently generated high-quality UI code following the pipeline described earlier. For given example, as illustrated above, the Vision-Language Model (VLM) first generated partitions for functional regions such as the sidebar. Subsequently, based on these partitions and the image, we constructed hierarchical layout tree . Finally, using the UI and the hierarchical layout tree , we generated the front-end source code."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Our proposed multi-agent framework introduces structured and interpretable approach to UI-tocode generation, offering both inference-time benefits and scalable training data generation. While the results demonstrate clear improvements over existing methods, several avenues remain open for future exploration. Interactive Design and Human-in-the-Loop Feedback. One key strength of our modular pipeline is its potential to support interactive design iteration. Since each stage, grounding, planning, and generation, is disentangled, user feedback can be incorporated at different abstraction levels. For instance, designers can manually adjust the layout tree or re-prompt specific components without restarting the entire process. Future work may further enhance this interactivity by integrating real-time preview, editable intermediate representations, and dialogue-based refinement."
        },
        {
            "title": "Preprint",
            "content": "Generality Across Design Domains. Although our experiments focus on web UI design, the modularity of the framework suggests applicability to broader domains such as mobile apps, desktop interfaces, or even design-to-code translation for game UIs. Adapting the grounding vocabulary and layout heuristics to these domains would require minimal architectural changes, and can be facilitated by expanding the component label set and domain-specific planning logic. Scalability and Real-World Deployment. The scalability of the data engine introduces opportunities for continuous learning in deployment environments. By logging user corrections or manual edits to generated code, the system could incrementally refine its grounding or generation modules, potentially leading to personalized UI code assistants. Ensuring robustness to noisy or lowresolution input screenshots and optimizing inference latency remain important considerations for practical adoption."
        },
        {
            "title": "7 CONCLUSION",
            "content": "We present ScreeenCoder, modular multi-agent framework for UI-to-code generation that addresses key limitations of existing end-to-end vision-language models. By decomposing the task into grounding, planning, and adaptive code generation stages, our system enables robust, interpretable, and high-fidelity front-end code synthesis. Furthermore, we extend the pipeline into scalable data engine, which supports dual-stage post-training of VLMs through supervised fine-tuning and reinforcement learning. Experimental results across diverse UI designs demonstrate state-ofthe-art performance in visual fidelity, structural alignment, and code correctness. Our work not only offers practical solution for automating front-end development, but also lays the foundation for broader research in multimodal program synthesis, interactive design systems, and dataset-driven vision-language alignment."
        },
        {
            "title": "REFERENCES",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:23716 23736, 2022. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. T. Beltramelli. pix2code: Generating code from graphical user interface screenshot. In Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems, pp. 16, 2018. Bolt. Introduction to bolt, 2025. intro-bolt. Accessed: 2025-07-23. URL https://support.bolt.new/building/ C. Chen, T. Su, G. Meng, Z. Xing, and Y. Liu. From ui design image to gui skeleton: neural machine translator to bootstrap mobile gui implementation. In Proceedings of the 40th International Conference on Software Engineering, pp. 665676, 2018. Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adapIn Proceedings of the IEEE/CVF tation of pretrained language models for image captioning. Conference on Computer Vision and Pattern Recognition, pp. 1803018040, 2022. A. A. J. Cizotto, R. C. T. de Souza, V. C. Mariani, and L. dos Santos Coelho. Web pages from mockup design based on convolutional neural network and class activation mapping. Multimedia Tools and Applications, pp. 127, 2023."
        },
        {
            "title": "Preprint",
            "content": "Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied mulIn International Conference on Machine Learning, pp. 84698488. timodal language model. PMLR, 2023. Google. Gemini api, 2024. URL https://ai.google.dev/gemini-api. Accessed: 202406-06. Yi Gui, Yao Wan, Zhen Li, Zhongyi Zhang, Dongping Chen, Hongyu Zhang, Yi Su, Bohua Chen, Xing Zhou, Wenbin Jiang, and Xiangliang Zhang. Uicopilot: Automating ui syntheIn Proceedings of the ACM on sis via hierarchical code generation from webpage designs. Web Conference 2025, WWW 25, pp. 18461855, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400712746. doi: 10.1145/3696410.3714891. URL https://doi.org/10.1145/3696410.3714891. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei Liu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin Wu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan Ding, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen, Yanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling, Yujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang, Chaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou, Chengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang, Gang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang Hao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen, Jianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng, Junfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, Ke Shen, Ke Wang, Keyu Pan, Kun Zhang, Kunchang Li, Lanxin Li, Lei Li, Lei Shi, Li Han, Liang Xiang, Liangqiang Chen, Lin Chen, Lin Li, Lin Yan, Liying Chi, Longxiang Liu, Mengfei Du, Mingxuan Wang, Ningxin Pan, Peibin Chen, Pengfei Chen, Pengfei Wu, Qingqing Yuan, Qingyao Shuai, Qiuyan Tao, Renjie Zheng, Renrui Zhang, Ru Zhang, Rui Wang, Rui Yang, Rui Zhao, Shaoqiang Xu, Shihao Liang, Shipeng Yan, Shu Zhong, Shuaishuai Cao, Shuangzhi Wu, Shufan Liu, Shuhan Chang, Songhua Cai, Tenglong Ao, Tianhao Yang, Tingting Zhang, Wanjun Zhong, Wei Jia, Wei Weng, Weihao Yu, Wenhao Huang, Wenjia Zhu, Wenli Yang, Wenzhi Wang, Xiang Long, XiangRui Yin, Xiao Li, Xiaolei Zhu, Xiaoying Jia, Xijin Zhang, Xin Liu, Xinchen Zhang, Xinyu Yang, Xiongcai Luo, Xiuli Chen, Xuantong Zhong, Xuefeng Xiao, Xujing Li, Yan Wu, Yawei Wen, Yifan Du, Yihao Zhang, Yining Ye, Yonghui Wu, Yu Liu, Yu Yue, Yufeng Zhou, Yufeng Yuan, Yuhang Xu, Yuhong Yang, Yun Zhang, Yunhao Fang, Yuntao Li, Yurui Ren, Yuwen Xiong, Zehua Hong, Zehua Wang, Zewei Sun, Zeyu Wang, Zhao Cai, Zhaoyue Zha, Zhecheng An, Zhehui Zhao, Zhengzhuo Xu, Zhipeng Chen, Zhiyong Wu, Zhuofan Zheng, Zihao Wang, Zilong Huang, Ziyu Zhu, and Zuquan Song. Seed1.5-vl technical report, 2025. URL https://arxiv.org/abs/2505.07062. Valeria Lelli, Arnaud Blouin, and Benoˆıt Baudry. Classifying and qualifying gui defects. 2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST), pp. 110, 2015. URL https://api.semanticscholar.org/CorpusID:2288032. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 1973019742. PMLR, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024. URL https: //arxiv.org/abs/2403.05525."
        },
        {
            "title": "Preprint",
            "content": "Kevin Moran, Boyang Li, Carlos Bernal-Cardenas, Dan Jelf, and Denys Poshyvanyk. Auto2018 IEEE/ACM 40th Internamated reporting of gui design violations for mobile apps. tional Conference on Software Engineering (ICSE), pp. 165175, 2018. URL https://api. semanticscholar.org/CorpusID:3634687. S. Natarajan and C. Csallner. P2a: tool for converting pixels to animated mobile application user interfaces. In Proceedings of the 5th International Conference on Mobile Software Engineering and Systems, pp. 224235, 2018. T. A. Nguyen and C. Csallner. Reverse engineering mobile application user interfaces with remaui (t). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), pp. 248259, 2015. OpenAI. Gpt-4v(ision) system card, September 2023. URL https://cdn.openai.com/ papers/GPTV_System_Card.pdf. Accessed: 2025-07-24. OpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o/. Accessed: 2024-06-06. Qwen. Qwen3 blog post, 2025. URL https://qwenlm.github.io/zh/blog/qwen3/. Accessed: 2025-07-23. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. URL https://arxiv.org/abs/2103.00020. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. URL https://arxiv.org/pdf/2402.03300. Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: How far are we from automating front-end engineering? ArXiv, abs/2403.03163, 2024. URL https: //api.semanticscholar.org/CorpusID:268248801. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2Code: Benchmarking multimodal code generation for automated front-end engineering. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 39563974, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025. naacl-long.199. URL https://aclanthology.org/2025.naacl-long.199/. Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems, 34:200212, 2021. Yuxuan Wan, Yi Dong, Jingyu Xiao, Yintong Huo, Wenxuan Wang, and Michael R. Lyu. Mrweb: An exploration of generating multi-page resource-aware web code from ui designs. ArXiv, abs/2412.15310, 2024. URL https://api.semanticscholar.org/ CorpusID:274965541. Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang, Shuqing Li, Yintong Huo, and Michael Lyu. Divide-and-conquer: Generating ui code from screenshots. Proceedings of the ACM on Software Engineering, 2(FSE):20992122, June 2025. ISSN 2994-970X. doi: 10.1145/3729364. URL http://dx.doi.org/10.1145/3729364. Fan Wu, Cuiyun Gao, Shuqing Li, Xin-Cheng Wen, and Qing Liao. Mllm-based ui2code automation guided by ui layout information. Proceedings of the ACM on Software Engineering, 2(ISSTA): 11231145, June 2025. ISSN 2994-970X. doi: 10.1145/3728925. URL http://dx.doi. org/10.1145/3728925."
        },
        {
            "title": "Preprint",
            "content": "J. Wu, X. Zhang, J. Nichols, and J. P. Bigham. Screen parsing: Towards reverse engineering of ui models from screenshots. In The 34th Annual ACM Symposium on User Interface Software and Technology, pp. 470483, 2021. Jingyu Xiao, Yuxuan Wan, Yintong Huo, Zhiyao Xu, and Michael R. Lyu. Interaction2code: How far are we from automatic interactive webpage generation? ArXiv, abs/2411.03292, 2024. URL https://api.semanticscholar.org/CorpusID:273821629. Jingyu Xiao, Ming Wang, Man Ho Lam, Yuxuan Wan, Junliang Liu, Yintong Huo, and Michael R. Lyu. Designbench: comprehensive benchmark for mllm-based front-end code generaArXiv, abs/2506.06251, 2025. URL https://api.semanticscholar.org/ tion. CorpusID:279244894. Mulong Xie, Sidong Feng, Zhenchang Xing, Jieshan Chen, and Chunyang Chen. Uied: hyIn Proceedings of the 28th ACM Joint Meeting on Eurobrid tool for gui element detection. pean Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2020, pp. 16551659, New York, NY, USA, 2020. Association for ComISBN 9781450370431. doi: 10.1145/3368089.3417940. URL https: puting Machinery. //doi.org/10.1145/3368089.3417940. Y. Xu, L. Bo, X. Sun, B. Li, J. Jiang, and W. Zhou. image2emmet: Automatic code generation from web user interface image. Journal of Software: Evolution and Process, 33(8):e2369, 2021. Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision). ArXiv, abs/2309.17421, 2023. URL https://api.semanticscholar.org/CorpusID:263310951. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv, abs/2304.10592, 2023. URL https://api.semanticscholar.org/CorpusID:258291930."
        }
    ],
    "affiliations": [
        "2ARISE Lab",
        "CUHK 1MMLab"
    ]
}