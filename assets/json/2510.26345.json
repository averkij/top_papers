{
    "paper_title": "MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data",
    "authors": [
        "Mykhailo Poliakov",
        "Nadiya Shvai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on https://github.com/mxpoliakov/MisSynth."
        },
        {
            "title": "Start",
            "content": "MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data"
        },
        {
            "title": "Mykhailo Poliakov and Nadiya Shvai",
            "content": "National University of Kyiv-Mohyla Academy, Kyiv, Ukraine. Contributing authors: mykhailo.poliakov@ukma.edu.ua; n.shvay@ukma.edu.ua; Abstract Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight finetuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on GitHub. Keywords: health misinformation, large language models, synthetic data generation, logical fallacy classification, parameter-efficient fine-tuning, retrieval-augmented generation"
        },
        {
            "title": "1 Introduction",
            "content": "Health-related misinformation has been identified as one of the major factors that deteriorate global health and lead to decrease in public trust in science (Brennen 1 5 2 0 2 0 3 ] . [ 1 5 4 3 6 2 . 0 1 5 2 : r et al. 2020). This threat is growing because all forms of falsehood are spreading farther and faster than truth online (Vosoughi et al. 2018). The problem is especially dangerous when real scientific findings are distorted. For instance, misleading reports often use selective and deceptive quotations of scientific work to support false claims (Beers et al. 2023). On the other hand, discredited and retracted research continues to be mentioned as valid, supporting arguments with empty research (Frederick 2023). These arguments use the credibility of the source to hide subtle logical fallacies. Detecting such fallacies is major challenge. It requires deep understanding of scientific context and logical reasoning. Often, the flawed thinking shortcuts that make readers susceptible to fallacies are more intuitive than the deliberate analysis required to debunk them (Lewandowsky et al. 2020). Even the largest large language models (LLMs) can perform poorly on this task. One recent benchmark highlights this performance gap by testing for implicit fallacious reasoning (Glockner et al. 2024). Other new datasets tools also show that LLMs lag far behind humans in identifying finegrained fallacies (Hong et al. 2024). comprehensive benchmark that unifies previous datasets further confirms these limitations (Helwe et al. 2024). This performance gap can be attributed to the scarcity of large, high-quality annotated datasets for training. We find that current methods are often insufficient. Traditional fact-checking systems are designed to find explicit counter-evidence (Nakov et al. 2021). Such systems are not suited for complex cases where evidence is slightly distorted rather than outright fabricated (Guo et al. 2022). Synthetic data can help address data scarcity (Møller et al. 2024). However, synthetic data often produces templated and unnatural examples. This creates critical distribution gap that risks training models to excel at detecting AI-generated misinformation while leaving them vulnerable to the diverse and unpredictable real-world misinformation (Li et al. 2024). We introduce MisSynth, new pipeline aiming to address this issue. Our novel technique employs retrieval-augmented generation (RAG) to produce realistic and context-sensitive synthetic data (Lewis et al. 2020). We use this data to fine-tune an LLM with parameter-efficient technique called Low-Rank Adaptation (LoRA) by Hu et al. (2021). Our experiments show this approach yields significant gains. For example, fine-tuned LLaMA 3.1 8B model improved its F1-score by over 35% (absolute gain) on the MISSCI test split. This demonstrates the effectiveness of our method, even with limited computational resources. Our primary contributions are as follows: We present MisSynth, novel RAG-based pipeline for generating high-quality synthetic data of logical fallacies. We show that fine-tuning with our synthetic data significantly improves an LLMs performance on the logical fallacy classification subtask of the MISSCI benchmark. We release the synthetic dataset generated by MisSynth (GPT-5 version) publicly. The main novelty is the integration of RAG with parameter-efficient fine-tuning (LoRA) specifically for logical fallacy classification. Unlike earlier data augmentation techniques that often produce templated or context-less examples, the MisSynth pipeline enforces same-source retrieval constraint. This crucial step ensures the generated synthetic arguments are grounded in the source scientific article and are 2 realistic. By utilizing this pipeline, we introduce an efficient and effective method for specializing large language models for complex scientific reasoning tasks, particularly in scenarios where high-quality annotated data is scarce. The rest of this paper is structured as follows. We first review related work. Then, we detail our methodology. Next, we present our experiments and results. Finally, we discuss our findings and suggest future research directions."
        },
        {
            "title": "2.4 MISSCI\nOur work uses a recent benchmark designed for fallacy detection. The MISSCI dataset\nprovides a formal framework for our task (Glockner et al. 2024). It models misinfor-\nmation as an argument where an inaccurate claim, ¯c, is supported by an accurate\npremise, P , and a fallacious premise, ¯P . The accurate premise alone does not support\nthe claim (P ̸⇒ ¯c), but the combination does (P ∪ ¯P ⇒ ¯c). Each fallacious step is a\ntriplet Ri = (si, ¯pi, f i), composed of scientific context si, a fallacious premise ¯pi, and\na fallacy class f i. The complete argument is represented as:",
            "content": "3 s0 =p0 , s1 p1 f1 , . . . , sN pN fN (1) The task involves identifying and classifying the fallacious premise pi and its type i. Our work focuses on the classification part of MISSCI. Its extension, MISSCIPlus, incorporates additional arguments identified in the original scientific texts into the original dataset (Glockner et al. 2025). Models must first find the relevant passage from the article before recognizing the fallacy."
        },
        {
            "title": "3 Methodology",
            "content": "Detecting health misinformation that misuses scientific claims is significant problem, partially due to the scarcity of real-world annotated data. This section describes MisSynth methodology, which tackles the data shortage. We generate synthetic data to fine-tune Large Language Models (LLMs) for this task. The complete process is shown in Figure 1. Our method first retrieves relevant text using RAG (3.1). An LLM then uses this text to create new fallacy examples (3.2). We use this new dataset to locally fine-tune model using LoRA (3.3). In addition, we detail our evaluation strategy (3.4)."
        },
        {
            "title": "3.1 RAG for publication context\nWe base synthetic examples on the same publication contexts that produce fallacious\nreasoning in MISSCI. For each instance in the dev split, we download the cited source\nS and segment it with a recursive character splitter (chunk size 512, overlap 64). Each\npassage dj ∈ S is embedded with a PubMedBERT biomedical encoder by Gu et al.\n(2021) ϕ, yielding",
            "content": "(2) We store the passages in the Langchains (Chase 2022) in-memory vector index along with metadata source(dj) = u. At retrieval time, we build the query from the inaccurate claim only, ej = ϕ(dj) Rm 4 Fig. 1 Overview of MisSynth synthetic data generation and fine-tuning pipeline. RAG retrieves an article excerpt (Ei) from source article (S) based on MISSCI claim (qi). This excerpt, along with dev split sample (A), is used by Generation LLM to create synthetic dataset (Dsyn). This dataset is then used to fine-tune model with LoRA, which is finally evaluated on the MISSCI test split. qi = c, and compute the cosine similarity: eqi = ϕ(qi) sim(qi, dj) = ej qi eqiej (3) (4) We then retrieve the top-k passages subject to same-source constraint, implemented as metadata filter: Rk(qi, u) = arg topk dj D, source(dj )=u sim(qi, dj) (5) with = 5. We concatenate the retrieved passages into single excerpt: Ei = concat(cid:0)Rk(qi, u)(cid:1). (6) which is then used to generate the final answer. This setup follows retrieval with dual encoders (one for the query and one for the RAG passages) and top-k similarity search, as well as multi-passage sequence for generation (Lewis et al. 2020). At the same time, the same-source filter enforces the MISSCI assumption that fallacious reasoning is based on the cited source."
        },
        {
            "title": "3.2 Synthetic Data\nLet an annotated MISSCI argument be A = (c, p0, R), where each reasoning step is\nRi = (si, pi, fi) with publication context si, fallacious premise pi\n, and fallacy class fi.\nFor each dev instance, we extract the set of gold fallacies and their classes:",
            "content": "5 , real ℓ format prompt with (c, p0, real, Ei), and use Generation LLM to produce , sreal ℓ real = (cid:8)(preal )(cid:9) (7) ℓ ℓ structured JSON. We generate two kinds of synthetic data:"
        },
        {
            "title": "3.2.1 Synthetic fallacious premises.\nWe sample K synthetic variants per dev split instance as triples of synthetic context,\nfallacious premise, and class:",
            "content": "(si,k, pi,k, fi,k) pθ (cid:16) s, p, (cid:12) (cid:12) c, p0, real, Ei (cid:17) , = 1, . . . , (8) Each item must use class from the fallacy inventory and be derived from the content of Ei."
        },
        {
            "title": "3.2.2 Synthetic claim–premise pairs.\nWhen enabled, we also sample M coherent claim / accurate-premise pairs supported\nby the same source and excerpt:",
            "content": "(9) to increase diversity of inputs, since each fallacious premises from the above (ci,m, p0,i,m) pθ = 1, . . . , (cid:0)c, p0 real, Ei (cid:1), contain the same real claimpremise pairs per instance."
        },
        {
            "title": "3.2.3 Prompting and parsing.",
            "content": "Prompts include the fallacy inventory (extracted from template file) and require strict JSON array with fields \"context\", \"fallacy\", and \"class\" (Appendices A, B). We skip instances with empty retrieval results or invalid JSON. The temperature of retrieval LLM is kept at 1.0, where applicable."
        },
        {
            "title": "3.2.4 Train/validation set for fine-tuning.\nWe convert synthetic items into instruction–completion pairs using MISSCI ’s \"classify\nwith definition\" template. For each synthetic fallacy:",
            "content": "xi,j = (cid:0)c, p0, si,j, pi,j (10) We form the training set Dsyn = {(xi,j, yi,j)}. The validation set uses only gold MISSCI dev examples (original interchangeable fallacies) formatted with the same template, ensuring that validation contains no synthetic completions. yi,j = \"Fallacy: ˆfi,j\" (cid:1), We also include random-baseline ablation that replaces synthetic contexts and premises with lorem ipsum while keeping answers intact, to test whether gains come from synthetic content rather than prompt template or answer structure."
        },
        {
            "title": "3.3 Fine-tuning\nWe adapt a frozen base model with parameters Φ0 using trainable LoRA (Hu et al.\n2021) parameters Θ while keeping Φ0 fixed. Let Z = (x, y) denote the training pairs\nproduced from Dsyn. We maximize the conditional likelihood with respect to the\nadapter parameters:",
            "content": "max Θ (cid:88) (cid:88) (x,y)Z t=1 log (cid:0)pΦ0+Φ(Θ) (ytx, y<t)(cid:1) (11) In LoRA, the task-specific increment Φ(Θ) is encoded by low-rank updates to selected linear projections, typically the attention projections Wq and Wv. For any adapted weight W0 Rdk in Φ0, we learn Rdr and Rrk with min d, and set = W0 + AB (12) α where only and are trainable and α is fixed scaling factor. All other parameters of Φ0 remain frozen."
        },
        {
            "title": "3.4 Evaluation\nWe evaluate the MisSynth pipeline by fine-tuning several LLMs using Low-Rank\nAdaptation (LoRA) and testing their performance on the classification sub-task of\nthe MISSCI benchmark. Our primary evaluation metrics are accuracy (Acc) and\nmacro-averaged F1-score (F1) on the MISSCI test split.",
            "content": "All fine-tuning experiments were performed using the LoRA technique with rank of = 8 on the attention projections (Wq and Wv). The training set Dsyn was generated from the MISSCI dev set using the Generation LLM at temperature of 1.0, where applicable. We use the gold MISSCI dev examples as consistent fine-tuning validation set across all runs (96 samples). All experiments were conducted locally on an M1 MacBook Pro with 32 GB of unified memory using the MLX framework by Hannun et al. (2023)."
        },
        {
            "title": "4 Results",
            "content": "This section details our experimental findings. We first optimize the synthetic data generation parameters in 4.1. We then select the best LLM to create the dataset in 4.2 and explore some of the created dataset statistics in 4.3. Finally, the most important results are presented in 4.4, where we benchmark several models fine-tuned on this data. These show the final benchmark performance. Our results confirm that fine-tuning with our synthetic data dramatically improves model performance."
        },
        {
            "title": "4.1 Optimization of Data Generation Parameters\nWe first analyzed the impact of varying the number of synthetic fallacious premises\n(K) and synthetic claim/premise pairs (M ) on the performance of a fine-tuned Phi-4\n(8-bit) model by Abdin et al. (2024). The results are presented in Table 1.",
            "content": "7 Table 1 Fine-tuned performance of Phi-4 (8-bit) with varying synthetic data parameters and . LoRA layers are 16. All fine-tuning runs were executed for 500 iterations. Performance is measured on the MISSCI test split. Generation LLM Val Loss 1 iter Val Loss 500 iters Acc F1 (macro) Train Samples 0 10 10 15 30 40 0 0 0 5 15 20 Vanilla Random baseline o4-mini o4-mini o4-mini o4-mini - 1.938 1.938 1.943 1.940 1.943 - 0.166 0.147 0.076 0.067 0.074 0.667 0.606 0.685 0.711 0.762 0.711 0.550 0.512 0.622 0.654 0.690 0.647 - 299 299 929 2344 2984 The vanilla Phi-4 model achieved an F1-score of 0.550. Fine-tuning consistently improved performance, demonstrating the effectiveness of the synthetic data, which is further validated by the significant drop in the validation loss from approximately 1.94 down to as low as 0.067 for all successful configurations. Notably, the Random baseline ablation underperformed (F1 of 0.512), despite showing an initial Val Loss of 1.938 dropping to 0.166, confirming that the model learned from the synthetic data, rather than the prompt template or answer structure alone. We observed maximum F1-score of 0.690 and maximum accuracy of 0.762 at = 30 and = 15. Increasing the data volume further to = 40 and = 20 led to decrease in performance, with the F1-score dropping to 0.647. Considering this peak in performance and the associated generation/fine-tuning costs, we selected the configuration = 30 and = 15 for subsequent experiments. This configuration achieved competitive F1-score of 0.690 (a 14% absolute gain over the vanilla model), representing the optimal setting with 2344 training samples and favorable validation loss reduction from 1.940 to 0.067."
        },
        {
            "title": "4.2 Selecting the Generation LLM\nNext, we investigated whether the quality of the LLM used for synthetic data genera-\ntion impacts the final fine-tuned model’s performance. Using the optimal parameters\n(K = 30, M = 15), we compared four different generator models (Table 2).",
            "content": "Table 2 Comparison of different LLMs used for generating synthetic data (Dsyn) for Phi-4 (8-bit) fine-tuning. = 30, = 15, LoRA layers are 16. All fine-tuning runs were executed for 500 iterations. Generation LLM Fine-tuned LLM Val Loss 1 iter Val Loss 500 iters Acc F1 (macro) o4-mini GPT-4.1 GPT-5 (medium) o3 Phi-4 (8-bit) Phi-4 (8-bit) Phi-4 (8-bit) Phi-4 (8-bit) 1.940 1.951 1.945 1.945 0.067 0.058 0.063 0.081 0.762 0.751 0.764 0. 0.690 0.653 0.705 0.621 8 We observed that the data generated by GPT-5 (medium) resulted in the highest F1-score (0.705) and accuracy (0.764), demonstrating its superior ability to generate high-quality training examples. Therefore, prioritizing maximum performance for this task, we selected GPT-5 (medium) as the best generation model for MisSynth, despite the higher generation cost."
        },
        {
            "title": "4.3 Optimal Synthetic Dataset\nWe publicly release our optimal synthetic dataset. We note that the dataset was gen-\nerated once using the final iteration of the MisSynth code with K = 30 and M = 15\n(GPT-5). This dataset is used for all subsequent experiments. The dataset was gener-\nated once. Table 3 details the distribution of fallacy categories. The synthetic dataset’s\ndistribution differs from the MISSCI splits. For instance, Fallacy of Exclusion com-\nprises a smaller portion (7.44%) compared to the test split (27.53%). Conversely,\nsome minority classes in the MISSCI test split, such as False Dilemma (4.19%) and\nImpossible Expectations (1.32%), are represented more frequently in the synthetic data\n(11.21% and 8.44%, respectively).",
            "content": "Table 3 Distribution of Fallacy Categories (Count and Percentage) across datasets. Fallacy Category MISSCI, dev split MISSCI, test split GPT-5 Dsyn dataset (K = 30, = 15) Ambiguity Biased Sample Fallacy Causal Oversimplification Fallacy of Division/Composition Fallacy of Exclusion False Dilemma False Equivalence Hasty Generalization Impossible Expectations 7 (7.29%) 10 (10.42%) 14 (14.58%) 7 (7.29%) 25 (26.04%) 8 (8.33%) 14 (14.58%) 6 (6.25%) 5 (5.21%) 44 (9.69%) 37 (8.15%) 73 (16.08%) 33 (7.27%) 125 (27.53%) 19 (4.19%) 85 (18.72%) 32 (7.05%) 6 (1.32%) 129 (14.32%) 84 (9.32%) 133 (14.76%) 73 (8.10%) 67 (7.44%) 101 (11.21%) 115 (12.76%) 123 (13.65%) 76 (8.44%) Overall 96 (100.00%) 454 (100.00%) 901 (100.00%) Table 4 shows the ROUGE recall (Lin 2004), measuring textual overlap between entities and their source article excerpt Ei. The synthetic Context (0.766) and Accurate Premise (0.862) show higher recall than the MISSCI dev split (0.635 and 0.741). In contrast, the synthetic Fallacy (0.493) and Claim (0.612) have lower recall than the dev split (0.608 and 0.642). This suggests that both synthetic data and MISSCI is well-grounded in the source article. Textual comparison of dataset entities are available in Appendix tables C1, C2, C3."
        },
        {
            "title": "4.4 Evaluation of Fine-tuned Models\nFinally, we benchmarked the performance gains achieved by fine-tuning various LLMs\nusing our GPT-5 Dsyn dataset (K = 30, M = 15). Table 5 compares the vanilla and\nfine-tuned performance.",
            "content": "9 Table 4 ROUGE recall between article excerpt Ei and entity. Dataset Entity ROUGE MISSCI dev split ROUGE GPT-5 Dsyn dataset (K = 30, = 15) Fallacy (K) Context (K) Claim (M ) Accurate Premise (M ) 0.608 0.635 0.642 0.741 0.493 0.766 0.612 0.862 Table 5 Comparison of different base models before and after fine-tuning with GPT-5 Dsyn (K = 30, = 15). All fine-tuning runs were executed for 500 iterations. Performance measured on the MISSCI test split. Fine-tuned LLM Gemma 3 (8-bit) LLaMA 3.1 (4-bit) LLaMA 2 (4-bit) Phi-4 (8-bit) Mistral Small 3.2 (4-bit) LLaMA 2 * GPT-4 * Val Loss 1 iter 3.324 2.451 2.145 1.945 2.124 - - Val Loss 500 iters 0.067 0.050 0.073 0.063 0.072 - - * Results by Glockner et al. (2024) Vanilla Acc Vanilla Fine Acc Fine F1 LLM Size LoRA Layers 0.531 0.414 0.326 0.667 0.698 0.577 0.738 0.377 0.334 0.218 0.550 0.553 0.464 0. 0.764 0.778 0.722 0.764 0.762 - - 0.691 0.711 0.681 0.705 0.718 - - 4B 8B 13B 15B 24B 70B - 32 32 32 16 16 - - The results confirm that the MisSynth significantly improves performance across different model architectures. All fine-tuned models showed substantial decreases in validation loss, with LLaMA 3.1 (8B) by Grattafiori et al. (2024) dropping from 2.451 to 0.050 and Gemma 3 (4B) by Team et al. (2025) dropping from 3.324 to 0.067, indicating successful adaptation to the fallacy classification task. The LLaMA 2 13B model (Touvron et al. 2023) showed the largest absolute improvement, increasing its F1-score from baseline of 0.218 to 0.681, alongside validation loss reduction from 2.145 to 0.073. The fine-tuned Mistral Small 3.2 model achieved the highest F1-score overall at 0.718 (a 16.5% absolute gain). Other models also showed strong performance, with LLaMA 3.1 achieving 0.711 (37.7% absolute gain) and Phi-4 reaching 0.705 (15.5% absolute gain). Notably, several fine-tuned smaller models outperformed the proprietary model. Our fine-tuned Mistral Small 3.2 (Mistral AI 2025), LLaMA 3.1, Phi-4, and Gemma 3 (F1 of 0.691) all surpassed the vanilla GPT-4 model (OpenAI et al. 2024), which was reported to have an F1 of 0.649. Due to VRAM limitations, we were unable to fine-tune or evaluate the LLaMA 2 70B (Touvron et al. 2023) model directly. Therefore, the reported vanilla performance for the LLaMA 2 70B (F1: 0.464, Acc: 0.577) and GPT-4 (F1: 0.649, Acc: 0.738) is taken from Table 3 of the original MISSCI paper (Glockner et al. 2024). Critically, the fine-tuned LLaMA 2 13B (F1: 0.681) substantially outperformed the 10 vanilla, much larger LLaMA 2 70B model (F1: 0.464). This highlights key finding: targeted training using high-quality, RAG-supported synthetic data can close the performance gap between small, parameter-efficient models and large foundation models for domain-specific tasks like fallacy classification. Table 6 Comparison of Vanilla vs. Fine-Tuned LLaMA 2 13B F1-Scores by Fallacy Category on the MISSCI test split. Fallacy Category Count Vanilla F1 (macro) Fine-Tuned F1 (macro) Absolute Gain Ambiguity Biased Sample Fallacy Causal Oversimplification Fallacy of Division/Composition Fallacy of Exclusion False Dilemma False Equivalence Hasty Generalization Impossible Expectations Macro Average F1 Accuracy 44 37 73 33 125 19 85 32 6 454 0.044 0.143 0.485 0.050 0.110 0.148 0.614 0.586 0.000 0.218 0.326 0.333 0.704 0.820 0.485 0.954 0.812 0.479 0.912 0.632 0.681 0.722 0.289 0.561 0.335 0.435 0.844 0.664 -0.135 0.326 0.632 0.463 0. The category-specific analysis of LLaMA 2 13B model results reveals that the largest absolute improvement in macro F1-score from 0.218 to 0.681 across all fallacy categories is driven by dramatic performance improvements across nearly all categories, especially those where the vanilla model was weakest. The most significant improvements were seen in Fallacy of Exclusion, which rose from an F1-score of 0.110 to 0.954, and False Dilemma, which increased from 0.148 to 0.812. Furthermore, the model learned to identify the highly minority class Impossible Expectations, improving from an F1-score of zero to 0.632. Strong gains were also observed in other low-performing categories such as Biased Sample Fallacy (0.143 to 0.704). Notably, performance on False Equivalence decreased from 0.614 to 0.479 after finetuning. Overall, the results demonstrate that our synthetic data generation pipeline is highly effective at strengthening model performance, particularly on challenging fallacy classes, significantly improving the models overall robustness, F1 score and accuracy."
        },
        {
            "title": "5 Discussion",
            "content": "We introduced MisSynth, novel pipeline for generating high-quality synthetic data to detect scientific fallacies. Our method significantly improves the performance of LLMs on the MISSCI benchmark. Fine-tuning even small models, such as LLaMA 3.1 8B, with our data yielded substantial gains, surpassing the performance of much larger vanilla models, like GPT-4. This demonstrates that targeted, parameter-efficient fine-tuning with context-aware synthetic data is an effective strategy for specialized reasoning tasks."
        },
        {
            "title": "5.3 Ethical considerations\nOur synthetic dataset was generated automatically by an LLM. No medical experts\nor health professionals reviewed the synthetic data. There is a potential danger that\nmalicious actors could exploit our synthetic data to spread health misinformation\nmore effectively.",
            "content": "Acknowledgements. We thank Max Glockner for validating the initial idea and appreciate his helpful feedback during the development of this work."
        },
        {
            "title": "Template",
            "content": "You are provided with claim, an accurate premise for the claim, list of real-world fallacious premises (fallacies) from the scientific article with the fallacy class, and relevant text exempt from this article. Claim: {claim} Accurate Premise: {premise} {fallacies} Article Excerpt: {article_excerpt} Task: Based on the example and relevant text from the article, create {n_entries} synthetic fallacies that differ from the provided real-world fallacies and their class in the JSON format: [ { }, \"context\": // Synthetic Context 1, \"fallacy\": // Synthetic Fallacy 1, \"class\": // Synthetic Class 1 12 { }, ... { } ] \"context\": // Synthetic Context 2, \"fallacy\": // Synthetic Fallacy 2, \"class\": // Synthetic Class 2 \"context\": // Synthetic Context {n_entries}, \"fallacy\": // Synthetic Fallacy {n_entries}, \"class\": // Synthetic Class {n_entries} Creating fallacies of the classes different from provided real-world examples is encouraged, but the class could be only from the fallacy inventory. {fallacy_inventory} Structure created fallacy text similarly to real-world examples. Appendix Synthetic Claim-Accurate Premise"
        },
        {
            "title": "Prompt Template",
            "content": "You are provided with claim, an accurate premise, list of real-world fallacious premises (fallacies) from the scientific article with the fallacy class, and relevant text exempt from this article. Claim: {claim} Accurate Premise: {premise} {fallacies} Article Excerpt: {article_excerpt} Task: Based on the example and relevant text from the article, create {n_entries} synthetic claim and accurate premise pairs that differ from the provided real-world premises in the JSON format. Make sure that the created claim-accurate premise pair is coherent. [ { }, { }, ... \"premise\": // Synthetic Accurate Premise 1, \"claim\": // Synthetic Claim 1, \"premise\": // Synthetic Accurate Premise 2, \"claim\": // Synthetic Claim 2, 13 { } ] \"premise\": // Synthetic Accurate Premise {n_entries}, \"claim\": // Synthetic Claim {n_entries} Structure created claims and accurate premises text similarly to real-world examples."
        },
        {
            "title": "Appendix C Optimal Synthetic Dataset Examples",
            "content": "Table C1 Comparison of MISSCI dev split vs. randomly chosen Synthetic Claim-Accurate Premise pairs (GPT-5 Dsyn dataset = 30, = 15, argument ID 171) Source Claim Accurate Premise MISSCI dev split COVID-19 immunity likely lasts for years. Synthetic Synthetic Synthetic SARS-CoV-2 cell memory may stabilize rather than rapidly decline over time. Long-term protection from COVID19 depends on durable immune memory. the conclusions Definitive duration of COVID-19 immunity are still premature. about immune types of Different cells contributing to immune memory and long-term protection remained detectable in the blood of recovered COVID-19 patients Data suggest cell memory may reach stable plateau beyond the first eight months after infection. Immune memory of against reinfection. The overall amount of data on protective immunity to SARS-CoV-2 remains limited. source long-term protective immunity the is"
        },
        {
            "title": "References",
            "content": "Abdin M, Aneja J, Behl H, et al (2024) Phi-4 technical report. URL https://arxiv. org/abs/2412.08905, arXiv:2412.08905 Beers A, Nguyen S, Starbird K, et al (2023) Selective and deceptive citation in the construction of dueling consensuses. Science Advances 9. https://doi.org/10.1126/ sciadv.adh1933 Boudry M, Paglieri F, Pigliucci (2015) The fake, the flimsy, and the fallacious: Demarcating arguments in real life. Argumentation 29(4):431456. https://doi.org/ 10.1007/s10503-015-9359-1, URL https://doi.org/10.1007/s10503-015-9359-1 Brennen J, Simon F, Howard P, et al (2020) Types, sources, and claims of covid-19 misinformation. Tech. rep., Reuters Institute for the Study of Journalism 14 Chase (2022) LangChain. URL https://github.com/langchain-ai/langchain Chung J, Kamar E, Amershi (2023) Increasing diversity while maintaining accuracy: Text data generation with large language models and human interventions. In: Rogers A, Boyd-Graber J, Okazaki (eds) Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, pp 575593, https://doi.org/ 10.18653/v1/2023.acl-long.34, URL https://aclanthology.org/2023.acl-long.34/ Frederick DE (2023) Zombie papers, the data deluge column. Library Hi Tech https://doi.org/10.1108/LHTN-10-2023-0194, URL https: https://www.emerald.com/lhtn/articleNews //doi.org/10.1108/LHTN-10-2023-0194, pdf/40/9/1/1790212/lhtn-10-2023-0194.pdf 40(9):16. Glockner M, Hou Y, Nakov P, et al (2024) Missci: Reconstructing fallacies in misrepresented science. In: Ku LW, Martins A, Srikumar (eds) Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Bangkok, Thailand, pp 4372 4405, https://doi.org/10.18653/v1/2024.acl-long.240, URL https://aclanthology. org/2024.acl-long.240/ Glockner M, Hou Y, Nakov P, et al (2025) Grounding fallacies misrepresenting scientific publications in evidence. In: Chiruzzo L, Ritter A, Wang (eds) Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Association for Computational Linguistics, Albuquerque, New Mexico, pp 97329767, https://doi.org/10.18653/v1/2025.naacl-long.491, URL https: //aclanthology.org/2025.naacl-long.491/ Grattafiori A, Dubey A, Jauhri A, et al (2024) The llama 3 herd of models. URL https://arxiv.org/abs/2407.21783, arXiv:2407.21783 Gu Y, Tinn R, Cheng H, et al (2021) Domain-specific language model pretraining for biomedical natural language processing. ACM Trans Comput Healthcare 3(1). https://doi.org/10.1145/3458754, URL https://doi.org/10.1145/3458754 Guo Z, Schlichtkrull M, Vlachos (2022) survey fact-checking. Transactions guistics 10:178206. //doi.org/10.1162/tacl_a_00454, pdf/doi/10.1162/tacl_a_00454/1987018/tacl_a_00454.pdf the Association of https://doi.org/10.1162/tacl_a_00454, URL automated on for Computational Linhttps: https://direct.mit.edu/tacl/articleHannun A, Digani J, Katharopoulos A, et al (2023) MLX: Efficient and flexible machine learning on apple silicon. URL https://github.com/ml-explore Helwe C, Calamai T, Paris PH, et al (2024) MAFALDA: benchmark and comprehensive study of fallacy detection and classification. In: Duh K, Gomez H, Bethard 15 (eds) Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Association for Computational Linguistics, Mexico City, Mexico, pp 48104845, https://doi.org/10.18653/v1/2024.naacl-long.270, URL https: //aclanthology.org/2024.naacl-long.270/ Hong R, Zhang H, Pang X, et al (2024) closer look at the self-verification abilities of large language models in logical reasoning. In: Duh K, Gomez H, Bethard (eds) Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Association for Computational Linguistics, Mexico City, Mexico, pp 900925, https://doi.org/10.18653/v1/2024.naacl-long.52, URL https: //aclanthology.org/2024.naacl-long.52/ Hu EJ, Shen Y, Wallis P, et al (2021) Lora: Low-rank adaptation of large language models. CoRR abs/2106.09685. URL https://arxiv.org/abs/2106.09685, 2106.09685 Jin Z, Lalwani A, Vaidhya T, et al (2022) Logical fallacy detection. In: Goldberg Y, Kozareva Z, Zhang (eds) Findings of the Association for Computational Linguistics: EMNLP 2022. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, pp 71807198, https://doi.org/10.18653/v1/2022.findings-emnlp. 532, URL https://aclanthology.org/2022.findings-emnlp.532/ Lewandowsky S, Cook J, Lombardi (2020) Debunking handbook 2020 Lewis P, Perez E, Piktus A, et al (2020) Retrieval-augmented generation for knowledge-intensive nlp tasks. In: Larochelle H, Ranzato M, Hadsell R, et al (eds) Advances in Neural Information Processing Systems, vol 33. Curran Associates, Inc., pp 94599474 Li Y, Wang D, Liang J, et al (2024) Reason from fallacy: Enhancing large language models logical reasoning through logical fallacy understanding. In: Duh K, Gomez H, Bethard (eds) Findings of the Association for Computational Linguistics: NAACL 2024. Association for Computational Linguistics, Mexico City, Mexico, pp 30533066, https://doi.org/10.18653/v1/2024.findings-naacl.192, URL https://aclanthology.org/2024.findings-naacl.192/ Lin CY (2004) Rouge: package for automatic evaluation of summaries. URL https: //aclanthology.org/W04-1013/ Mistral AI (2025) Mistral Small 3. URL https://mistral.ai/news/mistral-small-3 Møller AG, Pera A, Dalsgaard J, et al (2024) The parrot dilemma: Human-labeled vs. LLM-augmented data in classification tasks. In: Graham Y, Purver (eds) Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, St. Julians, Malta, pp 179192, https://doi.org/10.18653/v1/ 16 2024.eacl-short.17, URL https://aclanthology.org/2024.eacl-short.17/ Nakov P, Corney D, Hasanain M, et al (2021) Automated fact-checking for assisting human fact-checkers. In: Zhou ZH (ed) Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. International Joint Conferences on Artificial Intelligence Organization, pp 45514558, https://doi.org/10. 24963/ijcai.2021/619, URL https://doi.org/10.24963/ijcai.2021/619, survey Track OpenAI, Achiam J, Adler S, et al (2024) Gpt-4 technical report. URL https://arxiv. org/abs/2303.08774, arXiv:2303.08774 Ruiz-Dolz R, Lawrence (2023) Detecting argumentative fallacies in the wild: Problems and limitations of large language models. In: Alshomary M, Chen CC, Muresan S, et al (eds) Proceedings of the 10th Workshop on Argument Mining. Association for Computational Linguistics, Singapore, pp 110, https://doi.org/10.18653/v1/ 2023.argmining-1.1, URL https://aclanthology.org/2023.argmining-1.1/ Sennrich R, Haddow B, Birch (2016) Improving neural machine translation models with monolingual data. In: Erk K, Smith NA (eds) Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pp 8696, https://doi. org/10.18653/v1/P16-1009, URL https://aclanthology.org/P16-1009/ Team G, Kamath A, Ferret J, et al (2025) Gemma 3 technical report. URL https: //arxiv.org/abs/2503.19786, arXiv:2503. Touvron H, Lavril T, Izacard G, et al (2023) Llama: Open and efficient foundation language models. arXiv:2302.13971 Vosoughi S, Roy D, Aral (2018) The spread of online. aap9559, https://www.science.org/doi/pdf/10.1126/science.aap9559 true and false news 359(6380):11461151. https://doi.org/10.1126/science. https://www.science.org/doi/abs/10.1126/science.aap9559, Science URL Wachsmuth H, Naderi N, Habernal I, et al (2017) Argumentation quality assessment: Theory vs. practice. In: Barzilay R, Kan MY (eds) Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Vancouver, Canada, pp 250255, https: //doi.org/10.18653/v1/P17-2039, URL https://aclanthology.org/P17-2039/ Zhai Z, Li H, Han X, et al (2025) Ruozhibench: Evaluating llms with logical fallacies and misleading premises. URL https://arxiv.org/abs/2502.13125, arXiv:2502.13125 17 , 5 1 = , 0 3 = s d D 5 - G ( a i - P r A l t n e c m a . i v C M n r o 2 b 1 P . m a t t e o y n t h a \" / \" . ) 1 7 1 n g x o t n e a c a t n l x l e I I p E t t s I I e a c a o a a F - n i i t e e 9 1 - C m n i . r s . e r t m e o f o e i g e c s \" e r t t 9 1 - C - i t d s n . n h o d , e r m , t b c t d t l n h e e l c d i r e f \" s l l n i z r n z r - S y e e s n u n i s t r ff 9 1 - C h i S u m e t o o R s c r g i o l c e - l l y e v - l , . . ( i f e o - i 9 1 - C i B m fi p - R - a o n e n p o w s c - d n u e c . n 8 - 5 o l e t o i i t a . ) e fl , l s . i - l r m i n . e - e m n c D p o i e o a - s c m s e e , l t e . t n - p n b i t a u a . e f c r d f w e r m f t t e e fi / . t n / a F m d i u r e s e , t n r a n l l n i 9 1 - C . e f a / l y e , d t - / s D c a a l y e a . s e m t r - s u e , f h - a s y s l . e f a i n i A / T o y e a , f h a r t a h i p t o . i f , e s c , d t o c fi m e a C o t l t d a - l a - 0 1 h a f u l e w , t r d p y u e c r - i t e d h . 2 - - S e s y s l n . d s o e C e y p E l c h S p E p d S l x e C l e S e a c a o a a F 8 m B u t fi t t l d a s s t o i b e d a s c y t e ff a i e , - l r s r c o l t e m s c m o . n - m a - u u o r t t I 9 1 - C l t l c d . n . n h o 9 1 - C e e f t e r c e s h o o s f n m o y e ff h o a e s n u E y l n fi y i e a - o s r i f l c d i r l . e f t y i - I 9 1 - C z n l c y u i t n i e t e ff e n n e r i m i s t e u s i p . n , 5 1 = , 0 3 = s d D 5 - ( g C s m y l i t n h m a . l W - R o a C 3 b t . m a n e p n y n t t e \" / \" . ) 1 7 1 I m a p t 2 - - S a e o e s ; l e e o o h - a t t t p k b m t , s p r t t 9 1 - C d i l y . e n r m . e f i f h m i + 0 6 l c t c B l - a x l s y e e n h e e n e S e fl e d , e 2 - - S , m l . e + 0 9 t n d d l i i m . e h a l T - S 2 - - S , f h 7 1 c d b 7 1 e a s t m . t n i i t t . m i e / / A / i i e y H / n v E a 19 - c a i c - o s i u B / / o t x b o I"
        }
    ],
    "affiliations": [
        "National University of Kyiv-Mohyla Academy, Kyiv, Ukraine"
    ]
}