{
    "paper_title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
    "authors": [
        "Yanxu Meng",
        "Haoning Wu",
        "Ya Zhang",
        "Weidi Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen."
        },
        {
            "title": "Start",
            "content": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie School of Artificial Intelligence, Shanghai Jiao Tong University 5 2 0 2 1 2 ] . [ 1 9 6 7 5 1 . 8 0 5 2 : r Figure 1. Overview. Our proposed SceneGen framework takes single scene image and its corresponding object masks as inputs, and efficiently generates multiple 3D assets with coherent geometry, texture, and spatial arrangement in single feedforward pass."
        },
        {
            "title": "Abstract",
            "content": "3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, novel framework that takes scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the *: These authors contribute equally to this work. : Corresponding author. feature extraction module. Coupled with position head, this enables the generation of 3D assets and their relative spatial positions in single feedforward pass; (iii) we demonstrate SceneGens direct extensibility to multi-image input scenarios. Despite being trained solely on singleimage inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https: //mengmouxu.github.io/SceneGen. Everything you can imagine is real. Pablo Picasso 1. Introduction The growing demand for immersive digital environments in applications such as virtual/augmented reality (VR/AR) and embodied AI has spurred significant advancements in 3D content generation [6, 8, 9, 13, 14, 28]. While early efforts primarily focused on synthesizing individual 3D assets [33, 59, 69], recent research focus has shifted to the more challenging task of 3D scene generation. Generating realistic 3D scenes [5, 11, 15, 16, 19, 64, 65], conditioned on input text or images, requires synthesizing multiple assets with accurate geometry, texture, and spatial relationships. This primarily hinges on two key capabilities: (i) 3D asset generation for generating plausible asset geometric topologies from limited textual or visual input. (ii) spatial arrangement for managing inter-object spatial relationships to correctly handle support, occlusion, and other physical interactions among assets. Existing approaches can be divided into two paradigms: (i) retrieval-based methods [12, 38, 49, 61] typically employ LLMs for layout planning and retrieve matching 3D assets from existing libraries to assemble scenes. While straightforward, their flexibility is limited by the coverage of available assets. (ii) two-stage approaches [18, 35, 62] first generate individual 3D assets, then employ Vision-Language Models (VLMs) or optimization techniques to refine scene structure and spatial arrangement. Although more flexible, their reliance on iterative optimization inevitably leads to inefficiency and error accumulation. The most relevant works to ours are MIDI [23] and PartCrafter [34], which generate multiple assets or parts from single image. However, they still suffer from limited synthesis fidelity and inaccurate spatial relationships among assets. To address the aforementioned challenges, we propose SceneGen, novel 3D scene generation model designed to simultaneously generate multiple assets, including their geometry, texture, and spatial positions, within scene image, all in single feedforward pass. Concretely, our framework builds upon an existing single-asset generation model [59] and introduces three key modules, including feature extraction, feature aggregation, and output module. First, the feature extraction module strategically leverages offthe-shelf visual [42] and geometric [53] encoders to extract both asset-level and scene-level features within the scene. Subsequently, our proposed feature aggregation module, composed of local and global attention blocks, effectively integrates the extracted visual and geometric features while enabling inter-asset interactions during generation to ensure plausible geometric topologies. Benefiting from thorough scene information utilization, the latent features generated by SceneGen can be directly decoded into assets relative position, geometry, and texture through the position head and pretrained structure decoder within our output module. Moreover, despite being trained solely on single-image samples, SceneGen demonstrates remarkable generalization capability to multi-image input scenarios, achieving even better generation quality, which primarily stems from our dedicated architectural design. To conduct comprehensive and reliable evaluation of SceneGens performance on 3D scene generation, we systematically adopt multiple metrics focusing on both geometric and visual quality. Both quantitative and qualitative results demonstrate that our proposed SceneGen significantly outperforms previous methods in terms of generation quality and efficiency, which can generate textured scenes with four assets in about 2 minutes. The rest of this paper is organized as follows: In Sec. 2, we provide comprehensive review and discussion of related literature; Sec. 3 elaborates on details of our proposed SceneGen; Sec. 4 presents extensive quantitative and qualitative evaluations; Finally, Sec. 5 concludes with key insights and contributions of our work. To our knowledge, SceneGen is the first 3D scene generation model capable of simultaneously synthesizing geometry, texture, and relative positions of multiple 3D assets in single feedforward pass, without requiring optimization. We believe this work will inspire advances in high-quality, efficient 3D content generation and facilitate applications in downstream tasks. 2. Related Work 3D visual perception. Extensive research has advanced 3D visual perception, where traditional methods like SfM [48, 52] rely on computationally intensive optimization for 3D reconstruction. Notably, emerging feedforward methods [3, 27, 30, 51, 5355, 68, 71] have demonstrated efficient 3D perception, with DUSt3R [55] pioneering this trend and VGGT [53] establishing minimalist yet powerful paradigm that distills geometric priors from large-scale data without explicit 3D inductive biases or optimizations. 3D asset synthesis. Typically, 3D asset synthesis aims to generate object-centric geometry and texture from text or image inputs. The recent success of diffusion models [22] in 2D generation [37, 43, 47, 56, 57] has inspired the development of learning-based, scalable 3D content [6, 8, 9, 13, 14, 28] generation, which produce 3D asset in various representations, including explicit forms such as point clouds [40], voxels [25, 41], and SDFs [4, 32], as well as implicit ones like 3D Gaussians [20, 67] and NeRFs [1, 31, 60]. Subsequent advances leverage VAEs [29] for compressing 3D geometry or textures [33, 59, 69] and adopt hybrid meshtexture pipelines [24, 26, 58, 66], with TRELLIS [59] demonstrating scalable, high-fidelity generation via structured latents. Nevertheless, these methods remain restricted to single-asset synthesis and fundamentally lack the ability to model complex multi-asset scenes. 3D scene generation. Beyond single asset synthesis, 3D scene generation is more challenging yet valuable, aim2 SceneGen to multi-view input scenarios in Sec. 3.4. 3.1. Problem Formulation Our proposed SceneGen is single-stage feedforward 3D scene generation model (GScene), which takes scene image (I Scene) containing objects and corresponding masks ({mi}N i=1) as input, simultaneously generating 3D asset structure and texture representations ({Si}N i=1), and their relative positions ({P i}N i=1), formulated as: {(Si, i)}N i=1 = GScene({mi}N i=1, Scene) Figure 2. 3D Scene Generation. (a) Existing methods typically require segmenting target objects from the scene image; (b) Twostage methods like CAST [62] sequentially retrieve or generate individual assets, then assemble them via post-processing; (c) Methods such as MIDI [23] directly generate multiple assets from single image, but suffer from blurry details and unreasonable spatial layouts; (d) In contrast, our SceneGen jointly synthesizes the geometry, texture, and spatial positions of multiple assets in single feedforward pass, producing plausible 3D scenes. ing to produce multiple coordinated, physically plausible assets within scene. Prior text-based approaches primarily leverage LLMs for layout planning [12, 38, 49, 61] and retrieve suitable assets from existing libraries. Subsequent image-based methods employ segmentation [5, 10, 15, 19, 64], scene graph [11, 16, 65] and depth/point cloud alignment [10, 50, 62] to assist multiasset generation and arrangement. As depicted in Figure 2 (b), recent optimization-based methods [18, 35, 62] adopt VLMs for post-processing, refining scene structures through image/text-guided adjustments, but inevitably suffer from inefficiency. Other works (Figure 2 (c)), such as MIDI [23] and PartCrafter [34], explore scene generation conditioned on single image, but inherently sacrifice reconstruction fidelity due to their canonical-space representations. Our proposed SceneGen uniquely overcomes these limitations by integrating asset-level and scene-level features, enabling robust and efficient 3D scene generation. 3. Method In this work, we present SceneGen, designed to jointly perform 3D asset generation within scenes and predict relative spatial positions among assets. Here, we first formally describe our problem formulation in Sec. 3.1; followed by elaboration on our model architecture and training methodology in Sec. 3.2 and 3.3, respectively; finally, we extend Here, the position of each asset relative to pre-selected query asset, is denoted as = [ti, qi, si] R8, comprising ti R3 (translation), qi R4 (rotation quaternion), and si R1 (scale factor). By default, we select the asset with = 1 as the query asset, with its parameters fixed as: tquery = [0, 0, 0], qquery = [1, 0, 0, 0], squery = 1. 3.2. SceneGen Our proposed SceneGen (GScene) comprises three key stages: (i) feature extraction, employing scene visual encoder (ΦV ) and scene geometric encoder (ΦG) to extract visual and structural features within the scene, implemented using off-the-shelf DINOv2 [42] and VGGT [53], respectively; (ii) feature aggregation, comprising DiT [43] blocks, each integrating local attention block, global attention block, and feedforward network; and (iii) output module, which introduces position head (Ψpos) for predicting the spatial locations of assets and adopts off-theshelf sparse structure (SS) and structured latents (SLAT) decoders [59] for decoding scene geometry structures. By integrating these complementary modules, our SceneGen effectively captures both local asset-level and global scenelevel features, enabling it to simultaneously generate multiple 3D assets and predict their relative positions. Feature extraction. Our SceneGen starts with extracting both local and global features from given scene image (I scene) with the visual encoder (ΦV ) and geometric encoder (ΦG). Specifically, for each object with its corresponding segmentation mask (mi), we obtain four complementary feature representations: (i) the objects individual visual features (F ); (ii) the visual features of its mask (F mask global); and (iv) the global geometric features (F geo global), denoted as: ); (iii) scene global visual features (F V = ΦV (I scene mi), mask global = ΦV (I scene), geo = ΦV (mi), global = ΦG(I scene) Here, represents pixel-wise multiplication. These features are then concatenated along the sequence dimension into unified scene context (F scene ), formulated as: scene = [F i ; mask 3 ; global; geo global] Figure 3. Architecture Overview. SceneGen takes single scene image with multiple objects and corresponding segmentation masks as input. pre-trained local attention block first refines the texture of each asset. Then, our introduced global attention block integrates asset-level and scene-level features extracted by dedicated visual and geometric encoders. Finally, two off-the-shelf structure decoders and our position head decode these latent features into multiple 3D assets with geometry, texture, and relative spatial positions. Feature aggregation. Subsequently, SceneGen employs feature aggregation module to integrate the extracted scene context features (F scene ), enabling simultaneous generation of multiple 3D assets. The module consists of local attention block that refines details of individual assets, global attention block that incorporates scene context information and facilitates inter-asset interactions during generation, and feedforward network. Specifically, we parameterize the local attention blocks and feedforward networks with pre-trained weights from TRELLIS [59], flow-matching [36] model that sytnehsizes 3D content from noisy sparse structure latents. For clarity and conciseness, given the sparse structure latents ({xi}N i=1, where each xi RT C) of objects within scene, we denote the standard attention mechanism as Attention(Q, K, V), and elaborate on single DiT block as follows. The local attention block aims to enhance details of individual objects through asset-level self-attention (AS) and cross-attention (AC). Concretely, it focuses on fusing the latent features of each object (xi) with their corresponding visual features (F ) to yield refined representations of each object (xAC ), which can be formulated as: xAS = Attention(xi, xi, xi) = Attention(xAS xAC , i , ) To establish inter-dependencies among 3D assets, we propose the global attention block, comprising scene-level self-attention (SS) and cross-attention (SC), which capture inter-object relationships and integrate scene geometry, respectively. This ensures physically plausible spatial arrangements of generated assets. i ), denoted as: ˆxi = [pi; ri; xAC Similar to [53], we initialize one learnable position token (pi) and four register tokens (ri) [7] for refined features of each object (xAC ], where [; ] refers to concatenation along the token length dimension. Notably, we assign unique position token (pquery) and register tokens (rquery) to the query asset, while adopting shared position token (pi) and register tokens (ri) for other assets. For each asset feature (ˆxi RT C), we concatenate them along the token sequence dimension to form unified scene representation (X R(N )C), which is processed by our scene-level self-attention layer, resulting in updated tokens of each asset ({xSS i=1), formulated as: }N {xSS }N i=1 = Attention(X, X, X) Through this process, intra-asset and inter-asset information aggregation establishes essential shape and position awareness for coherent multi-asset generation. We then employ scene-level cross-attention to integrate multiple preextracted scene-aware features, thus incorporating 3D geometric context. The features of each asset are updated into geometry-aware representations ({xSC i=1), denoted as: = Attention(xSS xSC , scene ) }N , scene This preserves object-specific details while integrating global geometric constraints, which effectively addresses occlusion challenges and enables geometric refinement. Output module. After passing through DiT blocks, we obtain the updated position tokens ({ˆp}N i=1) and latent features ({x}N i=1) of each generated asset, which are 4 then decoded into their relative spatial positions ({P }N i=1), and structural and texture representations ({S}N i=1), respectively. For relative positions, we extract and concatenate the position tokens of all non-query assets, which are then decoded into corresponding 8D position vectors ({ ˆP i}N i=2) by our proposed position head (Ψpos), comprising four selfattention layers and linear layer, denoted as: { ˆP i}N i=2 = {[ˆti, ˆqi, ˆsi]}N i=2 = Ψpos({ˆpi}N i=2) Here, each spatial position vector ( ˆP i) represents an assets spatial position (translation, rotation, and scale) relative to the pre-selected query asset (qquery). Additionally, the latent features can be directly decoded into the geometry and texture of each asset ({S}N i=1) using off-the-shelf sparse structure generator (GS) and structured latents generator (GL) in TRELLIS [59], represented as: {S}N i=1 = GL(GS({x}N i=1)) 3.3. Training During training, only the global attention blocks, learnable position tokens, and position head are optimized, with all other parameters frozen to facilitate efficient training, as depicted in Figure 3. The technical details regarding training data and loss function designs are presented below. Training data. Our SceneGen model is trained on 3DFUTURE [14], containing photorealistic scene renderings with instance masks and asset annotations. This dataset comprises 12K training scenes and 4.8K test scenes, each featuring scene image with one or multiple objects. To better capture inter-object spatial relationships, we augment the training set by iteratively designating each asset as the query asset, and randomly permuting the remaining assets, which expands the effective training samples to 30K. Training objectives. Our SceneGen model is trained endto-end using composite loss function (L) comprising three key components: (i) the average conditional flow matching [36] loss (Lcfm), on each generated asset for supervising asset generation; (ii) the position loss (Lpos) for maintaining accurate relative spatial arrangements among assets; and (iii) the voxel-space collision loss (Lcoll) for enforcing physically plausible object placements. The overall objective function (L) combines these components with weighting factor (λ), which can be formulated as: = Lcfm + λ(Lpos + Lcoll) objective learns parameterized function vθ to approximate the velocity field (v(xi(t), t) = txi(t)), represented as: Lcfm(θ) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Et,ϵvθ(xi(t), t) (ϵ x0 )2 2 The position loss (Lpos) adopts µ-weighted Huber loss ( δP ) between the predicted positions ( ˆP = [ˆti, ˆqi, ˆsi]) for all non-query assets (i [2, . . . , ]) and their ground truth (P = [ti, qi, si]), denoted as: Lpos = (cid:88) i=2 (µt(ˆti ti)/dsceneδP +µqˆqi qiδP + µsˆsi siδP ) Here, the translation error component is normalized by the scene scale (dscene) of each sample to mitigate numerical instability caused by varying query asset selections. This stabilizes translation loss during training while improving generalization across distinct query asset configurations. The collision loss (Lcoll) quantifies surface collision in 64 64 64 voxel grid (V ). Concretely, the predicted sparse structure latents (xi) are decoded into point clouds ({pi}L i=1) via pretrained sparse structure decoder from TRELLIS [59], then transformed using predicted pose parameters ( ˆP i) and voxelized into . The collision loss is defined as the ratio of overlapping surface voxels to all surface voxels, using the Huber loss ( δC ), denoted as: Lcoll = IoUsceneδC = (cid:80) (cid:80) I[V > 1] I[V > 0] δC Here, IoUscene = 0 means there are no asset collisions. 3.4. Extension to Multi-view Inputs Despite only trained exclusively on single-image samples, our model surprisingly demonstrates inherent multi-view compatibility through its flexible feature extraction and conditioning strategy. Specifically, for scene with input views ({I ) are independently extracted via the visual encoder (ΦV ), while the geometric features are obtained from the unified scene representations encoded by aggregating information across all views with the geometric encoder (ΦG), denoted as: k=1), each views visual features (F Scene}K geo = ΦG({I Scene}K j=1)[k] Concretely, the flow matching loss establishes straight probability paths between distributions via linear interpolation: xi(t) = (1 t)x0 + tϵ, where ϵ (0, I), [0, 1], and x0 denotes the noise-free sparse structure latents for each of the assets in the scene. The conditional flow matching The final asset positions are determined by averaging predictions across all views. Experiments show that this multiview inference scheme improves asset generation quality through better geometric understanding, although the model has never been explicitly fine-tuned on such inputs. 5 Method Instance Specific Geometric Metrics CD-S CD-O F-Score-S F-Score-O IoU-B Image Category Visual Metrics PSNR SSIM LPIPS FID CLIP-S DINO-S Inference Time (s) PartCrafter [34] (cid:37) 0.2027 40. DepR [72] (cid:33) 0.0518 0.0862 63.02 47. 0.2989 Gen3DSR [10] (cid:33) 0.0521 0.0935 61.26 41.26 0. MIDI [23] (cid:33) 0.0501 0.0602 68.74 61.04 0.2493 Scene GT-Render Scene GT-Render Scene GT-Render Scene GT-Render 15.92 0.8885 0.1730 63.95 0.8059 15.43 0.8899 0.1660 78.26 0.7950 16.93 0.8814 0.1778 22.75 0.8711 15.45 0.8814 0.1711 28.26 0.8706 SceneGen (cid:33) 0.0118 0.0138 90.60 89.73 0.5818 Scene GT-Render 16.76 0.8903 0.1417 19.59 0.9152 17.59 0.8991 0.1234 12.34 0. 0.4334 0.4416 0.6892 0.7034 0.8322 0.8702 7.2 11.6 179.0 42. 26.0 Table 1. Quantitative Comparisons on the 3D-FUTURE Test Set. We evaluate the geometric structure using scene-level Chamfer Distance (CD-S) and F-Score (F-Score-S), object-level Chamfer Distance (CD-O) and F-Score (F-Score-O), and volumetric IoU of object bounding boxes (IoU-B). For visual quality, CLIP-S and DINO-S represent CLIP and DINOv2 image-to-image similarity, respectively. We report the time cost for generating single asset on single A100 GPU, and indicates adopting MV-Adapter [24] for texture rendering. 4. Experiments This section starts with the experimental settings in Sec. 4.1, followed by comprehensive quantitative and qualitative evaluations in Sec. 4.2 and Sec. 4.3, respectively. Finally, we conduct ablation studies in Sec. 4.4 to validate the effectiveness of our proposed model and strategies. 4.1. Experimental Settings Implementation details. All experiments are conducted on 8 NVIDIA A100 GPUS, where we train SceneGen for 240 epochs using the AdamW [39] optimizer with learning rate of 5 105 and batch size of 8. The weighting factor λ decays dynamically within [0.2, 1] using decay factor of 0.99, and the thresholds of Huber loss δP and δC are set to 0.02 and 0.05, respectively. To handle varying numbers of assets across training scenes, each training step dynamically samples scenes containing identical asset counts. During inference, we adopt 25 sampling steps with the classifier-free guidance (CFG) weight set to = 5.0. Evaluation metrics. We assess the generated 3D scenes from both geometric and visual perspectives. For geometry, we reconstruct point clouds from the synthesized asset surfaces and align them with the ground truth using FilterReg [17] for faster and more accurate registration than traditional Iterative Closest Point (ICP [2]). We then compute commonly used point cloud metrics, Chamfer Distance (CD) and F-Score, at both scene and object levels, as well as the volumetric IoU of asset bounding boxes. For visual quality, we focus on the scene texture rendering. Concretely, after alignment with the ground truth point cloud, we render the predicted scenes with Blender from the original input camera viewpoint. We consider two types of ground truth: (i) instance-masked scene images extracted using corresponding object masks, and (ii) images rendered from ground truth assets at the same viewpoint (excluding ambient lighting). We compare our rendered results with both types of ground truth using PSNR, SSIM, LPIPS [70], FID [21], CLIP [44] similarity, and DINOv2 [42] similarity to assess the texture quality of generated assets. Regarding efficiency, we report the inference time cost for synthesizing single 3D asset on single A100 GPU. More details will be included in Sec. C.2 of the Appendix. Baselines. We compare SceneGen with representative 3D scene generation methods, including PartCrafter [34], DepR [72], Gen3DSR [10], and MIDI [23], using their pre-trained models. Specifically, we adopt object masks to specify generation targets for all baselines except for PartCrafter, which does not support mask-based control. Instead, we directly provide PartCrater with extracted objects and the number of assets as input. Moreover, as PartCrafter and DepR do not offer code for texture rendering, our evaluation of these methods focuses on geometric quality, while visual quality is compared with Gen3DSR and MIDI (relying on MV-Adapter [24] for texture synthesis). Benchmarks. All evaluations are conducted on the 3DFUTURE [14] test set, comprising 4.8K scenes. Each scene contains photorealistic rendered image with one or more objects and corresponding segmentation masks as input. 4.2. Quantitative Results As presented in Table 1, we draw the following key observations: (i) geometric quality: SceneGen consistently outperforms existing methods across all scene-level and assetlevel metrics. This stems from its joint integration of local asset features and global scene context during generation. The interactions among multiple assets facilitate the model to produce physically plausible geometric structures, while the position head further improves the structural realism by explicitly predicting spatial arrangements. (ii) visual quality: SceneGen can render high-quality textures for generated 3D assets without relying on any external texture generation models. Moreover, whether using masked scene images or ground-truth renderings as references, our 6 Figure 4. Qualitative Comparisons on the 3D FUTURE Test Set and ScanNet++. Our proposed SceneGen is capable of generating physically plausible 3D scenes featuring complete structures, detailed textures, and precise spatial relationships, demonstrating superior performance over prior methods in terms of both geometric accuracy and visual quality on both the synthetic and real-world datasets. method consistently achieves the best performance across all metrics. This indicates that our synthesized assets are spatially closer to the ground truth while maintaining superior texture fidelity. and (iii) efficiency: While PartCrafter demonstrates clear advantage in inference speed, it suffers from limited generation quality and controllability. In contrast, SceneGen achieves both superior quality and strong balance between quality and efficiency, synthesizing 3D scene containing four assets with geometry and textures within 2 minutes on single A100 GPU. In addition, while the baseline methods, e.g., PartCrafter, DepR, and MIDI have been trained on 3D-FRONT [13], which may overlap with our test data, SceneGen still consistently outperforms them across all metrics, further demonstrating its effectiveness and superiority. 4.3. Qualitative Results Comparisons with baselines. As depicted in Figure 4, we qualitatively compare SceneGen with existing baselines on both the 3D FUTURE test set and in-the-wild ScnaNet++ [63], where they still struggle with 3D scene generation: PartCrafter lacks controllability over the generated targets and often mistakenly merges distinct assets, while both PartCrafter and DepR are limited to geometry generation and cannot render textures. More critically, all these methods exhibit difficulties in accurately understanding the spatial relationships among assets. In contrast, our proposed SceneGen precisely predicts the spatial relationships among assets and generates multiple 3D assets with accurate geometry and high-quality textures, without relying on any additional tools. These results clearly demonstrate the effectiveness and superiority of our approach. Extension to multi-image inputs. Benefiting from our architecture design, SceneGen can seamlessly handle multiimage inputs after being trained solely on single-image samples. Given the absence of suitable datasets for quantitative evaluation, we qualitatively assess the impact of multi-image inputs by randomly sampling several scenes geo global global mask ASS Geometric Metrics CD-S CD-O F-Score-S F-Score-O IoU-B Image Category Visual Metrics PSNR SSIM LPIPS FID CLIP-S DINO-S (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) 0.0118 0.0138 90.60 89.73 0.5818 (cid:33) (cid:33) 0.0183 0.0266 83. 74.71 0.4805 (cid:33) (cid:33) 0.0250 0.0286 79.08 73.46 0. (cid:37) (cid:33) 0.0310 0.0290 75.20 73.17 0.3825 (cid:37) (cid:37) 0.0764 0.0352 54. 70.55 0.1705 Scene GT-Render Scene GT-Render Scene GT-Render Scene GT-Render Scene GT-Render 16.76 17.59 15.89 16.27 15.56 15.86 15.30 15.55 13.32 13. 0.8903 0.1417 19.59 0.8991 0.1234 12.34 0.8845 0.1574 20.21 0.8918 0.1421 15.36 0.8806 0.1655 20.68 0.8873 0.1511 16.62 0.8773 0.1730 21.12 0.8837 0.1591 17.45 0.8418 0.2329 27.56 0.8464 0.2217 28.61 0.9152 0.9236 0.9049 0.9125 0.8980 0.9046 0.8932 0.9000 0.8399 0.8440 0.8322 0.8702 0.8063 0.8420 0.7850 0.8187 0.7737 0.8076 0.6059 0. Table 2. Ablations on SceneGen Variants. We progressively remove global geometric features (F geo mask visual features (F mask global), ), and substitute the scene-level self-attention (ASS) to validate each components contribution to SceneGen. global), global visual features (F global), global visual features (F Gen, assessing both geometric and visual quality of synthesized scenes. Concretely, we gradually remove global geometric features (F geo global), mask visual features (F mask ), and substitute the scene-level self-attention block (ASS) with simple asset-level selfattention block (AAS) to evaluate their impact on SceneGen. As depicted in Table 2, we have the following observations: (i) Removing any aforementioned components degrades the overall performance, confirming their importance in SceneGen; (ii) The geometric features primarily affect the structure of synthesized scenes, while the visual features further impact the visual quality; and (iii) The absence of scenelevel self-attention blocks eliminates inter-asset interactions during generation, leading to notable performance declines across all metrics. These results strongly demonstrate the necessity and effectiveness of our proposed feature extraction and aggregation modules for SceneGen. 5. Conclusion In this paper, we propose SceneGen, which takes single scene image and target asset masks as input, simultaneously synthesizing multiple 3D assets with structure and texture, as well as relative spatial positions in one feedforward pass. Specifically, we incorporate dedicated visual and geometric encoders to extract both asset-level and scene-level features, which are effectively fused with our introduced feature aggregation module. Notably, through our meticulous design, SceneGen can even directly generalize to multi-image inputs and achieve even better generation quality. Quantitative and qualitative evaluations demonstrate that SceneGen can generate physically plausible and mutually consistent 3D assets, significantly outperforming previous methods in terms of generation quality and efficiency. Acknowledgments Weidi would like to acknowledge the funding from Scientific Research Innovation Capability Support Project for Young Faculty (ZY-GXQNJSKYCXNLZCXM-I22). Figure 5. Qualitative Results with Multi-view Inputs. SceneGen can directly handle multi-view inputs in ScanNet++ and even achieves better generation quality, especially accurate structure. from ScanNet++ [63] and employing SAM2 [46] to obtain segmentation masks of corresponding objects. As depicted in Figure 5, compared to single-image inputs, incorporating multi-view images leads to 3D assets with more complete geometry and finer texture details. This illustrates that SceneGen can adaptively integrate complementary information from multiple views to produce higher-quality 3D scenes, further validating its practicality and scalability. More qualitative results will be included in Sec C.1 of the Appendix. 4.4. Ablation Studies To validate the effectiveness of our modules, we conduct comprehensive evaluations on several variants of Scene-"
        },
        {
            "title": "References",
            "content": "[1] Titas Anciukeviˇcius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy Mitra, and Paul Guerrero. Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2 [2] PJ Besl and Neil McKay. method for registration of 3-d shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 14(2):239256, 1992. 6, 15 [3] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. In Proceedings of the International Conference on Computer Vision, 2025. 2 [4] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 44564465, 2023. 2 [5] Tao Chu, Pan Zhang, Qiong Liu, and Jiaqi Wang. Buol: bottom-up framework with occupancy-aware lifting for panoptic 3d scene reconstruction from single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 49374946, 2023. 2, 3 [6] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object unIn Proceedings of the IEEE Conference on derstanding. Computer Vision and Pattern Recognition, 2022. 2 [7] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In Proceedings of the International Conference on Learning Representations, 2024. 4 [8] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Conference on Neural Information Processing Systems, 36:3579935813, 2023. 2 [9] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of the universe of annotated 3d objects. IEEE Conference on Computer Vision and Pattern Recognition, 2023. [10] Andreea Dogaru, Mert Ozer, and Bernhard Egger. Gen3dsr: Generalizable 3d scene reconstruction via divide and conquer from single view. In International Conference on 3D Vision, 2025. 3, 6 [11] Wenqi Dong, Bangbang Yang, Zesong Yang, Yuan Li, Tao Hu, Hujun Bao, Yuewen Ma, and Zhaopeng Cui. Hiscene: creating hierarchical 3d scenes with isometric view generation. arXiv preprint arXiv:2504.13072, 2025. 2, 3 [12] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. In Conference on Neural Information Processing Systems, 2023. 2, 3 [13] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the International Conference on Computer Vision, 2021. 2, 7 [14] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:33133337, 2021. 2, 5, 6, [15] Daoyi Gao, David Rozenberszki, Stefan Leutenegger, and Angela Dai. Diffcad: Weakly-supervised probabilistic cad model retrieval and alignment from an rgb image. ACM Transactions On Graphics, 43(4):115, 2024. 2, 3 [16] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Scholkopf. Graphdreamer: Compositional 3d In Proceedings of the scene synthesis from scene graphs. IEEE Conference on Computer Vision and Pattern Recognition, pages 2129521304, 2024. 2, 3 [17] Wei Gao and Russ Tedrake. Filterreg: Robust and efficient probabilistic point-set registration using gaussian filter and twist parameterization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 6, 15 [18] Zeqi Gu, Yin Cui, Zhaoshuo Li, Fangyin Wei, Yunhao Ge, Jinwei Gu, Ming-Yu Liu, Abe Davis, and Yifan Ding. Artiscene: Language-driven artistic 3d scene generation In Proceedings of the IEEE through image intermediary. Conference on Computer Vision and Pattern Recognition, 2025. 2, 3 [19] Can Gumeli, Angela Dai, and Matthias Nießner. Roca: Robust cad model retrieval and alignment from single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 40224031, 2022. 2, 3 [20] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric representation. In Proceedings of the European Conference on Computer Vision, pages 463479, 2024. 2 [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Conference on Neural Information Processing Systems, 2017. [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Conference on Neural Information Processing Systems, 2020. 2 [23] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, and Lu Sheng. Midi: Multi-instance diffuIn Proceedsion for single image to 3d scene generation. ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2364623657, 2025. 2, 3, 6, 14, 15 [24] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. In Pro9 ceedings of the International Conference on Computer Vision, 2025. 2, 6 [25] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation. In ACM SIGGRAPH Asia Conference, pages 19, 2022. 2 [26] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to highfidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. [27] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video generators for In Proceedings of the geometric 4d scene reconstruction. International Conference on Computer Vision, 2025. 2 [28] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel Chang, and Manolis Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2 [29] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations, 2014. 2 [30] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In Proceedings of the European Conference on Computer Vision, 2024. 2 [31] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In Proceedings of the International Conference on Learning Representations, 2024. 2 [32] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusionsdf: Text-to-shape via voxelized diffusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1264212651, 2023. 2 [33] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape arXiv synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025. [34] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compoarXiv preprint sitional arXiv:2506.05573, 2025. 2, 3, 6 latent diffusion transformers. [35] Lu Ling, Chen-Hsuan Lin, Tsung-Yi Lin, Yifan Ding, Yu Zeng, Yichen Sheng, Yunhao Ge, Ming-Yu Liu, Aniket Bera, and Zhaoshuo Li. Scenethesis: language and vision agentic framework for 3d scene generation. arXiv preprint arXiv:2505.02836, 2025. 2, 3 [36] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative In Proceedings of the International Conference modeling. on Learning Representations, 2023. 4, 5, 13 [37] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm - open-ended visual storytelling via latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. [38] Xinhang Liu, Yu-Wing Tai, and Chi-Keung Tang. Agentic 3d scene generation with spatially contextualized vlms. arXiv preprint arXiv:2505.20129, 2025. 2, 3 [39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, 2019. 6 [40] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. 2 [41] Norman Muller, Yawar Siddiqui, Porzi, Samuel Rota Bulo, Peter Kontschieder, and Matthias Nießner. Diffrf: Rendering-guided 3d radiance field diffusion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 43284338, 2023. Lorenzo [42] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. 2, 3, 6, 13 [43] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the International els with transformers. Conference on Computer Vision, 2023. 2, 3 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn Proceedings of the International Conference on vision. Machine Learning, 2021. 6 [45] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1217912188, 2021. [46] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. In Proceedings of the International Conference on Learning Representations, 2025. 8 [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synIn Proceedings of the thesis with latent diffusion models. IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2 [48] Johannes Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 2 [49] Fan-Yun Sun, Weiyu Liu, Siyi Gu, Dylan Lim, Goutam Bhat, Federico Tombari, Manling Li, Nick Haber, and Jiajun Wu. Layoutvlm: Differentiable optimization of 3d layout via vision-language models. In Proceedings of the IEEE Confer10 ence on Computer Vision and Pattern Recognition, 2025. 2, 3 [50] Xiang Tang, Ruotong Li, and Xiaopeng Fan. Towards geometric and textural consistency 3d scene generation via single image-guided model generation and layout optimization. arXiv preprint arXiv:2507.14841, 2025. [51] Aether Team, Haoyi Zhu, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Yang Zhou, Zizun Li, Junyi Chen, Chunhua Shen, Jiangmiao Pang, et al. Aether: Geometric-aware uniIn Proceedings of the International fied world modeling. Conference on Computer Vision, 2025. 2 [52] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry grounded deep structure from motion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2 [53] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. 2, 3, 4, 13, 14 [54] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perIn Proceedings of the ception model with persistent state. IEEE Conference on Computer Vision and Pattern Recognition, 2025. [55] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2 [56] Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, and Yanfeng Wang. Megafusion: Extend diffusion models towards higher-resolution image generation In Winter Conference on Applicawithout further tuning. tions of Computer Vision, 2025. 2 [57] Haoning Wu, Ziheng Zhao, Ya Zhang, Weidi Xie, and Yanfeng Wang. Mrgen: Diffusion-based controllable data engine for mri segmentation towards unannotated modalities. In Proceedings of the International Conference on Computer Vision, 2025. [58] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. In Conference on Neural Information Processing Systems, 2024. 2 [59] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. 2, 3, 4, 5, 13 [60] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein, Zexiang Xu, et al. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. In Proceedings of the International Conference on Learning Representations, 2024. 2 [61] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2, 3 [62] Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Wei Yang, Lan Xu, Jiayuan Gu, and Jingyi Yu. Cast: Component-aligned 3d scene reconstruction from an rgb image. In ACM SIGGRAPH Conference, 2025. 2, 3 [63] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 7, 8, 14 [64] Huangyue Yu, Baoxiong Jia, Yixin Chen, Yandan Yang, Puhao Li, Rongpeng Su, Jiaxin Li, Qing Li, Wei Liang, Song-Chun Zhu, et al. Metascenes: Towards automated In Proceedings replica creation for real-world 3d scans. of the IEEE Conference on Computer Vision and Pattern Recognition, pages 16671679, 2025. 2, [65] Guangyao Zhai, Evin Pınar Ornek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, and Benjamin Busam. Commonscenes: Generating commonsense 3d indoor scenes with scene graph diffusion. In Conference on Neural Information Processing Systems, 2023. 2, 3 [66] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics, 42(4):116, 2023. 2 [67] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. Gaussiancube: structured and explicit radiance representation for 3d generative modeling. In Conference on Neural Information Processing Systems, 2024. 2 [68] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimating geometry in the presence of motion. In Proceedings of the International Conference on Learning Representations, 2025. 2 [69] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions On Graphics, 43(4):120, 2024. 2 [70] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. [71] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. 2 [72] Qingcheng Zhao, Xiang Zhang, Haiyang Xu, Zeyuan Chen, Jianwen Xie, Yuan Gao, and Zhuowen Tu. Depr: Depth guided single-view scene reconstruction with instance-level diffusion. arXiv preprint arXiv:2507.22825, 2025. 6 11 SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass"
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Work 3. Method . 3.1. Problem Formulation . . . . 3.2. SceneGen . 3.3. Training . . . . . 3.4. Extension to Multi-view Inputs . . . . . . . . . . . . . . . . . 4. Experiments 4.1. Experimental Settings 4.2. Quantitative Results 4.3. Qualitative Results . . 4.4. Ablation Studies . . . . . . . . . . . . . . . . . . . . 5. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A. Preliminaries on 3D Foundation Models B. More Details about Training Data C. More Implementation Details C.1. Extension to Multi-image Inputs . . C.2. Evaluation Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. More Visualizations E. Limitations & Future Works . . E.1. Limitations . . E.2. Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 3 3 3 5 5 6 6 6 7 8 8 . . . . . . . . 13 13 13 . 13 . 14 16 17 . 17 . 17 A. Preliminaries on 3D Foundation Models Given the inherent challenges of directly generating 3D scene with multiple 3D assets from single image, SceneGen aims to fully leverage the visual and geometric priors embedded in state-of-the-art 3D foundation models. Therefore, we build our model based on TRELLIS [59], and adopt DINOv2 [42] and VGGT [53] as our visual and geometric encoders, respectively. In the following, we provide detailed introduction to TRELLIS and VGGT to better illustrate their roles. TRELLIS. For 3D asset (O), TRELLIS encodes its geometry and appearance into unified representation (z), denoted as: = {(zi, pi)}L i=1. Here, pi {0, 1, . . . , 1}3 denotes the positional index of an active voxel intersecting the surface of O, and zi RC represents the corresponding local latent feature, with and representing the 3D grid resolution and the total number of active voxels, respectively. The generation process adopts two cascaded rectified flow models: the sparse structure generator (GS) synthesizes the sparse voxel structure {pi}L i=1, encoding geometric priors by predicting its low-resolution feature gird (S); while the structured latents generator (GL) generates texture and appearance features {zi}L i=1 conditioned on {pi}. Both models are optimized via the conditional flow matching (CFM) [36] objective, which establishes straight probability paths between distributions through linear interpolation: x(t) = (1 t)x0 + tϵ, where x0 denotes data samples, ϵ (0, I), and [0, 1]. The velocity field (v(x, t) = tx) governs the reverse process, with the CFM objective formulated as: Lcfm(θ) = Et,x0,ϵvθ(x, t) (ϵ x0)2 2 Notably, the sparse structured generator (GS) learns rich geometric priors from large-scale 3D data, effectively capturing both object geometries and spatial relationships, thus delivering essential asset-level understanding capabilities. Our SceneGen is compatible with both the sparse structured generator and structured latents generator, thus can sequentially employ them to decode synthesized latent features into the geometry and texture of 3D assets. VGGT. Trained on large-scale 3D annotated data, VGGT can extract 3D scene features through purely feedforward network without explicit 3D inductive biases. For single or multi-view RGB inputs ({I i}s i=1), its aggregator derives scene geometric features ({F geo i=1 = VGGT({I i}s Here, geo represent features extracted by global self-attention and self-attention layers, respectively. These features are efficiently decoded by lightweight DPT layers [45] into depth maps, point maps, and tracks, validating their rich scene geometric representation capacity. and geo i=1 = {[F geo }s , geo {F geo }s i=1) via: i=1) ]}s i By integrating these complementary strengths, our SceneGen effectively captures both local asset-level and global scenelevel features from the input image, achieving robust performance on the challenging 3D scene generation task. B. More Details about Training Data We train our SceneGen model on the 3D-FUTURE [14] dataset, whose rich textures and diverse scene lighting conditions effectively simulate real-world environments and thus enhance the models generalization ability. In addition, we further scale up training data through data augmentation to ensure the model can robustly learn the relative spatial relationships among multiple assets. Concretely, for scene with objects, during training, we iteratively select each asset as the query asset and randomly shuffle the remaining assets. Considering GPU memory constraints, we set the maximum number of assets per scene to = 7 on single A100 GPU. For samples containing more than assets, we randomly select assets from them for training. Furthermore, following TRELLIS [59], we also leverage its aesthetic score filtering criterion to filter out assets with aesthetic scores below 4.5, ensuring the high quality of training data. The distribution of asset counts across training scenes is illustrated in Figure 6. C. More Implementation Details In this section, we provide comprehensive explanation of the implementation details discussed in the paper. Specifically, Sec. C.1 describes the specific strategies applied to extend SceneGen to multi-image inputs; and Sec. C.2 elaborates on the details of our evaluation protocols. C.1. Extension to Multi-image Inputs While our model is designed for 3D scene generation from single scene image and is trained only on datasets containing single-view images, SceneGen can be easily adapted to multi-view inputs by simply modifying the sampling process, without Figure 6. Distribution of Asset Counts in our Training Data and Original 3D-FUTURE. requiring additional training or fine-tuning. Concretely, during inference, our model can take images of the same scene from different viewpoints, along with their corresponding objects and instance masks, as input. The geometric encoder (ΦG) within our SceneGen, specifically, an off-the-shelf VGGT [53] aggregator, integrates geometric information from different viewpoints to obtain better geometric representations for each perspective. This enables SceneGen to synthesize better geometric structures during the generation process. Finally, we predict the relative positions among different assets from each viewpoint and use the mean of these predictions across all views as the final spatial position output. It is important to note that, to ensure correctness throughout the inference process, the input order of assets and their segmentation masks must remain consistent across different viewpoints. Given the current lack of training and quantitative evaluation data for multi-view 3D scene generation, this work presents qualitative results on scenes sampled from ScanNet++ [63] to demonstrate the scalability of SceneGen, and leaves the construction of suitable multi-view datasets and evaluation methods for future work. Method Alignment CD-S CD-S 1 CD-S 2 F-Score-S IoU-B MIDI [23] SceneGen (Ours) ICP FilterReg ICP FilterReg 0.1697 0.0501 0.0310 0.0118 0.0653 0. 0.0121 0.0052 0.1044 0.0223 0.0189 0.0066 41.64 68.74 83.74 90.60 0.1232 0. 0.5103 0.5818 Table 3. Geometric Metric Comparisons with Different Point Cloud Alignment Methods. C.2. Evaluation Protocols Geometric metrics. Following previous work [23], we conduct geometry evaluation in normalized 3D space (also referred to as canonical space, i.e., x, y, [1, 1]), where the ground truth and the synthesized query asset are first rigidly aligned 14 Figure 7. Examples of Visual Metrics Evaluation Protocols. Here, we present two complementary types of ground truth: instancemasked images may introduce slight differences due to potential occlusions, while GT-render images lack scene-level illumination. using point cloud registration algorithms. Unlike MIDI [23], which adopts the traditional Iterative Closest Point (ICP [2]) method that produces suboptimal alignment results, we employ FilterReg [17], fast, accurate, and robust point cloud alignment approach. As presented in Table 3, both MIDI and SceneGen achieve better overall performance when aligned via FilterReg, demonstrating the reliability of this alignment method compared to traditional ICP. Moreover, under both alignment strategies, SceneGen consistently outperforms MIDI, indicating that explicitly predicting the spatial positions among assets enables SceneGen to more accurately model the relationships among distinct 3D assets within the scene. Visual metrics. Beyond the commonly used geometric evaluations described above, we also consider several visual metrics 15 Figure 8. More Qualitative Comparisons on the 3D FUTURE Test Set. to assess the visual quality of generated scenes. Concretely, after aligning the synthesized point clouds with the ground truth scenes, we use Blender to render them with the identical camera parameters. The rendered images are then compared with two types of ground truth to compute perceptual metrics that reflect the visual quality of synthesized scenes. As illustrated in Figure 7, these include: (i) instance-masked scene images, which are extracted using the corresponding object masks, where the occlusion relationships between assets introduce differences relative to predicted renderings; and (ii) GT-Render images, which are rendered from the ground truth assets at the same viewpoint using Blender, but lack scene-level illumination and complete textures, resulting in textural discrepancies compared to predicted scenes. Thus, by computing visual metrics against both types of ground truth, we provide complementary evaluation of the visual quality of synthesized scenes. Efficiency. To ensure fair comparison across all methods, we report the average inference time over 500 trials of synthesizing scenes with single asset on single A100 GPU. Notably, our proposed SceneGen can directly generate 3D scenes containing 4 assets in single feedforward pass within 2 minutes on the same hardware, eliminating the need for timeconsuming sequential generation of individual 3D assets. D. More Visualizations In this section, we present additional visualization results on the 3D-FUTURE test set to qualitatively compare our SceneGen with representative baselines. As depicted in Figure 8, we have the following observations: (i) PartCrafter frequently suffers from missing or mixed-up assets due to its inability to control generation via object masks, despite already taking segmented objects and asset counts as input; (ii) Both PartCrafter and DepR can only generate scene geometry without rendering texture details; and (iii) All baseline methods (including Gen3DSR and MIDI) share the common limitation of incorrect spatial relationships among synthesized assets. In contrast, our SceneGen fully integrates visual and geometric features within the 16 scene to enable mutual influence among multiple assets during generation, producing 3D scenes with physically 3D scenes and high-quality texture details. E. Limitations & Future Works E.1. Limitations While our SceneGen demonstrates superior performance in 3D scene generation, it is not without its limitations. Specifically, although SceneGen demonstrates better texture generation and generalization capabilities compared to previous methods that rely on canonical representations, the narrow training data distribution limits its ability to generalize to non-indoor scenes, restricting its generalization to broader range of environments. Moreover, while SceneGen can generate multiple 3D assets and relative spatial positions in single feedforward pass, without relying on complex post-processing, it does not always handle contact relationships among objects, occasionally leading to asset overlaps or geometric inconsistencies. This is mainly because our single-stage framework does not explicitly enforce strict spatial or physical constraints among objects. E.2. Future Works To address the aforementioned limitations of SceneGen, we propose several directions for future improvement: (i) Constructing larger-scale 3D scene generation datasets that cover more diverse indoor and outdoor scenarios, to address biases in training data distribution and improve the generalization ability of models; (ii) Building suitable multi-view scene generation datasets to expand the application scope and practical potential of existing models; and (iii) Incorporating explicit physical priors or constraints to facilitate the model to better learn complex interactions among objects."
        }
    ],
    "affiliations": [
        "School of Artificial Intelligence, Shanghai Jiao Tong University"
    ]
}