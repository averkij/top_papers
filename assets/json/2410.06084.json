{
    "paper_title": "Diversity-Rewarded CFG Distillation",
    "authors": [
        "Geoffrey Cideron",
        "Andrea Agostinelli",
        "Johan Ferret",
        "Sertan Girgin",
        "Romuald Elie",
        "Olivier Bachem",
        "Sarah Perrin",
        "Alexandre RamÃ©"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing a crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, a novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) a distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with a diversity reward, promoting the generation of diverse outputs for a given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) text-to-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at https://google-research.github.io/seanet/musiclm/diverse_music/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 4 8 0 6 0 . 0 1 4 2 : r Diversity-Rewarded CFG Distillation Geoffrey Cideron, Andrea Agostinelli, Johan Ferret, Sertan Girgin, Romuald Elie, Olivier Bachem, Sarah Perrin*, Alexandre RamÃ©* Google DeepMind, * Equal advisory contribution Generative models are transforming creative domains such as music generation, with inference-time strategies like Classifier-Free Guidance (CFG) playing crucial role. However, CFG doubles inference cost while limiting originality and diversity across generated contents. In this paper, we introduce diversity-rewarded CFG distillation, novel finetuning procedure that distills the strengths of CFG while addressing its limitations. Our approach optimises two training objectives: (1) distillation objective, encouraging the model alone (without CFG) to imitate the CFG-augmented predictions, and (2) an RL objective with diversity reward, promoting the generation of diverse outputs for given prompt. By finetuning, we learn model weights with the ability to generate high-quality and diverse outputs, without any inference overhead. This also unlocks the potential of weight-based model merging strategies: by interpolating between the weights of two models (the first focusing on quality, the second on diversity), we can control the quality-diversity trade-off at deployment time, and even further boost performance. We conduct extensive experiments on the MusicLM (Agostinelli et al., 2023) textto-music generative model, where our approach surpasses CFG in terms of quality-diversity Pareto optimality. According to human evaluators, our finetuned-then-merged model generates samples with higher quality-diversity than the base model augmented with CFG. Explore our generations at googleresearch.github.io/seanet/musiclm/diverse_music. Keywords: Music Generation, Distillation, CFG, RL, Diversity, Model Merging 1. Introduction Generative models for creative domains. Art and entertainment domains historically driven by human creativity are undergoing profound transformation thanks to AI generative models. These models, often powered by Large Language Models (LLMs) or diffusion, can now generate texts (Gemini Team, 2023), images (Ramesh et al., 2022), videos (Ho et al., 2022), and audios (Agostinelli et al., 2023; Borsos et al., 2023; Cideron et al., 2024; Copet et al., 2023; DÃ©fossez et al.; Kreuk et al., 2023). To further refine the quality, these models are often augmented with inference methods during real-world deployment, ranging from simple temperature scaling to more refined methods like Beam search (Freitag and Al-Onaizan, 2017), test-time augmentation (Shanmugam et al., 2021), or MCTS (Kocsis and SzepesvÃ¡ri, 2006). particularly popular method for image and audio generation is classifier-free guidance (CFG) (Ho and Salimans, 2022), e.g., used in DALL-E (Ramesh et al., 2022) or AudioGen (Kreuk et al., 2023). CFG improves the models fidelity to the prompt by combining the logits of conditional and unconditional generations. Despite its benefits, CFG has two main limitations: it doubles the computational cost during deployment and reduces the diversity of generated content (Dhariwal and Nichol, 2021; Ho and Salimans, 2022; Kreuk et al., 2023; Meng et al., 2023), hindering the exploration of novel and diverse ideas cornerstone of creativity. Ideally, these models should not only fulfill user intent but also surprise them with unexpected and innovative outputs: the model should not generate systematically the same content (Hamilton, 2024). Quality-diversity trade-off. Effectively controlling the quality-diversity trade-off is thus extremely important but challenging. On the one hand, optimising quality usually reduces diversity; this Corresponding author: gcideron@google.com Diversity-Rewarded CFG Distillation Figure 1 Left. Illustration of the two objectives: CFG distillation (above) and the diversity reward (below), multiplied by the diversity coefficient ğ›½ in the joint finetuning objective. Right. Quality-diversity trade-off for different strategies. The first four lines represent the training trajectories of our approach, distilling CFG (with ğ›¾ = 3) with varying diversity coefficient ğ›½ in {0, 5, 10, 15}; every 500 training steps, we evaluate the quality and diversity of the generations. Larger values of ğ›½ lead to more diverse models yet slightly less quality. For linear interpolation (LERP), each cross corresponds to 0 ğœ† 1 when interpolating between the weights ğœƒğ‘ of quality-focused model (ğ›½ = 0) and those ğœƒğ‘‘ of diversity-focused model (ğ›½ = 15); the evaluated generations are obtained from the weights (1 ğœ†) ğœƒğ‘ + ğœ† ğœƒğ‘‘. For the CFG baseline, each dot corresponds to different value for the guidance factor 1 ğ›¾ 7. This plot shows that our method improves the quality-diversity trade-off; notably, LERP uncovers strong and steerable front of solutions by just interpolating between the weights of two models, at deployment time. limitation affects inference methods such as CFG but also finetuning stages such as RLHF (Kirk et al., 2024; Mohammadi, 2024). On the other hand, promoting diversity usually reduces quality (Brown et al., 2005), e.g., when increasing temperature at inference (Zhang et al., 2021). As further discussed in Section 4, quality-diversity algorithms (Lehman and Stanley, 2011; Mouret and Clune, 2015) seek to train population of models with diverse abilities. In contrast, optimising directly the diversity of generations of single model is an under-explored yet promising avenue, that we investigate here. Diversity-rewarded CFG distillation. In this work, we introduce novel finetuning strategy to enhance the quality-diversity trade-off in generative models for creative domains. Specifically, we combine distillation and reinforcement learning (RL) to optimise two complementary objectives. The first is novel CFG distillation objective where we distill the behavior of teacher into the student. Critically, the teacher is the CFG-augmented base model, rather than larger third-party model as commonly done in the literature (Hinton et al., 2015). This involves minimizing the KL divergence between the logits of the teacher and the student on the data distribution generated by the student (to reduce train-test mismatch), following the on-policy distillation framework of Agarwal et al. (2024). By distilling CFG into the model weights, we improve generation quality while eliminating CFGs inference overhead. The second is novel RL with diversity reward objective, maximising the diversity across pairs of generations for given prompt. Diversity is measured with by comparing pairs of generations by first embedding them and then computing their negative (cosine) similarity in this embedding space. Combining these two objectives allows the finetuned model to inherit the quality of CFG without its cost, while simultaneously maintaining diversity through the RL objective, thus solving the two main drawbacks of CFG at inference time. Diversity-Rewarded CFG Distillation Model merging. The hyperparameter ğ›½ (multiplier for the diversity reward) allows controlling the quality-diversity trade-off at training time, as shown in Figure 1 (right); in contrast, traditional CFG can do it at deployment time. To enable this level of control within our approach, we propose third contribution involving model merging. Specifically, we finetune two models, one focusing on quality (low ğ›½) and the other focusing on diversity (high ğ›½), and then combine their weights by linear interpolation (LERP) (Utans, 1996), balancing between quality and diversity. This follows from the linear mode connectivity property (Frankle et al., 2020) and recent advances in model merging (Ilharco et al., 2023; RamÃ© et al., 2022; Wortsman et al., 2022a), showing that weights finetuned from shared pretrained initialisation can be interpolated, despite the non-linearities in the architecture. Notably, RamÃ© et al. (2023) showed that interpolating between weights finetuned on different rewards trades their abilities off. Thus, we interpolate between the weights of quality-focused model and diversity-focused model: sliding the interpolating coefficient ğœ† between 0 and 1 uncovers strong and steerable quality-diversity front of solutions, without overhead at deployment. Crucially, we find that the interpolated model using ğœ† = 0.5 is our best model, outperforming models explicitly finetuned for intermediate values of the diversity hyperparameter ğ›½. Contribution 1: CFG distillation for quality. We distill the quality benefits of costly inferencetime strategy, CFG, into the weights of our model. Contribution 2: Reinforcement learning for diversity. We reward the model to create diverse generations, reducing the drop in diversity caused by CFG and finetuning. Contribution 3: Model merging for Pareto-optimality. We trade-off quality and diversity at deployment time by interpolating between the weights of quality-focused model and diversity-focused model. Music generation. We apply our strategy to text-to-music generation, creative task where balancing quality and diversity is key. Specifically, we finetune MusicLM (Agostinelli et al., 2023) and consistently improve the quality-diversity trade-off achieved by the CFG-augmented previous state-of-the-art (Cideron et al., 2024). Our experiments, featuring human evaluations, validate that our models generate more diverse music while maintaining high quality. 2. Diversity-rewarded CFG distillation Notations. Let ğ‘šğœƒ denote an auto-regressive model parameterised by ğœƒ. Given an input sequence ğ‘¥ from the space of sequences ğ‘‹, the model ğ‘šğœƒ defines policy ğœ‹ğœƒ by sequential sampling of an output sequence ğ‘¦ = (ğ‘ 1, ..., ğ‘ ğ¿) of length ğ¿. Specifically, given partial sequence ğ‘¦<ğ‘› = (ğ‘ 1, ..., ğ‘ ğ‘›1), the policy ğœ‹ğœƒ samples the next token ğ‘ ğ‘› with temperature ğ‘‡ in the softmax such as ğœ‹ğœƒ(ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥) exp(ğ‘§ğ‘›/ğ‘‡) where ğ‘§ğ‘› = ğ‘šğœƒ(ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥) is the corresponding logit. We also define ğ‘ğœƒ( ğ‘¦ğ‘¥) = (cid:206)ğ¿ ğœ‹ğœƒ(ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥) the probability of sampling the output sequence ğ‘¦ from input ğ‘¥. ğ‘›=1 2.1. CFG distillation for quality CFG. Given model ğ‘šğœƒ and partial sequence ğ‘¦<ğ‘›, CFG is an inference-time strategy that samples the next token by combining conditional logits ğ‘šğœƒ(ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥) (where ğ‘¥ is the users prompt) and unconditional/negative logits ğ‘šğœƒ(ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥ ) (where ğ‘¥ is an empty or negative prompt): ğ‘§ğ¶ğ¹ğºğ›¾ ğ‘› = ğ›¾ ğ‘šğœƒ(ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥) + (1 ğ›¾) ğ‘šğœƒ(ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥ ). (1) 3 Diversity-Rewarded CFG Distillation ğ‘› The next-token probability distribution of the CFG-augmented policy is then ğœ‹ğ¶ğ¹ğºğ›¾ (ğ‘ ğ‘› ğ‘¦<ğ‘›, ğ‘¥) exp(ğ‘§ğ¶ğ¹ğºğ›¾ /ğ‘‡). The guidance factor ğ›¾ controls the adherence to the prompts; higher values typically lead to higher quality (Ho and Salimans, 2022). We use ğ›¾ > 1, extrapolating rather than interpolating, as done in Kreuk et al. (2023). For example, if ğ‘¥ is Soulful jazz song. and ğ‘¥ is Bad audio quality., larger ğ›¾ increases the degree to which the generated sequence resembles typical elements of soulful jazz song while increasing audio quality. ğœƒ CFG distillation. One key limitation is that CFG doubles the inference cost, since it requires the computation of two sets of logits. To eliminate this overhead, we propose an objective that directly distills the benefits of CFG into the model weights. Knowledge distillation (Hinton et al., 2015) traditionally involves compressing large teacher model into smaller student model (Agarwal et al., 2024; Sanh et al., 2019), allowing for efficient deployment while approximating the teachers performance. In contrast, our teacher and student share the same architecture: the teacher (with weights ğœƒinit from initialisation) is simply augmented with CFG. We then follow the standard distillation approach that encourages the student to match logits from the teacher. On-policy distillation. If the teacher provides the label, the remaining question is: how to sample the input data? i.e., the completions on which teachers and students logits should match. Be given set of prompts, offline distillation uses inputs sampled from the teacher (Lin et al., 2020); in contrast, we adopt the on-policy distillation strategy from Agarwal et al. (2024), where data is online and dynamically sampled from the student itself. This reduces the train-test mismatch, a.k.a the exposure bias (Bengio et al., 2015), and was used in recent state-of-the-art LLMs (Gemma Team et al., 2024)1. CFG distillation objective. Overall, starting from the base pretrained initialisation ğœƒinit, we distill the logits obtained by ğœƒinit with CFG into the weights ğœƒ without CFG by maximising: (cid:34) â„š(ğœƒ) = ğ”¼ğ‘¥ğ‘‹ ğ”¼ğ‘¦ğ‘ğœƒ ( ğ‘¥ ) (cid:16) ğ¾ğ¿ ğœ‹ğœƒ ( ğ‘¦<ğ‘›, ğ‘¥) ğœ‹ğ¶ğ¹ğºğ›¾ ğœƒinit ( ğ‘¦<ğ‘›, ğ‘¥) (cid:35) (cid:35) (cid:17) . (cid:34) ğ¿ ğ‘›=1 (2) 2.2. RL for diversity Reduced diversity during alignment. Another key drawback of CFG is that it reduces the diversity of generated outputs as ğ›¾ increases. This occurs because stronger guidance encourages the model to adhere closely to the prompt, leading to outputs that are more similar to each other. This trend also emerges in our CFG distillation procedure (from Section 2.1). We confirm in Figure 2 (right) that diversity across generations from the student steadily decreases as training progresses. More generally, policy collapse is recurring challenge for alignment strategies; for instance, Kirk et al. (2024) demonstrate that RLHF significantly reduces diversity of outputs. This limits exploration of novel and diverse contents, crucial aspect for creative domains. To address this, we introduce strategy encouraging the model to generate diverse samples. This requires two components, detailed below: (1) diversity reward and (2) diversity-enforcing algorithm. Diversity reward: negative cosine similarity of embeddings. We first introduce simple method to quantify the diversity between two generations ğ‘¦1 and ğ‘¦2. We embed those generations with model ğ¸ and compute their cosine similarity in the embedding space. Then, the diversity is defined as ğ‘Ÿğ· ( ğ‘¦1, ğ‘¦2) = 1 ğ¸ ( ğ‘¦1 ) ğ¸ ( ğ‘¦2 ) ğ¸ ( ğ‘¦1 ) ğ¸ ( ğ‘¦2 ) . The key component of this diversity reward is thus the embedding model ğ¸. In our text-to-music application from Section 3, inspired by Futeral et al. (2024), we train novel embedding model ğ¸ using contrastive learning approach. This involves training ğ¸ to map 1Moreover, as we also encourage the generations from the student to optimise diversity reward, samples from the student will be readily available, making this on-policy distillation possible at no cost. 4 Diversity-Rewarded CFG Distillation audio segments from the same song to nearby points in the embedding space while pushing apart embeddings of segments from different songs; this is further described in Appendix C.1. Diversity algorithm: RL for diversity. We now leverage reinforcement learning (RL) to encourage the student policy to generate diverse samples for given prompt. Using the diversity reward defined above, the diversity objective to maximise is: ğ”»(ğœƒ) = ğ”¼ğ‘¥ğ‘‹ (cid:2)ğ”¼ğ‘¦1,ğ‘¦2ğ‘ğœƒ ( ğ‘¥ ) [ğ‘Ÿğ· ( ğ‘¦1, ğ‘¦2)](cid:3) . As shown in Appendix A, an unbiased estimator of ğ”»(ğœƒ) is then: ğ‘Ÿğ· ( ğ‘¦1, ğ‘¦2) log ğ‘ğœƒ( ğ‘¦1ğ‘¥) with ğ‘¥ ğ‘‹, ğ‘¦1, ğ‘¦2 ğ‘ğœƒ(ğ‘¥). (3) (4) Due to the similarity of Equation (4) with policy gradient (Sutton et al., 1999), this diversity can be optimised with standard algorithms such as REINFORCE (Williams, 1992), updating the parameters ğœƒ by increasing the likelihood of generations that lead to high (diversity) rewards. Overall, the only difference is that this gradient asks for multiple generations, matching the requirements of multi-sample RL algorithms recently used in RLHF (Ahmadian et al., 2024; Sessa et al., 2024). 2.3. Diversity-rewarded CFG distillation Combining both CFG distillation from Section 2.1 and the diversity reward from Section 2.2, we maximise the following novel diversity-rewarded CFG distillation objective: â„šğ”»(ğœƒ) = â„š(ğœƒ) + ğ›½ ğ”»(ğœƒ), (5) with ğ›½ the diversity hyperparameter scaling the RL diversity term to the KL quality distillation term. Low values of ğ›½ (e.g., 0) lead to policies focused on quality, while large values of ğ›½ (e.g., 15) lead to policies prioritising diversity at slight cost in terms of quality, as visible in Figure 1 (right). 2.4. Model merging for Pareto-optimal quality-diversity Tuning the hyperparameter ğ›½ allows for some control over the quality-diversity trade-off before training, but exploring the full spectrum of possibilities would require maintaining large set of models with different ğ›½ values. To address this, we leverage model merging, which enables combining the strengths of different models by simply interpolating their weights (Dimitriadis et al., 2023; Ilharco et al., 2023; RamÃ© et al., 2023). This allows us to efficiently adjust the quality-diversity trade-off at deployment time, based on the specific needs of the user or application, with only two finetunings. Specifically, we consider two models finetuned from shared pretrained initialisation with different values of ğ›½, ğœƒğ‘ focused on high quality (ğ›½ = 0) and ğœƒğ‘‘ focused on high diversity (high ğ›½). We then trade-off between their abilities simply by interpolating between their weights: ğœƒğ¿ğ¸ğ‘…ğ‘ƒ = (1 ğœ†) ğœƒğ‘ + ğœ† ğœƒğ‘‘, (6) with ğœ† [0, 1]. For instance, setting ğœ† close to 0 favors the high-quality model, generating outputs that closely adhere to the prompt, while ğœ† close to 1 favors the high-diversity model, resulting in more exploratory outputs. This lightweight approach uncovers strong and steerable quality-diversity front of solutions with minimal computational cost, similar to adjusting ğ›¾ in CFG during inference. 5 Diversity-Rewarded CFG Distillation 3. Experiments The experiments below are structured to address the following research questions. (1) CFG distillation: can we distill the quality of CFG into the weights of the model, and at what cost in terms of diversity? (2) Diversity RL: can we trade-off between quality and diversity by including diversity reward during finetuning? (3) Model merging: can we construct at inference time strong and steerable quality-diversity front of solutions with weight interpolation? 3.1. Text-to-music generation: task, setup and metrics Task. We explore those questions on text-to-music generation, creative task where quality and diversity are important factors. Indeed, single music prompt could map to many different valid generations. For instance, gentle bossa nova piece, relaxing acoustic guitar melody or an ethereal electronic soundscape would all be valid responses to the prompt peaceful sunset on beach. This makes text-to-music generation great testbed to study quality-diversity trade-offs. Model and setup. We follow the experimental setup from Cideron et al. (2024), using their LLM transformer-based architecture, where single autoregressive stage models the semantic and coarse acoustic features. We use their MusicRL-R checkpoint as the base model ğœƒinit, initialising all our experiments. We use the prompt dataset described in Section 4.1 from Cideron et al. (2024), combining multiple sources: synthetic prompts generated from the LaMDA model (Thoppilan et al., 2022), prompts collected via user interactions (Cideron et al., 2024), and prompts derived from MusicCaps (Agostinelli et al., 2023). For CFG, we set ğ›¾ = 3 and use the negative prompt Bad audio quality., as it performed best on the base model (cf. Appendix B). For the diversity reward, we train music embedding model ğ¸ by contrastive self-supervised learning (cf. Appendix C.1). Training details. The partial generations for distillation are sampled from the student in an online fashion (Agarwal et al., 2024) with temperature ğ‘‡ = 0.99. The RL algorithm is variant of REINFORCE (Williams, 1992) with baseline for variance reduction. We use batch size of 128 and learning rate of 0.00015 for all our finetunings. Metrics. We evaluate the quality of the generations with two metrics: (1) the MuLan score (Agostinelli et al., 2023), text adherence metric based on the MuLan embeddings (Huang et al., 2022), and (2) the User Preference score (Cideron et al., 2024), learned on 300k pairwise preferences of music generations to approximate the average human opinion score. We then define the general quality score as ğ‘ quality = ğœ” ğ‘ MuLan + ğ‘ User Preference, using ğœ” = 5 to enforce similar range of variations for the two scores. Critically, those metrics are not used during training. In Section 3.5, we confirm the insights obtained from this quality metric through human evaluation. 3.2. CFG distillation for quality We want to validate whether or not it is possible to match by distillation (at training time) the performances of CFG-augmentation (at inference time) without its inference cost overhead. CFG distillation optimisation. Figure 2 (left) shows the evolution of the KL divergence from Equation (2) between the policy and the CFG-augmented policy along training. When the objective is only to minimise the KL (i.e., ğ›½ = 0), the KL decreases to its lowest value, implying that the CFG-free model learns to approximate the CFG logits. ğ›½ is scheduled to linearly increase until step 1000, which explains why the KL increases after this step for ğ›½ > 0. CFG distillation improves quality. Figure 2 (middle) shows that training significantly improves quality matching the performances of the base model augmented with CFG and ğ›¾ = 3. This suggests 6 Diversity-Rewarded CFG Distillation that CFGs quality can be effectively distilled into the models weights, finding confirmed by human evaluations in Section 3.5. CFG distillation reduces diversity. Figure 2 (right) shows that the CFG-distilled model (ğ›½ = 0) suffers from decrease in diversity compared to its initialisation, the CFG-free base model. Notably, after only 1k training steps, CFG-distillation reaches the same level of diversity (around 0.37) than the CFG-augmented base model (grey horizontal line for ğ›¾ = 3). As side note, Figure 1 (right), Figure 4 (right) and Figure 6 (in Appendix B) show that higher values of ğ›¾ for CFG further reduce diversity. In the next subsection, we assess the efficiency of our diversity reward to promote diversity. Figure 2 Left. Evolution of the KL divergence between the CFG-distilled student and the CFG-augmented teacher along training. GKD distillation alone (ğ›½ = 0) decreases the KL between the two policies. Middle. Evolution of the quality along training, showing improved quality for all selected values of ğ›½. Right. Evolution of the diversity across generations along training, showing that CFG distillation alone reduces diversity, but that using diversity reward (ğ›½ 0) can actually increase it. The CFG line shows the quality/diversity performance of the CFG-augmented base model serving as teacher. The upper-bound line indicates the mean diversity of two generations (from the base model) for two different prompts. 3.3. RL for diversity Rewarding diversity. We now analyse the results obtained when including the diversity reward from Equation (3), scaled by the hyperparameter ğ›½ {0, 5, 10, 15} in the joint objective. As visible in Figure 2 (right), this increases the diversity across generations along training. For ğ›½ = 10 or ğ›½ = 15, diversity actually gets larger, and closer to the mean diversity between two generations for two different prompts, denoted as an empirical upper-bound. Quality-diversity trade-off. Figure 2 (left) shows that these diversity gains come at the cost of increased KL. This is because higher values of ğ›½ put more emphasis on the diversity term from Equation (3), hindering the minimisation of the KL distillation term from Equation (2). Then policies finetuned with larger values of ğ›½ have lower quality in Figure 2 (middle). This is summarised in Figure 1 (right), where quality-diversity trade-offs are plotted along training for different values of ğ›½. 3.4. Model merging for Pareto-optimal quality-diversity Quality-diversity trade-off. In Figure 3 we interpolate between the model with highest quality (i.e., CFG-distilled with ğ›½ = 0) and models with more diversity: by sliding the coefficient ğœ† from 0 (only the quality model) to 1 (only the diversity model), we construct fronts of solutions that surpass the performance of individual models finetuned for intermediate ğ›½ values. In particular, interpolating towards the solution with maximum diversity (ğ›½ = 15) yields strong and steerable Pareto front 7 Diversity-Rewarded CFG Distillation Figure 3 Quality-diversity trade-off for multiple strategies. The first four lines linear interpolate (LERP) between the quality-focused model (ğ›½ = 0) and more diverse models (those trained with ğ›½ > 0, or the base model), sliding ğœ† between 0 and 1 with step of 0.05. We also report the performance from the uniform (ğœ† = 1 4 ) averaging of the four models finetuned with different ğ›½, denoted as LERP(0, 5, 10, 15) uniform. We include inference-time baseline strategies CFG (when varying ğ›¾) and temperature sampling (when varying the temperature ğ‘‡) applied either on the base model or on the CFG-distilled model. describing the full range of possible trade-offs. This indicates that model merging achieves higher quality for given level of diversity, and vice versa, consistent with previous studies that show how merged models perform better than their non-merged counterparts (RamÃ© et al., 2022; Wortsman et al., 2022a). This is achieved with minimal overhead: only two RL finetuning runs are required, and merging the weights involves negligible computational cost. If only one finetuning is possible, then linearly interpolating towards the base initialisation can also help, by recovering features from pretraining (Wortsman et al., 2022b); this LERP(0, base) outperforms the CFG-augmented base model. If more compute is available for training and we can do four finetunings, then merging them uniformly performs even better, as visible in Figure 3 where the grey cross for LERP(0, 5, 10, 15) uniform is above other fronts. This validates that we can merge an arbitrary number of models. Figure 4 Left. Linear interpolation between the weights of model focused on quality (ğ›½ = 0) and model focused on diversity (ğ›½ = 15), sliding the coefficient ğœ† between 0 and 1. The dashed diagonal represents the expected values if abilities were traded-off linearly between those two models. While the diversity stays close to the diagonal, the quality is above it, explaining the benefits of model merging. Right. For comparison, we also include the results for CFG when sliding ğ›¾ between 1 and 7, performing worse than merged models. Diversity-Rewarded CFG Distillation Baselines. Figure 3 also displays the results for two inference-time baselines: CFG and temperature sampling. Applied to the base model, these strategies are Pareto-dominated by CFG-distilled models with diversity rewards. When applied to the already CFG-distilled model, they do not significantly improve quality nor diversity. Quality-diversity as function of ğœ†. Figure 4 (left) clarifies the effect of weight interpolation on quality and diversity as we slide ğœ† between 0 and 1. Diversity stays close to the diagonal (representing the expected diversity) while the quality is consistently above the diagonal. This highlights the ability to significantly increase diversity with only minor quality compromises via model merging. 3.5. Human evaluation Protocol. To conclude our experiments, we validate via human evaluation the improved qualitydiversity trade-offs. We evaluate five models: the base model (we expect low quality but high diversity), the base model with CFG ğ›¾ = 3 (high quality but low diversity), the CFG-distilled model for ğ›½ = 0 (high quality but low diversity) and ğ›½ = 15 (medium quality and high diversity), and finally LERP(0, 15) merging uniformly with ğœ† = 0.5 the two previous models (high quality and medium diversity). For quality (i.e., acoustic quality, text adherence, and musicality), we use the same evaluation protocol as in Cideron et al. (2024): the raters see two generations from different models coming from the same text prompt and rate them on scale from 1 to 5. We use 100 different prompts and each one is seen by 3 different raters. For diversity, we rely on similar protocol except that the raters see two pairs of music clips generated from the same text prompts and are rate how diverse the pairs of generations are. We use 50 different prompts and each one is rated by 3 raters. Figure 5 Left. Side-by-side human evaluation for quality. Right. Side-by-side human evaluation for diversity. ğ‘Š+ğ‘‡/2 ğ‘Š+ğ¿+ğ‘‡ with ğ‘Š the number of The score corresponds to the win rate of model over model B, computed as wins of over B, ğ‘‡ the number of ties, ğ¿ the number of losses of against B. This confirms that our approach improves the quality-diversity trade-off. For instance, the merged model LERP(0, 15) generates music with higher diversity than the CFG-augmented base model (ğ›¾ = 3) in 57% of the comparisons, while being rated as more qualitative half of the time (51%). Human evaluation for quality. Figure 5 (left) presents the side-by-side win rate for quality. The CFG-distilled model (i.e., ğ›½ = 0) performs on par with the CFG-augmented base model (i.e., CFG) with win rate of 0.50; they both outperform the base model without CFG (first line), with 0.66 and 0.68 win rate respectively. LERP(0, 15) achieves win rate above 0.50 against all models. 9 Diversity-Rewarded CFG Distillation Human evaluation for diversity. Figure 5 (right) presents the side-by-side win rate for diversity. The CFG-augmented base model (i.e., CFG) and the CFG-distilled model (i.e., ğ›½ = 0) are also evaluated similarly in terms of diversity (0.52 win rate for the former); yet, they are consistently seen less diverse than the three other models. Notably, the CFG-distilled model with diversity reward (i.e., ğ›½ = 15) has win rate of 0.73 and 0.79 against them. As side note, this ğ›½ = 15 model performs on par with the base model (0.50 win rate), though the quantitative metrics from Figure 2 (right) suggested that it was actually more diverse; in Section 5, we relate this discrepancy to the standard reward hacking (Amodei et al., 2016; Gao et al., 2023) phenomenon of RL. Human evaluation for quality-diversity trade-off. Human evaluations confirm the effectiveness of our approach in improving the quality-diversity trade-off. The diversity-focused model (ğ›½ = 15) exhibits higher quality than the base model while maintaining comparable diversity. Importantly, the merged model LERP(0, 15) exhibits higher diversity than the CFG-augmented base model while maintaining comparable quality, and without incurring the additional inference cost of CFG. Qualitative analysis. To facilitate qualitative analysis, we host music generated from all evaluated models on the website google-research.github.io/seanet/musiclm/diverse_music. An examination of generic prompts like Rock song. confirms that CFG improves quality (i.e., less acoustic artifacts, better musicality) but reduces diversity, as generated guitars tend to be very similar across different generations. In contrast to the CFG-augmented model, the ğ›½ = 15 model generates wider variety of rhythms while still demonstrating clear improvement in quality over the base model. On prompts like Opera singer or Male choir harmony, the quality-focused model ğ›½ = 0 generates conventional outputs, while the diverse model ğ›½ = 15 produces more unconventional and creative results, sometimes leading to unexpected elements like unusual instrumentation (e.g., drums). By averaging the weights of these two models (LERP), we can effectively balance these qualities, generating high-quality music that is both faithful to the prompt and more creative. 4. Related work Classifier-free guidance (CFG). Initially introduced for diffusion models (Ho and Salimans, 2022) and later adapted for autoregressive LLMs (Gafni et al., 2022; Sanchez et al., 2023; Wings, 2022), CFG has found widespread application in various generative domains, including image (Nichol et al., 2021; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Yu et al., 2022), video (Blattmann et al., 2023; Ho et al., 2022), and audio (Copet et al., 2023; Kreuk et al., 2023) generation. However, the detrimental impact of CFG on diversity is well-documented (Dhariwal and Nichol, 2021; Kreuk et al., 2023; Nichol et al., 2021), limiting its application when exploration is key. Distillation. Knowledge distillation (Hinton et al., 2015) is emerging as powerful technique to train state-of-the-art models (Gemma Team et al., 2024). By transferring knowledge from teacher, the student can perform better than with standard training on the same data (Gu et al., 2023; Lin et al., 2020; Sanh et al., 2019). Closer to our work, Meng et al. (2023) propose two-stage offline procedure to distill CFG-augmented diffusion model. In contrast, we employ single-stage on-policy distillation procedure (Agarwal et al., 2024) to distill CFG-augmented LLM, while introducing novel diversity-promoting RL algorithm and model merging for improved quality-diversity trade-off. Quality-diversity in LLMs. Zhang et al. (2021) compare the quality-diversity trade-offs of various inference-time strategies for LLMs, including temperature sampling, top-k sampling (Fan et al., 2018), and nucleus sampling (Holtzman et al., 2020). These methods perform similarly except when quality is prioritized over diversity, where nucleus sampling performs best. Regarding finetuning strategies, Chaudhari et al. (2024); Kirk et al. (2024); Li et al. (2024b); Mohammadi (2024) have shown that 10 Diversity-Rewarded CFG Distillation RLHF can negatively impact the diversity of generated text, often measured with metrics like BLEU. To the best of our knowledge, we are the first to introduce an RL algorithm to optimize diversity, which could potentially also solve those reductions in diversity in RLHF. In contrast, existing quality-diversity algorithms (Cideron et al., 2020; Cully et al., 2015; Ding et al., 2024; Lehman and Stanley, 2011; Mouret and Clune, 2015) aim at finding population of agents with both high-quality and diverse behaviors. Most similarly, Li et al. (2016); Zhang et al. (2018) also tried to increase the diversity across generations produced by single agent, but measured by the number of distinct ğ‘›-grams, and optimised with objectives based on mutual information (Shannon, 1948). Model merging for Pareto-optimality. Model merging via weight averaging (Utans, 1996) has two main applications in deep learning. First, it increases robustness and generalisation by reducing variance (RamÃ© et al., 2022; Wortsman et al., 2022a) and memorisation (Lin et al., 2024; RamÃ© et al., 2024). Second, merging models combines their strengths (Dimitriadis et al., 2023; Ilharco et al., 2023; Wang et al., 2024), property recently employed in RamÃ© et al. (2023) for multi-objective RL, where policies finetuned with different rewards are interpolated. Similarly, we interpolate between policies where only one of them is rewarded for diversity. Despite recent theoretical efforts to explain the empirical success of model merging (Ferbach et al., 2024; Ortiz-Jimenez et al., 2023; RamÃ© et al., 2024), complete understanding remains elusive. Music generation. Diffusion models (Huang et al., 2023; Liu et al., 2023; Schneider et al., 2023) and Transformers (Agostinelli et al., 2023; Copet et al., 2023) are now the state-of-the-art architectures for music generation. In this work, we leverage the Transformer-based approach from Agostinelli et al. (2023), casting music generation as categorical prediction task in the discrete token space provided by neural audio codec (DÃ©fossez et al., 2022; Zeghidour et al., 2022). RL-finetuning for music has previously been explored in Cideron et al. (2024); Guimaraes et al. (2017); Jaques et al. (2017); Kotecha (2018); Latif et al. (2023). In contrast, we are the first to distill CFG, to enforce diversity through RL, to apply model merging or to improve the quality-diversity trade-off for music. 5. Discussions and limitations Amplification and distillation. In this work, we leverage teacher-student framework where the teacher is an augmented version of the student model itself. This allows to distill the quality improvements obtained from potent but expensive inference strategy, eliminating the overhead at deployment. This echoes the principles of iterated distillation and amplification (IDA) (Cotra, 2018), where the model iteratively learns on data generated by an augmented version of itself. Examples of this can be seen in AlphaGo (Silver et al., 2016), achieving superhuman performance by distilling the knowledge of MCTS (Kocsis and SzepesvÃ¡ri, 2006), or in recent LLM works (Wang et al., 2023; Yu et al., 2024b), distilling System 2 into System 1 by imitating offline predictions obtained from Chain-of-Thought (Wei et al., 2022). Given the effectiveness of scaling inference-time strategies (Snell et al., 2024), we anticipate wider adoption of such amplification-then-distillation techniques. Extension to other models or modalities. The three main components of our approach distillation of an inference strategy, RL with diversity reward, and model merging could be readily adapted to other generative architectures. For instance, model merging is already popular strategy for diffusion models (Biggs et al., 2024; Purplekeyboard, 2022). Additionally, while our focus is on text-to-music, similar strategies could be applied to other setups involving text, image or video generation. In those domains, promoting diversity can foster fairer representations and pluralistic opinions (Lake et al., 2024; Srinivasan et al., 2024), but also facilitate exploring diverse reasoning paths (Liang et al., 2023; Yu et al., 2024a) and enhance multi-sample decoding strategies (Bertsch et al., 2023; Li et al., 2024a). 11 Diversity-Rewarded CFG Distillation Diversity measures. We used negative cosine similarity between embeddings as our diversity measure. Critically, we demonstrate in Appendix C.2 its superior correlation with human perception of diversity compared to token-level entropy: we suspect this is because diversity should be assessed at the sentence level for creative tasks. Yet, qualitative inspection suggests that this diversity measure can still be hacked (Amodei et al., 2016; Gao et al., 2023). For example, models finetuned to maximise diversity sometimes generate excessive background drums, likely due to bias of the underlying embedding model ğ¸. This could be changed by making ğ¸ more invariant to background drums; alternatively, inspired by Ding et al. (2024), we could learn directly human feedback (or AI-feedback) diversity embedding, by asking humans (or LLMs) to rate the similarity of generations. More broadly, relying on single diversity metric may be insufficient, as diversity can manifest in various ways (Levy et al., 2024); in music, diversity could focus on variations in voice, key, tempo, or other elements; in NLP, summarization task may prioritise syntactic diversity over semantic diversity. By incorporating multiple diversity rewards, each targeting specific variation, and then leveraging model merging, we could achieve fine-grained control over diversity at deployment time. 6. Conclusion In this work, we introduced diversity-rewarded CFG distillation, novel finetuning approach to enhance the quality-diversity trade-off in generative models. First, we online distilled CFG, eliminating its computational overhead at deployment. Second, to enhance diversity across generations, we incorporated an RL procedure that optimises diversity reward based on similarity embeddings. Third, we leveraged model merging to enable dynamic control over the quality-diversity trade-off at deployment time. Through extensive experiments on text-to-music generation, we demonstrated the validity of our strategy, with our finetuned-then-merged model performing best according to human evaluations. We believe that our work is promising direction for tasks where alignment and creativity are key, and that it can be easily extended to setups beyond text-to-music generation."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Daniele Calandriello and Nino Vieillard for insightful comments on drafts of the paper. We also thank the people involved in designing and building the distillation and RL training infrastructures used in our experiments: LÃ©onard Hussenot, Robert Dadashi, Pier Giuseppe Sessa, Matt Hoffman, Abe Friesen, Andrea Michi, Sabela Ramos, Piotr Stanczyk, Danila Sinopalnikov, AmÃ©lie HÃ©liou, Alexis Jacq, and Nikola Momchev. Finally, we thank the contributors of MusicLM: Timo Denk, Mauricio Zuluaga, Marco Tagliasacchi, Matt Sharifi, Michael Dooley, Mauro Verzetti, Damien Vincent, Matej Kastelic, ZalÃ¡n Borsos, Brian McWilliams, Victor Ungureanu, Ondrej Skopek, and Christian Frank. 12 Diversity-Rewarded CFG Distillation"
        },
        {
            "title": "References",
            "content": "R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R. Garea, M. Geist, and O. Bachem. On-policy distillation of language models: Learning from self-generated mistakes. In ICLR, 2024. (pp. 2, 4, 6, and 10) A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, et al. Musiclm: Generating music from text. arXiv preprint, 2023. (pp. 1, 3, 6, 11, and 21) A. Ahmadian, C. Cremer, M. GallÃ©, M. Fadaee, J. Kreutzer, A. ÃœstÃ¼n, and S. Hooker. Back to basics: Revisiting REINFORCE style optimization for learning from human feedback in LLMs. arXiv preprint, 2024. (p. 5) D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. ManÃ©. Concrete problems in AI safety. arXiv preprint, 2016. (pp. 10 and 12) S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In NeurIPS, 2015. (p. 4) A. Bertsch, A. Xie, G. Neubig, and M. R. Gormley. Its MBR all the way down: Modern generation techniques through the lens of minimum bayes risk. arXiv preprint, 2023. (p. 11) B. Biggs, A. Seshadri, Y. Zou, A. Jain, A. Golatkar, Y. Xie, A. Achille, A. Swaminathan, and S. Soatto. Diffusion soup: Model merging for text-to-image diffusion models. arXiv preprint, 2024. (p. 11) A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. (p. 10) Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghidour. AudioLM: language modeling approach to audio generation. TASLP, 2023. (p. 1) G. Brown, J. L. Wyatt, and P. TiÅˆo. Managing diversity in regression ensembles. JMLR, 2005. (p. 2) S. Chaudhari, P. Aggarwal, V. Murahari, T. Rajpurohit, A. Kalyan, K. Narasimhan, A. Deshpande, and B. C. da Silva. Rlhf deciphered: critical analysis of reinforcement learning from human feedback for llms. arXiv preprint, 2024. (p. 10) G. Cideron, T. Pierrot, N. Perrin, K. Beguir, and O. Sigaud. Qd-rl: Efficient mixing of quality and diversity in reinforcement learning. arXiv preprint, 2020. (p. 11) G. Cideron, S. Girgin, M. Verzetti, D. Vincent, M. Kastelic, Z. Borsos, B. McWilliams, V. Ungureanu, O. Bachem, O. Pietquin, M. Geist, L. Hussenot, N. Zeghidour, and A. Agostinelli. MusicRL: Aligning music generation to human preferences. In ICML, 2024. (pp. 1, 3, 6, 9, and 11) J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. DÃ©fossez. Simple and controllable music generation. In NeurIPS, 2023. (pp. 1, 10, and 11) A. Cotra. Iterated Distillation and Amplification, 2018. URL https://ai-alignment.com/itera ted-distillation-and-amplification-157debfd1616. (p. 11) A. Cully, J. Clune, D. Tarapore, and J.-B. Mouret. Robots that can adapt like animals. Nature, 2015. (p. 11) 13 Diversity-Rewarded CFG Distillation A. DÃ©fossez, L. MazarÃ©, M. Orsini, A. Royer, P. PÃ©rez, H. JÃ©gou, E. Grave, and N. Zeghidour. Moshi: speech-text foundation model for real-time dialogue. (p. 1) A. DÃ©fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural audio compression. TMLR, 2022. (p. 11) P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. (pp. 1 and 10) N. Dimitriadis, P. Frossard, and F. Fleuret. Pareto manifold learning: Tackling multiple tasks via ensembles of single-task models. In ICML, 2023. (pp. 5 and 11) L. Ding, J. Zhang, J. Clune, L. Spector, and J. Lehman. Quality diversity through human feedback: Towards open-ended diversity-driven optimization. In ICML, 2024. (pp. 11 and 12) A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. (p. 20) A. Fan, M. Lewis, and Y. Dauphin. Hierarchical neural story generation. In I. Gurevych and Y. Miyao, editors, ACL, 2018. (p. 10) D. Ferbach, B. Goujaud, G. Gidel, and A. Dieuleveut. Proving linear mode connectivity of neural networks via optimal transport. In AISTATS, 2024. (p. 11) J. Frankle, G. K. Dziugaite, D. M. Roy, and M. Carbin. Linear mode connectivity and the lottery ticket hypothesis. In ICML, 2020. (p. 3) M. Freitag and Y. Al-Onaizan. Beam search strategies for neural machine translation. arXiv preprint, 2017. (p. 1) M. Futeral, A. Agostinelli, M. Tagliasacchi, N. Zeghidour, and E. Kharitonov. Mad speech: Measures of acoustic diversity of speech. arXiv preprint, 2024. (pp. 4 and 20) O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and Y. Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In S. Avidan, G. J. Brostow, M. CissÃ©, G. M. Farinella, and T. Hassner, editors, ECCV, 2022. (p. 10) L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In ICML, 2023. (pp. 10 and 12) G. Gemini Team. Gemini: family of highly capable multimodal models. 2023. (p. 1) Gemma Team, M. Riviere, S. Pathak, P. G. Sessa, C. Hardin, S. Bhupatiraju, L. Hussenot, T. Mesnard, B. Shahriari, A. RamÃ©, et al. Gemma 2: Improving open language models at practical size. arXiv preprint, 2024. (pp. 4 and 10) Y. Gu, L. Dong, F. Wei, and M. Huang. Knowledge distillation of large language models. arXiv preprint, 2023. (p. 10) G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias, and A. Aspuru-Guzik. Objectivereinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint, 2017. (p. 11) S. Hamilton. Detecting mode collapse in language models via narration. arXiv preprint, 2024. (p. 1) 14 Diversity-Rewarded CFG Distillation G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in neural network. In NeurIPS, 2015. (pp. 2, 4, and 10) J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint, 2022. (pp. 1, 4, and 10) J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint, 2022. (pp. 1 and 10) A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degeneration. In ICLR, 2020. (p. 10) Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. W. Ellis. MuLan: joint embedding of music audio and natural language. In ISMIR, 2022. (p. 6) Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank, et al. Noise2music: Text-conditioned music generation with diffusion models. arXiv preprint, 2023. (p. 11) G. Ilharco, M. Tulio Ribeiro, M. Wortsman, S. Gururangan, L. Schmidt, H. Hajishirzi, and A. Farhadi. Editing models with task arithmetic. In ICLR, 2023. (pp. 3, 5, and 11) N. Jaques, S. Gu, D. Bahdanau, J. M. HernÃ¡ndez-Lobato, R. E. Turner, and D. Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In ICML, 2017. (p. 11) R. Kirk, I. Mediratta, C. Nalmpantis, J. Luketina, E. Hambro, E. Grefenstette, and R. Raileanu. Understanding the effects of RLHF on LLM generalisation and diversity. In ICLR, 2024. (pp. 2, 4, and 10) L. Kocsis and C. SzepesvÃ¡ri. Bandit based monte-carlo planning. In ECML, 2006. (pp. 1 and 11) N. Kotecha. Bach2bach: generating music using deep reinforcement learning approach. arXiv preprint, 2018. (p. 11) F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. DÃ©fossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi. Audiogen: Textually guided audio generation. In ICLR, 2023. (pp. 1, 4, and 10) T. Lake, E. Choi, and G. Durrett. From distributional to overton pluralism: Investigating large language model alignment. arXiv preprint, 2024. (p. 11) S. Latif, H. CuayÃ¡huitl, F. Pervez, F. Shamshad, H. S. Ali, and E. Cambria. survey on deep reinforcement learning for audio-based applications. Artificial Intelligence Review, 2023. (p. 11) J. Lehman and K. O. Stanley. Evolving diversity of virtual creatures through novelty search and local competition. In GECCO, 2011. (pp. 2 and 11) A. Levy, B. R. Shalom, and M. Chalamish. guide to similarity measures. arXiv preprint, 2024. (p. 12) J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. diversity-promoting objective function for neural conversation models. In NAACL, 2016. (p. 11) J. Li, Q. Zhang, Y. Yu, Q. Fu, and D. Ye. More agents is all you need. arXiv preprint, 2024a. (p. 11) M. Li, W. Shi, A. Pagnoni, P. West, and A. Holtzman. Predicting vs. acting: trade-off between world modeling & agent modeling. arXiv preprint, 2024b. (p. 10) T. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint, 2023. (p. 11) 15 Diversity-Rewarded CFG Distillation A. Lin, J. Wohlwend, H. Chen, and T. Lei. Autoregressive knowledge distillation through imitation learning. arXiv preprint, 2020. (pp. 4 and 10) Y. Lin, L. Tan, Y. Hao, H. Wong, H. Dong, W. Zhang, Y. Yang, and T. Zhang. Spurious feature diversification improves out-of-distribution generalization. In ICLR, 2024. (p. 11) H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. P. Mandic, W. Wang, and M. D. Plumbley. Audioldm: Textto-audio generation with latent diffusion models. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, ICML, 2023. (p. 11) C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho, and T. Salimans. On distillation of guided diffusion models. In CVPR, 2023. (pp. 1 and 10) B. Mohammadi. Creativity has left the chat: The price of debiasing language models. arXiv preprint, 2024. (pp. 2 and 10) E. Mostaque. Tweet on the examples of negative prompting, 2023. URL https://x.com/EMosta que/status/1596905782859436033. (p. 20) J.-B. Mouret and J. Clune. Illuminating search spaces by mapping elites. arXiv preprint, 2015. (pp. 2 and 11) A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, 2021. (p. 10) G. Ortiz-Jimenez, A. Favero, and P. Frossard. Task arithmetic in the tangent space: Improved editing of pre-trained models. In NeurIPS, 2023. (p. 11) Purplekeyboard. Reddit thread titled What is the point of the endless model merges?, 2022. URL https://www.reddit.com/r/StableDiffusion/comments/11kau9d/what_is_the_po int_of_the_endless_model_merges/. (p. 11) A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint, 2022. (pp. 1 and 10) A. RamÃ©, M. Kirchmeyer, T. Rahier, A. Rakotomamonjy, P. Gallinari, and M. Cord. Diverse weight averaging for out-of-distribution generalization. In NeurIPS, 2022. (pp. 3, 8, and 11) A. RamÃ©, G. Couairon, M. Shukor, C. Dancette, J.-B. Gaya, L. Soulier, and M. Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. In NeurIPS, 2023. (pp. 3, 5, and 11) A. RamÃ©, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and J. Ferret. WARM: On the benefits of weight averaged reward models. In ICML, 2024. (p. 11) R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. (p. 10) C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. (p. 10) G. Sanchez, H. Fan, A. Spangher, E. Levi, P. S. Ammanamanchi, and S. Biderman. Stay on topic with classifier-free guidance. arXiv preprint, 2023. (p. 10) 16 Diversity-Rewarded CFG Distillation V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint, 2019. (pp. 4 and 10) F. Schneider, O. Kamal, Z. Jin, and B. SchÃ¶lkopf. MoÃ»sai: Text-to-music generation with long-context latent diffusion. arXiv preprint, 2023. (p. 11) F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: unified embedding for face recognition and clustering. In CVPR, 2015. (p. 21) P. G. Sessa, R. Dadashi, L. Hussenot, J. Ferret, N. Vieillard, A. RamÃ©, B. Shariari, S. Perrin, A. Friesen, G. Cideron, et al. BOND: Aligning llms with best-of-ğ‘› distillation. arXiv preprint, 2024. (p. 5) D. Shanmugam, D. Blalock, G. Balakrishnan, and J. Guttag. Better aggregation in test-time augmentation. In ICCV, 2021. (p. 1) C. E. Shannon. mathematical theory of communication. The Bell system technical journal, 1948. (p. 11) D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search. Nature, 2016. (p. 11) C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint, 2024. (p. 11) H. Srinivasan, C. Schumann, A. Sinha, D. Madras, G. O. Olanubi, A. Beutel, S. Ricco, and J. Chen. Generalized people diversity: Learning human perception-aligned diversity representation for people images. In ACM FAccT, 2024. (p. 11) R. S. Sutton. Reinforcement learning: An introduction. Bradford Book, 2018. (p. 19) R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. NIPS, 1999. (p. 5) R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint, 2022. (p. 6) J. Utans. Weight averaging for neural networks and local resampling schemes. In AAAI, 1996. (pp. 3 and 11) K. Wang, R. Kidambi, R. Sullivan, A. Agarwal, C. Dann, A. Michi, M. Gelmi, Y. Li, R. Gupta, A. Dubey, et al. Conditioned language policy: general framework for steerable multi-objective finetuning. In EMNLP, 2024. (p. 11) P. Wang, Z. Wang, Z. Li, Y. Gao, B. Yin, and X. Ren. Scott: Self-consistent chain-of-thought distillation. In ACL, 2023. (p. 11) J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain-of-Thought prompting elicits reasoning in large language models. In NeurIPS, 2022. (p. 11) R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Reinforcement learning, 1992. (pp. 5, 6, and 19) R. H. Wings. Tweet on classifier-free guidance for autoregressive models, 2022. URL https: //x.com/RiversHaveWings/status/1478093658716966912. (p. 10) Diversity-Rewarded CFG Distillation M. Wortsman, G. Ilharco, S. Y. Gadre, R. Roelofs, R. Gontijo-Lopes, A. S. Morcos, H. Namkoong, A. Farhadi, Y. Carmon, S. Kornblith, and L. Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, 2022a. (pp. 3, 8, and 11) M. Wortsman, G. Ilharco, J. W. Kim, M. Li, H. Hajishirzi, A. Farhadi, H. Namkoong, and L. Schmidt. Robust fine-tuning of zero-shot models. In CVPR, 2022b. (p. 8) F. Yu, L. Jiang, H. Kang, S. Hao, and L. Qin. Flow of reasoning: Efficient training of llm policy with divergent thinking. arXiv preprint, 2024a. (p. 11) J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, B. Hutchinson, W. Han, Z. Parekh, X. Li, H. Zhang, J. Baldridge, and Y. Wu. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022. (p. 10) P. Yu, J. Xu, J. Weston, and I. Kulikov. Distilling system 2 into system 1. arXiv preprint, 2024b. (p. 11) N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi. Soundstream: An end-to-end neural audio codec. Trans. Audio Speech Lang. Process., 30, 2022. (p. 11) H. Zhang, D. Duckworth, D. Ippolito, and A. Neelakantan. Trading off diversity and quality in natural language generation. In A. Belz, S. Agarwal, Y. Graham, E. Reiter, and A. Shimorina, editors, ACL, 2021. (pp. 2, 10, and 21) Y. Zhang, M. Galley, J. Gao, Z. Gan, X. Li, C. Brockett, and B. Dolan. Generating informative and diverse conversational responses via adversarial information maximization. NeurIPS, 2018. (p. 11) 18 Diversity-Rewarded CFG Distillation Diversity-Rewarded CFG Distillation"
        },
        {
            "title": "Supplementary material",
            "content": "A. Policy gradient for the diversity reward In this appendix we consider the general case of optimising diversity across ğ‘ generations, where: ğ‘Ÿğ· ( ğ‘¦1, . . . , ğ‘¦ğ‘) = (cid:205)ğ‘–, ğ‘— {1,...,ğ‘ },ğ‘– ğ‘— ğ‘Ÿğ· ( ğ‘¦ğ‘–, ğ‘¦ ğ‘—) (cid:205)ğ‘–, ğ‘— {1,...,ğ‘ },ğ‘– ğ‘— 1 . (7) The key originality is that this reward considers multiple trajectories simultaneously, while the traditional policy gradient from Sutton (2018) considers single trajectory. We make the theoretical connection below. Lets consider symmetric multi-trajectories reward, where the expected reward is defined as follows: ğ½ (ğœƒ) = 1 ğ‘ ğ”¼ğ‘¥ğ‘‹ (cid:2)ğ”¼ğ‘¦1,...,ğ‘¦ğ‘ ğ‘ğœƒ ( ğ‘¥ ) [ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘)](cid:3) . If we expand the inner expectation: ğ”¼ğ‘¦1,...,ğ‘¦ğ‘ ğ‘ğœƒ ( ğ‘¥ ) [ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘)] = ğ‘¦1,...,ğ‘¦ğ‘ ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘) ğ‘ (cid:214) ğ‘–=1 ğ‘ğœƒ( ğ‘¦ğ‘–ğ‘¥). Then we take the gradient: ğ”¼ğ‘¦1,...,ğ‘¦ğ‘ ğ‘ğœƒ ( ğ‘¥ ) [ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘)] = ğœƒ (cid:34) ğ‘¦1,...,ğ‘¦ğ‘ ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘) (cid:35) ğ‘ğœƒ( ğ‘¦ğ‘–ğ‘¥) ğ‘ (cid:214) ğ‘–= = = = ğ‘ ğ‘¦1,...,ğ‘¦ğ‘ ğ‘—=1 ğ‘ğœƒ( ğ‘¦ ğ‘—ğ‘¥)ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘) ğ‘ (cid:214) ğ‘–=1,ğ‘– ğ‘— ğ‘ğœƒ( ğ‘¦ğ‘–ğ‘¥) ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘) ğ‘ (cid:214) ğ‘–=1 ğ‘ğœƒ( ğ‘¦ğ‘–ğ‘¥) ğ‘ ğ‘—=1 log ğ‘ğœƒ( ğ‘¦ ğ‘—ğ‘¥) ğ‘¦1,...,ğ‘¦ğ‘ ğ‘ ğ‘ (cid:214) log ğ‘ğœƒ( ğ‘¦ ğ‘—ğ‘¥)ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘) ğ‘ğœƒ( ğ‘¦ğ‘–ğ‘¥) ğ‘—=1 ğ‘¦1,...,ğ‘¦ğ‘ = ğ‘ ğ‘¦1,...,ğ‘¦ğ‘ log ğ‘ğœƒ( ğ‘¦1ğ‘¥)ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘) ğ‘–=1 ğ‘ (cid:214) ğ‘ğœƒ( ğ‘¦ğ‘–ğ‘¥). ğ‘–=1 (8) (9) (10) (11) (12) (13) (14) For Equation (12), we used ğ‘™ğ‘œğ‘”ğ‘¥ = ğ‘¥/ğ‘¥. For Equation (14), we used the symmetry of ğ‘Ÿğ· and change of variable. Hence, non biased estimator of the policy gradient for ğ½ (ğœƒ) is: ğ‘Ÿğ· ( ğ‘¦1, ..., ğ‘¦ğ‘) log ğ‘ğœƒ( ğ‘¦1ğ‘¥) where ğ‘¥ ğ‘‹, ğ‘¦1, ..., ğ‘¦ğ‘ ğ‘ğœƒ(ğ‘¥). (15) Due to the similarity of Equation (15) with the estimator of policy gradient for single-trajectory rewards, the diversity reward can be optimised with the usual policy gradient algorithms. As an example, the sampled gradient with batch size ğµ of the vanilla REINFORCE algorithm (Williams, 1992) would become 1 ğµğ‘ ğ‘Ÿğ· (cid:0) ğ‘¦ğ‘–,1, ..., ğ‘¦ğ‘–,ğ‘ (cid:1) log ğ‘ğœƒ (cid:0) ğ‘¦ğ‘–, ğ‘—ğ‘¥ğ‘–(cid:1) with ğ‘¥ğ‘– ğ‘‹, ğ‘¦ğ‘–, ğ‘— ğ‘ğœƒ(ğ‘¥ğ‘–). (cid:205)ğ‘ (cid:205)ğµ ğ‘–=1 ğ‘—=1 Diversity-Rewarded CFG Distillation B. CFG for text-to-music generation Figure 6 shows the impact of CFG when applied at inference time on the MusicRL-R base model. Specifically, we plot the quality-diversity front of solutions when sliding the adherence hyperparameter ğ›¾ for unconditional and negative prompting. We observe that CFG significantly decreases diversity going from 0.5 (for ğ›¾ = 1) to 0.3 (for ğ›¾ = 7). Critically, we also see that the front of solutions for negative prompting with Bad audio quality. is above the one revealed by unconditional prompting. This specific negative prompt, inspired by those used in image generation (Mostaque, 2023), was selected based on preliminary experiments where it outperformed other negative prompt candidates. Finally, for quality the best ğ›¾ is 3, which we use throughout this work. Figure 6 Effect of CFG on quality and diversity as function of ğ›¾. The quality is greatly improved at the expense of diversity. Negative prompting outperforms unconditional prompting. C. Diversity in music C.1. Embedding model for diversity Diversity in music. The objective of our diversity reward is to estimate the dissimilarity between two music segments. Diversity, especially in music, is multifaceted concept encompassing various aspects such as instrumentals, melodies, tempo. In this work we conceptualise diversity through the following question: Are these two audio excerpts derived from the same audio recording or not? Embedding. Drawing inspiration from Futeral et al. (2024), we train self supervised contrastive model with positive pairs coming from non-overlapping fixed-length chunks of the same audio segment. The model induces an embedding space where segments originating from the same longer audio are mapped to nearby points, while segments from different recordings are mapped further apart. This is useful because cosine similarity in the embedding space can quantify the similarity between two audio segments. Overall, 1-similarity is used as the diversity measure. Details. Following Futeral et al. (2024), our embedding model takes as input 4-seconds audio clip sampled at 16kHz. We generate the mel-spectrogram using window size of 512 samples, hop length of 256 samples, and 156 frequency bins. This results in 376 time frames, each with 156 dimensions. The model architecture is founded on compact Vision Transformer (ViT) (Dosovitskiy et al., 2021), 20 Diversity-Rewarded CFG Distillation Figure 7 Comparison between different measures of diversity; the one provided by our embedding model (left), the Elo rankings provided by human evaluation of diversity (middle) and finally the entropy per token (right). CFG at inference time (yellow) reduces all those measures of diversity. Models trained with our RL diversity reward (ğ›½ = 15 in green) have increased diversity according to our embedding model and human evaluation, but not in terms of entropy. comprising 12 layers. Each layer incorporates 6 attention heads, an embedding dimension of 512, and feed-forward layer with dimension of 1024 (totaling 25 million parameters). The final embeddings are averaged over time and then projected into 192-dimensional space. For training, we employ the semi-hard triplet loss (Schroff et al., 2015) with total batch size of 3840, for 200,000 steps, using 16 kHz audio excerpts sourced from the same training dataset as Agostinelli et al. (2023). Additionally, we augment the training data with noise, tempo and pitch shifts. C.2. Comparison of our diversity metric with human evaluation and entropy We compare the correlation between our diversity metric, the Elo score computed from human diversity evaluation in Section 3.5, and the entropy per generated token (which is commonly used proxy for diversity (Zhang et al., 2021)). Figure 7 highlights that our diversity measure has higher correlation with the human raters notion of diversity than entropy. As an example, ğ›½ = 15 scores high both in terms of Elo score and our diversity metric, while the entropy per token is as low as CFG (which is low diversity approach). Yet, our measure of diversity remains imperfect and thus can be hacked, as further discussed in Section 5."
        }
    ],
    "affiliations": [
        "Google DeepMind"
    ]
}