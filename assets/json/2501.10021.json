{
    "paper_title": "X-Dyna: Expressive Dynamic Human Image Animation",
    "authors": [
        "Di Chang",
        "Hongyi Xu",
        "You Xie",
        "Yipeng Gao",
        "Zhengfei Kuang",
        "Shengqu Cai",
        "Chenxu Zhang",
        "Guoxian Song",
        "Chao Wang",
        "Yichun Shi",
        "Zeyuan Chen",
        "Shijie Zhou",
        "Linjie Luo",
        "Gordon Wetzstein",
        "Mohammad Soleymani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for animating a single human image using facial expressions and body movements derived from a driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key shortcomings causing the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, a lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect a local control module with our model to capture identity-disentangled facial expressions, facilitating accurate expression transfer for enhanced realism in animated scenes. Together, these components form a unified framework capable of learning physical human motion and natural scene dynamics from a diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-of-the-art methods, creating highly lifelike and expressive animations. The code is available at https://github.com/bytedance/X-Dyna."
        },
        {
            "title": "Start",
            "content": "X-Dyna: Expressive Dynamic Human Image Animation Di Chang1,2 Hongyi Xu2 You Xie2 Yipeng Gao1 Zhengfei Kuang3 Shengqu Cai3 Shijie Zhou Chenxu Zhang2 Guoxian Song2 Chao Wang2 Linjie Luo2 1 University of Southern California 4 University of California Los Angeles 2 ByteDance 3 Stanford University 5 University of California San Diego Gordon Wetzstein3 Mohammad Soleymani1 Yichun Shi2 Zeyuan Chen2, 5 2 0 2 7 1 ] . [ 1 1 2 0 0 1 . 1 0 5 2 : r https://x-dyna.github.io/xdyna.github.io/ dichang@usc.edu Figure 1. Sampled animations generated by our X-Dyna, including zero-shot motion transfer, and dynamic human image animation with moving or static humans."
        },
        {
            "title": "Abstract",
            "content": "We introduce X-Dyna, novel zero-shot, diffusion-based pipeline for animating single human image using facial expressions and body movements derived from driving video, that generates realistic, context-aware dynamics for both the subject and the surrounding environment. Building on prior approaches centered on human pose control, X-Dyna addresses key shortcomings causing the loss of dynamic details, enhancing the lifelike qualities of human video animations. At the core of our approach is the Dynamics-Adapter, lightweight module that effectively integrates reference appearance context into the spatial attentions of the diffusion backbone while preserving the capacity of motion modules in synthesizing fluid and intricate dynamic details. Beyond body pose control, we connect local control module with our model to capture identity-disentangled facial expressions, facilitating accu- *Equally contributed as second authors rate expression transfer for enhanced realism in animated scenes. Together, these components form unified framework capable of learning physical human motion and natural scene dynamics from diverse blend of human and scene videos. Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna outperforms state-ofthe-art methods, creating highly lifelike and expressive animations. The code is available at https://github. com/bytedance/X-Dyna. 1. Introduction We investigate the task of human video generation, focusing on animating single human image using body movements and facial expressions derived from driving video of different person. This area has garnered growing interest owing to its numerous applications in digital arts, social media and virtual humans. Building on prior research [4, 16, 38, 41, 49, 54, 55], our goal is to advance the field of zero-shot human image animation by not only Figure 2. We leverage pretrained diffusion UNet backbone for controlled human image animation, enabling expressive dynamic details and precise motion control. Specifically, we introduce dynamics adapter that seamlessly integrates the reference image context as trainable residual to the spatial attention, in parallel with the denoising process, while preserving the original spatial and temporal attention mechanisms within the UNet. In addition to body pose control via ControlNet CP , we introduce local face control module CF that implicitly learns facial expression control from synthesized cross-identity face patch. We train our model on diverse dataset of human motion videos and natural scene videos simultaneously. Our model achieves remarkable transfer of body poses and facial expressions, as well as highly vivid and detailed dynamics for both the human and the scene. enhancing the accuracy of pose and expression transfer but also by incorporating vivid human dynamics, e.g., blowing hair and flowing garments, and natural environmental effects, e.g., waterfalls, rain, and fireworks. Recent approaches have tackled human image animation as controlled image-to-video diffusion task. These methods typically employ parallel UNet to incorporate reference appearance through mutual self-attentions [3], while body motion cues (e.g., 2D skeletons [4, 38, 54] and DensePose [49]) are integrated as spatial guidance through frameworks like ControlNet [53] or PoseGuider [16]. Temporal modules, such as AnimateDiff [9] and Align-YourLatents [1, 2], have been introduced to the diffusion backbone, trained from large-scale videos to enhance consistency and dynamics in visual sequence generation. Despite improvements in control precision and generation realism, the combined modules for human image animation often fall short in capturing intricate visual dynamics, leading to static backgrounds and rigid human motions. This shortcoming, rooted in both network design and training data distribution, ultimately compromises the lifelike quality of the generated videos. To this end, we propose X-Dyna, diffusion-based human image animation pipeline that achieves the accurate transfer of pose and facial expressions along with consistent and vivid human and background dynamics. We observe that the loss of dynamic details primarily arises from the strong appearance constraints on spatial attentions imposed by the appearance reference modules, typically formulated as trainable copy of parallel UNet. To address this, we introduce lightweight cross-frame attention module, Dynamics-Adapter, which seamlessly propagates the reference appearance context to the denoising process by feeding the denoised reference image in parallel with noised sequences to the model. It integrates with the diffusion backbone via trainable copy of the query projector and zero-initialized output projector, ensuring the backbones spatial and temporal generation capability stays intact. Unlike the standard I2V settings [1] that generate subsequent frames from the reference image, our design maintains appearance consistency from varying poses, in coordination with pose control modules. Notably, beyond body pose control, we employ local control module to capture identity-disentangled facial expressions, enhancing realism with accurate expression transfer. While prior image animation models are primarily trained on human videos with static backgrounds, our dynamics adapter enables the learning of subtle human dynamics and fluid environmental effects, in addition to body and facial expression controls, from diverse mixture of human and scene videos. Trained on curated dataset of 900h human dancing and natural scene videos, our method excels at accurately transferring the body poses and facial expressions while generating lifelike human and scene dynamics consistent with the reference image context. We comprehensively evaluate our model on challenging benchmarks [17, 28, 30], and XDyna outperforms state-of-the-art human image animation baselines both quantitatively and qualitatively, demonstrating superior dynamics expressiveness, identity preservation and visual quality. Our main contributions are: zero-shot diffusion-based human image animation model for both pose control and dynamics synthesis, trained on mixture of human and natural scene videos; An efficient Dynamics-Adapter module that effectively incorporates reference appearance while maintaining the foundation models capability in generating high-quality dynamics; local implicit face control module that enables refined, identity-disentangled facial expression control; and Demonstration of captivating zero-shot controllable human image animations and live photos with vivid dynamics. 2. Related Works 2.1. Diffusion Models for Human Video Animation Recent advancements [31] in latent diffusion models [34] have greatly advanced human image animation. Previous approaches [4, 16, 42, 49] commonly employed two-stage training paradigm: in the first stage, pose-driven image model is trained on individual video frames paired with corresponding pose images; in the second stage, temporal module is introduced to capture temporal dynamics, while the image generation model remains fixed. Following this framework, these methods integrate ReferenceNet [16] with UNet architecture to extract appearance features from reference characters. With progress in the video foundation models, recent works [41, 54] have simplified this process by directly fine-tuning Stable Video Diffusion [1], effectively replacing the two-stage training approach. As mentioned in Sec. 1, there are several human video animation methods [4, 16, 18, 25, 27, 38, 41, 42, 46, 49, 54, 55], including CLIP [33] embedding with ControlNet [53], ReferenceNet [4, 16, 49, 55] with ControlNet [53], and SVD [1] with Pose Encoder [44]. However, these methods are not capable of capturing dynamics-related semantic information from the reference image and cannot provide vivid animation of physical details from natural background and human foreground. 2.2. Dynamics Generation Dynamics generation has become critical area in video generation, focusing on creating realistic motion and temporal consistency. GAN-based methods such as TGAN [35] and MoCoGAN [39] pioneered the decomposition of motion and content, allowing for better temporal coherence. However, GANs often struggle with complex motion scenes, and artifacts may appear due to difficulties in modeling long-term dependencies. Later models, such as Progressive Growing of GANs [19], introduced gradual increases in resolution, achieving more stable results in video synthesis. Diffusion models have emerged as powerful alternatives for video generation, with methods like Video Diffusion Models [14] and AnimateDiff [9] incorporating temporal conditioning to ensure consistency across frames. AnimateDiff, for instance, applies temporal attention to produce smoother, continuous animations in human-centered videos. Similarly, Stable Video Diffusion [1] employs temporal modeling strategies to enhance dynamic texture quality, often surpassing GAN-based approaches in long-term coherence and photorealism. These works inspired most recent diffusion-based methods [7, 24] to further improve the ability for dynamics generation. 3. Method Given single reference image IR, the objective of XDyna is to reanimate the human subject with pose and expression sequence Pi derived from driving video, where = 1, . . . , denotes the frame index. Most prior approaches decompose this task into two main sub-tasks: (1) transferring the appearance of the individual and background from the reference image and (2) controlling the video frames based on the pose and expression sequence Pi. X-Dyna not only focuses on generating temporally smooth image sequences but also aims to enhance lifelike dynamics realism. We achieve this by creating vivid and expressive dynamics for both the foreground human and background scenes in an end-to-end fashion, eliminating the need for any foreground and background disentanglement preor post-processing steps. In Section. 3.1, we first examine existing network designs for transferring reference appearance and background, and identity their underlying causes for the loss of dynamic details. We then introduce our dynamics-adapter, which achieves accurate transfer of reference appearance with minimal impact on the diffusion backbones dynamics synthesis capability. To further enhance expression transfer and identity preservation, we integrate an additional local control module using synthetic cross-driven face images, as elaborated in Section 3.3. Our model design enables us to effectively learn human dynamics and environmental effects simultaneously from diverse fusion of human motion 3 Figure 3. a) IP-Adapter [50] can generate vivid texture from the reference image but fails to preserve the appearance. b) Though ReferenceNet [16] can preserve the identity from the human reference, it generates static background without any dynamics. c) DynamicsAdapter provides both expressive details and consistent identities. UNet-based denoising backbone network containing intervened layers of convolutions and attentions, is trained to learn the reverse denoising process. For our task, we employ pretrained text-to-image (T2I) diffusion model Stable Diffusion (SD) as the generative backbone, with the addition of ControlNet [53] module to incorporate 2D skeletal pose control as in recent human image animation work [4, 16, 38], as well as temporal modules [9] for enhanced consistency across generated video frames. Appearance Reference. Previous research has introduced various strategies to maintain appearance consistency with given reference image. Early approaches such as [42] represent reference appearance features using CLIP image embeddings, which are injected into the textconditioned cross-attention layers of the diffusion backbone. More recently, IP-Adapter [50] introduced novel approach where image CLIP embeddings are incorporated into the diffusion model via new cross-attention layers, which learn to predict residual over the original crossattention latents, as illustrated in (Figure 4 (a)). However, due to limitations in the CLIP image embeddings ability to capture detailed appearance information, this approach often results in noticeable identity loss and inconsistencies. The latest human image animation models [4, 16, 49, 55] have addressed these shortcomings by employing ReferenceNet module for appearance control. ReferenceNet, parallel and trainable duplication of the entire diffusion UNet, captures rich, detailed appearance features from single reference image and interconnects with the diffusion UNets self-attention layers through feature concatenation (Fig. 4 (b)). Although this method effectively transFigure 4. a) IP-Adapter [50] encodes the reference image as an image CLIP embedding and injects the information into the crossattention layers in SD as the residual. b) ReferenceNet [16] is trainable parallel UNet and feeds the semantic information into SD via concatenation of self-attention features. c) Dynamics-Adapter encodes the reference image with partially shared-weight UNet. The appearance control is realized by learning residual in the self-attention with trainable query and output linear layers. All other components share the same frozen weight with SD. and natural scenes videos (Section 3.4). Our pipeline is illustrated in Figure. 2. 3.1. Preliminary Latent Diffusion Model. Facilitated by pretrained autoencoder, latent diffusion models [34] are class of diffusion models [13, 36, 37] that synthesize desired samples in the image latent space, starting from Gaussian noise zT (0, 1) and refining through denoising steps. During training, latent representations of images are progressively corrupted by Gaussian noise ϵ, following the Denoising Diffusion Probabilistic Model (DDPM) framework [13]. 4 fers appearance features to the denoising process, the full set of trainable parameters in ReferenceNet often imposes strong and strict influence over all spatial pixels, resulting in static backgrounds and rigid dynamics, as visualized in Fig. 3. 3.2. Dynamics Adapter To address the aforementioned limitations in existing reference appearance control designs, we introduce DynamicsAdapter module which effectively transfers human appearance and background context from the reference image to the diffusion backbone, without compromising its generative capability for dynamic motion synthesis. Inspired by the attention mechanism in the I2V-Adapter which generates subsequent video frames from the given reference image guided by text prompts, our dynamics adapter is tailored for explicit cross-driven pose and expression control. Unlike I2V, our task accommodates motions that may differ significantly from the pose and expression of the reference image, often originating from subjects with distinct identities and body characteristics. To achieve this, is designed as shared-weight, parallel UNet branch that injects layer-by-layer self-attention guidance of reference appearance features. The self-attention calculation in the transformer blocks of the diffusion UNet can be represented as: Ai = softmax( QiK d )Vi, (1) where Qi, Ki, Vi are query, key, and value of the ith latent noise frame, respectively, and is the dimension of the key and query. To introduce reference appearance guidance through our dynamics adapter, we capitalize on the prior capabilities of the original UNet to generate the key KR and value VR from the denoised latent map of the reference image IR. Additionally trainable copy of query projector forms new query matrices from the latent noise of generation frame Ii. This enables cross-frame attention, computed as: = softmax( iK )VR. (2) We combine these two attention outputs with separate output projection matrices, WO and O, as follows: Outi = (AiWO) + (A iW O), (3) where is trainable output projector. This residual term enriches the original spatial attentions with correlated and detailed appearance information derived from the reference image. To implement this seamlessly, we initialize the query projector weights from the original UNet 5 and zero-initialize the output projection layer O, ensuring that the model begins with no effect from these modifications, thus preserving its pre-existing behavior. Our design keeps the generative diffusion backbone untouched, effectively disentangling appearance control from motion generation. This separation allows the diffusion backbone to focus exclusively on pose control and dynamic synthesis, supported by ControlNet [53] and temporal modules [9], while the dynamics adapter manages appearance consistency across frames. 3.3. Implicit Local Face Expression Control In human video synthesis, natural variations in facial expressions significantly enhance realism and expressiveness. While many human image animation models offer robust control over full body poses, there has been limited efforts in simultaneously controlling facial expressions. Previous approaches to representing head motion often use simplified face landmark maps, capturing only key points such as the neck, nose, eyes, and ears. However, these simplified signals lack the detail needed for expressive facial animation. Moreover, even basic facial skeleton encodes identity clues such as face shapes, which inadvertently influence face identity during cross-identity motion transfer. To address these limitations, we introduce S-Face ControlNet, control module in addition to body control, designed for identity-disentangled control over facial expressions and head poses, enabling more expressive and adaptable human video synthesis. Inspired by X-Portrait [47], instead of using an explicit face landmarks map from Ii, we crop the face patch and utilize pre-trained portrait reenactment network like FaceVid2Vid [43] to transfer facial expressions onto randomly selected subject with different facial attributes. This results in an identity-swapped face patch with close expressions, which is then reinserted at the original position of Ii, with other pixels masked as blank, and used as the conditional input to an additional expression ControlNet CF (Figure. 2). Unlike explicit motion control signals, this cross-identity training approach enables CF to learn identity-disentangled facial expressions and head movements implicitly from IT , reducing appearance leakage from the driving signal. Notably, we bypass the need for during inference, allowing expression control directly from the driving video. 3.4. Harmonic Data Fusion Training Prior human image animation models, especially those utilizing ReferenceNet for appearance control, generally mandate static backgrounds in training videos, which limits the capture of dynamic environmental details. On the other hand, collecting video data with both moving human and dynamic backgrounds for training is challenging. We therefore introduce mixed data training strategy, facilitating Table 1. Quantitative comparisons of X-Dyna with the recent state-of-the-art (SOTA) methods on dynamics texture generation. downward-pointing arrow indicates that lower values are better and vise versa. DTFVD [5] is calculated by replacing the FVD pre-trained backbone with one trained on DTDB [10]. FGDTFVD denotes the DTFVD is running on the foreground parts of the videos after segmentation, and BG-DTFVD denotes the DTFVD of the background parts. Method FG-DTFVD BG-DTFVD DTFVD MagicAnimate [49] Animate-Anyone [16] MagicPose [4] MimicMotion [54] X-Dyna 1.753 1.789 1.846 2.639 0.900 2.142 2.034 1.901 3.274 1.101 2.601 2.310 2.412 3.590 1.518 the diffusion backbone along with the temporal module to learn both human dynamics and background scene effects. Specifically, we integrate natural scene videos, such as waterfall, fireworks and wind, alongside real human motion videos for training. For videos without human, we leave the conditional inputs to the Pose ControlNet CP and SFace ControlNet CF blank, enabling the model to generalize background motion independently. By using this mixed data, our model not only achieves more realistic dynamic details than those trained solely on human videos but also reduces unintended effects of ControlNets on background motion from the blank region of pose and expression conditional map. 4. Experiments 4.1. Implementation Details Dataset For animation of human videos, we train our model using custom dataset including monocular camera recordings of 30-second human motions from 107,546 videos (900 hours in total) with both indoor and outdoor scenes. All the data were processed with cropped resolution of 896512. Sequences of low quality were filtered out with [15]. All videos feature real subjects showcasing diverse range of motions and expressions in various scenes. For data processing, we follow the approach outlined in DisCo [4, 42] but enlarge the cropping region to include the full body. For Harmonic Data Fusion Training, we use Skyscape [48] dataset, which contains 3000 time-lapse videos of dynamic sky scenes, e.g., cloudy skies and night scenes with moving stars. Model Training and Inference We utilize SD 1.5 as our generative backbone, and freeze its weights during the entire training phase. Prior to training, CP , CF , and trainable parameters in are initialized using SD 1.5, whereas the motion module is initialized with the weight of AnimateDiff [9] . Our training is conducted in stages, where we first train D, CP , and motion module with Harmonic Data Fusion Training for five epochs. Then, we freeze these modules and train CF for two epochs using human video data only. An AdamW optimizer is utilized with learning rate of 105 to train all modules. Each module undergoes training with 16 video frames in each step. During inference, we do not rely on the face-swapping network and directly feed the cropped local face patches from the driving video into S-Face ControlNet. 4.2. Evaluations and Comparisons Metrics. We use three different groups of data for evaluation. 1) To evaluate the overall human video generation quality, we use the test set split in TikTok [17] proposed by DisCo [42], and report quantitative metrics PSNR, SSIM, L1, LPIPS, FID and cd-FVD for human foreground generation, and FID and cd-FVD for background generation. cd-FVD denotes content-debiased-FVD, better Frechet Video Distance (FVD) [40] metric to reflect the overall generation quality, proposed by [8]. We also report Face Cosine Similarity (Face-Cos) to reflect the face identity preserving ability, following MagicPose [4]. This metric is designed to gauge the models capability to preserve the identity information of the reference image input. To compute this metric, we first align and crop the facial region in both the generated image and the ground truth. Subsequently, we calculate the cosine similarity between the extracted feature by AdaFace [21], frame by frame of the same subject in the test set, and report the averaged value. These metrics has been widely used in previous work [4, 42, 49, 54]. In addition, we report the rate of detected faces among all frames in percentage, denoted as Face-Det. 2) To evaluate the dynamics detail generation quality, we use self-collected test dataset from Pexels [30] with around 100 videos of 2 seconds each and report Dynamic Texture Frechet Video Distance (DTFVD) proposed by [5]. DTFVD is calculated by replacing the pre-trained backbone network in FVD with one trained on Dynamics Texture Database (DTDB) [10] for classification. This metric has also been widely used in other work [5, 24] to evaluate dynamics texture quality. We report DTFVD for both the whole videos and the background part of the videos after running human segmentation. 3) To further confirm the effectiveness of X-Dyna in dynamics detail generation, we conduct comprehensive user study for comparison to other previous work [4, 54]. We collect 50 static real and synthetic reference images from Pexels [30] and generated by MidJourney [28] to feed the model. We then ask users to judge the (1) dynamics quality of background nature, e.g., waterfall, fireworks, cloud, ocean, raining, snowing, grass, etc. (2) dynamics quality of human foreground, e.g., hair, clothes, etc. (3) appearance and identity preservation ability. Quantitative Comparison We compare our method to the state-of-the-art diffusion model-based human video Table 2. Quantitative comparisons of X-Dyna with the recent SOTA methods on human video animation. downward-pointing arrow indicates that lower values are better and vise versa. Face-Cos represents the cosine similarity of the extracted feature by AdaFace [21] of face area between generation and ground truth image. Face-Det denotes the percentage rate of detected valid faces among all frames. denotes the method is not open-sourced; hence, we used the unofficial implementation from [29] to run their method for inference. Method L1 PSNR LPIPS SSIM Face-Cos Face-Det FID cd-FVD FID cd-FVD Foreground Background MagicAnimate [4] Animate-Anyone [16] MagicPose [4] MimicMotion [54] 7.42e-05 11.8e-05 13.7e-05 9.78e-05 17.143 13.411 12.639 14. X-Dyna 7.15e-05 17.201 0.228 0.338 0.345 0.278 0.249 0.739 0.605 0.618 0. 0.724 0.297 0.402 0.396 0.193 0.497 92.1% 89.0% 85.5% 92.0% 94.8% 31.97 33.75 18.52 45. 22.56 237.59 233.39 537.96 150.01 325.35 38.86 34.27 24.43 60.32 25.59 176.17 203.59 480.14 194. 281.78 animation methods, including 1) CLIP embedding-based method DisCo [42]; 2) ReferenceNet-based methods MagicPose [4], and MagicAnimate [49]; and 3) SVD-based method MimicMotion [54]. The main focus of this work is to improve the dynamics details generation quality. Tab. 1 presents quantitative analysis of such quality. X-Dyna achieves significant improvements across different baseline models, indicating that the proposed method generates vivid expressiveness of dynamics. Following previous work, we used sequences 335 to 340 from the TikTok [17] dataset and additional self-collected videos by DisCo [42] to test the animation ability of human subjects. Note that since the TikTok test set only contains indoor scenes, whose background is all static without any motions. Tab. 2 presents quantitative analysis of human foreground subjects and background scenes from various methods, with segmentation using Segment Anything [22]. The proposed methods achieve competitive performance across previous state-of-the-art methods, which indicates that the proposed method generates high-quality videos that align with human reference. Qualitative Comparison We qualitatively compare the dynamics texture generation of X-Dyna with previous methods [4, 54] in Figure 5 and pose & facial expressions control in Figure 6. Note that MagicPose, MagicAnimate, and Animate-Anyone are recent representative works using ReferenceNet, and MimicMotion uses SVD for human video animation. Both MagicPose [4] and MimicMotion [54] exhibit limited expressiveness, with most of their generated dynamics appearing almost static. Please refer to additional video examples provided in the supplementary materials for clearer observations and further comparison. User Study We provide user study to compare X-Dyna with previous works [4, 54]. We collect reference images, pose conditions, and animation results from previous works and X-Dyna of 50 subjects as mentioned in Sec. 4.1. For each subject, we visualize different human poses and facial expressions and ask 100 users to rate the methods (from 0-5) according to the following three criteria: (1) dynamics quality of background nature (BG-Dyn) (2) dynamics Table 3. User study of X-Dyna. We collect the ratings (0-5) from 100 participants for 50 test cases in the test set. We ask them to rate the generation in terms of Foreground Dynamics (FG-Dyn), Background Dynamics (BG-Dyn) and Identity Preserving (ID). Method FG-Dyn BG-Dyn ID Overall MagicAnimate [49] Animate-Anyone [16] MagicPose [4] MimicMotion [54] X-Dyna 2.34 2.21 2.23 2.02 3.87 2.78 2.57 2.18 2.63 4.26 3.45 3.89 3.85 2.79 4.14 2.86 2.89 2.75 2.48 4. Table 4. Ablation Analysis of X-Dyna on dynamics texture generation and local facial expressions generation. w/RefNet denotes Dynamics-Adapter is replaced by ReferenceNet. w/IP-A denotes Dynamics-Adapter is replaced by an IP-Adapter. w/lmk denotes S-Face ControlNet is not used for fine-tuning and face landmarks are used together with the pose skeleton. wo/face denotes S-Face ControlNet is not used for fine-tuning. wo/fusion denotes Harmonic Data Fusion Training is not used for disentangled animation. Method FG-DTFVD BG-DTFVD DTFVD Face-Cos w/RefNet w/IP-A w/lmk wo/face wo/fusion X-Dyna 2.137 3.738 0.914 0.912 1.301 0.900 2.694 4.702 1.125 1.098 1.467 1. 2.823 4.851 1.589 1.550 1.652 1.518 0.466 0.292 0.406 0.442 0.495 0.497 quality of human foreground (FG-Dyn) (3) appearance and identity preservation ability (ID). We present the result of the average vote in Table 3. We observe that the user prefers X-Dyna more than ReferenceNet and SVD based works [4, 54], especially in terms of dynamic texture generation. More details can be found in the supplementary material. 4.3. Ablation Analysis In this section, comprehensive ablation analysis of XDyna is presented. We evaluate the effectiveness of our face expressions and ID enhancement modules on TikTok [17] test set in Tab. 4. To confirm the effectiveness of our Figure 5. Qualitative Comparison on Human in Dynamic Scene. While existing SOTA methods struggle to generate consistent and realistic scene dynamics involving humans, our method successfully produces dynamic human-scene interactions while preserving the structure of the reference image. Figure 6. Qualitative Comparison on Poses and Face Expressions Control. We show each method on test cases using the same reference image and pose skeleton. For improved visualization, zoomed-in view of the face area is also provided. Our method produces results that most closely match the ground truth and best preserve face identity. Dynamics-Adapter and Harmonic Data Fusion Training, we quantitatively evaluate the DTFVD on our self-collected data and present the result in Tab. 4. 4.4. Limitations and Future Works Despite its effectiveness in the dynamic expressiveness generation of human video animation, our X-Dyna has certain limitations, particularly in scenarios where the target pose significantly deviates from the reference human. For instance, during extreme zooming in or out, the appearance and identity may not be perfectly preserved. Additionally, our method struggles to generate perfect hand poses. We believe that these challenges can be addressed by collecting more high-quality data and employing advanced hand pose representations as input. In the future, we will explore applying DynamicsAdapter to more powerful base image and video diffusion models, such as SVD [1], SDXL [32] and Stable Diffusion 3 [6], to achieve better performance. Moreover, we will investigate adding the camera trajectory or drag control proposed in [11, 23, 45, 51] to our model so that we have more user-friendly condition. 5. Conclusion In this work, we propose X-Dyna, photorealistic human video animation pipeline with the ability of consistent motion control and vivid dynamics details generation. We propose an efficient Dynamics-Adapter module to preserve the human appearance reference while maintaining the foundation models ability to generate high-quality dynamics. 8 To boost the dynamics modeling capability further, we propose Harmonic Data Fusion Training strategy, mixing the training data from real-human and natural scene videos. Moreover, we incorporate two plug-in modules, an S-Face ControlNet for facial expressions editing and Face-IDAdapter for face local identity preservation enhancement. Finally, all proposed modules can be treated as extensions to SD and used for customized pre-trained weights of SDUNet. Extensive evaluation of various models also validates the effectiveness and generalizability of our model. Ethics Statement. Our work aims to improve human image animation from technical perspective and is not intended for malicious use like fake videos. Therefore, synthesized videos should clearly indicate their artificial nature."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3, 8 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [3] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023. 2 [4] Di Chang, Yichun Shi, Quankai Gao, Hongyi Xu, Jessica Fu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. In Forty-first International Conference on Machine Learning, 2023. 1, 2, 3, 4, 6, 7 [5] Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos Derpanis, and Bjorn Ommer. Stochastic image-to-video synthesis using cinns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 37423753, 2021. 6 [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 8 [7] Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Abrevaya, Michael Black, and Xuaner Zhang. Explorative inbetweening of time and space. arXiv preprint arXiv:2403.14611, 2024. 3 [8] Songwei Ge, Aniruddha Mahapatra, Gaurav Parmar, JunYan Zhu, and Jia-Bin Huang. On the content bias in frechet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 6 [9] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 4, 5, 6 [10] Isma Hadji and Richard Wildes. new large scale dynamic texture dataset with application to convnet understanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 320335, 2018. 6 [11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 8 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020. [14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [15] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe. Koniq-10k: An ecologically valid database for deep learning IEEE Transactions on of blind image quality assessment. Image Processing, 29:40414056, 2020. 6 [16] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 1, 2, 3, 4, 6, 7 [17] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1275312762, 2021. 3, 6, 7 [18] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. arXiv preprint arXiv:2304.06025, 2023. 3 [19] Tero Karras. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 3 [20] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In NeurIPS, 2022. [21] Minchul Kim, Anil Jain, and Xiaoming Liu. Adaface: In ProceedQuality adaptive margin for face recognition. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 6, 7 [22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 7 [23] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Col9 laborative video diffusion: Consistent multi-video generation with camera control. arXiv preprint arXiv:2405.17414, 2024. 8 [24] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander In Proceedings of Holynski. Generative image dynamics. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2414224153, 2024. 3, [25] Jinlin Liu, Kai Yu, Mengyang Feng, Xiefang Guo, and Miaomiao Cui. Disentangling foreground and background motion for enhanced realism in human video generation. arXiv preprint arXiv:2405.16393, 2024. 3 [26] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, ChuoLing Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: framework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019. 1 [27] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: PoseIn guided text-to-video generation using pose-free videos. Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 3 [28] Midjourney. midjourney. https://www.midjourney.com, 2024. 3, 6 [29] MooreThreads. Moorethreads/moore-animateanyone. https://github.com/MooreThreads/Moore-AnimateAnyone., 2024. 7 [30] Pexels. pexels. https://www.pexels.com/, 2024. 3, 6 [31] Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan Barron, Amit Bermano, Eric Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on diffusion models for visual computing. In Computer Graphics Forum, page e15063. Wiley Online Library, 2024. 3 [32] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 8 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3 [34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 3, 4, [35] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In Proceedings of the IEEE international conference on computer vision, pages 28302839, 2017. 3 [36] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 4 and Stefano Ermon. arXiv preprint [37] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 4 [38] Zhengyan Tong, Chao Li, Zhaokang Chen, Bin Wu, and Wenjiang Zhou. Musepose: pose-driven image-to-video framework for virtual human generation. arxiv, 2024. 1, 2, 3, 4 [39] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 15261535, 2018. 3 [40] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [41] Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, and Yanwei Fu. Vividpose: Advancing stable video diffusion for realistic human image animation. arXiv preprint arXiv:2405.18156, 2024. 1, 3 [42] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for referring arXiv preprint human dance generation in real world. arXiv:2307.00040, 2023. 3, 4, 6, 7 [43] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. 5 [44] Huawei Wei, Zejun Yang, and Zhisheng Wang. Aniportrait: Audio-driven synthesis of photorealistic portrait animation. arXiv preprint arXiv:2403.17694, 2024. [45] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anyIn European Conference thing using entity representation. on Computer Vision, pages 331348. Springer, 2025. 8 [46] Zhiqiang Xia, Zhaokang Chen, Bin Wu, Chao Li, Kwok-Wai Hung, Chao Zhan, Yingjie He, and Wenjiang Zhou. Musev: Infinite-length and high fidelity virtual human video generation with visual conditioned parallel denoising. arxiv, 2024. 3 [47] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 5 [48] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 6 [49] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 1, 2, 3, 4, 6, 7 [50] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 4, 2, 3 [51] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. [52] Lyumin Zhang. [major update] reference-only control mikubill/sd-webui-controlnet discussion #1236. 2 [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 2, 3, 4, 5 [54] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation arXiv preprint with confidence-aware pose guidance. arXiv:2406.19680, 2024. 1, 2, 3, 6, 7 [55] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. arXiv preprint arXiv:2403.14781, 2024. 1, 3, 11 X-Dyna: Expressive Dynamic Human Image Animation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Video Results We provide additional video results generated from X-Dyna in our project page. Comparison of Different Appearance Reference Module Designs: To demonstrate the effectiveness of our proposed Dynamics-Adapter, we provide visual comparisons with IP-Adapter and ReferenceNet. Please refer to the Different Architecture Designs section for details. Comparison to Previous Works: To evaluate the performance of X-Dyna in generating dynamic textures for human image animation, we present visual comparisons with previous state-of-the-art methods, including the ReferenceNet-based approach from [4] and the SVD-based method from [54]. Details can be found in the Comparison to Previous Works section. Ablation Study: To highlight the contribution of Harmonic Data Fusion Training to our pipeline, we present visualized ablation study. Please refer to the Effectiveness of Mix data training section of the project page. 7. Quantitative Evaluation of Cross-Driving"
        },
        {
            "title": "Reenactment",
            "content": "In this section, we present quantitative evaluations for crossdriving video generation. We generated 200 videos for XDyna and each baseline method using various in-the-wild driving motions and reference images. The overall quality of cross-driving generation is assessed using DTFVD and FID metrics, comparing the distribution of the generated videos with the training videos. To evaluate the control accuracy of facial expressions, we crop the face area of both generated and driving videos and calculate their mean difference of face landmarks by MediaPipe [26]. The numerical results are summarized in Tab. 5, where XDyna demonstrates superior face expression control accuracy (Face-Exp) and dynamics (DTFVD), and comparable perceptual quality (FID). 8. Details of User Study In this section, we provide comprehensive user study for qualitative comparison between X-Dyna and previous works [4, 16, 49, 54]. We generate 50 different human animation results from all baseline models and X-Dyna, where the results are anonymized and shuffled. On the online platform Prolific , we ask 100 users to rate these methods from 0(worst) - 5(best). Criteria for Judgment: Since our paper focuses on the dynamics of texture generation and motion control with hu1 Table 5. Quantitative comparisons of X-Dyna with recent state-of-the-art (SOTA) methods on cross-driving human animation. downward-pointing arrow indicates that lower values are better. DTFVD and FID are used to evaluate the overall quality of generated videos. Face-Exp denotes the absolute error of facial expressions between generated videos and driving videos. Method DTFVD FID Face-Exp MagicAnimate [49] Animate-Anyone [16] MagicPose [4] MimicMotion [54] X-Dyna 6.708 6.820 7.062 6.823 5. 250.75 253.29 244.25 258.91 246.16 0.134 0.123 0.121 0.109 0.105 man reference, the criteria for evaluation are (1) dynamics quality of background nature (BG-Dyn), (2) dynamics quality of human foreground (FG-Dyn), (3) appearance and identity preservation ability (ID). Results and Statistical Analysis: The result is presented in Tab. 3 of the main paper. In addition, we perform one-way analysis of variance (ANOVA) test on the ratings. ANOVA tests whether the means of multiple groups of data (methods in this case) are significantly different. For each metric, we compare the ratings across all five methods. Specifically, F-statistic measures the ratio of variance between group averaged values to the variance within groups. higher F-statistic indicates greater variability between group-averaged values relative to within-group variability. P-value tests the null hypothesis that all group means are equal. small p-value (typically 0.05) indicates significant differences between groups. As reported in Tab. 6, all metrics (FG-Dyn, BG-Dyn, ID, Overall) have p-values 0.05, indicating statistically significant differences between methods. The F-statistic for each metric shows the relative strength of these differences. X-Dyna consistently achieves the highest averaged ratings across all metrics (as seen in Tab. 3 of the main paper), and the differences are statistically significant. Metric FG-Dyn BG-Dyn ID Overall F-statistic 7.495 5.327 4.685 5. p-value 0.000007 0.000331 0.001016 0.000199 Table 6. ANOVA Test Results for Ratings from the User Study. 9. More Details on Prior Appearance Reference Control Designs ReferenceNet was initially introduced by AnimateAnyone [16]. It adopts the same architecture as the Appearance Encoder in MagicAnimate [49] and the Appearance Control Model in MagicPose [4]. Building upon prior advancements in dense reference image conditioning, such as the manipulation of self-attention layers in the UNet demonstrated by MasaCtrl [3] and Reference-only ControlNet [52], ReferenceNet enhances identity and background preservation, significantly improving single-frame fidelity. The naive self-attention calculation in the transformer blocks of the diffusion UNet can be represented as: Ai = softmax( QiK )Vi, (4) However, ReferenceNet introduces trainable duplicate of the base UNet, which computes conditional features from the reference image IR for each frame Ii. Unlike ControlNet, which integrates conditions additively in residual manner, ReferenceNet injects the features derived from IR directly into the spatial self-attention layers of the UNet blocks. This is achieved by concatenating the reference features with the original UNets self-attention hidden states. The process can be expressed as: Ai = softmax( QiK )V , (5) Qi = Qizi, i = Ki[zi, zr], = Vi [zi, zr], (6) where [] denotes concatenation operation and zi, zr denotes the self-attention hidden states from Ii, IR. This selfattention mechanism strictly queries and preserves the information from the reference image in the denoising process, including human identity and background. IP-Adapter [50] is composed of two key components: an image encoder that extracts features from the image prompt and adapted modules with decoupled cross-attention to integrate these features into the LDM UNet. pretrained CLIP image encoder is employed to extract features from the reference image IR. To effectively decompose the extracted global image embedding, lightweight trainable projection networkcomprising linear layer and Layer Normalization is utilized. This network projects the global image embedding into sequence of features, ensuring that the dimensionality of the projected image features matches the dimensionality of the text features used in the UNet. The integration of image features into the UNet is performed through adapted modules with decoupled crossIn the original LDM, text features from the attention. CLIP text encoder are incorporated into the UNet via crossattention layers. In this setup, given the query features zr derived from IR, the hidden states of the UNet for each frame Ii, and the text features zt, the output of the crossattention mechanism is defined as: = softmax( iK )V , i = zi, = i zt, = (7) zt, (8) Then, another cross-attention layer for each original layer in the UNet is added to inject image features. Given the image features zr, the output of this cross-attention is computed as follows: = softmax( iK d )V R, = zi, = zr, = (9) zr, (10) The same query Qi is shared between the image crossattention and the text cross-attention mechanisms. As result, only two additional trainable parameters, KR and , are introduced as linear layers for each crossattention module. The output of the image cross-attention is then combined with the output of the text cross-attention through simple addition operation. Accordingly, the final formulation of the decoupled cross-attention is denoted as: Out = + λA , (11) where λ is an adjustable parameter. When λ = 0, the model is the same as frozen pre-trained LDM. Stable Video Diffusion (SVD) [1] is diffusion-based video generation model that extends the latent diffusion framework originally designed for 2D image synthesis to produce high-resolution, temporally consistent videos from text and image inputs. SVD UNet introduces two types of temporal layers: 3D convolution layers and temporal attention layers, and temporal layers are also incorporated into the VAE decoder. For training, the DDPM [12] noise scheduler used in Stable Diffusion [34] is replaced by the EDM [20] scheduler, alongside EDMs sampling method. Unlike traditional DDPM models that rely on discrete timesteps for denoising, EDM uses continuous noise scale σt By incorporating σt as input to the model, EDM enables more flexible and effective sampling, utilizing continuous noise strengths instead of discrete timesteps during the denoising process. This end-to-end training paradigm enhances temporal consistency in video generation. However, SVD faces challenges when dealing with cross-driving 2 cases. The reference image is concatenated with the noisy latent and directly input to the UNet, leading the model to deform the reference image into the first frame of the video rather than encoding the reference image and learning its semantic information implicitly, as achieved by ReferenceNet [16], IP-Adapter [50], and Dynamics-Adapter. While fine-tuning the UNet, as in MimicMotion [54], is potential solution, it struggles to generalize to out-ofdomain identities beyond the training data, as shown in Fig. 5 of our main paper and the supplementary videos."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Stanford University",
        "University of California Los Angeles",
        "University of California San Diego",
        "University of Southern California"
    ]
}