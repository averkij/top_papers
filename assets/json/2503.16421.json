{
    "paper_title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance",
    "authors": [
        "Quanhao Li",
        "Zhen Xing",
        "Rui Wang",
        "Hui Zhang",
        "Qi Dai",
        "Zuxuan Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have led to remarkable improvements in visual quality and temporal coherence. Upon this, trajectory-controllable video generation has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised visual quality. Furthermore, these methods only support trajectory control in a single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, a novel image-to-video generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintaining object consistency and visual quality. Furthermore, we present MagicData, a large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, a comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https://quanhaol.github.io/magicmotion-site."
        },
        {
            "title": "Start",
            "content": "MagicMotion: Controllable Video Generation with Dense-to-Sparse"
        },
        {
            "title": "Trajectory Guidance",
            "content": "Quanhao Li1 Zhen Xing1 Rui Wang1 Qi Dai2 Zuxuan Wu1 1 Fudan University Hui Zhang1 2 Microsoft Research Asia 5 2 0 2 0 2 ] . [ 1 1 2 4 6 1 . 3 0 5 2 : r https://quanhaol.github.io/magicmotion-site/ Figure 1. Example videos generated by MagicMotion. MagicMotion consists of three stages, each supporting different level of control from dense to sparse: mask, box, and sparse box. Given an input image and any form of trajectory, MagicMotion can generate high-quality videos, animating objects in the image to move along the user-specified path."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in video generation have led to remarkable improvements in visual quality and temporal cohertrajectory-controllable video generaence. Upon this, tion has emerged to enable precise object motion control through explicitly defined spatial paths. However, existing methods struggle with complex object movements and multi-object motion control, resulting in imprecise trajectory adherence, poor object consistency, and compromised equal contributions. visual quality. Furthermore, these methods only support trajectory control in single format, limiting their applicability in diverse scenarios. Additionally, there is no publicly available dataset or benchmark specifically tailored for trajectory-controllable video generation, hindering robust training and systematic evaluation. To address these challenges, we introduce MagicMotion, novel image-tovideo generation framework that enables trajectory control through three levels of conditions from dense to sparse: masks, bounding boxes, and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly animates objects along defined trajectories while maintain1 ing object consistency and visual quality. Furthermore, we present MagicData, large-scale trajectory-controlled video dataset, along with an automated pipeline for annotation and filtering. We also introduce MagicBench, comprehensive benchmark that assesses both video quality and trajectory control accuracy across different numbers of objects. Extensive experiments demonstrate that MagicMotion outperforms previous methods across various metrics. Our project page are publicly available at https: //quanhaol.github.io/magicmotionsite/. 1. Introduction With the rapid development of diffusion models, video generation has witnessed significant progress in recent years. Earlier video generation approaches, such as AnimateDiff [16] and SVD [3], primarily rely on the UNet [48] structure, which results in videos with limited length and quality. Sora [4] has demonstrated the powerful capabilities of the DiT [38] architecture in text-to-video (T2V) generation. Following this breakthrough, subsequent models leveraging the DiT architecture [11, 73, 80] have achieved higher quality outputs and extended video duration. While DiT-based models [11, 73, 80] excel at producing high-quality and longer videos, many text-to-video approaches [16, 66, 68] lack precise control over attributes like object movement and camera motion [16, 65, 73]. Finegrained trajectory-controllable video generation emerges as solution, which is especially critical for generating controllable videos in real-world scenarios. Previous trajectory-controllable video generation methods can be categorized based on the type of control signals they use. These include points-based control [14, 17, 55, 59], optical flow-based control [5, 31, 50, 75, 78], bounding boxes-based control [23, 36, 40, 56, 60, 62, 72], masks-based control [74], and 3D trajectories-based control [13, 15, 57]. However, these methods exhibit several limitations. First, the trajectory control conditions used by these methods are unitary. Each method only support single type of control signal. However, sparse trajectories (e.g., points and optical flow) result in imprecise control over object shape and size, while dense trajectories (e.g., masks and 3D trajectories) are challenging for users to provide. Second, publicly available large-scale dataset for trajectory-controllable video generation is lacking. Existing VOS (Video Object Segmentation) datasets suffer from short video lengths [34, 35, 71], small scales [9, 10, 21, 39], or few foreground objects [12]. Third, unified benchmark for evaluating different methods is absent. Besides, previous work focuses solely on video quality and trajectory accuracy while neglecting the impact of the number of moving objects. We argue that controlling fewer or more objects presents different challenges, making it essential to include this factor in the evaluation metrics. To address these issues, we propose MagicMotion, controllable video generation framework with dense-tosparse trajectory guidance. To inject trajectory conditions into the generation process, we utilize an architecture similar to ControlNet [77] called Trajectory ControlNet to encode the trajectory information, which is later added to the original DiT model through zero-initialized convolution layer. MagicMotion supports three types of trajectory conditions: masks, boxes, and sparse boxes using progressive training strategy. Experiments show that the model can leverage the knowledge learned in the previous stage to achieve better performance than training from scratch. Additionally, we propose novel latent segment loss that helps the video generation model better understand the finegrained shape of objects with minimal computation. We also construct MagicData, high-quality public dataset comprising 51K video samples, each annotated with <video, text, trajectory> triplet. We design data pipeline that uses large language model [53] to extract the main moving objects in the video, and Segment Anything Model (SAM2) [45, 46] to annotate the segmentation masks and bounding boxes of these objects. Furthermore, we introduce MagicBench, large-scale comprehensive trajectory-controllable video generation benchmark. It categorizes all videos into 6 classes based on the number of foreground objects and evaluates models separately for each category in terms of both video generation quality and trajectory control accuracy. In conclusion, the main contributions of our work are summarized as follows: We present MagicMotion, trajectory-controllable image-to-video generation model that supports three types of control signals: masks, boxes, and sparse boxes. We introduce data curation and filtering mechanism, the first public dataset for and construct MagicData, trajectory-controlled video generation. We propose MagicBench, comprehensive benchmark for evaluating trajectory-controllable video generation models for both video quality and trajectory control accuracy across different numbers of controlled objects. 2. Related Works Video Diffusion Models Diffusion models [19, 32, 51, 52] have made great progress in image generation [8, 37, 41, 42, 47, 49, 76], which has led to the rapid development of video generation [3, 16, 20, 22, 26, 54, 63, 67, 68, 73]. VDM [20] is the first to apply diffusion models to video generation. Early works like AnimateDiff [16] and SimDA [66] attempt to insert temporal layers into pretrained T2I model for video generation. Subsequently, VideoCrafter [6] and SVD [3] use large-scale and high-quality data for training, acheiving better performance. However, these methods have trouble 2 Figure 2. Overview of MagicMotion Architecture (text prompt and encoder are omitted for simplicity). MagicMotion employs pretrained 3D VAE to encode the input trajectory, first-frame image, and training video into latent space. It has two separate branches: the video branch processes video and image tokens, and the trajectory branch uses Trajectory ControlNet to fuse trajectory and image tokens, which is later integrated to the video branch through zero-initialized convolution layer. Besides, diffusion features from DiT blocks are concatenated and processed by trainable segment head to predict latent segmentation masks, which contribute to our latent segment loss. on generating long videos with high quality, mainly due to the inherent limitations of the UNet architecture. The emergence of Sora [4] is significant success, demonstrating the potential of DiT [38] models to generate high-quality videos with tens of seconds. Recent video generation methods [11, 73, 80] are mainly based on DiT architecture, and have achieved great success in the open-source community. However, these methods rely solely on text or image guidance for video generation, lacking precise control over object or camera trajectory, which is crucial for high-quality video generation. Trajectory Controllable Video Generation Trajectory Controllable Video Generation has recently garnered significant attention for its ability to precisely control object and camera trajectories during video synthesis. Previous methods [31, 50, 75, 78] integrate optical flow maps into video generation through trajectory encoder. Recent works [17, 59] suggest using point maps as form of guidance. MotionCtrl [59] processes point maps with Gaussian filter and employs trainable encoders to encode object trajectories. Trackgo [17] represents objects using few key points and injects this information via an encoder and custom-designed adapter structure. Other works [23, 36, 40, 56] employ bounding box to control object trajectories. Boximator [56] employs trainable selfattention layer to fuse box and visual tokens inspired by GLIGEN [30]. Some training-free methods [23, 36, 40] purpose to modify attention layers or initial noised video latents to inject box signals. Additionally, certain methods [13, 15, 55, 57] explore the potential of 3D trajectories to achieve more sophisticated motion control. LeViTor [55] employs keypoint trajectory maps enriched with depth information, while others [13, 15, 57] construct custom 3D trajectories to represent object movements. However, sparse trajectories lead to imprecise control on objects shape and size, while dense trajectories are difficult In contrast, MagicMotion can confor users to provide. trol both dense and sparse trajectories, providing users with more flexible control over video generation. 3. Method 3.1. Overview Our work mainly focuses on trajectory-controllable video generation. Given an input image RHW 3, and several trajectory maps RT HW 3, the model can generate video RT HW 3 in line with the provided trajectories, where denotes the length of generated video. In the following sections, we first provide detailed explanation of our model architecture in Section 3.2. Next, we outline our progressive training procedure in Section 3.3. In Section 3.4, we introduce the Latent Segmentation Loss and demonstrate how it enhances the model capabilities on finegrained object shape. We then describe our dataset curation and filtering pipeline in Section 3.5. Finally, we present an in-depth overview of MagicBench in Section 3.6. 3.2. Model Architecture Base I2V generation model We utilize CogVideoX-5BI2V [73] as our base image to video model. CogVideoX is built upon DiT (Diffusion Transformer) architecture, incorporating 3D-Full Attention to generate high-quality videos. As shown in Fig. 2, the model takes an input image RHW 3 and corresponding video RT HW 3 and encodes them into latent representations 8 Zimage, Zvideo 8 16 using pretrained 3D VAE [27]. Later, Zimage is zero-padded to frames and concatenated with noised version of Zvideo and then fed into the Diffusion Transformer, where series of Transformer blocks iteratively denoise it over predefined number of steps. Finally, the denoised latent is decoded by 3D VAE decoder to get the output video Vout RT HW 3. 4 Trajectory ControlNet To ensure that the generated video follows the motion patterns given by the input trajectory maps RT HW 3, we adopt design similar to ControlNet [77] to inject trajectory condition. As shown in Fig. 2, we employ the 3D VAE encoder to encode the trajectory maps into Ztrajectory 8 16, which is then concatenated with the encoded video Zvideo and serves as input to Trajectory ControlNet. Specifically, Trajectory ControlNet is constructed with trainable copy of all pre-trained DiT blocks to encode the user-provided trajectory information. The output of each Trajectory ControlNet block is then processed through zero-initialized convolution layer and added to the corresponding DiT block in the base model to provide trajectory guidance. 8 4 3.3. Dense-to-Sparse Training Procedure Dense trajectory conditions, such as segmentation masks, offer more precise control than sparse conditions like bounding boxes but are less user-friendly. To address this, MagicMotion employs progressive training procedure, where each stage initializes its model with the weights from the previous stage. This enables three types of trajectory control ranging from dense to sparse. We found that this progressive training strategy helps the model achieve better performance compared to training from scratch with sparse conditions. Specifically, we adopt the following trajectory conditions across stages: stage1 uses segmentation masks, stage2 uses bounding boxes, and stage3 uses sparse bounding boxes, where fewer than 10 frames have box annotations. Additionally, we always set the first frame of the trajectory condition as segmentation mask to specify the foreground objects that should be moving. Our model uses velocity prediction following [73]. Let x0 be the initial video latents, ϵ be the gaussian noise, xt = 1 αt ϵ be the noised video latents, and vθ αt x0 + be the model output. The diffusion loss can be written as: Ldif usion = Et,ϵN (0,I),x0 (cid:104) (cid:13) (cid:13) (cid:13)x0 (cid:16) αt xt 1 αt vθ (cid:105) (cid:17)(cid:13) 2 (cid:13) (cid:13) 2 (1) 4 3.4. Latent Segmentation Loss Bounding box-based trajectory is able to control an objects position and size but lacks fine-grained shape perception. To address this, we propose Latent Segmentation Loss, which introduces segmentation mask information during model training and enhances the models ability to perceive fine-grained object shapes. Previous works [2, 61, 70, 79] have leveraged diffusion generation models for perception tasks, demonstrating that the features extracted by diffusion models contain rich semantic information. However, these models generally operate in the pixel space, which leads to extensive computational time and substantial GPU memory. To incorporate dense trajectory information while keeping computational costs within reasonable range, we propose utilizing lightweight segmentation head to predict segmentation masks directly in the latent space, eliminating the need for decoding operations. 4 4 8 16 Specifically, our segmentation head takes list of diffusion features Zf eature 16 3072 from each DiT block, and outputs latent segmentation mask Zsegment 8 16. We use light-weight architecture inspired by Panoptic FPN [28]. Each diffusion feature first passes through convolution layer to extract visual features 16 64. The resulting features are then concatenated and processed by another convolution layer followed by an upsampling layer to generate the final latent segmentation mask. 16 4 We compute the latent segment loss as the Euclidean distance between Zsegment and the ground truth mask trajectory latents Zmask, which can be written as, Lseg = Et,ϵN (0,I),z0[Zsegment Zmask2 2] (2) In practice, Lseg is only used in stage2 and stage3, providing the model with dense conditions information when trained with sparse conditions. In detail, we set the weight of Lseg as 0.5, and the original diffusion loss as 1. In total, our final loss function can be written as, = Ldif usion + λ Lseg (3) where λ is set to 0 in stage1, and 0.5 in stage2 and stage3. 3.5. Data Pipeline Trajectory controllable video generation requires video dataset with trajectory annotations. However, existing large-scale video datasets [1, 7, 25] only provide text annotations and lack trajectory data. Moreover, almost all previous works [17, 31, 59, 75, 78] use privately curated datasets, which are not publicly available. We present comprehensive and general data pipeline for generating high-quality video data with both dense (mask) and sparse (bounding box) annotations. As shown Figure 3. Overview of the Dataset Pipeline. The Curation Pipeline is used to construct trajectory annotations, while the Filtering Pipeline filters out unsuitable videos for training. in Fig. 3, the pipeline consists of two main stages: the Curation Pipeline and the Filtering Pipeline. The Curation Pipeline is responsible for constructing trajectory information from video-text dataset, while the Filtering Pipeline ensures that unsuitable videos are removed before training. Curation Pipeline. We begin our dataset curation process with Pexels [24], large-scale video-text dataset containing 396K video clips with text annotations. It encompasses videos featuring diverse subjects, various scenes, and wide range of movements. We utilize Llama3.1 [53] to extract the foreground moving objects from the textual annotations of each video. As shown in Fig. 3, we input the videos caption into the language model and prompt it to identify the main foreground objects mentioned in the sentence. If the model determines that the sentence does not contain any foreground objects, it simply returns empty and such videos are filtered out. Next, we utilize GroundedSAM2 [44, 46], grounded segmentation model that takes video along with its main objects as input and generates segmentation masks for each primary object. Each object is consistently annotated with unique color. Finally, bounding boxes are extracted from each segmentation mask using the coordinates of the top-left and bottom-right corners to draw the corresponding boxes. The color of the bounding box for each object remains consistent with its segmentation mask. Filtering Pipeline. Many videos contain only static scenes, which are not beneficial for training trajectorycontrolled video generation models. To address this, we use optical flow scores to filter out videos with little motion and dynamics. Specifically, we utilize UniMatch [69] to extract optical flow maps between frames and compute the mean absolute value of these flow maps as the optical flow score, representing the videos motion intensity. However, videos with background movement but static foreground can still have high motion scores. To address this, we further use UniMatch to extract optical flow scores for foreground objects based on segmentation masks and bounding boxes. Videos with low foreground optical flow scores are filtered out, ensuring MagicData includes only videos with moving foreground objects. The trajectory annotations generated by the curation pipeline require further refinement. As shown in Fig. 3, some videos contain too many foreground objects annotations, or their sizes may be too large or too small. To address this, we regulate these factors within reasonable range and filter out videos that fall outside the acceptable range. Specifically, based on extensive manual evaluation, we"
        },
        {
            "title": "DAVIS",
            "content": "FID() FVD() IoU%() IoU%() FID() FVD() IoU%() IoU%() Motion-I2V [50] ImageConductor [31] DragAnything [64] LeViTor [55] DragNUWA [75] SG-I2V [36] Tora [78] Ours (Stage1) Ours (Stage2) 156.59 234.35 137.57 189.63 140.99 117.04 144.49 87.13 93.27 354.10 331.76 253.40 194.53 185.52 168.82 245.23 112.69 107.21 56.19 51.76 66.30 39.96 66.88 68.78 58.95 91.57 76. 60.66 52.90 70.85 46.36 69.21 74.39 64.03 87.75 81.45 184.35 224.32 180.77 225.42 182.70 177.39 138.55 117.12 124.92 1558.32 1155.08 1166.22 922.68 1079.89 1170.60 766.76 579.94 760. 32.00 34.39 40.13 25.24 41.22 37.36 37.98 81.33 53.94 42.61 43.41 53.60 31.42 52.61 50.96 50.90 84.97 72.84 Table 1. Quantitative Comparison results on MagicBench and DAVIS. IoU and IoU refer to Mask IoU and Box IoU, respectively. Figure 4. Comparison results of different object number on MagicBench. To present the results more clearly, we have negated the FVD and FID scores. empirically set the optical flow score threshold to 2.0, limit the number of foreground object annotations from 1 to 3, and constrain the annotated area ratio to range of 0.008 to 0.83. The whole data curation and filtering pipeline yields us with MagicData, high quality dataset for trajectory controllable video generation containing 51K videos with both dense and sparse trajectory annotations. 3.6. MagicBench Previous works on trajectory-controlled video generation [17, 31, 36, 50, 58, 64, 78] have primarily been validated on DAVIS (which has relatively small dataset size), VIPSeg (where the annotated frames per video are insufficient), or privately constructed test sets. Thus there is an urgent need for large-scale, publicly available benchmark to enable fair comparisons across different models in this field. To merge this gap, we use the data pipeline mentioned in Sec. 3.5 to construct MagicBench, large-scale open benchmark consisting of 600 videos with corresponding trajectory annotations. MagicBench evaluates not only video quality and trajectory accuracy but also takes the number of controlled objects as key evaluation factor. Specifically, it is categorized into 6 groups based on the number of controlled objects, ranging from 1 to 5 objects and more than 5 objects, with each category containing 100 high-quality videos. Metrics. For evaluation metrics, we adopt FVD [43] to assess video quality and FID [18] to evaluate image quality, following [17, 55, 59, 64]. To quantify motion control accuracy, we use Mask IoU and Box IoU, which measure the accuracy of masks and bounding boxes, respectively. Specifically, given generated video Vgen, we use the groundtruth masks of the first frame Mgt(0) as input to SAM2 [44] to predict the masks Mgen of the foreground objects in Vgen. For each foreground object, we compute the Intersection over Union (IoU) between Mgen(i) and groundtruth masks Mgt(i) in each frame, then average these values to obtain Mask IoU. Similarly, we compute the IoU between the predicted and groundtruth bounding boxes for each foreground object in each frame and take the average as Box IoU. 4. Experiment 4.1. Experiment Settings Implementation employ CogVideoX 5B [73] as our base image-to-video model, which is trained to generate 49-frame video at resolution of 480 720. details. We 6 Figure 5. MagicMotion successfully controls the main objects moving along the provided trajectory, while all other methods exhibit significant defects marked with the orange box. Each stage of MagicMotion was trained on MagicData for one epoch. The training process consists of three stages. stage1 trains Trajectory ControlNet from scratch. In stage2, Trajectory ControlNet is further refined using the weights from stage1, while Segment Head is trained from scratch. Finally, in stage3, both Trajectory ControlNet and Segment Head continue training initialized with the weights from stage2. All training experiments were conducted on 4 NVIDIA A100-80G GPUs. We employed AdamW [33] as the optimizer, training with learning rate of 1e 5 and batch size of 1 on each GPU. During inference, we set steps to 50, the guidance scale to 6, and the weight of Trajectory ControlNet to 1.0 by default. Datasets. During training, we use MagicData as our training set. MagicData is annotated with dense to sparse trajectory information using the data pipeline described in Section 3.5. It comprises total of 51,000 <video, text, trajectory> triplets. During the training process, each video was resized to 480 720 and sampled out 49 frames per clip. For evaluation, we evaluate all methods on both MagicBench and DAVIS [39], using the comparison metrics illustrated in Sec 3.6. 4.2. Comparison with Other Approaches For thorough and fair comparisons, we compare our methods against 7 public trajectory controllable I2V methods [31, 36, 50, 55, 64, 75, 78]. Quantitative comparison and qualitative comparison results are shown below. Quantitative comparison. To compare MagicMotion with previous works, we use the first 49 frames of each video from DAVIS and MagicBench as the ground truth video. Since some methods [31, 36, 50, 55, 64, 75] do not support video generation up to 49 frames in length, we uniformly sample frames from these 49 frames for evaluation, where represents the video length that each method support. We leverage the mask and box annotations from these selected frames as trajectory inputs for mask or box-based methods. The center point of each frames mask is extracted as input for point or flow-based meth7 ods [31, 50, 55, 64, 75, 78]. As shown in Table 1, our method outperforms all previous approaches across all metrics both on MagicBench and DAVIS, demonstrating its ability to produce higher-quality videos and more precise trajectory control. Additionally, we evaluate each methods performance on MagicBench based on the number of controlled objects. As shown in Fig. 4, our method achieves the best results across all object number categories, further demonstrating the superiority of our approach. Qualitative comparison. Qualitative comparison results are shown in Fig. 5, with the input image, prompt and trajectory provided. As shown in Fig. 5, Tora [78] accurately controls the motion trajectory but struggles to maintain the shape of the objects. While DragAnything [64], ImageConductor [31], and MotionI2V [50] have difficulty preserving the consistency of the original subject, resulting in substantial deformation in subsequent frames. Meanwhile, DragNUWA [75], LeviTor [55], and SG-I2V [36] frequently produce artifacts and inconsistencies in fine details. In contrast, MagicMotion allows moving objects to follow the specified trajectory smoothly while preserving high video quality. 4.3. Ablation Studies In this section, we present ablation studies to validate the effectiveness of our MagicData dataset. Additionally, we demonstrate how our progressive training procedure and latent segment loss enhance the models understanding of precise object shapes under sparse control conditions, thereby improving trajectory control accuracy. Ablations on Dataset. To verify the effectiveness of MagicData, we constructed an ablation dataset by combining two public VOS datasets, MeViS [9] and MOSE [10]. For fair comparison, we trained MagicMotion stage2 for one epoch using either MagicData or the ablation dataset as the training set, both initialized with the same stage1 weights. We then evaluated the models on both MagicBench and DAVIS. As shown in Table 2, the model trained on MagicData outperforms the one trained on the ablation dataset across all metrics. Qualitative comparisons are shown in Fig. 6. In this case, we aim to gradually move the boy in the lower right corner to the center of the image. However, not using MagicData results to an unexpected child appears next to the boy. In contrast, the model trained with MagicData performs well, moving the boy along the specified trajectory while maintaining video quality. Ablations on Progressive Training Procedure. Progressive Training Procedure allows the model to leverage the weights learned in the previous stage, incorporating dense trajectory control information when trained with sparse trajectory conditions. To validate the effectiveness of this apMethod MagicBench/DAVIS FID() FVD() IoU%() IoU%() w/o MagicData 99.55/128. 125.19/768.27 73.39/50.95 78.66/70.05 Ours 93.27/124.91 107.21/760. 76.61/53.94 81.45/72.84 Table 2. Ablation Study on MagicData. The model trained with MagicData outperforms the one trained without it across all metrics. Figure 6. Ablation Study on MagicData. Not using MagicData causes the model to generate an unexpected child. proach, we train the model from scratch for one epoch using bounding boxes as trajectory conditions. We then compare its performance with MagicMotion stage2. As shown in Table 3, excluding Progressive Training Procedure weakens the models ability to perceive object shapes, ultimately reducing the accuracy of trajectory control. Qualitative comparisons in Fig. 7 further illustrate these effects, where the model trained without Progressive Training Procedure turns the womans head entirely into hair. Method MagicBench DAVIS IoU%() IoU%() IoU%() IoU%() w/o LSL 74.65 74.61 w/o PT Ours 76.61 78.02 79.49 81.45 49.19 49.22 53.94 64.19 67. 72.84 Table 3. Ablation Study on Progressive Training (PT) and Latent Segment Loss (LSL). Experimental results demonstrate that these techniques enhance the model with better comprehension on finegrained object shapes. Ablations on Latent Segment Loss. Latent Segment Loss makes the model predict dense segmentation masks while training with sparse trajectories, enhancing its ability to perceive fine-grained object shapes under sparse conditions. To evaluate the effectiveness of this technique, we train the model from stage1 for one epoch using bounding boxes as trajectory conditions and compare its performance with MagicMotion stage2. Table 3 shows that the absence of Latent Segment Loss reduces the models ability on ob-"
        },
        {
            "title": "References",
            "content": "[1] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for In IEEE International Conference on end-to-end retrieval. Computer Vision, 2021. 4 [2] Aditya Bhat, Rupak Bose, Chinedu Innocent Nwoye, and Nicolas Padoy. Simgen: diffusion-based framework for simultaneous surgical image and segmentation mask generation, 2025. 4 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 1 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2, 3 [5] Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, and Ning Yu. Go-with-the-flow: Motion-controllable video diffusion models using real-time warped noise, 2025. 2 [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. 2 [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, and Sergey Tulyakov. Panda-70m: Captioning 70m videos In Proceedings of with multiple cross-modality teachers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 4 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [9] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: large-scale benchmark for video segmentation with motion expressions. In ICCV, 2023. 2, 8 [10] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: new dataset for In ICCV, video object segmentation in complex scenes. 2023. 2, 8 [11] Weijie Kong et al. Hunyuanvideo: systematic framework for large video generative models, 2024. 2, 3 [12] Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Harshit, Mingzhen Huang, Juehuan Liu, Yong Xu, Chunyuan Liao, Lin Yuan, and Haibin Ling. Lasot: high-quality large-scale single object tracking benchmark, 2020. 2 [13] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multientity motion in video generation. In ICLR, 2025. 2, Figure 7. Ablation study on the progressive training procedure. Without it, the generated head shapes become noticeably distorted. ject shapes, leading to less precise trajectory control. Qualitative comparisons in Fig. 8 further highlight this effect. Without Latent Segment Loss, the womans arm in the generated video appears incomplete. Figure 8. Ablation Study on latent segment loss. Without it, the generated arms appear partially missing. 5. Conclusion In this paper, we proposed MagicMotion, trajectorycontrolled image-to-video generation method that uses ControlNet-like architecture to integrate trajectory information into the diffusion transformer. We employed progressive training strategy, allowing MagicMotion to support three levels of trajectory control: dense masks, bounding boxes and sparse boxes. We also utilized Latent to Segment Loss to enhance the ability of the model perceive fine-grained object shapes when only provided with sparse trajectory conditions. Additionally, we presented MagicData, high-quality annotated dataset for trajectory-controlled video generation, created through robust data pipeline. Finally, we introduced MagicBench, large-scale benchmark for evaluating trajectory-controlled video generation. MagicBench not only assessed video quality and trajectory accuracy but also took the number of controlled objects into account. Extensive experiments on both MagicBench and DAVIS demonstrated the superiority of MagicMotion compared to previous works. [14] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, Chen Sun, Oliver Wang, Andrew Owens, and Deqing Sun. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. 2 [15] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3daware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. 2, 3 [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toInternaimage diffusion models without specific tuning. tional Conference on Learning Representations, 2024. 2, 1 [17] Zhou Haitao, Wang Chuang, Nie Rui, Lin Jinxiao, Yu Dongdong, Yu Qian, and Wang Changhu. Trackgo: flexible and efficient method for controllable video generation. 2024. 2, 3, 4, 6 [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [21] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1348013492, 2023. 2 [22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [23] Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. Peekaboo: Interactive video generation via maskeddiffusion. arXiv preprint arXiv:2312.07509, 2023. 2, 3 Pexels-400k dataset. https : / / huggingface.co/datasets/jovianzm/Pexels400k, 2024. 5 [24] JovianZM. [25] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions, 2024. [26] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 2 [27] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 4 10 [28] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 63996408, 2019. 4 [29] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 3 [30] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. CVPR, 2023. 3 [31] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis, 2024. 2, 3, 4, 6, 7, 8, 1, 5 [32] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2 [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [34] Jiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guangrui Li, and Yi Yang. Vspw: large-scale dataset for video scene parsing in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 41334143, 2021. 2 [35] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic segmentation in the wild: benchmark. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2 [36] Koichi Namekata, Sherwin Bahmani, Ziyi Wu, Yash Kant, Igor Gilitschenski, and David B. Lindell. Sg-i2v: SelfIn guided trajectory control in image-to-video generation. The Thirteenth International Conference on Learning Representations, 2025. 2, 3, 6, 7, 8, 1, 4, 5 [37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 2, 3 [39] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Computer Vision and Pattern Recognition, 2016. 2, 7 [40] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models, 2024. 2, [41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [43] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. 6 [44] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 5, 6 [45] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos, 2024. 2 [46] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 2, 5 [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015. 2 [49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [50] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. SIGGRAPH 2024, 2024. 2, 3, 6, 7, 8, 1, 4, 5 [51] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 and Stefano Ermon. arXiv preprint [52] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 2, 5 quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. 2 [55] Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, and Limin Wang. Levitor: 3d trajectory oriented image-to-video synthesis. 2024. 2, 3, 6, 7, 8, 1, 4, 5 [56] Jiawei Wang, Yuchen Zhang, Jiaxin Zou, Yan Zeng, Guoqiang Wei, Liping Yuan, and Hang Li. Boximator: Generating rich and controllable motions for video synthesis, 2024. 2, 3 [57] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation, 2025. 2, 3 [58] Zhouxia Wang, Yushi Lan, Shangchen Zhou, and Chen Change Loy. ObjCtrl-2.5D: Training-free object control with camera poses. In arXiv preprint arXiv:2412.07721, 2024. 6 [59] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 3, 4, 6 [60] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, and Hongming Shan. Dreamvideo-2: Zero-shot subject-driven video customization with precise motion control, 2024. 2 [61] Zejia Weng, Xitong Yang, Zhen Xing, Zuxuan Wu, and Genrec: Unifying video generation arXiv preprint Yu-Gang Jiang. and recognition with diffusion models. arXiv:2408.15241, 2024. 4 [62] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. NeurIPS, 2024. [63] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 2 [64] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation, 2024. 6, 7, 8, 1, 2, 4, 5 [65] Zhen Xing, Qi Dai, Zihao Zhang, Hui Zhang, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Vidiff: Translating videos via multi-modal instructions with diffusion models. arXiv preprint arXiv:2311.18837, 2023. 2 [66] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient video In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 7827 7839, 2024. 2 [54] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High- [67] Zhen Xing, Qi Dai, Zejia Weng, Zuxuan Wu, and YuGang Jiang. Aid: Adapting image2video diffusion models for instruction-guided video prediction. arXiv preprint arXiv:2406.06465, 2024. 2 [68] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. 2 [69] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 5 [70] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models. arXiv preprint arXiv:2303.04803, 2023. 4 [71] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: large-scale video object segmentation benchmark, 2018. 2 [72] Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, and Jing Liao. Direct-a-video: Customized video generation with userIn Spedirected camera movement and object motion. cial Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers 24 (SIGGRAPH Conference Papers 24), page 12, New York, NY, USA, 2024. ACM. [73] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 4, 6, 1 [74] Guy Yariv, Yuval Kirstain, Amit Zohar, Shelly Sheynin, Yaniv Taigman, Yossi Adi, Sagie Benaim, and Adam Polyak. Through-the-mask: Mask-based motion trajectories for image-to-video generation, 2025. 2 [75] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2, 3, 4, 6, 7, 8, 1, 5 [76] Hui Zhang, Zuxuan Wu, Zhen Xing, Jie Shao, and Yu-Gang Jiang. Adadiff: Adaptive step selection for fast diffusion. arXiv preprint arXiv:2311.14768, 2023. 2 [77] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 4 [78] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation, 2024. 2, 3, 4, 6, 7, 8, 1, [79] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. ICCV, 2023. 4 [80] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 2, 3 12 A. Additional Experiments C. Additional Comparison results We conducted additional experiments using MagicMotion under various task settings, including camera motion control and video editing. We also generate videos by applying different motion trajectories to single input image. Baseline Comparisons. As shown in Table 4, we provide comparison of the backbones used by each method, along with the supported video generation length and resolution. Camera Motion Control. As shown in Fig. 9, MagicMotion enables precise control over camera motion, allowing for operations such as rotation, zoom, and pan. In the first row of Fig. 9, we enclose oranges within bounding boxes and apply rotation to the boxes. This results in video with simulated camera rotation effect. In the second and third rows, we adjust the size of the foreground objects bounding box to control its perceived distance from the camera, effectively achieving zoom-in and zoom-out effects. In the last two rows, we shift the bounding box to the left and downward, creating the effect of the camera moving in the opposite direction. Video Editing. As shown in Fig. 10, MagicMotion can be applied to video editing to generate high-quality videos. Specifically, we first use FLUX [29] to edit the first frame of the original video, which serves as the input for MagicMotion. Then, we extract the segment mask of the original video and use it as trajectory guidance for the MagicMotion Stage1. Using this approach, we transform black swan into diamond swan, make the camel walk in majestic palace, and turn hiking backpacker into an astronaut. Same input image with different trajectories Extensive experiments have demonstrated that MagicMotion enables objects to move along specified trajectories, generating high-quality videos. To further showcase the capabilities of MagicMotion, we use stage2 of MagicMotion to animate objects from the same input image along different motion trajectories. As shown in Fig. 11, MagicMotion successfully animates two bears, two fish, and the moon, each following their designated paths. B. Latent Segment Masks In this section, we provide more detailed demonstration of Latent Segmentation Masks. Specifically, we use MagicMotion Stage 3 to predict the latent segmentation masks for each frame based on sparse bounding box conditions. As shown in Fig. 12, MagicMotion accurately predicts the Latent Segmentation Masks throughout dynamic scenes, such as man gradually standing up to face robot and boys head slowly sinking into the water. This holds true for frames where only the bounding box trajectory is available and even for frames where no trajectory information is provided at all. 1 Motion-I2V [50] ImageConductor [31] DragAnything [64] LeViTor [55] DragNUWA [75] SG-I2V [36] Tora [78] MagicMotion Resolution Length 320*512 256*384 320*576 288*512 320*576 576*1024 480*720 480*720 16 16 25 16 14 14 49 49 Base Model AnimateDiff [16] AnimateDiff [16] SVD [3] SVD [3] SVD [3] SVD [3] CogVideoX [73] CogVideoX [73] Table 4. Comparisons on each methods backbone. Quantitative Comparisons on different object number Due to space constraints, we only included radar charts in the main text to compare the performance of different methods in controlling varying numbers of objects on MagicBench. Here, we provide the specific quantitative results. As shown in Table 5, Table 6, and Table 7, MagicMotion consistently outperforms other methods across all metrics by significant margin, especially when the number of moving objects is large. This demonstrates that other methods exhibit poorer performance when controlling larger number of objects. More qualitative comparison results. In this section, we provide additional qualitative comparison results with previous works. As shown in Fig.13, Fig.14, Fig.15, Fig.16, and Fig. 17, MagicMotion accurately controls object trajectories and generates high-quality videos, while other methods exhibit significant defects. For fully rendered videos, we refer the reader to Supplementary video.mp4 in supplementary material. D. Additional Ablation Results. Here, we provide additional qualitative comparison results from the ablation study. As shown in Fig. 18, not using MagicData for training results in the generation of woman with an extra hand. Not using the Progressive Training Procedure results in significant defects, such as dancing woman showing severe issues when turning, with second face appearing where her hair should be. Additionally, without the Latent Segment Loss, the womans lipstick is distorted into rectangular shape. E. More Details on MagicData Here, we provide some detailed statistical information about MagicData. On average, each video in MagicData Figure 9. Camera motion controlled results. By setting specific trajectory conditions, MagicMotion can control camera movements."
        },
        {
            "title": "Method",
            "content": "Motion-I2V [50] ImageConductor [31] DragAnything [64] LeViTor [55] DragNUWA [75] SG-I2V [36] Tora [78] Ours (Stage1) Ours (Stage2) MagicBench (Object Number = 1) MagicBench (Object Number = 2) FID() 148.4379 236.6626 147.2262 176.2211 141.6972 129.3680 124.8919 96.3028 106.8807 FVD() Mask IoU() Box IoU() 660.8655 674.4987 884.6453 492.4725 610.0368 547.8107 805.0145 473.2179 564.1036 0.6057 0.5706 0.6706 0.5536 0.6699 0.7144 0.6468 0.9359 0. 0.7142 0.6974 0.8088 0.7057 0.7769 0.8668 0.7776 0.9607 0.9017 FID() 178.7882 252.6367 169.6339 212.8940 163.5423 142.6613 156.2070 111.3980 122.6654 FVD() Mask IoU() Box IoU() 867.6057 879.8890 940.4941 542.8533 624.3985 619.2791 795.8535 428.3430 550.5857 0.5078 0.4786 0.6148 0.4352 0.6033 0.6315 0.5509 0.9080 0.6931 0.5938 0.5579 0.7232 0.5364 0.6809 0.7378 0.6584 0.9097 0.8256 Table 5. Quantitative Comparison results on MagicBench with moving objects number equals to 1 / 2. contains 346 frames, with typical height of 999 pixels and width of 1503 pixels. For more comprehensive understanding of the distribution and variability across the dataset, please refer to Fig. 20, which visualizes the detailed distribution of video frame counts, heights, and widths. During training, these videos are resized to 48 frames and converted to 720p resolution. F. More Details on MagicBench For evaluation purposes, all videos in MagicBench are sampled to 49 frames and resized to resolution of 720p. MagicBench is categorized into 6 classes based on the number of annotated foreground objects. Below, we provide one video example for each category, offering more intuitive understanding of MagicBench. 2 Figure 10. Video Editing Results. We use FLUX [29] to edit the first-frame image and MagicMotion Stage1 to move the foreground objects following the trajectory of the origin video. Figure 11. MagicMotion can generate videos using the same input image and different trajectories (marked by red arrows)."
        },
        {
            "title": "Method",
            "content": "Motion-I2V [50] ImageConductor [31] DragAnything [64] LeViTor [55] DragNUWA [75] SG-I2V [36] Tora [78] Ours (Stage1) Ours (Stage2) MagicBench (Object Number = 3) MagicBench (Object Number = 4) FID() 168.2760 243.8881 144.5718 195.1743 145.0172 126.4608 126.4421 89.1128 97.4697 FVD() Mask IoU() Box IoU() 842.6530 927.3884 925.2795 607.7522 642.4184 520.2733 742.4080 421.0036 440.2373 0.5366 0.5169 0.6332 0.3809 0.6420 0.6531 0.5926 0.9107 0. 0.6076 0.5578 0.6625 0.4671 0.7012 0.7068 0.6356 0.8797 0.8097 FID() 149.3808 221.8339 120.3439 183.5972 122.8971 97.3747 115.4566 73.5877 77.4146 FVD() Mask IoU() Box IoU() 744.5470 832.4498 901.7427 688.8164 512.5130 460.1303 779.0798 396.4754 442.0640 0.6018 0.5679 0.6946 0.3555 0.7085 0.7145 0.6226 0.9231 0.7998 0.6484 0.5417 0.7148 0.4044 0.7250 0.7423 0.6312 0.8896 0.8253 Table 6. Quantitative Comparison results on MagicBench with moving objects number equals to 3 / 4. 4 Figure 12. Latent Segment Masks visualization. MagicMotion can predict out latent segment masks of each frame even when only provided with sparse bounding boxes guidance."
        },
        {
            "title": "Method",
            "content": "Motion-I2V [50] ImageConductor [31] DragAnything [64] LeViTor [55] DragNUWA [75] SG-I2V [36] Tora [78] Ours (Stage1) Ours (Stage2) MagicBench (Object Number = 5) MagicBench (Object Number >5) FID() 148.1927 235.7079 120.7039 180.9895 130.7921 96.1895 117.2917 76.4964 79.5924 FVD() Mask IoU() Box IoU() 582.0029 857.3489 710.5812 578.5567 435.9205 453.1147 709.1618 374.6467 350.5010 0.6295 0.5180 0.7050 0.3913 0.7253 0.7431 0.6228 0.9155 0. 0.6267 0.4737 0.7011 0.4281 0.6988 0.7367 0.6111 0.8600 0.8123 FID() 146.4851 215.3864 122.6998 188.8038 142.0180 103.7038 148.9100 75.8724 75.6016 FVD() Mask IoU() Box IoU() 923.2557 963.6862 719.2442 763.3157 549.7680 596.4075 907.9254 449.3122 396.0661 0.4899 0.4536 0.6534 0.2812 0.6638 0.6616 0.4976 0.9012 0.8004 0.4511 0.3442 0.6045 0.2768 0.5709 0.6211 0.4866 0.7653 0.7124 Table 7. Quantitative Comparison results on MagicBench with moving objects number equals to 5 / above 5. 5 Figure 13. Qualitative Comparisons Results. MagicMotion successfully control the cat jumping over the bowl, while all other methods exhibit significant defects. 6 Figure 14. Qualitative Comparisons Results. MagicMotion successfully control the witch flying over the input trajectory, while all other methods exhibit significant defects. 7 Figure 15. Qualitative Comparisons Results. MagicMotion successfully control the elephant walking along the input trajectory, while all other methods exhibit significant defects. 8 Figure 16. Qualitative Comparisons Results. MagicMotion successfully control the robot moving along the input trajectory, while all other methods exhibit significant defects. 9 Figure 17. Qualitative Comparisons Results. MagicMotion successfully control the tigers head moving along the input trajectory, while all other methods exhibit significant defects. 10 Figure 18. Additional Ablation results. Figure 19. Detail information on MagicData. 11 Figure 20. MagicBench visualization. We provide one video as visual example for each object number category.."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Microsoft Research Asia"
    ]
}