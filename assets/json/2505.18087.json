{
    "paper_title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays",
    "authors": [
        "Hyungyung Lee",
        "Geon Choi",
        "Jung-Oh Lee",
        "Hangyul Yoon",
        "Hyuk Gi Hong",
        "Edward Choi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, a structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 7 8 0 8 1 . 5 0 5 2 : r CXReasonBench: Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays Hyungyung Lee1, Geon Choi1, Jung-Oh Lee2, Hangyul Yoon1, Hyuk Gi Hong3, Edward Choi1 1KAIST 2Seoul National University Hospital 3Seoul Medical Center {ttumyche, edwardchoi}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising applications in medical tasks, such as report generation and visual question answering. However, existing benchmarks focus mainly on the final diagnostic answer, offering limited insight into whether models engage in clinically meaningful reasoning. To address this, we present CheXStruct and CXReasonBench, structured pipeline and benchmark built on the publicly available MIMIC-CXR-JPG dataset. CheXStruct automatically derives sequence of intermediate reasoning steps directly from chest X-rays, such as segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. CXReasonBench leverages this pipeline to evaluate whether models can perform clinically valid reasoning steps and to what extent they can learn from structured guidance, enabling fine-grained and transparent assessment of diagnostic reasoning. The benchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases, each paired with up to 4 visual inputs, and supports multi-path, multi-stage evaluation including visual grounding via anatomical region selection and diagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with structured reasoning and generalization, often failing to link abstract knowledge with anatomically grounded visual interpretation. The code is available at https://github.com/ttumyche/CXReasonBench"
        },
        {
            "title": "Introduction",
            "content": "Large Vision-Language Models (LVLMs) have recently been applied to medical tasks such as report generation and visual question answering (VQA) [8, 9, 18]. To evaluate their performance, chest X-rays have become standard benchmark due to their clinical relevance and accessibility. However, existing benchmarks [6, 13, 17, 29] primarily assess the correctness of the final diagnostic answer, offering limited insight into the underlying reasoning process, critical omission in clinical decision-making. Some recent efforts [4, 14, 20, 30] incorporate explanations or visual grounding, but these still more emphasize outputs than the intermediate steps that lead to them. For example, when asked Which region shows abnormalities?, model might respond Cardiac region and add The heart appears enlarged. While the answer may seem plausible, it remains unclear whether the model truly recognized relevant anatomical structures, performed appropriate measurements, and applied clinical rules such as the cardiothoracic ratio [24], all of which are key components of trustworthy diagnostic reasoning. Without evaluating these intermediate steps, we cannot tell whether the model truly understands the image or relies on superficial patterns. To bridge this critical gap, we present CheXStruct and CXReasonBench, two complementary contributions designed to evaluate the intermediate steps of diagnostic reasoning from chest X-rays. CheXStruct (Figure 1) is fully automated pipeline that extracts clinically relevant reasoning steps Preprint. directly from chest X-ray images. While prior structured datasets [5, 7, 27] provide bounding box annotations linking report-derived findings to image regions, they primarily focus on high-level findings and offer limited supervision for intermediate reasoning steps. CheXStruct addresses this limitation by systematically modeling the full reasoning process: it begins with anatomical segmentation, then derives diagnostic measurements (e.g., cardiac width, thoracic width), computes diagnostic indices (e.g., cardiothoracic ratio), and applies clinical thresholds, following expert-defined guidelines. Recognizing the importance of the quality of the anatomical segmentation, we implemented task-specific quality control (QC) processes to ensure that the measurements and indices derived from the segmented regions are clinically reliable and consistent with expert-defined criteria. Images that fail to meet quality thresholds are automatically filtered out, ensuring that only reliable data is used for evaluation. CheXStruct performs the entire pipeline, segmentation, measurement, QC filtering, and structured information extraction automatically, producing multi-step reference answers for evaluating models diagnostic reasoning. Building on this structured pipeline, we introduce CXReasonBench (Figure 2), novel benchmark designed to evaluate whether models diagnostic reasoning process aligns with clinically grounded steps, not just diagnostic decisions. It compares model responses to the reference answers generated by CheXStruct across multiple intermediate stages, such as selecting diagnostic criteria, identifying anatomical structures, interpreting anatomical changes, computing quantitative measurements, and applying clinical thresholds, enabling fine-grained analysis of diagnostic reasoning. CXReasonBench includes two complementary evaluation paths. Path 1, Direct Reasoning Process Evaluation, evaluates models ability to reconstruct its diagnostic reasoning through multiple intermediate stages and adhere to standardized or expert-defined criteria. Path 2, Guided Reasoning and Re-evaluation, tests whether model can internalize and apply structured diagnostic reasoning when provided with medically accurate intermediate steps. These two paths support robust and transparent assessment of model reasoning, facilitate detailed failure analysis, and ensure that performance is consistent with genuine clinical understanding, not merely pattern-based shortcuts. Built using the MIMIC-CXR-JPG dataset [16], our benchmark includes 18,988 QA pairs covering 12 diagnostic tasks and 1,200 cases, supporting multi-path, multi-stage evaluation, including visual grounding through anatomical region selection and diagnostic measurements. Each QA pair is accompanied by up to 4 visual inputs (e.g., the original X-ray and overlaid segmentation masks). Based on this benchmark, in our evaluation of 10 LVLMs, we find that even the most capable models struggle with valid structured diagnostic reasoning, frequently failing to connect abstract diagnostic knowledge with anatomically grounded visual interpretation. While structured guidance aids diagnostic reasoning, most models fail to generalize this reasoning to new cases, largely due to persistent challenges in visual grounding, such as accurately identifying anatomical regions, despite occasional improvement in non-visual aspects like diagnostic criterion selection."
        },
        {
            "title": "2 Related Works",
            "content": "Table 1: Comparison of chest X-ray structuring frameworks. Multi. indicates segmentation masks, anatomical landmarks, diagnostic measurements, and diagnostic indices. Model-assisted indicates the use of segmentation model. Structuring Image Annotation Framework Target Chest ImaGenome [27] Chest X-rays Chest X-rays Chest X-rays PadChest-GR [7] GR-Bench [5] Source Reports Reports Reports Type Level Method Bounding box Coarse-grained Bounding box Coarse-grained Bounding box Coarse-grained Model-based Manual + Model-based Manual + Model-based CheXStruct (Ours) Chest X-rays Chest X-rays Multi. Fine-grained Rule-based (Model-assisted) Structuring Clinical Information from Chest X-rays Prior studies on structuring clinical information from chest X-rays [5, 7, 27] typically link radiology report sentences to specific regions using bounding boxes to align findings with X-rays. However, these approaches focus on coarsegrained annotations and rely on supervision from existing reports, offering limited insight into the underlying diagnostic reasoning. In contrast, CheXStruct is an automated framework that extracts clinically relevant information directly from chest X-rays by segmenting anatomical regions, deriving anatomical landmarks and diagnostic measurements, computing diagnostic indices, and applying clinical thresholds. This approach captures fine-grained reasoning steps, enabling more detailed and interpretable diagnostic information. See Table 1 for comparison. 2 Medical VQA Benchmarks Existing benchmarks focus on evaluating whether model arrives at the correct diagnostic answer, offering limited insight into its reasoning process. Earlier studies [6, 13, 17, 29] focus on visual perception, such as identifying abnormalities. Recent works [4, 14, 20, 30] introduce longer contexts and visual grounding, aiming to reflect implicit reasoning. However, these still emphasize answer correctness than evaluate intermediate diagnostic reasoning. CXReasonBench bridges this gap by explicitly evaluating models ability to perform and follow diagnostic reasoning through multi-stage, multi-path evaluations, including visual grounding via anatomical structure selection and diagnostic measurements. See Table 2 for comparison. Table 2: Comparison of medical VQA benchmarks. For visual grounding, CXReasonBench utilizes anatomical structure selection and diagnostic measurements, while GEMeX relies on bounding box. Benchmark QA Pairs Visual Reasoning Type Evaluation Format VQA-RAD [17] VQA-Med [6] PathVQA [13] PMC-VQA [29] MIMIC-CXR-VQA [4] MIMIC-Diff-VQA [14] GEMeX [20] MedXpertQA [30] CXReasonBench (Ours) 3.5k 5.5k 32.7k 227k 337k 70k 1.6M 2k 18.9k"
        },
        {
            "title": "3 CheXStruct",
            "content": "Perception Perception Perception Perception Implicit Reasoning Implicit Reasoning Implicit Reasoning Implicit Reasoning Single-stage Single-stage Single-stage Single-stage Single-stage Single-stage Single-stage Single-stage Visual Grounding Explicit Reasoning Multi-path, Multi-stage As shown in Figure 1, CheXStruct is fully automated pipeline designed to extract structured clinical information directly from chest X-rays. The pipeline performs anatomical segmentation, derives anatomical landmarks and diagnostic measurements, computes diagnostic indices, and applies clinical thresholds, followed by task-specific quality control based on expert-defined guidelines. Since CheXStruct relies on segmentation model trained to identify anatomical structures [21], it is not equipped to detect pathology-specific pixel patterns such as opacities or air-fluid levels. Therefore, CheXStruct currently supports only findings that can be derived through structural reasoning. To ensure both clinical relevance and technical feasibility, we collaborated with clinical experts to define 12 radiological findings and quality assessment tasks that align with this structural pipeline. Please refer to Appendix for detailed descriptions of the diagnostic tasks and the entire pipeline. 3.1 Structured Clinical Information Extraction To enable structured diagnostic reasoning, CheXStruct extracts clinically meaningful information from raw chest X-ray images through multi-step process. Defining Task-Specific Criteria Each of the 12 diagnostic tasks is grounded in clinical measurement guidelines, which fall into one of two categories: Standardized, Quantifiable Criteria For tasks with well-established criteria that do not depend on imaging metadata (e.g., pixel spacing that represents the real-world distance of each pixel), we adopt standard clinical measurement rules. For instance, Cardiomegaly is assessed using the cardiothoracic ratio (CTR), defined as the ratio of maximum horizontal cardiac width to thoracic width, which does not rely on absolute measurements (e.g., centimeters) that cannot be calculated based only on X-ray images. Expert-Defined Criteria To ensure reliable evaluation in tasks lacking standardized criteria or involving measurement ambiguity (e.g., when diagnosis depends on absolute measurements derived from imaging metadata), we collaborated with clinical experts to define quantifiable, clinically meaningful standards that guide structured diagnostic reasoning. For instance, Mediastinal Widening is typically assessed based on whether the mediastinal width exceeds 8cm, but such absolute measurements cannot be obtained directly from X-ray images without pixel spacing. To address this, we measure the mediastinal width and the lung width at the same axial level, and assess mediastinal widening based on their ratio, enabling consistent evaluation across images. Segmenting Anatomical Structures Each diagnostic task requires segmentation of specific anatomical regions. We use CXAS [21], chest X-ray segmentation model trained on expert-curated data, to obtain the necessary segmentation masks. For example, Cardiomegaly requires heart and lung masks. 3 Figure 1: Overview of the CheXStruct pipeline. Given chest X-ray, CheXStruct performs anatomical segmentation, derives anatomical landmarks and diagnostic measurements, computes diagnostic indices, and applies clinical thresholds, followed by task-specific quality control. Comprehensive Clinical Attribute Extraction From the segmented masks, we extract anatomical landmarks (e.g., cardiac and thoracic end points) and diagnostic measurements (e.g., cardiac and thoracic width), which are then used to compute diagnostic indices (e.g., CTR) and apply clinical thresholds according to the defined criteria (e.g., cardiomegaly if the CTR exceeds 0.5 in PA view). 3.2 Quality Control CheXStruct enforces task-specific quality control (QC) to ensure that the extracted structured information is both anatomically valid and clinically reliable. These QC rules, developed in collaboration with clinical experts, automatically exclude low-quality samples from the benchmark. Full descriptions of the QC rules can be found in the Appendix A.3 Quality Criteria Definition We define QC rules specific to each tasks anatomical structures and associated measurement criteria. For instance, for Inspiration Level, we verify that the right posterior ribs are segmented in anatomically consistent positions, maintaining proper spatial order and spacing. Threshold-Based Filtering Using the defined QC rules, CheXStruct filters out samples that fail to meet task-specific standards. For instance, we discard samples with overlapping or disordered rib masks (Inspiration Level). Diagnostic labels are assigned only to samples passing QC, based on expert-defined thresholds. 3.3 Automation and Data Scalability CheXStruct is fully automated and scales efficiently to large datasets. While clinical experts were involved in defining task-specific criteria and thresholds used in structured information extraction and quality control, the execution process itself requires no human intervention. Only image-task pairs that pass all relevant QC rules and yield clinically valid structured outputs are used in the final benchmark, ensuring both reliability and scalability."
        },
        {
            "title": "4 CXReasonBench",
            "content": "Building upon the structured information extracted in CheXStruct, CXReasonBench is multi-path, multi-stage evaluation framework designed to assess models ability to perform structured diagnostic reasoning using chest X-rays. Figure 2 describes the overall evaluation pipeline. Figure 2: Overview of the CXReasonBench evaluation pipeline. The evaluation begins with direct diagnostic question, followed by two possible paths depending on the models response. Path 1 evaluates the models ability to reconstruct its reasoning through intermediate steps or to apply expert-defined criteria when necessary. Path 2 provides structured guidance to develop the reasoning process when the model expresses lack of confidence. 4.1 Evaluation Pipeline Initial Diagnostic Decision and Path Assignment Each evaluation begins with binary diagnostic question (e.g., Does this patient have cardiomegaly?). The model is given three options: binary answer (e.g., Yes / No), or dont know. If the model selects binary answer, it proceeds to Path 1 (Section 4.1.1). If it selects dont know, it is directed to Path 2 (Section 4.1.2). 4.1.1 Path 1: Direct Reasoning Process Evaluation Path 1 evaluates whether the models diagnostic decision is supported by sequence of clinically coherent reasoning steps as below. At each stage, an incorrect response terminates the evaluation, under the assumption that clinically valid reasoning cannot be inferred beyond that point. Refer to Appendix B.3.1 for stage-specific reference answers and detailed definitions for each task. Stage 1. Diagnostic Criterion Selection The model identifies the criterion used for the initial diagnostic question, allowing us to determine if its decision was based on clinically accepted knowledge. Failure to select valid criterion prevents assessment of the diagnostic reasoning applied, rendering the rest of the reasoning process irrelevant. Stage 1.5. Refined Criterion Adoption (Expert-Defined Criteria Only) For diagnostic tasks where the original criteria are difficult to apply to images (e.g., reliance on imaging metadata) or 5 are inherently ambiguous, the model is provided expert-defined criteria from Section 3.1. If the model accepts it, it proceeds to Stage 2; otherwise, the model advances to Path 2. Stage 2. Anatomical Structure Identification The model selects all relevant anatomical regions or reference lines from highlighted chest X-ray images related to the criterion. If the model actually used the criterion for the initial diagnosis or genuinely accepted and understood an expert-defined criterion, it should have visually assessed the associated anatomical structures to apply the criterion correctly. This stage tests whether such visual grounding truly occurred. Stage 3. Measurement or Recognition The model applies the diagnostic criterion. In measurement-type tasks, it performs arithmetic computations based on anatomical measurements (e.g., calculating the CTR) and selects value range (e.g., [0.500.52]). In recognition-type tasks, it interprets anatomical changes (e.g., identifying tracheal deviation) and selects the appropriate label (e.g., shifted to the left). This stage ensures that the model explicitly applies the criterion to make decision. Failure at this stage indicates misinterpretation of the criterion or an inability to apply it, suggesting that the model may not be able to effectively use the criterion as intended. Stage 4. Final Decision The model makes final decision based on Stage 3. The evaluation of this decision depends on whether standardized diagnostic thresholds are available for the task. For tasks with standardized thresholds (e.g., CTR of 0.5 for cardiomegaly in PA view), no threshold is provided to the model. For other tasks, an expert-defined threshold from Section 3.1 is provided. 4.1.2 Path 2: Guided Reasoning and Re-evaluation Path 2 is triggered when the model either responds with dont know to the initial diagnostic question or rejects the expert-defined criterion in Stage 1.5. The goal here is to evaluate whether the model can follow structured guidance to develop its reasoning process and later apply this learned reasoning to new case within the same diagnostic task. Evaluation is terminated upon failure at any stage, as each step is inter-dependent; failure at an early stage indicates insufficient foundational knowledge for diagnostic reasoning, which impacts subsequent stages outlined below. Refer to Appendix B.3.2 for reference answers and definitions for each task and stage. Stage 1. Anatomical Structures Identification The model is shown highlighted chest X-rays and asked to identify specific anatomical structures (e.g., Which image shows the heart?). This stage evaluates whether the model can visually recognize and distinguish relevant anatomical regions, an essential prerequisite for accurate diagnostic reasoning. Stage 2. Guided Measurement or Recognition The model receives detailed visual annotations (e.g., labeled landmarks, coordinates) along with instructions for guided diagnostic assessment. For instance, in evaluating cardiomegaly, heart and thoracic widths are labeled, coordinates are given, and the cardiothoracic ratio calculation is explained. This stage tests the models ability to follow structured guidance and derive clinically meaningful conclusions from annotated images. Stage 3. Final Decision The model is provided with diagnostic threshold and asked to make final decision based on the result from Stage 2. This stage evaluates whether the model can apply the threshold appropriately to reach an accurate diagnostic conclusion. Re-evaluated Path 1 after Guidance If the model successfully completes guided reasoning, it is considered to have acquired the correct reasoning process. This path evaluates whether the model can internalize and independently apply that reasoning to new case within the same diagnostic task. The model is re-evaluated using the same structure as Path 1, except Stage 1.5 is omitted, as the criterion has already been introduced during guidance. Random Re-selection new case is randomly chosen from the set of cases that the model previously could not reach the final stage in Path 1, or initially responded to with dont know, excluding the one just used for guidance in Path 2. Re-evaluation Pipeline The model is asked the diagnostic question in the same format as the initial diagnostic decision. If the model responds incorrectly or with dont know, the evaluation ends, indicating that the model has not internalized the reasoning process or cannot generalize it to new case. If the model answers correctly, it re-enters Path 1 and resumes the evaluation. Note that, given test case, model (i.e. vision-language model) proceeds all stages while storing all previous questions and answers in its context. This ensures that the model can complete Path 1, or it can potentially internalize the lessons from Path 2 and complete Path 1 on the second attempt. 6 4.2 Benchmark Structure Benchmark Format The benchmark uses multiple-choice format with both single-choice (e.g., one diagnostic criterion) and multi-choice questions (e.g., all relevant anatomical structures) depending on the stage. Refer to Appendix B.4 for details. Two-round format with Need new option In certain stages, two-round format is used, though not always. Initially, the correct answer is intentionally excluded, and the model is presented with Need new option choice. If the model selects this option, second round is triggered, where the correct answer is revealed. This format assesses whether the model can recognize insufficient reasoning paths and appropriately defer its decision to subsequent, more complete option set. None of the above option When the Need new option is not included, None of the above option is provided instead. In this case, the correct answer is always included in the initial options. Selecting None of the above results in failure, and the model is expected to explain its reasoning. Benchmark Scale and Coverage CXReasonBench is constructed by applying CheXStruct to MIMICCXR-JPG [16]. To ensure manual review feasibility, we randomly sampled 100 cases for each of the 12 diagnostic tasks (1,200 in total), and clinical expert manually reviewed all corresponding segmentation masks for quality assurance (see Appendix A.4). From each case, we generate QA pairs across three distinct evaluation settings (Path 1, Path 2, and Re-evaluated Path 1), supporting multi-stage evaluation and resulting in 18,988 QA pairs in total: 8,044 in Path 1, 3,600 in Path 2, and 7,344 in Re-evaluated Path 1. Each QA pair is associated with up to 4 visual inputs (e.g., the original X-ray and overlaid segmentation masks). While our evaluation focuses on these clinically validated cases, CheXStruct supports scalable benchmark construction, enabling large-scale evaluation. The number of cases extracted by CheXStruct for each diagnostic task is summarized in Table 6."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Models We evaluate 10 LVLMs, including 3 closed-source and 7 open-source (refer to Table 3). Sampling Methods We use greedy decoding for deterministic responses and stochastic decoding for three diverse responses, with majority voting requiring at least two matches to determine correctness. Evaluation Metric We evaluate the models performance as follows. Except for Average Reasoning Depth, all metrics are computed using the Wilson score [25], which accounts for the number of attempts rather than relying on simple proportions, allowing for fairer comparison across models with varying numbers of attempts at each stage. Final Stage Completion (reported as Completion) measures how often the model successfully completes the reasoning process through to the final stage across the benchmark. Average Reasoning Depth (Depth) indicates the average number of stages the model reaches throughout the benchmark, reflecting its ability to progress through the reasoning pipeline. Decision Alignment (Alignment) applies only to Path 1 and Re-evaluated Path 1, measuring the agreement between the models initial and final decisions, and counting them as aligned only when both are correct. In Path 1, alignment can be reliably assessed for tasks with standardized diagnostic thresholds, where consistent application is expected across stages. For other tasks, the final decision is based on an expert-defined threshold provided at Stage 4, while the initial decision may rely on an implicit threshold, making alignment less definitive, though high scores may still reflect consistent reasoning. In Re-evaluated Path 1, since the expert-defined threshold has already been provided during Path 2, alignment can be assessed reliably across all tasks. Measurement Consistency (Consistency) For measurement-type tasks, the model is instructed to return the calculated value in the preceding stage along with its final decision in the final stage (i.e., Stages 3 and 4 for Path 1, and Stages 2 and 3 for Path 2). We evaluate consistency by checking whether this value falls within the range selected in the preceding stage. If it does, the earlier response is considered correct; otherwise, it is considered incorrect. Based on this, we report the consistency rate and further refine the other metrics to incorporate correctness in the preceding stage based on this consistency. These refined scores, shown in parentheses, reflect the models ability to maintain coherent and accurate reasoning across stages. Further details on the experimental setup can be found in Appendix C. Table 3: Path 1 and Re-evaluated Path 1 results with greedy sampling for overall tasks. Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Path Model Completion (0-100) Depth (0-4) Consistency (0-100) Alignment (0-100) Path 1 Gemini-2.5-Pro Gemini-2.5-Flash GPT-4. Pixtral-Large Llama-3.2-90B-Vision Qwen2.5-VL-72B Pixtral 12B Qwen2.5-VL-7B HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large Re-evaluated Llama-3.2-90B-Vision Path 1 Qwen2.5-VL-72B Pixtral 12B Qwen2.5-VL-7B HealthGPT-L14 RadVLM 17.03 (16.24) 12.83 (8.56) 8.32 (8.32) 3.73 (2.31) 0.38 (0.38) 2.34 (2.12) 0.0 (0.0) 1.21 (0.87) 1.27 (1.27) 0.64 (0.39) 0.0 (0.0) 6.03 (5.16) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 2.74 (2.74) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 1.96 (1.95) 1.4 (1.31) 1.15 (1.15) 1.0 (0.96) 0.53 (0.53) 0.67 (0.67) 0.3 (0.3) 0.49 (0.48) 0.32 (0.32) 0.28 (0.28) 1.78 (1.77) 1.41 (1.4) 1.11 (1.11) 1.13 (1.13) 1.17 (1.17) 1.08 (1.08) 1.0 (1.0) 1.0 (1.0) 0.0 (0.0) N/A (N/A) 68.4 43.76 61.22 28.5 61.27 34.67 36.39 14.86 36.39 25.07 49.71 46.98 36.39 N/A N/A 36.39 N/A N/A N/A N/A 60.88 (58.61) 50.29 (37.67) 39.8 (39.8) 36.74 (25.3) 23.32 (23.32) 38.45 (34.36) 0.0 (0.0) 45.44 (28.35) 33.87 (33.87) 32.5 (20.32) 0.0 (0.0) 19.58 (15.68) 0.0 (0.0) N/A (N/A) N/A (N/A) 22.52 (22.52) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 5.2 Results Results from Path 1: Direct Reasoning Process Evaluation Table 3 shows that current models struggle with valid structured diagnostic reasoning, primarily due to difficulties in bridging abstract diagnostic knowledge with visually grounded interpretation. Even when models correctly identify diagnostic criteria, they often fail to localize the relevant anatomical structures, revealing disconnect between conceptual understanding and visual grounding. These limitations are reflected in overall trends: closed-source models generally outperform open-source ones, yet even the strongest model (i.e., Gemini-2.5-Pro) typically only reaches Stage 2 and rarely completes the full reasoning process. Notably, even at Stage 2, the models performance varies by task (Tables 12 and 13). It performs well on tasks involving single structure, regardless of saliency. For instance, inclusion (lungs; 89%) and carina angle (carina; 79%). Tasks involving multiple structures also show strong performance when the structures are salient and the reference line is clear, as in cardiomegaly (heart and thoracic width; 75%). In contrast, performance drops when tasks involve less salient structures or rely on abstract reference lines, such as inspiration level (diaphragm, rib, and mid-clavicular line; 47%) and tracheal deviation (trachea and midline; 48%). These results suggest model performance is not determined solely by the number of structures involved, but by the combined challenges of structural saliency and clarity of the reference line. Effective diagnostic reasoning thus requires models to integrate fine-grained visual grounding with spatial abstraction across inter-dependent anatomical cues. Building on the challenges observed in earlier stages, the performance differences at Stage 3 can be further explained by training data composition  (Table 11)  . Medical-specialized models (e.g., HealthGPT, RadVLM) are primarily trained on medical imaging and clinical reports [13, 19, 28], which emphasize anatomical descriptions and visual interpretation. This supports relatively better performance on recognition-type tasks but limits capability on measurement-type tasks requiring arithmetic computation. In contrast, six of eight general-domain models show the opposite trend, which may be attributed to broader exposure to numerical reasoning and arithmetic operations in large-scale corpora [3, 11, 15]. These findings underscore the need for balanced training data to improve diagnostic reasoning across both recognition-type and measurement-type tasks. Results from Path 2: Guided Reasoning and Re-evaluation As shown in Tables 4 and 17, Gemini 2.5-Pro maintains strong and consistent performance across tasks, excelling at following structured guidance to reach diagnostic decisions. Notably, open-source models such as Qwen2.5-VL-72B and Pixtral-Large achieve reasoning depths comparable to GPT-4.1, reflecting growing potential for guided diagnostic reasoning. 8 Table 4: Path 2 results with greedy sampling for overall tasks. Values in parentheses denote refined scores incorporating measurement consistency. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Model Completion (0-100) Depth (0-3) Consistency (0-100) Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large Llama-3.2-90B-Vision Qwen2.5-VL-72B Pixtral 12B Qwen2.5-VL-7B HealthGPT-L14 RadVLM 55.81 (54.29) 34.65 (30.11) 19.94 (19.15) 19.04 (16.16) 9.55 (8.03) 28.45 (25.48) 4.22 (4.22) 5.38 (3.6) 2.89 (2.46) 0.0 (0.0) 2.62 (2.58) 1.75 (1.64) 0.99 (0.97) 1.21 (1.16) 0.37 (0.34) 1.2 (1.14) 0.45 (0.45) 0.46 (0.44) 0.21 (0.2) 0.05 (0.05) 69.93 57.73 68.7 51.89 38.18 52.81 43.26 11.26 37.98 N/A In measurement-type tasks  (Table 19)  , model performance varies with computational complexity. For simpler anatomical ratios, such as cardiomegaly, some models reliably reach an average depth close to 3, suggesting that clear visual cues and structured prompts facilitate multi-stage reasoning. In contrast, tasks requiring more complex spatial reasoning, like curvature estimation in descending aorta tortuous, remain difficult, with all models failing to exceed an average depth of 2. This reveals persistent limitation in complex visual reasoning, even with explicit guidance. Moreover, most models struggle to transfer guided reasoning to new cases  (Table 3)  , indicating that contextual exposure alone is insufficient for internalizing diagnostic reasoning skills. While some improvement is observed in non-visual knowledge, such as selecting diagnostic criteria, visual grounding, especially in identifying anatomical structures, remains major challenge. This persistent bottleneck highlights the need for training paradigms that explicitly align visual grounding with reasoning supervision, beyond what in-context learning alone can offer. Measurement Consistency Results Although this metric was originally designed to assess whether the model maintains coherent and accurate reasoning across stages, the results reveal deeper issue: current LVLMs often rely on visual cues or shortcuts to make diagnostic decisions, rather than applying diagnostic criteria through structured, measurement-based reasoning. As shown in Tables 3 and 4, most models exhibit lower consistency under Path 1 than Path 2 (Refer to Tables 13 and 19 for details). Since the same models perform better under Path 2, this suggests that the primary limitation lies not in maintaining coherence across reasoning stages, but in performing the necessary computations to apply diagnostic criteria under Path 1. In Path 2, where visual landmarks and explicit computational guidance are provided, models follow instructions and perform calculations in the earlier stage, and as result, the values returned in the final stage are consistent with the selected ranges. In contrast, under Path 1, models tend to approximate using heuristics, resulting in mismatches between the returned values and the selected ranges. This points to key opportunity for future development: while current LVLMs are capable of computation and instruction following, they need an approach to internalize and independently apply diagnostic criteria through structured reasoning. See Appendix for qualitative analyses supporting the measurement consistency results, along with additional evaluation results."
        },
        {
            "title": "6 Discussion",
            "content": "Limitations In this study, we focus on 12 diagnostic tasks that are structurally inferable using anatomical segmentation. As noted in Section 3, CXAS cannot capture pixel-level pathologies such as opacities or pleural effusion, which are therefore excluded from the benchmark. Additionally, the current image-only setting does not address findings that require additional patient context, such as pneumonia. These limitations highlight the need for extending the benchmark to better reflect the full spectrum of diagnostic reasoning. Future Directions To address these limitations, future efforts will expand CheXStruct to include more diagnostic tasks and datasets, enabling CXReasonBench to cover broader range of diagnostic reasoning. We also plan to develop CheXStruct further for automated radiology report generation and clinical quality assurance by verifying model-generated reports. Additionally, it will improve dataset quality by filtering training data through consistency checks between image-derived reasoning and human-written reports. Finally, we aim to create instruction-tuning datasets derived from structured reasoning to train models with interpretable and clinically grounded diagnostic capabilities."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019. [4] Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo Oh, Lei Ji, Eric Chang, Tackeun Kim, et al. Ehrxqa: multi-modal question answering dataset for electronic health records with chest x-ray images. Advances in Neural Information Processing Systems, 36:38673880, 2023. [5] Shruthi Bannur, Kenza Bouzid, Daniel Castro, Anton Schwaighofer, Anja Thieme, Sam Bond-Taylor, Maximilian Ilse, Fernando Pérez-García, Valentina Salvatelli, Harshita Sharma, et al. Maira-2: Grounded radiology report generation. arXiv preprint arXiv:2406.04449, 2024. [6] Asma Ben Abacha, Mourad Sarrouti, Dina Demner-Fushman, Sadid Hasan, and Henning Müller. Overview of the vqa-med task at imageclef 2021: Visual question answering and generation in the medical domain. In Proceedings of the CLEF 2021 Conference and Labs of the Evaluation Forum-working notes. 21-24 September 2021, 2021. [7] Daniel Castro, Aurelia Bustos, Shruthi Bannur, Stephanie Hyland, Kenza Bouzid, Maria Teodora Wetscherek, Maria Dolores Sánchez-Valverde, Lara Jaques-Pérez, Lourdes Pérez-Rodríguez, Kenji Takeda, et al. Padchest-gr: bilingual chest x-ray dataset for grounded radiology report generation. arXiv preprint arXiv:2411.05085, 2024. [8] Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali, Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef, Joseph Paul Cohen, Eduardo Pontes Reis, et al. Chexagent: Towards foundation model for chest x-ray interpretation. arXiv preprint arXiv:2401.12208, 2024. [9] Nicolas Deperrois, Hidetoshi Matsuo, Samuel Ruipérez-Campillo, Moritz Vandenhirtz, Sonia Laguna, Alain Ryser, Koji Fujimoto, Mizuho Nishio, Thomas Sutter, Julia Vogt, et al. Radvlm: multitask conversational vision-language model for radiology. arXiv preprint arXiv:2502.03333, 2025. [10] N. Gaggion, C. Mosquera, M. Aineseder, L. Mansilla, D. Milone, and E. Ferrante. CheXmask Database: large-scale dataset of anatomical segmentation masks for chest x-ray images (version 0.1). PhysioNet, 2023. https://doi.org/10.13026/dx54-8351. [11] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [13] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. [14] Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald Summers, and Yingying Zhu. Expert knowledge-aware image difference graph representation learning for difference-aware medical visual question answering. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 41564165, 2023. 10 [15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR, 2021. [16] Alistair EW Johnson, Tom Pollard, Nathaniel Greenbaum, Matthew Lungren, Chihying Deng, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxr-jpg, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. [17] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. [18] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. [19] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semanticallylabeled knowledge-enhanced dataset for medical visual question answering. In 2021 IEEE 18th international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. [20] Bo Liu, Ke Zou, Liming Zhan, Zexin Lu, Xiaoyu Dong, Yidi Chen, Chengqiang Xie, Jiannong Cao, Xiao-Ming Wu, and Huazhu Fu. Gemex: large-scale, groundable, and explainable medical vqa benchmark for chest x-ray diagnosis. arXiv preprint arXiv:2411.16778, 2024. [21] Constantin Seibold, Alexander Jaus, Matthias Fink, Moon Kim, Simon Reiß, Ken Herrmann, Jens Kleesiek, and Rainer Stiefelhagen. Accurate fine-grained segmentation of human anatomy in radiographs via volumetric pseudo-labeling. arXiv preprint arXiv:2306.03934, 2023. [22] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [23] Qwen Team. Qwen2.5-vl, January 2025. [24] Krystian Truszkiewicz, Rafał Poreba, and Paweł Gac. Radiological cardiothoracic ratio in evidence-based medicine. Journal of Clinical Medicine, 10(9):2016, 2021. [25] Edwin Wilson. Probable inference, the law of succession, and statistical inference. Journal of the American Statistical Association, 22(158):209212, 1927. [26] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [27] Joy Wu, Nkechinyere Agu, Ismini Lourentzou, Arjun Sharma, Joseph Paguio, Jasper Yao, Edward Dee, William Mitchell, Satyananda Kashyap, Andrea Giovannini, et al. Chest imagenome dataset for clinical reasoning. arXiv preprint arXiv:2108.00316, 2021. [28] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical vision-language processing. arXiv preprint arXiv:2303.00915, 2(3):6, 2023. [29] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. [30] Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362, 2025."
        },
        {
            "title": "A Details of CheXStruct",
            "content": "A.1 Diagnostic Tasks We defined set of 12 diagnostic tasks in collaboration with clinical experts. These tasks are categorized into two groups: radiological findings and image quality assessments. Radiological Finding The selected findings are diagnosable from chest X-rays alone, without requiring additional patient information such as clinical history or symptoms (e.g., pneumonia). Tasks: Cardiomegaly, Mediastinal Widening, Carina Angle, Trachea Deviation, Aortic Knob Enlargement, Ascending Aorta Enlargement, Descending Aorta Enlargement, and Descending Aorta Tortuous Image Quality Assessment These tasks evaluate the technical adequacy of image acquisition, ensuring that the radiograph meets basic quality standards for accurate interpretation. Tasks: Inclusion, Inspiration Level, Rotation, Projection A.2 Structured Clinical Information Extraction To enable structured diagnostic reasoning, CheXStruct extracts clinically meaningful information from raw chest X-ray images through multi-step process. A.2.1 Task-Specific Criteria Definition and Clinical Attribute Extraction This section presents detailed descriptions of each diagnostic tasks criteria and how corresponding clinical attributes are extracted. For each task, we define clinically grounded measurement criteria and illustrate the extraction process using visual examples based on segmentation masks and anatomical landmarks. These examples demonstrate how quantitative criteria are applied to derive consistent diagnostic evaluation across images. Each of the 12 diagnostic tasks is grounded in clinical measurement guidelines, which fall into one of two categories: Standardized, Quantifiable Criteria For tasks with well-established criteria that do not depend on imaging metadata (e.g., pixel spacing that represents the real-world distance of each pixel), we adopt standard clinical measurement rules. Cardiomegaly refers to an enlarged heart silhouette on chest X-rays. It is diagnosed by calculating the cardiothoracic ratio (CTR), defined as the maximal horizontal cardiac diameter divided by the maximal horizontal thoracic diameter. In the PA view, CTR exceeding 0.50 indicates cardiomegaly. See Figure 4. Carina Angle represents the angle formed at the bifurcation of the trachea into the right and left main bronchi. It is measured directly at the point of bifurcation, typically ranging between 40-80 degrees, although normal values may vary slightly across literature. See Figure 5. Trachea Deviation refers to the lateral displacement of the trachea from the midline on chest X-ray. It is assessed by determining whether the trachea lies along the midline defined by the spinous processes or is deviated from it. See Figure 6. Inclusion describes whether all relevant thoracic anatomy is fully captured in the image. Proper inclusion is confirmed when the image shows the lung apices, inner margins of the lateral ribs, and costophrenic angles. See Figure 7. Rotation refers to the misalignment of the patient during image acquisition. It is evaluated by checking the symmetry of the clavicles relative to the spinous processes. The spinous processes should be equidistant from the medial ends of both clavicles; any asymmetry indicates patient rotation. See Figure 8. Expert-Defined Criteria To ensure reliable evaluation in tasks lacking standardized criteria or involving measurement ambiguity (e.g., when diagnosis depends on absolute measurements derived from imaging metadata), we collaborated with clinical experts to define quantifiable, clinically meaningful criteria that guide structured diagnostic reasoning. Mediastinal Widening refers to abnormal broadening of the mediastinum. Traditionally defined as mediastinal width >8 cm at the aortic arch level on PA chest X-rays, this absolute measurement is not feasible in image-only settings. Instead, we assess the ratio of mediastinal width to thoracic width at the same level, providing consistent and image-based standard. See Figure 9. Aortic Knob Enlargement refers to the prominence of the aortic arch along the left mediastinal border. Traditionally, it has been assessed through visual estimation, which is susceptible to inter-observer variability. We instead quantify it as the ratio between the maximum width of the aortic knob and the median width of the trachea. The trachea is chosen as reference due to its stable anatomy, ensuring reproducible assessment. See Figure 10. Ascending Aorta Enlargement indicates abnormal dilation of the ascending aorta, visible along the right mediastinal border. Conventional evaluation relied on subjective visual inspection, which lacked standardized criteria. We define enlargement based on whether the aorta extends beyond an imaginary line connecting the right heart border and the inner margin of the right lung, providing consistent and interpretable rule. See Figure 11. Descending Aorta Enlargement denotes widening of the descending thoracic aorta, often seen as prominent left paraspinal contour. Visual estimation was the conventional approach, but it often led to inconsistency. We introduce ratio-based measurement between the aortas maximum width and the median tracheal width, offering consistent evaluation. See Figure 12. Descending Aorta Tortuous describes excessive curvature of the descending thoracic aorta. It was previously evaluated by subjective visual impression, leading to variability. To overcome this, we quantify tortuosity by dividing the aorta into five equal-length sections and annotating six coordinates. Curvature at each point is computed using finite difference methods (central, forward, and backward difference), enabling objective and reproducible evaluation. See Figure 13. Inspiration Level assesses the degree of lung expansion during image acquisition, as observed on chest X-ray. Adequate inspiration is typically indicated by the visualization of 910 posterior ribs above the diaphragm. It is measured by counting the number of right posterior ribs intersecting the right hemidiaphragm along the mid-clavicular line. Originally, this criterion lacked clearly defined reference line, which led to inconsistency in rib counts and variability across evaluators. By introducing the mid-clavicular line as standardized landmark, we ensure more consistent and reproducible assessment. See Figure 14. Projection refers to the orientation of the X-ray beam relative to the patient, typically posteroanterior (PA) or anteroposterior (AP) on chest X-rays. Originally, projection was assessed by visually inspecting scapular positioning, whether the scapulae were retracted (suggesting PA) or overlapping the lung fields (suggesting AP). However, this method lacked clear, objective criteria, leading to variability across evaluators. To address this, we compute the ratio of the overlapping area between each scapula and the lung field to the scapular area. higher overlap ratio indicates an AP view. This quantitative method provides standardized and reproducible approach to classification. See Figure 15. A.2.2 Segmenting Anatomical Structures We define our diagnostic tasks based on the anatomical structures segmented by CXAS [21], chest X-ray anatomy segmentation model trained on high-quality, expert-curated data from the PAX-Ray++ dataset. PAX-Ray++[21] was constructed through multi-stage pipeline: expert-annotated CT datasets were aggregated, used to train nnUNet models, and refined via inference on additional datasets such as AutoPET. These models were then applied to thoracic CT volumes to generate accurate 2D anatomical projections, which were post-processed to ensure consistency and label quality. The final dataset contains 14,754 projected X-ray images with 157 anatomical structures. Trained on this dataset, CXAS enables accurate multi-class segmentation of anatomical structures in chest X-rays. Table7 presents the performance of CXAS on the PAX-Ray++ test set. Figure 3 and Table 5 present the subset of anatomical structures segmented by CXAS that were used in CheXStruct for each diagnostic task. 13 Figure 3: Visualization of the anatomical structures of CXAS used in CheXStruct. 14 Table 5: Anatomical structures of CXAS used in CheXStruct per diagnostic task"
        },
        {
            "title": "Anatomical Structures",
            "content": "Heart, Right Lung, Left Lung Upper Mediastinum, Right Lung, Left Lung Trachea, Trachea Bifurcation, Vertebrae T1T7 Trachea Bifurcation Aortic Arch, Descending Aorta, Trachea, Trachea Bifurcation Ascending Aorta, Heart, Trachea, Trachea Bifurcation Cardiomegaly Mediastinal Widening Trachea Deviation Carina Angle Aortic Knob Enlargement Ascending Aorta Enlargement Descending Aorta Enlargement Descending Aorta, Trachea, Trachea Bifurcation Descending Aorta Tortuous Inclusion Inspiration Level Rotation Projection Descending Aorta Right Lung, Left Lung, Right Hemidiaphragm, Left Hemidiaphragm Right Posterior 1st11th Ribs, Right Lung, Right Hemidiaphragm Clavicle Right, Clavicle Left, Vertebrae T1T7 Scapula Right, Scapula Left, Right Lung, Left Lung Table 6: Number of cases and diagnostic label distribution per diagnostic task after quality control. Diagnostic Task Total Cases (Label Distribution) Cardiomegaly Mediastinal Widening Trachea Deviation Carina Angle Aortic Arch Enlarged Ascending Aorta Enlarged Descending Aorta Enlarged Descending Aorta Tortuous Inclusion Inspiration Level Rotation Projection 184,292 (Normal: 117,514, Enlarged: 66,778) 175,069 (Normal: 122,852, Enlarged: 52,217) 70,028 (Normal: 54,771, Deviated: 15,257) 122,085 (Normal: 27,818, Abnormal: 94,267) 174,949 (Normal: 156,445, Enlarged: 18,504) 82,708 (Normal: 78,318, Enlarged: 4,390) 115,019 (Normal: 114,301, Enlarged: 718) 120,330 (Normal: 117,368, Abnormal: 2,962) 144,221 (Included: 140,754, Excluded: 3,467) 148,222 (Good: 127,848, Poor: 20,374) 51,095 (Not Rotated: 40,906, Rotated: 10,189) 73,991 (PA: 39,743, AP: 34,248) Table 7: Performance of CXAS on PAX-Ray++ Anatomical structure F1 IoU Dice Anatomical structure 0.98 Right Lung 0.98 Left Lung Right Hemidiaphragm 0.97 0.96 Left Hemidiaphragm 0.97 Heart 0.91 Trachea 0.87 Tracheal Bifurcation 0.96 Scapula Right 0.95 Scapula Left 0.90 Clavicle Right 0.90 Clavicle Left 0.89 Aortic Arch 0.90 Ascending Aorta 0.93 Descending Aorta 0.82 Upper Mediastinum 0.96 0.96 0.95 0.93 0.94 0.84 0.78 0.92 0.92 0.83 0.82 0.81 0.83 0.87 0.72 0.98 0.98 0.97 0.96 0.97 0.91 0.87 0.96 0.95 0.90 0.90 0.89 0.90 0.93 0.82 Vertebrae T1 Vertebrae T2 Vertebrae T3 Vertebrae T4 Vertebrae T5 Vertebrae T6 Vertebrae T7 Posterior 1st Rib Right Posterior 2nd Rib Right Posterior 3rd Rib Right Posterior 4th Rib Right Posterior 5th Rib Right Posterior 6th Rib Right Posterior 7th Rib Right Posterior 8th Rib Right Posterior 9th Rib Right Posterior 10th Rib Right Posterior 11th Rib Right F1 0.89 0.89 0.88 0.87 0.86 0.85 0.85 0.72 0.74 0.72 0.73 0.74 0.74 0.75 0.76 0.77 0.78 0.77 IoU Dice 0.82 0.81 0.79 0.79 0.78 0.76 0.75 0.64 0.65 0.65 0.66 0.68 0.68 0.69 0.70 0.71 0.72 0.70 0.89 0.89 0.88 0.87 0.86 0.85 0.85 0.72 0.74 0.72 0.73 0.74 0.74 0.75 0.76 0.77 0.78 0.77 Figure 4: Measurement of cardiomegaly using the cardiothoracic ratio (CTR). 16 Figure 5: Measurement of the carina angle at the tracheal bifurcation. 17 Figure 6: Evaluation of tracheal deviation based on lateral displacement from the spinal midline. 18 Figure 7: Determination of anatomical inclusion by verifying visibility of key thoracic landmarks. 19 Figure 8: Assessment of patient rotation using clavicle symmetry relative to spinal midline. 20 Figure 9: Measurement of mediastinal widening using the mediastinum-to-thoracic width ratio. 21 Figure 10: Quantification of aortic knob enlargement using the ratio of aortic knob width to tracheal width. 22 Figure 11: Assessment of ascending aorta enlargement based on its extension beyond defined boundary. Figure 12: Measurement of descending aorta enlargement via the descending aorta width-to-trachealwidth ratio. 23 Figure 13: Quantitative evaluation of descending aorta tortuous using curvature derived from coordinates. Figure 14: Quantification of inspiration level by counting posterior ribs intersecting the diaphragm along the mid-clavicular line. 24 Figure 15: Assessment of projection type (PA vs. AP) via scapula-lung overlap ratio. 25 A.3 Quality Control To ensure the reliability and anatomical validity of the extracted clinical information, we apply multi-step quality control (QC) process. This includes (1) global image-level filtering, applied uniformly across all tasks, and (2) task-specific QC criteria tailored to the anatomical and clinical requirements of each diagnostic task. All quality control criteria were developed in close collaboration with clinical experts, who also determined appropriate thresholds based on their clinical knowledge and expectations regarding anatomical characteristics. A.3.1 Global Filtering of Raw Images We use the MIMIC-CXR-JPG dataset [16] as our base image source. Initially, we select all images labeled with ViewPosition of PA or AP in the metadata. However, we observed that these labels alone are not sufficient to ensure the validity of the chest X-ray content. To construct clean and trustworthy benchmark, we apply the following multi-step filtering process to exclude non-relevant or corrupted images: Step1. Segmentation-Based Filtering Using CXAS We utilize CXAS [21], chest X-ray anatomy segmentation model that can segment up to 157 anatomical structures relevant to chest X-ray. For each image, we apply CXAS to obtain the corresponding segmentation masks. We discard images in which the number of segmented anatomical structures falls below expertdefined threshold. low number of detected structures typically indicates one of the following: The image is not chest X-ray (e.g., abdominal or spinal radiograph, or an irrelevant modality). The image is corrupted or blank (e.g., all black, all white, or contains artifacts). Anatomical structures are not clearly visible due to poor patient positioning or motion artifacts. This step serves as robust first-pass filter that eliminates obviously invalid images based on anatomical structure segmentation. See Figure 16, Rows 15 for representative examples. Step 2: Anatomical Position-Based Filtering (Heart Mask Heuristic) For the remaining images, we apply heuristic based on the vertical position of the heart mask, one of the reliably segmented structures from CXAS. We compute the vertical ratio of the image above and below the heart mask. If the heart is located disproportionately high in the image (i.e., large portion of the image appears below the heart), this often indicates that the image contains substantial abdominal or pelvic anatomy. These images are excluded based on this anatomical positioning heuristic. See Figure 16, Row 6 for representative examples. Step 3: Gradient-Based Filtering to Remove Post-Processing Artifacts Even after anatomical filtering, some images suffer from post-processing artifacts, such as low contrast or improper windowing adjustments that obscure anatomical structures. To detect such cases, we compute the average edge strength across each image using gradient magnitude: Horizontal and vertical edge maps are computed using the Sobel operator. Gradient magnitude is calculated to quantify edge strength. The mean gradient magnitude across the image is used as filtering metric. Images with abnormally low edge strength typically lack sufficient anatomical contrast and are deemed visually uninformative. These are removed during this step. See Figure 16, Row 7 for representative examples. After applying the global filtering steps, we excluded 7,587 images, resulting in 235,747 high-quality PA and AP chest X-rays. This corresponds to 3.12% of the images initially labeled as PA or AP in the metadata being removed due to quality concerns. Figure 16: Representative Examples of Globally Filtered Images. Rows 15: Images removed due to small number of segmented anatomical structures using CXAS (Step 1). Row 6: Image removed due to abnormal anatomical positioning of heart mask (Step 2). Row 7: Image removed due to post-processing artifacts and low gradient strength (Step 3). 27 A.3.2 Task-Specific Quality Control To ensure reliable measurements for each diagnostic task, we apply task-specific quality control (QC) criteria following global filtering steps. These QC criteria are designed to verify that the anatomical structures relevant to each task are accurately segmented and that the extracted clinical information derived from the segmentation masks is also reliable. The QC criteria are applied based on the task-specific masks and extracted landmark points used in each diagnostic task, as described in Section A.2.1. Inclusion Lung Apex Points 1) LeftRight Y-Coordinate Symmetry: The absolute vertical difference between the left and right lung apex points is calculated. large discrepancy indicates potential segmentation error, as the lung apices are typically located at similar vertical levels. 2) Anatomical Plausibility of Vertical Location: The average y-coordinate of the apex points is used to assess their relative vertical positioning within the image. If the apex points are located too far down in the image, outside the typical thoracic boundary, they are considered anatomically implausible and filtered out. Lung Side Points 1) Relative Width Consistency: For each lung, the width between the outermost (side) point and the nearest inner lung border is measured. Based on the inclusion or exclusion label of each side point, we compute the left-to-right width ratio. Images showing abnormally imbalanced widths are excluded, as this suggests potential segmentation errors or inaccuracies in the extracted anatomical points. Lung Bottom Points 1) LeftRight Y-Coordinate Symmetry: Similar to the apex points, large vertical discrepancy between the left and right bottom lung points suggests possible misidentification or segmentation errors and is used as criterion for filtering. 2) Anatomical Plausibility of Vertical Location: The bottom points are expected to lie in the lower region of the thoracic cavity. If they are found too high in the image, they are likely erroneous and thus filtered. Inspiration Level Right Posterior Ribs 1) Mask Existence: If any of the right posterior rib masks are missing, the image is excluded. The absence of even single rib mask indicates that other ribs may not be correctly segmented or positioned, compromising anatomical reliability. 2) Overlap Area: Excessive overlap between adjacent rib masks suggests that two ribs were merged or wrongly localized. 3) Inter-Rib Distance: Unusual spacing (either too close or too far) indicates segmentation failure or anatomical misalignment. Right Hemidiaphragm If the diaphragm mask inappropriately invades the lung region, the image is excluded, as this indicates segmentation failure. Diaphragm masks located significantly below their expected anatomical position (e.g., well beneath the lung bases) are also excluded, as such positions reflect inaccurate segmentation rather than valid anatomical placement. Right Lung We compare lung masks from CXAS and CheXmask [10]. If both masks are well-segmented, approximated mid-clavicular lines from each should produce similar results. If the mid-clavicular line calculated using the CXAS mask differs significantly from that calculated using the combined CXASCheXmask mask, we exclude the image. This discrepancy suggests that the lung boundary is poorly segmented in at least one of the masks. Rotation Clavicles 1) Distance to Lung Inner Edge: The distance between each clavicle medial endpoint and the inner border of the corresponding lung mask is computed. Anatomically implausible distances, either too small (indicating over-penetration into the lung) or too large (indicating external or misplaced points), are filtered out. 28 2) LeftRight Y-Coordinate Symmetry: The absolute difference in y-coordinates between the left and right medial endpoints is measured. large vertical discrepancy suggests that one of the points may have been incorrectly detected, possibly outside the clavicle region. 3) Anatomical Plausibility of Vertical Location: The average y-coordinate of the left and right medial endpoints is used to estimate the vertical positioning of the clavicles within the image. Points located too close to the image top or bottom (outside the typical thoracic region) are excluded to ensure anatomical plausibility. Spinous Process 1) Standard Deviation of Point Positions: High variation among vertebral point coordinates indicate imprecise detection. Images with such inconsistencies are excluded. 2) Residual from Spinal Line Fitting: We fit smooth spinal line to the vertebral points. If the residuals (deviation from the fitted line) are large, it suggests poor alignment or noise in the points, leading to the exclusion of the image. Projection Scapular The width and height of the scapular mask must exceed expert-defined threshold relative to the overall image dimensions. Images with scapular masks that are too small or barely visible are excluded, as they indicate incomplete scapular segmentation. Lungs For each image, we locate the horizontal band corresponding to the height of the scapular mask. Within this band, we assess the segmented lung regions. We require that the lung regions in this zone exhibit sufficient width and height relative to the image dimensions. If the lung region is too narrow or compressed within this scapular band, the image is excluded. Cardiomegaly Heart 1) IoU-Based Filtering: We compute the Intersection-over-Union (IoU) between the heart mask and the Chest ImaGenome heart bounding box [27]. Images with low IoU are discarded, as this typically indicates mismatch in localization, either one of the annotations includes non-heart regions, or fails to capture the heart fully. 2) Center Alignment Filtering: The centroid coordinates of both the heart mask and the bounding box are extracted. Samples with large differences between these centroids are excluded, as such deviations imply at least one of the annotations may be misaligned with the actual heart location. Lungs We include only images labeled as having both left and right lung sides included, obtained during the Inclusion task. The same lung quality filtering criteria used in the Inclusion task are applied here to ensure anatomical plausibility. Mediastinal Widening Mediastinum Filtering is conducted using the same criteria as for the heart mask in the Cardiomegaly task. The IoU between the upper mediastinum mask and the Chest ImaGenome mediastinum bounding box [27] must exceed expert-defined threshold to ensure both are localized to the correct anatomical region. The distance between their centroids must be within an acceptable range, as large discrepancy may indicate failed segmentation or misaligned structure prediction. Lungs We include only images labeled as having both left and right lung sides included, obtained during the Inclusion task. The same lung quality filtering criteria used in the Inclusion task are applied here to ensure anatomical plausibility. Trachea Deviation Trachea 1) Height: The total height of the trachea mask must exceed minimum threshold to ensure the full trachea structure is captured within the thorax. 2) Normalized Width: The median width of the mask (computed across rows and normalized by image width) must fall within an anatomically reasonable range. 3) Width Variability: The standard deviation of the row-wise width should be low, as large variations indicate irregular or failed segmentation. Midline The same midline estimation and quality filtering strategy used in the Rotation task is applied here. Carina Angle 29 Carina 1) Carina Mask Shape: We filter out images where the carina mask has an abnormal aspect ratio, either too narrow, too wide, too short, or too tall, as these are likely mis-segmentations. 2) Carina Point Location: For well-predicted mask, the carina point should lie near the center of the mask. If the point is too far from the center, the mask or point may be misaligned. 3) Angle Stability: We compute carina angles using the carina point and three leftmost and three rightmost edge points of the mask. Samples with high standard deviation across these angles are excluded, as they indicate unreliable estimation. 4) Vertical Alignment: The coordinate difference between the carina point and the topmost point of the mask should be small, large difference suggests an inaccurately predicted carina point. Aortic Knob Enlargement Aortic Knob 1) Mask Alignment Check: We compare the x-coordinates of the outermost points predicted from the aortic arch mask and the descending aorta mask. If these coordinates are similar, it suggests that both masks are correctly segmented. 2) Descending Aorta Upper Region Height: We measure the height of the upper 30% portion of the descending aorta mask. mask that is too short in this region is likely not properly segmented, leading to an inaccurate estimation of the aortic border. Trachea The trachea mask is filtered using the same criteria described in the Trachea Deviation task. Ascending Aorta Enlargement Ascending Aorta 1) Height: The total height of the ascending aorta mask must exceed expert-defined threshold. short mask may indicate improper segmentation. 2) Relative Vertical Position: We compute the vertical distance between the bottom-right point of the trachea mask and the top-right point of the ascending aorta mask. large difference suggests that the ascending aorta mask is not located in its expected anatomical position and the image is discarded. Border Line This line is defined using the right heart border from the heart mask and the right edge point of the trachea mask. Only images that passed the quality checks in the Cardiomegaly task (for heart mask quality) and the Trachea Deviation task (for trachea mask quality) are included. Descending Aorta Tortuous Descending Aorta 1) Height: The total height of the mask must exceed expert-defined threshold, as short mask suggests incomplete segmentation of the aorta. 2) Width Consistency: During tortuosity computation, we extract six points from the right edge of the descending aorta mask at fixed vertical intervals. For each of these six right-edge points, we find the corresponding point on the left edge at the same y-coordinate. We calculate the width at each level and compute the standard deviation across the six measurements. large standard deviation suggests irregular or faulty segmentation and such images are discarded. Descending Aorta Enlargement Descending Aorta The descending aorta mask is filtered using the same criteria described in the Descending Aorta Tortuous task. Trachea The trachea mask is filtered using the same criteria described in the Trachea Deviation task. Table 6 shows the number of cases extracted by CheXStruct for each diagnostic task remaining after quality control, along with their corresponding diagnostic label distribution. A.4 Clinician Review A.4.1 Clinician-Guided Thresholding and Label Mapping While task-specific extraction and quality control (QC) criteria were defined in collaboration with clinicians (as described in Sections A.2.1 and A.3), each criterion required corresponding threshold to interpret extracted measurements in clinically meaningful way. 30 To establish these thresholds, we conducted structured review process with board-certified radiologists using representative samples spanning wide range of measured values (e.g., anatomical lengths, ratios, distances). The review involved the following two steps: Step 1: Threshold Definition for Quality Control (QC). For each criterion (e.g., the descending aorta must be sufficiently tall), we computed the relevant geometric indicator (e.g., aorta mask height), grouped images by value ranges, and presented representative examples to clinicians. Based on these examples, they determined value thresholds that distinguish anatomically valid segmentations from invalid ones. These thresholds were then used to filter out unreliable cases. Step 2: Diagnostic Label Mapping. For images that passed QC, we used the same range-based interface to collect diagnostic labels (e.g., enlarged, normal). Clinicians assigned labels to each measurement range, resulting in value-to-label mappings that enabled consistent and scalable annotation of the full dataset. This expert-in-the-loop process ensured that both QC filtering and diagnostic labeling were grounded in clinical expertise and aligned with the semantics of the task-specific criteria. A.4.2 Manual Segmentation Mask Review for Benchmark Validation As described in Section 4.2, we randomly sampled 100 cases per diagnostic task (1,200 in total) from the dataset constructed by applying CheXStruct to MIMIC-CXR-JPG [16]. All sampled cases had previously passed anatomical quality control and were deemed suitable for evaluation. To further ensure the reliability of the segmentation masks used to extract diagnostic measurements, clinical expert manually reviewed all task-relevant masks for each of the 12 diagnostic tasks. Among the 1,200 reviewed cases, 1,198 (99.83%) were confirmed to have high-quality segmentation masks. Two cases exhibited minor issues (e.g., slight truncation of the clavicle), but were retained in the benchmark because the diagnostic measurements were unaffected. This manual review confirms that CXReasonBench consists of high-quality samples with reliable anatomical segmentations, establishing it as robust and clinically meaningful benchmark for evaluating diagnostic reasoning across 12 tasks. representative screenshot of the clinician review interface used in both the thresholding and segmentation review processes is shown in Figure 17. Figure 17: Screenshots from clinician review. Representative screenshots of the image review interface and label entries provided by clinicians in Google Sheets."
        },
        {
            "title": "B Details of CXReasonBench",
            "content": "B.1 Overview The following sections present diagnostic questions and corresponding answers used in CXReasonBench. Section B.2 introduces the initial diagnostic question for each diagnostic task. Section B.3 provides the full set of questions and answers for each diagnostic task across all stages of each path. Finally, Section B.4 outlines the overall benchmark structure. Figures 20, 21, 22, 23, 24, and 25 illustrate examples of the model evaluation pipeline based on the diagnostic questions and answers. B.2 Initial Diagnostic Question The corresponding question for each diagnostic task is presented below. chest X-ray image is provided to the model along with the question. Each question is presented to the model along with the instruction: Please base your decision on the most established and clearly defined diagnostic criterion used in standard radiologic references. Avoid relying on indirect factors, which, while potentially relevant, are not the direct and primary criteria. If you choose dont know, you will receive guidance on how to systematically analyze the chest X-ray to improve your decision-making skills. Cardiomegaly Does this patient have cardiomegaly? The chest X-ray was taken in the {view position} view, where {view position} is either PA or AP. Mediastinal Widening Does this patient have mediastinal widening? Carina Angle Does this chest X-ray show normal carina angle? Trachea Deviation Is the trachea deviated in this chest X-ray? Aortic Knob Enlargement Does the aortic knob appear enlarged in this chest X-ray? Ascending Aorta Enlargement Does the ascending aorta appear enlarged in this chest X-ray? Descending Aorta Enlargement Does the descending aorta appear enlarged in this chest X-ray? Descending Aorta Tortuous Is the descending aorta tortuous in this chest X-ray? Inclusion Is the entire thoracic cage including the lung apices, inner margins of the lateral ribs, and costophrenic angles (CPAs) fully visible in this chest X-ray without being cropped? Inspiration Assess the level of inspiration in this chest X-ray. Was it taken with good or poor inspiration? Rotation Was the patient rotated during the chest X-ray? Projection Identify the view of this chest X-ray. B.3 Diagnostic Questions per Path Based on the models response to the initial diagnostic question, it proceeds to Path 1 if binary answer is selected (Section B.3.1), or to Path 2 if dont know is chosen (Section B.3.2). B.3.1 Path 1: Direct Reasoning Process Evaluation The following outlines the questions and answers for each task, organized by stage. Stage 1. Diagnostic Criterion Selection Question: What criterion was used to make the decision for the first question? Below are the answers for each diagnostic task. Cardiomegaly By calculating the cardiothoracic ratio, which is the ratio of the maximal horizontal cardiac diameter to the maximal horizontal thoracic diameter. Mediastinal Widening By evaluating the width of the mediastinum. Carina Angle By evaluating the angle of the carina. Trachea Deviation By checking if the trachea is displaced to one side from the midline. 32 Aortic Knob Enlargement By checking for any prominent bulge or abnormal widening of the aortic knob. Ascending Aorta Enlargement By checking for any bulge or abnormal widening of the ascending aorta. Descending Aorta Enlargement By checking for any abnormal dilation or widening of the descending aorta. Descending Aorta Tortuous By evaluating the shape of the descending aorta and checking for any signs of tortuosity, such as irregular bends or twists. Inclusion By checking if the chest X-ray includes the lung apices, inner margins of the lateral ribs, and costophrenic angles. Inspiration By counting the number of right posterior ribs or anterior rib visible above the right hemidiaphragm. Rotation By checking if the spinous processes are equidistant from the medial ends of the clavicles. Projection By checking if the scapulae were laterally retracted or if they overlapped the lung fields. Stage 1.5. Refined Criterion Adoption (Expert-Defined Criteria Only) For diagnostic tasks where the original criteria are difficult to apply to images (e.g., due to reliance on imaging metadata) or are inherently ambiguous, the model is provided with expert-defined criteria from Section A.2.1. The corresponding questions are listed below. Each question is presented to the model along with the instruction: If you choose No, you will receive guidance on how to systematically analyze the chest X-ray to improve your decision-making skills. Answer options are Yes or No. Mediastinal Widening The original criterion for assessing mediastinal widening involves measuring the mediastinal width in centimeters, typically using metadata or physical markers. However, in this evaluation setting where only images are provided without accompanying metadata, absolute measurements are not feasible. To maintain the fundamental diagnostic approach while adapting to this constraint, the criterion has been refined to use ratio-based measurement. Specifically, evaluators are now instructed to measure both the mediastinal width and the thoracic width at the same level, then calculate the ratio by dividing the mediastinal widthby the thoracic width. This updated approach enables consistent and objective assessments even in the absence of physical measurement units. Aortic Knob Enlargement The original criterion for assessing an enlarged aortic knob involved visually inspecting for any prominent bulge or abnormal widening of the aortic knob. However, this approach can be subjective and may vary among evaluators. To enhance consistency and objectivity in assessment, the criterion has been refined. The refined criterion involves measuring both the maximum width of the aortic knob and the median width of the trachea. Then, calculate the ratio by dividing the aortic knob width by the trachea width. This updated approach provides more standardized andreliable method for identifying an enlarged aortic knob. Ascending Aorta Enlargement The original criterion for assessing an enlarged ascending aorta involved visually inspecting for any bulge or abnormal widening of the ascending aorta. However, this approach can be subjective and may vary among evaluators. To enhance consistency and objectivity in assessment, the criterion has been refined. The refined criterion involves drawing an imaginary straight line connecting the inner boundary of the right lung and the right heart side. Then, determine whether the ascending aorta extends beyond this line. This approach provides more reproducible and objective way to assess aortic enlargement. Descending Aorta Enlargement The original criterion for assessing an enlarged descending aorta involved visually inspecting for any bulge or abnormal widening of the descending aorta. However, this approach can be subjective and may vary among evaluators. To enhance consistency and objectivity in assessment, the criterion has been refined. The refined criterion involves measuring both the maximum width of the descending aorta and the median width of the trachea. Then, calculate the ratio by dividing the descending aorta width by the trachea width. This updated approach provides more standardized and reliable method for identifying an enlarged descending aorta. Descending Aorta Tortuous The original criterion for assessing descending aorta tortuosity involved visually inspecting the shape of the descending aorta and checking for any signs of 33 tortuosity. However, this approach can be subjective and may vary among evaluators. To enhance consistency and objectivity in assessment, the criterion has been refined to include more quantitative approach.This refined method involves focusing on the thoracic portion of the descending aorta, particularly the region at the upper part of the heart. The region is divided into five equal sections, and six coordinates are determined at the top-left lung side of each division. Curvature is then calculated at each of these six coordinates using finite difference methods: forward and backward differences for the first and last points, and central differences for the middle points to ensure greater accuracy. The average curvature is computed across all six points to quantify the tortuosity of the descending aorta. This refined approach minimizes evaluator variability and provides more objective and reproducible assessment of aortic tortuosity. Inspiration The criterion has been refined to reduce ambiguity and ensure consistency in measurements between individuals, as the previous criterion lacked clear reference point, which could result in variations in interpretation. Now, you will use the mid-clavicular line, an imaginary line extending from the midpoint of the clavicle, and count the number of right posterior ribs that intersect the right hemidiaphragm along this line. This refinement creates more structured and consistent approach for determining the inspiration level. Projection The previous method for determining chest X-ray projection (PA or AP) relied on visually assessing scapular retraction or overlap with the lung fields. While this approach reflects the correct fundamental concept that scapular positioning is indicative of projection it lacked clear thresholds, leading to variability in interpretation among individuals. To reduce this ambiguity and enhance measurement consistency between evaluators, the criterion has been refined. The refined method maintains the original diagnostic logic but introduces more structured process: measuring the ratio of the overlapping area between the scapula and the lung to the total scapular area for both the right and left scapula. This update ensures more objective and standardized application of the same underlying principle. Stage 2. Anatomical Structure Identification Depending on whether the criteria are standardized and the number of anatomical structures involved, the question is phrased accordingly. For the anatomical structures involved in each task, refer to Figure 18. Prefix for the questions: In the following images, either segmentation mask of specific body part or reference line is shown. Standardized, Quantifiable Criteria, Single Anatomical Structure Based on the selected criterion, select the image that include the relevant body part or reference line required for applying that criterion. Standardized, Quantifiable Criteria, Anatomical Multiple Structures Based on the selected criterion, select all images that include the relevant body part or reference line required for applying that criterion. Expert-Defined Criteria, Single Anatomical Structure Based on the refined criterion, select the image that include the relevant body part or reference line required for applying that criterion. Expert-Defined Criteria, Multiple Anatomical Structures Based on the refined criterion, select all images that include the relevant body part or reference line required for applying that criterion. Stage 3. Measurement or Recognition Depending on the task type, the model either computes diagnostic indices from anatomical measurements (measurement-type), or interprets anatomical patterns and positional changes to assign label (recognition-type). Each task type has distinct format for both question and answer options, as follows: Measurement-type tasks: The model calculates diagnostic index from anatomical measurements and selects the appropriate value range (e.g., [0.500.52], [3050]). * Rotation Measure the horizontal distance (x-coordinate difference) from the medial end of each clavicle to the nearest point on the vertical line through the spinous processes. Then, compute the ratio of the shorter distance to the longer distance. Round the result to two decimal places, and determine which range the measured value falls into and select the correct option. * Projection Use the refined criterion outlined earlier to measure the ratio. Round the ratio to two decimal places, then determine which range it falls into for both the right and left sides, and select the correct option. * Cardiomegaly Use the criterion you selected to make the decision for the first question to measure the ratio. Round the ratio to two decimal places, then determine which range it falls into and select the correct option. * Mediastinal Widening Use the refined criterion outlined earlier to measure the ratio. Round the ratio to two decimal places, then determine which range it falls into and select the correct option. * Carina Angle Then, use the criterion you selected to make the decision for the first question to measure the angle. Round the value to the nearest whole number, then determine which range it falls into and select the correct option. * Aortic Knob Enlargement Use the refined criterion outlined earlier to measure the ratio. Round the ratio to two decimal places, then determine which range it falls into and select the correct option. * Descending Aorta Enlargement Use the refined criterion outlined earlier to measure the ratio. Round the ratio to two decimal places, then determine which range it falls into and select the correct option. * Descending Aorta Tortuous Use the refined criterion outlined earlier to measure the ratio. Round the ratio to four decimal places, then determine which range it falls into and select the correct option. Recognition-type tasks: The model interprets anatomical patterns or positional changes and selects the appropriate label. * Inclusion Use the criterion you selected to assess which parts of the thoracic cage are included or excluded, and select the correct option (e.g., Right apex: Included, Left apex: Included, Right rib edge: Included, Left rib edge: Included, Right costophrenic angle: Included, Left costophrenic angle: Included,, Right apex: Excluded, Left apex: Included, Right rib edge: Included, Left rib edge: Excluded, Right costophrenic angle: Excluded, Left costophrenic angle: Excluded,). * Inspiration Use the refined criterion outlined earlier to count the rib number and select the correct option (e.g., 7th rib, 8th rib) * Trachea Deviation Use the criterion you selected to assess the deviation and select the correct option (e.g., Deviated to the right, Deviated to the left, Not deviated). * Ascending Aorta Enlargement Use the refined criterion outlined earlier to assess whether the ascending aorta extends beyond the line. Select the appropriate label based on your measurement (e.g., Extend beyond the line, Does not extend beyond the line). Stage 4. Final Decision Depending on whether standardized threshold is available, the question is divided as follows: For tasks with standardized thresholds: The model must rely on commonly known diagnostic thresholds. * Cardiomegaly (PA view) Does this patient have cardiomegaly? * Trachea Deviation Is the trachea deviated in this chest X-ray? * Inclusion Is the entire thoracic cage - including the lung apices, inner margins of the lateral ribs, and costophrenic angles (CPAs) fully visible in this chest X-ray without being cropped? For other tasks with expert-defined thresholds: The model is provided with thresholds defined by clinical experts to guide diagnostic decisions, but specific values are omitted here for brevity. * Cardiomegaly (AP view) Does this patient have cardiomegaly? * Mediastinal Widening Does this patient have mediastinal widening? * Carina Angle Does this chest X-ray show normal carina angle? * Aortic Knob Enlargement Does the aortic knob appear enlarged in the chest X-ray? * Descending Aorta Enlargement Does the descending aorta appear enlarged in the chest X-ray? * Descending Aorta Tortuous Is the descending aorta tortuous in the chest X-ray? * Inspiration What is the inspiration level of the chest X-ray? * Ascending Aorta Enlargement Does the ascending aorta appear enlarged in the chest X-ray? * Rotation Was the patient rotated during the chest X-ray? * Projection Identify the view of the chest X-ray. 35 Figure 18: Visualization of anatomical structures required for each diagnostic task in Stage 2 of Path 1 and Stage 1 of Path 2. 36 B.3.2 Path 2: Guided Reasoning and Re-evaluation The following outlines the questions and answers for each task, organized by stage. Stage 1. Anatomical Structure Identification The following is the question format used in this stage. The placeholder {anatomical structure} is replaced with the anatomical structure name involved in each diagnostic task. Refer to Figure 18 for the list of anatomical structures associated with each task. Among the following images, each image either contains segmentation mask highlighting specific body part or reference line necessary for decision. Which image represents {anatomical structure}? Continuing from the previous question, which image corresponds to {anatomical structure}? Stage 2. Guided Measurement or Recognition For each diagnostic task, the model receives chest X-ray image with detailed visual annotations (e.g., segmentation masks, reference lines, landmarks) and task-specific instruction. Figure 19 shows examples of the visual annotations used for each task. Instructions for each diagnostic task are shown below. Inclusion To check whether the entire thoracic cage is shown in the chest X-ray, look to see if three important parts are visible: the tops of the lungs (lung apices), the inner edges of the side ribs, and the costophrenic angles (CPAs), which are the corners at the bottom of the lungs. In the image, colored point has been placed at each of these areas: red for the lung apices, green for the inner rib edges, and blue for the CPAs. Keep in mind: the point doesnt always mark the exact part of the body. If the part is visible in the image, the point shows its actual location. If its missing from the image (for example, if the top of the lung is cut off), the point just shows the general level where it should have appeared. If part is excluded, that means the body part isnt visible at the points location - it was cut off in the X-ray. Examine each point and decide whether the corresponding body part is visible in the image. Inspiration To assess the level of inspiration, draw an imaginary vertical line from the midpoint of the clavicle (mid-clavicular line). Then, count how many right posterior ribs intersect the right hemidiaphragm along this line. In the provided image, the mid-clavicular line and the right posterior rib intersecting the right hemidiaphragm along the line are marked. Look at the image and count which rib is intersecting. Rotation To assess the rotation of the patient during an X-ray, check whether the spinous processes are equidistant from the medial ends of the clavicles. In the provided image, the points corresponding to the medial ends of the clavicle and their coordinate values are marked. Additionally, straight line representing the spinous processes is given, with its slope and intercept provided. This line is defined in way that for any given y-coordinate, the corresponding x-coordinate can be determined using the slope and intercept. To measure the rotation: 1. For each medial clavicle point, use its y-coordinate to determine the corresponding x-coordinate on the spinous process line. 2. Compute the difference between this x-coordinate and the original x-coordinate of the medial clavicle point to obtain the distance. 3. Compare the two distances and determine the ratio of the shorter distance to the longer distance. 4. Round the result to two decimal places, check which range it falls into, and select the correct option. Projection To assess the projection, check whether the scapulae are laterally retracted or overlapping with the lung fields. In the provided image, segmentation masks for the left and right scapulae are drawn. The overlapping regions between the scapulae and the lung fields are highlighted in purple, while the remaining scapular regions are marked in red. Each mask also displays numerical values indicating the overlapping area and the total scapular area. If there is no overlapping region, no purple markings are displayed. To determine whether the scapulae are retracted or overlapping, calculate the ratio of the overlapping area between the scapula and the lung to the total scapular area for both the right and left scapula. Round the ratio to two decimal places, then determine which range it falls into for both the right and left sides, and select the correct option, choosing one option for each. Cardiomegaly To assess cardiomegaly, calculate the cardiothoracic ratio, which is the ratio of the maximal horizontal cardiac diameter to the maximal horizontal thoracic diameter. In the provided image, the x-coordinates for measuring cardiac width and thoracic width are marked, 37 along with lines representing both measurements. The coordinates and lines associated with the heart are highlighted in red, while those related to the lungs are highlighted in blue. Round the ratio to two decimal places, then determine which range it falls into and select the correct option. Mediastinal Widening To assess mediastinal widening, measure the mediastinal width and the thoracic width at the same level, then calculate their ratio by dividing the mediastinal width by the thoracic width. In the provided image, the x-coordinates for measuring mediastinal width and thoracic width are marked, along with lines representing both measurements. The coordinates and lines associated with the mediastinum are highlighted in red, while those related to the lungs are highlighted in blue. Round the ratio to two decimal places, then determine which range it falls into and select the correct option. Carina Angle To assess whether the carina angle is normal, measure the angle between the left and right main bronchi. In the provided image, the central point at the carina is marked as B, the right main bronchus as A, and the left main bronchus as C, with their respective coordinate values also indicated. Use these points to determine the angle formed between the two bronchi. Round the value to the nearest whole number, then determine which range it falls into and select the correct option. Trachea Deviation To assess tracheal deviation, use the spinous processes as reference to draw an imaginary straight line down the center of the vertebral bodies and evaluate whether the trachea aligns with this line. In the provided image, the trachea segmentation mask and nine points along the line are marked. For each point, determine whether the trachea is on, deviated toward the left (left lung side), or deviated toward the right (right lung side) of the point. The final label is determined by majority vote. If multiple labels share the highest count, assign the label based on the order in which the majority count was first reached. Based on your assessment, select the appropriate option. Aortic knob Enlargement To assess aortic knob enlargement, measure the maximum width of the aortic knob and the median width of the trachea, then calculate their ratio by dividing the aortic knob width by the trachea width. In the provided image, the x-coordinates for measuring the trachea and aortic knob widths are marked, along with lines representing both measurements. The coordinates and lines associated with the aortic knob are highlighted in red, while those related to the trachea are highlighted in blue. Round the ratio to two decimal places, then determine the corresponding range and select the appropriate option. Ascending Aorta Enlargement To assess ascending aorta enlargement, determine whether the ascending aorta extends beyond an imaginary straight line connecting the inner boundary of the right lung and the right heart border. In the provided image, the ascending aorta segmentation mask and this reference line are marked. Select the appropriate label based on your measurement. Descending Aorta Enlargement To assess descending aorta enlargement, measure the maximum width of the descending aorta and the median width of the trachea, then calculate their ratio by dividing the descending aorta width by the trachea width. In the provided image, the xcoordinates for measuring the trachea and descending aorta widths are marked, along with lines representing both measurements. The coordinates and lines associated with the descending aorta are highlighted in red, while those related to the trachea are highlighted in blue. Round the ratio to two decimal places, then determine the corresponding range and select the appropriate option. Descending Aorta Tortuous To assess descending aorta tortuosity, focus on the thoracic portion of the descending aorta, specifically the region at the upper part of the heart. Divide this region into five equal sections and determine the coordinates at the top-left lung side of each division, resulting in six total coordinates. Calculate the curvature at each of these six coordinates using finite difference methods: - The first and last points use forward and backward differences, respectively. - The middle points use central differences for higher accuracy. Compute the average curvature across all six points to quantify tortuosity. In the provided images, these six coordinates are marked. Round the ratio to four decimal places, then determine which range it falls into and select the correct option. Stage 3. Final Decision The same diagnostic question format used in Stage 4 of Path 1 is applied here. However, unlike Path 1, threshold is provided for all tasks, regardless of whether standardized threshold is available. 38 Figure 19: Visualization of the detailed visual annotations provided per diagnostic task in Stage 2 of Path 2. 39 B.4 Benchmark Structure CXReasonBench employs multiple-choice format with both single-choice (e.g., one diagnostic criterion) and multi-choice questions (e.g., all relevant anatomical structures) depending on the stage. B.4.1 Evaluation Formats: Basic and Two-Round with Need new option Except for Path 2, certain stages adopt either basic or two-round evaluation format, depending on whether the correct option is included in the initial round. The two-round format is enabled by including Need new option choice. This mechanism assesses whether the model can recognize that its current reasoning paths are insufficient and adapt to more clinically valid alternatives. Stage 1. Diagnostic Criterion Selection Basic Format: The correct diagnostic criterion is included in the initial options. The model selects one and proceeds. Two-Round Format: The correct option is intentionally excluded. Need new option choice is included instead. If selected, second round presents the correct option. valid second-round response is required to proceed. Stage 2. Anatomical Structures Identification The format varies depending on how many anatomical structures are clinically required for the diagnostic criterion: Task with Single Anatomical Structure: Follows the same format as Stage 1 (e.g., Carina angle - carina). Task with Multiple Anatomical Structures: one of three formats is used (e.g., Cardiomegaly heart, lung): Basic Format: All correct anatomical structures are included in the initial options. Two-Round (Partial Inclusion): Only one correct anatomical structure is included in the initial round. The model must select both the included correct anatomical structure and \"Need new option\" to access the second round, where the remaining anatomical structure becomes available. Two-Round (None Included): Neither anatomical structure is included initially. The model must select \"Need new option\" to proceed to second round where all correct anatomical structures are presented. None of the above option When the Need new option is not included, None of the above option is provided instead. In this case, the correct answer is always included in the initial options. Selecting None of the above results in failure, and the model is expected to explain its reasoning."
        },
        {
            "title": "C Experimental Setup Details",
            "content": "C.1 Dataset Resources MIMIC-CXR-JPG [16] URL: https://physionet.org/content/mimic-cxr-jpg/ License: PhysioNet Credentialed Health Data License 1.5.0 CXAS [21] URL: https://github.com/ConstantinSeibold/ChestXRayAnatomySegmentation License: CC BY NC 4.0 CheXmask [10] URL: https://physionet.org/content/chexmask-cxr-segmentation-data/ License: CC BY 4.0 Chest ImaGenome [27] URL: https://physionet.org/content/chest-imagenome/1.0.0/ License: PhysioNet Credentialed Health Data License 1.5.0 40 C.2 Model Resources The models used in this paper are categorized into closed-source and open-source models. The closed-source models include Gemini-2.5-Pro, Gemini-2.5-Flash, and GPT-4.1, all of which are accessed through proprietary APIs and subject to the respective providers terms of service. The specific model versions are as follows: Gemini-2.5-Pro [22]: gemini-2.5-pro-preview-03-25 Gemini-2.5-Flash [22]: gemini-2.5-flash-preview-04-17 GPT-4.1 [1]: GPT-4.1 (2025-04-14) The open-source models were obtained from Hugging Face [26] and GitHub. For each model, the corresponding Hugging Face repository or GitHub link, along with its license, is listed below: Pixtral-Large-Instruct-2411 [2] Hugging Face Path: mistralai/Pixtral-Large-Instruct-2411 License: Apache-2.0 license Qwen2.5-VL-72B-Instruct [23] Hugging Face Path: Qwen/Qwen2.5-VL-72B-Instruct License: Qwen LICENSE AGREEMENT Llama-3.2-90b-Vision-Instruct [12] Hugging Face Path: meta-llama/Llama-3.2-90B-Vision-Instruct License: Meta Llama 3 Community License Agreement Pixtral 12B [2] Hugging Face Path: mistralai/Pixtral-12B-2409 License: Apache-2.0 license Qwen2.5-VL-7B-Instruct [23] Hugging Face Path: Qwen/Qwen2.5-VL-7B-Instruct License: Qwen LICENSE AGREEMENT HealthGPT-L14 [18] Hugging Face Path: lintw/HealthGPT-L14 License: MIT License RadVLM [9] GitHub: https://github.com/uzh-dqbm-cmi/RadVLM License: CC BY-NC 4. Compute Requirements Our evaluation pipeline comprises three paths: Path 1, Path 2, and Reevaluated Path 1. Each diagnostic task involves multi-stage reasoning process, where progression to later stages depends on correct responses from earlier ones. Due to this dependency, the total evaluation time per model varies. On average, evaluations with closed-source models required approximately 2 GPU-hours per diagnostic task. For open-source models, evaluation times varied: Pixtral-Large-Instruct-2411, Qwen2.5VL-72B-Instruct, and LLaVA-3.2-90B-Vision-Instruct required around 4 GPU-hours per task, while other models completed evaluations within 2 GPU-hours. Under stochastic sampling, we generated three responses per query, effectively doubling the evaluation time compared to greedy sampling. Thus, stochastic sampling generally requires twice as much computation time as greedy sampling. Experiments were conducted on the following hardware: Pixtral-Large-Instruct-2411, Qwen2.5-VL-72B-Instruct, and LLaVA-3.2-90B-Vision-Instruct were run on 4 A100 GPUs. 41 Pixtral-12B, Qwen2.5-VL-7B-Instruct, HealthGPT-L14, and RadVLM were run on 1-4 RTX A6000 GPUs, depending on the model size. To comply with the MIMIC Data Use Agreement, Gemini models are run on Vertex AI, the GPT model is deployed on Azures HIPAA-compliant platform, and open-source LVLMs are evaluated locally. C.3 Sampling Methods We adopt two sampling strategies as follows: Greedy Sampling is used to generate single, near-deterministic response by setting the temperature to zero, minimizing output randomness. Stochastic Sampling generates three distinct responses per question to evaluate model stability and robustness. This is achieved by configuring the temperature to 0.7 and the top-p parameter to 0.95, which approximates the models default behavior. The reliability of responses is assessed using majority voting, where response is considered correct if at least two out of the three generated responses match the ground truth. C.4 Evaluation Metric Details As mentioned in Section 5.1, all evaluation metrics, except for Average Reasoning Depth, are computed using the Wilson score [25], which accounts for the number of attempts to enable fairer comparisons across models with varying attempt counts at each stage. The following section introduces an additional evaluation metric, the Stage-wise Score, to assess model performance at individual stages, followed by detailed explanation of the Wilson score and our adjusted Wilson score used across all proportion-based metrics. C.4.1 Additional Evaluation Metric The Stage-wise Score metric measures the proportion of correct responses at each stage. For each stage, this metric is calculated as the number of correct responses divided by the number of attempts, using the adjusted Wilson score (described below) to account for varying attempt counts and penalize uncertainty from small sample sizes. This metric evaluates the models ability to meet stage-specific requirements, such as selecting diagnostic criteria or identifying anatomical structures. C.4.2 Wilson Score The Wilson score interval is statistical method used to estimate the confidence interval for proportion, such as the success rate of model in binary outcome (e.g., success or failure in completing task). Unlike simple proportion (e.g., number of successes divided by total attempts), the Wilson score accounts for sample size variability and provides more robust estimate. For given success proportion = attempts, the Wilson score interval is calculated as follows: , where is the number of successes and is the number of Wilson score = ˆp + z2 2n 1 + z2 Lower bound = Upper bound = ˆp + z2 2n (cid:113) ˆp(1 ˆp) + z2 4n2 1 + z2 ˆp + z2 2n + (cid:113) ˆp(1 ˆp) + z2 4n2 1 + z2 (1) (2) (3) Here, ˆp = is the observed proportion, is the z-score corresponding to the desired confidence level (e.g., = 1.96 for 95% confidence interval), and is the total number of attempts. The Wilson 42 score (the midpoint of the interval) provides point estimate that adjusts for sample size, while the lower and upper bounds define the confidence interval. This method is particularly valuable in our evaluation because it allows fairer comparisons across models with different numbers of attempts at each stage of the reasoning process. For instance, model with fewer attempts might have higher simple proportion but greater uncertainty, which the Wilson score accounts for by penalizing small sample sizes. C.4.3 Adjusted Wilson Score To simplify the interpretation of our metrics, we modify the original Wilson score by incorporating the width of the confidence interval as penalty term, resulting in an adjusted Wilson score. This adjustment addresses the challenge of analyzing results with both point estimate (the Wilson score) and confidence interval, providing single, interpretable metric that balances performance with estimate reliability. The adjusted Wilson score is calculated as: Adjusted Wilson score = Wilson score 1 (cid:18) Upper bound Lower bound 2 (cid:19) (4) The term Upper boundLower bound represents half the width of the confidence interval, reflecting the uncertainty in the estimate. wider interval (indicating greater uncertainty, often due to smaller sample size) results in larger penalty, reducing the adjusted score. Conversely, narrower interval (indicating lower uncertainty) applies smaller penalty, preserving more of the original Wilson score. 2 The adjusted Wilson score is applied to our metrics (Stage-wise Score, Final Stage Completion, Final Stage Completion, Decision Alignment, and Measurement Consistency), except for Average Reasoning Depth, which is simple average and does not involve proportions. For each metric, we first compute the Wilson score and its confidence interval based on the number of successes and attempts at the relevant stage. We then apply the adjustment to produce single score that reflects both the models performance and the reliability of the estimate."
        },
        {
            "title": "D Additional Results",
            "content": "Qualitative Analysis of Measurement Consistency Results In Section 5.2, we concluded that the primary limitation of current LVLMs lies not in their ability to maintain coherence across reasoning stages, but in their tendency to rely on visual cues or shortcuts when making diagnostic decisions, rather than applying diagnostic criteria through structured, measurement-based reasoning. This section provides qualitative evidence supporting that conclusion, focusing on the top-performing model, Gemini-2.5-Pro. As illustrated in Figures 20 and 21, the model often omits explicit calculations under Path 1. In many cases, it returns only value range in Stage 4, rather than concrete numeric value, indicating that no actual computation was conducted in Stage 3. closer inspection of the Stage 3 responses confirms this: the model frequently justifies its choices with heuristic statements such as By visually comparing... or The ratio appears to be less than 0.5. careful visual estimation suggests the cardiac width is roughly 4446% of the thoracic width. These examples demonstrate that even the best-performing model tends to rely on visual estimation, rather than systematically applying diagnostic criteria. Moreover, as shown in Figures 22 and 23, even when the returned value in Stage 4 falls within the value range selected in Stage 3, the underlying reasoning remains heuristic. For example, the model states: My visual estimation places the angle within this range, perhaps towards the upper end, around 6575 degrees. In contrast, as shown in Figures 24 and 25, under Path 2, where explicit measurement instructions and visual landmarks are provided, the model demonstrates consistent and structured behavior. The returned value in the final stage reliably falls within the range selected in the previous stage. More importantly, the Stage 2 responses reveal that the model explicitly follows instructions, performs concrete computations based on visual landmarks, and selects value range grounded in the calculated result. These findings highlight critical weakness in current LVLMs: although their diagnostic decisions may appear reasonable, they often fail to engage with the diagnostic criteria necessary to support those conclusions, frequently resorting to visual estimation and heuristic reasoning instead of performing the required measurements. As result, their outputs, while superficially plausible, lack the structured, criteria-driven reasoning expected in clinical decision-making. This underscores the need for future research to enable LVLMs to internalize and autonomously apply diagnostic criteria through structured, measurement-based reasoning. Decision Alignment Analysis As shown in the Alignment columns of Tables 11, 12, and13 for Path 1 results, and Tables 23, 24, and25 for re-evaluated Path 1 results, all models consistently show higher decision alignment in recognition-type tasks than in measurement-type tasks. This trend stems from the qualitative nature of recognition tasks, which depend on visual pattern recognition rather than precise numerical calculations. Recognition tasks typically involve simple, discrete labels (e.g., deviated, included, or excluded), leading to clearer decision boundaries and reducing ambiguity during reasoning. In contrast, measurement-type tasks require accurate anatomical measurements and comparisons against specific thresholds (e.g., calculating CTR), increasing the likelihood of divergence between initial and final decisions. The discrete and visually intuitive nature of recognition tasks makes them less susceptible to misinterpretation, contributing to more consistent alignment across stages. Sampling Method Analysis Tables 8, 9, and 10 present stochastic sampling results, while Tables 3 and 4 present greedy sampling results. In the Path 1 evaluation, closed-source models demonstrate robust performance across all stages under both greedy and stochastic sampling. In contrast, opensource models show minimal performance variation in Stage 1 but substantial variability in other stages (See Tables 11 and 14). Stage 1 involves selecting diagnostic criteria based solely on textual input, requiring no visual grounding. This likely contributes to the stable performance of open-source models at this stage, regardless of sampling strategy. However, Stages 2 (anatomical structure identification), 3 (measurement or recognition), and 4 (final decision) require visual grounding, application of diagnostic criteria, and retention of prior reasoning steps. Performance in these stages fluctuates considerably for open-source models under stochastic sampling, with some responses improving while others decline. This instability suggests that open-source models possess less certain or robust multimodal diagnostic reasoning capabilities, especially when visual processing and multi-step reasoning are involved. In contrast, closed-source models maintain consistent performance across all stages of Path 1 and show similarly stable results across corresponding stages in Path 2. Table 8: Path 1 evaluation results with stochastic sampling for overall tasks. Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Models Completion (0-100) Depth (0-4) Consistency (0-100) Alignment (0-100) Gemini-2.5-Pro Gemini-2.5-Flash GPT-4. Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 17.26 (16.72) 11.66 (8.51) 8.69 (8.69) 5.08 (4.04) 0.3 (0.3) 2.7 (2.7) 0.0 (0.0) 0.0 (0.0) 0.48 (0.23) 0.29 (0.29) 1.99 (1.97) 1.37 (1.3) 1.15 (1.15) 1.09 (1.07) 0.57 (0.57) 0.4 (0.4) 0.38 (0.38) 0.36 (0.36) 0.28 (0.28) 0.22 (0.22) 70.15 47.56 55. 44.96 45.05 N/A 45.05 0.0 0.0 36.39 61.66 (59.62) 51.43 (39.54) 40.89 (40.89) 45.63 (37.84) 29.73 (29.73) 20.8 (20.8) 0.0 (0.0) 0.0 (0.0) 36.39 (18.2) 18.2 (18.2) 44 Figure 20: Qualitative example from the cardiomegaly task under Path 1, where the model returns the earlier selected value range in the final stage, rather than the specific value that had to be computed. Although the model selects correct range in Stage 3, it fails to perform an actual calculation and instead relies on visual estimation, as reflected in the heuristic reasoning shown. Table 9: Path 2 evaluation results with stochastic sampling for overall tasks. Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 3 matches the Stage 2 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Models Completion (0-100) Depth (0-3) Consistency (0-100) Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 49.12 (46.56) 32.44 (28.08) 22.14 (21.7) 26.2 (23.53) 2.58 (0.7) 10.7 (6.41) 7.92 (2.5) 4.56 (3.32) 0.0 (0.0) 3.43 (1.02) 45 2.12 (2.07) 1.61 (1.52) 1.19 (1.18) 1.44 (1.38) 0.12 (0.09) 0.64 (0.55) 0.46 (0.33) 0.37 (0.35) 0.04 (0.04) 0.08 (0.05) 73.44 59.73 71.93 60.53 11.89 37.52 24.26 34.06 N/A 12.13 Figure 21: Qualitative example from the carina angle task under Path 1, where the model returns the earlier selected value range in the final stage, rather than the specific value that had to be computed. The model chooses range based on visual heuristics and does not return concrete numeric value, indicating the absence of true measurement-based reasoning. Figure 22: Qualitative example from the cardiomegaly task under Path 1, where the returned value matches the selected range but is still based on visual approximation. Even though consistency is observed in the outputs, the reasoning lacks structured application of diagnostic criteria. 46 Figure 23: Qualitative example from the carina angle task under Path 1 showing apparent consistency. The models output falls within the selected range, but the reasoning is again heuristic rather than computational. Figure 24: Qualitative example from the cardiomegaly task under Path 2, where the model follows the given instructions and performs an explicit computation. The final-stage output reflects the calculated value, which aligns with the previously selected range. 47 Figure 25: Qualitative example from the aortic knob enlargement task under Path 2, demonstrating structured measurement-based reasoning. The model applies the instructed computation using visual landmarks and selects the value range accordingly. Table 10: Re-evaluated Path 1 result with stochastic sampling for overall tasks. Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Models Completion (0-100) Depth (0-4) Consistency (0-100) Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 0.0 (0.0) 0.0 (0.0) 2.72 (2.72) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 48 1.93 (1.86) 1.24 (1.21) 1.48 (1.48) 1.81 (1.66) 0.4 (0.4) 1.1 (1.1) 1.0 (1.0) 0.7 (0.7) N/A (N/A) 0.0 (0.0) 49.82 43.97 48.39 0.0 N/A N/A N/A N/A N/A N/A Table 11: Path 1 evaluation results with greedy sampling for overall, measurement-type, and recognition-type tasks, respectively. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage Stage 4 Completion Depth Consistency Alignment Stage-wise Score Overall Measurement Recognition Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 92.91 90.03 85.34 85.9 72.24 76.2 38.9 47.54 40.82 25.12 92.0 91.05 85.44 88.34 69.67 73.5 38.56 47.73 42.4 23.02 94.72 87.99 85.16 81.01 77.38 81.6 39.58 47.16 37.67 29.31 56.03 41.61 28.46 19.61 4.07 14.14 2.04 4.98 5.82 5.94 59.8 52.26 30.07 24.4 3.93 14.8 0.73 4.09 4.33 5.83 49.42 25.64 25.63 12.42 4.23 13.05 4.65 6.54 7.67 6. 30.18 (29.08) 35.6 (22.81) 25.98 (25.98) 28.56 (17.66) 10.49 (10.49) 22.73 (17.79) 31.56 (31.56) 17.69 (10.15) 34.8 (34.8) 24.56 (14.06) 21.28 (19.55) 36.79 (17.6) 30.06 (30.06) 30.91 (12.74) 15.74 (15.74) 26.59 (18.69) 36.39 (36.39) 15.21 (2.65) 16.13 (16.13) 21.01 (5.25) 71.58 (68.51) 64.4 (43.98) 52.3 (52.3) 37.02 (25.37) 37.52 (37.52) 43.2 (39.12) 0.0 (0.0) 45.44 (28.35) 56.8 (56.8) 32.5 (20.32) 67.26 (62.14) 66.74 (36.12) 48.0 (48.0) 32.27 (14.8) 37.52 (37.52) 40.61 (34.48) 0.0 (0.0) 33.06 (7.43) 36.39 (36.39) 32.5 (8.13) 17.03 (16.24) 12.83 (8.56) 8.32 (8.32) 3.73 (2.31) 0.38 (0.38) 2.34 (2.12) 0.0 (0.0) 1.21 (0.87) 1.27 (1.27) 0.64 (0.39) 13.44 (12.21) 14.66 (8.25) 8.68 (8.68) 3.89 (1.76) 0.57 (0.57) 2.48 (2.16) 0.0 (0.0) 0.68 (0.17) 0.34 (0.34) 0.51 (0.13) 1.96 (1.95) 1.4 (1.31) 1.15 (1.15) 1.0 (0.96) 0.53 (0.53) 0.67 (0.67) 0.3 (0.3) 0.49 (0.48) 0.32 (0.32) 0.28 (0.28) 1.92 (1.89) 1.45 (1.32) 1.07 (1.07) 0.98 (0.93) 0.4 (0.4) 0.59 (0.58) 0.24 (0.24) 0.43 (0.43) 0.26 (0.26) 0.24 (0.24) 45.74 33.23 19.85 25.04 0.0 16.29 26.74 21.4 53.48 31.67 78.05 59.71 60.9 46.52 N/A 48.39 0.0 70.19 77.22 32.5 23.3 9.16 7.61 3.41 0.0 2.05 0.0 2.26 3.14 0.91 2.04 1.28 1.3 1.03 0.78 0.83 0.42 0.59 0.45 0. 68.4 43.76 61.22 28.5 61.27 34.67 36.39 14.86 36.39 25.07 68.4 43.76 61.22 28.5 61.27 34.67 36.39 14.86 36.39 25.07 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 60.88 (58.61) 50.29 (37.67) 39.8 (39.8) 36.74 (25.3) 23.32 (23.32) 38.45 (34.36) 0.0 (0.0) 45.44 (28.35) 33.87 (33.87) 32.5 (20.32) 49.71 (45.93) 48.72 (29.8) 36.98 (36.98) 31.86 (14.68) 23.32 (23.32) 37.31 (31.18) 0.0 (0.0) 33.06 (7.43) 0.0 (0.0) 32.5 (8.13) 77.63 53.42 45.46 46.52 N/A 40.73 0.0 70.19 67.74 32. 49 Table 12: Path 1 evaluation results with greedy sampling on recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage Stage 3 Stage 4 Completion Depth Consistency Alignment Stage-wise Score Trachea Deviation Inclusion Inspiration Ascending Aorta Enlargement Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 93.12 96.33 78.72 85.52 76.82 90.4 49.57 77.86 41.72 42. 96.33 94.63 96.33 81.05 87.89 96.33 67.87 52.22 45.19 34.85 96.33 89.13 96.3 86.69 72.76 82.14 0.0 6.36 21.18 8.38 93.12 71.86 69.27 70.78 72.03 57.51 40.86 52.22 42.59 31.43 47.81 32.29 31.36 24.65 0.0 6.61 0.0 3.19 0.0 0.0 89.13 65.66 59.44 6.15 16.93 25.46 13.95 22.97 30.68 24.64 46.94 0.0 4.61 8.7 0.0 7.08 N/A 0.0 0.0 0. 13.79 4.59 7.1 10.19 0.0 N/A 0.0 0.0 0.0 0.0 46.57 30.46 23.1 12.43 N/A 37.52 N/A 0.0 N/A N/A 53.85 32.83 29.22 32.5 0.0 11.36 26.74 42.81 53.48 31.67 26.67 N/A 27.09 0.0 N/A 0.0 N/A N/A N/A N/A 55.89 36.39 0.0 55.25 N/A N/A N/A N/A N/A N/A 88.3 77.22 67.71 45.05 N/A 51.73 N/A N/A N/A N/A 69.09 65.53 78.61 29.73 N/A 45.05 0.0 70.19 77.22 32.5 80.65 N/A 36.39 N/A N/A N/A N/A N/A N/A N/A 74.17 36.39 N/A 64.77 N/A N/A N/A N/A N/A N/A 26.31 12.55 8.22 3.65 0.0 4.56 0.0 0.0 0.0 0.0 40.86 20.33 19.47 2.74 0.0 3.65 0.0 9.03 12.55 3.65 15.16 0.0 2.76 0.0 0.0 0.0 0.0 0.0 0.0 0. 10.89 3.76 0.0 7.26 0.0 0.0 0.0 0.0 0.0 0.0 2.06 1.59 1.29 1.2 0.84 1.07 0.55 0.86 0.46 0.47 2.97 2.18 2.07 0.95 1.1 1.31 0.88 0.88 0.91 0.54 1.82 0.5 1.05 1.0 0.7 0.94 0.0 0.05 0.22 0.07 1.31 0.85 0.81 0.97 0.49 0.0 0.26 0.58 0.21 0.33 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 88.3 77.22 67.71 45.05 N/A 51.73 N/A N/A N/A N/A 67.41 46.64 32.27 29.73 N/A 29.73 0.0 70.19 67.74 32.5 80.65 N/A 36.39 N/A N/A N/A N/A N/A N/A N/A 74.17 36.39 N/A 64.77 N/A N/A N/A N/A N/A N/A 50 Table 13: Path 1 evaluation results with greedy sampling on measurement-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage Stage 2 Stage 3 Stage 4 Completion Depth Consistency Alignment Stage-wise Score Cardiomegaly Carina Angle Rotation Mediastinal Widening Projection Aortic Knob Enlargement Descending Aorta Enlargement Descending Aorta Tortuous Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 94.41 91.57 93.12 96.33 96.23 87.92 54.01 66.91 45.19 26.31 96.33 96.3 96.15 94.63 87.89 90.53 48.69 75.79 44.32 13.5 96.33 96.33 96.26 93.12 74.77 94.63 22.9 15.16 45.19 9.03 81.05 94.63 88.06 77.86 81.29 96.33 40.0 64.07 45.19 24.61 96.33 87.89 82.94 79.97 33.14 15.16 8.15 14.75 25.46 2.74 96.33 94.63 77.66 89.13 50.61 71.76 46.06 74.77 45.19 36. 78.91 81.51 64.08 81.05 50.31 57.41 46.06 61.28 43.45 35.71 96.33 85.52 85.25 94.63 83.09 74.22 42.59 9.12 45.19 35.71 64.01 (59.43) 42.81 (30.58) 0.0 (0.0) 52.65 (40.95) N/A (N/A) 36.39 (36.39) 0.0 (0.0) 36.39 (0.0) N/A (N/A) N/A (N/A) 67.08 (57.49) 86.33 (81.54) 84.51 (84.51) 28.68 (8.2) 37.52 (37.52) 64.77 (64.77) N/A (N/A) 29.73 (14.86) 36.39 (36.39) N/A (N/A) 77.78 (66.11) 67.74 (39.51) 72.32 (72.32) 42.24 (10.56) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 55.25 (55.25) 55.25 (7.89) 58.44 (58.44) 42.24 (10.56) N/A (N/A) 61.27 (36.76) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.37 (36.37) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 82.42 (77.57) 77.3 (26.76) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 57.01 (57.01) 71.0 (30.43) 36.39 (36.39) 27.82 (18.55) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 32.5 (8.13) 13.02 (12.09) 9.2 (6.57) 0.0 (0.0) 8.15 (6.34) 0.0 (0.0) 3.45 (3.45) 0.0 (0.0) 2.74 (0.0) 0.0 (0.0) 0.0 (0.0) 26.31 (22.56) 32.58 (30.77) 37.38 (37.38) 9.92 (2.83) 4.56 (4.56) 9.88 (9.88) 0.0 (0.0) 2.74 (1.37) 2.74 (2.74) 0.0 (0.0) 18.61 (15.82) 11.67 (6.81) 10.1 (10.1) 4.56 (1.14) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 7.53 (7.53) 11.28 (1.61) 10.08 (10.08) 4.56 (1.14) 0.0 (0.0) 6.54 (3.93) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 4.71 (4.71) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 19.71 (18.55) 27.28 (9.44) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 8.92 (8.92) 25.27 (10.83) 7.16 (7.16) 3.95 (2.63) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 4.07 (1.02) 2.07 (2.05) 1.82 (1.76) 1.31 (1.31) 1.4 (1.36) 1.0 (1.0) 1.1 (1.1) 0.62 (0.62) 0.78 (0.76) 0.5 (0.5) 0.28 (0.28) 2.49 (2.43) 2.69 (2.65) 2.59 (2.59) 2.06 (1.84) 1.15 (1.15) 1.27 (1.27) 0.54 (0.54) 1.03 (1.02) 0.62 (0.62) 0.16 (0.16) 2.11 (2.06) 1.77 (1.68) 1.52 (1.52) 1.26 (1.21) 0.85 (0.85) 1.14 (1.12) 0.24 (0.24) 0.15 (0.15) 0.5 (0.5) 0.08 (0.08) 1.3 (1.3) 1.39 (1.21) 1.35 (1.35) 1.02 (0.97) 0.21 (0.21) 1.16 (1.12) 0.44 (0.44) 0.71 (0.71) 0.45 (0.45) 0.2 (0.2) 2.0 (2.0) 0.0 (0.0) 1.36 (1.36) 0.0 (0.0) 0.02 (0.02) 0.06 (0.06) 0.05 (0.05) 0.04 (0.04) 0.01 (0.01) 0.01 (0.01) 2.06 (2.04) 2.13 (1.74) 0.17 (0.17) 1.02 (1.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.4 (0.4) 0.0 (0.0) 0.4 (0.4) 1.38 (1.38) 1.83 (1.52) 0.26 (0.26) 1.07 (1.03) 0.0 (0.0) 0.0 (0.0) 0.06 (0.06) 0.35 (0.35) 0.0 (0.0) 0.36 (0.36) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.45 (0.4) 71.0 52.47 45.05 52.65 N/A 36.39 36.39 0.0 N/A N/A 72.68 82.67 91.62 26.2 61.27 64.77 N/A 29.73 36.39 N/A 67.14 42.59 72.32 25.07 N/A 0.0 N/A N/A N/A N/A 67.71 20.37 70.19 25.07 N/A 37.52 N/A N/A N/A N/A N/A N/A 51.73 N/A N/A N/A N/A N/A N/A N/A 74.83 30.31 N/A 0.0 N/A N/A N/A N/A N/A N/A 57.01 34.18 36.39 42.02 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 25.07 64.01 (59.43) 42.81 (30.58) 0.0 (0.0) 52.65 (40.95) N/A (N/A) 36.39 (36.39) 0.0 (0.0) 36.39 (0.0) N/A (N/A) N/A (N/A) 64.42 (55.22) 82.67 (78.08) 71.0 (71.0) 26.2 (7.49) 23.32 (23.32) 51.57 (51.57) N/A (N/A) 29.73 (14.86) 0.0 (0.0) N/A (N/A) 39.96 (33.97) 67.74 (39.51) 72.32 (72.32) 42.24 (10.56) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 38.47 (38.47) 31.97 (4.57) 42.16 (42.16) 42.24 (10.56) N/A (N/A) 61.27 (36.76) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.37 (36.37) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 49.17 (46.27) 32.95 (11.41) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 42.24 (42.24) 34.18 (14.65) 0.0 (0.0) 27.82 (18.55) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 32.5 (8.13) 74.66 57.63 25.57 22.9 0.0 14.12 4.38 4.82 0.0 0.0 78.91 91.65 67.42 63.76 14.18 13.8 0.0 19.92 21.66 25.39 65.01 48.69 31.13 20.7 5.48 13.54 0.0 0.0 0.0 0.0 30.7 19.63 21.76 12.49 0.0 7.46 0.0 3.91 0.0 0.0 57.01 N/A 42.88 N/A 0.0 25.07 0.0 0.0 0.0 0.0 58.34 48.39 0.0 13.34 N/A N/A N/A 0.0 N/A 0. 53.99 47.54 21.76 13.18 N/A N/A 0.0 0.0 N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 21.23 17.81 (16.54) 22.01 (15.72) 18.03 (18.03) 32.14 (25.0) N/A (N/A) 17.03 (17.03) 36.39 (36.39) 29.73 (0.0) N/A (N/A) N/A (N/A) 36.92 (31.64) 34.37 (32.46) 50.93 (50.93) 35.99 (10.28) 31.48 (31.48) 45.48 (45.48) N/A (N/A) 15.89 (7.95) 16.13 (16.13) 0.0 (0.0) 25.97 (22.08) 21.45 (12.51) 25.32 (25.32) 19.98 (5.0) 0.0 (0.0) 18.87 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 23.72 (23.72) 42.59 (6.08) 37.4 (37.4) 31.67 (7.92) N/A (N/A) 51.57 (30.94) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 12.29 (12.29) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 28.34 (26.67) 50.22 (17.38) N/A (N/A) 23.32 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 16.2 (16.2) 50.11 (21.48) 36.39 (36.39) 42.37 (28.25) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 42.02 (10.5) 51 Table 14: Path 1 evaluation results with stochstic sampling for overall, measurement-type, and recognition-type tasks, respectively. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Stage Completion Depth Consistency Alignment Stage-wise Score Overall Measurement Recognition Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 93.39 87.58 85.36 86.26 72.26 46.63 40.21 33.42 38.17 21. 92.14 86.64 85.87 86.35 71.63 38.39 39.31 34.99 38.6 19.21 95.9 89.46 84.34 86.08 73.53 59.0 41.79 30.28 37.32 26.26 56.37 41.12 30.64 17.03 1.48 13.38 5.63 5.49 2.48 5.27 60.23 51.32 31.5 20.49 1.48 N/A 5.56 5.53 2.02 6.28 49.62 25.82 29.14 10.98 1.48 13.38 5.81 5.44 3.29 3.26 30.06 (29.32) 32.29 (23.76) 25.17 (25.17) 25.9 (21.32) 14.95 (14.95) 49.17 (49.17) 14.61 (14.61) 9.91 (0.0) 22.54 (10.88) 17.39 (17.39) 20.42 (19.26) 32.76 (19.97) 24.39 (24.39) 28.15 (20.52) 29.9 (29.9) N/A (N/A) 12.12 (12.12) 14.86 (0.0) 23.32 (0.0) 13.54 (13.54) 71.83 (69.56) 62.29 (45.73) 53.41 (53.41) 49.47 (40.62) 29.73 (29.73) 51.2 (51.2) 0.0 (0.0) 0.0 (0.0) 36.39 (18.2) 18.2 (18.2) 65.74 (61.95) 65.81 (40.97) 52.07 (52.07) 52.68 (39.4) 29.73 (29.73) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 36.39 (0.0) 36.39 (36.39) 17.26 (16.72) 11.66 (8.51) 8.69 (8.69) 5.08 (4.04) 0.3 (0.3) 2.7 (2.7) 0.0 (0.0) 0.0 (0.0) 0.48 (0.23) 0.29 (0.29) 13.41 (12.56) 13.69 (8.97) 8.51 (8.51) 6.28 (4.66) 0.45 (0.45) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.37 (0.0) 0.44 (0.44) 1.99 (1.97) 1.37 (1.3) 1.15 (1.15) 1.09 (1.07) 0.57 (0.57) 0.4 (0.4) 0.38 (0.38) 0.36 (0.36) 0.28 (0.28) 0.22 (0.22) 1.95 (1.93) 1.38 (1.28) 1.05 (1.05) 1.11 (1.08) 0.48 (0.48) 0.0 (0.0) 0.33 (0.33) 0.37 (0.36) 0.24 (0.23) 0.18 (0.18) 46.93 31.34 26.34 22.53 0.0 49.17 22.06 0.0 21.76 25.07 80.98 55.23 55.43 43.05 N/A 51.2 0.0 N/A 36.39 0.0 24.0 7.59 9.06 2.96 0.0 5.4 0.0 0.0 0.68 0.0 2.05 1.35 1.35 1.06 0.74 0.8 0.47 0.34 0.38 0.29 70.15 47.56 55.86 44.96 45.05 N/A 45.05 0.0 0.0 36. 70.15 47.56 55.86 44.96 45.05 N/A 45.05 0.0 0.0 36.39 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 61.66 (59.62) 51.43 (39.54) 40.89 (40.89) 45.63 (37.84) 29.73 (29.73) 20.8 (20.8) 0.0 (0.0) 0.0 (0.0) 36.39 (18.2) 18.2 (18.2) 49.82 (46.42) 50.69 (32.85) 37.48 (37.48) 46.92 (35.24) 29.73 (29.73) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 36.39 (0.0) 36.39 (36.39) 79.43 52.91 45.99 43.05 N/A 20.8 0.0 N/A 36.39 0.0 Table 15: Path 1 evaluation results with stochastic sampling on recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage Stage 4 Completion Depth Consistency Alignment Stage-wise Score Trachea Deviation Inclusion Inspiration Ascending Aorta Enlargement Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 96.33 96.33 77.45 90.4 74.9 48.48 48.71 40.79 38.28 29.33 96.33 94.63 96.3 90.4 90.03 90.59 74.31 31.36 46.06 35. 96.33 93.12 96.3 86.69 68.56 29.73 0.0 13.01 17.6 10.69 94.63 73.74 67.3 76.82 60.62 67.19 44.13 35.96 47.35 29.09 46.94 31.43 38.1 21.08 0.0 0.0 0.0 0.0 0.0 0.0 93.12 67.59 62.81 7.53 5.9 40.13 17.42 21.76 13.15 13.02 42.98 0.0 3.69 6.8 0.0 0.0 N/A 0.0 0.0 0.0 15.43 4.25 11.97 8.5 0.0 N/A 0.0 0.0 0.0 0. 49.75 31.16 28.37 13.71 N/A N/A N/A N/A N/A N/A 44.26 26.48 28.13 34.38 0.0 49.17 22.06 0.0 21.76 25.07 33.93 N/A 29.73 0.0 N/A N/A N/A N/A N/A N/A 59.79 36.39 19.13 42.02 N/A N/A N/A N/A N/A N/A 88.97 77.22 75.79 45.05 N/A N/A N/A N/A N/A N/A 74.55 52.09 73.16 27.09 N/A 51.2 0.0 N/A 36.39 0. 83.18 N/A 36.39 N/A N/A N/A N/A N/A N/A N/A 77.22 36.39 36.39 57.01 N/A N/A N/A N/A N/A N/A 28.02 12.55 11.89 3.65 0.0 0.0 0.0 0.0 0.0 0.0 37.42 14.29 18.78 2.74 0.0 21.62 0.0 0.0 2.74 0.0 17.91 0.0 2.76 0.0 0.0 0.0 0.0 0.0 0.0 0.0 12.66 3.53 2.79 5.47 0.0 0.0 0.0 0.0 0.0 0. 2.12 1.58 1.43 1.21 0.82 0.7 0.54 0.53 0.42 0.32 2.87 2.08 2.1 1.06 1.0 2.0 0.99 0.44 0.59 0.45 1.84 0.88 1.04 0.98 0.69 0.5 0.0 0.1 0.17 0.1 1.38 0.86 0.85 0.98 0.47 0.0 0.33 0.3 0.35 0.28 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 88.97 77.22 75.79 45.05 N/A N/A N/A N/A N/A N/A 68.33 45.12 35.39 27.09 N/A 20.8 0.0 N/A 36.39 0.0 83.18 N/A 36.39 N/A N/A N/A N/A N/A N/A N/A 77.22 36.39 36.39 57.01 N/A N/A N/A N/A N/A N/A 53 Table 16: Path 1 evaluation results with stochastic sampling on measurement-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Alignment: Percentage of agreement between initial and final decisions; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage Stage 3 Stage 4 Completion Depth Consistency Alignment Stage-wise Score Cardiomegaly Carina Angle Rotation Mediastinal Widening Projection Aortic Knob Enlargement Descending Aorta Enlargement Descending Aorta Tortuous Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 96.33 79.6 92.86 96.33 84.93 N/A 51.12 31.36 44.62 23. 96.33 96.26 96.11 96.33 84.26 0.0 48.77 46.0 39.78 10.92 96.33 96.33 96.26 85.52 87.06 N/A 23.29 25.29 37.72 5.03 84.37 93.12 90.04 79.97 91.6 36.39 45.97 59.17 38.47 16.89 96.33 71.77 81.98 71.77 28.87 10.12 14.05 9.46 19.82 2.74 96.33 93.05 75.57 86.69 57.07 80.65 45.97 51.74 42.74 34.77 74.77 75.12 66.33 77.86 58.69 39.52 45.97 33.15 42.7 29. 96.3 87.89 87.78 96.33 80.56 63.66 N/A 23.72 42.94 30.12 61.18 (48.3) 45.13 (34.72) 36.37 (36.37) 58.44 (43.83) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 67.8 (64.03) 87.25 (80.54) 83.82 (83.82) 46.87 (32.81) 29.73 (29.73) N/A (N/A) 0.0 (0.0) N/A (N/A) 36.39 (0.0) N/A (N/A) 85.69 (85.69) 64.54 (39.44) 67.71 (67.71) 51.73 (51.73) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 49.52 (49.52) 57.01 (28.51) 51.73 (51.73) 45.05 (22.52) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (36.39) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 78.49 (72.45) 85.69 (38.95) N/A (N/A) 57.01 (42.76) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 51.73 (51.73) 55.25 (23.68) 36.39 (36.39) 57.01 (42.76) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (36.39) 15.16 (11.97) 9.2 (7.08) 6.95 (6.95) 8.15 (6.11) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 27.17 (25.66) 35.49 (32.76) 35.92 (35.92) 12.55 (8.78) 3.58 (3.58) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 2.99 (0.0) 0.0 (0.0) 21.18 (21.18) 15.16 (9.26) 8.3 (8.3) 4.56 (4.56) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 7.26 (7.26) 8.41 (4.21) 5.48 (5.48) 3.65 (1.83) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 3.67 (3.67) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 15.84 (14.62) 27.92 (12.69) 0.0 (0.0) 8.53 (6.4) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 7.25 (7.25) 13.37 (5.73) 7.73 (7.73) 6.55 (4.92) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 3.53 (3.53) 2.2 (2.14) 1.62 (1.58) 1.43 (1.43) 1.35 (1.31) 0.93 (0.93) N/A (N/A) 0.57 (0.57) 0.56 (0.5) 0.49 (0.49) 0.24 (0.24) 2.55 (2.51) 2.72 (2.66) 2.6 (2.6) 1.99 (1.92) 1.03 (1.03) 0.0 (0.0) 0.6 (0.6) 0.71 (0.71) 0.52 (0.49) 0.14 (0.14) 2.22 (2.22) 1.88 (1.76) 1.51 (1.51) 1.18 (1.18) 0.94 (0.94) N/A (N/A) 0.24 (0.24) 0.27 (0.27) 0.42 (0.42) 0.03 (0.03) 1.36 (1.36) 1.31 (1.24) 1.22 (1.22) 0.98 (0.96) 0.92 (0.92) N/A (N/A) 0.52 (0.52) 0.72 (0.72) 0.29 (0.29) 0.13 (0.13) 2.0 (2.0) 0.0 (0.0) 1.23 (1.23) 0.05 (0.05) 0.01 (0.01) 0.0 (0.0) 0.06 (0.06) 0.03 (0.03) 0.05 (0.05) 0.01 (0.01) 2.04 (2.01) 2.12 (1.79) 0.11 (0.11) 1.15 (1.11) 0.04 (0.04) N/A (N/A) 0.16 (0.16) 0.48 (0.48) 0.02 (0.02) 0.34 (0.34) 1.27 (1.27) 1.36 (1.22) 0.29 (0.29) 1.06 (1.04) 0.0 (0.0) 0.0 (0.0) 0.13 (0.13) 0.18 (0.18) 0.09 (0.09) 0.28 (0.28) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.28 (0.28) 61.18 55.89 51.73 49.52 N/A N/A N/A 0.0 N/A N/A 82.67 80.69 91.23 54.31 45.05 N/A 45.05 N/A 0.0 N/A 85.69 47.01 67.71 51.73 N/A N/A N/A N/A N/A N/A 70.19 32.5 51.73 29.73 N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A N/A N/A N/A N/A N/A N/A 69.46 37.27 N/A 42.24 N/A N/A N/A N/A N/A N/A 51.73 31.97 36.39 42.24 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 61.18 (48.3) 40.35 (31.04) 27.09 (27.09) 58.44 (43.83) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 65.2 (61.58) 75.0 (69.23) 69.8 (69.8) 27.02 (18.91) 29.73 (29.73) N/A (N/A) 0.0 (0.0) N/A (N/A) 36.39 (0.0) N/A (N/A) 50.11 (50.11) 64.54 (39.44) 55.25 (55.25) 51.73 (51.73) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 35.76 (35.76) 42.24 (21.12) 36.37 (36.37) 45.05 (22.52) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (36.39) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 50.27 (46.4) 43.52 (19.78) N/A (N/A) 57.01 (42.76) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.37 (36.37) 38.47 (16.49) 0.0 (0.0) 42.24 (31.68) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (36.39) 78.91 55.58 29.6 19.47 0.0 N/A 0.0 27.82 0.0 0. 83.25 87.67 71.24 60.35 8.9 N/A 8.49 0.0 14.16 29.97 70.78 49.57 33.74 20.97 0.0 N/A 0.0 0.0 0.0 0.0 31.39 20.98 18.68 9.24 0.0 N/A 5.34 10.86 0.0 0.0 36.39 N/A 37.35 0.0 0.0 N/A 25.07 0.0 0.0 0.0 64.55 49.72 0.0 15.53 0.0 N/A 0.0 0.0 0.0 0.0 56.32 44.37 29.9 17.86 N/A N/A 0.0 0.0 0.0 0. N/A N/A N/A N/A N/A N/A N/A N/A N/A 20.28 21.27 (16.79) 23.28 (17.91) 20.28 (20.28) 33.4 (25.05) N/A (N/A) N/A (N/A) N/A (N/A) 29.73 (0.0) N/A (N/A) N/A (N/A) 36.4 (34.38) 38.47 (35.51) 46.74 (46.74) 27.63 (19.34) 29.9 (29.9) N/A (N/A) 36.37 (36.37) N/A (N/A) 23.32 (0.0) 0.0 (0.0) 26.36 (26.36) 29.78 (18.2) 19.56 (19.56) 17.46 (17.46) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 24.27 (24.27) 27.25 (13.63) 21.14 (21.14) 26.06 (13.03) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 8.9 (8.9) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) 20.98 (19.36) 46.52 (21.15) N/A (N/A) 35.76 (26.82) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 13.64 (13.64) 31.28 (13.41) 29.73 (29.73) 28.57 (21.43) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 27.09 (27.09) 54 Table 17: Path 2 evaluation results with greedy sampling for overall, measurement-type, and recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 3 matches the Stage 2 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Completion Depth Consistency Stage-wise Score Overall Measurment Recognition Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 69.61 60.54 44.89 70.88 15.36 40.45 28.16 29.09 18.25 8.3 75.15 61.28 58.01 70.88 12.42 41.83 32.28 29.09 20.43 12.44 36.39 57.94 9.91 N/A 21.23 30.76 11.69 N/A 7.34 0.0 57.85 (56.2) 41.42 (35.77) 32.37 (31.07) 22.17 (18.87) 37.41 (28.61) 47.52 (41.0) 10.66 (10.66) 19.15 (6.96) 11.14 (8.64) 0.0 (0.0) 61.42 (59.5) 47.86 (40.6) 37.0 (35.51) 22.17 (18.87) 33.27 (19.19) 46.15 (38.69) 13.33 (13.33) 19.15 (6.96) 13.37 (10.37) 0.0 (0.0) 71.54 (66.1) 66.66 (58.75) 73.92 (69.81) 64.38 (50.98) 50.27 (38.64) 67.26 (55.0) 43.26 (43.26) 23.96 (7.43) 35.5 (31.33) N/A (N/A) 77.4 (71.05) 67.53 (57.37) 73.92 (69.81) 64.38 (50.98) 51.7 (31.35) 64.7 (50.7) 43.26 (43.26) 23.96 (7.43) 35.5 (31.33) N/A (N/A) 55.81 (54.29) 34.65 (30.11) 19.94 (19.15) 19.04 (16.16) 9.55 (8.03) 28.45 (25.48) 4.22 (4.22) 5.38 (3.6) 2.89 (2.46) 0.0 (0.0) 59.05 (57.28) 40.88 (35.04) 27.41 (26.33) 19.04 (16.16) 5.63 (3.35) 29.32 (25.93) 5.27 (5.27) 5.38 (3.6) 3.46 (2.95) 0.0 (0.0) 2.62 (2.58) 1.75 (1.64) 0.99 (0.97) 1.21 (1.16) 0.37 (0.34) 1.2 (1.14) 0.45 (0.45) 0.46 (0.44) 0.21 (0.2) 0.05 (0.05) 2.55 (2.52) 1.99 (1.86) 1.3 (1.27) 1.21 (1.16) 0.22 (0.18) 1.26 (1.19) 0.54 (0.54) 0.46 (0.44) 0.25 (0.24) 0.08 (0.08) 36.39 18.88 0.0 N/A 44.31 57.16 0.0 N/A 0.0 N/A 36.39 63.6 N/A N/A 48.37 85.13 N/A N/A N/A N/A 36.39 12.86 0.0 N/A 17.39 22.32 0.0 N/A 0.0 0.0 3.0 0.88 0.17 N/A 0.66 0.8 0.08 N/A 0.03 0.0 69.93 57.73 68.7 51.89 38.18 52.81 43.26 11.26 37.98 N/A 69.93 57.73 68.7 51.89 38.18 52.81 43.26 11.26 37.98 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 55 Table 18: Path 2 evaluation results with greedy sampling on recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Completion Depth Consistency Stage-wise Score Trachea Deviation Inclusion Inspiration Ascending Aorta Enlargement Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM N/A N/A 0.0 N/A 37.52 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 75.69 0.0 N/A 7.34 N/A N/A N/A N/A 0.0 36.39 40.19 29.73 N/A 18.83 30.76 11.69 N/A 7.34 0.0 N/A N/A N/A N/A 36.37 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 11.69 N/A N/A 36.39 N/A N/A N/A N/A N/A 36.39 26.06 0.0 N/A 60.16 57.16 0.0 N/A 0.0 N/A N/A N/A N/A N/A 45.05 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 70.19 N/A N/A 36.39 N/A N/A N/A N/A N/A 36.39 57.01 N/A N/A 63.66 85.13 N/A N/A N/A N/A N/A N/A 0.0 N/A 29.9 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 9.94 0.0 N/A 7.34 N/A N/A N/A N/A 0.0 36.39 15.77 0.0 N/A 14.93 22.32 0.0 N/A 0.0 0.0 N/A N/A 0.0 N/A 1.4 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 1.01 0.0 N/A 0.09 N/A N/A N/A N/A 0.0 3.0 0.76 0.5 N/A 0.49 0.8 0.08 N/A 0.03 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 56 Table 19: Path 2 evaluation results with greedy sampling on measurement-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 3 matches the Stage 2 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage Stage 3 Completion Depth Consistency Stage-wise Score Cardiomegaly Carina Angle Rotation Mediastinal Widening Projection Aortic Knob Enlargement Descending Aorta Enlargement Descending Aorta Tortuous Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 57.01 45.05 86.75 N/A 0.0 61.18 N/A N/A N/A N/A N/A 36.39 47.27 N/A N/A 40.19 N/A N/A N/A 13.28 N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A 57.01 55.92 48.54 N/A 17.33 27.09 N/A 0.0 23.14 20.37 96.19 96.11 75.81 89.07 10.74 65.82 45.05 62.56 14.43 N/A 81.58 75.9 60.81 54.62 13.01 36.04 14.73 19.57 22.87 N/A 79.12 43.44 77.37 49.52 11.13 33.34 23.65 17.85 20.4 0.0 79.97 76.15 67.54 90.31 22.32 29.17 45.69 45.48 21.3 16.13 57.01 (57.01) 29.73 (29.73) 30.64 (30.64) N/A (N/A) N/A (N/A) 65.67 (65.67) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (36.39) 25.07 (25.07) N/A (N/A) N/A (N/A) 57.93 (36.86) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 57.01 (57.01) 38.3 (38.3) 27.25 (27.25) N/A (N/A) 40.79 (30.59) 36.39 (36.39) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 61.82 (58.08) 88.48 (74.57) 50.11 (46.53) 29.0 (24.54) 0.0 (0.0) 74.17 (74.17) 0.0 (0.0) 12.72 (0.0) 0.0 (0.0) N/A (N/A) 81.58 (81.58) 41.17 (41.17) 51.07 (51.07) 18.73 (18.73) 20.37 (20.37) 31.84 (19.6) 20.37 (20.37) 18.03 (0.0) 19.79 (19.79) N/A (N/A) 91.03 (91.03) 52.51 (52.51) 50.27 (50.27) 27.82 (27.82) 45.48 (15.16) 35.8 (27.54) 15.31 (15.31) 18.02 (0.0) 17.03 (17.03) N/A (N/A) 20.08 (12.27) 48.43 (11.53) 24.57 (17.74) 13.14 (4.38) 59.71 (29.85) 21.23 (10.61) 17.63 (17.63) 27.82 (27.82) 30.03 (15.01) 0.0 (0.0) 57.01 (57.01) 36.39 (36.39) 81.58 (81.58) N/A (N/A) N/A (N/A) 78.49 (78.49) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (36.39) 36.39 (36.39) N/A (N/A) N/A (N/A) 65.82 (41.88) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 57.01 (57.01) 77.22 (77.22) 57.01 (57.01) N/A (N/A) 58.44 (43.83) 36.39 (36.39) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 94.58 (88.84) 94.0 (79.21) 79.63 (73.94) 87.54 (74.07) N/A (N/A) 63.66 (63.66) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 81.58 (81.58) 72.32 (72.32) 89.84 (89.84) 64.77 (64.77) 36.39 (36.39) 78.49 (48.3) 36.39 (36.39) 29.73 (0.0) 45.05 (45.05) N/A (N/A) 91.03 (91.03) 82.42 (82.42) 89.84 (89.84) 45.05 (45.05) 64.77 (21.59) 78.49 (60.38) 36.39 (36.39) 36.39 (0.0) 36.39 (36.39) N/A (N/A) 83.18 (50.84) 73.98 (17.62) 83.18 (60.08) 60.16 (20.05) 47.2 (23.6) 51.57 (25.78) 57.01 (57.01) 29.73 (29.73) 25.07 (12.54) N/A (N/A) 57.01 (57.01) 29.73 (29.73) 29.6 (29.6) N/A (N/A) 0.0 (0.0) 52.71 (52.71) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (36.39) 23.32 (23.32) N/A (N/A) N/A (N/A) 30.36 (19.32) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 57.01 (57.01) 27.97 (27.97) 19.98 (19.98) N/A (N/A) 9.34 (7.01) 27.09 (27.09) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 61.82 (58.08) 87.18 (73.47) 46.69 (43.36) 27.83 (23.55) 0.0 (0.0) 57.92 (57.92) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 81.58 (81.58) 39.52 (39.52) 36.46 (36.46) 12.94 (12.94) 4.45 (4.45) 14.6 (8.99) 5.06 (5.06) 3.81 (0.0) 6.83 (6.83) N/A (N/A) 79.12 (79.12) 29.69 (29.69) 44.14 (44.14) 24.51 (24.51) 8.15 (2.72) 15.51 (11.93) 5.34 (5.34) 5.06 (0.0) 5.34 (5.34) 0.0 (0.0) 17.75 (10.85) 35.7 (8.5) 19.11 (13.81) 10.89 (3.63) 11.87 (5.94) 7.08 (3.54) 10.68 (10.68) 18.02 (18.02) 5.15 (2.57) 0.0 (0.0) 3.0 (3.0) 2.0 (2.0) 1.61 (1.61) N/A (N/A) 0.0 (0.0) 2.16 (2.16) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 3.0 (3.0) 1.2 (1.2) N/A (N/A) N/A (N/A) 1.21 (0.97) N/A (N/A) N/A (N/A) N/A (N/A) 0.07 (0.07) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 3.0 (3.0) 1.28 (1.28) 1.0 (1.0) N/A (N/A) 0.35 (0.3) 1.0 (1.0) N/A (N/A) 0.0 (0.0) 0.22 (0.22) 0.14 (0.14) 2.38 (2.29) 2.88 (2.59) 2.08 (2.0) 1.55 (1.46) 0.07 (0.07) 2.64 (2.64) 1.0 (1.0) 0.85 (0.8) 0.12 (0.12) N/A (N/A) 3.0 (3.0) 1.94 (1.94) 1.49 (1.49) 0.87 (0.87) 0.15 (0.15) 0.68 (0.57) 0.18 (0.18) 0.24 (0.2) 0.32 (0.32) N/A (N/A) 2.71 (2.71) 1.15 (1.15) 1.86 (1.86) 1.25 (1.25) 0.24 (0.15) 0.67 (0.6) 0.29 (0.29) 0.22 (0.18) 0.25 (0.25) 0.0 (0.0) 1.23 (1.09) 1.68 (1.04) 1.14 (1.03) 1.18 (1.04) 0.52 (0.39) 0.44 (0.38) 0.7 (0.7) 1.0 (1.0) 0.32 (0.28) 0.09 (0.09) 57 57.01 36.39 81.58 N/A N/A 78.49 N/A N/A N/A N/A N/A 36.39 36.39 N/A N/A 45.24 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 57.01 77.22 57.01 N/A 49.52 36.39 N/A N/A N/A N/A 85.94 76.6 71.0 69.18 N/A 74.17 N/A 0.0 N/A N/A 81.58 72.32 89.84 64.77 36.39 45.13 36.39 0.0 45.05 N/A 91.03 82.42 89.84 45.05 27.82 55.89 36.39 0.0 36.39 N/A 47.01 22.75 55.21 28.57 39.0 34.38 57.01 45.05 32.5 N/A Table 20: Path 2 evaluation results with stochastic sampling for overall, measurement-type, and recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 3 matches the Stage 2 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Completion Depth Consistency Stage-wise Score Overall Measurement Recognition Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 64.78 59.98 47.04 74.79 9.91 38.91 25.78 27.87 5.47 5.87 88.07 60.21 58.58 74.79 14.08 42.4 28.91 27.88 6.84 5.89 18.2 59.17 23.96 N/A 1.57 31.91 13.25 27.84 0.0 5.83 59.31 (56.05) 38.9 (33.77) 31.69 (31.03) 30.37 (27.22) 15.15 (3.47) 26.6 (18.37) 15.46 (8.18) 15.07 (12.48) 0.0 (0.0) 25.63 (7.43) 65.04 (60.96) 42.54 (35.93) 40.07 (39.13) 30.37 (27.22) 17.04 (3.9) 25.35 (13.0) 19.32 (10.22) 16.02 (12.13) 0.0 (0.0) 34.17 (9.91) 77.56 (67.03) 74.08 (63.03) 71.23 (68.04) 71.86 (58.78) 36.66 (9.01) 52.93 (37.63) 36.39 (24.26) 42.63 (32.07) N/A (N/A) 36.39 (12.13) 87.85 (74.69) 75.29 (60.56) 76.21 (72.56) 71.86 (58.78) 36.66 (9.01) 53.17 (30.21) 36.39 (24.26) 41.59 (28.01) N/A (N/A) 36.39 (12.13) 49.12 (46.56) 32.44 (28.08) 22.14 (21.7) 26.2 (23.53) 2.58 (0.7) 10.7 (6.41) 7.92 (2.5) 4.56 (3.32) 0.0 (0.0) 3.43 (1.02) 64.58 (60.75) 36.3 (30.7) 29.5 (28.83) 26.2 (23.53) 3.87 (1.05) 11.84 (5.41) 9.9 (3.12) 4.73 (2.87) 0.0 (0.0) 5.4 (1.61) 2.12 (2.07) 1.61 (1.52) 1.19 (1.18) 1.44 (1.38) 0.12 (0.09) 0.64 (0.55) 0.46 (0.33) 0.37 (0.35) 0.04 (0.04) 0.08 (0.05) 2.43 (2.35) 1.77 (1.65) 1.41 (1.39) 1.44 (1.38) 0.17 (0.13) 0.71 (0.57) 0.56 (0.39) 0.37 (0.34) 0.05 (0.05) 0.09 (0.05) 36.39 26.18 12.13 N/A 0.0 29.1 0.0 13.16 N/A 0.0 36.39 70.45 36.39 N/A N/A 52.45 N/A 46.26 N/A N/A 18.2 18.92 7.43 N/A 0.0 8.41 0.0 4.22 0.0 0.0 1.5 1.05 0.75 N/A 0.01 0.49 0.09 0.37 0.0 0.05 73.44 59.73 71.93 60.53 11.89 37.52 24.26 34.06 N/A 12. 73.44 59.73 71.93 60.53 11.89 37.52 24.26 34.06 N/A 12.13 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 58 Table 21: Path 2 evaluation results with stochastic sampling on recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Completion Depth Consistency Stage-wise Score Trachea Deviation Inclusion Inspiration Ascending Aorta Enlargement Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM N/A N/A 29.73 N/A 0.0 14.75 N/A 3.19 N/A 23.32 N/A N/A 36.39 N/A 0.0 91.84 N/A 90.26 N/A 0.0 36.39 77.62 0.0 N/A 0.0 13.66 N/A 7.93 0.0 0.0 0.0 40.71 29.73 N/A 6.29 7.39 13.25 10.0 0.0 0.0 N/A N/A 36.39 N/A N/A 35.83 N/A 0.0 N/A 0.0 N/A N/A 0.0 N/A N/A 23.99 N/A 18.27 N/A N/A 36.39 19.33 N/A N/A N/A 14.57 N/A 0.0 N/A N/A N/A 33.04 0.0 N/A 0.0 42.02 0.0 34.38 N/A N/A N/A N/A 36.39 N/A N/A 64.77 N/A N/A N/A N/A N/A N/A N/A N/A N/A 51.63 N/A 40.79 N/A N/A 36.39 79.63 N/A N/A N/A 36.39 N/A N/A N/A N/A N/A 61.27 N/A N/A N/A 57.01 N/A 51.73 N/A N/A N/A N/A 29.73 N/A 0.0 7.99 N/A 0.0 N/A 0.0 N/A N/A 0.0 N/A 0.0 17.3 N/A 10.57 N/A 0.0 36.39 16.86 0.0 N/A 0.0 2.79 N/A 0.0 0.0 0.0 0.0 20.98 0.0 N/A 0.0 5.57 0.0 6.32 0.0 0.0 N/A N/A 1.5 N/A 0.0 0.28 N/A 0.01 N/A 0.2 N/A N/A 1.0 N/A 0.0 1.41 N/A 1.24 N/A 0. 3.0 1.19 0.0 N/A 0.0 0.15 N/A 0.05 0.0 0.0 0.0 0.92 0.5 N/A 0.04 0.14 0.09 0.17 0.0 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 59 Table 22: Path 2 evaluation results with stochastic sampling on measurement-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 3 matches the Stage 2 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage Stage 3 Completion Depth Consistency Stage-wise Score Cardiomegaly Carina Angle Rotation Mediastinal Widening Projection Aortic Knob Enlargement Descending Aorta Enlargement Descending Aorta Tortuous Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM N/A 45.05 87.22 N/A 15.36 73.76 N/A 56.2 0.0 0.0 N/A 45.05 51.57 N/A 20.36 10.1 27.09 10.59 0.0 14.72 N/A N/A 0.0 N/A 20.37 55.8 N/A 37.03 0.0 17. N/A 52.86 47.01 N/A 7.23 46.06 N/A 28.6 12.05 0.0 96.3 95.42 77.98 89.57 12.93 80.68 52.65 67.21 11.72 N/A 82.42 77.98 63.81 55.92 8.57 12.55 16.41 7.88 7.82 0.0 91.43 36.95 70.82 64.54 7.19 15.55 19.49 5.42 9.12 0.0 82.14 68.14 70.2 89.13 20.64 44.73 N/A 10.08 14.03 9.47 N/A (N/A) 0.0 (0.0) 31.19 (31.19) N/A (N/A) 32.5 (0.0) 24.48 (13.99) N/A (N/A) 17.29 (9.6) N/A (N/A) N/A (N/A) N/A (N/A) 45.05 (45.05) 47.27 (47.27) N/A (N/A) 29.9 (14.95) 28.26 (18.84) 36.39 (0.0) 18.02 (18.02) N/A (N/A) 29.73 (29.73) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 16.46 (11.52) N/A (N/A) 9.93 (4.97) N/A (N/A) 36.39 (0.0) N/A (N/A) 46.69 (43.36) 25.39 (25.39) N/A (N/A) 23.32 (0.0) 24.03 (12.94) N/A (N/A) 19.34 (9.67) 0.0 (0.0) N/A (N/A) 67.59 (61.19) 89.7 (71.99) 45.19 (45.19) 40.55 (36.97) 0.0 (0.0) 54.07 (13.83) 0.0 (0.0) 13.15 (4.38) 0.0 (0.0) N/A (N/A) 82.42 (82.42) 51.19 (47.99) 58.4 (58.4) 32.95 (32.95) 0.0 (0.0) 24.18 (8.06) 21.76 (21.76) 23.32 (23.32) 0.0 (0.0) N/A (N/A) 91.43 (91.43) 35.39 (35.39) 51.91 (51.91) 36.68 (36.68) 32.5 (16.25) 26.06 (19.55) 19.13 (19.13) 27.09 (27.09) 0.0 (0.0) N/A (N/A) 18.73 (8.81) 29.75 (7.76) 21.14 (14.53) 11.31 (2.26) 18.09 (0.0) 5.24 (5.24) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 36.39 (0.0) N/A (N/A) N/A (N/A) 82.42 (82.42) N/A (N/A) 29.73 (0.0) 59.71 (34.12) N/A (N/A) 52.65 (29.25) N/A (N/A) N/A (N/A) N/A (N/A) 45.05 (45.05) 57.01 (57.01) N/A (N/A) 45.05 (22.52) 36.37 (24.24) 36.39 (0.0) 36.39 (36.39) N/A (N/A) 36.39 (36.39) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 74.17 (51.92) N/A (N/A) 45.05 (22.52) N/A (N/A) 36.39 (0.0) N/A (N/A) 79.63 (73.94) 51.73 (51.73) N/A (N/A) 36.39 (0.0) 50.27 (27.07) N/A (N/A) 42.24 (21.12) N/A (N/A) N/A (N/A) 95.13 (86.13) 93.04 (74.68) 79.63 (79.63) 90.11 (82.16) N/A (N/A) 74.49 (19.05) N/A (N/A) 42.02 (14.01) N/A (N/A) N/A (N/A) 82.42 (82.42) 81.58 (76.48) 91.23 (91.23) 74.17 (74.17) N/A (N/A) 51.73 (17.24) 36.39 (36.39) 36.39 (36.39) N/A (N/A) N/A (N/A) 91.43 (91.43) 72.32 (72.32) 89.84 (89.84) 67.71 (67.71) 45.05 (22.52) 42.24 (31.68) 36.39 (36.39) 36.39 (36.39) N/A (N/A) N/A (N/A) 82.42 (38.78) 80.11 (20.9) 81.58 (56.08) 55.43 (11.09) 27.09 (0.0) 36.39 (36.39) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 (0.0) N/A (N/A) 0.0 (0.0) 30.18 (30.18) N/A (N/A) 7.94 (0.0) 16.02 (9.16) N/A (N/A) 9.54 (5.3) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 45.05 (45.05) 42.02 (42.02) N/A (N/A) 12.05 (6.02) 3.72 (2.48) 27.09 (0.0) 2.93 (2.93) 0.0 (0.0) 11.27 (11.27) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 10.8 (7.56) N/A (N/A) 4.6 (2.3) 0.0 (0.0) 17.03 (0.0) N/A (N/A) 32.47 (30.15) 18.77 (18.77) N/A (N/A) 3.12 (0.0) 9.92 (5.34) N/A (N/A) 6.24 (3.12) 0.0 (0.0) 0.0 (0.0) 67.59 (61.19) 88.06 (70.68) 42.52 (42.52) 39.17 (35.72) 0.0 (0.0) 41.93 (10.73) 0.0 (0.0) 7.86 (2.62) 0.0 (0.0) N/A (N/A) 82.42 (82.42) 48.01 (45.01) 43.1 (43.1) 24.14 (24.14) 0.0 (0.0) 4.56 (1.52) 6.38 (6.38) 3.41 (3.41) 0.0 (0.0) 0.0 (0.0) 91.43 (91.43) 18.15 (18.15) 42.44 (42.44) 32.45 (32.45) 4.82 (2.41) 4.98 (3.73) 6.12 (6.12) 3.26 (3.26) 0.0 (0.0) 0.0 (0.0) 16.89 (7.95) 22.38 (5.84) 16.93 (11.64) 9.03 (1.81) 3.05 (0.0) 2.76 (2.76) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 9.47 (0.0) N/A (N/A) 1.0 (1.0) 1.63 (1.63) N/A (N/A) 0.23 (0.13) 1.18 (1.04) N/A (N/A) 0.82 (0.75) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 3.0 (3.0) 2.17 (2.17) N/A (N/A) 0.36 (0.28) 0.14 (0.13) 1.0 (0.33) 0.12 (0.12) 0.0 (0.0) 0.21 (0.21) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 0.14 (0.14) 0.82 (0.76) N/A (N/A) 0.46 (0.44) 0.0 (0.0) 0.3 (0.1) N/A (N/A) 1.37 (1.32) 0.94 (0.94) N/A (N/A) 0.08 (0.06) 0.73 (0.64) N/A (N/A) 0.41 (0.35) 0.08 (0.08) 0.0 (0.0) 2.49 (2.35) 2.91 (2.53) 1.96 (1.96) 1.83 (1.76) 0.1 (0.1) 1.9 (1.15) 0.78 (0.78) 0.91 (0.81) 0.06 (0.06) N/A (N/A) 3.0 (3.0) 2.11 (2.04) 1.68 (1.68) 1.18 (1.18) 0.07 (0.07) 0.18 (0.14) 0.21 (0.21) 0.09 (0.09) 0.05 (0.05) 0.0 (0.0) 3.0 (3.0) 0.78 (0.78) 1.75 (1.75) 1.61 (1.61) 0.11 (0.08) 0.23 (0.22) 0.24 (0.24) 0.06 (0.06) 0.07 (0.07) 0.0 (0.0) 1.23 (1.05) 1.23 (0.88) 1.12 (1.01) 1.13 (0.99) 0.26 (0.21) 0.52 (0.52) N/A (N/A) 0.09 (0.09) 0.13 (0.13) 0.12 (0.04) 60 N/A N/A 82.42 N/A 0.0 45.12 N/A 39.22 N/A N/A N/A 45.05 57.01 N/A 29.73 36.37 0.0 36.39 N/A 36.39 N/A N/A N/A N/A N/A 48.48 N/A 29.73 N/A 0.0 N/A 71.0 51.73 N/A 0.0 40.35 N/A 32.5 N/A N/A 82.52 71.83 79.63 78.33 N/A 24.09 N/A 27.82 N/A N/A 82.42 73.67 91.23 74.17 N/A 27.09 36.39 36.39 N/A N/A 91.43 72.32 89.84 67.71 29.73 42.24 36.39 36.39 N/A N/A 37.4 24.5 51.63 21.91 0.0 36.39 N/A N/A N/A 0.0 Table 23: Re-evaluated Path 1 results with greedy sampling for overall, measurement-type, and recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Stage 4 Completion Depth Consistency Alignment Stage-wise Score Overall Measurement Recognition Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 54.2 55.21 46.11 76.22 52.41 52.58 51.73 36.39 0.0 N/A 54.2 60.19 46.11 76.22 51.16 50.99 51.73 36.39 0.0 N/A N/A 42.76 N/A N/A 54.91 63.66 N/A N/A N/A N/A 56.24 43.97 26.98 14.12 16.98 12.39 0.0 0.0 N/A N/A 56.24 46.59 26.98 14.12 18.04 10.86 0.0 0.0 N/A N/A N/A 37.43 N/A N/A 14.86 23.14 N/A N/A N/A N/A 23.15 (22.88) 16.78 (14.84) 17.96 (17.96) 0.0 (0.0) 0.0 (0.0) 18.7 (18.7) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 23.15 (22.88) 16.08 (13.16) 17.96 (17.96) 0.0 (0.0) 0.0 (0.0) 9.91 (9.91) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 19.58 (15.68) 0.0 (0.0) N/A (N/A) N/A (N/A) 22.52 (22.52) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 13.98 (8.78) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 1.78 (1.77) 0.0 (0.0) 1.41 (1.4) 6.03 (5.16) 1.11 (1.11) 0.0 (0.0) 1.13 (1.13) 0.0 (0.0) 1.17 (1.17) 0.0 (0.0) 1.08 (1.08) 2.74 (2.74) 1.0 (1.0) 0.0 (0.0) 1.0 (1.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) 1.78 (1.77) 0.0 (0.0) 1.44 (1.41) 3.02 (1.81) 1.11 (1.11) 0.0 (0.0) 1.13 (1.13) 0.0 (0.0) 1.13 (1.13) 0.0 (0.0) 1.02 (1.02) 0.0 (0.0) 1.0 (1.0) 0.0 (0.0) 1.0 (1.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A 18.2 N/A N/A 0.0 45.05 N/A N/A N/A N/A N/A 36.39 N/A N/A N/A 45.05 N/A N/A N/A N/A N/A 13.54 N/A N/A 0.0 21.91 N/A N/A N/A N/A N/A 1.35 N/A N/A 1.25 1.5 N/A N/A N/A N/A 49.71 46.98 36.39 N/A N/A 36.39 N/A N/A N/A N/A 49.71 46.98 36.39 N/A N/A 36.39 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 (0.0) 19.58 (15.68) 0.0 (0.0) N/A (N/A) N/A (N/A) 22.52 (22.52) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 13.98 (8.78) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A 36.39 N/A N/A N/A 45.05 N/A N/A N/A N/A 61 Table 24: Re-evaluated Path 1 results with greedy sampling for recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Stage 4 Completion Depth Consistency Alignment Stage-wise Score Trachea Deviation Inclusion Inspiration Ascending Aorta Enlargement Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM N/A N/A N/A N/A 45.05 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 58.44 N/A N/A N/A N/A N/A N/A N/A N/A N/A 27.09 N/A N/A 64.77 63.66 N/A N/A N/A N/A N/A N/A N/A N/A 29.73 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 38.47 N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A N/A 0.0 23.14 N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A N/A N/A 45.05 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A N/A N/A 45.05 N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 27.09 N/A N/A 0.0 21.91 N/A N/A N/A N/A N/A N/A N/A N/A 1.5 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 1.38 N/A N/A N/A N/A N/A N/A N/A N/A N/A 1.33 N/A N/A 1.0 1.5 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A N/A N/A 45.05 N/A N/A N/A N/A 62 Table 25: Re-evaluated Path 1 results with greedy sampling for measurement-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Stage 4 Completion Depth Consistency Alignment Stage-wise Score Cardiomegaly Carina Angle Rotation Mediastinal Widening Projection Aortic Knob Enlargement Descending Aorta Enlargement Descending Aorta Tortuous Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 42.24 N/A 52.65 N/A N/A 55.25 N/A N/A N/A N/A N/A N/A 0.0 N/A N/A 64.77 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 46.0 45.05 N/A 51.73 36.39 N/A N/A N/A N/A 91.6 95.47 69.46 80.11 N/A 58.44 N/A N/A N/A N/A 49.52 34.38 74.17 N/A 36.39 45.48 N/A 36.39 0.0 N/A 47.01 55.25 36.37 N/A 61.27 51.57 N/A N/A N/A N/A 58.44 69.87 45.05 72.32 55.25 45.05 51.73 N/A N/A N/A 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 12.2 (11.48) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 3.37 (3.17) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 29.73 (14.86) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 11.72 (5.86) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) 1.75 (1.75) N/A (N/A) 1.0 (1.0) N/A (N/A) N/A (N/A) 1.14 (1.14) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 1.5 (1.5) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 2.0 (2.0) 1.0 (1.0) 1.0 (1.0) N/A (N/A) 1.33 (1.33) 1.0 (1.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 2.13 (2.11) 2.16 (2.15) 1.77 (1.77) 1.26 (1.26) N/A (N/A) 1.0 (1.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 1.5 (1.5) 0.5 (0.5) 1.2 (1.2) N/A (N/A) 1.0 (1.0) 0.67 (0.67) N/A (N/A) 1.0 (1.0) 0.0 (0.0) N/A (N/A) 1.28 (1.28) 1.86 (1.86) 1.33 (1.33) N/A (N/A) 1.2 (1.2) 0.83 (0.83) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 2.0 (2.0) 1.67 (1.56) 1.5 (1.5) 1.0 (1.0) 1.0 (1.0) 1.0 (1.0) 1.0 (1.0) N/A (N/A) N/A (N/A) N/A (N/A) 36.39 N/A 36.39 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 63.66 74.83 36.39 N/A N/A N/A N/A N/A N/A N/A 45.05 N/A N/A N/A N/A N/A N/A N/A N/A N/A 51.73 36.39 36.39 N/A N/A N/A N/A N/A N/A N/A 51.73 29.73 N/A N/A N/A N/A N/A N/A N/A N/A 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 12.2 (11.48) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 29.73 (14.86) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 51.73 N/A 20.37 N/A N/A 27.82 N/A N/A N/A N/A N/A N/A N/A N/A N/A 27.82 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 29.9 0.0 N/A 27.09 0.0 N/A N/A N/A N/A 94.16 86.65 60.16 28.25 N/A 20.37 N/A N/A N/A N/A 42.02 0.0 21.91 N/A 0.0 0.0 N/A 0.0 N/A N/A 57.92 64.77 29.73 N/A 23.32 0.0 N/A N/A N/A N/A 55.25 51.63 29.73 0.0 21.76 0.0 0.0 N/A N/A N/A 27.09 (27.09) N/A (N/A) 36.39 (36.39) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 29.73 (29.73) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 16.69 (15.02) 21.76 (20.48) 17.03 (17.03) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 32.5 (32.5) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 28.26 (28.26) 21.76 (21.76) 36.39 (36.39) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 34.38 (34.38) 20.8 (10.4) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 63 Table 26: Re-evaluated Path 1 results with stochastic sampling for overall, measurement-type, and recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage Stage 3 Stage 4 Completion Depth Consistency Stage-wise Score Overall Measurement Recognition Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM 70.86 50.38 51.92 55.06 14.56 38.97 36.39 30.01 N/A 0.0 70.86 58.92 54.14 55.06 14.56 40.36 36.39 41.83 N/A 0.0 N/A 24.76 36.39 N/A N/A 36.89 N/A 18.2 N/A N/A 67.96 44.15 30.32 36.5 0.0 8.94 0.0 0.0 N/A N/A 67.96 45.77 34.66 36.5 0.0 14.91 0.0 0.0 N/A N/A N/A 34.38 0.0 N/A N/A 0.0 N/A 0.0 N/A N/A 27.55 (19.48) 17.49 (13.12) 14.04 (14.04) 12.72 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 27.55 (19.48) 15.57 (10.33) 14.04 (14.04) 12.72 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 14.86 (14.86) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 14.86 (14.86) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 1.93 (1.86) 0.0 (0.0) 1.24 (1.21) 0.0 (0.0) 1.48 (1.48) 2.72 (2.72) 1.81 (1.66) 0.0 (0.0) 0.4 (0.4) 0.0 (0.0) 1.1 (1.1) 0.0 (0.0) 1.0 (1.0) 0.0 (0.0) 0.7 (0.7) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 1.93 (1.86) 0.0 (0.0) 1.44 (1.41) 0.0 (0.0) 1.55 (1.55) 3.11 (3.11) 1.81 (1.66) 0.0 (0.0) 0.4 (0.4) 0.0 (0.0) 1.25 (1.25) 0.0 (0.0) 1.0 (1.0) 0.0 (0.0) 0.0 (0.0) 0.9 (0.9) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A 27.09 N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 0.0 N/A N/A 0.0 N/A 0.0 N/A N/A N/A 0.62 1.0 N/A N/A 0.88 N/A 0.5 N/A N/A 49.82 43.97 48.39 0.0 N/A N/A N/A N/A N/A N/A 49.82 43.97 48.39 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 64 Table 27: Re-evaluated Path 1 results with stochastic sampling for recognition-type tasks. Stage-wise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage Stage 2 Stage 3 Stage 4 Completion Depth Consistency Stage-wise Score Trachea Deviation Inclusion Inspiration Ascending Aorta Enlargement Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM N/A N/A 0.0 N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A 0.0 N/A N/A N/A 34.38 N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 27.09 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A 0.0 N/A N/A N/A 0.0 N/A N/A N/A 0.0 N/A N/A N/A N/A N/A 0.0 N/A N/A N/A 0.0 N/A 0.0 N/A N/A N/A N/A 1.0 N/A N/A 0.5 N/A N/A N/A N/A N/A N/A N/A N/A N/A 1.0 N/A 1.0 N/A N/A N/A 1.25 N/A N/A N/A 1.0 N/A N/A N/A N/A N/A 0.0 N/A N/A N/A 1.0 N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A N/A 29.73 N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.39 N/A 36.39 N/A N/A N/A 49.52 N/A N/A N/A 36.39 N/A N/A N/A N/A N/A 0.0 N/A N/A N/A 45.05 N/A 0.0 N/A N/A 65 Table 28: Re-evaluated Path 1 results with stochastic sampling for measurement-type tasks. Stagewise Score: Percentage of cases that completed each stage; Completion: Percentage of cases completing all reasoning stages; Depth: Average number of reasoning stages reached; Consistency: Percentage of cases where the value returned at Stage 4 matches the Stage 3 response; Refined scores incorporating measurement consistency are shown in parentheses. N/A indicates that the model did not reach the required stage to compute the corresponding metric. Task Models Stage 1 Stage 2 Stage 3 Stage Completion Depth Consistency Stage-wise Score Cardiomegaly Carina Angle Rotation Mediastinal Widening Projection Aortic Knob Enlargement Descending Aorta Enlargement Descending Aorta Tortuous Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM Gemini-2.5-Pro Gemini-2.5-Flash GPT-4.1 Pixtral-Large-Instruct-2411 Llama-3.2-90B-Vision-Instruct Qwen2.5-VL-72B-Instruct Pixtral 12B Qwen2.5-VL-7B-Instruct HealthGPT-L14 RadVLM N/A N/A 46.0 N/A 0.0 51.57 N/A 47.27 N/A N/A N/A 36.39 36.37 N/A 0.0 36.39 N/A N/A N/A N/A N/A N/A N/A N/A N/A 45.05 N/A N/A N/A 0.0 N/A 63.66 36.37 N/A 0.0 36.39 N/A 36.39 N/A N/A 94.65 95.19 77.22 81.81 N/A N/A N/A N/A N/A N/A 72.32 37.69 61.27 45.05 N/A 36.39 N/A N/A N/A N/A 61.02 58.44 57.01 36.39 36.39 N/A 36.39 N/A N/A N/A 55.43 62.18 64.77 57.01 36.39 36.39 N/A N/A N/A 0.0 N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 37.52 (25.01) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 16.85 (16.85) 15.82 (14.38) 0.0 (0.0) 21.14 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 26.06 (13.03) 0.0 (0.0) 0.0 (0.0) 29.73 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 28.81 (28.81) 0.0 (0.0) 51.73 (51.73) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 38.47 (19.23) 24.51 (12.26) 32.5 (32.5) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 29.73 (29.73) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) 0.0 (0.0) 21.76 (21.76) 0.0 (0.0) 0.0 (0.0) 0.0 (0.0) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) N/A (N/A) 0.71 (0.71) N/A (N/A) 0.0 (0.0) 1.0 (1.0) N/A (N/A) 0.8 (0.8) N/A (N/A) N/A (N/A) N/A (N/A) 1.0 (1.0) 1.33 (1.33) N/A (N/A) 0.0 (0.0) 1.0 (1.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 1.5 (1.5) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A (N/A) 1.7 (1.6) 1.0 (1.0) N/A (N/A) 0.0 (0.0) 1.0 (1.0) N/A (N/A) 1.0 (1.0) N/A (N/A) N/A (N/A) 2.15 (2.15) 2.09 (2.08) 1.75 (1.75) 1.47 (1.38) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 2.0 (1.89) 1.0 (1.0) 1.4 (1.4) 2.5 (2.0) N/A (N/A) 1.0 (1.0) N/A (N/A) N/A (N/A) N/A (N/A) N/A (N/A) 1.68 (1.68) 1.25 (1.25) 2.5 (2.5) 2.0 (2.0) 1.0 (1.0) N/A (N/A) 1.0 (1.0) N/A (N/A) N/A (N/A) N/A (N/A) 1.9 (1.7) 1.62 (1.54) 2.17 (2.17) 1.25 (1.25) 1.0 (1.0) 2.0 (2.0) N/A (N/A) N/A (N/A) N/A (N/A) 0.0 (0.0) N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 36.37 N/A N/A N/A N/A N/A N/A N/A N/A 75.79 65.82 N/A 0.0 N/A N/A N/A N/A N/A N/A 29.73 N/A N/A 0.0 N/A N/A N/A N/A N/A N/A 61.27 N/A 51.73 N/A N/A N/A N/A N/A N/A N/A 32.5 29.73 45.05 N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.0 N/A N/A 23.32 N/A 0.0 N/A N/A N/A 0.0 45.05 N/A N/A 0.0 N/A N/A N/A N/A N/A N/A N/A N/A N/A 29.73 N/A N/A N/A N/A N/A 39.22 29.73 N/A N/A 0.0 N/A 0.0 N/A N/A 92.18 87.48 53.66 39.5 N/A N/A N/A N/A N/A N/A 52.65 64.77 29.9 45.05 N/A 0.0 N/A N/A N/A N/A 68.58 31.97 42.24 36.39 0.0 N/A 0.0 N/A N/A N/A 58.44 51.2 42.02 25.07 0.0 36.39 N/A N/A N/A N/A"
        }
    ],
    "affiliations": [
        "KAIST",
        "Seoul Medical Center",
        "Seoul National University Hospital"
    ]
}