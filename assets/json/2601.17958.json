{
    "paper_title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
    "authors": [
        "Ido Andrew Atad",
        "Itamar Zimerman",
        "Shahar Katz",
        "Lior Wolf"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary."
        },
        {
            "title": "Start",
            "content": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors"
        },
        {
            "title": "Lior Wolf",
            "content": "Blavatnik School of Computer Science and AI, Tel Aviv University {idoatad,zimemran1,shaharkatz3}@mail.tau.ac.il, wolf@cs.tau.ac.il 6 2 0 2 5 2 ] . [ 1 8 5 9 7 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Attention matrices are fundamental to transformer research, supporting broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the models global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, novel formulation that captures the entire transformer as single, input-dependent linear operator expressed through high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering theoretically coherent and expressive linear representation of the models computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as supplementary. https://github.com/idoatad/ TensorLens"
        },
        {
            "title": "Introduction",
            "content": "Transformer-based architectures (Vaswani et al., 2017) have revolutionized deep learning by exhibiting remarkable scaling properties, enabling effective models with millions or even billions of parameters that can be trained on extensive datasets containing trillions of tokens. This advancement has led to breakthroughs that include large language models (LLMs) such as ChatGPT (Brown 1 Figure 1: Transformers are re-formulated as datacontrolled linear operators, characterized by an inputdependent high-order attention tensor . This formulation enables unified self-attention representation that captures the entire Transformer architecture, including sub-components such as FFN layers, normalization, embedding layers, and residual connections. et al., 2020), Vision Transformers (Dosovitskiy et al., 2020), Diffusion Transformers (Peebles and Xie, 2023), and others. The core component of the Transformer responsible for capturing interactions between tokens is the self-attention mechanism. Self-attention can be viewed as data-controlled linear operator (Poli et al., 2023; Massaroli et al., 2020) that is represented by an input-dependent attention matrix. Due to the row-wise softmax normalization, these attention matrices are somewhat interpretable, offering insight into how each layer updates the output representations through weighted linear combination of input value vectors. As result, attention matrices have been employed in wide range of research domains, including (i) explainability and interpretability through attribution methods and model analysis (Abnar and Zuidema, 2020; Katz et al., 2024), (ii) model editing and intervention techniques (Chefer et al., 2023; Katz and Wolf, 2025; Ali et al., 2025a), (iii) distillation and training techniques (Touvron et al., 2021; Zhang et al., 2024), (iv) inductive bias and regularization methods (Li et al., 2018; Attanasio et al., 2022; Zimerman and Wolf), among others. To push these applications further, substantial effort has been invested in developing extended representations of attention that go beyond individual attention matrices. prominent example is the attention rollout technique (Abnar and Zuidema, 2020), which averages attention matrices across heads within the same layer and then integrates across the layers by applying multiplication. More recent approaches propose improved aggregation methods across heads. For example, Kobayashi et al. (2020) leverages the output projection layer to aggregate heads more precisely, and subsequent work incorporates the feed-forward layer into the formulation (Kobayashi et al., 2023). Additionally, in non-transformer models, implicit attention formulations have been introduced even for several architectures, including Mamba (Ali et al., 2025b; Dao and Gu, 2024), RWKV and Griffin (Zimerman et al., 2025), and others. Following this line of work, we ask: What is the most comprehensive formulation that attention can encompass in Transformers? Is it possible to represent the entire Transformer as data-controlled linear operator that captures all of its parameters and is theoretically grounded, rather than relying on heuristically aggregated attention matrices? We fundamentally address this question by reformulating the entire Transformer model, including all of its components (feed-forward networks (FFNs), activation functions, LayerNorm, skip connections, embedding layers, and others) as single data-controlled linear operator as visualized in Figure 1. key insight of our work is that such formulation requires high-order tensor attention tensors, not just matrices, to fully encompass the models behavior. Our formulation is theoretically grounded, and our empirical analysis shows that it better reflects the model than previously proposed attention forms. Moreover, we demonstrate that the tensor structure can approximate linear relations (Hernandez et al., 2024) better than the matrix alternatives, underscoring its capacity to reveal LLM functionalities previously explored in mechanistic interpretability research. Our main contributions are as follows: (i) We introduce TensorLens, novel high-order tensor formulation that represents the entire Transformer as data-controlled linear operator, yielding generalized attention maps that can replace standard attention matrices and their cross-layer aggregations. (ii) We provide theoretical justification showing that this formulation is principled, more precise than prior attention variants, and encompasses all model parameters. (iii) We empirically show that TensorLens better reflects model behavior through perturbation-based evaluations. Finally, (iv) we demonstrate that TensorLens provides robust foundation for mechanistic interpretability tools, such as approximating linear relations from LLM embeddings."
        },
        {
            "title": "2 Background & Related Work",
            "content": "This section provides the scientific context for discussing our approach to precisely aggregating attention matrices via high-order tensors. 2.1 Extended Attention Matrices Due to their importance, extended formulations of attention matrices have been widely explored over the years. In particular, Kobayashi et al. (2020) demonstrated that attention analysis can be refined by incorporating the output projection matrix when analyzing Transformer heads. Additionally, Kobayashi et al. (2021) proposed further refinement by incorporating the residual connections and normalization layers into the attention formulation, resulting in more precise formulation. These approaches were further extended by Kobayashi et al. (2023) who also incorporated the FFN sublayer into the attention analysis. Moreover, Abnar and Zuidema (2020) introduced the attention rollout technique, which aggregates attention weights across multiple layers by multiplying the per-layer attention matrices. The rollout method was applied by Modarressi et al. (2022) to aggregate the extended attention matrices of Kobayashi et al. (2021) across layers. Finally, most similarly to our work, Elhage et al. (2021) analyze 2 layer attentiononly transformers using 4th order tensors to describe the end-to-end function of the model. Our approach builds on these works by proposing more precise formulation that explicitly captures all Transformer blocks and their sub-components."
        },
        {
            "title": "2.2 Attention as High-Order Tensors",
            "content": "Several prior works have proposed architectures that extend the matrix-based self-attention mechanism to higher-order tensors (Omranpour et al., 2025; Ma et al., 2019; Gao et al., 2020; Zhang et al., 2025), primarily to enhance expressivity (Sanford et al., 2023). However, these approaches often 2 come at the cost of reduced efficiency, prompting efforts to improve their computational performance (Liang et al., 2024). While related, this line of work focuses on architectural modifications to the Transformer, rather than reinterpreting the vanilla self-attention mechanism through tensorbased formulation, as done in this work. 2.3 Explainability Attribution Methods Attribution methods aim to explain the decisions of neural networks (NNs) by quantifying the contribution of each neuron or input feature to the models output (Das and Rad, 2020). These tools are primarily used for interpretability and are crucial for making NNs more trustworthy and understandable (Doshi-Velez and Kim, 2017). Attribution can be either class-specific, where the explanation targets particular output class (for example, why the model predicted cat over dog), or classagnostic, where the method provides general explanation of the models behavior regardless of any specific output (Hassija et al., 2024). While classspecific methods are valuable for understanding individual decisions, class-agnostic methods offer insights into the models global processing, emergent patterns, and internal representations. Both perspectives are complementary and play central role in building explainable AI systems. Popular class-specific attribution methods include gradient-based techniques such as Input Gradient (Shrikumar et al., 2017; Baehrens et al., 2010), and Layer-wise Relevance Propagation (LRP) (Bach et al., 2015; Achtibat et al., 2024; Bakish et al., 2025). In contrast, common classagnostic methods include activation maximization (Erhan et al., 2009), probing techniques (Alain and Bengio, 2016), and the extraction of attention maps (Abnar and Zuidema, 2020). This paper focuses on developing class-agnostic explainability method for Transformers, based on more generalized and insightful formulation of attention matrices via Tensors."
        },
        {
            "title": "3 Method: TensorLens",
            "content": "A standard Transformer architecture with layers and hidden representations RLD for any [N ] is defined as follows: [N ] : n+1 = Transformern(X n) , (1) where each Transformer block is defined by: Zn = LayerNormn(Attentionn(X n) + n) , (2) n+1 = LayerNormn(FFNn(Zn) + Zn) . (3) Here, LayerNorm denotes the layer normalization operation (Ba et al., 2016), FFN is the feed-forward layer, and Attention is the self-attention layer. The superscript indicates the signals, parameters or operations corresponding to the n-th layer, and each intermediate representation is matrix in RLD, where is the sequence length and is the hidden dimension. We also assume that the input and output are multiplied by an embedding matrices Ein and Eout. Intuition. Our key insight is that each subcomponent of the Transformer can be represented as data-controlled linear operator defined by data-dependent matrix. However, while some components, such as attention, mix interactions between tokens, others, like the FFN, mix across dimensions. As result, their combination cannot be represented by single matrix. Instead, it requires tensor-based operator to capture both types of interactions. To materialize our insight, we show that each sub-layer in Transformers can be represented as tensor-based data-control linear operator in Section 3.2, followed by how these tensors can be aggregated to represent each block and the entire model in Sections 3.3, 3.4, and 3.5. schematic visualization of the method is presented in Figure 2."
        },
        {
            "title": "3.1 Prerequisites",
            "content": "Our formulation of the Transformer as tensors builds on the following rules for vectorizing matrix operations and tensor calculations (Itskov, 2007): Bilinear Map. bilinear map AXB can be vectorized using the Kronecker product as: vec(cid:2) (cid:124)(cid:123)(cid:122)(cid:125) LL (cid:124)(cid:123)(cid:122)(cid:125) LD (cid:124)(cid:123)(cid:122)(cid:125) DD (cid:3) = (cid:16) (cid:124) (cid:123)(cid:122) LDLD (cid:17) (cid:125) . vec [X] (cid:124) (cid:123)(cid:122) (cid:125) LD Matrix Multiplication. matrix multiplication XM can be vectorized as: (cid:16) (cid:3) = (cid:17) . vec(cid:2) IL (cid:124)(cid:123)(cid:122)(cid:125) LL (cid:124)(cid:123)(cid:122)(cid:125) LD (cid:124)(cid:123)(cid:122)(cid:125) DD IL (cid:123)(cid:122) LDLD (cid:124) vec [X] (cid:124) (cid:123)(cid:122) (cid:125) LD (cid:125) Element-wise Hadamard Product. An elementwise Hadamard product is vectorized as: (cid:124)(cid:123)(cid:122)(cid:125) LD (cid:3) = diag (vec [H]) (cid:125) (cid:123)(cid:122) LDLD (cid:124) . vec [X] (cid:124) (cid:123)(cid:122) (cid:125) LD vec(cid:2) (cid:124)(cid:123)(cid:122)(cid:125) LD 3 Figure 2: Method: schematic visualization of our method, where each sub-component of the transformer architecture, including self-attention, LayerNorm, FFNs, input and output embedding layers, and the residual connection (which is omitted here for simplicity), is formulated as data-control linear operator represented by high-order tensor in RLDLD. These tensors are composed into per-block tensors (n) for each layer [N ], which are then used to construct the final linear operator representing the entire Transformer. Tensor Contractions. For an input matrix RLD and 4th order tensor RLDLD, we define the tensor contraction (X) as: [L] : (X)[i,:] = (cid:88) j=1 T[i,:,j,:] (cid:124) (cid:123)(cid:122) (cid:125) DD X[j,:] (cid:124) (cid:123)(cid:122) (cid:125) RD . (4) Unfolding the tensor into matrix Tmat RLDLD, the vectorized tensor contraction follows vec [T (X)] = Tmatvec [X] . (5) In the following sections we overload notation, referring to Tmat as ."
        },
        {
            "title": "3.2 Block-by-Block Tensorization",
            "content": "We now show how each sub-layer in the Transformer architecture (LayerNorm, self-attention, FFN, residual) can be Tensorized into linear tensor form. For simplicity, in this section we omit superscripts and weight biases, derivation including biases is in Appendix B. Tensorized Self-Attention. Recall that given an input X, the multi-head self-attention layer with heads is parameterized by key Wk,h, query Wq,h, value Wv,h, and output Wo,h projections for each head [H], and is defined by: Attn(X) = (cid:88) h=1 Ah Wv,h Wo,h , (6) Ah = softmax(QhK ) , Qh = XWq,h, Kh = XWk,h . (7) (8) 4 Vectorising and grouping heads gives the following attention tensor A: vec[Attn(X)] = (cid:88) (cid:16) h=1 (cid:124) (Wv,hWo,h) Ah (cid:123)(cid:122) vec[X] . (cid:17) (cid:125) (9) Tensorized LayerNorm. Recall that LayerNorm applies an affine transformation based on the input statistics and operates independently on each token: LN(X) = γ Xµ σ + β , (10) where γ RD and β are learnable parameters, and µ and σ RL are the per-token statistics , all broadcasted to match RLD. With pre-computed variance σ2, the LayerNorm can be tensorized by: vec(cid:2)LN(X)(cid:3) = vec(cid:2)diag(cid:0) 1 σ (cid:1)X(ID 11 )diag(γ)(cid:3) (11) = (cid:2)(ID 11 )diag(γ)(cid:3) diag(cid:0) 1 σ (cid:124) (cid:123)(cid:122) vec[X] , (cid:1) (cid:125) where (ID 11 ) RDD is the mean centering function in matrix form, with 1 RD column vector of all ones. Tensorized FFN. Given an activation function ϕ, the FFN is defined by two linear layers as follows: FFN(X) = ϕ(XM1)M2. (12) The element-wise activation can be converted to Z, an input-dependent hadamard product ϕ(Z) and tensorized as: vec(cid:2)ϕ(Z)(cid:3) = diag (cid:124) (cid:18) vec (cid:20) ϕ(Z) (cid:21)(cid:19) (cid:123)(cid:122) Ψ (cid:125) vec[Z] . (13) Resulting in the full vectorized form of the FFN as follows: vec(cid:2)FFN(X)(cid:3) = (cid:0)M 2 IL (cid:124) 1 IL (cid:1)Ψ(cid:0)M (cid:123)(cid:122) (cid:1) (cid:125) vec[X] , (14) which is characterized by tensor M. Tensorized Residual. For some sub-layer g, the residual connection can be written as: Yres = + g(X) . (15) Vectorizing this equation yields: vec(cid:2)Yres (cid:3) = (cid:0)I + G(cid:1)vec(cid:2)X(cid:3) , (16) where is the tensor associated with (e.g. or for attention and FFN accordingly) and RLDLD an identity matrix."
        },
        {
            "title": "3.3 Transformer Block as Tensor",
            "content": "A Transformer block is defined in Eq.1 and is obtained by stacking the sub-layers according to Eq.2 and Eq. 3. Thus, stacking the tensors obtained from the self-attention, residual, normalizations and FFNs as in Eqs. (9), (11), (14) (16), produces the tensor associated with the n-th block: = Ln 2 (Mn + I) Ln 1 (An + I) , (17) for post-layernorm block. For similar derivation for pre-layernorm block, see Appendix B. Thus, the entire model is fully represented as chain of high-order tensor transformations: vec[F(X)] = vec[T (X)] = (cid:32) (cid:89) (cid:33) vec(cid:2)X(cid:3) , n=1 (19) which completes the transition from individual layer operations to unified tensor-based view of the full Transformer expressed by (X). Interpretation as Generalized Attention. We denote by the linearized representation of the entire Transformer, expressed as 4th-order tensor of dimensions D, where is the sequence length and is the hidden dimension. This tensor captures the influence of each input tokenchannel pair on every output tokenchannel pair. Conceptually, can be interpreted as generalization of the conventional attention matrix to higher-order attention tensor, modeling both intertoken dependencies and intra-token (cross-channel) interactions. In the unvectorized form, each position [L] in the output is obtained by sum of linear transformations of the input, which are defined by slices of the overall tensor: [L] : (X)[i,:] = (cid:88) T[i,:,j,:] (cid:124) (cid:123)(cid:122) (cid:125) DD [j,:] (cid:124) (cid:123)(cid:122) (cid:125) . (20) Our goal with generalized attention is not to propose new architecture, but to formulate attention in way that yields representation encapsulating more components and computations, following the line of work described in Section 2.1. Importantly, our method is not limited to end-to-end linearization alone. By restricting the composition to chosen subset of layers or heads, we can obtain generalized attention matrices at any desired granularity."
        },
        {
            "title": "3.5 From Tensor to Matrix by Collapsing",
            "content": "Given the tensor formulation of single Transformer block in Section 3.3 (Eq. 17), we now construct the full model as composition of such block tensors. Let denote the tensor representation of the n-th block, the entire Transformer function can be expressed as nested application of block tensors over the input sequence denoted as 0 = X, yielding the following recursive structure: F(X) = (N ) (n1) (1)(X) . (18) While the tensor representation provides richer and more comprehensive view of Transformer computations, it is often less interpretable and more difficult to visualize due to its 4th-order structure and the sheer number of elements (L2D2 in total). To address this, we propose simple yet effective technique for collapsing the tensor into more compact, matrix-like form, akin to the standard attention matrix. Specifically, we reduce the channel dimensions using the following three approaches: (i) Norm over feature dimensions: By taking the 5 norm of the dimension related to the channel as follows: i, [L] : Norm ij = (cid:13) (cid:13)T[i,:,j,:] (cid:13) (cid:13)2 , (21) resulting in matrix Norm RLL. (ii) Projection using output and input embedding vectors: Let 0, RLD be the hidden states inserted and extracted from the Transformer layers. We contract over the channel dimensions using 0, as both input and output projection weights: i, [L] : IO ij = [i,:]T[i,:,j,:]X 0 [j,:] . (22) Following Eq. 20, taking an inner product with [i,:] on both sides yields: [i,:] [i,:] = [i,:] (cid:88) T[i,:,j,:]X 0 [j,:] , (23) (cid:13) (cid:13)X (cid:13) [i,:] (cid:13) 2 (cid:13) (cid:13) (cid:88) = IO i,j , meaning IO 0 i,j reflects the contribution of the input [i,:]. [j,:] to the output This approach can be applied to the entire Trans- (iii) former or to selected subset of blocks. Class-specific projection using output embedding matrices: For chosen output class/token c, [:,c] RD be the corresponding column in the let Eout classification head/unembedding matrix, we contract the tensor over the channel dimensions using 0, Eout [:,c] to get: i, [L] : CLS (c,i)j = Eout [:,c]T[i,:,j,:]X 0 [j,:] . (24) Similarly to Eq. 23, for each class and output position i, the input contributions CLS (c,i)j sum up to the logit of the class (excluding biases, see Appendix B). Eq. 24 provides comprehensive view of the Transformer as linear operator. It encapsulates all model parameters, including the Transformer and embedding layers, and serves as direct local approximation of the full Transformer computation. As the first formulation that explicitly captures all model parameters within unified tensor-based representation, it offers principled foundation for analyzing and interpreting Transformer computations through the lens of high-order linear operators. 6 3.6 Theoretical Analysis Our method relies on linearization of the entire Transformer computation at given input. natural question arises: how well does this linearization approximate the original function locally? We address this in Proposition 1, where we provide data and model-dependent bound on the forward approximation error. It is important to note that alternative methods, which either do not incorporate all model parameters in their formulation or avoid tensor-level operations, are generally not capable of producing such bounds. Even when they can be applied, the resulting approximations are typically significantly looser. Proposition 1. The approximation error of the tensor TX computed on input X, when evaluating the transformer function at (X + ϵ) is bounded by: TX (X + ϵ) (X + ϵ)2 TX 2 ϵ2 + (X + ϵ) (X)2 , where TX 2 is bounded by constants of the transformer weights. (25) The complete proof is provided in Appendix E. The core intuition is that each sub-component of the Transformer can be linearly approximated using tensor operations, enabling the error to be bounded by recursively applying standard first-order linear approximation techniques, which can be composed across layers to yield global bound."
        },
        {
            "title": "4 Experiments",
            "content": "We empirically assess the representation power of our tensor formulation as proxy for Transformer behavior, comparing it to other attention aggregation techniques via perturbation tests in Section 4.1. Then, in Section 4.2, we demonstrate that the tensor representation is valuable tool for mechanistic interpretability and model understanding."
        },
        {
            "title": "4.1 Perturbation Tests",
            "content": "To assess the representational power of our attention aggregation method, we adopted an input perturbation scheme similar to Chefer et al. (2021); Ali et al. (2022). This evaluation strategy gradually masks input tokens in the order determined by their computed relevance scores. When the highest-scoring tokens are masked first (positive perturbation), we expect the models accuracy to rapidly decline. We assess explanation quality using the Area Under the Curve (AUC) metric, which Figure 3: Perturbation Tests in Vision: Effect of perturbations on final hidden representations of DeiT-Base. Measured by the mean squared error between the last hidden-state of the [CLS] token in the original and perturbed input (higher is better). Figure 4: Perturbation Tests in NLP: Effect of token perturbations on final hidden representations of BERT-Base. captures the models accuracy as function of the percentage of masked input elements, ranging from 0% to 30%. As baselines, we use eight aggregation variants that combine two methods for cross-layer aggregation and four methods for intra-layer aggregation. For cross-layer aggregation, we apply either multiplicative composition, as in Attention Rollout (Abnar and Zuidema, 2020) (Rollout), or simple averaging (Mean). For intra-layer aggregation, we use the following four methods: (i) averaging of attention matrices (Attn), (ii) valueweighted attention as proposed by Kobayashi et al. (2020) (W. Attn), (iii) value-weighted attention that also includes the residual connection and LayerNorm as proposed by Kobayashi et al. (2021) (W. AttnResLN), and (iv) global encoding variant (GlbEnc) that further incorporates the second LayerNorm into the formulation (Modarressi et al., 2022). We compare these class-agnostic baselines with the variants defined in Eqs. 21 and 22, denoted as Tensor,Norm and Tensor,In+Out, respectively. Perturbation in Vision. In the vision domain, we evaluate our methods using DeiT by Touvron et al. (2021), on the ImageNet-1K test set, considering both the base and small model sizes. Results for the base model are shown in Fig. 3. As can be seen, across all perturbation levels, tensor-based aggregation methods consistently outperform the baselines. When incorporating both input and output embeddings (Tensor,In+Out), the total AUC exceeds 0.82, and reaches 0.66 when using the tensor norm (Tensor,Norm). In contrast, all aggregation methods that are not based on tensors fall below 0.6, highlighting the superior robustness of our formulation. The results for DeIT-small, which follow similar trend are provided in Appendix A. Perturbations in NLP. In the NLP domain, we evaluate our method across several models on sequences of length 128, including both encoderonly and decoder-only architectures. For the encoder-only setting, we conduct experiments with BERT (Lu et al., 2019) and RoBERTa (Liu et al., 2019) on the IMDB dataset. Results for BERT are shown in Figure 4, where tensor-based aggregation 7 (a) Bias (b) Common Sense (c) Factual Figure 5: Relation Decoding: Accuracy relative to original model computation, for different relation categories on Pythia-1B, with = 3 training samples per relation. Results are averaged across 6 train-test splits, with standard deviation shown in error bars. Random baselines shown as horizontal dashed lines. consistently outperforms all baselines across all perturbation levels. When incorporating both input and output embeddings (Tensor,In+Out), the total AUC exceeds 0.158, and reaches 0.101 when using only the tensor norm (Tensor,Norm). In contrast, all non-tensor aggregation methods fall below 0.09, underscoring the superior robustness of our formulation. Moreover, in Appendix A, we also report results for RoBERTa in Table 1, as well as for the more recent ModernBert (Warner et al., 2024) and Gemma3 (Team et al., 2025) in Table 2. In these figures, the observed pattern closely aligns with that of BERT. When evaluating decoder-only models, the results are less clear-cut. In this setting, we tested several LLMs, including Pythia-1B (Biderman et al., 2023), Pico-570M (Martinez et al., 2024), and Phi1.5 (Li et al., 2023), using the WikiText-103 dataset. As shown in Table 1 (Appendix A), although our method consistently achieved the top or secondbest AUC scores across benchmarks, the overall findings are less conclusive. One possible explanation is that auto-regressive language models are trained to predict the next token and therefore tend to exhibit inherently local behavior (Fang et al., 2024). This characteristic may reduce the informativeness of perturbation-based evaluations, making the results appear less definitive."
        },
        {
            "title": "4.2 Approximation of Relation Decoding",
            "content": "The tensor formulation , which we uncover from the forward pass of the model, mathematically describes linear transformation between tokens in the same sentence. In this section we evaluate the quality of our method as local approximation for the Transformer computation through the lens of linear relation decoding. Introduced by Hernandez et al. (2024), linear relation decoding examines sets of relations, such as teacher typically works at school, composed of triplets (s, r, o) connecting subject to an object via relation r. Hernandez et al. (2024) illustrate how to produce transformation between tokens embeddings, such as ones that output school for teacher or hospital for doctor. Their method was based on approximating the Jacobian matrix of the models prediction relative to the subject token s. Since our tensor formulation is multi-linear transformation that describes such input-output relations, our goal is to examine to what extend it can match the linear representations performances of Hernandez et al. (2024) which were tailored for this task. In order to create per-relation transformation, we compute the mean tensor extracted from examples Xi = (si, r, oi) of relation r: (cid:101)Tr = 1 (cid:88) i=1 TXi , (26) and measure the similarity of the tensor function (cid:101)Tr (X) to that of the original model, on held-out test set of subject-object pairs of the same relation. For experimental setup, we follow Hernandez et al. (2024) and prepend each example Xi in the mean calculation with the remaining 1 train examples as few-shot examples, so that the model is more likely to generate the answer given under the relation over other plausible tokens. Further experimental details are described in Appendix D. We report the approximation accuracy 8 as the percentage of examples in which the toppredicted object matches the original output. As seen in Figure 5, approximating the models computation using our tensor method achieves higher accuracy than the LRE baseline of Hernandez et al. (2024) on most relations examined. In some tasks, such as occupation-age, we found both methods to achieve results close to that of random guess, which we associate with the inherent limitation of describing the models internal processes solely via linear transformations of the input. Overall, it is evident that our multi-linear approximation provides better capacity than previous linear methods to describe the function of the entire model as whole. We find these results to strengthen our claim that the tensor formulation reflects the models internal representations."
        },
        {
            "title": "5 Conclusions",
            "content": "This work presents technique for aggregating attention matrices across both Transformer blocks and all sub-components within each block. The resulting formulation is theoretically grounded and more comprehensive than prior approaches and it is based on representing the Transformer as high-order data-controlled linear operator. This formulation captures the internal interactions of the model, including contributions from components such as the FFN, embeddings, LayerNorm, and others. Practically, we emphasize that this formulation can be used as drop-in replacement for attention matrices and their aggregations, in order to enhance many existing interpretability, analysis, and intervention techniques. An example of direct application to mechanistic interpretability and model understanding is demonstrated in our relation-based analysis."
        },
        {
            "title": "Limitations",
            "content": "While TensorLens offers more precise formulation of the Transformer through self-attentionbased representation compared to prior work, it has several limitations. First, some of the linearization techniques are chosen for their simplicity rather than being derived from the intrinsic properties of optimally approximated tensors. For example, this includes the activation decomposition in Eq. 13. Second, the high-order tensor representation is GPU-memory intensive. We partially mitigate this with memory-optimized computation method as described in Appendix C, however, our experiments are limited to models up to 1B parameters and moderate input lengths. Third, although the formulation is comprehensive and practical for visualization and model interpretation, the full potential of the tensor-based approach remains underexplored. In particular, it opens the door to new perspectives on rank collapse, sparsity, and training dynamics through the lens of tensor properties."
        },
        {
            "title": "Ethics Statement",
            "content": "This work focuses on developing theoretically grounded and interpretable representation of Transformer-based models via high-order attention tensors. Our research does not involve human subjects, personally identifiable data, or the generation of potentially harmful content. All evaluations are conducted on publicly available datasets such as ImageNet, IMDB, and WikiText-103, adhering to their respective licenses and intended usage. We acknowledge that improved model interpretability tools, such as those proposed in this work, may be used both to enhance trust in machine learning systems and to expose or exploit model vulnerabilities. We believe that the positive implications such as greater transparency, accountability, and error analysis outweigh potential misuse. Nonetheless, we encourage responsible use of our methods in alignment with ethical AI principles."
        },
        {
            "title": "References",
            "content": "Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. arXiv preprint arXiv:2005.00928. Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, and Wojciech Samek. 2024. Attnlrp: attention-aware layer-wise relevance propagation for transformers. arXiv preprint arXiv:2402.05602. Guillaume Alain and Yoshua Bengio. 2016. Understanding intermediate layers using linear classifier probes. arXiv preprint arXiv:1610.01644. Ameen Ali, Thomas Schnake, Oliver Eberle, Grégoire Montavon, Klaus-Robert Müller, and Lior Wolf. 2022. Xai for transformers: Better explanations through conservative propagation. In International conference on machine learning, pages 435451. PMLR. Ameen Ali Ali, Shahar Katz, Lior Wolf, and Ivan Titov. 2025a. Detecting and pruning prominent but detrimental neurons in large language models. In Second Conference on Language Modeling. Ameen Ali Ali, Itamar Zimerman, and Lior Wolf. 2025b. The hidden attention of mamba models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15161534. Giuseppe Attanasio, Debora Nozza, Dirk Hovy, and Elena Baralis. 2022. Entropy-based attention regularization frees unintended bias mitigation from lists. arXiv preprint arXiv:2203.09192. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. 2016. Layer normalization. stat, 1050:21. Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140. Arun Das and Paul Rad. 2020. Opportunities and challenges in explainable artificial intelligence (xai): survey. arXiv preprint arXiv:2006.11371. Finale Doshi-Velez and Been Kim. 2017. Towards rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, and 1 others. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, and 1 others. 2021. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12. Baehrens, Schroeter, Harmeling, Kawanabe, Hansen, and K-R Müller. 2010. How to explain individual classification decisions. Journal of Machine Learning Research. Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2009. Visualizing higher-layer features of deep network. University of Montreal, 1341(3):1. Yarden Bakish, Itamar Zimerman, Hila Chefer, and Lior Wolf. 2025. Revisiting lrp: Positional attribution as the missing ingredient for transformer explainability. arXiv preprint arXiv:2506.02138. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, and 1 others. 2023. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110. Hila Chefer, Shir Gur, and Lior Wolf. 2021. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 782791. Tri Dao and Albert Gu. 2024. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. 2024. What is wrong with perplexity for long-context language modeling? arXiv preprint arXiv:2410.23771. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. 2020. Kronecker attention networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 229 237. Vikas Hassija, Vinay Chamola, Atmesh Mahapatra, Abhinandan Singal, Divyansh Goel, Kaizhu Huang, Simone Scardapane, Indro Spinelli, Mufti Mahmud, and Amir Hussain. 2024. Interpreting black-box models: review on explainable artificial intelligence. Cognitive Computation, 16(1):4574. Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, Yonatan Belinkov, and David Bau. 2024. Linearity of relation decoding in transformer language models. In The Twelfth International Conference on Learning Representations. Mikhail Itskov. 2007. Tensor algebra and tensor analysis for engineers: with applications to continuum mechanics. Springer. Shahar Katz, Yonatan Belinkov, Mor Geva, and Lior Wolf. 2024. Backward lens: Projecting language model gradients into the vocabulary space. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2390 2422. 10 Shahar Katz and Lior Wolf. 2025. Reversed attention: On the gradient descent of attention layers in GPT. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 11251152, Albuquerque, New Mexico. Association for Computational Linguistics. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only weight: Analyzing transformers with vector norms. arXiv preprint arXiv:2004.10102. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2021. Incorporating residual and normalization layers into analysis of masked language models. arXiv preprint arXiv:2109.07152. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2023. Analyzing feed-forward blocks in transformers through the lens of attention maps. arXiv preprint arXiv:2302.00456. Jian Li, Zhaopeng Tu, Baosong Yang, Michael Lyu, and Tong Zhang. 2018. Multi-head attention with disagreement regularization. arXiv preprint arXiv:1810.10183. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463. Yingyu Liang, Zhenmei Shi, Zhao Song, and Yufa Zhou. 2024. Tensor attention training: Provably efficient learning of higher-order transformers. arXiv preprint arXiv:2405.16411. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32. Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song. 2019. tensorized transformer for language modeling. Advances in neural information processing systems, 32. Richard Diehl Martinez, Pietro Lesci, and Paula Buttery. 2024. Tending towards stability: Convergence challenges in small language models. arXiv preprint arXiv:2410.11451. Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. 2020. Dissecting neural odes. Advances in neural information processing systems, 33:39523963. Ali Modarressi, Mohsen Yadollah Fayyaz, Yaghoobzadeh, and Mohammad Taher Pilehvar. 2022. GlobEnc: Quantifying global token attribution by incorporating the whole encoder layer in transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 258271, Seattle, United States. Association for Computational Linguistics. Soroush Omranpour, Guillaume Rabusseau, and Reihaneh Rabbany. 2025. Higher order transformers: Efficient attention mechanism for tensor structured data. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. 2023. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pages 2804328078. PMLR. Clayton Sanford, Daniel Hsu, and Matus Telgarsky. 2023. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36:3667736707. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning important features through propagating activation differences. In International conference on machine learning, pages 31453153. PMLR. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, and 1 others. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. 2021. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 1034710357. PMLR. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. 2024. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. Preprint, arXiv:2412.13663. 11 Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Ré. 2024. Lolcats: On low-rank linearizing of large language models. arXiv preprint arXiv:2410.10254. Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, and Andrew Yao. 2025. Tensor product attention is all you need. arXiv preprint arXiv:2501.06425. Itamar Zimerman, Ameen Ali, and Lior Wolf. 2025. Explaining modern gated-linear rnns via unified implicit attention formulation. In ICLR. Itamar Zimerman and Lior Wolf. Viewing transformers through the lens of long convolutions layers. In Fortyfirst International Conference on Machine Learning."
        },
        {
            "title": "A Additional Perturbation Experiments",
            "content": "In addition to the perturbation tests presented in Section 4.1, this section presents further experiments with RoBERTa, DeiT-small, and the more recent ModernBert and Gemma3. The results are shown in Figures 6 and 7, respectively. As illustrated, across all benchmarks, our method achieves higher AUC scores, consistently outperforming the baselines for all perturbation fractions. Furthermore, in Table 1, we present perturbation results for decoder-only models, while Table 2 reports results for more modern models, including ModernBert (Warner et al., 2024) and Gemma3 (Team et al., 2025). Figure 6: Perturbation Tests in NLP: Effect of token perturbations on final hidden representations of RoBERTaBase. Measured by the mean squared error between the last hidden-state of the [CLS] token in the original and perturbed input (higher is better) Figure 7: Perturbation Tests in Vision: Effect of perturbations on final hidden representations of DeiT-Small. Method: Tensor (ours) Rollout over layers Mean over heads & layers LLM Metric Norm In+Out In+Class Attn W-Attn W-AttnResLN GlbEnc Attn W-Attn W-AttnResLN GlbEnc Pyth Pico Phi HS-MSE AOPC HS-MSE AOPC HS-MSE AOPC 3.708 0.147 0.222 0.129 0.892 0.141 3.603 0.146 0.223 0.128 0.887 0. 3.683 0.147 0.22 0.129 0.886 0.143 0.306 0.044 0.03 0.019 0.079 0. 0.213 0.035 0.014 0.020 0.067 0.023 0.233 0.037 0.090 0.067 0.053 0. N.A N.A 0.036 0.036 N.A N.A 3.483 0.141 0.211 0.125 0.864 0. 3.650 0.147 0.22 0.127 0.905 0.141 3.708 0.148 0.222 0.129 0.934 0. N.A N.A 0.225 0.129 N.A N.A Table 1: Next Token Prediction Perturbation. Results are AUC (higher is better) of (i) HS-MSE: Mean squared error between the last hidden-state of the final token in the original and perturbed input. (ii) AOPC: Absolute difference of the soft-maxed probability of the original predicted token, between the original and perturbed input. The GlbEnc results are not presented for the Pythia and Phi models, since their method is inapplicable for parallelresidual architectures. Pyth for Pythia. 13 LLM Method: Metric Tensor (ours) Rollout over layers Mean over heads & layers Norm In+Out Attn W-Attn W-AttnResLN GlbEnc Attn W-Attn W-AttnResLN GlbEnc ModernBert HS-MSE 0.081 Gemma3 HS-MSE 0.029 0.112 0.049 0.05 0.014 0. 0.014 0.064 0.019 0.063 0.017 0. 0.023 0.072 0.024 0.077 0.02 0. 0.021 Table 2: Perturbation Tests in NLP with Modern Models: Effect of token perturbations on final hidden representations of ModernBert-Base and Gemma3-270M, trained for sentiment prediction on the IMDB dataset. Results are AUC of HS-MSE: Mean squared error between the last hidden-state of the final token in the original and perturbed input (higher is better)."
        },
        {
            "title": "B Tensor Derivation with Biases",
            "content": "Here we reiterate the tensor derivation introduced in Section 3 while including the transformer weight biases. The perturbation experiments in Section 4.1 use the tensor without biases as described in Section 3, and the relation decoding experiments in Section 4.2 use the full affine transformation described here. We denote the model biases as RLD, broadcasting the original RD biases to each sequence position L. For each module in the transformer block, we get an affine transformation of the form: vec(cid:2)f (X)(cid:3) = (f )vec[X] + vec[B(f )] Tensorized Self-Attention. For an input RLD, multi-head self attention is defined by: Attn(X) = (cid:88) h=1 Ah (X Wv,h + Bv,h) Wo,h + Bo,h = (cid:88) h= Ah (X Wv,h) Wo,h + Battn , where Battn = Bv,hWo,h + Bo,h. The biases of the query and key projections are absorbed in the attention matrix Ah. Vectorising and grouping heads gives the attention tensor A: vec[Attn(X)] = (cid:88) (cid:16) h=1 (cid:124) (Wv,hWo,h) Ah (cid:123)(cid:122) (cid:17) (cid:125) vec[X] + vec[Battn] RLD . where RLDLD is flattened to matrix as defined in Eq. (5). Tensorized LayerNorm. With weights γ RDD and bias β RLD, the LayerNorm is similarly vectorized as: LayerNorm(X) = γ Xµ σ2+ε + β , vec(cid:2)LayerNorm(X)(cid:3) = (cid:2)(ID 11 (cid:124) )diag(γ)(cid:3) diag(cid:0) (cid:123)(cid:122) vec[X] + vec[β] RLD . 1 σ2+ε (cid:1) (cid:125) Tensorized FFN Given an activation function ϕ, the FFN is defined by two linear layers as follows: FFN(X) = ϕ(XM1 + BM1)M2 + BM2. The element-wise activation can be converted to an input-dependent hadamard product ϕ(Z) Z, and tensorized as: vec(cid:2)ϕ(Z)(cid:3) = diag (cid:18) vec (cid:20) ϕ(Z) (cid:21)(cid:19) vec[Z] . (cid:125) (cid:124) (cid:123)(cid:122) Ψ 14 Resulting in the full vectorized form of the FFN as follows: vec(cid:2)FFN(X)(cid:3) = (cid:0)M (cid:1) 2 IL 1 IL (cid:124) (cid:1)Ψ(cid:0)M (cid:123)(cid:122) vec[X] + vec[BM2] + (cid:0)M 2 IL (cid:123)(cid:122) vec[BFFN] (cid:124) (cid:125) (cid:1)Ψvec[BM1] (cid:125) RLD . which is characterized by tensor M. Transformer Block as Tensor. As defined in Eq. (17), stacking the tensors obtained from the selfattention, residual, normalizations, produces the tensor associated with the n-th post-layernorm block: = Ln 2 (Mn + I) Ln 1 (An + I) + vec[Bn block] Where the bias of each sub-module is transformed by the following ones as: 1 ] + Ln 2 ] + Ln FFN] + Mn (vec[βn block] = vec[βn 2 (vec[Bn vec[Bn 1 vec[Bn attn])) The derivation for pre-layernorm block is obtained similarly, by changing the order of the components: 2 ) (I + AnLn Entire Transformer as Tensor. As shown in Eq. (19), the entire model is fully represented as chain of high-order tensor transformations. Adding biases results in the final affine transformation: = (I + MnLn 1 ) + vec[Bn block] vec[F(X)] = (cid:32) (cid:89) (cid:33)"
        },
        {
            "title": "T n",
            "content": "n=1 vec(cid:2)X(cid:3) + vec[Bfull] , where the bias of each block is recursively transformed by the following ones: (cid:17) vec[Bfull] = vec[BN block] + (cid:16) vec[BN block ] + . Memory-Efficient Tensor Computation The tensor computation introduced in Section 3 relies on multiplications of large matrices in RLDLD, which may be prohibitive for larger models and longer input sequences. In practice, we use memoryefficient computation method based on the following observations. (i) Given an input X, patching the original Transformer function to use the precomputed attention matrices, FFN activations, and LayerNorm variance from the forward pass on yields linear function (cid:101)FX whose Jacobian is exactly the desired tensor: TX = (cid:101)FX (X) RLDLD. (27) (ii) The tensor TX RLDLD captures the influence of each input token-channel pair on every output token-channel pair. Since in both the input attribution and relation decoding experiments we are only interested in the influence on single output position ℓout [L] (either the last token or the [CLS] token), it suffices to compute only the 3-dimensional tensor slice corresponding to that position, i.e., TX[ℓout,:,:,:] RDLD. Thus, in our experiments we compute only this 3-d slice using the Jacobian of the patched Transformer function, as in Eq. (27). If needed, the full 4-d Jacobian and tensor can be computed in memory-efficient manner using forward-mode differentiation, such that the full tensor is never materialized on the GPU. This is done by applying the patched Transformer function (cid:101)FX to unit (basis) matrices Eℓ,d RLD, (cid:0)Eℓ,d(cid:1) i,j = (cid:40) 1, 0, = ℓ = else, such that ℓ [L], [D] : TX[:,:,ℓ,d] = (cid:101)FX (cid:16) Eℓ,d(cid:17) . (28) This allows computing the entire tensor using (possibly batched) forward passes, trading GPU memory for compute time."
        },
        {
            "title": "D Relation Decoding Experiment",
            "content": "We mostly adopt the experimental setup and relations dataset introduced in Hernandez et al. (2024), using the relation categories of bias, common sense, and factual. Although, in order to adapt to our tensor method we introduce several changes: (i) In order to perform the mean tensor approximation in Eq (26), we must filter the samples within each relation to those of the most common token length. (ii) Due to limited academic computational resources, we evaluate on Pythia-1B, which is smaller model than used by Hernandez et al. (2024). (iii) Since we use smaller LM, we further filter the test samples only to those in which the correct object is within the top-20 tokens predicted by the model. For each relation type we use = 3 training examples to compute the mean tensor, and the LRE weights of Hernandez et al. (2024), and report results averaged on 6 random seeds of train-test splits. For the mean tensor calculation in Eq. (26), we use the full affine transformation with biases described in Appendix to obtain: (cid:101)Tr = 1 (cid:88) (TXi + Bi) . This is necessary to obtain an accurate approximation of the original model. Although the LRE method was developed for extracting linear relations from intermediate hidden-state, we compare it to ours using the input embeddings for legitimate comparison. This method was shown as an ablation in their work. Additionally, the LRE baseline requires an additional hyper-parameter β which scales the Jacobian matrices in their method. We use their default value for Pythias GPT-NeoX architecture of β = 2.5, which gave the best results in grid-search of other proposed values in their repository."
        },
        {
            "title": "E Tensor Approximation Error Bound",
            "content": "Proposition 1 states that the approximation error of the tensor TX computed on input X, when evaluating the transformer function at (X + ϵ) is bounded by: TX (X + ϵ) (X + ϵ)2 TX 2 ϵ2 + (X + ϵ) (X)2 . (29) Here we prove this claim and provide the explicit bound on the spectral norm of the tensor TX 2, when flattened as matrix in RLDLD. First, using the full derivation with biases in Appendix B, for any input embedding RLD and perturbation ϵ RLD we have: TX (X + ϵ) (X + ϵ)2 = TX vec [X + ϵ] + vec [BX ] vec [F(X + ϵ)] = TX vec [ϵ] vec [F(X) F(X + ϵ)]2 TX 2 ϵ2 + F(X + ϵ) F(X)2 (30) To bound TX 2, we bound the spectral norm of the tensor of each sub-module of the transformer block when flattened as matrix (as defined in Section 3), and then combine the bounds within the block and across layers. Self Attention. Denoting the combined value-output projection per head Wv,hWo,h as vo we get: Ah (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 x1 Ah (cid:113) Ah A2 = (cid:17) (cid:16) (cid:88)"
        },
        {
            "title": "W h\nvo",
            "content": "(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) hH (cid:13) (cid:88) (cid:13)W (cid:13) (cid:13) (cid:13)W (cid:13) hH (cid:13) (cid:13) (cid:13)2 (cid:88) vo vo (cid:13) (cid:13) (cid:13)2 (31) 16 FFN. The tensor of the FFN block for input is defined as: (cid:20) ϕ(X) M = (cid:0)M 2 IL (cid:1)diag vec (cid:18) (cid:21)(cid:19) (cid:0)M 1 IL (cid:1) , with the element-wise activation function ϕ. Standard choices of ϕ such as GELU and SilU follow ϕ(x) x, so we have: (cid:18) diag vec (cid:13) (cid:13) (cid:13) (cid:13) (cid:20) ϕ(X) (cid:21)(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)2 = (cid:13) (cid:13) (cid:13) (cid:13) ϕ(X) (cid:13) (cid:13) (cid:13) (cid:13) 1 Overall for the whole FFN: M2 M22 M12 (32) LayerNorm. The LayerNorm tensor is defined as: LX = (cid:2)(ID 11 )diag(γ)(cid:3) diag(cid:0) 1 σX (cid:1) , For the left side of the mean centering matrix and γ we have: (cid:13) (cid:13)ID 11 (cid:13) 1 , diag(γ)2 = γ (cid:13) (cid:13) (cid:13)2 And for the right side of the variance σX RLL we have: (cid:13) (cid:13)diag(cid:0) 1 (cid:13) 1 σX (cid:13) (cid:13) (cid:13) σX = (cid:1)(cid:13) (cid:13) (cid:13)2 (cid:13) (cid:13) (cid:13) = max lL"
        },
        {
            "title": "1\nVar (cid:2)X[l,:]",
            "content": "(cid:3) Where Var (cid:2)X[l,:] (cid:3) is the variance of at position L. Importantly, this is the only data-dependent quantity in our bound, depending on the minimal variance of each of the hidden-states X[l,:] RD at the input to the layer norm. We denote it as and the overall bound for the LayerNorm tensor is: ξ(LN,X) = min lL Var (cid:2)X[l,:] (cid:3) , Lx2 γ ξ(LN,X) . (33) Whole Transformer. The tensor of post-layernorm tramformer layer is Combining the bounds of each component from Eq. (31),(32),(33) we get: for the whole transformer: = Ln 2 (Mn + I) Ln 1 (An + I) . TX 2 (cid:89) n=1 (cid:89) n=1 Ln 2 (Mn + 1) Ln 1 (An + 1) 2 2 ,X) (cid:0) γn ξ(LNn 1 1 ,X) γn ξ(LNn (M 1 2 2 2 + 1) (cid:88) (cid:13) (cid:13)W (cid:13) vo (cid:13) (cid:13) (cid:13) + 1(cid:1) (35) Together with Eq. 30, this completes the proof of Proposition 1. We note that although this bound is data-dependent, the value of is typically small constant. Lx2 γ ξ(LN,X)"
        }
    ],
    "affiliations": [
        "Blavatnik School of Computer Science and AI, Tel Aviv University"
    ]
}