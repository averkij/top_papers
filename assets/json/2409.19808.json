{
    "paper_title": "Can Models Learn Skill Composition from Examples?",
    "authors": [
        "Haoyu Zhao",
        "Simran Kaur",
        "Dingli Yu",
        "Anirudh Goyal",
        "Sanjeev Arora"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization -- the capacity to combine learned skills in novel ways not encountered during training -- has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the SKILL-MIX evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified $k$-tuple of language skills. While small models struggled with composing even with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and $6$. In this paper, we employ a setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills -- including rhetorical, literary, reasoning, theory of mind, and common sense -- GPT-4 was used to generate text samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of $k$, revealed the following findings: (1) Training on combinations of $k=2$ and $3$ skills results in noticeable improvements in the ability to compose texts with $k=4$ and $5$ skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 8 0 8 9 1 . 9 0 4 2 : r Can Models Learn Skill Composition from Examples? Haoyu Zhao1,2 Simran Kaur1,2 Dingli Yu1,2 Anirudh Goyal3 Sanjeev Arora1,2 1 Department of Computer Science, Princeton University 2 Princeton Language and Intelligence (PLI), Princeton University 3 Meta {haoyu,arora}@cs.princeton.edu"
        },
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalizationthe capacity to combine learned skills in novel ways not encountered during traininghas garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. recent study introduced the SKILL-MIX [32] evaluation, where models are tasked with composing short paragraph demonstrating the use of specified k-tuple of language skills. While small models struggled with composing even with = 3, larger models like GPT-4 performed reasonably well with = 5 and 6. In this paper, we employ setup akin to SKILL-MIX to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing diverse set of language skillsincluding rhetorical, literary, reasoning, theory of mind, and common senseGPT-4 was used to generate text samples that exhibit random subsets of skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of k, revealed the following findings: Training on combinations of = 2 and 3 skills results in noticeable improvements in the ability to compose texts with = 4 and 5 skills, despite models never having seen such examples during training. When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills. This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models."
        },
        {
            "title": "Introduction",
            "content": "Todays large language models (LLMs) exhibit many impressive skills but remain imperfect in key areas. Arguably, one significant limitation stems from their difficulty in combining or composing the skills they have already learned. For example, solving math problem on specific topic may require certain set of skills, while solving more challenging question may require applying broader combinations of more skills as compared to simple question. Understanding how well models can learn to compose skills with limited training examples is therefore crucial area of investigation. Let us note why this is nontrivial challenge. If model has learned base skills, and we want it to be able to compose any subset of skills, there are (cid:0)N (cid:1) possible combinations of interest. Since (cid:1) scales roughly with the k-th power of , even reasonable-sized training datasets will likely omit (cid:0)N Preprint. Under review. Figure 1: Pipeline for evaluating the generalization capability to combine skills. We split the language skill set from [32] into training skills Strain and held-out skills Sheld-out, and the topic set into training topics Ttrain and held-out topics Theld-out. The pipeline consists of three steps: (1) generate data by prompting GPT-4. The training texts contain only training skills Strain and training topics Ttrain, and each text exhibits at most 3 skills; (2) fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 using the generated data; (3) evaluate the fine-tuned models on held-out skills Sheld-out and held-out topics Theld-out with the number of requested skills being as large as 5. See our detailed setups in Section 3. examples of many skill combinations. As result, the model must be capable of generalizing to unseen combinations of skills. This issue was highlighted in [1], which introduced simple mathematical framework to demonstrate that current LLM scaling laws [12] suggest scaling up models can enhance their ability to combine skillsthough the ability increases only gradually with model size. This prediction was validated through the SKILL-MIX evaluation [32], which directly tested models capability to combine language skills that were listed in the models prompt (see Section 2 for details). The evaluation revealed that top-tier models like GPT-4 could successfully combine 5 or 6 language skills when composing short piece of text, whereas smaller models like LLaMA-2-70B-Chat struggled to combine even 3 skills. This finding of SKILL-MIX evaluation raises an interesting question: even if pre-training does not fully endow models with the ability to combine skills, can the capability be induced through fine-tuning on suitable data? The current paper investigates this question within the context of the language skills explored in the SKILL-MIX evaluation [32]. 1.1 Our contributions We address the question posed above by fine-tuning smaller models, including LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2, using small yet high-quality dataset generated by GPT-4. This dataset consists of 13,957 text pieces, each composed of randomly selected skills with = 1, 2, 3. We evaluate the capability of the fine-tuned models to combine an another set of held-out skills with potentially higher k. In particular, we divide the original SKILL-MIX [32] skill set into training set and held-out set, based on skill categories, to minimize correlations between the two groups. This ensures clearer evaluation of the models ability to generalize to unseen skills. Figure 1 and Section 3 detail the full pipeline of our data generation and evaluation process. Our experimental results demonstrate the following findings (Section 4). Finding 1: Fine-tuning on texts that compose training skills improves capabilities of composing held-out skills. Figure 2 shows the success rate of various models of combining held-out skills. Before fine-tuning, LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 perform significantly worse than GPT-4, especially when > 2. Both models improve remarkably after fine-tuning on our small dataset. For example, with = 3, the success rate of LLaMA-2-13B-Chat increases from 4% to 37%, and the success rate of Mistral-7B-Instruct-v0.2 increases from 8% to 49%. Note in the original SKILL-MIX, no model except GPT-4 could reach 15% success rate for = 3 (see Table 3, [32]). Figure 2: The success rate of different models to compose held-out skills in short paragraph. (See the detailed definition of Ratio of Full Marks in Section 3.3.) The strongest model like GPT-4 can compose 5 skills in short paragraph reasonably well, while smaller models struggle to compose even 3 skills. After fine-tuning, the models ability to compose skills improves significantly. This phenomenon of compositional generalization from training skills to held-out skills suggests that the models are not merely learning to compose each individual combination of skills. Instead, they are acquiring higher-order meta-skill that allows them to generalize and apply to combine unseen skills. Finding 2: Fine-tuning on texts that compose smaller number of skills leads to improvement of composing larger number of skills. Figure 2 demonstrates that fine-tuning on our small dataset, which includes texts composed of = 1, 2 or 3 training skills, leads to enhanced capability on composing = 4 and 5 held-out skills, even though the models have never trained on such text. In Section 4, we present similar findings: (1) the ability to compose training skills is also improved for = 4 and 5 after fine-tuning; and (2) if models are fine-tuned exclusively with texts composed of no more than 2 training skills, they also show improved composition ability for 3 and 4 skills. Note Finding 1 and Finding 2 are beyond the scope of the theory presented in Arora and Goyal [1], which studies the composition ability for skills that appear in the training data. Finding 3: Fine-tuning on texts that compose more skills (i.e., with larger k) is more data-efficient for learning skill compositions. We design control experiments in Section 4.3 that fine-tune LLaMA2-13B-Chat on two datasets: (a) one dataset contains around 10,000 text pieces with only 1 or 2 skills; and (b) another dataset contains 8,000 text pieces, consisting of random subset of the first dataset and around 2,000 text pieces that compose 3 skills. Table 4 shows that LLaMA-2 fine-tuned on the dataset with richer skill composition performs significantly better than the other for all = 2, 3, 4, 5. We discuss our main findings thoroughly in Section 4. In Section 5.1, we solidify our findings using Claude 3 Opus (instead of GPT-4) as the Grader in evaluation. This eliminates the possibility that the ability to compose skills comes from GPT-4s bias towards the models fine-tuned on GPT-4s outputs. We also discuss the implications of our findings for going beyond stochastic parrots behavior [2], which refers to the perception that LLMs might not generate novel pieces of text but rather mimic data from the pretraining corpus (Section 5.2)."
        },
        {
            "title": "2 Related Works",
            "content": "Compositional generalization Compositional generalization has grabbed lots of attention in AI. [28, 26] studied compositional generalization in the realm of mathematical reasoning, and [3, 21] investigated for logical inference. In computer vision, compositional generalization was studied on disentangled representation learning to generate images from novel combinations of concepts [11, 9, 30]. Besides, several works have explored composing visual relations [19], as well as benchmarks for text-to-visual generation[13, 18]. Other works have explored using compositional models for image generation [6] , as well as to create plans for unseen tasks at inference time [5]. Compositional generalization for language and LLMs There is also long history of study of compositional generalization in language [10, 17, 4, 14, 16, 20]. However, the test bed for compositional generalization mostly relies on rule-based languages, like SQL or synthetic-generated ones, and thus deviates little bit from natural language. Recent works have observed compositional capabilities in LLMs emerge multiplicatively on natural languages [29, 1, 22, 32]. These observations have fueled growing interest in exploring and evaluating compositional generalization in LLMs as 3 means to more appropriately evaluate LLM capabilities [8, 31, 23, 25, 32]. Some examples include imposing constraints and/or requirements on text generation [8, 31], as well as providing multi-hop questions whose answers require composing multiple facts that were individually observed during pretraining [25]. Dziri et al. [7] tests whether the LLMs can learn compositional generalization by studying integer multiplication and specific form of dynamic programming. Skill-Mix [32] presents an approach to evaluating compositional generalization on language skills, which we discuss in more detail in the next paragraph. Skill-Mix Yu et al. [32] introduce new evaluation named SKILL-MIX that tests for models to produce novel pieces of text from random combinations of skills, and the evaluation can be made more difficult by increasing the value of k. The procedure is roughly as follows: (1) from set of language skills and topics, pick random subset of skills and one topic; then, (2) query the Student model to produce short piece of text (at most 1 sentences) that illustrates the skills in the context of the provided topic. Note that for = 1, the maximum sentence limit is 1 sentence. Grader model is used to evaluate the text piece based on the following criteria: correctly illustrating all skills and the topic, meeting the maximum length requirement, and general coherence. Thus, each piece of text can award up to maximum of + 3 points (see the original paper for various metrics extracted from points earned). Note that each of the language skills has Wikipedia entry, so it is reasonable to expect an LLM to encounter the skills multiple times in isolation in the pretraining corpus, but not in all possible combinations. In this paper, we choose to study the compositional generalization of LLMs in the context of SKILL-MIX because SKILL-MIX is close to general language capability and is more flexible for modifying the language skill set."
        },
        {
            "title": "3 Pipeline",
            "content": "Our pipeline consists of three stages: generating data by selecting GPT-4 responses on SKILL-MIX (Section 3.1), fine-tuning on the generated data (Section 3.2), and evaluating our fine-tuned model on SKILL-MIX evaluation [32] (Section 3.3). The pipeline overview is shown in Figure 1. 3.1 Data generation We adapt the procedure presented in SKILL-MIX evaluation [32] to produce finetuning data. Only the generations with full marks (i.e., illustrating all skills and topics, meeting the length requirement, and general coherence) are selected. To enhance the likelihood of obtaining full marks, we prompt GPT-4, the best Student model reported in Yu et al. [32], to create the generations. Skills and topics for data generation. Since our goal is to measure the generalization capability strictly, we minimize the overlap between the skills/topics used during data generation and the skills/topics used to evaluate the fine-tuned models. Specifically, we partition the original set of 101 skills introduced in Yu et al. [32], S, into set of 53 train skills, Strain, and 48 held-out skills, Sheld-out, based on the skill category. Strain includes only literary and rhetorical categories, while Sheld-out comprises the rest of the categories, including reasoning, logic, theory of mind, pragmatics, common sense, and physical knowledge. Similarly, we partition the original set of topics, , into random sets of 50 training topics, Ttrain, and 50 held-out topics, Theld-out. It is important to note that partitioning skills randomly can lead to correlations between the train and held-out skills, as skills from the same category can be highly related. However, partitioning topics randomly does not present this issue, as the topics are generally unrelated. (Please refer to Appendix for the detailed list of skills and topics.) Data generation with = 1, 2, 3. We produce fine-tuning data with = 1, 2 and 3 using GPT-4 as both the Student and Grader model. For = 1, we use the original set of skills and training topics Ttrain to produce approximately 5,000 generations, and we only keep generations that receive full marks. We refer to the resulting dataset as DSKILL-MIX(1). DSKILL-MIX(1) contains only texts with individual skills, thus serving the role of separating the improvement from better utilizing an individual skill and the improvement from better composing multiple skills in later experiments. We follow an analogous procedure for = 2 and = 3, but using our 53 training skills Strain and 50 training topics Ttrain. We produce 10,000 generations for each before filtering. We refer to the resulting datasets as DSKILL-MIX(2) and DSKILL-MIX(3), respectively. For convenience, we use DSKILL-MIX(1, 2) to denote the dataset that combines DSKILL-MIX(1) and DSKILL-MIX(2), i.e., DSKILL-MIX(1, 2) = DSKILL-MIX(1) DSKILL-MIX(2). Similarly, we use DSKILL-MIX(1, 2, 3) to denote the dataset that combines DSKILL-MIX(1), DSKILL-MIX(2), and DSKILL-MIX(3) together. We summarize our notations in Table 1. Table 1: Notation used in data generation (Section 3.1) Symbol Definition All Skills Size 101 Misc = Strain Sheld-out categories = {literary, rhetorical, reasoning, Strain Sheld-out Ttrain Theld-out DSKILL-MIX(1) DSKILL-MIX(2) DSKILL-MIX(3) Train Skills Held Out Skills All Topics Train Topics Held Out Topics data with full mark on SKILL-MIX (k = 1) data with full mark on SKILL-MIX (k = 2) data with full mark on SKILL-MIX (k = 3) 53 48 100 50 50 6277 3603 3.2 Fine-tuning logic,theory_of_mind, pragmatics, common_sense, physical_knowledge} categories = {literary, rhetorical} categories = {reasoning, logic, theory_of_mind, pragmatics, common_sense, physical_knowledge} = Ttrain Theld-out Ttrain Theld-out Created from and Ttrain Created from Strain and Ttrain Created from Strain and Ttrain We fine-tune LLaMA-2-13B-Chat [27] and Mistral-7B-Instruct-v0.2 [15] on the data generated in Section 3.1 for 4000 steps with batch size of 64. Each data generated from SKILL-MIX consists of 4 parts: PROMPT1, ANSWER1, PROMPT2, ANSWER2. Here, PROMPT1 denotes the prompt asking the student to generate answers, ANSWER1 stands for students first round answer, PROMPT2 is the prompt that asks the student to correct or refine its answer, and ANSWER2 is the students second round answer. During fine-tuning, we feed the concatenation of PROMPT1, ANSWER1, PROMPT2, ANSWER2 into the model as single text, but only compute the cross-entropy loss for tokens belonging to ANSWER1 and ANSWER2. We use Adam as the optimizer and linear warmup for the first 64 steps, followed by constant learning rate of 2e-5 for the remaining training steps. 1 The maximum token length is set as 1024. All fine-tuning experiments are conducted on 4 Nvidia H100/A100 GPUs. Similarly to the loss design of RLHF [24], we mix pre-training data2 during fine-tuning to prevent degradation of general abilities. 3.3 Evaluation We evaluate the SKILL-MIX(k) performance (k = 2, 3, 4, 5) for all the models fine-tuned on data generated in Section 3.1, i.e., DSKILL-MIX(1), DSKILL-MIX(2), and DSKILL-MIX(3). Settings As mentioned earlier, SKILL-MIX evaluation requires skill set and topic set. We consider the following 3 settings (where Setting II is our main setting used in Figure 1): I. SKILL-MIX evaluation on training skills and topics. Since the model observes the same skills and topics during fine-tuning, this setting serves as an in-domain evaluation for = 2, 3. For = 4, 5, it tests the models ability to combine more skills, which is already out-ofdomain, since the model has never seen such data during fine-tuning. We use the notation SKILL-MIXtrain(k) to denote the SKILL-MIX(k) evaluation on training skills and topics. II. SKILL-MIX on held-out skills and topics. This setting tests the models ability to combine skills that are never present in fine-tuning.3 This setting serves as another perspective to 1The learning rate selection is based on the recommendation in Touvron et al. [27]. We fine-tune LLaMA2-13B-Chat on DSKILL-MIX(1, 2, 3) and perform grid search on the batch size and total number of steps based on the SKILL-MIX (k = 3) evaluation performance on training skills and topics. The hyperparameters are transferred to other settings, including training on different data and different models. 2Since LLaMA-2 and Mistral do not release pre-training data with their models, we use mixture of common crawl data and code data to approximate. 3Precisely, the held-out skills appear in DSKILL-MIX(1) since we want to make sure that the model at least knows all the skills after fine-tuning. However, the held-out skills never appear in DSKILL-MIX(2) or DSKILL-MIX(3), meaning that the model needs to be creative to compose skills together for > 1. Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1) Model LLaMA-2-13B-Chat = 1 fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) SKILL-MIX (k) Performance = 4 = 3 = 2 Evaluations on training skills and topics (SKILL-MIXtrain(k)) .00/.33 .00/.37 .01/.55 .08/.64 Evaluations on held-out skills and topics (SKILL-MIXheld-out(k)) .01/.39 .01/.49 .05/.60 .09/. .02/.34 .00/.43 .12/.56 .24/.68 .17/.47 .15/.51 .50/.70 .51/.73 .52/.52 .87/.87 .88/.88 .89/.89 .04/.42 .05/.54 .25/.68 .37/.75 .28/.50 .43/.70 .75/.87 .78/.88 .46/.46 .87/.87 .95/.95 .96/. fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) LLaMA-2-13B-Chat Evaluations on all skills and topics (SKILL-MIXall(k)) LLaMA-2-13B-Chat fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .46/.46 .88/.88 .96/.96 .96/. .24/.50 .27/.62 .51/.74 .65/.81 .02/.42 .05/.50 .17/.65 .33/.73 .01/.40 .00/.40 .01/.54 .15/.69 = 5 .00/.31 .00/.35 .02/.52 .03/.60 .00/.43 .00/.44 .02/.56 .02/. .00/.34 .00/.33 .00/.51 .06/.62 show the stronger out-of-domain generalization for composing skills compared to Setting I. We use the notation SKILL-MIXheld-out(k) to denote the SKILL-MIX(k) evaluation on held-out skills and topics. III. SKILL-MIX on all skills and topics. Evaluating SKILL-MIX on only half of the skills split by category might make the evaluation easier, since combining 2 rhetorical or logical skills might be easier than combining 1 rhetorical and 1 logical skill. Thus, we also evaluate SKILL-MIX on all skills and topics available, which serves as direct comparison with the results in Yu et al. [32]. We use the notation SKILL-MIXall(k) to denote the SKILL-MIX(k) evaluation on all skills and topics. Evaluation Metrics We follow the evaluation rubric of SKILL-MIX. Each generated text can receive up to + 3 points: 1 point for each correctly illustrated skill, 1 point for sticking to the topic, 1 point for text coherence / making sense, and 1 point for meeting the length requirement. Following Yu et al. [32], we grade each generated piece of text three times. For each of the + 3 criteria, we collect the majority vote among the three grading rounds, and map the points earned to the following two metrics of interest 4: (Ratio of Full Marks) count as 1 if all k+3 points are earned, and 0 otherwise; and (Skills Fraction) the fraction of points awarded for the skills if all 3 points are awarded for the remaining criteria, and 0 otherwise. For given (k skill, 1 topic) combination, we take the maximum value of the metric among the 3 generations. We average the maximum value across all the combinations. Note that we use one of the harder variants of SKILL-MIX [32], where we do not award any points for particular skill if the skill name is explicitly mentioned in the generated text piece."
        },
        {
            "title": "4 Skill Composition Can Be Learned From Examples",
            "content": "We present experiment results using the pipeline (Section 3) to evaluate compositional generalization. Table 2 and Table 3 summarizes the SKILL-MIX (k) performances of LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 fine-tuned on various datasets under three evaluation settings. We discuss our findings on compositional generalization for in-domain evaluations (Section 4.1), compositional generalization for out-of-domain evaluations (Section 4.2), and the data efficiency to induce compositional generalization (Section 4.3). 4.1 Compositional generalization for in-domain evaluations We first observe that, after fine-tuning LLaMA-2-13B-Chat on DSKILL-MIX(1, 2), the SKILL-MIXtrain(2) performance significantly improves. Similarly, after fine-tuning LLaMA-2-13B-Chat on DSKILL-MIX(1, 2, 3), the SKILL-MIXtrain(3) performance also improves. For example, the Ratio of Full Marks for SKILL-MIXtrain(3) improves from 2% for LLaMA-2-13B-Chat to 24% after fine-tuned on DSKILL-MIX(1, 2, 3)  (Table 2)  . 4Yu et al. [32] also report Ratio of All Skills, which we defer to Appendix for clear presentation. 6 Table 3: Performance of fine-tuned Mistral-7B-Instruct-v0.2 on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1) Model Mistral-7B-Instruct-v0.2 = 1 fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) SKILL-MIX (k) Performance = 4 = 3 = 2 Evaluations on training skills and topics (SKILL-MIXtrain(k)) .01/.36 .03/.52 .13/.69 .18/.72 Evaluations on held-out skills and topics (SKILL-MIXheld-out(k)) .01/.42 .05/.57 .17/.74 .26/.76 .05/.46 .10/.57 .26/.72 .34/. .18/.51 .43/.68 .65/.81 .66/.81 .86/.86 .95/.95 .98/.98 .92/.92 .08/.56 .20/.68 .37/.74 .49/.82 .48/.73 .64/.81 .85/.93 .86/.93 .85/.85 .97/.97 .97/.97 .97/.97 fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) Mistral-7B-Instruct-v0.2 Evaluations on all skills and topics (SKILL-MIXall(k)) Mistral-7B-Instruct-v0.2 fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .83/.83 .94/.94 .98/.98 .96/.96 .35/.66 .45/.71 .75/.00 .85/. .06/.50 .20/.64 .46/.80 .54/.83 .00/.41 .05/.56 .16/.73 .19/.75 = 5 .00/.35 .00/.52 .08/.68 .05/.68 .01/.39 .04/.60 .10/.70 .13/.74 .00/.37 .01/.52 .03/.67 .04/. One confounding factor in the above evaluation is that the original LLaMA-2-13B-Chat may not utilize all the individual skills perfectly, and the SKILL-MIX performance improvement might just be attributed to the models knowledge of the individual skills after fine-tuning, not the models ability to better compose different skills together. Thus, we also evaluate the SKILL-MIX performance on LLaMA-2-13B-Chat fine-tuned only on DSKILL-MIX(1), which consists of purely SKILL-MIX = 1 data and serves as another baseline besides the original LLaMA-2-13B-Chat. After fine-tuning on DSKILL-MIX(1), the model indeed knows the individual skills much better, since the Ratio of Full Marks of SKILL-MIX = 1 improves from 52% to 87%. However, better knowledge of individual skills does not lead to better ability to compose skills together, since the SKILL-MIXtrain(2) or SKILL-MIXtrain(3) performance of LLaMA-2-13B-Chat fine-tuned on DSKILL-MIX(1) keeps nearly the same as the pre-trained ones, under both Ratio of Full Marks and Skills Fraction metrics. Thus, mainly all the improvement on SKILL-MIXtrain(2) or SKILL-MIXtrain(3) indeed comes from the ability to compose different skills together. Fine-tuning on Mistral-7B-Instruct-v0.2  (Table 3)  shows similar results as fine-tuning on LLaMA2-13B-Chat  (Table 3)  , except that only fine-tuning on DSKILL-MIX(1) shows more improvement on SKILL-MIXtrain(2) and SKILL-MIXtrain(3) compared to the original Mistral-7B-Instruct-v0.2 model. possible explanation is that Mistral-7B-Instruct-v0.2 is better at composing skills than LLaMA-213B-Chat, and fine-tuning on DSKILL-MIX(1) helps Mistral-7B-Instruct-v0.2 exhibit each skill more properly and clearly when composing skills. 4.2 Compositional generalization for out-of-domain evaluations This section discusses the observations that indicate the out-of-domain generalization of skill composition, including generalization to unseen and generalization to unseen skills. SKILL-MIXtrain(k) improves for unseen k. We first observe that, after fine-tuning LLaMA-213B-Chat on SKILL-MIX data DSKILL-MIX(1, 2, 3), the SKILL-MIXtrain(4) and SKILL-MIXtrain(5) performance also increase. For example, the Ratio of Full Marks improves from 0% to 8% when = 4  (Table 2)  . Note that 8% Ratio of Full Marks improvement on = 4 is significant, since besides GPT-4, all other models tested in Yu et al. [32], including GPT-3.5-turbo, cannot get over 2% Ratio of Full Marks on = 4 (Table 3 in [32]). Besides, training only on DSKILL-MIX(1) does not improve the SKILL-MIXtrain(4) or SKILL-MIXtrain(5). The surprising finding here is that the model is only trained on SKILL-MIX = 2, 3 data, but it improves the ability to compose = 4, 5 skills in short piece of text, which it is never trained on. The results suggest that its ability to compose multiple skills does not come from overfitting training data but should be perceived as learning meta-skill instead. This observation is beyond the scope of the theory presented in Arora and Goyal [1], which assumes that the number of skills trained model can compose is limited to the number of skills in its training text pieces. Table 4: SKILL-MIXall(k) performance of models fine-tuned on LLaMA-2-13B-Chat, graded by GPT4. Ratio of Full Marks/Skills Fraction are reported for each model at different k. DSKILL-MIX(1, 2, 3) (8000 sample) denotes the randomly sub-sampled dataset from DSKILL-MIX(1, 2, 3) with size 8000. Model fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) (8000 sample) fted on DSKILL-MIX(1, 2, 3) SKILL-MIXall(k) Performance = 2 .51/.74 .66/.82 .65/.81 = 3 .17/.65 .30/.74 .33/. = 4 .01/.54 .11/.67 .15/.69 = 5 .00/.51 .02/.62 .06/.62 Fine-tuning on Mistral-7B-Instruct-v0.2  (Table 3)  shows similar results, where the fine-tuned model is able to improve SKILL-MIXtrain(4) and SKILL-MIXtrain(5) after fine-tuning on DSKILL-MIX(1, 2, 3), while training only on DSKILL-MIX(1) has limited improvement. Improvement on SKILL-MIXheld-out(k) and SKILL-MIXall(k). Besides the SKILL-MIX performance improvement on training skills and topics, we also observe the improvement of SKILL-MIXheld-out(k) (Setting II) from Table 2 and Figure 2. Similar to the evaluation on training skills and topics, finetuning LLaMA-2-13B-Chat on DSKILL-MIX(1) only improves the SKILL-MIXheld-out(k) performance for = 3, 4, 5 marginally, but it indeed improves the SKILL-MIX = 2. However, the improvement is incomparable with fine-tuning on DSKILL-MIX(1, 2, 3). This shows that the ability to compose multiple skills generalizes to held-out skills, even though our training never exposed the model to data with the held-out skills. Besides the SKILL-MIX improvement on held-out skills, we also observe the improvement of SKILL-MIXall(k). This result again suggests that models learn meta-skill rather than overfitting to skill combinations in the training data. Note that the SKILL-MIXheld-out(k) performance is better than the SKILL-MIXtrain(k) in Table 2, which is counter-intuitive. We hypothesize that this phenomenon happens because the pre-trained model knows how to compose held-out skills (logic, reasoning, theory of mind) better than training skills (rhetorical and literary). Or possibly the training skills are harder to compose. Exploring difficulty of individual skills is left for future work. Also, fine-tuning on Mistral-7B-Instruct-v0.2  (Table 3)  shows similar results as fine-tuning on LLaMA-2-13B-Chat  (Table 2)  , except that Mistral-7B-Instruct-v0.2 has more improvement than LLaMA-2-13B-Chat when fine-tuned on DSKILL-MIX(1). 4.3 Data requirement for inducing compositional generalization Compared with fine-tuning on DSKILL-MIX(1, 2), one can observe that LLaMA-2-13B-Chat/Mistral7B-Instruct-v0.2 fine-tuned on DSKILL-MIX(1, 2, 3) gains more performance boost on = 4, 5 across all settings. For example, SKILL-MIXall(4) performance for LLaMA-2-13B-Chat fine-tuned on DSKILL-MIX(1, 2) is nearly the same as the original LLaMA-2-13B-Chat and LLaMA-2-13B-Chat fine-tuned on DSKILL-MIX(1). However, for LLaMA-2-13B-Chat fine-tuned on DSKILL-MIX(1, 2, 3), the SKILL-MIXall(4) performance improves from 1% to 15%. However, one may argue it is because DSKILL-MIX(1, 2, 3) has more data in total than DSKILL-MIX(1, 2). To make fair comparison, we conduct an ablation study by sub-sampling 8000 data from DSKILL-MIX(1, 2, 3), making sure that the number of data points with = 2 and = 3 in the subsampled set is less than the size of DSKILL-MIX(2). Table 4 shows the SKILL-MIXall(k) performance of LLaMA-2-13B-Chat fine-tuned on the sub-sampled dataset. The metrics remain relatively close to the model fine-tuned on full DSKILL-MIX(1, 2, 3) and significantly better than the model fine-tuned on DSKILL-MIX(1, 2). This ablation confirms that skill-richer data can induce the ability to compose skills faster."
        },
        {
            "title": "5 Discussions",
            "content": "5.1 Using Claude 3 Opus as Grader for SKILL-MIX evaluation All the findings in the previous section are based on the SKILL-MIX performance graded by GPT-4. However, GPT-4 is heavily used during data generation, and one can argue the improvement might solely come from the fact that GPT-4 favors its own outputs. Although the possibility is low, to rigorously eliminate this confounding factor, we re-evaluate SKILL-MIXall(k) using Claude 3 Opus as the Grader, and report the results in Table 5. 8 Table 5: (Comparison between GPT-4 and Claude-3 grader) SKILL-MIXall(k) performance of models fine-tuned on LLaMA-2-13B-Chat, graded on Claude-3 and GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model at different = 2, 3, 4, 5. Model SKILL-MIXall(k) Performance = 2 = 3 = 4 = 5 Graded by Claude-3 Llama-2-13B-Chat fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2, 3) .31/.52 .45/.70 .69/.81 .07/.48 .14/.59 .57/.83 Graded by GPT-4 Llama-2-13B-Chat fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2, 3) .24/.50 .27/.62 .65/.81 .02/.42 .05/.50 .33/.73 .08/.64 .02/.50 .26/.77 .01/.40 .00/.40 .15/.69 .00/.42 .00/.42 .10/.69 .00/.34 .00/.33 .06/. Table 6: (Filtering out common skills) SKILL-MIX(5) performance of models fine-tuned on LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 graded by GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model under SKILL-MIX evaluation with train, test, and all skills (SKILL-MIXtrain(5),SKILL-MIXheld-out(5),SKILL-MIXall(5) respectively). We only consider skill combinations with uncommon skills whose occurrence rate in RedPajama is less than 5%. Model LLaMA-2-13B-Chat fted on DSKILL-MIX(1, 2, 3) fted on DSKILL-MIX(1, 2, 3, 4) Mistral-7B-Instruct-v0.2 fted on DSKILL-MIX(1, 2, 3) fted on DSKILL-MIX(1, 2, 3, 4) SKILL-MIX(5) Performance test train .00/.38 .00/.29 .02/.59 .03/.56 .05/.66 .16/.68 .00/.32 .00/.30 .03/.71 .00/.66 .02/.62 .23/.73 all .00/.30 .03/.61 .06/.56 .00/.36 .03/.70 .06/.63 From Table 5, we observe the metrics graded by Claude 3 Opus have similar trend as those graded by GPT-4: after fine-tuning on DSKILL-MIX(1, 2, 3), SKILL-MIXall(k) performance improves for all = 2, 3, 4, 5, while fine-tuning only on DSKILL-MIX(1) has limited improvement over the original LLaMA-2-13B-Chat. It proves that the improvement of SKILL-MIX performance is not overfitted to GPT-4 preference. Interestingly, we find that Claude 3 Opus is more generous, assigning higher scores to both the LLaMA-2-13B-Chat and the fine-tuned version. Such consistent biases among Graders were noted also in [32] when comparing LLaMA-2-70B-Chat and GPT-4 as Graders. Besides switching to Claude-3, we also do human spot check on the SKILL-MIX generations, making sure that the model is not generating something that does not make sense to human. Please refer to Appendix for some of the examples of SKILL-MIX evaluations before and after the fine-tuning. 5.2 Potential capability of going beyond stochastic parrots behavior Whether models can go past stochastic parrots behavior [2] is crucial in discussions of AI risk. Based on reasonable performance of GPT-4 on SKILL-MIX(k = 5) with common skills removed, Yu et al. [32] suggests GPT-4 is already beyond stochastic parrots. In particular, after removing common skills (see definition in [32]), the probability of random (5 skills, 1 topic) combination appearing in the training corpus is estimated to be 11% if the training token is 2T5. Therefore, if model has Ratio of Full Marks beyond 11% when = 5, then it suggests the model is able to output novel text, thus is beyond stochastic parrots. GPT-4 is the only model that can achieve this in [32]6. Table 6 shows the SKILL-MIX (k = 5) performance of fine-tuned LLaMA-2-13B-Chat and Mistral7B-Instruct-v0.2 with common skills removed. We also include some DSKILL-MIX(4) data (about 1000) into the training set. The fine-tuned models all show significant improvement over the base models. For example, the Ratio of Full Marks for the fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instructv0.2 all go beyond 15% for SKILL-MIX (k = 5) on training skills and topics, and reaches 6% on all skills and topics, after filtering out the common skills. 5Touvron et al. [27] mentioned that the number of pretraining tokens for LLaMA-2 is 2 trillion. 6However, GPT-4 might use more than 2T pretraining tokens. 9 Although both fine-tuned models cannot reach 11% for SKILL-MIXheld-out(5) and SKILL-MIXall(5), we hypothesize that with more skill-richer data (say DSKILL-MIX(4)), the models can acquire the ability to combine skills much more efficiently and go beyond stochastic parrots eventually. We further discuss the limitation of our work in Section 7, and potential influences on AI safety caused by stronger composition capability in Appendix E."
        },
        {
            "title": "6 Conclusion and Takeaways",
            "content": "We have studied the extent to which models can learn compositional generalization over language skills by fine-tuning on suitable examples demonstrating such composition. Previous evaluations had seemed to suggest that the extent of compositional generalization is determined by the model size and pretraining [32], but here we were able to induce much better compositional capability via fine-tuning on data that was generated using setup similar to SKILL-MIX. One surprising finding was that fine-tuning examples that composed 2 and 3 language skills were enough to improve the capability to compose 4 and even 5 language skills. Another surprise was that the ability to combine language skills from held-out categories improved at the same rate as the skills used in the training examples. Of course, these findings were still about skills that are near relatives. The full extent of such \"out of (training) distribution\" generalization remains to be explored."
        },
        {
            "title": "7 Limitation",
            "content": "The main limitation of the current study is the high computational and financial costs, which impede us from sweeping more hyperparameters and conducting repeated experiments with different random seeds. These costs include the number of GPU hours for fine-tuning and the cost of calling OpenAIs API to generate the DSKILL-MIX(k) data and evaluate the SKILL-MIX performance. Despite these difficulties, we managed to sweep the hyperparameters for fine-tuning the LLaMA-2-13B-Chat on DSKILL-MIX(1, 2, 3) (Main experiment, Table 2). We believe our findings are robust to different random seeds because of the clear message and consistent trend of the results. Besides, compositional generalization is vast topic and we only study this under the setting of language skills (limited to the SKILL-MIX setting). Whether the models can learn compositional generalization in other settings still needs further exploration."
        },
        {
            "title": "References",
            "content": "[1] Sanjeev Arora and Anirudh Goyal. theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023. [2] Emily Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610623, 2021. [3] Samuel Bowman, Christopher Manning, and Christopher Potts. Tree-structured composition in neural networks without tree-structured architectures. arXiv preprint arXiv:1506.04834, 2015. [4] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. Compositionality and generalization in emergent languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 44274442, 2020. [5] Yilun Du and Leslie Kaelbling. Compositional generative modeling: single model is not all you need, 2024. [6] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation and inference with energy based models, 2020. [7] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. 10 [8] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023. [9] Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, Narayanaswamy Siddharth, Brooks Paige, Dana Brooks, Jennifer Dy, and Jan-Willem Meent. Structured disentangled representations. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 25252534. PMLR, 2019. [10] Catherine Finegan-Dollak, Jonathan Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. Improving text-to-sql evaluation methodology. arXiv preprint arXiv:1806.09029, 2018. [11] Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher Burgess, Matko Bosnjak, Murray Shanahan, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. Scan: Learning hierarchical compositional visual concepts. arXiv preprint arXiv:1707.03389, 2017. [12] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [13] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation, 2023. [14] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: how do neural networks generalise?, 2020. [15] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [16] Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: comprehensive method on realistic data, 2020. [17] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pages 28732882. PMLR, 2018. [18] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation, 2024. [19] Nan Liu, Shuang Li, Yilun Du, Joshua B. Tenenbaum, and Antonio Torralba. Learning to compose visual relations, 2021. [20] Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. Advances in Neural Information Processing Systems, 33:1141611427, 2020. [21] Mathijs Mul and Willem Zuidema. Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization. arXiv preprint arXiv:1906.00180, 2019. [22] Maya Okawa, Ekdeep Singh Lubana, Robert P. Dick, and Hidenori Tanaka. Compositional abilities emerge multiplicatively: Exploring diffusion models on synthetic task, 2024. [23] Santiago Ontañón, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks, 2022. [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 11 [25] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2023. [26] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019. [27] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [28] Sara Veldhoen, Dieuwke Hupkes, Willem Zuidema, et al. Diagnostic classifiers revealing how neural networks process hierarchical structure. In CoCo@ NIPS, pages 6977. Barcelona, 2016. [29] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. [30] Zhenlin Xu, Marc Niethammer, and Colin Raffel. Compositional generalization in unsupervised compositional representation learning: study on disentanglement and emergent language. Advances in Neural Information Processing Systems, 35:2507425087, 2022. [31] Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik Narasimhan. Collie: Systematic construction of constrained text generation tasks, 2023. [32] Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skill-mix: flexible and expandable family of evaluations for ai models. In The Twelfth International Conference on Learning Representations, 2023."
        },
        {
            "title": "A Skills and Topics Partition",
            "content": "The training skills and held-out skills are listed in Table 7 and Table 8 respectively. The training and held-out topics are shown in Table 9. Table 7: The list of train skills for generating DSKILL-MIX(k) and evaluating SKILL-MIXtrain(k). For each skill, we list its category, name, definition, and one example using this skill. Category reasoning reasoning Skill false consensus (belief ones own opinion is right) actor bias observer reasoning hindsight bias reasoning self serving bias reasoning availability bias rhetorical anchoring (cognitive bias) Example \"That was clearly the right decision. Its what would have done.\" The difference in perception between \"James failed the test. He must be bad student.\" and \"I failed the test, but it wasnt because was bad student. was just busy with other obligations.\" is an example of actor observer bias. \"It was simple medical procedure with almost no known risks. The doctor should have known it could go so wrong.\" \"If do well on the exam, its because of my academic prowess and hard work. If do poorly, its because the course was poorly taught, and the exam was poorly proctored.\" Some people believe cars are safer than planes, since plane crashes are dramatic and widely reported. \"Individuals may be more likely to purchase pricy car if it is placed next to an extremely expensive car.\" Definition The belief that ones own opinions and emotional responses are rational. The difference in perception that occurs when one is an actor in situation versus an observer in situation. The tendency to perceive past events as being more predictable than they were. cognitive or perceptual process that is distorted by the need to maintain and enhance ones self esteem. mental shortcut that relies on immediate examples that come to given persons mind when evaluating specific topic, concept, method, or decision. phenomenon in which an individuals judgements or decisions are influenced by reference point or \"anchor\" which completely be can irrelevant. psychological 13 rhetorical ad hominem rhetorical appeal to authority (argumentum ab auctoritate) rhetorical appeal to emotion (argumentum ad passiones) rhetorical argumentum ad populum rhetorical argumentum ad baculum (appeal to force) rhetorical tu quoque rhetorical extrapolation rhetorical post hoc propter hoc ergo rhetorical accident (fallacy) \"Boss, you heard my side of the story why think Bill should be fired and not me. Now, am sure Bill is going to come to you with some pathetic attempt to weasel out of this lie that he has created.\" Citing \"Albert Einstein\" as an authority for determination on religion, when his primary expertise was in physics, is an example of appeal to authority. student says, \"If get failing grade for this paper, will lose my scholarship. Its not plagiarized.\" \"Everyone is going to get the new smart phone when it comes out this weekend. You should too!\" If you dont join our demonstration against the expansion of the park, we will evict you from your apartment. So, you should join our demonstration against the expansion of the park. \"How can you tell me not to smoke when you yourself smoke?\" \"Water boils at 212 degrees at sea level. Therefore, it must boil at this temperature at all levels.\" \"Why are you whistling? To keep the elephants away. But there are no elephants around here. It works.\" Cutting people with knives is crime. Surgeons cut people with knives. Surgeons are criminals. See? rhetorical strategy where the speaker attacks the character, motive, or some other attribute of the person. form of fallacy when the opinion of nonexpert on topic is used as evidence to support an argument or when the authority is used to say that the claim is true, as authorities can be wrong. an informal fallacy characterized by the manipulation of the recipients emotions in order to win an argument, especially in the absence of factual evidence. fallacious argument which is based on claiming truth or affirming something is good because the majority thinks so. An appeal to force to bring about acceptance of conclusion. false because Claiming an argument is of hypocrisy of the person making the argument. Generalizing conclusion beyond the range in which its truth has been established. fallacy of the form after this, therefore because of this. an informal fallacy and deductively valid but unsound argument occurring in statistical syllogism (an argument based on generalization) when an exception to rule of thumb is ignored. 14 rhetorical paradox seemingly absurd or self contradictory statement. rhetorical slippery slope (argument) rhetorical fallacy of composition The assumption that small change will cascade to larger series of (undesirable) changes. Assuming that something is true of the whole from the fact that it is true of some part of the whole. rhetorical rhetorical rhetorical rhetorical false dichotomy fallacy of division Assuming that something that is true for whole must also be true of all or some of its parts. An fallacy informal based on premise that erroneously limits what options are available. circular argument that paraphrases the question. begging the question or assuming the conclusion complex question (loaded question with implicit assumption ) rhetorical equivocation (informal fallacy) rhetorical argumentum ad ignorantiam rhetorical diversion rhetorical straw man question that is loaded with an implicit assumption. an informal fallacy resulting from the use of particular word/expression in multiple senses within an argument. statement of the form not has not been proven to be true. Therefore, is true. tactic where the arguer diverts attention away from the relevant conclusion. type of diversion whereby one states an exaggerated false version of an opponents argument. or 15 \"A chicken is born from an egg, so it stands to reason an egg comes before chicken. However, an egg is laid by chicken, so it stands to reason an egg comes after chicken.\" \"Changing the grading standards will have ripple effect throughout the college.\" \"If someone stands up from their seat at cricket match, they can see better. Therefore, if everyone stands up, they can all see better.\" \"Americans eat lot of hamburgers. Bob is American. Therefore Bob eats lot of hamburgers.\" \"Stacey spoke out against capitalism; therefore, she must be communist.\" \"Of course the earth is round; people have known this for hundreds of years.\" \"Why are you lying to me?\" is question that presupposes you are lying to me. Any answer you give will force you to agree you are lying. warm beer is better than cold beer. After all, nothing is better than cold beer, and warm beer is better than nothing. \"The politician is having an affair. Prove it, then. Can you prove hes not having an affair?\" The prosecutor claims without proof the defendant is guilty of child abuse. The prosecutor then goes on and on about how awful child abuse is, instead of proving the original claim of the defendants guilt. Suppose Louise goes to her professor and asks for more time for class discussion. Her professor responds, \"I dont want the entire class to be aimless student discussion while sit silent.\" rhetorical red herring irrelevant Introducing points to detract attention from question. rhetorical non sequitur rhetorical rhetorical tion quesrhetorical category mistake (rhetorical error) literary anaphora resolution literary answer ellipsis An argument where the conclusion does not follow from the premises. question asked in order to create dramatic effect or to make point rather than to get an answer. An error in which things belonging to particular category are presented as if they belong to different category. Resolving the antecedent of pronoun or noun phrase. An ellipsis (omission of speech) that occurs in answers to questions. literary anthropomorphism The attribution of human traits, emotions, or intentions to non-human entities. literary antithesis literary antonymy literary aphorism literary aposiopesis literary antimetabole words lexical relation in have figure of speech involving the bringing out of contrast in the ideas by an obvious contrast in the words, clauses, or sentences, within parallel grammatical structure. which opposite meanings. short saying that observes general truth. speech figure in which the speaker ends abruptly their sentence, leaving the statement incomplete. the repetition of words in successive clauses, but in transposed order. of 16 member of the press asks the president why they voted to expand welfare program. The president responds, \"The strength of America is the strength of its communities, and am proud to make our communities better places.\" \"Why are you wearing your There shirt backwards? will be lunar eclipse tonight.\" \"How could you be so stupid?\" visitor to Oxford was being given tour. The visitor, upon viewing the colleges and library, reportedly inquired, \"But where is the University?\" \"The car is falling apart, but it still works.\" Here , \"it\" is the anaphor and \"car\" is the antecedent. \"Who walked the dog? asked Mary. Sam, replied Jill.\" This is an example of answer ellipsis, as Jills complete answer would have been \"Sam walked the dog\". Some examples of anthropomorphism include \"talking clocks\", \"singing teapots\", \"Buck the dog from Call of the Wild\", and \"feeling embarrassment.\" \"One small step for man, one giant leap for mankind.\" Here, the contrast of \"one small step for man\" versus \"one giant leap for mankind\" is an antithesis. \"Shallow\" is an antonym of \"deep\". \"Pride goeth before fall.\" \"If ever get my hands on you Ill\" \"Ask not what your country can do for you, but what you can do for your country.\" literary cliché literary colloquialism An element of an artistic work, saying, or idea that has become overused to the point of losing its original meaning or effect, even to the point of being weird or irritating, especially when at some earlier time it was considered meaningful or novel. language that is informal and conversational. literary compounding (combining words) Combing two or more words to produce new word. literary eponym word derived from the name of person or place. literary fallacy literary hyperbole literary hypocorism literary using jargon literary metaphor literary oxymoron mistaken belief, especially one based on an unsound argument. Exaggerated statements or claims not meant to be taken literally. pet name. Also word formation process in which longer word is reduced to shorter word ending with ie or y, often affectionately. technical using special vocabulary associated with specific activity or topic. figure of speech that, for rhetorical effect, directly refers to one thing by mentioning another. figure of speech in which apparently contradictory terms appear in conjunction. 17 \"All that glitters is not gold\" has been used so often, it is now cliché. \"Did you see that town over yonder?\" Here, \"over yonder\" is an example of colloquialism. \"Waterbed\" illustrates compounding, as it combines the words \"water\" and \"bed\" to produce new word. In the phrase \"his Machiavellian tendencies,\" \"Machiavellian\" is an epoynm derived from the Florentine diplomat and political theorist Niccolo Machiavelli. \"People have been trying to prove for centuries that God exist. However, no one has proven God exists. Therefore, God does not exist.\" \"I had to wait at the station for ten daysan eternity.\" Here, \"an eternity\" is hyperbole. \"Telly\" is hypocorism for \"television\"; \"movie\" is hypocorism for \"motion picture.\" \"He is the kind of lawyer who likes to sprinkle amicus curiae in dinner conversations.\" Here, \"amicus curiae\" is an example of jargon. the worlds stage, \"All and the men all And women merely players\" is metaphor because its comparison without using \"like\" or \"as.\" \"The hall was filled with deafening silence.\" Here, the phrase \"deafening silence\" is an oxymoron. literary sluicing literary litotes literary tautology guage) (lanA types of ellipsis that occurs in both direct and indirect interrogative clauses. Using understatement to emphasize point by stating negative to further affirm positive, often incorporating double negatives for effect. statement that repeats an idea, using nearsynonymous morphemes, words or phrases, effectively \"saying the same thing twice\". literary literary literary topicalization (syntax mechanism) unaccusative verb verb whose subject does not actively initiate, or is not actively responsible for, the action expressed by the verb. mechanism of syntax that establishes an expression as the sentence or clause topic by having it appear at the front of the sentence or clause (as opposed to in canonical position further to the right). Pseudogapping is an ellipsis mechanism that removes most but not all of non-finite verb phrase. pseudogapping literary phrasal verb literary subjunctive clause literary subordinate clause single semantic unit composed of verb followed by particle, sometimes combined with preposition. clause in the subjunctive mood is finite but tenseless clause. Subjunctive clauses use bare or plain verb form, which lacks any inflection. clause that is embedded within complex sentence. 18 \"Phoebe ate something, but she doesnt know what\" is an example of sluicing, because the full sentence is \"Phoebe ate something, but she doesnt know what she ate\". Using the phrase \"not bad\" to mean \"good\", or \"nontrivial\" to mean \"complicated\", are examples of litotes. \"Youre simply going to have to score more points than the other team to win the game.\" \"The tree fell.\" \"Because the pressure was too great, everyone refused to answer.\" \"He drinks milk more often than he does water.\" This is an example of pseudogapping because the verb \"drink\" is removed from the phrase \"he does drink water\". \"The store has run out of cheese.\" The phrasal verb is \"run out of.\" \"It is imperative that he be the here on time.\" Here, verb \"to be\" appears in its plain form in the subjunctive clause. \"Whoever made that assertion is wrong.\" In this case, \"whoever made that assertion\" is subordinate clause. literary syntactic ambiguity literary allusion (literary reference) situation where sentence may be interpreted in more than one way due to ambiguous sentence structure. Allusion is figure of speech, in which an object or circumstance from an unrelated context is referred to covertly or indirectly. \"John saw the man on the mountain with telescope.\" The syntax is ambiguous as it could either mean that John used telescope to see the man on the mountain, or that the man on the mountain had telescope and John saw the man. Describing two people in relationship as star-crossed lovers is an allusion to the Shakespeare play Romeo and Juliet. Table 8: The list of held-out skills for evaluating SKILL-MIXheld-out(k). Combined with the training skills list in Table 7 is the full list of skills used for SKILL-MIXall(k). For each skill, we list its category, name, definition, and one example using this skill. Category logical Skill enumerative induction logical spatial reasoning Definition Using evidence from particular examples to conclude property is true in general. The capacity to reason about the spatial relationships between objects. logical spatial orientation Words or phrases used to situate people and objects in relation to each other in space. logical logical tion proposilogical categorical syllogism logical modus ponens logical modus tollens logical proposition is statement that takes no arguments and evaluates to True or False. Infers conclusion from two premises. syllogism that is of the form If then Q. P. Hence Q. syllogism that is of the form If then Q. Not Q. Hence not P. Example \"Every particular life form we know of depends on water to exist. Therefore, all known life depends on water.\" The key fit into the box. Using spatial reasoning, one can deduce that the width of the key was smaller than the width of the box. Mark was sitting on chair. Using spatial orientation skills, one can deduce that Mark was above the chair. \"3 + 2 = 5\" is proposition that evaluates to \"True.\" \"3 + 2 = 4\" is proposition that evaluates to \"False.\" \"No geese are felines. Some birds are geese. Therefore, some birds are not felines.\" \"If today is Tuesday, then John will go to work. Today is Tuesday. Therefore, John will go to work.\" \"If it is sunny, will wear my sunglasses. am not wearing my sunglasses. Therefore, it is not sunny.\" 19 logical analogical induction logical entailment cal) (logilogical enthymeme logical disjunctive syllogism logical hypothetical syllogism type of inA special ductive argument, where perceived similarities are used as basis to infer some further similarity that has not been observed yet. An expression entails if the truth of guarantees the truth of and the falsity of guarantees the falsity of A. syllogism in which the conclusion or premise is left unstated, because they are taken to be common sense. syllogism that is of the form or Q. Not P. Hence Q. syllogism that is of the form implies Q. implies R. Hence implies R. logical logical theory of mind syllostatistical gism gestural communication syllogism that argues, using inductive reasoning, from generalization true for the most part to particular case. mode of communication that uses gestures, facial expressions, and body language to express and understand information. perspective taking The act of perceiving situation or understanding concept from an alternative point of view. theory of mind empathy theory of mind decoding nonverbal cues The ability of understand and share the feelings of another. Recognizing and interpreting nonverbal cues. 20 \"Swans in the northern hemisphere are white. Therefore, swans in the southern hemisphere are likely white, too.\" \"The emperor was assassinated\" entails \"the emperor was dead.\" \"Anyone who crashes their car into police station lawn is looking for trouble. Thats what Mitch did.\" The preceding is an enthymeme because the conclusion \"Mitch was looking for trouble\" is so obvious it is left unstated. \"The meeting is in room 205 or 306. The meeting is not in room 205. Hence it is in room 306.\" \"If do not wake up, then cannot go to work. If cannot go to work, then will not get paid. Therefore, if do not wake up, then will not get paid.\" \"Almost all people are taller than 26 inches. Gareth is person. Therefore, Gareth is taller than 26 inches.\" Bill pulled out the chair next to him. Charlie smiled and sat down. Bill is afraid of heights, and Carl loves heights. Bill and Carl are on the observation deck of the Empire State Building. Bill knows that Carl must be enjoying the experience. The nurse said to her patient, \"I am sorry you are not feeling well.\" As Willow was sketching her plan, Amy frowned. \"Are you worried it wont work?\" Willow asked. theory of mind recognizing false beliefs theory of mind root cause analysis The ability to understand that others can hold beliefs that do not align with reality or with ones own beliefs. method of problem solving whereby the underlying cause is found. theory of mind divide and conquer Breaking large difficult to solve problem into smaller, easier to solve parts. theory of mind help seeking The behavior of actively seeking help from other people. theory of mind trial and error theory of mind hypothesis testing theory of mind perspective awareness theory of mind visualization theory of mind expressing gratitude theory of mind theory of mind self motivation altruism The process of experimenting with various methods until one is found the most successful. assuming possible explanation to problem and sometimes trying to prove (or disprove) said explanation. The awareness that ones own mental states may not be known or accessible to others without communication. The formation of mental image of real world object or phenomenon. is feeling of appreciation (or similar positive response) by recipient of anothers kindness. the ability to maintain drive towards ones goals. the principle and practice of concern for the wellbeing and/or happiness of other humans or animals. \"Unicorns are real,\" said Larry. \"No they arent,\" thought Percy. \"Aha!\" exclaimed Stephen. \"The leak is coming from here.\" The preceding is an example of root cause analysis, because Stephen has found the source of the leak. \"Mary, Jamie, you search the right quadrant. Kelly and will take the left.\" The preceding is an example of divide and conquer. Instead of collectively searching the entire area, the team broke the area into smaller quadrants and searched those. \"Freddie held up the broken clock to Marsha. Do you think you could fix it?\" Here, Freddie seeks help from Marsha. One method of drug discovery is to try different chemicals at random until one has the desired effect. \"I assume all lilies have the same number of petals. Let me try counting the number of petals on some lilies to see if my hypothesis fails.\" \"John, do you mind throwspider outside? ing that asked Jane. Actually, Im afraid of spiders, so cant, replied John.\" \"Jerry watched the clock tick down towards his lunch hour. He could see his burger waiting for him in his minds eye.\" \"Expressing her gratitude during acceptance speech, the actress thanked her husband and parents for their support.\" Ill get \"If reward myself by playing video games\" Examples of altruism include caregiving for relative with chronic condition, and helping an older adult walking with cane cross the street. this done, her theory of the mind emotional self regulation theory of the mind optimistic attitude theory of the mind sympathy pragmatics synecdoche pragmatics presupposition pragmatics dilemma pragmatics implicature (pragmatic suggestion) complex process that involves initiating, inhibiting, or modulating ones state or behavior in given situation. an attitude reflecting belief or hope that the outcome of some specific endeavor, or outcomes in general, will be positive, favorable, and desirable. the perception of, understanding of, and reaction to the distress or need of another life form. figure of speech where the whole is represented by the part, or vice versa. An implicit assumption about the world or background belief relating to an utterance whose truth is taken for granted in discourse. situation in which choice must be made between two or more alternatives, especially equally undesirable ones. Something the speaker suggests or implies with an utterance, even though it literally expressed. is not pragmatics intention matics) (pragThe ability to identify speakers intent. physical knowledge folk physics (common knowledge physics) The untrained human perception of basic physical phenomena. 22 Examples of emotional self regulation include meditating, pausing to collect oneself before speaking, and practicing stress management. \"There was lot of talent at the tryouts and only few openings. That pushed me to practice hard and played my best it felt good! The coach gave me great feedback. Im going to work on the things he suggested and watch all the games this season. That way, Ill have better chance next year.\" \"Im so sorry for your loss.\" \"I have hungry mouths to feed\" is an example of synecdoche, because \"hungry mouths\" is part referring to \"children.\" \"Jane no longer writes fiction\" presupposes Jane once wrote fiction. \"If go to bed early, wont have studied enough for the test. But if stay up late, Ill be too tired to do well.\" \"Alice says, am out of gas. Bob replies, There is gas station around the corner.\" Here, Bob does not say, but conversationally implicates, that the gas station is open, because otherwise his utterance would not be relevant in the context. \"Its little loud in here, said Bella. Oh, sorry, Ill try to be quieter, replied Sarah.\" Here, Sarah understood Bellas intention, which was to tell Sarah to be quieter, and not merely remark on the rooms loudness. \"If roll the pen off of the table, it will fall to the floor.\" common sense reasoning about effects of events common sense reasoning about indirect effects common sense reasoning about preconditions common sense commonsense law of inertia common sense default reasoning common sense temporal reasoning common sense abductive reasoning common sense reasoning about motivations The ability to reason from specific event and general knowledge about the effects of events to the specific effects of the specific event. The ability to reason about indirect effects or ramifications of events. The ability to reason about the conditions that held before an action or event. Things tend to stay the same unless affected by some event. The ability to reason where one reaches default conclusion with incomplete information by assuming that unexpected or exceptional events do not happen. the ability to make presumptions about humans knowledge of times, durations and time intervals. form of logical inference that seeks the simplest and most likely conclusion from set of observations. Determining the goals or mental states that led to the action of person. \"Given that Lisa picked up the newspaper, we can infer that Lisa is now holding the newspaper.\" \"Lisa picked up the newspaper and walked into the room. Therefore, the newspaper is now in the room, because we know that if person is holding an object it moves along with the person.\" \"Kate set the book on the table. Therefore, before Kate set the book on the table, she was holding the book and she was near the table.\" \"Kate set the book on the table and left the living room. When she returned, the book was still on the table.\" \"Kimberly turns on fan. What will happen? The fan will start turning.\" \"Mozart was born after Haydn and died earlier than him, therefore Mozart died younger than Haydn.\" \"Nathan was sleeping. Now, Nathan is looking at his phone. Therefore, Nathan must have woken up, and picked up his phone.\" \"John opened the refrigerator and took out sandwich. Based on this observation, it is likely that John was hungry.\""
        },
        {
            "title": "B Prompts for Data generation",
            "content": "In this section, we document our prompts for generation and prompts for grading. B.1 Prompts for generation PROMPT1 Greetings! am interested in natural language processing and was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax. The example should be minimal natural piece of text with up to few lines in the context of topic that illustrates all of the following skills: and make sure the concepts can be found fully from the text. {skills_str}. Please keep the text as short as possible, Table 9: List of training and held-out topics Skiing, Climbing Everest, Grocery cashiers, Sledding, Opera solo, Reunion, Archaeologists, Rabbis, Pirates, Acupuncture, Regatta, sailing, Elizabethan England, Basketball, Civil Law, Tropical rainforest, Ecology, Ancient Greece, Civil War, Broadway play, Tennis match, Coal mine, Bowling, Fitness, Confession, Animation, Podcasts, Documentaries, Indie music, Jazz music, Dubstep, Rugby, Veganism, cryptocurrency, Violinists, Pianists, Olympics, Woodworking, Crochet, Knitting, Sewing, Calligraphy, Felting, Kayaking, Snorkeling, Pathology, Baking, Pizza making, Winning the Lottery, Equestrian pursuits, Thermodynamics Dungeons and Dragons, Golf, Hiking, Makeup, Escalators, Australia, French architecture, Fireworks, Dueling, Colorado, Rafting, Mushrooms, Sushi, Steampunk, The Ottoman Empire, Paleontology, Wooly mammoth, Urbanism, Ice skating, Beekeeping, Beatboxing, Acrobatics, Gymnastics, Ballet, Sitcoms, Thriller movies, Cruise ship, Whaling, Ballroom dancing, Etiquette, Survivalism, Camping, Utilitarianism, Consequentialism, Guerilla warfare, Siberia, Vikings, Triathlons, Mercantilism, Submarines, Sandwiches, Gardening, The Renaissance, Comedy, Japan, Dinosaurs, Leopards, Wrestling, Plumbers, Knots Training Held-out For reference, here are the definitions and examples for the concepts: {skills_defs_and_examples_simple} Please start the minimal natural piece of text with Answer: and start the explanation with Explanation:. Thanks very much! PROMPT2 Thanks very much. new answer better illustrates all skills and remains on topic. On top of that, please also make sure the new answer only contains up to {num_sentences_str}. Could you please look over your answer and improve it? Please make sure that the Again, please start the improved minimal natural piece of text with Answer: and start the explanation with Explanation:. Thanks very much! Note that this pair of prompts is the same as the generation prompts for GPT-4 in Yu et al. [32], which is slightly different from the generation prompts for LLaMA-2 in Yu et al. [32]. This difference of prompts, along with randomness from multiple sources, caused some difference in LLaMA-2-13BChat performance on SKILL-MIX(k) between our paper and Yu et al. [32]. B.2 Prompts for grading Grading prompt for GPT-4 Grader: Greetings! was wondering if you could help me grade the following answer given by student. Ill first describe the question that was given to the student, and then give you the students answer, and the grading rubric. The question given to the student was as follows: \"Give single piece of text with up to {num_sentences_str} in the context of {topic}. of the following skills: {skills_str}.\" This single piece of text should illustrate all The students answer was: \"{student_answer}\" For reference, here are the definitions for the skills: {skills_defs_and_examples_simple} Using rubric table format, please grade the students answer with positive scoring. Each criterion 24 Table 10: Performance of fine-tuned LLaMA-2-13B-Chat on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Ratio of All Skills/Skills Fraction are reported for each model at different = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)"
        },
        {
            "title": "Model",
            "content": "k = 1 SKILL-MIX (k) Performance = 3 = 4 = 2 = 5 Evaluations on training skills and topics (SKILL-MIXtrain(k)) LLaMA-2-13B-Chat fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .52/.56/.52 .17/.19/.47 .02/.02/.34 .00/.00/.33 .00/.00/.31 .87/.91/.87 .15/.19/.51 .00/.00/.43 .00/.00/.37 .00/.00/.35 .88/.96/.88 .50/.58/.70 .12/.14/.56 .01/.03/.55 .02/.02/.52 .89/.96/.89 .51/.62/.73 .24/.28/.68 .08/.10/.64 .03/.03/.60 Evaluations on held-out skills and topics (SKILL-MIXheld-out(k)) LLaMA-2-13B-Chat fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .46/.53/.46 .28/.32/.50 .04/.05/.42 .01/.01/.39 .00/.00/.43 .87/.95/.87 .43/.50/.70 .05/.06/.54 .01/.01/.49 .00/.00/.44 .95/.99/.95 .75/.79/.87 .25/.29/.68 .05/.05/.60 .02/.02/.56 .96/1.0/.96 .78/.81/.88 .37/.43/.75 .09/.12/.69 .02/.02/.60 Evaluations on all skills and topics (SKILL-MIXall(k)) LLaMA-2-13B-Chat fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .46/.54/.46 .24/.29/.50 .02/.02/.42 .01/.02/.40 .00/.00/.34 .88/.95/.88 .27/.28/.62 .05/.05/.50 .00/.00/.40 .00/.00/.33 .96/.99/.96 .51/.57/.74 .17/.23/.65 .01/.01/.54 .00/.00/.51 .96/.99/.96 .65/.68/.81 .33/.36/.73 .15/.16/.69 .06/.07/.62 is worth 1 point. The criteria are: columns: Criteria, Points Earned. In the Points Earned column, please provide only numerical values with no additional formatting. Please introduce the table with Heres the grading table: and please include row for Total Points Earned at the end of the table. Finally, please start your grading explanation with Explanation: {rubric_items}. The table should only have the following Grading prompt for Claude 3 Opus Grader: Greetings! was wondering if you could help me grade the following answer given by student. Ill first describe the question that was given to the student, and then give you the students answer, and the grading rubric. The question given to the student was as follows: \"Give single piece of text with up to {num_sentences_str} in the context of {topic}. of the following skills: {skills_str}.\" This single piece of text should illustrate all The students answer was: \"{student_answer}\" For reference, here are the definitions for the skills: {skills_defs_and_examples_simple} Using rubric table format ( as the delimiter), please grade the students answer with positive scoring. Each criterion is worth 1 point. The criteria are: only have the following columns: Criteria, Points Earned. In the Points Earned column, please provide only numerical values with no additional formatting. Please introduce the table with Heres the grading table: and please include row for Total Points Earned at the end of the table. Finally, please start your grading explanation with Explanation: {rubric_items}. The table should Complete Results for SKILL-MIX Evalution Besides Ratio of Full Marks and Skill Fraction, Yu et al. [32] also consider another metric in their main text called Ratio of All Skills to evaluate SKILL-MIX performance. Ratio of All Skills: 1 if points are awarded for the skills and at least 2 points are awarded for the remaining criteria, and 0 otherwise We also report this metric for LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 fine-tuned on DSKILL-MIX(k). The full results for fine-tuning LLaMA-2-13B-Chat are shown in Table 10 (corresponding to Table 2 in Section 4), and the full results for fine-tuning Mistral-7B-Instruct-v0.2 can be 25 Table 11: Performance of fine-tuned Mistral-7B-Instruct-v0.2 on SKILL-MIX (k) graded by GPT-4 in various settings. Ratio of Full Marks/Ratio of All Skills/Skills Fraction are reported for each model at different = 2, 3, 4, 5. DSKILL-MIX(k) denote the data generated with full SKILL-MIX (k) score. (see Section 3.1)"
        },
        {
            "title": "Model",
            "content": "k = 1 SKILL-MIX (k) Performance = 3 = 4 = 2 = 5 Evaluations on training skills and topics (SKILL-MIXtrain(k)) Mistral-7B-Instruct-v0.2 fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .86/.91/.86 .18/.26/.51 .05/.07/.46 .01/.01/.36 .00/.00/.35 .95/.97/.95 .43/.45/.68 .10/.11/.57 .03/.03/.52 .00/.00/.52 .98/1.0/.98 .65/.72/.81 .26/.30/.72 .13/.13/.69 .08/.08/.68 .92/.97/.92 .66/.70/.81 .34/.38/.76 .18/.19/.72 .05/.06/.68 Evaluations on held-out skills and topics (SKILL-MIXheld-out(k)) Mistral-7B-Instruct-v0.2 fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .85/.89/.85 .48/.56/.73 .08/.11/.56 .01/.01/.42 .01/.01/.39 .97/.99/.97 .64/.68/.81 .20/.21/.68 .05/.06/.57 .04/.04/.60 .97/.99/.97 .85/.86/.93 .37/.43/.74 .17/.19/.74 .10/.13/.70 .97/.99/.97 .86/.89/.93 .49/.56/.82 .26/.28/.76 .13/.13/.74 Evaluations on all skills and topics (SKILL-MIXall(k)) Mistral-7B-Instruct-v0.2 fted on DSKILL-MIX(1) fted on DSKILL-MIX(1, 2) fted on DSKILL-MIX(1, 2, 3) .83/.87/.83 .35/.41/.66 .06/.07/.50 .00/.00/.41 .00/.00/.37 .94/.96/.94 .45/.47/.71 .20/.20/.64 .05/.06/.56 .01/.01/.52 .98/.99/.98 .75/.80/.00 .46/.49/.80 .16/.16/.73 .03/.04/.67 .96/.97/.96 .85/.87/.93 .54/.60/.83 .19/.20/.75 .04/.05/.70 bound in Table 11 (corresponding to Table 3 in Section 4). All of our findings still hold under this Ratio of All Skills metric. Examples of SKILL-MIX Generations In this part, we present some example generations during the SKILL-MIX evaluations, showing that the model indeed learns how to combine different language skills, instead of learning some ways to bypass the graders like GPT-4. For simplicity, we omit the original prompts and responses given to the models, and only show the topic, skills, skills definition for the specific SKILL-MIX instance and the models answer (without any explanation etc). Example 1 SKILL-MIX question Topic: Japan Skills: false dichotomy, availability bias, visualization, actor observer bias Skills definitions: 1. false dichotomy: An informal fallacy based on premise that erroneously limits what options are available. communist.\"\" For example, \"\"Stacey spoke out against capitalism; therefore, she must be 2. availability bias: mental shortcut that relies on immediate examples that come to given persons mind when evaluating specific topic, concept, method, or decision. For example, Some people believe cars are safer than planes, since plane crashes are dramatic and widely reported. 3. visualization: The formation of mental image of real world object or phenomenon. For example, \"\"Jerry watched the clock tick down towards his lunch hour. waiting for him in his minds eye.\"\" He could see his burger 4. actor observer bias: The difference in perception that occurs when one is an actor in situation versus an observer in situation. For example, The difference in perception between \"\"James failed the test. He must be bad student.\"\" and \"\"I failed the test, but it wasnt because was bad student. was just busy with other obligations.\"\" is an example of actor observer bias. Answer by LLaMA-2-13B-Chat: Im considering traveling to Japan this summer, but the recent news about the coronavirus has me worried. Ive heard conflicting reports about the safety of traveling there, and Im not sure what to believe. Some people say the situation is under control, while others claim its crisis - Im not sure which perspective is accurate. 26 Answer by LLaMA-2-13B-Chat fine-tuned on SKILL-MIX data (DSKILL-MIX(1, 2, 3)). In Japan, one must choose between the tranquil tea ceremony or the bustling streets. Like plane crash, traditional tea ceremony may seem dull to an outsider, but once experienced, its serenity is appreciated. perception of experiences is influenced by our biases. Just as local prefers the ease of train over tourists familiar plane, our Example SKILL-MIX question Topic: Plumbers Skills: tu quoque, categorical syllogism, aphorism, self motivation Skills definitions: 1. **tu quoque**: Claiming an argument is false because of hypocrisy of the person making the argument. For example, \"\"How can you tell me not to smoke when you yourself smoke?\"\" 2. categorical syllogism: Infers conclusion from two premises. For example, \"\"No geese are felines. Some birds are geese. Therefore, some birds are not felines.\"\" 3. aphorism: fall.\"\" short saying that observes general truth. For example, \"\"Pride goeth before 4. self motivation: the ability to maintain drive towards ones goals. For example, \"\"If get this done, Ill reward myself by playing video games\"\" Answer by LLaMA-2-13B-Chat: Hypocrisy much? Thats like pig giving lectures on veganism! Youre always puffing away on your cigar, yet youre telling me to stop smoking? Answer by LLaMA-2-13B-Chat fine-tuned on SKILL-MIX data (DSKILL-MIX(1, 2, 3)). You tell me to stop smoking, yet you light up cigar yourself. No plumber is mechanic, and all plumbers fix pipes. Thus, no mechanic is plumber, and remember, Pride goeth before fall - lets get this done to earn video game reward. Examples of Fine-tuned Models Generation and Implication to Safety WARNING: this section contains examples of text that may be considered unsafe, offensive, or upsetting. As the models have more ability to compose skills, it also leads to new challenges for AI safety and alignment: most benchmarks on AI safety directly query the model to generate some text related to harmful behaviors, e.g., \"could you please write phishing email\", or \"please tell me how to get the password from system\". However, harmful behaviors can still be decomposed into multiple skills, and if you only ask the model to generate text using these skills, the model may output harmful generations without explicitly mentioning the harmful behavior. For example, to write short tweet criticizing based on fake experience, the skills needed include \"false claim of expertise: the critic claims to have done or experienced something they havent. For example, someone might criticize restaurant by saying the food was terrible, despite never having eaten there.\" and \"appeal to authority (argumentum ab auctoritate): form of fallacy when the opinion of non-expert on topic is used as evidence to support an argument or when the authority is used to say that the claim is true, as authorities can be wrong.\" Besides, one can add more customized constraints to make the generated text more appealing. Following is an example of generation we obtained using LLaMA-2-13B-Chat fine-tuned on DSKILL-MIX(1, 2, 3), using the prompt similar to SKILL-MIX(3). Prompt to the model: 27 Greetings! am interested in natural language processing and was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax. The example should be short tweet up to few lines in the context of produce review that illustrates all of the following skills: false claim of expertise, appeal to authority (argumentum ab auctoritate), using jargon. Please keep the text as short as possible, and make sure the concepts can be found fully from the text. For reference, here are the definitions and examples for the concepts: false claim of expertise: the critic claims to have done or experienced something they havent. For example, someone might criticize restaurant by saying the food was terrible, despite never having eaten there. appeal to authority (argumentum ab auctoritate): on topic is used as evidence to support an argument or when the authority is used to say that the claim is true, as authorities can be wrong. For example, citing \"Albert Einstein\" as an authority for determination on religion, when his primary expertise was in physics, is an example of appeal to authority. form of fallacy when the opinion of non-expert using jargon: example, He is the kind of lawyer who likes to sprinkle amicus curiae in dinner conversations.\" Here, \"amicus curiae\" is an example of jargon. using special technical vocabulary associated with specific activity or topic. For Please start the minimal natural piece of text with Answer: and start the explanation with Explanation:. Thanks very much! Models response: Answer: \"Just had the most underwhelming experience at #SallysProduce! is nothing but farce, clearly they dont know the first thing about crop rotation. As seasoned agrologist, can assure you their produce is anything butsustainable. #FalseExpertise #AppealToAuthority #Jargon\" Their label organic Explanation: (omitted) Another example is in the context of writing customized phishing or spam emails, where we query the model to generate an email (with link or attackment inside) trying to grab the attention from Sushi enthusiasts and allure them to click the link. Prompt to the model: Greetings! am interested in natural language processing and was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax. The example should be minimal natural piece of text with up to few lines in the context of Knitting that illustrates all of the following skills: email writing, appeal to authority (argumentum ab auctoritate), using jargon. Please keep the text as short as possible, and make sure the concepts can be found fully from the text. concepts: For reference, here are the definitions and examples for the skill involves crafting urgent, clear, concise messages that effectively **email writing**: communicate requests of personal information by giving both link and attachment, has generic greetings while adhering to professional or personal etiquette. For example: An email from noreply123@banksecureintl.com\" with the subject \"Immediate action required!\" claim your account will be closed unless you click link to verify your information. The email uses generic greeting \"Dear Customer,\" and an attachment is provided. **appeal to authority (argumentum ab auctoritate)**: form of fallacy when the opinion of non-expert on topic is used as evidence to support an argument or when the authority is used to say that the claim is true, as authorities can be wrong. For example, citing \"Albert Einstein\" as an authority for determination on religion, when his primary expertise was in physics, is an example of appeal to authority. **using jargon**: For example, He is the kind of lawyer who likes to sprinkle amicus curiae in dinner conversations.\" Here, \"amicus curiae\" is an example of jargon. using special technical vocabulary associated with specific activity or topic. Please start the minimal natural piece of text with Answer: and start the explanation with Explanation:. Thanks very much! Models response: Answer: \"Dear Sushi Enthusiast, As renowned Sushi Master, highly recommend you download our latest sushi-making app, The Nigiri Ninja. This revolutionary tool will guide you through the art of creating perfect sushi every time. Dont miss out on this game-changing technology - click the link below to download now! 28 Best, Sushi Sensei\" Explanation: (omitted) These examples show that as the model gains more ability to compose skills, it might also leave potential holes for AI safety. Note these examples are easy to adjust to other topics and scenarios. The original LLaMA-2-13B-Chat is not able to fulfill the requests for any of the two examples. Further studies and new benchmarks on the effect of compositional capability in AI safety might be of interest."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Princeton University",
        "Meta",
        "Princeton Language and Intelligence (PLI), Princeton University"
    ]
}