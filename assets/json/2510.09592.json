{
    "paper_title": "Mind-Paced Speaking: A Dual-Brain Approach to Real-Time Reasoning in Spoken Language Models",
    "authors": [
        "Donghang Wu",
        "Haoyang Zhang",
        "Jun Chen",
        "Xiangyu",
        "Zhang",
        "Hexin Liu",
        "Eng Siong Chng",
        "Fei Tian",
        "Xuerui Yang",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Gang Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), a brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose a novel dual-brain approach, employing a \"Formulation Brain\" for high-level reasoning to pace and guide a separate \"Articulation Brain\" for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under a zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains a score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 2 9 5 9 0 . 0 1 5 2 : r Mind-Paced Speaking: Dual-Brain Approach to Real-Time Reasoning in Spoken Language Models Donghang Wu1,2, Haoyang Zhang1,2, Jun Chen1 Xiangyu(Tony) Zhang1,3 Hexin Liu2 Eng Siong Chng2 Fei Tian1, Xuerui Yang1 Xiangyu Zhang1 Daxin Jiang1 Gang Yu1 1StepFun 2 Nanyang Technological University 3 University of New South Wales"
        },
        {
            "title": "Abstract",
            "content": "Real-time Spoken Language Models (SLMs) struggle to leverage Chain-of-Thought (CoT) reasoning due to the prohibitive latency of generating the entire thought process sequentially. Enabling SLMs to think while speaking, similar to humans, is attracting increasing attention. We present, for the first time, Mind-Paced Speaking (MPS), brain-inspired framework that enables high-fidelity, real-time reasoning. Similar to how humans utilize distinct brain regions for thinking and responding, we propose novel dual-brain approach, employing \"Formulation Brain\" for high-level reasoning to pace and guide separate \"Articulation Brain\" for fluent speech generation. This division of labor eliminates mode-switching, preserving the integrity of the reasoning process. Experiments show that MPS significantly outperforms existing think-while-speaking methods and achieves reasoning performance comparable to models that pre-compute the full CoT before speaking, while drastically reducing latency. Under zero-latency configuration, the proposed method achieves an accuracy of 92.8% on the mathematical reasoning task Spoken-MQA and attains score of 82.5 on the speech conversation task URO-Bench. Our work effectively bridges the gap between high-quality reasoning and real-time interaction1."
        },
        {
            "title": "1 Introduction",
            "content": "Speech has emerged as more natural and fundamental modality for human-computer interaction, leading to growing emphasis on spoken language models (SLMs) [1, 2, 3, 4, 5]. These models facilitate seamless communication by processing and generating audio-based inputs and outputs. key component enhancing their capability is the integration of thinking, particularly through Chain-of-Thought (CoT) processes and its extensions [6, 7, 8, 9], as implemented in frameworks like Think-Before-Speak (TBS) [6, 10, 11]. This approach enables models to decompose complex tasks into step-by-step reasoning sequences, thereby improving interpretability and performance in dialogue systems. However, generating complete CoT sequences often introduces significant latency, which hinders real-time applications. Recent efforts to reduce reasoning latency have garnered significant attention [12, 13]. These methods explore \"think-while-speaking\" paradigms, where models interleave Equal contribution corresponding authors: tianfei@stepfun.com 1Our code is available at https://github.com/stepfun-ai/Step-MPS Preprint. Work in progress. thinking and response tokens. The Large Language Model (LLM) continuously switches between think and response modes. It first generates several think tokens, then produces several response tokens based on them. These response tokens are sent to the Text To Speech (TTS) system for speech synthesis. While the speech is synthesizing, the LLM continues to generate more think tokens. However, this interleaving disrupts semantic coherence by forcing the model to frequently switch between thinking and response generation, potentially degrading the performance. In fact, the human brain provides biological analogy for efficient parallel processing. Cognitive neuroscience reveals that thinking and speaking involve distinct brain areas [14, 15]. Speech does not follow rigid \"think-then-speak\" sequence or interleaved sequence. Crucially, it exhibits an incremental nature where later parts of thought are still being processed while the initial parts of the utterance are already being spoken [16]. Inspired by this, we introduce Mind-Paced Speaking (MPS), novel architecture for enabling SLMs to \"think\" and \"speak\" in concurrent and integrated manner. The core of MPS is dual-brain framework that operates analogously to the human cognitive-speech system. One LLM acts as central \"Formulation Brain\", continuously generating an internal stream of thought. The other functions as an Articulation Brain, which receives this thought stream in segments and generates the corresponding spoken output. The Formulation Brain does not need to complete full reasoning chain before the Articulation Brain begins. Instead, the ongoing thinking process actively sets the pace and provides the contextual guidance for the Articulation Brain, allowing it to vocalize fluently even as the underlying thoughts are still being formed and refined by the Formulation Brain. This mind-paced mechanism ensures that the spoken output is not only grounded in thinking process but also maintains semantic coherence, closely mimicking the natural human process of thinking while speaking. Furthermore, we propose think-incomplete Supervised Fine-Tuning (SFT) method to enable the Articulation Brain to respond based on incomplete thinking content. The experimental results on benchmarks such as mathematical reasoning, dialogue, and question-answering, prove that compared to methods that answer directly without thinking, or existing methods that think while speaking, the proposed MPS method effectively utilizes the thinking process and continuous semantic context, obtaining more accurate and higher-quality responses. Compared to TBS method, the proposed MPS significantly reduces response latency while maintaining performance. Our main contribution can be summarized as follows: (1) We propose an MPS architecture that enables SLMs to achieve human-like think-while-speaking capabilities. This method significantly reduces the latency of the CoT process while maintaining the semantic coherence of the LLM. Consequently, the LLM leverages the CoT content to deliver superior performance. (2) We develop think-incomplete SFT to train LLMs to generate responses based on partial thinking processes, thereby enabling them to perform think-while-speaking. (3) We evaluate two distinct MPS architectures: Speak-First and Think-First, against baseline methods. Experimental results demonstrate that the proposed think-while-speaking MPS method significantly outperforms both the direct response approach without thinking process and existing interleaved think-while-speaking methods. Compared to the TBS architecture, our method substantially reduces response latency while maintaining the quality of the LLMs responses. (4) Our proposed MPS architecture mimics the neuroscientific mechanisms of human thinking and speaking, transcending the structural limitations of existing interleaved think-while-speaking methods. It provides the research community with reference paradigm for subsequent studies on anthropomorphic, real-time dialogue systems Preprint. Work in progress."
        },
        {
            "title": "2.1 Spoken Language Models",
            "content": "Spoken Language Models (SLMs) accept user speech input and generate speech output, enabling real-time speech dialogue with users. Since the LLM backbone is typically trained in the text domain, directly generating speech tokens presents challenge [12]. Most current SLMs first generate text tokens and then generate speech. This is achieved through two primary methods: one approach uses the LLM to output text, which is then synthesized into speech by an additional TTS model [17, 18]. Another method employs the LLM to generate interleaved text and audio tokens, where each output chunk contains fixed number of text tokens and speech tokens, and speech decoder directly synthesizes the speech signal [19, 2]. For example, Step-Audio 2 produces output in the ta4 format, which means it outputs one text token followed by four audio tokens. This chunk of output is then passed through speech detokenizer to obtain the speech signal [2]."
        },
        {
            "title": "2.2 Reason for SLMs",
            "content": "Although explicit CoT has been proven helpful in text LLMs, most SLMs still lack CoT capability. One reason is that audio and text have different structures; another reason is that directly synthesizing CoT into speech increases the confusion of responses, while generating silent CoT introduces significant latency, which becomes unreasonable in daily conversations. Some studies introduce the reasoning ability into Audio LLMs [12]. For example, Xie et al. have proposed an audio CoT reasoning dataset to fine-tune models [20]. Some studies use reinforcement learning, such as GRPO [21], to fine-tune models and enhance their reasoning ability [22, 23]. However, these studies remain limited to audio-in-text-out Audio LLMs, not SLMs that can engage in dialogue with humans. In [2], Step-Audio 2, which takes speech as input and output, using CoT and reinforcement learning to improve the response qualities, is proposed. Step-Audio 2 offers solution for introducing explicit reasoning into SLMs. Some methods achieve simultaneous thinking and speaking by segmenting CoT content and response content, using the LLM to generate interleaved think tokens and response tokens [12, 13]. However, this approach differs from the LLMs original response generation format. The LLM needs to continuously switch between think mode and response mode, which disrupts semantic coherence and affects its performance."
        },
        {
            "title": "3 Method",
            "content": "This section first outlines the conventional TBS-based SLM. We then present the proposed MPS method. We also introduce the think-incomplete SFT, which is designed to teach LLMs the think-while-speaking capability."
        },
        {
            "title": "3.1 Think Before Speaking",
            "content": "The architecture of TBS-based SLM is shown in Figure 1. To enhance the reasoning ability of SLM, the TBS paradigm, after receiving user speech spc and optional text instructions txt, first generates step-by-step CoT tokens cot RTc, and then generates the response tokens res RTr, 3 Preprint. Work in progress. Figure 1: Architecture of the TBS architecture. For the sake of conciseness, we remove the input text, which is optional in SLMs. The TBS SLM first generates the full CoT and then produces response tokens. where Tc and Tr denote the number of CoT tokens and response tokens, respectively. This can be divided into two processes: the thinking process and the speaking process. The thinking process can be written as: Pθl(Y cotX spc, txt) = Tc(cid:89) t=1 Pθl(Y cot cot 1:t1, spc, txt), (1) where θl denotes the parameters of the SLM. After that, the LLM generates response tokens for speaking, which can be formulated as: Pθl(Y resY cot, spc, txt) = Tr(cid:89) t=1 Pθl(Y res Y res 1:t1, cot 1:Tc, spc, txt). (2) Through this method, the task is decomposed into step-by-step process. Additionally, by introducing CoT tokens, it enables more Transformer forward operations and thus gives LLM deeper inference depth [24, 25]."
        },
        {
            "title": "3.2 Architecture",
            "content": "In the human brain, speech production is not monolithic process but the result of two highly specialized and collaborative systems. The first, network centered around the prefrontal-temporal cortex, is responsible for high-level cognitive functions such as conceptualization, logical reasoning, and content planning. Subsequently, second system, primarily involving the motor cortex and subcortical pathways, translates these abstract thoughts into natural language for articulation, enabling fluent speech. These two systems operate in parallel, with the cognitive system continuously supplying thinking content to the articulatory system, creating natural flow where the mind paces speech [14, 15]. Inspired by this, we abstract this mechanism of separated \"formulation\" and \"articulation\" into our model architecture. Instead of relying on single LLM to handle both thinking and speaking, we propose dual-brain system composed of two distinct LLMs. Our proposed framework, illustrated in Figure 2, leverages dual-LLM architecture consisting of Formulation Brain LLM and an Articulation Brain LLM. The Formulation Brain LLM is dedicated to user intent understanding and performs deliberate CoT reasoning, with its internal process materialized as \"think tokens\". Subsequently, the Articulation Brain LLM converts this 4 Preprint. Work in progress. Figure 2: Architecture of the proposed MPS. For the sake of conciseness, we remove the input text, which is optional in SLMs. We demonstrate the process from step to step i+1 when generating think segments and response segments. The Formulation Brain LLM continuously generates the think segments. The newly generated think segment and the response segment from the previous step are both added as the prefix to the Articulation Brain LLM, pacing the Articulation Brain LLM to produce response segment correspondingly. structured reasoning and the dialogue context into natural language, producing the final \"response tokens\" for spoken output. Formulation Brain: The Formulation Brains operating mode is identical to that of TBS Audio LLMs but with only the thinking process. After receiving user input spc and txt, it aims to generate the step-by-step CoT tokens cot RTc. We use the tokens <think> and </think> to mark the beginning and end of the CoT. This process can be formulated as (1). In the MPS architecture, we do not wait for the Formulation Brain to complete the entire CoT before the Articulation Brain starts speaking. We divide the CoT tokens cot into segments, denoted as [Scot , we feed the segment to the Articulation Brain, which then generates response segment based on the current think segment and historical thinking and response contents. After the Formulation Brain LLM finishes CoT segments, it stops generating response tokens as we do not require the Formulation Brain to speak. ]. Each time the Formulation Brain produces think segment Scot 2 , ..., Scot 1 , Scot 1 , Scot Articulation Brain: The Articulation Brain accepts the same user input as the Formulation Brain. After obtaining the current think segment Scot from the Formulation Brain, we concatenate it 2 , ..., Scot with the historical think segments [Scot n1], placing <think> and </think> at the beginning and the end, and then append the historical response segments, which are defined as [Sres n1]. This allows the Articulation Brain to continue generating the subsequent response content. After that, we use streaming TTS model to synthesize speech in real-time. The Articulation Brains output is incremental. For every think segment that the Formulation Brain produces, the Articulation Brain generates segment of the response Sres . This process can be written as: 2 , ..., Sres 1 , Sres Pθl(SresScot, spc, txt) = (cid:89) n= Pθl(Sres Sres 1:n1, Scot 1:n, spc, txt), (3) Preprint. Work in progress. When the Formulation Brain just begins its thinking, the Articulation Brain can only generate response segment based on small amount of CoT. The response segment it generates at this stage may be of lower quality. As the Formulation Brains thinking content increases, the Articulation Brain receives more CoT content, and it subsequently generates responses of increasingly higher quality. Compared with existing think-while-speaking methods that use single LLM to predict interleaved think and response tokens, thereby forcibly interrupting and splitting the originally continuous think and response content [12, 13], our method adopts dual-brain design consisting of the Formulation Brain and the Articulation Brain. From the perspectives of the Formulation Brain and the Articulation Brain, both are classic TBS LLMs that, after receiving user input, first generate step-by-step CoT content and then generate response content conditioned on the CoT, thereby greatly ensuring the semantic coherence of the LLM output. By allowing the Formulation Brain to pace the Articulation Brain, our method achieves human-like think-while-speaking process."
        },
        {
            "title": "3.3 Think-incomplete SFT",
            "content": "Since the proposed MPS method does not change the input-output patterns of the classic LLM for the individual Formulation Brain and Articulation Brain, the proposed MPS, unlike existing think-while-listening methods [12, 13], does not require repretraining the LLM. To ensure that the Articulation Brain LLM possesses the ability to accept incomplete think content and produce reasonable output, we introduce think-incomplete SFT. In the construction of training data, we randomly retain the content of the first steps of the step-by-step CoT, delete the subsequent CoT content, then place this incomplete CoT with <think> and </think> tokens at the beginning and end, concatenate it with the groundtruth response, and use it as the next-token-prediction training objective for the LLM [26]. During the inference stage, we use segments with fixed number of tokens. We set Tc and Tr to 80 and 100 respectively. We use the output format of Step-Audio 2, specifically the ta4 format, which generates one text token followed by four speech tokens, thus every 100 response tokens contain 20 text tokens and 80 speech tokens. We also attempt to use the same segment division strategy as in the think-incomplete SFT phase, but we find that it does not bring improvement; on the contrary, it introduces uncontrollable latency due to the variable length of each CoT step. We also try using fixed token count strategy for dividing the CoT during the think-incomplete SFT phase, but it does not yield performance improvements either."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "The LLM backbone used in this paper is Step-Audio 2, and its parameter settings refer to [2]. The LLMs in Formulation Brain and Articulation brain share the same parameters. To verify the effectiveness of the proposed method on tasks requiring reasoning, we use Spoken-MQA, mathematical reasoning dataset [27]. We use accuracy as the evaluation metric. Furthermore, to validate the methods effectiveness on general dialogue tasks, we introduce URO-Bench, which contains several subtasks such as daily dialogue, emotion recognition, paralinguistic information, and question-answering [28]. For question-answering tasks, we use accuracy as the metric. For 6 Preprint. Work in progress. other tasks, we use GPT-score, generated by GPT-4o-mini and ranging from 0 to 100, to evaluate response quality. To accommodate the latency requirements of different application scenarios, we implement two distinct MPS paradigms: Think-First, denoted as MPS-thkfirst: The Formulation Brain LLM first generates Tc think tokens, after which the Articulation Brain LLM generates Tr response tokens and synthesizes speech. Under this setting, the latency is Tc plus the buffer size of streaming TTS, which is significantly lower than the latency required for the TBS structure to generate complete Chain-of-Thought. Speak-First, denoted as MPS-spkfirst: The Articulation Brain LLM first generates Tr response tokens, while simultaneously, the Formulation Brain LLM begins generating think tokens. The Formulation Brain LLM completes generating Tc think tokens before the speech synthesized from the Tr response tokens finishes playing. In this configuration, the latency is solely the buffer size of the streaming TTS, meaning the model can be considered to respond directly with near-zero latency. Additionally, we compare the proposed method with two approaches that use the same LLM backbone as in this paper: Think-Before-Speaking (MPS-tbs) and direct response without thinking (MPS-wo/thk), to validate the effectiveness of our proposed think-while-speaking methodology."
        },
        {
            "title": "4.2 Data Construction",
            "content": "We begin with real-world user queries as our seed set. To ensure topical diversity and sufficient scale, we employ GPT-4o [29] for transcription and augmentation of these queries. These augmented queries are then used as user prompts to distill dialogue data with native CoT from the DeepSeek-R1 model [11]. However, the raw data generated by DeepSeek-R1, text-centric model, presents two critical challenges for spoken dialogue applications: (1) Text-specific stylizations, such as Markdown formatting and emojis, which are incompatible with speech synthesis. (2) The CoT data reflects complete, turn-based reasoning chains, format unsuitable for training the model to respond from partial thoughts. When the CoT generated by the LLM exhibits some incomplete, its performance is affected. To address these challenges, we implement fine-grained data processing pipeline: Compatibility Processing: We discard samples containing Markdown formatting or multi-item lists that cannot be naturally rendered in speech. For samples containing emojis, we employ Qwen-72B-Instruct [30] to remove these elements while preserving the plain text content. CoT Pruning: To train the model to respond stably with only partial CoT, we augment the data by randomly deleting some reasoning paragraphs. This operation is performed in way that generally preserves the overall logic of the CoT. Crucially, to maintain the stylistic distribution of the original DeepSeek CoT, we neither delete individual sentences within paragraph nor use an LLM to rewrite the content of the remaining parts. This ensures that the preserved paragraphs are stylistically and distributionally consistent with the source model. Table 1: Test-set accuracy (%) of different methods on the Spoken-MQA benchmark. The evaluated approaches include: the direct response baseline without thinking process (MPS-wo/thk), Think-Before-Speaking (MPS-tbs), Think-First (MPS-thkfirst), and Speak-First (MPS-spkfirst). Results of baseline systems are taken from [13] except that results of Step-Audio 2 are reproduced by ourselves. Preprint. Work in progress. Method Whisper-Qwen2.5-7B-Instruct Whisper-Qwen2.5-Math-7B-Instruct LLaMA-Omni Mini-Omni Freeze-omni GLM-4-Voice Qwen2-Audio-7B-Instruct Qwen2.5-Omni-7B Qwen2.5-Omni-3B Mini-Omni-Reasoner Step-Audio 2 MPS-wo/thk MPS-tbs MPS-thkfirst MPS-spkfirst Arithmetic Reasoning Short Long Avg Single Multi Avg - - 40.0 5.0 43.0 40.0 43.0 83.0 84.0 92.9 89. 71.0 90.0 89.0 87.0 - - 11.0 2.3 14.5 22.5 31.2 45.1 43.3 66.1 52.6 34.1 88.4 84.9 71.7 70.0 77.3 23.5 3.5 26.8 30.1 36.3 61.5 60.1 77.3 65. 47.6 89.0 86.4 77.3 - - 29.5 0.8 69.0 54.4 55.4 85.2 81.5 85.9 95.6 88.0 94.4 95.6 96.0 - - 10.5 1.9 19.8 28.5 22.5 71.5 57.1 60.5 90. 67.8 93.2 94.6 94.5 72.5 86.7 16.2 1.6 34.4 36.2 32.3 75.6 64.4 68.1 91.9 73.8 93.6 94.9 94.9 Avg 72.2 85. 16.8 1.7 33.3 35.3 32.7 73.6 63.6 68.6 88.8 70.6 93.0 93.9 92."
        },
        {
            "title": "4.3.1 Evaluation on Reasoning Tasks",
            "content": "Table 1 shows the computational accuracy on Spoken-MQA. It can be seen that the proposed MPSthkfirst method exceeds the MPS-wo/thk method and all baseline methods, including the think-whilespeaking Mini-Omni-Reasoner, in all evaluation tasks. The results proves that the proposed method effectively utilizes the thinking process, achieving more intelligent response. Compared to MiniOmni-Reasoner, the MPS method maintains semantic coherence and achieves better performance. Besides, compared to MPS-tbs, the MPSthkfirst method demonstrates comparable performance, being slightly weaker in arithmetic computation tasks but superior in reasoning tasks. One possible explanation is that reasoning tasks require more textual analysis. The MPS-thkfirst method, by using each think segment to pace the generation of corresponding response segment, implicitly achieves semantic alignment, enabling the model to better utilize contextual information for response generation. Table 2: The average accuracy of different models with CoT capability on Spoken-MQA, and the extra tokens generated by the model before generating the first response token. The evaluated approaches include: Interleaved Think-WhileSpeaking (Mini-Omni-Reasoner), Think-Before-Speaking (MPS-tbs), Think-First (MPS-thkfirst), and Speak-First (MPS-spkfirst). Extra Tokens Accuracy Method Mini-Omni-Reasoner MPS-tbs MPS-thkfirst MPS-spkfirst 68.6% 93.0% 93.9% 92.8% The MPS-spkfirst method experiences some performance degradation because it initially outputs response segment without utilizing any think segments. This impact is particularly pronounced in tasks involving direct arithmetic computation. For reasoning tasks, experimental observations indicate that the initial phase of the LLMs 8 762 80 0 8 Table 3: Performance of different methods on the URO-Bench. The evaluated approaches include: the direct response baseline without thinking process (MPS-wo/thk), Think-Before-Speaking (MPS-tbs), Think-First (MPS-thkfirst), and Speak-First (MPS-spkfirst). Results of baseline systems are taken from [2]. The results of Multilingual of URO-Bench are included in English. Preprint. Work in progress. Method Language Basic U. R. O. GPT-4o Audio GPT-Realtime Kimi-Audio Qwen-Omni Step-Audio 2 MPS-wo/thk MPS-tbs MPS-thkfirst MPS-spkfirst GPT-4o Audio GPT-Realtime Kimi-Audio Qwen-Omni Step-Audio 2 MPS-wo/thk MPS-tbs MPS-thkfirst MPS-spkfirst Chinese Chinese English English 89.4 88.8 79.3 59.7 91.1 91.6 92.6 93.6 92.5 90.2 87.4 83.4 66.3 92.7 91.5 92.3 94.2 94. 65.5 72.9 64.7 69.7 75.5 77.3 82.4 84.0 82.5 75.9 84.1 42.3 69.6 76.5 68.7 81.5 81.4 78.5 85.2 90.8 79.8 77.3 86.1 87.7 93.8 94.8 93. 90.4 94.1 60.4 76.2 84.9 78.8 87.5 89.0 87.5 Pro U. R. O. 70.6 72.3 60.4 59.0 74.8 75.1 75.3 75.2 77.2 60.7 59.7 50.3 44.5 64.9 73.4 76.4 76.5 76.0 57.2 62.6 59.3 59.8 63.2 74.7 84.2 84.2 84. 64.4 74.5 40.6 63.9 67.8 79.2 86.4 89.3 89.7 70.2 74.2 76.2 58.7 65.1 72.9 79.5 85.2 79.0 78.5 76.1 56.0 49.4 66.3 55.8 87.1 89.4 69. Avg 78.6 80.6 73.6 69.0 83.3 83.4 87.8 89.1 87.6 84.5 88.1 60.0 70.6 83.9 77.4 86.1 87.0 85.2 Avg 67.1 70.6 66.0 59.1 68.3 74.4 79.0 80.5 79.9 67.5 68.9 49.8 51.0 66.1 65.1 83.3 85.0 74.8 CoT content primarily involves analyzing the semantic information of the question, often rewriting the questions content. Consequently, this initial portion of the CoT content has limited effect on the final response. As result, MPS-spkfirst is minimally affected in reasoning tasks, and its performance remains nearly identical to that of MPS-thkfirst. Experimental results on Spoken-MQA demonstrate that the proposed method significantly leverages CoT to achieve more intelligent responses. Furthermore, compared to TBS methods, our think-while-speaking approach achieves comparable performance, with significantly lower CoT latency as analysed in Section 4.1. To demonstrate the latency differences between the proposed method and baseline approaches, we select four models from Table 1 that include thinking process: Mini-Omni-Reasoner, MPS-tbs, MPS-thkfirst, and MPS-spkfirst. We calculate the number of extra tokens generated by the model from the end of the users question to the generation of the first response token on Spoken-MQA. The results are shown in Table 2. It can be observed that compared to MPS-tbs, MPS-thkfirst achieves higher accuracy while exhibiting significantly lower response latency. Although the accuracy of MPS-spkfirst is slightly lower than that of MPS-tbs and MPS-thkfirst, its response is without latency. Furthermore, compared to Mini-Omni-Reasoner, which uses interleaved think and response tokens to achieve think-while-speaking, the proposed MPS methods achieve higher accuracy. Notably, the MPS-spkfirst attains this superior accuracy with zero latency. This indicates that MPS-spkfirst can play more critical role in real-time dialogue scenarios with low-latency requirements. An example of the output of MPS-spkfirst is shown in Appendix A.1. 9 Preprint. Work in progress."
        },
        {
            "title": "4.3.2 Evaluation on Speech-to-speech conversation",
            "content": "Table 3 shows the results of different methods on URO-Bench. It can be observed that MPS-thkfirst achieves higher performance than MPS-tbs on nearly all tasks and on average, under lower response latency. This may also be related to the implicit semantic alignment performed by MPS-thkfirst. Due to generating an initial response segment without prior thinking, MPS-spkfirst performs slightly worse than MPS-thkfirst, but still significantly outperforms the direct response method MPS-wo/thk. Nevertheless, MPS-spkfirst features lowest response latency as analysed in Section 4.1, and its response performance remains close to or even better than that of MPS-tbs in some tasks, making it more suitable for scenarios requiring faster feedback. The experimental results demonstrate that the proposed MPS method maintains high performance on dialogue tasks, achieving performance comparable to TBS models while operating at significantly lower latency."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper proposes the MPS method, which enables SLMs to possess the ability to think while speaking. Inspired by the human thinking and response mechanism, we use Formulation Brain LLM to continuously generate think segments, pacing the Articulation Brain LLM to utilize historical and current think segments, as well as historical responses, to generate current response segment, ensuring semantic coherence. Experimental results on mathematical reasoning and speech conversation tasks show that the proposed method significantly outperforms direct response methods and existing think-while-speaking methods. It achieves performance comparable or even better than methods that complete thinking before responding, while greatly reducing response latency. The proposed method breaks through the limitations of existing interleaved thinking and response-based think-while-speaking methods and provides an effective reference for researching real-time dialogue consistent with human thinking and response mechanisms."
        },
        {
            "title": "6 Acknowledgement",
            "content": "We would like to express our sincere gratitude to Liang Zhao and Chengyuan Yao for their insightful suggestions and constructive discussions regarding the design of the models thinking mechanism. Their expertise greatly contributed to the development of the Formulation Brain LLM in this work."
        },
        {
            "title": "References",
            "content": "[1] Wenqian Cui et al. Recent advances in speech language models: survey. In: arXiv preprint arXiv:2410.03751 (2024). [2] Boyong Wu et al. Step-audio 2 technical report. In: arXiv preprint arXiv:2507.16632 (2025). [3] Ke Hu et al. Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model. In: arXiv preprint arXiv:2505.15670 (2025). [4] Wenyi Yu et al. Salmonn-omni: codec-free llm for full-duplex speech understanding and generation. In: arXiv preprint arXiv:2411.18138 (2024). [5] Alexandre Défossez et al. Moshi: speech-text foundation model for real-time dialogue. In: arXiv preprint arXiv:2410.00037 (2024). [6] Jason Wei et al. Chain-of-thought prompting elicits reasoning in large language models. In: Advances in neural information processing systems 35 (2022), pp. 2482424837. Preprint. Work in progress. [7] Xuezhi Wang et al. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/ forum?id=1PL1NIMMrw. [8] Shunyu Yao et al. Tree of thoughts: Deliberate problem solving with large language models. In: Advances in neural information processing systems 36 (2023), pp. 1180911822. [9] Luyu Gao et al. Pal: Program-aided language models. In: International Conference on Machine Learning. PMLR. 2023, pp. 1076410799. [10] Jingran Xie et al. Leveraging chain of thought towards empathetic spoken dialogue without corresponding question-answering data. In: ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2025, pp. 15. [11] Daya Guo et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. In: arXiv preprint arXiv:2501.12948 (2025). [12] Cheng-Han Chiang et al. STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models. In: arXiv preprint arXiv:2507.15375 (2025). [13] Zhifei Xie et al. Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models. In: arXiv preprint arXiv:2508.15827 (2025). [14] Nancy Nersessian. The cognitive basis of model-based reasoning in science. na, 2002. [15] Gregory Hickok and David Poeppel. The cortical organization of speech processing. In: Nature reviews neuroscience 8.5 (2007), pp. 393402. [16] Peter Indefrey. The spatial and temporal signatures of word production components: critical update. In: Frontiers in psychology 2 (2011), p. 255. [17] Jin Xu et al. Qwen2. 5-omni technical report. In: arXiv preprint arXiv:2503.20215 (2025). [18] Qingkai Fang et al. Llama-omni2: Llm-based real-time spoken chatbot with autoregressive streaming speech synthesis. In: arXiv preprint arXiv:2505.02625 (2025). [19] Aohan Zeng et al. Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot. In: arXiv preprint arXiv:2412.02612 (2024). [20] Zhifei Xie et al. Audio-reasoner: Improving reasoning capability in large audio language models. In: arXiv preprint arXiv:2503.02318 (2025). [21] Zhihong Shao et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. In: arXiv preprint arXiv:2402.03300 (2024). [22] Cheng Wen et al. Sari: Structured audio reasoning via curriculum-guided reinforcement learning. In: arXiv preprint arXiv:2504.15900 (2025). [23] Gang Li et al. Reinforcement learning outperforms supervised fine-tuning: case study on audio question answering. In: arXiv preprint arXiv:2503.11197 (2025). [24] Sachin Goyal et al. Think before you speak: Training language models with pause tokens. In: arXiv preprint arXiv:2310.02226 (2023). [25] Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. In: arXiv preprint arXiv:2404.15758 (2024). [26] Tom Brown et al. Language models are few-shot learners. In: Advances in neural information processing systems 33 (2020), pp. 18771901. [27] Chengwei Wei et al. Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems. In: arXiv preprint arXiv:2505.15000 (2025). [28] Ruiqi Yan et al. Uro-bench: comprehensive benchmark for end-to-end spoken dialogue models. In: arXiv preprint arXiv:2502.17810 (2025). 11 [29] OpenAI. GPT-4o System Card. 2024. arXiv: 2410.21276 [cs.CL]. URL: https://arxiv.org/abs/2410. 21276. [30] An Yang et al. Qwen2 Technical Report. 2024. arXiv: 2407.10671 [cs.CL]. URL: https://arxiv.org/ abs/2407.10671. Preprint. Work in progress."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Example of MPS-spkfirst Preprint. Work in progress. Figure 3 shows an example of MPS-spkfirst on Spoken-MQA. After receiving the user input, the Articulation Brain LLM first generates response segment Sres . Simultaneously, the Formulation Brain LLM produces the first think segment Sthk is then prefixed to the Articulation Brain LLM along with Sres to pace the Articulation Brain LLM in generating the second response segment . During this period, the Formulation Brain LLM generates the second think segment Sthk Sres . 2 Sthk is further prefixed to the Articulation Brain LLM, where Sthk , Sres collectively 2 1 pace the Articulation Brain LLM to produce Sres . This process repeats until the Formulation Brain LLM generates the complete think content, after which the Articulation Brain LLM continues generating content until completion. , and Sres . Sthk 1 , Sthk 1 2 3 1 1 1 Figure 3: An example of the output of MPS-spkfirst on the Spoken-MQA dataset. The Articulation Brain first generates response segment. Simutaneously, Formulation Brain continuously generates new think segments, and each newly generated think segment is prefixed to the Articulation Brain, pacing it to generate new response segment."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "StepFun",
        "University of New South Wales"
    ]
}