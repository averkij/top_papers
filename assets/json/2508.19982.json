{
    "paper_title": "Diffusion Language Models Know the Answer Before Decoding",
    "authors": [
        "Pengxiang Li",
        "Yefan Zhou",
        "Dilxat Muhtar",
        "Lu Yin",
        "Shilin Yan",
        "Li Shen",
        "Yi Liang",
        "Soroush Vosoughi",
        "Shiwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go \"all-in\" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet."
        },
        {
            "title": "Start",
            "content": "Pengxiang Li1, Yefan Zhou2, Dilxat Muhtar6,7, Lu Yin3, Shilin Yan, Li Shen4, Yi Liang5 Soroush Vosoughi2, Shiwei Liu6,7 1The Hong Kong Polytechnic University 5Google DeepMind 2Dartmouth College 6Max Planck Institute for Intelligent Systems 3University of Surrey 7ELLIS Institute Tubingen 4Sun Yat-sen University 5 2 0 2 7 2 ] . [ 1 2 8 9 9 1 . 8 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high-quality outputs. In this work, we highlight and leverage an overlooked property of DLMsearly answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random re-masking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go all-in (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4 while preserving high generation quality. These results recast DLM decoding as problem of when to stop sampling, and demonstrate that early decode convergence provides simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet."
        },
        {
            "title": "INTRODUCTION",
            "content": "Along with the rapid evolution of diffusion models in various domains (Ho et al., 2020; Nichol & Dhariwal, 2021; Ramesh et al., 2021; Saharia et al., 2022; Jing et al., 2022), Diffusion language models (DLMs) have emerged as compelling and competitively efficient alternative to autoregressive (AR) models for sequence generation (Austin et al., 2021; Lou et al., 2023; Shi et al., 2024; Sahoo et al., 2024; Nie et al., 2025; Gong et al., 2024; Ye et al., 2025). Primary strengths of DLMs over AR models include, but are not limited to, efficient parallel decoding and flexible generation orders. More specifically, DLMs decode all tokens in parallel through iterative denoising and remasking steps. The remaining tokens are typically refined with low-confidence predictions over successive rounds (Nie et al., 2025). Despite the speed-up potential of DLMs, the inference speed of DLMs is slower than AR models in practice, due to the lack of KV-cache mechanisms and the significant performance degradation associated with fast parallel decoding (Israel et al., 2025a). Recent endeavors have proposed excellent algorithms to enable KV-cache (Ma et al., 2025a; Liu et al., 2025a; Wu et al., 2025a) and improve the performance of parallel decoding (Wu et al., 2025a; Wei et al., 2025a; Hu et al., 2025). In this paper, we aim to accelerate the inference of DLMs from different perspective, motivated by an overlooked yet powerful phenomenon of DLMsearly answer convergence. Through extensive analysis, we observed that: strikingly high proportion of samples can be correctly decoded during"
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "the early phase of decoding for both semi-autoregressive remasking and random remasking. This trend becomes more significant for random remasking. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Motivated by this finding, we introduce Prophet, training-free fast decoding strategy designed to capitalize on early answer convergence. Prophet continuously monitors the confidence gap between the top-2 answer candidates throughout the decoding trajectory, and opportunistically decides whether it is safe to decode all remaining tokens at once. By doing so, Prophet achieves substantial inference speed-up (up to 3.4) while maintaining high generation quality. Our contributions are threefold: Empirical observations of early answer convergence: We demonstrate strikingly high proportion of samples (up to 99%) can be correctly decoded during the early phase of decoding for both semi-autoregressive remasking and random remasking. This underscores fundamental redundancy in conventional full-length slow decoding. fast decoding paradigm enabling early commit decoding: We propose Prophet, which evaluates at each step whether the remaining answer is accurate enough to be finalized immediately, which we call Early Commit Decoding. We find that the confidence gap between the top-2 answer candidates serves as an effective metric to determine the right time of early commit decoding. Leveraging this metric, Prophet dynamically decides between continued refinement and immediate answer emission. Substantial speed-up gains with high-quality generation: Experiments across diverse benchmarks reveal that Prophet delivers up to 3.4 reduction in decoding steps. Crucially, this acceleration incurs negligible degradation in accuracy-affirming that early commit decoding is not just computationally efficient, but also semantically reliable for DLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 DIFFUSION LARGE LANGUAGE MODEL The idea of adapting diffusion processes to discrete domains traces back to the pioneering works of Sohl-Dickstein et al. (2015); Hoogeboom et al. (2021). general probabilistic framework was later developed in D3PM (Austin et al., 2021), which modeled the forward process as discrete-state Markov chain progressively adding noise to the clean input sequence over time steps. The reverse process is parameterized to predict the clean text sequence based on the current noisy input by maximizing the Evidence Lower Bound (ELBO). This perspective was subsequently extended to the continuous-time setting. Campbell et al. (2022) reinterpreted the discrete chain within continuoustime Markov chain (CTMC) formulation. An alternative line of work, SEDD (Lou et al., 2023), focused on directly estimating likelihood ratios and introduced denoising score entropy criterion for training. Recent analyses in MDLM (Shi et al., 2024; Sahoo et al., 2024; Zheng et al., 2024) and RADD (Ou et al., 2024) demonstrate that multiple parameterizations of MDMs are in fact equivalent. Motivated by these groundbreaking breakthroughs, practitioners have successfully built productlevel DLMs. Notable examples include commercial releases such as Mercury (Labs et al., 2025), Gemini Diffusion (DeepMind, 2025), and Seed Diffusion (Song et al., 2025b), as well as opensource implementations including LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025). However, DLMs face an efficiency-accuracy tradeoff that limits their practical advantages. While DLMs can theoretically decode multiple tokens per denoising step, increasing the number of simultaneously decoded tokens results in degraded quality. Conversely, decoding limited number of tokens per denoising step leads to high inference latency compared to AR models, as DLMs cannot naively leverage key-value (KV) caching or other advanced optimization techniques due to their bidirectional nature. 2.2 ACCELERATION METHODS FOR DIFFUSION LANGUAGE MODELS To enhance the inference speed of DLMs while maintaining quality, recent optimization efforts can be broadly categorized into two complementary directions. The first direction focuses on integrating KV caching into the denoising process. These approaches can be further subdivided into two"
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "main strategies. One strategy leverages the empirical observation that hidden states exhibit high similarity across consecutive denoising steps, enabling approximate caching (Ma et al., 2025b; Liu et al., 2025b). The alternative strategy restructures the denoising process in semi-autoregressive or block-autoregressive manner, allowing the system to cache states from previous context or blocks. These methods may optionally incorporate cache refreshing that update stored cache at regular intervals (Wu et al., 2025b; Arriola et al., 2025; Wang et al., 2025b; Song et al., 2025a). On the other hand, the second direction focuses on optimizing sampling methods or reducing the total denoising steps through reinforcement learning (Song et al., 2025b). Sampling optimization methods aim to increase the number of tokens decoded at each denoising step through different selection strategies. These approaches employ various statistical measuressuch as confidence scores or entropyas thresholds for determining the number of tokens to decode simultaneously. The token count can also be dynamically adjusted based on denoising dynamics (Wei et al., 2025b) or through alignment with small off-the-shelf AR models (Israel et al., 2025b). Different from the above optimization methods, our approach stems from the observation that DLMs can correctly predict the final answer at intermediate steps, enabling early commit decoding to reduce inference time. Note that the early answer convergence has also been discovered by an excellent concurrent work (Wang et al., 2025a), where they focus on averaging predictions across time steps for improved accuracy, whereas we develop an early commit decoding method that reduces computational steps while maintaining quality."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "3.1 BACKGROUND ON DIFFUSION LANGUAGE MODELS Concretely, let x0 pdata(x0) be clean input sequence. At an intermediate noise level [0, ], we denote by xt the corrupted version obtained after applying masking procedure to subset of its tokens. Forward process. The corruption mechanism can be expressed as Markov chain q(x1:T x0) = (cid:89) t= q(xt xt1), (1) which gradually transforms the original sample x0 into maximally degraded representation xT . At each step, additional noise is injected, so that the sequence becomes progressively more masked as increases. While the forward process in Eq.(1) is straightforward, its exact reversal is typically inefficient because it unmasks only one position per step (Campbell et al., 2022; Lou et al., 2023). To accelerate generation, common remedy is to use the τ -leaping approximation (Gillespie, 2001), which enables multiple masked positions to be recovered simultaneously. Concretely, transitioning from corruption level to an earlier level < can be approximated as qst = (cid:89) i=1 qst(xi xt), qst(xi xt) = 1, , ts q0t(xi xi = [MASK], xi xi = [MASK], xi = [MASK], xi xt), xi = xi t, = [MASK], = [MASK]. Here, q0t(xi xt) is predictive distribution over the vocabulary, supplied by the model itself, whenever masked location is to be unmasked. In conditional generation (e.g., producing response x0 given prompt p), this predictive distribution additionally depends on p, i.e., q0t(xi xt, p). (2) Reverse generation. To synthesize text, one needs to approximate the reverse dynamics. The generative model is parameterized as pθ(x0:T ) = pθ(xT ) (cid:81)T t=1 pθ(xt1 xt) = (cid:81)T t=1 q(xt1 x0) pθ(x0 xt). (3) i. Prediction This reverse process naturally decomposes into two complementary components. step. The model pθ(x0 xt) attempts to reconstruct clean sequence from the corrupted input"
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "at level t. We denote the predicted sequence after this step by xt 0 = pθ(x0 xt). (2) ii. Re-masking step. Once candidate reconstruction xt 0 is obtained, the forward noising mechanism is reapplied in order to produce partially corrupted sequence xt1 that is less noisy than xt. This re-masking can be implemented in various ways, such as masking tokens uniformly at random or selectively masking low-confidence positions (Nie et al., 2025). Through the interplay of these two stepsprediction and re-maskingthe model iteratively refines an initially noisy sequence into coherent text output. 0, i.e. xt"
        },
        {
            "title": "3.2 EARLY ANSWER CONVERGENCY",
            "content": "In this section, we investigate the early emergence of correct answers in DLMs. We conduct comprehensive analysis using LLaDA-8B (Nie et al., 2025) on two widely used benchmarks: GSM8K (Cobbe et al., 2021) and MMLU (Hendrycks et al., 2021). Specifically, we examine the decoding dynamics, that is, how the top 1 predicted token evolves across positions at each decoding step, and report the percentage of the full decoding process at which the top 1 predicted tokens first match the ground truth answer tokens. In this study, we only consider samples where the final output contains the ground truth answer. For low confidence remasking, we set Answer length at 256 and Block length at 32 for GSM8K, and Answer length at 128 and Block length to 128 for MMLU. For random remasking, we set Answer length at 256 and Block length at 256 for GSM8K, and Answer length at 128 and Block length at 128 for MMLU. (a) w/o suffix prompt (low-confidence remasking) (b) w/ suffix prompt (low-confidence remasking) (c) w/o suffix prompt (random remasking) (d) w/ suffix prompt (random remasking) Figure 1: Distribution of early correct answer detection during decoding process.. Histograms show when correct answers first emerge during diffusion decoding, measured as percentage of total decoding steps, using LLaDA 8B on GSM8K. Red and orange dashed lines indicate 50% and 70% completion thresholds, with corresponding statistics showing substantial early convergence. Suffix prompting (b,d) dramatically accelerates convergence compared to standard prompting (a,c). This early convergence pattern demonstrates that correct answer tokens stabilize as top-1 candidates well before full decoding. I. high proportion of samples can be correctly decoded during the early phase of decoding. Figure 1(a) demonstrates that when remasking with the low-confidence strategy, 24.2% samples are already correctly predicted in the first half steps, and 7.9% samples can be correctly decoded in"
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "the first 25% steps. These two numbers will be further largely boosted to 97.2% and 88.5%, when shifted to random remasking as shown in Figure 1-(c). II. Our suffix prompt further amplifies the early emergence of correct answers. Adding the suffix prompt Answer: significantly improves early decoding. With low confidence remasking, the proportion of correct samples emerging by the 25% step rises from 7.9% to 59.7%, and by the 50% step from 24.2% to 75.8% (Figure 1-(b)). Similarly, under random remasking, the 25% step proportion increases from 88.5% to 94.6%. III. Decoding dynamics of chain-of-thought tokens. We further examine the decoding dynamics of chain-of-thought tokens in addition to answer tokens, as shown in Figure 2. First, most nonanswer tokens fluctuate frequently before being finalized. Second, answer tokens change far less often and tend to stabilize earlier, remaining unchanged for the rest of the decoding process. (a) w/o suffix prompt (b) w/ suffix prompt Figure 2: Decoding dynamics across all positions based on maximum-probability predictions. Heatmaps track how the top-1 token changes at each position, if it is decoded at the current step, over the course of decoding. (a) Without our suffix prompts, correct answer tokens reach maximum probability at step 119. (b) With our suffix prompts, this occurs earlier at step 88, showing that the model internally identifies correct answers well before the final output. Results are shown for LLaDA 8B solving problem index 700 from GSM8K under low-confidence decoding. Gray indicates positions where the top-1 prediction remains unchanged, orange marks positions where the prediction changes to different token, blue denotes the step at which the corresponding y-axis position is actually decoded, and green box highlights the answer region where the correct answer remains stable as the top-1 token and can be safely decoded without further changes as the decoding process progresses."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "Built upon the above findings, we introduce Prophet, training-free fast decoding algorithm designed to accelerate the generation phase of DLMs. Prophet by committing to all remaining tokens in one shot and predicting answers as soon as the models predictions have stabilized, which we call Early Commit Decoding. Unlike conventional fixed-step decoding, Prophet actively monitors the models certainty at each step to make an informed, on-the-fly decision about when to finalize the generation. Confidence Gap as Convergence Metric. The core mechanism of Prophet is the Confidence Gap, simple yet effective metric for quantifying the models conviction for given token. At any decoding step t, the DLM produces logit matrix Lt RN V, where is the sequence length and is the vocabulary size. For each position i, we identify the highest logit value, L(1) t,i , and the"
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "Figure 3: An illustration of the Prophets early-commit-decoding mechanism. (a) Standard fullstep decoding completes all predefined steps (e.g., 10 steps), incurring redundant computations after the answer has stabilized (at t=6). (b) Prophet dynamically monitors the models confidence (the Confidence Gap). It triggers an early commit decoding as soon as the answer converges, saving significant portion of the decoding steps (in this case, 55%) without compromising the output quality. second-highest, L(2) t,i . The confidence gap gt,i is defined as their difference: This value serves as robust indicator of predictive certainty. large probability gap signals that the prediction has likely converged, with the top-ranked token clearly outweighing all others. gt,i = L(1) t,i L(2) t,i . (4) Early Commit Decoding. The decision of when to terminate the decoding loop can be framed as an optimal stopping problem. At each step, we must balance two competing costs: the computational cost of performing additional refinement iterations versus the risk of error from premature and potentially incorrect decision. The computational cost is function of the remaining steps, while the risk of error is inversely correlated with the models predictive certainty, for which the Confidence Gap serves as robust proxy. Prophet addresses this trade-off with an adaptive strategy that embodies principle of time-varying risk aversion. Let denote = (Tmax t)/Tmax as the decoding progress, where Tmax is the total number of decoding steps, and τ (p) is the threshold for early commit decoding. In the early, noisy stages of decoding (when progress is small), the potential for significant prediction improvement is high. Committing to an answer at this stage carries high risk. Therefore, Prophet acts in risk-averse manner, demanding an exceptionally high threshold (τhigh) to justify an early commit decoding, ensuring such decision is unequivocally safe. As the decoding process matures (as increases), two things happen: the models predictions stabilize, and the potential computational savings from stopping early diminish. Consequently, the cost of performing one more step becomes negligible compared to the benefit of finalizing the answer. Prophet thus becomes more risk-tolerant, requiring progressively smaller threshold (τlow) to confirm convergence. This dynamic risk-aversion policy is instantiated through our staged threshold function, which maps the abstract trade-off between inference speed and generation certainty onto concrete decision rule: gt τ (p), where τ (p) = τhigh τmid τlow if < 0.33 if 0.33 < 0.67 if 0.67 (5) Once the exit condition is satisfied at step t, the iterative loop is terminated. The final output is then constructed in single parallel operation by filling any remaining [MASK] tokens with the argmax of the current logits Lt ."
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "Algorithm Summary. The complete Prophet decoding procedure is outlined in Algorithm 1. The integration of the confidence gap check adds negligible computational overhead to the standard DLM decoding loop. Prophet is model-agnostic, requires no retraining, and can be readily implemented as wrapper around existing DLM inference code. Compute logits: Lt = Mθ(xt) Calculate average confidence gap gt over positions using Eq. 4. Calculate progress: (Tmax t)/Tmax if gt τ (p) then Algorithm 1 Prophet: Early Commit Decoding for Diffusion Language Models 1: Input: Model Mθ, prompt xprompt, max steps Tmax, generation length Ngen 2: Input: Threshold function τ (), answer region positions 3: Initialize sequence xT concat(xprompt, [MASK]Ngen) 4: Let Mt be the set of masked positions at step t. 5: for = Tmax, Tmax 1, . . . , 1 do 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end for 20: Return x0 Determine tokens to unmask Ut Mt via re-masking strategy. ˆx0 argmax(Lt, dim = 1) Update xt1 xt, replacing tokens at positions Ut with those from ˆx0. ˆx0 argmax(Lt, dim = 1) x0 xt. Fill positions in Mt with tokens from ˆx0. Return end if Prophets Early-Commit-Decoding Check Check condition from Eq. 5 Terminate and finalize Standard DLM Refinement Step Return result after full iterations if no early commit decoding"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate Prophet on diffusion language models (DLMs) to validate two key hypotheses: first, that Prophet can preserve the performance of full-budget decoding while using substantially fewer denoising steps; second, that our adaptive approach provides more reliable acceleration than naive static baselines. Through comprehensive experiments across diverse benchmarks, we demonstrate that Prophet achieves significant computational savings with negligible quality degradation. 5.1 EXPERIMENTAL SETUP We conduct experiments on two state-of-the-art diffusion language models: LLaDA-8B (Nie et al., 2025) and Dream-7B (Ye et al., 2025). For each model, we compare three decoding strategies: Full uses the standard diffusion decoding with the complete step budget of Tmax = 50; Half serves as naive baseline that uniformly reduces the budget to Tmax/2 = 25 steps; and Prophet employs early commit decoding with dynamic threshold scheduling. The threshold parameters are set to τhigh = 8.0, τmid = 5.0, and τlow = 3.0, with transitions occurring at 33% and 67% of the decoding progress. These hyperparameters were selected through preliminary validation experiments. Our evaluation spans three capability domains to comprehensively assess Prophets effectiveness. For general reasoning, we use MMLU (Hendrycks et al., 2021), ARC-Challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), TruthfulQA (Lin et al., 2021), WinoGrande (Sakaguchi et al., 2021), and PIQA (Bisk et al., 2020). Mathematical and scientific reasoning are evaluated through GSM8K (Cobbe et al., 2021) and GPQA (Rein et al., 2023), while planning capabilities are assessed using Countdown and Sudoku tasks (Gong et al., 2024). All experiments employ greedy decoding to ensure deterministic and reproducible results. 5.2 MAIN RESULTS AND ANALYSIS The results of our experiments are summarized in Table 1. Across the general reasoning tasks, Prophet demonstrates its ability to match or even exceed the performance of the full baseline. For example, using LLaDA-8B, Prophet achieves 54.0% on MMLU and 83.5% on ARC-C, both of which"
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "Table 1: Benchmark results on LLaDA-8B-Instruct and Dream-7B-Instruct. Sudoku and Countdown are evaluated using 8-shot setting; all other benchmarks use zero-shot evaluation. Detailed configuration is listed in the Appendix. Benchmark LLaDA 8B LLaDA 8B (Ours) Dream-7B Dream-7B (Ours) MMLU ARC-C Hellaswag TruthfulQA WinoGrande PIQA GSM8K GPQA Countdown Sudoku 54.1 83.2 68.7 34.4 73.8 80.9 77.1 25.2 15.3 35.0 General Tasks 54.0 (2.34) 83.5 (1.88) 70.9 (2.14) 46.1 (2.31) 70.5 (1.71) 81.9 (1.98) Mathematics & Science 76.8 (1.69) 25.7 (1.82) Planning Tasks 15.3 (2.67) 38.0 (2.46) 67.6 88.1 81.2 55.6 62.5 86.1 75.3 27. 14.6 89.0 66.1 (2.47) 87.9 (2.61) 81.9 (2.55) 53.2 (1.83) 62.0 (1.45) 86.6 (2.29) 74.9 (1.76) 26.6 (1.66) 14.6 (2.37) 89.0 (3.40) are statistically on par with the full 50-step decoding. Interestingly, on HellaSwag, Prophet (70.9%) not only improves upon the full baseline (68.7%) but also the half baseline (70.5%), suggesting that early commit decoding can prevent the model from corrupting an already correct prediction in later, noisy refinement steps. Similarly, Dream-7B maintains competitive performance across benchmarks, with Prophet achieving 66.1% on MMLU compared to the full models 67.6%a minimal drop of 1.5% while delivering 2.47 speedup. On more complex mathematics and science benchmarks, Prophet continues to prove its reliability. For the GSM8K dataset, Prophet with LLaDA-8B obtains an accuracy of 76.8%, nearly matching the full baselines 77.1% and outperforming the half baselines 76.2%. The advantage of our adaptive approach is particularly evident on the GPQA benchmark. Here, the naive half-step baseline suffers significant performance drop (from 25.2% to 21.2%), whereas Prophet successfully recovers the full models performance, achieving an accuracy of 25.7%. This highlights Prophets role as safe acceleration technique that avoids the pitfalls of premature, static termination. While the planning task results are still being finalized, the consistent trends across reasoning and math suggest that Prophets adaptive strategy will be equally beneficial for tasks requiring structured generation. In summary, our empirical results strongly support the central hypothesis of this work: DLMs often determine the correct answer long before the final decoding step. Prophet successfully capitalizes on this phenomenon by dynamically monitoring the models predictive confidence. It terminates the iterative refinement process as soon as the answer has stabilized, thereby achieving significant computational savings with negligible, and in some cases even positive, impact on task performance. This stands in stark contrast to static truncation methods, which risk cutting off the decoding process prematurely and harming accuracy. Prophet thus provides robust and model-agnostic solution to accelerate DLM inference, enhancing their practicality for real-world deployment."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we identified and leveraged fundamental yet overlooked property of diffusion language models: early answer convergence. Our analysis revealed that up to 99% of instances can be correctly decoded using only half the refinement steps, challenging the necessity of conventional full-length decoding. Building on this observation, we introduced Prophet, training-free early commit decoding paradigm that dynamically monitors confidence gaps to determine optimal termination points. Experiments on LLaDA-8B and Dream-7B demonstrate that Prophet achieves up to 3.4 reduction in decoding steps while maintaining generation quality. By recasting DLM decoding as an optimal stopping problem rather than fixed-budget iteration, our work opens new avenues for efficient DLM inference and suggests that early convergence is core characteristic of how these models internally resolve uncertainty."
        },
        {
            "title": "REFERENCES",
            "content": "Marianne Arriola, Aaron Gokaslan, Justin T. Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models, 2025. URL https://arxiv.org/abs/2503.09573. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, 2020. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Google DeepMind. Gemini-diffusion, 2025. URL https://blog.google/technology/ google-deepmind/gemini-diffusion/. Daniel Gillespie. Approximate accelerated stochastic simulation of chemically reacting systems. The Journal of chemical physics, 115(4):17161733, 2001. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:1245412465, 2021. Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed Abdelfattah, Jae-sun Seo, Zhiru Zhang, and Udit Gupta. Accelerating diffusion language model inference via efficient kv caching and guided diffusion. arXiv preprint arXiv:2505.21467, 2025. Daniel Israel, Guy Van den Broeck, and Aditya Grover. Accelerating diffusion llms via adaptive parallel decoding. arXiv preprint arXiv:2506.00413, 2025a. Daniel Israel, Guy Van den Broeck, and Aditya Grover. Accelerating diffusion llms via adaptive parallel decoding, 2025b. URL https://arxiv.org/abs/2506.00413. Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. Advances in neural information processing systems, 35:2424024253, 2022. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, and Volodymyr Kuleshov. Mercury: Ultra-fast language models based on diffusion, 2025. URL https://arxiv.org/abs/2506.17298."
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025a. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching, 2025b. URL https://arxiv.org/abs/2506.06295. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025a. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models, 2025b. URL https://arxiv.org/abs/2505.15781. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad IEEE transactions on pattern analImage super-resolution via iterative refinement. Norouzi. ysis and machine intelligence, 45(4):47134726, 2022. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. PMLR, 2015. Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, and Xipeng Qiu. Sparse-dllm: Accelerating diffusion llms with dynamic cache eviction, 2025a. URL https://arxiv.org/abs/2508.02558."
        },
        {
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "content": "Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, and Hao Zhou. Seed diffusion: large-scale diffusion language model with high-speed inference, 2025b. URL https://arxiv.org/abs/2508.02193. Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, and Chunhua Shen. Time is feature: Exploiting temporal dynamics in diffusion language models, 2025a. URL https://arxiv.org/abs/2508.09138. Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing, aug 2025b. URL https://arxiv. org/abs/2508.09192. arXiv:2508.09192. Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, and Linfeng Zhang. Accelerating difarXiv preprint fusion large language models with slowfast: The three golden principles. arXiv:2506.10848, 2025a. Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, and Linfeng Zhang. Accelerating diffusion large language models with slowfast sampling: The three golden principles, 2025b. URL https: //arxiv.org/abs/2506.10848. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025a. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding, 2025b. URL https://arxiv.org/abs/2505.22618. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "(a) MMLU w/o suffix prompt (low confidence) (b) MMLU w/ suffix prompt (low confidence) (c) MMLU w/o suffix prompt (random) (d) MMLU w/ suffix prompt (random) Figure 4: Distribution of early correct answer detection during decoding process. Histograms show when correct answers first emerge during diffusion decoding, measured as percentage of total decoding steps, using LLaDA 8B on MMLU. Red and orange dashed lines indicate 50% and 70% completion thresholds, with corresponding statistics showing substantial early convergence. Suffix prompting (b,d) dramatically accelerates convergence compared to standard prompting (a,c). This early convergence pattern demonstrates that correct answer tokens stabilize as top-1 candidates well before full decoding."
        }
    ],
    "affiliations": [
        "Dartmouth College",
        "ELLIS Institute Tubingen",
        "Google DeepMind",
        "Max Planck Institute for Intelligent Systems",
        "Sun Yat-sen University",
        "The Hong Kong Polytechnic University",
        "University of Surrey"
    ]
}