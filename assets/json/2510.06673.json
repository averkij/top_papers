{
    "paper_title": "Heptapod: Language Modeling on Visual Signals",
    "authors": [
        "Yongxin Zhu",
        "Jiawei Chen",
        "Yuanzhe Chen",
        "Zhuo Chen",
        "Dongya Jia",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Yuping Wang",
        "Yuxuan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Heptapod, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs \\textbf{causal attention}, \\textbf{eliminates reliance on CFG}, and \\textbf{eschews the trend of semantic tokenizers}. Our key innovation is \\textit{next 2D distribution prediction}: a causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of $2.70$, significantly outperforming previous causal autoregressive approaches. We hope our work inspires a principled rethinking of language modeling on visual signals and beyond."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 3 7 6 6 0 . 0 1 5 2 : r Heptapod: Language Modeling on Visual Signals Yongxin Zhu1,, Jiawei Chen1, Yuanzhe Chen1, Zhuo Chen1, Dongya Jia1, Jian Cong1, Xiaobin Zhuang1, Yuping Wang1, Yuxuan Wang1 1ByteDance Seed Work done during an internship at ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "We introduce Heptapod1, an image autoregressive model that adheres to the foundational principles of language modeling. Heptapod employs causal attention, eliminates reliance on CFG, and eschews the trend of semantic tokenizers. Our key innovation is next 2D distribution prediction: causal Transformer with reconstruction-focused visual tokenizer, learns to predict the distribution over the entire 2D spatial grid of images at each timestep. This learning objective unifies the sequential modeling of autoregressive framework with the holistic self-supervised learning of masked autoencoding, enabling the model to capture comprehensive image semantics via generative training. On the ImageNet generation benchmark, Heptapod achieves an FID of 2.70, significantly outperforming previous causal autoregressive approaches. We hope our work inspires principled rethinking of language modeling on visual signals and beyond. Date: October 9, 2025 Correspondence: Yongxin Zhu at yongxin.zhu@bytedance.com, Zhuo Chen at zhuo.chen1@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "The emergence of Large Language Models (LLMs) [1, 38, 39] has precipitated paradigm shift in artificial intelligence. Their success is widely attributed to simple yet powerful recipe: scalable Transformer with causal attention [48], an efficient BPE tokenizer for character-level compression [44], and straightforward self-supervised objective of next-token prediction. This formula has catalyzed surge of work that seeks to transplant these principles to other modalities [6, 21, 33, 45, 46, 60]. As illustrated in Fig. 1 (Left), typical visual generation framework mirrors this structure: tokenizer compresses high-dimensional pixels into latent space, and generative model learns the distribution over the resulting representations. However, directly transferring the language modeling paradigm from one-dimensional text to the two-dimensional visual domain has proven challenging, which has prompted many approaches to incorporate what we term external semanticsinformation or guidance not learned from the next-token prediction objectiveto bridge the performance gap. primary manifestation of this dependency is the heavy reliance on Classifier-Free Guidance (CFG) [16], an inference-time technique that refines the generated distribution by bayesian correction. While effective, reliance on this external corrective mechanism not only obscures intrinsic limitations of the models ability in 1Heptapod refers to the alien species in the film Arrival (2016). Their circular logograms encode complete messages holistically and reflect non-linear perception of time. This instantaneous, atemporal writing system parallels the core idea of our framework. 1 Figure 1 (Left) typical visual latent generative framework that incorporates external semantics. (Top Right) Leading autoregressive models exhibit steep performance drops when CFG is disabled (VAR results from Chen et al. [5]). (Bottom Right) Generation quality degrades when the external SSL model DINO is removed (CFG is disabled). learning from visual signals, but also leads to intensity oversaturation and reduced sample diversity. As shown in Fig. 1 (Top Right), the performance of leading visual generative models [35, 45, 46] degrades substantially when CFG is disabled, highlighting dependence on this external crutch. This pattern suggests that the self-sufficient language modeling paradigm has not yet cleanly translated to visual signals. Another popular strategy embeds external semantics directly into the tokenizer, treating it as the linchpin for success [12, 52, 54]. Inspired by semantic tokenizers in speech [13, 21, 22, 32, 33], these approaches learn visual vocabularies by distilling from pretrained self-supervised learning (SSL) models [10, 27, 29, 37, 60]. As demonstrated in Fig. 1 (Bottom Right), incorporating an external SSL model such as DINO [34] can markedly improve image generation. However, we contend that this departs from the principles that underpin LLM success. The BPE tokenizer is semantically agnostic and its sole purpose is faithful data compression. The semantic relationships are not engineered into the tokenizer but rather emerge within the Transformer under the next-token prediction objective. By analogy, constructing semantic tokenizer for text from pretrained embeddings such as GloVe [36] or BERT [8] would run counter to the very principles that made LLMs successful. Moreover, in audio, semantic tokenizers are known to sacrifice fidelity (e.g., acoustic detail) [20, 57]. This suggests dilemma of impossibility triangle between reconstruction, generation, and semantic/SSL representation [41, 50, 52], where optimizing one objective can compromise the others. This introspection leads to the central question of our work: Can we devise visual generative learning paradigm that returns to first principles, where the tokenizer is dedicated solely to faithful reconstruction, and complex semantics emerge implicitly within the Transformer through the next-token prediction objective? Addressing this question requires confronting core ambiguity: the notion of next token is ill-defined in two-dimensional space. Unlike text, which has natural 1D temporal order, images lack an intrinsic sequence. Any patch in 2D grid could be considered next by spatial proximity or semantic relatedness (e.g., two eyes in portrait). To resolve this, we introduce Heptapod, framework that generalizes next-token prediction from 1D sequence to holistic 2D distribution prediction. As illustrated in Fig. 2, our framework consists of standard causal Transformer that ingests visual tokens produced by reconstruction-focused tokenizer (such as VQ-VAE [47] or VAE [19]). Unlike traditional autoregressive models that predict the token at single designated next position, our model is trained to predict, in parallel, the distribution of the token at every subsequent spatial position in the image. This design enables the model to capture complex spatial dependencies and holistic image semantics, rather than relying on hand-crafted order. In this framework, discrete token prediction is handled via linear classification head with cross-entropy loss, whereas continuous 2 Figure 2 Illustration of Heptapods next 2D distribution prediction framework. The model operates on sequence of visual tokens from simple reconstruction-focused tokenizer. The Transformer autoregressively predicts the distributions over remaining positions in the 2D grid in parallel for every input tokens. The loss is then computed across all these future positions, treating the prefix as the visible context (like MAEs unmasked patches) and the remaining grid as targets. This forces the model to develop holistic representation, bridging the gap between 1D language modeling and 2D spatial understanding. token prediction employs diffusion-style head [25] with MSE loss, both viewed as instances of modeling distributions over latent visual representations. By training Transformer with 1D causal attention to predict full 2D distribution, we compel the model to develop holistic understanding of the image. To accurately predict the distribution at any position, the Transformer must encode compact, predictive representation of the images structure and semantics. Furthermore, from self-supervised learning perspective, our objective unifies autoregressive modeling with Masked Autoencoding (MAE) [14]: the causal prefix serves as the unmasked context, while predicting the entire 2D grid in parallel is analogous to reconstructing masked patches. In summary, our contributions are: We challenge the prevailing trend of incorporating external semantics into visual generative models, advocating for paradigm that decouples reconstruction (tokenizer) from semantic learning (Transformer). We introduce next 2D distribution prediction, novel objective that generalizes autoregressive modeling to non-sequential data by reformulating next-token prediction as holistic spatial task. We provide unified perspective that integrates the core principle of MAE-style SSL into the next-token prediction paradigm in causally coherent manner."
        },
        {
            "title": "2.1 Visual Tokenization",
            "content": "Visual tokenization for language modeling generally follows two distinct philosophies. The first is grounded in the principle of reconstruction, aiming to produce compressed yet faithful representation of an image. This line was pioneered by models such as VQ-VAE [47] and VAE [19], with subsequent advances improving perceptual quality and rate-distortion trade-offs. For example, VQGAN [9] introduced perceptual and adversarial losses to specifically enhance reconstruction fidelity. More recent research on VQ-VAE has focused on addressing codebook collapse to further improve reconstruction quality, gradually bridging the gap to continuous VAE [31, 51, 52, 58, 59]. Despite these methods yield powerful tokenizers for image compression and reconstruction, persistent challenge remains: autoregressive models trained on these tokenizers often struggle to learn visual signals, resulting in generative quality heavily dependent on CFG [16]. These limitations motivated second philosophy, inspired by the semantic tokenizer in speech generation [21]. In this vein, recent approaches [24, 27, 29, 37, 50, 60] distill prior knowledge from pre-trained SSL models such as DINO [2, 34] or CLIP [40, 56], either to construct semantic vocabulary or to inject semantic information directly into the generative model. However, this strategy is inherently constrained by the capabilities of external SSL models and often suffers from information loss that impairs reconstruction quality [20]. Moreover, joint optimization of reconstruction and semantic objectives often reveals fundamental tension between these competing goals [50]. In contrast, we sidestep this dilemma. We adopt straightforward reconstruction-based tokenizer but fundamentally alter the learning objective. By introducing next-2D distribution prediction 3 objective, we enable the Transformer to learn visual semantics directly, relieving the tokenizer of this burden and preserving its focus on faithful compression and reconstruction."
        },
        {
            "title": "2.2 Image Language Modeling",
            "content": "The application of autoregressive language modeling to the visual domain began with pioneering works such as iGPT [6], which operated directly on pixels but did not achieve the generative prowess observed in text-based language models. Subsequent work [9, 45, 51, 53] combined VQGAN tokenizers with Transformers, establishing dominant paradigm for visual autoregressive generation. However, when trained with standard next token prediction objective, these models typically require CFG [16] to produce competitive samples, indicating that direct transfer of language modeling methodology from text to visual signals does not fully address the challenge of modeling complex visual distributions. In parallel, alternative approaches have diverged from the classic autoregressive framework. For instance, MAR [25] proposes generalized autoregressive framework but abandons the causal attention mechanism. Similarly, VAR [46] performs coarse-to-fine autoregression but employs non-causal attention within each scale. Both methods depart from the causal structure characteristic of LLMs, which we seek to preserve and extend. Our work charts distinct course. We retain the causal attention mechanism and utilize simple reconstruction-based tokenizer. By reformulating next token prediction as holistic next 2D distribution prediction, we force the causal Transformer to develop global understanding of the visual space. This approach bridges the gap between local sequential processing and holistic spatial generation, adhering to the first principles that underpin successful language modeling."
        },
        {
            "title": "3 On the Nature of Visual Tokens and Their Predictive Modeling",
            "content": "Adapting the language modeling paradigm to the visual domain necessitates foundational choice regarding the nature of visual token and the objective function used for its prediction. Unlike text, where discrete tokens are naturally suited to sequential modeling, visual signals can be represented either as discrete latent codes from VQ-VAE or as continuous latent vectors from VAE. Historically, the choice of visual tokenizer was driven by the superior reconstruction fidelity of VAE. However, recent work [49] has demonstrated that VQ-VAE models with sufficiently large codebooks can match or even surpass VAE, thereby narrowing the distinction between these two tokenization strategies. In this work, we step back from this ongoing debate and argue that from the perspective of language modeling, the distinction between discrete and continuous tokens is mathematically immaterial. The core task of language modeling is to model the distribution of the next token zt, conditioned on the context encoded in the Transformers hidden state ht1. The choice of token simply dictates the methodology used to parameterize and optimize this distribution, but the objective of accurate conditional modeling remains the same. For discrete tokens, the distribution is categorical one over finite vocabulary, which can be directly parameterized with softmax layer and optimized via cross-entropy loss. Minimizing this loss is equivalent to directly maximizing the log-likelihood of the observed token sequence, providing straightforward and powerful learning signal. The modeling of continuous tokens is more complex as their distributions typically defy representation by simple parametric form. To address this, diffusion is adopted as it can model complex non-parametric distributions [25]. The diffusion objective is typically an MSE loss on the noise or the original token, which has been shown equivalent to maximizing lower bound on the log-likelihood of the data [17, 18]. Although the probabilistic objectives align, their distinct parameterizations have significant implications for training dynamics, which our experiments corroborate in Sec. 5.2. The cross-entropy loss provides direct and sharply defined gradient signal such that the model receives unambiguous feedback distinguishing the ground truth token from all other distractors. In contrast, the implicit learning signal provided by the diffusion loss is averaged over continuum of noise levels. While effective for learning smooth densities, it dilutes the gradient signal and potentially slows the convergence."
        },
        {
            "title": "4 Next 2D Distribution Prediction",
            "content": "In the above discussion, we established that whether discrete or continuous visual tokens, the learning objective can be unified as modeling the token distribution. However, the results presented in Fig. 1 show that 4 Figure 3 (Left) Spatial correlations in VQ-VAE vs. semantic tokenizers. For three reference tokens (87th, 138th and 203rd, bounded by red lines), we compute cosine similarity to all other tokens in the grid. (Right) Attention maps of the final layer in autoregressive Transformer trained with each tokenizer. Under VQ-VAE, attention concentrates on spatial neighbors (local interpolation), while semantic tokens yield attention on spatially distant yet semantically related regions (long-range dependencies). Following DiGIT [60], semantic tokens are obtained by K-Means on DINO hidden states. Additional examples are provided in Appendix B. naive next-token prediction objective applied to visual tokens yields unsatisfactory performance. This raises fundamental question: for two-dimensional image, what does next token actually mean? In this section, we analyze the root of this problem and introduce our solution next 2D distribution prediction."
        },
        {
            "title": "4.1 The Curse of Locality in Visual Autoregressive Models",
            "content": "The struggles of language modeling on visual signals stem from fundamental difference between text and images in information density and spatial correlation. In the textual domain, language is highly abstract and compressed. Predicting the next token requires understanding long-range grammatical structures and semantic dependencies beyond local patterns. Consequently, high-level semantics emerge naturally as byproduct of optimizing next-token prediction. Images, by contrast, are highly redundant [14]. As illustrated in Fig. 3, VQ-VAE tokens exhibit strong local correlations that neighboring tokens are overwhelmingly similar. Under teacher forcing with fixed scan order, visual autoregressive model quickly discovers shortcut to excel at local interpolation. It can substantially reduce loss by perfectly predicting adjacent, highly correlated tokens, with little incentive to capture the long-range dependencies that are crucial for global structure but provide only marginal additional loss reduction. Optimization thus gravitates to local minimum that favors textures and low-level detail over holistic semantics. This precisely explains why semantic tokenizers often boost performance. SSL models are typically forced to learn long-range denpendencies via contrastive learning [2] or high-ratio masked prediction [14]. By distilling knowledge from SSL models, semantic tokens, as shown in Fig. 3, pre-package long-range semantic relationships into the tokens themselves. The autoregressive model is then forced to learn these pre-computed global dependencies. The attention maps show that the autoregressive model attents to the spatially distant yet semantically related regions, revealing the long-range dependencies. However, this workaround departs from the first principles of language modeling, where semantics should emerge inside the Transformer from the learning objective rather than be engineered into the tokenizer. This compromise not only hurts reconstruction fidelity [20, 50] but also limits the models ability to learn knowledge beyond that of the pretrained SSL model. Our work aims to break this impasse to design learning paradigm that retains the fidelity of reconstruction-based tokenizer while intrinsically compelling the Transformer to learn global semantics. Motivated by the semantic tokenizer, we redefine next token for visual data to explicitly target long-range dependencies. At each timestep, rather than predicting single next token at specified location, the model predicts the distribution over the tokens at all positions in the 2D spatial grid given the current causal prefix, eliminating the local interpolation shortcut. To accurately predict spatially nonadjacent patches, the model must infer the global structure and semantics from the visible context. In this new paradigm, understanding global semantics is no longer an option, but necessity for optimizing the objective. 5 Figure 4 (Left) The text language modeling with next 1D distribution prediction. (Middle) Vanilla image language modeling with next 1D distribution prediction. The 2D sptial postion is left-shifted in the input to specify the next target. (Right) Our next 2D distribution prediction. The 2D spatial positions are not shifted. The model must be prepared to predict any future position."
        },
        {
            "title": "4.2 The 2D Prediction Objective",
            "content": "The core mechenism of next 2D distribution prediction is to elevate the autoregressive target from 1D vocabulary to 2D vocabulary (the distribution over the entire image grid), while fully preserving the Transformers 1D causal attention mechanism. To understand this, we compare this paradigm shift in Fig. 4. In text, the notion of next is defined by the 1D temporal order. The model receives the token xt and its 1D time position Tt at the timestep and its task is to predict the token distribution for the timestep + 1. The model does not need to explicitly decide which position to predict, as it is always the subsequent one. For images, the next token could belong to any of the remaining spatial positions. To resolve this, vanilla image autoregressive models must explicitly be told which spatial position to predict next by left-shifting the 2D position sequence. This forces the 1D causal attention to reason on the 2D spatial space explicitly. Instead, our next 2D distribution prediction framework treats the current token and its own 2D spatial position as one token from 2D vocabulary. The model is given no information about which spatial position it should predict next so that it must be prepared to predict any of them. From language modeling perspective, next 2D distribution prediction expands the vocabulary from 1D token space to 2D (position, token) space. yet the underlying Transformer remains standard 1D causal model. This design offloads the complexity of modeling 2D space from the causal Transformer to specialized prediction head, encouraging the model to learn global semantics and long-range dependencies without altering its causal nature. During inference, the model predicts distribution over the 2D grid at each step, then samples (position, token) pair to serve as the input for the next timestep util the grid is filled."
        },
        {
            "title": "4.3 Architectures for 2D Prediction Head",
            "content": "The core of next 2D distribution prediction framework is prediction head that maps the 1D causal Transformers output to full 2D spatial distribution. We explore two architectural variants that trade off modeling scope and computational cost, as illustrated in Fig. 5. 6 Figure 5 Architectural variants of the 2D prediction head. (Left) The global prediction head employs bidirectional transformer to model full-image spatial dependencies. (Right) The local prediction head stacks cross attention and bidirectional attention to capture 2D spatial dependencies within local chunk. The first variant, depicted in the left of Fig. 5, models global spatial dependencies across the entire image grid. At timestep t, the hidden states from the causal Transformer h1:t represent the unmasked portion of the image. To predict the complete 2D grid, we combine h1:t with set of learnable mask tokens, each augmented with learnable spatial positional embedding corresponding to distinct masked position (t + 1 to ). The concatenated sequence is then processed by bi-directional Transformer, allowing every position to attend to every other. The model is enforced to explicitly reason about global spatial relationships and infer the content of the masked regions based on the visible context. The loss is computed only on outputs associated with masked positions, forcing the model to perform holistic prediction of the unseen future from the causal past. While this global head captures long-range dependencies effectively, its computational cost scales with the grid size. To improve efficiency, we design second variant that operates on local regions, as shown in the right of Fig. 5. We partition the image into non-overlapping chunks. At each timestep, the prediction head focuses only on the chunk containing the next token to be predicted. The head interleaves cross-attention and bidirectional attention layers. Learnable queries, corresponding to positions within the target chunk, first attend to the historical context from the causal Transformers hidden states via cross-attention. Subsequently, bidirectional self-attention layers operate exclusively within the chunk, modeling local spatial dependencies in that specific region. This design reduces the computational complexity from the scale of the entire image to that of single chunk. Both architectures, whether global or local, implement our frameworks core principle: single 1D causal step triggers comprehensive 2D spatial prediction. This mechanism compels the model to develop holistic understanding of visual data within standard autoregressive training process."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "We conduct autoregressive image generation experiments on ImageNet-1K [7] dataset at resolution of 256 256. We report FID [15] and IS [43] as metrics. To assess intrinsic generative capability and ensure fair comparison, we disable CFG in all experiments. We evaluate our framework with both discrete and continuous visual tokenizers as discussed in Sec. 3. For discrete tokenization, we use the VQGAN tokenizer from LlamaGen [45], which attains rFID of 2.19. For continuous tokenization, we use the VAE tokenizer from MAR [25], which achieves rFID of 1.43. Both tokenizers compress 256 256 image into 16 16 latent grid, yielding sequence of 256 tokens. For continuous tokens, we follow MAR [25] and adopt diffusion-style MLP layer. 7 Figure 6 Training convergence for Heptapod-L with discrete VQ tokenizer vs. continuous VAE tokenizer. The VQbased model converges faster initially, while the VAE-based model ultimately attains superior generative performance. The dotted line indicates each tokenizers rFID. Unless otherwise stated, we follow MAR [25] and match the size of the prediction head to the causal Transformer, including hidden width, number of layers, number of attention heads, and the MLP expansion ratio. The local prediction head contains one additional attention block per layer compared to the global variant. We provide the model configurations and ablations on the prediction head in Appendix A. All models are trained for 800 epochs on the ImageNet training split with batch size of 2048 and learning rate of 8e 4, using AdamW with β1 = 0.9 and β2 = 0.95."
        },
        {
            "title": "5.2 Convergence Efficiency with Different Tokenizers",
            "content": "As discussed in Sec. 3, our framework can accommodate both discrete and continuous visual tokens, as both ultimately aim to maximize the data log-likelihood. To empirically investigate the practical implications of this choice, we train two models of identical scale (Heptapod-L) using the next 2D distribution prediction objective. The first model utilizes discrete VQ tokenizer with cross-entropy loss to learn the distribution, while the second employs continuous VAE tokenizer paired with diffusion MLP head [25]. The results reveal trade-off between convergence speed and final generative quality. As shown in Fig. 6, the training dynamics of the two models differ significantly. The model trained with the discrete VQ tokenizer exhibits markedly faster and smoother initial convergence, achieving lower FID and higher IS in early epochs. This supports the hypothesis that cross-entropy provide more efficient optimization signal. However, the performance of the VAE-based model improves dramatically after 150 epochs, ultimately surpassing the VQ-based model. This eventual superiority aligns with the underlying reconstruction fidelity of the tokenizer itself. Notably, the final performance gap in generative quality between the two models closely mirrors this gap in their reconstruction capabilities. The results lead to two main conclusions. First, our experiments validate that reconstruction-focused tokenizers, whether discrete or continuous, provide viable foundation for language modeling with next 2D distribution prediction. Both approaches enable the model to converge effectively and achieve strong generative results. However, we observe that the reconstruction quality of the tokenizer sets practical upper bound on the final generative performance of the autoregressive model. Given its superior performance upon full convergence, we adopt the VAE tokenizer as our default choice for subsequent experiments. Second, the choice of tokenizer and its corresponding loss function has significant impact on training efficiency, which may become factor when scaling up the model. This analysis raises an intriguing possibility: discrete VQ tokenizer, if its reconstruction quality were improved to match or exceed that of the VAE, could potentially offer the best of both worlds, combining the rapid convergence of discrete optimization with generative performance of continuous VAE. 8 Figure 7 Effect of window size and supervision density on generation performance for Heptapod-B."
        },
        {
            "title": "5.3 Properties of Next 2D Distribution Prediction",
            "content": "To characterize the properties of the next 2D distribution prediction objective, we conduct an ablation on the prediction head design. Table 1 shows that randomizing the generation order in standard 1D next-token paradigm (1D-random) improves over fixed raster scan (1D-raster), but Heptapod (2D-random) delivers substantially larger gains. This suggests that explicitly predicting distributions over the 2D images is more impactful than the 1D next-token prediction on visual signals. As described in Sec. 4.3, the prediction head can be configured to model the token distribution over different spatial extents. We further analyze the interplay between the size of prediction target (\"window size\") and the supervision density (number of tokens per sequence used for loss computation). We directly compares the two architectural variants of the prediction head. The global variant corresponds to window size of 256, where the bidirectional Transformer head predicts the entire image grid at each step. The chunk-based variant corresponds to smaller window sizes (e.g., 16, 64), where the cross-attention head predicts localized region. To ensure fair comparison of architectural trade-offs, we keep the total computational budget for supervision approximately constant, approximated by the product of window size and supervision density n. Table 1 Impact of distribution sampler (1D vs. 2D) and generation order. large size model is trained for 400 epochs with CFG disabled. 1D-raster predicts single next token in fixed raster scan. 1D-random predicts single next token at uniformly random spatial positions. 2D-random (Heptapod) predicts distributions over all spatial positions and samples (position, token) pair each step. Sampler & Order 1D-raster 1D-random 2D-random (Heptapod) FID 19.23 13.07 3. IS 62.3 91.4 185.3 Figure 7 presents the FID and IS curves for five configurations. The results show clear trend: models employing larger prediction window consistently outperform those with smaller window. The global variant (w256n1), which predicts the entire 256-token grid, achieves the best performance. In contrast, chunk-based models with smaller window size (e.g., w16n16, w64n4) yield worse performance, even when the supervision density (n) is increased to maintain similar computational budget. While increasing within moderately sized window (w16n16 vs. w16n32) can accelerate convergence, it does not surpass the full-window baseline and converges to similar final results. These findings lead to crucial conclusion: the spatial extent of the prediction target is more critical than the supervision density to learn long-term semantics. By compelling the model to predict distributions over the entire spatial context, the prediction head forces the model to capture long-range dependencies and holistic semantics. Restricting the prediction to smaller localized chunks, even with denser supervision, limits the global understanding and hinders the final generative quality. Based on this analysis, we adopt the 9 Table 2 ImageNet 256 256 class-conditional generation without CFG. NAR: non-autoregressive masked prediction. : uses an external SSL model. : results copied from Chen et al. [5]. Part for VAR indicates bidirectional attention within each scale (i.e., partially non-causal). Type Causal Attn. Model #Params FID IS Diffusion No NAR No VAR Part MAR No AR Yes AR Yes LDM-4 [42] DiT-XL [35] SiT-XL [30] REPA [55] MAETok [4] LightningDiT [50] MaskGIT [3] MAGVIT-v2 [52] TiTok [54] VAR-d20 [46] VAR-d24 [46] VAR-d30 [46] MAR-B [25] MAR-L [25] MAR-H [25] RQ-Transformer [23] RQ-Transformer [23] LlamaGen-XL [45] LlamaGen-XXL [45] LlamaGen-3B [45] DiGIT [60] Heptapod-B (Ours) Heptapod-L (Ours) Heptapod-H (Ours) 400M 675M 675M 675M 675M 675M 227M 307M 287M 600M 1.0B 2.0B 208M 479M 943M 1.4B 3.8B 775M 1.4B 3B 732M 208M 478M 941M 10.56 9.62 8.30 5.90 2.31 2. 6.18 3.65 4.44 8.48 6.20 5.26 3.48 2.60 2.35 8.71 7.55 15.55 14.65 9.38 3.39 5.85 3.62 2.70 103.5 121.5 131.7 157.8 216.5 205. 182.1 200.5 168.2 129.5 154.3 175.6 192.4 221.4 227.8 119.0 134.0 79.2 86.3 112.9 205.96 149.6 190.8 229.8 global prediction head as the default for subsequent experiments. This ablation highlights the importance of maximizing the window size in the prediction objective and supports our core hypothesis that holistic prediction task is key to effective visual language modeling."
        },
        {
            "title": "5.4 Benchmarking with Previous Generative Models\nWe evaluate Heptapod against various generative models on ImageNet 256 × 256 conditional generation\nbenchmark and the results are provided in Table 2. We train Heptapod with the w256n1 setting as described\nin Sec. 5.3. We provide generated examples in Fig. 9.",
            "content": "A central goal of our work is to revitalize the causal autoregressive paradigm for vision. Compared to previous causal autoregressive models such as LlamaGen [45], Heptapod demonstrates dramatic improvement. Heptapod-H achieves an FID of 2.70 and an IS of 229.8, decisively outperforming LlamaGen-3B (FID 9.38, IS 112.9) while using less than one-third of the parameters. These improvements underscore the efficacy of next 2D distribution prediction framework. By reformulating the learning objective, we enable standard causal Transformer to learn rich visual semantics implicitly, task where prior autoregressive models have struggled. Unlike many leading models (e.g., LightningDiT [50], MAETok [4], DiGIT [60]) that rely on external pretrained SSL model to inject semantics into tokenizer, Heptapod uses sole reconstruction-focused tokenizer. Moreover, all reported results are obtained without CFG, avoiding reliance on inference-time heuristics and highlighting intrinsic generative capability. This demonstrates that visual semantics can emerge from well-posed generative objective, rather than being engineered into the tokenization. While MAR achieves better results through bidirectional attention, Heptapod with causal attention attains competitive results that approach MAR. The causal attention inherently operates at lower computational cost compared to 10 Figure 8 Effect of supervision density under global prediction window (w = 256). Increasing the number of tokens per sequence for loss computation (density n) consistently improves FID and IS. Results are obtained with Heptapod-B and CFG is disabled. their bidirectional counterparts. In particular, Heptapods adherence to causal attention ensures that it can be seamlessly integrated into multimodal LLM, preserving the architectural coherence and deployment properties of language models. These results support our central thesis that by returning to first principles and introducing learning objective tailored to visual signals, causal attention model trained without CFG and without semantic tokenizers can achieve strong performance. Heptapod thus offers practical path for integrating visual generative training into the language modeling paradigm."
        },
        {
            "title": "5.5 Scale Supervision Density\nIn Sec. 5.3, we observed that with a small prediction window (e.g., w16), simply increasing the number of\ntokens for loss computation from 16 to 32 (w16n16 vs. w16n32) yielded no improvement. This suggested\nthat when the model’s view is restricted to local context, denser supervision alone is insufficient to improve\nperformance. We revisit this relationship under the global prediction window size of 256, where the model\nis forced to predict the entire image. For example, w256n1 computes the loss on only a single token per\nsequence, whereas w256n8 increases this density eightfold. As shown in Fig. 8, when the model is trained to\npredict the full 2D distribution, increasing supervision density leads to consistent improvements in both FID\nand IS metrics.",
            "content": "This contrast reveals crucial insight: the effectiveness of dense supervision depends on the richness of the semantic signal. When the model is confined to small window size, denser supervision fails to provide meaningful learning signal about global structure, hence negligible gains. With larger window size, the model must account for long-range dependencies and denser supervision provides richer gradients that improve both convergence and final quality. While scaling supervision density markedly accelerates convergence and improves final performance, it also increases the computational cost. Therefore, developing methods to enhance the training efficiency of our framework remains an important direction for future research."
        },
        {
            "title": "6.1 Connection with Multi-token Prediction",
            "content": "There is conceptual link between our framework and the multi-token prediction (MTP) method in LLMs [11, 28]. MTP enriches the learning signal by training the model to predict several subsequent tokens at each 11 step rather than just the immediate successor, which can mitigate exposure bias and accelerate training. In this context, our next 2D distribution prediction can be viewed as the extension of MTP to the visual domain: predicting the entire remainder of the token grid. This predict future tokens objective is exceptionally suited for images, as it resolves the inherent ambiguity of next token in non-sequential 2D space. The mandate to predict distributions for all future locations necessitates the learning of global structure and semantics, as local interpolation is no longer sufficient strategy. We posit that this shift toward long-range prediction objective is promising direction for language modeling in vision and beyond."
        },
        {
            "title": "6.2 Language Modeling on Acoustic Signals",
            "content": "Our framework, especially the chunk-based prediction head described in Sec. 4.3, suggests promising path toward language modeling on acoustic signals. Unlike images, audio can be extremely long or even unbounded in streaming settings, making the global prediction computationally infeasible. The chunk-based approach, which limits prediction to finite local window, naturally aligns with this characteristic. Although our method is motivated by the emergence of visual semantics under holistic prediction, it is unclear whether the same principle would hold for acoustic signals. Addressing this question is key direction for future work."
        },
        {
            "title": "6.3 Tokenizer in Heptapod",
            "content": "Although our current implementation employs visual tokenizer (such as VQ-VAE or VAE) to compress images, the core principle of our framework is fundamentally agnostic to the form of the visual tokens. The prediction head simply models distributions over the 2D spatial positions, regardless of whether those positions correspond to discrete tokens or continuous patches. Recent advances, such as Fractal Generative Models [26], have demonstrated that it is possible to achieve high-fidelity image generation directly in pixel space through hierarchical modeling techniques, bypassing the need for tokenization in Fig. 1."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we revisited the growing reliance on externally engineered semantics in visual language modeling and advocated for return to the first principles that have driven the success of LLMs. By decoupling reconstruction from semantic learning and introducing the next 2D distribution prediction, we presented novel framework that extends autoregressive modeling beyond the constraints of sequential data. Our approach encourages the model to develop holistic understanding of images by predicting the 2D distribution, effectively bridging the strengths of both autoregressive generation and masked autoencoding paradigms. This perspective not only resolves the inherent ambiguities of next-token prediction in the visual domain, but also lays foundation for unified models that integrate generative and understanding capabilities across modalities."
        },
        {
            "title": "References",
            "content": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 96509660, October 2021. [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1131511325, June 2022. [4] Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=dzwUOiBlQW. [5] Huayu Chen, Hang Su, Peize Sun, and Jun Zhu. Toward guidance-free AR visual generation via condition contrastive alignment. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=kGvXIlIVLM. [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 16911703. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/chen20s.html. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. doi: 10.1109/CVPR.2009.5206848. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423/. [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, June 2021. [10] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. [11] Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Roziere, David Lopez-Paz, and Gabriel Synnaeve. Better and faster large language models via multi-token prediction. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1570615734. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/gloeckle24a.html. [12] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1573315744, June 2025. [13] Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis CONNEAU, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, Roy Schwartz, and Yossi Adi. Textually pretrained speech language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 6348363501. Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ Curran Associates, c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf. 13 [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern are scalable vision learners. Recognition (CVPR), pages 1600016009, June 2022. [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf. [16] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https://openreview.net/forum?id=qw8AKxfYbI. [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 68406851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf. [18] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 2169621707. Curran Associates, Inc., 2021. URL https://proceedings. neurips.cc/paper_files/paper/2021/file/b578f2a52a0229873fefc2a4b06377fa-Paper.pdf. [19] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [20] Felix Kreuk, Adam Polyak, Jade Copet, Eugene Kharitonov, Tu Anh Nguyen, Morgan Rivière, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, and Yossi Adi. Textless speech emotion conversion using discrete & decomposed representations. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1120011214, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.769. URL https://aclanthology.org/2022.emnlp-main.769/. [21] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, and Emmanuel Dupoux. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:13361354, 2021. doi: 10.1162/tacl_a_00430. URL https://aclanthology.org/2021.tacl-1.79. [22] Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, and Wei-Ning Hsu. Textless speech-to-speech translation on real data. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 860872, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.63. URL https://aclanthology.org/2022. naacl-main.63. [23] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1152311532, June 2022. [24] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. [25] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 56424 56445. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 66e226469f20625aaebddbe47f0ca997-Paper-Conference.pdf. [26] Tianhong Li, Qinyi Sun, Lijie Fan, and Kaiming He. Fractal generative models. arXiv preprint arXiv:2502.17437, 2025. [27] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=QE1LFzXQPL. 14 [28] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [29] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [30] Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision ECCV 2024, pages 2340, Cham, 2024. Springer Nature Switzerland. ISBN 978-3-031-72980-5. [31] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQVAE made simple. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=8ishA3LxN8. [32] Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoît Sagot, Abdelrahman Mohamed, and Emmanuel Dupoux. Generative spoken dialogue language modeling. Transactions of the Association for Computational Linguistics, 11:250266, 2023. doi: 10.1162/tacl_a_00545. URL https://aclanthology.org/2023.tacl-1.15/. [33] Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Mary Williamson, Gabriel Synnaeve, Juan Pino, Benoît Sagot, and Emmanuel Dupoux. SpiRit-LM: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052, 2025. doi: 10.1162/tacl_a_00728. URL https://aclanthology.org/2025.tacl-1.2/. [34] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. Featured Certification. [35] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205, October 2023. [36] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans, editors, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 15321543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL https://aclanthology.org/D14-1162/. [37] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 25452555, June 2025. [38] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. [39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/radford21a.html. [41] Vivek Ramanujan, Kushal Tirumala, Armen Aghajanyan, Luke Zettlemoyer, and Ali Farhadi. When worse is better: Navigating the compression-generation tradeoff in visual tokenization. arXiv preprint arXiv:2412.16326, 2024. [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. 15 [43] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings. neurips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf. [44] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Katrin Erk and Noah A. Smith, editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162/. [45] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [46] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8483984865. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/ file/9a24e284b187f662681440ba15c416fb-Paper-Conference.pdf. [47] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https: //proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf. [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, and Illia Polosukhin. Attention is all you need. R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/ file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [49] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=NYe2JuN3v3. Featured Certification, Reproducibility Certification. [50] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 1570315712, June 2025. [51] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=pfNyExj7z2. [52] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=gzqrANCF4g. [53] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. [54] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 128940128966. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/ 2024/file/e91bf7dfba0477554994c6d64833e9d8-Paper-Conference.pdf. [55] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=DJSZGGZYVi. [56] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. 16 In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1197511986, October 2023. [57] Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=AF9Q8Vip84. [58] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vq-gan to 100,000 with utilization rate of 99%. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 12612 12635. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 1716d022edeac750e57a2986a7135e13-Paper-Conference.pdf. [59] Yongxin Zhu, Bocheng Li, Yifei Xin, Zhihua Xia, and Linli Xu. Addressing representation collapse in vector quantized models with one linear layer. arXiv preprint arXiv:2411.02038, 2024. [60] Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, and Lidong Bing. Stabilize the latent space for image autoregressive modeling: unified perspective. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 2863628661. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/ file/325ce3291a509ddacc1e08f457b4d86c-Paper-Conference.pdf. 17 Figure 9 Example results generated by Heptapod-H. 18 Table 3 Model Configuration for causal Transformer."
        },
        {
            "title": "Depth\nHidden Size\nFFN Dim\nAttention Heads\nHead Dim",
            "content": "Base 12 768 3072 12 64 Large Huge 16 1024 4096 16 64 20 1280 5120 16 80 Table 4 Ablation study on the configuration of prediction head for Heptapod-L with w256n1. We vary the depth of layers between the causal Transformer and the prediction head while keeping the total fixed at 32 (e.g., 16/16, 24/8, 31/1). Depth (Causal Transformer / Prediction Head) 31 / 1 24 / 8 16 / FID 14.74 4.88 3.62 IS 98.15 158.93 190."
        },
        {
            "title": "A Model Configuration",
            "content": "Table 3 summarizes the three model scales for causal Transformer (Base, Large, Huge) used throughout our experiments. For continuous tokenizer, following MAR [25], we configure the diffusion-style denoising MLP head with {6, 8, 12} blocks and widths {1024, 1280, 1536} for the Base, Large, and Huge models, respectively. Table 4 investigates how to allocate depth between the causal Transformer and the 2D prediction head for Heptapod-L. balanced split (16/16) achieves the best FID and IS. Shifting layers from the head to the backbone (24/8) degrades performance, and an extreme allocation (31/1) leads to sharp drop in both FID and IS. These results indicate that the prediction head must retain sufficient depth to project the 1D causal context into 2D spatial distributions and to model position-wise interactions at scale."
        },
        {
            "title": "B Visualization of Tokenizer and Attention",
            "content": "19 Figure 10 Visualization of semantic tokens with cosine similarity. 20 Figure 11 Visualization of VQ-VAE tokens with cosine similarity. 21 Figure 12 Visualization of attention maps with semantic tokenizer. 22 Figure 13 Visualization of attention maps with VQ-VAE tokenizer."
        }
    ],
    "affiliations": [
        "ByteDance Seed"
    ]
}