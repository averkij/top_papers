{
    "paper_title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
    "authors": [
        "Zhenya Yang",
        "Zhe Liu",
        "Yuxiang Lu",
        "Liping Hou",
        "Chenxuan Miao",
        "Siyi Peng",
        "Bailan Feng",
        "Xiang Bai",
        "Hengshuang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation."
        },
        {
            "title": "Start",
            "content": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation Zhenya Yang1, Zhe Liu1, Yuxiang Lu1, Liping Hou2, Chenxuan Miao1, Siyi Peng2, Bailan Feng2, Xiang Bai3, Hengshuang Zhao1(cid:66) 1The University of Hong Kong, 2 Huawei Noahs Ark Lab, 3Huazhong University of Science and Technology 5 2 0 2 4 1 ] . [ 1 1 5 7 2 1 . 2 1 5 2 : r Figure 1. (a) Overview of our GenieDrive. It predicts physically accurate future occupancy given the initial state and driving controls, and renders the occupancy into video, enabling physics-aware multi-view driving video generation. (b) and (c) Performance of 4D occupancy forecasting and video generation. GenieDrive achieves the highest occupancy forecasting accuracy using the fewest parameters (bubble size denotes model size) and facilitates 8 longer multi-view driving video generation with notably enhanced generation quality."
        },
        {
            "title": "Abstract",
            "content": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closedloop evaluation. However, existing methods often rely on single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose VAE that encodes occupancy into latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model Project leader (cid:66)Corresponding author the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 parameters. Additionally, Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multiview consistent, and physics-aware driving video generation. Further materials and visualizations can be found on the project webpage. 1. Introduction is crucial for auPhysics-aware driving world model tonomous driving, as it enables the simulation of various potential futures based on different driving actions, facilitating driving planning [10, 54, 62, 67, 71], long-tailed driving data synthesis [43, 51, 60, 72], and closed-loop evaluation [12, 25, 61]. Recent advances in interactive world models, such as Genie3 [1] and others [18, 37, 44, 65], have significantly pushed the boundaries of visual and physical realism in general world models, sparking interest in developing an interactive and physics-aware driving world model to generate visually and physically realistic driving videos. Existing video-based driving world models [1416, 23, 67] heavily rely on video diffusion models [5, 20, 38, 50, 63] that function as black box models, taking conditions such as driving actions as input and producing videos as output. These methods establish their understanding on driving scenarios only by learning the denoising process [19, 33, 34] on driving video data. However, due to the lack of physical modeling and constraints, these methods are easily biased by video data distribution, preventing them from establishing physics-aware world model and leading to incorrect predictions or generation of the future. For example, nearly all existing public driving video datasets [6, 26] contain significant portion of videos where the ego car only goes straight. Models trained on these datasets may develop biased world model, leading the ego car to favor going straight. Consequently, when we command the model to turn right, it may still generate videos where the car goes straight. We argue that this limitation is primarily due to the black box design, which tends to overfit the training data rather than genuinely understanding the 4D representation of driving scenes and the physical relationships between conditions and videos. To address this limitation, we propose GenieDrive, two-stage driving world model that introduces 4D occupancy as an intermediate representation. It acts as physical constraint to ensure accurate 4D modeling and provides physics prior to guide subsequent video generation. GenieDrive consists of lightweight occupancy world model for 4D occupancy generation and an enhanced video generator that transforms 4D occupancy into physicsaware multi-view videos. 4D occupancy provides highresolution 3D scene layouts and dynamic evolution, making it an effective representation of driving scenarios. However, achieving both accurate and efficient compression of 4D occupancy remains challenging for existing methods [17, 32, 52]. To break this trade-off, we introduce triplane VAE that compresses occupancy into highly compact latent representation, using only 58% of the latent size in previous methods while achieving superior reconstruction performance. Additionally, we propose Mutual Control Attention (MCA) to accurately model the influence of driving controls on 4D scene evolution, and we train the occupancy world model end-to-end to better align the VAE representation with the forecasting task. For our video generator, we incorporate our proposed Multi-View Attention (MVA) into pretrained video generation model to enable multi-view driving video generation with guidance from our 4D occupancy. normalization strategy is further introduced to align the output of the newly integrated MVA with the pretrained model, ensuring stable and efficient finetuning. With these designs, we achieve accurate and interactive control over physics-informed 4D occupancy generation, enabling physics-aware multi-view driving video synthesis. Our contributions can be summarized as follows: We propose GenieDrive, driving world model that enables highly controllable, multi-view consistent, and physics-aware long driving video generation. We introduce tri-plane VAE to effectively compress high-resolution occupancy, and an occupancy world model with Mutual Attention and end-to-end training to achieve efficient and accurate 4D occupancy forecasting. We design Normalized Multi-View Attention module and integrate it into the pretrained video diffusion model, enabling it to learn multi-view relations in driving scenarios stably and efficiently. Experiments demonstrate that our occupancy world model improves forecasting mIoU by 7.20%, runs at 41FPS, and uses only 3.4M parameters. Our video generator further excels in controllable multi-view driving video generation, achieving 20.7% reduction in FVD. 2. Related Work 2.1. Video-Based Driving World Model With the advancement of video diffusion models, series of works in autonomous driving have sought to leverage this powerful tool for generating driving videos. Early efforts [28, 53, 70] were limited to short, single-view videos, while recent methods have achieved the generation of long sequences and high-resolution driving videos [15, 16, 21, 23, 40, 67]. Some works have also extended single-view generation to multi-view generation [14, 15, 40, 68]. However, these methods often require substantial computational resources. Additionally, relying on this black box design to directly map conditions to videos can easily give rise to bias due to unbalanced driving video data, resulting in physically inconsistent predictions. In this paper, we introduce an intermediate representation that serves as physical constraint for driving scenes and provides physics prior for generation, facilitating more efficient model training and enabling physics-aware driving video generation. 2.2. Occupancy-Based Driving World Model The occupancy world model predicts future occupancy based on historical data and driving actions, with existing methods categorized as diffusion-based and autoregressivebased. Diffusion-based methods, such as OccSora [52], DynamicCity [4], DOME [17], and COME [41], typically Figure 2. Overall framework of GenieDrive. Our GenieDrive adopts two-stage generation pipeline that first predicts future occupancy and then generates multi-view driving videos. In the occupancy generation stage, the current occupancy is encoded using tri-plane VAE and processed by our Mutual Control Attention (MCA). The predicted occupancy is rendered into multi-view semantic maps, which are then fed into the DiT blocks enhanced by our Normalized Multi-View Attention (MVA) module to produce the final driving videos. combine continuous VAE and DiT [38], using DDPM [19] for training and DDIM [42] for accelerated inference. In contrast, autoregressive methods like OccWorld [69], OccLLM [59], OccLlama [55], and 2-World [32] utilize VQVAE [48] to encode occupancy into discrete tokens, predicting future tokens with causal transformers or LLMs [46]. While diffusion methods require more computational resources for training, autoregressive methods face challenges due to lossy discrete representations, necessitating complex model designs to mitigate this loss. Our approach combines the feature-preserving ability of VAE with the low training costs of autoregressive modules, achieving SOTA performance in occ forecasting without unnecessary complexities. 2.3. Occupancy-Guided Video Generation Although several previous works [30, 35, 36] also utilize occupancy representation to guide driving video generation, our method is fundamentally different from theirs. UniScene [30] and InfiniCube [36] require BEV maps as input for occupancy generation, functioning more like translators. In contrast, our occupancy is generated using the proposed occupancy world model, which relies solely on historical observations and given driving actions. While WoVoGen [35] can predict future occupancy, it is limited to very short sequences of just 6 video frames. Our occupancy world model, on the other hand, supports highaccuracy generation of long occupancy sequences, enabling the generation of up to 241 video frames. 3. Methods As shown in Figure 2, our GenieDrive operates in two-stage generation process. We first use the proposed lightweight occupancy world model (Section 3.1) to predict future occupancy, which then serves as physical guidance for generating physics-aware driving videos (Section 3.2). 3.1. Lightweight Occupancy World Model The key objective of our occupancy world model is to generate accurately controlled future occupancy for the subsequent video generation, without introducing excessive computational overhead. To achieve this goal, we propose compact latent representation, precise driving control modeling, and an improved training strategy. Compact Latent Tri-plane Representation. Effectively compressing occupancy can significantly enhance the efficiency of subsequent 4D occupancy generation. Observing considerable redundancy in occupancy and inspired by low-rank decomposition techniques [7, 8, 22], we propose VAE that compresses occupancy into tri-plane [7], greatly reducing redundancy while preserving the most important features. Given an occupancy input RHW D, we first downsample it to volume feature RhwdC using 3D convolution filter gϕ, where is the channel number. Then we utilize three transformers to effectively compress it to latent tri-plane, which consists of three latent planes including Zyz RwdC, Zxz RhdC and Zxy RhwC. To get each latent plane, we project the occupancy feature on X, Y, axes respectively. The implementation of our projection operation is inspired by the [CLS] token in BERT [11]. For example, to obtain the Zxy plane, we first rearrange the occupancy feature as follows: = rearrange(S, C (h w) C), (1) Next, we concatenate learnable token Pxy RC to the rearranged feature, resulting in = cat(Pxy, S), where R(h w)(d+1)C. We then perform self-attention through transformer: Zxy = Fxy(S), treating the output learnable token as the projected result. Similarly, we have learnable tokens Pyz and Pxz, along with transformers Fyz and Fxz to obtain the corresponding projected latent planes Zyz and Zxz. To recover the occupancy from latent planes, we perform the decoding: ˆO = fψ(Zxy Zyz Zxz + PE(x, y, z)), (2) where we use the Hadamard product to obtain feature volume of shape RhwdC from the latent tri-plane. The positional encoding from the learnable module PE is added to the recovered feature volume. This is then passed through fψ, an upsampling 3D convolution filter, to produce the reconstructed occupancy ˆO RHW DC. Using the ground-truth occupancy and the reconstructed occupancy ˆO, our VAE is trained with the following loss function, which consists of cross-entropy loss, Lovasz-softmax loss [3], and KL-divergence loss: LVAE = LCE(O, ˆO) + LLov(O, ˆO) + LKL(Z, (0, I)). (3) Through this self-supervised training, the VAE learns to preserve the most important features in compact latent tri-plane, which serves as an effective and efficient representation for subsequent occupancy generation. Next Occupancy Prediction. We generate future occupancy autoregressively. Given the past occupancy, we aim to predict the occupancy at the next timestep based on driving controls, which can be formularized as follows: ˆZt+1 = Fpred(Zt, c, [Zt1, . . . , Ztk]), (4) where Zt = cat(Zxy,t, Zyz,t, Zxz,t) is the latent tri-plane at timestep t, is the control signal in driving scenes such as command and trajectory, and is the window size of the historical occupancy. The autoregressive network Fpred is modeled using transformer as shown in Figure 2. To adequately model the interaction between occupancy and driving controls, we propose the following Mutual Control Attention (MCA): where and cl denote the latent representations of the occupancy and the control at layer l. We utilize the intermediate transformation supervision introduced in [32], which employs MLP head ftrans to decode the latent control signal from an intermediate transformer layer into transformation matrix, supervising it with the ground truth transformation matrix . This can be formulated as follows: Lreg = t+1 , ftrans(cm )2, (8) where is the intermediate layer at which we apply the supervision. We then fuse the latent tri-plane with the control latent cm using cross-attention and pass the result to the subsequent spatialtemporal transformer blocks ST [58]. The final output is supervised using the latent representation of the ground-truth future occupancy. The complete training objective is as follows: (cid:88) Lpred = βtZt+1, ST(Attn(Z , cm , cm ))2 + λLreg, t=0 (9) where λ is hyperparameter that balances the strength of the intermediate supervision, βt is the weight for the occupancy predicted at each timestep, and is the number of occupancy frames to be forecasted. End-to-End Training for Representation Alignment. All existing works on 4D driving occupancy generation adopt two-stage training paradigm: they first train VAE or VQVAE with reconstruction objective to encode occupancy into latent representation, and then perform diffusion or autoregressive prediction in the learned latent space. We argue that the representation learned from reconstruction may not be optimal for subsequent generation [29, 64]. To mitigate the misalignment between the representation learned from reconstruction and the subsequent forecasting task, we propose to train our tri-plane VAE and next-occupancy prediction module end-to-end using the following objective: (cid:88) LE2E = βtOt+1, fθ(Fpred(F{xy,yz,xz}gϕ(Ot), c))2, t= (10) where F{xy,yz,xz} gϕ, Fpred, and fψ denote the VAE encoder, the prediction module, and the VAE decoder, respectively. The regularization loss Lreg is still applied during end-to-end training. Although end-to-end training may seem straightforward, it does not perform well in previous methods, as observed in our experiments. We will provide detailed analysis of this in the experimental section. 3.2. Physics-Aware Driving Video Generation = + Attn(QZl , Kcl, Vcl ), l+1 = + Attn(QZl , KZl , VZl ), cl+1 = cl + Attn(Qcl , KZl+1 , VZl+1 ), (5) (6) (7) Based on the 4D occupancy generated by our lightweight occupancy world model, we leverage its spatial and temporal physical information to guide driving video generation, yielding physics-aware outputs. To achieve this, we Figure 3. Comparison of Trajectory-Controlled Driving Video Generation. Our method can generate physics-aware future frames for the trajectories Turn Left, Go Straight, and Turn Right. In contrast, Vista [16] and Epona [67] struggle with Turn Left and Turn Right. first project the 4D occupancy into image space using the splatting algorithm [27, 30, 73] to serve as physics condition. We then propose the Normalized Multi-View Attention, which enables stable video generation fine-tuning with physical guidance from the projected 4D occupancy. Occupancy Splatting as Physical Guidance. To adapt the generated 4D occupancy to the pretrained video generation model while preserving its spatial and temporal physical information, we project the occupancy into multi-view image space and render it into semantic maps using splatting algorithms [27, 30, 73] as follows: = argmax( (cid:88) siαi i1 (cid:89) (1 αj)), (11) iN j=1 where is the number of occupancy primitives in 3D space, si is the one-hot embedding of the semantic label for the i-th primitive, and ai is the opacity of the splatted primitive. With these rendered semantic maps as conditions, we finetune flow-based [13, 33, 34] pretrained video generation model with v-prediction loss as follows: Lvideo = Ex0,x1,M,tu(xt, M, t; θ) vt2, (12) where θ denotes the video model parameters, vt is the ground-truth velocity, u(xt, M, t; θ) is the velocity predicted by the model, and xt is the noised sample interpolated between the clean sample x0 and pure noise x1. Normalized Multi-View Attention for Stable FineTuning. Simply fine-tuning the video generation model on driving video data is insufficient. The pretrained model is designed for single-view video generation, while we need to produce consistent multi-view driving videos. Although the rendered semantic maps can provide multi-view consistent guidance, the conditioned video generation remains spatially unaware, as each views video is generated in isolation. Moreover, due to the quadratic complexity of attention [49], simply flattening the time dimension t, feature dimension w, and multi-view into one long sequence to perform self-attention is prohibitively expensive. To mitigate these limitations, we propose an efficient Multi-View Attention (MVA), inspired by the observation that coherence primarily exists in the driving scene at the same height across different views: = rearrange(Z, (t w) (t h) (n w) C), = + SelfAttn(Z), (13) (14) where we perform attention only at the same height across different views. is the number of channels, and is the latent output from the pretrained DiT block. By inserting our proposed MVA block after the DiT block of the video model, the receptive field spans all timesteps, feature-map patches, and views, enabling the modeling of multi-view correlations in driving videos. However, since the newly introduced module is untrained, directly integrating it into the pretrained video model will collapse the learned prior. Inspired by the normalization techniques [2, 24, 39, 57], we propose adding cross normalization to stabilize the finetuning process. Denoting = SelfAttn(Z), the Normalized Multi-View Attention is formulated as follows: = + η (cid:18) µM σM (cid:19) σZ + µZ , (15) where we normalize then rescale it to the distribution of Z. η is hyperparameter that adjusts the strength of the multi-view attention. With this normalization, MVA can be gradually optimized without collapsing the pretrained prior. 4. Experiments We evaluate our method on the NuScenes dataset [6], which contains 700 training scenes and 150 validation scenes. For occupancy, we use the 2 Hz annotations provided by Occ3D [45]. The pretrained video generation model used in our experiments is Wan2.1-1.3B [50]. We generate multi-view driving videos at 12 Hz and evaluate them using FVD [47], mIoU, and mAP, following previous works [9, 15, 31]. All experiments are conducted on server equipped with 8 NVIDIA L40S GPUs (48 GB VRAM each). 4.1. Evaluation on Physics-Aware Video Generation To demonstrate the effectiveness of our GenieDrive, we compare it against two advanced driving world models, Figure 4. Occupancy guided physics-aware multi-view video generation. Given the same initial occupancy and frames, our model can generatively predict the future occupancy based on driving controls. Then we use these generated occupancies to guide the video generation. We display the simulated physics-aware futures for go straight, turn right and turn left. Vista [16] and Epona [67]. To assess the models ability to generate physics-aware feedback for driving actions, we provide the same initial image of the driving scenario and input three different trajectories representing Turn Left, Go Straight, and Turn Right to Vista, Epona, and our method. As shown in Figure 3, all methods effectively handle the Go Straight control, but only our GenieDrive generates physically plausible videos for Turn Left and Turn Right. To be more precise, when the Turn Left trajectory is input, Vista only shows slight tendency to turn left but fails to follow the trajectory effectively, while Epona produces an inconsistent scene. In the case of the Turn Right trajectory, Vista remains stationary and does not move, whereas Epona generates video that goes straight. Only our method produces physically plausible driving videos for all trajectories and maintains consistency in the generated driving scene. It is important to note that both Vista and Epona only support single-view generation, while our GenieDrive inherently supports multi-view generation. We also illustrate the detailed generation process of our method in Figure 4, where we visualize the generated occupancy driven by control inputs in the first stage and the resulting multi-view videos in the second stage. As shown in the figure, our occupancy world model predicts physically accurate future occupancy from the driving controls, while our enhanced video generation model precisely transforms the generated occupancy into multi-view and temporally consistent driving videos. The 4D occupancy in our method serves as physical constraint that enforces accuracy in the 4D space, ensuring physics-aware generation when projected into the lower-dimensional video space. 4.2. Evaluation on 4D Occupancy Forecasting We follow the experiment setting proposed in OccWorld [69], which predicts 6 future occupancy frames based on 4 past occupancy frames. mIoU and IoU are used to evaluate reconstruction and forecasting accuracy. We also compare inference speed and parameter count, both of which are important for real-world vehicle applications. As shown in Table 1, compared to the previous state-of-the-art method 2-World [32], our method achieves remarkable increase in forecasting performance, with 7.2% increase in mIoU and 4% increase in IoU. Moreover, our tri-plane VAE compresses occupancy into latent tri-plane that is only 58% the size used in previous methods, while still maintaining superior reconstruction performance. This compact latent representation also contributes to fast inference (41 FPS) and minimal parameter count of only 3.47M (inTable 1. Performance of 4D Occupancy forecasting. We compare our method with the most competitive methods on reconstruction, forecasting accuracy, inference speed and parameter count respectively. Our method achieves superior performance across all metrics."
        },
        {
            "title": "Input",
            "content": "Recon. mIoU (%) 2s 1s 3s Avg. Recon. IoU (%) 2s 1s 3s Avg. FPS Params"
        },
        {
            "title": "Occ\nOcc",
            "content": "OccWorld [69] OccSora [52] OccLLaMA [55] Occ & Text Occ & Text Occ-LLM [59] Occ & Camera DFIT [66] DOME [17] Occ UniScene [30] COME [41] 2-World [32]"
        },
        {
            "title": "Occ\nOcc",
            "content": "66.38 25.78 15.14 10.51 17.14 62.29 34.63 25.07 20.18 26.63 18.00 72.39 66.97 32.77 22.04 14.40 23.07 68.78 41.39 33.68 29.97 35.01 20.00 174.19 75.20 25.05 19.49 15.26 19.93 63.79 34.56 28.53 24.41 29.17 36.65 32.14 28.77 32.52 40.28 31.24 25.29 32.27 24.02 21.65 17.29 20.99 31.68 21.29 15.19 22.71 > 7 > 7 - - - - - - - - Occ & Box & Map 72.90 35.37 29.59 25.08 31.76 64.10 38.34 32.70 29.09 34.84 1.72 83.08 35.11 25.89 20.29 27.10 77.25 43.99 35.36 29.74 36.36 6.54 397.55 69.47 83.08 42.75 32.97 26.98 34.23 77.25 50.57 43.47 38.36 44.13 0.30 692.97 81.22 47.62 38.58 32.98 39.73 68.30 54.29 49.43 45.69 49.80 37.04 22.71 M"
        },
        {
            "title": "Occ",
            "content": "86.15 50.47 41.47 35.83 42.59 75.53 56.87 51.46 47.08 51.80 41."
        },
        {
            "title": "3.47 M",
            "content": "Table 2. Testing Longer Occupancy Forecasting. Without any additional training, we evaluate our method and comparison methods on 4s, 5s, and 6s occupancy forecasting. While others degrade sharply, our approach maintains strong performance even at 6s. Table 3. Ablation on end-to-end training, model design and representation. We also apply E2E training on other comparative methods to demonstrate not all methods benefit from E2E. CR represents continuous representation. Method OccWorld [17] DOME [17] UniScene [30] 2-World [32] 4s 7.79 23.50 16.44 28. mIoU (%) 6s 5s 6.29 19.37 16.41 25.32 5.76 17.27 15.61 22.55 Avg. 6.61 20.05 16.15 25.49 4s 17.23 13.84 14.31 41.75 IoU (%) 6s 5s 15.31 10.14 14.33 38.47 14.54 8.15 14.15 35.41 Avg. 15.69 10.71 14.26 38. Ours 31.16 27.17 23.66 27.33 42. 39.00 35.60 39.14 cluding the VAE and prediction module). Longer Occupancy Forecasting. Our video generation module takes the generated 4D occupancy as condition. To produce long driving videos, our occupancy world model must possess the ability to generate extended sequences of occupancy. Therefore, we further evaluate the forecasting results at 4 s, 5 s, and 6 s, comparing them with previous competitive methods. As shown in Table 2, the forecasting performance of OccWorld [69], DOME [17], and UniScene [30] sharply degrades as time increases, while our method maintains strong prediction accuracy. Our performance at 6 even surpasses that of most methods at 4 s, demonstrating that our approach can effectively forecast occupancy over longer horizons. Effectiveness of End-to-End Training. All previous methods in 4D occupancy generation train VQ-VAE/VAE and the subsequent AR/Diffusion model in isolation, meaning that the second stage of training operates solely in the latent space learned from the VAE. We argue that the latent representation learned from reconstruction may not be the optimal representation for subsequent occupancy generation or forecasting [29, 64]. To align the latent representation with the subsequent forecasting task, we perform end-to-end training, which jointly optimizes the parameters of the VAE and the forecasting module. Experimental results in Table 3 demonstrate that end-to-end training significantly enhances the forecasting performance of our Method DOME DOME + E2E 2-World 2-World + E2E w/o MCA w/o E2E w/o CR w/o CR & E2E mIoU (%) 3s 2s 25.89 0.40 38.58 10.10 37.86 38.61 38.04 36.90 20.29 0.24 32.98 9.99 30.48 32.96 32.70 19. 1s 35.11 0.66 47.62 10.18 48.54 47.81 46.54 49.50 Avg. 27.10 0.43 39.73 10.09 38.96 39.79 39.09 35. 1s 43.99 0.77 54.29 20.11 55.88 55.57 54.87 55.81 IoU (%) 3s 2s 35.36 0.46 49.43 20.34 48.35 49.97 49.82 47. 29.74 0.31 45.69 20.31 41.81 45.83 46.00 29.70 Avg. 36.36 0.51 49.80 20.25 48.68 50.46 50.23 44.36 Our Full Method 50.47 41.47 35.83 42.59 56.87 51. 47.08 51.80 model. We also perform end-to-end training on our comparison methods, specifically the state-of-the-art diffusionbased method DOME [17] and the autoregressive method 2-World [32]. As shown in Table 3, end-to-end training completely collapses the performance of DOME and significantly degrades the performance of 2-World. The collapse in DOME occurs primarily because the end-to-end training using diffusion loss encourages simpler latent space, which achieves lower training loss but ultimately degrades generation performance [29]. For the reduced performance of 2-World, we argue that the use of discrete representation and inter-scene encoding [32] contributes to this issue. To further validate our view, we implement variant of our method that adds vector quantization to our VAE, referred to as w/o Continuous Representation (CR). We observe that the performance of our methods with and without CR is very close prior to end-to-end training. However, the variant using discrete representation suffers performance drop after E2E training, while the other shows improvement. This result further demonstrates that using continuous representation is beneficial for subsequent E2E training. Ablation Study. To further demonstrate the effectiveness of our proposed Mutual Control Attention (MCA), we replace it with naive module in which the latent represenFigure 5. (a) Comparison with UniScene [30]. Our method generates longer driving videos while maintaining high quality. (b) Ablation Study. Removing normalization during fine-tuning results in noticeable grid artifacts and blurry outputs, while removing the MVA leads to multi-view inconsistent generation. (c) Driving Scenario Editing. With our two-stage generation method, edits such as removal or insertion can be easily applied to the occupancy, allowing for the generation of edited driving videos. Table 4. Quantitative Comparison of Multi-view Driving Video Generation. Our GenieDrive achieves outstanding performance across various generation lengths and metrics. Vista is multiview invariant proposed in [30]. Table 5. Ablation Study on Video Generation. We removed the Multi-View Attention and normalization modules separately and re-ran the fine-tuning. Omitting these modules degraded video generation quality, especially without the normalization module. Method Frames Cond. FVD mIoU mAP Panacea [56] Vista [16] MagicDrive [14] MagicDrive-V2 [15] WoVoGen [35] UniScene [30] GenieDrive-S UniScene-Rollout GenieDrive-M GenieDrive-L GenieDrive-L Rollout 8 25 60 241 6 8 8 32 37 81 241 BEV Action BEV & 3D Box BEV & 3D Box Occ Occ Occ Occ Occ Occ Occ 139.00 112.65 217.94 94.84 417.70 70.52 55.93 610.15 98.06 92.78 137.25 - - 18.27 20.40 - 21.75 31. 18.69 31.44 31.03 31.03 - - 11.86 18.17 - 10.32 21.23 6.24 19.84 18.89 18.89 tations of occupancy and control perform self-attention independently. As shown in Table 3, removing MCA leads to significant performance drop, particularly at 3 s. These results indicate that MCA effectively models the influence of action control on future occupancy and improves longhorizon occupancy prediction. 4.3. Evaluation on Multi-view Video Generation We train three scales of driving video generation models that differ only in video length: for 8 frames (0.7 s), for 37 frames (3 s), and for 81 frames (7 s). Through rollout, our model can generate 241-frame (20 s) multi-view driving video. Regarding generation quality, our GenieDrive shows significant improvement across all metrics compared to previous occupancy-based methods, as shown in Table 4. Moreover, our GenieDrive enables longer video generation, while previous occupancy-based methods are typically limited to several seconds, as shown in Figure 5 (a). Our approach demonstrates consistently competitive performance in short, medium, and long video generation among existing multi-view driving video generation methods. The mIoU and mAP of our method surpass Method FVD mIoU mAP w/o Normalized MVA 120.16 212.67 w/o Normalization Full method 98. 30.12 21.49 31.44 18.77 10.04 19.84 those of the previous state-of-the-art method, MagicDriveV2 [15], indicating that our generation results are better aligned with control and match the real driving video distribution. We further evaluate our method to demonstrate its capability for editable driving data synthesis, as well as the effectiveness of each proposed component. Editable Driving Data Synthesis. Introducing occupancy as an intermediate representation not only provides physics priors for video generation but also enables convenient editing of the driving scene in 3D or 4D space. As shown in Figure 5(c), we can easily remove or insert objects in occupancy space and then generate driving videos conditioned on the edited occupancy. This capability effectively supports the manipulation of specific elements, such as vehicles and barriers, in the generated videos, which is crucial for highly controllable driving data synthesis, including the creation of out-of-distribution driving scenarios. Ablation Study. Our occupancy-guided video generation model benefits from the Normalized Multi-View Attention, which helps stably learn multi-view relations in driving videos. To illustrate the effectiveness of the normalization and Multi-View Attention, we remove each component individually and evaluate the resulting video generation performance. As shown in Figure 5(b), removing the normalization during fine-tuning causes the model to produce videos with noticeable grid artifacts and blurry details. This degradation is further reflected by the significant performance drop in Table 5. Removing the Multi-View Attention leads to inconsistent multi-view generation, as illustrated in Figure 5(b), where the same vehicle exhibits varying appearances in different views. 5. Conclusion We introduce GenieDrive, driving world model that effectively addresses the limitations of existing methods in physics-aware driving video generation. By introducing 4D occupancy as an intermediate representation, we achieve highly controllable and physics-aware outputs. Extensive experiments demonstrate that GenieDrive provides physically plausible feedback from driving actions and achieves superior performance in 4D occupancy forecasting and driving video synthesis. We hope that GenieDrive, as powerful physics-aware driving world model, can further enhance closed-loop evaluation with its strong ability to generate visually and physically realistic driving videos."
        },
        {
            "title": "References",
            "content": "[1] Genie 3: new frontier for world models, 2025. 2 [2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv:1607.06450, 2016. 5 [3] Maxim Berman, Amal Rannen Triki, and Matthew Blaschko. The lovasz-softmax loss: tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In CVPR, 2018. 4 [4] Hengwei Bian, Lingdong Kong, Haozhe Xie, Liang Pan, Yu Qiao, and Ziwei Liu. Dynamiccity: Large-scale 4d occupancy generation from dynamic scenes. In ICLR, 2025. 2 [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv:2311.15127, 2023. 2 [6] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In CVPR, 2020. 2, 5, 3 [7] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. [8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022. 3 [9] Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, et al. Eccv 2024 w-coda: 1st workshop on multimodal perception and comprehension of corner cases in autonomous driving. arXiv:2507.01735, 2025. 5 [10] Yuntao Chen, Yuqi Wang, and Zhaoxiang Zhang. Drivinggpt: Unifying driving world modeling and planning with multi-modal autoregressive transformers. In ICCV, 2025. 2 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. 4 [12] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In CoRL, 2017. 2, 3 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 5 [14] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. In ICLR, 2024. 2, 8 [15] Ruiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhenguo Li, and Qiang Xu. Magicdrive-v2: High-resolution long video generation for autonomous driving with adaptive control. In ICCV, 2025. 2, 5, [16] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. In NeurIPS, 2024. 2, 5, 6, 8 [17] Songen Gu, Wei Yin, Bu Jin, Xiaoyang Guo, Junming Wang, Haodong Li, Qian Zhang, and Xiaoxiao Long. Dome: Taming diffusion model into high-fidelity controllable occupancy world model. arXiv:2410.10429, 2024. 2, 7, 1, 6 [18] Xianglong He, Chunli Peng, Zexiang Liu, Boyang Wang, Yifan Zhang, Qi Cui, Fei Kang, Biao Jiang, Mengyin An, Yangyang Ren, et al. Matrix-game 2.0: An opensource, real-time, and streaming interactive world model. arXiv:2508.13009, 2025. 2 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2, 3 [20] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In NeurIPS, 2022. 2 [21] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv:2309.17080, 2023. [22] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 3 [23] Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, and Ping Tan. Drivingworld: Constructing world model for autonomous driving via video gpt. arXiv:2412.19505, 2024. 2 [24] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 5 [25] Xiaosong Jia, Zhenjie Yang, Qifeng Li, Zhiyuan Zhang, and Junchi Yan. Bench2drive: Towards multi-ability benchmarking of closed-loop end-to-end autonomous driving. In NeurIPS, 2024. 2 [26] Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, et al. Towards learning-based planning: The nuplan benchmark for real-world autonomous driving. In ICRA, 2024. [27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. TOG, 2023. 5 [28] Seung Wook Kim, Jonah Philion, Antonio Torralba, and Sanja Fidler. Drivegan: Towards controllable high-quality neural simulation. In CVPR, 2021. 2 [29] Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning of latent diffusion transformers. In ICCV, 2025. 4, 7 [30] Bohan Li, Jiazhe Guo, Hongsi Liu, Yingshuang Zou, Yikang Ding, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, et al. Uniscene: Unified occupancy-centric driving scene generation. In CVPR, 2025. 3, 5, 7, 8 [31] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In ECCV, 2022. 5 [32] Zhimin Liao, Ping Wei, Ruijie Zhang, Shuaijia Chen, Haoxuan Wang, and Ziyang Ren. I2-world: Intra-inter tokenizaIn ICCV, tion for efficient dynamic 4d scene forecasting. 2025. 2, 3, 4, 6, 7, 1 [33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 2, [34] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, 5 [35] Jiachen Lu, Ze Huang, Zeyu Yang, Jiahui Zhang, and Li Zhang. Wovogen: World volume-aware diffusion for controllable multi-camera driving scene generation. In ECCV, 2024. 3, 8 [36] Yifan Lu, Xuanchi Ren, Jiawei Yang, Tianchang Shen, Zhangjie Wu, Jun Gao, Yue Wang, Siheng Chen, Mike Chen, Sanja Fidler, et al. Infinicube: Unbounded and controllable dynamic 3d driving scene generation with worldguided video models. In ICCV, 2025. 3 [37] Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, and Kaipeng Zhang. Yume: An interactive world generation model. arXiv:2507.17744, 2025. 2 [38] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv:2212.09748, 2022. 2, 3 [39] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv:2408.06070, 2024. 5 Gaia-2: controllable multi-view generative world model for autonomous driving. arXiv:2503.20523, 2025. 2 [41] Yining Shi, Kun Jiang, Qiang Meng, Ke Wang, Jiabao Wang, Wenchao Sun, Tuopu Wen, Mengmeng Yang, and Diange Yang. Come: Adding scene-centric forecasting control to occupancy world model. arXiv:2506.13260, 2025. 2, 7 [42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3 [43] Zhihang Song, Zimin He, Xingyu Li, Qiming Ma, Ruibo Ming, Zhiqi Mao, Huaxin Pei, Lihui Peng, Jianming Hu, Danya Yao, et al. Synthetic datasets for autonomous drivIEEE Transactions on Intelligent Vehicles, ing: survey. 2023. 2 [44] HunyuanWorld Team, Zhenwei Wang, Yuhao Liu, Junta Wu, Zixiao Gu, Haoyuan Wang, Xuhui Zuo, Tianyu Huang, Wenhuan Li, Sheng Zhang, et al. Hunyuanworld 1.0: Generating immersive, explorable, and interactive 3d worlds from words or pixels. arXiv:2507.21809, 2025. [45] Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, and Hang Zhao. Occ3d: large-scale 3d occupancy prediction benchmark for autonomous driving. In NeurIPS, 2023. 5, 1 [46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023. 3 [47] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. 5 [48] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. 2017. 3 [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 5 [50] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv:2503.20314, 2025. 2, [51] Jiawei Wang, Haowei Sun, Xintao Yan, Shuo Feng, Jun Gao, and Henry Liu. Terasim-world: Worldwide safetycritical data synthesis for end-to-end autonomous driving. arXiv:2509.13164, 2025. 2 [52] Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, and Jiwen Lu. Occsora: 4d occupancy generation models as world simulators for autonomous driving. arXiv:2405.20337, 2024. 2, 7 [53] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddrive world models for autonomous driving. In ECCV, 2024. 2 [54] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In CVPR, 2024. 2 [40] Lloyd Russell, Anthony Hu, Lorenzo Bertoni, George Fedoseev, Jamie Shotton, Elahe Arani, and Gianluca Corrado. [55] Julong Wei, Shanshuai Yuan, Pengfei Li, Qingda Hu, Occllama: An Zhongxue Gan, and Wenchao Ding. occupancy-language-action generative world model for autonomous driving. arXiv:2409.03272, 2024. 3, 7 occupancy world model for autonomous driving. In ECCV, 2024. 3, 6, 7, 1 [70] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. In ECCV, 2024. 2 [71] Yupeng Zheng, Pengxuan Yang, Zebin Xing, Qichao Zhang, Yuhang Zheng, Yinfeng Gao, Pengfei Li, Teng Zhang, Zhongpu Xia, Peng Jia, XianPeng Lang, and Dongbin Zhao. World4drive: End-to-end autonomous driving via intentionaware physical latent world model. In ICCV, 2025. 2 [72] Xin Zhou, Dingkang Liang, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan Zhang, Feiyang Tan, Hengshuang Zhao, and Xiang Bai. Hermes: unified self-driving world model for simultaneous 3d scene understanding and generation. arXiv:2501.14729, 2025. 2 [73] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In VIS, 2001. [56] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In CVPR, 2024. 8 [57] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. 5 [58] Mingxing Xu, Wenrui Dai, Chunmiao Liu, Xing Gao, Weiyao Lin, Guo-Jun Qi, and Hongkai Xiong. Spatialtemporal transformer networks for traffic flow forecasting. arXiv:2001.02908, 2020. 4 [59] Tianshuo Xu, Hao Lu, Xu Yan, Yingjie Cai, Bingbing Liu, and Yingcong Chen. Occ-llm: Enhancing autonomous driving with occupancy-based large language models. In ICRA, 2025. 3, 7 [60] Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, and Hao Zhao. Challenger: Affordable adversarial driving video generation. arXiv:2505.15880, 2025. 2 [61] Xuemeng Yang, Licheng Wen, Tiantian Wei, Yukai Ma, Jianbiao Mei, Xin Li, Wenjie Lei, Daocheng Fu, Pinlong Cai, Min Dou, et al. Drivearena: closed-loop generative simulation platform for autonomous driving. In ICCV, 2025. [62] Yu Yang, Jianbiao Mei, Yukai Ma, Siliang Du, Wenqing Chen, Yijie Qian, Yuxiang Feng, and Yong Liu. Driving in the occupancy world: Vision-centric 4d occupancy forecasting and planning via world models for autonomous driving. In AAAI, 2025. 2 [63] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2023. 2 [64] Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. 4, 7 [65] Deheng Ye, Fangyun Zhou, Jiacheng Lv, Jianqi Ma, Jun Zhang, Junyan Lv, Junyou Li, Minwen Deng, Mingyu Yang, Qiang Fu, et al. Yan: Foundational interactive video generation. arXiv:2508.08601, 2025. 2 [66] Haiming Zhang, Ying Xue, Xu Yan, Jiacheng Zhang, Weichao Qiu, Dongfeng Bai, Bingbing Liu, Shuguang Cui, and Zhen Li. An efficient occupancy world model via decoupled dynamic flow and image-assisted training. arXiv:2412.13772, 2024. 7 [67] Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiao-Xiao Long, et al. Epona: Autoregressive diffusion world model for autonomous driving. In ICCV, 2025. 2, 5, [68] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-enhanced world models for diverse driving video generation. arXiv:2403.06845, 2024. 2 [69] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning 3d GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Implementation Details of Tri-Plane VAE = = In our tri-plane VAE, we first apply 3D convolution filter gϕ to downsample the occupancy RHW into feature volume RhwdC, where = 4 and = 64. We then decompose into three latent planes: Zxy RhwC, Zyz RwdC, and Zxz RhdC. For the occupancy resolution in Occ3D-NuScenes [45], we have = 200, = 200, and = 16, resulting in R5050464, Zxy R505064, Zyz R50464, and Zxz R50464. To ensure that the tri-plane latent representation can be easily processed by the subsequent attention module, we concatenate the three latent planes into unified feature R505864, as shown in Figure 6. In contrast, previous works such as OccWorld [69], DOME [17], and 2-World [32] typically compress the occupancy into BEV feature of shape R5050128. Thus, our latent tri-plane representation occupies only 50 58 64 50 50 128 100% = 58% (16) of the size used in previous methods, while still achieving superior occupancy reconstruction performance (86.15 mIoU and 75.53 IoU). The superior performance and efficiency of our method primarily stem from the fact that our latent representation is more 3D-aware, rather than compressing the full 3D occupancy into 2D BEV feature as in previous BEV-based approaches. Our compact latent representation also greatly reduces the parameter count of the subsequent occupancy prediction module, enabling our occupancy world model to achieve state-of-the-art performance while using only 3.4M parameters. We train our triplane VAE for 210 epochs with dropout rate of 0.5. By epoch 140, the model already reaches 83.34 mIoU, and additional training further improves performance. The VAE can be trained very efficiently, requiring only 6 hours on 8 NVIDIA L40 GPUs. 7. Details of 4D Occupancy Forecasting We adopt an encoderdecoder design similar to that of 2World [32] for occupancy forecasting. In our encoder, we propose the Mutual Control Attention (MCA), which iteratively injects information between the occupancy latent (a tri-plane) and the control latent. We then apply shallow MLP, referred to as the transformation head, to transform the control latent into representation used for intermediate supervision, as described in Eq. 8 of the main subFigure 6. Concatenated Tri-Plane Feature. To make tri-plane representation more suitable for the following processing, we concatenate three planes to get unified feature representation. mission. We also fuse the latent tri-plane with this transformed latent using cross-attention. This fusion design allows different regions of the tri-plane latent to be influenced by the transformed control latent to varying degrees. The fused latent tri-plane is then fed into the subsequent spatial temporal blocks. More precisely, the spatial component corresponds to self-attention applied within the latent triplane itself, while the temporal component concatenates the previous latent tri-planes with the current one along the channel dimension, followed by an MLP that projects the concatenated channels from (k + 1)C back to C. The output latent tri-plane is passed to the tri-plane VAE decoder to obtain the predicted occupancy for the next timestep. 8. Qualitative Comparison on Forecasting We also provide qualitative comparison with previous methods, including OccWorld [69], DOME [17], and 2World [32], in Figure 8. We use bounding boxes to highlight the main differences between each method and the ground truth. Two scene examples are shown in Figure 8. For the first scene, OccWorld produces an unreasonable road surface, DOME diminishes vehicles in its forecasting results, and 2-World predicts inaccurate occupancy compared to the ground truth. In contrast, our method provides physically reasonable forecasts, particularly in capturing the dynamics of driving vehicles (blue and yellow occupancy). We further highlight that only our method consistently predicts pedestrians (red occupancy) across different timesteps, whereas the comparative methods tend to lose this detail as the timestep increases. In the second scene, the methods need to forecast the driving behavior of the truck (purple occupancy). OccWorld gradually deforms the truck, resulting in an unnatural shape in later predictions. In DOMEs Table 6. Reconstruction and Forecasting Performance Change in End-to-End Training. denotes reconstruction and denotes forecasting. As the number of training epochs increases, forecasting performance gradually improves, whereas reconstruction performance decreases. Epoch R. mIoU R. IoU F. mIoU F. IoU 0 4 8 12 16 20 24 86.15 73.89 73.05 71.31 70.07 68.31 67. 75.53 66.39 65.06 63.65 63.13 62.32 62.33 39.79 41.64 42.36 42.53 42.59 42.49 42.43 50.46 50.71 51.47 51.71 51.80 51.76 51.80 prediction, the truck eventually vanishes. 2-World fails to model reasonable driving trajectory and ultimately produces an unrealistically elongated truck. In contrast, our method predicts the correct driving behavior and maintains consistent truck geometry across timesteps. It is also important to note that, in our prediction results, the right-turn behavior of the following truck is reasonable, as the previous observations do not provide sufficient guidance for predicting straight trajectory. Figure 7. Comparison of Video Generation Efficiency. We compare our method with previous approaches in terms of training time, number of GPUs used for training, inference time, and VRAM consumption during inference. Table 7. Comparison of Generation Resolutions. Vista and Epona can generate only single-view driving videos, whereas MagicDrive-V2 and our method can generate multi-view videos. 9. End-to-End Training of Occupancy World Method Vista [16] Epona [67] MagicDrive-V2 [15] Ours Resolution 576 1024 512 1024 6 848 6"
        },
        {
            "title": "Model",
            "content": "At first, we freeze the tri-plane VAE and train the prediction module for 48 epochs. We then unfreeze all parameters of both the VAE and the prediction module and perform endto-end training for an additional 24 epochs. During end-toend (E2E) training, we observe that the forecasting performance increases while the reconstruction performance decreases. The detailed performance changes during end-toend training are reported in Table 6. As shown in the table, forecasting accuracy reaches its peak at epoch 16 and then To further begins to degrade, mainly due to overfitting. illustrate the effect of E2E training, we provide qualitative comparison in Figure 9. As shown in the figure, after E2E training, our method can accurately forecast pedestrians, whereas the variant without E2E training often removes pedestrians in its predictions. Moreover, with E2E training, our occupancy world model better preserves the consistency of roadway features during forecasting. Additionally, E2E training helps the model predict vehicle dynamics more precisely, while the variant without E2E tends to cause vehicles to vanish in the prediction results. These results demonstrate that E2E training contributes to better detail preservation and more accurate forecasting. To illustrate the effect of end-to-end training on the comparison methods DOME [17] and 2-World, we present their forecasting results before and after E2E training in Figure 10. As shown, E2E training significantly degrades the forecasting performance of DOME, while 2-World shows no improvement and also experiences decline in prediction accuracy. 10. Efficiency of Video Generation Previous video-based driving world models or driving video generation models are typically trained from scratch, which is computationally expensive. We summarize the training and inference efficiency of different methods in Figure 7. As shown in the figure, previous works such as Vista [16], Epona [67], and MagicDrive-V2 [15] require large number of GPUs (32128) and long training periods (1921080 h). In contrast, we leverage pretrained video generation models to reduce training cost. With our twostage generation framework and fine-tuning strategy, the total training time is reduced to one week (3 days for training the first-stage occupancy world model and 4 days for fine-tuning the video model) using only 8 GPUs. Moreover, our method also achieves superior inference efficiency: the average generation speed reaches 4.36 per frame, and the VRAM consumption is only 11.72 GB. MagicDrive-V2 requires 39.76 GB of VRAM by performing sequence parallelism across 8 GPUs, whereas other methods, including ours, can perform inference on single GPU. We also list the generation resolution of each method in Table 7. Vista and Epona generate only single-view videos, while MagicDrive-V2 and our method support six-view video generation. 11. More Video Generation Results 11.1. Long Video Generation Our GenieDrive-L produces 81-frame multi-view driving videos, and by applying the rollout operation, it can further generate 241-frame (20s) sequencesthe longest video length in the NuScenes [6] dataset. We provide two representative samples in Figure 11. As shown, even after two rollouts, our method consistently maintains high generation quality and strong multi-view coherence in both daytime and nighttime scenarios. 11.2. Driving Scenario Editing By editing the occupancy and then generating driving videos guided by the edited occupancy, our method can easily remove or insert objects within driving scenes. To illustrate how our method performs scene editing, we compare the original video with the corresponding edited version in Figure 12. As shown, our method gradually removes the car in the first scene and naturally inserts truck onto the roadway in the second scene. The edited results appear natural and reasonable, maintaining both spatial and temporal consistency in the generated driving videos. These results further demonstrate that our method enables effective and realistic editing of driving scenarios. This convenient and controllable scene editing capability can greatly enhance out-of-distribution driving data generation. 11.3. Sim-to-Real Driving Scenario Generation The sim-to-real gap is largely caused by the unrealistic rendering quality of the simulator. However, there is no obvious discrepancy between synthetic occupancy and realworld occupancy. Therefore, we leverage occupancy from the CARLA simulator [12] and use our method to transfer the synthetic occupancy into realistic multi-view driving videos. As shown in Figure 13, we visualize the original simulated driving scenes alongside the corresponding simto-real results produced by our method. The results show that GenieDrive can accurately capture the driving behavior in simulation and generate corresponding realistic outcomes in real-world scenarios. Moreover, our method preserves fine details from the synthetic scenes, such as surrounding vegetation and vehicles. This capability can substantially enhance the realism of synthetic data, thereby further mitigating the sim-to-real gap. Figure 8. Qualitative Comparison of 4D Occupancy Forecasting. We highlight the differences using bounding boxes. Previous methods tend to produce unreasonable predictions or miss important details such as pedestrians. In contrast, our method generates physically reasonable results while preserving the detailed structures of the driving scene. Figure 9. Qualitative Comparison Before and After End-to-End Training. We highlight the differences using bounding boxes. The visualization results show that end-to-end training helps our occupancy world model forecast pedestrians, cars, trucks, trailers, and roadway features more accurately. In contrast, the variant without end-to-end training tends to lose these details in its predictions. Figure 10. Effect of End-to-End Training on the Comparison Methods. We visualize the impact of end-to-end (E2E) training on the comparison methods DOME [17] and 2-World [32] by presenting the ground truth along with their predictions before and after E2E training. For DOME, the forecasting capability completely breaks down after E2E training. For 2-World, E2E training fails to produce more accurate forecasts and further leads to noticeable loss of scene details. Figure 11. Long Video Generation Examples. We provide two examples of our generated 20-second multi-view driving videos: one captured under daytime conditions and the other at night. Our method maintains both generation quality and multi-view consistency even over such long 20-second sequences. Figure 12. Driving Scenes Editing. We visualize the editing process applied to driving videos. Both Removal and Insertion operations take effect progressively over time. Compared with the original video, the edited results demonstrate that our method can effectively remove and insert objects within driving scenes. Figure 13. Sim-to-Real Generation. The left side shows the BEV map of simulated driving scenes in the CARLA simulator. Our method possesses the ability to transform these simulated scenarios into realistic multi-view driving videos. The visualization results demonstrate that our method not only generates accurate ego-vehicle behaviors, such as left turns and overtaking, but also preserves important scene details, including surrounding vehicles highlighted with red boxes."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "Huazhong University of Science and Technology",
        "The University of Hong Kong"
    ]
}